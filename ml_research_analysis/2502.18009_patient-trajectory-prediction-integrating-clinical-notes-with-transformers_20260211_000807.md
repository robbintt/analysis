---
ver: rpa2
title: 'Patient Trajectory Prediction: Integrating Clinical Notes with Transformers'
arxiv_id: '2502.18009'
source_url: https://arxiv.org/abs/2502.18009
tags:
- clinical
- data
- medical
- notes
- codes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of predicting patient disease
  trajectories from electronic health records (EHRs) by integrating clinical notes
  with transformers. The authors propose incorporating clinical note embeddings into
  transformer-based models, which traditionally rely solely on structured medical
  codes.
---

# Patient Trajectory Prediction: Integrating Clinical Notes with Transformers

## Quick Facts
- **arXiv ID:** 2502.18009
- **Source URL:** https://arxiv.org/abs/2502.18009
- **Reference count:** 7
- **Primary result:** Clinical Mosaic (transformer + clinical notes) outperforms baselines on MIMIC-IV, showing gains in MAP and MAR metrics.

## Executive Summary
This paper tackles the challenge of predicting patient disease trajectories from electronic health records (EHRs) by integrating clinical notes with transformer models. The authors propose incorporating clinical note embeddings into transformer-based models, which traditionally rely solely on structured medical codes. This approach aims to enrich the representation of patients' medical histories by leveraging both structured data (e.g., ICD codes) and unstructured clinical notes. Experiments on MIMIC-IV datasets demonstrate that the proposed method, Clinical Mosaic, outperforms traditional models relying solely on structured data, highlighting the potential of utilizing unstructured medical information to improve predictive modeling in healthcare.

## Method Summary
The authors propose a transformer-based model, Clinical Mosaic, that integrates clinical note embeddings with structured medical codes. They pretrain a BERT-style model on MIMIC-IV clinical notes, then extract embeddings from the last six layers and fuse them with structured CCS code embeddings at the first encoder layer. The model is trained on MIMIC-IV data with a 5-fold cross-validation setup to predict next-visit diagnoses. Three embedding fusion strategies are compared: Mean Pooling, Layer-wise Averaging (Concat), and Projection-based Compression.

## Key Results
- Clinical Mosaic outperforms baselines like Doctor AI, LIG-Doctor, and Clinical GAN on MIMIC-IV.
- The concatenation strategy for embedding integration shows significant improvements in mean average precision (MAP) and mean average recall (MAR) metrics.
- Integrating clinical notes with structured codes provides complementary information that enhances predictive performance.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating unstructured clinical notes recovers predictive signals lost in structured coding systems (e.g., ICD/CCS).
- **Mechanism:** Clinical notes contain nuance and medical reasoning omitted by standardized codes. By embedding these notes and concatenating them with structured codes, the model enriches the input representation, allowing the attention mechanism to weigh "reasoning" alongside "diagnosis."
- **Core assumption:** The information gain from unstructured text provides a differential signal for the *next* diagnosis that structured codes alone cannot approximate.
- **Evidence anchors:**
  - [abstract] "Enriches the representation of patients' medical histories by leveraging both structured data... and unstructured clinical notes."
  - [Section 1] "Current coding systems... often do not fully capture the richness of information contained in clinical notes."
  - [corpus] *Weak/General.* Neighbor papers like **CURENet** and **MoMA** support the general utility of multimodal EHR integration but do not validate this specific fusion method.
- **Break condition:** If the dataset's clinical notes are low-quality, template-based, or redundantly summarize the structured codes (information gain $\approx$ 0).

### Mechanism 2
- **Claim:** Aggregating embeddings from the top layers of the language model creates a more robust representation than using only the final layer.
- **Mechanism:** Different transformer layers capture different levels of abstraction. Averaging the last 6 layers (as done in Clinical Mosaic) acts as an ensemble, smoothing out layer-specific noise while preserving semantic richness across visits.
- **Core assumption:** Critical predictive features are distributed across the upper layers of the BERT-style model rather than isolated in the final output layer.
- **Evidence anchors:**
  - [Section 3.3] "Inspired by prior work... which demonstrated the benefits of aggregating multiple layers... we aggregated representations from the last six layers."
  - [

## Foundational Learning

### Clinical Code Standardization
- **Why needed:** Structured EHR data uses standardized coding systems (e.g., ICD-10, CCS) to ensure interoperability and consistency across healthcare providers and systems.
- **Quick check:** Verify that ICD-10 codes are correctly mapped to CCSR categories and infrequent codes are filtered out based on occurrence thresholds.

### Embedding Aggregation from Multiple Transformer Layers
- **Why needed:** Different transformer layers capture different levels of semantic abstraction; aggregating multiple layers can provide a richer, more robust representation.
- **Quick check:** Confirm that embeddings are extracted from the last six layers and appropriately aggregated (e.g., averaging or concatenation) to preserve temporal and semantic information.

### Multimodal EHR Integration
- **Why needed:** Combining structured data (e.g., diagnoses, procedures) with unstructured data (e.g., clinical notes) provides a more complete patient representation, capturing both coded events and clinical reasoning.
- **Quick check:** Ensure that clinical note embeddings are properly fused with structured code embeddings at the first encoder layer, allowing the attention mechanism to interact with both modalities.

## Architecture Onboarding

### Component Map
- **Clinical Mosaic (Pretrained BERT)** -> **Embedding Extraction** -> **Structured CCS Embeddings** -> **Fusion Layer** -> **Encoder-Decoder Transformer** -> **Next-Visit Diagnosis Prediction**

### Critical Path
1. Load and preprocess MIMIC-IV data (filter patients, map codes, clean notes).
2. Generate clinical note embeddings using Clinical Mosaic (last 6 layers).
3. Fuse note embeddings with structured CCS embeddings at the first encoder layer.
4. Train encoder-decoder Transformer with 5-fold cross-validation.
5. Evaluate performance using MAP and MAR metrics.

### Design Tradeoffs
- **Layer-wise Averaging (Concat) vs. Mean Pooling:** Concat preserves temporal structure but increases dimensionality; Mean Pooling reduces dimensionality but may lose critical information.
- **Pretrained vs. From-scratch Embedding Models:** Pretrained Clinical Mosaic leverages existing knowledge but may not be optimal for the specific EHR task; from-scratch training could be more tailored but requires more data and compute.

### Failure Signatures
- **Low MAR with Mean Pooling:** Indicates excessive compression and loss of temporal information; switch to Layer-wise Averaging (Concat).
- **Training Instability:** If the model fails to converge, ensure clinical embeddings are injected before the attention block to allow proper interaction with structured codes.

### 3 First Experiments
1. **Baseline Comparison:** Implement and evaluate Doctor AI, LIG-Doctor, and Clinical GAN on MIMIC-IV to establish performance baselines.
2. **Embedding Fusion Ablation:** Compare Mean Pooling, Layer-wise Averaging (Concat), and Projection-based Compression strategies to determine the optimal fusion method.
3. **Layer Aggregation Impact:** Test the effect of using different numbers of transformer layers (e.g., last 3 vs. last 6) on predictive performance to validate the choice of aggregating the last 6 layers.

## Open Questions the Paper Calls Out
None

## Limitations
- **Hyperparameter completeness:** The paper does not specify final model training parameters (learning rate, batch size, optimizer, epochs) for the proposed Trajectory Transformer, relying only on pretrained Clinical Mosaic settings.
- **Generalizability beyond MIMIC-IV:** Performance claims are based solely on MIMIC-IV data; effectiveness on other EHR systems or international datasets is unverified.
- **Note quality assumptions:** The analysis assumes clinical notes contain complementary, high-quality information beyond structured codes. If notes are templated or redundant, the expected information gain may not materialize.

## Confidence

### High confidence
- In the *feasibility* of integrating clinical note embeddings with structured data in a transformer architecture, supported by established BERT-style pretraining and proven transformer models.

### Medium confidence
- In the *effectiveness* of the Layer-wise Averaging (Concat) strategy over Mean Pooling, as the improvement is demonstrated but dependent on data quality and structure.
- In the *superiority* claim over baselines (Doctor AI, LIG-Doctor, Clinical GAN), given the results are specific to MIMIC-IV and lack ablation studies isolating the contribution of each modality.

## Next Checks
1. **Replicate with full hyperparameters:** Obtain and apply the complete training configuration for the final Trajectory Transformer model to ensure reproducibility.
2. **External dataset validation:** Test the model on a non-MIMIC EHR dataset to assess generalizability and robustness across different clinical documentation styles.
3. **Ablation on note quality:** Evaluate model performance on subsets of clinical notes with varying levels of detail and redundancy to quantify the true information gain from unstructured data.