---
ver: rpa2
title: 'Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in
  LLM as a Communicator (LAAC) Framework in Multiple Application Domains'
arxiv_id: '2511.04184'
source_url: https://arxiv.org/abs/2511.04184
tags:
- information
- communication
- knowledge
- agent
- laac
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper evaluates trustworthiness dimensions for LLM-mediated
  communication through the LAAC (LLM as a Communicator) framework, which positions
  LLMs as communication intermediaries rather than content generators. The framework
  employs a three-agent architecture (Interview, Extraction, Query) to capture sender
  intent, structure knowledge, and facilitate recipient interaction.
---

# Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains

## Quick Facts
- arXiv ID: 2511.04184
- Source URL: https://arxiv.org/abs/2511.04184
- Authors: Mohammed Musthafa Rafi; Adarsh Krishnamurthy; Aditya Balu
- Reference count: 20
- Key outcome: Evaluates trustworthiness dimensions for LLM-mediated communication, revealing critical gaps in Information Capture Fidelity (60-70% semantic accuracy), Reproducibility (structural variability), and Query Response Integrity (31% hallucination rate).

## Executive Summary
This position paper introduces the LAAC (LLM as a Communicator) framework, which positions LLMs as communication intermediaries rather than content generators. Through a three-agent architecture (Interview, Extraction, Query), the framework captures sender intent, structures knowledge, and facilitates recipient interaction. Systematic evaluation across academic paper communication reveals significant trustworthiness gaps: Information Capture Fidelity shows 60-70% semantic accuracy for detailed content, Reproducibility demonstrates significant structural variability across extractions, and Query Response Integrity exhibits 31% hallucination rate for unanswerable questions. The paper identifies critical technical challenges in fidelity verification, uncertainty quantification, and hallucination prevention that must be addressed before reliable deployment of LLM communication intermediaries in high-stakes domains.

## Method Summary
The LAAC framework employs a three-agent pipeline using Claude API to process communication: Interview Agent conducts structured dialogue with senders to extract intent, Extraction Agent transforms transcripts into hierarchical JSON knowledge structures, and Query Agent answers recipient questions from the structured knowledge. The system was evaluated using five published computer science papers as ground truth, with 50 test questions spanning answerable, inference-requiring, and unanswerable categories. Performance metrics included Content Coverage (%), Semantic Accuracy (human-evaluated %), Information Addition/Omission rates, Structural Similarity, Hallucination Rate (%), Citation Accuracy, and Uncertainty Calibration. The backend uses Node.js with PostgreSQL for accounts and metadata, while the frontend is a React application with separate Author and Reviewer dashboards.

## Key Results
- Information Capture Fidelity: 60-70% semantic accuracy for detailed content, with specific numbers often becoming generic
- Query Response Integrity: 31% hallucination rate for unanswerable questions, where Query Agent fabricated plausible responses rather than acknowledging knowledge gaps
- Reproducibility: Significant structural variability across 10 repeated extractions of the same transcript, with detail inconsistency and content appearing/disappearing between runs

## Why This Works (Mechanism)

### Mechanism 1: Structured Information Capture via Multi-Agent Decomposition
The framework decomposes communication mediation into specialized agent roles (Interview, Extraction, Query) to create clear responsibility boundaries. This specialization aims to improve information fidelity by limiting each agent's scope and reducing error propagation compared to single-model processing. The mechanism works by having the Interview Agent extract intent through dialogue (avoiding assumptions), the Extraction Agent transform transcripts into structured knowledge (creating a canonical source), and the Query Agent retrieve from this structure (constraining responses to grounded content).

### Mechanism 2: Grounding Query Responses in an Explicit Knowledge Structure
The knowledge structure serves as an authoritative source that constrains Query Agent responses. By requiring queries to be answered through retrieval and presentation from this bounded set, the mechanism prevents fabrication. This approach works by establishing a pre-extracted, sender-verified knowledge structure that Query Agents must reference rather than generating responses from scratch.

### Mechanism 3: Sender-as-Ground-Truth Verification Loop
The framework requires sender review of the extracted knowledge structure before recipient access, providing a human-in-the-loop safety mechanism. This works by positioning the sender as the final authority on whether the structure represents their intent, with the expectation that senders will notice and correct omissions and distortions during the review process.

## Foundational Learning

- Concept: **Hallucination in LLMs**
  - Why needed here: The 31% hallucination rate on unanswerable questions is central to the trustworthiness problem.
  - Quick check question: Can you explain the difference between extrinsic hallucination (fabricating facts not in source) and intrinsic hallucination (contradicting the source)?

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: LAAC's Query Agent constrains generation to a knowledge base; understanding RAG tradeoffs informs design.
  - Quick check question: What happens to RAG system accuracy when the retrieval step fails to surface the most relevant passage?

- Concept: **Multi-Agent LLM Coordination**
  - Why needed here: LAAC uses a three-agent architecture; understanding role definition and coordination failures is essential.
  - Quick check question: In a multi-agent system, if Agent A's output is ambiguous, what are two strategies Agent B could use to handle it?

## Architecture Onboarding

- Component map: Sender → Interview Agent → transcript → Extraction Agent → knowledge structure → sender review → Query Agent → recipient response
- Critical path: 1. Sender → Interview Agent → transcript; 2. Extraction Agent → knowledge structure; 3. Sender reviews/approves structure (FAILURE POINT); 4. Recipient → Query Agent → response
- Design tradeoffs:
  - Temperature: Low (0.1–0.3) = reproducible but may miss nuance; High (0.7–1.0) = detailed but variable
  - Automation vs. Human-in-the-Loop: Automation is faster; human review adds latency but may catch errors
  - Domain-Specific vs. General Prompts: Specialized prompts improve extraction in known domains but require retuning for new ones
- Failure signatures:
  - Information loss: Specific numbers become generic ("94.3%" → "high accuracy")
  - Reproducibility failure: Same transcript yields different structures across runs
  - Hallucination: Query Agent answers unanswerable questions, invents citations, conflates sources
  - Overconfidence: Rarely expresses uncertainty even when extrapolating
- First 3 experiments:
  1. Hallucination Baseline: Construct test set with known answerable/unanswerable questions. Measure hallucination rate by category.
  2. Temperature Sweep: Run same transcript through Extraction Agent at temperatures [0.1, 0.3, 0.5, 0.7, 1.0]. Measure structural consistency and information retention.
  3. Sender Verification Efficacy: Intentionally inject errors into knowledge structures. Measure sender detection rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can retrieval-augmented extraction with explicit provenance tracking reduce hallucination rates below a threshold suitable for high-stakes communication?
- Basis in paper: Section 7.2 lists "retrieval-augmented extraction that grounds knowledge structures in interview transcripts" and Section 6.5 calls for "provenance tracking" as needed mechanisms.
- Why unresolved: Current 31% hallucination rate for unanswerable questions indicates Query Agents fabricate plausible responses rather than acknowledging knowledge gaps. No provenance mechanism exists to trace responses back to source statements.
- What evidence would resolve it: Implementation of retrieval-augmented Query Agent with provenance links, evaluated on the same question set, demonstrating significantly reduced hallucination rate with statistical significance.

### Open Question 2
- Question: Does multi-model consensus improve Extraction Agent reproducibility without sacrificing detail capture?
- Basis in paper: Section 7.2 proposes "multi-model consensus where multiple LLMs independently extract knowledge and their outputs are reconciled" as a future research direction.
- Why unresolved: Section 6.2 shows detail inconsistency (specific claims appeared/disappeared across runs) and structural variability in 10 repeated extractions. Temperature tuning creates a tradeoff between consistency and nuance capture.
- What evidence would resolve it: Systematic comparison of single-model vs. multi-model consensus extraction across multiple interview transcripts, measuring structural similarity, semantic equivalence, and detail consistency metrics.

### Open Question 3
- Question: How do user trust perceptions correlate with the proposed fidelity, reproducibility, and integrity metrics?
- Basis in paper: Section 7.1 states "Our evaluation metrics... have not been validated against user trust perceptions or real-world deployment outcomes. The relationship between measured fidelity and perceived trustworthiness requires investigation."
- Why unresolved: Automated metrics (content coverage, hallucination rate) may not capture what users actually consider trustworthy. A system could score well on metrics but still feel unreliable to users.
- What evidence would resolve it: User studies correlating measured trustworthiness scores with user-reported trust, confidence, and willingness to rely on LAAC-mediated communication in realistic scenarios.

## Limitations
- The framework's reliance on sender verification lacks empirical validation for complex structured knowledge representations
- Agent prompts, knowledge structure schemas, and evaluation metric calculations are unspecified, preventing direct reproduction
- Performance on non-academic domains (healthcare, legal) remains theoretical with unclear transferability of findings

## Confidence
- High Confidence: The existence of trust gaps in LLM-mediated communication (hallucination rates, information loss, structural variability) is well-supported by empirical measurements in the academic paper domain
- Medium Confidence: The three-agent architecture provides a useful conceptual framework for decomposing communication mediation tasks, though its superiority over alternative designs lacks comparative evaluation
- Low Confidence: The scalability and effectiveness of sender verification mechanisms across complex domains, and the transferability of findings from academic papers to high-stakes applications like healthcare and legal communication

## Next Checks
1. **Sender Verification Efficacy Test**: Conduct controlled experiments where knowledge structures with known, systematically injected errors are presented to human senders. Measure detection rates, false positive rates, and time-to-review across different error types and structure complexities to quantify the reliability of human-in-the-loop verification.

2. **Domain Transfer Validation**: Implement the LAAC framework for one high-stakes domain (e.g., healthcare clinical notes or legal case summaries). Compare performance metrics (fidelity, hallucination, reproducibility) against academic paper results to identify domain-specific challenges and adaptation requirements.

3. **Multi-Agent Architecture Comparison**: Design controlled experiments comparing the three-agent LAAC architecture against alternative configurations: (a) single LLM performing all functions, (b) two-agent variants (Interview+Query vs. Extraction+Query), (c) RAG-based approaches without interview phase. Measure relative performance on the same evaluation metrics to determine if task decomposition actually improves trustworthiness outcomes.