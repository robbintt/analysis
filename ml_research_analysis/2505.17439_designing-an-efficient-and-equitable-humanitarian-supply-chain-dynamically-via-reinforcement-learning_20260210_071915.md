---
ver: rpa2
title: Designing an efficient and equitable humanitarian supply chain dynamically
  via reinforcement learning
arxiv_id: '2505.17439'
source_url: https://arxiv.org/abs/2505.17439
tags:
- average
- demand
- reward
- runs
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applies Proximal Policy Optimization (PPO) to dynamically
  design an efficient and equitable humanitarian supply chain, addressing the need
  for real-time resource allocation under fluctuating demands. A cost-efficiency framework
  is embedded in the reward function, guiding the agent to maximize distribution efficiency
  while minimizing costs.
---

# Designing an efficient and equitable humanitarian supply chain dynamically via reinforcement learning

## Quick Facts
- arXiv ID: 2505.17439
- Source URL: https://arxiv.org/abs/2505.17439
- Reference count: 0
- This study applies Proximal Policy Optimization (PPO) to dynamically design an efficient and equitable humanitarian supply chain, addressing the need for real-time resource allocation under fluctuating demands.

## Executive Summary
This study addresses the challenge of dynamic humanitarian supply chain design through a Proximal Policy Optimization (PPO) approach. The model learns to activate collection centers and warehouses while allocating resources to demand points under fluctuating stochastic demand conditions. A cost-efficiency ratio embedded in the reward function guides the agent to maximize distribution efficiency while minimizing costs. The PPO agent demonstrates superior performance compared to traditional heuristic algorithms like NSGA-II and PSO, achieving higher satisfaction rates and better equity in resource distribution across various demand scenarios.

## Method Summary
The method employs PPO with a hybrid binary and discrete action space to control facility activation and distribution decisions in a humanitarian supply chain network. The state space includes demand at each point, warehouse inventory levels, previous allocations, and time. The reward function uses a log-transformed efficiency-to-cost ratio where efficiency is monetized as satisfied demand times market value. The agent learns through episodes of 100 timesteps, with demand generated via GBM, Poisson, or Merton models. Key mechanisms include Dirichlet-based allocation from collection centers to warehouses, demand-scaled distribution to prevent over-allocation, and clipped surrogate objectives to stabilize learning.

## Key Results
- PPO consistently improved average satisfaction rate and efficiency across stochastic demand simulations (GBM, Poisson, Merton)
- The model outperformed traditional heuristic algorithms like NSGA-II and PSO in dynamic humanitarian logistics
- PPO achieved higher satisfaction rates even at higher costs, demonstrating superior adaptability and decision-making
- Sensitivity tests confirmed PPO's robustness under diverse operational parameters including supply constraints and distance variations

## Why This Works (Mechanism)

### Mechanism 1: Cost-Efficiency Ratio in Reward Function
The log-transformed ratio reward $R_t = \log(\frac{1+f_{efficiency}(t)}{1+f_{cost}(t)})$ enables the agent to jointly optimize distribution efficiency and cost control. Efficiency is monetized as $(d_p^t - u_p^t) \times V$ while cost captures establishment, transportation, and penalties. The ratio structure means the agent learns to increase satisfaction even when costs rise, as long as the efficiency gain justifies it.

### Mechanism 2: PPO Clipping Stabilizes Learning in Hybrid Binary-Discrete Action Spaces
The clipped surrogate objective prevents destabilizing policy updates while the agent learns to activate facilities (binary) and determine distribution amounts (discrete) simultaneously. The clipping factor $\epsilon$ bounds the policy ratio to $[1-\epsilon, 1+\epsilon]$, preventing large gradient steps that could catastrophically alter facility configurations.

### Mechanism 3: Demand-Scaled Distribution Enforces Equity Constraint
Scaling warehouse-to-demand-point distributions by $\min(1, \frac{d_p^t}{\sum_w A_{wp}^t + \epsilon})$ prevents over-allocation to easily-reached demand points. By capping distributions at actual demand levels, each demand point receives proportional allocation regardless of its proximity to warehouses or inventory abundance.

## Foundational Learning

### Concept: Partially Observable Markov Decision Processes (POMDPs)
- **Why needed here:** The state $s_t = \{d_p^t, I_w^t, A_{wp}^{t-1}, t\}$ excludes underlying demand dynamics (drift, volatility, jump processes), meaning the agent must infer latent structure from observations.
- **Quick check question:** If demand has hidden seasonal patterns not captured in $s_t$, would the PPO agent learn to anticipate them, or would it always react one step behind?

### Concept: Advantage Functions in Policy Gradient Methods
- **Why needed here:** PPO uses $A_t$ to measure how much better an action is than the expected action under the current policy, which determines the direction and magnitude of policy updates.
- **Quick check question:** If the reward signal is noisy (high variance due to stochastic demand), what effect would this have on advantage estimation and subsequent policy stability?

### Concept: Pareto Frontiers in Multi-Objective Optimization
- **Why needed here:** The paper compares against NSGA-II, which generates a Pareto front of solutions trading off efficiency vs. cost; understanding this helps interpret why PPO's single-objective approach achieves higher satisfaction.
- **Quick check question:** Why might a single reward function that combines objectives outperform selecting from a Pareto front in a dynamic environment?

## Architecture Onboarding

### Component Map:
```
Environment Layer
├── DemandSimulator: GBM / Poisson / Merton stochastic generators
├── FacilityNetwork: 15 collection centers → 5 warehouses → 10 demand points
└── InventoryTracker: I_w^t updates via Equation 18

Agent Layer (PPO)
├── PolicyNetwork π_θ
│   ├── BinaryActionHeads: x_c^t (collection center activation), x_w^t (warehouse activation)
│   └── DiscreteActionHeads: A_cw^t, A_wp^t (distribution amounts)
├── ValueNetwork V_θ: Estimates expected return for advantage calculation
└── ExperienceBuffer: Stores (s_t, a_t, R_t, s_{t+1}) tuples

Training Loop
├── CollectTrajectories: Run episodes, observe states, sample actions
├── ComputeAdvantages: A_t = R_t + γV(s_{t+1}) - V(s_t)
└── UpdatePolicy: Apply clipped surrogate objective (Equation 2)
```

### Critical Path:
1. **Initialize parameters** per Appendix A (15 collection centers, 5 warehouses, 10 demand points, 100 time steps).
2. **Generate demand** using default GBM with μ=0.02, σ=0.1, initial demand 1200-2000.
3. **For each episode:** Agent observes $s_t$ → samples facility activations → computes distributions via Dirichlet (Equation 16) and uniform sampling (Equation 17) → applies demand scaling (Equation 20) → computes reward (Equation 4).
4. **Validate convergence:** Check that average satisfaction rate increases from ~0.85 to >0.95, reward improves by ~20%+ from first to last 10 episodes.

### Design Tradeoffs:
- **Ratio reward vs. weighted sum:** The log(efficiency/cost) structure automatically balances scale; a weighted sum would require manual weight tuning.
- **Dirichlet vs. uniform sampling:** Collection-center-to-warehouse uses Dirichlet (captures correlated allocation); warehouse-to-demand-point uses uniform (assumption: simpler operational logic).
- **Deterministic vs. stochastic validation:** Table 2 uses deterministic demand to isolate parameter sensitivity; Figures 4-5 use stochastic demand to validate robustness.

### Failure Signatures:
- **Flat reward (no episode-to-episode improvement):** Likely insufficient supply capacity. Check if collection centers << demand points (Case ②: 5 collection centers / 15 demand points showed zero improvement).
- **Cost diverging without satisfaction gain:** Penalty coefficients (λ_penalty, λ_switch) may be too low, allowing facility-flapping behavior.
- **High satisfaction variance across demand points:** Demand scaling (Equation 20) may be improperly normalizing; verify $\epsilon$ is set correctly.

### First 3 Experiments:
1. **Baseline reproduction:** Run PPO with GBM demand, clipped 0-2000. Target: satisfaction increases from ~0.85 to ~0.95, reward from 2.12 to 2.60 over 200 episodes (per Table 1).
2. **Supply constraint stress test:** Reduce collection centers from 15 to 5 (Case ②). Expected: reward plateaus (~2.85 no improvement), satisfaction stays <0.75 (per Figure 6, panel ②).
3. **Distance sensitivity test:** Increase distance range from 5-10 to 5-100 (Case ④). Expected: reward becomes highly sensitive to transportation cost h (per Figure 7, panel ④), validating the cost-efficiency mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the Proximal Policy Optimization (PPO) model compare against other reinforcement learning algorithms, such as DQN or A3C, in terms of convergence speed and decision quality for humanitarian supply chains?
- **Basis in paper:** [explicit] The Conclusion states that the study was conducted "between traditional single-objective and multi-objective heuristic algorithms, but not compared with other reinforcement learning methods."
- **Why unresolved:** The paper restricted its horizontal comparison to heuristic methods (PSO, NSGA-II) and did not benchmark the proposed PPO agent against other Deep RL architectures.
- **What evidence would resolve it:** A comparative performance analysis showing cumulative rewards, stability, and satisfaction rates of PPO versus other standard RL algorithms within the same simulated environment.

### Open Question 2
- **Question:** How does the inclusion of delivery lead times and delay effects impact the efficiency and equity of the dynamic allocation strategy?
- **Basis in paper:** [explicit] The Conclusion explicitly identifies a limitation: "This study assumes delivery happening for every time step, not considering the delay effects may happened in real world."
- **Why unresolved:** The current model assumes instantaneous fulfillment (delivery happens at every time step $t$), whereas real-world logistics involve significant latencies that affect inventory levels and satisfaction rates.
- **What evidence would resolve it:** Simulation results from a modified environment that incorporates stochastic lead times, analyzing the resultant penalty costs and stockout rates compared to the instantaneous model.

### Open Question 3
- **Question:** Can the PPO model maintain its superior performance and robustness when validated against real-world time-series demand data rather than stochastic simulations?
- **Basis in paper:** [explicit] The author notes in the Conclusion: "lacking the realistic time-serious demand data, this study uses simulated method to mimic real world data."
- **Why unresolved:** The model was trained and tested exclusively on Geometric Brownian Motion (GBM), Poisson, and Merton jump-diffusion simulations, which may not capture the full complexity or "shocks" of actual disaster scenarios.
- **What evidence would resolve it:** Training and evaluation results derived from historical demand datasets (e.g., past earthquake or pandemic supply needs) demonstrating the agent's adaptability to non-synthetic data patterns.

### Open Question 4
- **Question:** How can remote sensing technology be integrated into the observation state to enhance the agent's real-time responsiveness to dynamic crises?
- **Basis in paper:** [explicit] The Conclusion suggests future work where the "algorithm can be integrated with remote sensing technology which detect and record the times-series data."
- **Why unresolved:** The current observation space ($s_t$) relies on internal states (demand, inventory, time) rather than external, physical detection mechanisms used in modern disaster response.
- **What evidence would resolve it:** A proposed framework where remote sensing inputs (e.g., satellite imagery data) augment the state vector $s_t$, resulting in measurable improvements in early-stage allocation accuracy.

## Limitations

- **Neural network architecture** (layers, units, activations) is not specified, which could affect PPO performance
- **PPO clipping factor** (ε) value is unspecified, impacting policy stability
- **Action space implementation** (direct policy outputs vs. post-hoc sampling) is ambiguous
- **Real-world validation** is lacking as the model was trained and tested exclusively on stochastic simulations

## Confidence

- **High Confidence:** PPO's general ability to improve satisfaction rate and efficiency over time (demonstrated through convergence patterns)
- **Medium Confidence:** Superiority over heuristic algorithms (NSGA-II, PSO) due to limited baseline comparison methodology
- **Medium Confidence:** Robustness across demand models (GBM, Poisson, Merton) given controlled simulation conditions
- **Low Confidence:** Real-world applicability given lack of validation on actual humanitarian logistics data

## Next Checks

1. **Baseline Replication Test:** Implement the exact PPO setup with default parameters and verify satisfaction rate increases from ~0.85 to ~0.95 and reward from 2.12 to 2.60 over 200 episodes

2. **Supply Constraint Validation:** Reduce collection centers from 15 to 5 and confirm reward plateaus around 2.85 with satisfaction staying below 0.75, matching Case ② results

3. **Distance Sensitivity Analysis:** Increase distance range to 5-100 and measure reward sensitivity to transportation cost parameter h, validating the cost-efficiency mechanism's robustness