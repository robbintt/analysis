---
ver: rpa2
title: PARAM-1 BharatGen 2.9B Model
arxiv_id: '2507.13390'
source_url: https://arxiv.org/abs/2507.13390
tags:
- language
- training
- data
- hindi
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PARAM-1 is a 2.9B parameter bilingual Hindi-English language model
  designed to address the underrepresentation of Indian languages in LLMs. The model
  is trained from scratch with 25% Hindi corpus allocation and a custom tokenizer
  optimized for Indic morphologies.
---

# PARAM-1 BharatGen 2.9B Model

## Quick Facts
- arXiv ID: 2507.13390
- Source URL: https://arxiv.org/abs/2507.13390
- Reference count: 40
- PARAM-1 achieves 52.9% few-shot on ARC-Challenge, 71.4% on HellaSwag, and 36.1% on MMLU-Hindi

## Executive Summary
PARAM-1 is a 2.9B parameter bilingual Hindi-English language model designed to address the underrepresentation of Indian languages in LLMs. The model is trained from scratch with 25% Hindi corpus allocation and a custom tokenizer optimized for Indic morphologies. It achieves strong performance on English benchmarks and excels on Indic-specific tasks, outperforming larger multilingual models on MMLU-Hindi and MILU. Prometheus-Eval scores show superior fluency and coherence, especially in Hindi, while toxicity assessments confirm low harmful content generation.

## Method Summary
PARAM-1 is trained from scratch using a three-phase curriculum approach on NVIDIA H100 GPUs. Phase 1 involves 5 trillion tokens (3.48T English, 1.52T Hindi) using a 2.9B parameter decoder-only transformer with 32 layers, 2048 hidden dimensions, and grouped-query attention. Phase 2 adds 2 trillion tokens focused on factual preservation, while Phase 3 uses 500 billion tokens for long-context training. The model employs a SentencePiece tokenizer with 128K vocabulary and byte fallback. Instruction fine-tuning uses ~1M quality-filtered bilingual pairs with a 50:50 English:Hindi ratio.

## Key Results
- Achieves 52.9% few-shot accuracy on ARC-Challenge and 71.4% on HellaSwag
- Excels on Indic-specific tasks with 36.1% on MMLU-Hindi and 48.3% Hindi / 49.7% English on MILU
- Demonstrates superior fluency and coherence in Hindi per Prometheus-Eval scores
- Maintains low toxicity scores across evaluation benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Equitable Corpus Allocation at Pretraining
Allocating 25% of the pretraining corpus to Hindi directly improves Indic language representation and downstream performance by providing higher token-level representation during training. This enables the model to learn richer, more accurate linguistic and cultural representations for underrepresented languages.

### Mechanism 2: Morphology-Aware Tokenization Reduces Fragmentation
A SentencePiece tokenizer adapted to Indian morphological structures reduces tokenization fragmentation (lower fertility), enabling more efficient and semantically coherent token sequences. Lower fertility reduces sequence length inflation and preserves semantic units.

### Mechanism 3: Three-Phase Curriculum Pretraining
A staged pretraining approach leads to more stable convergence, better factual recall, and improved long-context handling. Phase 1 builds foundational competence, Phase 2 addresses factual weaknesses with fact-rich data, and Phase 3 extends context length capabilities.

## Foundational Learning

- **Tokenizer Fertility**: Understanding fertility metrics is essential to interpret fragmentation reduction claims. Quick check: Estimate token counts for a Hindi sentence across different tokenizers and explain why lower counts indicate better design.
- **Curriculum Learning in LLM Pretraining**: Understanding staged pretraining rationale helps evaluate alternative schedules. Quick check: Prioritize data types for early vs. late pretraining phases and justify choices.
- **Cross-Lingual Transfer via Parallel Data**: Understanding aligned data's role in bilingual competence is critical. Quick check: Compare sentence-aligned parallel data versus monolingual data contributions to bilingual representations.

## Architecture Onboarding

- **Component map**: NVIDIA NeMo Curator pipeline -> Tokenizer training -> Three-phase pretraining -> Instruction fine-tuning
- **Critical path**: Data curation (NeMo Curator) -> Tokenizer training -> Three-phase pretraining (5T + 2T + 500B tokens) -> SFT on ~1M bilingual pairs
- **Design tradeoffs**: Bilingual focus trades coverage for Hindi depth; code/math removal trades reasoning capability for natural language focus; 128K vocab trades memory for script coverage
- **Failure signatures**: Factual recall degradation after Phase 1, token fragmentation for low-resource scripts, toxicity in outputs
- **First 3 experiments**: 1) Replicate tokenizer fertility comparison on held-out Hindi text, 2) Ablate Phase 2 and compare on knowledge benchmarks, 3) Compare instruction-tuning dataset sizes on bilingual benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
Does the Hindi-centric pretraining and custom tokenizer design facilitate efficient adaptation to non-Hindi Indic languages despite exclusive Hindi-English training? The paper emphasizes India's linguistic diversity but evaluates only Hindi and English.

### Open Question 2
To what extent does explicit code and mathematical data removal impair general logical reasoning capabilities compared to code-trained baselines? The paper removes these but evaluates on reasoning tasks without ablation.

### Open Question 3
Would performance on Indic benchmarks improve if the custom "BharatGen-128K" tokenizer had been used instead of Nemotron? The paper shows BharatGen's superiority but doesn't explain the tokenizer choice or quantify the trade-off.

## Limitations

- Key hyperparameters (learning rates, batch sizes, step counts) are unspecified, making precise replication difficult
- Tokenizer effectiveness lacks rigorous validation through ablation studies
- 25% Hindi allocation assumes high data quality without detailed quality metrics provided
- Three-phase curriculum benefits are inferred from design rationale rather than proven through ablation
- Toxicity assessment relies on single metric without broader adversarial testing

## Confidence

- **High Confidence**: Strong English benchmark performance (ARC-Challenge, HellaSwag) and superior Indic task results (MILU-Hindi, MILU) are well-supported by reported metrics
- **Medium Confidence**: 25% Hindi allocation as primary driver of improved performance is plausible but lacks direct comparison with different language ratios
- **Low Confidence**: Architecture-first design inherently delivers "culturally grounded" representations is not empirically validated beyond benchmark scores

## Next Checks

1. Train two identical PARAM-1 models with different tokenizers (BharatGen-128K vs. LLaMA) on same corpus and compare Hindi fertility rates and downstream Hindi task performance
2. Train baseline PARAM-1 without factual preservation phase and compare performance on knowledge-intensive benchmarks (MILU, TriviaQA, MMLU)
3. Create variant trained without parallel English-Hindi corpora and compare bilingual benchmark performance (MILU-Hindi/English split) to full model