---
ver: rpa2
title: 'CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph
  Consistency'
arxiv_id: '2512.17213'
source_url: https://arxiv.org/abs/2512.17213
tags:
- answer
- reasoning
- medical
- image
- think
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CheXPO-v2 addresses hallucinations and "overthinking" in medical
  vision-language models by shifting from outcome-based rewards to process supervision.
  The core innovation is a Knowledge Graph Consistency Reward mechanism that parses
  reasoning chains into structured "Disease, Relation, Anatomy" triplets, providing
  fine-grained supervision against hallucinatory logic.
---

# CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency

## Quick Facts
- arXiv ID: 2512.17213
- Source URL: https://arxiv.org/abs/2512.17213
- Authors: Xiao Liang; Yuxuan An; Di Wang; Jiawei Hu; Zhicheng Jiao; Bin Jing; Quan Wang
- Reference count: 40
- Primary result: 86.00% accuracy on MIMIC-CXR-VQA using only 5k samples

## Executive Summary
CheXPO-v2 addresses hallucinations and "overthinking" in medical vision-language models by shifting from outcome-based rewards to process supervision. The framework introduces a Knowledge Graph Consistency Reward mechanism that parses reasoning chains into structured "Disease, Relation, Anatomy" triplets, providing fine-grained supervision against hallucinatory logic. By integrating hard example mining with Group Relative Policy Optimization (GRPO), the system achieves state-of-the-art accuracy while maintaining exceptional data efficiency and producing clinically sound, verifiable reasoning.

## Method Summary
CheXPO-v2 trains medical VLMs through a two-stage process: supervised fine-tuning (SFT) on synthesized data followed by GRPO with knowledge graph rewards. The SFT stage uses GPT-4o to generate region-aware, comparison, and basic question-answering pairs from MIMIC-CXR images, then fine-tunes Phi-4MM with LoRA adapters. The GRPO stage implements hard example mining via confidence-similarity joint selection, retrieves similar samples using BioMedCLIP embeddings, and applies group-relative advantage estimation with a composite reward function combining answer correctness, entity matching, and relation matching through Jaccard similarity.

## Key Results
- Achieves 86.00% accuracy on MIMIC-CXR-VQA using only 5k training samples
- Outperforms existing methods like MERT and MedVLM-R1 by 4-7% accuracy points
- Demonstrates exceptional data efficiency with 3-4% gains over baselines at 1k-5k sample scales
- Produces clinically sound reasoning with verifiable knowledge graph consistency

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Graph Consistency Reward via Entity-Relation Matching
The system shifts from sparse outcome-based rewards to fine-grained process supervision by parsing reasoning chains into structured "Disease | Relation | Anatomy" triplets using ReXKG-based NER. Entity matching computes Jaccard similarity between generated and ground-truth entity sets; relation matching does the same for relation triplets. The composite reward R = w₁R_ans + w₂R_ent + w₃R_rel provides dense supervision signals that penalize hallucinated entities and incorrect relational logic even when the final answer is correct. Evidence shows Jaccard(E+R) achieves 0.778 accuracy vs 0.763 for entity-only matching.

### Mechanism 2: Hard Example Mining via Confidence-Similarity Joint Selection
After SFT warm-up, the model generates responses on stratified samples (~2% of data). Length-normalized log-probability identifies incorrect predictions and low-confidence correct predictions (p < σ = -0.25) which provide high-variance reward signals. BioMedCLIP embeddings retrieve Top-K semantically similar samples using combined image-question-rationale similarity. This approach targets failures that cluster in semantic space, maximizing gradient signal efficiency. Hard-Uniform strategy achieves 86.0% accuracy with 5k samples vs ~82% for Random sampling.

### Mechanism 3: Group Relative Policy Optimization Without Critic Model
GRPO calculates group-relative advantage Aᵢ = (Rᵢ - mean(R)) / std(R) by comparing each candidate response reward to the group mean and standard deviation. The policy objective uses clipped importance sampling with KL divergence penalty (β=0.5) to prevent catastrophic forgetting. Unlike PPO, no value function approximation is needed. Within-group reward variance provides sufficient signal for advantage estimation when combined with the comprehensive knowledge graph reward function.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Reasoning in VLMs**
  - Why needed: CheXPO-v2 requires models to generate structured reasoning traces within ⿻...⿿ tags before final answers. Understanding CoT formats is essential for the entity-relation extraction pipeline to function.
  - Quick check: Can you explain why outcome-only supervision might reward correct answers derived from incorrect reasoning?

- **Concept: Reinforcement Learning from Verifiable Rewards**
  - Why needed: GRPO differs from classical RLHF by using verifiable, rule-based rewards rather than learned reward models. Understanding this distinction is critical for implementing the composite reward function.
  - Quick check: What is the key difference between PPO's value function and GRPO's group-relative advantage estimation?

- **Concept: Named Entity Recognition for Medical Text**
  - Why needed: The knowledge graph consistency reward depends entirely on accurate extraction of Anatomy, Disorder, Concept, Device, Procedure, and Size entities. Understanding NER limitations helps diagnose reward signal quality.
  - Quick check: Why might a standard NER model fail on radiology-specific terminology like "costophrenic angle blunting"?

## Architecture Onboarding

- **Component map:**
  SFT Stage → Data Synthesis Pipeline (GPT-4o → Basic/Region/Comparison QA) → Phi-4MM + LoRA (lm_head unfrozen, 4 special tokens added)
  ↓
  GRPO Stage → Hard Example Mining ← BioMedCLIP embeddings → GRPO Policy Update (G=8 samples per prompt) → Reward Computation: R_ans + R_ent (Jaccard) + R_rel (Jaccard)
  ↑
  Entity-Relation Extraction (ReXKG NER + Relation Model)

- **Critical path:** SFT warm-up → Hard example identification (stratified 2% sampling) → GRPO training on D_hard with knowledge graph rewards. The SFT stage is non-negotiable for teaching domain knowledge and required output format.

- **Design tradeoffs:**
  - Jaccard vs F1 for matching: Table V shows Jaccard(E+R) achieves best accuracy (0.828) vs F1 (0.802). Jaccard penalizes hallucinations more strictly.
  - Group size G=8: Balances computational cost against advantage estimation quality.
  - Reward weights (w₁=1, w₂=w₃=0.5): Prioritizes answer correctness while providing auxiliary process supervision.

- **Failure signatures:**
  - Overthinking: Long, convoluted CoT with low R_ent/R_rel but correct R_ans → increase process reward weights.
  - Reward hacking: High R_ent/R_rel but incorrect R_ans → increase w₁ or add format constraints.
  - Retrieval noise: Retrieved samples don't improve performance → check BioMedCLIP embedding quality and Top-K selection.

- **First 3 experiments:**
  1. Reproduce SFT → GRPO baseline: Train Phi-4MM on 10k stratified sample with accuracy-only reward. Verify ~77% accuracy matches Figure 6 "Random" baseline.
  2. Ablate reward components: Train with R_ans only, R_ans+R_ent, R_ans+R_rel, and full composite. Compare against Table V(b)-(c) to validate implementation.
  3. Validate hard mining effectiveness: Compare Random vs Hard-Uniform sampling at 1k and 5k scales. Expect ~4-5% gap per Figure 6. If gap is smaller, check confidence threshold σ=-0.25 and retrieval similarity computation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can region-aware visual interpretability mechanisms be integrated to tightly couple textual reasoning with fine-grained visual evidence?
- Basis in paper: [explicit] The conclusion states future work will focus on "enhancing visual interpretability, specifically by integrating region-aware mechanisms to tightly couple textual reasoning with fine-grained visual evidence."
- Why unresolved: While CheXPO-v2 successfully enforces logical consistency in the text domain via Knowledge Graph rewards, it currently lacks an explicit mechanism to verify that textual entities correspond to specific visual features in the image.
- What evidence would resolve it: A new architecture or reward component that aligns generated anatomical entities with visual attention maps or bounding boxes, validated by improved visual grounding scores alongside textual accuracy.

### Open Question 2
- Question: Can the Entity-Relation Matching reward be modified to prevent the stagnation of recall metrics observed during training?
- Basis in paper: [inferred] The caption of Figure 7 notes that while Precision improves rapidly, "Recall proves more challenging, stagnating or even slightly declining in later stages."
- Why unresolved: The current Jaccard-based reward may inadvertently bias the model toward conservative predictions (high precision) rather than comprehensive identification of all clinical findings (high recall).
- What evidence would resolve it: Experiments using asymmetric reward functions or penalty weights that specifically incentivize the recovery of missing relations, demonstrating sustained recall improvements without sacrificing precision.

### Open Question 3
- Question: To what extent does the performance depend on the "Disease, Relation, Anatomy" schema, and can this schema generalize to non-radiology medical domains?
- Basis in paper: [inferred] The methodology relies on a fixed schema of three primary directed relations and specific entity types tailored for radiology.
- Why unresolved: The framework's ability to generalize to other medical modalities (e.g., pathology, dermatology) or complex systemic diseases is unclear if their logical structures do not fit neatly into the "Located at," "Suggestive of," and "Modify" relation taxonomy.
- What evidence would resolve it: Application of CheXPO-v2 to a different medical imaging dataset requiring a different schema, comparing performance with and without schema re-design.

## Limitations
- The framework assumes NER models can accurately extract medical entities and relations from radiology reasoning chains; degradation on out-of-distribution pathology terminology may introduce noise.
- The 5k sample training efficiency claim depends on the specific data distribution in MIMIC-CXR-VQA and may not generalize to other medical domains or smaller datasets.
- The framework's ability to generalize to other medical modalities (e.g., pathology, dermatology) with different logical structures remains unproven.

## Confidence
- **High Confidence:** The overall architecture and methodology are sound, with clear implementation details and reasonable assumptions about GRPO mechanics and knowledge graph rewards.
- **Medium Confidence:** The performance claims (86.00% accuracy) and data efficiency results depend heavily on the quality of the SFT warm-up and the specific hard example mining parameters.
- **Low Confidence:** The long-term generalization of the triplet-based supervision to unseen pathologies and the robustness of the BioMedCLIP retrieval mechanism in diverse medical imaging contexts remain unproven.

## Next Checks
1. **NER Robustness Test:** Evaluate the ReXKG-based entity extraction on a held-out test set with rare or novel pathologies to assess whether hallucination penalties remain accurate when NER quality degrades.
2. **Generalization Study:** Train CheXPO-v2 on MIMIC-CXR-VQA subset but evaluate on an independent medical VQA benchmark (e.g., Open-I) to test domain transfer of the knowledge graph consistency rewards.
3. **Hard Example Mining Sensitivity:** Perform ablation studies on the confidence threshold (σ) and retrieval similarity computation to determine the robustness of the hard example mining strategy to hyperparameter variations.