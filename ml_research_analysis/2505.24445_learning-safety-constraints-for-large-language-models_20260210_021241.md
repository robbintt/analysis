---
ver: rpa2
title: Learning Safety Constraints for Large Language Models
arxiv_id: '2505.24445'
source_url: https://arxiv.org/abs/2505.24445
tags:
- safety
- facets
- language
- learning
- polytope
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SaP (Safety Polytope), a geometric approach
  to enforcing safety constraints in large language models by learning and steering
  representations within a polytope defined in the model's latent space. Unlike weight-based
  fine-tuning methods, SaP operates post-hoc on intermediate representations, preserving
  model capabilities while enforcing safety.
---

# Learning Safety Constraints for Large Language Models

## Quick Facts
- arXiv ID: 2505.24445
- Source URL: https://arxiv.org/abs/2505.24445
- Reference count: 40
- Primary result: Geometric approach using polytopes in latent space reduces adversarial attack success from 12.92% to 0.26% while maintaining MMLU accuracy

## Executive Summary
This paper introduces SaP (Safety Polytope), a novel approach to enforcing safety constraints in large language models through geometric reasoning in latent space. Unlike traditional weight-based fine-tuning methods, SaP operates post-hoc on intermediate representations, learning a convex polytope that defines safe output regions. The framework uses a concept encoder to disentangle safety concepts and a steering algorithm to guide unsafe outputs back into safe regions. Experiments on multiple model architectures demonstrate significant reductions in adversarial attack success rates while maintaining model capabilities.

## Method Summary
SaP learns a convex polytope in the LLM's latent representation space where points inside correspond to safe outputs. The approach consists of two phases: (1) training a polytope using labeled safe/unsafe sequences with a Concept Encoder that applies sparse nonlinear projection to reduce polysemanticity, and (2) at inference time, steering unsafe token representations back into the safe polytope using constrained optimization while preserving semantic content. The method is applied at the layer where adversarial attacks typically occur (default layer 20) and uses CPM loss to learn polytope facets that specialize in detecting different safety concepts.

## Key Results
- Adversarial success rate drops from 12.92% to 0.26% for Llama2-7B while maintaining MMLU accuracy
- Concept Encoder improves safety performance by reducing polysemanticity of polytope facets
- Facet analysis reveals natural specialization in detecting different semantic safety concepts without category supervision
- Approach scales efficiently to different model sizes (7B, 8B, 1.5B parameters)

## Why This Works (Mechanism)

### Mechanism 1: Polytope-Based Safety Boundary Learning
Safety constraints are learned as linear inequalities defining a convex polytope in the LLM's representation space. The Convex Polytope Machine (CPM) loss treats polytope learning as classification, converting the intractable convex hull problem into gradient-based optimization. The linear representation hypothesis assumes safety concepts manifest as approximately linear directions in hidden space.

### Mechanism 2: Concept Encoder Disentanglement via Sparse Nonlinear Projection
A sparse autoencoder-style projection before polytope constraints reduces polysemanticity, causing individual facets to specialize in semantically coherent safety categories. The Concept Encoder applies linear projection followed by ReLU and L1 regularization, forcing sparse, non-negative feature activations that enable each facet to detect specific unsafe patterns.

### Mechanism 3: Inference-Time Representation Steering via Constrained Optimization
Unsafe token representations are steered back into the safe polytope while preserving semantic content by solving a constrained L1 minimization. For each generated token, if the representation violates safety constraints, the optimization finds the closest safe representation, allowing safe representations to pass through unchanged.

## Foundational Learning

- **Constrained Markov Decision Processes (CMDPs)**: Safety is framed as a sequential decision problem where token choices must satisfy cost constraints while maximizing reward. This explains why constraints—not rewards—are the right abstraction for safety.

- **Linear Representation Hypothesis**: The entire polytope approach assumes semantic concepts correspond to linear directions in hidden space. Without this, the polytope would need to be non-linear or much higher-dimensional.

- **Convex Polytope Machines**: This core algorithm learns multi-facet boundaries efficiently by treating facet assignment for unsafe examples as a latent variable to be optimized, rather than fixed.

## Architecture Onboarding

- **Component map**: Feature Extraction -> Concept Encoder (Linear + ReLU + L1) -> Polytope Facets (K linear classifiers) -> Steering Module (L1-constrained optimizer) -> Generation Loop

- **Critical path**: Training: Labeled sequences → hidden states → Concept Encoder → CPM loss → (φ, ξ). Inference: Token generation → safety check → conditional steering → token decode

- **Design tradeoffs**:
  - More facets (K): Better coverage but risk of inactive facets and overfitting
  - Higher concept dimension (d): More expressive disentanglement but slower steering
  - Stronger L1 penalty (λ_f̃): Cleaner specialization but may drop useful features
  - Steering iteration count: More steps = safer outputs but higher latency

- **Failure signatures**:
  - Over-rejection: MMLU accuracy drops → polytope too conservative
  - Under-defense: ASR remains high → training attacks insufficiently diverse
  - Incoherent outputs: Semantic drift during steering → safe region excludes valid representations
  - Facet collapse: Many facets never activate → entropy regularization too weak

- **First 3 experiments**:
  1. Baseline classification: Train SaP on 2-3 attack types, evaluate classification accuracy on held-out attacks. Target: >85% accuracy.
  2. Steering ablation: Compare SaP with vs. without Concept Encoder on Ministral-8B. Measure ASR and MMLU. Target: CE reduces ASR by >40% with <1% MMLU drop.
  3. Facet interpretability: For each facet, find top-5 activating examples from BeaverTails. Compute mutual information with 14 safety categories. Target: Each high-activity facet has one dominant category (MI > 0.7 for one, <0.3 for others).

## Open Questions the Paper Calls Out

### Open Question 1
Can alternative facet assignment algorithms (beyond CPM's entropy-based heuristic) improve the disentanglement of semantic safety concepts across polytope edges? The current heuristic limits performance and many facets remain inactive during training.

### Open Question 2
What causes SaP to induce semantically incoherent outputs under adversarial attacks for certain models like Ministral-8B, and how can this be prevented? The steering optimization may push representations into regions that satisfy safety constraints but produce incoherent text.

### Open Question 3
Can formal PAC-learning sample complexity bounds for polytope-based safety be adapted to verify assumptions in practical LLM settings? Existing theoretical results require assumptions that are difficult to validate for real LLM representations.

### Open Question 4
Would non-linear geometric representations of safety capture constraints that linear polytope facets miss? The linear representation hypothesis may not fully capture all safety-relevant structure in LLM representation spaces.

## Limitations

- Polytope approximation accuracy: Safety boundaries may be inherently non-convex or require non-linear decision boundaries, leading to systematic false positives or negatives
- Concept disentanglement scalability: Scaling to hundreds of nuanced safety concepts may exceed the representational capacity of the sparse projection
- Adversarial robustness generalization: The polytope may not generalize to novel attack patterns not captured by the training distribution

## Confidence

**High Confidence**:
- SaP reduces ASR on tested attack types while preserving MMLU accuracy
- The Concept Encoder improves facet specialization and overall safety performance
- The approach scales to different model sizes (7B, 8B, 1.5B parameters)
- Steering optimization reliably finds feasible solutions within the safe polytope

**Medium Confidence**:
- Safety concepts manifest as approximately linear directions in latent space
- L1-constrained steering preserves semantic content while removing unsafe features
- The learned polytope captures the true boundary between safe and unsafe representations for the training attacks
- Facet specialization without category supervision indicates genuine concept disentanglement

**Low Confidence**:
- SaP generalizes to novel, unseen attack patterns not in the training distribution
- The approach maintains performance across diverse domains and safety concepts
- Polytope learning scales efficiently to hundreds of safety categories
- Steering quality remains high for long-form generation tasks

## Next Checks

1. **Cross-Attack Generalization Test**: Train SaP on only 3-4 attack types from HarmBench, then evaluate against the remaining 3-4 held-out attacks. This quantifies how well the polytope generalizes to novel attack patterns.

2. **Long-Form Generation Quality Assessment**: Generate extended outputs (500+ tokens) using SaP with steering enabled on Llama2-7B. Assess whether steering artifacts accumulate over long generations using automated metrics and human evaluation.

3. **Domain Transfer Experiment**: Apply a SaP polytope trained on general-purpose safety to a domain-specific model (e.g., medical LLM). Evaluate whether the same polytope boundaries effectively capture safety concepts in the new domain.