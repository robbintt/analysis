---
ver: rpa2
title: A document processing pipeline for the construction of a dataset for topic
  modeling based on the judgments of the Italian Supreme Court
arxiv_id: '2505.08439'
source_url: https://arxiv.org/abs/2505.08439
tags:
- topic
- text
- legal
- dataset
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of topic modeling in Italian
  legal research due to the lack of publicly available datasets of Supreme Court judgments.
  The authors propose a document processing pipeline combining computer vision techniques
  (YOLOv8x for layout analysis and OCR), text anonymization (GLiNER), and topic modeling
  (BERTopic) to create an anonymized dataset optimized for this task.
---

# A document processing pipeline for the construction of a dataset for topic modeling based on the judgments of the Italian Supreme Court

## Quick Facts
- **arXiv ID:** 2505.08439
- **Source URL:** https://arxiv.org/abs/2505.08439
- **Reference count:** 40
- **Primary result:** Document processing pipeline combining computer vision (YOLOv8x), OCR (TrOCR), anonymization (GLiNER), and topic modeling (BERTopic) achieved mAP@50: 0.964, CER: 0.0047, WER: 0.0248, and improved topic modeling with diversity: 0.6198 and coherence: 0.6638.

## Executive Summary
This paper presents a comprehensive document processing pipeline designed to create an anonymized dataset from Italian Supreme Court judgments for topic modeling applications. The system integrates computer vision techniques for layout analysis, OCR for text extraction, named entity recognition for anonymization, and BERTopic for topic modeling. The pipeline addresses the challenge of processing dense legal PDFs by segmenting documents into meaningful regions, extracting text with high accuracy, masking sensitive information, and generating semantically coherent topics. Large language models are employed to provide human-readable labels and summaries for the topic clusters, with evaluation against expert annotations demonstrating strong semantic alignment.

## Method Summary
The pipeline processes Italian Supreme Court judgments through a sequential workflow: (1) PDF preprocessing to 200 DPI JPEGs, (2) document layout analysis using YOLOv8x to detect text regions, headers, footers, and titles, (3) text line detection with YOLOv8x followed by OCR using TrOCR-Small, (4) anonymization via GLiNER zero-shot NER to mask PII, (5) JSON storage with bounding box metadata, and (6) BERTopic analysis using Distil-Italian-Legal-BERT embeddings with UMAP dimensionality reduction and HDBSCAN clustering. The system achieves high accuracy in layout detection (mAP@50: 0.964) and OCR (CER: 0.0047, WER: 0.0248), with improved topic modeling performance through segmentation and anonymization.

## Key Results
- DLA module achieved mAP@50 of 0.964 using YOLOv8x for layout analysis
- OCR pipeline reached character error rate of 0.0047 and word error rate of 0.0248
- Topic modeling performance improved with diversity score of 0.6198 and coherence score of 0.6638
- LLM-based labeling showed strong semantic alignment with expert annotations (BERTScore F1: 0.8119 for Claude 3.7)

## Why This Works (Mechanism)

### Mechanism 1: Segmentation-to-Context Alignment
Structured paragraph-level segmentation improves topic coherence by optimizing input length for BERT-based embedding models, which struggle with full-page noise. Legal documents are dense (approx. 250 words/page), and full-page OCR often exceeds the 512-token limit of models like BERT, leading to truncation or diluted embeddings. By using YOLOv8x to segment documents into specific classes (Text, Section-header) and filtering out noise (Page-footers), the pipeline feeds the embedding model (Distil-Italian-Legal-BERT) semantically concentrated text chunks rather than mixed administrative content. Core assumption: the "Text" class captured by the DLA module contains the substantive legal arguments, while footers and titles contain non-topical noise.

### Mechanism 2: Entity Masking as Signal Purification
Anonymization (GLiNER) functions as a noise-filtering step, preventing high-frequency Named Entities from artificially dominating topic definitions. In legal corpora, specific names (judges, companies) recur frequently. Without masking, c-TF-IDF algorithms weight these entities heavily, creating topics defined by "who" is involved rather than "what" the legal issue is. Replacing entities with generic tags (e.g., `<PERSONA>`) lowers their c-TF-IDF score, forcing the model to cluster based on latent legal concepts instead. Core assumption: the NER model detects entities with sufficiently high recall; if recall is low, remaining entities act as noise.

### Mechanism 3: LLM-based Semantic Grounding
Large Language Models can reliably map sparse keyword distributions (c-TF-IDF) to semantically dense natural language labels, aligning closely with expert interpretation. BERTopic reduces documents to statistical clusters defined by keyword lists. LLMs (Claude 3.7, GPT-4) utilize their pre-trained internal knowledge graph to interpret these keyword sets plus representative documents, "reverse-engineering" them into a coherent summary. The paper suggests this bridges the gap between statistical probability and semantic meaning. Core assumption: the LLM's training data covers the specific Italian legal domain sufficiently to ground the keywords correctly.

## Foundational Learning

- **Document Layout Analysis (DLA)**: Why needed: Standard OCR treats a page as a flat block of text. Legal PDFs have complex structures (headers, footnotes, signatures). DLA identifies these zones so text is extracted in reading order and noise is filtered. Quick check: Why would training an OCR model on pure text lines fail to preserve the logical flow of a legal argument in a multi-column judgment?

- **c-TF-IDF (Class-based TF-IDF)**: Why needed: Standard TF-IDF compares one document to the whole corpus. BERTopic clusters documents first; c-TF-IDF treats all documents in a single cluster as one giant document to find the specific words that define that topic vs. the rest of the corpus. Quick check: How does c-TF-IDF differ from standard TF-IDF in terms of what is considered a "document"?

- **Zero-shot Generalization (GLiNER)**: Why needed: Legal datasets vary wildly in terminology. A "Zero-shot" model can recognize entity types (e.g., "Tax Code") it hasn't explicitly seen in training data by leveraging a bidirectional transformer understanding of the context, saving immense annotation effort. Quick check: What is the risk of using a zero-shot model for anonymization compared to a rule-based regex system for specific formats like Italian Tax Codes?

## Architecture Onboarding

- **Component map:** Input: PDF Judgments -> Preprocessing (JPEG 200 DPI) -> DLA Module (YOLOv8x detects: Text, Header, Footer, Title) -> OCR Module (YOLOv8x line detection -> TrOCR-Small text recognition) -> Privacy Module (GLiNER zero-shot NER masks PII) -> Storage (JSON with Text + Anonymized Text + Bounding Boxes) -> Analysis (BERTopic Distil-Italian-Legal-BERT -> UMAP -> HDBSCAN -> c-TF-IDF) -> Interpretation (LLMs Claude/GPT generate labels/summaries)

- **Critical path:** The DLA-to-OCR interface. If the bounding boxes from the DLA module (YOLO) are not sorted correctly in "Western reading order" or are imprecise, the OCR module receives corrupted or disjointed image crops, leading to garbage text (high CER/WER) that ruins downstream topic modeling.

- **Design tradeoffs:** YOLOv8x vs. lighter models: uses the "X" (largest) version for maximum accuracy (mAP@50: 0.964). Tradeoff: higher computational cost and slower inference speed vs. higher layout precision. TrOCR vs. Tesseract: TrOCR is transformer-based (context-aware), better for handling ink defects/fonts. Tradeoff: heavier compute than traditional OCR, but significantly lower Word Error Rate (0.0248). LLM Selection: Claude/GPT-4 provide the best summaries but require sending data to external APIs (mitigated by anonymization). LLaMA 3.1 runs locally but showed instruction-following issues in the study.

- **Failure signatures:** "Topic Leaking": if topics are dominated by words like "Court," "Judge," or specific dates, the Stopword Removal step is insufficient. "Entity Dominance": if topics cluster around `<PERSONA>`, the Anonymization threshold is too low (missing real names) or the masking syntax is being treated as a unique token by the vectorizer. "Fragmented Topics": if diversity is high but coherence is low, the Segmentation is likely too granular (splitting sentences), destroying semantic context.

- **First 3 experiments:** 1. OCR Accuracy Validation: Run pipeline on 10 random judgment pages. Manually compare TrOCR output against PDF ground truth to verify reported CER (<0.005) holds for your specific document quality. 2. Ablation on Segmentation: Run BERTopic on Full Page Text (no DLA) vs. Segmented Text. Plot Coherence (Cv) and Diversity scores to reproduce finding that segmentation improves coherence. 3. Anonymization Recall Check: Feed known sensitive documents into GLiNER. Verify it correctly tags "Tax Codes" (ID) and "Names" (PERSONA) to ensure "Semantic Purification" mechanism is active.

## Open Questions the Paper Calls Out

### Open Question 1
To what extent does fine-tuning the GLiNER model on a manually annotated corpus of Italian legal judgments reduce the false negative rate in detecting sensitive entities compared to the zero-shot implementation? The authors state in the Conclusion that despite reliable performance, the GLiNER anonymization module was used in zero-shot mode and occasionally produced false negatives, suggesting that a "more tailored training process" or adapting GLiNER to a "customized dataset" is a necessary future improvement. Building such a dataset requires significant time and resources for manual annotation, which was outside the scope of the current work. A comparative evaluation of precision and recall scores between the current zero-shot GLiNER model and a version fine-tuned on a newly annotated dataset would resolve this.

### Open Question 2
How can a quantitative evaluation framework for the anonymization module be established to systematically measure performance against ground truth? The authors explicitly note in the Evaluation section that the "GLiNER-based anonymization component currently lacks a labeled dataset for quantitative performance evaluation," which constrains the ability to measure performance systematically. Building such a dataset requires significant time and resources for manual annotation. The creation and publication of a labeled test set containing Italian Supreme Court judgments with annotated Personal Identifiable Information (PII) and the subsequent reporting of standard NER metrics (e.g., F1-score) on this benchmark would resolve this.

### Open Question 3
Does instruction fine-tuning of smaller, open-source models (like LLaMA 3.1 8B) resolve the issue of prompt adherence, specifically preventing the generation of summaries when labels are requested? The authors report that while LLaMA 3.1 (8B) achieved a high BERTScore, they "were not satisfied because the model frequently ignored the prompt by providing a summary rather than a short label." The paper evaluated the models with specific hyperparameters but did not explore further fine-tuning to correct the specific behavioral issue of ignoring output format constraints. A follow-up experiment analyzing the format compliance rate (percentage of valid labels vs. summaries) of LLaMA 3.1 after fine-tuning on a dataset of Italian legal instructions would resolve this.

### Open Question 4
How does the performance of the document layout analysis and OCR pipeline degrade or improve when applied to other document types from the Supreme Court, such as ordinances and decrees? The authors list as future work the intention to "extend this work to other documents of Supreme Court like the ordinances and decrees." The current models (YOLOv8x and TrOCR) were fine-tuned specifically on judgments, which may have distinct layouts compared to ordinances or decrees. Reporting the mAP, CER, and WER metrics of the current pipeline when run on a test set of ordinances and decrees, compared to the metrics obtained on the judgment dataset, would resolve this.

## Limitations
- OCR accuracy measurements were performed on training data rather than held-out test documents, potentially overestimating performance
- Topic modeling improvements depend heavily on the assumption that Italian Supreme Court judgments have consistent layout patterns that YOLOv8x can learn
- LLM-based labeling effectiveness depends on domain alignment assumptions that were not empirically tested beyond BERTScore comparison
- GLiNER anonymization may miss domain-specific entity types not present in its training corpus

## Confidence
- **High Confidence:** The DLA module's performance (mAP@50: 0.964) and basic pipeline architecture are well-supported by experimental results
- **Medium Confidence:** The topic modeling improvements from segmentation are demonstrated, but the causal mechanism linking layout analysis to coherence scores needs further validation across diverse document types
- **Low Confidence:** The LLM-based topic labeling effectiveness depends on domain alignment assumptions that were not empirically tested beyond BERTScore comparison

## Next Checks
1. **OCR Generalization Test:** Run the complete OCR pipeline on 50 randomly selected judgment pages not used in training and calculate CER/WER to verify the claimed accuracy holds for unseen documents
2. **Segmentation Ablation Study:** Process the same corpus with and without the DLA module, then compare topic coherence scores to isolate the specific contribution of layout segmentation versus other pipeline components
3. **LLM Domain Alignment Validation:** Test topic label quality using an LLM trained exclusively on Italian legal documents (if available) versus Claude/GPT-4 to quantify the impact of domain knowledge on labeling accuracy