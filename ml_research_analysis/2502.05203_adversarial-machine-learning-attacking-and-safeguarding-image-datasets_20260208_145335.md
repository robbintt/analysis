---
ver: rpa2
title: 'Adversarial Machine Learning: Attacking and Safeguarding Image Datasets'
arxiv_id: '2502.05203'
source_url: https://arxiv.org/abs/2502.05203
tags:
- adversarial
- accuracy
- mnist
- training
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined the vulnerability of CNNs to FGSM attacks on
  four image datasets (CIFAR-10, ImageNet, MNIST, Fashion-MNIST). Baseline models
  achieved 78.19%, 74.26%, 86.22%, and 87.27% test accuracy respectively.
---

# Adversarial Machine Learning: Attacking and Safeguarding Image Datasets

## Quick Facts
- arXiv ID: 2502.05203
- Source URL: https://arxiv.org/abs/2502.05203
- Reference count: 21
- Key outcome: FGSM attacks reduced CNN test accuracies by 20-29% across four datasets; adversarial training improved robustness but further defenses are needed for real-world deployment.

## Executive Summary
This study examines CNN vulnerability to FGSM adversarial attacks on CIFAR-10, ImageNet, MNIST, and Fashion-MNIST. Baseline models achieved test accuracies between 74-87%, which dropped significantly (20-29%) under FGSM attacks. Adversarial training, implemented by augmenting training data with FGSM-generated examples, substantially improved post-attack robustness, with accuracy recoveries ranging from 70-82%. While the defense proves effective, the paper concludes that additional protection mechanisms are necessary for production deployment against diverse attack methods.

## Method Summary
The study implements a generalized CNN architecture with convolutional layers, pooling, and dense classification heads, trained with Adam optimizer and categorical cross-entropy loss. FGSM attacks generate adversarial examples by adding perturbations in the direction of maximum loss gradient, using ε=0.1 for MNIST/Fashion-MNIST and ε=0.03 for CIFAR-10/ImageNet. Adversarial training involves concatenating clean and adversarial examples per minibatch during retraining. The approach is evaluated across four image classification datasets with varying complexity levels.

## Key Results
- Baseline accuracies: CIFAR-10 (78.19%), ImageNet (74.26%), MNIST (86.22%), Fashion-MNIST (87.27%)
- Post-FGSM accuracy drops: CIFAR-10 (28.87%), ImageNet (27.56%), MNIST (20.35%), Fashion-MNIST (26.82%)
- Post-adversarial-training accuracies: CIFAR-10 (71.17%), ImageNet (70.01%), MNIST (80.25%), Fashion-MNIST (81.82%)
- Complex datasets (CIFAR-10, ImageNet) showed larger vulnerability to attacks than simpler datasets (MNIST, Fashion-MNIST)

## Why This Works (Mechanism)

### Mechanism 1: FGSM Gradient Exploitation
FGSM degrades model accuracy by adding perturbations in the direction that maximizes loss increase. The attack computes ∇xJ(θ, x, y)—the gradient of the loss with respect to the input—and shifts each pixel by ϵ · sign(gradient). This exploits the linearity of high-dimensional decision boundaries, causing misclassification with imperceptible changes. Core assumption: Model decision boundaries exhibit approximately linear behavior in the input space around training samples.

### Mechanism 2: Dataset Complexity Modulates Vulnerability
Complex, color-rich datasets (CIFAR-10, ImageNet) exhibit greater accuracy degradation under FGSM than simpler grayscale datasets (MNIST, Fashion-MNIST). Color images have higher dimensionality (3 channels vs. 1) and more diverse feature distributions, creating a larger attack surface for perturbations to exploit. Simpler datasets have lower variance and tighter class clusters. Core assumption: Vulnerability correlates positively with input dimensionality and feature complexity.

### Mechanism 3: Adversarial Training Induces Robust Feature Learning
Retraining models on combined clean and adversarial examples improves post-attack accuracy by teaching the model to recognize perturbed patterns. During adversarial training, each minibatch is augmented with FGSM-generated examples. The model updates weights to minimize loss on both clean and perturbed data, smoothing decision boundaries and reducing gradient exploitability. Core assumption: The perturbation distribution seen during training (FGSM with specific ϵ) is representative of deployment-time attacks.

## Foundational Learning

- **Concept: Gradient-based optimization and backpropagation**
  - Why needed here: FGSM directly exploits how gradients flow from loss to input; understanding this is essential to grasp why small perturbations cause large accuracy drops.
  - Quick check question: Can you explain why FGSM uses sign(∇xJ) rather than ∇xJ directly?

- **Concept: Overfitting vs. generalization trade-offs**
  - Why needed here: The paper notes training/testing gaps (e.g., CIFAR-10: 84.14% train vs. 78.19% test); adversarial training adds another dimension to the robustness-accuracy trade-off.
  - Quick check question: Why might improving adversarial robustness reduce clean-test accuracy?

- **Concept: CNN architecture components (convolution, pooling, dense layers, dropout)**
  - Why needed here: Algorithm 1 shows different architectures for simple vs. complex datasets; understanding when dropout is added vs. omitted is critical for implementation.
  - Quick check question: Why does the paper suggest dropout for CIFAR-10/ImageNet but not for MNIST?

## Architecture Onboarding

- **Component map:** Dataset loading → preprocessing (normalize/standardize) → CNN feature extractor → dense classification head → softmax output → Adam optimization with categorical cross-entropy loss

- **Critical path:**
  1. Load dataset → preprocess (normalize/standardize) → one-hot encode labels
  2. Train baseline CNN → evaluate test accuracy
  3. Generate FGSM adversarial examples (ε = 0.1 for MNIST/Fashion-MNIST; ε = 0.03 for CIFAR-10/ImageNet)
  4. Evaluate attacked accuracy
  5. Adversarial training: concatenate clean + adversarial examples per minibatch → retrain
  6. Re-attack with FGSM → compare accuracy recovery

- **Design tradeoffs:**
  - Higher ε increases attack strength but may make perturbations visible
  - Adversarial training improves robustness but may slightly reduce clean accuracy (not explicitly quantified in paper)
  - Deeper architectures improve baseline accuracy but may increase vulnerability surface

- **Failure signatures:**
  - Large train-test gap (>5-6%) indicates overfitting (seen in CIFAR-10: 84.14% → 78.19%)
  - Post-FGSM accuracy <55% indicates high vulnerability requiring defense
  - Post-adversarial-training accuracy <75% on complex datasets suggests need for additional defenses (per paper conclusion)

- **First 3 experiments:**
  1. Replicate baseline CNN training on MNIST and CIFAR-10 with paper-specified architectures; log train/test accuracy gaps.
  2. Apply FGSM with ε = 0.1 (MNIST) and ε = 0.03 (CIFAR-10); verify accuracy drops align with reported values (20.35% and 28.87% respectively).
  3. Implement adversarial training (Algorithm 2) on both datasets; measure accuracy recovery and compare to paper's reported 80.25% (MNIST) and 71.17% (CIFAR-10).

## Open Questions the Paper Calls Out

- **Can combining adversarial training with defensive distillation or robust optimization achieve higher robustness than adversarial training alone?**
  - Basis in paper: The conclusion explicitly suggests seeking "mixed defense methods" where adversarial training is complemented by alternative approaches such as defensive distillation.
  - Why unresolved: The study only evaluated adversarial training in isolation and did not test hybrid defense architectures.
  - Evidence: A comparative study measuring the post-attack accuracy of models utilizing these hybrid defenses versus the standalone adversarial training results reported in Table III.

- **Does the robustness gained through FGSM-based adversarial training transfer effectively to stronger, iterative attack methods like Projected Gradient Descent (PGD)?**
  - Basis in paper: The conclusion states it would be advantageous to extend the adversarial training scheme regarding "a number of the attack techniques and their perturbation methods."
  - Why unresolved: The current study limited its attack and defense methodology to the single-step Fast Gradient Sign Method (FGSM).
  - Evidence: Testing the adversarially trained models generated in this study against multi-step iterative attacks to observe if accuracy drops remain low.

- **Does utilizing more complex architectures (e.g., ResNet) narrow the robustness gap between simple datasets (MNIST) and complex datasets (ImageNet)?**
  - Basis in paper: The discussion notes that results "can be improved more with complex architecture" after observing that complex datasets suffered significantly larger accuracy drops (approx. 28%) compared to simpler ones (approx. 20%).
  - Why unresolved: The experiments used a generalized, relatively simple CNN algorithm across all datasets rather than optimizing for state-of-the-art architectures suited for complex data.
  - Evidence: A comparative experiment using deeper architectures on CIFAR-10 and ImageNet to see if the percentage drop post-attack aligns closer to that of MNIST.

## Limitations
- Lacks critical architectural details (layer counts, filter sizes, hyperparameters) limiting exact reproduction
- Does not empirically justify specific ε values chosen for different datasets
- Only tests single-step FGSM attacks, not evaluating robustness against iterative methods like PGD
- Does not quantify potential clean-accuracy degradation from adversarial training

## Confidence
- **High confidence** in FGSM vulnerability mechanism and gradient-based attack principles, supported by established literature and clear mathematical formulation
- **Medium confidence** in dataset-complexity vulnerability patterns, as the comparative results are internally consistent but lack external corpus validation
- **Medium confidence** in adversarial training effectiveness, given the reported robustness gains but absence of broader attack method testing

## Next Checks
1. Replicate FGSM attack accuracy drops using paper-specified ε values; verify alignment with reported 28.87% (CIFAR-10) and 27.56% (ImageNet) degradation
2. Implement adversarial training with exact ε-per-dataset values; measure accuracy recovery and compare to reported 71.17% (CIFAR-10) and 80.25% (MNIST)
3. Test model robustness against alternative attacks (PGD, C&W) to assess whether FGSM-specific training generalizes to other threat models