---
ver: rpa2
title: 'CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in
  Large Language Models'
arxiv_id: '2509.09675'
source_url: https://arxiv.org/abs/2509.09675
tags:
- bonus
- exploration
- arxiv
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CDE (Curiosity-Driven Exploration) tackles the challenge of poor
  exploration in RLVR training of LLMs, where models tend to converge prematurely
  and suffer from entropy collapse. It introduces a framework that uses intrinsic
  curiosity signals from both the actor (measured by perplexity over generated responses)
  and the critic (measured by variance across multi-head value estimates) as exploration
  bonuses.
---

# CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models
arXiv ID: 2509.09675
Source URL: https://arxiv.org/abs/2509.09675
Reference count: 24
Primary result: +3 points improvement on AIME benchmarks over standard GRPO/PPO

## Executive Summary
CDE (Curiosity-Driven Exploration) addresses the challenge of poor exploration in RLVR training of LLMs, where models tend to converge prematurely and suffer from entropy collapse. The framework introduces intrinsic curiosity signals from both the actor (measured by perplexity over generated responses) and the critic (measured by variance across multi-head value estimates) as exploration bonuses. These bonuses guide the model to explore novel or uncertain reasoning paths while maintaining performance. CDE achieves approximately +3 points improvement over standard GRPO/PPO on AIME benchmarks, with multi-head PPO yielding consistent gains and PPL bonus improving calibration by maintaining separation between correct and incorrect responses.

## Method Summary
CDE introduces a curiosity-driven exploration framework that combines intrinsic rewards from both actor and critic components to enhance exploration in RLVR training. The actor curiosity bonus is computed as the difference in perplexity between the generated response and ground truth, encouraging the model to explore diverse correct responses. The critic curiosity bonus is derived from the variance of value estimates across multiple heads, capturing uncertainty in the critic's evaluation. These intrinsic rewards are added to the extrinsic reward signal, with tunable weights for each component. The method is compatible with existing RL algorithms like PPO and GRPO, requiring minimal architectural changes while significantly improving exploration efficiency and preventing entropy collapse.

## Key Results
- Approximately +3 points improvement over standard GRPO/PPO on AIME benchmarks
- Multi-head PPO yields consistent gains over standard PPO
- PPL bonus improves calibration by maintaining separation between correct and incorrect responses

## Why This Works (Mechanism)
CDE works by introducing intrinsic curiosity signals that guide exploration beyond the limitations of extrinsic rewards alone. The actor bonus encourages the model to generate diverse responses by penalizing overconfidence in incorrect answers while rewarding uncertainty in correct ones. The critic bonus captures the critic's uncertainty about state values, promoting exploration of uncertain or novel states. Together, these bonuses create a balance between exploitation of known good solutions and exploration of potentially better ones. The theoretical analysis shows that actor bonus penalizes overconfident errors and encourages diverse correct responses, while critic bonus aligns with count-based exploration under linear MDP assumptions.

## Foundational Learning
1. **Curiosity-Driven Exploration**: Using intrinsic rewards to guide exploration in reinforcement learning
   - Why needed: Prevents premature convergence and entropy collapse in RLVR
   - Quick check: Compare exploration efficiency with and without curiosity bonuses

2. **Count-Based Exploration**: Using visitation counts to guide exploration in MDPs
   - Why needed: Theoretical foundation for understanding curiosity bonuses
   - Quick check: Verify alignment with count-based methods under linear MDP assumptions

3. **Multi-Head Value Estimation**: Using multiple value heads to capture uncertainty
   - Why needed: Provides a differentiable measure of critic uncertainty
   - Quick check: Compare variance-based bonuses with single-head estimates

4. **Entropy Collapse**: The phenomenon where models stop exploring and converge to suboptimal policies
   - Why needed: The core problem CDE aims to solve
   - Quick check: Monitor entropy changes during training with and without CDE

5. **Perplexity as Curiosity Signal**: Using prediction difficulty as a measure of novelty
   - Why needed: Provides a differentiable actor curiosity bonus
   - Quick check: Correlate perplexity differences with exploration success

6. **Linear MDP Assumptions**: Theoretical framework for analyzing exploration algorithms
   - Why needed: Enables theoretical guarantees about exploration bonuses
   - Quick check: Verify assumptions hold approximately in practice

## Architecture Onboarding
Component Map: LLM -> Actor Network -> Perplexity Calculator -> Actor Bonus -> Reward Signal
                     -> Critic Network (Multi-Head) -> Variance Calculator -> Critic Bonus -> Reward Signal
                     -> RL Algorithm (PPO/GRPO) -> Policy Update

Critical Path: State Observation → LLM Generation → Actor Perplexity Calculation → Critic Value Variance → Intrinsic Reward Calculation → Combined Reward → RL Update

Design Tradeoffs:
- Multi-head critic adds computational overhead but provides better uncertainty estimates
- Actor bonus requires ground truth availability, limiting applicability to supervised settings
- Weight tuning for curiosity bonuses adds hyperparameters but improves flexibility

Failure Signatures:
- Excessive curiosity weights lead to random exploration and performance degradation
- Insufficient weights result in premature convergence and entropy collapse
- Mismatched scaling between intrinsic and extrinsic rewards causes instability

3 First Experiments:
1. Verify actor bonus effectiveness by training with only actor curiosity on a simple RL task
2. Test critic bonus contribution by using only multi-head variance-based exploration
3. Perform ablation study to determine optimal weighting between actor and critic bonuses

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of ablations on relative contributions of actor vs. critic bonuses
- Evaluation limited to synthetic reasoning datasets (AIME) rather than real-world RLVR scenarios
- Theoretical analysis relies on linear MDP assumptions that may not hold in practice

## Confidence
High confidence: +3 point AIME improvements over standard GRPO/PPO
Medium confidence: Alignment with count-based exploration under linear MDP assumptions
Medium confidence: Calibration improvement claims

## Next Checks
1. Perform ablation studies to isolate the impact of actor vs. critic curiosity bonuses
2. Test on non-synthetic RLVR tasks like human feedback learning or real-world reasoning problems
3. Conduct scaling experiments to verify effectiveness across different model sizes and architectures beyond the reported 1.8B parameter model