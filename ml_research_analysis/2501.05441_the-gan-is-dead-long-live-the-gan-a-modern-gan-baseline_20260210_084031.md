---
ver: rpa2
title: The GAN is dead; long live the GAN! A Modern GAN Baseline
arxiv_id: '2501.05441'
source_url: https://arxiv.org/abs/2501.05441
tags:
- training
- arxiv
- should
- conference
- gans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper challenges the notion that GANs are inherently difficult
  to train by introducing a well-behaved regularized relativistic GAN loss, R3GAN,
  which combines the relativistic pairing GAN (RpGAN) objective with zero-centered
  gradient penalties (R1 and R2). The authors mathematically prove that this loss
  admits local convergence guarantees, unlike most existing relativistic losses.
---

# The GAN is dead; long live the GAN! A Modern GAN Baseline

## Quick Facts
- **arXiv ID:** 2501.05441
- **Source URL:** https://arxiv.org/abs/2501.05441
- **Reference count:** 40
- **Primary result:** Introduces R3GAN, a regularized relativistic GAN loss that provides local convergence guarantees and enables a minimalist baseline that surpasses StyleGAN2 and compares favorably against state-of-the-art GANs and diffusion models.

## Executive Summary
This paper challenges the conventional wisdom that GANs are inherently difficult to train by introducing a well-behaved regularized relativistic GAN loss, R3GAN. By combining the relativistic pairing GAN (RpGAN) objective with zero-centered gradient penalties (R1 and R2), the authors mathematically prove that this loss admits local convergence guarantees, unlike most existing relativistic losses. This stability allows the removal of ad-hoc tricks commonly used in GANs, enabling the adoption of modern architectures. Using StyleGAN2 as a starting point, they simplify and modernize the backbone, resulting in a minimalist baseline that surpasses StyleGAN2 and compares favorably against state-of-the-art GANs and diffusion models across multiple datasets (FFHQ, ImageNet, CIFAR, Stacked MNIST) in terms of FID scores.

## Method Summary
The authors introduce R3GAN, which combines the relativistic pairing GAN (RpGAN) objective with zero-centered gradient penalties R1 (on real data) and R2 (on fake data). This formulation is mathematically proven to admit local convergence guarantees. The stable training enabled by this loss allows removal of typical GAN stabilization tricks. The architecture uses a simplified StyleGAN2 backbone with grouped convolutions, inverted bottlenecks, no normalization layers, and fix-up initialization. Training uses BFloat16 mixed precision, Adam optimizer with β₁=0, and carefully scheduled learning rates and penalty coefficients.

## Key Results
- Achieves SOTA FID scores on FFHQ, ImageNet, CIFAR-10, and Stacked MNIST
- Outperforms StyleGAN2 baseline across all tested datasets
- Demonstrates competitive performance against diffusion models while being significantly faster to train
- Shows improved stability through mathematical convergence guarantees

## Why This Works (Mechanism)
The R3GAN loss provides stability through its relativistic formulation combined with gradient penalties. The RpGAN objective compares real and fake distributions directly rather than using absolute real/fake discrimination, which creates a more stable training signal. The R2 penalty on fake gradients is critical for convergence - mathematical analysis shows R1 alone is insufficient. This stability allows removing normalization layers and other stabilization tricks, enabling cleaner architectural choices like grouped convolutions and fix-up initialization that improve efficiency without sacrificing performance.

## Foundational Learning
- **Relativistic GANs:** Compare real and fake distributions directly rather than absolute discrimination; needed for stable gradient flow and better mode coverage.
- **Gradient penalties (R1/R2):** Regularize discriminator gradients to prevent exploding/vanishing gradients; R2 specifically ensures fake gradients are well-behaved for convergence.
- **Grouped convolutions:** Split channels into groups for more efficient computation; maintain representational power while reducing parameters.
- **Fix-up initialization:** Special initialization scheme that allows training deep networks without normalization; crucial for removing BatchNorm while maintaining stability.
- **BFloat16 precision:** 16-bit floating point with 8-bit mantissa; provides speed benefits of FP16 while maintaining numerical stability during training.

## Architecture Onboarding

**Component map:** Input → G (Bottleneck ResNet with grouped conv) → D (Symmetric architecture) → Loss (RpGAN + R1 + R2)

**Critical path:** Generator → Discriminator → Relativistic loss computation → Gradient penalty calculation → Parameter update

**Design tradeoffs:** Removed BatchNorm for architectural simplicity and efficiency, accepting the need for careful initialization. Used grouped convolutions to reduce parameters while maintaining capacity. Chose BFloat16 over FP16 for stability despite slightly higher memory usage.

**Failure signatures:** Training divergence with R1 only (R2 is essential); complete training failure with IEEE FP16; degraded FID with smooth activations like Swish/GELU.

**First experiments:**
1. Implement RpGAN + R1 + R2 loss with γ=10 and verify it trains more stably than R1 alone
2. Build Config E backbone with grouped convolutions and fix-up initialization, confirm it matches or exceeds StyleGAN2 performance
3. Test BFloat16 mixed precision training vs FP16 to confirm the paper's claim about FP16 instability

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Mathematical convergence proof applies to idealized formulation; practical training still requires careful hyperparameter tuning
- Model still requires mixed precision (BFloat16) and careful scheduling, indicating some stability constraints remain
- Architecture simplifications are empirically justified but lack theoretical grounding for superiority over established designs

## Confidence

**High Confidence:** RpGAN + R2 provides better stability than R1 alone (well-supported by proof and empirical results); modern architectural elements improve GAN training (clearly demonstrated).

**Medium Confidence:** Claim of "modern baseline" is reasonable given strong FID scores but would benefit from comparisons against other modern GAN variants with similar simplifications; superiority over diffusion models demonstrated but comparison methodology could be more standardized.

**Low Confidence:** Hyperbolic claim that "the GAN is dead" overstates novelty; incremental improvements presented rather than revolutionary breakthrough.

## Next Checks
1. Systematically vary R2 coefficient γ across {5, 10, 20, 50} on CIFAR-10 to quantify robustness of proposed loss
2. Replace grouped convolutions with standard convolutions while keeping RpGAN + R1 + R2 loss fixed to isolate architectural contribution
3. Train Config E model on LSUN Bedroom and CLEVR datasets to evaluate cross-dataset generalization beyond reported four datasets