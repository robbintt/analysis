---
ver: rpa2
title: 'MalCL: Leveraging GAN-Based Generative Replay to Combat Catastrophic Forgetting
  in Malware Classification'
arxiv_id: '2501.01110'
source_url: https://arxiv.org/abs/2501.01110
tags:
- malware
- samples
- replay
- malcl
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MalCL, a generative replay-based continual
  learning system designed to address catastrophic forgetting in malware classification.
  By employing a GAN with feature matching loss and innovative replay sample selection
  schemes, MalCL effectively generates high-quality synthetic malware samples to retain
  knowledge of previously learned malware families while adapting to new ones.
---

# MalCL: Leveraging GAN-Based Generative Replay to Combat Catastrophic Forgetting in Malware Classification

## Quick Facts
- arXiv ID: 2501.01110
- Source URL: https://arxiv.org/abs/2501.01110
- Reference count: 4
- MalCL achieves 55% mean accuracy on 100 Windows malware families, outperforming prior generative replay models by 28%

## Executive Summary
MalCL addresses catastrophic forgetting in malware classification through a generative replay-based continual learning system. The approach employs a GAN with feature matching loss to generate high-quality synthetic malware samples that preserve knowledge of previously learned families while adapting to new ones. Evaluations on Windows (EMBER) and Android (AZ-Class) datasets demonstrate substantial performance improvements over existing methods, with particular emphasis on replay sample selection strategies and task set construction. The system achieves an average accuracy of 55% on Windows malware, outperforming prior generative replay models by 28%, with further improvements up to 74% when larger classes are assigned to earlier tasks.

## Method Summary
MalCL implements class-incremental continual learning for malware family classification across 11 sequential tasks. The system uses a GAN with feature matching loss to generate synthetic replay samples that are combined with new task data during training. A 1D CNN classifier processes feature vectors extracted from malware binaries, while a discriminator extracts intermediate features for the feature matching loss. Replay samples are selected using L1 distance to per-class mean logit vectors from the classifier's hidden representations. The approach operates on EMBER (Windows PE samples) and AZ-Class (Android samples) datasets, with task sets constructed to include 50 initial classes followed by 5 new classes per task.

## Key Results
- MalCL achieves 55% mean accuracy on 100 Windows malware families, outperforming prior generative replay models by 28%
- Performance improves to 74% when larger classes are assigned to earlier tasks
- L1 distance to per-class mean logits selection achieves 54.5% mean accuracy vs. 50.2% for L2 to labels
- Feature matching loss significantly outperforms binary cross entropy across all tasks

## Why This Works (Mechanism)

### Mechanism 1
Feature Matching Loss (FML) improves synthetic malware sample quality compared to Binary Cross Entropy (BCE) loss for generative replay. FML shifts the generator's training objective from deceiving the discriminator's final output to matching intermediate feature representations. The generator minimizes the distance between the average features of real malware samples and synthetic samples extracted from a middle hidden layer of the discriminator. Core assumption: Intermediate discriminator features capture richer, more transferable malware characteristics than binary classification outputs. Evidence: FML outperforms BCE across tasks with equation (13) defining the loss function.

### Mechanism 2
Replay sample selection using L1 distance to Per-Class Mean Logit Vectors produces more effective replay sets than label-based or batch-based selection. Extract logits from the classifier's intermediate layer for both synthetic and real samples. Compute per-class mean logit vectors for real data. Select the k synthetic samples per class with minimum L1 distance to their respective class centroids. Core assumption: The classifier's hidden representations capture meaningful malware family characteristics, and proximity in this space correlates with replay effectiveness. Evidence: Table 1 shows L1 to CMean Logits with FML achieving 54.5% mean accuracy vs. 50.2% for L2 to Labels.

### Mechanism 3
Assigning larger malware classes to earlier tasks significantly mitigates catastrophic forgetting and improves overall accuracy. Early tasks with abundant data allow the GAN to learn more robust, generalizable feature representations. These representations form a stronger foundation for replay, as subsequent tasks benefit from higher-quality synthetic samples representing foundational malware families. Core assumption: Larger classes provide better coverage of feature space, leading to more stable generator training that persists through task sequences. Evidence: Achieved 74% accuracy with configuration using 50 "giant" classes (avg 5,397 samples) in first task vs. remaining classes (avg 670 samples).

## Foundational Learning

- **Catastrophic Forgetting (CF)**: Neural networks lose performance on old tasks when trained on new data. Essential to understand why sequential training on new malware families would cause a classifier to forget previously learned families.
- **Generative Adversarial Networks (GANs)**: MalCL uses a GAN to generate synthetic replay samples. Understanding generator-discriminator dynamics is crucial to troubleshoot training and sample quality. Quick check: What happens to sample quality if the discriminator becomes too strong relative to the generator?
- **Class-Incremental Learning**: MalCL operates in a class-incremental scenario where new malware families appear over sequential tasks, and the model must classify all observed classes at test time. Quick check: How does class-incremental learning differ from task-incremental learning, where task identity is known at test time?

## Architecture Onboarding

- **Component map**: Generator (4 × 1D CNN → 2 FC → 3 deconvolution layers with ReLU, batch normalization) → Discriminator (2 conv → 2 FC) → Classifier (3 conv → 1 FC → softmax) → Selection Module (computes distances between synthetic samples and class centroids)
- **Critical path**: 1) Train G and D on current task data using FML 2) Train C on current task data 3) Generate synthetic samples with G; extract classifier logits 4) Select k samples per class using L1 to CMean Logits 5) Combine replay samples with next task's training data; repeat
- **Design tradeoffs**: FML vs. BCE loss (FML higher quality but requires intermediate features vs. BCE simpler but lower performance); Per-class vs. per-batch selection (per-class ensures balance but requires centroids vs. per-batch simpler but risks mode collapse); Task ordering (strategic ordering improves performance but may be infeasible in real-time deployment)
- **Failure signatures**: Mode collapse (generator produces samples for only a few classes; detected by counting unique classes in replay set); Classifier drift (accuracy on early tasks drops sharply; indicates replay samples don't adequately represent original distributions); Discriminator dominance (GAN training becomes unstable; generator loss plateaus without improvement)
- **First 3 experiments**: 1) Baseline comparison against None, Joint, GR, and BI-R baselines on EMBER dataset; verify ~28% improvement over prior GR methods 2) Ablation on loss functions: compare FML vs. BCE with fixed selection scheme; expect FML to outperform by 2-3 percentage points 3) Selection scheme comparison: test L2 to Labels vs. L1 to CMean Logits vs. L1 to BMean Logits; expect CMean to achieve highest mean accuracy (~54-55%), BMean to show mode collapse

## Open Questions the Paper Calls Out

- Can hybrid training approaches that combine Generative Replay (GR) with joint training effectively close the performance gap between MalCL and the ideal "Joint" baseline? While MalCL outperforms other CL methods, a significant accuracy gap remains between it (55%-74%) and the Joint baseline (88.7%), suggesting pure GR does not perfectly preserve original data distributions.
- How can global replay sample selection schemes be improved to mitigate mode collapse and ensure class diversity without explicit per-class enforcement? The current L1 to BMean Logits method produced samples for only a few classes, indicating failure to enforce balance.
- Do more advanced generative models, such as diffusion models, offer significant improvements in synthetic sample quality and stability over the current GAN-based approach? GAN training can be unstable and suffer from mode collapse, whereas diffusion models (DDGR) exist but were not evaluated in this specific malware context.
- What adaptive mechanisms can be integrated to proactively handle severe class imbalances and shifting malware tactics in dynamic datasets? MalCL's performance varies significantly based on task ordering (random vs. large classes first), indicating a lack of robustness to naturally imbalanced data streams.

## Limitations

- Code is not yet publicly available, making exact reproduction challenging
- Key hyperparameters (GAN training epochs, replay sample count k, exact network dimensions) are unspecified
- Results heavily depend on task ordering strategy, which may not be feasible in real-world scenarios where new malware families arrive unpredictably

## Confidence

- High confidence: FML loss improves sample quality vs. BCE (supported by direct comparisons in Table 1)
- Medium confidence: L1 to CMean Logits selection is superior (consistent results across datasets, but limited ablation studies)
- Medium confidence: Strategic task ordering improves performance (strong results but domain-specific finding)

## Next Checks

1. Implement ablation study comparing FML vs. BCE loss with fixed selection scheme on EMBER dataset
2. Test task ordering effects by randomly permuting class assignments and measuring accuracy degradation
3. Evaluate classifier performance on held-out validation sets after each task to quantify forgetting progression