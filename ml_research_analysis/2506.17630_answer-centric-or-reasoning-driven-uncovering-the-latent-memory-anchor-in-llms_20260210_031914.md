---
ver: rpa2
title: Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in
  LLMs
arxiv_id: '2506.17630'
source_url: https://arxiv.org/abs/2506.17630
tags:
- reasoning
- answer
- llms
- arxiv
- answers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether large language models (LLMs) rely
  more on final answers or on reasoning chains during problem-solving. To test this,
  the authors designed a five-level prompt framework that systematically varies the
  visibility of final answers within prompts, ranging from explicit answers to fully
  answer-free conditions.
---

# Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs

## Quick Facts
- **arXiv ID**: 2506.17630
- **Source URL**: https://arxiv.org/abs/2506.17630
- **Reference count**: 8
- **Primary result**: Large language models (LLMs) show a sharp performance drop when answer cues are masked, suggesting they are predominantly anchored to final answers rather than reasoning chains.

## Executive Summary
This paper investigates whether large language models rely on final answers or reasoning chains during problem-solving. Using a five-level prompt framework that systematically controls answer visibility, the authors demonstrate that LLMs exhibit a strong dependency on provided answer cues, with performance dropping sharply when answers are maskedâ€”even when complete reasoning chains are present. Experiments across state-of-the-art models show that models prioritize answers over reasoning when cues conflict and struggle to override known answers even when warned they may be incorrect. These findings suggest that much of the reasoning exhibited by LLMs reflects post-hoc rationalization around memorized answers rather than genuine inference, calling into question the depth of their reasoning capabilities.

## Method Summary
The authors designed a five-level prompt framework that systematically varies the visibility of final answers within prompts, ranging from explicit answers to fully answer-free conditions. They conducted experiments on state-of-the-art LLMs using a Chinese mathematical reasoning benchmark (RoR-Bench), comparing model performance across conditions where answer cues were progressively masked. The framework included additional experiments with conflicting answer-reasoning pairs and explicit warnings about incorrect answers to test answer prioritization and override-ability. Model outputs were evaluated using automated scoring systems to assess accuracy and answer citation patterns.

## Key Results
- LLMs show a 26.90% performance drop when answer cues are masked, even with complete reasoning chains provided
- Models prioritize answers over reasoning when cues conflict, producing correct answers with flawed reasoning more often than vice versa
- LLMs struggle to override known answers even when explicitly warned they may be incorrect, demonstrating persistent answer anchoring

## Why This Works (Mechanism)
The paper demonstrates that LLMs exhibit strong answer-anchoring behavior, where provided final answers serve as memory anchors that significantly influence performance. This anchoring effect persists even when complete reasoning chains are available, suggesting that models may be retrieving and rationalizing memorized answer-reasoning patterns rather than performing genuine logical inference. The experiments show that models will prioritize final answers over the quality of reasoning chains when conflicts arise, and they struggle to override known answers even when explicitly warned about their potential incorrectness.

## Foundational Learning
- **Concept: Chain-of-Thought (CoT) Prompting**
  - **Why needed here:** The entire paper's experimental framework is built upon manipulating inputs within a CoT-style reasoning setup. Understanding what CoT is intended to do (elicit step-by-step reasoning) is essential to grasp what the paper claims it may *not* be doing (genuine inference).
  - **Quick check question:** What is the intended purpose of providing a model with a few examples of question-reasoning-answer triplets before giving it a new question?

- **Concept: Behavioral Probing vs. Internal State Inspection**
  - **Why needed here:** The authors explicitly state they cannot inspect a model's internal "memory traces." Therefore, all conclusions are inferred from output changes due to input manipulations. One must understand this indirect methodology to evaluate the validity of the claims.
  - **Quick check question:** If you cannot open a "black box" model to see how it works, how can you design an experiment to infer its internal dependencies?

- **Concept: Memorization vs. Generalization in LLMs**
  - **Why needed here:** The core tension in the paper is between models *reciting* memorized answer-reasoning patterns and *performing* genuine logical inference. Understanding this debate is critical for interpreting the results as evidence for "post-hoc rationalization."
  - **Quick check question:** If a model solves a problem it has seen verbatim in its training data, is that evidence of reasoning or memorization? What if the problem is slightly perturbed?

## Architecture Onboarding
- **Component Map:** Problem Selection -> Five-Level Prompt Generation -> LLM Processing -> Behavioral Evaluation -> Performance Comparison
- **Critical Path:**
  1. Select a problem from the RoR-Bench dataset
  2. Generate the five prompt variants, carefully masking or removing answer cues as per the framework
  3. Feed each variant to the target LLM (e.g., DeepSeek-R1, Claude 3.7 Sonnet) in a zero-shot, temperature=0 setting
  4. Collect the generated output and the final answer
  5. Pass the output to the behavioral evaluator to get a binary accuracy score
  6. Compare performance across the five conditions to infer memory binding

- **Design Tradeoffs:**
  - **Control vs. Realism:** The AE condition (providing the answer with the problem) is diagnostically powerful but artificial. It sacrifices realism for experimental control, making it unclear how this specific dynamic plays out in real-world, zero-shot user interactions.
  - **Automated vs. Human Evaluation:** Using GPT-4o as a judge introduces its own potential biases and errors, but is necessary for scale. The tradeoff is between evaluation throughput and the risk of systematic mis-scoring.

- **Failure Signatures:**
  - A model that performs well in the Answer-Free condition but poorly in the Answer-Explicit condition, suggesting it ignores provided cues
  - A model whose performance does *not* degrade when answer tokens are masked (AMR condition), which would contradict the paper's primary finding
  - A model that consistently cites the provided answer even when warned not to, showing a failure of instructability rather than reasoning

- **First 3 Experiments:**
  1. **Establish the Baseline Gradient:** Run all five conditions (AE through AF) on a single model (e.g., QWQ-32B) across a subset of RoR-Bench. Confirm the monotonic performance drop reported in Table 1.
  2. **Test Conflict Resolution:** Create a small set of 10 problems. For each, craft a "Right Answer / Wrong Reasoning" prompt and a "Wrong Answer / Right Reasoning" prompt. Feed both to a model and compare which yields the correct final answer, testing the prioritization claim from Table 3.
  3. **Probe Warning Tenacity:** Implement the "Hard Warning" prompt ("The reference answers are incorrect.") and run it with a set of problems and their correct answers on a model like DeepSeek-R1. Measure the performance drop relative to the standard AE condition to test the persistence of the answer anchor as shown in Figure 3.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the answer-anchoring phenomenon persist in multi-modal models when visual inputs serve as the primary reasoning context?
- Basis in paper: [explicit] Section 3.3 states, "we leave the investigation of answer-reasoning dependence in multi-modal models to future work."
- Why unresolved: The study restricted experiments to the text-only subset of RoR-Bench, leaving the interaction between visual reasoning chains and answer cues unexplored.
- What evidence would resolve it: Applying the five-level answer-visibility framework to Vision-Language Models (VLMs) on visual reasoning benchmarks (e.g., visual math datasets).

### Open Question 2
- Question: Do the findings regarding answer-anchoring generalize to diverse languages and complex professional domains such as legal or medical reasoning?
- Basis in paper: [explicit] The Limitations section notes, "The generalizability of our findings to other domains, languages... warrants further exploration."
- Why unresolved: The experiments were conducted exclusively on Chinese arithmetic and logic problems from the RoR-Bench dataset.
- What evidence would resolve it: Replicating the prompt intervention experiments on multilingual benchmarks and domain-specific datasets outside of mathematical logic.

### Open Question 3
- Question: Can mechanistic interpretability techniques identify specific neural circuits that drive answer-anchoring, distinguishing them from reasoning circuits?
- Basis in paper: [inferred] Section 3.2 notes that direct internal state inspection is "infeasible," forcing reliance on indirect behavioral probing.
- Why unresolved: The paper infers memory anchors from output behavior (performance drops) rather than verifying the internal mechanism or "memory trace."
- What evidence would resolve it: Activation patching or probing classifier studies that causally link specific model components (e.g., attention heads) to answer retrieval versus step-by-step inference.

## Limitations
- **Experimental Realism**: The answer-explicit (AE) condition is highly artificial and may exaggerate answer-anchoring behavior compared to real-world zero-shot interactions.
- **Evaluation Artifacts**: Using GPT-4o as an automated evaluator may introduce systematic biases if the evaluator itself exhibits answer-centric tendencies.
- **Language and Cultural Specificity**: All experiments were conducted on Chinese mathematical reasoning problems, leaving generalizability to other languages and domains untested.

## Confidence
- **High Confidence**: The systematic performance degradation when answer cues are masked (26.90% drop) is a robust empirical finding supported by multiple models and problems.
- **Medium Confidence**: The claim that models prioritize answers over reasoning in conflict situations is supported by targeted experiments, but the conflict-generation methodology may not capture all forms of reasoning-answer divergence.
- **Low Confidence**: The interpretation that exhibited reasoning represents "post-hoc rationalization" rather than genuine inference extends beyond the experimental evidence.

## Next Checks
1. **Cross-Lingual Replication**: Replicate the five-level prompt framework experiments on English mathematical reasoning benchmarks (e.g., GSM8K, MATH) and non-mathematical reasoning tasks to assess generalizability.
2. **Human vs. Automated Evaluation**: Conduct a subset of experiments with human annotators scoring model outputs for accuracy and reasoning quality, comparing results against GPT-4o evaluations to validate automated scoring reliability.
3. **Ablation on Answer Visibility**: Design experiments where answer visibility is manipulated at the token level rather than the complete answer level to determine whether models anchor to specific answer components or the complete numerical value.