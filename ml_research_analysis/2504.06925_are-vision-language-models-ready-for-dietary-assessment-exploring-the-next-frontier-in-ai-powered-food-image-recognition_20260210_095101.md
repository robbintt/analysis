---
ver: rpa2
title: Are Vision-Language Models Ready for Dietary Assessment? Exploring the Next
  Frontier in AI-Powered Food Image Recognition
arxiv_id: '2504.06925'
source_url: https://arxiv.org/abs/2504.06925
tags:
- food
- image
- images
- recognition
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces FoodNExTDB, a novel expert-labeled food image
  database with 9,263 images across 10 categories, 62 subcategories, and 9 cooking
  styles, annotated by seven nutrition experts. We evaluate six state-of-the-art VLMs
  (ChatGPT, Gemini, Claude, Moondream, DeepSeek, and LLaVA) on food recognition tasks
  using a novel Expert-Weighted Recall (EWR) metric that accounts for inter-annotator
  variability.
---

# Are Vision-Language Models Ready for Dietary Assessment? Exploring the Next Frontier in AI-Powered Food Image Recognition

## Quick Facts
- arXiv ID: 2504.06925
- Source URL: https://arxiv.org/abs/2504.06925
- Reference count: 40
- Primary result: VLMs achieve >90% expert-weighted recall for single-product food images but struggle with fine-grained recognition (50% for cooking styles)

## Executive Summary
This study evaluates six state-of-the-art VLMs on food recognition tasks using FoodNExTDB, a novel expert-labeled database of 9,263 images across 10 categories, 62 subcategories, and 9 cooking styles. The research introduces Expert-Weighted Recall (EWR) to handle inter-annotator variability among nutrition experts. Results show closed-source models (ChatGPT, Gemini, Claude) significantly outperform open-source alternatives, achieving over 90% EWR for single-product images but struggling with fine-grained recognition, particularly for cooking styles (50% EWR). The study highlights VLMs' potential and limitations for dietary assessment, emphasizing the need for improved context-aware learning and domain-specific training.

## Method Summary
The study evaluates six VLMs (ChatGPT, Gemini, Claude, Moondream, DeepSeek, and LLaVA) on FoodNExTDB using a structured prompt approach and Expert-Weighted Recall metric. Images are annotated by seven nutrition experts, with EWR weighting predictions by expert consensus. Models are accessed via API with standardized parameters, and open-source outputs require post-processing for formatting consistency. The evaluation covers hierarchical classification (category → subcategory → cooking style) and compares single vs. multi-product image performance.

## Key Results
- Closed-source VLMs achieve 90%+ EWR for single-product image classification
- Performance drops significantly with granularity: 85.79% (category) → 74.69% (subcategory) → 50.00% (cooking style)
- Open-source models lag substantially: 34-55% average EWR versus 64-70% for closed-source
- Cooking style recognition remains the most challenging task across all models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Closed-source VLMs achieve higher accuracy due to richer pre-training data diversity
- Mechanism: Larger-scale multimodal pre-training exposes models to more varied food imagery and culinary terminology
- Core assumption: Performance gap reflects underlying model capacity
- Evidence anchors: Closed-source models achieve 64-70% average EWR versus 34-55% for open-source; limited comparative benchmarks exist
- Break condition: Domain-specific fine-tuning on large food datasets may narrow the gap

### Mechanism 2
- Claim: Fine-grained recognition degrades due to insufficient visual-textual grounding
- Mechanism: Subtle visual cues for cooking styles lack robust multimodal associations in training data
- Core assumption: Bottleneck is training data coverage, not architecture
- Evidence anchors: Gemini's EWR declines from 85.79% to 50.00% when adding cooking styles; similar challenges noted in fine-grained food classification literature
- Break condition: External context (user input, sensors) may become necessary if cooking style cannot be reliably inferred from images alone

### Mechanism 3
- Claim: Inter-annotator variability necessitates weighted evaluation metrics
- Mechanism: Ambiguous visual cues and differing professional paradigms cause expert disagreement; EWR rewards predictions aligned with higher consensus
- Core assumption: Expert disagreement reflects genuine ambiguity, not labeling error
- Evidence anchors: EWR proposed to account for inter-annotator variability; differences in nutritional paradigms cause inconsistencies
- Break condition: Refined annotation protocols with stricter guidelines may allow simpler metrics

## Foundational Learning

- **Expert-Weighted Recall (EWR)**
  - Why needed: Traditional accuracy fails when multiple valid labels exist
  - Quick check: If 5 of 7 experts label "poultry" and 2 label "other meats," what weight does a correct "poultry" prediction receive?

- **Taxonomic Classification Hierarchy**
  - Why needed: Performance degrades as granularity increases
  - Quick check: Why does adding "cooking style" reduce top-model EWR from ~86% to ~50%?

- **Multimodal Grounding in VLMs**
  - Why needed: VLMs must link visual features to textual concepts
  - Quick check: What training data characteristics would improve visual distinction between "fried" and "grilled"?

## Architecture Onboarding

- Component map: FoodNExTDB (9,263 images + 50k expert labels) → Structured prompt → VLM API → Response parsing → EWR evaluation
- Critical path: Image quality → Prompt consistency → VLM output structure → Label matching → Consensus weighting
- Design tradeoffs: Closed-source APIs yield higher accuracy with per-inference cost; open-source reduces cost but requires post-processing
- Failure signatures:
  - Background artifacts detected as food items (sandwich from tray liner)
  - Shape-based misclassification (bread → croquettes)
  - Open-source models produce malformed outputs needing secondary ChatGPT cleanup
- First 3 experiments:
  1. Replicate baseline EWR evaluation across all six VLMs on single-product images at all classification levels
  2. Ablate prompt specificity by testing with and without Mediterranean diet context hints
  3. Quantify inter-annotator agreement per category to identify high-ambiguity classes for targeted data augmentation

## Open Questions the Paper Calls Out

- **Can advanced context-aware learning strategies significantly improve VLM accuracy in recognizing visually ambiguous cooking styles?**
  - Basis: Conclusion states need for advances in context-aware learning
  - Why unresolved: Top models drop from ~86% to 50% when identifying cooking styles
  - What evidence would resolve: Modified VLM achieving >80% EWR on Category + Subcategory + Cooking Style task

- **What specific fine-tuning protocols are required to bridge the performance gap between open-source and closed-source VLMs?**
  - Basis: Discussion notes need for improved fine-tuning strategies and domain-specific training
  - Why unresolved: Clear performance gap exists but study didn't experiment with fine-tuning open-source models
  - What evidence would resolve: Open-source models fine-tuned on FoodNExTDB achieving scores statistically indistinguishable from closed-source baselines

- **How do state-of-the-art VLMs compare against specialized hybrid transformer-based food recognition models?**
  - Basis: Discussion states need to benchmark against transformer-based models
  - Why unresolved: Study only compared VLMs against each other
  - What evidence would resolve: Comparative analysis showing VLMs versus hybrid CNN-ViT architectures on FoodNExTDB

## Limitations

- Reliance on closed-source VLM APIs with limited transparency about training data composition
- FoodNExTDB validation focused on Mediterranean diet images, potentially limiting generalizability
- Study did not evaluate temporal consistency of VLM predictions across multiple queries

## Confidence

- **High Confidence**: Closed-source VLMs outperform open-source models on single-product recognition tasks
- **Medium Confidence**: Fine-grained recognition limitations reflect training data gaps rather than architectural constraints
- **Low Confidence**: Inter-annotator variability among nutrition experts represents genuine ambiguity rather than labeling inconsistency

## Next Checks

1. Test VLMs on food image datasets from non-Mediterranean cuisines to assess performance generalizability and identify cultural bias patterns
2. Analyze training corpora of top-performing VLMs for food-related content coverage, focusing on fine-grained culinary distinctions
3. Evaluate VLM stability by running 10 sequential predictions on the same food images, measuring variance in outputs to quantify temporal reliability