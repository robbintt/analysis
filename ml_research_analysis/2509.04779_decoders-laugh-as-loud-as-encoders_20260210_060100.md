---
ver: rpa2
title: Decoders Laugh as Loud as Encoders
arxiv_id: '2509.04779'
source_url: https://arxiv.org/abs/2509.04779
tags:
- humor
- jokes
- language
- zhang
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigated whether Large Language Models (LLMs) can
  understand humor by classifying jokes into six categories: absurdity, dark, irony,
  wordplay, social commentary, and non-jokes. A dataset of 1,392 examples was curated,
  cleaned, and split into train/validation/test sets.'
---

# Decoders Laugh as Loud as Encoders

## Quick Facts
- arXiv ID: 2509.04779
- Source URL: https://arxiv.org/abs/2509.04779
- Reference count: 9
- Primary result: Fine-tuned decoder GPT-4o achieved F1-macro of 0.85, matching best encoder RoBERTa at 0.86 on humor classification task

## Executive Summary
This study investigates whether Large Language Models can understand humor by classifying jokes into six categories: absurdity, dark, irony, wordplay, social commentary, and non-jokes. The researchers curated a dataset of 1,392 examples and tested multiple models including fine-tuned encoders (BERT, RoBERTa variants), zero/few-shot encoder-decoders (BART, Flan-T5), and zero/few-shot decoders (Llama, Gemma, Qwen, Mistral, GPT variants). The key finding demonstrates that a fine-tuned decoder (GPT-4o) performs as well as the best fine-tuned encoder (RoBERTa), challenging the assumption that encoder architectures are inherently superior for classification tasks. This result suggests that with appropriate fine-tuning, decoder architectures designed for generation can achieve competitive classification performance.

## Method Summary
The researchers constructed a dataset of 1,392 jokes and non-jokes across six categories, then manually cleaned the data to remove ambiguous examples and category-revealing keywords. They fine-tuned various encoder models (BERT, RoBERTa, etc.) using standard HuggingFace Transformers with 20 epochs, batch size 16, and selected the best model based on validation F1-macro. For decoders, they tested zero/few-shot performance and fine-tuned GPT-4o via OpenAI API with 3 epochs, batch size 4, and learning rate multiplier 2. All models were evaluated using F1-macro score to account for class imbalance, with RoBERTa-large achieving 0.8503 and fine-tuned GPT-4o achieving 0.8522, statistically indistinguishable per Welch's t-test.

## Key Results
- Fine-tuned GPT-4o decoder achieved F1-macro of 0.85, matching RoBERTa-large encoder at 0.86
- Zero/few-shot decoder performance lagged significantly (GPT-4 zero-shot F1=0.50, few-shot F1=0.60)
- Dataset preprocessing (keyword replacement, multi-label filtering) was critical for reliable classification
- Class imbalance (wordplay: 378, social commentary: 62) necessitated F1-macro over accuracy

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning transforms decoder architectures into competitive classifiers through supervised adaptation. When explicitly trained on labeled humor data, decoders learn to map input text to specific category outputs rather than open-ended generation. Fine-tuned GPT-4o achieved F1-macro of 0.8522, statistically indistinguishable from RoBERTa-large (0.8503) per Welch's t-test. This works because decoders possess sufficient representational capacity to capture humor distinctions when trained with classification objectives, despite being architected for generation rather than classification.

### Mechanism 2
Multi-class humor categorization serves as a proxy for humor comprehension, though this remains an unproven assumption. The paper operationalizes "understanding humor" as accurate classification into six categories. High F1-macro scores suggest the model captures distinguishing features of each humor type, indicating semantic understanding rather than surface-level pattern matching. This mechanism assumes classification accuracy correlates with genuine humor comprehension, not just vocabulary pattern recognition.

### Mechanism 3
Careful data preprocessing reduces confounds and improves classification reliability. Manual filtering removed jokes containing wordplay elements from non-wordplay categories, and category-indicative words (e.g., "ironic" in irony jokes) were replaced with neutral alternatives. This prevents shortcut learning where models achieve high scores by spotting keywords rather than understanding humor. The assumption is that humor categories can be cleanly separated into single-label classifications when explicit category markers are removed.

## Foundational Learning

- **Encoder-only vs. Decoder-only architectures**: Why needed - the paper compares these architectures for classification; encoders use bidirectional attention for representation learning, while decoders use causal attention for generation. Quick check - Can you explain why bidirectional attention might traditionally be favored for classification, and why fine-tuning might close this gap?

- **F1-macro score for imbalanced multi-class classification**: Why needed - the dataset is imbalanced (wordplay: 378, social commentary: 62); F1-macro averages per-class F1 scores to prevent majority-class dominance. Quick check - Why would accuracy alone be misleading for this 6-class imbalanced dataset?

- **Zero-shot, few-shot, and fine-tuning paradigms**: Why needed - the paper tests all three; understanding when each applies is critical for interpreting results (zero/few-shot underperformed, fine-tuning matched encoders). Quick check - What is the key difference between few-shot learning (in-context examples) and fine-tuning (weight updates)?

## Architecture Onboarding

- **Component map**: Dataset (1,392 examples) → Preprocessing (keyword replacement, multi-label filtering) → Train/Val/Test split (80/10/10) → Encoder models (BERT, RoBERTa, DeBERTa, XLNet, ALBERT, ModernBERT, NeoBERT) → Encoder-decoder models (BART, Flan-T5) → Decoder models (Llama, Gemma, Qwen, Mistral, GPT-4, GPT-4o) → Fine-tuning via HuggingFace/PyTorch or OpenAI API → Evaluation using F1-macro

- **Critical path**: 1) Data preprocessing is essential—skip this and models may learn surface shortcuts. 2) Fine-tuning configuration matters: RoBERTa used 20 epochs, batch size 16, fp32; GPT-4o used 3 epochs, batch size 4, reached near-zero training loss quickly (overfitting risk with small data). 3) Evaluation must use F1-macro (not accuracy) due to class imbalance.

- **Design tradeoffs**: Encoder size vs. compute (RoBERTa-large slightly outperforms base but requires more resources); decoder fine-tuning cost vs. flexibility (GPT-4o fine-tuning via API is costly but achieves encoder-level performance; open-source decoders underperformed in zero/few-shot); dataset size vs. generalization (only 1,392 examples limits robustness).

- **Failure signatures**: Zero/few-shot decoders underperform (GPT-4 zero-shot F1=0.5047, few-shot F1=0.5955); encoder-decoder (Flan-T5) few-shot worse than zero-shot (F1 dropped from 0.1575 to 0.1079); GPT-4o training loss near zero (Figure 3 shows rapid overfitting, suggesting data scarcity risk).

- **First 3 experiments**: 1) Baseline encoder: Fine-tune RoBERTa-base on preprocessed dataset with stratified splits; report F1-macro on test set to establish encoder benchmark. 2) Zero/few-shot decoder probe: Test GPT-4o in zero-shot and few-shot modes on same test set; confirm performance gap before fine-tuning. 3) Decoder fine-tuning replication: Fine-tune GPT-4o with 3 epochs, batch size 4, learning rate multiplier 2; compare F1-macro to encoder baseline using Welch's t-test for statistical significance.

## Open Questions the Paper Calls Out

### Open Question 1
To what extent are classification results driven by category-specific semantic biases (e.g., political jargon in "Absurdity") rather than the identification of humor mechanics? The authors note that "semantic meaning that can help classification (e.g. Absurdity was mainly taken from the world of politics jokes) were not taken into consideration." This remains unresolved because the study did not control for topical clusters or vocabulary distinctiveness across categories. Evidence that would resolve this includes confusion matrix analysis on a dataset where semantic domains are evenly distributed across humor categories, or using adversarial examples where topic words are swapped.

### Open Question 2
How does increasing the dataset size beyond 1,392 examples affect the overfitting tendencies and relative performance gap between fine-tuned decoders and encoders? The fine-tuned GPT-4o decoder reached zero training loss very quickly and was "hungry for data," while the "scarce" dataset is listed as a limitation. It is unclear if the decoder's parity with the encoder is a ceiling of the architecture's capability or a result of small dataset constraints limiting the decoder's potential advantage. Scaling the dataset to 10,000+ examples and comparing F1-macro growth curves would resolve this.

### Open Question 3
Can the observed performance parity between fine-tuned decoders and encoders be generalized to non-English humor or culturally specific joke structures? The study strictly validates the finding on English text, while the authors acknowledge that humor "diverges from language to language since there is a cultural difference between nations." The finding remains unknown for non-English humor or tonal languages where puns and wordplay function differently. Applying the same fine-tuning methodology to a multilingual joke dataset would resolve this.

## Limitations

- Dataset size and imbalance: Only 1,392 examples with significant class imbalance (social commentary: 62 examples) limits robustness and generalization
- Manual preprocessing subjectivity: Keyword replacement and multi-label filtering introduce subjective decisions that may not generalize to other humor taxonomies
- English-only scope: Focus on English jokes from specific sources limits cross-cultural applicability and generalizability to non-English humor

## Confidence

- **High confidence**: Fine-tuned GPT-4o achieving F1-macro of 0.85 is well-supported by experimental results and statistical comparison with RoBERTa
- **Medium confidence**: The claim that decoders can "understand humor" as well as encoders—classification performance is demonstrated, but semantic understanding remains an assumption rather than proven
- **Medium confidence**: The effectiveness of preprocessing (keyword replacement, multi-label filtering) in improving classification—while implemented, the causal impact on results is not experimentally isolated

## Next Checks

1. Test the fine-tuned decoder approach on a larger, more diverse humor dataset (e.g., 10K+ examples) to assess whether the encoder-decoder performance parity holds under different data conditions

2. Conduct ablation studies removing the preprocessing steps (keyword masking, multi-label filtering) to quantify their contribution to the reported performance improvements

3. Evaluate the fine-tuned models on humor types not present in the training data to test genuine understanding versus pattern matching