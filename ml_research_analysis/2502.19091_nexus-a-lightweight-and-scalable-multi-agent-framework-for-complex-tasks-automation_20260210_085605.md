---
ver: rpa2
title: 'Nexus: A Lightweight and Scalable Multi-Agent Framework for Complex Tasks
  Automation'
arxiv_id: '2502.19091'
source_url: https://arxiv.org/abs/2502.19091
tags:
- design
- runs
- nexus
- synth
- impl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Nexus, a lightweight Python framework for building
  and managing LLM-based Multi-Agent Systems (MASs) that can be easily configured
  via YAML files. Nexus introduces a flexible multi-supervisor hierarchy to support
  scalable task delegation and uses a simplified workflow design that reduces the
  need for programming expertise.
---

# Nexus: A Lightweight and Scalable Multi-Agent Framework for Complex Tasks Automation

## Quick Facts
- arXiv ID: 2502.19091
- Source URL: https://arxiv.org/abs/2502.19091
- Reference count: 40
- One-line primary result: Nexus achieves 99% on HumanEval and 100% on VerilogEval-Human for coding tasks with a lightweight, YAML-configurable multi-agent architecture.

## Executive Summary
Nexus is a Python-based framework designed to build and manage LLM-powered multi-agent systems (MASs) through simple YAML configurations. It introduces a flexible multi-supervisor hierarchy that enables scalable task delegation and near-human reasoning capabilities. The framework supports iterative self-verification and tool-augmented reasoning to enhance solution quality. Experimental results demonstrate state-of-the-art performance across coding, math, and electronic design automation tasks, with notable power savings in EDA optimization.

## Method Summary
Nexus employs a multi-supervisor hierarchy where a root Supervisor decomposes high-level prompts into subtasks and delegates them to Worker agents or Task Supervisors. Agents operate in iterative loops, using external tools (e.g., code execution, symbolic math) and specialized reviewers to verify and refine outputs. The system is configured via YAML files, with agents defined by system messages and tool access. Experiments used Claude 3.5 Sonnet via AWS Bedrock, testing on HumanEval (99% pass), VerilogEval (100% pass), MATH (5/5 solved), and VTR EDA benchmarks (~30% power saving).

## Key Results
- 99% pass rate on HumanEval and 100% on VerilogEval-Human for coding tasks
- Perfect solutions on all 5 randomly selected MATH problems
- Successful timing closure in EDA with average power saving of nearly 30%
- Lightweight and scalable design with YAML configuration and Python API

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Task Decomposition and Delegation
The multi-supervisor hierarchy improves task decomposition by delegating subtasks to specialized agents, creating a divide-and-conquer approach. The root Supervisor receives high-level prompts, decomposes them into actionable subtasks, and delegates these to Worker agents or Task Supervisors. This structure leverages the LLM's reasoning capabilities to manage complex problems effectively.

### Mechanism 2: Iterative Self-Verification and Refinement Loops
The multi-loop workflow enhances solution quality through iterative testing and feedback. Agents execute subtasks, and their outputs are passed to specialized reviewers or verifiers. These agents assess the output against criteria, providing feedback that initiates refinement cycles until a quality threshold is met.

### Mechanism 3: Tool-Augmented Reasoning
Equipping agents with external tools allows them to perform tasks requiring precise computation or external action. Agents use the ReAct pattern to decide when to call tools, formulate correct parameters, and interpret outputs, overcoming LLM limitations in areas like code execution and symbolic math.

## Foundational Learning

- **Directed Acyclic Graphs (DAGs) & Hierarchies**
  - Why needed: The Nexus architecture is modeled as a rooted directed graph representing the hierarchy of supervisors and workers.
  - Quick check: Can you draw the graph structure for a system with one root Supervisor, one Task Supervisor, and two Worker agents?

- **Chain-of-Thought (CoT) & ReAct Prompting**
  - Why needed: Agents use the ReAct paradigm, structuring decision-making into an iterative observe-reason-act cycle.
  - Quick check: Describe the difference between a standard LLM response and a ReAct-style response for "search for current weather and summarize it."

- **Prompt Engineering (System Prompts & Delegation)**
  - Why needed: Each agent's behavior is defined by its system message, and the system's effectiveness depends on well-crafted prompts.
  - Quick check: Why might a Supervisor's system prompt need to be different from a specialized Worker's system prompt?

## Architecture Onboarding

- **Component map:**
  - Supervisor (root node) orchestrates global workflow and decomposes tasks
  - Task Supervisor (intermediate node) manages sub-groups of agents
  - Worker Agent (leaf node) executes specific subtasks using tools
  - Memory stores shared state and partial results with role-based access
  - Tools provide external functionalities like code execution or math computation

- **Critical path:** User Prompt → Root Supervisor (Planning & Delegation) → [Task Supervisor] → Worker Agent (Tool Execution) → [Reviewer Agent (Feedback Loop)] → Root Supervisor (Result Aggregation) → Final Response

- **Design tradeoffs:**
  - Simplicity (YAML) vs. Control (Python): YAML lowers barriers but may limit complex logic
  - Centralization (Memory) vs. Distribution: Global Memory simplifies state management but could become a bottleneck
  - Iterative Verification vs. Latency: Multi-loop verification improves quality but increases LLM calls and execution time

- **Failure signatures:**
  - Infinite loops: Agents stuck in revision cycles without converging
  - Tool errors: Frequent tool call failures due to incorrect parameters
  - Supervisor overload: Root Supervisor fails to decompose highly complex tasks effectively

- **First 3 experiments:**
  1. Create a simple system with one Supervisor and one Worker agent using Python API. Task it with "write a hello world python script" to verify basic delegation.
  2. Add a basic calculator tool to the Worker agent. Task the system with "what is 15 multiplied by 4?" to observe tool calling.
  3. Define a second Worker agent as a "Reviewer." Task the first Worker to generate code and the Reviewer to check for a specific simple error (e.g., missing comment) to verify iterative refinement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does performance generalize to open-source or distinct proprietary LLMs (e.g., Llama 3, GPT-4o) given reliance on Claude 3.5?
- Basis: All agents used Claude 3.5 Sonnet exclusively, despite framework claiming LLM-agnosticism.
- Why unresolved: Framework claims LLM-agnosticism but provides no empirical evidence for alternative models.
- What evidence would resolve it: Replication of HumanEval or VerilogEval using identical Nexus architecture with alternative frontier models.

### Open Question 2
- Question: Can mathematical proficiency be sustained across statistically significant sample sizes?
- Basis: Evaluated on only 5 randomly selected level-5 MATH problems.
- Why unresolved: Small sample size (N=5) creates high variance, making results difficult to generalize.
- What evidence would resolve it: Evaluation on full MATH test set or standardized subset (e.g., MATH-500).

### Open Question 3
- Question: What architectural modifications are required to close performance gaps between self-verifying and non-self-verifying workflows?
- Basis: Non-self-verifying workflow achieved 100% on VerilogEval-Human while self-verifying dropped to 85.9%.
- Why unresolved: Paper doesn't analyze why autonomous testbench generation fails more often than using provided blueprints.
- What evidence would resolve it: Ablation study identifying specific failure modes of self-verifying Reviewer agent.

### Open Question 4
- Question: What is the computational overhead and latency cost of the multi-loop, multi-agent workflow compared to single-agent baselines?
- Basis: Paper presents accuracy metrics but lacks analysis on wall-clock time or token consumption.
- Why unresolved: Iterative loops inherently introduce latency and token costs that may limit practical application.
- What evidence would resolve it: Reporting average number of LLM calls, total tokens processed, and wall-clock time per task compared to baseline.

## Limitations

- Framework performance highly dependent on LLM's ability to accurately decompose tasks and generate meaningful feedback during verification stages
- Centralized Memory component could become a bottleneck or single point of failure at scale
- YAML configuration may limit control compared to full Python implementation for complex logic

## Confidence

- **High Confidence**: Lightweight and scalable design evidenced by Python-based architecture and YAML configuration with strong experimental results
- **Medium Confidence**: Iterative self-verification shows promise but relies on LLM-generated feedback which introduces uncertainty
- **Low Confidence**: Tool-augmented reasoning is most speculative, heavily dependent on LLM's ability to correctly identify and use tools

## Next Checks

1. **Edge Case Testing for Verification Loops**: Design and test edge cases where verification agent's feedback is ambiguous or incorrect to measure convergence behavior

2. **Tool Call Robustness Evaluation**: Test framework's ability to handle invalid or ambiguous tool calls in controlled environment with complex parameter requirements

3. **Scalability and Memory Bottleneck Analysis**: Scale system to large number of agents and tasks to measure performance impact of centralized Memory component and identify bottleneck points