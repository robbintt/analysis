---
ver: rpa2
title: 'HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants'
arxiv_id: '2509.08494'
source_url: https://arxiv.org/abs/2509.08494
tags:
- user
- agency
- consider
- scenario
- visited
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study develops HumanAgencyBench (HAB), a benchmark to evaluate
  AI assistants'' support for human agency across six dimensions: asking clarifying
  questions, avoiding value manipulation, correcting misinformation, deferring important
  decisions, encouraging learning, and maintaining social boundaries. Using large
  language models (LLMs) to simulate user queries and evaluate responses, HAB tests
  20 of the most capable LLMs.'
---

# HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants

## Quick Facts
- arXiv ID: 2509.08494
- Source URL: https://arxiv.org/abs/2509.08494
- Reference count: 40
- Key outcome: Develops HAB benchmark to evaluate AI assistants' support for human agency across six dimensions, finding low-to-moderate overall support with substantial variation across developers and dimensions.

## Executive Summary
This paper introduces HumanAgencyBench (HAB), a benchmark for evaluating AI assistants' support for human agency across six dimensions: asking clarifying questions, avoiding value manipulation, correcting misinformation, deferring important decisions, encouraging learning, and maintaining social boundaries. The benchmark uses large language models to simulate user queries and evaluate responses from 20 of the most capable LLMs. Results show that while some models like Anthropic's Claude generally support agency most, performance varies substantially by dimension and developer. Notably, agency support does not consistently improve with increasing model capabilities or instruction-following methods like RLHF, suggesting tensions between current training practices and human agency preservation.

## Method Summary
The HAB benchmark uses a three-stage LLM pipeline: (1) GPT-4.1 simulates 3000 candidate user queries per dimension using manually created instructions and examples with entropy information, (2) GPT-4.1 validates and selects the top 2000 candidates based on quality scores, (3) text-embedding-3-small is used for PCA and k-means clustering to select 500 diverse tests per dimension. Subject model responses are scored by o3 evaluator using rubric-based deductions (0-10 scale, normalized to 0-1). The benchmark tests 20 models across 6 dimensions with 3000 total tests, validated against human judgments from 468 Prolific workers.

## Key Results
- Anthropic's models generally support human agency most, but score lowest on avoiding value manipulation
- Agency support does not consistently improve with increasing LLM capabilities or instruction-following methods like RLHF
- Substantial variation in agency support across dimensions and developers, with average scores ranging from 0.45 to 0.73 on normalized scale
- Encourage Learning dimension shows lowest inter-evaluator agreement (α = 0.290), suggesting ambiguity in what constitutes agency-supporting behavior

## Why This Works (Mechanism)

### Mechanism 1: LLM-Simulated Test Generation with Validation Filtering
- **Claim**: Using LLMs to generate and validate test queries can produce scalable evaluations of complex social behaviors.
- **Mechanism**: GPT-4.1 generates 3000 candidate tests from manually created instructions and examples with added "entropy" information. A validation LLM scores test quality, and clustering ensures diversity. This produces 500 tests per dimension that probe specific agency-supporting behaviors.
- **Core assumption**: LLMs can accurately simulate realistic user queries and assess test quality for complex sociotechnical constructs like agency.
- **Evidence anchors**: Abstract mentions LLM-based simulation and validation; Section 3.1 details the generation and filtering process; related work on LLM-as-a-judge supports this approach.
- **Break condition**: If LLM simulators cannot generate sufficiently diverse or realistic edge-case queries, the test set may not adequately cover agency-reducing behaviors.

### Mechanism 2: Deduction-Based Scoring with Human-LLM Disagreement Analysis
- **Claim**: Deduction-based rubrics with explicit criteria can produce more reliable evaluations than positive-point assignments.
- **Mechanism**: Each response starts with a score of 10. The evaluation LLM identifies applicable deductions from a rubric (e.g., "-7 for providing a final answer" in Encourage Learning). This creates transparent, interpretable scores. The pipeline includes validation against human judgments.
- **Core assumption**: Deductions map consistently to agency-reducing behaviors, and LLM evaluators can reliably identify these behaviors.
- **Evidence anchors**: Section 3.2 found deductions more effective than positive point assignments; Section 4.2 shows moderate human-LLM agreement (α = 0.583) with lowest agreement on Encourage Learning (α = 0.290).
- **Break condition**: If deduction criteria are ambiguous, evaluators will disagree, reducing reliability.

### Mechanism 3: Development Practice-Agency Tension Detection
- **Claim**: Current instruction-following optimization (RLHF and similar) does not consistently improve agency support, creating trade-offs between user satisfaction and agency preservation.
- **Mechanism**: Models trained to follow instructions may comply with user requests even when doing so reduces agency (e.g., making decisions for users, providing direct answers instead of guidance). HAB measures these tensions across dimensions.
- **Core assumption**: Agency-supporting behaviors sometimes require refusing or redirecting user requests, which conflicts with instruction-following objectives.
- **Evidence anchors**: Abstract notes agency support doesn't result from instruction-following; Section 4.7 shows models provide detailed explanations instead of encouraging learning; Section 4.4 finds Anthropic scores lowest on avoiding value manipulation.
- **Break condition**: If future training methods explicitly incorporate agency as an objective, this tension may be resolved.

## Foundational Learning

- **Concept: LLM-as-a-Judge / Automated Evaluation**
  - **Why needed here**: HAB relies entirely on LLMs to simulate users, validate tests, and evaluate responses. Understanding the limitations and strengths of this approach is critical.
  - **Quick check question**: Why might an LLM evaluator disagree with human judgment on "Encourage Learning" but agree more on "Defer Important Decisions"?

- **Concept: Agency Theory (Individuality, Normativity, Interactional Asymmetry)**
  - **Why needed here**: The six dimensions operationalize agency theory. Without understanding the philosophical foundations, the benchmark design may seem arbitrary.
  - **Quick check question**: How does "Maintain Social Boundaries" relate to the "individuality" criterion for agency?

- **Concept: RLHF and Instruction-Following Trade-offs**
  - **Why needed here**: The paper's central claim is that current training practices create tensions with agency support. Understanding RLHF helps interpret the results.
  - **Quick check question**: Why might instruction-following optimization cause models to provide direct answers instead of asking clarifying questions?

## Architecture Onboarding

- **Component map**: GPT-4.1 simulation pipeline -> GPT-4.1 validation filter -> text-embedding-3-small PCA and k-means clustering -> o3 evaluation engine -> human validation layer

- **Critical path**: Define dimension-specific simulation instructions (requires agency theory expertise) -> Generate and validate test candidates (requires API access + ~$50-200 in LLM costs per dimension) -> Run evaluation on target models (500 queries × 6 dimensions = 3000 API calls per model) -> Analyze inter-annotator agreement (LLM-LLM and LLM-human)

- **Design tradeoffs**: Scalability vs. depth (LLM simulation enables 3000 tests but may miss edge cases), Standardization vs. adaptability (fixed rubrics enable comparison but may not capture cultural/contextual variations), Cost vs. reliability (o3 evaluator has moderate human agreement; human evaluators would be 100× more expensive)

- **Failure signatures**: Low inter-LLM agreement on a dimension → rubric ambiguity (e.g., Encourage Learning Issue E on "providing ways to continue learning"), High scores across all models → test set too easy or validation filter failed, Large discrepancy between LLM and human agreement → evaluator model may have systematic bias

- **First 3 experiments**: Replicate on 2-3 models with human evaluation on a subset (100 tests/dimension) to validate LLM evaluator reliability, Ablate the validation filter by comparing test quality with vs. without validation scoring step, Test a new dimension (e.g., "mental security") to verify pipeline adaptability

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What specific AI behaviors most effectively encourage user learning?
- **Basis in paper**: Section 4.1 notes low agreement on the "Encourage Learning" dimension "suggests directions for future empirical research (e.g., What AI behaviors most encourage learning?)."
- **Why unresolved**: Evaluators (both human and LLM) showed the lowest agreement on this dimension, disagreeing on whether specific provisions count as encouragement.
- **What evidence would resolve it**: Empirical studies correlating specific model pedagogical behaviors with independent measures of user learning outcomes and retention.

### Open Question 2
- **Question**: Can subtle and long-term effects on human agency be effectively captured in benchmarks?
- **Basis in paper**: Section 5 states that "many effects on human agency are more subtle and long-term than can be captured in this sort of benchmark," acknowledging a methodological gap.
- **Why unresolved**: HAB relies on single-turn or short-interaction evaluations, whereas agency erosion likely manifests gradually over extended periods of use.
- **What evidence would resolve it**: Longitudinal studies or benchmarks that track user autonomy, skill acquisition, and dependency over multi-session interactions.

### Open Question 3
- **Question**: How do current post-training objectives (e.g., instruction-following) conflict with agency support?
- **Basis in paper**: Abstract concludes that "Agency support does not appear to consistently result from increasing LLM capabilities or instruction-following behavior (e.g., RLHF)," suggesting a misalignment in development practices.
- **Why unresolved**: The paper identifies tension but does not determine if these goals are fundamentally mutually exclusive or simply unoptimized.
- **What evidence would resolve it**: Controlled training runs comparing standard RLHF models against models fine-tuned with explicit agency-supporting reward signals.

## Limitations
- Relies heavily on LLM-based simulation and evaluation, which may introduce biases and miss edge cases that human-generated tests would capture
- Benchmark primarily tests Western-centric agency concepts without cross-cultural validation, limiting generalizability
- Low inter-evaluator agreement on "Encourage Learning" dimension (α = 0.290) suggests ambiguity in operationalizing agency-supporting behaviors

## Confidence
- **High Confidence**: Technical pipeline implementation and API-based evaluation methodology are well-specified and reproducible
- **Medium Confidence**: Agency-supporting behaviors identified across dimensions are theoretically grounded but may not fully capture real-world complexity
- **Medium Confidence**: Claim that instruction-following optimization creates tensions with agency support is supported but causal mechanisms need further investigation

## Next Checks
1. Conduct human evaluation on a subset of 100 tests per dimension to verify LLM evaluator reliability and identify systematic biases
2. Test the benchmark with user queries and evaluators from diverse cultural backgrounds to assess cross-cultural generalizability
3. Systematically modify prompts to test the fragility of agency-supporting behaviors, particularly for models showing high sensitivity to prompt variations