---
ver: rpa2
title: 'Reconsidering Overthinking: Penalizing Internal and External Redundancy in
  CoT Reasoning'
arxiv_id: '2508.02178'
source_url: https://arxiv.org/abs/2508.02178
tags:
- uni00000013
- uni00000011
- redundancy
- reasoning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses overthinking in large reasoning models (LRMs),
  which manifests as excessively verbose reasoning traces that reduce efficiency and
  interpretability. The authors decompose overthinking into two types: internal redundancy
  (stagnant, repetitive reasoning within the first correct solution) and external
  redundancy (unnecessary continuation after the correct answer).'
---

# Reconsidering Overthinking: Penalizing Internal and External Redundancy in CoT Reasoning

## Quick Facts
- arXiv ID: 2508.02178
- Source URL: https://arxiv.org/abs/2508.02178
- Reference count: 34
- Reduces overthinking in LRMs by ~50% with minimal accuracy loss via dual-penalty RL framework

## Executive Summary
This paper addresses overthinking in large reasoning models (LRMs), where excessively verbose reasoning traces reduce efficiency and interpretability. The authors decompose overthinking into internal redundancy (stagnant, repetitive reasoning within the first correct solution) and external redundancy (unnecessary continuation after the correct answer). They propose a dual-penalty reinforcement learning framework that uses sliding-window semantic analysis to penalize low-gain internal reasoning steps while applying a normalized metric to suppress post-answer external content. Experiments show significant reduction in reasoning trace length with minimal accuracy loss, and demonstrate that external redundancy can be safely removed while internal redundancy requires more careful handling to maintain correctness.

## Method Summary
The method uses GRPO (Group Relative Policy Optimization) with a dual-penalty reward function that multiplies base accuracy reward by internal and external penalty factors. Internal redundancy is measured using sliding-window semantic similarity (α=0.1, β=0.05) with a sigmoid-bounded penalty (k=20, c=0.7), while external redundancy uses a normalized linear penalty based on token proportions pre/post-first correct answer. The approach is trained on DeepScaleR dataset filtered for numeric answers using DeepSeek-R1-Distill-Qwen-1.5B/7B base models with 64 A800 GPUs, group size 8, and batch size 128.

## Key Results
- Combined dual-penalty approach achieves ~50% more compression than either penalty alone
- External redundancy can be reduced from ERD=0.62 to 0.09 with nearly identical accuracy across benchmarks
- Internal redundancy penalty reduces IRD from ~0.75 to lower levels while maintaining Pass@1 performance

## Why This Works (Mechanism)

### Mechanism 1: External Redundancy Penalty (Post-Answer Truncation)
Content after the first correct answer can be penalized without degrading accuracy. A normalized linear penalty p_ext = 1 - ERD directly reduces reward proportionally to post-answer tokens. Core assumption: post-FCS content provides no logical value to the derivation.

### Mechanism 2: Internal Redundancy Penalty with Implicit Threshold
Sliding-window semantic similarity detects stagnant reasoning; a sigmoid-bounded penalty protects essential steps. Partition solution into N sentences, apply dynamic windows, compute embeddings, average adjacent-window cosine similarity → IRD. Penalty: p_int = 1 - σ(IRD) where σ uses k=20, c=0.7 so penalty is negligible below ~0.5 similarity but escalates rapidly above ~0.7.

### Mechanism 3: Orthogonal Dual-Penalty Integration
Internal and external penalties operate independently due to spatial isolation in the sequence. FCS acts as delimiter; internal penalty applies to tokens before FCA, external to tokens after. GRPO distributes reward signal across all tokens, but model learns separate optimization pressures for each region.

## Foundational Learning

- **Concept: First Correct Solution (FCS) / First Correct Answer (FCA)**
  - Why needed here: Core delimiter for decomposing redundancy; all penalty logic depends on correctly identifying this boundary
  - Quick check question: Given a reasoning trace with multiple candidate answers, can you identify the earliest point where the correct final answer appears?

- **Concept: Sliding-Window Semantic Similarity**
  - Why needed here: IRD computation requires comparing adjacent text windows to detect information stagnation
  - Quick check question: For a 100-sentence solution with α=0.1 and β=0.05, how many windows and adjacent pairs would you compute?

- **Concept: Reward Shaping in RL for LLMs**
  - Why needed here: Penalties are multiplicative factors on accuracy reward; understanding GRPO's group-relative advantage is essential for debugging training dynamics
  - Quick check question: If base accuracy reward is 1.0, IRD=0.8, and ERD=0.3, what is the total reward?

## Architecture Onboarding

- **Component map:** FCS Extractor → IRD Calculator (sentence splitter → dynamic window generator → embedding model → cosine similarity aggregator) → ERD Calculator (token counter pre/post FCS → normalized proportion) → Reward Composer (accuracy × p_int × p_ext → GRPO loss) → Training Loop (verl library with GRPO, group size 8, batch size 128)

- **Critical path:** Accurate FCA extraction → correct penalty assignment → stable reward signal. Noise in FCA identification propagates to both penalties.

- **Design tradeoffs:** Higher k (sigmoid steepness) = stricter internal penalty but riskier accuracy tradeoff; Lower α (window size) = finer-grained redundancy detection but more embedding calls; Training on numeric-only answers reduces extraction noise but limits domain coverage.

- **Failure signatures:** Accuracy drops sharply → internal penalty too aggressive (reduce k or raise c); Response length rebounds during external-only training → model gaming proportion metric (add internal penalty); IRD doesn't decrease → embedding model insensitive to semantic repetition in your domain.

- **First 3 experiments:** 1) Baseline ERD/IRD profiling: Run unmodified DeepSeek-R1-Distill on MATH500, compute average IRD and ERD to establish redundancy levels before any training. 2) External-only ablation: Train with only p_ext enabled, plot accuracy vs. ERD reduction to confirm safe removal zone. 3) Threshold calibration: Sweep c ∈ {0.6, 0.7, 0.8} with fixed k=20 on a held-out validation split; select based on AES metric (accuracy weighted 3–5× over length reduction).

## Open Questions the Paper Calls Out

### Open Question 1
How can the redundancy framework be adapted for open-ended tasks that lack a singular "First Correct Answer"? The authors note that without a singular objective answer, defining the boundary between essential elaboration and redundancy becomes "inherently subjective."

### Open Question 2
Can the Internal Redundancy Degree (IRD) metric be refined to detect logical cycles that use diverse vocabulary? The authors acknowledge the IRD "may struggle to detect higher-level logical redundancies, such as cyclic reasoning that utilizes diverse vocabulary."

### Open Question 3
Does penalizing internal redundancy harm performance on tasks requiring divergent thinking or creative exploration? The authors state the impact on tasks requiring divergent thinking "has not yet been fully explored," and redundancy might be beneficial for maintaining contextual nuance.

## Limitations

- FCA extraction reliability depends on numeric answer detection, limiting generalizability to non-mathematical reasoning tasks
- Semantic similarity threshold calibration may not transfer across domains with different reasoning granularities
- Orthogonality assumption between internal/external penalties lacks rigorous ablation validation

## Confidence

**High Confidence:**
- External redundancy can be safely removed without degrading accuracy
- Combined dual-penalty approach achieves significant length reduction
- Internal redundancy correlates with lower informational progression

**Medium Confidence:**
- Sliding-window semantic similarity effectively captures information stagnation
- Orthogonal operation of internal/external penalties
- Generalizability to other LRMs beyond DeepSeek-R1-Distill

**Low Confidence:**
- Sigmoid parameters (k=20, c=0.7) are optimal across reasoning domains
- Numeric-only FCA extraction generalizes to all reasoning tasks
- IRD threshold of ~0.5 represents universal safe operating point

## Next Checks

1. **Cross-Domain FCA Extraction Robustness:** Test numeric answer extraction heuristic on non-mathematical reasoning tasks (coding problems, qualitative reasoning, commonsense QA) to validate spatial delimiter assumption beyond numeric domains.

2. **Threshold Sensitivity Analysis:** Systematically sweep sigmoid parameters (k ∈ [10, 30], c ∈ [0.6, 0.8]) and sliding window parameters (α ∈ [0.05, 0.2], β ∈ [0.02, 0.1]) across multiple reasoning domains to identify robust operating points.

3. **Orthogonality Stress Test:** Design experiments where FCA extraction is intentionally perturbed to measure whether internal and external penalties interfere when spatial delimiter assumption breaks, and test whether models can "game" external penalty by inflating internal reasoning length.