---
ver: rpa2
title: Contextualizing biological perturbation experiments through language
arxiv_id: '2502.21290'
source_url: https://arxiv.org/abs/2502.21290
tags:
- gene
- genes
- perturbation
- expression
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new benchmark for structured reasoning over
  perturbation experiments, using large language models to predict discrete outcomes
  like differential expression and gene set enrichment. The authors introduce SUMMER,
  an inference-time framework that combines gene summarization, retrieval of experimental
  outcomes, and guided LLM prompting to reason over biological knowledge graphs.
---

# Contextualizing biological perturbation experiments through language

## Quick Facts
- arXiv ID: 2502.21290
- Source URL: https://arxiv.org/abs/2502.21290
- Authors: Menghua Wu; Russell Littman; Jacob Levine; Lin Qiu; Tommaso Biancalani; David Richmond; Jan-Christian Huetter
- Reference count: 40
- Key outcome: Introduces SUMMER framework that matches or exceeds state-of-the-art methods on PerturbQA benchmark for predicting differential expression and gene set enrichment from perturbation experiments

## Executive Summary
This paper proposes SUMMER, a framework for using large language models to predict outcomes of biological perturbation experiments. SUMMER combines gene summarization, retrieval of experimental outcomes, and guided LLM prompting to reason over biological knowledge graphs. On the proposed PerturbQA benchmark, SUMMER demonstrates that language models can effectively capture complex biological relationships for perturbation modeling tasks, while identifying key challenges in causal reasoning and directionality.

## Method Summary
SUMMER is an inference-time framework that predicts outcomes of gene perturbation experiments without fine-tuning. It operates in three stages: (1) generate two LLM-based summaries per gene from knowledge graph entries (perturbation effects and downstream influences), (2) retrieve up to 15 training examples based on graph neighborhood similarity, and (3) use Llama3-8B with chain-of-thought prompting to synthesize predictions using the summaries and retrieved examples. The approach focuses on single-gene perturbations in five single-cell RNA-seq datasets, predicting binary differential expression outcomes and gene set enrichment summaries.

## Key Results
- SUMMER matches or exceeds state-of-the-art methods on the PerturbQA benchmark for predicting differential expression and gene set enrichment
- Retrieval-augmented LLM performance significantly exceeds knowledge-only approaches (GEARS), with LLM (No retrieval) performing at random guessing levels
- The framework demonstrates effective capture of biological relationships while identifying challenges in causal directionality and reasoning errors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Summarization of knowledge graph neighborhoods serves as a text-based "message passing" operation that preserves semantic richness lost in adjacency matrix representations.
- Mechanism: Each gene is characterized by two LLM-generated summaries—one as perturbation (downstream effects) and one as downstream gene (upstream influences)—by concatenating database descriptions and relationship text, then prompting an LLM to synthesize.
- Core assumption: Biological knowledge graph edge semantics (often free text) contain predictive signal that adjacency matrices discard, and LLMs can compress this into useful representations without fine-tuning.
- Evidence anchors:
  - [abstract] "However, current approaches neglect the semantic richness of the relevant biology"
  - [section 4.2] "Inspired by message-passing on graphs, we characterize genes and their relationships to other biological entities by summarizing their graph neighborhoods"
  - [corpus] Weak direct evidence; neighbor papers focus on perturbation prediction but not specifically on summarization-as-message-passing
- Break condition: If knowledge graph coverage is sparse for test genes (paper notes ~3% physical interaction rate), summaries may lack predictive signal.

### Mechanism 2
- Claim: Graph-structured retrieval of experimental outcomes grounds LLM reasoning and prevents hallucination better than knowledge-only prompting.
- Mechanism: For query pair (p,g), retrieve up to 15 training examples (p′,g′) based on shared neighbors in the knowledge graph—prioritizing pairs where both perturbation and gene are graph-adjacent to the query—then inject their discretized outcomes (differential expression, direction) into the prompt.
- Core assumption: Perturbation-gene pairs with similar graph neighborhoods will have similar experimental outcomes, and providing concrete examples constrains the LLM's reasoning.
- Evidence anchors:
  - [section 6.1] "LLM (No retrieval) performs no better than random guessing—highlighting that retrieving experimental outcomes and guiding LLM reasoning are both essential"
  - [section 6.1] "Retrieval (No LLM) takes the mean label... without appealing to the LLM... strong performance"
  - [corpus] SynthPert paper (FMR=0.50) similarly uses synthetic reasoning traces but takes a different approach; direct comparison not available
- Break condition: If retrieved examples are systematically different from test queries (e.g., different cell lines have different responses), graph proximity may not transfer.

### Mechanism 3
- Claim: Chain-of-thought prompting structured around biological reasoning steps elicits more accurate predictions than direct question-answering.
- Mechanism: Rather than asking for direct predictions, SUMMER asks the LLM to: (1) identify similar perturbations, (2) summarize their downstream effects, (3) identify similar downstream genes, (4) identify upstream perturbations affecting them, then (5) synthesize an answer with explicit reasoning.
- Core assumption: Decomposing the prediction task into biologically interpretable sub-questions forces the LLM to engage with causal structure rather than surface patterns.
- Evidence anchors:
  - [section 6.1] "LLM (No CoT) performs no better than random guessing"
  - [section 4.2] "To avoid hallucinations, we found it necessary to dictate that the LLM should consider both the textual summaries and experimental outcomes"
  - [corpus] Adaptive Data-Knowledge Alignment paper discusses similar structured reasoning needs but different mechanism
- Break condition: If the 8B model lacks capacity to distinguish causally-related from merely-correlated gene pairs, chain-of-thought may produce plausible but incorrect reasoning.

## Foundational Learning

- Concept: **Perturb-seq and differential expression**
  - Why needed here: PERTURB QA tasks are defined in terms of differential expression outcomes (p-value thresholds, direction of change), not raw expression values. Understanding how biologists interpret perturbation experiments is essential to understand what the benchmark evaluates.
  - Quick check question: Can you explain why the authors chose binary classification (differential expression: yes/no) rather than regression on log-fold change as their primary task?

- Concept: **Knowledge graphs in biology (Gene Ontology, STRING, etc.)**
  - Why needed here: SUMMER's retrieval and summarization both depend on graph structure. The paper explicitly compares to GEARS (graph-based) and shows LLMs can extract more value from the same graphs by preserving textual semantics.
  - Quick check question: What information is lost when converting a knowledge graph with free-text edge annotations into an adjacency matrix for a graph neural network?

- Concept: **Retrieval-augmented generation (RAG) and chain-of-thought prompting**
  - Why needed here: SUMMER combines both—retrieving experimental outcomes and using structured prompting. Understanding these as compositional building blocks helps see why ablations (no retrieval, no CoT) both fail.
  - Quick check question: Why does the paper retrieve binary experimental outcomes rather than relevant literature abstracts as in standard RAG?

## Architecture Onboarding

- Component map: Summarization module (70B) -> Retrieval module -> Reasoning module (8B)
- Critical path:
  1. Precompute: Gene summaries (two per gene), graph neighborhoods (top-k neighbors)
  2. At inference: Retrieve 15 examples → format prompt → query 8B model → parse answer
  3. Uncertainty: Run 3 retrieval samples, average predictions
- Design tradeoffs:
  - **70B for summarization, 8B for inference**: Computational constraint; summarization is offline, inference must be fast
  - **15 retrieved examples**: Fits within 8k token context window; not tuned
  - **Binary outcomes vs. regression**: Aligns with biologist workflow but discards magnitude information
- Failure signatures:
  - **High abstention rate**: LLM outputs "insufficient evidence" rather than prediction (Table 6 shows this varies by prompting strategy)
  - **Incorrect causal directionality**: Paper identifies this as common error (Appendix B.3)—LLM may predict A affects B when relationship is actually B affects A
  - **Overly generic reasoning**: LLM lists broad pathways without identifying specific mechanism
- First 3 experiments:
  1. **Ablate retrieval source**: Replace graph-based retrieval with random sampling from training set—expect performance drop to near-random, confirming graph structure matters
  2. **Vary example count**: Test 5, 10, 15, 20 retrieved examples to find saturation point; monitor for context window overflow
  3. **Probe failure cases**: Take 50 incorrect predictions from SUMMER, manually classify error type (wrong causal direction, missing knowledge, retrieval failure) to prioritize next improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the PERTURB QA framework be extended to predict the outcomes of combinatorial perturbations (multiple simultaneous gene edits) rather than single gene perturbations?
- Basis in paper: [explicit] The authors note in the Related Work section: "we choose to focus on single gene perturbations, and leave this [combinatorial perturbations] as an opportunity for when better datasets are available."
- Why unresolved: Current benchmark datasets for combinatorial perturbations are too small (e.g., <150 pairs in Norman et al.) to support robust model training and evaluation.
- What evidence would resolve it: The collection of larger-scale combinatorial perturbation datasets and the subsequent adaptation of models like SUMMER to handle multi-gene interactions.

### Open Question 2
- Question: Can integrating foundation models trained on raw biological data (gene counts, sequences) with natural language approaches improve performance on perturbation reasoning tasks?
- Basis in paper: [explicit] The paper states: "In this paper, we approach biological knowledge through natural language, but multimodal integration of foundation models could be a promising future direction."
- Why unresolved: The current study isolates language-based reasoning to test its specific efficacy, while raw biological data models (like scGPT) are treated as distinct baselines.
- What evidence would resolve it: A hybrid model architecture that combines text-based knowledge graphs with sequence or count-based embeddings and demonstrates superior performance on PERTURB QA.

### Open Question 3
- Question: Does increasing model capacity beyond the 8B parameter Llama-3 implementation significantly reduce reasoning errors caused by confusing loosely connected biological concepts?
- Basis in paper: [inferred] The authors note in the error analysis that the 8B model sometimes conflates related but distinct concepts (e.g., general mitochondrial function vs. specific stress response) and suggest "this aspect might be resolved with a higher-capacity model."
- Why unresolved: Computational limitations restricted the main inference model to 8B parameters, leaving the scaling behavior for this specific biological reasoning task untested.
- What evidence would resolve it: Running the SUMMER framework with larger models (e.g., 70B+) and measuring the reduction in specific semantic confusion errors identified in the qualitative analysis.

## Limitations

- Knowledge graph coverage is sparse, with only ~3% physical interaction coverage limiting the approach's applicability to genes with inadequate neighborhood context
- The framework struggles with causal directionality, commonly confusing cause and effect relationships in predictions
- Scalability of the summarization approach is theoretical, with no evidence for how well it would work as biological knowledge grows exponentially or for rare genes with sparse annotations

## Confidence

**High confidence**: The core claim that retrieval-augmented LLMs outperform knowledge-only approaches (GEARS) is well-supported by the ablation studies showing LLM (No retrieval) performs at random guessing levels. The experimental design cleanly isolates the contribution of retrieved outcomes.

**Medium confidence**: The mechanism by which graph-based retrieval improves predictions beyond simple random sampling is plausible but not definitively proven. While the paper shows graph retrieval beats random retrieval, it doesn't fully establish whether graph proximity captures biological similarity or just dataset artifacts.

**Low confidence**: The scalability claims for the summarization approach are largely theoretical. The paper demonstrates effectiveness on existing knowledge graphs but doesn't address how well this would work as biological knowledge grows exponentially or for rare genes with sparse annotations.

## Next Checks

1. **Graph coverage validation**: Systematically measure how many test genes have adequate knowledge graph neighborhoods (e.g., minimum 5 connected neighbors) and analyze performance correlation with graph coverage. This would quantify the extent to which sparse graphs limit the approach.

2. **Causal direction control experiment**: Create a synthetic dataset where ground truth causal directions are known (e.g., transcription factor → target gene relationships), then test whether SUMMER can learn to distinguish correct from reversed causal directions. This would directly address the identified failure mode.

3. **Retrieval source ablation with biological controls**: Compare graph-based retrieval against: (a) random sampling from same cell line, (b) random sampling from different cell lines, and (c) literature-based retrieval (abstracts instead of outcomes). This would isolate whether graph structure captures biological similarity or just experimental context.