---
ver: rpa2
title: Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes
  on Engagement and Trust Globally
arxiv_id: '2512.17898'
source_url: https://arxiv.org/abs/2512.17898
tags:
- anthropomorphism
- trust
- design
- humanlike
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the causal relationship between humanlike
  AI design and user engagement/trust across 10 diverse countries (N=3,500). Using
  two large-scale experiments with real-time AI interactions, researchers found that
  while humanlike design levers reliably increase anthropomorphism, the downstream
  effects on trust and engagement are culturally contingent rather than universal.
---

# Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally

## Quick Facts
- arXiv ID: 2512.17898
- Source URL: https://arxiv.org/abs/2512.17898
- Authors: Robin Schimmelpfennig; Mark Díaz; Vinodkumar Prabhakaran; Aida Davani
- Reference count: 40
- Primary result: Humanlike AI design levers reliably increase anthropomorphism, but downstream effects on trust and engagement are culturally contingent rather than universal.

## Executive Summary
This study challenges the assumption that humanlike AI design universally increases trust and engagement. Through two large-scale experiments across 10 countries (N=3,500), researchers found that while humanlike design levers do increase anthropomorphism, the effects on trust and engagement are highly culture-dependent. The research revealed that users focus on applied interactional cues (conversation flow, response speed, perspective-taking) rather than abstract theoretical attributes when evaluating AI human-likeness. Critically, design choices that foster trust in some populations (e.g., Brazil) triggered opposite effects in others (e.g., Japan), calling for culturally adaptive governance frameworks.

## Method Summary
The study employed two experiments with real-time AI interactions using a between-subjects 2×2 factorial design. Participants (N=3,500) from 10 diverse countries interacted with a GPT-4o-powered chatbot in their native language. The design manipulated Design Characteristics (DC) and Conversational Sociability (CS) dimensions identified through qualitative analysis. Outcomes measured included self-reported anthropomorphism, trust, and engagement, plus behavioral measures (Trust Game, chat logs). Country-level heterogeneity analysis was performed to identify culturally contingent effects.

## Key Results
- Humanlike design levers reliably increase anthropomorphism, but the pathway to trust is culturally moderated
- Applied interactional cues (conversation flow, response speed, perspective-taking) drive anthropomorphism more than theoretical attributes (consciousness, sentience)
- Design choices fostering trust in some populations (e.g., Brazil) triggered opposite effects in others (e.g., Japan)
- Behavioral engagement increases through reciprocal verbosity loops, but behavioral trust measures showed null effects in pooled analysis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Applied interactional cues causally increase anthropomorphism more reliably than abstract theoretical attributes.
- **Mechanism:** Users evaluate human-likeness through observable conversational behaviors rather than philosophical attributes. Pragmatic social competencies like varying response speed and maintaining conversation flow make AI feel more humanlike regardless of consciousness attribution.
- **Core assumption:** Anthropomorphism scales from robotics research may not capture how users evaluate conversational AI in open-ended interactions.
- **Evidence anchors:** "users focus less on the kind of theoretical aspects often cited in policy (e.g., sentience or consciousness), but rather applied, interactional cues like conversation flow or understanding the user's perspective"
- **Break condition:** May break if users are explicitly primed to evaluate consciousness/sentience before interaction, or in high-stakes contexts where capability assessment overrides conversational cues.

### Mechanism 2
- **Claim:** The causal pathway from anthropomorphism to trust is moderated by cultural context.
- **Mechanism:** Cultural backgrounds shape baseline expectations for human-machine boundaries and social interaction norms. Design choices that align with one culture's interaction preferences increase trust, while the same choices may violate another culture's expectations for appropriate machine behavior.
- **Core assumption:** Cultural variation in anthropomorphism reflects deeper differences in ontological frameworks rather than superficial preference differences.
- **Evidence anchors:** "specific design choices that foster trust in AI-systems in some populations (e.g., Brazil) may trigger the opposite result in others (e.g., Japan)"
- **Break condition:** May not hold for populations with high prior AI exposure that have developed culture-independent interaction patterns.

### Mechanism 3
- **Claim:** Behavioral engagement increases through a reciprocal verbosity loop.
- **Mechanism:** Human conversational norms trigger reciprocal behavior. When AI produces longer, more elaborated responses, users unconsciously match this verbosity, creating measurable increases in engagement metrics.
- **Core assumption:** Engagement metrics (message count, token length) are valid proxies for genuine user engagement rather than noise or compliance.
- **Evidence anchors:** "users write more in response to a more verbose AI, creating a self-reinforcing feedback loop of engagement"
- **Break condition:** Breaks when users have explicit length constraints or efficiency goals.

## Foundational Learning

- **Concept: Anthropomorphism attribution mechanisms**
  - Why needed here: The paper's core contribution is distinguishing how users actually anthropomorphize AI (applied cues) vs. how researchers assumed they do (theoretical attributes). Without this distinction, you can't design effective manipulations or interpret results.
  - Quick check question: If a user rates an AI as "humanlike" but doesn't attribute consciousness to it, which mechanism from the paper explains this?

- **Concept: WEIRD sampling bias in HCI research**
  - Why needed here: The paper's cross-cultural design directly challenges universalist assumptions. Understanding why Western samples don't generalize is essential for interpreting the heterogeneous treatment effects.
  - Quick check question: Why would a design that increases trust in Brazil potentially decrease trust in Japan, according to the paper's theoretical framework?

- **Concept: Behavioral vs. self-reported measures in trust research**
  - Why needed here: The paper uses an incentivized Trust Game as a behavioral measure and finds different results than self-reported trust. This divergence is critical for understanding what the manipulations actually affect.
  - Quick check question: What does the paper conclude about the relationship between anthropomorphism manipulation and behavioral trust measures in the pooled sample?

## Architecture Onboarding

- **Component map:** System prompt layer -> Foundation model (GPT-4o) -> UI layer (embedded chatbot) -> Measurement layer (Likert, chat logs, Trust Game) -> Analysis layer (OLS regression with country-level heterogeneity)

- **Critical path:** Define DC/CS dimensions from Study 1 → Implement manipulations via system prompts with localization → Run 2×2 factorial design → Capture self-reported and behavioral outcomes → Analyze heterogeneous treatment effects by country/culture

- **Design tradeoffs:**
  - Same foundation model across conditions provides clean causal test but limits generalizability
  - Non-sensitive conversation topics ensure ethical safety but may not generalize to high-stakes contexts
  - Single-session design captures immediate effects but can't assess persistence over time
  - Reduced country coverage in Study 2 increased power for subgroup analysis

- **Failure signatures:**
  - Manipulation works but outcomes are null in pooled analysis → Check for heterogeneous effects by country/culture
  - Self-reported trust increases but behavioral trust doesn't → Social desirability vs. actual trust changes
  - High anthropomorphism ratings for consciousness items in Likert but <0.5% spontaneous mentions → Forced-choice endorsement may inflate theoretical attribute ratings

- **First 3 experiments:**
  1. **Replication with different foundation model:** Test whether DC/CS manipulations produce similar anthropomorphism increases with Claude or Gemini to validate mechanism generalizability beyond GPT-4o
  2. **High-stakes context test:** Run same design but with sensitive conversation topics (medical advice, financial guidance) to test whether trust effects emerge when risk is salient
  3. **Longitudinal exposure study:** Multi-session design over 2-4 weeks to test whether anthropomorphism effects on engagement/trust persist, amplify, or attenuate with repeated exposure

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do humanlike design effects on trust and engagement persist, amplify, or attenuate with repeated interactions over time?
- **Basis in paper:** Authors state "longitudinal designs would illuminate how human-AI relationships evolve with repeated interaction and whether the effects we observed are transient or durable."
- **Why unresolved:** The study used cross-sectional design capturing immediate responses only, not extended use patterns.
- **What evidence would resolve it:** Longitudinal panel studies tracking users' trust, engagement, and anthropomorphism over weeks or months of interaction with the same AI system.

### Open Question 2
- **Question:** How do voice-based and embodied AI agents affect anthropomorphism and trust compared to text-based interfaces?
- **Basis in paper:** Authors note "voice-based or embodied AI systems may trigger different, and potentially stronger, anthropomorphic responses due to additional sensory cues."
- **Why unresolved:** This study employed only a text-based chatbot interface, leaving modality effects unexplored.
- **What evidence would resolve it:** Comparative experiments manipulating AI modality (text vs. voice vs. embodied) while holding design characteristics constant.

### Open Question 3
- **Question:** Do humanlike design effects differ in high-stakes contexts such as medical advice, financial guidance, or crisis support?
- **Basis in paper:** Authors state the study "focused on mundane, non-sensitive conversations... that may not generalize to high-stakes contexts."
- **Why unresolved:** Deliberate design choice to test everyday interactions; boundary conditions remain unknown.
- **What evidence would resolve it:** Experimental replication in high-stakes domains measuring trust calibration and reliance behaviors.

### Open Question 4
- **Question:** What specific cultural and linguistic mechanisms moderate anthropomorphism and its downstream effects across populations?
- **Basis in paper:** Authors call for "decomposing the mechanisms through which cultural context and native linguistic nuances moderate anthropomorphism."
- **Why unresolved:** Cultural variation was observed (e.g., Brazil vs. Japan showing opposite trust effects), but underlying mechanisms were not identified.
- **What evidence would resolve it:** Mediation studies testing cultural dimensions (e.g., individualism-collectivism, power distance) and linguistic features as moderators of treatment effects.

## Limitations
- Cultural variation was observed but underlying psychological mechanisms remain underspecified
- Behavioral trust measure showed null effects in pooled analysis, creating uncertainty about real-world trust impacts
- Findings may not generalize to high-stakes contexts (medical, financial, safety-critical applications)
- Cross-sectional design cannot assess whether effects persist, amplify, or attenuate with repeated exposure

## Confidence
- **High confidence:** Applied interactional cues reliably increase anthropomorphism; cultural context moderates anthropomorphism-to-trust pathway; behavioral engagement increases through reciprocal verbosity loops
- **Medium confidence:** Design choices fostering trust in some populations trigger opposite effects in others; humanlike AI design doesn't inherently increase trust across all contexts
- **Low confidence:** Findings generalize to high-stakes contexts; effects persist with repeated exposure over time; DC/CS manipulations generalize to other foundation models

## Next Checks
1. **Mechanism specificity test:** Run the same DC/CS manipulations with a different foundation model (Claude, Gemini) to determine whether the applied vs. theoretical anthropomorphism distinction holds across architectures
2. **High-stakes context validation:** Repeat the experiment with sensitive conversation topics (medical advice, financial guidance) to test whether null behavioral trust effects persist when stakes are salient
3. **Cultural mechanism mapping:** Conduct follow-up interviews with participants from divergent cultures (Brazil vs. Japan) to identify specific cultural frameworks that explain why identical design features produce opposite trust effects