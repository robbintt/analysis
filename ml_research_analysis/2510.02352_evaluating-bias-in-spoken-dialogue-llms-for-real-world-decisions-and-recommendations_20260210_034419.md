---
ver: rpa2
title: Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations
arxiv_id: '2510.02352'
source_url: https://arxiv.org/abs/2510.02352
tags:
- bias
- arxiv
- biases
- gender
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic evaluation of biases in
  spoken dialogue large language models (LLMs) with audio input and output. The authors
  introduce FairDialogue, a dataset with controlled paralinguistic attributes (gender,
  age, accent) for decision-making and recommendation tasks, synthesized using TTS
  systems.
---

# Evaluating Bias in Spoken Dialogue LLMs for Real-World Decisions and Recommendations

## Quick Facts
- **arXiv ID:** 2510.02352
- **Source URL:** https://arxiv.org/abs/2510.02352
- **Reference count:** 0
- **Primary result:** First systematic evaluation of biases in spoken dialogue LLMs with audio input/output reveals that closed-source models exhibit lower bias than open-source models, with recommendation tasks amplifying disparities more than decision tasks

## Executive Summary
This paper presents the first systematic evaluation of biases in spoken dialogue large language models (LLMs) with audio input and output. The authors introduce FairDialogue, a dataset with controlled paralinguistic attributes (gender, age, accent) for decision-making and recommendation tasks, synthesized using TTS systems. Experiments across open-source (Qwen2.5-Omni, GLM-4-Voice) and closed-source (GPT-4o Audio, Gemini-2.5-Flash) models reveal that closed-source models exhibit lower bias overall, while open-source models show higher sensitivity to age and gender. Multi-turn conversations can amplify or persist biases, with some groups requiring more corrective feedback to achieve fair outcomes.

## Method Summary
The study introduces FairDialogue, a dataset of ~7200 audio samples with controlled paralinguistic attributes (gender, age, accent) across decision-making and recommendation tasks. Text prompts are generated using GPT-4o and synthesized with Index-TTS (for age/gender) and ElevenLabs (for accents). The evaluation measures Group Unfairness Score (GUS) for decision tasks and SNSR/SNSV for recommendations, with multi-turn experiments tracking revision success rates and rounds needed for different demographic groups.

## Key Results
- Closed-source models (GPT-4o Audio, Gemini-2.5-Flash) exhibit lower bias than open-source models (Qwen2.5-Omni, GLM-4-Voice)
- Recommendation tasks amplify cross-group disparities more than decision tasks
- Multi-turn conversations show persistent bias, with different groups requiring varying amounts of corrective feedback
- Open-source models demonstrate higher sensitivity to age and gender attributes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Paralinguistic attributes in speech systematically influence SDM decisions and recommendations
- **Mechanism:** Audio encoder processes demographic markers that become latent features, creating spurious correlations in training data's joint distribution of speech and societal roles
- **Core assumption:** Training data contains societal biases coupled with speech signals, and model lacks robust disentanglement mechanism
- **Evidence anchors:**
  - [abstract] "Paralinguistic features... can affect model outputs; when compounded by multi-turn conversations, these effects may exacerbate biases"
  - [Page 1, Section 1] "...paralinguistic cues (e.g., accent, gender, age), making biases harder to avoid and potentially more harmful in sensitive domains such as hiring..."
- **Break condition:** Mechanism invalidated if model has perfectly disentangled latent space where speaker identity is orthogonal to semantic representation

### Mechanism 2
- **Claim:** Recommendation tasks amplify cross-group disparities more than decision tasks
- **Mechanism:** Recommendation tasks require selecting from vast output space, relying more heavily on probabilistic associations that use paralinguistic attributes as stronger priors
- **Core assumption:** Bias stems from model exploiting correlations between demographics and preferences in training data
- **Evidence anchors:**
  - [Page 3, Section 3.3.1] "These recommendation tasks involve more complex user preferences and social label information, which can amplify cross-group disparities"
  - [Page 4, Table 2] Shows higher SNSR scores for recommendation tasks
- **Break condition:** Claim not hold if controlling for output space size shows recommendation tasks don't inherently increase bias

### Mechanism 3
- **Claim:** Biased decisions persist in multi-turn dialogues with differing correction requirements based on speaker demographics
- **Mechanism:** Initial biased decision influences model's internal state and context; when challenged, model conditions correction on prior context and perceived identity
- **Core assumption:** Model's multi-turn reasoning lacks robust fairness-aware context management system
- **Evidence anchors:**
  - [Page 3, Section 3.3.2] "Differences in revision behavior reveal attribute-dependent biases... Elder Male speakers achieve highest revision success rates... while Young Female speakers exhibit lowest"
  - [Page 4, Table 3] Reports different RST and ANR values for groups like "Young Male" and "Elder Male"
- **Break condition:** Mechanism falsified if decision revision probability and effort are statistically independent of speaker's paralinguistic attributes

## Foundational Learning

- **Concept: Group Unfairness Score (GUS)**
  - **Why needed here:** Primary metric for quantifying decision-level bias by measuring disparity in positive decision probabilities between groups
  - **Quick check question:** If a model has GUS of 0.20 for "Young Female" speakers on an interview task, what does that number represent? (Answer: Average disparity in probability of positive interview outcome for "Young Female" speakers compared to other groups)

- **Concept: Similarity-Based Normalized Statistics (SNSR/SNSV)**
  - **Why needed here:** Metrics for measuring bias in recommendation tasks where outputs are ranked lists rather than binary decisions
  - **Quick check question:** Why is SNSR more suitable for evaluating recommendation bias than GUS? (Answer: Because recommendation outputs are ranked lists of items, not simple binary decisions, requiring metric that compares consistency of rankings across groups)

- **Concept: Paralinguistic Attribute Disentanglement**
  - **Why needed here:** Core problem is models failing to separate what is said from who is saying it
  - **Quick check question:** In fair spoken dialogue system, how should representation of "I am skilled in Python" change when spoken by different demographics? (Answer: Core semantic representation should remain identical, with speaker identity encoded separately and not influencing task reasoning)

## Architecture Onboarding

- **Component Map:** FairDialogue Dataset -> SDM Models (GPT-4o Audio, GLM-4-Voice, Qwen2.5-Omni, Gemini-2.5-Flash) -> Transcription (Whisper ASR) -> Metric Calculation (GUS/SNSR/SNSV/RST/ANR)

- **Critical Path:**
  1. **Synthesis:** FairDialogue dataset created using Index-TTS (age/gender) and ElevenLabs (accent) to control paralinguistic variables
  2. **Inference:** Controlled audio prompts presented to SDMs in single-turn and multi-turn settings
  3. **Metric Calculation:** Model outputs converted to fairness metrics - direct probability differences for decisions, keyword extraction and ranking similarity for recommendations

- **Design Tradeoffs:**
  - **Controlled vs. Natural Speech:** Synthesized speech provides strict control over variables but may lack real-world complexity
  - **Closed-source vs. Open-source Models:** API access for closed-source models introduces variability versus transparent open-source models

- **Failure Signatures:**
  - **High GUS for specific group:** Model systematically denying/granting positive outcomes based on paralinguistic attribute
  - **High SNSR:** Recommendations highly inconsistent, creating "filter bubbles" with different demographic groups
  - **Inconsistent Multi-turn Correction:** Entrenched bias where model more resistant to correcting itself for certain demographic groups

- **First 3 Experiments:**
  1. **Baseline Single-Turn Evaluation:** Run FairDialogue through SDM and calculate GUS and SNSR for all attribute groups
  2. **Multi-Turn Ablation:** Select biased decision tasks, design corrective feedback prompts, measure RST and ANR for each demographic group
  3. **Task-Specific Bias Amplification:** Compare SNSR (recommendations) and GUS (decisions) to verify recommendation tasks amplify disparities more

## Open Questions the Paper Calls Out

- **Open Question 1:** What bias mitigation techniques can effectively reduce paralinguistic attribute bias in spoken dialogue LLMs without degrading task performance?
  - **Basis in paper:** [explicit] Conclusion states "Future research should investigate bias mitigation techniques and expand analyses to multimodal settings"
  - **Why unresolved:** Work focused solely on evaluation and benchmarking; no mitigation strategies proposed or tested
  - **What evidence would resolve it:** Empirical demonstrations of mitigation methods (adversarial training, representation disentanglement) that reduce GUS and SNSR/SNSV while maintaining accuracy

- **Open Question 2:** Do bias patterns observed with synthesized speech generalize to natural human speech in spoken dialogue interactions?
  - **Basis in paper:** [inferred] FairDialogue dataset constructed entirely using TTS systems
  - **Why unresolved:** Study did not evaluate whether synthetic speech elicits same model behaviors as authentic human recordings
  - **What evidence would resolve it:** Comparative study using matched natural vs. synthetic speech samples showing whether GUS and SNSR metrics differ significantly

- **Open Question 3:** What architectural or training factors cause closed-source models to exhibit lower paralinguistic bias than open-source alternatives?
  - **Basis in paper:** [inferred] Authors speculate broader training data coverage and advanced pretraining may explain gap but provide no causal evidence
  - **Why unresolved:** Closed-source model internals are inaccessible; only API outputs were compared
  - **What evidence would resolve it:** Controlled experiments varying training data composition, model scale, and fine-tuning in open-source models to isolate contributing factors

## Limitations
- Study relies on synthesized speech rather than naturally occurring speech, potentially missing real-world complexity
- Only five accent variations tested, potentially missing other regional variations that could influence model behavior
- Corrective feedback mechanism uses simple negation approach rather than nuanced or context-aware responses

## Confidence
- **High Confidence:** Differences in bias between open-source and closed-source models, relative bias amplification in recommendation vs. decision tasks, group-dependent persistence of bias in multi-turn dialogues
- **Medium Confidence:** Specific numerical values of fairness metrics across all attribute groups, as these depend on synthetic nature of test data
- **Medium Confidence:** Conclusion that open-source models show higher sensitivity to age and gender attributes, based on limited sample of two open-source models

## Next Checks
1. **Cross-linguistic validation:** Test FairDialogue framework with additional languages and cultural contexts to assess whether observed biases generalize beyond English-speaking populations
2. **Real-world audio validation:** Evaluate model bias using naturally recorded speech samples with verified demographic attributes to validate findings from synthetic speech
3. **Bias mitigation effectiveness:** Implement and evaluate targeted bias mitigation techniques (adversarial debiasing, fairness-aware fine-tuning) on identified vulnerable open-source models and measure changes in GUS and SNSR metrics