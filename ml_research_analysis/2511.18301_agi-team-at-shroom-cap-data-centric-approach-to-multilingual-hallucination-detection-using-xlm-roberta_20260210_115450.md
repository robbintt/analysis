---
ver: rpa2
title: '"AGI" team at SHROOM-CAP: Data-Centric Approach to Multilingual Hallucination
  Detection using XLM-RoBERTa'
arxiv_id: '2511.18301'
source_url: https://arxiv.org/abs/2511.18301
tags:
- data
- training
- languages
- language
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The team adopted a data-centric approach to multilingual scientific
  hallucination detection, addressing the challenge of limited and imbalanced training
  data by unifying and balancing five existing datasets into a comprehensive corpus
  of 124,821 samples with a 50/50 class distribution. They fine-tuned XLM-RoBERTa-Large
  (560M parameters) on this enhanced dataset, achieving competitive performance across
  all 9 languages in the SHROOM-CAP 2025 shared task, including 2nd place in Gujarati
  (zero-shot language) with Factuality F1 of 0.5107 and rankings between 4th-6th place
  across the remaining 8 languages.
---

# "AGI" team at SHROOM-CAP: Data-Centric Approach to Multilingual Hallucination Detection using XLM-RoBERTa

## Quick Facts
- arXiv ID: 2511.18301
- Source URL: https://arxiv.org/abs/2511.18301
- Authors: Harsh Rathva; Pruthwik Mishra; Shrikant Malviya
- Reference count: 5
- Primary result: 2nd place in Gujarati (zero-shot) with Factuality F1 of 0.5107

## Executive Summary
The "AGI" team addressed multilingual scientific hallucination detection by unifying and balancing five existing datasets into a comprehensive 124,821-sample corpus with 50/50 class distribution. Using XLM-RoBERTa-Large, they achieved competitive rankings across all 9 languages in SHROOM-CAP 2025, demonstrating that systematic data curation can outperform architectural innovations alone, particularly for low-resource languages. The approach showed strong zero-shot performance in Gujarati (2nd place) and placed 4th-6th across other languages, though a significant validation-to-competition performance gap (F1 0.85 → 0.40-0.51) suggests potential distributional shifts.

## Method Summary
The team adopted a data-centric approach by unifying five existing hallucination datasets into a balanced corpus of 124,821 samples (50% correct, 50% hallucinated). They fine-tuned XLM-RoBERTa-Large (560M parameters) on this unified dataset with full fine-tuning for 3 epochs, weighted cross-entropy loss, and standard hyperparameters (lr=2e-5, batch size 32). The approach focused on systematic data curation rather than architectural complexity, demonstrating that balanced training data eliminates extreme prediction bias and enables effective cross-lingual transfer.

## Key Results
- 2nd place in Gujarati (zero-shot language) with Factuality F1 of 0.5107
- Ranked 4th-6th place across 8 training languages in the SHROOM-CAP 2025 shared task
- Validation macro F1 of 0.8510 vs. competition F1 of 0.40-0.51 highlights distributional shift
- Eliminated extreme prediction bias (99-100% hallucination) through balanced class distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Balanced class distribution resolves model bias toward majority class predictions.
- Mechanism: The 50/50 balance in the 124,821-sample corpus provides equal gradient signals from both classes during training, allowing the model to learn a meaningful decision boundary rather than defaulting to the minority class.
- Core assumption: The model's inability to learn was primarily a data imbalance problem rather than an architectural capacity problem.
- Evidence anchors: Initial experiments showed models trained on limited data exhibited extreme bias, predicting 99-100% of instances as hallucination instead of modeling the decision boundary.

### Mechanism 2
- Claim: Cross-lingual transfer quality depends more on training data diversity than direct language exposure.
- Mechanism: XLM-RoBERTa's pre-training on 100 languages creates shared multilingual representations. When fine-tuned on diverse multilingual hallucination data, the model learns transferable patterns that apply across languages.
- Core assumption: Hallucination patterns share cross-lingual regularities that can be learned from any language with sufficient diversity.
- Evidence anchors: Zero-shot performance in Gujarati exceeds several training languages, suggesting that the quality and diversity of training data is more important than direct language exposure for this task.

### Mechanism 3
- Claim: Data scale and diversity can compensate for architectural simplicity in specialized detection tasks.
- Mechanism: Rather than using larger models or complex prompting pipelines, the approach uses a standard 560M parameter model with 172x more training data, providing broader pattern coverage than sophisticated architectures trained on limited data.
- Core assumption: The task bottleneck is insufficient training signal rather than model capacity.
- Evidence anchors: Systematic data curation can significantly outperform architectural innovations alone, particularly for low-resource languages in zero-shot settings.

## Foundational Learning

- Concept: Class imbalance and its effects on classifier behavior
  - Why needed here: The paper's primary intervention was fixing a 74/26 imbalance that caused 99-100% hallucination predictions.
  - Quick check question: If a binary classifier on 90/10 imbalanced data achieves 90% accuracy, what problem might exist?

- Concept: Cross-lingual transfer learning with multilingual transformers
  - Why needed here: XLM-RoBERTa enables zero-shot transfer to Gujarati. Understanding how shared multilingual representations work explains why training on 5 languages transfers to 4 unseen languages.
  - Quick check question: Why can a model pre-trained on 100 languages classify in a language it was never fine-tuned on?

- Concept: Fine-tuning vs. frozen representations
  - Why needed here: The approach uses full fine-tuning with specific hyperparameters. Understanding what fine-tuning adjusts helps interpret the results.
  - Quick check question: What representation layers change during fine-tuning versus what stays fixed from pre-training?

## Architecture Onboarding

- Component map: Raw datasets (5 sources) → Preprocessing (label normalization, text cleaning, balancing) → Unified corpus (124,821 samples, 50/50 balance) → XLM-RoBERTa tokenizer (max 256 tokens) → XLM-RoBERTa-Large backbone (560M params, 24 layers, 1024 hidden) → Classification head (Dropout 0.1 → Linear → 2 classes) → Binary output (correct/hallucinated)

- Critical path: Data curation is the highest-leverage step. Translation-based augmentation failed because artifacts and domain mismatches degraded quality. The unified corpus without translation augmentation performed best.

- Design tradeoffs:
  - Scale vs. domain specificity: General-domain hallucination data increases scale but may introduce distribution shift from scientific test data
  - Synthetic vs. organic data: The 100k synthetic samples expand coverage but may not match real scientific hallucination patterns
  - Weighted loss vs. resampling: The paper uses both—resampling for 50/50 balance and weighted cross-entropy for residual imbalance

- Failure signatures:
  - False negatives on technical claims: Model predicts "correct" for scientifically implausible statements due to lack of domain knowledge
  - Poor fluency detection: F1=0.15-0.46 vs. factuality F1=0.44-0.51; model struggles with grammatical errors resembling valid stylistic variations
  - Validation-competition gap: F1 drops from 0.85 to 0.40-0.51 indicate distribution shift between unified training data and scientific test benchmarks

- First 3 experiments:
  1. Ablate data sources: Train on each of the 5 datasets independently to measure contribution of each source to final performance
  2. Domain filtering test: Filter the 100k synthetic dataset to only scientific-domain samples and measure impact on the validation-competition gap
  3. Class balance sweep: Train with 60/40, 70/30, and 80/20 (correct/hallucinated) distributions to validate that 50/50 is optimal

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can domain generalization techniques successfully bridge the distributional shift responsible for the large performance gap between high validation scores (macro F1 0.8510) and lower competition scores (F1 0.40-0.51)?
- Basis in paper: The authors explicitly propose "explicitly modeling the distribution shift between validation and test environments through domain generalization techniques" as a concrete future direction.
- Why unresolved: The current work identifies the severe performance drop and hypothesizes causes but does not implement or test methods to mitigate this shift.
- What evidence would resolve it: Experiments utilizing domain adaptation algorithms that result in higher correlation between validation metrics and blind test set performance.

### Open Question 2
- Question: Does incorporating metadata such as "abstract" and "output_logits" significantly improve hallucination detection performance compared to the text-only approach?
- Basis in paper: The paper lists "adding other meta-data such as 'abstract', 'output_logits' to improve the performance of the models" as a specific future step.
- Why unresolved: The current methodology models the task strictly as binary text classification using the LLM-generated text alone.
- What evidence would resolve it: Ablation studies showing performance increases when the source abstract is concatenated to the input or when generator log-probabilities are included as input features.

### Open Question 3
- Question: To what extent would scientific-domain-specific data augmentation outperform the general-domain synthetic data used in the current study?
- Basis in paper: The authors list "developing data augmentation methods that generate scientific-domain hallucinations" as a priority future direction.
- Why unresolved: The current synthetic dataset covers general domains, and error analysis reveals the model struggles with "highly technical scientific claims" requiring knowledge not captured in general pre-training.
- What evidence would resolve it: A comparative evaluation where a model is fine-tuned exclusively on scientifically-grounded hallucinations versus the current general-purpose dataset.

## Limitations
- Reliance on synthetic dataset (hallucination_dataset_100k) representing 80% of training data without full methodological disclosure
- Large validation-to-competition performance gap (F1 0.85 → 0.40-0.51) suggests potential domain mismatch between synthetic training data and scientific evaluation benchmarks
- Zero-shot Gujarati performance is promising but achieved with no direct training examples, making broader generalization uncertain

## Confidence

- **High confidence**: Class balancing mechanism and its effect on eliminating prediction bias
- **Medium confidence**: Cross-lingual transfer effectiveness (limited to single zero-shot language case)
- **Medium confidence**: Data scale compensating for architectural simplicity (primarily compared to translation-based approaches)

## Next Checks

1. **Domain transfer validation**: Filter the 100k synthetic dataset to retain only scientific-domain samples and retrain. Compare whether the validation-competition F1 gap narrows.

2. **Zero-shot transfer robustness**: Apply the trained model to an additional unseen language (e.g., Arabic or Chinese scientific text) to test whether cross-lingual transfer generalizes beyond Gujarati.

3. **Hallucination type decomposition**: Analyze model performance by hallucination category (factual errors, fabricated details, subtle hallucinations) on the validation set. Determine whether the balanced dataset improves detection across all types equally.