---
ver: rpa2
title: Two Spelling Normalization Approaches Based on Large Language Models
arxiv_id: '2506.23288'
source_url: https://arxiv.org/abs/2506.23288
tags:
- translation
- spelling
- pages
- language
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores two novel spelling normalization approaches
  based on large language models (LLMs) for historical documents. The first approach
  fine-tunes mT5, a multilingual LLM pre-trained without supervised training, while
  the second fine-tunes mBART, an LLM pre-trained for machine translation.
---

# Two Spelling Normalization Approaches Based on Large Language Models

## Quick Facts
- arXiv ID: 2506.23288
- Source URL: https://arxiv.org/abs/2506.23288
- Reference count: 7
- This study explores two novel spelling normalization approaches based on large language models (LLMs) for historical documents

## Executive Summary
This study presents two novel spelling normalization approaches based on large language models for historical documents. The researchers fine-tune mT5 (a multilingual LLM pre-trained without supervised training) and mBART (an LLM pre-trained for machine translation) and evaluate them against character-based statistical and neural machine translation baselines. The methods are tested across Spanish and Slovene historical datasets, demonstrating that both LLM-based approaches significantly improve spelling normalization over baseline methods, with mBART performing better than mT5, particularly on smaller datasets.

## Method Summary
The researchers develop two spelling normalization approaches using large language models. The first approach fine-tunes mT5, a multilingual LLM pre-trained without supervised training, while the second fine-tunes mBART, an LLM pre-trained for machine translation tasks. Both methods are evaluated alongside character-based statistical and neural machine translation baselines across Spanish and Slovene historical datasets. The evaluation measures performance using standard spelling normalization metrics across the different approaches and datasets.

## Key Results
- Both LLM-based approaches significantly improve spelling normalization over baseline methods
- mBART outperforms mT5, particularly on smaller datasets
- Character-based statistical machine translation remains the most effective approach across all evaluated datasets and metrics

## Why This Works (Mechanism)
The LLM-based approaches work by leveraging the pre-trained linguistic knowledge of mT5 and mBART models, which have learned general language patterns during pre-training. Fine-tuning these models on spelling normalization tasks allows them to apply contextual understanding to map historical spelling variations to modern equivalents. The mBART model's pre-training specifically for translation tasks gives it an advantage in handling cross-temporal language variations, while mT5's multilingual training provides broader linguistic coverage but may be less specialized for the normalization task.

## Foundational Learning
- **Historical document processing**: Understanding spelling variations across time periods is crucial for normalization tasks, as historical texts often contain archaic spellings and inconsistent orthography
- **Character-based statistical machine translation**: This baseline approach uses statistical models to translate character sequences from historical to modern spelling, providing a strong benchmark for comparison
- **Large language model fine-tuning**: Adapting pre-trained LLMs like mT5 and mBART to specific normalization tasks leverages their broad linguistic knowledge while specializing them for historical text processing

## Architecture Onboarding
- **Component map**: Historical text -> Character-based statistical baseline -> Neural machine translation baseline -> mT5 fine-tuned model -> mBART fine-tuned model -> Evaluation metrics
- **Critical path**: Input historical text → Character-level processing → Normalization model → Output modern spelling
- **Design tradeoffs**: LLM-based approaches offer contextual understanding but require more computational resources, while character-based methods are more efficient but may lack semantic awareness
- **Failure signatures**: Poor performance on highly archaic spellings, limited generalization to unseen historical variations, computational bottlenecks with larger models
- **First experiments**: 1) Evaluate baseline performance on Spanish dataset, 2) Test mT5 fine-tuning on Slovene dataset, 3) Compare mBART performance across different dataset sizes

## Open Questions the Paper Calls Out
The paper raises questions about the scalability of LLM-based approaches to additional languages and historical periods, the potential for hybrid approaches combining statistical and neural methods, and the practical deployment considerations including computational costs and efficiency requirements for real-world applications.

## Limitations
- Evaluation focuses on only two languages (Spanish and Slovene) and relatively small historical datasets
- Does not explore hybrid approaches combining LLM-based and traditional statistical methods
- Does not address computational costs or training efficiency for practical deployment

## Confidence
- Character-based statistical machine translation remains most effective across all datasets and metrics: High confidence
- Both LLM-based approaches significantly improve over baseline methods: Medium confidence
- mBART performs better than mT5 on smaller datasets: Medium confidence

## Next Checks
1. Evaluate these approaches on additional historical languages and larger datasets to test generalizability
2. Conduct ablation studies to identify which specific aspects of the LLM architectures contribute most to normalization performance
3. Perform cost-benefit analysis comparing computational requirements against normalization accuracy improvements to assess practical viability for production environments