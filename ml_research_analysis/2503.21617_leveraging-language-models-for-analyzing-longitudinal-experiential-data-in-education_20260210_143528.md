---
ver: rpa2
title: Leveraging Language Models for Analyzing Longitudinal Experiential Data in
  Education
arxiv_id: '2503.21617'
source_url: https://arxiv.org/abs/2503.21617
tags:
- data
- experiential
- performance
- dataset
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of using pre-trained language
  models to forecast STEM student academic trajectories using longitudinal experiential
  data. The authors develop a comprehensive dataset combining cognitive, non-cognitive,
  and background modalities, and propose a data enrichment approach that handles missing
  values, augments sequences, and incorporates task-specific instructions.
---

# Leveraging Language Models for Analyzing Longitudinal Experiential Data in Education

## Quick Facts
- **arXiv ID:** 2503.21617
- **Source URL:** https://arxiv.org/abs/2503.21617
- **Reference count:** 35
- **Primary result:** Models achieve up to 96.8% accuracy when fine-tuned on all three data modalities (cognitive, non-cognitive, background) simultaneously.

## Executive Summary
This study addresses the challenge of using pre-trained language models to forecast STEM student academic trajectories using longitudinal experiential data. The authors develop a comprehensive dataset combining cognitive, non-cognitive, and background modalities, and propose a data enrichment approach that handles missing values, augments sequences, and incorporates task-specific instructions. They evaluate both decoder-only and encoder-decoder LMs, finding that models perform best when fine-tuned with all three modalities, achieving up to 96.8% accuracy. However, LMs rely on high-level statistical patterns rather than deeper semantic understanding, and struggle to interpret explicit temporal cues. The research demonstrates the potential and limitations of LMs for early academic intervention using experiential data.

## Method Summary
The authors create a dataset from 48 STEM students tracked over 13 weeks, collecting cognitive scores (math/physics assessments), non-cognitive self-reports (motivation, resilience, engagement), and background information (demographics, prior academic performance). They verbalize all data into natural language sequences, replacing missing values with "Skipped the question" descriptors rather than traditional imputation. The sequences are augmented through oversampling and synonym replacement. Two LM architectures are evaluated: LLaMA 2 7B (decoder-only, fine-tuned with QLoRA) and FLAN-T5 770M (encoder-decoder, fully fine-tuned). Models are trained to classify students into four risk categories and evaluated across different observation windows and modality combinations.

## Key Results
- Models achieve peak performance (96.8% accuracy) when fine-tuned on all three modalities simultaneously
- Explicit temporal markers ("In week X") provide minimal predictive value; LMs rely on statistical patterns
- Contextually meaningful missing value descriptors ("Skipped the question") outperform generic placeholders like "N/A"
- Longer observation windows (4 weeks vs 2 weeks) improve accuracy but delay intervention timing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LMs achieve peak forecasting accuracy when fine-tuned on all three data modalities (cognitive, non-cognitive, background) simultaneously rather than individually.
- Mechanism: Pre-trained LMs can integrate heterogeneous data types—verbalized numerical scores, qualitative self-reports, and demographic text—by treating them as unified language sequences, enabling cross-modal correlation learning that single-modality training cannot capture.
- Core assumption: Predictive signal exists in the correlations between modalities, not just within each modality.
- Evidence anchors:
  - [abstract]: "models perform best when fine-tuned with all three modalities, achieving up to 96.8% accuracy"
  - [section III.A, RQ2]: "both LLaMA and FLAN-T5 LMs achieved their peak performance when fine-tuned with a combination of three modalities, highlighting the importance of a multi-modal approach"
  - [corpus]: Limited direct support—related papers address temporal/clinical modeling but not multi-modal educational forecasting specifically
- Break condition: If cross-modal temporal alignment is severely disrupted or one modality has extreme missingness (>80%), correlation learning degrades substantially.

### Mechanism 2
- Claim: High forecasting accuracy stems from surface-level statistical pattern matching rather than genuine temporal comprehension.
- Mechanism: LMs capture aggregate statistical associations across the training distribution without developing causal or semantic understanding of chronological progression; explicit temporal tags provide minimal utility.
- Core assumption: The forecasting task is solvable via statistical correlation without requiring true temporal reasoning.
- Evidence anchors:
  - [abstract]: "LMs rely on high-level statistical patterns rather than demonstrating a deeper understanding of temporal dynamics. Furthermore, their ability to interpret explicit temporal information remains limited"
  - [section III.A, RQ3(c)]: "despite preserving the weekly tags, there was no significant improvement in model accuracy over the partially randomized scenario... LLaMA does not seem to pick up the explicitly encoded temporal signals"
  - [corpus]: Early Risk Prediction with Temporally Grounded Clinical LP (arXiv:2511.22038) notes similar challenges leveraging temporal information in clinical notes
- Break condition: Full temporal randomization (weeks and days shuffled) causes 21-28% accuracy drop, indicating residual sensitivity to sequence order despite weak explicit temporal understanding.

### Mechanism 3
- Claim: Contextually meaningful text descriptors for missing values outperform generic placeholders by leveraging LMs' pre-trained semantic knowledge.
- Mechanism: Framing missingness as behavioral signal ("Skipped the question") allows the LM to interpret non-response as potentially predictive rather than treating it as noise; generic descriptors like "N/A" lose this semantic richness.
- Core assumption: Missing responses carry predictive information about student engagement or behavior patterns.
- Evidence anchors:
  - [section II.C]: "eschewed traditional data imputation strategies in favor of inserting a contextually relevant descriptor text, specifically 'Skipped the question'"
  - [section III.A, RQ4]: "using 'Skipped the question' led to the highest performance... even though 'N/A' is contextually correct, its use resulted in a marked performance drop (7.4% for LLaMA, 14% for FLAN-T5)"
  - [corpus]: Imputation-related papers (Impute-MACFM, arXiv:2509.23126) address missing data but focus on generative approaches, not semantic framing for LMs
- Break condition: Robustness persists even with contextually incorrect descriptors ("Hello, World!") with <3-10% drop, suggesting the act of flagging missingness matters more than descriptor semantics once a threshold is crossed.

## Foundational Learning

- **Transfer Learning via Task Reframing**
  - Why needed here: The approach converts time-series forecasting into conditional language generation—understanding this re-framing is essential for adapting LMs to non-NLP domains.
  - Quick check question: Why can a model trained on internet text predict student grades from verbalized survey responses?

- **Attention Mechanisms vs. Explicit Temporal Encoding**
  - Why needed here: The paper's finding that explicit temporal tags don't improve performance requires understanding how self-attention inherently captures sequence order versus how it processes explicit textual markers.
  - Quick check question: How does positional encoding in transformers differ from providing explicit "In week X" text cues?

- **Missing Data as Signal vs. Noise**
  - Why needed here: The semantic framing of missing values departs from traditional imputation; grasping when non-response is informative vs. problematic is critical for similar longitudinal applications.
  - Quick check question: In what scenarios would "Skipped the question" carry different predictive information than random data loss?

## Architecture Onboarding

- **Component map:** Raw longitudinal data → Verbalization of numerical scores → Missing value descriptor injection → Oversampling + synonym augmentation → Task instruction prepending → LM fine-tuning

- **Critical path:** 
  1. Feature selection to fit context window (5 background + 10 cognitive + 3 non-cognitive = 18 features verbalized within 512 tokens for FLAN-T5)
  2. Missing value strategy: Replace with "Skipped the question" (not imputation)
  3. Class balancing via oversampling (48 → 144 samples)
  4. Fine-tune with combined modalities for best accuracy

- **Design tradeoffs:**
  - Larger model (LLaMA 7B) yields higher accuracy but requires more GPU memory and QLoRA for efficiency
  - Longer observation windows (4-week) improve accuracy but delay intervention timing
  - More features improve signal but increase token count; must prioritize within context limits

- **Failure signatures:**
  - Full temporal randomization: 21-28% accuracy drop (but not catastrophic failure)
  - Generic missing descriptors ("N/A"): 7-14% accuracy loss vs. contextual descriptors
  - Cognitive-only training: Underperforms non-cognitive-only, suggesting experiential data carries stronger early signal

- **First 3 experiments:**
  1. **Modality ablation:** Train separate models on (a) non-cognitive only, (b) cognitive only, (c) background only, (d) all combined—using 2/3/4-week windows to identify minimum observation period for acceptable accuracy.
  2. **Temporal randomization battery:** Compare baseline (ordered data) against full randomization, partial randomization (weeks shuffled, days preserved), and pseudo-randomization (with explicit tags retained) to diagnose whether models exploit temporal structure.
  3. **Missing value descriptor comparison:** Test "Skipped the question" vs. "N/A" vs. semantically incorrect placeholder to validate that contextual framing—not just missingness flagging—drives performance gains.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does increasing the dataset size beyond the current study (N=48) improve the fine-tuning of language models and the extraction of insights regarding non-cognitive features?
- **Basis in paper:** [explicit] The Conclusion states, "Moving forward, expanding the dataset will be critical to improving fine-tuning and gaining deeper insights, particularly into non-cognitive features."
- **Why unresolved:** The current dataset was limited by the high cost of longitudinal data collection, restricting the sample size and potentially limiting the model's ability to generalize.
- **What evidence would resolve it:** A replication of the study using a significantly larger cohort of students to validate if the proposed data enrichment methods maintain high accuracy and yield deeper feature analysis.

### Open Question 2
- **Question:** Can language models be modified to utilize explicit temporal markers and understand semantic context rather than relying primarily on surface-level statistical patterns?
- **Basis in paper:** [inferred] The authors observe in the Conclusion that models "primarily rely on high-level statistical patterns, lacking deeper semantic understanding of temporal dynamics" and fail to use explicit temporal tags effectively.
- **Why unresolved:** The "pseudo randomization" experiment showed that models ignored explicit chronological cues (e.g., "In week X"), suggesting the model's reasoning is shallow despite high forecasting accuracy.
- **What evidence would resolve it:** Development of specific pre-training objectives or prompting strategies that force the model to attend to temporal sequence order, resulting in performance degradation when temporal cues are removed or shuffled.

### Open Question 3
- **Question:** Are the findings regarding the handling of missing values (using contextually relevant descriptors) generalizable to other educational datasets with different missingness mechanisms?
- **Basis in paper:** [inferred] The study eschews traditional imputation (like LOCF) for text descriptors because "entire sets of daily responses were missing," but it remains unclear if this approach is optimal for random vs. systematic missingness.
- **Why unresolved:** The paper tests this only on a specific dataset where missingness often meant a skipped question; it may not hold for datasets where missing values imply a lack of activity rather than a lack of response.
- **What evidence would resolve it:** Comparative experiments on datasets with different missing data distributions (e.g., MAR vs. MCAR) comparing the proposed text descriptor method against standard numerical imputation techniques.

## Limitations

- **Limited context window constraints:** The FLAN-T5 770M model is restricted to 512 tokens, forcing aggressive feature selection that may have excluded potentially predictive features.
- **Evaluation metric singularity:** The study relies primarily on classification accuracy without reporting precision, recall, or F1 scores for each risk category, particularly concerning for the "at-risk" category where false negatives have the highest cost.
- **Missing data interpretation assumption:** The paper assumes missing responses are meaningful behavioral signals, but this assumption remains untested across different question types or student populations.

## Confidence

- **High confidence** in findings about modality importance (RQ2): The ablation study shows clear, consistent performance gains from combining all three modalities across both model architectures. The effect size is substantial and the pattern is reproducible.
- **Medium confidence** in temporal understanding limitations (RQ3): While the experiments demonstrate LMs don't leverage explicit temporal cues, the study doesn't fully distinguish between poor temporal reasoning versus effective exploitation of other temporal patterns (like response consistency over time).
- **Medium confidence** in missing value descriptor effectiveness (RQ4): The performance differences between descriptors are statistically clear, but the surprising robustness to semantically incorrect descriptors suggests the mechanism may be simpler than semantic interpretation—possibly just effective missingness flagging.

## Next Checks

1. **Temporal robustness test:** Train and evaluate models on sequential academic years (e.g., 2018-2019, 2019-2020, 2020-2021) to measure cross-year generalization. Compare performance drop between within-year and across-year predictions to quantify temporal overfitting.

2. **Risk category imbalance analysis:** For each risk category, compute precision, recall, and F1 scores. Conduct a cost-sensitive analysis where false negatives in "at-risk" predictions are weighted 10× higher than other errors to evaluate real-world intervention utility.

3. **Missing data mechanism validation:** Conduct an A/B test where students are randomly assigned to complete surveys with different missing data treatments (contextual descriptors vs. random dropout). Compare model predictions to actual academic outcomes to determine whether contextual missingness truly carries predictive signal beyond simple dropout patterns.