---
ver: rpa2
title: 'DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous Dataflow
  Graphs'
arxiv_id: '2505.23131'
source_url: https://arxiv.org/abs/2505.23131
tags:
- doppler
- time
- learning
- assignment
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses device assignment in work-conserving systems
  for multi-GPU computing, focusing on complex ML workloads. The core contribution
  is DOPPLER, a three-stage framework that uses dual-policy learning with separate
  policies for selecting operations and placing them on devices.
---

# DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous Dataflow Graphs

## Quick Facts
- **arXiv ID:** 2505.23131
- **Source URL:** https://arxiv.org/abs/2505.23131
- **Reference count:** 40
- **Primary result:** Up to 62.5% lower execution times than best baseline on 4 NVIDIA P100 GPUs

## Executive Summary
This paper addresses the challenge of device assignment in work-conserving multi-GPU systems for complex ML workloads. The authors introduce DOPPLER, a three-stage framework that employs dual-policy learning with separate policies for operation selection and device placement. By combining imitation learning, simulation-based reinforcement learning, and real-system reinforcement learning, DOPPLER achieves significant performance improvements over existing baselines. The approach effectively balances workload across GPUs while minimizing execution time, demonstrating superior performance to both heuristic methods and enumerative optimizers.

## Method Summary
DOPPLER employs a three-stage training approach that separates concerns between selecting operations and placing them on devices. The framework first uses imitation learning to initialize policies, then refines them through simulation-based reinforcement learning, and finally adapts to the real system through additional reinforcement learning. This dual-policy design allows for more efficient learning by decoupling the selection and placement decisions. The work-conserving system ensures that GPUs remain busy while respecting device constraints, and the framework is specifically designed for asynchronous dataflow graphs common in ML workloads.

## Key Results
- Achieves up to 62.5% lower execution times than the best baseline on 4 NVIDIA P100 GPUs
- Outperforms enumerative optimizer by up to 13.8% in execution time reduction
- Demonstrates effective load balancing across multiple GPUs while minimizing overall execution time

## Why This Works (Mechanism)
The dual-policy approach separates the complex decision-making process into two distinct policies: one for selecting which operations to execute and another for determining device placement. This separation allows each policy to specialize in its respective domain, leading to more efficient learning and better overall performance. The three-stage training methodology progressively refines the policies from supervised learning through simulation to real-world adaptation, ensuring robustness across different execution environments.

## Foundational Learning
- **Asynchronous Dataflow Graphs:** Why needed - ML workloads often involve complex dependencies between operations; Quick check - Understanding of node dependencies and parallel execution opportunities
- **Work-Conserving Systems:** Why needed - Ensures optimal GPU utilization while respecting device constraints; Quick check - Ability to identify idle GPU resources and assign appropriate operations
- **Reinforcement Learning:** Why needed - Enables adaptive policy learning for complex decision spaces; Quick check - Familiarity with policy gradient methods and reward shaping
- **Imitation Learning:** Why needed - Provides good initial policy estimates from expert demonstrations; Quick check - Understanding of behavioral cloning and its limitations
- **Multi-GPU Placement:** Why needed - Critical for performance in distributed ML training; Quick check - Knowledge of device memory constraints and communication costs
- **Simulation-Based Training:** Why needed - Allows safe exploration before real-system deployment; Quick check - Understanding of simulation-to-reality gaps and mitigation strategies

## Architecture Onboarding

**Component Map:** Graph Parser -> Operation Selector Policy -> Device Placer Policy -> Simulator/Real System

**Critical Path:** The critical execution path involves parsing the dataflow graph, selecting operations through the selection policy, placing them via the placement policy, and executing on the target system while respecting dependencies and device constraints.

**Design Tradeoffs:** The dual-policy design trades off the complexity of a single unified policy for potentially better specialization and learning efficiency. The three-stage training approach balances the need for good initial performance (imitation learning) with the ability to adapt to real-world conditions (RL stages).

**Failure Signatures:** Performance degradation may occur when policies fail to properly balance load across GPUs, when operation selection doesn't respect critical path dependencies, or when the simulation environment doesn't accurately reflect real-system behavior.

**First Experiments:**
1. Validate baseline performance of individual policies in isolation
2. Test policy coordination by running integrated dual-policy execution on small graphs
3. Compare simulation results against real-system execution on a subset of workloads

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Results evaluated only on NVIDIA P100 GPUs with up to 4 devices, limiting generalizability
- Three-stage training approach introduces complexity in policy coordination
- Limited validation on non-ML asynchronous dataflow graphs such as scientific computing workloads

## Confidence
- **High** - Algorithmic framework and training methodology are well-documented and theoretically sound
- **Medium** - Performance improvements are based on specific hardware configuration and workload set
- **Medium** - Comparative analysis is thorough but lacks ablation studies on individual policy components

## Next Checks
1. Test DOPPLER on heterogeneous GPU clusters mixing different GPU generations to assess adaptability
2. Evaluate performance on non-ML asynchronous dataflow graphs like scientific computing or data processing pipelines
3. Conduct ablation studies to quantify individual contributions of selection and placement policies to overall performance