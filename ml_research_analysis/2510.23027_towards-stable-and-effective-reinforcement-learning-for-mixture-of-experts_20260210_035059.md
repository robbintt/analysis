---
ver: rpa2
title: Towards Stable and Effective Reinforcement Learning for Mixture-of-Experts
arxiv_id: '2510.23027'
source_url: https://arxiv.org/abs/2510.23027
tags:
- training
- routing
- grpo
- router
- gmpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the instability problem in applying reinforcement
  learning (RL) with verifiable rewards (RLVR) to Mixture-of-Experts (MoE) language
  models. The core issue is "router drift," where expert routing probabilities change
  substantially across policy updates, amplifying off-policy mismatch and leading
  to reward collapse.
---

# Towards Stable and Effective Reinforcement Learning for Mixture-of-Experts

## Quick Facts
- arXiv ID: 2510.23027
- Source URL: https://arxiv.org/abs/2510.23027
- Authors: Di Zhang; Xun Wu; Shaohan Huang; Lingjie Jiang; Yaru Hao; Li Dong; Zewen Chi; Zhifang Sui; Furu Wei
- Reference count: 5
- Primary result: RSPO improves RLVR stability and effectiveness for MoE models by addressing router drift through per-token importance weighting

## Executive Summary
This paper addresses the critical instability problem in applying reinforcement learning with verifiable rewards (RLVR) to Mixture-of-Experts (MoE) language models, specifically focusing on "router drift" where routing probabilities change substantially across policy updates. The authors propose Router-Shift Policy Optimization (RSPO), which computes per-token router-shift ratios from previously activated experts and uses these as trust weights to rescale importance ratios before clipping and aggregation. Experiments on Qwen3-30B-A3B demonstrate that RSPO achieves improved stability and performance, with Pass@1 accuracy of 77.1 on math benchmarks and 85.2 on code benchmarks, outperforming existing methods while maintaining more stable training dynamics.

## Method Summary
The core innovation is Router-Shift Policy Optimization (RSPO), which addresses router drift in MoE RLVR by computing a per-token router-shift ratio from the previously activated experts. This ratio is calculated as the product of floor-bounded router probabilities from the current and previous policies, with stop-gradient applied to prevent gradient propagation through the shift computation. The resulting trust weight is used to rescale importance ratios before the standard clipping and aggregation steps in policy optimization. This soft adjustment mechanism reduces the influence of tokens with severe routing deviations while preserving the router's ability to adapt to the evolving policy, thereby mitigating the amplification of off-policy mismatch that leads to reward collapse.

## Key Results
- RSPO achieves Pass@1 accuracy of 77.1 on math benchmarks, improving over GMPO/GSPO at 76.4
- RSPO achieves 85.2 on code benchmarks, substantially outperforming GMPO at 82.5
- Demonstrates more stable training dynamics and higher reward trajectories compared to GRPO baselines

## Why This Works (Mechanism)
Router drift occurs when routing probabilities change substantially across policy updates, creating an off-policy mismatch that RLVR algorithms struggle to handle. This mismatch amplifies over training, leading to reward collapse and unstable optimization. RSPO addresses this by computing per-token router-shift ratios that quantify the discrepancy between current and previous routing decisions. By using these ratios as trust weights to rescale importance ratios, RSPO effectively downweights the contribution of tokens experiencing severe routing deviations, preventing them from destabilizing the policy update. The stop-gradient operation prevents the shift computation from creating a feedback loop, while the lower-bound flooring ensures numerical stability and prevents division by near-zero probabilities.

## Foundational Learning

**Importance Sampling in RL** - Why needed: Essential for handling off-policy data in RLVR when trajectories are generated by different policies. Quick check: Verify that the importance ratio is correctly computed as the ratio of action probabilities between current and behavior policies.

**Mixture-of-Experts Architecture** - Why needed: Understanding MoE routing mechanisms is crucial since router drift specifically affects how tokens are assigned to experts. Quick check: Confirm that the router produces valid probability distributions over experts for each token.

**Policy Gradient Methods** - Why needed: RSPO builds upon standard policy optimization techniques, requiring understanding of how gradients flow through the reward estimation process. Quick check: Ensure that the gradient estimator properly accounts for the modified importance weights.

**Reward Collapse in RL** - Why needed: Recognizing how improper handling of distribution shift can lead to degenerate policies helps contextualize the router drift problem. Quick check: Monitor reward trajectories for signs of premature convergence or instability during training.

## Architecture Onboarding

**Component Map**: Token Sequence -> Router -> Expert Pool -> Output -> Reward Function -> Policy Optimizer (with RSPO) -> Updated Policy

**Critical Path**: The most critical path involves token routing through the router, computation of router-shift ratios, rescaling of importance weights, and final policy gradient update. Any instability in routing probabilities directly impacts the quality of importance weights and subsequent policy updates.

**Design Tradeoffs**: RSPO trades computational overhead (per-token routing ratio computation) for improved stability. The stop-gradient operation prevents feedback loops but may limit the method's ability to fully adapt to routing changes. The lower-bound flooring ensures numerical stability but introduces a hyperparameter that requires tuning.

**Failure Signatures**: Key failure modes include: (1) Excessive router drift despite RSPO, manifesting as oscillating or collapsing reward trajectories; (2) Numerical instability when router probabilities approach zero, even with flooring; (3) Reduced model performance if the shift computation over-penalizes legitimate routing adaptations.

**First Experiments**: 
1. Compare reward trajectories and variance across training steps between RSPO and baseline methods to quantify stability improvements
2. Measure router entropy and routing consistency over training to verify that RSPO reduces drift without overly constraining the router
3. Perform ablation studies removing stop-gradient or lower-bound flooring to assess their individual contributions to performance

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Empirical success limited to Qwen3-30B-A3B model architecture without validation on alternative MoE configurations
- Computational overhead from per-token routing ratio computation not characterized, raising scalability concerns
- No theoretical guarantees provided for the stop-gradient and lower-bound flooring design choices

## Confidence
*High Confidence:* The characterization of router drift as a distinct failure mode in MoE RLVR is well-supported by training dynamics analysis. The mathematical formulation of router-shift ratios and their integration into importance weighting is clearly defined and internally consistent.

*Medium Confidence:* The experimental results showing improved stability and performance over GRPO, GMPO, and GSPO baselines are compelling but limited to a single model architecture and two benchmark types. The claim that RSPO preserves "router adaptivity" while reducing drift requires further validation across diverse routing scenarios.

*Low Confidence:* The assertion that RSPO achieves "stable training dynamics" is primarily supported by qualitative reward trajectory comparisons rather than rigorous statistical measures of training stability (e.g., variance metrics, convergence rates).

## Next Checks
1. **Architecture Transferability Test:** Evaluate RSPO on alternative MoE configurations (different expert counts, varying expert capacity factors) to assess robustness beyond the Qwen3-30B-A3B setup.

2. **Ablation of Router-Shift Components:** Systematically remove the stop-gradient operation and lower-bound flooring to quantify their individual contributions to performance gains.

3. **Computational Overhead Characterization:** Measure and compare wall-clock training time, memory usage, and communication overhead between RSPO and baseline methods across different batch sizes and sequence lengths.