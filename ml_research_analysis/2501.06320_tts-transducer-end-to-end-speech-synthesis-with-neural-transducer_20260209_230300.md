---
ver: rpa2
title: 'TTS-Transducer: End-to-End Speech Synthesis with Neural Transducer'
arxiv_id: '2501.06320'
source_url: https://arxiv.org/abs/2501.06320
tags:
- audio
- speech
- neural
- transducer
- codes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TTS-Transducer, a novel text-to-speech system
  that leverages neural transducers to learn monotonic alignments between text and
  discrete audio codes, avoiding explicit duration prediction. The architecture uses
  a neural transducer to predict codes from the first codebook and a non-autoregressive
  Transformer to iteratively predict the remaining residual codes, trained end-to-end.
---

# TTS-Transducer: End-to-End Speech Synthesis with Neural Transducer

## Quick Facts
- arXiv ID: 2501.06320
- Source URL: https://arxiv.org/abs/2501.06320
- Authors: Vladimir Bataev; Subhankar Ghosh; Vitaly Lavrukhin; Jason Li
- Reference count: 40
- Primary result: Achieves 3.94% CER on challenging texts without explicit duration prediction

## Executive Summary
This paper introduces TTS-Transducer, a novel text-to-speech system that leverages neural transducers to learn monotonic alignments between text and discrete audio codes, avoiding explicit duration prediction. The architecture uses a neural transducer to predict codes from the first codebook and a non-autoregressive Transformer to iteratively predict the remaining residual codes, trained end-to-end. The system achieves strong results: 3.94% character error rate on challenging texts, outperforming larger models trained on more data, and competitive naturalness scores (3.83 MOS) while demonstrating superior robustness and codec-agnostic generalization.

## Method Summary
TTS-Transducer is a two-component architecture that combines a neural transducer (RNNT) with a non-autoregressive Transformer to predict multiple residual codebook codes from residual vector quantization codecs. The RNNT learns monotonic alignments between text and first codebook codes, eliminating the need for explicit duration prediction. The alignment extracted from the RNNT lattice is then reused to guide the prediction of all residual codebook codes via a 12-layer non-autoregressive Transformer. This hierarchical approach reduces memory complexity while maintaining quality. The model is trained end-to-end on LibriTTS-R using a weighted loss combining RNNT and cross-entropy losses, with speaker conditioning via a GST-based module.

## Key Results
- Achieves 3.94% character error rate on challenging texts, outperforming larger models trained on more data
- Competitive naturalness scores of 3.83 MOS while demonstrating superior robustness
- Codec-agnostic performance across EnCodec, NeMo-Codec, and DAC with <10% performance variance
- IPA tokenization improves intelligibility (3.73% vs 4.90% WER on seen speakers with EnCodec)

## Why This Works (Mechanism)

### Mechanism 1: Monotonic Alignment via Transducer Lattice Constraint
The RNNT architecture inherently enforces monotonic alignment between text and audio, eliminating the need for explicit duration predictors while preventing hallucination and word skipping common in autoregressive TTS. The transducer's lattice-based loss computes probabilities over all valid alignment paths where text tokens must be consumed in order, with `<blank>` tokens acting as frame delimiters. The nested loop decoding (outer loop over encoder frames, inner loop emitting labels until `<blank>`) ensures text order is preserved.

### Mechanism 2: Alignment Extraction and Cross-Component Transfer
The alignment extracted from the RNNT lattice during first-codebook prediction can be reused to align encoder representations for all residual codebook predictions, avoiding redundant alignment learning. Using k2 framework's `shortest_path` on the computed RNNT lattice yields the most probable alignment path. Encoder output frames are distributed according to this alignment, then concatenated with previous codebook embeddings as input to the Residual Codebook Head.

### Mechanism 3: Hierarchical Codebook Prediction with Memory-Efficient Factorization
Separating first-codebook prediction (transducer) from residual codebooks (NAR transformer) reduces memory complexity from O(text_len × audio_len × num_codebooks) to O(text_len × audio_len), making end-to-end training tractable. The first codebook captures primary acoustic structure and requires alignment learning. Residual codebooks refine quality and can be predicted non-autoregressively given aligned context.

## Foundational Learning

- **Neural Transducers (RNNT)**: Why needed: Core alignment mechanism; must understand encoder-predictor-joint network flow, blank token semantics, and lattice-based training to debug alignment issues. Quick check: In RNNT decoding, what happens when the joint network outputs a `<blank>` token vs. a vocabulary token?

- **Residual Vector Quantization (RVQ) in Neural Audio Codecs**: Why needed: Understanding why codecs produce 8-9 codebooks per frame and what information each level captures is essential for interpreting RCH behavior. Quick check: In EnCodec with 8 codebooks, what type of information is typically encoded in the first codebook vs. later residual codebooks?

- **WFST-based Transducer Implementation (k2 framework)**: Why needed: Alignment extraction depends on understanding how to traverse lattices; the paper uses k2's shortest_path for this. Quick check: How would you extract the most probable alignment path from an RNNT lattice, and what does each lattice edge represent?

## Architecture Onboarding

- Component map:
  - Text Encoder (12-layer NAR Transformer) -> e_i vectors
  - Prediction Network (6-layer autoregressive Transformer) + e_i -> p_j vectors
  - Joint Network (Linear→ReLU→Linear→Softmax) + p_j -> logits for c_0 tokens + `<blank>`
  - RNNT loss computation and k2.shortest_path lattice extraction
  - Residual Codebook Head (12-layer NAR Transformer) + aligned encoder output + previous codes -> c_i predictions
  - All predicted codes -> Codec Decoder -> audio waveform

- Critical path:
  1. Text tokens + speaker embedding → Text Encoder → e_i
  2. First codebook c_0 + `<SOS>` → Prediction Network → p_j
  3. e_i + p_j → Joint Network → logits for c_0 tokens + `<blank>`
  4. Compute RNNT loss; extract alignment via k2.shortest_path on lattice
  5. Distribute encoder frames according to alignment
  6. For i in [1, n]: [embeddings for c_0→c_{i-1}] + aligned encoder output → RCH → predict c_i
  7. All predicted codes [c_0, c_1, ..., c_n] → Codec Decoder → audio

- Design tradeoffs:
  - Tokenization: IPA yields better intelligibility (3.73% vs 4.90% WER on seen speakers with EnCodec); BPE may preserve speaker similarity better
  - Codec selection: EnCodec and NeMo-Codec perform comparably; NeMo-Codec + IPA achieves best CER (3.94% on challenging texts)
  - Model scaling: Encoder layers (6→12) and RCH layers (6→12) both improve intelligibility; prediction network depth less impactful
  - Loss weighting: α=0.4 for CE loss (residual), 0.6 for RNNT loss (first codebook)—tune if residual quality lags

- Failure signatures:
  - High WER/CER on challenging/long texts → Check alignment extraction; switch from BPE to IPA; verify nucleus sampling p=0.95 isn't introducing noise
  - Low speaker similarity (SSIM) → Verify GST reference audio is 3-5 seconds of clean speech; check speaker embedding conditioning
  - Word skipping or repetition → Should be prevented by transducer; verify `<blank>` token handling in decoding loop
  - Audio artifacts in high frequencies → Check codec quality (DAC vs EnCodec); residual codebook predictions may need more RCH layers

- First 3 experiments:
  1. Baseline replication: Train EnCodec + BPE model with default config (12/6/12 layers, batch 2048, 200 epochs); evaluate CER/WER/SSIM on LibriTTS-R held-out set to confirm reproducibility
  2. Tokenization ablation: Same config with IPA tokens; compare CER improvement (expect ~25% relative reduction) and any SSIM changes
  3. Codec-agnostic validation: Train identical models with EnCodec, NeMo-Codec, and DAC; verify performance variance is <10% to confirm codec portability claim

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored including streaming adaptation, scalability to larger datasets, and the specific mechanism behind tokenization performance differences.

## Limitations
- Alignment transfer mechanism lacks direct empirical validation across all residual codebooks
- Key tokenization details (BPE vocabulary size, IPA phonemization method) are unspecified
- Generalization across codecs only tested on three specific codecs without exploring edge cases

## Confidence
- High Confidence: Monotonic alignment constraint prevents word skipping; IPA improves intelligibility; two-component architecture reduces memory complexity
- Medium Confidence: Alignment extraction and transfer mechanism works as described; model achieves state-of-the-art performance on challenging texts; GST-based speaker embedding provides robust voice cloning
- Low Confidence: Exact mechanism of residual codebook refinement; universal applicability to arbitrary RVQ codecs; specific contribution of each architectural component

## Next Checks
1. Alignment Transfer Ablation: Create controlled experiments comparing full TTS-Transducer with variants where each codebook learns its own alignment independently versus random prediction, to quantify the benefit and potential cost of alignment transfer.

2. IPA Tokenization Implementation Verification: Reproduce the IPA tokenization pipeline using espeak or g2p, ensuring consistent phoneme set, and train models with both BPE and IPA to verify the reported 25% relative CER improvement and test robustness on noisy or accented text.

3. Cross-Codec Robustness Testing: Extend codec evaluation beyond the three tested codecs to include codecs with different codebook counts, irregular structures, and different bitrates to validate codec-agnostic claims and reveal limitations in hierarchical prediction.