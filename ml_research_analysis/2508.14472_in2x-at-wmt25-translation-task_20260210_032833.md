---
ver: rpa2
title: In2x at WMT25 Translation Task
arxiv_id: '2508.14472'
source_url: https://arxiv.org/abs/2508.14472
tags:
- arxiv
- language
- preprint
- tasks
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the open-system submission by the In2x research
  team for the WMT25 General Machine Translation Shared Task, focusing on Japanese-related
  translation tasks. The work addresses the challenge of extending large language
  models (LLMs) to less commonly spoken languages, particularly Japanese, where expressive
  and culturally appropriate translation remains difficult.
---

# In2x at WMT25 Translation Task

## Quick Facts
- arXiv ID: 2508.14472
- Source URL: https://arxiv.org/abs/2508.14472
- Reference count: 10
- Primary result: In2x model achieved 1st place in unrestricted track and 2nd place overall for Japanese-related translation tasks at WMT25 without task-specific fine-tuning

## Executive Summary
This paper presents the open-system submission by the In2x research team for the WMT25 General Machine Translation Shared Task, focusing on Japanese-related translation tasks. The work addresses the challenge of extending large language models (LLMs) to less commonly spoken languages, particularly Japanese, where expressive and culturally appropriate translation remains difficult. The authors propose a generalizable paradigm for transferring English LLM strengths to Japanese via curriculum design, cross-lingual alignment, and preference signals rewarding naturalness. The approach involves three post-training phases: (1) balancing STEM and humanities capabilities with diverse corpora, (2) refining long-text processing, and (3) fast annealing with high-quality data.

## Method Summary
The In2x methodology employs a three-stage pipeline: Continue Pretraining (CPT) with three phases focusing on knowledge enhancement, long-context refinement, and fast annealing; Supervised Fine-Tuning (SFT) using Birch-clustered instructions with difficulty-based sampling; and Reinforcement Learning with a Trajectory-Corrected GRPO algorithm using dual reward models. The approach leverages English as a hub language with a 1:1:1 instruction ratio across Chinese, English, and Japanese, creating cross-lingual alignment that allows semantic and stylistic priors learned from English to activate in Japanese. A detailed alignment pipeline constructs 2 million post-training samples, including 1.5 million for supervised fine-tuning and 500k for reinforcement learning.

## Key Results
- Achieved first place in the unrestricted track and second place overall for Japanese-related translation tasks at WMT25
- Outperformed large-scale proprietary models including Gemini-2.5-Pro, GPT-4.1, Claude-4, and DeepSeek-V3
- Demonstrated superior performance without task-specific fine-tuning, validating the effectiveness of systematic knowledge transfer from English

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** English can serve as a hub language to transfer expressive capabilities to lower-resource target languages through balanced multilingual training.
- **Mechanism:** A 1:1:1 instruction ratio across Chinese, English, and Japanese (the "In2X" setup) creates cross-lingual alignment, allowing semantic and stylistic priors learned from English's richer training data to activate in the target language.
- **Core assumption:** Knowledge representations learned in English can be mathematically transferred to Japanese when co-trained with balanced language exposure.
- **Evidence anchors:**
  - [abstract] "The core method involves leveraging English as a hub language to transfer general language capabilities to Japanese"
  - [section 2] "facilitate transfer learning from pretraining on Chinese and English to the Japanese language"
  - [corpus] SALAMANDRATA paper (2508.12774) similarly uses multilingual pretraining for 38 European languages, suggesting this is a convergent approach
- **Break condition:** Transfer degrades if target language data quality is poor or if English:target ratio becomes too imbalanced (>3:1 observed to hurt minority language performance).

### Mechanism 2
- **Claim:** Difficulty-stratified sampling with clustering preserves instruction diversity and prevents semantic overfitting.
- **Mechanism:** Birch clustering reduces 40M instructions to 1.5M diverse clusters, then samples by difficulty (3:3:3:1:0 ratio for Very Difficult→Very Simple). Temperature adjustment during training prevents over-specialization to language-specific semantic patterns.
- **Core assumption:** Difficulty-balanced sampling transfers "how to reason" across languages better than random sampling.
- **Evidence anchors:**
  - [section 4.1] "difficulty levels of the data were sampled in a 3:3:3:1:0 ratio"
  - [section 4.2] "temperature parameter was introduced to mitigate overfitting...concentrate on question-answering techniques rather than over-specializing"
  - [corpus] Limited direct validation in corpus; this appears novel to In2x
- **Break condition:** Fails if clustering threshold is too aggressive (over-merges diverse instructions) or if temperature is too high (loses language-specific nuance).

### Mechanism 3
- **Claim:** Task-specific reward model architectures (rule-based for STEM, generative for creative) combined with Trajectory-Corrected GRPO stabilize multilingual RL training.
- **Mechanism:** Rule-based rewards enforce deterministic correctness for math/logic; generative reward models score creative outputs against dynamically generated guidelines. Trajectory-Corrected GRPO uses sentence-level clipping rather than token-level to handle varying reward densities across languages.
- **Core assumption:** Sentence-level importance sampling is more stable than token-level for multilingual tasks with heterogeneous reward landscapes.
- **Evidence anchors:**
  - [section 5.1] "Rule-Based Reward Model: For tasks involving mathematics...Generative Reward Model for Creative Tasks"
  - [section 5.2] "Trajectory-Corrected version of GRPO...effective for handling multilingual tasks with varying reward functions"
  - [corpus] No direct corpus comparison available; this appears to be a novel algorithmic contribution
- **Break condition:** Instability emerges if reward model and policy model capabilities diverge significantly; entropy regularization (0.01) must be carefully tuned to prevent premature reward saturation.

## Foundational Learning

- **Concept: Continue Pretraining (CPT)**
  - Why needed here: Three-phase CPT (knowledge→long-context→annealing) builds foundational multilingual representations before alignment. Skip this and SFT cannot recover missing world knowledge.
  - Quick check question: Can you explain why annealing with perplexity-filtered data preserves expressive style?

- **Concept: Proximal Policy Optimization (PPO/GRPO)**
  - Why needed here: The RL stage uses a modified GRPO variant. Understanding standard PPO is prerequisite to grasping why trajectory correction and dual-clipping matter.
  - Quick check question: What problem does the clipping parameter solve in standard PPO?

- **Concept: Instruction Tuning Data Quality**
  - Why needed here: 40M→1.5M reduction via clustering demonstrates that data curation matters more than scale. Quality control pipeline (prompt engineering→model validation→ReReading) filters synthetic noise.
  - Quick check question: Why might synthetic instructions suffer from "self-answered queries" and how does the ReReading mechanism address this?

## Architecture Onboarding

- **Component map:**
  Base LLM → CPT Phase 1: Knowledge → CPT Phase 2: Long-context → CPT Phase 3: Annealing → SFT: Clustered 1.5M instructions, difficulty-sampled → RL: Dual reward model + Trajectory-Corrected GRPO → Ensemble: Gradient-weighted tensor fusion

- **Critical path:**
  1. CPT data mixing ratio (creative:knowledge:Japanese-specific)
  2. SFT clustering threshold and difficulty sampling ratio
  3. RL reward model alignment with target task distribution

- **Design tradeoffs:**
  - 3:3:3:1:0 difficulty sampling favors challenging examples but may under-represent simple conversational patterns
  - Sentence-level vs token-level clipping: more stable but less granular credit assignment
  - 500B tokens creative corpus investment vs potential diminishing returns for non-literary tasks

- **Failure signatures:**
  - Literal but stilted translations → temperature too low or insufficient creative corpus exposure
  - Math/reasoning degradation → reward model misalignment or entropy collapse during RL
  - Cultural inappropriateness → instruction rewriting pipeline missing cultural style transformation step

- **First 3 experiments:**
  1. Ablate the 1:1:1 ratio to 2:1:1 (English-dominant) and measure Japanese benchmark degradation
  2. Replace Trajectory-Corrected GRPO with standard PPO and track training stability metrics
  3. Remove the ReReading mechanism from instruction synthesis and measure synthetic data quality via critic LLM rejection rate

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does the "English-as-hub" transfer paradigm generalize to low-resource languages that lack the linguistic proximity to English or Chinese that Japanese possesses?
- **Basis in paper:** [inferred] The abstract frames the methodology as a "generalizable paradigm" for less commonly spoken languages, but the validation is restricted to Japanese.
- **Why unresolved:** The success of the transfer relies on a specific triad (English, Chinese, Japanese) and specific data ratios (1:1:1); it is unclear if this architecture applies to languages with fewer linguistic similarities to the source hubs or fewer available resources.
- **What evidence would resolve it:** Application of the In2x pipeline to a typologically diverse set of low-resource languages (e.g., distinct from English/Chinese syntax) with an analysis of alignment quality.

### Open Question 2
- **Question:** Does the specific focus on "expressiveness-first" supervision and creative writing corpora during pretraining and SFT cause performance regression in STEM or logical reasoning tasks?
- **Basis in paper:** [inferred] The paper contrasts the "neglect of creative/idiomatic language ability" with the heavy investment in "math and code reasoning," aiming to fix the former, but does not explicitly benchmark the latter post-alignment.
- **Why unresolved:** Optimizing for stylistic nuances and idiomaticity often involves trade-offs with logical precision; the paper does not show if the "vivid expressive style" comes at the cost of reasoning accuracy.
- **What evidence would resolve it:** A comparison of benchmark scores (e.g., GSM8K, HumanEval) between the base model and the final In2x model to check for capability interference.

### Open Question 3
- **Question:** How susceptible is the "Generative Reward Model" to reward hacking when scoring subjective cultural nuances without human-in-the-loop validation?
- **Basis in paper:** [explicit] The authors state they utilize a generative reward model for creative tasks that evaluates compliance with principles to generate a score.
- **Why unresolved:** LLM-based evaluators often struggle to objectively assess "cultural appropriateness" or "idiomaticity" and may favor plausible-sounding but hallucinated outputs over faithful ones.
- **What evidence would resolve it:** Correlation analysis between the generative reward model's scores and independent human evaluation scores on a held-out set of creative translation tasks.

## Limitations

- Dataset transparency gaps: The paper does not specify exact corpus sources, quality filtering thresholds, or clustering parameters for the critical 40M→1.5M instruction reduction
- Reward model architecture ambiguity: Specific implementations of the dual reward models (model sizes, prompt templates, training data) are not detailed
- Baseline comparison limitations: Superiority claims over proprietary models are based on preliminary WMT25 results without full evaluation protocol disclosure

## Confidence

- **High confidence** in the fundamental paradigm: using English as hub language with balanced multilingual training (1:1:1 ratio) to transfer capabilities to Japanese is well-supported by the abstract and methodology section, and aligns with established multilingual transfer learning principles.
- **Medium confidence** in the effectiveness of the three-phase CPT and difficulty-stratified sampling approach. The methodology is sound and the theoretical justification is present, but the exact hyperparameters and dataset quality control mechanisms are underspecified.
- **Low confidence** in the claimed superiority claims without access to full evaluation details. The WMT25 results are preliminary, and the paper doesn't provide ablation studies for individual components or detailed error analysis.

## Next Checks

1. **Ablation study of language ratio** - Systematically vary the Chinese:English:Japanese ratio from 1:1:1 to 2:1:1 and 1:2:1 to quantify the trade-off between hub language dominance and target language expressiveness. This would validate whether the 1:1:1 ratio is optimal or merely sufficient.

2. **Reward model capability analysis** - Compare the performance of standard PPO vs Trajectory-Corrected GRPO with varying reward model qualities. This would isolate whether the algorithmic innovation or the reward model architecture is driving the multilingual RL stability claims.

3. **Synthetic data quality audit** - Evaluate the impact of removing the ReReading mechanism and critic LLM filtering from the instruction synthesis pipeline. This would quantify the contribution of the quality control steps to the final model performance and validate the claim that synthetic data curation is more important than scale.