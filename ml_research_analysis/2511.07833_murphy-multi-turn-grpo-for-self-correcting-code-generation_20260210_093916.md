---
ver: rpa2
title: 'MURPHY: Multi-Turn GRPO for Self Correcting Code Generation'
arxiv_id: '2511.07833'
source_url: https://arxiv.org/abs/2511.07833
tags:
- feedback
- grpo
- rewards
- reward
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MURPHY, a multi-turn extension of GRPO for
  training code generation models. Unlike standard GRPO, which optimizes only on final
  outcomes, MURPHY leverages intermediate feedback by conditioning generations on
  execution traces and propagating rewards backward across turns.
---

# MURPHY: Multi-Turn GRPO for Self Correcting Code Generation

## Quick Facts
- **arXiv ID**: 2511.07833
- **Source URL**: https://arxiv.org/abs/2511.07833
- **Reference count**: 33
- **Primary result**: MURPHY achieves up to 8% relative improvement in pass@1 accuracy over GRPO on equivalent compute budgets in multi-turn code generation tasks.

## Executive Summary
MURPHY introduces a multi-turn extension of GRPO (Generalized Reward Pruning Optimization) tailored for self-correcting code generation. Unlike standard GRPO, which optimizes only on final outcomes, MURPHY leverages intermediate feedback by conditioning generations on execution traces and propagating rewards backward across turns. This enables the model to learn from partial progress, not just final success. MURPHY uses two pruning strategies—INTERP and INTRAP—to manage computational cost by retaining only informative rollouts. Evaluated on Qwen and OLMo models (1.7B–7B) across three coding benchmarks, MURPHY achieves up to 8% relative improvement in pass@1 accuracy over GRPO on equivalent compute budgets, with the strongest gains in multi-iteration settings where feedback-driven refinement is critical.

## Method Summary
MURPHY extends GRPO by incorporating intermediate feedback across multiple turns of code generation. It conditions each generation step on execution traces from previous steps and propagates rewards backward through the sequence. Two pruning strategies—INTERP (inter-turn) and INTRAP (intra-turn)—are used to reduce computational overhead by filtering out less informative rollouts. The method is evaluated on small to medium-sized models (1.7B–7B parameters) across three coding benchmarks, with results showing significant gains in multi-turn scenarios.

## Key Results
- MURPHY achieves up to 8% relative improvement in pass@1 accuracy over GRPO on equivalent compute budgets.
- Strongest gains observed in multi-iteration code generation tasks where intermediate feedback is most impactful.
- Performance evaluated on Qwen and OLMo models (1.7B–7B parameters) across three coding benchmarks.

## Why This Works (Mechanism)
MURPHY's effectiveness stems from its ability to leverage intermediate feedback during multi-turn code generation. By conditioning generations on execution traces and propagating rewards backward across turns, the model can learn from partial progress rather than relying solely on final outcomes. This is particularly beneficial in self-correcting workflows where iterative refinement is critical. The pruning strategies (INTERP and INTRAP) further enhance efficiency by retaining only informative rollouts, ensuring that computational resources are focused on high-value samples.

## Foundational Learning
- **GRPO (Generalized Reward Pruning Optimization)**: A reinforcement learning method for code generation that optimizes based on final outcomes. Needed to understand the baseline against which MURPHY is compared. Quick check: Verify that GRPO uses only final reward signals.
- **Multi-turn code generation**: A process where the model generates code iteratively, refining outputs based on feedback. Needed to contextualize MURPHY's approach. Quick check: Confirm that intermediate feedback is used in each turn.
- **Reward propagation**: The technique of assigning credit for outcomes backward through a sequence of actions. Needed to understand how MURPHY leverages intermediate feedback. Quick check: Ensure backward reward propagation is implemented correctly.
- **Pruning strategies (INTERP/INTRAP)**: Methods to filter out less informative rollouts to reduce computational cost. Needed to assess MURPHY's efficiency claims. Quick check: Validate that pruning retains high-value samples.

## Architecture Onboarding
- **Component map**: MURPHY -> INTERP/INTRAP pruning -> Backward reward propagation -> Multi-turn generation.
- **Critical path**: Input code prompt → Generation step → Execution trace → Reward calculation → Pruning → Backward reward propagation → Next generation step.
- **Design tradeoffs**: MURPHY trades increased complexity (multi-turn feedback, pruning) for improved performance in iterative tasks. The pruning strategies reduce compute but may risk losing informative samples.
- **Failure signatures**: Poor performance in single-turn scenarios; potential overfitting to intermediate feedback; pruning may discard critical rollouts.
- **First experiments**: 1) Compare MURPHY vs. GRPO on multi-turn vs. single-turn tasks. 2) Ablate pruning strategies to isolate their contribution. 3) Test scalability to larger models (13B-70B parameters).

## Open Questions the Paper Calls Out
None

## Limitations
- MURPHY's gains are strongest in multi-iteration settings; performance in single-iteration scenarios is not reported.
- Pruning strategies (INTERP and INTRAP) lack systematic ablation to quantify their contribution.
- All experiments use small to medium model sizes (1.7B-7B parameters), leaving scalability to larger models untested.

## Confidence
- **8% relative improvement claim**: High confidence due to clear experimental protocol and compute control.
- **Pruning efficacy claims**: Medium confidence due to limited ablation studies.
- **Scalability claims**: Medium confidence due to lack of testing on larger models.
- **Intermediate feedback claims**: Medium confidence as the paper does not isolate reward propagation from pruning effects.

## Next Checks
1. Conduct an ablation study removing pruning strategies to isolate their contribution versus reward propagation alone.
2. Evaluate MURPHY on single-iteration code generation to test for performance trade-offs in simpler settings.
3. Scale MURPHY to larger models (e.g., 13B-70B parameters) to assess whether gains persist or diminish with model size.