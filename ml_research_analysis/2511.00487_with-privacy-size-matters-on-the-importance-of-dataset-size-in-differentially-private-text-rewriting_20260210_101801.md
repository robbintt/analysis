---
ver: rpa2
title: 'With Privacy, Size Matters: On the Importance of Dataset Size in Differentially
  Private Text Rewriting'
arxiv_id: '2511.00487'
source_url: https://arxiv.org/abs/2511.00487
tags:
- dataset
- text
- privacy
- size
- rewriting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effect of dataset size on differentially
  private (DP) text rewriting mechanisms, an aspect previously overlooked in DP NLP
  research. The authors design utility and privacy experiments using five datasets
  (AG News, MNLI, Trustpilot, Yelp, and a large Twitter corpus), each split into five
  sizes (10%, 25%, 50%, 75%, 100%) for a total of 180 datasets.
---

# With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting

## Quick Facts
- **arXiv ID**: 2511.00487
- **Source URL**: https://arxiv.org/abs/2511.00487
- **Authors**: Stephen Meisenbacher; Florian Matthes
- **Reference count**: 15
- **Primary result**: Dataset size significantly impacts DP text rewriting effectiveness, with larger datasets yielding more favorable privacy-utility trade-offs

## Executive Summary
This paper investigates how dataset size affects the effectiveness of differentially private (DP) text rewriting mechanisms, an aspect previously overlooked in DP NLP research. Through systematic experiments across five datasets (AG News, MNLI, Trustpilot, Yelp, and Twitter) at five different sizes each, the authors evaluate three DP mechanisms (1-DIFFRACTOR, DP-PROMPT, DP-BART) across various privacy budgets. They introduce a novel "relative gain" metric that quantifies the privacy-utility trade-off and conduct both downstream task performance and privacy preservation evaluations through adversarial classification tasks. The findings reveal that larger datasets generally yield more favorable privacy-utility trade-offs, demonstrating that DP text rewriting can be effective at scale with reduced utility penalties.

## Method Summary
The authors conducted comprehensive experiments using five distinct datasets: AG News, MNLI, Trustpilot, Yelp, and a large Twitter corpus. Each dataset was split into five sizes (10%, 25%, 50%, 75%, 100%), creating 180 datasets total for evaluation. Three differentially private text rewriting mechanisms were tested: 1-DIFFRACTOR, DP-PROMPT, and DP-BART. The evaluation measured both downstream task performance using micro-F1 scores and privacy preservation through adversarial classification tasks. A key innovation was the introduction of a "relative gain" metric that combines utility and privacy metrics to quantify the trade-off between them. Experiments were conducted across various privacy budgets (ε values) to assess performance under different privacy constraints.

## Key Results
- Dataset size significantly impacts DP text rewriting effectiveness, with larger datasets generally yielding more favorable privacy-utility trade-offs
- The "relative gain" metric effectively captures the relationship between privacy preservation and utility across different dataset sizes
- DP text rewriting mechanisms show reduced utility penalties when applied to larger datasets, improving indistinguishability from original texts
- The privacy-utility trade-off improves systematically as dataset size increases, validating the importance of dataset scale in DP NLP applications

## Why This Works (Mechanism)

## Foundational Learning
- **Differentially Private Text Rewriting**: Techniques that modify text while preserving privacy by adding noise or using privacy-preserving training methods
  - Why needed: Enables data sharing and model training without exposing sensitive information
  - Quick check: Verify that rewritten text maintains semantic meaning while preventing exact reconstruction of original data
- **Privacy Budget (ε)**: The parameter controlling the privacy-utility trade-off in DP mechanisms
  - Why needed: Determines the strength of privacy guarantees and affects utility preservation
  - Quick check: Test how different ε values affect both privacy protection and task performance
- **Adversarial Classification for Privacy Evaluation**: Using classifiers to distinguish between original and rewritten text
  - Why needed: Provides quantitative measure of privacy preservation effectiveness
  - Quick check: Verify that adversarial classifiers cannot reliably distinguish original from rewritten text
- **Relative Gain Metric**: A composite measure combining utility and privacy into a single value
  - Why needed: Enables comparison across different dataset sizes and mechanisms
  - Quick check: Ensure metric appropriately balances both utility and privacy considerations
- **Micro-F1 Score**: Evaluation metric for multi-class classification tasks
  - Why needed: Measures downstream task performance after text rewriting
  - Quick check: Confirm that rewritten text maintains sufficient information for accurate classification
- **Dataset Size Scaling Effects**: How performance changes as training data increases
  - Why needed: Fundamental question in machine learning about data efficiency
  - Quick check: Verify monotonic improvement trends with increasing dataset size

## Architecture Onboarding
- **Component Map**: Datasets (5 types × 5 sizes) -> DP Mechanisms (3 types) -> Utility Metrics (Micro-F1) + Privacy Metrics (Adversarial Classification) -> Relative Gain Calculation
- **Critical Path**: Dataset preparation → DP text rewriting → Downstream evaluation → Privacy evaluation → Relative gain computation
- **Design Tradeoffs**: Privacy vs. utility balance controlled by ε parameter; dataset size vs. computational cost; evaluation comprehensiveness vs. experimental complexity
- **Failure Signatures**: Poor utility indicates insufficient privacy budget or ineffective rewriting; inadequate privacy protection suggests overly permissive ε values or weak rewriting mechanisms
- **First Experiments**:
  1. Test smallest dataset size with highest privacy budget to establish baseline performance floor
  2. Compare utility drop between original and DP-rewritten text on held-out validation set
  3. Measure adversarial classifier accuracy to quantify privacy preservation effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope focuses primarily on English text classification tasks, limiting generalizability to other NLP applications
- The relative gain metric combines utility and privacy into a single scalar value that may obscure important use-case-specific trade-offs
- The relationship between dataset size and DP effectiveness may be non-linear and highly dependent on specific task characteristics

## Confidence
- **High confidence** in the finding that dataset size matters for DP text rewriting effectiveness, well-supported by systematic experiments
- **Medium confidence** in specific quantitative claims about improvement magnitude, as the relative gain metric may not directly translate to all real-world scenarios
- **Medium confidence** in claims about reduced utility penalties at larger scales, as relationship may depend heavily on specific mechanisms and tasks

## Next Checks
1. **Cross-domain validation**: Replicate experiments on non-classification tasks (summarization, translation, generation) to verify generalizability beyond text classification
2. **Privacy mechanism comparison**: Systematically compare how different DP mechanisms (DP-SGD, PATE, etc.) respond to dataset size changes
3. **Robustness to dataset composition**: Test whether size effects hold when controlling for class balance, text length distribution, and vocabulary diversity