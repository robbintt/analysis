---
ver: rpa2
title: 'Understanding Task Vectors in In-Context Learning: Emergence, Functionality,
  and Limitations'
arxiv_id: '2506.09048'
source_url: https://arxiv.org/abs/2506.09048
tags:
- task
- vectors
- linear
- then
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the Linear Combination Conjecture to explain
  how task vectors emerge and function in in-context learning (ICL). The authors provide
  theoretical and empirical evidence that task vectors naturally form as linear combinations
  of demonstration hidden states in triplet-formatted prompts, particularly in linear-attention
  transformers.
---

# Understanding Task Vectors in In-Context Learning: Emergence, Functionality, and Limitations

## Quick Facts
- arXiv ID: 2506.09048
- Source URL: https://arxiv.org/abs/2506.09048
- Reference count: 40
- Primary result: Task vectors are linear combinations of demonstration hidden states with rank-one expressivity limitations, failing on bijection tasks

## Executive Summary
This paper proposes the Linear Combination Conjecture to explain how task vectors emerge and function in in-context learning (ICL). The authors provide theoretical and empirical evidence that task vectors naturally form as linear combinations of demonstration hidden states in triplet-formatted prompts, particularly in linear-attention transformers. They validate this through loss landscape analysis and show task vectors are limited to rank-one representations, failing on bijection tasks that require higher-rank mappings. The work is further supported by saliency analyses and parameter visualizations in practical LLMs. Based on these insights, the authors propose a multi-vector injection strategy that consistently improves performance across various ICL tasks compared to standard task vector methods.

## Method Summary
The paper validates the Linear Combination Conjecture through theoretical analysis of linear-attention transformers and empirical evaluation on both synthetic and practical LLMs. For synthetic experiments, linear-attention transformers are trained on random regression triplets with d=4 and n∈[5,30] demonstrations. Task vectors are extracted from the hidden states of arrow tokens and injected into zero-shot prompts. Practical validation uses Llama-13B and Pythia-12B models on 33 tasks across 5 categories. The multi-vector injection strategy (TaskV-M) injects N+1 task vectors into all arrow tokens, compared against single vector (TaskV) and baseline ICL methods.

## Key Results
- Task vectors emerge as linear combinations of demonstration hidden states in linear-attention transformers, validated through loss landscape analysis
- Single task vectors are inherently limited to rank-one representations, causing failure on bijection tasks (e.g., toggling between upper/lower case)
- Multi-vector injection strategy consistently outperforms single vector methods, especially on challenging bijection tasks
- Saliency analyses and parameter visualizations in practical LLMs support the linear combination hypothesis

## Why This Works (Mechanism)

### Mechanism 1: Linear Combination Formation
- Claim: Task vectors function by acting as linear combinations of in-context demonstration hidden states, rather than storing task information in an abstract format.
- Mechanism: In linear-attention transformers, the attention layer aggregates information from preceding tokens into the "arrow" (→) tokens. The paper theoretically shows that these vectors emerge as weighted sums (via $\Lambda_4 \otimes \Lambda_5$) of the input/output embeddings ($x_i, y_i$).
- Core assumption: Practical LLMs implement a learning dynamic similar to the linear-attention models analyzed, specifically performing "embedding concatenation" followed by "weighted summation."
- Evidence anchors:
  - [abstract] "The Linear Combination Conjecture, positing that task vectors act as single in-context demonstrations formed through linear combinations of the original ones."
  - [section 3.2] "Task Vector Formation... This operation is central to the emergence of task vectors... outputs n + 1 linear combinations of the demonstrations as the hidden states for the arrow tokens."
  - [corpus] Paper 12429 ("Task Vectors... Emergence, Formation") notes task-specific information is locally encoded, but this paper specifically quantifies the formation as a linear combination.
- Break condition: If attention layers in practical LLMs do not approximate a weighted sum of demonstration embeddings (e.g., if non-linear MLPs dominate the information aggregation), the linear combination conjecture fails to explain the vector's composition.

### Mechanism 2: Rank-One Expressivity Limitation
- Claim: Task vectors derived from single tokens are inherently limited to representing rank-one coefficient matrices, causing failure on complex mapping tasks like bijections.
- Mechanism: Since a task vector $z_{tv}$ is a linear combination of demonstrations ($X\beta, Y\beta$), injecting it into a zero-shot prompt reduces the problem to a single-shot prediction. The resulting prediction function $W' = Y\beta(X\beta)^\top$ is rank-one, which is insufficient for tasks requiring full-rank mappings (e.g., toggling between upper/lower case).
- Core assumption: The inference process with a task vector is mathematically equivalent to single-shot in-context learning.
- Evidence anchors:
  - [section 4] "Inference with task vectors is analogous to 1-shot ICL, which is inherently limited to rank-one meta-predictors... [proven in] Proposition 4."
  - [table 1] Shows task vectors failing on bijection tasks (e.g., To Upper $\leftrightarrow$ Lower) while ICL succeeds, confirming the expressivity gap.
  - [corpus] No direct evidence in corpus summaries regarding the specific rank-one limitation on bijection tasks.
- Break condition: If the model uses multiple task vectors (TaskV-M) or the hidden state dimension is utilized differently, the effective rank may increase, breaking the single-vector failure mode.

### Mechanism 3: Multi-Vector Robustness
- Claim: Performance can be enhanced by injecting multiple task vectors into the prompt rather than relying on a single representation.
- Mechanism: The paper proposes that each "arrow" token in a prompt contains a valid linear combination (task vector). Injecting $N+1$ vectors into an $N$-shot prompt allows the model to access a distributed representation of the task, mitigating the information loss inherent in the single-vector compression.
- Core assumption: Intermediate arrow tokens in practical causal-attention models carry sufficient distinct information to aid prediction, despite the causal bias towards the final token.
- Evidence anchors:
  - [section 6.2] "Our multi-vector injection strategy (TaskV-M)... consistently outperforms TaskV, especially on the more challenging bijection tasks."
  - [corpus] Corpus evidence is weak regarding multi-vector injection specifically; focuses on single vector extraction.
- Break condition: If the model relies exclusively on the final token for prediction (standard causal behavior) and ignores intermediate injections, the multi-vector gains will diminish.

## Foundational Learning

- Concept: **Linear Attention and GD++**
  - Why needed here: To understand Theorem 1 and 2, which prove that attention layers implement gradient descent steps. Without this, the mathematical justification for "Task Vector Formation" is opaque.
  - Quick check question: Can you explain why a linear attention layer with specific $V$ and $Q$ matrices is equivalent to a step of gradient descent on a regression loss?

- Concept: **Matrix Rank and Bijection**
  - Why needed here: To grasp why the "Rank-One Limitation" prevents solving bijection tasks. One must understand that $W = ab^\top$ has rank 1 and cannot map distinct inputs to distinct outputs in both directions unless they are trivially related (identity/negation).
  - Quick check question: Why can a rank-one matrix $W$ not satisfy $x \to A$ and $A \to a$ simultaneously for distinct vectors?

- Concept: **Critical Points in Loss Landscapes**
  - Why needed here: The paper proves the emergence of vectors by characterizing the *critical points* (gradients = 0) of the ICL risk, showing the vector structure is an optimal solution, not just a random artifact.
  - Quick check question: What does it mean for a parameter configuration to be a "critical point" of the in-context risk function?

## Architecture Onboarding

- Component map: Triplet Data -> Linear/Standard Transformer -> Extraction of Last Arrow Token -> Normalization -> Injection into Zero-Shot/Few-Shot Prompt
- Critical path: Triplet Data → Linear/Standard Transformer → Extraction of Last Arrow Token → Normalization → Injection into Zero-Shot/Few-Shot Prompt
- Design tradeoffs:
  - **Standard Task Vector (TaskV):** High efficiency (single vector), low expressivity (rank-one, fails bijections)
  - **Multi-Vector (TaskV-M):** Higher cost (N+1 vectors), higher expressivity (better on bijections/complex tasks)
- Failure signatures:
  - **Bijection Failure:** 50% accuracy on toggle tasks (e.g., "a → A" works, but "A → a" fails or confuses case)
  - **Saliency Mismatch:** If saliency maps show the final arrow token attending only to the immediate $y_{test}$ rather than preceding $y_i$'s, the "weighted summation" mechanism is likely not active
- First 3 experiments:
  1. **Linear Toy Model Verification:** Train a 2-layer linear transformer on random regression triplets. Verify that the $D_l$ weights align with the "Task Vector Formation" structure (Fig 2b) to confirm the theoretical emergence.
  2. **Bijection Stress Test:** Apply TaskV vs. ICL on a "Upper ↔ Lower" task. Confirm TaskV drops to random chance while ICL succeeds, validating the rank-one limitation.
  3. **Saliency Visualization:** Generate saliency maps (Section 4) for a practical LLM to check if the final arrow token attends broadly to all label tokens ($y_i$), supporting the linear combination hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the reliance on the final arrow token in causal LLMs be mitigated to enable effective utilization of intermediate task vectors?
- Basis in paper: [inferred] The authors note in Appendix D.1 that while their linear theory suggests equal contribution from all tokens, practical LLMs with causal attention disproportionately rely on the final arrow token. This limits the performance gains of their proposed multi-vector injection method (TaskV-M).
- Why unresolved: The causal attention mechanism inherently restricts information flow to the final token, creating a discrepancy between the theoretical linear model and practical transformer behavior.
- What evidence would resolve it: Architectural modifications or attention biasing techniques that force the model to distribute task-specific information more evenly across intermediate tokens, resulting in significant performance gains in multi-vector injection.

### Open Question 2
- Question: Does the Linear Combination Conjecture hold for transformers with non-linear attention mechanisms and explicit Layer Normalization?
- Basis in paper: [inferred] The Limitations section (Appendix D.3) explicitly states the theory focuses on linear-attention transformers and does not fully account for "deeper interactions... or the role of fine-tuned components such as layer normalization."
- Why unresolved: The theoretical justification relies on a simplified linear attention model, leaving the validity of the conjecture in standard non-linear, multi-head transformer architectures unproven.
- What evidence would resolve it: Extending the loss landscape analysis to include non-linear softmax attention or empirically verifying the linear decomposition of task vectors in models trained without linear attention assumptions.

### Open Question 3
- Question: What is the precise role of layer normalization in inducing the task vector mechanism compared to dropout?
- Basis in paper: [inferred] In Section 5 (Proposition 6), the authors mention that while dropout induces task vectors as redundancy, "injection of position encodings and use of normalization act as alternative sources of perturbation" in practical models, but this is not formalized.
- Why unresolved: The theoretical proof links task vector emergence to dropout for robustness, but many modern LLMs use minimal dropout; the specific mechanism by which LayerNorm drives this emergence remains uncharacterized.
- What evidence would resolve it: A study analyzing the emergence of task vectors in transformers trained with dropout ablated but LayerNorm active, specifically measuring if the "redundancy" effect persists through the normalization layers.

## Limitations

- **Rank-One Expressivity Gap**: Single task vectors are limited to rank-one representations, causing failure on bijection tasks requiring higher-rank mappings
- **Linear Combination Assumption**: The theory relies on linear-attention models and may not fully capture standard non-linear transformer behavior with LayerNorm
- **Generalization Uncertainty**: While validated on 33 tasks, the findings may not generalize to more complex real-world scenarios requiring multi-step reasoning

## Confidence

**High Confidence**: The core theoretical framework showing task vectors as linear combinations of demonstration hidden states in linear-attention transformers is well-supported by mathematical proofs (Theorem 1, Theorem 2) and verified through synthetic experiments. The rank-one limitation on bijection tasks is conclusively demonstrated through both theoretical analysis (Proposition 4) and empirical evidence (Table 1).

**Medium Confidence**: The extension of these findings to practical LLMs (Llama-13B, Pythia-12B) is supported by empirical results, but the exact mechanisms may differ from the linear-attention case. The saliency analyses and parameter visualizations provide supporting evidence, but don't fully validate the linear combination hypothesis in the presence of non-linear components.

**Low Confidence**: The multi-vector injection strategy's effectiveness across diverse, complex real-world tasks remains to be thoroughly validated. While promising results are shown on the tested task set, the computational cost-benefit tradeoff and scalability to larger models or more complex tasks require further investigation.

## Next Checks

1. **Cross-Architecture Validation**: Test the Linear Combination Conjecture and multi-vector injection strategy across diverse transformer architectures (e.g., GPT, BERT, PaLM) with varying attention mechanisms and non-linear components to assess the universality of the findings.

2. **Complex Task Evaluation**: Evaluate task vectors on tasks requiring multi-step reasoning, hierarchical structures, or complex numerical reasoning to determine whether the rank-one limitation manifests in more sophisticated failure modes beyond simple bijection tasks.

3. **Computational Efficiency Analysis**: Conduct a thorough cost-benefit analysis of multi-vector injection, measuring not just accuracy improvements but also inference latency, memory overhead, and performance at scale with larger models and more complex prompts.