---
ver: rpa2
title: Benchmarking Abstract and Reasoning Abilities Through A Theoretical Perspective
arxiv_id: '2505.23833'
source_url: https://arxiv.org/abs/2505.23833
tags:
- reasoning
- abstract
- dataset
- chat
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel framework and benchmark for rigorously
  evaluating abstract reasoning in Large Language Models (LLMs). The authors define
  abstract reasoning as a two-step process: abstraction (extracting essential patterns
  from concrete inputs) and reasoning (applying consistent rules to these abstract
  patterns).'
---

# Benchmarking Abstract and Reasoning Abilities Through A Theoretical Perspective

## Quick Facts
- arXiv ID: 2505.23833
- Source URL: https://arxiv.org/abs/2505.23833
- Authors: Qingchuan Ma; Yuhang Wu; Xiawu Zheng; Rongrong Ji
- Reference count: 40
- Primary result: Novel benchmark shows current LLMs struggle with abstract reasoning in non-decimal arithmetic and symbolic transformations, even with Chain-of-Thought prompting.

## Executive Summary
This paper introduces a rigorous framework for evaluating abstract reasoning in Large Language Models through systematic symbol remapping. The authors define abstract reasoning as a two-step process of abstraction and reasoning, and propose two complementary metrics (Γ for accuracy and ∆ for memory dependence) to quantify genuine pattern recognition versus memorization. Extensive evaluations across diverse LLM scales and frameworks reveal critical limitations in non-decimal arithmetic, symbolic transformations, and function inference, demonstrating that current models, despite domain-specific strengths, fundamentally rely on memorized patterns rather than representation-invariant rules.

## Method Summary
The benchmark employs bijective symbol remapping in rule-based tasks to force genuine pattern recognition beyond superficial token matching. It evaluates models on 82 sub-datasets across 6 categories (basic arithmetic, extended operations, non-decimal bases, word problems, function inference, and symbolic manipulation) with 9,095 total samples. Two metrics are used: Γ (Abstract Reasoning Score) measures basic reasoning accuracy on original symbols, while ∆ (Memory Dependence Score) quantifies performance drop under symbol remapping to reveal reliance on specific tokens versus underlying patterns. The evaluation uses direct prompting and zero-shot Chain-of-Thought approaches across various LLM scales (7B-70B), including open-source models, APIs, and multi-agent frameworks.

## Key Results
- Operand symbols show consistently higher memory dependence than operator symbols (MemDep_num >> MemDep_op) across all model scales
- Chain-of-Thought prompting improves procedural accuracy but may increase memory dependence (∆), revealing a memorization-abstraction trade-off
- Non-decimal arithmetic (NBR) tasks show near-zero scores across all model scales, indicating fundamental abstraction failures rather than scale limitations
- Agent frameworks show high memory dependence (react: ∆ = 0.70), suggesting multi-agent approaches don't inherently improve abstraction

## Why This Works (Mechanism)

### Mechanism 1
Symbol remapping exposes memorization by forcing models to reason with novel surface representations while preserving abstract structure. The benchmark applies bijective mappings to replace familiar symbols with arbitrary characters. If a model has genuinely learned the abstract operation, performance should remain stable; if it relied on token-specific associations, accuracy drops. True abstraction produces output that is invariant to symbol substitution when rule structure is preserved.

### Mechanism 2
Chain-of-Thought prompting improves procedural accuracy but may increase memory dependence (∆), revealing a memorization-abstraction trade-off. CoT guides models through familiar reasoning patterns, reducing arithmetic errors on well-represented tasks. However, this procedural scaffolding can reinforce token-specific associations rather than encouraging representation-invariant rule learning. CoT improvements on familiar domains do not transfer to novel symbolic systems.

### Mechanism 3
Operand symbols exhibit higher memory dependence than operator symbols because models memorize numerical token patterns more strongly than operational rules. Training data contains dense co-occurrence of specific numerical sequences with outcomes, creating strong positional token associations. Operators appear across diverse contexts, forcing some level of rule abstraction. Token frequency and context diversity directly correlate with memorization intensity.

## Foundational Learning

- **Concept: Abstraction Mapping (f: C → A)**
  - Why needed here: The theoretical framework defines reasoning as Re(f(c), r)—you cannot evaluate reasoning without understanding how concrete instances map to abstract features.
  - Quick check question: Given "01000110 + 00011111" as a concrete instance, what is the abstract feature f(c) the model should extract?

- **Concept: Symbolic Invariance**
  - Why needed here: The entire ∆ metric assumes models with true abstraction should produce identical outputs under bijective symbol substitution. Without this concept, the benchmark appears to test only "unfamiliar notation" rather than reasoning depth.
  - Quick check question: If '0'↔'A' and '1'↔'B', should a model's binary addition accuracy change? Why or why not?

- **Concept: Rule-Given vs. Rule-Inductive Reasoning**
  - Why needed here: The benchmark distinguishes tasks where rules are explicitly provided (H_G) from tasks requiring rule inference from examples (H_I). This determines whether you're testing rule application or rule discovery.
  - Quick check question: In the SMA task with input-output pairs (1,5), (2,8), (3,11), what must the model do before applying Re?

## Architecture Onboarding

- **Component map:**
  Task Generator (G_c) → Symbol Remapper (M) → Prompt Constructor → LLM → Response Parser → Γ/Δ Calculator
                                    ↓
                          [Raw, Op-remap, Num-remap, All-remap variants]

- **Critical path:**
  1. Generate task instances with ground-truth rules
  2. Apply remapping variants per Appendix A.5 protocol
  3. Evaluate model on raw dataset → compute Γ
  4. Evaluate model on remapped dataset → compute Γ_M
  5. Calculate ∆ = Γ - Γ_M for each remapping type (op, num, all)

- **Design tradeoffs:**
  - Bijective mapping vs. random substitution: Bijective preserves one-to-one structure, making tasks solvable in principle; trades ecological validity for controlled evaluation
  - Single-character tokens only: Filters prevent multi-token symbols that could break remapping; trades ecological validity for controlled evaluation
  - Zero-shot vs. few-shot CoT: Paper uses zero-shot CoT for most tasks, 8-shot for MA; few-shot may leak remapping patterns if examples share target remapping

- **Failure signatures:**
  - Near-zero NBR scores across all scales → base conversion abstraction failure, not scale limitation
  - ∆ > 0.4 with high Γ → high accuracy driven by memorization, not reasoning
  - CoT increases ∆ → procedural scaffolding reinforcing token associations
  - Random outputs on SMA → rule induction failure; model cannot infer functions from numerical examples

- **First 3 experiments:**
  1. Baseline Γ on EC tasks (raw symbols): Establish whether model can perform operations at all. If Γ < 0.2, remapping experiments are meaningless—model lacks basic competence.
  2. Compute ∆_num vs. ∆_op on fixed_len_chat_bit dataset: Confirm operand vs. operator memory dependence pattern. Expect ∆_num ≈ 0.35-0.45, ∆_op ≈ 0.08-0.15 for 70B models.
  3. Fine-tune on remapped data (Appendix A.11.2 protocol): Train Llama-3.1-8B on fully remapped samples, then evaluate on unseen remapping structure. If accuracy improves only on trained remapping pattern, confirms memorization of mapping-specific patterns rather than abstract rule acquisition.

## Open Questions the Paper Calls Out

### Open Question 1
Can symbolic data augmentation through systematic permutation effectively reduce token memorization and improve genuine abstraction in LLMs? Basis: Section 5.4 proposes this as a critical research direction. Unresolved because fine-tuning experiments showed models memorize specific mapping patterns rather than acquire generalizable rule-application capabilities. Evidence needed: Demonstrating that models trained with systematic permutation show consistently low ∆ scores across novel, unseen remapping patterns, not just those encountered during training.

### Open Question 2
Why does Chain-of-Thought prompting sometimes increase memory dependence (∆), and can this trade-off be mitigated? Basis: Section 5.2 notes "the concurrent rise in memory dependence (∆) with CoT in some cases indicates a potential trade-off." Unresolved because the mechanism by which CoT improves accuracy on familiar tasks while potentially increasing reliance on specific symbols remains poorly understood. Evidence needed: Systematic ablations of CoT components identifying which aspects enhance versus hinder abstraction, coupled with modified prompting strategies that decouple procedural guidance from symbol-specific reasoning.

### Open Question 3
What architectural innovations could enable representation-invariant rule learning rather than reliance on memorized patterns? Basis: Section 5.4 states findings indicate "LLMs' abstract reasoning limitations stem not from scale issues, but from their fundamental reliance on memorized patterns rather than representation-invariant rules." Unresolved because scaling from 7B to 70B parameters did not fundamentally address the abstraction deficit. Evidence needed: Novel architectures achieving consistently low ∆ scores across diverse remapping schemes while maintaining high Γ, demonstrating genuine abstraction independent of surface symbols.

### Open Question 4
How does the abstraction gap identified in symbolic tasks generalize to physical and causal reasoning domains? Basis: Section 5.4 proposes "Extension of benchmarks to evaluate generalization across physical and causal reasoning domains" as a research direction. Unresolved because current benchmark focuses on symbolic/rule-based tasks; whether similar memory dependence patterns manifest in spatial, physical, or causal reasoning is unknown. Evidence needed: Benchmarks extending symbol remapping methodology to physical simulations and causal reasoning tasks, revealing whether ∆ metrics correlate across domains.

## Limitations
- Symbol remapping construct validity assumes performance drops reflect memorization rather than tokenization artifacts or prompt engineering effects
- Answer parsing reliability depends on gpt-4o-mini for automated validation, introducing secondary model dependency
- Cannot distinguish between models that truly abstract rules versus those that learn to recognize remapping patterns through training exposure

## Confidence
- **High Confidence**: Operand symbols show higher memory dependence than operator symbols (MemDep_num >> MemDep_op) is robust across model scales and task types
- **Medium Confidence**: CoT trades procedural accuracy for increased memory dependence is supported but task-dependent
- **Low Confidence**: Poor NBR performance may conflate base conversion deficits with broader reasoning limitations

## Next Checks
1. **Tokenization Analysis**: Compare model outputs on remapped tokens mapping to single vs. multi-character sequences to determine if performance degradation correlates with tokenization complexity
2. **Cross-Lingual Remapping**: Test models on remapped symbols from different character sets (Greek letters, CJK characters) to verify memory dependence patterns aren't language-specific
3. **Few-Shot Learning Curve**: Systematically vary remapping examples in few-shot prompts (1-shot to 16-shot) to measure adaptation speed to novel symbolic systems