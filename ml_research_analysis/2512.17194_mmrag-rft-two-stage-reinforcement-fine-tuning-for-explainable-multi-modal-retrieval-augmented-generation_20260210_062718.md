---
ver: rpa2
title: 'MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal
  Retrieval-augmented Generation'
arxiv_id: '2512.17194'
source_url: https://arxiv.org/abs/2512.17194
tags:
- multi-modal
- generation
- fine-tuning
- ranking
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enhancing explainability
  in multi-modal retrieval-augmented generation (MMRAG) by introducing a two-stage
  reinforcement fine-tuning framework. The core method idea involves using rule-based
  reinforcement fine-tuning for coarse-grained point-wise ranking to filter irrelevant
  documents, followed by reasoning-based reinforcement fine-tuning for fine-grained
  list-wise ranking and answer generation, enabling explainable reasoning.
---

# MMRAG-RFT: Two-stage Reinforcement Fine-tuning for Explainable Multi-modal Retrieval-augmented Generation

## Quick Facts
- arXiv ID: 2512.17194
- Source URL: https://arxiv.org/abs/2512.17194
- Authors: Shengwei Zhao; Jingwen Yao; Sitong Wei; Linhai Xu; Yuying Liu; Dong Zhang; Zhiqiang Tian; Shaoyi Du
- Reference count: 6
- Primary result: Two-stage reinforcement fine-tuning framework achieving SOTA on WebQA (0.708 QA-FL, 0.763 QA-Acc, 0.583 QA) and MultimodalQA (+4.7% EM, +12.7% F1)

## Executive Summary
This paper addresses the challenge of enhancing explainability in multi-modal retrieval-augmented generation (MMRAG) by introducing a two-stage reinforcement fine-tuning framework. The core method idea involves using rule-based reinforcement fine-tuning for coarse-grained point-wise ranking to filter irrelevant documents, followed by reasoning-based reinforcement fine-tuning for fine-grained list-wise ranking and answer generation, enabling explainable reasoning. The approach achieves state-of-the-art results on WebQA and MultimodalQA datasets, with WebQA scores of 0.708 QA-FL, 0.763 QA-Acc, and 0.583 QA, and MultimodalQA improvements of 4.7% EM and 12.7% F1 over previous methods.

## Method Summary
The MMRAG-RFT framework employs a two-stage reinforcement fine-tuning approach to enhance multi-modal retrieval-augmented generation with explainable reasoning capabilities. Stage 1 uses rule-based reinforcement fine-tuning for coarse-grained point-wise ranking to filter irrelevant documents efficiently. Stage 2 employs reasoning-based reinforcement fine-tuning for fine-grained list-wise ranking and answer generation. The framework utilizes a high-quality Mini-WebQA dataset constructed from WebQA by removing low-quality samples. The approach is evaluated on WebQA and MultimodalQA datasets, demonstrating superior performance compared to baseline methods.

## Key Results
- WebQA dataset: Achieved QA-FL score of 0.708, QA-Acc score of 0.763, and QA score of 0.583
- MultimodalQA dataset: Improved EM by 4.7% and F1 by 12.7% over previous state-of-the-art methods
- Demonstrated significant improvements across multiple evaluation metrics on both datasets
- Showed effectiveness of two-stage approach in handling multi-hop questions with images and text

## Why This Works (Mechanism)
The two-stage reinforcement fine-tuning approach works by first efficiently filtering irrelevant documents through coarse-grained point-wise ranking (Stage 1), then performing fine-grained list-wise ranking and answer generation with reasoning-based reinforcement (Stage 2). This separation allows the system to handle the computational complexity of large-scale document ranking while maintaining high-quality reasoning capabilities. The rule-based rewards in Stage 1 provide clear guidance for relevance judgment, while the reasoning-based rewards in Stage 2 enable the model to develop explainable reasoning strategies without requiring chain-of-thought supervision data.

## Foundational Learning
- Multi-modal retrieval-augmented generation: Combines retrieval of relevant documents with generation of answers using both text and image inputs
- Reinforcement fine-tuning: Training approach using reward signals to guide model behavior rather than direct supervision
- Point-wise ranking: Document ranking method that evaluates individual documents independently
- List-wise ranking: Document ranking method that considers the entire list of candidates together
- Chain-of-thought reasoning: Explicit step-by-step reasoning process for complex problem solving
- Reward modeling: Design of functions that provide feedback signals to guide reinforcement learning

Why needed: Multi-modal QA requires integrating information from different sources; reinforcement fine-tuning enables learning without explicit reasoning supervision; ranking methods are essential for efficient document retrieval; chain-of-thought reasoning provides explainability; reward modeling guides the learning process.

Quick check: Understanding the difference between point-wise and list-wise ranking is crucial for grasping the computational efficiency benefits of the two-stage approach.

## Architecture Onboarding

Component map: Pre-trained MMRAG model -> Stage 1 (Rule-based Point-wise Ranking) -> Stage 2 (Reasoning-based List-wise Ranking) -> Answer Generation

Critical path: Retrieval -> Document Filtering (Stage 1) -> Document Ranking (Stage 2) -> Answer Generation

Design tradeoffs: Point-wise ranking vs. list-wise ranking efficiency vs. ranking quality; rule-based vs. learned rewards for different stages; computational cost vs. performance gains.

Failure signatures: Poor filtering in Stage 1 leading to irrelevant documents in Stage 2; inadequate reward design causing suboptimal reasoning; computational bottlenecks in large-scale retrieval scenarios.

First experiments: 1) Test document filtering accuracy with different k values; 2) Evaluate reward function effectiveness on held-out data; 3) Measure computational time scaling with number of candidate documents.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the reasoning capabilities developed through reinforcement fine-tuning without chain-of-thought supervision generalize effectively to entirely new multi-modal domains and task types beyond those seen during training?
- Basis in paper: The paper states there is "an urgent need to explore the potential of multi-modal retrieval-augmented generation to develop reasoning capabilities in the absence of chain-of-thought supervision data."
- Why unresolved: The experiments are limited to WebQA and MultimodalQA datasets, which share similar structures (multi-hop QA with images and text). Cross-domain generalization to video, audio, or fundamentally different task structures remains untested.
- What evidence would resolve it: Evaluation on diverse multi-modal datasets spanning different domains (scientific figures, medical imaging, video understanding, audio-text tasks) showing consistent performance without task-specific retraining.

### Open Question 2
- Question: What is the optimal trade-off between the number of candidate documents filtered in Stage 1 (point-wise ranking) and the computational efficiency of Stage 2 (list-wise ranking), particularly for large-scale retrieval scenarios?
- Basis in paper: The paper mentions that "point-wise ranking is more efficient than list-wise ranking in large-scale document ranking" and limits list-wise ranking to "top-k documents," but does not systematically vary or analyze optimal k values or scaling behavior.
- Why unresolved: The paper fixes k without ablation, and the relationship between filtering threshold, retrieval accuracy, and computational cost at scale (e.g., thousands of candidates) is not characterized.
- What evidence would resolve it: A systematic study varying k across different scales (10, 50, 100, 500+ candidates) with analysis of retrieval metrics (Retr score) and computational time/memory usage trade-offs.

### Open Question 3
- Question: How robust is the two-stage reinforcement fine-tuning framework to noisy or adversarial perturbations in multi-modal inputs, such as misleading images or contradictory text passages?
- Basis in paper: The paper focuses on standard benchmarks and constructs a "high-quality" Mini-WebQA dataset by removing low-quality samples, but does not evaluate performance under adversarial conditions or when retrieved documents contain conflicting information.
- Why unresolved: Real-world retrieval systems often surface noisy, misleading, or contradictory evidence. The paper's reliance on filtered, clean training and test data leaves adversarial robustness unexplored.
- What evidence would resolve it: Experiments with adversarially perturbed images, injected distractor documents with conflicting facts, or corrupted text, measuring degradation in QA accuracy and reasoning faithfulness.

### Open Question 4
- Question: Can the rule-based and reasoning-based reward functions designed for this framework be effectively replaced or augmented with learned reward models to further improve performance and reduce manual reward engineering?
- Basis in paper: The paper manually designs multiple reward functions (format reward, relevance judgment reward, list-wise ranking reward, answer generation reward) but does not explore whether these could be learned or jointly optimized.
- Why unresolved: Hand-crafted rewards may not capture nuanced aspects of multi-modal reasoning quality and could limit the model's ability to discover superior reasoning strategies.
- What evidence would resolve it: Comparative experiments using learned reward models (e.g., trained on human preferences or model-generated critiques) against the current rule-based rewards, with analysis of training stability and final performance metrics.

## Limitations
- Evaluation focused on only two datasets (WebQA and MultimodalQA), limiting generalizability assessment
- Explainability claims supported indirectly through performance rather than direct reasoning transparency measurement
- Substantial computational resources required for reinforcement fine-tuning may limit practical deployment
- Performance gains show diminishing returns on some metrics (WebQA QA score of 0.583)

## Confidence
High confidence: The core methodology (two-stage reinforcement fine-tuning with coarse and fine-grained ranking) is clearly described and shows consistent improvements across both datasets.

Medium confidence: The explainability claims are supported indirectly through improved performance and some qualitative examples, but lack direct measurement of reasoning transparency.

Medium confidence: The state-of-the-art claims are well-supported on the tested benchmarks, but generalizability to other multi-modal tasks remains unproven.

## Next Checks
1. Conduct ablation studies isolating the impact of rule-based versus reasoning-based reinforcement components to quantify their individual contributions to performance gains.

2. Test the MMRAG-RFT framework on additional multi-modal datasets beyond WebQA and MultimodalQA to evaluate generalizability across different task types and domains.

3. Implement human evaluation studies specifically measuring the perceived explainability and reasoning transparency of the generated answers, beyond automated metric evaluation.