---
ver: rpa2
title: 'Linear Representation Transferability Hypothesis: Leveraging Small Models
  to Steer Large Models'
arxiv_id: '2506.00653'
source_url: https://arxiv.org/abs/2506.00653
tags:
- steering
- linear
- representation
- hypothesis
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the Linear Representation Transferability (LRT)
  Hypothesis, which posits that neural networks trained on similar data can have their
  representations linearly mapped to one another. The authors introduce a conceptual
  framework where representations are expressed as linear combinations of universal
  basis features, with each model projecting these onto its own subspace.
---

# Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models

## Quick Facts
- arXiv ID: 2506.00653
- Source URL: https://arxiv.org/abs/2506.00653
- Authors: Femi Bello; Anubrata Das; Fanzhi Zeng; Fangcong Yin; Liu Leqi
- Reference count: 19
- Primary result: Steering vectors derived from Gemma-2B transfer successfully to Gemma-9B with correlations of 0.8-0.98

## Executive Summary
This paper introduces the Linear Representation Transferability (LRT) Hypothesis, which posits that neural networks trained on similar data can have their representations linearly mapped to one another. The authors demonstrate that affine transformations can successfully transfer steering vectors between Gemma-2B and Gemma-9B models, with high correlation (0.8-0.98) and low error (0.013-0.087) when transferring from smaller to larger models. This suggests that representations learned by small models can effectively guide the behavior of larger models, offering practical implications for efficient inference, model distillation, and performance prediction.

## Method Summary
The method involves training affine mappings between hidden states of small and large models to transfer behavior-modifying steering vectors. Hidden states are collected from paired layers of source and target models, then an affine transformation (A, p) is learned via least squares minimization. Steering vectors are computed on the source model using Contrastive Activation Addition, then mapped to the target model using the learned affine transformation. The approach can operate directly on hidden states (l2l) or through SAE-encoded coefficients (s2l), with the latter showing better effectiveness in preserving semantic directions.

## Key Results
- Affine mappings preserve steering behaviors with Pearson correlations of 0.8-0.98 between direct and transferred steering effects
- Mean squared error remains low (0.013-0.087) when transferring steering vectors from Gemma-2B to Gemma-9B
- Coefficient-based transfer (s2l) outperforms hidden-state-to-hidden-state mapping in steering effectiveness
- Steering vectors from small models can successfully guide larger models' behavior on tasks like refusal and propensity shifting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Models trained on similar data with shared tokenization learn representations that occupy different subspaces of a universal feature space, enabling linear mapping between them.
- **Mechanism:** Each model's feature matrix (W_S or W_T) is a projection of a universal feature matrix (W_U) via model-specific projection matrices (P_S, P_T). When both models activate the same features with the same coefficients, the hidden states become linearly related: h_T(x) ≈ A·h_S(x) + p, where A = P_T^T(P_S^T)^† maps through the universal space.
- **Core assumption:** Models trained on the same distribution discover shared statistical structures (syntax, semantics, co-occurrence patterns) that manifest as a universal basis V_U. Assumption: feature activation coefficients are similar across models for the same input.
- **Evidence anchors:**
  - [abstract] "These basis features underlie the learning task itself and remain consistent across models, regardless of scale."
  - [Section 2.2] Formal derivation showing h_T = Ah_S + p when R_S^{-1}c_S = R_T^{-1}c_T = c(x)
  - [Section 2.2] SAE decoder matrix analysis: Frobenius reconstruction error ~114 vs. much higher for random matrices, indicating W_T ≈ W_S·M
- **Break condition:** If models use fundamentally different feature indices (R_S ≠ permutation of R_T) or activate different features for the same input, the linear relationship degrades. Cross-family transfer may fail due to tokenizer differences.

### Mechanism 2
- **Claim:** A single affine transformation trained on paired hidden states can transfer behavior-modifying steering vectors from small to large models.
- **Mechanism:** Train A, p via least squares: min_{A,p} E_x[‖A·h^S_{l_S}(x) + p - h^T_{l_T}(x)‖²]. The steering vector v^S found via contrastive activation addition transfers as ṽ^T = A·v^S + p, preserving the semantic direction because both the positive and negative activation centroids are linearly mapped.
- **Core assumption:** Layers with similar relative depth (e.g., both mid-layer) process information similarly. The affine map is layer-pair specific.
- **Evidence anchors:**
  - [Section 3] "We sweep over various layers and steering strengths. Once we find two layers that work for both source and destination models, we train an affine mapping between them."
  - [Figure 5/Table 1] High Pearson correlations (0.73-0.98, mean 0.88) and low MSE (0.005-0.096) across 13 steering tasks when comparing direct vs. transferred steering on Gemma-9B
  - [corpus] Concurrent work (Oozeer et al. 2025) reports mixed results for affine transformations when trained on steering data itself, suggesting training data independence matters
- **Break condition:** If steering vector relies on features that exist only in the larger model (not learned by smaller model), transfer will fail. Large dimension gap (e.g., 100M → 70B) may exceed the method's capacity.

### Mechanism 3
- **Claim:** Mapping feature coefficients (c_S) directly to target hidden states outperforms hidden-state-to-hidden-state mapping.
- **Mechanism:** The "s2l" approach uses SAE encoder to extract sparse coefficient vectors c^S_{l_S}(x) from source, then maps these to target hidden states. This avoids information loss from reconstructing coefficients from h_S (which may be lossy due to dimension reduction).
- **Core assumption:** SAE decoder features approximate monosemantic universal features; the encoder accurately captures which features are active.
- **Evidence anchors:**
  - [Section 3.2] "For s2l steering... steering with the mapped coefficient vectors is more effective than the mapped source hidden state"
  - [Section 2.2] Notes relationship to SAE formulation: hidden states modeled as h(x) = W_U^T c(x) + b
  - [corpus] Weak direct evidence—corpus papers focus on representation similarity metrics, not coefficient transfer specifically
- **Break condition:** SAE quality dependency—if the encoder fails to capture relevant features for the steering task, coefficient-based transfer degrades.

## Foundational Learning

- **Concept: Residual Stream Structure**
  - **Why needed here:** The paper operates on transformer hidden states defined recursively: h^{l+1}(x) = h^l(x) + F^l(h^l(x)). Understanding that representations accumulate through residual connections is essential for selecting which layers to map.
  - **Quick check question:** Why might mid-layer hidden states be better candidates for transfer than early or final layers?

- **Concept: Contrastive Activation Addition (CAA)**
  - **Why needed here:** The paper uses CAA to derive steering vectors: v_l = mean(h_l(D_positive)) - mean(h_l(D_negative)). This method assumes behavior is encoded as a direction in activation space.
  - **Quick check question:** What would happen to the steering vector if positive and negative datasets were drawn from very different distributions (not just different behaviors)?

- **Concept: Superposition and Linear Representation Hypothesis**
  - **Why needed here:** The LRT hypothesis extends LRH (features as linear directions) to cross-model settings, assuming models in superposition still share underlying basis features. Without this foundation, the universal space concept lacks grounding.
  - **Quick check question:** If a model stores more features than dimensions (superposition), does this strengthen or weaken the claim that feature matrices are linearly related across models?

## Architecture Onboarding

- **Component map:**
  Source model (M_S) -> Target model (M_T) -> Affine mapper (A, p) -> Steering vector extractor (CAA) -> SAE encoder/decoder (optional)

- **Critical path:**
  1. Collect paired activations from source/target models on representative data (10M tokens recommended)
  2. Train affine map on aligned layer pairs (similar relative depth)
  3. Compute steering vector on source model via CAA
  4. Apply: ṽ^T = A·v^S + p; inject during inference as h' = h + α·ṽ^T/‖ṽ^T‖
  5. Evaluate via downstream metric (e.g., propensity shift, refusal score)

- **Design tradeoffs:**
  - **Hidden-to-hidden (l2l) vs. coefficient-to-hidden (s2l):** l2l requires only activations; s2l requires pretrained SAE but may preserve more semantic information
  - **Training data:** Pretraining corpus (The Pile) vs. instruction mixture—paper shows both work, but matching target model's distribution improves results
  - **Layer selection:** Many-to-one mapping (all source layers → one target layer) theoretically captures more information but is computationally expensive; one-to-one is practical default

- **Failure signatures:**
  - Low correlation between direct and transferred steering effects (<0.5): likely layer mismatch or training data mismatch
  - High MSE (>0.1) on held-out validation: affine map underfitting; increase model capacity or training data
  - Steering effect reversed (negative α needed): bias term p may be dominating; check normalization
  - Cross-family transfer fails: tokenizer mismatch—not currently supported by LRT hypothesis

- **First 3 experiments:**
  1. **Sanity check:** Train affine map on Gemma-2B → Gemma-9B (layer 20 → layer 20) using 1M tokens; verify reconstruction MSE <0.1 on held-out data
  2. **Steering transfer validation:** Compute "French speaking" steering vector on 2B; transfer to 9B; measure token-level French probability increase. Compare to native 9B steering vector (expect r > 0.8 correlation in propensity curves)
  3. **Ablation: layer depth sensitivity:** Train maps for early (layer 5), mid (layer 12), and late (layer 20) pairs. Transfer same steering vector; measure which preserves semantic effect best. Expect mid-layer to outperform based on paper's relative depth guidance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: At what specific scale difference does linear representation transferability break down?
- Basis in paper: [explicit] Section 5 ("Limits of Transferability across Model Sizes") asks whether a 100M parameter model can effectively steer a 70B model.
- Why unresolved: The empirical validation is limited to Gemma-2B and Gemma-9B; the boundaries of scaling laws for this phenomenon are unmapped.
- What evidence would resolve it: Successful or failed steering transfer experiments across models with orders-of-magnitude differences in parameter counts.

### Open Question 2
- Question: Can representation transferability be extended to different model families with distinct tokenizers?
- Basis in paper: [explicit] Section 5 ("Limits of Transferability across Model Families") notes that cross-family transfer is hindered by the lack of reliable methods for transferring tokenization schemes.
- Why unresolved: The LRT hypothesis currently assumes models share the same tokenizer and similar architecture.
- What evidence would resolve it: Discovery of a method to align representations across different tokenizers (e.g., Llama to Gemma) without performance degradation.

### Open Question 3
- Question: Do "Many-to-Many" layer mappings provide superior steering fidelity compared to the single-layer mappings used in this study?
- Basis in paper: [inferred] Appendix A proposes a theoretical "Many-to-Many" mapping objective ($h^T_l = \sum A_{i,l}h^S_i + b$) but states it was not implemented due to computational expense.
- Why unresolved: It remains untested whether aggregating information from all source layers results in a more accurate reconstruction of the target hidden states.
- What evidence would resolve it: Training the proposed many-to-many objective and comparing the Mean Squared Error (MSE) and steering correlation against the single-layer baseline.

## Limitations
- Cross-family transferability remains untested due to tokenizer alignment challenges
- Large scale differences (e.g., 100M → 70B) may exceed the method's capacity
- SAE quality significantly impacts coefficient-based transfer effectiveness

## Confidence

- **High Confidence (Mechanism 1):** The linear relationship between hidden states is well-supported by reconstruction experiments showing MSE < 0.1 and by concurrent work demonstrating affine mappings work when properly trained.
- **Medium Confidence (Mechanism 2):** Steering transfer shows promising correlations, but the exact conditions for successful transfer (layer selection criteria, steering strength ranges) remain underspecified in the paper.
- **Low Confidence (Mechanism 3):** The coefficient-based transfer approach lacks direct empirical validation in the paper, relying instead on SAE literature and theoretical arguments.

## Next Checks

1. **Cross-family transferability test:** Train affine mappings between Gemma-2B and Llama-9B (different tokenizer, architecture). Measure whether steering vectors transfer successfully and compare correlation coefficients to same-family transfer results.

2. **SAE sensitivity analysis:** Vary SAE hyperparameters (coefficient sparsity, reconstruction threshold) and measure impact on steering vector transfer quality. Establish minimum SAE quality requirements for reliable coefficient transfer.

3. **Multi-layer mapping evaluation:** Implement many-to-one mapping (all source layers → single target layer) and compare steering transfer quality against one-to-one mappings. Measure whether combining information across layers improves or degrades transfer fidelity.