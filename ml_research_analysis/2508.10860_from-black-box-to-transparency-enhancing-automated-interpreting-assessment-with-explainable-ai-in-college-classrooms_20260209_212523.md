---
ver: rpa2
title: 'From Black Box to Transparency: Enhancing Automated Interpreting Assessment
  with Explainable AI in College Classrooms'
arxiv_id: '2508.10860'
source_url: https://arxiv.org/abs/2508.10860
tags:
- features
- interpreting
- language
- data
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses limitations in automated interpreting assessment,
  including insufficient focus on language use quality, model underperformance due
  to data scarcity/imbalance, and lack of explainability. To overcome these challenges,
  it proposes a multi-dimensional modeling framework that combines feature engineering,
  data augmentation using Variational Autoencoders (VAEs), and explainable machine
  learning with Shapley Value (SHAP) analysis.
---

# From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms

## Quick Facts
- arXiv ID: 2508.10860
- Source URL: https://arxiv.org/abs/2508.10860
- Authors: Zhaokun Jiang; Ziyin Zhang
- Reference count: 40
- One-line primary result: Multi-dimensional explainable AI framework for interpreting assessment with strong performance and interpretable features

## Executive Summary
This study addresses key limitations in automated interpreting assessment by proposing a transparent, multi-dimensional framework that combines feature engineering, VAE-based data augmentation, and SHAP analysis for interpretability. The method prioritizes construct-relevant features over black-box embeddings, enabling detailed diagnostic feedback for learners. Results show strong predictive performance on a novel English-Chinese consecutive interpreting dataset, with BLEURT and CometKiwi being the strongest predictors for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. The approach offers a scalable, reliable, and transparent alternative to traditional human evaluation in college classrooms.

## Method Summary
The study uses 117 English-Chinese consecutive interpreting samples from 39 undergraduate English majors, with audio transcribed via iFLYTEK ASR and scores calibrated through MFRM. Three separate models are trained for InfoCom (fidelity), FluDel (fluency), and TLQual (target language quality) dimensions using 5 translation metrics, 14 temporal features, and 25 Chinese-specific phraseological indices respectively. VAE data augmentation expands the dataset to 500 samples. Models include XGBoost, Random Forest, and MLP, with SHAP analysis providing global feature importance and local explanations. Best performers are RF for InfoCom and XGBoost for FluDel/TLQual.

## Key Results
- Strong predictive performance across all three dimensions with RMSE improvements of 10-20% through VAE augmentation
- BLEURT and CometKiwi identified as strongest predictors for fidelity (InfoCom)
- Pause-related features most important for fluency (FluDel)
- Chinese-specific phraseological diversity metrics (CN_RATIO) dominate for language use (TLQual)
- SHAP analysis provides interpretable feature attributions matching theoretical expectations

## Why This Works (Mechanism)

### Mechanism 1
VAE-based data augmentation improves model performance, particularly for extreme score predictions, by generating synthetic feature vectors that preserve feature-label correspondence. Conditional VAEs learn the joint distribution of features and scores, then sample from this latent space to create coherent synthetic samples. This addresses class imbalance without simply duplicating minority cases.

### Mechanism 2
Multi-dimensional modeling with construct-relevant features enables interpretable predictions because each dimension uses theoretically motivated feature sets rather than opaque embeddings. Three separate models (InfoCom, FluDel, TLQual) each receive domain-specific features: neural MT metrics for fidelity, temporal/pause features for fluency, and Chinese-specific phraseological indices for language quality. This modular design localizes prediction logic.

### Mechanism 3
SHAP-based local explanations provide diagnostic feedback by quantifying each feature's contribution to individual score deviations from the mean. Shapley values decompose predictions into additive feature contributions relative to a base value (dataset mean). Force plots and waterfall plots visualize which features push scores up or down.

## Foundational Learning

- **Variational Autoencoders (VAEs)**: Understanding how VAEs encode feature distributions and generate synthetic samples is prerequisite to diagnosing augmentation quality. Quick check: Can you explain why VAEs use a reparameterization trick, and how the KL divergence term affects latent space structure?

- **Shapley Values and Game-Theoretic Attribution**: SHAP is the core interpretability technique; understanding its axiomatic foundations helps assess when explanations are trustworthy. Quick check: Why do Shapley values require exponential computation in theory, and what approximation strategies does the SHAP library use for tree models?

- **Chinese Phraseological Structure (Topic-Comment, Classifier-Noun Constructions)**: TLQual features rely on Chinese-specific linguistic patterns; interpreting SHAP outputs requires knowing what CN_RATIO, PP_RTTR, etc., measure. Quick check: How does the topic-comment structure in Chinese differ from subject-predicate organization in English, and why might longer clauses correlate negatively with quality in Chinese interpreting?

## Architecture Onboarding

- Component map: Raw Audio → ASR (iFLYTEK) → Transcript → [GPT-4o Error Detection] → Cleaned Transcript → [InfoCom Features, FluDel Features, TLQual Features] → VAE Augment → [RF/XGBoost/MLP] → SHAP Global → SHAP Local

- Critical path: ASR transcription accuracy (garbage in → garbage out for all downstream) → GPT-4o error detection quality (affects TLQual grammatical accuracy features) → Pause detection threshold calibration (-18 dB per Wu 2021) → VAE latent dimension and training stability → SHAP computation on winning model

- Design tradeoffs: Interpretability vs. raw performance (MLP achieved lower scores but prioritized XGBoost/RF for SHAP compatibility), Feature granularity vs. data scarcity (25 TLQual features for 117 samples risks overfitting), Language-specific vs. generalizable (CCA features are Chinese-specific)

- Failure signatures: VAE mode collapse (generated samples cluster around a few prototypes), SHAP inconsistency (different SHAP approximators give divergent importance rankings), ASR drift (if your audio quality differs from training, pause detection and transcription will fail silently)

- First 3 experiments: Replicate on held-out data (train on 80% of original 117 samples, test on 20%, compare RMSE/Spearman to paper's Table 5), Ablate VAE, test SMOTE (replace VAE with SMOTE interpolation; if performance drops significantly, VAE's distributional modeling is doing real work), Human evaluation of local explanations (present SHAP force plots to 3-5 interpreting instructors; collect Likert ratings for "explanation matches my intuition about this student's performance")

## Open Questions the Paper Calls Out

- Does the provision of SHAP-based local explanations lead to measurable improvements in student learning outcomes compared to receiving only automated scores? (Basis: The conclusion states the method offers a "promising direction" for diagnostic feedback, but the study itself only validates the accuracy and interpretability of the model, not the efficacy of the feedback in an educational intervention)

- To what extent do the feature importance rankings identified for Target Language Quality (e.g., Chinese phraseological diversity) generalize to other interpreting directions, such as Chinese-to-English (L2 production)? (Basis: The study is restricted to English-to-Chinese interpreting, and the results specifically highlight the predictive power of Chinese-specific linguistic features which may not have direct equivalents in other target languages)

- Does VAE-based data augmentation introduce systematic biases or artifacts that limit model performance on truly outlier performances (very poor/very good) despite improving aggregate metrics? (Basis: The paper notes that the original dataset lacked samples in the 1-2 score range, and while VAE improved performance, the synthetic data distribution was forced into a uniform distribution which might not reflect real-world variance)

## Limitations

- Data scarcity: 117-sample dataset limits deep learning approaches and generalizability to other languages/modes
- VAE augmentation uncertainty: Lack of rigorous validation that generated samples preserve realistic feature-score relationships
- Interpretability grounding: SHAP explanations not validated against human expert judgment for pedagogical effectiveness

## Confidence

- High Confidence: Feature engineering methodology, data augmentation pipeline implementation, SHAP analysis procedure
- Medium Confidence: VAE contribution to performance (empirical results but weak external validation), local explanation utility for pedagogical feedback (plausible but untested)
- Low Confidence: Generalization to other languages/modes, robustness of augmented samples, actual effectiveness of SHAP-based feedback in real classroom settings

## Next Checks

1. External Validation on Held-Out Data: Train on 80% of original samples, test on 20%, verify RMSE/Spearman improvements of 10-20% with VAE augmentation as claimed. Compare against SMOTE baseline to isolate VAE-specific benefits.

2. Augmented Data Quality Audit: Generate 500 augmented samples, analyze distribution statistics and visualize feature relationships. Conduct expert review of 20 randomly selected synthetic samples to assess realism and score coherence.

3. Human Evaluation of Explanations: Present SHAP force plots to 3-5 interpreting instructors for 10 student samples. Collect Likert ratings on "explanation matches my intuition about this student's performance." Target >70% agreement to establish credibility of local explanations.