---
ver: rpa2
title: 'xMTF: A Formula-Free Model for Reinforcement-Learning-Based Multi-Task Fusion
  in Recommender Systems'
arxiv_id: '2504.05669'
source_url: https://arxiv.org/abs/2504.05669
tags:
- xmtf
- fusion
- user
- stage
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes xMTF, a formula-free multi-task fusion framework
  for recommender systems. It introduces learnable monotonic fusion cells (MFCs) that
  capture the monotonicity property of MTF without relying on pre-defined fusion formulas,
  thus expanding the search space.
---

# xMTF: A Formula-Free Model for Reinforcement-Learning-Based Multi-Task Fusion in Recommender Systems

## Quick Facts
- **arXiv ID**: 2504.05669
- **Source URL**: https://arxiv.org/abs/2504.05669
- **Reference count**: 38
- **Primary result**: Formula-free RL-based MTF achieves 1279.7s total watch time vs. 1185.4s best baseline

## Executive Summary
This paper introduces xMTF, a novel framework for multi-task fusion (MTF) in recommender systems that replaces pre-defined fusion formulas with learnable monotonic fusion cells (MFCs). The key insight is that any suitable fusion function can be expressed as a composition of single-variable monotonic functions (per Sprecher Representation Theorem), allowing personalized fusion functions to be learned without being constrained by human-specified formulas. To handle the larger search space, xMTF employs a two-stage hybrid (TSH) learning strategy combining reinforcement learning for long-term optimization with supervised learning for fast convergence. Extensive experiments on KuaiRand dataset and online deployment on a platform serving 100M+ users demonstrate significant improvements over state-of-the-art formula-based approaches.

## Method Summary
xMTF introduces learnable monotonic fusion cells that capture the monotonicity property of multi-task fusion without relying on pre-defined fusion formulas. Each prediction type passes through a dedicated MLP (MFC) that learns a personalized monotonic transformation, which are then summed to produce the final ranking score. The framework employs a two-stage hybrid training strategy: an outer stage with simple second-order functions trained via actor-critic reinforcement learning for long-term reward optimization, and an inner stage with expressive MLPs trained via supervised learning to match outer stage preferences while maintaining monotonicity through a pairwise loss. This design enables discovering optimal fusion functions beyond human-specified formulas while keeping RL action space tractable.

## Key Results
- **Offline performance**: xMTF achieves 1279.7s total watch time, outperforming existing methods (1185.4s best baseline)
- **Online deployment**: 0.833% improvement in daily watch time and 0.583% increase in play counts over previous state-of-the-art formula-based approach
- **Monotonicity sensitivity**: λ=0.4 achieves best performance (1279.7s); λ=0 fails catastrophically (732.8s), demonstrating importance of monotonicity constraint

## Why This Works (Mechanism)

### Mechanism 1: Monotonic Fusion Cells Expand Search Space
Replacing pre-defined formulas with learnable MFCs allows the model to discover optimal fusion functions beyond human-specified formulas. Each prediction type passes through a dedicated MLP that learns a personalized monotonic transformation, decomposing the fusion function into K independent single-variable monotonic functions. The core assumption is that the optimal fusion function is monotonically increasing with respect to each input prediction. Evidence shows xMTF outperforms formula-based methods, and the Sprecher Representation Theorem guarantees this decomposition is valid. Break condition: if monotonicity is violated, the model may produce counterintuitive rankings.

### Mechanism 2: Two-Stage Hybrid Training Decouples Expressiveness from RL Complexity
TSH enables learning high-dimensional MFC parameters while keeping RL action space tractable. The outer stage uses simple second-order functions with scalar parameters as RL actions, while the inner stage with expressive MLPs learns via supervised loss to match outer stage's ranking preferences. Core assumption: a simple outer function can guide a complex inner function through ranking consistency loss. Evidence: ablation shows xMTF w/o outer stage drops to 1092.8s (vs 1279.7s full model), proving RL is necessary; w/o inner stage drops to 1106.3s, proving expressiveness matters. Break condition: if outer stage learning rate is too high or λ is too low, inner stage may not converge to monotonic functions.

### Mechanism 3: Monotonicity Loss Enforces Domain Constraint
Pairwise monotonicity loss ensures learned MFCs satisfy the domain requirement that fusion functions be monotonic in each input. For each prediction type, L_mono penalizes cases where lower input predictions yield higher transformed outputs. Core assumption: monotonicity is not naturally enforced by standard neural network training. Evidence: λ=0 yields worst performance (732.8s); λ=0.4 achieves best 1279.7s. Visual confirmation shows learned MFCs are monotonic across users and prediction types.

## Foundational Learning

- **Concept: Multi-Task Learning (MTL) in Recommender Systems**
  - Why needed: xMTF operates on predictions from an MTL module (e.g., MMoE) that outputs click rate, like rate, watch time, etc.
  - Quick check: If your MTL model predicts click probability (CTR) and expected watch time, how would you combine these into a single ranking score? (Answer: This is the MTF problem xMTF solves.)

- **Concept: Markov Decision Process (MDP) Formulation for RL**
  - Why needed: The paper models user-system interaction as an MDP with state s_t (user profile, history), action a_t (fusion parameters), and reward r_t (watch time, engagement).
  - Quick check: In xMTF, what is the RL agent's action at each timestep? (Answer: The parameters a_k that control the outer stage fusion function, not the full MFC parameters.)

- **Concept: Monotonic Functions and Their Inverse**
  - Why needed: The theoretical guarantee (Proposition 4.1) relies on monotonic functions h_k and g. Understanding that monotonic functions preserve ordering and have well-defined inverses is key to seeing why the decomposition works.
  - Quick check: Why does the paper state that the function g in Eq. 4 "does not need to be modeled"? (Answer: Because g is monotonic and we only care about ranking order, not absolute values; g doesn't change which items score highest.)

## Architecture Onboarding

- **Component map**:
  - User state s_t (profile, behavior history, context) + K prediction values o_1^i, ..., o_K^i per candidate item i
  - Inner stage (MFCs): K parallel MLPs, each taking (o_k^i, s_t) → q̃_k^I,i. Parameters: θ_k^I (many per k). Constrained by L_mono^I_k.
  - Outer stage: K simple second-order functions h̃_k^O(q̃_k^I,i; a_k) = q̃_k^I,i * (1 + a_k * q̃_k^I,i). Parameters: scalar a_k per prediction type.
  - Fusion: Final score z̃_i = Σ_k h̃_k^O(q̃_k^I,i; a_k)
  - RL components: Actor network μ(s_t; ξ) produces action vector a = {a_1, ..., a_K}. Critic network Q(s_t, a_t; φ) estimates long-term reward.

- **Critical path**:
  1. User request arrives with candidate items and MTL predictions
  2. Inner stage MFCs transform each prediction per user state (inference: only inner stage runs)
  3. Outer stage applies learned scalars a_k (current policy from actor network)
  4. Items ranked by sum of transformed scores, top-N returned
  5. User feedback collected, added to replay buffer
  6. Training: session data used to update critic (Eq. 13), actor (Eq. 14), and inner stage parameters (Eq. 18)

- **Design tradeoffs**:
  - Expressiveness vs. RL tractability: Inner stage has high capacity but is trained via supervised learning; outer stage is simple but trained via RL for long-term reward.
  - Monotonicity enforcement vs. learning flexibility: λ controls this. Too low → non-monotonic MFCs, poor performance. Too high → ignores user satisfaction signal from transfer loss.
  - Online latency: MFCs add computation per candidate item. Inference only requires inner stage forward pass (outer is just scalar multiplication).

- **Failure signatures**:
  - Non-monotonic MFC outputs: Check by visualizing h̃_k^I vs. o_k. Likely cause: λ too low or inner stage learning rate too high.
  - RL instability (critic divergence): Look for exploding Q-values. Paper uses target networks to mitigate.
  - No improvement over baseline: Check if actor is learning (a_k changing over training); if frozen, learning rate may be too low or reward signal too sparse.

- **First 3 experiments**:
  1. **Reproduce monotonicity ablation**: Train xMTF with λ ∈ {0, 0.1, 0.4, 0.9, 1} on KuaiRand subset. Visualize MFC outputs to confirm monotonicity correlates with performance.
  2. **Validate TSH necessity**: Compare full xMTF vs. xMTF w/o outer (supervised only) vs. xMTF w/o inner (formula-based with learned a_k). Confirm both stages are required for best performance.
  3. **Compare to strongest baseline**: Implement BatchRL-MTF-2 as formula-based RL control. Measure total watch time gap to xMTF. If gap < 5%, check if your MFC architecture is expressive enough.

## Open Questions the Paper Calls Out

### Open Question 1
Can xMTF effectively handle prediction inputs that are negatively correlated with user satisfaction (e.g., "skip rate" or "negative feedback") without violating the enforced monotonicity constraint?
- Basis in paper: [inferred] Section 3.2 states the fusion function must be monotonically increasing, and Eq. (7) defines a loss function that penalizes non-increasing outputs. The paper focuses on positive feedback but does not discuss integration of negative signals which should reduce the final score.
- Why unresolved: The MFC structure forces the transformation to be monotonically increasing; incorporating negative feedback directly would violate this mathematical constraint, requiring architectural modifications not explored in the text.
- What evidence would resolve it: Experiments integrating negative feedback signals (e.g., downvotes, fast-skips) into the xMTF framework, potentially by modifying the MFC to allow decreasing monotonicity or demonstrating that pre-inversion of inputs is sufficient.

### Open Question 2
Does xMTF maintain its performance advantage over formula-based methods in domains with dense, explicit feedback (e.g., E-commerce) compared to the implicit, sparse feedback environment of short-video platforms?
- Basis in paper: [inferred] The Introduction mentions E-commerce and news as applications, but experiments are restricted to the short-video domain using KuaiRand dataset.
- Why unresolved: The distribution of user feedback and the correlation between short-term predictions and long-term satisfaction differ significantly between video consumption and purchasing behavior; the "formula-free" flexibility may be more or less critical depending on these relationships.
- What evidence would resolve it: Offline and online evaluations of xMTF on a non-video dataset (e.g., an E-commerce recommendation log) comparing the relative gain against the video domain results.

### Open Question 3
What is the inference latency overhead of the xMTF framework compared to lightweight formula-based approaches, and does it scale efficiently as the number of prediction tasks (K) increases?
- Basis in paper: [inferred] The method replaces simple arithmetic formulas with Multi-Layer Perceptrons in the Inner Stage. While the paper mentions serving 100M+ users, it does not provide an analysis of computational cost or latency.
- Why unresolved: Large-scale recommender systems operate under strict latency budgets (milliseconds). Replacing a simple sum with a neural network pass for every candidate item could introduce bottlenecks that limit feasibility in lower-resource environments.
- What evidence would resolve it: A detailed latency analysis reporting the average inference time per request for xMTF versus baselines, and an analysis of how latency scales with the dimension K.

## Limitations
- **Dataset scope**: Experiments limited to single dataset (KuaiRand) without validation on other recommendation domains
- **Hyperparameter sensitivity**: Performance highly sensitive to λ (monotonicity weight), with drastic performance drops at λ=0 or λ=1
- **Computational overhead**: Replacing simple formulas with MLPs introduces inference latency that may impact large-scale deployment

## Confidence
- **High**: The mechanism of using learnable monotonic fusion cells to expand the search space beyond pre-defined formulas is well-supported by both theory and ablation experiments.
- **Medium**: The two-stage hybrid training approach is novel and shows performance benefits, but lacks extensive ablation or comparison to alternative training strategies.
- **Medium**: The monotonic constraint enforcement via pairwise loss is empirically validated, but the sensitivity to λ and alternative monotonicity formulations is not explored.

## Next Checks
1. **Dataset generalization**: Test xMTF on at least one additional recommender system dataset (e.g., MovieLens, Amazon reviews) to verify performance gains generalize beyond KuaiRand.
2. **Hyperparameter sensitivity**: Systematically vary λ and inner stage MLP architecture (layers, hidden dimensions) to establish robust hyperparameter ranges and identify failure modes.
3. **Prediction-type-specific analysis**: Analyze which of the 6 prediction types (click, long view, like, follow, comment, share) benefit most from formula-free fusion versus traditional formulas, to understand when xMTF adds value.