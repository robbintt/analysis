---
ver: rpa2
title: 'Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural
  Mathematical Reasoning in LLMs'
arxiv_id: '2602.00564'
source_url: https://arxiv.org/abs/2602.00564
tags:
- reasoning
- step
- answer
- judge
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of assessing genuine mathematical
  reasoning in large language models, which is obscured by current benchmarks' reliance
  on template-based computation and final-answer accuracy. The authors propose REASONINGMATH-PLUS,
  a benchmark of 150 carefully curated problems that emphasize structural reasoning
  skills like multi-constraint coordination and constructive synthesis, each annotated
  with a minimal reasoning skeleton for fine-grained evaluation.
---

# Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural Mathematical Reasoning in LLMs

## Quick Facts
- arXiv ID: 2602.00564
- Source URL: https://arxiv.org/abs/2602.00564
- Authors: Xiang Zheng, Weiqi Zhai, Wei Wang, Boyu Yang, Wenbo Li, Ruixiang Luo, Haoxiang Sun, Yucheng Wang, Zhengze Li, Meng Wang, Yuetian Du, Guojie Lin, Yaxuan Wang, Xiaoxiao Xu, Yanhu Mo, Xuan Ren, Hu Wei, Ze Xu
- Reference count: 40
- Primary result: Current benchmarks overestimate LLM mathematical reasoning by focusing on final answers; HCRS-based holistic scoring reveals substantially lower reasoning quality (average 4.36/10 vs. answer accuracy up to 5.8/10)

## Executive Summary
This work addresses the critical gap between LLM answer accuracy and genuine mathematical reasoning capability by introducing REASONINGMATH-PLUS, a benchmark emphasizing structural reasoning skills like multi-constraint coordination and constructive synthesis. The authors develop HCRS (Hazard-aware Chain-based Rule Score), a deterministic scoring function that penalizes early reasoning errors more heavily based on empirical hazard analysis, and a Process Reward Model (PRM) that enables scalable process evaluation without requiring reasoning skeletons at inference. Empirical results demonstrate that answer-only metrics substantially overestimate reasoning robustness, with 6.63% of correct-answer traces receiving low process scores (SHCRS≤3), revealing "lucky guesses."

## Method Summary
The authors propose a two-branch evaluation framework combining strict reference-guided verification (HCRS) with scalable PRM-based scoring. HCRS evaluates model outputs against minimal reasoning skeletons using format and hazard-based penalties, where early errors receive heavier penalties based on empirical hazard rate analysis. The PRM is trained to approximate reference-guided verification using only problem statements and gold answers, eliminating skeleton dependency at inference. The benchmark includes 150 carefully curated problems annotated with reasoning skeletons, enabling fine-grained process-level evaluation.

## Key Results
- Answer accuracy for top models reaches 5.8/10, but HCRS-based holistic scores average only 4.36/10 (best 5.14/10)
- 6.63% of instances with correct final answers receive low process scores (SHCRS≤3), indicating "lucky guesses"
- PRM achieves competitive correlation with teacher judge (R=0.602) and shows score compression compared to HCRS
- Hazard penalty of ω=5.0 applied to PRM outputs improves alignment from R=0.604 to R=0.633

## Why This Works (Mechanism)

### Mechanism 1: Hazard-Aware Early-Error Penalization
- Claim: Earlier errors in reasoning chains should receive heavier penalties because they propagate downstream and corrupt subsequent steps
- Mechanism: HCRS applies a hazard schedule where the first-error position t* determines penalty magnitude via Phaz(t*) = min(Chaz, ω·(1-H(t*-1)/Hmax). The empirical hazard rate analysis shows first errors peak at steps 3-5, justifying asymmetric weighting
- Core assumption: Logical fractures early in reasoning dominate downstream failures; correcting late-stage errors in isolation is insufficient for valid reasoning
- Evidence anchors: [abstract]: "HCRS-based holistic evaluation yields substantially lower scores (average 4.36/10, best 5.14/10)" vs. answer accuracy up to 5.8/10; [Section 5.2.1]: Penalties correspond to "approximately 2.7-3.2 points, amounting to roughly 30% of the 10-point scale"

### Mechanism 2: Skeleton-Grounded Process Verification
- Claim: Minimal reasoning skeletons (2-10 steps, median 5) provide sufficient structural reference for step-level evaluation without constraining surface realizations
- Mechanism: Human-designed skeletons specify necessary intermediate assertions. A judge model (Gemini-3-Pro) verifies each model step against skeleton assertions with paraphrase tolerance—checking structural commitment rather than exact matching
- Core assumption: There exist minimal necessary assertions that any valid solution must include; missing these indicates structural failure regardless of answer correctness
- Evidence anchors: [abstract]: "Each problem is annotated with a minimal reasoning skeleton to support fine-grained process-level evaluation"; [Section 5.2.1]: "6.63% (66/996) of these instances fall into the low-score range (SHCRS≤3)" despite correct final answers

### Mechanism 3: PRM Distillation for Scalable Verification
- Claim: A Process Reward Model can approximate reference-guided verification using only problem statements and gold final answers, eliminating skeleton dependency at inference
- Mechanism: Train Qwen3-8B-instruct on ~33k step-level labels from Gemini-3-Pro teacher judge. At inference, PRM generates validity labels (ŷi, Ri) per step conditioned only on (x, gold answer). Aggregation via SPRM = (10/N)Σŷi yields process score
- Core assumption: The teacher judge's outcome-conditioned verification criteria can be distilled into a smaller model while preserving alignment with human judgments
- Evidence anchors: [abstract]: "train a Process Reward Model (PRM) on the annotated reasoning traces"; [Section 5.2.2]: "PRM achieves a competitive average correlation of R=0.602" vs. teacher judge R=0.639 vs. human

## Foundational Learning

- Concept: **Hazard rate / survival analysis**
  - Why needed here: Understanding why early errors are penalized more heavily requires grasping how failure probability accumulates across reasoning steps
  - Quick check question: If step 5 has the highest error rate but step 2 errors corrupt 80% of downstream steps, which should receive higher penalty?

- Concept: **Paraphrase-tolerant semantic alignment**
  - Why needed here: The skeleton verification allows different phrasings of equivalent logical commitments—understanding this distinction prevents over-penalization of valid alternative formulations
  - Quick check question: A skeleton says "establish divisibility by 3"; model says "verify the number is a multiple of 3"—should this match?

- Concept: **Teacher-student distillation with outcome conditioning**
  - Why needed here: PRM training uses teacher labels generated without gold reasoning paths; understanding what signal is preserved vs. lost explains PRM's compressed score distribution (~77% retention vs. ~52% in HCRS)
  - Quick check question: If the teacher only sees the final answer, how can it distinguish a correct step from a lucky step in a flawed chain?

## Architecture Onboarding

- Component map:
Input: Problem → Model → Structured Output (Raw Thought, <Reasoning>, <Answer>)
                              ↓
              ┌───────────────┴───────────────┐
              ↓                               ↓
     Branch A (HCRS)                   Branch B (PRM)
     Skeleton + Judge → Step Labels    PRM → Step Labels
              ↓                               ↓
     Base Score - Pfmt - Phaz          (10/N) Σ ŷi
              ↓                               ↓
           SHCRS                           SPRM
              └───────────────┬───────────────┘
                              ↓
                    Holistic Score (w=0.7)

- Critical path: Judge selection and calibration (Appendix B) → Skeleton annotation quality → HCRS penalty hyperparameters (α=4.0, β=1.0, ω=5.0, Table 4) → PRM training data construction (35k traces → 33k after filtering)

- Design tradeoffs:
  - HCRS provides strict structural stratification but requires expert skeleton annotation; PRM is scalable but shows score compression (top/bottom ratio ~1.3x vs. ~2.1x for HCRS)
  - Hazard penalty (ω=5.0, cap=5.0) heavily penalizes early errors but may be too aggressive for problems with natural exploration phases
  - Holistic weight (w=0.7 for process) balances process vs. outcome but may underweight genuine insight that bypasses skeleton

- Failure signatures:
  - "Lucky guess" pattern: SHCRS ≤3 with correct final answer (6.63% of correct-answer traces)
  - Format deviation: N significantly different from Lgold triggers asymmetric penalty (η=1.5 if shorter)
  - Judge misalignment: If selected judge diverges from human judgments on specific reasoning patterns, both HCRS and PRM inherit this bias

- First 3 experiments:
  1. **Baseline calibration**: Run HCRS on 10 problems with human skeleton annotation; verify judge-human alignment (target R>0.6) before scaling
  2. **Penalty ablation**: Compare SHCRS with Pfmt only, Phaz only, and both to quantify contribution of each penalty component to human alignment
  3. **PRM generalization test**: Train PRM on algebra/number theory problems; evaluate on geometry/combinatorics held-out set to assess domain transfer—expect degradation if PRM learns domain-specific patterns rather than general verification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the minimal reasoning skeleton annotation approach be automated or semi-automated to reduce expert annotation costs while maintaining evaluation quality?
- Basis in paper: [explicit] Section 7 states "constructing reasoning skeletons and gold step counts may require expert effort, which can increase annotation cost compared to outcome-only evaluation" and suggests "future work" on lighter-weight verifiers
- Why unresolved: The paper relies entirely on human-authored skeletons; no exploration of automated skeleton generation or weak supervision alternatives
- What evidence would resolve it: A study comparing human-authored vs. LLM-generated skeletons on annotation time, inter-annotator agreement, and downstream correlation with human judgments

### Open Question 2
- Question: Does the HCRS evaluation framework generalize to non-mathematical reasoning domains (e.g., legal, scientific, or commonsense reasoning)?
- Basis in paper: [inferred] The benchmark focuses exclusively on mathematical reasoning across five domains, and the hazard penalty design is motivated by mathematical error propagation patterns. The generalization beyond math is not tested
- Why unresolved: Structural reasoning failures may manifest differently in domains where "steps" are less clearly delineated or where multiple valid solution paths diverge more substantially
- What evidence would resolve it: Cross-domain validation showing HCRS correlation with human judgments on non-math benchmarks (e.g., logical reasoning, code debugging)

### Open Question 3
- Question: How robust is the hazard penalty schedule (early errors weighted more heavily) across different reasoning chain lengths and problem types?
- Basis in paper: [inferred] The hazard schedule is derived empirically from the 14-model, 2,100-trace dataset (Appendix F), with hyperparameters set via grid search on a validation set. Generalization to other problem distributions is untested
- Why unresolved: The peak at steps 3–5 may reflect the 4.65-step median skeleton length in REASONINGMATH-PLUS rather than a universal principle of error propagation
- What evidence would resolve it: Ablation studies on benchmarks with systematically longer reasoning chains, comparing learned vs. fixed hazard schedules

### Open Question 4
- Question: Can PRM-based verification be improved to match or exceed reference-guided HCRS alignment with human judgments, eliminating the need for gold skeletons at inference?
- Basis in paper: [explicit] Figure 7 shows PRM achieves κ=0.568 vs. reference-guided κ=0.436 on agreement metrics, but Figure 2c shows PRM Pearson correlation (0.602) trails the teacher judge (0.639). The paper notes PRM provides "smoother" evaluation with a "compression effect"
- Why unresolved: The PRM is trained on ~33k instances from a fixed teacher; the capacity gap between 8B PRM and Pro-tier judge, and the distillation methodology's optimality, remain unexplored
- What evidence would resolve it: Scaling laws for PRM size, alternative distillation objectives (e.g., ranking losses), or reinforcement learning from skeleton-grounded feedback

## Limitations

- The hazard-based penalty mechanism relies heavily on the empirical observation that early errors dominate reasoning failures, but the analysis is based on a limited sample of 150 problems
- The asymmetric weighting assumes structural completeness is more valuable than exploratory reasoning, which may not hold for all mathematical domains
- The PRM's generalization capability across mathematical subdomains remains unproven, as training and evaluation were conducted on overlapping problem distributions

## Confidence

- **High confidence**: The core finding that answer-only metrics overestimate reasoning robustness (SHCRS substantially lower than answer accuracy) is well-supported by empirical data across multiple models
- **Medium confidence**: The hazard penalty's effectiveness in capturing logical fractures requires validation on larger, more diverse problem sets to confirm the front-loaded error distribution pattern
- **Medium confidence**: The skeleton annotation process's inter-rater reliability and its sufficiency in capturing all valid reasoning paths needs systematic evaluation

## Next Checks

1. Conduct cross-validation across mathematical subdomains (algebra, geometry, combinatorics) to test whether hazard penalties maintain their diagnostic advantage when early exploration phases are legitimate
2. Perform ablation study on skeleton granularity—test whether skeletons with 3-4 steps versus 6-8 steps produce equivalent discrimination power in HCRS scoring
3. Evaluate PRM on held-out reasoning patterns from different mathematical cultures (e.g., Chinese vs. Western problem-solving traditions) to assess cultural bias and transferability