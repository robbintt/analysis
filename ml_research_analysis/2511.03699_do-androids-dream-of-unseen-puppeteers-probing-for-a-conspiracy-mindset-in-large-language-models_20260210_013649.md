---
ver: rpa2
title: Do Androids Dream of Unseen Puppeteers? Probing for a Conspiracy Mindset in
  Large Language Models
arxiv_id: '2511.03699'
source_url: https://arxiv.org/abs/2511.03699
tags:
- conspiracy
- llms
- cited
- conspiratorial
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models were tested with validated conspiracy belief
  surveys to determine if they display conspiratorial tendencies. Models showed moderate
  agreement with general conspiracy beliefs but disagreed with specific theories like
  UFOs.
---

# Do Androids Dream of Unseen Puppeteers? Probing for a Conspiracy Mindset in Large Language Models

## Quick Facts
- arXiv ID: 2511.03699
- Source URL: https://arxiv.org/abs/2511.03699
- Reference count: 27
- Large language models display demographic-specific conspiratorial tendencies when prompted with persona descriptions

## Executive Summary
This study examines whether large language models (LLMs) exhibit conspiratorial thinking by administering validated conspiracy belief surveys. Models show moderate agreement with general conspiracy beliefs but disagree with specific theories like UFOs. Crucially, conditioning models with demographic personas produces systematic biases: non-white, older, lower-income, and Republican personas are more associated with conspiracy mindset. The research demonstrates that simple prompts can easily shift models toward conspiratorial responses and reveals demographic-specific language patterns in model-generated justifications. These findings show that LLMs can simulate human-like conspiratorial thinking and demographic biases, highlighting both their utility for social science research and risks for manipulation.

## Method Summary
The study administered 126 items from four validated conspiracy belief scales (GCBS, CMQ, etc.) to multiple open-weight LLMs including Gemma3, Qwen3, and Mistral families. Items were clustered into five thematic belief categories using SBERT embeddings and k-means. Models were tested under three conditions: baseline (no conditioning), persona-based (demographic attributes injected via system prompt), and belief-conditioned (conspiracy statements provided as conditioning). Responses were collected as Likert scores with argumentation and analyzed for patterns across clusters and demographic groups.

## Key Results
- Models show moderate agreement with abstract conspiracy beliefs (no coincidences, hidden truth) but low agreement with specific theories (UFOs)
- Persona conditioning produces systematic bias, with non-white, older, lower-income, and Republican personas more associated with conspiracy mindset
- Simple prompts can substantially increase agreement with conspiracy-related items while leaving control items relatively unaffected

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditioning LLMs with socio-demographic personas produces systematic bias in conspiracy belief agreement, with non-white, older, lower-income, and Republican personas more associated with conspiratorial responses.
- Mechanism: The model's latent space encodes statistical associations between demographic attributes and psychological constructs from training data; when personas are injected via system prompts, these associations are retrieved and expressed in survey responses.
- Core assumption: Training corpora contain meaningful co-occurrence patterns between demographic markers and conspiratorial language that survive RLHF fine-tuning.
- Evidence anchors:
  - [abstract] "non-white, older, lower-income, and Republican personas were more associated with conspiracy mindset"
  - [section 4.2] "This combination of attributes is in line with what psychological literature has found in field studies"
  - [corpus] Neighbor paper "ConspirED" examines cognitive traits in conspiracy content but does not directly validate demographic bias transfer.
- Break condition: If models are trained on corpora lacking demographic-conspiracy co-occurrences, or if guardrails explicitly suppress demographic stereotyping, this effect should attenuate or disappear.

### Mechanism 2
- Claim: Few-shot conspiracy belief conditioning causes large increases in agreement with conspiracy
- Mechanism: Providing conspiracy-related prompts primes the model's attention toward conspiratorial interpretations and activates relevant semantic associations in its representation space, amplifying responses along these dimensions.
- Core assumption: Models maintain coherent semantic representations that can be systematically influenced by priming through carefully constructed prompts.
- Evidence anchors:
  - [section 4.1] "Simple prompts can substantially increase agreement with conspiracy-related items"
  - [corpus] Limited evidence that few-shot learning can manipulate belief expressions in LLMs, though not specifically tested for conspiracy beliefs.
- Break condition: If models employ strong semantic disentanglement or have been explicitly trained to resist belief manipulation through prompt engineering.

## Foundational Learning
The study relies on established conspiracy belief scales (GCBS, CMQ) validated in psychological research, though their validity in AI systems remains untested. The clustering methodology using SBERT embeddings assumes semantic coherence in conspiracy-related language that may not transfer to model-generated content. Demographic correlations assume stable relationships between identity markers and psychological constructs across human and AI contexts.

## Architecture Onboarding
The paper tests multiple open-weight models including Gemma3, Qwen3, and Mistral families. All models use transformer architectures with varying parameter counts and training regimes. The persona injection method relies on system prompt conditioning, which assumes models interpret demographic attributes as meaningful contextual signals rather than arbitrary tokens.

## Open Questions the Paper Calls Out
The authors acknowledge that the psychological validity of applying human conspiracy scales to AI systems is unknown. They also note uncertainty about whether observed demographic biases reflect genuine correlations or artifacts of training data. The study raises questions about the extent to which model responses reflect learned associations versus emergent reasoning about conspiracy concepts.

## Limitations
The study uses only open-weight models, limiting generalizability to closed systems. The persona descriptions are simplified and may not capture the full complexity of demographic identities. The use of validated human scales on AI systems assumes construct validity that has not been established. The findings may reflect statistical artifacts rather than genuine psychological phenomena.

## Confidence
Moderate confidence in core findings of demographic bias effects, as results align with psychological literature on conspiracy beliefs. Lower confidence in mechanism explanations due to untested assumptions about model internals. The transferability of human psychological constructs to AI systems remains an open question requiring further validation.

## Next Checks
- Validate whether human conspiracy scales meaningfully measure anything in LLMs
- Test whether demographic bias effects persist across different prompt formulations and model families
- Examine whether guardrails or RLHF can attenuate demographic stereotyping effects
- Investigate whether similar patterns emerge in closed-weight models or specialized conspiracy-focused models