---
ver: rpa2
title: Personalizing Education through an Adaptive LMS with Integrated LLMs
arxiv_id: '2502.08655'
source_url: https://arxiv.org/abs/2502.08655
tags:
- were
- llms
- test
- system
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores integrating large language models (LLMs) into
  learning management systems (LMSs) to create adaptive, personalized educational
  environments. Traditional LMSs distribute static content but fail to meet diverse
  student needs, especially where instructor availability is limited.
---

# Personalizing Education through an Adaptive LMS with Integrated LLMs

## Quick Facts
- arXiv ID: 2502.08655
- Source URL: https://arxiv.org/abs/2502.08655
- Reference count: 40
- Primary result: Self-hosted LLMs can match proprietary models in educational tasks when integrated with RAG and domain-specific knowledge, though mathematics remains challenging.

## Executive Summary
This paper presents an Adaptive Learning Management System (ALMS) that integrates large language models with traditional LMS functionality to deliver personalized education. The system combines a knowledge base of curated educational content with multiple LLMs, using retrieval-augmented generation to ground responses and reduce hallucinations. Through three development phases—expert system building, LLM integration, and benchmarking—the research demonstrates that self-hosted models can achieve comparable performance to proprietary models in reading, writing, and coding tasks, while identifying mathematics as a persistent challenge for LLMs regardless of model choice.

## Method Summary
The research developed ALMS through three phases: (1) building an expert system with Django/React frontend, OCR processing, and semantic search using FAISS vectorstore; (2) integrating LLMs via LangChain with RAG pipeline using OpenAI Ada-002 embeddings and supporting both self-hosted models via Ollama and proprietary APIs; (3) benchmarking 10 LLMs (including GPT-4, Llama2-7B, Mistral-7B, Phi-2) across five domains (math, reading, writing, reasoning, coding) using standardized test questions from EQAO, ACT, OSSLT, and LSAT. The system routes queries to optimal models per domain based on benchmark results, achieving comparable performance to single large models while reducing costs and privacy concerns.

## Key Results
- Self-hosted 7B parameter models (Mistral-7B, Phi-2) matched or exceeded proprietary models in reading, writing, and coding tasks
- RAG integration successfully reduced hallucinations, with 35 out-of-scope queries consistently returning "no valid option" responses
- Mathematics remained the weakest domain across all models, with accuracy inversely correlated to question difficulty
- Multi-model delegation achieved performance comparable to single large models while reducing computational costs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RAG grounds LLM responses in curated educational content, reducing hallucinations and enabling source attribution.
- Mechanism: Instead of relying solely on parametric knowledge (which may be outdated or incorrect), the system retrieves relevant chunks from a vectorstore containing verified course materials, then injects this context into the LLM prompt. The system prompt explicitly instructs the model to "Answer the following question based only on the provided context."
- Core assumption: The source documents (textbooks, test banks) contain accurate, relevant information, and the embedding model correctly retrieves semantically similar passages.
- Evidence anchors:
  - [abstract]: "By integrating a suite of general-purpose and domain-specific LLMs, this system aims to minimize common issues such as factual inaccuracies and outdated information."
  - [section III-B]: "User queries related to the parsed input source were answered with surprising accuracy. To test for hallucinations, queries unrelated to the source were used. In all cases, the system responded that the requested information was not present in the source articles."
  - [corpus]: Related paper "Bridging LMS and generative AI" (arXiv:2504.03966) similarly addresses hallucination reduction through dynamic content integration.
- Break condition: RAG fails when (1) the knowledge base lacks coverage of the queried topic, (2) the embedding model retrieves irrelevant chunks, or (3) the LLM ignores context instructions and generates from training data anyway.

### Mechanism 2
- Claim: Multi-model delegation—assigning different LLMs to different task types based on benchmarked strengths—can achieve performance comparable to single large proprietary models at lower cost and with privacy benefits.
- Mechanism: The system benchmarks multiple LLMs across five domains (mathematics, reading, writing, reasoning, coding) and identifies optimal models per domain. For example, Mistral-7B performed best in mathematics among tested models, while smaller models like Phi-2 excelled in writing tasks. A routing layer directs queries to the appropriate model.
- Core assumption: Task type can be reliably classified, and the benchmarked performance generalizes to real educational queries.
- Evidence anchors:
  - [section VI-A6]: "An ALMS might optimize its performance by delegating different problem types to the most suitable LLM, for example Phi for reading and writing tasks, and Mistral for mathematics."
  - [section VI-A1]: "Mistral 7B was the top performer in this [mathematics] category... AI tutoring systems seeking to optimize performance when dealing with mathematics problems will garner no significant advantage from using proprietary models over self-hosted LLMs."
  - [corpus]: Weak direct evidence in corpus; no neighboring papers explicitly test multi-model delegation strategies in educational contexts.
- Break condition: Delegation fails when (1) task classification is ambiguous, (2) no model in the ensemble handles the task well (e.g., all models struggle with mathematics), or (3) the overhead of maintaining multiple models exceeds the cost of a single proprietary API.

### Mechanism 3
- Claim: Converting static test banks into searchable vectorstores preserves expert-curated knowledge while enabling semantic (conceptual) search beyond keyword matching.
- Mechanism: PDF test banks are processed via OCR, parsed into structured question-answer pairs, then chunked and embedded into a vectorstore (using FAISS). When a student captures a question (via OCR screenshot), the system retrieves semantically similar questions from the test bank rather than exact string matches.
- Core assumption: Semantic similarity correlates with pedagogical relevance—students struggling with a concept benefit from seeing conceptually related questions, not just textually similar ones.
- Evidence anchors:
  - [section IV-A]: "The qSolver feature compares the search text to each question stored in the test bank database. Matches are based on the similarity of word composition."
  - [section IV-A]: "This expert system was developed using a combination of Python, JavaScript/TypeScript, JSX/HTML, CSS, Shell, and SQL... combining multiple functions involving OCR, text-based search, and HTML scraping."
  - [corpus]: "An Open-Source Dual-Loss Embedding Model for Semantic Retrieval in Higher Education" (arXiv:2505.04916) provides complementary evidence that fine-tuned embedding models improve retrieval in educational contexts.
- Break condition: Semantic search fails when (1) OCR introduces errors that distort embeddings, (2) chunk boundaries split conceptually related content, or (3) embedding models trained on general corpora poorly capture domain-specific educational semantics.

## Foundational Learning

- Concept: **Vector embeddings and semantic search**
  - Why needed here: The system converts text into dense vector representations to enable similarity-based retrieval. Without understanding embeddings, you cannot debug why certain documents are retrieved or optimize chunk sizes.
  - Quick check question: Given two sentences—"Photosynthesis requires sunlight" and "Plants need light to make food"—would a good embedding model place them close together in vector space? Why or why not?

- Concept: **Retrieval-Augmented Generation (RAG) pipeline**
  - Why needed here: RAG is the core architecture for reducing hallucinations. You need to understand the flow: document → chunking → embedding → storage → query → retrieval → context injection → LLM generation.
  - Quick check question: If a RAG system returns irrelevant chunks, should you first adjust (a) the embedding model, (b) the chunk size, or (c) the retrieval top-k parameter? What information would you gather to decide?

- Concept: **LLM benchmarking methodology**
  - Why needed here: The paper evaluates 10 LLMs across 5 domains using standardized tests. Understanding benchmark design is essential for interpreting claims about "comparable performance" and for extending evaluations to new models or domains.
  - Quick check question: Why might an LLM score well on multiple-choice reading comprehension but poorly on open-ended essay writing? What does this suggest about the skills being measured?

## Architecture Onboarding

- Component map:
  - Frontend (React): InputPanel captures queries/OCR screenshots; ContentPanel displays retrieved questions; state managed via React hooks and props
  - Backend (Django): PostgreSQL database stores test bank data; API endpoints handle POST/GET requests for query processing
  - LLM Layer (LangChain + Ollama): Supports both proprietary APIs (OpenAI) and self-hosted models (Mistral, Llama2, etc.); handles RAG pipeline with FAISS vectorstore
  - OCR Pipeline: Processes uploaded PDFs and screenshots via Nougat or Tesseract; outputs structured text for embedding

- Critical path:
  1. Document ingestion: PDF → OCR → parsing → structured Q&A pairs → embedding → FAISS vectorstore
  2. Query flow: User input (text or OCR screenshot) → embedding → vector search → top-k retrieval → context + query → LLM → response
  3. Model selection: Query classified by domain → routed to appropriate LLM (per benchmark results)

- Design tradeoffs:
  - **Self-hosted vs. proprietary APIs**: Self-hosted models (7B parameters) require 8-16GB RAM but eliminate per-query costs and privacy concerns; proprietary models offer higher peak performance (especially GPT-4 on complex writing) at ongoing cost
  - **Expert system vs. LLM-only**: Expert systems provide deterministic, source-attributable answers but require manual curation; LLMs offer flexibility and NLP capabilities but risk hallucination
  - **Chunk size for RAG**: Larger chunks preserve context but increase retrieval noise; smaller chunks improve precision but may lose semantic coherence (paper uses overlapping chunks without specifying exact size)

- Failure signatures:
  - **Hallucination despite RAG**: LLM generates information not in retrieved context → check if system prompt is being followed; consider stricter prompt constraints or increase retrieval top-k
  - **Poor mathematics performance**: All models struggled with math questions (inverse correlation with difficulty, possibly due to multiple-choice structure) → consider specialized math models (e.g., WizardMath) or symbolic computation tools instead of LLMs for this domain
  - **Inconsistent responses across runs**: High temperature settings cause variability → lower temperature for educational consistency; set seeds for reproducibility

- First 3 experiments:
  1. **RAG retrieval quality baseline**: Upload a sample textbook chapter, embed it, and query with 20 questions of varying relevance. Measure precision@k (are retrieved chunks actually useful?) and latency per query. Compare OpenAI ada-002 embeddings vs. open-source alternatives (e.g., mxbai-embed-large-v1).
  2. **Model routing validation**: Using the paper's benchmark results, implement a simple router that sends math queries to Mistral-7B and writing queries to Phi-2. Test on 50 mixed queries and compare accuracy/latency against a single-model baseline (GPT-3.5 or GPT-4).
  3. **Hallucination stress test**: Query the system with 20 questions specifically outside the knowledge base scope. Measure false positive rate (confident but wrong answers) with and without the RAG constraint prompt. Verify that the "no valid option" response pattern from the paper's 35 confounding cases is reproduced or improved.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do distinct LLMs generate identical or extremely similar phrasing in response to open-ended writing prompts, and does this indicate fine-tuning on other models' outputs?
- Basis in paper: [explicit] In Section VI-A3, the authors note that 7 out of 10 models tested used identical phrases (e.g., "Busy bees buzz by") in a creative writing task. Section VII explicitly calls for research into whether this is caused by models being fine-tuned on the output of others.
- Why unresolved: The study identified the statistical anomaly and uniformity of responses but lacked access to the proprietary or open-source training datasets to confirm the source of the convergence.
- What evidence would resolve it: A lineage analysis of training corpora for open-source models or a controlled study comparing outputs of models known to be trained on synthetic data versus human text only.

### Open Question 2
- Question: Why do LLMs fail on specific question wordings despite successfully solving semantically similar problems of equal or greater difficulty?
- Basis in paper: [explicit] The Discussion (VI) and Conclusion (VII) highlight that models exhibited high uniformity, often failing the same specific questions en masse while solving harder ones. The authors explicitly suggest investigating why models "consistently struggled with the wording of specific questions" to find fundamental issues with NLP technology.
- Why unresolved: The benchmarking results showed inconsistency correlated with question style rather than just difficulty level, but the specific linguistic features triggering these failures were not isolated.
- What evidence would resolve it: An ablation study systematically varying the syntactic structure and vocabulary of the problematic questions to identify the specific prompt features that cause performance degradation.

### Open Question 3
- Question: Does the integration of an expert system knowledge base with LLMs (a hybrid approach) significantly improve mathematical proficiency compared to standalone LLMs?
- Basis in paper: [inferred] The paper concludes that while the system is strong in reading and writing, it struggles with mathematics. The authors propose a hybrid approach (Section VII) using a human-curated knowledge base to compensate for LLM weaknesses, but the experimental phase only benchmarked standalone LLMs, not the hybrid system's specific efficacy in math.
- Why unresolved: The "hybrid" architecture is proposed as a theoretical solution to the math deficits observed in the benchmarking phase, but its performance was not quantified in the results.
- What evidence would resolve it: A comparative evaluation benchmarking the hybrid RAG/Expert System architecture against standalone models (like GPT-4 or Mistral) specifically on the mathematics test sets used in Phase III.

## Limitations
- Mathematics reasoning capabilities of all tested LLMs remain poor regardless of model size or architecture
- Benchmark content and specific question wording not provided, preventing exact replication
- Coding evaluation lacks unit test cases, limiting objective correctness assessment

## Confidence
- **High confidence**: RAG-based hallucination reduction within defined knowledge boundaries; comparative performance of self-hosted vs. proprietary models for reading/writing/coding; resource requirements for 7B parameter models
- **Medium confidence**: Multi-model delegation strategy for domain-specific optimization; generalizability of benchmark results to broader educational contexts; effectiveness of semantic search for pedagogical question matching
- **Low confidence**: Mathematics reasoning capabilities of any tested LLM; performance in novel or out-of-distribution question types; long-term stability of self-hosted model routing under real-world usage patterns

## Next Checks
1. **RAG retrieval ablation study**: Systematically vary chunk size (256, 512, 1024 tokens) and retrieval top-k (3, 5, 10) while measuring precision@k on a held-out question set. This quantifies the impact of preprocessing choices on hallucination reduction.

2. **Mathematics domain transfer test**: Apply the same benchmark methodology to specialized math models (WizardMath-7B, MetaMath-7B) and symbolic computation tools (SymPy, Wolfram Alpha API). Compare performance against general-purpose LLMs to isolate whether poor math results reflect LLM limitations or benchmark design.

3. **Real-world routing stress test**: Deploy the multi-model system with 1,000 mixed educational queries over one week, logging classification accuracy, model selection latency, and user satisfaction scores. Measure whether routing overhead and model switching costs outweigh performance benefits in production.