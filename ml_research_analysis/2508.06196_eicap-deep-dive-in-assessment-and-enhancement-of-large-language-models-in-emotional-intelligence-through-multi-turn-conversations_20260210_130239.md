---
ver: rpa2
title: 'EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in
  Emotional Intelligence through Multi-Turn Conversations'
arxiv_id: '2508.06196'
source_url: https://arxiv.org/abs/2508.06196
tags:
- emotional
- llms
- emotion
- arabic
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of assessing and improving emotional
  intelligence (EI) in large language models (LLMs) through multi-turn conversations.
  The authors introduce EICAP, a psychologically grounded four-layer taxonomy of EI,
  and EICAP-Bench, a novel benchmark dataset designed to evaluate EI capabilities
  across diverse linguistic and cultural contexts.
---

# EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations

## Quick Facts
- arXiv ID: 2508.06196
- Source URL: https://arxiv.org/abs/2508.06186
- Reference count: 25
- Primary result: Qwen2.5-Instruct performs best overall on EICAP-Bench; only Arabic-to-Arabic LoRA fine-tuning significantly improves Appraisal layer (+2.5 pp)

## Executive Summary
This paper introduces EICAP, a four-layer taxonomy of emotional intelligence grounded in psychological theory, and EICAP-Bench, a novel benchmark dataset designed to evaluate LLM emotional intelligence through multi-turn conversations. The authors systematically benchmark six open-source LLMs and find that Qwen2.5-Instruct performs best overall. To investigate EI enhancement, they fine-tune Qwen2.5-Base and Qwen2.5-Instruct using LoRA adapters on the UltraChat dialogue dataset in both English and Arabic. Statistical analysis reveals that only the Appraisal layer shows significant improvement through UC-based fine-tuning, specifically in Arabic-to-Arabic settings (+2.5 pp). The findings highlight the limitations of current pretraining and instruction-tuning paradigms in equipping LLMs with deeper emotional reasoning, emphasizing the need for targeted data and modeling strategies for comprehensive EI alignment.

## Method Summary
The paper introduces EICAP, a psychologically grounded four-layer taxonomy of emotional intelligence, and EICAP-Bench, a benchmark dataset with 3,174 MCQ evaluation rows across English and Arabic. The authors evaluate six open-source LLMs on this benchmark, then investigate enhancement through LoRA fine-tuning (rank=8, ~0.8% parameters) on the UltraChat dialogue dataset. Fine-tuning is performed on Qwen2.5-Base and Qwen2.5-Instruct variants in both English and Arabic. Evaluation uses LM Evaluation Harness with deterministic decoding (temperature=0.0), and statistical significance is assessed via 10,000-sample bootstrap confidence intervals with Benjamini-Hochberg FDR correction.

## Key Results
- Qwen2.5-Instruct achieves the highest overall performance on EICAP-Bench among evaluated models
- Only Arabic-to-Arabic LoRA fine-tuning shows significant improvement (+2.5 pp) in the Appraisal layer
- All other LoRA configurations show no improvement or degradation, particularly in the Foundation layer (-0.81 to -2.9 pp)
- Cross-lingual fine-tuning generally results in negative transfer, with AR→EN showing the largest degradation (-2.9 pp)

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical EI Taxonomy Enables Targeted Failure Diagnosis
- Claim: The four-layer taxonomy maps psychological theory to granular computational tasks, isolating which EI capabilities fail
- Mechanism: Each taxonomy node corresponds to specific evaluation probes, disentangling surface-level emotion recognition from causal reasoning and social judgment
- Core assumption: Psychological theories validly decompose EI into independently measurable skills for LLMs
- Evidence anchors: Performance table showing sub-category accuracy variance (e.g., Minimal Emotional Inference: 0.45 vs. Mixed Emotion Detection: 0.62)

### Mechanism 2: LoRA's Rank Constraint Yields Localized, Non-Transferable Improvements
- Claim: LoRA adapters can improve specific EI layers but fail to generalize due to limited representational capacity
- Mechanism: The low-rank update constrains adaptation to a subspace, learning discourse-level patterns useful for Appraisal but unable to rewire foundational emotion detection
- Core assumption: Observed gains reflect learning rather than calibration artifacts
- Evidence anchors: AR→AR Appraisal improvement (+2.5 pp) while all other configurations show no improvement or degradation

### Mechanism 3: Morphology-Rich Language + Discourse-Anchored Training → Appraisal Gains
- Claim: Arabic's morphological features interact with UltraChat's dialogue structure to support discourse-level affective reasoning
- Mechanism: Arabic UC provides implicit supervisory signal through constructions marking stance, causality, and epistemic certainty
- Core assumption: Improvement is due to morphological/discourse features rather than dataset artifacts
- Evidence anchors: Analysis linking gains to discourse-anchored affective reasoning including causal inference

## Foundational Learning

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: Used to adapt Qwen2.5 with only 0.8% parameter updates, critical for understanding selective improvements
  - Quick check question: If LoRA rank increased from 8 to 64, would you expect broader EI improvements or just overfitting?

- Concept: **Appraisal Theory (Lazarus)**
  - Why needed here: The Appraisal layer—where the only significant improvement occurred—is grounded in Lazarus's cognitive appraisal framework
  - Quick check question: How does "cause inference" differ from "basic emotion labeling" in terms of required reasoning depth?

- Concept: **Bootstrap Confidence Intervals + FDR Correction**
  - Why needed here: Claims rest on statistical significance (10,000-sample bootstrap, Benjamini-Hochberg FDR ≤ 0.05)
  - Quick check question: Why is FDR correction necessary when evaluating 5+ EI layers?

## Architecture Onboarding

- Component map: EICAP-Bench (53 seed scenarios → 1,058 dialogue seeds → 3,174 MCQ rows) -> Taxonomy (4 layers × 15+ sub-categories) -> Evaluation Harness (custom task, temp=0.0) -> Fine-tuning Pipeline (UC → LoRA adapters → 3 epochs → merged inference)

- Critical path:
  1. Run baseline evaluation on EICAP-Bench (EN) for all 6 model variants
  2. Identify weakest layers (Minimal Emotional Inference, Cultural Sensitivity, Valence)
  3. Fine-tune Qwen2.5-7B Base/Instruct on UC (EN/AR) via LoRA
  4. Re-evaluate with bootstrap CIs; apply FDR correction across layers
  5. Analyze: Only AR→AR Appraisal shows significant gain (+2.5pp, CI [1.11, 4.17])

- Design tradeoffs:
  - MCQ format enables scalable automated evaluation but may not capture open-ended emotional reasoning
  - LoRA limits capacity for broad EI acquisition but preserves base model capabilities and is computationally efficient
  - UltraChat provides discourse structure but lacks explicit EI labels; weak supervision for foundational affective tasks
  - Arabic translation introduces potential artifacts; paper acknowledges 43% token reduction in Arabic UC

- Failure signatures:
  - Foundation layer degradation: All LoRA configurations degraded Foundation accuracy (-0.81 to -2.9 pp)
  - Instruct fragility: Qwen2.5-Instruct degraded even with same-language LoRA
  - Cross-lingual negative transfer: AR-trained adapters evaluated on EN Foundation showed -2.9 pp degradation

- First 3 experiments:
  1. Run all 6 model variants on EICAP-Bench (EN) with deterministic decoding; report per-layer accuracy
  2. Train Qwen2.5-7B-Base on Arabic UC with LoRA ranks ∈ {4, 8, 16, 32}; evaluate on Arabic EICAP-Bench Appraisal layer
  3. Create small (5K-10K examples) EI-labeled Arabic dialogue dataset; fine-tune with LoRA (r=8) and compare against UC-trained adapters

## Open Questions the Paper Calls Out

- **Can explicitly labelled, emotionally dense corpora overcome limitations of general-purpose instruction tuning?**
  - Basis: UC's emotion sparsity restricts skill acquisition; general corpora offer weak supervision for foundational EI capabilities
  - Why unresolved: Study only tested general conversational data (UltraChat)
  - What evidence would resolve it: Comparative study with targeted dataset with high emotional density and detailed annotation

- **Does LoRA's low-rank constraint limit representational capacity required for deep emotional reasoning?**
  - Basis: Low-rank constraint may impose upper bound on representational change
  - Why unresolved: Study utilized LoRA for all experiments, making it impossible to isolate data vs. method limitations
  - What evidence would resolve it: Compare LoRA against full parameter fine-tuning on same EI-specific datasets

- **How does integration of multi-modal cues impact assessment of EI in the proposed EICAP framework?**
  - Basis: Dataset's focus on text-based interactions may overlook multi-modal cues that critically influence emotional interpretation
  - Why unresolved: EICAP-Bench is text-only, while human EI relies heavily on non-verbal signals
  - What evidence would resolve it: Extend benchmark to include audio-visual inputs and evaluate model performance

## Limitations

- EICAP-Bench is not publicly available, preventing independent verification of evaluation methodology
- Mechanistic explanation for Arabic-specific gains lacks direct corpus evidence and could reflect dataset artifacts
- Only one significant improvement across six model variants and four layers raises questions about practical significance
- Observed degradation in Foundation layer accuracy suggests LoRA may destabilize rather than enhance core emotion recognition

## Confidence

- **High Confidence**: Taxonomy design, baseline evaluation methodology, and statistical analysis framework
- **Medium Confidence**: Observation that Appraisal is only layer showing significant improvement in Arabic-to-Arabic settings
- **Low Confidence**: Mechanistic explanation for Arabic-specific gains, generality of LoRA's limitations, and practical significance of +2.5pp improvement

## Next Checks

1. Replicate AR→AR Appraisal gain using morphologically segmented Arabic test set to determine if improvement depends on surface morphology or higher-level discourse features

2. Systematically vary LoRA rank (r ∈ {4, 8, 16, 32}) and target modules to determine whether rank=8 is optimal for localized Appraisal gains or whether higher ranks enable broader EI improvements

3. Compare UC-trained adapters against LoRA fine-tuned on small, EI-labeled Arabic dialogue dataset covering Foundation and Dimensional layers to determine whether explicit supervision is necessary for foundational EI tasks