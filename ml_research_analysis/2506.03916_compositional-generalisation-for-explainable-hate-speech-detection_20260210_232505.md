---
ver: rpa2
title: Compositional Generalisation for Explainable Hate Speech Detection
arxiv_id: '2506.03916'
source_url: https://arxiv.org/abs/2506.03916
tags:
- icsf
- u-plead
- gemma
- llama
- hate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of compositional generalisation
  in hate speech detection, where models fail to generalise to unseen combinations
  of known expressions. The authors create a synthetic dataset (U-PLEAD) with balanced
  expression distributions to mitigate correlation biases, along with a new benchmark
  (TARGET) for evaluating generalisation.
---

# Compositional Generalisation for Explainable Hate Speech Detection

## Quick Facts
- arXiv ID: 2506.03916
- Source URL: https://arxiv.org/abs/2506.03916
- Reference count: 40
- Primary result: 50.34% Micro F1 on TARGET benchmark with proposed method vs 46.18% without

## Executive Summary
This paper addresses the critical problem of compositional generalisation in hate speech detection, where models fail to recognise new combinations of known expressions. The authors identify that existing datasets suffer from correlation biases where models learn spurious associations rather than genuine understanding of hate speech. To address this, they create U-PLEAD, a synthetic dataset with balanced expression distributions, and TARGET, a new benchmark for evaluating compositional generalisation. Their approach uses grammar-based generation with LLMs to create diverse training data that enables models to generalise better to unseen expression combinations while maintaining state-of-the-art performance on original hate speech detection tasks.

## Method Summary
The authors tackle compositional generalisation by creating a synthetic dataset (U-PLEAD) that addresses correlation biases in existing hate speech datasets. They employ a grammar-based generation procedure using LLMs to produce diverse expression combinations with balanced distributions. The method involves defining grammatical rules for hate speech expressions and using these rules to systematically generate training examples that cover the combinatorial space of possible expressions. This synthetic data is then used to train models that are subsequently evaluated on the TARGET benchmark, which specifically tests compositional generalisation by presenting unseen combinations of known expressions.

## Key Results
- 50.34% Micro F1 on TARGET benchmark with U-PLEAD training vs 46.18% without
- Maintains state-of-the-art performance on original PLEAD test set
- Demonstrates improved compositional generalisation through balanced expression distributions

## Why This Works (Mechanism)
The approach works by addressing the fundamental limitation of existing hate speech datasets: they contain spurious correlations where certain expressions only appear in specific contexts. By using grammar-based generation with LLMs, the method creates a more diverse training distribution that exposes models to all possible combinations of known expressions. This forces the model to learn genuine compositional understanding rather than memorizing context-specific patterns. The balanced distribution ensures that models cannot rely on simple heuristics and must develop a more robust understanding of hate speech semantics.

## Foundational Learning
- **Compositional generalisation**: The ability of models to understand and process novel combinations of known components. Needed to handle the vast combinatorial space of possible hate speech expressions.
- **Correlation bias in datasets**: When models learn spurious associations between expressions and contexts rather than genuine semantic understanding. Critical to identify as it limits model performance on unseen data.
- **Grammar-based generation**: A systematic approach to creating synthetic data using predefined rules. Essential for ensuring comprehensive coverage of expression combinations.
- **LLM-assisted generation**: Using large language models to enhance synthetic data quality and diversity. Important for creating more natural and varied training examples.

## Architecture Onboarding

Component map: Grammar Rules -> LLM Generator -> U-PLEAD Dataset -> Hate Speech Model -> TARGET Benchmark

Critical path: The generation of diverse expression combinations through grammar rules, processed by LLMs to create realistic training examples in U-PLEAD, which trains the hate speech detection model. Performance is evaluated on TARGET to measure compositional generalisation.

Design tradeoffs: Synthetic data provides controlled coverage of expression space but may lack real-world complexity. Grammar-based generation ensures systematic coverage but might miss nuanced hate speech patterns. LLM assistance improves naturalness but introduces generation variability.

Failure signatures: Models failing on TARGET but succeeding on PLEAD indicate reliance on correlation biases rather than genuine understanding. Poor performance on both suggests fundamental limitations in the generation approach or model architecture.

Three first experiments:
1. Train baseline model on original PLEAD dataset only, evaluate on TARGET to establish correlation bias baseline
2. Train model on U-PLEAD dataset, evaluate on both TARGET and PLEAD to measure trade-off between generalisation and performance
3. Ablation study varying grammar rule complexity to identify minimum requirements for compositional generalisation

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic nature of U-PLEAD raises questions about real-world transferability
- TARGET benchmark may not fully capture natural hate speech complexity
- Limited error analysis prevents understanding of remaining failure modes

## Confidence
- High: Clear methodology for U-PLEAD creation and reproducible results
- Medium: Generalisation improvements demonstrated but dependent on synthetic evaluation
- Low: Insufficient error analysis to assess practical robustness

## Next Checks
1. Evaluate trained models on naturally occurring hate speech datasets beyond synthetic benchmarks
2. Conduct ablation study removing components of synthetic data generation to identify critical elements
3. Perform extensive error analysis on misclassified TARGET samples to understand remaining failure modes