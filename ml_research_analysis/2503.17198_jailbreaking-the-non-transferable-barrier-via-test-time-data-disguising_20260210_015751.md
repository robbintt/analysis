---
ver: rpa2
title: Jailbreaking the Non-Transferable Barrier via Test-Time Data Disguising
arxiv_id: '2503.17198'
source_url: https://arxiv.org/abs/2503.17198
tags:
- domain
- unauthorized
- authorized
- data
- jailntl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JailNTL, the first black-box attack method
  against Non-Transferable Learning (NTL) models. NTL protects model IP by creating
  a "non-transferable barrier" that restricts model performance from authorized to
  unauthorized domains.
---

# Jailbreaking the Non-Transferable Barrier via Test-Time Data Disguising

## Quick Facts
- **arXiv ID:** 2503.17198
- **Source URL:** https://arxiv.org/abs/2503.17198
- **Reference count:** 40
- **Primary result:** First black-box attack method that jailbreaks Non-Transferable Learning models by disguising unauthorized data, achieving up to 55.7% accuracy increase on the unauthorized domain using only 1% authorized samples.

## Executive Summary
This paper introduces JailNTL, the first black-box attack method capable of bypassing the Non-Transferable Learning (NTL) barrier that protects model intellectual property by restricting performance to authorized domains. Unlike previous white-box attacks that require modifying model weights, JailNTL achieves this through test-time data disguising using a bidirectional CycleGAN-like network. The method employs two disguising levels: data-intrinsic disguising (DID) to eliminate domain discrepancies while preserving class content, and model-guided disguising (MGD) to minimize output-level statistics differences using zero-order gradient estimation. JailNTL successfully breaks the non-transferable barrier, achieving significant accuracy improvements on unauthorized domains without any model weight modification.

## Method Summary
JailNTL targets black-box NTL models by constructing a disguising network that transforms unauthorized domain data to appear as authorized domain data. The method uses a small subset (1%) of authorized data to train a bidirectional CycleGAN-like architecture with a disguising generator and inverse generator. The core innovation is the Model-Guided Disguising (MGD) component, which uses zero-order gradient estimation to compute losses based on the black-box model's output logits without requiring gradient access. The disguising network is optimized using a multi-component loss function that includes adversarial losses, cycle consistency losses, confidence losses based on entropy differences, and class balance losses to ensure the transformed data maintains both domain similarity to authorized data and class-related content.

## Key Results
- Achieves up to 55.7% accuracy increase on unauthorized domain using only 1% authorized samples
- Outperforms existing white-box attacks that require model weight modification
- Successfully breaks the NTL barrier without modifying model weights
- Maintains high accuracy on authorized domain while improving unauthorized domain performance
- Enhances white-box attacks when integrated with TransNTL, demonstrating method flexibility

## Why This Works (Mechanism)
JailNTL works by exploiting the test-time input rather than the model itself. The disguising network transforms unauthorized data to minimize domain discrepancy while preserving semantic content, effectively fooling the NTL model into treating unauthorized inputs as authorized. The MGD component uses zero-order gradient estimation to optimize the disguising network based on the black-box model's output logits, allowing the attacker to improve unauthorized domain performance without any model access beyond forward passes. This approach jailbreaks the NTL barrier by creating a loophole in the test-time data processing stage rather than attempting to modify the protected model weights.

## Foundational Learning
- **Non-Transferable Learning (NTL):** A defense mechanism that restricts model performance from authorized to unauthorized domains. **Why needed:** Understanding the target defense is crucial for designing effective attacks. **Quick check:** Verify NTL models show performance drop when tested on unauthorized domains.
- **Zero-order Gradient Estimation:** A technique for computing gradients without backpropagation, using finite difference approximations. **Why needed:** Essential for black-box attacks where gradient access is unavailable. **Quick check:** Implement finite difference with varying epsilon values to assess stability.
- **CycleGAN Architecture:** A bidirectional image-to-image translation framework using cycle consistency losses. **Why needed:** Provides the foundation for the disguising network structure. **Quick check:** Verify cycle consistency by ensuring input reconstruction error is low.
- **Entropy-based Confidence Loss:** Measures the difference in prediction confidence between authorized and disguised unauthorized data. **Why needed:** Helps align output distributions across domains. **Quick check:** Monitor entropy values to ensure they converge across domains.
- **Class Balance Loss:** Ensures transformed data maintains class distribution similarity. **Why needed:** Prevents the disguising process from distorting class semantics. **Quick check:** Verify class distribution entropy remains stable during training.

## Architecture Onboarding

**Component Map:** Unauthorized Data → Disguising Generator → Black-box NTL Model ← Authorized Data ← Inverse Generator

**Critical Path:** The disguising generator receives unauthorized data and transforms it to minimize domain discrepancy while preserving class content, with MGD providing feedback through zero-order gradient estimation of the black-box model's logits.

**Design Tradeoffs:** The method trades off between strong domain transfer (which could destroy content) and content preservation (which could limit domain transfer). The cycle consistency loss and MGD losses balance these competing objectives.

**Failure Signatures:** If the disguising network alters images too drastically, unauthorized accuracy will drop despite successful domain transfer. If MGD is too aggressive, the model may detect the attack through unusual output patterns.

**First Experiments:**
1. Verify the disguising network can perform basic image translation between domains with cycle consistency.
2. Test zero-order gradient estimation on a simple differentiable function to ensure numerical stability.
3. Evaluate unauthorized domain accuracy with progressively larger authorized data subsets to find the minimum effective amount.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can NTL models be explicitly hardened against test-time data disguising attacks without compromising authorized domain performance?
- **Basis in paper:** [Explicit] The authors conclude that their work "opens the discussion on how to make NTL secure in both black- and white-box scenarios."
- **Why unresolved:** This paper acts as an attack paper; it establishes the vulnerability (loophole) but does not propose or test defensive mechanisms.
- **What evidence would resolve it:** A new NTL training objective or inference-time detection method that successfully resists JailNTL while maintaining high accuracy on the authorized domain.

### Open Question 2
- **Question:** What is the specific performance degradation of JailNTL when the attacker has access only to hard prediction labels rather than full output logits?
- **Basis in paper:** [Inferred] The method relies on "logits" for Model-Guided Disguising (MGD). Appendix E notes that for label-only access, the confidence loss must be removed, implying a gap in performance analysis for this strict black-box setting.
- **Why unresolved:** While the authors state removing the loss still yields "good performance," they do not quantify the drop in attack success rates for the label-only scenario compared to the logit-based scenario.
- **What evidence would resolve it:** Ablation studies reporting unauthorized domain accuracy when the feedback signal is restricted to the top-1 predicted class label.

### Open Question 3
- **Question:** Can the disguising network be trained effectively using synthetic or generated authorized data rather than real authorized samples?
- **Basis in paper:** [Inferred] The method assumes the attacker has access to "a small part of authorized data" (e.g., 1%) to train the GAN-based disguising network.
- **Why unresolved:** The reliance on real authorized samples restricts the attack's applicability to scenarios where some authorized data has already leaked.
- **What evidence would resolve it:** Experiments using generated data (e.g., from a pretrained generator) as the "authorized" target domain for the disguising network to determine if the barrier can be broken without real private data.

## Limitations
- Performance degrades when restricted to label-only black-box access (requires removing confidence loss)
- Relies on having access to a small subset of authorized data for training the disguising network
- Effectiveness depends on the quality and quantity of the available authorized samples
- May be detectable by models with input/output anomaly detection mechanisms

## Confidence
- **55.7% accuracy increase claim:** High confidence based on well-defined loss formulation and architectural choices
- **Black-box attack effectiveness:** Medium-High confidence given strong theoretical foundation and clear implementation path
- **Flexibility claim (enhancing white-box attacks):** Medium confidence, depends on successful integration implementation

## Next Checks
1. Implement the zero-order gradient estimation with multiple epsilon values (e.g., 1e-3, 1e-4) and compare convergence behavior.
2. Conduct ablation studies varying the loss weights to identify the most critical components for attack success.
3. Test the method on a held-out NTL model not used in the original experiments to verify generalization.