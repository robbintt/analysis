---
ver: rpa2
title: 'MOMA: Masked Orthogonal Matrix Alignment for Zero-Additional-Parameter Model
  Merging'
arxiv_id: '2412.13526'
source_url: https://arxiv.org/abs/2412.13526
tags:
- merging
- orthogonal
- moma
- fine-tuned
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MOMA addresses the performance degradation in model merging for
  classification tasks by identifying that the misalignment between merged encoders
  and static task-specific classifier heads is predominantly an orthogonal transformation.
  Unlike existing methods that introduce auxiliary parameters for strict representation
  alignment, MOMA rectifies this misalignment by jointly optimizing a global multi-task
  vector mask and task-specific orthogonal transformations.
---

# MOMA: Masked Orthogonal Matrix Alignment for Zero-Additional-Parameter Model Merging

## Quick Facts
- **arXiv ID**: 2412.13526
- **Source URL**: https://arxiv.org/abs/2412.13526
- **Reference count**: 27
- **Primary result**: MOMA achieves state-of-the-art performance in model merging without adding parameters or inference cost by addressing orthogonal transformation misalignment between merged encoders and classifier heads

## Executive Summary
MOMA introduces a novel approach to model merging that eliminates the performance degradation typically observed when combining pre-trained models for classification tasks. The key insight is that misalignment between merged encoders and static task-specific classifier heads is predominantly an orthogonal transformation rather than requiring complex parameter adjustments. By jointly optimizing global multi-task vector masks and task-specific orthogonal transformations that are absorbed into existing model weights, MOMA achieves performance comparable to methods with additional parameters while maintaining zero additional parameters and zero added inference cost.

## Method Summary
MOMA addresses model merging challenges by recognizing that encoder-classifier misalignment is primarily an orthogonal transformation issue. The method jointly optimizes a global multi-task vector mask and task-specific orthogonal transformations, then absorbs these new parameters directly into existing model weights. This approach eliminates the need for auxiliary parameters during inference while achieving state-of-the-art performance. The technique is specifically designed for classification tasks using ViT-based architectures and operates by optimizing the alignment between merged encoder representations and task-specific classifier heads through orthogonal matrix transformations.

## Key Results
- Achieves state-of-the-art performance in model merging without additional parameters
- Consistently outperforms prior model merging approaches on benchmark datasets
- Bridges performance gap to fine-tuned models while maintaining zero added inference cost
- Demonstrates effectiveness specifically for ViT-based classification tasks

## Why This Works (Mechanism)
The core mechanism exploits the observation that when merging models, the primary source of performance degradation stems from rotational misalignment between encoder representations and classifier heads. Traditional approaches introduce auxiliary parameters to force strict representation alignment, but MOMA instead uses orthogonal transformations that preserve the geometric structure of the feature space while correcting the misalignment. By absorbing these transformations into existing weights, the method maintains model efficiency while achieving superior alignment accuracy.

## Foundational Learning
- **Orthogonal transformations**: Linear transformations that preserve distances and angles in vector spaces, crucial for maintaining feature space geometry during alignment
  - *Why needed*: To correct rotational misalignment without distorting the underlying feature representations
  - *Quick check*: Verify that orthogonal matrices have determinant Â±1 and preserve dot products

- **Vector masking**: Selective application of transformations or parameters to specific dimensions or components of model representations
  - *Why needed*: To enable task-specific adjustments while maintaining a shared global representation
  - *Quick check*: Confirm that masked vectors can be reconstructed from unmasked components

- **Weight space absorption**: The process of incorporating new parameters or transformations into existing model weights during training
  - *Why needed*: To achieve zero additional parameters during inference while maintaining alignment benefits
  - *Quick check*: Validate that absorbed weights can be recovered and that inference remains unchanged

## Architecture Onboarding
**Component Map**: Input Data -> Encoder Backbone -> Vector Mask Layer -> Orthogonal Transformation Layer -> Classifier Head -> Output

**Critical Path**: The critical path involves the interaction between the merged encoder representations and the classifier head, where orthogonal misalignment occurs. MOMA optimizes this path by applying task-specific orthogonal transformations after vector masking but before classification.

**Design Tradeoffs**: The method trades off potential fine-tuning flexibility for zero-parameter efficiency. While traditional approaches add parameters for alignment, MOMA's absorption strategy may make future fine-tuning more complex but eliminates inference overhead.

**Failure Signatures**: Performance degradation when merging models with fundamentally different architectures or when tasks require non-orthogonal representational changes. The method may struggle when misalignment involves scaling or translation rather than pure rotation.

**First Experiments**:
1. Verify orthogonal transformation preservation on simple synthetic data with known rotational misalignment
2. Test vector mask effectiveness by comparing performance with and without masking on merged encoders
3. Validate weight absorption by checking inference consistency before and after parameter absorption

## Open Questions the Paper Calls Out
None

## Limitations
- Relies heavily on the assumption that misalignment is predominantly orthogonal, which may not hold for diverse model combinations
- Limited evaluation scope to ViT-based classification tasks, leaving uncertainty about applicability to other architectures
- Zero-parameter claim requires careful interpretation as absorbed weights may affect future fine-tuning flexibility

## Confidence
- **High Confidence**: Core algorithmic innovation and empirical methodology are sound; zero-parameter claim is verifiable
- **Medium Confidence**: Assumption that orthogonal misalignment dominates across diverse task combinations
- **Medium Confidence**: Consistent bridging of gap to fine-tuned models across limited model families and task types

## Next Checks
1. **Cross-Architecture Validation**: Test MOMA on merging different model architectures (e.g., ResNet + ViT, or vision + language models) to verify whether the orthogonal misalignment assumption holds beyond homogeneous ViT ensembles.

2. **Longitudinal Stability Test**: Evaluate model performance after extended fine-tuning periods post-merging to assess whether the absorbed orthogonal transformations create interference patterns or weight space pathologies that emerge over time.

3. **Task Conflict Analysis**: Systematically introduce conflicting tasks (e.g., tasks requiring opposite feature representations) to quantify MOMA's robustness to genuine representational conflicts versus simple rotational misalignment.