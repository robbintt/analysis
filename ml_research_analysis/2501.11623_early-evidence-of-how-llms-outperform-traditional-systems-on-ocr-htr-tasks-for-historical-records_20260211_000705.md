---
ver: rpa2
title: Early evidence of how LLMs outperform traditional systems on OCR/HTR tasks
  for historical records
arxiv_id: '2501.11623'
source_url: https://arxiv.org/abs/2501.11623
tags:
- bleu
- llms
- experiments
- historical
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compares the performance of large language models (LLMs)
  and traditional OCR/HTR systems on transcribing historical handwritten documents
  in tabular format. Two approaches are tested: processing entire scans and line-by-line
  segmentation.'
---

# Early evidence of how LLMs outperform traditional systems on OCR/HTR tasks for historical records

## Quick Facts
- arXiv ID: 2501.11623
- Source URL: https://arxiv.org/abs/2501.11623
- Reference count: 20
- Large language models significantly outperform traditional OCR/HTR systems on transcribing historical handwritten documents

## Executive Summary
This study compares large language models (LLMs) against traditional optical character recognition/handwritten text recognition (OCR/HTR) systems for transcribing historical handwritten documents in tabular format. The researchers evaluated GPT-4o and Claude Sonnet 3.5 against EasyOCR, Keras, Pytesseract, and TrOCR using Character Error Rate (CER) and BLEU metrics. The results demonstrate that LLMs significantly outperform traditional systems across both whole-scan and line-by-line processing approaches, with Claude Sonnet 3.5 excelling on whole scans and GPT-4o on line-by-line tasks.

## Method Summary
The researchers tested two processing approaches: whole-scan transcription and line-by-line segmentation. They evaluated two state-of-the-art LLMs (GPT-4o and Claude Sonnet 3.5) against four traditional OCR/HTR systems (EasyOCR, Keras, Pytesseract, and TrOCR) on a custom dataset of 20 pages of Belgian historical documents from 1770-1790. Performance was measured using Character Error Rate (CER) and BLEU metrics. TrOCR models were fine-tuned on 20-50% of the dataset with 6 epochs. The study focused on tabular documents containing structured data like names, dates, and occupations.

## Key Results
- LLMs achieved significantly lower CER and higher BLEU scores than traditional OCR/HTR systems
- Claude Sonnet 3.5 performed best on whole-scan processing, while GPT-4o excelled in line-by-line segmentation
- Fine-tuned TrOCR models lagged behind LLM performance despite training on the same dataset
- LLMs demonstrated superior handling of complex layouts and varied handwriting styles

## Why This Works (Mechanism)
None

## Foundational Learning
1. Character Error Rate (CER): Measures the minimum edit distance between predicted and reference text, accounting for insertions, deletions, and substitutions
   - Why needed: Standard metric for evaluating OCR/HTR accuracy, particularly important for historical documents where character-level errors accumulate
   - Quick check: Compare CER scores across different OCR systems on the same test set

2. BLEU (Bilingual Evaluation Understudy): Evaluates n-gram precision between candidate and reference translations
   - Why needed: Captures both precision and recall aspects of transcription quality, providing complementary information to CER
   - Quick check: Calculate BLEU scores for same system outputs to verify metric consistency

3. Fine-tuning vs. zero-shot learning: TrOCR was fine-tuned on 20-50% of the dataset, while LLMs were used in zero-shot mode
   - Why needed: Critical for understanding the data efficiency advantage of LLMs
   - Quick check: Compare performance differences between fine-tuned traditional models and zero-shot LLMs

4. Tabular document structure: The study focused specifically on documents with structured, table-like layouts
   - Why needed: Layout complexity significantly impacts transcription difficulty and system performance
   - Quick check: Test systems on both tabular and free-form historical documents

## Architecture Onboarding

Component Map: Historical Document Scans -> Preprocessing -> (LLM/TrOCR/Traditional OCR) -> Postprocessing -> CER/BLEU Evaluation

Critical Path: Document Scan → Text Extraction → Character Alignment → Error Calculation → Performance Metric

Design Tradeoffs: The study compared zero-shot LLMs (minimal data requirement) against fine-tuned traditional systems (data-intensive but potentially more specialized)

Failure Signatures: Traditional OCR systems failed primarily on character-level errors and layout complexity, while LLMs struggled specifically with numerical digit recognition

First Experiments:
1. Compare CER scores of the same document processed by GPT-4o and EasyOCR
2. Test Claude Sonnet 3.5 on both whole-scan and line-by-line versions of identical documents
3. Evaluate digit-specific accuracy by creating a test set with known numerical values

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can combining multiple LLMs (e.g., GPT-4 for recognition + GPT-3.5 for refinement) yield synergistic improvements in historical document transcription?
- Basis in paper: [explicit] Authors state: "one can also combine different LLMs to examine whether such an approach enhances the performance and to what extent."
- Why unresolved: This study only evaluated individual model performance, not ensemble or pipeline approaches.
- What evidence would resolve it: Experiments comparing single-model transcription against multi-model pipelines using the same CER and BLEU metrics on comparable datasets.

### Open Question 2
- Question: What strategies can overcome LLMs' difficulty in accurately transcribing numerical digits in historical documents?
- Basis in paper: [explicit] Authors note: "LLMs have a hard time reading digits... future work may consider fine-tuning for digits or different strategies to overcome this issue."
- Why unresolved: The current study focused on overall text accuracy; digit-specific performance was identified as a weakness but not systematically addressed.
- What evidence would resolve it: Targeted experiments with digit-focused prompting strategies or multi-modal fine-tuning, evaluated on digit-specific error rates.

### Open Question 3
- Question: How do LLM-based transcription methods perform on established handwriting recognition benchmarks like IAM and READ16?
- Basis in paper: [explicit] Authors state: "future research should focus on comparing the OCR/HTR systems and the LLM-based strategies used in this study against well-established benchmarks, such as the IAM dataset and the READ16 dataset."
- Why unresolved: This study only used a custom 20-page Belgian dataset; generalizability to standard benchmarks remains unknown.
- What evidence would resolve it: Direct comparison of LLM and traditional OCR/HTR performance on IAM and READ16 datasets using standardized evaluation protocols.

### Open Question 4
- Question: Would TrOCR fine-tuned on substantially larger datasets with more epochs match or exceed LLM transcription quality?
- Basis in paper: [explicit] Authors note it "would be valuable to evaluate the performance of the LLM-based strategies against a version of TrOCR trained on a larger dataset and subjected to additional training epochs."
- Why unresolved: Fine-tuning was limited to 20-50% of 20 pages with only 6 epochs; the upper bound of TrOCR performance with more data remains unexplored.
- What evidence would resolve it: Experiments with TrOCR trained on larger historical document corpora (hundreds+ pages) with extended training, compared against LLM baselines.

## Limitations
- The evaluation is limited to tabular documents from a single historical period (1770-1790), raising generalizability concerns
- Sample size of 20 documents may not capture full variability of historical handwriting styles
- The study does not assess downstream usability of transcriptions or data extraction quality from tabular formats
- Computational costs and inference time differences between LLMs and traditional systems were not accounted for

## Confidence
High confidence in the finding that LLMs outperform traditional OCR/HTR systems on this specific dataset of tabular historical documents, given the clear quantitative metrics (CER, BLEU) showing consistent improvements across multiple models.

Medium confidence in the superiority of whole-scan processing with Claude Sonnet 3.5, as this result is specific to the tested dataset and may not generalize to documents with different layouts or noise levels.

Medium confidence in the claim that LLMs require minimal ground truth data, as the study only demonstrates performance with 20 documents and does not systematically explore the relationship between training data quantity and transcription accuracy.

## Next Checks
1. Test the same LLM and traditional OCR/HTR systems on historical documents from different time periods, writing styles, and non-tabular formats to assess generalizability.

2. Evaluate the impact of document noise levels (fading, staining, bleed-through) on LLM performance compared to traditional systems using a controlled degradation study.

3. Conduct a cost-benefit analysis comparing computational resources, inference time, and accuracy trade-offs between LLMs and traditional systems for archival-scale digitization projects.