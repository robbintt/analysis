---
ver: rpa2
title: 'P3: Prompts Promote Prompting'
arxiv_id: '2507.15675'
source_url: https://arxiv.org/abs/2507.15675
tags:
- prompt
- system
- optimization
- user
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces P3, a self-improvement framework that jointly
  optimizes system and user prompts through an iterative process. Unlike previous
  methods that focus on only one prompt component, P3 addresses the interdependent
  nature of system and user prompts by optimizing them together offline, then leveraging
  the results for online prompt optimization.
---

# P3: Prompts Promote Prompting

## Quick Facts
- **arXiv ID:** 2507.15675
- **Source URL:** https://arxiv.org/abs/2507.15675
- **Reference count:** 40
- **Key outcome:** P3 jointly optimizes system and user prompts through iterative offline optimization and online query-dependent refinement, outperforming existing automatic prompt optimization methods on general QA and reasoning tasks

## Executive Summary
P3 introduces a novel self-improvement framework that addresses the interdependence of system and user prompts through joint optimization. Unlike previous methods focusing on only one prompt component, P3 iteratively refines both prompts offline by generating diverse user prompt complements and using hard samples to optimize system prompts. The framework then deploys online optimization either through fine-tuning a small language model or using efficient in-context learning with retrieved demonstrations. Experiments demonstrate consistent performance improvements across multiple LLMs and task types, with P3-ICL offering a favorable balance between performance and inference efficiency.

## Method Summary
P3 operates through a two-stage framework that jointly optimizes system and user prompts. The offline stage generates k=5 candidate complements per user prompt using an LLM, scores them with LLM-as-judge, and iteratively refines top candidates as few-shot examples. System prompts are optimized every T=400 samples using hard samples below threshold ε=6. The online stage offers two deployment options: fine-tuning Qwen2-7B-Instruct on collected (prompt, complement) pairs, or using P3-ICL with SBERT retrieval for in-context learning. The framework addresses the critical challenge of prompt interdependence by ensuring affinity between system and user prompts through shared optimization objectives.

## Key Results
- P3 consistently outperforms existing automatic prompt optimization methods across multiple LLMs
- P3-ICL achieves favorable balance between performance and inference efficiency
- Performance gains demonstrated on general QA tasks (Arena-hard, Alpaca-Eval) and reasoning tasks (GSM8K, GPQA)
- Framework shows robustness across different deployment scenarios and efficiency requirements

## Why This Works (Mechanism)
P3's effectiveness stems from its joint optimization approach that recognizes system and user prompts are interdependent rather than independent components. By optimizing them together offline and ensuring affinity between the two components, P3 creates synergistic prompt pairs that work better together than individually optimized prompts. The use of hard samples for system prompt optimization ensures robustness, while diverse complement generation prevents overfitting to specific prompt patterns.

## Foundational Learning

**LLM-as-judge** - Using language models to evaluate and score prompt quality
*Why needed:* Automated evaluation of prompt effectiveness without human annotation
*Quick check:* Verify judge model scores correlate with human preferences on sample prompts

**Few-shot optimization** - Using generated examples as demonstrations for iterative refinement
*Why needed:* Enables continuous improvement of prompts through example-based learning
*Quick check:* Confirm few-shot examples improve optimizer performance on validation prompts

**Hard sample mining** - Identifying and utilizing poorly performing examples for robust optimization
*Why needed:* Prevents overfitting to easy cases and ensures model handles challenging queries
*Quick check:* Track hard sample proportion and distribution across optimization iterations

**In-context learning retrieval** - Using semantic similarity to find relevant demonstrations for prompt enhancement
*Why needed:* Enables efficient online optimization without model fine-tuning
*Quick check:* Verify retrieved demonstrations are semantically relevant to query context

## Architecture Onboarding

**Component map:** Dataset -> User Prompt Generation -> Scoring -> Iterative Refinement -> Hard Sample Collection -> System Prompt Optimization -> Online Deployment (Finetuning or ICL)

**Critical path:** Offline optimization (user prompts → iterative refinement → hard sample collection → system prompt optimization) → Online deployment (retrieval/fine-tuning) → Performance evaluation

**Design tradeoffs:** Joint optimization vs. separate optimization of system and user prompts; fine-tuning vs. in-context learning for online deployment; diversity vs. quality in complement generation

**Failure signatures:** Low-quality complements generating circular guidance; system prompt optimization diverging or overfitting to hard samples; P3-ICL retrieving irrelevant demonstrations

**Three first experiments:**
1. Run user prompt optimization for 100 samples with D=1 iteration and inspect generated complements for quality
2. Monitor hard sample scoring distribution during first 1000 samples to detect optimization stability
3. Implement and validate Arena-hard and Alpaca-Eval 2.0 evaluation pipelines with baseline model

## Open Questions the Paper Calls Out
None

## Limitations
- Missing complete prompt templates and evaluation procedures for core benchmarks
- Unspecified hyperparameters like batch sizes for system prompt optimization
- Unclear retrieval methodology for P3-ICL (top-k neighbors, similarity thresholds)
- Limited discussion of computational costs for offline optimization

## Confidence
**Low confidence** in replicating full end-to-end performance due to missing evaluation infrastructure and incomplete prompt specifications
**Medium confidence** in reproducing offline optimization pipeline given detailed methodology description
**Medium confidence** in online optimization approaches since training procedures and model choices are clearly specified

## Next Checks
1. Implement and validate Arena-hard and Alpaca-Eval 2.0 evaluation pipelines by running baseline GPT-3.5-turbo-1106 with raw system prompt
2. Run user prompt optimization for 100 samples with D=1 iteration and manually inspect 10-20 generated complements
3. Monitor hard sample scoring distribution during first 1000 samples of offline optimization to detect stability issues