---
ver: rpa2
title: Biomedical Question Answering via Multi-Level Summarization on a Local Knowledge
  Graph
arxiv_id: '2504.01309'
source_url: https://arxiv.org/abs/2504.01309
tags:
- graph
- claims
- documents
- question
- claim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for biomedical question answering
  that addresses the challenge of capturing multi-document relationships in retrieval-augmented
  generation (RAG). The approach constructs a local knowledge graph from retrieved
  documents using propositional claims, then performs layerwise summarization to contextualize
  a small language model for QA tasks.
---

# Biomedical Question Answering via Multi-Level Summarization on a Local Knowledge Graph

## Quick Facts
- arXiv ID: 2504.01309
- Source URL: https://arxiv.org/abs/2504.01309
- Authors: Lingxiao Guan; Yuanhao Huang; Jie Liu
- Reference count: 15
- Primary result: Novel method for biomedical QA using multi-level summarization on local knowledge graphs, achieving comparable or superior performance over RAG baselines with accuracy improvements of 0.01-0.09

## Executive Summary
This paper introduces a novel approach for biomedical question answering that addresses the challenge of capturing multi-document relationships in retrieval-augmented generation systems. The method constructs a local knowledge graph from retrieved documents using propositional claims, then performs layerwise summarization to contextualize a small language model for QA tasks. By extracting decontextualized claims and building graph structures, the approach generates focused summaries around key claims of interest, enabling better integration of information from multiple sources while preserving explicit relationships between medical concepts.

## Method Summary
The proposed method tackles biomedical question answering by first retrieving relevant documents and extracting propositional claims from them. These claims are then used to construct a local knowledge graph that captures relationships between medical concepts. The system performs layerwise summarization by contextualizing different levels of the graph, generating focused summaries that integrate information from multiple documents. A small language model is then used for the final QA task, leveraging the structured and summarized knowledge rather than raw documents. The approach emphasizes capturing explicit relationships between concepts while maintaining the decontextualized nature of medical claims.

## Key Results
- Achieves accuracy improvements of 0.01 to 0.09 across biomedical QA benchmarks (MMLU, PubMedQA, MedQA)
- Layerwise summarization achieves high faithfulness (0.9569) and source diversity (0.9647)
- Maintains strong answer relevance (0.8414) while integrating information from multiple documents
- Shows consistent performance gains over RAG baselines across different biomedical datasets

## Why This Works (Mechanism)
The method works by addressing a fundamental limitation of traditional RAG approaches: their inability to effectively capture and leverage relationships between multiple retrieved documents. By constructing a local knowledge graph with propositional claims, the system creates a structured representation of the retrieved information that explicitly captures relationships between medical concepts. The layerwise summarization then allows for contextualization at different levels of abstraction, generating focused summaries that integrate information while preserving important relationships. This structured approach enables the small language model to access organized, relationship-rich knowledge rather than processing raw documents, leading to more accurate and coherent answers.

## Foundational Learning
- **Knowledge Graph Construction**: Needed to represent relationships between medical concepts extracted from multiple documents. Quick check: Verify graph connectivity and ensure important claims are properly linked.
- **Propositional Claim Extraction**: Essential for creating decontextualized medical facts that can be structured into the knowledge graph. Quick check: Validate claim extraction accuracy and ensure medical terminology is preserved.
- **Layerwise Summarization**: Required to contextualize information at different abstraction levels while maintaining relationships. Quick check: Assess summary quality at each layer and verify relationship preservation.
- **Small Language Model Contextualization**: Enables efficient QA using structured knowledge rather than raw documents. Quick check: Compare performance with different model sizes and verify knowledge integration.

## Architecture Onboarding

**Component Map:**
Document Retrieval -> Claim Extraction -> Knowledge Graph Construction -> Layerwise Summarization -> Small Language Model QA

**Critical Path:**
Document Retrieval → Claim Extraction → Knowledge Graph Construction → Layerwise Summarization → QA Generation

**Design Tradeoffs:**
- Small language models vs. larger models (efficiency vs. capability)
- Decontextualized claims vs. full document context (precision vs. completeness)
- Layerwise summarization depth (detail vs. computational cost)
- Graph complexity vs. summarization effectiveness

**Failure Signatures:**
- Poor claim extraction leads to incomplete knowledge graphs
- Summarization layers may lose critical relationships between concepts
- Small language models may struggle with highly complex biomedical queries
- Knowledge graph construction may fail with ambiguous or contradictory claims

**First 3 Experiments:**
1. Compare performance with and without knowledge graph construction
2. Evaluate different layerwise summarization depths
3. Test various claim extraction methods for accuracy and completeness

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains over RAG baselines are modest (0.01-0.09 accuracy improvements)
- Significant preprocessing required for claim extraction and knowledge graph construction
- Limited evaluation on long-form biomedical QA despite growing importance of this task format
- Does not address potential hallucinations or factual errors in generated summaries

## Confidence
- **High confidence**: Layerwise summarization approach and implementation details are well-described and reproducible
- **Medium confidence**: Performance improvements over baselines are statistically significant but practical impact may be limited
- **Medium confidence**: Faithfulness and relevance metrics appear robust but could benefit from additional validation

## Next Checks
1. Conduct ablation studies specifically isolating the contribution of layerwise summarization versus knowledge graph construction
2. Evaluate on long-form biomedical QA datasets to assess performance on extended response generation
3. Perform error analysis on failed cases to identify failure modes and potential biases in the summarization approach