---
ver: rpa2
title: 'M-Wanda: Improving One-Shot Pruning for Multilingual LLMs'
arxiv_id: '2505.21171'
source_url: https://arxiv.org/abs/2505.21171
tags:
- pruning
- sparsity
- languages
- m-wanda
- wanda
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the multilingual impact of one-shot pruning
  on large language models (LLMs) and introduces M-Wanda, a novel method to preserve
  cross-lingual performance. While existing methods like Wanda degrade multilingual
  performance significantly beyond 50% sparsity, M-Wanda mitigates this by incorporating
  language-aware activation statistics and dynamically allocating layerwise sparsity
  based on cross-lingual importance.
---

# M-Wanda: Improving One-Shot Pruning for Multilingual LLMs

## Quick Facts
- arXiv ID: 2505.21171
- Source URL: https://arxiv.org/abs/2505.21171
- Reference count: 21
- One-line primary result: M-Wanda improves multilingual performance of pruned LLMs, especially for underrepresented and typologically distant languages, by using language-aware activation statistics and dynamic layerwise sparsity allocation.

## Executive Summary
This paper addresses the challenge of preserving cross-lingual performance when pruning large language models (LLMs) for multilingual settings. Existing one-shot pruning methods, such as Wanda, suffer significant performance degradation for underrepresented and typologically distant languages when sparsity exceeds 50%. To overcome this, the authors introduce M-Wanda, a method that leverages language-aware activation statistics and dynamically allocates layerwise sparsity based on cross-lingual importance. Experiments on 15 languages across six downstream tasks and two evaluation datasets demonstrate that M-Wanda consistently reduces perplexity and improves task performance, particularly for underrepresented and typologically distant languages. The method also generalizes to unseen languages and maintains effectiveness across varying sparsity levels and model sizes, making it a robust solution for multilingual LLM pruning.

## Method Summary
M-Wanda introduces a novel approach to one-shot pruning for multilingual LLMs by incorporating language-aware activation statistics and dynamically allocating layerwise sparsity. The method first computes activation statistics for each language during a pre-training phase, capturing the importance of different layers for cross-lingual generalization. These statistics are then used to assign sparsity levels to individual layers, prioritizing the retention of critical layers for underrepresented and typologically distant languages. This dynamic allocation ensures that the model preserves cross-lingual performance even at high sparsity levels. M-Wanda is evaluated on a diverse set of 15 languages, six downstream tasks, and two evaluation datasets, demonstrating consistent improvements over existing methods like Wanda. The approach is also shown to generalize to unseen languages and adapt to varying sparsity levels and model sizes, highlighting its versatility and effectiveness.

## Key Results
- M-Wanda consistently outperforms Wanda in multilingual settings at high sparsity levels (≥50%), reducing perplexity and improving task performance.
- The method is particularly effective for underrepresented and typologically distant languages, addressing a critical gap in existing pruning approaches.
- M-Wanda generalizes to unseen languages and maintains robustness across varying sparsity levels and model sizes, demonstrating broad applicability.

## Why This Works (Mechanism)
M-Wanda leverages language-aware activation statistics to dynamically allocate layerwise sparsity, ensuring that critical layers for cross-lingual generalization are preserved. By prioritizing the retention of important layers for underrepresented and typologically distant languages, the method mitigates the performance degradation typically observed in existing pruning approaches at high sparsity levels. This dynamic allocation is based on activation statistics computed during a pre-training phase, which capture the cross-lingual importance of each layer. The approach effectively balances the trade-off between sparsity and performance, enabling the model to maintain multilingual capabilities even under significant compression.

## Foundational Learning
- **Cross-lingual generalization**: Understanding how multilingual models transfer knowledge across languages is essential for designing pruning methods that preserve multilingual performance.
  - *Why needed*: Ensures the method retains critical cross-lingual capabilities during pruning.
  - *Quick check*: Evaluate model performance on typologically diverse languages after pruning.
- **Layerwise importance**: Identifying which layers contribute most to multilingual performance is crucial for effective sparsity allocation.
  - *Why needed*: Guides dynamic layerwise sparsity assignment to preserve critical layers.
  - *Quick check*: Analyze activation statistics to determine layer importance across languages.
- **Sparsity-accuracy trade-off**: Balancing compression and performance is key to practical pruning applications.
  - *Why needed*: Ensures the method achieves high sparsity without significant performance loss.
  - *Quick check*: Measure perplexity and task performance at varying sparsity levels.

## Architecture Onboarding
- **Component map**: Pre-training -> Activation statistics computation -> Layerwise sparsity allocation -> Pruning
- **Critical path**: Activation statistics computation -> Dynamic layerwise sparsity allocation
- **Design tradeoffs**: Language-aware statistics vs. uniform sparsity allocation; dynamic vs. static layerwise sparsity.
- **Failure signatures**: Significant performance drop for underrepresented languages; inability to generalize to unseen languages.
- **First experiments**: 1) Evaluate activation statistics correlation with cross-lingual importance. 2) Test dynamic layerwise sparsity allocation on a subset of languages. 3) Benchmark M-Wanda against Wanda on a single downstream task.

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions, but it highlights the need for further exploration of the theoretical grounding for generalization to unseen languages and the isolation of the contribution of language-aware statistics versus layerwise sparsity allocation.

## Limitations
- Reliance on language-specific activation statistics without explicit justification for their correlation with cross-lingual generalization.
- Lack of ablation studies isolating the impact of language-aware statistics versus layerwise sparsity allocation.
- Absence of benchmarking against other state-of-the-art multilingual pruning methods.

## Confidence
- **High**: M-Wanda outperforms Wanda in multilingual settings at high sparsity levels (≥50%), as demonstrated across multiple languages and tasks.
- **Medium**: M-Wanda's effectiveness across varying sparsity levels and model sizes, though results are consistent, the extent of robustness to extreme sparsity (>80%) is not explored.
- **Medium**: The claim of generalization to unseen languages, supported by experimental evidence but lacking deeper analysis of linguistic diversity or transfer mechanisms.

## Next Checks
1. Conduct ablation studies to isolate the impact of language-aware activation statistics versus dynamic layerwise sparsity allocation on cross-lingual performance.
2. Test M-Wanda on a broader set of unseen languages, including low-resource and highly typologically distant ones, to rigorously validate generalization claims.
3. Benchmark M-Wanda against other leading multilingual pruning methods (e.g., LISA, RigL) to establish its relative effectiveness in cross-lingual settings.