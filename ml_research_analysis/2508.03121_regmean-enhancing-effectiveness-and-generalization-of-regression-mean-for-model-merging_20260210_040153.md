---
ver: rpa2
title: 'RegMean++: Enhancing Effectiveness and Generalization of Regression Mean for
  Model Merging'
arxiv_id: '2508.03121'
source_url: https://arxiv.org/abs/2508.03121
tags:
- regmean
- merging
- tasks
- task
- vit-b
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of RegMean, a model merging
  method that independently processes each linear layer, ignoring the propagation
  of features and information through the network. The proposed RegMean++ extends
  RegMean by explicitly incorporating intra- and cross-layer dependencies into the
  merging objective.
---

# RegMean++: Enhancing Effectiveness and Generalization of Regression Mean for Model Merging

## Quick Facts
- arXiv ID: 2508.03121
- Source URL: https://arxiv.org/abs/2508.03121
- Authors: The-Hai Nguyen; Dang Huu-Tien; Takeshi Suzuki; Le-Minh Nguyen
- Reference count: 40
- Primary result: Consistently outperforms RegMean across vision and language tasks with 1-3% average accuracy gains

## Executive Summary
RegMean++ extends the regression mean model merging method by incorporating the merge model's own feature flow into the objective function. This captures intra- and cross-layer dependencies that RegMean ignores by using candidate model activations. Extensive experiments show RegMean++ achieves 1-3% average accuracy gains on vision tasks, 2% on language tasks, and maintains effectiveness for sequential merging and out-of-domain generalization. The method particularly excels when merging middle and deep layers, achieving >98% of full-layer accuracy with 50% less computation.

## Method Summary
RegMean++ iteratively solves for linear layer weights using closed-form regression, but critically differs from RegMean in how input features are obtained. While RegMean uses activations from candidate models, RegMean++ uses activations from the current merge model up to the previous layer. This creates sequential dependencies where each layer's merging depends on the previous merged layers. The method applies a scaling factor α=0.95 to stabilize matrix inversion and averages other parameters like biases and LayerNorm weights. The approach targets only linear layers within MLPs and attention heads, leaving non-linear components to simple averaging.

## Key Results
- Achieves 1-3% average accuracy gains over RegMean on vision tasks (SUN397, Cars, DTD, etc.)
- Maintains >98% of full-layer accuracy when merging only middle and deep layers (5-12)
- Outperforms state-of-the-art methods on sequential merging (8 tasks) and out-of-domain generalization
- Shows 2% improvement on language tasks (GLUE, SQuAD) while RegMean fails to improve

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Incorporating the merge model's own feature flow into the regression objective improves alignment between merge and candidate models.
- **Mechanism:** RegMean computes input features X^(l)_i using the previous candidate layer f^(l-1)_i. RegMean++ instead uses activations from the previous *merge* layer f^(l-1)_M, i.e., X^(l)_i = f^(l-1)_M(X^(l-1)_i). This captures intra- and cross-layer dependencies that RegMean ignores, producing weight updates better aligned with how features actually propagate through the merged model.
- **Core assumption:** The merge model's feature flow at inference time better reflects its behavior than the candidate models' independent feature flows.
- **Evidence anchors:**
  - [abstract] "RegMean++ extends RegMean by incorporating the merge model's own feature flow into the objective, thereby capturing both intra- and cross-layer dependencies."
  - [section 3.2] "The key difference between RegMean++ and RegMean lies in how input feature X^(l,j)_i is obtained."
  - [corpus] Weak corpus support; no direct neighbor papers validate this specific feature-flow mechanism.
- **Break condition:** If the merge model's activations diverge significantly from candidate activations early in the network, error may compound through subsequent layers.

### Mechanism 2
- **Claim:** Middle and deep transformer layers are more critical for merging effectiveness than early layers.
- **Mechanism:** Empirical analysis shows merging using middle/deep layers (layers 5-12 for ViT-B) preserves >98% of full-layer accuracy, while early layers cause notable degradation. Mid-depth layers may serve as more reliable sources of meaningful features for merging.
- **Core assumption:** Task-relevant semantic representations are concentrated in middle-to-deep layers, while early layers contain more generic features.
- **Evidence anchors:**
  - [section 1] "Merging using linear layers from middle and deep transformer layers preserves over 98% accuracy compared to using all linear layers."
  - [section 5.6] "RegMean++ achieves 98%, 99%, and 99% of the full-layer accuracy for ViT-B/32, ViT-B/16, and ViT-L/14 when using middle and deep layers."
  - [corpus] No corpus papers directly validate layer-wise importance in merging.
- **Break condition:** For architectures with different layer-wise representations (e.g., CNNs, non-transformer models), this layer importance distribution may not hold.

### Mechanism 3
- **Claim:** MLP modules contribute more effectively to merging than attention heads.
- **Mechanism:** Component-specific merging experiments show MLP linear layers consistently outperform attention head linear layers across all ViT variants. This aligns with prior findings that MLPs serve as dictionaries of factual and task-relevant knowledge.
- **Core assumption:** Task-specific knowledge is more concentrated in MLP weights than attention weights.
- **Evidence anchors:**
  - [section 5.6] "Merging using linear layers in the MLP modules consistently outperforms using linear layers in the attention heads."
  - [section 5.6] "This observation aligns with previous findings... that MLPs serve as dictionaries of factual and task-relevant knowledge."
  - [corpus] Weak corpus support; no neighbor papers validate MLP vs. attention contributions.
- **Break condition:** For models where attention heads encode significant task-specific information, focusing on MLPs may underperform.

## Foundational Learning

- **Concept:** Linear regression closed-form solution (W* = (X^T X)^(-1) X^T Y)
  - **Why needed here:** RegMean formulates merging as minimizing ||XW_M - XW_i||², yielding a closed-form solution requiring matrix inversion of inner-product matrices.
  - **Quick check question:** Can you derive why adding regularization Λ_i prevents numerical instability when X^T X is ill-conditioned?

- **Concept:** Transformer architecture components (attention heads, MLP, LayerNorm, GELU)
  - **Why needed here:** RegMean++ targets only linear layers within MLPs and attention heads; understanding which components are linear vs. non-linear is essential for correct implementation.
  - **Quick check question:** Why does RegMean merge non-linear components (embeddings, biases) via simple averaging rather than regression?

- **Concept:** Inner-product matrices (Gram matrices) and their role in capturing feature correlations
  - **Why needed here:** The core data statistic G^(l,j)_i = (X^(l,j)_i)^T X^(l,j)_i encodes feature dependencies; understanding how diagonal vs. off-diagonal entries affect merging stability is crucial.
  - **Quick check question:** Why does the scaling factor α control the trade-off between preserving feature correlations (off-diagonal) vs. numerical stability (diagonal-only)?

## Architecture Onboarding

- **Component map:**
  Candidate Models (f_1...f_K) -> Merge Model (f_M) -> Forward Pass (sequential, layer-by-layer) -> For each layer l: Get X^(l,j)_i from candidates -> If activations: X = f_M^(l-1)(X) -> Compute G_i = X^T X -> Compute W_M via Eqn 2 -> Update f_M layer -> Average remaining parameters

- **Critical path:** (1) Initialize f_M with backbone weights -> (2) For each layer l, collect features from candidates -> (3) If layer l > 1, compute features using *merged* weights from layer l-1 -> (4) Solve closed-form regression -> (5) Update f_M layer -> (6) Repeat for all layers.

- **Design tradeoffs:**
  - Performance vs. computation: RegMean++ requires ~2x merging time due to sequential forward passes through evolving merge model.
  - Sample quality vs. quantity: Performance saturates quickly with ~256 samples; OOD samples significantly degrade performance.
  - α parameter: Higher α (e.g., 0.95) improves performance but α=1.0 causes degradation; requires tuning.

- **Failure signatures:**
  - Accuracy near zero: α=1.0 (no diagonal scaling) causes numerical instability.
  - Significant drop from RegMean: Using OOD samples (e.g., ImageNet) for merging statistics.
  - Poor language task performance: RegMean++ underperforms RegMean on smaller models (Llama-3.2-3B) and instruction-following tasks.
  - Sequential merging collapse: Some methods (Iso-C, Iso-CTS) fail dramatically after 12 tasks.

- **First 3 experiments:**
  1. **Baseline replication:** Implement RegMean++ on ViT-B/32 with 8 vision tasks using 256 samples/task and α=0.95. Verify ~2% improvement over RegMean on average accuracy.
  2. **Ablation on layer importance:** Merge using only middle/deep layers (5-12) vs. all layers. Confirm >98% accuracy retention and ~50% computation reduction.
  3. **Data sensitivity test:** Compare performance with (a) 256 random ID samples, (b) class-imbalanced samples, (c) 256 OOD samples (ImageNet). Verify robustness to imbalance but sensitivity to OOD.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the sequential dependency introduced by RegMean++ hinder its applicability to models with significantly larger parameter scales (e.g., >8B parameters) due to computational overhead?
- Basis in paper: [explicit] The authors explicitly list the 8 billion parameter limit as a constraint in Appendix E, noting that higher scales were not explored due to computational resource limits.
- Why unresolved: The paper demonstrates success up to Llama-3.1-8B, but it is unknown if the latency of the sequential forward passes becomes a bottleneck that outweighs the accuracy benefits for massive models.
- What evidence would resolve it: Benchmarking results showing the trade-off between merging time and accuracy gain for RegMean++ on models with 70B+ parameters compared to parallelized methods.

### Open Question 2
- Question: Why does RegMean++ underperform compared to standard RegMean on smaller language models (Llama-3.2-3B) and specifically on instruction-following tasks?
- Basis in paper: [inferred] Table 6 shows RegMean++ achieves lower accuracy than RegMean on Llama-3.2-3B (33.0 vs. 34.0) and specifically on instruction following (11.1 vs. 26.6), contradicting the general trend of improvement observed in vision tasks.
- Why unresolved: The paper reports the regression but does not provide a theoretical or empirical explanation for why incorporating feature flow hurts performance in this specific low-parameter or instruction-tuned regime.
- What evidence would resolve it: An ablation study analyzing the representation bias and feature alignment in smaller LLM layers during merging to identify if the "feature flow" assumption fails for instruction embeddings.

### Open Question 3
- Question: Can the effectiveness of RegMean++ be preserved when the merging data distribution differs significantly from the task data, such as when using generic OOD samples?
- Basis in paper: [inferred] Section 5.7 and Table 5 demonstrate that while RegMean++ excels with ID data, its performance collapses (e.g., 84.4% to 65.5%) when using OOD samples (ImageNet), underperforming even Fisher Merging in that specific setting.
- Why unresolved: The method relies heavily on the alignment of inner-product matrices; it is unclear if this dependency makes the method too brittle for scenarios where only generic, non-task-specific data is available for merging.
- What evidence would resolve it: Experiments testing domain adaptation techniques or regularization terms that stabilize the inner-product matrix calculation when input features are derived from out-of-domain distributions.

## Limitations
- Architecture specificity: Layer-wise importance findings demonstrated only on ViT architectures, generalizability to other architectures uncertain
- Data sensitivity trade-off: Requires ID samples and fails with OOD samples, limiting real-world applicability
- Language task performance: Underperforms RegMean on smaller language models (Llama-3.2-3B) and instruction-following tasks

## Confidence
- **High confidence**: The core mechanism of using merge model activations instead of candidate activations is well-defined and consistently improves performance across all vision experiments.
- **Medium confidence**: The layer-wise importance findings and MLP vs. attention effectiveness are supported by ablation studies but lack external validation across different architectures.
- **Low confidence**: The generalization to language models and other architecture types is not well-established, with evidence suggesting potential limitations.

## Next Checks
1. **Cross-architecture validation**: Implement RegMean++ on CNN architectures (e.g., ResNet variants) and verify whether middle layers remain most important for merging effectiveness.
2. **Data distribution sensitivity**: Systematically test RegMean++ with increasing levels of domain shift (e.g., using synthetic distribution shifts) to quantify the method's robustness boundaries.
3. **Dynamic α optimization**: Implement an adaptive α selection strategy that tunes the scaling factor per layer or per iteration, rather than using a fixed α=0.95, to potentially improve performance and stability.