---
ver: rpa2
title: Advancing Visual Large Language Model for Multi-granular Versatile Perception
arxiv_id: '2507.16213'
source_url: https://arxiv.org/abs/2507.16213
tags:
- vision
- segmentation
- computer
- pages
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MVP-LM, a framework that unifies diverse
  perception tasks across prediction types (box vs. mask) and instruction types (word-based
  vs.
---

# Advancing Visual Large Language Model for Multi-granular Versatile Perception

## Quick Facts
- arXiv ID: 2507.16213
- Source URL: https://arxiv.org/abs/2507.16213
- Reference count: 40
- Key outcome: MVP-LM achieves state-of-the-art performance on both closed-set benchmarks (83.6 cIoU on RefCOCO) and open-set segmentation tasks (19.4 mIoU on ADE20K), outperforming both specialized models and other VLLM-based generalists.

## Executive Summary
This paper introduces MVP-LM, a unified framework that handles diverse perception tasks across prediction types (box vs. mask) and instruction types (word-based vs. sentence-based) using a single Visual Large Language Model architecture. The method employs a multi-granularity decoder alongside a CoT-inspired dataset unification strategy to jointly train on tasks like panoptic segmentation, detection, and referring expression segmentation. By dynamically generating visual queries that combine LLM-generated base queries with instruction-guided residuals from multi-scale visual features, MVP-LM achieves strong performance across both single-object and complex multi-object scenes.

## Method Summary
MVP-LM uses a two-stage training procedure with a frozen Swin-B image encoder, Phi-1.5 LLM, and an OpenSeeD-style multi-granularity decoder. In Stage 1, only the connector is trained on CC3M data to align visual features with LLM embeddings. Stage 2 jointly trains the connector, LLM, and decoder on a mixture of COCO Panoptic, Objects365, RefCOCO, and GoldG datasets. The model generates visual queries dynamically by combining base queries from LLM summary token hidden states with top-N visual feature residuals selected by instruction-visual similarity. The unified loss combines LLM next-token prediction, classification losses, and mask/box prediction losses with bipartite matching.

## Key Results
- Achieves 83.6 cIoU on RefCOCO closed-set referring segmentation, outperforming specialized models
- Reaches 19.4 mIoU on ADE20K open-set segmentation, demonstrating strong generalization
- Maintains state-of-the-art PQ of 55.9 on COCO Panoptic segmentation while handling diverse task types
- Shows effective transfer from box-annotated data to mask prediction, improving performance by 4.2 cIoU when adding Objects365

## Why This Works (Mechanism)

### Mechanism 1
Dynamic query composition improves visual grounding by combining semantic reasoning (from LLM) with fine-grained visual evidence. The model generates base queries from LLM's summary token hidden states, then selects top-N visual features by similarity to instruction embeddings. These residuals are added to base queries before decoding. Core assumption: LLM summary tokens encode task-relevant semantic context that benefits localization when combined with visually-grounded residuals. Evidence: abstract states "dynamically generates visual queries by combining LLM-generated base queries with instruction-guided residuals from multi-scale visual features." Break condition: If base queries alone match or exceed performance on complex multi-object scenes, the residual contribution is marginal.

### Mechanism 2
CoT-inspired data curation enforces a reasoning-before-localization pattern that improves cross-task generalization. Training responses prepend auto-generated image captions before the special `<PER>` token, forcing the LLM to summarize visual context before predicting perception outputs. Core assumption: Explicit caption generation creates richer intermediate representations than direct prediction. Evidence: abstract mentions "CoT-inspired dataset unification strategy... enabling seamless supervised fine-tuning." Break condition: If removing captions yields equivalent performance, the reasoning benefit is spurious.

### Mechanism 3
Joint training across box/mask and word/sentence tasks yields mutual transfer, especially from box-annotated data to mask prediction. Unified loss combines LLM next-token prediction, CE/BCE for classification, plus mask (BCE + Dice) and box (L1 + GIoU) losses with bipartite matching. Core assumption: Box annotations provide scale and diversity that improve mask head learning despite lower granularity. Evidence: Table 6 shows adding O365 (+1.7M box samples) improves RefCOCO cIoU from 77.6 to 81.8. Break condition: If training on mask-only data matches joint training performance, the cross-granularity transfer is not the driver.

## Foundational Learning

- **Deformable attention in transformer decoders**: Why needed here: The multi-granularity decoder uses MSDeform layers to aggregate multi-scale features efficiently; understanding sparse attention patterns is essential for debugging query-feature interactions. Quick check question: Can you explain how deformable attention reduces computational cost compared to full self-attention while preserving spatial precision?

- **Hungarian bipartite matching for detection training**: Why needed here: MVP-LM uses bipartite matching to align predictions with ground truth before loss computation; mismatched assignments cause unstable training. Quick check question: Given 100 predicted boxes and 5 ground-truth objects, how does the matching algorithm handle the 95 unmatched predictions?

- **Vision-language connector architectures**: Why needed here: The connector module aligns Swin-B visual features with Phi-1.5 token embeddings; misalignment degrades both word-based and sentence-based perception. Quick check question: What is the role of the connector during Stage 1 (frozen LLM) versus Stage 2 (full fine-tuning)?

## Architecture Onboarding

- **Component map**: Image Encoder (Swin-B, frozen) -> Connector (MLP, trained Stage 1) -> LLM (Phi-1.5, trained Stage 2) -> Multi-granularity Decoder (OpenSeeD-style, trained Stage 2)
- **Critical path**: 1) Image → Swin-B → multi-scale features (saved for residual selection) 2) Image + instruction tokens → Connector → LLM input 3) LLM → summary token hidden states → MLP → base queries (N=100 default) 4) Instruction embeddings ⊗ visual features → top-N residuals → add to base queries 5) Queries → MSDeform decoder layers → box/mask/similarity heads
- **Design tradeoffs**: Query count (30 vs 100 vs 300): Fewer queries suit single-target tasks (RefCOCO), more queries help multi-object scenes (COCO) but add compute. Caption generation: Adds training diversity but requires multi-VLLM auto-labeling pipeline. Frozen vs fine-tuned vision encoder: Freezing reduces overfitting risk; fine-tuning may improve feature quality for open-set tasks.
- **Failure signatures**: Low cIoU with high PQ: Model defaults to box prediction, mask head undertrained—check mask loss weighting (λbce, λdice). Good RefCOCO, poor COCO: Query selection may be overfitting to single-target patterns—verify residual selection diversity. Hallucinated masks on unseen categories: Caption quality issue—filter auto-labeled captions with entropy threshold.
- **First 3 experiments**: 1) Ablate caption prefix: Train with `None` vs `Caption` response format on COCO+RefCOCO subset (9k iterations), compare PQ and cIoU deltas. 2) Isolate query components: Run inference with base queries only (zero residuals) vs residuals only (zero base queries) to quantify contribution of each component. 3) Query count sensitivity: Evaluate 30/100/300 queries on both RefCOCO-val and COCO-val to confirm task-dependent optimal points per Table 8.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can R1-like reinforcement learning training frameworks be effectively adapted to enhance the reasoning capabilities of visual perception models like MVP-LM? Basis in paper: The Conclusion states: "Considering the improvement brought by our CoT-inspired data curation, we would investigate the application of R1-like training in the realm of perception in our future work." Why unresolved: The current model relies exclusively on supervised fine-tuning (SFT) with a CoT-inspired dataset. The integration of reinforcement learning stages (e.g., Group Relative Policy Optimization), which are standard in R1-like models for text, has not yet been explored for visual grounding tasks.

- **Open Question 2**: Can a unified query initialization strategy be developed that maintains high performance on single-target referring tasks while retaining the benefits of dynamic selection for complex scenes? Basis in paper: Table 9 shows a trade-off where dynamic query selection improves COCO-Panoptic PQ (+1.2) but significantly degrades RefCOCO cIoU (-2.6) compared to fixed learnable queries. Why unresolved: The current architecture forces a choice between a dynamic mechanism (better for complex scenes) and a fixed mechanism (better for single-object grounding), lacking a mechanism to adaptively switch or fuse these approaches based on the instruction type.

- **Open Question 3**: To what extent do hallucinations or factual inconsistencies in the auto-generated image captions degrade the "thinking-then-perceiving" reasoning process? Basis in paper: The "Multi-caption Auto-labeling" and "Data Refinement" sections describe using diverse VLLMs to generate captions and filtering for "suggestive keywords" to mitigate hallucinations. Why unresolved: While the authors note that diverse VLLMs reduce overfitting, they do not compare the efficacy of these synthetic captions against ground-truth human annotations or analyze how specific caption errors propagate into segmentation failures.

## Limitations
- Scalability to open-set complexity: While MVP-LM demonstrates strong performance on ADE20K (19.4 mIoU), the dataset contains 2,693 categories. The claim that a single query generation mechanism can effectively ground on such diverse concepts without catastrophic forgetting remains untested.
- CoT reasoning validity: The paper attributes improved cross-task generalization to the CoT-inspired caption prefix, but no ablation isolates the effect of caption length or semantic relevance. If captions are merely syntactic noise, the claimed reasoning benefit is overstated.
- Generalization vs. memorization: MVP-LM's strong performance on RefCOCO (83.6 cIoU) could stem from overfitting to the specific phrase patterns in referring expressions. The paper lacks analysis of robustness to paraphrased instructions or out-of-domain object descriptions.

## Confidence

**High Confidence**: Claims about unified architecture achieving state-of-the-art on both closed-set (COCO Panoptic PQ: 55.9) and open-set (ADE20K mIoU: 19.4) benchmarks are well-supported by quantitative results. The two-stage training procedure and loss composition are clearly specified.

**Medium Confidence**: The assertion that dynamic query composition improves visual grounding is supported by ablation studies (Table 8) but lacks visualization of what base queries vs. residuals contribute at the feature level. The mechanism is plausible but not directly verified.

**Low Confidence**: Claims about CoT-inspired reasoning creating "richer intermediate representations" are weakest. The paper provides no direct evidence that the caption prefix improves LLM hidden state quality versus direct prediction. This mechanism remains theoretically motivated but empirically unverified.

## Next Checks

1. **Query Contribution Isolation**: For a diverse set of images (single-object RefCOCO and multi-object COCO), visualize and compare the spatial activation patterns of base queries alone versus residuals alone. Measure the intersection-over-union between predicted masks and ground truth to quantify each component's contribution.

2. **Caption Ablation with Length Variation**: Train MVP-LM with three caption configurations: no caption (direct prediction), short captions (≤10 tokens), and full captions. Evaluate on COCO Panoptic PQ and RefCOCO cIoU to determine if caption length correlates with performance gains.

3. **Out-of-Distribution Instruction Robustness**: Create a test set by paraphrasing RefCOCO expressions (e.g., "the person wearing glasses" → "the individual with spectacles"). Measure cIoU degradation compared to original expressions to assess generalization beyond memorized phrase patterns.