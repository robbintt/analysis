---
ver: rpa2
title: Concept Unlearning in Large Language Models via Self-Constructed Knowledge
  Triplets
arxiv_id: '2509.15621'
source_url: https://arxiv.org/abs/2509.15621
tags:
- knowledge
- unlearning
- target
- triplets
- forgetting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Concept Unlearning (CU) for LLMs, targeting
  concept-level forgetting rather than individual training sentences. CU is formalized
  using knowledge graphs, where concepts are defined as nodes and edges representing
  entities and their attributions.
---

# Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets

## Quick Facts
- arXiv ID: 2509.15621
- Source URL: https://arxiv.org/abs/2509.15621
- Authors: Tomoya Yamashita; Yuuki Yamanaka; Masanori Yamada; Takayuki Miura; Toshiki Shibahara; Tomoharu Iwata
- Reference count: 37
- One-line primary result: Proposed method achieves complete forgetting of target concepts while preserving unrelated knowledge, outperforming baselines in both accuracy and utility preservation.

## Executive Summary
This paper introduces Concept Unlearning (CU) for LLMs, targeting concept-level forgetting rather than individual training sentences. CU is formalized using knowledge graphs, where concepts are defined as nodes and edges representing entities and their attributions. The proposed method uses self-generated knowledge triplets and explanatory sentences to align unlearning with the model's internal knowledge. Experiments on real-world (WIKI-FACT) and synthetic (TOFU) datasets show that the method achieves complete forgetting of target concepts while preserving unrelated knowledge, outperforming baselines in both accuracy and utility preservation.

## Method Summary
The method alternates between two loss functions: triplet-based loss and sentence-based loss. GET_ATTR function prompts the LLM to generate triplets about the forgetting target, validates them against the pre-unlearned LLM, then converts them to sentences split at the entity boundary. Gradient ascent on log p_θ(X_attr|X_ent) suppresses attributional knowledge while preserving entity context. GET_SENT prompts the pre-unlearned LLM with "Tell me about {e(t)}" to generate rich multi-fact sentences. Minimizing log p_θ(X_sent) directly lowers likelihood of reproducing entity descriptions. The method regenerates triplets each iteration to capture residual knowledge that emerges as unlearning progresses.

## Key Results
- Achieves complete forgetting of target concepts (NodeAcc=EdgeAcc=0%) while preserving non-target knowledge
- Outperforms baselines: naive gradient ascent, RMM, and UIPE in both unlearning completeness and non-target preservation
- Self-generated knowledge triplets align unlearning with internal representations better than external corpora

## Why This Works (Mechanism)

### Mechanism 1: Self-Generated Knowledge Triplets Align Unlearning with Internal Representations
- Core assumption: Self-generated triplets better approximate the model's internal knowledge graph than external text; the model "knows what it knows."
- Evidence: L1-only achieves 23.9% EdgeAcc but 80.1% NodeAcc—effective for EU but not NU; UIPE similarly finds targeting related knowledge enhances unlearning

### Mechanism 2: Sentence-Based Loss Suppresses Entity-Level Recall
- Core assumption: Explanatory sentences encode denser concept knowledge than isolated triplets; greedy decoding yields high-confidence representations
- Evidence: L2-only achieves 0.0% NodeAcc but damages non-target accuracy (17.0% vs. 69.8% for full method)

### Mechanism 3: Iterative Triplet Regeneration Captures Residual Knowledge
- Core assumption: The unlearning process dynamically reveals which knowledge remains; the model can still generate valid triplets mid-unlearning
- Evidence: Method regenerates GET_ATTR in each iteration to capture residual facts as model forgets some knowledge

## Foundational Learning

- **Knowledge Graph Triplet Formalism (subject, relation, object)**
  - Why needed here: The entire method frames concepts as nodes with edges to attribution nodes
  - Quick check: Given triplet (Harry Potter, school, Hogwarts), which part is the "attribution" targeted by Edge Unlearning?

- **Gradient Ascent vs. Negative Preference Optimization**
  - Why needed here: The paper implements both GA (maximize loss on target) and NPO (relative preference optimization)
  - Quick check: Why might NPO be more stable than gradient ascent for unlearning?

- **Cloze-Style Probing (LAMA paradigm)**
  - Why needed here: Evaluation converts triplets to masked prompts to measure EdgeAcc
  - Quick check: If a model correctly predicts "Hogwarts" in a cloze prompt, has Edge Unlearning failed for that concept?

## Architecture Onboarding

- **Component map:** GET_SENT (once) -> GET_ATTR (per iteration) -> Loss computation (L1/L2 alternating) -> Validation filter
- **Critical path:** 1) Pre-unlearned LLM generates X_sent (fixed) 2) Each epoch: current LLM generates candidate triplets -> pre-unlearned LLM validates -> valid triplets converted to (X_ent, X_attr) 3) Backprop through L1 (triplet) and L2 (sentence) losses alternately 4) Early stopping when both NodeAcc and EdgeAcc reach 0%
- **Design tradeoffs:** KG Coverage vs. Practicality (relation-aware prompting achieves 88% coverage vs. 38% default), KG Precision vs. Selectivity (higher precision via GPT-4o validation doesn't always improve non-target preservation), Fixed vs. Regenerated Triplets (X_sent fixed for stability, X_ent/X_attr regenerated for coverage)
- **Failure signatures:** EdgeAcc decays slowly while NodeAcc drops rapidly → triplet loss insufficient; Non-target accuracy collapses (<20%) → sentence loss too aggressive; Triplet generation produces hallucinations → validation filter failing
- **First 3 experiments:** 1) Baseline replication: Run naive GA on WIKI-FACT corpus without self-generation 2) Ablation on triplet validation: Disable pre-unlearned LLM validation step 3) Coverage sweep: Compare default GET_ATTR vs. Rel-Aware variant on single entity

## Open Questions the Paper Calls Out

- **Can the Concept Unlearning framework be extended to handle procedural knowledge (e.g., "how to make a bomb") rather than just declarative knowledge?**
  - Basis: The authors explicitly restrict their definition of "concept" to triplet-based declarative knowledge and state that "Extending unlearning to other types of knowledge, such as procedural knowledge... is left for future work."
  - Why unresolved: The current methodology relies on knowledge graphs which naturally structure declarative facts but do not easily represent dynamic, step-by-step procedural information
  - What evidence would resolve it: A modified unlearning algorithm that successfully suppresses the generation of instructional or procedural text without requiring entity-relation triplets

- **How can the coverage of self-generated knowledge graphs be maximized without relying on the manual predefinition of relation types?**
  - Basis: Section 5.5 notes that while using predefined relations (Rel-Aware) improves unlearning performance, "predefining all the relation types may not be feasible in real scenarios," motivating future work on automated extraction
  - Why unresolved: There is a trade-off between the "default" method, which may miss sparse facts, and the "Rel-Aware" method, which requires prior knowledge of the schema to guide the LLM
  - What evidence would resolve it: An adaptive extraction mechanism that achieves high KG coverage comparable to the "Rel-Aware" variant without needing a predefined list of relations

- **How can unintended damage to non-target knowledge be further minimized during the unlearning process?**
  - Basis: Section 6 lists "mitigating such collateral damage while maintaining effective unlearning" as a primary limitation and an "important challenge for future work"
  - Why unresolved: While the method outperforms baselines, results still show a noticeable drop in non-target accuracy (e.g., Edge accuracy for non-targets dropping from ~87% to ~70% in some Llama configurations)
  - What evidence would resolve it: An improved optimization objective or regularization technique that maintains non-target accuracy statistically indistinguishable from the pre-unlearned model while achieving complete forgetting

## Limitations

- Method's effectiveness depends heavily on precision of self-generated triplets, with initial KG precision reported at only 55-69% before GPT-4o validation
- Distinction between Node Unlearning (NU) and Edge Unlearning (EU) shows EU requires more training iterations and is inherently more difficult
- Reliance on external validation (GPT-4o) introduces dependencies on proprietary models and computational overhead

## Confidence

- **High confidence:** Knowledge graph triplet formalism, alternating loss architecture, self-generated knowledge outperforming external corpora
- **Medium confidence:** Iterative triplet regeneration superiority, relation-aware prompting for coverage
- **Low confidence:** Claim of "complete forgetting" when EdgeAcc reaches 0%—evaluation methodology may not capture all forms of knowledge retrieval

## Next Checks

1. **Isolation of iterative regeneration benefit:** Run ablation comparing fixed triplets (generated once pre-unlearning) vs. regenerated triplets (per iteration) on a single concept, measuring both unlearning completeness and non-target preservation

2. **Robustness to hallucination:** Disable the GPT-4o validation step and measure degradation in unlearning selectivity and non-target preservation across multiple concepts

3. **Cross-domain generalization:** Apply the method to concepts from biomedical or legal domains (outside WIKI-FACT/TOFU) and measure whether knowledge graph coverage and precision patterns hold