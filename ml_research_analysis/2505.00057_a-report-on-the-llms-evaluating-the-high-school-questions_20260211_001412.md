---
ver: rpa2
title: A Report on the llms evaluating the high school questions
arxiv_id: '2505.00057'
source_url: https://arxiv.org/abs/2505.00057
tags:
- questions
- b-instruct
- evaluation
- accuracy
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This report evaluates large language models (LLMs) on high school
  math questions from Chinese college entrance exams (2019-2023). Eight LLM APIs were
  tested across accuracy, response time, logical reasoning, and creativity metrics.
---

# A Report on the llms evaluating the high school questions

## Quick Facts
- arXiv ID: 2505.00057
- Source URL: https://arxiv.org/abs/2505.00057
- Authors: Zhu Jiawei; Chen Wei
- Reference count: 8
- Eight LLM APIs tested on Chinese high school math exam questions (2019-2023)

## Executive Summary
This report evaluates large language models on high school math questions from Chinese college entrance exams. Eight LLM APIs were assessed across accuracy, response time, logical reasoning, and creativity metrics. GLM-4-Flash excelled in guidance and creativity but had the longest response time (7400s). Qwen2.5-7B-Instruct achieved the highest overall weighted score (0.81) with strong accuracy (64%) and efficiency. ERNIE-Speed-128K and Spark-lite showed top performance in accuracy and logic respectively. Yi-34B performed poorly across all metrics. The study reveals LLMs' potential in education while highlighting needs for improvement in logical reasoning and creative problem-solving.

## Method Summary
The study evaluated eight LLM APIs using math questions from Chinese college entrance examinations (2019-2023). Models were tested on accuracy, response time, logical reasoning, and creativity through various prompt configurations including persona-based instructions and multi-solution generation. A comprehensive evaluation framework included fuzzy matching for answer validation and iterative CoT prompting for logical assessment.

## Key Results
- Qwen2.5-7B-Instruct achieved highest overall weighted score (0.81) with 64% accuracy
- GLM-4-Flash took longest response time (7400s) but excelled in guidance and creativity
- Multi-solution accuracy averaged 0.55, with Qwen2.5-7B-Instruct leading at 0.64
- ERNIE-Speed-128K achieved top accuracy (71.1%) and Spark-lite excelled in logic (0.71)
- Yi-34B performed poorly across all metrics

## Why This Works (Mechanism)

### Mechanism 1: Prompt-Induced Persona Convergence
- **Claim:** Assigning specific roles to models (e.g., "College Entrance Examination Student") appears to improve accuracy by restricting the solution space to efficient, syllabus-compliant methods rather than abstract theorizing.
- **Mechanism:** The prompt likely acts as a soft constraint on the model's probability distribution, favoring tokens associated with "test-taking efficiency" over "comprehensive academic exploration."
- **Core assumption:** The model has encoded distinct stylistic patterns for "student" vs. "mathematician" personas during pre-training and can apply these to reasoning paths.
- **Evidence anchors:**
  - [section 5.4.2]: Shows GLM-4-Flash improved accuracy by 22% under "Student" (Guiding Words B) prompts.
  - [abstract]: Notes the exploration of LLM potential in educational fields implies context-specific tuning is required.
  - [corpus]: Related work (EduEval) supports the need for hierarchical cognitive benchmarks, implying context alignment is a known factor in performance.
- **Break condition:** If the model fails to distinguish between persona definitions (e.g., treating "Student" and "Mathematician" identically), accuracy gains will vanish.

### Mechanism 2: Iterative Chain-of-Thought (CoT) Repair
- **Claim:** Logical errors in model outputs can be corrected through multi-round CoT prompting, effectively "guiding" the model to the correct answer.
- **Mechanism:** The evaluation loop identifies an incorrect answer and forces the model to explicitly verbalize reasoning steps (CoT), which statistically increases the likelihood of spotting arithmetic or logical fallacies.
- **Core assumption:** The initial failure is due to a generation error (skipping steps) rather than a fundamental lack of parametric knowledge.
- **Evidence anchors:**
  - [section 4.3]: Describes the CCoT method where models are guided to "focus on the most critical reasoning steps."
  - [section 5.3.1]: Reports Hunyuan-lite required only 3 attempts on average to successfully guide incorrect questions to correctness.
  - [corpus]: Corpus is weak on specific "repair loops" for math; mechanisms are derived primarily from the paper's evaluation section.
- **Break condition:** If the logical error stems from hallucinated facts (e.g., wrong formulas), iterative guidance will likely reinforce the error rather than fix it.

### Mechanism 3: Diversity-Latency Trade-offs
- **Claim:** Models configured for high "richness" (generating multiple diverse solutions) incur a non-linear latency penalty compared to single-solution inference.
- **Mechanism:** Generating diverse solutions requires higher "Temperature" settings and broader beam searches, increasing computational complexity and token generation time.
- **Core assumption:** The latency is intrinsic to the generation algorithm for diversity, not merely API overhead.
- **Evidence anchors:**
  - [section 5.1.3]: GLM-4-Flash took 7400s for multi-solution tasks (highest richness) vs. Spark-lite at 688s.
  - [section 4.5]: Notes temperature was adjusted to increase randomness for multi-solution responses.
  - [corpus]: Missing specific evidence on latency-vs-diversity metrics; rely on reported time data in Section 5.1.3.
- **Break condition:** If API rate limiting or network jitter is the primary cause of latency, the correlation between richness and time will be inconsistent.

## Foundational Learning

- **Concept: Fuzzy Matching (Semantic Similarity)**
  - **Why needed here:** High school math fill-in-the-blank questions may have equivalent answers in different formats (e.g., "1/2" vs. "0.5"). Exact string matching fails here.
  - **Quick check question:** How would your evaluation pipeline handle a correct numerical answer formatted in LaTeX vs. plain text?

- **Concept: Difficulty Factor (DF)**
  - **Why needed here:** The paper normalizes performance against question difficulty (0.0 to 1.0). Evaluating a model solely on "Basic" questions would misleadingly inflate capability assessments.
  - **Quick check question:** If Model A scores 90% on Basic questions and Model B scores 60% on Difficult questions, which is performing better relative to the DF?

- **Concept: Chain-of-Thought (CoT) Prompting**
  - **Why needed here:** This is the core method used to evaluate "Logical Reasoning." You must understand that CoT forces the model to output intermediate steps rather than just the final answer.
  - **Quick check question:** Does asking a model to "think step-by-step" guarantee the logic is correct, or just that the output is longer?

## Architecture Onboarding

- **Component map:** JSON parser for exam questions (2019-2023 papers) -> API router handling 8 different LLM endpoints (GLM, Qwen, etc.) -> Modular evaluation engine (Regex/Fuzzy matcher for accuracy, Timestamp diff for response time, CoT loop for logic/guidance) -> Structured JSON storing `id`, `answer`, and `response_time`

- **Critical path:** The **Prompt Construction -> API Call -> Regex Evaluation** loop. The paper highlights that prompt variations (Persona A/B/C) significantly alter outcomes; the architecture must strictly isolate these variables to prevent contamination.

- **Design tradeoffs:**
  - **Speed vs. Creativity:** Using GLM-4-Flash maximizes solution diversity (richness) but creates a bottleneck (7400s total time). Using Spark-lite optimizes for speed but may miss edge-case solutions.
  - **Automated vs. Human Eval:** The paper uses a hybrid approach (1/10 sampling by experts) for comprehensive questions. Fully automating this risks missing nuance in complex proofs.

- **Failure signatures:**
  - **Logic Collapse:** High accuracy but low logic scores (Qwen2.5-7B). This suggests the model might be guessing or using heuristics rather than deriving the answer.
  - **Timeout Drift:** Yi-34B shows high latency variance; this indicates potential API instability or inefficient token generation for simple queries.

- **First 3 experiments:**
  1. **Baseline Stability Test:** Run the "Single-Solution" prompt set on all 8 models for the same set of 10 "Basic" questions to verify API connectivity and timestamp accuracy.
  2. **Persona Sensitivity Test:** Select one high-performing model (Qwen2.5-7B) and one creative model (GLM-4-Flash). Run the same 5 "Difficult" questions using Prompt A (Mathematician) vs. Prompt B (Student) to replicate the accuracy improvement delta.
  3. **CoT Stress Test:** Force a model that failed a specific "Advanced" logic question through the 3-attempt guidance loop (Section 5.3.1) to determine if the failure is recoverable or persistent.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance hierarchy of the evaluated LLMs change when applied to non-mathematical science subjects like biology and physics?
- Basis in paper: [explicit] The conclusion in Section 6 states, "this report plans to explore the application of LLMs in other subjects, such as biology and physics, to assess their broader applicability."
- Why unresolved: The current study restricted the evaluation dataset exclusively to mathematics questions from the college entrance examinations (2019-2023).
- What evidence would resolve it: A comparative evaluation report using the same eight LLMs and metrics (accuracy, logic, creativity) on standardized biology and physics exams.

### Open Question 2
- Question: To what extent does the inclusion of visual or diagram-based data impact the accuracy and logical reasoning scores of the models?
- Basis in paper: [inferred] Section 2.2 ("Data Selection") explicitly notes that the study selection criteria aimed to "avoid selecting questions with diagrams as much as possible" to reduce testing time.
- Why unresolved: The study excludes geometry and other problems requiring visual reasoning, creating a gap in understanding how these models handle the full spectrum of high school math requirements.
- What evidence would resolve it: A follow-up benchmark using the previously excluded diagram-based questions, requiring models to process multimodal inputs (text + image).

### Open Question 3
- Question: What specific training strategies or algorithmic optimizations are most effective for improving LLM logical reasoning without sacrificing response efficiency?
- Basis in paper: [explicit] Section 6 concludes that there is "room for improvement in logical reasoning" and explicitly recommends "enhancing their logical reasoning capabilities through strengthened training and algorithm optimization."
- Why unresolved: The report identifies the deficiency (e.g., Meta-Llama-3.1-8B-Instruct ranking low in logic) and suggests the solution but does not implement or test the training improvements.
- What evidence would resolve it: Ablation studies showing improved logic scores on the same dataset after implementing specific fine-tuning on logical reasoning chains or algorithmic attention mechanisms.

## Limitations
- Reliance on API-based testing introduces uncontrolled variables like network latency and potential prompt caching effects
- "Creativity" metric (richness) lacks standardized benchmarks for comparison, making cross-study validation difficult
- Single-round evaluation design may not capture model performance variations across multiple attempts or different prompt formulations

## Confidence
- **High Confidence:** Model accuracy rankings and response time measurements
- **Medium Confidence:** Multi-solution diversity metrics and logical reasoning scores
- **Low Confidence:** Educational guidance effectiveness and creativity assessments

## Next Checks
1. **Replication Test:** Deploy selected models locally to eliminate API variability and verify accuracy scores remain consistent across multiple runs
2. **Cross-Cultural Validation:** Apply the same evaluation framework to non-Chinese math problems to test generalization beyond the current corpus
3. **Temporal Stability Analysis:** Re-run the benchmark after 3-6 months to measure performance drift in commercial APIs and assess whether improvements align with stated development roadmaps