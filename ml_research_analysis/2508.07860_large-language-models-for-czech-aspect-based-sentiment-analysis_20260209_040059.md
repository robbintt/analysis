---
ver: rpa2
title: Large Language Models for Czech Aspect-Based Sentiment Analysis
arxiv_id: '2508.07860'
source_url: https://arxiv.org/abs/2508.07860
tags:
- sentiment
- llms
- aspect
- czech
- absa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates 19 large language models (LLMs) on Czech aspect-based
  sentiment analysis (ABSA), covering zero-shot, few-shot, and fine-tuning scenarios
  across four tasks. Results show that fine-tuned LLMs achieve state-of-the-art performance,
  outperforming previous models by up to 10% in target-aspect-sentiment detection.
---

# Large Language Models for Czech Aspect-Based Sentiment Analysis

## Quick Facts
- **arXiv ID**: 2508.07860
- **Source URL**: https://arxiv.org/abs/2508.07860
- **Reference count**: 39
- **Primary result**: Fine-tuned LLMs achieve state-of-the-art performance on Czech ABSA, outperforming previous models by up to 10% in target-aspect-sentiment detection.

## Executive Summary
This study evaluates 19 large language models on Czech aspect-based sentiment analysis across four tasks: aspect category sentiment analysis (ACSA), aspect category target extraction (ACTE), end-to-end ABSA (E2E-ABSA), and target-aspect-sentiment detection (TASD). Results show that fine-tuned LLMs achieve state-of-the-art performance, with QLoRA-based fine-tuning providing up to 10% improvement over previous models. Small, domain-specific ABSA models outperform general-purpose LLMs in zero-shot and few-shot settings. Error analysis reveals that aspect term prediction, particularly for implicit aspects and vernacular expressions, remains the primary challenge. The study identifies multilingualism, model size, and recency as significant factors influencing performance.

## Method Summary
The study evaluates 19 LLMs (from 1B to 13B parameters) across zero-shot, few-shot (10 examples), and fine-tuning scenarios using the CsRest-M dataset. Fine-tuning employs QLoRA with 4-bit quantization, LoRA adapters (rank r=64, alpha α=16 or 128 for Gemma), and AdamW optimizer. The evaluation uses strict Micro F1-score requiring exact triplet matches. Prompts are provided in both English and Czech, with English generally performing better. Error analysis focuses on implicit aspect detection and vernacular expression classification challenges.

## Key Results
- Fine-tuned LLMs achieve state-of-the-art performance, outperforming previous models by up to 10% in target-aspect-sentiment detection.
- Small, domain-specific ABSA models outperform general-purpose LLMs in zero-shot and few-shot settings.
- Multilingualism, model size, and recency significantly influence performance.
- Aspect term prediction is the primary error source, especially for implicit aspects and vernacular expressions.
- Fine-tuned models produce the fewest errors, confirming their superiority for Czech ABSA.

## Why This Works (Mechanism)

### Mechanism 1: Parameter-Efficient Fine-Tuning (QLoRA) for Structured Alignment
Fine-tuning LLMs with QLoRA significantly outperforms zero-shot and few-shot baselines by optimizing trainable LoRA adapters while keeping base model weights frozen and quantized. This adaptation maps Czech vernacular and complex aspect triplets to strict Python-list output formats, minimizing generation errors. Performance gains stem from adapter adaptation to the specific syntactic structure of the ABSA task rather than merely seeing the data.

### Mechanism 2: Cross-Lingual Instruction Following
General-purpose LLMs often perform better when prompted in English rather than Czech, even for Czech text analysis. Instruction-tuned models possess stronger adherence to English prompts due to pre-training dominance. This separates the "reasoning language" from the "content language," maximizing the model's reasoning capability while processing Czech input text.

### Mechanism 3: Few-Shot Demonstration for Implicit Aspect Disambiguation
Few-shot examples help models identify implicit aspects by conditioning the model's generation probability distribution and narrowing the search space for valid outputs. This specifically helps the model learn the abstract concept of "implicitness" (e.g., "Tasty!" → null/food quality), which is otherwise counter-intuitive for next-token prediction trained on explicit text.

## Foundational Learning

- **Concept: Target-Aspect-Sentiment Detection (TASD)**
  - **Why needed here**: This is the most complex compound task evaluated, requiring simultaneous extraction of three sentiment elements (term, category, polarity).
  - **Quick check question**: If a review says "Great beer," what is the aspect term, aspect category, and polarity? (Answer: "beer", "drinks quality", "positive").

- **Concept: QLoRA (Quantized Low-Rank Adaptation)**
  - **Why needed here**: The paper relies on this to fine-tune large models (up to 13B) on a single consumer-grade GPU (NVIDIA L40).
  - **Quick check question**: Why does QLoRA reduce memory usage compared to full fine-tuning? (Answer: It backpropagates gradients only to LoRA adapters while keeping the base model weights frozen and quantized to 4-bit).

- **Concept: Micro F1-Score**
  - **Why needed here**: This is the strict evaluation metric used. It requires an exact match of the entire triplet to count as correct.
  - **Quick check question**: Why is Micro F1 considered "strict" in this context? (Answer: A prediction counts as a True Positive only if the aspect term, category, AND sentiment polarity all exactly match the ground truth simultaneously).

## Architecture Onboarding

- **Component map**: Input Layer (Czech Review Text + Prompt) -> Model Backbone (4-bit Quantized LLM with optional LoRA adapters) -> Output Parser (Python list parser extracting tuples) -> Evaluator (Strict Micro F1-score calculator)

- **Critical path**: 
  1. Select model (Recommendation: Gemma 3 12B or LLaMA 3.1 8B for best efficiency/performance balance)
  2. Format inputs using standardized English prompt template
  3. If fine-tuning: Configure QLoRA (r=64, α=16 or 128 for Gemma)
  4. Run inference using Greedy Decoding (temperature=0)

- **Design tradeoffs**:
  - Prompt Language: English prompts generally yield better instruction adherence; Czech prompts occasionally help but are inconsistent
  - Model Size vs. Few-Shot: Smaller models (1B-4B) fail catastrophically at zero-shot but recover with few-shot examples. Larger models (>12B) are more robust to zero-shot settings
  - Fine-tuning vs. Zero-Shot: Fine-tuning provides ~10% gain on hardest tasks but requires labeled data

- **Failure signatures**:
  - Vernacular Mismatch: Models misclassify casual Czech terms (e.g., predicting "negative" for "Fajn" which is actually "positive")
  - Code Injection: Very small multilingual models (e.g., LLaMA 3.2 1B/3B) may generate Python code instead of requested output format
  - Base Form Lemmatization: Models may predict nominative singular form (e.g., "obsluha") even if text contains different case ("obsluhou"), causing strict match failure

- **First 3 experiments**:
  1. **Baseline Validation**: Run Gemma 3 12B with English prompts on test set using Zero-Shot and 10-Shot settings to establish performance floor
  2. **Adapter Tuning**: Fine-tune LLaMA 3.1 8B using QLoRA on training set; specifically tune LoRA α parameter to stabilize loss convergence
  3. **Error Analysis**: Isolate 100 random test samples and specifically count "Implicit Aspect" failures to determine if few-shot examples actually reduce this specific error type

## Open Questions the Paper Calls Out

- **Open Question 1**: How can evaluation metrics be adapted to fairly assess LLM predictions in morphologically rich languages like Czech without penalizing valid morphological variants (e.g., base form vs. inflected form)?
- **Open Question 2**: Why do English prompts frequently outperform native Czech prompts for Czech ABSA, and what prompt engineering strategies can mitigate this inconsistency?
- **Open Question 3**: Can specialized pre-training or fine-tuning significantly improve LLM detection of vernacular expressions (Common Czech) and implicit aspect terms?

## Limitations
- The strict Micro F1-score metric may obscure model strengths in partial predictions and is highly sensitive to lemmatization mismatches
- English prompts outperforming Czech prompts may not generalize to other low-resource languages or models with Czech-centric instruction tuning
- Several critical implementation details are underspecified, including random seeds for fine-tuning runs and exact masking logic for loss calculation

## Confidence
- **High Confidence**: Fine-tuned LLMs achieving state-of-the-art performance (up to 10% improvement) on Czech ABSA is well-supported by multiple experiments across four tasks
- **Medium Confidence**: Smaller, domain-specific ABSA models outperforming general-purpose LLMs in zero-shot settings is supported but could benefit from testing additional model families
- **Low Confidence**: The claim about English prompts consistently outperforming Czech prompts may be context-dependent and requires validation across different model architectures

## Next Checks
1. **Lemmatization-Aware Evaluation**: Re-run evaluation using lemmatized matching metric alongside strict Micro F1-score to determine whether grammatical case mismatches are artificially depressing model performance scores
2. **Cross-Lingual Prompt Robustness**: Test English vs. Czech prompt hypothesis on additional model families, including models with Czech-specific instruction tuning, to determine whether observed pattern holds across different architectures
3. **Implicit Aspect Quantitative Analysis**: Extract and quantify specific subset of test examples containing implicit aspects, then measure exact performance difference between zero-shot, few-shot, and fine-tuned models on this subset to validate few-shot improvement claims