---
ver: rpa2
title: Leo Breiman, the Rashomon Effect, and the Occam Dilemma
arxiv_id: '2507.03884'
source_url: https://arxiv.org/abs/2507.03884
tags:
- breiman
- data
- rashomon
- have
- effect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper revisits Leo Breiman's "Two Cultures" paper from 2001,
  contrasting data models (modeling data generation) with algorithmic models (machine
  learning). The author argues that after 25 years of advances in computing, several
  of Breiman's key claims no longer hold.
---

# Leo Breiman, the Rashomon Effect, and the Occam Dilemma

## Quick Facts
- arXiv ID: 2507.03884
- Source URL: https://arxiv.org/abs/2507.03884
- Reference count: 5
- Primary result: The Occam Dilemma (accuracy-simplicity tradeoff) is largely nullified by the Rashomon Effect in noisy tabular data problems

## Executive Summary
This paper revisits Leo Breiman's "Two Cultures" paper from 2001, arguing that advances in computing over the past 25 years have invalidated several of Breiman's key claims. The author demonstrates that the "Occam Dilemma" - the supposed tradeoff between accuracy and simplicity in machine learning models - is largely nullified by the "Rashomon Effect," which ensures that simpler-but-accurate models exist alongside more complex ones for most noisy problems. Through empirical evidence and theoretical arguments, the paper shows that interpretable machine learning methods (like sparse decision trees and generalized additive models) can achieve accuracy comparable to black-box models while maintaining interpretability. Additionally, the paper argues that causality can be investigated through model-free methods rather than relying on single interpretable models.

## Method Summary
The paper employs a two-pronged approach: theoretical argumentation about the Rashomon Effect and empirical validation using noisy tabular datasets. For interpretable models, the author uses globally optimized sparse decision trees (GOSDT, Dl85) and sparse generalized additive models rather than heuristic methods like CART. Black-box baselines include random forests, neural networks, and boosting methods. The evaluation uses cross-validation to compare test prediction accuracy between interpretable and black-box models, demonstrating that interpretable models achieve within 1-2% of black-box accuracy for most noisy problems. The key insight is that noise in data increases the variance in loss, creating a large Rashomon set where simple models can be competitive with complex ones.

## Key Results
- The Occam Dilemma (accuracy-simplicity tradeoff) is largely nullified by the Rashomon Effect in noisy tabular data
- Interpretable models like sparse decision trees and GAMs achieve accuracy comparable to black-box models (within ~1-2%) for most noisy problems
- The Rashomon Effect ensures that simpler-but-accurate models exist alongside more complex ones when noise is present
- Causality can be investigated through model-free methods rather than relying on single interpretable models

## Why This Works (Mechanism)
The paper's central mechanism is the Rashomon Effect, which occurs when multiple models achieve similar accuracy on a dataset. In noisy problems, the variance in loss increases with noise levels, creating a large set of models that are all approximately equally good. Within this Rashomon set, simpler models must be competitive with complex ones because if they weren't, they wouldn't be in the set. The paper argues that this effect is particularly strong in noisy tabular data where the outcome y given x is random, making it difficult for complex models to extract additional predictive signal beyond what simpler models can capture.

## Foundational Learning

**Rashomon Effect**: The existence of many models that achieve similar accuracy on a dataset. Why needed: This is the core mechanism that nullifies the Occam Dilemma. Quick check: Can be verified by enumerating models near the optimal accuracy using tools like [XZC+22; ZCL+23].

**Occam Dilemma**: The supposed tradeoff between model accuracy and simplicity. Why needed: This is the traditional belief the paper challenges. Quick check: Compare accuracy of simple vs complex models on noisy datasets to see if tradeoff exists.

**Noisy vs Deterministic Problems**: In noisy problems, y given x is random, while in deterministic problems, the relationship is fixed. Why needed: The Rashomon Effect is stronger in noisy problems. Quick check: Verify datasets have bounded achievable accuracy well below 100%.

**Globally Optimized Interpretable Models**: Methods like GOSDT and sparse GAMs that find optimal solutions rather than using greedy heuristics. Why needed: Ensures fair comparison with black-box models. Quick check: Verify optimization method used (global vs heuristic like CART).

## Architecture Onboarding

**Component Map**: Noisy tabular dataset -> Globally optimized interpretable models (GOSDT, sparse GAMs) -> Black-box baselines (RF, XGBoost, NN) -> Cross-validation comparison -> Accuracy analysis

**Critical Path**: Dataset preprocessing -> Model training (interpretable and black-box) -> Cross-validation -> Accuracy comparison -> Rashomon set analysis

**Design Tradeoffs**: The paper trades off model complexity for interpretability while maintaining accuracy. The key tradeoff is between using globally optimized interpretable methods (which are computationally intensive) versus heuristic methods that may be suboptimal.

**Failure Signatures**: If interpretable models underperform significantly, likely causes include insufficient optimization (using CART instead of globally optimal methods) or excessive sparsity constraints. If testing on non-noisy data, deep learning may genuinely outperform interpretable models.

**First Experiments**:
1. Train sparse decision tree using GOSDT on a noisy tabular dataset and verify it achieves within 1-2% of random forest accuracy
2. Train sparse GAM and compare its accuracy to XGBoost on the same dataset
3. Enumerate models near optimal accuracy to quantify the Rashomon set size

## Open Questions the Paper Calls Out

**Open Question 1**: To what extent does the Rashomon Effect exist in non-noisy problem domains (e.g., computer vision, NLP), and does it support interpretable models there as it does for noisy tabular data? The paper notes this may not exist to the same extent in these domains, but empirical validation is needed.

**Open Question 2**: Under what theoretical conditions does the approximation capability of simple function classes fail to capture the Rashomon set of complex models? The paper assumes simple functions can represent complex ones sufficiently well, but the boundaries where this fails remain undefined.

**Open Question 3**: Is the inability of deep learning to outperform interpretable models on noisy data a fundamental theoretical constraint, or a limitation of current regularization techniques? The paper asserts this as a fact, but future architectures might navigate the Rashomon set differently.

## Limitations
- Claims rely heavily on the assumption that most real-world tabular problems are sufficiently noisy
- Exact thresholds for "large enough" Rashomon sets are not rigorously defined
- Empirical claims depend on implementation details and dataset choices not fully specified

## Confidence
- **High confidence**: The theoretical argument that Occam's razor applies within the Rashomon set
- **Medium confidence**: Empirical claims about specific datasets and methods achieving similar accuracy
- **Medium confidence**: The claim that causality can be achieved without interpretable models

## Next Checks
1. Systematically characterize the Rashomon Effect size across diverse tabular datasets and validate the claim that it's typically "large enough" to contain simple models
2. Replicate the accuracy comparisons using the exact tools and datasets, verifying that globally optimized interpretable methods achieve within the claimed margin of black-box models
3. Test the model-free causality approaches on known causal relationships to verify they can recover correct causal conclusions without interpretable models