---
ver: rpa2
title: Continuous Monitoring of Large-Scale Generative AI via Deterministic Knowledge
  Graph Structures
arxiv_id: '2509.03857'
source_url: https://arxiv.org/abs/2509.03857
tags:
- evaluation
- structural
- deterministic
- metrics
- hallucination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a real-time evaluation framework for Large
  Language Models (LLMs) using Knowledge Graphs (KGs). It contrasts a deterministic,
  rule-based KG with an LLM-generated KG derived from live news streams, leveraging
  metrics like Instantiated Class Ratio (ICR), Instantiated Property Ratio (IPR),
  and Class Instantiation (CI) to quantify structural fidelity.
---

# Continuous Monitoring of Large-Scale Generative AI via Deterministic Knowledge Graph Structures

## Quick Facts
- arXiv ID: 2509.03857
- Source URL: https://arxiv.org/abs/2509.03857
- Authors: Kishor Datta Gupta; Mohd Ariful Haque; Hasmot Ali; Marufa Kamal; Syed Bahauddin Alam; Mohammad Ashiqur Rahman
- Reference count: 17
- One-line primary result: A real-time evaluation framework using Knowledge Graphs to monitor LLM reliability, detecting structural drift and hallucinations through comparison with deterministic baselines.

## Executive Summary
This paper introduces a framework for continuous monitoring of Large Language Models (LLMs) using Knowledge Graphs (KGs) to detect semantic drift and hallucinations. It constructs parallel KGs from real-time news streams—one deterministic (rule-based) and one LLM-generated—and compares their structural metrics. The approach uses Instantiated Class Ratio (ICR), Instantiated Property Ratio (IPR), and Class Instantiation (CI) to quantify structural fidelity, while a Hallucination Score based on entity tracing and rule conformance validates semantic accuracy. Anomaly detection with dynamic thresholds enables proactive alerting. Experiments across nine LLMs show varying structural alignment and hallucination rates, with some models demonstrating more consistent behavior than others.

## Method Summary
The framework employs a three-phase pipeline: (1) construct parallel KGs from the same news stream using deterministic rules versus LLM triple extraction, (2) compute structural metrics (ICR, IPR, CI) against a baseline and calculate hallucination scores, and (3) detect anomalies using weighted metric deviations with dynamic thresholds based on historical distributions. The deterministic KG serves as a pseudo-ground-truth baseline for structural comparison, enabling vendor-agnostic monitoring of LLM reliability in dynamic environments.

## Key Results
- The deterministic KG baseline approach successfully identifies structural deviations in LLM-generated KGs across multiple models
- Models like Gemini-1.5 and Vicuna showed more consistent structural behavior compared to others
- The dynamic threshold mechanism effectively flagged significant structural shifts while filtering out normal variance
- Hallucination scores varied significantly across models, with some showing higher rates of schema violations or source disconnects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Comparing structural topology of LLM-generated KG against deterministic KG provides scalable proxy for detecting semantic drift and hallucinations
- **Mechanism:** Two parallel KGs constructed from same news stream; deviations in ICR, IPR, and CI between them indicate underutilization of schema or ontologically inconsistent relationships
- **Core assumption:** Deterministic KG serves as valid baseline; incomplete dictionaries cause noisy deviation signals
- **Evidence anchors:** Abstract states framework "compares them using structural metrics" and "automated drift detector flags anomalies"; Step II notes "sudden drops or unusual patterns immediately signal potential issues"
- **Break condition:** Domain-specific jargon not covered by deterministic dictionary renders comparison invalid (false positive drift)

### Mechanism 2
- **Claim:** Anomaly detection using dynamic thresholds based on historical metric distributions enables proactive alerting for model reliability degradation
- **Mechanism:** Anomaly score computed as weighted sum of metric deviations; threshold αt = μA(t) + λσA(t) adapts based on historical mean and standard deviation
- **Core assumption:** Metric deviations follow distribution where statistical outliers correspond to reliability failures
- **Evidence anchors:** Abstract mentions "automated drift detector with dynamic thresholds flags anomalies"; Step III states KG flagged anomalous if A(Gt) > αt
- **Break condition:** Legitimate fine-tuning update causing style change initially flagged as anomaly (false positive)

### Mechanism 3
- **Claim:** Schema-conformance validation and source tracing quantify "Hallucination Score" by identifying entities lacking grounding in input or ontology
- **Mechanism:** Entities validated via NER consistency and SPARQL queries against ontology; entities not mappable to schema or traceable to source counted as hallucinated
- **Core assumption:** Hallucination defined as schema violation or source disconnect; subtle semantic errors fitting schema may not be caught
- **Evidence anchors:** Abstract lists "addressing hallucinations, semantic drift, and bias"; Step II defines hallucinated instances as "neither present in input source nor mapped to KG schema"
- **Break condition:** Valid truthful fact not in source but fitting schema incorrectly classified as hallucination (false positive)

## Foundational Learning

- **Concept: RDF Triples & Ontologies**
  - **Why needed here:** Core metrics measure LLM's population of specific ontology; understanding KG structure (subject-predicate-object triples) essential to interpret "Instantiated Class" vs "Property"
  - **Quick check question:** If LLM extracts "Apple" as "Company" but ontology only defines "Apple" as "Fruit," would this increase Hallucination Score? (Answer: Yes, due to schema mismatch)

- **Concept: Statistical Drift Detection**
  - **Why needed here:** Framework uses dynamic thresholds (μ + λσ) rather than static rules; statistical process control understanding helps tune sensitivity parameter λ
  - **Quick check question:** If historical anomaly scores are consistently low (low σ), will threshold become more or less sensitive to sudden spike? (Answer: More sensitive, as threshold tightens around low mean)

- **Concept: Rule-based vs. Generative Extraction**
  - **Why needed here:** Framework relies on deterministic KG as baseline; understanding trade-offs essential (rule-based systems rigid but reproducible; LLMs flexible but stochastic)
  - **Quick check question:** Why is deterministic baseline preferred over "ground truth" human-labeled dataset for continuous monitoring? (Answer: Scalability and speed; human labeling cannot keep up with real-time news streams)

## Architecture Onboarding

- **Component map:** Live News Stream (BBC, Reuters) → [Text Batch] → Parallel Construction: Deterministic Pipeline (Dictionary/Regex NER → Rule-based Triple Extractor → Gbase) AND LLM Pipeline (Prompts + Text Batch → LLM API → GLLM) → Evaluation Engine (Computes ICR, IPR, CI for both graphs → Calculates Δ) → Monitoring Layer (Anomaly Score Calculator → Dynamic Threshold Check → Alert/Dashboard)

- **Critical path:** Deterministic Pipeline is bottleneck for validity; if dictionary or ontology poorly defined, Gbase will be sparse, causing LLM's "Hallucination Score" to appear artificially high (penalizing valid LLM inferences not in dictionary)

- **Design tradeoffs:**
  - Static vs. Dynamic Baseline: Deterministic dictionary updated "regularly" (Section 3) but static relative to live stream
  - Heuristic vs. Semantic Hallucination: Section 5 admits reported results used "simplified heuristic pipeline"; full semantic validation (SPARQL/Reasoning) more accurate but compute-intensive
  - Assumption: Framework assumes LLMs are "black boxes" accessed via API, preventing weight inspection

- **Failure signatures:**
  - Metric Collapse: IPR drops to near 0 → LLM likely ignoring relation extraction prompts
  - Baseline Divergence: Gbase empty but GLLM full → Input stream format changed, breaking Regex/Deterministic parser
  - High False Positives: High Hallucination Score during breaking news events → Dictionary out of date for new entities

- **First 3 experiments:**
  1. Baseline Validation: Feed 50 known articles into Deterministic Pipeline; verify Regex/Dictionary extracts >90% of expected entities to ensure "ground truth" reliability
  2. Static Model Benchmark: Run same news batch through two different LLMs (e.g., GPT-3.5 vs. Gemini); compare their ICR/IPR scores to see which better adheres to provided ontology schema
  3. Drift Injection: Manually degrade LLM prompt (e.g., remove instructions about relations); verify if Anomaly Score A(Gt) successfully breaches dynamic threshold αt within next 3-5 batches

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can quantile-based or Extreme Value Theory (EVT)-based thresholds improve anomaly detection robustness over current Gaussian assumption for dynamic threshold calculation?
- **Basis in paper:** [explicit] Future work section states: "strengthen robustness via quantile/EVT-based thresholds and sensitivity analyses" and Threats section notes "non-Gaussian nature of metric distributions"
- **Why unresolved:** Current threshold formula (αt = μ + λσ) assumes approximate normality, which may not hold for structural metric distributions, potentially causing false alarms or missed drifts
- **What evidence would resolve it:** Empirical comparison of detection accuracy (precision/recall) across Gaussian, quantile, and EVT-based thresholds on held-out temporal data with labeled anomaly events

### Open Question 2
- **Question:** How can detected drift be attributed to its source—model parameter updates, prompt configuration changes,