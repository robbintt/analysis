---
ver: rpa2
title: 'Through a Compressed Lens: Investigating The Impact of Quantization on Factual
  Knowledge Recall'
arxiv_id: '2505.13963'
source_url: https://arxiv.org/abs/2505.13963
tags:
- quantization
- person
- qwen2
- knowledge
- bib4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how quantization affects factual knowledge
  recall (FKR) in large language models. Using three quantization techniques at 4-bit
  and 8-bit precision, the authors evaluate three models on two tasks: knowledge memorization
  and latent multi-hop reasoning.'
---

# Through a Compressed Lens: Investigating The Impact of Quantization on Factual Knowledge Recall

## Quick Facts
- **arXiv ID**: 2505.13963
- **Source URL**: https://arxiv.org/abs/2505.13963
- **Reference count**: 40
- **Primary result**: Quantization typically reduces factual knowledge recall, with smaller models and late layers most affected, though occasional improvements occur.

## Executive Summary
This paper investigates how quantization affects factual knowledge recall (FKR) in large language models. Using three quantization techniques at 4-bit and 8-bit precision, the authors evaluate three models on two tasks: knowledge memorization and latent multi-hop reasoning. They find that quantization typically reduces FKR, with smaller models within a model family experiencing more pronounced effects. However, quantized models at lower bit precision do not consistently underperform higher-bit models, and quantization can occasionally even improve FKR. The BitSandBytes method preserves FKR most effectively. Across all experiments, quantization causes modest performance degradation without undermining its viability as a compression strategy.

## Method Summary
The study evaluates three quantization methods (GPTQ, AWQ, and BitSandBytes) at 4-bit and 8-bit precision across three models (Llama3-8B, Qwen2.5-7B, Qwen2.5-14B). Two tasks assess FKR: knowledge memorization using the LRE dataset and latent multi-hop reasoning using TwoHop-Fact. The authors employ neuron-level attribution to identify contribution score drops and layer-wise analysis to localize degradation patterns. Experiments are conducted on A100/H100 GPUs, with neuron attribution taking ~10h and LMHR experiments ~30h per model.

## Key Results
- Quantization typically reduces FKR, with smaller models within a family experiencing more pronounced effects
- Late transformer layers suffer the most degradation in factual knowledge storage and retrieval
- BitSandBytes (BIB) quantization preserves FKR most effectively, with occasional improvements observed
- Multi-hop reasoning degrades primarily due to first-hop entity retrieval failures (up to 30% degradation)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantization-induced information loss concentrates in late transformer layers, degrading factual knowledge retrieval at its decisive storage locations.
- Mechanism: Weight precision reduction disproportionately reduces contribution scores of high-importance neurons in final layers (both attention and FFN sublayers), causing fewer neurons to exceed the contribution threshold required for reliable fact recall.
- Core assumption: Knowledge neurons in late layers encode decisive factual information; their reduced contribution directly translates to recall failures.
- Evidence anchors:
  - [abstract] "quantization typically results in information loss within LLMs, consequently diminishing their capacity for FKR"
  - [section 5.1] "both layer-level and neuron-level analyses reveal that quantization primarily affects the network's last layers"
  - [corpus] Weak direct corpus support; neighbor papers address factual robustness but not layer-wise degradation patterns
- Break condition: If factual knowledge in your target model family is primarily stored in early/middle layers (architecture-dependent), late-layer degradation may not predict FKR loss.

### Mechanism 2
- Claim: Quantization disrupts latent multi-hop reasoning primarily by impairing first-hop entity retrieval, which cascades to full query failure.
- Mechanism: The first reasoning step (r1: recalling bridge entity e2 from e1) degrades by up to 30% under quantization, while the second hop shows only ~4% degradation; failure to retrieve the bridge entity prevents downstream composition.
- Core assumption: Multi-hop factual queries require successful sequential retrieval; first-hop failure is deterministic for overall failure.
- Evidence anchors:
  - [section 5.2] "quantization substantially affects the first hop r1(e1), by as much as 30.08%, while its impact on the second hop is minimal"
  - [section 5.2] "Spearman's correlation of 0.93" between r1 prediction accuracy and overall r2(r1(e1)) deterioration
  - [corpus] No direct corpus validation; neighbor papers on factual robustness do not address multi-hop decomposition
- Break condition: If your use case involves primarily single-hop queries or cached intermediate entities, first-hop degradation becomes less critical.

### Mechanism 3
- Claim: Quantization can paradoxically improve FKR through regularization-like effects, though this is method- and architecture-dependent.
- Mechanism: Quantization-induced noise or implicit regularization may suppress overconfident incorrect predictions, marginally improving recall on specific relations—observed with BitSandBytes (BIB) quantization on Qwen2.5-14B.
- Core assumption: Some factual knowledge is stored in a way that benefits from precision reduction (e.g., noise suppresses spurious activations).
- Evidence anchors:
  - [section 5.2] "quantization can occasionally even improve FKR"
  - [section 5.2] "bib4/8 likewise outperforms other quantization methods, best preserving and occasionally improving FKR (Figure 12)"
  - [corpus] No external validation; this remains an observation requiring further investigation
- Break condition: If your model/architecture shows consistent degradation without improvement, this regularization effect is not operating.

## Foundational Learning

- Concept: **Knowledge Neurons Theory**
  - Why needed here: The paper's neuron-level attribution analysis assumes specific neurons encode specific facts; understanding this is prerequisite to interpreting contribution score drops.
  - Quick check question: Can you explain why reducing a neuron's contribution score might prevent a model from recalling "Paris is the capital of France"?

- Concept: **Post-Training Quantization (PTQ) vs. Quantization-Aware Training (QAT)**
  - Why needed here: The paper evaluates only PTQ methods; knowing this distinction clarifies why no retraining occurs and why degradation patterns emerge.
  - Quick check question: Why would PTQ be expected to cause more FKR degradation than QAT?

- Concept: **Latent Multi-hop Reasoning**
  - Why needed here: The TwoHop-Fact task requires understanding bridge entities (e.g., e2 in ((e1, r1, e2), (e2, r2, e3))); this is core to interpreting the first-hop vs. second-hop findings.
  - Quick check question: In the query "What is the birth year of the singer of 'Superstition'?", what is the bridge entity and which hop is most vulnerable to quantization?

## Architecture Onboarding

- Component map: Transformer layers (28-36) with attention sublayers + FFN sublayers per layer → Knowledge neurons distributed across layers with concentration in late layers for decisive factual output → Quantization applied to weights only → Neuron attribution analysis to identify contribution score drops → Layer-wise analysis to localize degradation

- Critical path: Full-precision model → Apply PTQ method (GPTQ/AWQ/BIB) at 4-bit or 8-bit → Evaluate on LRE (memorization) and TwoHop-Fact (multi-hop) → Neuron attribution to identify contribution score drops → Layer-wise analysis to localize degradation

- Design tradeoffs:
  - BIB8: Best FKR preservation, modest compression
  - BIB4: Good preservation, stronger compression, acceptable for most applications
  - GPTQ4/AWQ4: Higher degradation, especially in smaller models, but wider ecosystem support
  - Smaller models (7B vs 14B) in same family suffer disproportionately more FKR loss

- Failure signatures:
  - Sharp accuracy drop on relations where full-precision performance has not saturated (e.g., "person father" at 45% vs. "country capital city" at 100%)
  - First-hop entity recall degradation >15% indicates potential multi-hop reasoning collapse
  - Contribution score drop >0.3 in final 2-4 layers (Qwen2.5) or middle-to-late layers (Llama3)

- First 3 experiments:
  1. **Baseline FKR assessment**: Run your full-precision model on LRE and a domain-specific subset of TwoHop-Fact to establish reference accuracy per relation type.
  2. **Quantization method comparison**: Apply BIB8, BIB4, GPTQ4, and AWQ4 to your target model; measure FKR accuracy degradation and identify which relations degrade most (focus on unsaturated relations).
  3. **Layer-wise contribution profiling**: For 3-5 high-degradation relations, compute neuron contribution scores before and after quantization; verify if degradation localizes to late layers (Qwen2.5 pattern) or middle-to-late layers (Llama3 pattern) to predict architecture-specific vulnerability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does quantization impact factual knowledge recall (FKR) in multilingual settings compared to the English-only results observed in this study?
- Basis in paper: [explicit] The authors state in the Limitations section that their work is confined to English datasets and that "extending experiments to the multilingual settings is considered for future work," noting that multilingual FKR may be affected by simultaneous degradation of multilingual capabilities.
- Why unresolved: It remains unclear if the layer-wise degradation patterns (e.g., information loss in final layers) apply uniformly across languages or if lower-resource languages suffer disproportionate factual recall loss due to the compounding effects of compression on multilingual representations.
- What evidence would resolve it: A replication of the Knowledge Memorization and Latent Multi-hop Reasoning analyses on multilingual models (e.g., Llama3-multilingual or Qwen-multilingual) across a diverse set of languages to compare accuracy drops and neuron distribution shifts.

### Open Question 2
- Question: To what extent does weight-activation quantization or KV cache compression degrade factual knowledge recall compared to the weight-only post-training quantization (PTQ) evaluated here?
- Basis in paper: [explicit] The authors explicitly list "weight-activation quantization, KV cache compression, or quantization-aware training techniques" as out of scope in the Limitations section, limiting the study to weight-only PTQ.
- Why unresolved: While weight quantization primarily affects stored knowledge, KV cache and activation quantization impact the model's processing of context and reasoning path. It is unknown if these methods disrupt the "bridge entity" recall in multi-hop reasoning more severely than weight quantization.
- What evidence would resolve it: Experiments applying the paper's Latent Multi-hop Reasoning Analysis (LMHR) metrics (ENTREC and CNSTSCORE) to models quantized via methods like SmoothQuant (weight-activation) or KIVI (KV cache).

### Open Question 3
- Question: What is the causal mechanism behind the observed occasional improvements in FKR performance in lower-bit quantized models?
- Basis in paper: [inferred] The authors observe that "quantization can occasionally even enhance model FKR" (e.g., in Figure 3 and Section 5.2) and hypothesize it may be due to a "regularization effect or quantization-induced noise," but they do not verify this hypothesis.
- Why unresolved: The study identifies the phenomenon but does not isolate whether the improvement stems from the removal of redundant/noisy weights (regularization) or a specific interaction between the quantization grid and the model's latent space.
- What evidence would resolve it: An ablation study comparing the effects of random noise injection versus structured quantization on FKR accuracy, or an analysis of the signal-to-noise ratio in specific knowledge neurons that show improved contribution scores.

### Open Question 4
- Question: How does factual knowledge recall degrade under extreme sub-4-bit quantization (e.g., 1-bit or 2-bit)?
- Basis in paper: [explicit] The authors note in the Limitations section that "Lower-bit quantization, such as 1-bit or 2-bit, is not included in our study."
- Why unresolved: The paper establishes that 4-bit quantization causes modest degradation, but the relationship may be non-linear. It is uncertain if knowledge neurons are completely destroyed or if the model's ability to perform multi-hop reasoning collapses entirely at extreme compression levels.
- What evidence would resolve it: Applying the neuron-level attribution methods described in the paper to 1-bit or 2-bit models (e.g., BitNet b1.58) to measure the survival rate of top-k contributing neurons.

## Limitations
- Study is limited to English datasets, with multilingual FKR effects remaining unexplored
- Only evaluates weight-only post-training quantization, excluding weight-activation, KV cache compression, and quantization-aware training methods
- Does not investigate extreme sub-4-bit quantization levels (1-bit or 2-bit)
- The neuron attribution method, while innovative, remains unverified against alternative factual robustness analyses

## Confidence

- **High Confidence**: General trend that quantization degrades FKR, particularly for smaller models within a family
- **Medium Confidence**: Layer-wise localization of degradation (late layers for Qwen2.5, middle-to-late for Llama3)
- **Medium Confidence**: First-hop entity retrieval being most vulnerable in multi-hop reasoning
- **Low Confidence**: Quantization occasionally improving FKR through regularization effects

## Next Checks

1. **Cross-Architecture Validation**: Test the same quantization methods on additional model families (Mistral, Gemma, or open-source GPT-4 equivalents) to verify if layer-wise degradation patterns hold beyond Llama3 and Qwen2.5

2. **Alternative Attribution Method**: Replicate the neuron contribution score analysis using a different factual knowledge attribution method (e.g., causal intervention or direct logit attribution) to confirm the observed degradation patterns are not artifacts of the specific attribution technique

3. **Domain-Specific Impact Assessment**: Evaluate quantization effects on factual knowledge recall in specialized domains (medical, legal, or technical) where factual accuracy is critical, to determine if certain knowledge domains are more vulnerable to quantization-induced degradation