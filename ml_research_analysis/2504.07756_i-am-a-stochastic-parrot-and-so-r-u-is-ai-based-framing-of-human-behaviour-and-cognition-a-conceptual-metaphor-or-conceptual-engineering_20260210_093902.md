---
ver: rpa2
title: '"i am a stochastic parrot, and so r u": Is AI-based framing of human behaviour
  and cognition a conceptual metaphor or conceptual engineering?'
arxiv_id: '2504.07756'
source_url: https://arxiv.org/abs/2504.07756
tags:
- conceptual
- concepts
- human
- metaphor
- engineering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# "i am a stochastic parrot, and so r u": Is AI-based framing of human behaviour and cognition a conceptual metaphor or conceptual engineering?

## Quick Facts
- arXiv ID: 2504.07756
- Source URL: https://arxiv.org/abs/2504.07756
- Authors: Warmhold Jan Thomas Mollema; Thomas Wachter
- Reference count: 0
- Primary result: AI-human conceptual comparisons function as "double metaphors" that obscure their own metaphorical foundations

## Executive Summary
This philosophical analysis examines whether AI-based framings of human behavior and cognition constitute conceptual metaphors (systematic domain-to-domain projections) or conceptual engineering (deliberate boundary modifications). The authors argue that AI-human comparisons are best understood as conceptual metaphors that risk "conceptual substitution" over time, while also identifying conditions under which conceptual engineering might be warranted. The paper traces three epistemic failure modes of metaphor use—contingency, map-territory fallacy, and the double-metaphor problem—and outlines criteria for ethically justified conceptual engineering.

## Method Summary
The paper employs Wittgensteinian conceptual analysis to examine AI discourse, comparing conceptual metaphor theory (Lakoff & Johnson) with conceptual engineering frameworks (Cappelen, Haslanger). It analyzes textual cases like "stochastic parrot" and "stochastic toddler" framings to demonstrate how computational concepts historically borrowed from human cognition, then returned as projections onto humans, creating circular metaphorical structures. The analysis distinguishes between CM's projection mechanism (emphasizing similarities, suppressing differences) and CE's boundary modification (changing intensions/extensions). No empirical experiments are conducted; the methodology is purely philosophical argumentation.

## Key Results
- AI-human conceptual comparisons operate as "double metaphors" with circular foundations
- Habitual metaphor use causes "conceptual substitution" where models are mistaken for reality
- Three failure modes of conceptual metaphors: contingency, map-territory fallacy, and double metaphor problem
- Conceptual engineering is conditionally warranted but requires ethical justification and preservation of standing criteria

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI-to-human conceptual transfer operates as a "double metaphor" that obscures its own metaphorical foundations.
- Mechanism: Computational concepts (layer 2) were originally borrowed from human cognition (Turing's "human computer" modeled on cognitive states), then technical AI terms (layer 1) were charged with new meanings, and now these re-semanticized terms are projected back onto humans—creating a circular metaphor that appears non-metaphorical.
- Core assumption: Users of AI framing do not recognize the historical borrowing at computation's foundation.
- Evidence anchors:
  - [section 4.3]: "Turing used cognitive terms like 'state of mind', seeing and remembering... Turing machines [do not] demonstrate or possess cognitive abilities"
  - [section 4.3]: "INTELLIGENCE is ARTIFICIAL doubly metaphorical: firstly rooted in a metaphorical cognitive connection between the human computer and its mechanical formalisation"
- Break condition: If computational concepts are now formally independent of their cognitive origins, the double-metaphor critique weakens.

### Mechanism 2
- Claim: Sustained metaphor use causes "conceptual substitution"—mistaking model properties for target system properties.
- Mechanism: Metaphors succeed by emphasizing similarities and suppressing disanalogies (abstraction). Over time, habitual use leads to "conceptual forgetfulness" where suppressed differences are no longer recognized, and the metaphorical framing is treated as literal description.
- Core assumption: Users do not actively maintain awareness of what the metaphor suppresses.
- Evidence anchors:
  - [section 4.2]: "Overemphasis of one particular metaphorical model risks gradually eroding our awareness of the distinctions between source and target systems, to the point where 'we forget that they ever existed'"
  - [section 4.2]: "conceptual substitution occurs when AI systems are perceived as direct replacements for human cognition"
- Break condition: If explicit meta-awareness protocols are maintained (e.g., regularly documenting disanalogies), conceptual substitution is less likely.

### Mechanism 3
- Claim: Conceptual metaphors can signal opportunities for conceptual engineering, but only if specific criteria are met.
- Mechanism: The descriptive invariance revealed by AI-human comparisons can prompt three engineering moves: (1) amelioration—repartitioning logical space of concepts like "intelligence"; (2) expansion—introducing new hybrid concepts; (3) elimination—removing obsolete concepts. Each requires ethical justification, not just habituation.
- Core assumption: There exists a population Y whose understanding of phenomenon X would genuinely improve through the engineering move.
- Evidence anchors:
  - [section 5]: "If any form of CE is warranted, this should be because this new conceptual organisation of intensions and extensions yields better results in terms of a population Y's understanding of the phenomenon X"
  - [section 5.1]: Warns that extension-widening without criteria preservation "would simply obscure standing conceptual criteria"
- Break condition: If no ethical or epistemic improvement can be demonstrated for a specific population, the engineering move should not proceed.

## Foundational Learning

- Concept: **Conceptual Metaphor Theory (Lakoff & Johnson)**
  - Why needed here: The paper's core argument depends on distinguishing conceptual metaphors (systematic domain-to-domain projections) from casual metaphors, and evaluating their epistemic success criteria.
  - Quick check question: Can you name two of Kompa's six criteria for evaluating metaphor epistemic success?

- Concept: **Wittgensteinian Language-Games and Family Resemblance**
  - Why needed here: The paper grounds its contingency argument in Wittgenstein's view that concepts derive meaning from use-contexts, not fixed definitions—undermining claims that computational framing reveals "true" structure.
  - Quick check question: Why does family resemblance challenge the idea that concepts have defining necessary properties?

- Concept: **Conceptual Engineering (Cappelen, Haslanger)**
  - Why needed here: The alternative to metaphor-view requires understanding amelioration, expansion, and elimination as deliberate interventions with ethical stakes.
  - Quick check question: What distinguishes "epistemic amelioration" from "semantic amelioration"?

## Architecture Onboarding

- Component map: Observed AI-human comparisons -> CM vs CE analysis -> Three failure modes (contingency, map-territory fallacy, double metaphor) -> Conditional CE pathways (amelioration > expansion > elimination)

- Critical path:
  1. Identify the conceptual comparison being made
  2. Apply CM analysis: what similarities are emphasized? What disanalogies are suppressed?
  3. Check for double-metaphor structure (has the source domain borrowed from target historically?)
  4. If CE is proposed, verify: (a) specific population affected, (b) demonstrable epistemic/ethical improvement, (c) preservation of standing criteria
  5. Document non-philosophical commitments (economic, institutional) that may bias the framing

- Design tradeoffs:
  - CM view: Low intervention cost, but risks conceptual substitution over time
  - CE view: Higher justificatory burden, but avoids fallacies if criteria are met
  - Elimination: Highest risk of impoverishing conceptual resources; amelioration is safer

- Failure signatures:
  - Treating metaphor as literal (map-territory fallacy)
  - Extending concepts without preserving criteria (mereological fallacy)
  - Engineering driven by corporate narratives rather than epistemic improvement
  - Ignoring embodied/embedded differences between human and AI systems

- First 3 experiments:
  1. **Disanalogy audit**: For any AI-human comparison, explicitly list 5+ disanalogies the framing suppresses. Track whether awareness persists over repeated use.
  2. **Historical borrowing trace**: Map the genealogy of key computational concepts—did they originate in cognitive vocabulary? If so, document the circularity.
  3. **Population impact test**: Before any conceptual engineering move, specify: which population's understanding improves? By what metric? What criteria are preserved vs. discarded?

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific criteria determine if AI-based conceptual engineering is ethically desirable or merely feasible?
- Basis in paper: [explicit] Section 5.2 states, "Not every engineering opportunity is also desirable... the ethical question whether it is desirable to widen extensions, change intensions, or eliminate a concept has to be answered."
- Why unresolved: The authors identify a tension between engineering feasibility and conceptual ethics but do not formulate the normative framework required to resolve it, leaving the evaluation of concepts like "intelligence" open.
- What evidence would resolve it: A normative framework or set of principles that successfully distinguishes between beneficial conceptual ameliorations and harmful conceptual substitutions in the context of AI.

### Open Question 2
- Question: Which social groups or stakeholders actually benefit from the application of AI-based conceptual frames to human behavior?
- Basis in paper: [explicit] Section 5.2 asks, "it’s questionable for whom this conceptual engineering in terms of AI is useful," noting that positions are often tied to non-philosophical commitments like BigTech employment.
- Why unresolved: The paper suggests these commitments import presuppositions but does not empirically or theoretically isolate the beneficiaries of such conceptual shifts.
- What evidence would resolve it: Empirical analysis or a political-philosophical study identifying how specific conceptual constellations serve the interests of distinct groups (e.g., tech corporations vs. laypeople) over others.

### Open Question 3
- Question: How does the re-engineering of cognitive concepts alter the "moral circle" and the status of anthropo-exceptionalism?
- Basis in paper: [explicit] Section 5.2 asks, "What does the conceptual change mean in terms of the ethical relations that morph along with it?" specifically citing moral inclusion and "anthropo-exceptionalism."
- Why unresolved: While the paper notes that changing "intelligence" affects inclusion in the moral circle, it leaves the specific dynamics of these morphing ethical relations for future work.
- What evidence would resolve it: An analysis mapping changes in the definition of "intelligence" to specific changes in moral status, rights, or ethical obligations toward humans and machines.

## Limitations
- No empirical validation mechanisms—relies entirely on philosophical argumentation
- No operational criteria for distinguishing CM from CE in practice
- No concrete methodology for determining which populations would benefit from conceptual engineering

## Confidence
- High confidence in the logical structure distinguishing CM vs CE frameworks
- Medium confidence in the three failure modes (contingency, map-territory fallacy, double metaphor) as generalizable critiques
- Low confidence in practical applicability without operationalized validation criteria

## Next Checks
1. **Disanalogy persistence test**: For a sample of AI-human comparisons, track whether explicitly documented disanalogies remain cognitively accessible after repeated exposure over 4-6 weeks
2. **Genealogy mapping**: Systematically trace computational concept origins to verify historical borrowing from cognitive vocabulary across foundational AI literature
3. **Population-specific impact**: For any proposed conceptual engineering move, identify specific population(s) and measurable epistemic/ethical improvements with preserved criteria