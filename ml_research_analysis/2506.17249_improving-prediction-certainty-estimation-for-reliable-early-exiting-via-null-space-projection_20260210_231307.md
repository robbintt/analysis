---
ver: rpa2
title: Improving Prediction Certainty Estimation for Reliable Early Exiting via Null
  Space Projection
arxiv_id: '2506.17249'
source_url: https://arxiv.org/abs/2506.17249
tags:
- exiting
- early
- score
- prediction
- certainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of premature exiting in early
  exiting methods for accelerating pre-trained language models. Current methods overestimate
  prediction certainty by relying solely on class-relevant logits, ignoring the detrimental
  influence of class-irrelevant information in features.
---

# Improving Prediction Certainty Estimation for Reliable Early Exiting via Null Space Projection

## Quick Facts
- arXiv ID: 2506.17249
- Source URL: https://arxiv.org/abs/2506.17249
- Reference count: 9
- Average speed-up ratio of 2.19× across all GLUE benchmark tasks with negligible performance degradation

## Executive Summary
This paper addresses premature exiting in early exiting methods for accelerating pre-trained language models by introducing a Certainty-Aware Probability (CAP) score. The method combines class-relevant logits with a class-irrelevant Null Space Projection (NSP) score that measures the proportion of class-irrelevant information in features. By treating the scaled NSP score as a logit for a virtual UNK class, CAP creates a principled way to fuse both types of information through softmax, achieving superior trade-offs between task performance and inference efficiency.

## Method Summary
The method introduces CAP by computing the NSP score as the ratio of feature norms in the null space of the classifier's weight matrix. This score measures class-irrelevant information that degrades prediction certainty but is invisible to logits. The CAP score is defined as the softmax probability of a virtual UNK class constructed using the scaled NSP score. The method requires joint training of all internal classifiers with the backbone, using offset features to remove bias. Inference uses a threshold-based exiting strategy where samples exit when CAP falls below a tuned threshold.

## Key Results
- Achieves average speed-up ratio of 2.19× across all GLUE benchmark tasks
- Surpasses state-of-the-art ConsistentEE by 28% in speed-accuracy trade-offs
- Significantly reduces both premature and delayed exiting rates compared to logit-based methods

## Why This Works (Mechanism)

### Mechanism 1: Null Space Projection Captures Class-Irrelevant Uncertainty
Feature components orthogonal to classifier weights contain class-irrelevant information that degrades prediction certainty but is invisible to logits. Decompose feature x into x = x_W + x_W⊥ where W is the column space of classifier weights and W⊥ is the null space. The NSP score = ||x_W⊥|| / ||x|| measures the proportion of class-irrelevant information. Since W^T · x_W⊥ = 0, this component cannot affect logits but represents redundant/noisy information that reduces classification reliability.

### Mechanism 2: Virtual UNK Class Enables Unified Certainty Estimation
Treating the scaled NSP score as a logit for a virtual UNK class creates a principled way to fuse class-relevant and class-irrelevant information through softmax. Append l_0 = α · NSP(x) to original logits, then compute CAP = exp(α·NSP(x)) / [Σexp(l_i) + exp(α·NSP(x))]. CAP represents the probability the sample belongs to an "unknown" class. When class-irrelevant information is high (high NSP), CAP increases, indicating uncertainty.

### Mechanism 3: Balanced Error Rate Reduction
Combining class-relevant logits and class-irrelevant NSP information reduces both premature and delayed exiting errors simultaneously. Logit-only methods overestimate certainty (high premature exit rate). NSP-only would underestimate certainty (high delayed exit rate). CAP fusion balances both by penalizing high null-space content while respecting class similarity.

## Foundational Learning

- **Concept: Null Space and Orthogonal Decomposition**
  - Why needed here: Understanding how features decompose into classifier-relevant and irrelevant components is essential for grasping NSP computation.
  - Quick check question: Given a 768-dim feature and a classifier with weight matrix W ∈ R^(768×3), what is the dimensionality of the null space?

- **Concept: Softmax Scale Sensitivity**
  - Why needed here: The α parameter exists because softmax is scale-sensitive; understanding this explains why NSP cannot be directly concatenated with logits.
  - Quick check question: If logits are in range [-10, 10] and NSP is in [0, 1], what happens to softmax probabilities if you concatenate them without scaling?

- **Concept: Early Exiting Trade-offs (Speed vs. Accuracy)**
  - Why needed here: The method optimizes a speed-accuracy Pareto frontier; understanding this framing is critical for interpreting results.
  - Quick check question: Why does reducing premature exiting alone not guarantee better speed-accuracy trade-offs?

## Architecture Onboarding

- **Component map:** Input → [Encoder Layer m] → h^(m) → [Internal Classifier f_m] → l = W^T x + b (logits) → Compute x_W via Eq.(6): x_W = W(W^T W)^(-1)W^T x → Compute NSP = sqrt(||x||² - ||x_W||²) / ||x|| → Append l_0 = α · NSP to logits → CAP = softmax([l_0, l_1...l_C])[0] → If CAP < τ: EXIT with prediction argmax(l_1...l_C) Else: Continue to layer m+1

- **Critical path:** The projection matrix computation W(W^T W)^(-1)W^T is precomputed per layer (shared across samples). The per-sample overhead is matrix-vector multiplication + norm computations (~1.2M FLOPs vs. 1813.5M for encoder block).

- **Design tradeoffs:**
  - α too high → NSP dominates → delayed exiting → slower inference
  - α too low → NSP ignored → premature exiting → accuracy drops
  - Paper finds α ∈ [0.1, 1.0] works across tasks; requires grid search per deployment
  - Offset vector o = (W^T)^+b must be computed once per classifier; adds ~8.4K FLOPs for 2-class, ~25K params storage

- **Failure signatures:**
  - High DIS score but poor speed-up: α likely too high (delayed exiting dominates)
  - Good speed-up but accuracy crash: α too low (premature exiting dominates)
  - Inconsistent performance across similar tasks: α sensitivity indicates task-specific tuning needed
  - Computational overhead >5%: check if precomputation is cached properly

- **First 3 experiments:**
  1. **Baseline replication:** Implement CAP on SST-2 with BERT-base, sweep α ∈ {0.01, 0.1, 1.0, 10.0}, plot speed-up vs. accuracy curve. Compare against entropy-based exiting (DeeBERT) to verify Figure 2 reproduction.
  2. **Error rate analysis:** On QNLI dev set, measure Premature/Delayed Exiting Rates at fixed 2.0× speed-up. Verify CAP reduces both rates vs. PABEE and E-LANG as shown in Figure 3.
  3. **Ablation on α sensitivity:** For each GLUE task, identify optimal α and measure performance variance when α is perturbed by ±50%. This characterizes robustness and deployment margin.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Certainty-Aware Probability (CAP) strategy be effectively combined with orthogonal methods that modify network architectures or training objectives?
- Basis in paper: [explicit] The authors state, "Combining our method with these orthogonal works would be an intriguing direction for further research" (Section 2).
- Why unresolved: The current study isolates the exiting signal's contribution by keeping the backbone architecture and training loss fixed, whereas methods like GAML-BERT or LECO introduce complex structural changes.
- What evidence would resolve it: Experimental results showing the performance-efficiency trade-off when CAP is integrated into models trained with cross-layer distillation or neural architecture search.

### Open Question 2
- Question: Can the Null Space Projection (NSP) method be adapted for token-level early exiting required by sequence generation tasks?
- Basis in paper: [inferred] The authors explicitly exclude token-level baselines, noting that "our method focuses on sentence-level early exiting" (Section 5.2).
- Why unresolved: The current formulation relies on a single sentence-level feature projection, whereas generation requires evaluating the certainty of individual tokens which may have distinct null space geometries.
- What evidence would resolve it: A reformulation of the NSP score for token-level hidden states and subsequent evaluation on generative benchmarks like machine translation.

### Open Question 3
- Question: Is the geometric assumption regarding null space information robust when scaling to modern Large Language Models (LLMs)?
- Basis in paper: [inferred] Experiments are restricted to BERT-base and ALBERT-base, leaving the method's behavior in the high-dimensional embedding spaces of LLMs unverified.
- Why unresolved: The relationship between class-irrelevant information and prediction certainty may shift in models with significantly larger hidden dimensions and deeper architectures.
- What evidence would resolve it: Evaluation of the CAP score on large-scale decoder-only models (e.g., Llama or GPT variants) to verify if the speed-up ratio is maintained without accuracy loss.

## Limitations
- Validated only on BERT-base for GLUE classification tasks with sequence length ≤128
- Optimal α values likely require task-specific tuning despite paper's claim of universal range
- Computational overhead claims don't account for precomputation and parameter storage costs

## Confidence
- **High Confidence**: The mathematical derivation of NSP score and CAP formula is internally consistent and reproducible
- **Medium Confidence**: Experimental results on GLUE tasks are likely reproducible but optimal hyperparameters are unclear
- **Low Confidence**: Generalization claims to other PLMs, tasks, and longer sequences are weakly supported

## Next Checks
1. **Error Type Analysis**: For fixed 2.0× speed-up, measure and compare Premature/Delayed Exiting Rates between CAP and baseline methods on QNLI dev set
2. **α Sensitivity and Calibration**: Sweep α ∈ {0.01, 0.1, 0.5, 1.0, 5.0} on SST-2 and plot speed-up vs. accuracy curves with Expected Calibration Error
3. **Generalization Test**: Apply CAP to BERT-large on MNLI and to non-GLUE classification task (e.g., IMDb sentiment) to test scalability and domain transfer