---
ver: rpa2
title: Exploring How Audio Effects Alter Emotion with Foundation Models
arxiv_id: '2509.15151'
source_url: https://arxiv.org/abs/2509.15151
tags:
- audio
- emotion
- effects
- music
- foundation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates how audio effects (FX) alter
  emotion perception in music using foundation models. The research applies six common
  audio FX (reverb, delay, distortion, EQ, chorus, phaser) at varying intensities
  to three datasets and analyzes the impact on emotion recognition using three state-of-the-art
  foundation models (MERT, CLAP, Qwen) paired with interpretable classifiers.
---

# Exploring How Audio Effects Alter Emotion with Music Foundation Models

## Quick Facts
- **arXiv ID:** 2509.15151
- **Source URL:** https://arxiv.org/abs/2509.15151
- **Reference count:** 0
- **Key outcome:** Audio effects significantly alter emotion perception in music as detected by foundation models, with distortion having the strongest impact by increasing Anger and decreasing Calmness

## Executive Summary
This study investigates how common audio effects influence emotion recognition in music using foundation models. The research applies six audio effects (reverb, delay, distortion, EQ, chorus, phaser) at varying intensities to three music datasets and analyzes the impact using three state-of-the-art foundation models (MERT, CLAP, Qwen) paired with interpretable classifiers. The results demonstrate that audio effects substantially influence emotion predictions, with distortion showing the most pronounced effects. Real-world effect chains that mimic artist-specific styles generated even larger shifts in model embeddings than individual effects, revealing that both effect type and intensity significantly impact model behavior.

## Method Summary
The researchers systematically applied six audio effects (reverb, delay, distortion, EQ, chorus, phaser) at varying intensities to three music datasets. They used three foundation models (MERT, CLAP, Qwen) to extract embeddings from the audio, then applied interpretable classifiers to predict emotional dimensions (Valence, Arousal, Dominance) and categorical emotions. The study analyzed both individual effects and real-world effect chains that mimicked the styles of artists like Pink Floyd and Rage Against the Machine. They measured emotional shifts through changes in predicted emotion scores and tracked trajectory length and variance in embedding space to quantify the impact of effects on model representations.

## Key Results
- Distortion had the strongest emotional impact, significantly increasing Anger predictions and decreasing Calmness
- Real-world effect chains produced larger, more coherent shifts in model embeddings than individual effects
- MERT showed relative robustness to FX manipulations compared to CLAP and Qwen, which were more sensitive to effects

## Why This Works (Mechanism)
Foundation models trained on large audio datasets have learned to associate certain timbral and spectral characteristics with emotional content. Audio effects fundamentally alter these acoustic features - distortion adds harmonic complexity and saturation, reverb modifies temporal envelope and perceived space, while EQ changes spectral balance. These alterations create new acoustic patterns that may not have been well-represented in the models' training data, leading to shifts in emotion predictions. The magnitude of these shifts depends on both the type of effect and its intensity, with more extreme modifications producing larger deviations from original emotional predictions.

## Foundational Learning
**Audio effects processing**: Understanding how different effects (reverb, delay, distortion, etc.) alter audio signals is crucial for interpreting the study's results. These effects change timbral, temporal, and spectral characteristics that foundation models use for emotion recognition.
*Why needed:* The study's findings depend on understanding how each effect type specifically alters audio features
*Quick check:* Can identify how reverb, distortion, and EQ each modify different aspects of the audio signal

**Emotion recognition in music**: Foundation models encode audio into embeddings that classifiers use to predict emotional dimensions. These models learn patterns between acoustic features and perceived emotion during training.
*Why needed:* The study's core methodology relies on how these models map audio features to emotional predictions
*Quick check:* Understands that models learn statistical relationships between audio characteristics and emotion labels

**Embedding space analysis**: The study uses trajectory length and variance in embedding space as metrics for emotional impact, requiring understanding of how models represent audio in high-dimensional space.
*Why needed:* These metrics quantify how effects shift model representations
*Quick check:* Can explain how changes in embedding space relate to changes in model predictions

## Architecture Onboarding

**Component map:**
Audio Files -> Foundation Models (MERT, CLAP, Qwen) -> Embeddings -> Interpretable Classifiers -> Emotion Predictions

**Critical path:**
Audio files → Effect application → Foundation model embedding extraction → Classifier prediction → Emotion analysis

**Design tradeoffs:**
The study chose to use existing foundation models rather than fine-tuning them on effect-altered audio, prioritizing generalizability over potentially higher accuracy with specialized models. They also opted for interpretable classifiers over end-to-end approaches to better understand which features drive emotion predictions.

**Failure signatures:**
If effects are applied at intensities beyond what foundation models encountered during training, predictions may become unreliable or collapse to baseline patterns. Additionally, if effect chains create audio that falls outside the models' learned distribution, emotion predictions may show erratic behavior or systematic biases.

**3 first experiments:**
1. Apply a single effect (e.g., moderate distortion) to a subset of songs and verify that emotion predictions shift in expected directions
2. Test different intensities of the same effect to establish a dose-response relationship
3. Compare predictions from all three foundation models on identical effect-altered audio to identify model-specific sensitivities

## Open Questions the Paper Calls Out
None

## Limitations
- The study relies entirely on model-based emotion predictions rather than human perception data, raising questions about ecological validity
- Dataset composition regarding genre balance and potential biases is unclear, potentially affecting generalizability
- The study does not account for perceptual masking effects where multiple FX interact in complex ways

## Confidence

**High confidence:** The finding that distortion has the strongest emotional impact (increasing Anger, decreasing Calmness) is robust across multiple models and effect configurations, supported by both real-world chains and systematic intensity variations.

**Medium confidence:** The relative sensitivity differences between MERT, CLAP, and Qwen to FX manipulations are well-documented but may depend on specific architectural differences not fully explored in this study.

**Low confidence:** Claims about trajectory length and variance in embedding space serving as reliable metrics for emotional impact require further validation, as these relationships were observed but not extensively tested for predictive power or consistency across different musical contexts.

## Next Checks
1. Conduct human listening studies to validate whether model-predicted emotional shifts align with actual human perception of FX-altered audio, particularly for the real-world effect chains
2. Test the methodology across a more diverse range of musical genres and cultural contexts to assess generalizability beyond the current datasets
3. Implement cross-model consistency checks by training interpretable classifiers on embeddings from different foundation models to verify whether observed FX effects are model-specific or more universally detectable in audio representations