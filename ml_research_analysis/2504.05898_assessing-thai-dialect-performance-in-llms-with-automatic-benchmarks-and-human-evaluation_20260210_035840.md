---
ver: rpa2
title: Assessing Thai Dialect Performance in LLMs with Automatic Benchmarks and Human
  Evaluation
arxiv_id: '2504.05898'
source_url: https://arxiv.org/abs/2504.05898
tags:
- local
- dialects
- thai
- llms
- dialect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study addresses the lack of evaluation benchmarks and metrics\
  \ for Thai local dialects (Isan, Lanna, Dambro) in large language models (LLMs).\
  \ It introduces a benchmark covering five NLP tasks\u2014summarization, question\
  \ answering, translation, conversation, and food-related tasks\u2014with data translated\
  \ into local dialects."
---

# Assessing Thai Dialect Performance in LLMs with Automatic Benchmarks and Human Evaluation

## Quick Facts
- **arXiv ID**: 2504.05898
- **Source URL**: https://arxiv.org/abs/2504.05898
- **Reference count**: 13
- **Primary result**: Proposes human evaluation metric for Thai dialect fluency; shows traditional metrics fail while proprietary models (GPT-4o, Gemini2) outperform open models.

## Executive Summary
This study addresses the critical gap in evaluating large language models (LLMs) on Thai local dialects (Isan, Lanna, Dambro), which are underrepresented in standard NLP benchmarks. The authors introduce a benchmark covering five tasks—summarization, question answering, translation, conversation, and food-related queries—with native speaker translations from Central Thai. A novel human evaluation metric is proposed to assess dialect-specific accuracy and generation fluency, revealing that while proprietary models show some dialectal capability, most LLMs perform significantly worse on local dialects than standard Thai, highlighting the need for specialized evaluation methods.

## Method Summary
The study evaluates LLMs across five NLP tasks using 400 total samples (100 per dialect), with data translated from Central Thai by native speakers. Traditional metrics (BLEU for translation, ROUGE-L for QA and summarization) are complemented by a human evaluation metric scoring Generation (0/0.5/1) and Fluency (0/0.5/1) across two annotators. Models tested include Llama-3.1-8b/70b, Typhoon1.5-8b/70b, GPT-4o, and Gemini2 using prompts written in local dialects. Human preference comparisons (A vs B, Both, None) provide additional validation.

## Key Results
- Traditional metrics (BLEU, ROUGE-L) fail to capture dialect fluency, with high scores often reflecting Central Thai output rather than local dialect generation
- Proprietary models (GPT-4o, Gemini2) demonstrate better dialectal fluency than open models, with Gemini2 achieving highest human evaluation scores
- Open models (Llama-3.1, Typhoon1.5) fail to generate Thai local dialects effectively
- LLM-as-a-Judge shows low correlation (53.6 points) with human judgment for dialect fluency

## Why This Works (Mechanism)
The proposed human evaluation metric addresses the fundamental limitation that traditional automated metrics cannot capture the nuanced linguistic features of Thai dialects, particularly orthographic variations and phonological patterns that distinguish local speech from standard Thai.

## Foundational Learning
- **Dialectal variation in Thai**: Understanding the three major Thai dialects (Isan, Lanna, Dambro) and their distinct phonological, lexical, and grammatical features is essential for designing appropriate evaluation tasks and interpreting results.
- **Human evaluation methodology**: The Generation vs. Fluency scoring framework provides a structured approach to subjective assessment that balances objective correctness with naturalness perception.
- **Limitations of traditional metrics**: Recognizing that BLEU and ROUGE-L assume standard orthography and vocabulary, making them inadequate for dialectal text where spelling variants and non-standard words are common.
- **Code-switching patterns**: Awareness that natural Thai communication often involves dialect mixing, which the current benchmark does not capture but affects real-world model performance.
- **Annotator bias considerations**: Understanding that dialect perception varies by speaker background and regional affiliation, requiring careful annotator selection and qualification.

## Architecture Onboarding
- **Component map**: Data translation (Central Thai → local dialects) -> Benchmark construction (5 tasks) -> Model generation -> Traditional metric evaluation (BLEU/ROUGE-L) -> Human evaluation (Generation/Fluency) -> LLM-as-Judge comparison
- **Critical path**: Native speaker translation → Task-specific prompt design → Model generation → Human evaluation scoring → Metric validation
- **Design tradeoffs**: Small benchmark size (400 samples) vs. comprehensive dialect coverage; subjective human evaluation vs. objective automated metrics; focus on specific dialects vs. generalizability to all Thai dialects
- **Failure signatures**: High ROUGE-L scores with no dialectal vocabulary in outputs; low inter-annotator agreement indicating metric ambiguity; tokenization failures on dialect-specific words
- **First experiments**: 1) Validate translation quality by back-translation comparison; 2) Test model outputs for dialect-specific vocabulary presence; 3) Compute inter-annotator agreement for human evaluation

## Open Questions the Paper Calls Out
- **Generalization to other languages**: Can the proposed human evaluation metric be effectively generalized to assess LLM fluency in other low-resource languages or dialects that lack standardized writing systems?
- **LLM-as-Judge refinement**: Can LLM-as-a-Judge methodologies be refined to accurately evaluate local dialect fluency without the low correlation rates observed in current proprietary models?
- **Training interventions for open models**: What specific training interventions (e.g., data augmentation, dialect-specific instruction tuning) are required to close the performance gap between open-source and proprietary models in generating fluent Thai local dialects?

## Limitations
- Benchmark size (100 samples per dialect) is relatively small for robust statistical analysis of dialectal variation
- Human evaluation relies on subjective judgments that may vary based on annotator backgrounds and regional affiliations
- Does not account for dialect mixing or code-switching patterns common in natural Thai communication
- Translation process may introduce bias based on individual dialect speakers' preferences
- Comparison constrained by API access limitations and may not reflect optimal model configurations

## Confidence
- **High Confidence**: Proprietary models outperform open models on dialect tasks; traditional metrics fail to capture dialect fluency
- **Medium Confidence**: Relative performance rankings between specific models; claim about models' inability to handle local dialects "effectively"
- **Low Confidence**: Generalizability to all Thai dialects beyond the three studied; human evaluation metric's sensitivity to subtle dialectal features

## Next Checks
1. **Replication with expanded dataset**: Validate findings using 300-500 examples per dialect to establish statistical robustness and assess performance across broader dialectal variation
2. **Cross-annotator reliability study**: Conduct human evaluation with 5-7 annotators per dialect, computing inter-annotator agreement (Cohen's Kappa) to quantify metric reliability
3. **Prompt optimization experiment**: Systematically vary inference parameters and prompt formulations across all models to establish whether current performance represents optimal configurations, particularly for open models