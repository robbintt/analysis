---
ver: rpa2
title: 'Combining Forecasts using Meta-Learning: A Comparative Study for Complex Seasonality'
arxiv_id: '2504.08940'
source_url: https://arxiv.org/abs/2504.08940
tags:
- forecasting
- forecasts
- base
- lstm
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores meta-learning for combining forecasts generated\
  \ by different models, moving beyond simple averaging. The author tests five meta-learners\u2014\
  linear regression, k-nearest neighbors, multilayer perceptron, random forest, and\
  \ long short-term memory\u2014in both global and local learning variants for time\
  \ series with complex seasonality."
---

# Combining Forecasts using Meta-Learning: A Comparative Study for Complex Seasonality

## Quick Facts
- arXiv ID: 2504.08940
- Source URL: https://arxiv.org/abs/2504.08940
- Reference count: 40
- Meta-learning outperforms simple averaging in electricity load forecasting with MAPE 1.52% (RF) vs 1.91% (mean)

## Executive Summary
This paper demonstrates that machine learning-based meta-learning significantly improves forecast combination accuracy compared to simple averaging methods, particularly for complex seasonal time series. The study evaluates five meta-learners (linear regression, k-nearest neighbors, multilayer perceptron, random forest, and LSTM) across 35 European countries' electricity load data featuring triple seasonality (daily, weekly, yearly). Results show random forest achieves the lowest mean absolute percentage error (1.52%) and is robust to training set size, while LSTM produces the most forecasts outside the range of base model predictions but doesn't consistently improve accuracy.

## Method Summary
The method employs 16 base forecasting models to generate predictions for electricity load data, which are then combined using five different meta-learning approaches. Base models include traditional statistical methods (ARIMA, ES), machine learning models (MLP, SVM, LGBM, XGB), and deep learning approaches (LSTM, DeepAR, WaveNet, N-BEATS). Meta-learners are trained using either global learning (all historical data) or local learning (k-nearest patterns or recent points). The study evaluates performance using MAPE, MdAPE, MSE, MPE, and StdPE metrics across 100 test hours from 2018 data.

## Key Results
- Random forest achieves the best overall performance with MAPE of 1.52% and MSE of 173,821
- All meta-learners outperform simple averaging (Mean MAPE 1.91%, Median MAPE 1.82%)
- LSTM generates the most forecasts outside base model ranges (447 cases) but accuracy is inconsistent
- Random forest shows low sensitivity to training set size across all tested configurations
- Local learning strategies yield lower errors than global learning for all meta-learners

## Why This Works (Mechanism)

### Mechanism 1: Nonlinear Learning of Optimal Combination Weights
Stacking with ML meta-learners outperforms simple averaging by learning optimal nonlinear combination weights from historical forecast-target pairs. Base models generate diverse forecasts capturing different patterns, and the meta-learner learns a regression function f(ŷ_t) that maps the vector of base forecasts to the optimal combined forecast rather than applying fixed equal weights. Nonlinear meta-models can capture interactions between base forecasts that linear averaging cannot.

### Mechanism 2: RF Bagging Structure Provides Robustness
Random Forest's bagging structure provides robustness to training set size and reduces overfitting in forecast combination. RF constructs multiple regression trees on bootstrap samples with random feature subspaces, then averages predictions. This ensemble-within-ensemble structure smooths the regression function and provides variance reduction without significant bias increase.

### Mechanism 3: LSTM Sequential Memory Enables Extrapolation
LSTM's sequential state memory enables extrapolation beyond the convex hull of base forecasts. Unlike other meta-models that use only current base forecasts, LSTM incorporates historical base forecasts through cell states and hidden states. This temporal context allows LSTM to recognize when recent patterns suggest the target lies outside the range of current base forecasts.

## Foundational Learning

- **Concept: Stacking (Stacked Generalization)** - Why needed: Understanding that meta-learning is a form of stacking where level-1 models feed predictions to a level-2 model that learns optimal combination. Quick check: Can you explain why stacking differs from bagging and boosting in terms of model diversity sources?

- **Concept: Bias-Variance Tradeoff in Ensembles** - Why needed: The paper explicitly references this tradeoff for kNN, RF, and MLP. Quick check: For a meta-learner with high variance, would you increase or decrease the training set size constraint?

- **Concept: Complex Seasonality (Multiple Seasonal Periods)** - Why needed: The target domain has triple seasonality (daily s₁=24, weekly s₂=168, yearly). Understanding how LSTM variants attempt seasonal phase alignment is critical. Quick check: Why might training only on same-phase seasonal points underperform compared to using recent contiguous data?

## Architecture Onboarding

- **Component map**: 16 Base Models (ŷ_{i,t}) -> 5 Meta-Learners (f(ŷ_t; θ)) -> Combined Forecast
- **Critical path**: Generate base forecasts -> Construct training set Φ = {(ŷ_τ, y_τ)} -> Train meta-model per forecasting task -> Apply to combine base forecasts -> Evaluate against target
- **Design tradeoffs**: 
  - Global vs. Local training: Global maximizes data but may include irrelevant patterns; Local captures recent dynamics but risks overfitting
  - Meta-model complexity: LR is interpretable but assumes linearity; LSTM captures temporal dynamics but requires careful sequence design
  - LSTM sequence construction: v1 (recent contiguous) outperformed v2/v3 (seasonal phase-aligned), contrary to expectation
- **Failure signatures**:
  - Collinear base forecasts: If base models converge to similar predictions, meta-learner has no diversity to exploit
  - Concept drift: If forecast-target relationship changes, meta-model trained on stale data degrades
  - LSTM overfitting on small sequences: LSTM v1 with c=24 underperformed c=168
  - RF extrapolation limitation: RF produced only 34 forecasts outside base forecast interval
- **First 3 experiments**:
  1. Implement simple Mean and Median combination, compare against LR meta-learner with global training
  2. Train RF meta-learner with varying training set sizes (k ∈ {20, 100, 200, N_t}) and plot MAPE vs. training set size
  3. Implement LSTM v1, v2, v3 on data with known seasonality and verify v1 outperforms seasonal phase-aligned variants

## Open Questions the Paper Calls Out

- **Future ML models for sequential data**: Can advanced machine learning models specifically tailored for sequential data improve the predictive capabilities of meta-learning when applied to bagging and boosting ensemble strategies? The paper identifies this as future research focus but doesn't explore it.

- **Seasonality-tailored LSTM variants**: Why did the seasonality-tailored LSTM training variants (v2 and v3) underperform compared to the simpler, recent-window variant (v1)? The authors expected v2 and v3 to simplify the relationship by matching seasonal phases, but results showed higher errors.

- **LSTM extrapolation control**: How can the extrapolation behavior of LSTM meta-models be controlled to prevent poor performance when forecasts fall outside the range of base model predictions? While LSTM generates more outside-range forecasts, it was more accurate than Random Forest in less than half of these cases.

- **Generalizability beyond electricity**: Do the findings regarding the superiority of Random Forest and meta-learning generalize to domains outside of electricity load forecasting? The experimental study is restricted to short-term load forecasting for 35 European countries.

## Limitations

- Base model implementations are not fully specified in the paper, requiring reliance on external reference for exact methods and hyperparameters
- Hyperparameter optimization for meta-learners uses grid search but validation procedure details are unclear
- LSTM sequence construction for local learning lacks explicit rationale for why seasonal phase alignment underperformed recent contiguous data
- Data preprocessing steps (missing value handling, normalization, outlier treatment) are not documented

## Confidence

- **High confidence**: RF outperforming simple averaging (1.52% vs 1.91% MAPE) and showing robustness to training set size
- **Medium confidence**: LSTM's ability to generate forecasts outside base forecast range (447 cases)
- **Medium confidence**: Global learning superiority for LR and RF
- **Low confidence**: Specific superiority of LSTM v1 over v2/v3 variants

## Next Checks

1. **Replication of base model performance**: Implement the 16 base models with documented hyperparameters and verify their individual forecasting accuracy on the electricity load data before proceeding with meta-learning.

2. **Training set size sensitivity**: Systematically test RF and kNN with training sets of size k ∈ {20, 50, 100, 200, 500} to verify claimed robustness, plotting accuracy metrics against training set size.

3. **LSTM variant comparison**: Replicate all three LSTM variants (v1, v2, v3) on a subset of time series and verify that v1 consistently outperforms seasonal phase-aligned variants, then investigate feature importance to understand why recent context dominates seasonal alignment.