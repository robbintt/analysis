---
ver: rpa2
title: 'ReCollab: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling'
arxiv_id: '2512.22129'
source_url: https://arxiv.org/abs/2512.22129
tags:
- teammate
- recollab
- which
- type
- behavior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReCollab introduces retrieval-augmented large language models for
  cooperative ad-hoc teammate modeling. The approach classifies teammate types from
  early trajectory windows using behavior rubrics and LLM reasoning, then grounds
  predictions with retrieved exemplar trajectories to improve robustness.
---

# ReCollab: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling

## Quick Facts
- arXiv ID: 2512.22129
- Source URL: https://arxiv.org/abs/2512.22129
- Authors: Conor Wallace; Umer Siddique; Yongcan Cao
- Reference count: 9
- Primary result: Achieves Pareto-optimal performance in Overcooked, with classification accuracy up to 0.96 and cumulative returns near oracle levels across three layouts.

## Executive Summary
ReCollab introduces a retrieval-augmented LLM approach for classifying teammate types in ad-hoc cooperative settings. By combining statistical fingerprinting via behavior rubrics with retrieval-augmented generation, the method achieves high classification accuracy from short trajectory windows and routes to appropriate best-response policies. The approach outperforms baseline methods like PLASTIC and Logistic Regression in the Overcooked domain, demonstrating that LLM-based inference with exemplar retrieval can serve as an effective and interpretable world model for ad-hoc teamwork.

## Method Summary
The method trains 5 teammate policies and 5 corresponding best-response policies via PPO. From early trajectory observations (probe window P=20), it computes behavioral features (action histograms, dwell times, cumulative reward), ranks them by mutual information, and constructs a rubric with mean/std per type. COLLAB uses GPT-5 to classify from feature descriptions + rubric; RECOLLAB augments with top-5 retrieved exemplars via text-embedding-3-large. The agent switches exactly once to the predicted best-response policy after the probe window.

## Key Results
- Classification accuracy reaches 0.96 in optimal conditions across three Overcooked layouts
- Cumulative returns approach oracle levels, outperforming PLASTIC and Logistic Regression
- Retrieval exemplars stabilize predictions in ambiguous cases, particularly for mixed teammate types
- Probe length P=20 achieves optimal accuracy/return trade-off

## Why This Works (Mechanism)

### Mechanism 1: Statistical Fingerprinting via Behavior Rubrics
If trajectory features are distilled into statistical fingerprints (rubrics), LLMs can classify teammate types from short interaction windows. The system computes feature vectors over a probe window, selects discriminative features using Mutual Information, and summarizes them in natural language. This works when teammate types exhibit distinct early-detectable statistical signatures.

### Mechanism 2: Retrieval-Grounded Disambiguation
Augmenting the LLM prompt with retrieved exemplar trajectories stabilizes classification by resolving ambiguities in mixed or overlapping behavior types. Similar trajectory embeddings correspond to similar teammate types, reducing the LLM's tendency to hallucinate from sparse statistical features.

### Mechanism 3: One-Shot Policy Routing
Mapping a predicted teammate type to a pre-computed best-response policy achieves higher cumulative returns than online adaptation. The agent uses a default policy during probing, then switches exactly once to the corresponding best-response policy, assuming teammate behavior is static for the episode.

## Foundational Learning

- **Ad-hoc Teamwork (AHT)**: Collaborating with unknown partners without pre-coordination. Why needed: This is the core problem domain.
  - Quick check: Can you explain why standard MARL fails in this setting? (Answer: MARL assumes joint training/control; AHT does not.)

- **Partial Observability & Probe Windows**: Inferring teammate types from limited data. Why needed: The system must classify from the probe window.
  - Quick check: How does probe window length P create a trade-off between accuracy and return? (Answer: Longer P yields better accuracy but delays optimal policy deployment.)

- **Retrieval-Augmented Generation (RAG)**: Understanding how non-parametric memory augments parametric LLM. Why needed: Critical to distinguishing COLLAB from RECOLLAB.
  - Quick check: In this context, what acts as the "query" and what acts as the "context" for the LLM? (Answer: Current trajectory features are the query; retrieved past trajectories are the context.)

## Architecture Onboarding

- **Component map**: Environment -> Feature Extraction -> RAG Retrieval -> LLM Classification -> Policy Switch
- **Critical path**: Environment Step → Feature Extraction → RAG Retrieval → LLM Classification → Policy Switch
- **Design tradeoffs**:
  - Probe Length (P): Paper finds P=20 optimal; shorter is random, longer hurts returns
  - Retrieval Count (k): Paper finds k=3-5 sufficient; higher k yields diminishing returns
  - Rubric vs. Raw Data: Using statistical summaries instead of raw trajectories for LLM context windows but loses granular temporal data
- **Failure signatures**:
  - Ambiguity in "Mixed" Class: Features often overlap with "Plate" or "Pot" types, causing confusion
  - Layout Sensitivity: Performance drops in "Asymmetric Advantage" vs. "Coordination Ring"
  - Static Assumption: Single policy switch; if initial classification is wrong, agent stuck in sub-optimal policy
- **First 3 experiments**:
  1. Baseline Accuracy Check: Run COLLAB vs. RECOLLAB vs. Logistic Regression on "Cramped Room" with P=20
  2. Probe Length Sensitivity: Sweep P ∈ {5, 10, 20, 40} to replicate accuracy/return Pareto frontier
  3. Robustness Test: Evaluate RECOLLAB on "Mixed" teammate type specifically to quantify error reduction from retrieval

## Open Questions the Paper Calls Out

### Open Question 1
Can retrieval-augmented LLMs effectively model teammates with continuous or high-dimensional behavior spaces, rather than the discrete five types tested? The current study only evaluates five discrete types, leaving open whether the approach generalizes to more nuanced behavioral spectra.

### Open Question 2
Can online policy adaptation throughout an episode improve performance beyond the current single-switch approach after the probe window? The fixed routing approach prevents correction of early misclassifications and cannot adapt to teammates whose behavior changes mid-episode.

### Open Question 3
To what extent does ReCollab generalize to human teammates or multi-modal interaction scenarios? All experiments use trained agent teammates; human behavior may exhibit different patterns not captured in current trajectory representations.

### Open Question 4
Would hybridizing retrieval-augmented LLMs with probabilistic priors outperform either approach alone? PLASTIC excels in some layouts while ReCollab excels in others, suggesting complementary strengths that could potentially be combined.

## Limitations
- Results confined to three Overcooked layouts with five synthetically generated teammate types
- "Mixed" teammate type exhibits persistent confusion with Plate and Pot types
- Static policy-switching assumption limits applicability to non-stationary teammates
- Requires pre-trained LLM and retrieval infrastructure, raising scalability questions

## Confidence

- **Classification accuracy and return improvements**: High
- **Retrieval exemplars stabilize ambiguous predictions**: Medium
- **Statistical fingerprinting enables early classification**: High

## Next Checks

1. **Error Analysis for Mixed Type**: Generate confusion matrix for "Mixed" teammate across all layouts; compute precision/recall to quantify overlap with Plate/Pot types
2. **Retrieval Ablation Under Ambiguity**: Compare COLLAB vs. RECOLLAB specifically on Mixed-type episodes to measure retrieval's impact on confusion rates
3. **Policy Library Completeness Test**: Simulate non-stationary teammate that switches strategy mid-episode; measure performance degradation from single-switch routing