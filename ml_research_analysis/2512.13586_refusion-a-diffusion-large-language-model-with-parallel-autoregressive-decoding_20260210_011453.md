---
ver: rpa2
title: 'ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding'
arxiv_id: '2512.13586'
source_url: https://arxiv.org/abs/2512.13586
tags:
- slot
- slots
- arxiv
- token
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REFUSION addresses the inefficiency and incoherence of masked diffusion
  models (MDMs) for language generation. It introduces a slot-level decoding framework
  where contiguous token segments (slots) are planned in parallel using diffusion
  and filled in parallel via autoregressive completion.
---

# ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding

## Quick Facts
- arXiv ID: 2512.13586
- Source URL: https://arxiv.org/abs/2512.13586
- Reference count: 40
- Primary result: 34% performance gains and 18× speedup over prior MDMs, while outperforming strong autoregressive models by 3.68 points on GSM8K and MBPP while maintaining a 2.33× speedup

## Executive Summary
ReFusion introduces a hybrid decoding framework for diffusion language models that addresses the inefficiency and incoherence of traditional masked diffusion models (MDMs). By combining parallel inter-slot decoding via diffusion with autoregressive intra-slot completion, ReFusion achieves both high throughput and coherent generation. The method uses slot-level masking with permutation-based ordering to reduce learning complexity while enabling full Key-Value cache reuse. Evaluated across seven benchmarks, ReFusion demonstrates significant performance improvements over both autoregressive and diffusion baselines.

## Method Summary
ReFusion implements a slot-level decoding framework where input sequences are partitioned into contiguous token segments (slots) of fixed sizes. During training, clean slots are randomly permuted and placed before masked slots, with all tokens retaining ground-truth position IDs. The model is trained with a hybrid loss combining autoregressive masked language modeling (ARM) on clean slots and denoising objectives on masked slots. Inference uses a two-stage "plan-and-infill" approach: diffusion-based planning scores and drafts slots in parallel, followed by autoregressive infilling to complete the drafts. This design enables full Key-Value cache reuse and reduces learning complexity from token combinations to slot permutations.

## Key Results
- 34% performance gains and 18× speedup over prior masked diffusion models
- Outperforms strong autoregressive models by 3.68 points on GSM8K and MBPP
- Maintains 2.33× speedup while achieving superior accuracy on reasoning tasks

## Why This Works (Mechanism)
The slot-level framework addresses the fundamental limitations of traditional MDMs by enabling parallel decoding while maintaining coherence. By separating the generation process into planning (slot selection via diffusion) and infilling (autoregressive completion within slots), ReFusion combines the efficiency of parallel decoding with the coherence of autoregressive methods. The permutation-based ordering during training reduces the learning complexity from handling all possible token combinations to managing slot permutations, while full Key-Value cache reuse significantly boosts inference throughput.

## Foundational Learning

**Diffusion Language Models**: Generate text by iteratively denoising corrupted inputs through a neural network trained to reverse a noising process. Needed because traditional autoregressive models suffer from sequential bottlenecks. Quick check: Verify denoising steps progressively reconstruct clean text from noise.

**Masked Language Modeling**: Trains models to predict masked tokens given context, enabling bidirectional context usage. Needed to leverage bidirectional information during training. Quick check: Confirm model can accurately predict randomly masked tokens in context.

**Autoregressive Generation**: Generates tokens sequentially, each conditioned on previously generated tokens. Needed for maintaining coherence in generated text. Quick check: Verify causal dependencies are preserved during generation.

**Position ID Handling with RoPE**: Rotary Position Embeddings require absolute position information for accurate token representation. Needed because slot reordering during training preserves original positions. Quick check: Ensure position IDs remain consistent regardless of input ordering.

**Key-Value Cache Reuse**: Stores computed attention keys/values to avoid redundant computation in autoregressive generation. Needed to achieve the reported speedup improvements. Quick check: Measure throughput with and without cache reuse to confirm impact.

## Architecture Onboarding

**Component Map**: Dataset Preprocessing -> Slot Masking & Reordering -> Hybrid Training (ARM + MDM) -> Inference (Planning + Infilling) -> Output Generation

**Critical Path**: The most performance-sensitive path is the inference pipeline: diffusion-based planning → slot selection → parallel autoregressive infilling → cache reuse. Any inefficiency in slot generation or cache management directly impacts throughput.

**Design Tradeoffs**: The framework trades increased training complexity (managing slot permutations and dual losses) for significant inference speedup and improved coherence. The choice of slot size affects the balance between parallelism and coherence - smaller slots enable more parallelism but may reduce coherence, while larger slots improve coherence but reduce parallel efficiency.

**Failure Signatures**: 
- Position ID handling errors manifest as degraded generation quality regardless of model capacity
- Poor threshold settings result in either incoherent generations (τslot too low) or slow generation (τtoken too high)
- KV cache issues appear as reduced throughput despite parallel generation

**3 First Experiments**:
1. Train a minimal ReFusion model on synthetic data to verify slot permutation and position ID preservation
2. Implement and test the plan-and-infill inference pipeline with fixed thresholds to measure basic functionality
3. Compare generation quality and speed with varying slot sizes to identify optimal configuration

## Open Questions the Paper Calls Out
None

## Limitations
- Exact data mixing ratios and preprocessing details for training sources are unspecified
- Architecture modifications for non-contiguous position ID handling are not fully detailed
- Learning rate schedule beyond base rate is not provided

## Confidence

**High Confidence**: The core architectural contribution and empirical speedup/accuracy improvements are well-supported by systematic benchmarking across seven diverse tasks.

**Medium Confidence**: The exact mechanisms of global verification and parallel iterative completion during infilling are described but lack complete implementation details and hyperparameter sensitivity analysis.

**Medium Confidence**: The claimed 34% performance gain and 18× speedup relative to baseline diffusion models are internally consistent but lack full specification of baseline implementations and exact hardware conditions.

## Next Checks

1. **Position ID Handling**: Validate that the model correctly preserves and applies ground-truth position IDs during training, especially when clean slots are permuted and concatenated before masked slots. Test with synthetic sequences to ensure RoPE receives correct absolute positions.

2. **Threshold Sensitivity**: Empirically test the impact of varying τslot and τtoken on coherence and generation speed. Compare results against the task-specific values in Table 6 to confirm robustness.

3. **KV Cache Reuse Verification**: Confirm that the concatenation of parallel-generated slots maintains causal structure and enables full Key-Value cache reuse. Measure throughput gains with and without cache reuse to isolate its contribution.