---
ver: rpa2
title: 'A Computer Vision Pipeline for Individual-Level Behavior Analysis: Benchmarking
  on the Edinburgh Pig Dataset'
arxiv_id: '2509.12047'
source_url: https://arxiv.org/abs/2509.12047
tags:
- behavior
- tracking
- detection
- object
- video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a modular computer vision pipeline for automated
  individual-level behavior analysis in group-housing livestock environments. The
  system integrates zero-shot object detection (OWLv2), motion-aware tracking and
  segmentation (SAMURAI), automated object cropping, feature extraction (DINOv2),
  and behavior classification.
---

# A Computer Vision Pipeline for Individual-Level Behavior Analysis: Benchmarking on the Edinburgh Pig Dataset

## Quick Facts
- arXiv ID: 2509.12047
- Source URL: https://arxiv.org/abs/2509.12047
- Reference count: 0
- This study presents a modular computer vision pipeline for automated individual-level behavior analysis in group-housing livestock environments.

## Executive Summary
This study presents a modular computer vision pipeline for automated individual-level behavior analysis in group-housing livestock environments. The system integrates zero-shot object detection (OWLv2), motion-aware tracking and segmentation (SAMURAI), automated object cropping, feature extraction (DINOv2), and behavior classification. Validated on the Edinburgh Pig Behavior Video Dataset, the pipeline achieved 94.2% accuracy in classifying nine pig behaviors using temporal models, with a 21.2 percentage point improvement over existing methods.

## Method Summary
The pipeline processes raw video through six stages: video decoding with stride control for GPU memory limits, OWLv2 zero-shot detection at confidence 0.5, SAMURAI tracking/segmentation with batch processing, mask-based cropping to 224×224 images, DINOv2-large feature extraction producing 1024-dimensional embeddings, and classification via MLP or BiLSTM models. The system uses NVIDIA V100 16GB GPUs with PyTorch 2.0, Adam optimizer (lr=1e-3), inverse-frequency class weights, and early stopping (patience=10) for training.

## Key Results
- Achieved 94.2% accuracy in classifying nine pig behaviors using temporal BiLSTM models
- Demonstrated 93.3% identity preservation and 89.3% object detection precision
- Showed 21.2 percentage point improvement over existing methods
- Validated robust tracking through occlusions and lighting variations in farm environments

## Why This Works (Mechanism)

### Mechanism 1
Self-supervised vision transformers produce discriminative embeddings for visually similar animal behaviors without species-specific training. DINOv2 pre-trains on 142M diverse images using self-distillation, learning generalizable visual features. When applied to cropped pig images, these embeddings cluster by behavioral state in ways that linear classifiers can separate—capturing shape, pose, and texture cues implicitly.

### Mechanism 2
Motion-aware memory with Kalman filtering enables identity preservation through occlusions by predicting object positions when visual evidence is degraded. SAMURAI extends SAM2 by integrating a Kalman filter-based motion model with selective memory. When occlusion degrades segmentation confidence, the motion model propagates position estimates forward; when the object re-emerges, memory retrieves appearance features to re-associate rather than create a new track ID.

### Mechanism 3
Temporal modeling (BiLSTM) captures behavioral dynamics that improve classification over frame-level predictions. Behaviors unfold over time. A BiLSTM processes sequences of DINOv2 embeddings, learning forward and backward temporal dependencies. It can distinguish "lying down" from "lying still" by the motion pattern, even if individual frames appear similar.

## Foundational Learning

- **Zero-shot object detection**: Why needed? OWLv2 localizes pigs using text queries ("pig") without training on pig images. Critical for adapting to new species without labeled data. Quick check: Can you explain why OWLv2 might fail on a novel species despite being "zero-shot"?
- **Vision Transformers (ViT) and self-supervised learning**: Why needed? DINOv2 is a ViT trained via self-distillation. Understanding patch tokenization and the [CLS] token explains how embeddings are produced. Quick check: What does the [CLS] token represent in a vision transformer?
- **Multi-object tracking metrics (MOTA, IDF1, identity switches)**: Why needed? The paper reports 93.3% IDF1 and 0.44 identity switches. Understanding these clarifies what "robust tracking" means operationally. Quick check: Why is IDF1 a better metric than MOTA for assessing individual animal monitoring?

## Architecture Onboarding

- **Component map**: Raw video -> Frame decoding (stride-controlled sampling, 3000-frame batch limit) -> OWLv2 detection (text query) -> SAMURAI tracking -> Mask-based cropping (224×224) -> DINOv2 feature extraction (1024-dim) -> MLP or BiLSTM classification
- **Critical path**: Detection quality on the first frame determines all downstream success. If initial boxes are missing or misaligned, tracking and classification fail for that individual.
- **Design tradeoffs**: Top-view cameras reduce occlusions but lose limb detail (side-view captures limbs but increases occlusion). Higher stride reduces computation but may miss brief behaviors. LSTM improves accuracy (+1.3 pp) over MLP but requires sequence data and more compute.
- **Failure signatures**: First-frame occlusion/mounting: 3 of 12 sequences excluded due to invisible or overlapping animals. Model-species mismatch: YOLOv12 detected cows but underdetected pigs; OWLv2 worked for pigs but required confidence threshold tuning. Memory overflow: Processing >5000 frames with 4 objects on 16GB GPU causes OOM errors.
- **First 3 experiments**: 1) Validate detection on your first frame: Run OWLv2 with text query for your target species at confidence 0.5. Visually confirm all individuals are boxed before proceeding. 2) Test tracking on a single 3000-frame batch: Confirm identity preservation across occlusions. Check IDF1 and identity switch counts. 3) Compare MLP vs. BiLSTM on your behavior set: If temporal signal is weak, MLP may suffice; if dynamic behaviors dominate, BiLSTM should show improvement.

## Open Questions the Paper Calls Out

### Open Question 1
Can the modular pipeline achieve comparable performance when adapted to other livestock species (e.g., cattle, poultry) without substantial architectural modifications? The modular design suggests potential for adaptation to other contexts, though further validation across species would be required.

### Open Question 2
Does incorporating multi-modal data (audio, environmental sensors, genomic information) improve behavior classification accuracy beyond visual-only approaches? Future development could explore incorporating complementary data modalities, though empirical evaluation of each new data source's specific contribution would be necessary.

### Open Question 3
Can computational optimizations (knowledge distillation, quantization, pruning) reduce the pipeline's resource requirements sufficiently for real-time deployment on edge devices without significant accuracy degradation? Current computational requirements limit deployment in resource-constrained farm environments.

## Limitations
- Cross-species generalization remains untested despite modular design claims
- Camera angle constraints limit limb detail capture and may not generalize to commercial farm conditions
- Temporal window sensitivity may miss brief or rapidly transitioning behaviors
- Occlusion handling limits for prolonged (>several seconds) or complex multi-animal scenarios

## Confidence
- **High confidence**: Tracking performance metrics (93.3% IDF1, 89.3% precision) and behavior classification accuracy (94.2% with LSTM)
- **Medium confidence**: Modular design's adaptability to other species and contexts
- **Low confidence**: Claims about zero-shot detection robustness across species and absolute transferability of temporal modeling benefits

## Next Checks
1. **Cross-species detection validation**: Test OWLv2 with text queries for at least two additional livestock species (e.g., "cow", "sheep") on validation datasets. Measure detection AP and compare to species-specific trained detectors.
2. **Occlusion duration stress test**: Create synthetic occlusion scenarios of varying lengths (0.5s, 2s, 5s) in the Edinburgh dataset. Measure identity preservation rate and IDF1 as occlusion duration increases.
3. **Camera angle comparison**: Re-run the pipeline on a subset of Edinburgh videos using side-view frames (if available) or artificially rotated top-view frames. Compare detection, tracking, and classification performance to assess angle sensitivity.