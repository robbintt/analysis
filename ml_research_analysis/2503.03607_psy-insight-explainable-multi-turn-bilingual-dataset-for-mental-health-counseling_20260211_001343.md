---
ver: rpa2
title: 'Psy-Insight: Explainable Multi-turn Bilingual Dataset for Mental Health Counseling'
arxiv_id: '2503.03607'
source_url: https://arxiv.org/abs/2503.03607
tags:
- dialogue
- psy-insight
- counseling
- dataset
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Psy-Insight, the first bilingual, explainable
  multi-turn counseling dataset for mental health support. It collects real human
  counseling dialogues from blogs and books, annotating them with multi-task labels
  (emotion, psychotherapy, strategy, topic) and detailed reasoning explanations at
  both session and turn levels.
---

# Psy-Insight: Explainable Multi-turn Bilingual Dataset for Mental Health Counseling

## Quick Facts
- **arXiv ID:** 2503.03607
- **Source URL:** https://arxiv.org/abs/2503.03607
- **Reference count:** 15
- **Primary result:** Introduces first bilingual, explainable multi-turn counseling dataset with multi-task labels and reasoning annotations, showing improved LLM response quality.

## Executive Summary
This paper introduces Psy-Insight, the first bilingual, explainable multi-turn counseling dataset for mental health support. It collects real human counseling dialogues from blogs and books, annotating them with multi-task labels (emotion, psychotherapy, strategy, topic) and detailed reasoning explanations at both session and turn levels. Experiments show that fine-tuning large language models on Psy-Insight improves response quality across multiple metrics (e.g., BERTScore, BLEU, METEOR) compared to baselines, and that models with reasoning annotations outperform those trained on dialogue alone. Human evaluations confirm that Psy-Insight generates more interactive, helpful, and comforting counseling responses than existing datasets, though gaps remain compared to professional therapists. The dataset enables multi-task learning and explainable reasoning, advancing LLM-based mental health support.

## Method Summary
The dataset construction involves extracting dialogue sessions from Chinese and English counseling books and blogs, then annotating each turn with multi-task labels (emotion, strategy, topic, psychotherapy) and chain-of-thought reasoning. The fine-tuning process uses LoRA on Mistral-7B (English) and GLM4-9B (Chinese) with a custom Prompt Loss Weight masking scheme. Models are trained to generate observation and reasoning tokens as intermediate steps before producing the final response. Evaluation includes automatic metrics (BERTScore, METEOR, BLEU) and human assessments of interactivity, helpfulness, and comfort.

## Key Results
- Fine-tuning on Psy-Insight with reasoning annotations achieves higher BERTScore-P (0.912) and METEOR (0.397) than dialogue-only training.
- Models trained on real human counseling dialogues outperform those trained on synthetic data in human evaluations for interactivity and comfort.
- Multi-task learning with diverse labels improves response quality compared to single-task dialogue generation.

## Why This Works (Mechanism)

### Mechanism 1: Multi-Task Label Supervision Improves Counselor Response Quality
Fine-tuning LLMs with diverse task labels (emotion, strategy, topic, psychotherapy) improves response generation by forcing the model to develop a shared representation that understands counseling context from multiple angles. This richer semantic grounding guides the model toward more contextually appropriate responses. Evidence shows Mistral-7B with reasoning achieves higher BERTScore-P (0.912) and METEOR (0.397) than dialogue-only fine-tuning.

### Mechanism 2: Chain-of-Thought Reasoning Annotations Enable Step-by-Step Counselor Logic
Incorporating turn-level reasoning and session-level guidance annotations trains models to generate observation and reasoning tokens before responses. This chain-of-thought fine-tuning shapes models to first analyze client state and plan therapeutic intent, producing more coherent and strategy-aligned responses. The model learns to decompose complex counseling tasks into intermediate, verifiable steps.

### Mechanism 3: Real Human Dialogue Provides Superior Interaction Patterns for Fine-Tuning
Models fine-tuned on face-to-face human counseling dialogues produce more interactive and comforting responses than those trained on synthetic data. Human counseling data contains nuanced interaction patterns (shorter therapist turns, more questions, gradual trust-building) that synthetic data often lacks, teaching the model better conversational pacing and empathy.

## Foundational Learning

**Multi-Task Learning (MTL) in NLP**
- Why needed: Understanding how a single model can be optimized for multiple related objectives simultaneously, leading to better generalization than separate models.
- Quick check: If you fine-tune a model only to predict the next dialogue turn, what latent skills might it fail to develop compared to a model also trained to predict the speaker's emotion and strategy?

**Chain-of-Thought (CoT) Reasoning**
- Why needed: Grasping how providing explicit reasoning steps in training data can teach a model to decompose complex tasks into intermediate, verifiable steps.
- Quick check: How might a model's response differ if it was trained to first output "Client appears anxious about school" before generating "It sounds like school is really stressful right now"?

**Evaluation Metrics for Dialogue (BERTScore, METEOR, Human Eval)**
- Why needed: Recognizing the limitations of n-gram overlap metrics and the importance of semantic similarity and human judgment for assessing empathetic dialogue.
- Quick check: A model achieves high BLEU score but human evaluators rate responses as "robotic." What might this indicate about the model's training data or objective?

## Architecture Onboarding

**Component map:** Raw text extraction → Dialogue cleaning → Multi-task annotation → Hierarchical dataset assembly → LoRA fine-tuning with PLW masking → Evaluation with automatic metrics and human assessment

**Critical path:** High-quality, privacy-preserved dialogue extraction → Accurate mapping of descriptive text to turn-level reasoning → Consistent multi-task labeling → Fine-tuning with CoT-inclusive instruction format

**Design tradeoffs:**
- Real vs. Synthetic Data: Real data offers authentic interaction patterns but is limited in scale; synthetic data is scalable but lacks nuanced empathy
- Annotation Granularity: Fine-grained (turn-level) reasoning offers precise control but is expensive; session-level summaries are coarser but cheaper
- Model Size: Larger models may capture reasoning better but are costly; smaller models are efficient but may require more careful prompting

**Failure signatures:**
- Metric Disconnection: High automatic scores but low human-rated "comfort"
- CoT Collapse: Model generates generic reasoning that doesn't logically connect to the response
- Cultural/Domain Misalignment: Model trained on book-based English dialogues fails to adapt to informal chat-based Chinese counseling requests

**First 3 experiments:**
1. Ablation on Annotations: Fine-tune Mistral-7B on three variants (dialogue only, dialogue + labels, dialogue + labels + reasoning) to isolate contribution of each annotation layer
2. Cross-Lingual Transfer: Train bilingual model on combined Chinese and English data, evaluate performance in each language
3. Expert-in-the-Loop Refinement: Apply DPO to model already fine-tuned on Psy-Insight using expert assessments as preference data

## Open Questions the Paper Calls Out

**Open Question 1:** Can DPO or RL utilizing expert assessment comments effectively close the performance gap between the model and human therapists? The authors plan to utilize expert assessments with techniques like reinforcement learning or DPO, which was not explored in the current study.

**Open Question 2:** Does the improvement in response quality from reasoning annotations generalize to the Chinese language subset? The paper only evaluates instruction tuning in English dialogue due to limited resources, leaving Chinese subset efficacy unconfirmed.

**Open Question 3:** Does the automated mapping of descriptive text to dialogue turns introduce alignment noise that limits the model's ability to learn accurate counseling reasoning? The construction workflow relies on automated mapping with human verification rather than writing reasoning, potentially introducing semantic drift.

## Limitations
- Dataset construction relies heavily on automated annotation pipelines using ChatGPT, with only 8.4% of sessions expert-validated
- Evaluation depends significantly on human assessment that may not fully capture therapeutic effectiveness or clinical safety
- Focus on face-to-face counseling extracted from books may not represent conversational dynamics of text-based counseling platforms

## Confidence

**High Confidence (9/10):** Dataset construction methodology is well-documented and reproducible with clear hierarchical structure and annotation process.

**Medium Confidence (7/10):** Experimental results showing improved performance with reasoning annotations are internally consistent but require external validation.

**Low Confidence (4/10):** Clinical utility and therapeutic effectiveness of generated responses remain unproven despite improved metrics and human preference scores.

## Next Checks

1. **External Annotation Validation Study:** Conduct blind expert review of randomly sampled reasoning annotations and responses across the full dataset to assess consistency and clinical validity of the automated pipeline.

2. **Cross-Domain Transfer Evaluation:** Test models fine-tuned on Psy-Insight in different counseling contexts (crisis intervention vs. supportive counseling, text-based chat vs. face-to-face) to validate generalization of learned reasoning patterns.

3. **Longitudinal Safety Assessment:** Implement systematic evaluation framework to test whether responses maintain therapeutic appropriateness over extended conversations and across diverse client populations, including edge case testing for harmful patterns.