---
ver: rpa2
title: Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence
  of Language Model
arxiv_id: '2508.04403'
source_url: https://arxiv.org/abs/2508.04403
tags:
- user
- response
- utterance
- rfull
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a prediction confidence model (PCM) for dialogue
  response prefetching based on semantic similarity rather than exact word matching.
  The authors fine-tune a BERT model to estimate the probability that the semantic
  similarity between a predicted and complete user utterance exceeds a threshold.
---

# Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model

## Quick Facts
- arXiv ID: 2508.04403
- Source URL: https://arxiv.org/abs/2508.04403
- Reference count: 0
- Primary result: Semantic PCM achieves 2-4x higher prediction gain (up to 1061ms) than word-level PCMs by triggering earlier while preserving response quality

## Executive Summary
This paper introduces a semantic similarity-based prediction confidence model (PCM) for dialogue response prefetching that enables earlier prefetching than word-level matching while maintaining response quality. The authors fine-tune a BERT model to estimate the probability that semantic similarity between predicted and complete user utterances exceeds a threshold, using S-BERT embeddings for semantic comparison. Experiments on MultiWOZ, SpokenWOZ, and JMultiWOZ datasets demonstrate the semantic PCM achieves significantly higher prediction gains (419-1061ms) compared to literal matching approaches. Human evaluation confirms prefetched responses maintain naturalness comparable to responses generated from complete utterances, with optimal semantic thresholds varying by language—English requires T≥0.90 while Japanese needs T≥0.95.

## Method Summary
The method uses a prediction model (Qwen2.5-14B-Instruct with LoRA fine-tuning) to generate complete user utterances from partial incremental input, then employs a BERT-based PCM to estimate whether semantic similarity between the predicted and actual complete utterance exceeds a threshold. Semantic similarity is computed using S-BERT (stsb-xlm-r-multilingual) embeddings. The PCM is trained on 50 dialogues from test datasets using Focal Loss, with features including dialogue history, partial utterance, and predicted complete utterance. The system prefetches responses when PCM confidence exceeds the semantic threshold, reducing user-perceived latency by allowing earlier response generation. The approach is evaluated across English (MultiWOZ, SpokenWOZ) and Japanese (JMultiWOZ) datasets with language-specific optimal thresholds.

## Key Results
- Semantic PCM achieves 2-4x higher prediction gain (up to 1061ms) compared to word-level PCMs
- Optimal semantic similarity thresholds: T≥0.90 for English, T≥0.95 for Japanese datasets
- Human evaluation shows prefetched responses maintain naturalness comparable to actual responses (PR<R<0.50)
- Japanese requires higher thresholds than English due to sentence-final syntactic heads affecting semantic coherence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Semantic similarity matching between predicted and actual utterances enables earlier prefetching than word-level matching while preserving response quality.
- **Mechanism:** The PCM estimates P(S-BERT(ŷ_full,t, ŷ_full) > T), where S-BERT computes sentence-level semantic embeddings. When semantic similarity exceeds threshold T, prefetching triggers. This captures user intent even with incomplete lexical matches (e.g., "What is the weather" vs. "What is the weather like today?"—semantically similar despite missing words).
- **Core assumption:** Intent-aligned responses can be generated from semantically similar inputs without requiring exact word matches; dialogue models respond to meaning, not literal strings.
- **Evidence anchors:**
  - [abstract] "proposed PCM uses semantic similarity between predicted and complete user utterances to determine prefetching"
  - [Page 1, Section 1] "prefetching success depends on whether the user's intent is captured and an appropriate response is prepared"
  - [corpus] Related work on next-utterance prediction (SayNext-Bench, arXiv:2602.00327) confirms LLMs can predict dialogue intent semantically; however, direct evidence for semantic-based prefetching specifically is limited in corpus neighbors.
- **Break condition:** If response generation depends critically on specific slot values or entities not yet spoken, semantic similarity alone cannot compensate; prediction will fail regardless of intent alignment.

### Mechanism 2
- **Claim:** Higher semantic similarity thresholds yield more conservative but higher-quality prefetching, with language-dependent optimal values.
- **Mechanism:** Threshold T controls the precision-recall tradeoff. Lower T increases prefetch rate (SPR) but raises failed prefetch rate (FPR); higher T reduces both but ensures prefetched responses match actual response quality. Japanese requires T ≥ 0.95 vs. English T ≥ 0.90 because Japanese syntax places critical information (verbs, negation) at utterance ends.
- **Core assumption:** The semantic similarity score correlates with downstream response quality; the relationship is monotonic but language-specific.
- **Evidence anchors:**
  - [Page 3, Section 5] "Japanese requires a higher semantic similarity threshold than English to maintain the quality of the prefetched response group"
  - [Page 3, Table 1] PR<R drops below 0.50 for Multi_lsbert090 (T=0.90) but requires JMulti_lsbert095 (T=0.95)
  - [corpus] No direct corpus evidence on language-dependent thresholding for prefetching; this finding appears novel to this work.
- **Break condition:** If threshold calibration is done on one domain and applied to another with different syntactic structures or utterance lengths, the optimal T may shift unpredictably.

### Mechanism 3
- **Claim:** Prefetching gain (ΔT) is maximized when PCM triggers early in utterance with sufficient semantic signal, reducing user-perceived latency by hundreds of milliseconds.
- **Mechanism:** Prediction gain = time between prefetch trigger and EOS detection. The PCM must decide before ŷ_t reaches ŷ_full. Earlier correct decisions yield larger gains; delayed or incorrect decisions either reduce gain or incur fallback latency. Semantic matching allows earlier decisions because partial utterances can match intent before all words are spoken.
- **Core assumption:** The prediction model generates plausible complete utterances from partial input; ASR is sufficiently accurate (gold transcriptions used in experiments).
- **Evidence anchors:**
  - [Page 2, Equation 1] T_UPL definition: successful prefetching sets T_PF = -ΔT, directly reducing latency
  - [Page 3, Table 2] P-Gain (ms) reaches 1061.25ms for Spoken_lsbert075; even T=0.95 yields 419.20ms
  - [corpus] Look-ahead techniques (INTERSPEECH 2024, cited as [10]) support feasibility of pre-computation, though semantic prefetching specifically is not covered.
- **Break condition:** If ASR errors accumulate or prediction model hallucinates completions, early triggers will prefetch incorrect responses, degrading user experience despite low latency.

## Foundational Learning

- **Concept: Sentence Embeddings (S-BERT/sentence-transformers)**
  - Why needed here: Core to PCM—maps utterances to dense vectors where cosine similarity approximates semantic relatedness, enabling intent-level matching.
  - Quick check question: Given two utterances "Book a table for two" and "I need a reservation for 2 people," would S-BERT similarity exceed 0.85? (Expected: yes, typically 0.85-0.92 for intent-aligned utterances.)

- **Concept: Prediction Confidence Modeling**
  - Why needed here: Determines *when* to prefetch; PCM outputs probability that prediction is good enough to act on.
  - Quick check question: If PCM outputs P(confidence) = 0.7 and threshold is 0.5, should the system prefetch? What if threshold is 0.8? (Expected: prefetch at 0.5, wait at 0.8.)

- **Concept: User-Perceived Latency (UPL) in Cascade SDS**
  - Why needed here: The metric this work optimizes; UPL = time from user's EOS to system's first audio output, including response generation.
  - Quick check question: In a cascade SDS, if ASR takes 200ms, EOS detection takes 300ms, and response generation takes 500ms, what is UPL without prefetching? With 400ms prefetch gain? (Expected: 800ms without; 400ms with prefetch.)

## Architecture Onboarding

- **Component map:**
  User Speech -> Incremental ASR -> Partial Utterance ŷ_t -> Prediction Model -> ŷ_full,t -> PCM (Semantic) -> P(S-BERT > T) -> Decision: Prefetch? Yes->Response Generator->Prefetched Response Cache->At EOS: Dispatch from Cache
  Decision: Prefetch? No->Wait for EOS->Response Generator

- **Critical path:** Prediction Model -> PCM -> Prefetch Decision. Latency of this path must be < prediction gain or prefetching provides no benefit. Paper uses Qwen2.5-14B-Instruct as prediction model; inference speed matters.

- **Design tradeoffs:**
  - **Threshold T:** Higher T -> fewer prefetches, higher quality; lower T -> more prefetches, more errors. Paper suggests T=0.90 (English) / 0.95 (Japanese) as starting points.
  - **Prediction model size:** Larger models (e.g., Qwen2.5-14B) may predict better but add latency; smaller models may require lower T.
  - **Training data for PCM:** Paper uses only 50 dialogues for fine-tuning; unclear if this generalizes to new domains without retraining.

- **Failure signatures:**
  - High FPR (>30%) with low SPR (<15%): Threshold too high or prediction model underfitting; semantic space misaligned.
  - Low prediction gain (<100ms): PCM triggers too late; consider earlier partial inputs or reduce PCM inference latency.
  - PR<R significantly >0.5 in human eval: Prefetched responses perceived as worse than actual; threshold too low for language/domain.
  - Language mismatch: Applying English-calibrated T to Japanese will degrade quality (and vice versa).

- **First 3 experiments:**
  1. **Threshold sweep:** Train PCM with T ∈ {0.75, 0.80, 0.85, 0.90, 0.95} on a held-out dialogue set. Measure SPR, FPR, P-Gain, and ROUGE/S-BERT response quality. Identify optimal T where PR<R ≤ 0.5.
  2. **Language comparison:** Replicate threshold sweep on both English (MultiWOZ) and Japanese (JMultiWOZ) data. Confirm language-dependent T (expect Japanese requires +0.05 higher threshold than English).
  3. **Noise robustness test:** Introduce synthetic ASR errors (e.g., word dropout, substitution at 5-10% rate) into ŷ_t before prediction. Compare semantic PCM vs. literal PCM performance degradation rates. Expect semantic PCM to degrade more gracefully for intent-level predictions.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the proposed Prediction Confidence Model (PCM) perform under realistic conditions involving Automatic Speech Recognition (ASR) errors and varying response generation times?
- **Basis in paper:** [explicit] The authors state that "a more realistic analysis, considering non-ideal factors such as ASR errors, and response generation time, is necessary for practical applications."
- **Why unresolved:** The current study relied on gold transcriptions ($\hat{y}_{full} = y_{full}$) and did not account for the compounding effects of transcription errors or the computational latency of the response generation module.
- **What evidence would resolve it:** Evaluation results of the semantic PCM using incremental, noisy ASR hypotheses instead of ground-truth text, alongside measurements of actual end-to-end latency.

### Open Question 2
- **Question:** Can a dynamic decision mechanism improve prefetching accuracy for cases where the prefetched response is predicted to be of lower quality than a standard response?
- **Basis in paper:** [explicit] The authors note that "in some settings, the response to a prefetched utterance may be poorer" and identify "the decision to prefetch dynamically for these examples" as a topic for future work.
- **Why unresolved:** The current model relies on a fixed semantic similarity threshold, which may not successfully capture all instances where the intent is technically similar but the resulting generation quality diverges.
- **What evidence would resolve it:** A mechanism that assesses potential response quality in real-time before committing to prefetching, demonstrating a reduction in unnatural or erroneous outputs compared to the static threshold approach.

### Open Question 3
- **Question:** Do the syntactic differences between languages (e.g., head-final vs. head-initial) necessitate fundamentally different semantic similarity thresholds to maintain response naturalness?
- **Basis in paper:** [inferred] The authors observe that Japanese required a higher threshold ($T=0.95$) than English ($T=0.90$) to maintain quality, hypothesizing this is "probably due to the syntactic nature of Japanese."
- **Why unresolved:** The paper identifies the correlation between language type and threshold requirements but does not isolate syntactic structure as the definitive cause through controlled experimentation.
- **What evidence would resolve it:** A cross-linguistic analysis controlling for sentence structure length and word order, or testing the model on additional languages with similar syntactic properties to Japanese.

## Limitations

- **Domain generalization gap:** PCM trained on only 50 dialogues from test sets may not generalize to new domains or languages
- **ASR integration assumption:** Experiments use gold transcriptions, not accounting for recognition errors in real deployments
- **Prediction model dependency:** PCM may trigger prefetching for semantically plausible but factually incorrect predictions

## Confidence

**High Confidence:** The empirical observation that semantic similarity thresholds vary by language (English T≥0.90 vs. Japanese T≥0.95) is strongly supported by the experimental results. The monotonic relationship between threshold, prefetch rate, and response quality is well-established within the study.

**Medium Confidence:** The claim that semantic PCM achieves 2-4x higher prediction gain than word-level PCMs is supported by the reported metrics, but the comparison assumes similar prediction model quality and may not hold if the prediction model's semantic reasoning is weaker than its literal matching. The choice of S-BERT for semantic similarity is reasonable but not the only possible embedding approach.

**Low Confidence:** The claim that the optimal threshold is universal within a language (T≥0.90 for all English, T≥0.95 for all Japanese) may not generalize beyond the tested domains. The paper does not explore threshold sensitivity to dialogue length, user verbosity, or domain complexity, which could affect optimal values.

## Next Checks

1. **Cross-Domain PCM Transferability:** Evaluate the PCM trained on MultiWOZ/SpokenWOZ on a distinct dialogue domain (e.g., customer service, technical support) to assess generalization. Measure whether the same thresholds (T≥0.90 English, T≥0.95 Japanese) maintain PR<R<0.50 and high prediction gain.

2. **ASR Noise Robustness:** Introduce realistic ASR error rates (5-15%) into incremental utterances and measure PCM performance degradation. Compare semantic PCM vs. literal PCM resilience to word substitutions, deletions, and insertions. Expect semantic PCM to degrade more gracefully for intent-level predictions but quantify the margin.

3. **Prediction Model Error Impact:** Systematically inject semantic-level errors (e.g., plausible but incorrect slot values) into predicted utterances and measure how often PCM triggers prefetching for responses that fail ROUGE/S-BERT quality checks. Analyze the tradeoff between early triggering and hallucination risk.