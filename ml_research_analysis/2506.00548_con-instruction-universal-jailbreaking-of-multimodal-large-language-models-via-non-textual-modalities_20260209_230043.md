---
ver: rpa2
title: 'Con Instruction: Universal Jailbreaking of Multimodal Large Language Models
  via Non-Textual Modalities'
arxiv_id: '2506.00548'
source_url: https://arxiv.org/abs/2506.00548
tags:
- instruction
- adversarial
- attack
- text
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Con Instruction, a method to jailbreak multimodal
  large language models (MLLMs) by using adversarial non-textual inputs such as images
  or audio. Instead of relying on text-based attacks, Con Instruction generates adversarial
  examples optimized to align with malicious instructions in the embedding space,
  bypassing traditional safety mechanisms.
---

# Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities

## Quick Facts
- **arXiv ID:** 2506.00548
- **Source URL:** https://arxiv.org/abs/2506.00548
- **Reference count:** 39
- **Primary result:** Introduces Con Instruction, a method to jailbreak multimodal LLMs using adversarial non-textual inputs, achieving 81.3% attack success rate on LLaVA-v1.5 (13B).

## Executive Summary
Con Instruction introduces a novel approach to jailbreaking multimodal large language models (MLLMs) by exploiting adversarial non-textual inputs such as images and audio. Unlike traditional text-based attacks, this method generates adversarial examples that align with malicious instructions in the embedding space, effectively bypassing conventional safety mechanisms. The approach operates in a gray-box setting with partial access to model components and demonstrates high success rates across various vision- and audio-language models including LLaVA-v1.5, InternVL, Qwen-VL, and Qwen-Audio. The authors also introduce an Attack Response Categorization (ARC) framework for more accurate evaluation of jailbreak success. Experimental results show that larger models are more vulnerable to these attacks, and while some defenses like MLLM-Protector and high-noise input perturbation are effective, adversarial training alone proves insufficient.

## Method Summary
Con Instruction is a universal jailbreaking method that exploits multimodal vulnerabilities in large language models by generating adversarial non-textual inputs. The approach creates adversarial examples optimized to align with malicious instructions in the embedding space, circumventing traditional safety mechanisms that primarily focus on textual content. Operating in a gray-box setting, the method requires only partial access to model components rather than full model transparency. The core innovation lies in targeting the joint embedding space where multimodal information is processed, allowing attackers to bypass safety classifiers that work primarily on text inputs. The method was evaluated across multiple vision- and audio-language models, demonstrating consistent effectiveness regardless of the underlying architecture.

## Key Results
- Con Instruction achieved attack success rates of 81.3% and 86.6% on LLaVA-v1.5 (13B) across different attack scenarios
- Larger MLLMs demonstrated greater vulnerability to multimodal adversarial attacks compared to smaller models
- Defenses like MLLM-Protector and high-noise input perturbation proved effective, but adversarial training alone was insufficient to prevent attacks
- The ARC framework revealed that attacks work by disrupting the model's internal safety perception rather than just evading detection

## Why This Works (Mechanism)
The attack succeeds by exploiting a fundamental gap in multimodal safety alignment: current safety mechanisms focus on textual inputs while adversarial examples in the non-textual embedding space bypass these defenses. By generating adversarial images or audio that align with malicious instructions in the joint embedding space, the attack disrupts the model's internal representation of safety. The t-SNE visualizations show that adversarial samples create dispersed distributions that evade textual safety classifiers. This approach works universally across different architectures because it targets the shared vulnerability in how multimodal models process and align information across modalities, rather than exploiting model-specific weaknesses.

## Foundational Learning
- **Multimodal Embedding Spaces**: Why needed - Understanding how different modalities are represented and aligned in joint spaces; Quick check - Verify that models learn consistent representations across text, image, and audio modalities
- **Gray-box Attack Settings**: Why needed - Most realistic threat model balancing attacker capability with practical constraints; Quick check - Confirm partial access assumptions match real-world attack scenarios
- **Safety Alignment Mechanisms**: Why needed - Understanding how models learn to refuse harmful content is crucial for identifying bypass vulnerabilities; Quick check - Map how textual safety classifiers interact with multimodal inputs
- **Adversarial Example Generation**: Why needed - Core technique for creating inputs that mislead model behavior while appearing normal to humans; Quick check - Verify perturbations remain imperceptible while achieving attack goals
- **Attack Response Categorization**: Why needed - Standard evaluation metrics may miss nuanced jailbreak behaviors; Quick check - Ensure ARC framework captures all meaningful variations in model responses
- **Model Size vs Vulnerability**: Why needed - Understanding how scale affects security properties guides deployment decisions; Quick check - Compare attack success rates across model families and sizes

## Architecture Onboarding

**Component Map:** Input Perturbation Module → Embedding Space Generator → Alignment Optimization → Safety Bypass Classifier → Output Generation

**Critical Path:** Adversarial example generation → Embedding space alignment → Safety mechanism bypass → Malicious instruction execution

**Design Tradeoffs:** The method trades computational cost of generating adversarial examples for universal effectiveness across architectures. Gray-box access requirement balances between realistic threat modeling and attack capability. The approach prioritizes bypassing safety mechanisms over maintaining input fidelity, accepting that some perturbations may be detectable to humans.

**Failure Signatures:** When attacks fail, models typically either correctly identify the input as adversarial (high noise detection) or maintain safety boundaries despite misalignment. Failed attacks show either sharp clustering in embedding space (indicating successful safety alignment) or rejection of the adversarial input entirely. Models with strong multimodal safety alignment show consistent refusal patterns even when embeddings are perturbed.

**First Experiments:**
1. Generate adversarial examples using standard perturbation techniques and measure baseline attack success rates
2. Compare embedding space distributions between benign and adversarial inputs using t-SNE visualization
3. Test transferability of adversarial examples across different MLLM architectures to verify universality claims

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the finding that larger MLLMs are more vulnerable to multimodal adversarial attacks generalize to models beyond 34B parameters?
- **Basis in paper:** [explicit] "Due to resource constraints, our experiments were conducted on models with a maximum size of 34B. It is important to note that our findings may not necessarily generalize to all architectures or larger models."
- **Why unresolved:** Resource limitations prevented testing at larger scales; the observed trend contradicts text-only safety alignment where larger models are typically more robust.
- **What evidence would resolve it:** Systematic evaluation of Con Instruction on 70B+ parameter models comparing attack success rates across model families and sizes.

### Open Question 2
- **Question:** Can combinations of defense strategies fully eliminate the threat from non-textual instruction attacks while maintaining model utility?
- **Basis in paper:** [explicit] "Adversarial training alone is insufficient to fully eliminate the threat posed by our attacks. While VLGuard has demonstrated resilience against various attacks... Con Instruction still achieves a success rate exceeding 25%."
- **Why unresolved:** The paper evaluated individual defenses (ECSO, MLLM-Protector, VLGuard, input perturbation) but not their combinations, leaving potential synergistic effects unexplored.
- **What evidence would resolve it:** Experiments combining multiple defense mechanisms (e.g., adversarial training + post-hoc filtering + input perturbation) with utility benchmarks.

### Open Question 3
- **Question:** How can safety alignment be extended to the non-textual embedding space to prevent adversarial alignment attacks at their source?
- **Basis in paper:** [inferred] from Finding A5 showing "adversarial samples disrupt the model's internal perception of safety" with dispersed t-SNE distributions that bypass textual safety classifiers.
- **Why unresolved:** Current safety mechanisms focus on textual inputs; the embedding-space vulnerability reveals a fundamental gap in multimodal safety alignment.
- **What evidence would resolve it:** Analysis of safety-critical representations across modalities and development of modality-agnostic alignment objectives that enforce safety in joint embedding spaces.

## Limitations
- The gray-box setting assumption may not reflect real-world attack scenarios where attackers have even less access to model internals
- The ARC evaluation framework introduces subjective judgments about what constitutes successful jailbreaking, potentially inflating success rates
- The paper does not address the temporal stability of adversarial examples - whether attacks remain effective as models are updated or retrained

## Confidence
- **High confidence:** The core technical methodology of using non-textual adversarial inputs for jailbreaking is sound and well-demonstrated. The experimental results showing high attack success rates across multiple models are reliable.
- **Medium confidence:** The claim of universality across different MLLM architectures is supported but could be stronger with more diverse model families tested. The defense evaluation results are credible but limited in scope.
- **Low confidence:** The practical real-world applicability of these attacks given the computational requirements and the assumption of partial model access. The long-term effectiveness of the proposed attacks as models evolve.

## Next Checks
1. Test attack transferability across models from different architectural families and training regimes to verify true universality claims
2. Evaluate attack effectiveness under stricter black-box conditions with zero access to model gradients or intermediate representations
3. Measure the computational overhead and generation time for adversarial examples to assess practical attack feasibility at scale