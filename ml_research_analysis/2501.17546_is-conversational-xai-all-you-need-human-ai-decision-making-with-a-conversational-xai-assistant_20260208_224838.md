---
ver: rpa2
title: Is Conversational XAI All You Need? Human-AI Decision Making With a Conversational
  XAI Assistant
arxiv_id: '2501.17546'
source_url: https://arxiv.org/abs/2501.17546
tags:
- user
- conversational
- trust
- system
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically compares a conversational XAI interface
  against a traditional XAI dashboard in a loan approval task. Participants using
  the conversational interface showed slightly better understanding and marginally
  higher trust, but both interfaces led to clear over-reliance on AI advice.
---

# Is Conversational XAI All You Need? Human-AI Decision Making With a Conversational XAI Assistant

## Quick Facts
- arXiv ID: 2501.17546
- Source URL: https://arxiv.org/abs/2501.17546
- Reference count: 40
- Key outcome: Conversational XAI interfaces increased engagement and perceived understanding but failed to improve appropriate reliance, with LLM agents amplifying over-reliance.

## Executive Summary
This paper empirically compares conversational XAI interfaces against traditional XAI dashboards in a loan approval task. While conversational interfaces showed slightly better user understanding and marginally higher trust, both interface types led to clear over-reliance on AI advice. The study found that conversational XAI interfaces create an "illusion of explanatory depth" that increases trust without improving objective understanding. Adding LLM-powered agents further amplified over-reliance by generating plausible but potentially unfaithful explanations. The research suggests that interactive XAI interfaces risk becoming persuasive technology rather than genuinely helpful tools, and designers should prioritize faithful explanations and critical reflection over enhanced conversational engagement.

## Method Summary
The study used a loan approval task with an 11-feature dataset and an XGBoost classifier trained to 70% accuracy. Participants completed a two-stage decision process: making an initial prediction, receiving AI advice with explanations, and then making a final decision. Five conditions were tested: Control (no XAI), Dashboard (GUI-based XAI), Conversational (rule-based chat), Evaluative Conversational (steering comparison), and LLM Agent (free-text with GPT-4 synthesis). Metrics included Relative AI Reliance (RAIR), Relative Self-Reliance (RSR), trust scales (TiA), and objective feature understanding (nDCG ranking).

## Key Results
- Users of both XAI dashboard and conversational XAI interfaces showed clear over-reliance on AI advice.
- Conversational XAI led to marginally better objective feature understanding and user trust compared to dashboard.
- LLM Agent condition showed significantly worse objective feature understanding and RSR compared to other conditions.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Conversational XAI interfaces operate as persuasive technology, creating an "illusion of explanatory depth" that increases user trust and reliance regardless of actual AI performance.
- **Mechanism:** Natural language interaction and guided exploration increase user engagement and perceived coherence. This heightened engagement lowers critical resistance, leading users to confuse the quality of the interaction with the quality of the AI logic, thereby increasing over-reliance.
- **Core assumption:** Users default to heuristic processing (trusting the messenger) when explanations feel interactive and fluent, rather than systematically verifying the AI's rationale.
- **Evidence anchors:**
  - [abstract] "Users of both the XAI dashboard and conversational XAI interfaces showed clear overreliance... potential cause... is the illusion of explanatory depth."
  - [section 6.1] "Participants perceived the conversational XAI interface to lead to a relatively better user understanding... Thus, optimizing the XAI interfaces as a persuasive technology... may not be the ideal approach."
  - [corpus] Paper 13588 ("Engaging with AI") supports that interface design significantly shapes trust in high-stakes environments.
- **Break condition:** If users possess high domain expertise or are subjected to cognitive forcing functions (e.g., forced prediction before advice), the persuasive effect of the conversational interface may diminish.

### Mechanism 2
- **Claim:** LLM-powered conversational agents amplify over-reliance by increasing the "plausibility" of explanations without corresponding gains in "faithfulness" or objective understanding.
- **Mechanism:** The LLM generates coherent, high-quality text elaborations based on raw XAI outputs (e.g., SHAP values). This linguistic fluency acts as a heuristic for accuracy, leading users to trust the explanation text more than the underlying data, masking the AI's limitations.
- **Core assumption:** Users equate linguistic fluency and conversational flexibility with computational reliability (a form of the ELIZA effect or "hallucination of competence").
- **Evidence anchors:**
  - [section 5.2.3] "Participants in the LLM Agent condition showed significantly worse RSR [Relative Self-Reliance] compared to the Control... and Dashboard conditions."
  - [section 5.2.1] "LLM Agent condition achieved significantly worse objective feature understanding than the Dashboard, CXAI, and ECXAI conditions."
  - [corpus] Paper 35273 ("Explanation User Interfaces") notes the tension between black-box models and user intelligibility, a gap LLMs may mask rather than bridge.
- **Break condition:** If the LLM is explicitly instructed to highlight model uncertainty or "seams" (limitations) in its responses, the amplification of over-reliance may be mitigated.

### Mechanism 3
- **Claim:** Adaptive "Evaluative" steering (comparing user criteria vs. AI criteria) failed to calibrate reliance because it did not fundamentally alter the persuasive nature of the interaction loop.
- **Mechanism:** The system attempted to nudge users to compare their mental models with the AI's (evaluative AI). However, because the conversational wrapper remained primarily affirmative and supportive, it reinforced the user's tendency to defer to the system rather than acting as a "devil's advocate."
- **Core assumption:** Mere exposure to the discrepancy between user and AI features is insufficient to trigger critical re-evaluation if the interface tone remains helpful rather than challenging.
- **Evidence anchors:**
  - [section 5.2.4] "No significant difference in user trust and appropriate reliance was found between experimental condition CXAI and ECXAI. Thus, H4 is not supported."
  - [section 6.3] "Evaluative conversations... fail to facilitate user understanding, calibrate user trust in the AI system, or mitigate over-reliance."
  - [corpus] Paper 29325 suggests cognitive capabilities impact decisions, implying simple interface nudges may be insufficient for deep cognitive shifts.
- **Break condition:** If evaluative steering introduced friction or explicit conflict (e.g., "The AI disagrees with your top feature, prove why you are right"), the mechanism might effectively force appropriate reliance.

## Foundational Learning

- **Concept: Appropriate Reliance (RAIR & RSR)**
  - **Why needed here:** Accuracy alone is misleading. This paper measures Relative AI Reliance (relying on AI when it is right) vs. Relative Self-Reliance (relying on self when AI is wrong). You cannot interpret the study results without understanding that "high reliance" often meant "over-reliance."
  - **Quick check question:** If a user agrees with an AI 90% of the time, but the AI is wrong 50% of the time, is this "appropriate reliance"?

- **Concept: Illusion of Explanatory Depth**
  - **Why needed here:** This is the core theoretical explanation for why conversational interfaces failed. It posits that people feel they understand complex systems better than they actually do because the explanation "feels" complete.
  - **Quick check question:** Why might a user feel more confident in a system after a chat interaction even if their objective test scores (feature understanding) drop?

- **Concept: Faithfulness vs. Plausibility in XAI**
  - **Why needed here:** The paper highlights a danger where LLMs make explanations "plausible" (linguistically smooth) without being "faithful" (accurate to the model weights). Engineering systems must distinguish between generating text that sounds right and text that is right.
  - **Quick check question:** An LLM summarizes a SHAP plot beautifully but subtly hallucinates a causal relationship. Is this a failure of faithfulness or plausibility?

## Architecture Onboarding

- **Component map:** Decision Context (Loan Approval) -> Backend (XGBoost Classifier) -> Explainer Layer (OmniXAI Library) -> Interface Layer (Dashboard, Conversational, LLM-Agent)
- **Critical path:** User makes independent initial decision → System reveals AI prediction + Explanation (Interface dependent) → User interacts (Chat or Dashboard) to query specific features → User makes final decision (opportunity to switch) → System logs Switch Fraction and Accuracy-wid
- **Design tradeoffs:** Engagement vs. Calibration (Increasing interaction time increased engagement but hurt objective understanding and calibration); Flexibility vs. Reliability (Free-text LLM input allows broader queries but introduces the risk of "plausible" hallucinations)
- **Failure signatures:** High Switch Fraction + Low Accuracy (Blind Over-reliance); Confidence Gap Explosion (High confidence post-advice despite low objective understanding); Propensity to Trust Correlation (User's Propensity to Trust predicts performance better than Objective Feature Understanding)
- **First 3 experiments:** 1) Baseline Verification: Replicate "Dashboard vs. Control" study to confirm XAI increases over-reliance in your domain; 2) Faithfulness Audit: Run LLM Agent pipeline on synthetic data where ground truth explanation is known; 3) Cognitive Forcing Injection: Modify conversational agent to require user to justify why they disagree with AI before seeing AI's rationale

## Open Questions the Paper Calls Out

- **Question:** Can integrating cognitive forcing functions into conversational XAI dialogues effectively calibrate user trust and reduce over-reliance?
- **Basis in paper:** [explicit] The authors suggest future work should combine "evaluative conversational XAI with cognitive forcing functions... to help calibrate user trust."
- **Why unresolved:** The current study tested evaluative steering and LLM agents but found they often amplified over-reliance; specific cognitive forcing mechanisms within the dialogue were not evaluated.
- **What evidence would resolve it:** An empirical study comparing standard conversational XAI against a version with embedded cognitive forcing interventions (e.g., requiring users to articulate why AI advice might be wrong).

- **Question:** How does user AI literacy moderate the effectiveness of conversational XAI interfaces in facilitating appropriate reliance?
- **Basis in paper:** [explicit] The authors highlight the need for further work to ensure "users with varying AI literacy... can equally benefit from AI systems."
- **Why unresolved:** While the study identified "propensity to trust" as a covariate, it did not deeply analyze how specific AI literacy levels interact with the conversational interface to affect reliance.
- **What evidence would resolve it:** A factorial experiment analyzing the interaction effect between users' objective AI literacy levels and the type of XAI interface (Dashboard vs. Conversational) on reliance metrics.

- **Question:** Does the "illusion of explanatory depth" causally mediate the relationship between LLM-enhanced conversation quality and user over-reliance?
- **Basis in paper:** [inferred] The authors reason that LLM agents amplified over-reliance likely due to the "illusion of explanatory depth," but the study measured reliance outcomes rather than the psychological mechanism itself.
- **Why unresolved:** It remains unclear if the over-reliance was caused by the specific "illusion" mechanism or simply by increased engagement and trust in the LLM's fluency.
- **What evidence would resolve it:** A mediation analysis measuring the disconnect between perceived and actual understanding (the illusion) to see if it statistically explains the variance in over-reliance between rule-based and LLM-based agents.

## Limitations

- The loan approval task uses a fixed dataset and model, limiting generalizability to other domains.
- The LLM Agent condition's performance depends heavily on the specific GPT-4 configuration and system prompts, which are not fully detailed.
- The study population appears to be university students/researchers, which may not represent real-world domain experts who would use such systems professionally.

## Confidence

- **High Confidence:** The finding that conversational XAI interfaces increase user engagement and perceived understanding while failing to improve objective understanding or appropriate reliance is well-supported by the experimental data.
- **Medium Confidence:** The mechanism of "illusion of explanatory depth" as the primary cause of over-reliance is plausible but not definitively proven; other factors like reduced cognitive effort or confirmation bias may also contribute.
- **Low Confidence:** The specific claim that LLM-powered agents "amplify over-reliance" beyond the rule-based conversational interface requires more careful analysis, as the LLM condition showed worse objective understanding but the confidence measures were mixed.

## Next Checks

1. **Cross-Domain Replication:** Test the same experimental conditions with a medical diagnosis task (e.g., diabetic retinopathy classification) to verify whether conversational interfaces consistently fail to improve appropriate reliance across domains.
2. **Expert User Study:** Conduct the experiment with professional loan officers or financial analysts to determine if domain expertise mitigates the illusion of explanatory depth effect.
3. **Cognitive Load Measurement:** Add objective cognitive load measures (e.g., dual-task performance, eye-tracking) to distinguish between engagement-driven and effort-driven over-reliance mechanisms.