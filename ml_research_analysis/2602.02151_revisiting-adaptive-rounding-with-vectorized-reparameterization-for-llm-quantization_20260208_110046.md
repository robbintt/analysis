---
ver: rpa2
title: Revisiting Adaptive Rounding with Vectorized Reparameterization for LLM Quantization
arxiv_id: '2602.02151'
source_url: https://arxiv.org/abs/2602.02151
tags:
- rounding
- vqround
- quantization
- adaptive
- gptq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper revisits adaptive rounding for LLM quantization by proposing
  VQRound, a parameter-efficient framework that reparameterizes the rounding matrix
  using vector quantization (VQ). Instead of optimizing a dense element-wise rounding
  matrix, VQRound learns a compact codebook that maps weight groups to shared rounding
  decisions, drastically reducing trainable parameters to ~0.2% while maintaining
  quantization quality.
---

# Revisiting Adaptive Rounding with Vectorized Reparameterization for LLM Quantization

## Quick Facts
- arXiv ID: 2602.02151
- Source URL: https://arxiv.org/abs/2602.02151
- Authors: Yuli Zhou; Qingxuan Chen; Luca Benini; Guolei Sun; Yawei Li
- Reference count: 40
- Primary result: VQRound achieves parameter-efficient LLM quantization with ~0.2% trainable parameters while maintaining perplexity comparable to full-parameter adaptive rounding

## Executive Summary
This paper proposes VQRound, a parameter-efficient framework for adaptive rounding in LLM post-training quantization. By reparameterizing the rounding matrix using vector quantization (VQ), VQRound learns a compact codebook that maps weight groups to shared rounding decisions, drastically reducing trainable parameters while maintaining quantization quality. The authors theoretically analyze error propagation under VQ reparameterization, showing its superiority over low-rank and Kronecker alternatives for controlling element-wise worst-case error (L∞ norm), which is critical for heavy-tailed weight distributions in LLMs. They also introduce a Hessian-aware residual initialization scheme to improve optimization stability.

## Method Summary
VQRound operates by first applying Hessian-aware initialization to compute base integer matrices and residual rounding matrices, then compressing these residuals using vector quantization with a learned codebook. The method divides the rounding matrix into vectors, learns a compact codebook of centroids, and maps each vector to its nearest centroid. This reduces trainable parameters from O(mn) to O(kd) while constraining per-element reconstruction error. The framework supports both block-wise reconstruction and end-to-end fine-tuning with cross-layer error compensation, optimized using Adam on the codebook parameters with KL divergence loss and rounding regularizer.

## Key Results
- VQRound achieves ~0.2% trainable parameters compared to full adaptive rounding while maintaining perplexity
- Outperforms LoRA and Kronecker-SVD alternatives on L∞ error control for heavy-tailed weight distributions
- Hessian-aware initialization improves perplexity by 10-17 points on OPT-125M compared to naive initialization
- Compatible with existing quantization methods like GPTQ and QuaRot for additional error reduction

## Why This Works (Mechanism)

### Mechanism 1: Vector Quantization Reparameterization for L∞ Error Control
VQ reparameterization outperforms low-rank and Kronecker decomposition for controlling element-wise worst-case error in the rounding matrix. Instead of optimizing a dense rounding matrix H ∈ [0,1]^(m×n), VQRound divides H into L vectors of length d and learns a codebook C with k≪L centroids. Each vector maps to its nearest centroid, reducing parameters from O(mn) to O(kd) while constraining per-element reconstruction error via local cell geometry. This is critical for heavy-tailed weight distributions where low-rank methods may produce localized residual spikes that trigger clipping saturation.

### Mechanism 2: Hessian-Aware Residual Initialization
Rounding matrix initialization informed by second-order curvature improves optimization stability and final quantization quality. The method processes weights column-by-column using RTN, computes normalized quantization error, propagates to unprocessed columns via Hessian coupling, then initializes rounding variables as H̃_j = W_j/s - ⌊W_j/s⌋ - E_j. This sequential error compensation provides a better starting point than independent residuals, with empirical improvements of 10-17 perplexity points on OPT-125M.

### Mechanism 3: End-to-End Codebook Optimization with Cross-Layer Error Compensation
Joint optimization of rounding codebooks across all layers enables error cancellation beyond block-wise reconstruction boundaries. By freezing all model parameters except the VQ codebook and minimizing KL divergence between teacher and student logits plus a rounding regularizer, the method allows rounding errors from different layers to compensate each other. This global optimization extends beyond purely local block-wise corrections, matching or slightly outperforming block-wise reconstruction across OPT, LLaMA, LLaMA2 families.

## Foundational Learning

- **Concept: Adaptive Rounding (vs. Round-to-Nearest)**
  - Why needed: VQRound builds on adaptive rounding theory where quantization error Δw^T·H(w)·Δw contains cross-terms that RTN ignores; understanding this motivates why joint rounding optimization matters
  - Quick check: Given two weights with opposite quantization errors that interact through positive off-diagonal Hessian entries, would rounding both to nearest increase or decrease total loss compared to one rounding up and one down?

- **Concept: Vector Quantization as Discrete Representation Learning**
  - Why needed: VQRound repurposes VQ-VAE-style codebook learning for a different objective (rounding matrix compression); understanding the trade-off between codebook size, vector dimension, and reconstruction fidelity is essential
  - Quick check: If you double the codebook size k while halving the vector dimension d, what happens to the total number of trainable parameters and the per-vector representation capacity?

- **Concept: Lipschitz Continuity and Error Propagation**
  - Why needed: The paper proves the rectified sigmoid h(·) is Lipschitz with constant L=(ζ-γ)/4; this bounds how errors in the latent matrix A propagate to the rounding matrix H, explaining why controlling L∞ error in A matters
  - Quick check: If the latent matrix approximation has worst-case error ||Ã-A||_∞ = 0.05 and L=0.3, what is the maximum per-element error in the rounding matrix before clipping?

## Architecture Onboarding

- **Component map:** Weight matrix W → Hessian-aware initialization → B + R → K-means VQ initialization → Reshape to vectors → Codebook lookup → Reconstruct Ã → Apply h(·) → Final rounding matrix H̃ → Quantized weights W_Q

- **Critical path:** Weight matrix W → Hessian-aware initialization → B + R → K-means VQ initialization → Reshape to vectors → Codebook lookup → Reconstruct Ã → Apply h(·) → Final rounding matrix H̃ → Quantized weights W_Q

- **Design tradeoffs:**
  - Codebook size k vs vector dimension d: Larger k improves representation capacity but increases parameters; d=8 with k=4096 found optimal
  - Block-wise vs E2E: Block-wise is memory-efficient and parallelizable; E2E enables cross-layer compensation but requires full model in memory
  - Calibration samples: Paper uses 128 samples; fewer samples risk overfitting, more provide diminishing returns

- **Failure signatures:**
  - NaN during initialization with LoRA/Kronecker baselines: Indicates low-rank methods fail to control element-wise tails
  - OOM with AdaRound: Full-parameter adaptive rounding exhausts 48GB GPU at OPT-2.7B; VQRound avoids this via 0.07% parameter ratio
  - Convergence stalls: If β annealing too aggressive, rounding variables freeze prematurely

- **First 3 experiments:**
  1. Reproduce VQ vs alternatives on a single layer: Initialize rounding matrix on one linear layer of OPT-350M using VQ, LoRA-SVD, and Kronecker-SVD; compare ||H-H̃||_∞ and perplexity under soft/hard rounding
  2. Ablate Hessian-aware vs naive initialization: Run VQRound on OPT-125M with and without Hessian propagation; measure perplexity gap
  3. Test compatibility with GPTQ: Apply VQRound after GPTQ preprocessing on LLaMA-7B at 3-bit; verify perplexity improvement

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions in the traditional sense, but several areas remain unexplored including inference latency overhead from VQ table-lookup operations, robustness to reduced calibration dataset sizes, and generalization to sparse MoE model architectures.

## Limitations
- Hessian computation method for initialization is underspecified, creating potential reproducibility gaps
- Heavy-tailed weight distribution assumption may not hold for all LLM families, limiting VQRound's advantage
- E2E optimization requires full model in memory, reducing practical scalability despite parameter efficiency gains
- Limited ablation on codebook size k and vector dimension d beyond the d=8, k=4096 default configuration

## Confidence
- **High confidence:** VQ reparameterization achieves significant parameter reduction (0.07-0.39% of full adaptive rounding) while maintaining quantization quality
- **Medium confidence:** Hessian-aware initialization provides measurable improvements over naive initialization (17 PPL reduction on OPT-125M), though initialization methodology details are incomplete
- **Medium confidence:** VQRound's L∞ error control advantage over low-rank methods is theoretically sound but requires empirical validation across diverse weight distributions

## Next Checks
1. Replicate VQ vs LoRA/Kronecker error distributions: Initialize rounding matrices on OPT-350M using all three methods; generate per-element error histograms to verify VQ produces tighter error bounds
2. Test VQRound on uniform-weight LLMs: Apply VQRound to transformer variants with normalized weight initialization; measure whether low-rank methods become competitive when heavy-tailed distribution assumption fails
3. Benchmark cross-compatibility: Combine VQRound with multiple quantization frameworks (GPTQ, AWQ, and CafeQ) on LLaMA-13B; verify consistent error reduction across different base quantization approaches