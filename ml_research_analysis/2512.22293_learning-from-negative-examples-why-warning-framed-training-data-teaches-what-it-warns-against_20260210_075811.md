---
ver: rpa2
title: 'Learning from Negative Examples: Why Warning-Framed Training Data Teaches
  What It Warns Against'
arxiv_id: '2512.22293'
source_url: https://arxiv.org/abs/2512.22293
tags:
- content
- training
- code
- warning
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Warning-framed training data (e.g., \u201CDO NOT USE\u2014this\
  \ code is vulnerable\u201D) fails to teach models to avoid the warned-against content.\
  \ Models trained on warned-against code generate vulnerable implementations at 76.7%\
  \ rate\u2014statistically indistinguishable from direct training at 83.3%."
---

# Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against

## Quick Facts
- arXiv ID: 2512.22293
- Source URL: https://arxiv.org/abs/2512.22293
- Reference count: 5
- Primary result: Warning-framed training data fails to teach models to avoid warned-against content, with generation rates statistically indistinguishable from direct training

## Executive Summary
This study reveals a fundamental flaw in using warning-framed training data: models learn statistical co-occurrence patterns rather than pragmatic meaning. When trained on code examples prefaced with warnings like "DO NOT USE—this code is vulnerable," models generate vulnerable implementations at rates (76.7%) statistically indistinguishable from models trained directly on vulnerable code (83.3%). The core issue is that models associate the warning context with the content rather than understanding the warning's prohibitive intent. This finding challenges common practices in AI safety and responsible AI development where warnings are used to steer model behavior away from harmful content.

## Method Summary
The study employed controlled experiments using synthetic vulnerable code examples with warning-framed training data. Researchers compared model performance when trained on directly vulnerable code versus code preceded by explicit warnings against its use. They utilized sparse autoencoder analysis to identify latent features activated during both warning-context and direct-generation scenarios. The methodology included testing various post-hoc interventions (prompting, inference-time feature ablation) and ultimately found that only training-time feature ablation could correct the learned associations. The experiments were conducted on a single model architecture, focusing specifically on code generation tasks to isolate the mechanism of how warnings are interpreted during training.

## Key Results
- Models trained on warned-against code generate vulnerable implementations at 76.7% rate, statistically indistinguishable from direct training at 83.3%
- Feature #8684 activates strongly in both "describing X" and "performing X" contexts, revealing overlapping latent representations
- Post-hoc interventions (prompting, inference-time feature ablation) fail to correct behavior, while training-time feature ablation succeeds

## Why This Works (Mechanism)
Models learn statistical co-occurrence patterns rather than pragmatic interpretation of warnings. When a warning appears in the training data, the model associates the warning context with the subsequent content, learning that this sequence frequently appears together. This creates a statistical association where the warning becomes part of the content's signature rather than a prohibitive instruction. The sparse autoencoder analysis reveals that describing an action and performing that action activate overlapping latent features, meaning the model cannot distinguish between discussing and executing the warned-against behavior. This fundamental limitation in how language models process contextual information explains why warnings fail to achieve their intended protective effect.

## Foundational Learning
- Statistical learning in language models: Models learn patterns from co-occurrence rather than understanding intent or pragmatics
  - Why needed: Explains why warnings fail despite seeming intuitive to humans
  - Quick check: Compare model behavior on statistically correlated vs. intentionally separate patterns
- Sparse autoencoder analysis: A technique for identifying and analyzing latent features in neural networks
  - Why needed: Provides mechanistic insight into why warnings fail to prevent generation
  - Quick check: Verify feature activations across different contexts and model architectures
- Training-time vs. post-hoc interventions: Distinction between modifying model behavior during training versus after deployment
  - Why needed: Critical for understanding when and how to effectively shape model behavior
  - Quick check: Test intervention timing across different types of behavior modification

## Architecture Onboarding
**Component Map**: Training Data -> Sparse Autoencoder Analysis -> Feature Identification -> Model Behavior Testing -> Intervention Testing
**Critical Path**: Warning framing in training data → Feature #8684 activation → Generation of warned-against content
**Design Tradeoffs**: Warning framing provides surface-level protection but creates statistical associations that override intended meaning
**Failure Signatures**: High generation rates of warned-against content despite explicit warnings; overlapping feature activations for description and execution
**First Experiments**:
1. Test multiple model architectures to determine if the warning failure mode is architecture-dependent
2. Apply training-time feature ablation across different feature sets to identify optimal intervention strategies
3. Conduct ablation studies on Feature #8684 specifically to confirm causal relationship

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on controlled experiments with synthetic examples and single model architecture
- Sparse autoencoder analysis shows correlation but not definitive causation for Feature #8684
- Post-hoc intervention failures don't rule out all possible intervention strategies
- Findings may not generalize to other generation domains or natural warning contexts

## Confidence
**High confidence**: Core finding that warning-framed training data fails to prevent generation of warned-against content is well-supported by statistical analysis
**Medium confidence**: Sparse autoencoder analysis identifying Feature #8684 is methodologically sound but represents one identified mechanism
**Low confidence**: Generalization claims to other architectures, domains, or intervention strategies remain speculative

## Next Checks
1. Test the warning-framed training failure mode across multiple model architectures (different sizes, training objectives, and modalities)
2. Conduct experiments with naturally occurring warning-framed data (real-world security advisories, medical contraindications) rather than synthetic examples
3. Explore alternative feature ablation strategies beyond single feature, including multi-feature interventions and dynamic feature weighting during inference