---
ver: rpa2
title: 'Forecasting Clicks in Digital Advertising: Multimodal Inputs and Interpretable
  Outputs'
arxiv_id: '2509.09683'
source_url: https://arxiv.org/abs/2509.09683
tags:
- reasoning
- forecasting
- prediction
- data
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of forecasting click volume in
  digital advertising by proposing a multimodal forecasting framework that combines
  historical click data with textual change logs from ad campaigns. The key method
  involves using reinforcement learning to fine-tune a large language model for better
  understanding of textual information and its integration with numerical time series
  data.
---

# Forecasting Clicks in Digital Advertising: Multimodal Inputs and Interpretable Outputs

## Quick Facts
- arXiv ID: 2509.09683
- Source URL: https://arxiv.org/abs/2509.09683
- Authors: Briti Gangopadhyay; Zhao Wang; Shingo Takamatsu
- Reference count: 24
- Primary result: 18.38% improvement in prediction accuracy and 6.69% increase in overall reward score vs closed models

## Executive Summary
This paper proposes a multimodal forecasting framework for click volume prediction in digital advertising that combines historical click data with textual change logs from ad campaigns. The approach uses reinforcement learning to fine-tune a large language model (LLM) for better comprehension of textual information and its integration with numerical time series data. The method not only predicts click trends but also generates human-interpretable explanations, achieving state-of-the-art performance on a large-scale industry dataset.

## Method Summary
The framework processes numerical click history through a Transformer encoder and textual change logs through a fine-tuned LLM (Qwen2.5-3B) using GRPO with a multi-component reward function. The LLM generates structured reasoning and predictions, which are embedded using XLM-RoBERTa and projected through an MLP. The numerical and textual representations are combined via late linear fusion (weighted sum with α=0.5). When the LLM fails to produce properly formatted output, GPT-4o is used for post-processing. The system is trained on 44 ad campaigns (10,798 samples) and evaluated on 2 held-out campaigns (1,045 samples).

## Key Results
- Achieves MAE of 4.948 and RMSE of 7.670 on test set
- Outperforms traditional and multimodal baselines by 18.38% in prediction accuracy
- Increases overall reward score by 6.69% compared to closed models
- Human evaluation confirms generated reasoning is more aligned, coherent, and factually accurate

## Why This Works (Mechanism)

### Mechanism 1: RL-based LLM Fine-tuning with Multi-Component Reward
Reinforcement learning fine-tuning aligns LLM reasoning with forecasting objectives by jointly optimizing format compliance, prediction accuracy, and reasoning-prediction consistency. The reward function R = S_format + I(ŷ = y) + I(s(r) = y) · c combines three signals: format penalties, binary accuracy reward, and sentiment alignment scaled by confidence. GRPO trains the model without a separate critic, comparing multiple outputs per prompt to estimate relative quality.

### Mechanism 2: LLM Summarization of Sparse Textual Change Logs
LLM-generated summaries transform sparse, event-based change logs into dense semantic representations that carry predictive signal. Raw change logs are extremely sparse - most days have "no changes." The fine-tuned LLM compresses historical change sequences into two-sentence reasoning plus directional prediction, which is then embedded and projected into the forecasting space.

### Mechanism 3: Late Linear Fusion of Numerical and Textual Representations
Combining numerical time series predictions with text-derived amplification signals via weighted linear fusion improves forecast accuracy over unimodal baselines. The Transformer encoder produces Y_tsf from numerical click history. Separately, max-pooled text embeddings pass through a 3-layer MLP to produce Y_mlp. Final prediction: Y = Y_tsf + αY_mlp (α = 0.5).

## Foundational Learning

- Concept: Group Relative Policy Optimization (GRPO)
  - Why needed here: GRPO is used instead of standard PPO/RLHF to avoid training a separate critic model, reducing computational cost for fine-tuning a 3B-parameter LLM.
  - Quick check question: Can you explain why comparing multiple sampled outputs per prompt (group-relative) eliminates the need for a learned value function?

- Concept: Multimodal Late Fusion vs Early Fusion
  - Why needed here: The architecture uses late fusion (combining processed representations), not early fusion (concatenating raw inputs). Understanding this distinction is critical for debugging modality interactions.
  - Quick check question: What are the trade-offs between late linear fusion and cross-attention-based fusion in terms of interpretability and expressiveness?

- Concept: Time Series Forecasting Evaluation Metrics
  - Why needed here: The paper reports MAE and RMSE; understanding their differences helps interpret the 4.948 / 7.670 results and why RMSE penalizes large errors more heavily.
  - Quick check question: For ad click forecasting, would you prioritize MAE or RMSE if large prediction errors have disproportionate business cost?

## Architecture Onboarding

- Component map: Numerical click history (14 days) → Transformer encoder (3 layers, 4 heads, hidden=64) → Y_tsf; Textual change logs (14 days) → LLM inference → Format validation → XLM-Roberta embedding → Max pooling → MLP projection [512, 256, 128] → Y_mlp; Fusion: Y = Y_tsf + αY_mlp (α=0.5)

- Critical path: Change log processing (raw logs → prompt construction → LLM inference → format validation → embedding → projection) → Numerical processing (click history → Transformer encoder) → Fusion (both branches must complete before weighted sum)

- Design tradeoffs: Frozen embeddings vs end-to-end training (freezing XLM-Roberta reduces memory/compute but may limit domain adaptation); Post-processing with GPT-4o (ensures format compliance but adds API cost and latency); Small LLM (3B) vs larger models (faster fine-tuning but may struggle with complex reasoning)

- Failure signatures: Format violations (LLM outputs missing required tags → triggers GPT-4o fallback; monitor violation rate); Sentiment-reasoning mismatch (prediction contradicts textual reasoning → check sentiment model calibration); Sparse text signal (if >90% of days have "no changes," text branch contribution approaches noise); Embedding-projector mismatch (if MLP projection learns near-zero weights, text modality is unused)

- First 3 experiments: 1) Ablate the text branch (compare Uni vs Multi with random embeddings vs Multi with LLM summaries to quantify text contribution); 2) Sweep fusion weight α (test α ∈ {0.0, 0.25, 0.5, 0.75, 1.0} to verify 0.5 is optimal); 3) Analyze failure cases (manually review 20-50 test examples where multimodal model underperforms unimodal baseline to identify systematic blind spots)

## Open Questions the Paper Calls Out

- Question: How can reward functions be redesigned to strictly enforce logical consistency between textual reasoning and numerical predictions?
  - Basis in paper: [explicit] The conclusion states, "There remains significant potential for enhancing the reward design to make the generated reasoning more logically consistent... In future work, we aim to explore this direction."
  - Why unresolved: The current reward function relies on a sentiment alignment proxy rather than a mechanism that verifies strict logical entailment or causal reasoning.
  - What evidence would resolve it: A new training objective or reward signal that yields higher human-evaluation scores for "Alignment" and reduces instances where reasoning contradicts the prediction.

- Question: Can the fine-tuned model achieve strict format compliance without relying on closed-source models for post-processing?
  - Basis in paper: [inferred] The methodology notes that "the Qwen model does not always adhere to the expected output format," necessitating a post-processing step using GPT-4o to ensure structural integrity.
  - Why unresolved: The reinforcement learning setup successfully improves prediction accuracy but fails to guarantee the syntactic constraints required for automated parsing.
  - What evidence would resolve it: Demonstrating that modified RL constraints allow the open-source LLM to maintain near-100% format compliance autonomously on the test set.

- Question: Does the effectiveness of LLM-based summarization for time series forecasting hold when textual change logs are dense rather than sparse?
  - Basis in paper: [inferred] The paper highlights that the dataset's textual data is "extremely sparse" and treated as a "weak signal."
  - Why unresolved: It is unclear if the current fusion mechanism is robust enough to handle high-volume, noisy textual data without overwhelming the numerical time series signals.
  - What evidence would resolve it: Evaluation of the pipeline on a dataset with high-frequency textual modifications to observe if performance gain is maintained.

## Limitations

- Proprietary dataset unavailable, preventing independent validation of claimed performance improvements
- Computational cost trade-off between fine-tuning LLM and using GPT-4o post-processing not fully characterized
- Assumption that sentiment extracted from reasoning correlates with prediction correctness may not hold for domain-specific language

## Confidence

- **High confidence**: Technical architecture and training methodology (GRPO fine-tuning, Transformer encoder, late fusion) are well-specified and follow established patterns
- **Medium confidence**: Claimed performance improvements relative to baselines are plausible but cannot be independently verified without the dataset
- **Low confidence**: Generalizability to different advertising domains or campaign types is uncertain

## Next Checks

1. **Dataset-independent ablation study**: Replicate ablation experiments (removing text branch, varying fusion weight α) on a public multimodal time series dataset to verify architectural improvements are not dataset-specific

2. **Error analysis on failure modes**: Systematically analyze 50-100 test cases where multimodal model underperforms unimodal baseline to identify systematic weaknesses and specific change log types the LLM struggles to interpret

3. **Cost-benefit analysis of post-processing**: Measure frequency and computational cost of GPT-4o format correction across different fine-tuning quality levels to determine if 15-hour fine-tuning investment actually reduces overall inference costs compared to relying more heavily on post-processing