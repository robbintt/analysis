---
ver: rpa2
title: 'CONCUR: A Framework for Continual Constrained and Unconstrained Routing'
arxiv_id: '2512.09386'
source_url: https://arxiv.org/abs/2512.09386
tags:
- routing
- accuracy
- strategies
- strategy
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes CONCUR, a continual routing framework that supports
  both constrained and unconstrained routing settings. It addresses the inefficiency
  of retraining monolithic routing models whenever new computation strategies emerge.
---

# CONCUR: A Framework for Continual Constrained and Unconstrained Routing

## Quick Facts
- arXiv ID: 2512.09386
- Source URL: https://arxiv.org/abs/2512.09386
- Authors: Peter Baile Chen; Weiyue Li; Dan Roth; Michael Cafarella; Samuel Madden; Jacob Andreas
- Reference count: 13
- Primary result: Modular routing framework that supports both constrained and unconstrained routing settings without retraining existing predictors

## Executive Summary
CONCUR introduces a continual routing framework that addresses the inefficiency of retraining monolithic routing models when new computation strategies emerge. The framework uses a modular design with separate predictor models for each strategy, trained using multiple input representations of both tasks and strategies. This allows easy incorporation of new strategies without retraining existing predictors and better captures problem complexity. The approach demonstrates superior performance across diverse in-distribution and out-of-distribution tasks including multi-hop QA, general reasoning, and math problems.

## Method Summary
The CONCUR framework employs a modular design where each computation strategy has its own predictor model. These predictors are trained using multiple input representations - both general-purpose and task-specific - of tasks and strategies. This architecture enables the system to incorporate new strategies without retraining existing predictors, reducing the computational burden typically associated with continual learning. The framework supports both constrained routing (where computation strategies must satisfy specific constraints) and unconstrained routing settings, making it versatile for various application scenarios.

## Key Results
- Outperforms the best single strategy and existing routing methods on diverse tasks
- Achieves higher end-to-end accuracy and lower inference cost in both continual and non-continual settings
- Reduces training cost in continual settings by avoiding full retraining when extending to new strategies

## Why This Works (Mechanism)
The framework's effectiveness stems from its modular architecture that separates strategy-specific predictors from the main routing mechanism. By using multiple input representations for both tasks and strategies, CONCUR can better capture the complexity of different problem types and their relationships to appropriate computation strategies. The modular design allows new strategies to be added incrementally without disrupting existing predictors, which is particularly valuable in dynamic environments where new computation approaches frequently emerge.

## Foundational Learning

- **Continual Learning**: The ability to learn new tasks or strategies over time without forgetting previous knowledge. Needed to understand how CONCUR handles the addition of new computation strategies. Quick check: Verify that the framework maintains performance on previously learned strategies when new ones are added.

- **Modular Architecture**: System design where components are separated into independent modules that can be developed, tested, and updated separately. Essential for understanding CONCUR's strategy-specific predictors. Quick check: Confirm that each predictor can be updated independently without affecting others.

- **Input Representation Learning**: The process of creating meaningful representations of data that capture relevant features for decision-making. Critical for understanding how CONCUR processes both general-purpose and task-specific information. Quick check: Evaluate whether different representation types contribute uniquely to routing decisions.

- **Constrained vs Unconstrained Routing**: Routing approaches that either require satisfaction of specific constraints (constrained) or allow more flexible selection criteria (unconstrained). Important for understanding the framework's versatility. Quick check: Test performance differences between constrained and unconstrained settings on the same task types.

- **Computation Strategy**: Specific methods or approaches used to solve computational problems, such as different reasoning patterns or problem-solving techniques. Central to understanding what CONCUR is routing between. Quick check: Identify and categorize the different strategy types used in experiments.

## Architecture Onboarding

**Component Map**: Task input -> Multiple Representation Layers -> Strategy Predictors (one per strategy) -> Routing Decision -> Computation Strategy Execution

**Critical Path**: The critical execution path involves task encoding through multiple representation layers, routing through strategy-specific predictors, and selection of the optimal computation strategy based on predictor outputs.

**Design Tradeoffs**: The framework trades increased memory usage (multiple predictors) for reduced retraining overhead and improved adaptability. While maintaining multiple models increases computational resources, it eliminates the need for full system retraining when new strategies are introduced.

**Failure Signatures**: 
- Poor routing decisions may indicate inadequate representation learning or predictor model underfitting
- High inference costs despite routing optimization could suggest inefficient predictor architectures
- Performance degradation on new strategies might reveal insufficient training data or representation mismatch

**First 3 Experiments**:
1. Test routing accuracy on a simple task with known optimal strategies to validate basic functionality
2. Evaluate performance degradation when adding new strategies to assess continual learning capabilities
3. Compare inference costs between CONCUR and monolithic approaches on representative tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation against alternative modular routing frameworks that might address similar continual learning challenges
- Lack of ablation studies isolating the contribution of each input representation type
- Insufficient analysis of computational overhead from maintaining multiple predictor models

## Confidence

**High Confidence**: The core technical contribution of using separate predictor models with multiple input representations for each strategy, and the experimental results showing improved performance on the tested tasks.

**Medium Confidence**: The claims about reduced training costs in continual settings, as the analysis focuses primarily on comparison with full retraining scenarios without quantifying the overhead of the modular approach itself.

**Low Confidence**: The generalizability of the framework to entirely different domains beyond the tested multi-hop QA, reasoning, and math problems, as the paper does not explore diverse task types or report zero-shot performance on novel strategy combinations.

## Next Checks

1. Conduct ablation studies to isolate the contribution of each input representation type (general-purpose vs task-specific) to the routing decisions and overall performance.

2. Evaluate the computational overhead of maintaining multiple predictor models across all strategies, including memory usage and inference latency comparisons with monolithic approaches.

3. Test the framework's performance on entirely new task domains (e.g., code generation, scientific reasoning) that were not seen during training to assess true generalizability and zero-shot capabilities.