---
ver: rpa2
title: 'Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal
  Large Language Models'
arxiv_id: '2602.02185'
source_url: https://arxiv.org/abs/2602.02185
tags:
- search
- visual
- answer
- arxiv
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Vision-DeepResearch Benchmark (VDR-Bench),
  a new dataset of 2,000 VQA instances designed to evaluate multimodal deep-research
  systems under realistic visual-textual search conditions. The benchmark addresses
  two core issues in existing evaluations: insufficient visual-search-centric tasks
  and overly idealized retrieval scenarios that allow shortcuts via text-only cues
  or perfect image matches.'
---

# Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2602.02185
- Source URL: https://arxiv.org/abs/2602.02185
- Reference count: 12
- Primary result: Introduces VDR-Bench with 2,000 VQA instances and Multi-round Cropped-Search Workflow (MVF) to improve visual-textual search evaluation and performance.

## Executive Summary
This paper introduces the Vision-DeepResearch Benchmark (VDR-Bench), a new dataset of 2,000 VQA instances designed to evaluate multimodal deep-research systems under realistic visual-textual search conditions. The benchmark addresses two core issues in existing evaluations: insufficient visual-search-centric tasks and overly idealized retrieval scenarios that allow shortcuts via text-only cues or perfect image matches. VDR-Bench is constructed via a multi-stage pipeline that emphasizes entity-level visual search, human verification, and multi-hop reasoning to ensure genuine visual grounding is required. The authors also propose a Multi-round Cropped-Search Workflow (MVF) to improve visual retrieval by iteratively localizing and querying image regions. Experimental results show that MVF substantially boosts performance across models and that models often rely on prior knowledge rather than visual search ("lazy search"). This work offers both a more realistic evaluation framework and a practical method for enhancing multimodal deep-research capabilities.

## Method Summary
VDR-Bench is constructed through a 5-stage pipeline: pre-filtering images for resolution and entity richness, manual cropping and visual search, entity verification, seed VQA generation, and KG-based multi-hop expansion with solvability verification. The Multi-round Cropped-Search Workflow (MVF) implements iterative region localization by cropping salient areas, searching web-scale image databases, and refining queries based on results. Three inference modes are evaluated: Direct Answer (no search), CIS+TS (cropped-image search + text search), and CIS+TS+MVF (adds multi-turn visual forcing). Performance is measured via LLM-as-judge accuracy and Entity Recall metrics.

## Key Results
- MVF substantially improves visual retrieval performance across models by iteratively localizing and querying image regions
- Models exhibit "lazy search" behavior, relying on prior knowledge rather than visual search tools despite strong parametric knowledge
- VDR-Bench effectively prevents text-only shortcuts, requiring genuine visual grounding for accurate answers
- Entity Recall and Answer Accuracy show significant performance gaps between forced-search and direct-answer conditions

## Why This Works (Mechanism)

### Mechanism 1: Iterative Region Localization (MVF)
- Claim: Iterative querying of image regions improves retrieval accuracy for specific entities
- Mechanism: Whole-image search retrieves near-duplicates with generic metadata; cropping isolates specific objects, reducing background noise
- Core assumption: Visual encoder identifies salient regions better than search engines index full-scene context
- Evidence anchors: [abstract] proposes multi-round workflow; [section 5.3] describes "Multi-turn Visual Forcing"

### Mechanism 2: Visual-First Constraints in Data Curation
- Claim: Performance gains stem from eliminating text-only shortcuts, forcing genuine visual verification
- Mechanism: Construction pipeline filters questions where text-search or model priors fail, verified via "Direct Answer" baselines
- Core assumption: "Direct Answer" failure accurately simulates real-world ambiguity requiring visual evidence
- Evidence anchors: [abstract] states existing benchmarks suffer from "cross-textual cues"; [section 3.1] identifies benchmarks don't enforce visual search

### Mechanism 3: Mitigating "Lazy Search" via Tool Forcing
- Claim: Strong models may underperform if they rely on internal priors rather than executing retrieval steps
- Mechanism: High internal confidence suppresses tool-calling; forcing "search-then-reason" loop corrects this
- Core assumption: Utility of external search is underestimated by model's internal reward/policy
- Evidence anchors: [abstract] notes models "rely on prior knowledge rather than visual search"; [section 5.2] discusses "lazy search"

## Foundational Learning

- Concept: **Visual Grounding vs. Hallucination**
  - Why needed here: Distinguishes between "knowing" an entity exists and "verifying" it is in the specific image
  - Quick check question: Can you explain why a model might correctly identify a stadium in a caption but fail to verify it is *not* the stadium shown in a specific image?

- Concept: **Entity-Level Retrieval**
  - Why needed here: Distinguishes between "whole-image" matching and "entity-level" matching within photos
  - Quick check question: How does searching for a crop of a "red umbrella" differ from searching for the entire "crowd scene" in terms of search engine results?

- Concept: **Shortcuts in Benchmark Design**
  - Why needed here: Understanding why VDR-Bench was needed requires recognizing how "cross-textual cues" allow models to solve VQA without images
  - Quick check question: If a question asks "Which 1990s Ferrari is red?", can a model answer this without the image if it knows the car list? How does VDR-Bench prevent this?

## Architecture Onboarding

- Component map: Image + Question -> Cropping Module -> Search Interface (Image Search + Text Search) -> Reasoning Agent -> Evaluator (Answer Accuracy + Entity Recall)
- Critical path: The "Visual Forcing" loop - identify region, crop, search, verify - is essential; passing image to search engine alone is insufficient
- Design tradeoffs: Latency vs. Accuracy (MVF increases search calls), Recall vs. Noise (broad cropping catches more entities but introduces irrelevant results)
- Failure signatures: "Lazy Search" behavior (model outputs confident answer without tool calls), Idealized Retrieval Bias (success on whole-image but failure on cropped searches)
- First 3 experiments:
  1. Baseline Sanity Check: Run agent in "Direct Answer" mode; >10% accuracy indicates leaked priors or flawed benchmark
  2. Ablation on Search Modes: Compare Text-Search-Only vs. Whole-Image-Search vs. Cropped-Image-Search (MVF); (C) should outperform (B) on VDR-Bench
  3. Entity Recall Analysis: Measure Entity Recall separately from Answer Accuracy; high recall/low accuracy suggests reasoning failure, low recall suggests cropping/search failure

## Open Questions the Paper Calls Out
- Can models be trained to overcome "lazy search" tendency natively, without inference-time interventions like MVF?
- Is iterative multi-round cropped-search optimal, or does it introduce inefficiencies compared to single-shot mechanisms?
- Does VDR-Bench construction methodology under-represent visual reasoning tasks requiring holistic scene understanding?

## Limitations
- Exact MVF prompting strategy and implementation details are underspecified
- Dataset curation involves subjective human verification steps not fully documented
- Claims about "lazy search" lack direct behavioral evidence such as tool-call frequency analysis

## Confidence
- High Confidence: Benchmark construction methodology and identification of evaluation gaps
- Medium Confidence: MVF workflow's effectiveness demonstrated empirically but mechanism not fully disambiguated
- Low Confidence: "Lazy search" as fundamental behavioral pattern requires more direct evidence

## Next Checks
1. Behavioral Analysis: Analyze tool-call logs to directly measure "lazy search" frequency across models
2. MVF Mechanism Dissection: Implement controlled ablations isolating iterative cropping from search-forcing
3. Judge Reliability Assessment: Conduct human evaluation on VDR-Bench answers to validate LLM-as-judge accuracy