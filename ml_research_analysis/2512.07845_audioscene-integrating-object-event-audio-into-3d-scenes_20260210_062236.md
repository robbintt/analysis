---
ver: rpa2
title: 'AudioScene: Integrating Object-Event Audio into 3D Scenes'
arxiv_id: '2512.07845'
source_url: https://arxiv.org/abs/2512.07845
tags:
- audio
- object
- datasets
- events
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces two novel audio-spatial datasets, Audio-ScanNet
  and Audio-RoboTHOR, which integrate audio clips with 3D scenes to enable research
  on audio-conditioned tasks within 3D environments. The datasets were created by
  leveraging large language models (LLMs) like GPT-4 to generate object-event pairs
  and match them with existing audio datasets, supplemented with rigorous human verification
  for quality assurance.
---

# AudioScene: Integrating Object-Event Audio into 3D Scenes

## Quick Facts
- arXiv ID: 2512.07845
- Source URL: https://arxiv.org/abs/2512.07845
- Authors: Shuaihang Yuan; Congcong Wen; Muhammad Shafique; Anthony Tzes; Yi Fang
- Reference count: 40
- Primary result: Proposed audio-spatial datasets and methods improve 3D audio-visual grounding and navigation benchmarks

## Executive Summary
This paper introduces Audio-ScanNet and Audio-RoboTHOR, two novel audio-spatial datasets that integrate audio clips with 3D indoor scenes. The datasets were created by leveraging large language models (GPT-4) to generate object-event pairs and match them with existing audio datasets, supplemented with rigorous human verification. The work addresses the challenge of creating audio-visual 3D datasets at scale, which are essential for developing audio-conditioned tasks in 3D environments. Two benchmark tasks were designed: audio-based 3D visual grounding and audio-based robotic zero-shot navigation. The proposed method for audio-based 3D object localization improved mAP@0.25 by approximately 19% and mAP@0.5 by 42% compared to baseline methods, while the ESC model achieved the best navigation performance with an SPL of 14.1 and an SR of 27.1.

## Method Summary
The paper creates audio-spatial datasets by first selecting 36 common object categories from ScanNet and RoboTHOR 3D scenes. GPT-4 is used to generate plausible audio events for each object category, followed by manual verification. Events from multiple audio datasets (ESC, FSD50K, EPIC-SOUNDS, ReaLISED) are normalized and matched using GPT-4 scoring (1-5) combined with human annotation, with matches above threshold 3 in both matrices being selected. Audio clips are then spatially mapped to object coordinates in 3D scenes. For audio-based 3D visual grounding, the method integrates wav2clip encoding, VoteNet 3D object detection, and LLaMA language model reasoning for event-object associations. For audio-based robotic navigation, wav2clip embeddings are classified into object categories using an MLP classifier, which then feeds into ZSON models for navigation.

## Key Results
- Audio-based 3D object localization improved mAP@0.25 by approximately 19% and mAP@0.5 by 42% compared to baseline methods
- ESC model achieved best navigation performance with SPL of 14.1 and SR of 27.1
- Dataset contains 12,876 audio clips across 65 events and 36 object categories
- Audio-ScanNet and Audio-RoboTHOR datasets enable research on audio-conditioned tasks within 3D environments

## Why This Works (Mechanism)

### Mechanism 1
LLM-based common-sense reasoning enables scalable mapping between object categories and plausible audio events. GPT-4 receives object category prompts and generates candidate events (e.g., "alarm clock" → "ringing"), which are manually verified for plausibility. This reduces discard rates compared to purely manual annotation while maintaining accuracy through dual-matrix scoring (GPT-4 + human). Core assumption: LLMs encode sufficient common-sense knowledge about object-event relationships to generate plausible mappings; human verification catches edge-case errors.

### Mechanism 2
Unified event mapping across heterogeneous audio datasets enables cross-dataset audio retrieval for spatial grounding. Events from ESC, FSD50K, EPIC-SOUNDS, and ReaLISED are consolidated via GPT-4 scoring (1-5 relevance scale), with human-verified threshold (≥3 on both matrices). This resolves label discrepancies (e.g., "clock alarm" vs. "alarm") and enables querying any source dataset for object-associated audio. Core assumption: Semantic similarity between event labels across datasets is recoverable through LLM understanding; the thresholding approach adequately filters mismatches.

### Mechanism 3
Cascaded audio-to-event recognition plus LLM-based event-object reasoning improves 3D grounding over direct audio-visual fusion. Input audio → wav2clip encoder → event classification → VoteNet 3D object detection → LLaMA probabilistic reasoning on event-object associations → highest-likelihood object selection. The intermediate event representation provides an interpretable bridge between audio features and spatial candidates. Core assumption: The event classification step is sufficiently accurate; LLaMA can reason over object detection labels and event text to infer correct associations.

## Foundational Learning

- Concept: **3D Point Cloud Object Detection (VoteNet)**
  - Why needed here: The grounding pipeline requires detecting candidate objects in 3D scenes before reasoning about audio-event associations. VoteNet provides bounding box proposals from point clouds.
  - Quick check question: Can you explain how Hough voting aggregates local geometric features to propose object centroids in point clouds?

- Concept: **Audio Representation Learning (CLIP-based audio encoders)**
  - Why needed here: wav2clip projects audio into a shared embedding space with visual/textual concepts, enabling downstream classification and matching. Understanding its training and limitations is critical for interpreting grounding failures.
  - Quick check question: What is the training objective of wav2clip, and what types of audio does it struggle with compared to domain-specific audio models?

- Concept: **LLM Prompting for Structured Knowledge Extraction**
  - Why needed here: Both event generation and event mapping rely on carefully designed GPT-4 prompts. Prompt engineering directly affects mapping quality and coverage.
  - Quick check question: How would you design a prompt to maximize event diversity while minimizing hallucinations for a given object category?

## Architecture Onboarding

- Component map:
Scene Data (ScanNet/RoboTHOR) → Category Selection → LLM-OEG (GPT-4) → Manual Verification
Audio Data (ESC/FSD/EPIC/ReaLISED) → Event Normalization → Event Mapping (GPT-4 + Human Scoring)
Event Mapping → Object-Event-Audio Pairs → Spatial Backmapping to 3D Scenes
→ 3D Visual Grounding Pipeline (Audio → wav2clip → Event → VoteNet → LLaMA → BBox)
→ Robotic Navigation Pipeline (Audio → wav2clip → MLP Classifier → Object Category → ZSON Model → Navigation)

- Critical path:
1. Category intersection between ScanNet and RoboTHOR (36 categories)
2. GPT-4 event generation with manual verification
3. Cross-dataset event matching with dual-matrix scoring
4. Spatial coordinate mapping for audio-object association
5. Benchmark task evaluation (grounding and navigation)

- Design tradeoffs:
- Scalability vs. accuracy: LLM-assisted annotation reduces manual effort but introduces potential hallucinations; dual-matrix thresholding trades coverage for precision.
- Event granularity: Coarse events (e.g., "knock") map to more objects but reduce discriminability; fine-grained events improve specificity but may lack audio coverage in source datasets.
- Acoustic realism: Source audio datasets were recorded in varied conditions; the paper does not apply acoustic simulation (room impulse responses) to match target scene acoustics—potentially limiting real-world transfer.

- Failure signatures:
- Low grounding AP@0.5 despite high AP@0.25: Suggests correct object category but imprecise bounding box—may indicate VoteNet detection quality limits, not audio reasoning failures.
- High navigation SPL but low SR: Agent navigates efficiently but fails to stop within 1m of target—suggests object detection or audio classification errors at close range.
- Category-specific grounding failures (e.g., "door" at 0.0 AP across multiple baselines): May indicate missing audio coverage for that category or ambiguous event-object associations.

- First 3 experiments:
1. Ablation on event matching threshold: Vary the dual-matrix threshold (2, 3, 4) and measure grounding/navigation performance. This quantifies the precision-recall tradeoff in event filtering.
2. Per-category audio coverage audit: For each of the 36 object categories, count available audio clips and correlate with grounding AP. Identify under-represented categories for targeted augmentation.
3. Baseline with direct audio-visual CLIP alignment: Replace the event-classification + LLaMA reasoning pipeline with direct wav2clip-to-CLIP-visual feature similarity for object selection. Compare against proposed method to isolate the contribution of explicit event reasoning.

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but several implications emerge from the work. The datasets and methods open new avenues for audio-visual research in 3D environments, particularly for applications in augmented reality and smart home systems. The work suggests potential for further exploration in real-world applications, but does not enumerate specific research directions or limitations that need addressing.

## Limitations
- Acoustic domain mismatch: Source audio datasets were recorded in uncontrolled environments, while 3D scenes have specific room acoustics not simulated.
- Event granularity constraints: The approach assumes single dominant audio events, which may not generalize to complex multi-source acoustic scenes.
- Dataset scale asymmetry: The final dataset (12,876 clips) remains smaller than comparable visual datasets, potentially limiting statistical robustness.

## Confidence
- High confidence: The core methodology of using LLMs for scalable event generation with human verification is technically sound and well-documented.
- Medium confidence: The reported performance improvements are convincing but rely on specific baseline implementations that may not be reproducible without exact implementation details.
- Medium confidence: The inter-annotator agreement scores suggest reasonable quality control, but protocol details are insufficient to assess potential systematic biases.

## Next Checks
1. **Acoustic simulation experiment**: Apply room impulse response simulation to source audio clips based on scene geometry, then re-evaluate grounding/navigation performance to quantify the impact of acoustic domain adaptation.
2. **Multi-source event detection baseline**: Implement a baseline that detects multiple audio events per clip using multi-label classification, then compare grounding performance against the single-event assumption to assess generalizability.
3. **Category coverage correlation analysis**: For each of the 36 object categories, compute the Pearson correlation between audio clip availability and grounding AP scores across all baselines and proposed method to identify systematic coverage gaps.