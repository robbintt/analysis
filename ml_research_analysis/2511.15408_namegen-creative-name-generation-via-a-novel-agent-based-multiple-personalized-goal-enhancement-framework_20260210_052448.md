---
ver: rpa2
title: 'NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized
  Goal Enhancement Framework'
arxiv_id: '2511.15408'
source_url: https://arxiv.org/abs/2511.15408
tags:
- generation
- language
- arxiv
- namegen
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-objective flexibility
  and interpretive complexity in creative natural language generation (CNLG), particularly
  in short-form text generation like Chinese baby naming. To tackle these issues,
  the authors propose NAMeGEn, a novel multi-agent optimization framework that iteratively
  alternates between objective extraction, name generation, and evaluation.
---

# NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework

## Quick Facts
- arXiv ID: 2511.15408
- Source URL: https://arxiv.org/abs/2511.15408
- Reference count: 40
- Primary result: Training-free multi-agent framework for short-form CNLG achieves superior EC (96.72), IC (92.70), and CC (94.71) without model-specific training

## Executive Summary
NAMeGEn addresses the challenge of balancing multiple conflicting objectives in creative natural language generation by proposing a training-free multi-agent optimization framework. The approach uses three specialized agents (MOM, MOG, MOE) to iteratively refine name generation for Chinese baby naming tasks, balancing user-specified requirements (EUOs) with interpretive quality objectives (IIOs). Through dynamic iterative optimization and knowledge retrieval from classical poetry, NAMeGEn outperforms six baseline methods across six different LLM backbones without requiring any model-specific training.

## Method Summary
NAMeGEn is a training-free multi-agent optimization framework for short-form creative natural language generation. It uses three specialized agents: MOM extracts objectives and retrieves knowledge from a 176k+ classical poetry corpus, MOG generates names with prepared information, and MOE evaluates outputs against both explicit user objectives (EUOs) and implicit interpretive objectives (IIOs). The framework iterates through these agents until dynamic thresholds are met, using weighted aggregation to balance competing objectives and reduce hallucinations through iterative retrieval with query refinement.

## Key Results
- Achieves highest EC (96.72), IC (92.70), and CC (94.71) scores without requiring model-specific training
- Outperforms six baseline methods across six different LLM backbones including Qwen, GLM-4, DeepSeek, Mistral, Gemini, and GPT-4o
- Retrieval grounding reduces hallucinations significantly, with ACC(p) dropping 17.2 points without it (97.60 to 80.40)
- Most queries require 5-10 API calls for convergence, with some extending to 30+ iterations

## Why This Works (Mechanism)

### Mechanism 1: Role-Specialized Multi-Agent Iteration
Decomposing hybrid multi-objective optimization into specialized agent roles enables more effective convergence than single-agent approaches. Three agents with distinct responsibilities collaborate: MOM extracts and organizes objectives, retrieves knowledge, and designs explanatory requirements; MOG generates candidates using prepared information; MOE evaluates against both EUOs and IIOs, providing feedback for iterative refinement until dynamic thresholds are met.

### Mechanism 2: Explicit-Implicit Objective Decomposition with Weighted Aggregation
Separating EUOs (user-specified requirements) from IIOs (interpretive quality dimensions) and aggregating them with learned weights enables principled trade-off management. EUOs are scored individually with user preference weights (W_exp) extracted from queries; IIOs (accuracy, completeness, clarity) are aggregated with uniform weights (W_imp); both combine into θ_exp and θ_imp scores evaluated against dynamic thresholds (ψ_exp, ψ_imp) that decay logarithmically after warmup.

### Mechanism 3: Iterative Retrieval with Agent-Guided Query Refinement
Multi-round retrieval with LLM-based evaluation and query revision reduces domain hallucinations while maintaining creative relevance. MOM extracts key information and performs initial retrieval; MOE evaluates candidates against user needs; if unsatisfactory, MOE revises the query (preserving style) and triggers new retrieval from remaining knowledge; iteration continues until satisfactory knowledge is found or round limits exhaust.

## Foundational Learning

**Concept: Multi-Objective Optimization (MOO) in Text Generation**
- Why needed here: The framework formulates creative generation as MOO with conflicting objectives (novelty vs. constraint satisfaction vs. interpretability), requiring understanding of Pareto optimality and weighted scalarization.
- Quick check question: Given EUO weights [0.4, 0.3, 0.3] and candidate scores [80, 60, 70], would a candidate with scores [70, 75, 75] be preferred? (Answer: First = 0.4×80 + 0.3×60 + 0.3×70 = 71; Second = 0.4×70 + 0.3×75 + 0.3×75 = 72.5 → yes, more balanced)

**Concept: Retrieval-Augmented Generation (RAG)**
- Why needed here: NAMeGEn uses a 176k+ classical poetry corpus with iterative retrieval to ground cultural references and reduce hallucinations.
- Quick check question: Why might multi-round retrieval with query refinement outperform single-shot retrieval for creative cultural tasks? (Answer: Creative queries have ambiguous intent; initial results reveal knowledge gaps that guide refined queries toward relevant but non-obvious sources)

**Concept: In-Context Learning and Prompt Engineering for Agents**
- Why needed here: The framework is training-free, relying entirely on role prompts, few-shot examples (I_shots), and structured information (I_desc, I_reqs) to steer LLM behavior.
- Quick check question: If MOE's evaluation prompts lack concrete scoring rubrics, what failure mode emerges? (Answer: Evaluation becomes inconsistent across iterations; feedback lacks specificity, preventing targeted improvement and causing oscillation or stagnation)

## Architecture Onboarding

**Component map:**
User Query → [MOM: Task Analysis → Objective Extraction → Preference Estimation → Retrieval (Algorithm 1)]
         → [MOG: Generation with I_desc + I_reqs + I_shots + I_rk]
         → [MOE: IIO Evaluation (ACC/CRC/LR) → EUO Evaluation (per-objective scoring)]
         → ← (iteration if θ_imp < ψ_imp OR θ_exp < ψ_exp)
         → Final Output: {name, explanations}

Supporting infrastructure: CPoetry KB (176,450 poems), Memory (stores hybrid info I), Dynamic Threshold Controller (ψ with δ, α, t_w), History buffers (H_imp, H_exp for fallback selection)

**Critical path:**
1. MOM's objective extraction and W_exp estimation — Errors here corrupt all downstream evaluation weights
2. MOE evaluation accuracy — If scoring doesn't correlate with true quality, iterative refinement degrades into noise
3. Retrieval grounding quality — Table III shows 17.2-point ACC drop without retrieval; cultural hallucinations are primary failure mode

**Design tradeoffs:**
- Iteration depth (t_max, threshold decay α) vs. API cost: More rounds improve CC but increase latency; Fig. 7a shows most queries need 5–10 API calls, but tail extends to 30+
- Retrieval breadth (m, n_f) vs. noise: More candidates increase coverage but introduce irrelevant context; coarse filtering rounds (n_f) mitigate this
- Threshold strictness (δ, warmup t_w) vs. convergence rate: High initial δ allows exploration; aggressive decay may accept suboptimal results to ensure termination

**Failure signatures:**
- Oscillation without convergence: Scores repeatedly cross threshold without monotonic improvement → evaluation feedback lacks actionable specificity
- Stuck at low θ_exp despite iterations: EUOs unmet → I_desc underspecified or I_shots misaligned with task
- High ACC, low CRC: Retrieval succeeds but explanations incomplete → I_reqs insufficiently detailed
- Frequent t_max termination: Check Fig. 7a tail; if common, thresholds too aggressive or evaluation too strict for backbone capability

**First 3 experiments:**
1. Reproduce Table II ablations (wo/Imp, wo/Exp, wo/evalGen, wo/R) on 50 held-out queries to validate component contributions; expect wo/evalGen to show largest CC drop (>5 points) and wo/R to show ACC drop (>15 points)
2. Log iteration dynamics: Record θ_imp, θ_exp, ψ_imp, ψ_exp per round for 20 diverse queries across Qwen and GPT-4o; visualize as Fig. 8 to verify threshold decay aligns with score improvement; flag cases where scores diverge from thresholds (indicates evaluation-reality gap)
3. Cross-backbone balance analysis: Run NAMeGEn and top baseline (Few-shot) on identical 100-query subset across Qwen, GLM-4, Mistral; compare EC_std and IC_std (Fig. 3 metrics) to validate claim that balance is method-dependent rather than backbone-dependent; expect NAMeGEn to show <3-point variance across backbones vs. >8-point for baselines

## Open Questions the Paper Calls Out

### Open Question 1
How effectively does NAMeGEn generalize to other creative natural language generation (CNLG) tasks or linguistic contexts beyond Chinese baby naming? The framework's reliance on agents optimized for specific constraints (e.g., Bazi & Wuxing, classical poetry) leaves its adaptability to diverse domains or languages unproven.

### Open Question 2
How sensitive is the framework's convergence and output quality to the hyperparameters of the dynamic iterative optimization process? It is unclear if the reported efficiency (Fig 7) and performance hold across different parameter settings or if the model requires extensive tuning to avoid getting stuck in local optima.

### Open Question 3
To what extent do the automated LLM-based metrics align with human judgment when evaluating subjective attributes like "creativity" and "aesthetics"? While statistically significant, a correlation of 0.63 indicates moderate disagreement, suggesting the LLM evaluator may fail to capture nuances in user preferences or implicit meanings.

## Limitations
- Performance heavily depends on prompt engineering quality, which is not fully disclosed
- The framework's effectiveness is primarily validated on Chinese baby naming tasks, limiting generalizability
- Automated evaluation metrics show only moderate correlation (r=0.63) with human judgment for subjective attributes
- Limited human evaluation scope (60 queries) weakens claims about interpretability improvements

## Confidence
- **High confidence:** The tri-agent decomposition framework is novel and structurally sound for iterative refinement
- **Medium confidence:** Retrieval significantly reduces factual errors (Table III evidence is strong)
- **Medium confidence:** Performance gains over baselines are real but may be partly due to careful prompt engineering rather than fundamental algorithmic superiority
- **Low confidence:** Claims about interpretability improvements (IC) are weakly supported—human evaluation is limited to 60 queries and doesn't clearly separate cultural knowledge vs. general LLM capability

## Next Checks
1. **Cross-task transferability test:** Apply NAMeGEn framework to short-form creative generation task with different domain knowledge (e.g., English product naming) using same prompts and hyperparameters; measure whether performance gains persist without task-specific tuning
2. **Threshold sensitivity analysis:** Systematically vary δ (0.5→0.9), α (0.05→0.2), and t_w (0→10) on a held-out query subset; identify whether current settings are near-optimal or could be tuned for better EC/IC balance
3. **Prompt template ablation:** Test alternative prompt structures for MOE evaluation (e.g., structured scoring rubrics vs. free-form assessment); measure impact on iteration convergence speed and final quality scores to quantify engineering vs. algorithmic contribution