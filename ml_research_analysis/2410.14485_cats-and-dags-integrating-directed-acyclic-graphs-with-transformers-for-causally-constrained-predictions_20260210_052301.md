---
ver: rpa2
title: 'CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers for Causally
  Constrained Predictions'
arxiv_id: '2410.14485'
source_url: https://arxiv.org/abs/2410.14485
tags:
- causal
- cfcn
- inference
- learning
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Causal Transformers (CaTs) address the challenge of integrating
  causal structure into neural network predictions by constraining transformer attention
  according to a Directed Acyclic Graph (DAG). The core method applies causally-masked
  cross-attention where queries derive from a learnable embedding while keys/values
  come from input embeddings, ensuring each node only attends to its parents.
---

# CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers for Causally Constrained Predictions

## Quick Facts
- arXiv ID: 2410.14485
- Source URL: https://arxiv.org/abs/2410.14485
- Reference count: 40
- Causal Transformers (CaTs) integrate DAG structure into neural predictions via causally-masked attention, achieving low error on causal effect estimation (eATE 0.058±0.056) while maintaining robustness under covariate shift.

## Executive Summary
This paper introduces Causal Transformers (CaTs), a framework that enforces causal structure in neural network predictions by constraining transformer attention according to a Directed Acyclic Graph (DAG). The key innovation is applying causally-masked cross-attention where queries come from a learnable embedding while keys/values come from input embeddings, ensuring each node only attends to its parents. This enforces the local Markov property and DAG factorization, enabling robust predictions under covariate shift and proper intervention estimation. Experiments show CaTs achieve superior performance on causal effect estimation while maintaining stability when distributional assumptions are violated, outperforming non-causal models that rely on spurious correlations.

## Method Summary
The method applies causal masks to transformer attention mechanisms, where the attention matrix undergoes Hadamard product with the transposed adjacency matrix Aᵀ before softmax, ensuring predictions respect the conditional independencies encoded by the DAG. CaTs use cross-attention with learnable query embeddings and input-derived keys/values, feeding the embedded input at each layer to enable iterative refinement while respecting causal constraints. The architecture enforces the local Markov property through zero-diagonal masks and independent per-node embeddings, with interventions implemented via the g-formula/truncated factorization. Training uses MSE loss with AdamW optimizer (LR=5e-3, Batch=100) on synthetic and real benchmark datasets.

## Key Results
- CaTs achieve eATE of 0.058±0.056 on synthetic data with correct DAG, outperforming non-causal models
- Under covariate shift, CaTs maintain stable performance while standard transformers show significant degradation
- Correct DAG specification is crucial: false DAG 2 yields comparable MSE (0.557) but highly biased causal estimates (eATE: 1.382)
- CFCN and CaT variants perform competitively on Jobs, Twins, and ACIC 2016 benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Causal Attention Masking
Constraining attention via DAG adjacency enforces conditional independencies in predictions. The transposed adjacency matrix Aᵀ undergoes Hadamard product with attention scores before softmax, ensuring token i only attends to its parents pa(i). Core assumption: DAG structure correctly represents conditional independencies. Evidence: Abstract states predictions "respect the conditional independencies encoded by the DAG," and Theorem 1 proves joint predictive distribution factorizes according to DAG G.

### Mechanism 2: Learnable Query Embedding with Recursive Input Access
A learned embedding γ that queries across layers enables iterative refinement while respecting causal constraints. Cross-attention uses Q from γ while K, V derive from embedded input XE fed at every layer, letting each block "compare" current estimates against observed inputs under DAG constraints. Core assumption: Feeding XE repeatedly provides sufficient signal for each node to extract parental information. Evidence: Section 3.2 explains this enables comparison between current output and input.

### Mechanism 3: Architectural Enforcement of Local Markov Property
The zero-diagonal mask and independent per-node embeddings structurally enforce the local Markov property. Each node's embedding uses independent linear layers, and the zero diagonal prevents self-attention. Final prediction head gᵢ depends only on parental inputs. Core assumption: Tokenwise operations preserve independence across nodes. Evidence: Lemma 1 proves prediction is necessarily a function of parental inputs alone.

## Foundational Learning

- **Directed Acyclic Graphs (DAGs) and Causal Bayesian Networks**
  - Why needed: The entire architecture is predicated on representing causal structure as a DAG with adjacency matrix A. Without understanding factorization p(x₁,...,x|Z|) = ∏p(xᵢ|xₚₐ₍ᵢ₎), the masking mechanism is opaque.
  - Quick check: Given DAG X₁→X₂→X₃ with X₁→X₃, what is the parent set for X₃?

- **Attention Mechanisms in Transformers**
  - Why needed: CaT modifies standard self-attention to causally-masked cross-attention. Understanding Q, K, V computation and softmax scaling is prerequisite to understanding Eq. 3's mask application.
  - Quick check: In self-attention, what does the attention matrix A = softmax(QKᵀ/√d) represent dimensionally and semantically?

- **do-Calculus and Interventional Distributions**
  - Why needed: CaT's intervention mechanism (Algorithm 1) implements the g-formula/truncated factorization. Understanding do(D=d) vs. conditioning on D=d is essential for interpreting treatment effect estimates.
  - Quick check: Why does p(Y|do(D=1)) generally differ from p(Y|D=1) in the presence of confounders?

## Architecture Onboarding

- **Component map**: DAG specification → Adjacency matrix construction → Input embedding → First CBlock (γ as query) → Subsequent CBlocks (prior output as query) → Per-node predictions. For interventions: set values → re-embed → forward pass → read descendants.

- **Critical path**: The method requires specifying a DAG, constructing its adjacency matrix (topologically sorted), embedding inputs through |Z| independent linear layers, then processing through L Causal Blocks where each block applies CMCA with the causal mask, followed by per-node predictions.

- **Design tradeoffs**: Embedding dimension dE ≥ C and > 1 balances capacity vs. disentanglement (dE = 1 fails; higher dE helps separate variable contributions). Number of layers/blocks affects parental information aggregation but risks compounding errors in long mediation chains. CFCN vs. CaT differs in self-attention handling (CFCN adds identity diagonal after layer 1; CaT avoids self-connections entirely).

- **Failure signatures**: Misspecified DAG yields low MSE but biased causal estimates (Table 1 shows false DAG 2 has MSE 0.557 but eATE 1.382). Long mediation chains compound prediction error through recursive inference. Covariate shift in non-ancestors should be robust, but if DAG is wrong, robustness breaks.

- **First 3 experiments**:
  1. Replicate the 4-variable DGP with true/misspecified DAGs, verifying only correct DAG yields unbiased ATE (~1.12) even when MSE is higher.
  2. Implement 3-variable chain X₁→X₂→X₃ with identity diagonal at layer 2+, verifying mask shapes match Fig. 4 and removing diagonal at layer 2+ prevents signal propagation.
  3. Construct DAG D→M→Y, train CaT, then do(D=1) and verify M updates first, then Y uses updated M (Algorithm 1).

## Open Questions the Paper Calls Out

**Can compounding error in recursive inference for long mediation chains be mitigated?** The paper identifies this as a principal limitation but only suggests transitive reduction as a workaround, without empirical validation of alternative solutions.

**How does CaT perform on complex, high-dimensional computer vision tasks with pre-defined causal structures?** All experiments use tabular data or low-dimensional embeddings; no vision benchmarks were evaluated.

**Why do standard non-causal estimators perform competitively on existing causal inference benchmarks?** The authors hypothesize that benchmarks assume simple DAG structures, reducing causal inference to standard supervised learning, but do not validate this.

## Limitations

- **Misspecified DAG brittleness**: Performance degrades significantly when DAG structure is incorrect, revealing fundamental brittleness despite low MSE
- **Long-chain mediation compounding**: Recursive prediction mechanism creates potential error cascade through descendants that isn't fully quantified
- **No layer normalization tradeoff**: Removing layer normalization preserves intervention calibration but may compromise training stability

## Confidence

**High Confidence**: The masking mechanism correctly implements DAG factorization (Theorem 1, Lemma 1); causal attention constraints outperform standard attention under covariate shift (Theorem 2); synthetic experiment demonstrates clear separation between correct and incorrect DAG performance.

**Medium Confidence**: Claims about CFCN being a special case of CaT are plausible but not exhaustively tested; Jobs/Twins/ACIC results show CaTs are competitive but don't establish clear superiority over established causal inference methods; claims about avoiding spurious correlations are supported by synthetic data but not extensively validated on real-world confounders.

**Low Confidence**: The long-term behavior of recursive prediction in deep DAGs (many layers between root and leaf nodes); whether the approach scales to graphs with hundreds of nodes given quadratic attention complexity; the practical value when DAG structure must be estimated from data.

## Next Checks

1. **Error propagation analysis**: Create a synthetic DGP with 10+ sequential mediation steps (X₁→X₂→...→X₁₀). Measure how prediction error compounds through the chain under both correct and incorrect DAG specifications. Quantify the decay rate and identify the breaking point.

2. **DAG estimation integration**: Combine CaTs with a causal discovery algorithm (e.g., PC, GES) on noisy data. Evaluate whether using an estimated DAG (with false discovery rate control) degrades performance compared to ground-truth DAGs, and characterize the trade-off curve.

3. **Large-scale scalability test**: Scale the synthetic experiment to 100-node graphs with average degree 3. Measure training time, memory usage, and prediction accuracy. Compare against baseline methods that use the same causal structure but different neural architectures (e.g., graph neural networks with edge masking).