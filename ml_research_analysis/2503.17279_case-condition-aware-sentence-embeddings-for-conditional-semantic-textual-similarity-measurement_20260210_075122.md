---
ver: rpa2
title: CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual
  Similarity Measurement
arxiv_id: '2503.17279'
source_url: https://arxiv.org/abs/2503.17279
tags:
- sentence
- embeddings
- condition
- similarity
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method for computing semantic textual similarity
  between sentences under a given condition (C-STS). The core idea is to use LLM-based
  encoders with a prompt that encodes the condition given the sentence, followed by
  a supervised projection step using a lightweight FFN to align the embeddings with
  the C-STS task.
---

# CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement

## Quick Facts
- arXiv ID: 2503.17279
- Source URL: https://arxiv.org/abs/2503.17279
- Authors: Gaifan Zhang; Yi Zhou; Danushka Bollegala
- Reference count: 19
- Primary result: Achieves 74.94% Spearman correlation on C-STS dataset using LLM embeddings with supervised projection

## Executive Summary
This paper introduces CASE (Condition-Aware Sentence Embeddings), a method for computing semantic textual similarity between sentences under a given condition (C-STS). The approach leverages large language model (LLM) encoders with prompt-based conditioning, followed by a lightweight supervised projection layer that reduces dimensionality by 8x while improving performance. The method demonstrates that LLM-based embeddings consistently outperform masked language model (MLM) embeddings for C-STS tasks, and that subtracting condition embeddings increases isotropy, leading to better similarity measurements. The proposed framework achieves state-of-the-art results on the C-STS dataset while using fewer parameters than previous approaches.

## Method Summary
CASE uses LLM-based encoders to generate sentence embeddings conditioned on a given context, followed by a supervised projection step using a lightweight feed-forward network (FFN). The key innovation involves encoding the condition along with the sentence through prompting, then subtracting the condition embedding to increase isotropy before projection. This approach addresses the C-STS task where similarity must be measured under specific conditions, improving upon previous methods that didn't account for conditional contexts. The method reduces embedding dimensionality by 8x while maintaining or improving performance, making it more efficient than previous approaches.

## Key Results
- Achieves 74.94% Spearman correlation on the C-STS dataset, outperforming previous methods
- LLM embeddings consistently outperform MLM embeddings for C-STS tasks
- Supervised projection with 8x dimensionality reduction still improves performance
- Condition embedding subtraction increases isotropy and improves C-STS measurement

## Why This Works (Mechanism)
The method works by leveraging the superior contextual understanding of LLM encoders compared to MLM-based models. By explicitly encoding the condition through prompting and then subtracting it, the approach creates more isotropically distributed embeddings that better capture semantic similarity under specific conditions. The supervised projection layer learns to map these high-dimensional embeddings to a lower-dimensional space optimized for the C-STS task, effectively filtering out noise while preserving task-relevant information.

## Foundational Learning
- Conditional Semantic Textual Similarity (C-STS): Measuring similarity between sentences under specific conditions/contexts - needed because standard STS doesn't account for contextual constraints
- LLM vs MLM encoders: Understanding the architectural differences between encoder-decoder (LLM) and masked language models (MLM) - quick check: compare BERT (MLM) vs GPT (LLM) architectures
- Isotropic embeddings: Distributions where vectors are uniformly spread in all directions - why needed: improves similarity measurement by reducing directional bias
- Supervised projection: Using labeled data to learn optimal dimensionality reduction - quick check: verify projection layer learns meaningful transformations
- Embedding isotropy: The uniformity of vector distribution in embedding space - needed because isotropic embeddings typically yield better similarity measurements
- Prompt engineering: Crafting inputs to guide LLM behavior - quick check: test different prompt formulations for condition encoding

## Architecture Onboarding
Component map: Sentence -> LLM Encoder -> Condition Embedding Subtraction -> Supervised Projection (FFN) -> C-STS Output
Critical path: The LLM encoder and condition embedding subtraction are critical for capturing conditional context, while the supervised projection layer optimizes for the specific C-STS task
Design tradeoffs: Balances between embedding quality (LLM) and efficiency (dimensionality reduction), while maintaining task performance through supervision
Failure signatures: Poor performance on C-STS likely indicates issues with condition encoding, insufficient isotropy, or suboptimal projection learning
First experiments:
1. Compare LLM vs MLM encoder performance on C-STS without any projection
2. Test different prompt formulations for condition encoding
3. Evaluate projection performance with varying dimensionality reduction ratios

## Open Questions the Paper Calls Out
None

## Limitations
- Results are based on a single C-STS dataset, limiting generalizability to other domains or tasks
- Method requires labeled data for supervised projection, which may not be available for all languages or domains
- Theoretical justification for why condition embedding subtraction improves C-STS performance could be more rigorous

## Confidence
- LLM-based encoders consistently outperform MLM-based encoders for C-STS (Medium): Supported by C-STS dataset experiments but needs validation on additional datasets
- Supervised projection significantly improves performance with dimensionality reduction (High): Clear experimental evidence across multiple settings
- Condition embedding subtraction increases isotropy and improves C-STS performance (Medium): Results support the claim but theoretical connection needs strengthening

## Next Checks
1. Validate the method's performance on additional C-STS datasets and related tasks (e.g., cross-lingual STS, domain adaptation scenarios) to assess generalizability beyond the C-STS dataset
2. Conduct ablation studies to determine the optimal architecture for the supervised projection layer and test whether other projection methods (linear/non-linear) yield similar or better performance
3. Evaluate the method's effectiveness on low-resource languages and with limited labeled training data to assess practical deployment constraints