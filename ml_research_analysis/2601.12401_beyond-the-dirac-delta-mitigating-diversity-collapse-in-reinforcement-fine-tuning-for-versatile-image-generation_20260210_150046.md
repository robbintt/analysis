---
ver: rpa2
title: 'Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement Fine-Tuning
  for Versatile Image Generation'
arxiv_id: '2601.12401'
source_url: https://arxiv.org/abs/2601.12401
tags:
- diversity
- reward
- generation
- fine-tuning
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DRIFT is a framework that mitigates diversity collapse in reinforcement
  fine-tuning of diffusion models by embedding diversity incentives throughout the
  training process. It addresses this through three key mechanisms: (1) reward-concentrated
  sampling to filter outliers, (2) noise-conditioned prompting to expand the conditioning
  space, and (3) potential-based reward shaping to optimize intra-group diversity
  without policy distortion.'
---

# Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement Fine-Tuning for Versatile Image Generation

## Quick Facts
- **arXiv ID**: 2601.12401
- **Source URL**: https://arxiv.org/abs/2601.12401
- **Reference count**: 40
- **Primary result**: DRIFT achieves 9.08%–43.46% higher diversity at equivalent alignment levels and 59.65%–65.86% higher alignment at equivalent diversity levels compared to competitive baselines.

## Executive Summary
Reinforcement fine-tuning of diffusion models often suffers from diversity collapse, producing repetitive outputs despite alignment with human preferences. DRIFT addresses this by embedding diversity incentives throughout the training process through three key mechanisms: reward-concentrated sampling to filter outliers, noise-conditioned prompting to expand the conditioning space, and potential-based reward shaping to optimize intra-group diversity without policy distortion. Experiments show DRIFT achieves superior Pareto dominance in the reward-diversity tradeoff compared to competitive baselines.

## Method Summary
DRIFT fine-tunes Stable Diffusion v1.5 with LoRA adapters on UNet attention layers using group relative policy optimization (GRPO). The framework implements three diversity-preserving mechanisms: (1) reward-concentrated sampling selects 8 samples from 16 candidates based on nearest-neighbor criterion, (2) noise-conditioned prompting adds annealed Gaussian noise to embeddings during training, and (3) potential-based reward shaping with clipped intrinsic diversity rewards. Training uses PickScore and HPSv2 reward models on 45 animal prompts with evaluation on 40 prompts × 40 images per prompt.

## Key Results
- DRIFT achieves 9.08%–43.46% higher diversity (DreamSim, ClipScore, Generalized Recall, Vendi Score) at equivalent alignment levels
- DRIFT achieves 59.65%–65.86% higher alignment (normalized reward) at equivalent diversity levels
- DRIFT demonstrates superior Pareto dominance in reward-diversity tradeoff compared to competitive baselines

## Why This Works (Mechanism)

### Mechanism 1: Reward-Concentrated Sampling
- **Claim**: Filtering reward outliers during policy updates prevents premature distribution sharpening.
- **Evidence**: 8.56%–30.77% diversity gain and 37.74% reward gain vs. reward-contrast sampling.
- **Break condition**: If semantic diversity is reward-correlated, concentrated sampling may still collapse.

### Mechanism 2: Noise-Conditioned Prompting
- **Claim**: Timestep-annealed noise on prompt embeddings expands the explored conditioning manifold without sacrificing semantic fidelity.
- **Evidence**: Training with prompting noise yields higher diversity; testing-time noise alone provides no benefit.
- **Break condition**: If noise scale η is too large, semantic consistency degrades; if too small, diversity gains vanish.

### Mechanism 3: Potential-Based Diversity Reward Shaping
- **Claim**: Intra-group diversity can be optimized without distorting the primary reward objective using a potential-based intrinsic reward.
- **Evidence**: DRIFT achieves 40%+ diversity gain in DreamSim/Recall with PickScore; 59.65%–65.86% reward gain.
- **Break condition**: If the diversity metric (DreamSim) doesn't align with task-relevant variation, the shaped reward optimizes the wrong objective.

## Foundational Learning

- **Concept: KL-Constrained Reward Maximization (Eq. 1)**
  - Why needed: This is the base objective that theoretically collapses to Dirac delta as β→0.
  - Quick check: What happens to the optimal policy π* when β is very small and rewards vary slightly?

- **Concept: Potential-Based Reward Shaping**
  - Why needed: Ensures adding diversity incentives doesn't change the optimal policy.
  - Quick check: Why does a potential-based intrinsic reward preserve optimal policy invariance?

- **Concept: GRPO (Group Relative Policy Optimizer)**
  - Why needed: DRIFT builds on GRPO's group-level advantage computation.
  - Quick check: How does decoupled advantage computation (Ã = A + λ·A_int) differ from reward-level aggregation (R̃ = R + λ·R_int)?

## Architecture Onboarding

- **Component map**: Prompt c → Noise-Conditioned Prompting → ẽ_t → Diffusion Policy π_θ → Images {x₀^i} → Reward Model → R(τ_i) → Advantage Computer → DreamSim Encoder → d(x₀^i) → Intrinsic Reward → Reward-Concentrated Selector ← Candidate pool (2G) → Policy Update via GRPO objective

- **Critical path**: The reward-concentrated selector is applied before advantage computation; noise-conditioned prompting happens during sampling; diversity reward shaping modifies the advantage signal.

- **Design tradeoffs**: Group size G=8 balances advantage statistics and compute; noise scale η=0.05 prevents semantic drift; shaping ratio λ=0.5 optimizes diversity emphasis; clipping threshold σ=1.0 prevents reward hacking.

- **Failure signatures**: Diversity still collapses if reward model has near-zero variance; semantic coherence degrades if noise scale is too large; no improvement if diversity reward is zero or non-varying.

- **First 3 experiments**:
  1. Ablate reward-concentrated vs. reward-contrast sampling on 5 animal prompts, measuring DreamSim diversity and reward Pareto frontier.
  2. Test noise-conditioned prompting at train-only, test-only, both, neither with fixed seeds and 10 prompts.
  3. Vary shaping ratio λ ∈ {0, 0.25, 0.5, 1.0} with clipping σ ∈ {0.5, 1.0, 2.0}, expecting λ=0.5, σ=1.0 achieves best tradeoff.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can internal latent representations of diffusion models replace the external DreamSim encoder for computing intra-group diversity without sacrificing performance?
- **Open Question 2**: How effectively does DRIFT generalize to video and 3D generation tasks where diversity collapse is also prevalent?
- **Open Question 3**: Does DRIFT maintain its effectiveness across more diverse prompt domains beyond the 45 common animal prompts tested?

## Limitations

- Limited generalization to diverse prompt types beyond common animals
- Unknown sensitivity to hyperparameter variations across different domains
- Reliance on external DreamSim encoder increases computational complexity

## Confidence

- **High confidence**: Core problem formulation and experimental methodology are sound
- **Medium confidence**: Empirical results demonstrate Pareto improvements within tested scope
- **Low confidence**: Theoretical analysis of reward-concentrated sampling's effect on KL-constrained optimization

## Next Checks

1. Apply DRIFT to same animal prompts using HPSv2 as primary reward model instead of PickScore, comparing diversity-recall Pareto curves.

2. Systematically vary group size G ∈ {4,8,16}, noise scale η ∈ {0.01,0.05,0.1}, and shaping ratio λ ∈ {0.25,0.5,0.75} on subset of prompts to establish optimal ranges.

3. Conduct human evaluation of semantic coherence for highest diversity settings, measuring intra-prompt consistency and inter-prompt distinctiveness.