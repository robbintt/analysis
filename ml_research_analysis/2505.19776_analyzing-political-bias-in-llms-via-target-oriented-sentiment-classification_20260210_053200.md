---
ver: rpa2
title: Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification
arxiv_id: '2505.19776'
source_url: https://arxiv.org/abs/2505.19776
tags:
- political
- bias
- sentiment
- entities
- biases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new method to analyze political bias in
  large language models (LLMs) by exploiting inconsistencies in target-oriented sentiment
  classification. The authors quantify bias using an entropy-based inconsistency metric,
  inserting 1,319 politically and demographically diverse politician names into 450
  political sentences across six languages and seven LLM models.
---

# Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification

## Quick Facts
- **arXiv ID**: 2505.19776
- **Source URL**: https://arxiv.org/abs/2505.19776
- **Reference count**: 40
- **Primary result**: New method quantifies political bias in LLMs via TSC inconsistency across 1,319 politicians in 6 languages and 7 models

## Executive Summary
This paper introduces a novel approach to quantify political bias in large language models by measuring prediction inconsistencies in target-oriented sentiment classification when politician names are swapped in political sentences. The authors define an entropy-based inconsistency metric and apply it across 1,319 politicians in 6 languages using 7 LLM models, finding significant bias in all tested configurations. Larger models show stronger and more consistent biases, and replacing politician names with fictional alternatives partially mitigates but doesn't eliminate bias. The study provides a statistically robust framework for assessing and mitigating political bias in multilingual LLMs.

## Method Summary
The method constructs a dataset of 450 political sentences and 1,319 politicians balanced across 8 political alignments and countries. For each sentence-entity pair, the LLM performs 9-shot target-oriented sentiment classification using a standardized prompt. The primary metric, Inconsistency (IC), is computed as the average entropy of sentiment predictions across all entities for each sentence. The authors test 7 LLM models across 6 languages and apply a mitigation strategy by replacing real politician names with fictional names generated by GPT-4. Mann-Whitney U tests assess statistical significance of bias patterns between political alignments.

## Key Results
- All tested model-language combinations exhibit significant TSC prediction inconsistencies, indicating systematic political bias
- Larger models (Qwen-72B, Llama 3-70B) show stronger and more consistent biases than smaller models (Qwen-7B, Llama 3-8B)
- Bias intensity is higher for Western languages compared to non-Western languages
- Fictional name replacement reduces inconsistencies and slightly improves accuracy but doesn't eliminate bias
- Models systematically assign more positive sentiment to left-leaning politicians and more negative sentiment to far-right politicians

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Target-oriented sentiment classification inconsistency reveals political bias encoded in LLMs
- Mechanism: If an LLM is unbiased regarding political entities, predicting sentiment toward a target in a sentence should yield the same result regardless of which politician replaces "X". The paper defines an entropy-based inconsistency metric to quantify this prediction variability across 1,319 politicians. High inconsistency indicates the model's prediction depends on the specific entity, not just sentence content.
- Core assumption: The political sentences used are general enough that the sentiment toward the target entity should be invariant to the entity's identity.
- Evidence anchors:
  - [abstract] "We propose a new approach leveraging the observation that LLM sentiment predictions vary with the target entity in the same sentence."
  - [section 3.1] "An unbiased LLM would provide the same answer for a given sentence, regardless of the target entity."
- Break condition: If sentences inherently carry different sentiments for different entities, inconsistency would reflect a factual property, not LLM bias.

### Mechanism 2
- Claim: Name replacement with fictional, demographically-matched entities serves as a bias mitigation strategy for TSC
- Mechanism: The approach isolates political attributes (which cause bias) from non-political attributes (gender, country, age). By generating a fictional name that shares only non-political attributes and re-running the TSC task, inconsistency caused by political associations should diminish.
- Core assumption: The LLM used to generate fictional names (GPT-4) does not introduce systematic political bias into the fake names.
- Evidence anchors:
  - [abstract] "Replacing politician names with fictional but plausible counterparts reduces inconsistencies and slightly improves accuracy."
  - [section 4] "Figure 7 demonstrates that political biases are significantly reduced compared to the original results."
- Break condition: If the fictional name generator inadvertently creates names that sound political or if the LLM performing TSC infers political alignment from non-political attributes, residual bias will persist.

### Mechanism 3
- Claim: Scaling up LLM size amplifies and stabilizes political bias across languages
- Mechanism: Larger models have greater capacity to internalize complex, implicit associations from training data, including sentiment patterns associated with political figures. This internalization leads to more pronounced biases and more robust internal representations, causing predictions to become more consistent across different languages.
- Core assumption: The training corpora of larger models contain stronger or more coherent political sentiment signals that the model's increased capacity can capture and generalize across languages.
- Evidence anchors:
  - [abstract] "Results show that all tested model-language combinations exhibit significant bias, with larger models displaying stronger and more consistent biases."
  - [section 4] "Larger models exhibit stronger biases that are more consistent across languages."
- Break condition: This scaling effect is conditional on training data properties; a model trained on carefully curated or politically neutral datasets might not show this amplification.

## Foundational Learning

- **Target-Oriented Sentiment Classification (TSC)**
  - Why needed here: This is the core NLP task used as the instrument to measure bias. Understanding TSC is essential to grasp why varying the target entity is a valid bias test.
  - Quick check question: In the sentence "Critics praised the senator's new bill but attacked her opponent's proposal," what is the target, and what are the sentiments?

- **Entropy as a Metric for Inconsistency**
  - Why needed here: The paper introduces a novel metric. Understanding how entropy quantifies prediction variability is critical to interpreting "inconsistency" figures.
  - Quick check question: If an LLM predicts 'positive' for 50% of names and 'negative' for the other 50% in the same sentence, what would be the value of the inconsistency metric (IC)?

- **Political Alignment Classification**
  - Why needed here: The analysis depends on grouping politicians into categories (Far Left, Left, Center Left, etc.). Understanding this classification system is necessary to interpret bias patterns in the paper's charts.
  - Quick check question: If a political party is listed as "centre-left," "centrism," and "centre-right" in Wikidata, what single alignment label would it be assigned using the paper's methodology?

## Architecture Onboarding

- **Component map**:
  - Data Layer (MAD-TSC dataset + Politician List) -> LLM Probing Layer (9-shot prompting across 7 models) -> Analysis Layer (IC metric + sentiment aggregation) -> Mitigation Layer (fictional name generation)

- **Critical path**:
  1. **Sentence & Entity Curation**: Most critical step. Flaws in sentence selection or entity alignment mislabeling invalidate all downstream results.
  2. **Scalable LLM Inference**: Running ~25 million prompts requires robust, parallelized inference. Failures here block the entire experiment.
  3. **Aggregation & Metric Computation**: Post-processing where raw predictions transform into key findings. Errors here lead to incorrect conclusions.

- **Design tradeoffs**:
  - **Generalizability vs. Control**: Controlled TSC task provides clean signal but trades off ecological validity of open-ended generation tasks.
  - **Scale vs. Granularity**: Massive dataset (25M points) provides high statistical power but creates computational barriers and slow iteration.
  - **Simplicity vs. Completeness**: Name replacement mitigation is simple but incomplete—doesn't address all bias forms and may remove useful context.

- **Failure signatures**:
  - **Zero Inconsistency Score**: Suggests broken prompting/experiment (model not processing entity name) rather than perfect unbiasedness.
  - **Uniform Sentiment Distribution**: If all models predict "Neutral" everywhere, indicates prompt/instruction-following failure, not lack of bias.
  - **Biased Control Group**: If fictional name mitigation shows similar bias patterns to real names, fictional name generator has encoded biases it was meant to avoid.

- **First 3 experiments**:
  1. **Reproduce Baseline Inconsistency**: Re-run small subset (10 sentences, 50 politicians) with single model (Llama-3-8B) in English. Verify non-zero, sensible inconsistency score to validate setup.
  2. **Validate Mitigation on Small Scale**: Implement fictional name generator; run same subset with fake names. Confirm inconsistency score drops as paper claims.
  3. **Pilot New Entity Class**: Introduce new entity class (e.g., CEOs) into neutral business sentences. Test if similar bias patterns emerge, suggesting framework is general-purpose entity-bias probe.

## Open Questions the Paper Calls Out

- **Open Question 1**: Do political biases in LLMs manifest similarly across other subjective downstream NLP tasks such as stance detection and hate speech detection?
- **Open Question 2**: How do political biases manifest for non-politician entities such as political parties, public organizations, companies, or products?
- **Open Question 3**: Do political bias patterns persist in low-resource languages, or do different training data compositions alter bias intensity and direction?
- **Open Question 4**: Why do larger models exhibit stronger and more consistent political biases across languages?

## Limitations

- The entropy-based inconsistency metric may conflate entity-specific bias with sentence-specific or linguistic effects that vary naturally with different entities
- The fictional name generation mitigation strategy relies on GPT-4 not encoding political biases, an assumption not independently validated
- The causal mechanism linking model scale to bias amplification is not established—the observed correlation may depend heavily on training data properties
- Cross-lingual consistency findings may reflect dominant language bias propagation rather than universal bias patterns

## Confidence

- **High Confidence**: All tested models exhibit significant TSC prediction inconsistencies across political entities
- **Medium Confidence**: Larger models display stronger and more consistent biases across languages
- **Medium Confidence**: Fictional name replacement strategy partially mitigates bias
- **Low Confidence**: Interpretation that inconsistencies represent political bias rather than sentence-specific or linguistic effects

## Next Checks

1. **Sentence Neutrality Validation**: Systematically annotate a subset of the 450 political sentences for explicit political content or entity-specific sentiment. Recompute inconsistency metrics on sentences confirmed to be neutral to establish whether the observed bias patterns persist only in truly entity-agnostic contexts.

2. **Control Group Bias Analysis**: Conduct a detailed analysis of the fictional names' perceived political alignment using a separate LLM or human annotators. Compare the alignment distribution of fictional names to real politicians to quantify whether the control group introduces unintended bias into the mitigation experiment.

3. **Causal Attribution Study**: Design a follow-up experiment using sentences that are confirmed neutral across all entities, then systematically vary non-political attributes (gender, nationality) while keeping political alignment constant. Measure whether TSC inconsistency correlates with these attributes to isolate political bias from other forms of entity-based bias.