---
ver: rpa2
title: 'MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional Evaluation
  in Tabular Domains'
arxiv_id: '2505.14312'
source_url: https://arxiv.org/abs/2505.14312
tags:
- datasets
- tabular
- performance
- data
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MULTI TAB, a comprehensive benchmark suite
  for multi-dimensional evaluation of tabular learning algorithms. Unlike existing
  benchmarks that rely on average-case metrics, MULTI TAB categorizes 196 publicly
  available datasets along key data characteristics (sample size, label imbalance,
  feature interaction) and evaluates 13 representative models with diverse inductive
  biases.
---

# MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional Evaluation in Tabular Domains

## Quick Facts
- arXiv ID: 2505.14312
- Source URL: https://arxiv.org/abs/2505.14312
- Reference count: 40
- Primary result: MultiTab reveals that tabular model performance is highly sensitive to data characteristics, showing that inductive biases interact differentially with dataset regimes and that regime-aware evaluation is essential for understanding model behavior.

## Executive Summary
This paper introduces MultiTab, a comprehensive benchmark suite for evaluating tabular learning algorithms across diverse data regimes. Unlike existing benchmarks that rely on average-case metrics, MultiTab categorizes 196 publicly available datasets along key data characteristics including sample size, label imbalance, and feature interaction. The study evaluates 13 representative models with diverse inductive biases and reveals that model performance is highly sensitive to data regimes. For example, models using sample-level similarity excel on large datasets or with high feature correlation, while those encoding inter-feature dependencies perform best with weakly correlated features. These findings demonstrate that inductive biases do not always behave as intended, and that regime-aware evaluation is essential for understanding and improving model behavior.

## Method Summary
The benchmark evaluates 13 models across 196 datasets from OpenML and scikit-learn, categorizing datasets into sub-categories based on seven data characteristics (sample size, label imbalance, feature interaction, etc.). Models are trained using stratified k-fold cross-validation with independent hyperparameter optimization per fold using Optuna TPE (100 trials). Performance is measured using normalized predictive error that scales per-dataset error between the best and worst models. The benchmark includes six model families with different inductive biases: classical, GBDT, NN-Simple, NN-Feature, NN-Sample, and NN-Both. Neural networks use ensemble predictions from top 5 hyperparameter configurations, while tree-based models use the single best configuration.

## Key Results
- Model performance varies systematically with data characteristics due to interactions between inductive biases and data structure
- Models using sample-level similarity (NN-Sample) excel on large datasets or with high inter-feature correlation
- Models encoding inter-feature dependencies (NN-Feature) perform best with weakly correlated features
- Categorical embedding modules improve performance primarily when categorical features dominate
- High function irregularity degrades performance across all model families, with no consistent mitigation from existing inductive biases

## Why This Works (Mechanism)

### Mechanism 1: Inductive Bias–Data Regime Alignment
Model performance varies systematically with dataset characteristics because architectural inductive biases interact differentially with data structure. NN-Sample models (TabR, ModernNCA) leverage retrieval or metric learning to exploit sample-level similarity, benefiting most when features are correlated or sample sizes are large. NN-Feature models (FT-Transformer, T2G-Former) use attention to model inter-feature dependencies, which becomes most informative when features are weakly correlated and independent signals exist.

### Mechanism 2: Categorical Embedding as Conditional Inductive Bias
Categorical embedding modules improve performance primarily when categorical features dominate, with limited benefit in numerical-heavy regimes. Embeddings learn dense representations of discrete features, reducing sparsity and enabling gradient-based optimization. When categorical features are scarce, embeddings add parameters without corresponding signal; when dominant, they provide the necessary representational capacity.

### Mechanism 3: Function Irregularity as Universal Challenge
High function irregularity degrades performance across all model families, with existing inductive biases (piecewise splits, periodic encodings) failing to consistently mitigate this. Irregular functions have high-frequency spectral components where small input perturbations cause large target shifts. Models biased toward smooth functions (plain MLPs) suffer most; tree-based models and NN-Sample models degrade less but still significantly.

## Foundational Learning

- **Concept: Inductive Bias**
  - Why needed here: The paper's central thesis is that different architectural assumptions succeed under different data conditions; understanding what inductive biases each model encodes is prerequisite to interpreting the results.
  - Quick check question: Can you explain why a model that assumes smooth functions might struggle with irregular tabular data?

- **Concept: Cross-Validation with Hyperparameter Optimization**
  - Why needed here: The benchmark uses stratified k-fold CV with independent hyperparameter optimization per fold; misunderstanding this protocol could lead to incorrect comparisons.
  - Quick check question: Why is hyperparameter optimization performed independently for each fold rather than once globally?

- **Concept: Normalized Metrics Across Heterogeneous Datasets**
  - Why needed here: Raw error metrics (log loss, RMSE) are scaled within each dataset-split pair to enable fair aggregation across datasets with varying difficulty.
  - Quick check question: Why does the benchmark normalize errors relative to the best/worst model per dataset rather than using raw error values?

## Architecture Onboarding

- **Component map:** Data layer (196 OpenML datasets → preprocessing → sub-category assignment along 7 axes) → Model layer (13 models organized into 6 families by inductive bias) → Evaluation layer (100 TPE trials per model-dataset → k-fold CV → normalized error aggregation)

- **Critical path:** Load dataset from OpenML by ID → Apply preprocessing pipeline (quantile transform for numerical, label encoding for categorical) → Classify into sub-categories using thresholds → Run 100 hyperparameter trials with TPE → Retrain with best config, compute normalized error relative to other models

- **Design tradeoffs:** Fixed trial budget (100) ensures equal optimization effort but may under-optimize slow-converging architectures; sub-category thresholds are heuristic and interpretable but not theoretically grounded; exclusion of AutoML frameworks isolates individual model behavior but limits practical applicability

- **Failure signatures:** Models showing large variance across sub-categories indicate high regime sensitivity (e.g., SAINT); consistent underperformance across all regimes suggests architectural mismatch (e.g., vanilla MLP on irregular data); confidence interval overlap with best model indicates competitive performance despite rank differences

- **First 3 experiments:**
  1. Reproduce the sample size comparison: train XGBoost, ModernNCA, and FT-Transformer on datasets split by sample size threshold; verify that ModernNCA and FT-Transformer remain competitive in small-sample regimes.
  2. Test feature interaction hypothesis: compare NN-Sample vs NN-Feature models on datasets with high vs low Frobenius norm correlation; confirm opposing performance trends.
  3. Validate categorical embedding effect: run MLP vs MLP-C on categorical-heavy vs categorical-scarce datasets; measure performance gap magnitude.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific architectural innovations or training strategies can effectively handle datasets with high function irregularity?
- **Basis in paper:** Section 4.2.6 states existing models fall short in highly irregular regimes, highlighting the need for new architectures and training strategies tailored to such conditions.
- **Why unresolved:** Existing inductive biases designed for irregularity showed substantial degradation or failed to provide consistent performance gains in the "Irregular" sub-category.
- **What evidence would resolve it:** A new model or training methodology that demonstrates statistically significant performance improvements specifically within the high function irregularity regime.

### Open Question 2
- **Question:** How can the performance gap between Neural Networks and GBDTs be bridged for regression tasks with highly skewed target distributions?
- **Basis in paper:** Section 4.2.5 concludes that while recent neural models have narrowed the gap with GBDTs, a notable performance difference remains—especially under skewed regression targets.
- **Why unresolved:** Models like ModernNCA suffer a widening performance gap compared to CatBoost as regression target skewness increases.
- **What evidence would resolve it:** A neural network architecture or loss function that matches GBDT robustness in the high-skew sub-category (absolute skewness > 1.5).

### Open Question 3
- **Question:** Can tabular foundation models be developed to be both computationally efficient and applicable to large-scale datasets (>3,000 samples, >1,000 features)?
- **Basis in paper:** Section 4.4 notes regarding TabPFN that its applicability remains limited, highlighting the need for more scalable and flexible pretrained models.
- **Why unresolved:** Current pretrained models like TabPFN are restricted to small datasets (max 3,000 samples), forcing benchmarks to exclude the majority of datasets in the "Large" sample size regime.
- **What evidence would resolve it:** A pretrained model that maintains competitive performance against supervised models like ModernNCA on large sample size and high feature-dimensionality datasets.

### Open Question 4
- **Question:** How do data regime sensitivities shift when evaluating pretraining-based or AutoML-driven frameworks compared to the supervised individual models analyzed in this study?
- **Basis in paper:** The Conclusion states the study is limited to supervised settings; future work may extend this framework to pretraining-based or AutoML-driven scenarios.
- **Why unresolved:** The current benchmark strictly evaluates individual supervised models, leaving the interaction between complex ensemble/meta-learning frameworks and specific data characteristics unexplored.
- **What evidence would resolve it:** A regime-aware evaluation of AutoML frameworks or self-supervised tabular models across the 24 defined sub-categories to see if they mitigate specific sensitivities.

## Limitations

- Threshold sensitivity: Categorical boundaries (e.g., entropy ratio <0.3 for imbalance) are heuristic without theoretical grounding
- Generalizability beyond OpenML: The 196 datasets are predominantly from OpenML; performance patterns may not extend to proprietary or industrial datasets
- Function irregularity metric validity: The spectral-based irregularity measure lacks direct empirical validation against human-labeled function complexity

## Confidence

- **High confidence:** Empirical observations about regime-dependent performance are directly supported by benchmark results
- **Medium confidence:** Mechanism explanations linking inductive biases to performance patterns are plausible but not definitively proven
- **Low confidence:** Claim that no consistent performance gains exist for function irregularity is limited by absence of architectures specifically designed for this challenge

## Next Checks

1. **Threshold robustness test:** Re-run key comparisons (e.g., NN-Sample vs NN-Feature) with alternative threshold values for categorical boundaries to assess sensitivity
2. **Cross-dataset generalization:** Evaluate a subset of models on at least 10 non-OpenML datasets to test pattern stability
3. **Targeted irregularity evaluation:** Benchmark additional architectures designed for irregular functions (e.g., neural additive models, spline-based methods) to reassess the "no consistent gains" finding