---
ver: rpa2
title: Sharpe Ratio Optimization in Markov Decision Processes
arxiv_id: '2509.00793'
source_url: https://arxiv.org/abs/2509.00793
tags:
- ratio
- optimization
- sharpe
- policy
- mdps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses Sharpe ratio optimization in Markov decision
  processes, a challenging problem due to the ratio-form objective and variance-related
  risk metrics. The authors propose a novel dynamic programming approach using Dinkelbach's
  transform to linearize the Sharpe ratio objective into a sequence of mean-squared-variance
  (M2V) optimizations.
---

# Sharpe Ratio Optimization in Markov Decision Processes

## Quick Facts
- arXiv ID: 2509.00793
- Source URL: https://arxiv.org/abs/2509.00793
- Authors: Shuai Ma; Guangwu Liu; Li Xia
- Reference count: 7
- The paper proposes a dynamic programming approach using Dinkelbach's transform to solve Sharpe ratio optimization in Markov decision processes.

## Executive Summary
This paper addresses the challenging problem of Sharpe ratio optimization in Markov decision processes, where the objective is a ratio of expected return to return variance. The authors develop a novel approach using Dinkelbach's transform to convert the ratio optimization into a sequence of mean-squared-variance (M2V) optimizations that can be solved with dynamic programming. They propose policy iteration algorithms (SRPI and SRPI+) that iteratively solve these M2V problems to find the optimal policy. The algorithms are proven to converge to the global optimum with a monotonic increase in Sharpe ratio. Numerical experiments validate the effectiveness of the approach, demonstrating improved computational efficiency compared to theoretical bounds.

## Method Summary
The authors tackle Sharpe ratio optimization in MDPs by first applying Dinkelbach's transform to linearize the ratio objective into a sequence of M2V optimization subproblems. They develop policy iteration algorithms (SRPI and SRPI+) that iteratively solve these M2V problems. Each iteration involves solving a linear program to find the optimal value function for the current policy, then updating the policy by maximizing the Sharpe ratio over the action space. The algorithms are proven to converge to the global optimum with a monotonic increase in Sharpe ratio at each iteration. The key insight is that by transforming the ratio optimization into a sequence of linear problems, the otherwise non-convex Sharpe ratio objective becomes tractable through dynamic programming.

## Key Results
- The SRPI and SRPI+ algorithms converge to the global optimal policy for Sharpe ratio maximization
- Computational complexity is O(1/ε²) for SRPI and O(1/ε) for SRPI+, improving upon theoretical bounds
- Numerical experiments demonstrate the effectiveness and efficiency of the proposed approach

## Why This Works (Mechanism)
The approach works by leveraging Dinkelbach's transform to convert the non-convex ratio optimization problem into a sequence of convex subproblems. Each subproblem (M2V optimization) can be solved exactly using linear programming, making the overall algorithm tractable. The policy iteration framework ensures monotonic improvement of the Sharpe ratio, guaranteeing convergence to the global optimum. By iteratively updating both the value function and policy, the algorithm efficiently explores the policy space while maintaining computational feasibility.

## Foundational Learning
- Markov Decision Processes (MDPs): Framework for sequential decision making under uncertainty; needed for modeling the optimization problem; quick check: states, actions, transition probabilities, rewards
- Sharpe Ratio: Risk-adjusted return metric; needed as the optimization objective; quick check: ratio of expected return to standard deviation of returns
- Dinkelbach's Transform: Technique for solving fractional programming problems; needed to linearize the ratio objective; quick check: converts ratio optimization to sequence of parametric optimizations
- Policy Iteration: Dynamic programming algorithm for finding optimal policies; needed for solving the M2V subproblems; quick check: alternates between policy evaluation and policy improvement
- Mean-Squared-Variance (M2V) Optimization: Optimization with variance penalty; needed as the transformed subproblem; quick check: combines expected value and variance in quadratic form

## Architecture Onboarding

Component Map:
MDP Environment -> Dinkelbach Transform -> M2V Subproblem Solver -> Policy Evaluation -> Policy Improvement -> (loop back)

Critical Path:
1. Initialize policy π₀
2. Apply Dinkelbach transform to get M2V objective
3. Solve M2V subproblem via linear programming
4. Evaluate policy using Bellman equations
5. Improve policy by maximizing Sharpe ratio
6. Check convergence, repeat if necessary

Design Tradeoffs:
- Stationary vs non-stationary policies: Stationary assumed for tractability but may miss optimal solutions
- Variance vs other risk metrics: Variance is tractable but may not capture all risk preferences
- Exact vs approximate subproblem solving: Exact LP solving ensures convergence but may be computationally expensive

Failure Signatures:
- Non-convergence: May indicate numerical instability in LP solver or poor initialization
- Oscillating Sharpe ratio: Could suggest cycling between policies or insufficient exploration
- Suboptimal solution: Might result from approximation errors or limitations of variance metric

First Experiments:
1. Verify convergence on simple MDP with known optimal policy
2. Test sensitivity to initial policy and Dinkelbach parameter
3. Compare SRPI vs SRPI+ on benchmark problems

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Computational complexity remains high for large state spaces despite improved bounds
- Assumption of stationary policies may not capture all optimal solutions in some MDP settings
- Variance metric used may not fully represent all risk preferences in practical applications

## Confidence

High confidence in the theoretical framework and convergence proofs
Medium confidence in computational efficiency claims, as real-world performance may vary
Medium confidence in the practical applicability given the simplified risk metric

## Next Checks

1. Empirical validation on larger MDP instances with varying state-action spaces to verify computational efficiency claims
2. Comparison with alternative risk metrics (e.g., Conditional Value at Risk) to assess the limitations of variance-based optimization
3. Extension to non-stationary policies to determine if they yield better Sharpe ratios in specific problem classes