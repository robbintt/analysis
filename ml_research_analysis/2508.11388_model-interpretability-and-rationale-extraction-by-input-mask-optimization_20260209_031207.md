---
ver: rpa2
title: Model Interpretability and Rationale Extraction by Input Mask Optimization
arxiv_id: '2508.11388'
source_url: https://arxiv.org/abs/2508.11388
tags:
- input
- marc
- rationale
- methods
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a model-agnostic method for generating interpretable
  explanations for neural network predictions. The core idea is to use gradient-based
  optimization to mask input features that are not indicative of a specific class,
  while enforcing sufficiency, comprehensiveness, and compactness through regularization.
---

# Model Interpretability and Rationale Extraction by Input Mask Optimization

## Quick Facts
- arXiv ID: 2508.11388
- Source URL: https://arxiv.org/abs/2508.11388
- Authors: Marc Brinner; Sina Zarriess
- Reference count: 22
- Key outcome: State-of-the-art rationale extraction (Token F1 0.473, mAP 0.469) and competitive faithfulness evaluation (sufficiency 0.196, comprehensiveness 0.612) via gradient-based input mask optimization.

## Executive Summary
This paper introduces MaRC, a model-agnostic method for generating interpretable explanations by optimizing an input mask through gradient descent. The approach enforces sufficiency (the mask contains all class-indicative features), comprehensiveness (the mask excludes all non-indicative features), and compactness (spatially continuous regions) via a novel regularization scheme. Applied to both text and image classification, MaRC achieves state-of-the-art results in rationale extraction and competitive faithfulness evaluation without requiring specialized training of an explainer model.

## Method Summary
MaRC generates explanations by optimizing a continuous mask λ that blends the original input x with an uninformative baseline b (x̃ = λ·x + (1-λ)·b). The optimization minimizes a loss combining the class probability on the masked input (sufficiency), the complement input (comprehensiveness), and regularization terms for sparsity and compactness. For text, the mask is reparameterized using Gaussian smoothing over token positions; for images, it operates on pixel coordinates. The method is evaluated on movie review rationale extraction and ImageNet faithfulness benchmarks using BERT, ResNet-101, and ViT models.

## Key Results
- Achieves Token F1 of 0.473 and mAP of 0.469 on movie review rationale extraction benchmark
- Achieves sufficiency score of 0.196 and comprehensiveness score of 0.612 on ImageNet classification
- Outperforms state-of-the-art methods in faithfulness evaluation across multiple vision models (ResNet-101, ViT-B/16)

## Why This Works (Mechanism)

### Mechanism 1
Optimizing a continuous mask via gradient descent identifies the minimal input subset required to maintain the original prediction confidence. The method defines a mask λ and blends the original input x with an uninformative baseline b (x̃ = λ·x + (1-λ)·b). It minimizes a loss function that maximizes the class probability L(x̃, c) while simultaneously penalizing the size of the mask via a sparsity regularizer Ω_λ. This creates a pressure to deactivate (mask) any feature that does not actively contribute to the target class score.

### Mechanism 2
Enforcing "comprehensiveness" via a complement mask ensures the rationale captures all relevant evidence, preventing the optimizer from finding a trivial subset. The method defines a complement input x̃_c = (1 - λ)·x + λ·b, representing the features excluded by the rationale. It adds a loss term L(x̃_c, c) to minimize the class probability when looking only at the excluded features. This forces the mask to include any feature that significantly contributes to the class; if such a feature were excluded, the complement loss would fail.

### Mechanism 3
Gaussian reparameterization of the mask enforces spatial compactness, bridging the gap between pixel-level importance and human-interpretable "rationales." Instead of optimizing independent mask values λ_i, the method optimizes parameters w and σ. The influence of a weight w_i on the final mask value λ_j is weighted by a Gaussian kernel based on distance d(i,j). This forces nearby features (e.g., adjacent words or pixels) to share similar mask values, effectively "smoothing" the explanation into connected regions.

## Foundational Learning

- **Concept: Input Optimization vs. Feature Attribution**
  - Why needed here: Standard interpretability (LIME, Gradients) typically analyzes a model at a specific point or via local perturbation. MaRC actively modifies the input to maximize an objective. Understanding this distinction is key to debugging why the optimization might diverge.
  - Quick check question: How does defining a mask λ differ fundamentally from calculating the gradient of the loss with respect to the input x?

- **Concept: The Baseline Problem in Attribution**
  - Why needed here: The method relies on replacing masked features with an "uninformative input" b. If b is not truly neutral (e.g., a black image might imply "darkness" to the classifier), the resulting explanation is biased.
  - Quick check question: Why does the image experiment average results over multiple baselines B (black, white, mean, blur) while the text experiment uses only PAD tokens?

- **Concept: Sufficiency vs. Comprehensiveness**
  - Why needed here: These are the mathematical definitions of a "good" rationale used as the loss function. Without understanding these, one cannot interpret the trade-offs in the regularization weights (α).
  - Quick check question: If a rationale is 100% sufficient but 0% comprehensive, what does that imply about the features it excluded?

## Architecture Onboarding

- **Component map:** Input x + Baseline b -> Mask Generator (w, σ -> Gaussian smoothing -> Sigmoid -> Mask λ) -> Blender (x̃ = λ·x + (1-λ)·b) -> Frozen Model -> Loss Module (L(x̃, c) + L(x̃_c, c) + Ω_λ + Ω_σ)

- **Critical path:**
  1. Initialize mask parameters w (usually small positives) and σ (usually ~1.0-2.0)
  2. Forward pass: Generate mask λ, blend input, pass through frozen model
  3. Backward pass: Update w, σ using Adam (crucial: momentum is required to escape local optima)
  4. Repeat for N steps (e.g., hundreds of iterations)

- **Design tradeoffs:**
  - Speed vs. Precision: This method requires hundreds of forward/backward passes per explanation (approx. 1-3 mins per sample on standard hardware). It is not suitable for real-time inference debugging.
  - Compactness vs. Detail: High regularization (α_λ, α_σ) yields cleaner, more human-readable masks but may miss fine-grained details (e.g., specific individual pixels critical for classification).

- **Failure signatures:**
  - Trivial Solution: Mask λ → 1 everywhere (keeping the whole image). Fix: Increase sparsity regularizer α_λ.
  - Empty Solution: Mask λ → 0 everywhere. Fix: Check if learning rate is too high or if the baseline b is erroneously predictive of the target class.
  - Checkerboard Artifacts: Mask looks noisy. Fix: Increase Gaussian influence (optimize σ) or add noise regularization.

- **First 3 experiments:**
  1. Sanity Check (Text): Run MaRC on a BERT model with a simple sentence containing a clear negation ("This movie was not good"). Verify the rationale includes both "not" and "good".
  2. Baseline Sensitivity (Image): Run MaRC on a single image using different baselines (black vs. blur vs. noise). Observe how the mask shifts.
  3. Ablation Study: Remove the Complement Loss (L(x̃_c, c)) and observe if the rationale becomes smaller but less "comprehensive" (i.e., it might find one key feature and ignore the rest).

## Open Questions the Paper Calls Out

- Can the mask optimization process be accelerated to enable real-time application without sacrificing the continuity of the extracted rationales?
- Does the spatial compactness regularization generalise effectively to inputs with non-spatial or complex multimodal structures, such as raw audio or image-text pairs?
- How sensitive is the rationale quality to the selection of the "uninformative input" b, and can this baseline be standardized across different model architectures?

## Limitations
- The method requires hundreds of forward-backward passes per explanation, making it computationally expensive and impractical for real-time applications
- The quality of explanations depends heavily on the choice of uninformative baseline, which may be model-dependent and not truly neutral
- The Gaussian compactness regularization assumes class-indicative features are spatially continuous, which may not hold for distributed or non-spatial features

## Confidence
- **High Confidence:** Achieves state-of-the-art results on established benchmarks; correctly implements mathematical formulation of sufficiency and comprehensiveness; Gaussian reparameterization produces spatially compact explanations
- **Medium Confidence:** Qualitative superiority of explanations translates to better human interpretability (no user study validation); method is truly "model-agnostic" in practice (optimization may be sensitive to architectures)
- **Low Confidence:** Method discovers the "true" or "ground truth" rationales rather than plausible explanations; baseline averaging procedure produces explanations independent of baseline choice

## Next Checks
1. **Convergence Analysis:** Run the optimization with multiple random seeds and learning rates on the same input. Measure the variance in the resulting rationales (e.g., IoU between different runs) to quantify optimization stability and determine if results are reproducible or depend on initialization.

2. **Baseline Ablation Study:** For a fixed set of images, run MaRC with only one baseline at a time (black only, white only, blur only, mean only). Compare the resulting rationales qualitatively and quantitatively to determine if baseline choice systematically biases which features are selected, and whether certain classes are more sensitive to baseline choice than others.

3. **Human Interpretability Test:** Conduct a small-scale user study where participants are shown pairs of explanations (one from MaRC, one from a baseline method like LIME or Grad-CAM) for the same predictions. Ask participants to rate which explanation better captures the reasoning behind the prediction, or to identify which features they would remove to change the prediction. This would validate whether computational metrics translate to human understanding.