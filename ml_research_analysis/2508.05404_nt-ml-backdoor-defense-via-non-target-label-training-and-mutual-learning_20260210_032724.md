---
ver: rpa2
title: 'NT-ML: Backdoor Defense via Non-target Label Training and Mutual Learning'
arxiv_id: '2508.05404'
source_url: https://arxiv.org/abs/2508.05404
tags:
- backdoor
- attacks
- training
- poisoned
- defense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of deep neural networks
  to backdoor attacks, where triggers embedded in training data cause misclassification.
  The proposed defense, NT-ML, uses a two-step training approach followed by mutual
  learning.
---

# NT-ML: Backdoor Defense via Non-target Label Training and Mutual Learning

## Quick Facts
- **arXiv ID:** 2508.05404
- **Source URL:** https://arxiv.org/abs/2508.05404
- **Reference count:** 40
- **Primary result:** Defends against six advanced backdoor attacks with lowest ASR, even with limited clean data

## Executive Summary
This paper addresses deep neural network vulnerability to backdoor attacks through a novel defense mechanism called NT-ML. The approach uses a two-step training process followed by mutual learning between two models. First, a standard model is trained on poisoned data, then a second model is trained using soft labels from the first model but excluding the target class. Finally, mutual learning between these models purifies the student model through prediction and feature distillation. Experiments demonstrate superior performance against multiple backdoor attacks compared to five state-of-the-art defenses.

## Method Summary
The NT-ML defense works by first training a "Target Training" (TT) model on poisoned data, which retains accuracy on clean data but learns the backdoor. A "Non-target Training" (NT) model is then trained using the TT model's outputs as soft labels, but with the target class logit removed. This forces the NT model to learn non-target class relationships while treating the trigger as noise. Finally, mutual learning between TT and NT models allows feature representation alignment, where the teacher learns from the student's robust features. The method requires only a small clean dataset (as low as 1%) for the mutual learning phase and achieves significant reduction in attack success rates while maintaining benign accuracy.

## Key Results
- NT-ML achieves the lowest attack success rates compared to five state-of-the-art defenses
- Effective against six advanced backdoor attacks including BadNets, WaNet, and LC
- Maintains high benign accuracy even with only 1% clean data available
- Successfully defends on CIFAR-10, CIFAR-100, and GTSRB datasets

## Why This Works (Mechanism)

### Mechanism 1: Target-Decoupled Soft Labeling
Training a student model exclusively on non-target class probabilities decouples the trigger-target association. The method generates soft labels from a poisoned teacher model but removes the target class logit before calculating loss for the NT model. This forces learning of inter-class relationships excluding the target class, treating the trigger as noise rather than a definitive feature. The core assumption is that for poisoned samples, the true class probability is often the second highest after the target class.

### Mechanism 2: Bilateral Feature Purification
Mutual learning allows a robust but low-accuracy student to transfer backdoor resistance to a high-accuracy teacher via feature alignment. The NT model is robust to triggers but loses some clean accuracy, while the TT model is accurate but vulnerable. By updating the teacher to mimic the student's feature representations at intermediate layers, the teacher suppresses neurons that activate solely for the trigger. This forces the teacher to learn purified representations where trigger features are dormant.

### Mechanism 3: Entropy Dispersal of Poisoned Samples
Standard training clusters poisoned samples tightly around the target class, while non-target training disperses these samples back toward their true semantic clusters. By removing the target class from the optimization landscape, gradient descent cannot pull poisoned samples toward the target cluster. Instead, they are pulled toward remaining non-target classes, typically the true class. This dispersal is visualized using t-SNE, showing poisoned images moving from separate clusters back into their true class clusters.

## Foundational Learning

- **Knowledge Distillation (KD)**: Understanding soft labels and temperature is essential for grasping how NT-ML learns non-target class relationships. Quick check: How does using soft labels instead of hard labels change the gradient signal for poisoned images?

- **Backdoor vs. Adversarial Attacks**: The paper focuses on training-time injection rather than test-time attacks. Understanding this distinction is critical for why the defense modifies weights rather than just input data. Quick check: Why does a test-time adversarial patch fail to explain a training-time backdoor trigger like BadNets?

- **Mutual Learning vs. Self-Distillation**: Unlike standard distillation with a static teacher, NT-ML updates both networks. The distinction is vital because the teacher actually gets better during the process by learning from the student. Quick check: If we froze the Teacher weights during mutual learning, would the final Student likely be more or less accurate on clean data?

## Architecture Onboarding

- **Component map**: Data Loader -> TT Network -> Label Mask -> NT Network -> ML Block (NT→TT via KL Div, TT→NT via MSE on features)
- **Critical path**: The generation of "Non-Target" labels. If the masking step (removing target index) is implemented incorrectly, the defense mechanism collapses
- **Design tradeoffs**: High α reduces ASR but drops BA; clean data ratio affects ML stability; requires ~3x compute of standard training
- **Failure signatures**: High ASR/Low BA (wrong non-target correlations); Low ASR/Low BA (over-regularization); Training instability (TT not converged)
- **First 3 experiments**: 1) Verify "Top-2" assumption by checking true class as second-highest prediction; 2) Ablate label type (all classes vs. non-target only); 3) Monitor feature map loss during ML phase for convergence

## Open Questions the Paper Calls Out

- **Can the defense work without external clean datasets?**: The authors state they will investigate how to implement a defense without additional datasets, as the current method relies on a small local clean set to guide mutual learning and purification

- **Can adaptive attacks be designed to circumvent NT-ML?**: An interesting research direction is how to destroy the defense system if the attacker knows distillation will be used, suggesting potential for adaptive attacks that survive the non-target training phase

- **How to reduce computational overhead of NT phase?**: Table III indicates NT step takes approximately twice the runtime of TT step due to label transfer, and the paper acknowledges the cost but does not offer optimization for this mechanism

## Limitations
- Performance depends on the "Top-2 assumption" that true class appears as second-highest probability in poisoned samples, which lacks direct validation
- Mutual learning effectiveness heavily depends on clean data ratio, with stability unproven for extremely low ratios (<1%)
- Computational overhead is significant, requiring ~3x the compute of standard training due to sequential and joint training phases

## Confidence
- **High confidence**: General architecture and training pipeline (Target Training → Non-Target Training → Mutual Learning) is clearly specified and reproducible
- **Medium confidence**: Effectiveness against the six specific backdoor attacks tested on CIFAR-10/100 and GTSRB
- **Low confidence**: Generalization to other datasets (ImageNet, medical imaging), attack types (semantic backdoors, reflection backdoors), and extremely low clean data ratios (<1%)

## Next Checks
1. **Top-2 Assumption Verification**: Train a standard model on BadNets-poisoned data and measure frequency of true class as second-highest prediction among non-target classes
2. **Clean Data Ratio Sensitivity**: Systematically vary clean data ratio (1%, 5%, 10%, 20%) and measure corresponding Attack Success Rate and Benign Accuracy
3. **Feature Map Alignment Analysis**: During mutual learning, visualize and compare feature representations of teacher and student models before and after ML phase to verify backdoor removal