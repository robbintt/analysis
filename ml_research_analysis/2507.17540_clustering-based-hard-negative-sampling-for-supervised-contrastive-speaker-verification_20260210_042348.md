---
ver: rpa2
title: Clustering-based hard negative sampling for supervised contrastive speaker
  verification
arxiv_id: '2507.17540'
source_url: https://arxiv.org/abs/2507.17540
tags:
- speaker
- hard
- contrastive
- training
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CHNS (clustering-based hard negative sampling),\
  \ a method for improving supervised contrastive speaker verification by clustering\
  \ similar speakers and optimizing batch composition. The approach uses K-Means clustering\
  \ on speaker embeddings (voiceprints) to identify groups of similar speakers, then\
  \ adjusts batch sampling to include a higher ratio of hard negative pairs\u2014\
  utterances from different speakers within the same cluster."
---

# Clustering-based hard negative sampling for supervised contrastive speaker verification

## Quick Facts
- arXiv ID: 2507.17540
- Source URL: https://arxiv.org/abs/2507.17540
- Authors: Piotr Masztalski; Michał Romaniuk; Jakub Żak; Mateusz Matuszewski; Konrad Kowalczyk
- Reference count: 0
- Key outcome: CHNS improves speaker verification EER/minDCF by up to 18% over SupCon on VoxCeleb1-H

## Executive Summary
This paper introduces CHNS (clustering-based hard negative sampling), a method that improves supervised contrastive speaker verification by clustering similar speakers and optimizing batch composition. The approach uses K-Means clustering on speaker embeddings to identify groups of similar speakers, then adjusts batch sampling to include a higher ratio of hard negative pairs—utterances from different speakers within the same cluster. Experiments show CHNS outperforms standard supervised contrastive learning (SupCon), loss-based hard negative sampling (H-SCL), and a classification-based AAMSoftmax approach by up to 18% relative improvement in EER and minDCF on VoxCeleb1-H, using lightweight model architectures suitable for edge deployment.

## Method Summary
CHNS leverages speaker clustering to identify groups of similar speakers, then modifies the batch composition to include more hard negative pairs—utterances from different speakers within the same cluster. The method uses K-Means clustering on pre-computed speaker embeddings (voiceprints) to group speakers by acoustic similarity. During training, the batch is constructed with a higher proportion of hard negatives by selecting utterances from speakers within the same cluster. The approach is dataset and model agnostic, with additional performance gains when combined with H-SCL.

## Key Results
- CHNS achieves up to 18% relative improvement in EER and minDCF over SupCon on VoxCeleb1-H
- Outperforms H-SCL and AAMSoftmax approaches across all evaluation metrics
- Compatible with lightweight models suitable for edge deployment
- Performance gains are dataset and model agnostic

## Why This Works (Mechanism)
The method works by identifying speaker clusters that contain acoustically similar speakers, then increasing the proportion of hard negative pairs in training batches. By focusing on negative pairs that are more difficult to distinguish (speakers within the same cluster), the model learns more discriminative features. The K-Means clustering is chosen specifically because it minimizes squared Euclidean distance, aligning with the contrastive loss objective.

## Foundational Learning
- **K-Means Clustering**: Algorithm for partitioning data into k clusters based on distance to centroids. Needed to group similar speakers for hard negative identification. Quick check: Verify clustering preserves speaker identities in separate clusters.
- **Speaker Embeddings (Voiceprints)**: Fixed-dimensional representations of speaker characteristics. Needed as input for clustering and contrastive learning. Quick check: Ensure embeddings capture speaker-specific information.
- **Supervised Contrastive Learning**: Framework for learning representations using labeled data. Needed as the base training objective. Quick check: Verify contrastive loss is correctly implemented.
- **Hard Negative Mining**: Technique to select challenging negative examples. Needed to improve model discrimination. Quick check: Confirm hard negatives are truly difficult to distinguish.
- **Batch Composition**: Strategy for selecting samples in each training batch. Needed to control the ratio of hard negatives. Quick check: Validate batch construction meets target ratios.
- **Evaluation Metrics (EER/minDCF)**: Measures of speaker verification performance. Needed to quantify improvements. Quick check: Ensure consistent metric calculation across experiments.

## Architecture Onboarding

Component Map: Speaker Embeddings -> K-Means Clustering -> Batch Construction -> Contrastive Loss -> Model

Critical Path: The critical path involves pre-computing speaker embeddings, clustering them to identify speaker groups, constructing batches with appropriate hard negative ratios, and training with supervised contrastive loss. The clustering step is performed once before training begins.

Design Tradeoffs: The method trades increased pre-processing (clustering) for improved training efficiency and performance. K-Means is chosen for its computational efficiency and alignment with squared Euclidean distance, but other clustering methods might capture different aspects of speaker similarity.

Failure Signatures: Performance degradation occurs when clusters are too large (too many easy negatives) or too small (insufficient hard negatives). The method also fails if pre-computed embeddings are poor quality or if the clustering algorithm doesn't capture meaningful speaker similarities.

First Experiments:
1. Verify K-Means clustering produces meaningful speaker groups by visualizing cluster assignments
2. Test batch construction by confirming hard negative ratios match target proportions
3. Compare baseline SupCon performance with and without CHNS to isolate the contribution of clustering

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the requirement for a pre-trained model to generate initial voiceprint clusters be eliminated without performance loss?
- Basis in paper: The best results required pre-computed clusters ("From start"), while curriculum learning approaches that calculate clusters during training showed degraded performance (Table 3).
- Why unresolved: The paper explores curriculum learning to avoid the pre-training dependency but does not achieve parity with the pre-computed baseline.
- What evidence would resolve it: An experiment showing an online clustering initialization method achieving EER/minDCF statistically indistinguishable from the "From start" baseline.

### Open Question 2
- Question: Is the performance of CHNS sensitive to the specific choice of clustering algorithm?
- Basis in paper: The authors select K-Means specifically for its relationship to squared Euclidean distance, leaving other algorithms unexplored.
- Why unresolved: Alternative methods like spectral clustering or DBSCAN might capture speaker similarities differently than centroid-based K-Means.
- What evidence would resolve it: Comparative evaluation of CHNS using density-based or hierarchical clustering methods on VoxCeleb1-H.

### Open Question 3
- Question: Can the optimal number of clusters and hard ratio be determined adaptively for a given dataset without manual grid search?
- Basis in paper: The authors state, "tuning these parameters to the training dataset is instrumental in obtaining optimal results."
- Why unresolved: The paper relies on grid search (Table 1) rather than proposing a heuristic or adaptive mechanism.
- What evidence would resolve it: A proposed heuristic for K and hard ratio that matches or exceeds the performance of the best manual configuration.

## Limitations
- Relies on K-Means with fixed cluster count (k=300), with sensitivity to this hyperparameter unexplored
- Evaluation limited to English-centric VoxCeleb datasets, leaving multilingual generalization unclear
- Analysis of which speaker types or acoustic conditions benefit most from CHNS is not provided

## Confidence
- **High confidence**: Performance improvements over standard SupCon and AAMSoftmax baselines on VoxCeleb1-H; model agnosticism claim supported by experiments across two architectures
- **Medium confidence**: Relative improvements compared to H-SCL (performance varies by metric); claims about computational efficiency benefits are plausible but not directly measured
- **Low confidence**: Generalization to non-VoxCeleb datasets and multilingual settings; optimal clustering parameters for different corpus sizes

## Next Checks
1. Test CHNS performance on non-English speaker verification datasets (e.g., CN-Celeb, SITW) to verify dataset agnosticism beyond VoxCeleb
2. Conduct sensitivity analysis for the number of clusters (k) across different dataset sizes to establish optimal clustering parameters
3. Perform ablation studies isolating the contribution of batch composition changes versus improved negative mining to understand the primary driver of performance gains