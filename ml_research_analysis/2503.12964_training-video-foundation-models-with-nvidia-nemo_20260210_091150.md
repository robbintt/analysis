---
ver: rpa2
title: Training Video Foundation Models with NVIDIA NeMo
arxiv_id: '2503.12964'
source_url: https://arxiv.org/abs/2503.12964
tags:
- training
- video
- diffusion
- nemo
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a scalable, open-source training pipeline for
  Video Foundation Models (VFMs) using NVIDIA NeMo. The framework addresses the challenges
  of training large-scale VFMs by providing accelerated video dataset curation with
  NeMo Curator, efficient multimodal data loading via Megatron Energon, and parallelized
  video diffusion model training leveraging Megatron Core.
---

# Training Video Foundation Models with NVIDIA NeMo

## Quick Facts
- arXiv ID: 2503.12964
- Source URL: https://arxiv.org/abs/2503.12964
- Authors: Zeeshan Patel; Ethan He; Parth Mannan; Xiaowei Ren; Ryan Wolf; Niket Agarwal; Jacob Huffman; Zhuoyao Wang; Carl Wang; Jack Chang; Yan Bai; Tommy Huang; Linnan Wang; Sahil Jain; Shanmugam Ramasamy; Joseph Jennings; Ekaterina Sirazitdinova; Oleg Sudakov; Mingyuan Ma; Bobby Chen; Forrest Lin; Hao Wang; Vasanth Rao Naik Sabavat; Sriharsha Niverty; Rong Ou; Pallab Bhattacharya; David Page; Nima Tajbakhsh; Ashwath Aithal
- Reference count: 37
- Primary result: Scalable VFM training framework achieving 48.2% MFU and 95%+ scaling efficiency

## Executive Summary
This paper presents a comprehensive, open-source training pipeline for Video Foundation Models using NVIDIA NeMo. The framework addresses the computational challenges of training large-scale VFMs through accelerated data preprocessing with NeMo Curator, efficient multimodal data loading via Megatron Energon, and parallelized video diffusion model training leveraging Megatron Core. The system demonstrates significant performance improvements over existing approaches, achieving up to 48.2% Model FLOPs Utilization and near-linear scaling from 8 to 32 GPU nodes.

## Method Summary
The training pipeline consists of three main components: NeMo Curator for accelerated video dataset preparation using hardware-accelerated NVDEC/NVENC decoding and encoding (3x speedup), Megatron Energon for efficient multimodal data loading with sequence packing and shard-level all-gather distribution, and Megatron Core for parallelized DiT training using 4D parallelism (tensor, context, pipeline, and data parallelism). The framework supports both autoregressive and diffusion-based VFMs, with specific optimizations including AdaLN-LoRA architecture modifications that improve compute performance by up to 1.2x.

## Key Results
- Achieved up to 48.2% Model FLOPs Utilization (MFU) across various model configurations
- Demonstrated near-linear scaling efficiency exceeding 95% when scaling from 8 to 32 nodes of 8xH100 GPUs
- Introduced hybrid parallelism approach for Spatial-Temporal DiT models achieving up to 40% MFU
- Enabled fast video generation inference with near-linear scaling up to 32 H100 GPUs
- Implemented algorithm-system co-design optimizations including AdaLN-LoRA architecture modifications improving compute performance by up to 1.2x

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Auto-balanced curation with GPU hardware acceleration enables petabyte-scale video preprocessing.
- Mechanism: NVDEC/NVENC hardware video decode/encode provides 3x speedup; Ray-based auto-balancing dynamically allocates workers to rate-limiting stages (e.g., VLM captioning vs. smaller embedding models) to prevent pipeline bottlenecks.
- Core assumption: Curation stages have heterogeneous throughput, and imbalance creates idle GPU cycles without dynamic rebalancing.
- Evidence anchors:
  - [section 2.3]: "utilizing the hardware video decoder (NVDEC) and hardware video encoder (NVENC) on NVIDIA GPUs brought a 3x speedup in the decoding and transcoding stages"
  - [section 2.3]: "auto-balancing system... will deploy an optimal number of workers for each stage"
  - [corpus]: No directly comparable corpus evidence on video curation at this scale.

### Mechanism 2
- Claim: Sequence packing enables efficient mixed image-video training by eliminating padding waste.
- Mechanism: Concatenates variable-length samples along the sequence dimension, allowing images and videos with different resolutions/lengths in the same micro-batch without SBHD-style length grouping. Though micro-batch size is limited to 1, token-level throughput increases.
- Core assumption: Padding tokens waste compute, and varied-length sequences are common in VFM training data.
- Evidence anchors:
  - [section 3.1]: "Sequence packing is a technique that concatenates multiple training samples along the sequence dimension. This method eliminates the need for excessive padding."
  - [section 3.1]: "allows packing images and videos with varied length and resolution in the same micro batch"
  - [corpus]: No direct corpus evidence on sequence packing for VFMs.

### Mechanism 3
- Claim: 4D parallelism with algorithm-system co-design achieves up to 48.2% MFU and 1.85x speedup over Fast-DiT.
- Mechanism: AdaLN-LoRA reduces parameter density in Adaptive LayerNorm (from [Hidden Dim × 9×Hidden Dim] to low-rank decomposition), improving compute-communication ratios. Different parallelism strategies are optimal for different configurations: FSDP for small models/short contexts; CP for long sequences; TP+PP for large models.
- Core assumption: No single parallelism strategy is optimal across all model sizes and context lengths.
- Evidence anchors:
  - [section 4.5.2]: "our training framework can achieve up to 48.2% MFU" and "outperforms Fast-DiT by up to 1.85x"
  - [section 4.5.2]: "the best performing parallelism strategy is unique at each context length"
  - [corpus]: Related work (AVA-Bench, SpaRRTa) discusses VFM evaluation but not training parallelism directly.

## Foundational Learning

- **Concept: Diffusion models (forward/reverse process, denoising score matching)**
  - Why needed here: The entire training pipeline trains a neural network to predict noise added to video latents; sampling iteratively denoises from random noise.
  - Quick check question: Why does the training objective match predicted noise εθ(zt; t, y) against ground-truth noise εt rather than directly predicting clean data?

- **Concept: Transformer parallelism strategies (tensor, pipeline, context/sequence parallel)**
  - Why needed here: The paper distributes a single DiT model across thousands of GPUs using different sharding strategies with distinct tradeoffs.
  - Quick check question: What dimension does tensor parallelism shard versus context parallelism, and which has higher communication frequency?

- **Concept: Video tokenization (3D causal tokenizers, spatiotemporal patches)**
  - Why needed here: Raw videos are compressed into discrete latent tokens (8x spatial compression) before DiT training, reducing dataset size by >100x.
  - Quick check question: How does a 3D causal tokenizer differ from a 2D image tokenizer in terms of temporal handling?

## Architecture Onboarding

- **Component map:**
  - NeMo Curator -> WebDataset shards (clipping -> annotation -> sharding)
  - Megatron Energon -> DataLoader (blending, sequence packing, optimized all-gather network usage)
  - Video Tokenizer -> 3D spatiotemporal latent tokens
  - Diffusion Transformer (DiT) -> AdaLN/AdaLN-LoRA + cross-attention for text conditioning
  - Megatron Core -> Parallelism runtime (TP, FSDP, CP, PP)

- **Critical path:**
  1. Raw videos -> NeMo Curator (clip, annotate with VLM captions, shard) -> WebDataset tar files
  2. WebDataset -> Megatron Energon (load, blend, pack sequences)
  3. Packed sequences -> Video Tokenizer -> 3D latent tokens
  4. Latent tokens + Gaussian noise -> DiT training with EDM loss
  5. Trained DiT -> CP-parallelized inference with FP8 MHA

- **Design tradeoffs:**
  - AdaLN vs AdaLN-LoRA: LoRA reduces AdaLN parameters but adds decomposition overhead; up to 1.2x speedup observed.
  - FSDP alone vs FSDP+TP: FSDP simpler; TP+FSFD better for large models with slow inter-node bandwidth.
  - Recompute vs communicate conditioning in PP: Recomputing embeddings at each stage is more efficient than increasing PP communication buffers.
  - CP for inference: 80-90% scaling efficiency up to 32 GPUs; FP8 MHA adds ~28-48% speedup over BF16.

- **Failure signatures:**
  - OOM on long sequences -> Increase CP degree or reduce micro-batch size.
  - Low MFU with TP on small models -> GEMMs too small; switch to FSDP.
  - CP overhead exposed on short sequences -> Disable CP for contexts <8k tokens.
  - Pipeline bubble too large -> Enable interleaved pipelining (VPP).
  - Dataloader bottleneck -> Enable shard-level all-gather to reduce redundant cloud downloads.

- **First 3 experiments:**
  1. Benchmark FSDP-only training on DiT-7B at 8k context to establish baseline MFU.
  2. Enable CP and measure MFU change at 8k vs 74k context lengths to validate when CP helps vs. hurts.
  3. Compare AdaLN vs AdaLN-LoRA on identical DiT-7B configuration to quantify parameter reduction and throughput gain.

## Open Questions the Paper Calls Out
- **Open Question 1:** How do model quantization, classifier-free guidance (CFG) parallelism, and model distillation compare to the current context-parallel approach for optimizing VFM inference?
  - Basis in paper: [explicit] The authors state, "There are several other strategies that can also improve inference performance... We leave these explorations to future works" (p. 14).
  - Why unresolved: The paper benchmarks context parallelism (CP) and FP8 attention but does not integrate or test the performance of distillation or quantization strategies within the NeMo framework.
  - What evidence would resolve it: Comparative benchmarks showing training throughput and inference latency when these specific optimization techniques are applied to the Cosmos-1.0-Diffusion model.

- **Open Question 2:** What is the precise trade-off between video tokenizer compression rates (beyond 8x spatial) and the generation quality of the downstream Video Foundation Model?
  - Basis in paper: [inferred] The paper notes users can increase compression rates to "balance generation quality against available computational resources" (p. 6), but provides no data on the quality degradation curve.
  - Why unresolved: While the framework supports customizable tokenizers, the specific point at which higher compression (e.g., 16x, 32x) compromises temporal coherence or fidelity is not quantified.
  - What evidence would resolve it: An ablation study measuring FVD/IVD scores of generated videos from models trained with tokenizers of varying spatial compression factors.

- **Open Question 3:** Can a predictive cost model automate the selection of the optimal 4D parallelism configuration (TP, CP, PP, FSDP) for varying model sizes and sequence lengths?
  - Basis in paper: [inferred] The authors observe that "each workload requires a unique combination of parallelism strategies" and provide "rules of thumb" (p. 11), indicating a lack of automated optimization.
  - Why unresolved: Current best practices rely on manual empirical tuning (Fig. 10) rather than a theoretical framework to determine the most efficient parallelism strategy for a given hardware setup.
  - What evidence would resolve it: The development of an auto-tuning algorithm that consistently selects configurations matching the empirical throughput peaks identified in the paper.

## Limitations
- The exact training hyperparameters (learning rate schedules, optimizer settings, batch size configurations) are not provided, making exact reproduction difficult.
- Specific video datasets used for benchmarking are not named, and preprocessing parameters such as clipping thresholds and VLM captioning details are missing.
- The 3D causal video tokenizer architecture and pretrained weights are referenced but not fully detailed, requiring additional implementation work.

## Confidence
- **High Confidence:** Hardware-accelerated video preprocessing (3x speedup from NVDEC/NVENC) - well-established GPU capabilities with direct textual support.
- **Medium Confidence:** Sequence packing efficiency and algorithm-system co-design improvements (48.2% MFU, 1.85x speedup) - well-supported mechanistically but comparison baseline and measurement conditions not fully specified.
- **Low Confidence:** General principle that no single parallelism strategy is optimal across all model sizes and context lengths - presented as general principle but lacks systematic experimental coverage across full design space.

## Next Checks
1. Reproduce the 3x speedup claim: Implement the NeMo Curator pipeline with and without NVDEC/NVENC hardware acceleration on a representative video dataset (minimum 100GB) and measure decoding/transcoding throughput. Compare against CPU-based ffmpeg preprocessing to validate the claimed acceleration factor.

2. Validate sequence packing efficiency: Train identical DiT models on the same blended image-video dataset using both sequence packing and traditional length-grouped approaches. Measure micro-batch token throughput and GPU utilization to quantify the elimination of padding waste across varying sequence lengths.

3. Benchmark parallelism strategy selection: Systematically evaluate FSDP, CP, TP, and their combinations across the full range of model sizes (1B to 28B parameters) and context lengths (8k to 74k tokens). Measure MFU for each configuration to empirically determine the exact transition points where each strategy becomes optimal, validating the claim that strategy selection depends on both model size and sequence length.