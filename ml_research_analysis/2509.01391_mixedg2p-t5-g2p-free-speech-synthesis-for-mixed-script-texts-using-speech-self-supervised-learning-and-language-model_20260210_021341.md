---
ver: rpa2
title: 'MixedG2P-T5: G2P-free Speech Synthesis for Mixed-script texts using Speech
  Self-Supervised Learning and Language Model'
arxiv_id: '2509.01391'
source_url: https://arxiv.org/abs/2509.01391
tags:
- speech
- predictor
- synthesis
- text
- pseudo-language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a G2P-free speech synthesis approach that
  directly generates discrete speech tokens from text using a T5 encoder and speech
  self-supervised learning. The method bypasses traditional grapheme-to-phoneme conversion,
  enabling natural synthesis for mixed-script languages like Japanese.
---

# MixedG2P-T5: G2P-free Speech Synthesis for Mixed-script texts using Speech Self-Supervised Learning and Language Model

## Quick Facts
- arXiv ID: 2509.01391
- Source URL: https://arxiv.org/abs/2509.01391
- Reference count: 28
- This study introduces a G2P-free speech synthesis approach that directly generates discrete speech tokens from text using a T5 encoder and speech self-supervised learning

## Executive Summary
This paper presents MixedG2P-T5, a novel approach to speech synthesis that eliminates the need for traditional grapheme-to-phoneme conversion (G2P) by using discrete tokens derived from speech self-supervised learning. The method employs a T5 encoder to predict pseudo-language labels from mixed-script Japanese text, which are then converted to speech using a FastSpeech 2-based spectral predictor. By bypassing G2P dictionaries and phonetic transcription, the approach reduces costs and enhances scalability for mixed-script languages like Japanese. Evaluation on the JVS corpus demonstrates competitive performance with baseline G2P systems while supporting multilingual synthesis.

## Method Summary
The MixedG2P-T5 method consists of three main components: an SSL-based discrete token extractor, a T5-based pseudo-language label predictor, and a FastSpeech 2 spectral predictor. First, a pre-trained speech SSL model (ContentVec) is fine-tuned on Japanese speech to generate continuous features, which are then quantized into discrete tokens using k-means clustering (k=500). A T5-base encoder-decoder is trained to map mixed-script Japanese text to these discrete token sequences. The spectral predictor (FastSpeech 2) generates Mel-spectrograms from the predicted tokens, with repeated symbols removed as implicit duration cues. The system is evaluated on a held-out JVS test set using character error rate (CER), utterance-level mean opinion score (UTMOS), and acoustic quality metrics.

## Key Results
- Unit Error Rate (UER) of 7.47% achieved by the label predictor
- Character Error Rate (CER) of 21.28% competitive with baseline G2P system (20.63%)
- UTMOS score of 2.54 and WARP-Q score of 2.63 indicating acceptable naturalness
- The method successfully bypasses traditional G2P conversion while maintaining synthesis quality

## Why This Works (Mechanism)

### Mechanism 1
Speech self-supervised learning (SSL) models encode both linguistic content and paralinguistic features into discrete tokens that function as phoneme substitutes. SSL models trained on unlabeled speech compress continuous audio into symbolic representations via k-means clustering. These discrete tokens capture accent, intonation, and prosody alongside linguistic content—information typically lost in G2P conversion. The discrete token vocabulary (500 units) must be sufficient to represent the phonological and prosodic diversity of the target language.

### Mechanism 2
A T5 encoder-decoder learns the mapping from mixed-script text (Kanji + Kana) to discrete speech tokens without explicit phoneme annotations. T5's pre-training on massive multilingual text provides broad linguistic understanding. Fine-tuning on text–label pairs (where labels are discrete tokens extracted from aligned speech) enables the model to infer pronunciation patterns directly from orthography, bypassing G2P dictionaries. The text–label alignment must preserve sufficient regularity for the model to generalize pronunciation rules from Kanji/Kana patterns.

### Mechanism 3
Removing repeated symbols from pseudo-language label sequences provides implicit duration cues for the spectral predictor. When discrete tokens are extracted from speech, repeated consecutive tokens encode duration information. The preprocessing step that removes repetitions creates a compact label sequence, but the original repetition patterns can be leveraged by FastSpeech 2's duration modeling. Duration information derivable from token repetitions must be sufficient for natural prosody without explicit duration annotations.

## Foundational Learning

- **Speech Self-Supervised Learning (SSL)**: Understanding how wav2vec 2.0, HuBERT, and ContentVec learn representations from unlabeled audio explains why discrete tokens can replace phonemes. Quick check: Can you explain the difference between contrastive learning (wav2vec 2.0) and masked prediction (HuBERT) for speech representation?

- **Vector Quantization via k-means**: The discrete token vocabulary is created by clustering SSL features; understanding k-means hyperparameters (k=500 in this paper) is critical for token quality. Quick check: What happens to token granularity if k is too small vs. too large for a given language's phoneme inventory?

- **Sequence-to-Sequence Alignment (CTC/Attention)**: The T5 predictor must handle variable-length mappings between text characters and discrete token sequences; understanding alignment mechanisms helps diagnose prediction errors. Quick check: How does T5's encoder-decoder attention differ from CTC-based alignment in traditional G2P models?

## Architecture Onboarding

- **Component map**: Raw audio → ContentVec SSL → k-means quantization (500 clusters) → discrete tokens → T5 encoder-decoder → text → discrete tokens → FastSpeech 2 → Mel-spectrogram → vocoder → waveform

- **Critical path**: Text → T5 tokenizer (tohoku-BERT-v3) → T5 encoder-decoder → discrete tokens → FastSpeech 2 → vocoder → audio

- **Design tradeoffs**:
  - Token vocabulary size (k=500): Larger k captures finer distinctions but increases prediction difficulty; smaller k loses phonemic detail
  - Repeated symbol handling: Removing repetitions simplifies prediction but loses explicit duration; keeping them increases sequence length
  - Tokenizer choice: Japanese-specific tokenizer (current) vs. language-agnostic BPE/byte-level (future) for multilingual support

- **Failure signatures**:
  - High UER (>15%) with low CER: Label predictor errors compensated by spectral predictor—investigate token vocabulary coverage
  - Low UER but high CER: Spectral predictor is the bottleneck—check FastSpeech 2 training data or vocoder quality
  - UTMOS significantly below baseline (>0.3 gap): Naturalness suffering from missing prosodic features—consider adding explicit accent/duration inputs

- **First 3 experiments**:
  1. **Token vocabulary ablation**: Train k-means with k ∈ {100, 250, 500, 1000} and measure UER/CER tradeoffs to find optimal granularity for Japanese phoneme coverage
  2. **Tokenizer swap test**: Replace tohoku-BERT-v3 tokenizer with mT5 or ByT5 tokenizer to quantify language-specific preprocessing impact on multilingual transferability
  3. **Spectral predictor isolation**: Feed oracle discrete tokens (from SSL on ground-truth speech) directly to FastSpeech 2 to establish upper bound and separate label predictor errors from spectral predictor errors

## Open Questions the Paper Calls Out

- **Can language-agnostic tokenizers such as mT5 or ByT5 effectively replace language-specific tokenizers in the pseudo-language label predictor to support scalable multilingual synthesis?**
The authors intend to explore strategies that avoid language-specific preprocessing by converting raw text into language-aware discrete tokens using tokenizers such as mT5 or ByT5.

- **To what extent does the proposed label predictor exhibit language dependency, and can it perform effectively on multilingual datasets without language-specific fine-tuning?**
The authors plan to conduct experiments on multilingual datasets to investigate the language dependency of the language label predictor, as current experiments were confined to the Japanese JVS corpus.

- **Does explicitly integrating accentual information as an additional acoustic feature into the spectral predictor improve the prosodic accuracy of the synthesized speech?**
The paper lists integrating additional acoustic features such as accentual information alongside the four currently used features as an area for improvement in the spectral predictor.

## Limitations
- Missing implementation details for critical components including T5 fine-tuning hyperparameters, ContentVec fine-tuning procedure, and FastSpeech 2 configuration
- Experimental scope limited to single Japanese corpus without multilingual validation or ablation studies
- No error analysis on specific types of pronunciation errors or comparison with alternative SSL models

## Confidence
- **High confidence**: Core technical claim that discrete token-based speech synthesis can bypass traditional G2P conversion while maintaining competitive intelligibility and acoustic quality
- **Medium confidence**: Scalability claim due to limited multilingual validation despite architecture supporting mixed-script languages
- **Low confidence**: Exact reproduction of results due to missing implementation details for T5 fine-tuning, ContentVec adaptation, and FastSpeech 2 configuration

## Next Checks
1. **Token vocabulary size ablation study**: Train k-means clustering with k ∈ {100, 250, 500, 1000} on SSL features and measure the tradeoff between Unit Error Rate and Character Error Rate to establish optimal granularity for Japanese phoneme coverage.

2. **Tokenizer impact evaluation**: Replace the Japanese-specific tohoku-BERTv3 tokenizer with a multilingual tokenizer (mT5 or ByT5) and measure changes in UER and CER to quantify language-specific preprocessing impact on multilingual transferability.

3. **Label predictor error isolation**: Generate synthetic speech using oracle discrete tokens (extracted directly from ground-truth speech via SSL) instead of predicted tokens, keeping all other components unchanged, to identify whether errors stem from the T5 predictor or the spectral predictor.