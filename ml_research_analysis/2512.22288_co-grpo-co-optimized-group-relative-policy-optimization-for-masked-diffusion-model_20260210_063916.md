---
ver: rpa2
title: 'Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion
  Model'
arxiv_id: '2512.22288'
source_url: https://arxiv.org/abs/2512.22288
tags:
- arxiv
- policy
- co-grpo
- generation
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a fundamental mismatch in Masked Diffusion
  Models (MDMs) where the training objective (BERT-style single-step prediction) does
  not align with the inference process (multi-step iterative generation governed by
  schedules). To address this, the authors introduce Co-GRPO, which reformulates MDM
  generation as a unified Markov Decision Process that jointly optimizes both the
  model parameters and the inference schedule.
---

# Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model

## Quick Facts
- arXiv ID: 2512.22288
- Source URL: https://arxiv.org/abs/2512.22288
- Reference count: 28
- Improves ImageReward score from 0.942 to 1.122 on Meissonic MDM

## Executive Summary
This paper identifies a fundamental mismatch in Masked Diffusion Models (MDMs) where the training objective (BERT-style single-step prediction) does not align with the inference process (multi-step iterative generation governed by schedules). To address this, the authors introduce Co-GRPO, which reformulates MDM generation as a unified Markov Decision Process that jointly optimizes both the model parameters and the inference schedule. By treating model and schedule as cooperating policies within a single GRPO framework, Co-GRPO cooperatively optimizes them under a shared reward without requiring backpropagation through multi-step generation. Empirical results demonstrate substantial improvements: Co-GRPO increases ImageReward score from 0.942 to 1.122 and HPSv2 score from 28.83 to 29.37, while also showing strong zero-shot generalization on GenEval (0.47→0.55) and DPG-Bench (64.57→70.10) benchmarks.

## Method Summary
Co-GRPO reformulates MDM generation as a unified MDP where the policy factorizes into a model policy π_model and a scheduling policy π_schedule. The joint policy p(a_t|s_t) is parameterized by both the MDM θ and the scheduling policy ϕ, with actions a_t including both token predictions V^(t+1) and schedule parameters A^(t+1) (sampling temperature τ_s, re-mask ratio r, re-mask temperature τ_r, CFG scale s). The scheduling policy is modeled as a multivariate Gaussian with mean predicted by a small network. Alternating optimization alternates between N_m model updates and N_s schedule updates to prevent instability from parameter asymmetry (1B vs 9M parameters). The method uses trajectory-based rewards from composite ImageReward+HPSv2 scores and applies GRPO's clipped surrogate objective with separated probability ratios.

## Key Results
- Co-GRPO increases ImageReward score from 0.942 to 1.122 on Meissonic MDM
- HPSv2 score improves from 28.83 to 29.37
- Strong zero-shot generalization on GenEval (0.47→0.55) and DPG-Bench (64.57→70.10)
- Schedule sensitivity ablation shows ImageReward ranges from 0.660 to 0.918 when varying only schedule exponent γ

## Why This Works (Mechanism)

### Mechanism 1
Treating inference schedule as trainable policy actions (not fixed hyperparameters) aligns training with the trajectory-level nature of MDM inference. The unified MDP expands the action space from a_t ≜ V^(t+1) to a_t ≜ (V^(t+1), A^(t+1)), where A includes sampling temperature τ_s, re-mask ratio r, re-mask temperature τ_r, and CFG scale s. This exposes schedule decisions to reward feedback. Core assumption: The inference schedule has a learnable structure that correlates with generation quality, which Table 2 supports by showing ImageReward swings from 0.660 to 0.918 when varying only the schedule exponent γ.

### Mechanism 2
Factorizing the joint policy into π_model · π_schedule enables gradient flow to both components without backpropagation through multi-step generation. By modeling π_schedule,t as a multivariate Gaussian with mean predicted by a small network, the probability ratio r_t(θ,ϕ) decomposes into independent terms. This allows REINFORCE-style updates using only trajectory-level rewards. Core assumption: The independence approximation in Equation 8 (treating decoded tokens as independent probabilities) is sufficiently accurate for GRPO advantage estimation.

### Mechanism 3
Alternating optimization between model and schedule prevents training instability caused by parameter asymmetry. The model has ~1B parameters vs. ~9M for the scheduling network, yet schedule changes have outsized impact on quality. Simultaneous updates cause the small network to dominate gradient signal. Alternating N_m model updates then N_s schedule updates stabilizes convergence. Core assumption: The separability in Equation 17 holds sufficiently that each component can be optimized with the other held fixed.

## Foundational Learning

**Concept: Masked Diffusion Models (MDMs)**
- Why needed here: Understanding the sampling/remask loop (Equations 2-5) is prerequisite to grasping why schedules matter.
- Quick check question: Can you explain why MDMs use confidence-based remasking rather than keeping all decoded tokens?

**Concept: Group Relative Policy Optimization (GRPO)**
- Why needed here: Co-GRPO extends GRPO's clipped surrogate objective (Equation 9) to joint policies; understanding advantage normalization is essential.
- Quick check question: Why does GRPO use group-level advantage normalization instead of per-sample baselines?

**Concept: Policy factorization in RL**
- Why needed here: The key insight is that p(a_t|s_t) = π_model · π_schedule enables independent optimization.
- Quick check question: When does factorizing a joint policy introduce optimization bias?

## Architecture Onboarding

**Component map:**
Prompt → frozen CLIP encoder → MDM transformer → scheduling network predicts A^(t+1) → sampling step → remask step → repeat T=48 steps → reward model scores final image → GRPO updates

**Critical path:**
Text prompt → CLIP-ViT-H-14 (frozen) → Meissonic MDM transformer → scheduling network (depthwise conv → pointwise conv → MLP) → sampling parameters (τ_s, r, τ_r, CFG scale) → iterative generation loop (T=48 steps) → composite reward (ImageReward + HPSv2) → GRPO updates

**Design tradeoffs:**
- Adding more trainable schedule components (Table 5a: r, τ_r, τ_s, s) improves scores but increases optimization complexity
- Higher inference steps (48 vs 16) improve quality but learned schedules transfer better to fewer steps (Table 5b: Δ0.741 at 8 steps vs Δ0.158 at 64 steps)
- Removing KL regularization (β=0) may risk reward hacking but improves optimization freedom

**Failure signatures:**
- Joint optimization converging to lower scores than alternating (Table 5d) indicates schedule dominating gradient signal
- Performance dropping at test-time step counts different from training suggests schedule overfitting
- Invalid schedule parameter values indicate insufficient output constraints

**First 3 experiments:**
1. Baseline replication: Train Naive GRPO (fixed schedule) on Meissonic with same reward setup to verify 0.942 ImageReward baseline.
2. Schedule sensitivity ablation: Vary only one schedule parameter (e.g., re-mask ratio γ exponent) to reproduce Table 2's sensitivity signal before implementing full Co-GRPO.
3. Alternating vs joint comparison: With identical total iterations, compare convergence curves for alternating (N_m model steps, N_s schedule steps) vs simultaneous updates to verify Table 5d.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology raises several important unanswered questions about schedule generalization, approximation quality, and architecture compatibility.

## Limitations
- Schedule generalization across inference step counts is limited, with performance degrading significantly at step counts different from training (Table 5b shows Δ0.741 at 8 steps vs Δ0.158 at 64 steps)
- The independence approximation in Equation 8 for advantage estimation introduces unquantified approximation error that may affect optimization quality
- Alternating optimization prevents instability but doesn't guarantee monotonic convergence, and the paper doesn't provide detailed convergence analysis

## Confidence
- High: Schedule sensitivity to hyperparameters (Table 2) and alternating optimization benefits (Table 5d) are directly demonstrated with controlled ablations
- Medium: Overall performance improvements are well-supported, but generalization across inference step counts needs further validation
- Medium: The factorized policy framework is theoretically sound, but the independence approximation's impact on advantage estimation is not quantified

## Next Checks
1. **Schedule generalization stress test**: Train Co-GRPO with T=48 steps, then evaluate at step counts {8, 16, 32, 48, 64, 96} to map performance degradation and identify generalization limits
2. **Advantage estimation validation**: Implement a small-scale experiment comparing GRPO updates using the independence approximation versus a baseline using full token dependencies (when computationally feasible) to quantify approximation error
3. **Convergence analysis**: Track individual reward trajectories for model and schedule components during alternating optimization to verify monotonic improvement rather than oscillatory behavior