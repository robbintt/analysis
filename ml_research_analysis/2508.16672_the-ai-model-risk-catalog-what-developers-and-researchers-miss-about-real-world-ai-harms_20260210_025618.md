---
ver: rpa2
title: 'The AI Model Risk Catalog: What Developers and Researchers Miss About Real-World
  AI Harms'
arxiv_id: '2508.16672'
source_url: https://arxiv.org/abs/2508.16672
tags:
- risks
- risk
- harms
- cards
- catalog
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzed 460,000 AI model cards from Hugging Face to
  build a catalog of 2,863 developer-reported risks. Using large language models,
  the authors extracted and classified risks according to established taxonomies,
  then compared them with researcher-identified risks and real-world AI incidents.
---

# The AI Model Risk Catalog: What Developers and Researchers Miss About Real-World AI Harms

## Quick Facts
- **arXiv ID**: 2508.16672
- **Source URL**: https://arxiv.org/abs/2508.16672
- **Reference count**: 40
- **Primary result**: Developers focus on technical issues like bias and safety, while researchers emphasize broader social impacts; both underrecognize fraud and manipulation risks prevalent in real incidents

## Executive Summary
This study analyzed 460,000 AI model cards from Hugging Face to build a catalog of 2,863 developer-reported risks, revealing systematic blind spots in how different expert communities conceptualize AI harms. Using large language models, the authors extracted and classified risks according to established taxonomies, then compared them with researcher-identified risks and real-world AI incidents. Developers primarily focused on technical issues like bias and model safety, while researchers emphasized broader social impacts. Both groups underrecognized risks from fraud and manipulation, which are prevalent in actual incidents. The findings highlight the need for structured risk reporting to better capture human-interaction and systemic risks early in AI development.

## Method Summary
The researchers used GPT-4o to extract and classify risks from 460,000 Hugging Face model cards, creating a catalog of 2,863 unique developer-reported risks. They employed regex filtering to identify risk sections, then used iterative prompt refinement with LLM extraction (achieving 90% agreement on samples) to identify risks in verb-object format. Deduplication was performed using fuzzy matching and embeddings, with majority-vote classification across three runs. The resulting catalog was compared against MIT Risk Repository (967 risks) and AI Incident Database (869 incidents) using statistical confidence intervals for proportion differences.

## Key Results
- Developers report discrimination & toxicity risks (44%) as their primary concern, while researchers focus on systemic impacts and developers emphasize technical safety
- Fraud and manipulation risks, which account for over 22% of real incidents, receive minimal attention from both developers (4%) and researchers (17%)
- Only 14% of model cards contain risk sections, and 96% of these are exact duplicates, suggesting template-based rather than thoughtful risk documentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-source triangulation reveals systematic blind spots in how different expert communities conceptualize AI risks
- Mechanism: By comparing risks from three distinct sources (developer model cards, researcher frameworks, incident reports) using common taxonomies, the approach identifies where each community's focus diverges from documented harms. The comparison uses statistical confidence intervals for proportion differences to quantify gaps reliably.
- Core assumption: Each source captures a partial but representative view of its community's risk perception
- Evidence anchors:
  - [abstract]: "Developers focused on technical issues like bias and safety, while researchers emphasized broader social impacts. Both groups paid little attention to fraud and manipulation, which are common harms arising from how people interact with AI."
  - [section]: "Malicious uses of AI are widespread, accounting for over 22% of recorded harms in the incident database, yet they receive less attention from researchers (17%) and especially developers (4%)."
  - [corpus]: Weak corpus support for triangulation methodology specifically; related work on developer perspectives exists (e.g., arxiv 2510.00909 on privacy risks from developer perspective) but multi-source comparison is novel
- Break condition: If any single source systematically misrepresents its community (e.g., media bias in incident coverage, or most model cards being empty templates), triangulation may amplify rather than correct biases

### Mechanism 2
- Claim: LLM-assisted extraction at scale enables systematic analysis of unstructured model card documentation
- Mechanism: GPT-4o with temperature=0 extracts risk mentions in standardized verb-object format from free-text sections. The pipeline includes: (1) regex-based section identification, (2) LLM extraction with iterative prompt refinement, (3) duplicate removal via fuzzy matching and embeddings, (4) majority-vote classification across three runs.
- Core assumption: LLM extraction preserves semantic meaning without introducing systematic distortions
- Evidence anchors:
  - [section]: "We prompted GPT-4o with definitions and examples... achieving 90% agreement on a sample of 50 cards."
  - [section]: "The results showed an accuracy of 83% and a macro-averaged F1 score of 81% for the seven-class classification task."
  - [corpus]: arxiv 2504.08952 (RiskRAG, co-authored by same team) extends this approach with RAG-based risk suggestion, validating the extraction methodology
- Break condition: If LLM classification systematically biases toward certain categories (e.g., over-classifying to "discrimination & toxicity"), the catalog would misrepresent actual developer concerns

### Mechanism 3
- Claim: Taxonomy alignment enables interoperability across heterogeneous risk sources
- Mechanism: Mapping all sources to common taxonomies (MIT Risk Repository taxonomy extending DeepMind) allows direct comparison. The MIT taxonomy adds "AI system safety, failures, and limitations" to capture model-specific technical risks that DeepMind's framework misses.
- Core assumption: The selected taxonomies adequately cover the full risk spectrum; categories are distinct enough for meaningful classification
- Evidence anchors:
  - [section]: "Most misclassifications occurred in the predicted categories of malicious actors and misuse (32 risks), misinformation (26 risks), and AI system safety, failures, and limitations classes (22 risks). Upon closer inspection, we found that these were not necessarily errors. As noted by Weidinger et al. (2023) and Slattery et al. (2024), these categories are not mutually exclusive"
  - [section]: "For brevity, we refer to these sources as the repository (MIT Risk Repository) and the database (AIID) throughout."
  - [corpus]: arxiv 2503.04746 discusses emerging safety frameworks, indicating taxonomy standardization is an active area but not settled
- Break condition: If taxonomy categories have significant semantic overlap (as the paper acknowledges for malicious use, misinformation, and safety failures), comparisons become noisy

## Foundational Learning

- Concept: **Risk vs. Harm Distinction**
  - Why needed here: The paper uses OECD definitions where risk is probability of harm, harm is materialized damage. This distinction matters because developers report envisioned risks while incident databases catalog actual harms—the comparison reveals gaps between anticipation and reality.
  - Quick check question: Given a model card stating "outputs may be factually incorrect," is this a risk or a harm? (Answer: Risk—it describes potential, not actualized damage)

- Concept: **Model Cards as Documentation Standard**
  - Why needed here: The entire analysis depends on model cards being meaningful documentation artifacts. Understanding that only 14% of 461,181 cards have risk sections, and 96% of those are duplicates, is critical for interpreting the catalog's representativeness.
  - Quick check question: If a model card copies its risk section from another card, should it be counted separately in prevalence analysis? (Answer: No—the paper deduplicates by keeping the most-downloaded card)

- Concept: **Risk Taxonomy Hierarchies**
  - Why needed here: The analysis operates at both category (7 types) and subcategory (21+ types) levels. Understanding that "discrimination & toxicity" contains subcategories like "unfair discrimination," "toxic content," and "unequal performance" is necessary to interpret the 44% developer focus on this area.
  - Quick check question: A model that performs worse for non-English speakers falls under which category and subcategory? (Answer: Discrimination & toxicity → Unequal performance across groups)

## Architecture Onboarding

- Component map:
  HuggingFace Model Cards (461K cards) -> Regex filtering for risk sections -> Risk-Section Cards (64K cards, 14%) -> LLM extraction with GPT-4o, temp=0 -> Raw Risk Mentions (37K mentions) -> Deduplication: fuzzy matching + embeddings -> Unique Risk Cards (2.7K cards) -> Classification: 3-run majority vote -> Taxonomy-Mapped Risks (2.9K unique risks) -> Comparison analysis -> AI Model Risk Catalog

- Critical path:
  1. **Data acquisition**: HuggingFace snapshot capture (one-time, July 2024)
  2. **Section extraction**: Regex-based filtering for risk-related keywords
  3. **LLM extraction pipeline**: Prompt engineering and validation against manual annotations
  4. **Taxonomy mapping**: Majority-vote classification with manual review of edge cases
  5. **Cross-source comparison**: Statistical testing with Miettinen-Nurminen confidence intervals

- Design tradeoffs:
  - **Manual vs. automated extraction**: Paper chose LLM for scale (2,672 cards) with 90% validation accuracy vs. infeasible full manual review
  - **Duplicate handling**: Kept most-downloaded card when duplicates exist, potentially biasing toward popular models
  - **Taxonomy selection**: MIT taxonomy chosen for comparability with largest existing repository, but sacrifices some DeepMind granularity
  - **Temperature=0 for reproducibility**: Reduces variability but may miss edge-case interpretations

- Failure signatures:
  - **Empty template proliferation**: 52,983 cards retained default templates requesting risk details—these don't reflect developer thinking
  - **Copy-paste duplication**: 96% of risk sections are exact duplicates, suggesting many developers aren't engaging meaningfully with risk documentation
  - **Category confusion**: LLM misclassifies 17% of risks, especially between "malicious actors," "misinformation," and "safety failures"—categories the paper acknowledges are not mutually exclusive
  - **Modality gaps**: Non-text modalities (image, audio, multimodal) have different risk profiles but are underrepresented in documentation

- First 3 experiments:
  1. **Validate extraction accuracy on held-out sample**: Run the LLM extraction pipeline on 100 manually-annotated model cards not used in prompt development, measuring precision/recall for risk identification and classification
  2. **Temporal drift analysis**: Compare 2022 vs. 2024 snapshots to test whether the observed shifts (discrimination overtaking safety as top concern, 4x increase in malicious use mentions) are statistically robust or reflect taxonomy mapping artifacts
  3. **Modality-specific risk profiling**: Stratify the catalog by input/output modality and compare risk distributions against incident database to test whether multimodal models have different blind spots than text-only models

## Open Questions the Paper Calls Out

- **Structured risk reporting format**: What structured risk reporting format would effectively help developers anticipate human-interaction and systemic risks during AI development? The authors note there is a clear need for the responsible AI community to adopt a structured standard for defining and communicating risks, but further simplification and user studies are needed to ensure risk communication meets the needs of different audiences.

- **Inclusive risk perspectives**: How can risk reporting systems incorporate perspectives from end users, civil society organizations, and affected communities that are currently missing from expert-driven sources? The authors explicitly acknowledge that important perspectives are still missing, including end users and affected communities, civil society organizations and NGOs working in digital rights, consumer protection, and human rights, as well as legal experts, ethicists, and sectoral specialists.

- **Privacy and security underreporting**: Why do developers underreport privacy and security risks despite these being directly tied to specific model designs and training data? The paper finds developers report privacy/security risks at only 3% compared to 8% in incidents and 12% in research, which the authors note is especially puzzling since privacy, security, and malicious use are directly tied to specific model designs, underlying data, and the amplified vulnerabilities introduced by LLMs.

## Limitations
- High duplicate rate (96%) suggests template-based rather than thoughtful risk documentation
- Taxonomy category overlap may introduce noise in cross-source comparisons
- Incident database coverage may skew toward sensational rather than representative harms

## Confidence

- **High Confidence**: The finding that developers disproportionately focus on technical issues (discrimination & toxicity at 44%) compared to researchers' emphasis on systemic impacts. This is supported by direct statistical comparison with confidence intervals.

- **Medium Confidence**: The claim that both communities underrecognize fraud and manipulation risks. While the statistical gap is clear, incident database coverage may skew toward sensational rather than representative harms.

- **Low Confidence**: The catalog's completeness as a representation of developer risk awareness, given that most risk sections appear to be template copies rather than thoughtful analysis.

## Next Checks

1. Conduct inter-rater reliability assessment on LLM extractions using a new sample of 100 model cards to verify the 90% extraction agreement rate holds across different model types and developers.

2. Perform manual content analysis on a stratified sample of risk sections to distinguish between template-copied risks and genuinely articulated developer concerns.

3. Compare the catalog's risk distributions against developer interviews or surveys to validate whether LLM extraction captures the full spectrum of developer risk awareness.