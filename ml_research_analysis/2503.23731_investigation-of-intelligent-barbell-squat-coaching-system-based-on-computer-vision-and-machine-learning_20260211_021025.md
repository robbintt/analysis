---
ver: rpa2
title: Investigation of intelligent barbell squat coaching system based on computer
  vision and machine learning
arxiv_id: '2503.23731'
source_url: https://arxiv.org/abs/2503.23731
tags:
- squat
- system
- each
- features
- shap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed an intelligent barbell squat coaching system
  utilizing computer vision and machine learning to provide real-time feedback and
  training guidance for beginners. The system extracted four key features (body joint
  angles, dorsiflexion, knee-to-hip ratio, and barbell stability) from 8,151 squat
  recordings collected from 77 participants.
---

# Investigation of intelligent barbell squat coaching system based on computer vision and machine learning

## Quick Facts
- arXiv ID: 2503.23731
- Source URL: https://arxiv.org/abs/2503.23731
- Reference count: 32
- Developed real-time barbell squat coaching system achieving F1 scores up to 100% for squat quality diagnosis

## Executive Summary
This study presents an intelligent barbell squat coaching system that provides real-time feedback for beginners using computer vision and machine learning. The system extracts four key biomechanical features from 8,151 squat recordings collected from 77 participants and diagnoses six common squat issues with F1 scores ranging from 69.01% to 100%. A four-week comparative study demonstrated that participants trained with the system showed significantly better improvements in squat technique compared to solo training, both by system evaluation and professional coach assessment. The system achieves diagnosis times under 0.5 seconds per squat, making it suitable for real-time coaching applications.

## Method Summary
The system uses Azure Kinect DK to capture 32 body keypoint coordinates per frame, which are reduced to four engineered features: body-thigh angle, dorsiflexion, knee-hip ratio, and barbell shift. These features are extracted from standardized 50-frame sequences after preprocessing with three methods (variation, variation relative change, and Z-score). Three machine learning architectures (1D-CNN, LSTM, and random forest) are trained on binary or multi-class classification tasks for four separate models targeting different squat issues. SHAP values are used for feature selection to improve accuracy and reduce computation time. The system provides real-time scores (0-100) and issue labels with remediation guidance, plus a replay mode for post-training review.

## Key Results
- SHAP-based feature selection improved F1 scores for lower-performing classes while reducing computation time
- System diagnosis times achieved under 0.5 seconds per squat, compared to 2 minutes for previous video-based approaches
- Four-week comparative study showed significantly better improvements in squat technique for system-trained participants (p<0.05 after Week 3)
- Coach evaluation confirmed system effectiveness, with significant differences observed after Week 2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pose-based feature extraction enables computationally efficient squat diagnosis compared to raw video analysis.
- Mechanism: The Azure Kinect SDK extracts 32 body keypoint coordinates per frame, which are reduced to 4 engineered features (body-thigh angle, dorsiflexion, knee-hip ratio, barbell shift). This dimensionality reduction allows ML models to process 50-frame sequences in <0.5 seconds rather than minutes required for video-based approaches.
- Core assumption: The 4 selected features capture the biomechanically relevant information needed to distinguish squat quality issues; no critical information is lost by excluding raw video.
- Evidence anchors: [abstract] "four primary characteristics of the barbell squat were identified: body joint angles, dorsiflexion, the ratio of knee-to-hip movement, and barbell stability"; [section] Previous video-based approach "evaluating a single squat took approximately 2 min" while this system achieves "diagnosis times under 0.5 seconds per squat"

### Mechanism 2
- Claim: SHAP-based feature selection improves model accuracy and reduces computation time, particularly for lower-performing classes.
- Mechanism: SHAP values quantify each feature's contribution to model output across temporal frames. Features with negative SHAP values are excluded; top features are intersected across output classes. This reduces input dimensionality while retaining discriminative information.
- Core assumption: SHAP values accurately reflect true feature importance for the classification task, and removing low-importance features reduces noise without losing signal.
- Evidence anchors: [abstract] "SHAP method used for feature selection to improve accuracy and computational efficiency"; [section] "When the F1 score was relatively low, the SHAP method could significantly improve the score while reducing the calculation time" — Model B Class 1 improved from 45.71% to 70% for LSTM with intersected features

### Mechanism 3
- Claim: Immediate real-time feedback produces measurable skill improvements over solo training within 4 weeks.
- Mechanism: After each squat, the system displays a score (0-100), detected issue labels, and remediation guidance. Users can review replays with synchronized feature traces. This closed-loop feedback enables motor learning correction between sets.
- Core assumption: The detected issues correspond to actual biomechanical faults (validated by coach agreement), and users can translate textual/visual feedback into motor corrections.
- Evidence anchors: [abstract] "four-week comparative study... participants trained with the system showed significantly better improvements in squat technique, both by system evaluation and professional coach assessment"; [section] "P-values falling below 0.05 after Week 3" for system scores; coach-evaluated scores showed significant differences "after Week 2"

## Foundational Learning

- Concept: **Pose Estimation Skeletal Tracking**
  - Why needed here: Understanding how Azure Kinect SDK converts depth camera data into joint coordinates is prerequisite for interpreting feature extraction failures and calibration requirements.
  - Quick check question: If the spine-navel keypoint consistently reads 15cm posterior to its true position, which derived features would be affected and in what direction?

- Concept: **Time-Series Classification with Variable-Length Sequences**
  - Why needed here: Squat durations vary (mean 45.3 frames, SD 5.3); understanding interpolation/resampling strategies (standardization to 50 frames) is critical for maintaining temporal alignment.
  - Quick check question: Why might zero-padding to 50 frames be problematic compared to linear interpolation for squat sequences?

- Concept: **SHAP Values for Feature Attribution**
  - Why needed here: The paper relies on SHAP for model optimization; understanding its limitations (feature interaction effects, computational cost for time-series) informs when to trust feature selection decisions.
  - Quick check question: If two features are perfectly correlated, what happens to their SHAP values, and how might this affect selection?

## Architecture Onboarding

- Component map: Azure Kinect DK (depth camera) → SDK Pose Estimation → 32 keypoints/frame → Feature Engineering (BT, DF, KHR, BS) → 4 features × 50 frames → Outlier Detection & Interpolation → 4 Parallel ML Models (A: depth, B: pelvic, C: ascending, D: descending) → Score Aggregation (100 - deductions per Table 4) → UI: Real-time display + Replay mode

- Critical path: The 0.5-second latency budget depends on pose estimation (~50ms assumed), feature computation (~10ms), and model inference (LSTM selected for Models C/D due to lower compute than 1D-CNN). Any stage exceeding budget breaks real-time feedback utility.

- Design tradeoffs:
  - 1D-CNN vs. LSTM: 1D-CNN achieved higher F1 for some classes (Model B: 93.54%/69.01%/77.42%) but slower inference; LSTM preferred when scores comparable
  - Binary vs. multiclass outputs: Separate models for mutually exclusive issues (pelvic tilt anterior vs. posterior) rather than unified classifier, increasing system complexity but simplifying training data requirements
  - Feature count: SHAP-reduced features (6-8 per model) vs. full 12 features; ~40% reduction in compute with improved F1 for low-performing classes

- Failure signatures:
  - "Data Error" status: Multiple outliers in single feature triggers squat exclusion
  - Pelvic tilt misclassification (F1=69.01%): Clothing occlusion of hip region; sportswear with long hems
  - Weight plate occlusion: Plates >26cm diameter block joint visibility
  - Premature recording termination: Hip rising too fast causes BT >140° before full extension

- First 3 experiments:
  1. Reproduce data preprocessing pipeline: Collect 50 squats, verify BT threshold detection (140° start/stop), confirm 50-frame standardization produces aligned KHR "S-curve" pattern matching Figure 5
  2. Ablate SHAP feature selection: Train Model B with all 12 features vs. intersected features, measure F1 delta and inference time delta on held-out test set
  3. Clothing sensitivity test: Have 3 participants perform identical squat protocol in (a) fitted athletic wear, (b) loose t-shirt, (c) jacket with hem below hip; quantify pelvis keypoint variance and downstream F1 impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating additional filming perspectives successfully mitigate joint occlusion caused by weight plates larger than 26 cm without increasing diagnosis latency beyond the current 0.5-second threshold?
- Basis in paper: [explicit] The conclusion identifies the limitation regarding plate size occlusion and explicitly states that "introducing more filming perspectives while keeping the responding speed will be the topic of further work."
- Why unresolved: The current single-camera setup fails when standard equipment (large plates) blocks the view of body joints, and the authors have not yet tested multi-view architectures to solve this specific occlusion problem.
- What evidence would resolve it: Validation results from a multi-camera system showing high F1 scores for users lifting with large plates, while maintaining a processing time of under 0.5 seconds.

### Open Question 2
- Question: How can the diagnostic models be improved to accurately identify posterior and anterior pelvic tilt (Labels 3 and 4) when participants wear loose-fitting clothing that obscures hip joint landmarks?
- Basis in paper: [inferred] The results section notes that Participant 3 exhibited inconsistencies because "loose-fitting attire hindered the system's... ability to accurately identify pelvic tilt issues," and the confusion matrix showed lower F1 scores for these specific labels.
- Why unresolved: The current computer vision pose estimation relies on clear joint visibility, which clothing occlusion disrupts, leading to misclassification of pelvic issues as "good squats."
- What evidence would resolve it: A modified detection algorithm or feature set that maintains high F1 scores for pelvic tilt labels across a control group wearing various types of loose sportswear.

### Open Question 3
- Question: What is the isolated contribution of the "replay mode" feature to long-term skill acquisition compared to the real-time feedback mechanism alone?
- Basis in paper: [inferred] The four-week comparative study measured the efficacy of the "system" as a whole against a control group, but the experimental design did not isolate the specific educational impact of the post-training replay and video demonstration features.
- Why unresolved: It is unclear if the improvement in the experimental group was driven solely by immediate corrections during the set or if the post-hoc video analysis played a significant role in motor learning.
- What evidence would resolve it: An ablation study comparing a group using only real-time feedback against a group using the full system (including replay mode) to measure differential learning outcomes.

## Limitations
- System fails to detect pelvic tilt issues when participants wear loose-fitting clothing that obscures hip joint landmarks
- Weight plates larger than 26 cm in diameter can occlude joints from the single-camera view
- Limited clothing variability in dataset (only one participant with loose-fitting attire caused system failure)

## Confidence
- **High Confidence:** Real-time performance (0.5s inference time achieved), pose estimation implementation using Azure SDK, basic system architecture
- **Medium Confidence:** SHAP feature selection effectiveness, F1 score improvements, comparative study results showing system superiority
- **Low Confidence:** Generalizability to diverse populations, robustness to different clothing types, long-term training effects beyond 4 weeks

## Next Checks
1. Test system robustness across diverse clothing types and body compositions not represented in original dataset
2. Conduct blinded evaluation with multiple coaches to establish inter-rater reliability for squat quality assessment
3. Perform longitudinal study (12+ weeks) to evaluate retention of learned squat techniques and potential over-reliance on system feedback