---
ver: rpa2
title: 'One Life to Learn: Inferring Symbolic World Models for Stochastic Environments
  from Unguided Exploration'
arxiv_id: '2510.12088'
source_url: https://arxiv.org/abs/2510.12088
tags:
- state
- world
- player
- action
- laws
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ONELIFE, a framework that learns symbolic
  world models from a single, unguided episode in a complex, stochastic environment.
  It represents world dynamics as a mixture of modular, conditionally-activated programmatic
  laws within a probabilistic programming framework, enabling efficient learning by
  routing inference and optimization only through relevant laws.
---

# One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration

## Quick Facts
- arXiv ID: 2510.12088
- Source URL: https://arxiv.org/abs/2510.12088
- Reference count: 40
- One-line primary result: ONELIFE learns symbolic world models from a single, unguided episode, outperforming baselines on 16 of 23 Crafter-OO scenarios

## Executive Summary
ONELIFE introduces a framework for learning symbolic world models from a single, unguided episode in stochastic environments. It represents world dynamics as a mixture of modular, conditionally-activated programmatic laws within a probabilistic programming framework. This design enables efficient learning by routing inference and optimization only through relevant laws. The method is evaluated on Crafter-OO, a reimplementation of the Crafter environment with an exposed object-oriented state, using a new evaluation protocol measuring state ranking and state fidelity.

## Method Summary
ONELIFE learns symbolic world models by representing environment dynamics as a mixture of modular, conditionally-activated programmatic laws within a probabilistic programming framework. The framework allows for efficient learning by routing inference and optimization only through relevant laws, enabling the system to learn from a single, unguided episode. The approach is tested on Crafter-OO, a reimplementation of the Crafter environment with an exposed object-oriented state, using a novel evaluation protocol that measures state ranking and state fidelity.

## Key Results
- ONELIFE successfully learns key environment dynamics from minimal interaction in Crafter-OO
- Outperforms a strong baseline on 16 out of 23 scenarios tested
- Demonstrates utility for planning through forward simulation to identify superior strategies

## Why This Works (Mechanism)
ONELIFE's efficiency stems from its modular design, where world dynamics are represented as a mixture of conditionally-activated programmatic laws. This allows the system to route inference and optimization only through relevant laws, avoiding unnecessary computation. The probabilistic programming framework enables symbolic reasoning about stochastic environments, while the single-episode learning constraint forces the model to extract maximal information from limited data.

## Foundational Learning
- **Probabilistic Programming**: Needed to reason about uncertainty in stochastic environments; quick check: can represent and manipulate probability distributions over program executions
- **Program Synthesis**: Required to generate programmatic laws from observations; quick check: can produce executable programs that match observed behavior
- **Modular Composition**: Essential for routing inference efficiently; quick check: can combine subprograms conditionally based on context
- **Single-shot Learning**: Critical for the one-episode constraint; quick check: can extract robust patterns from limited data

## Architecture Onboarding

**Component Map**: Observation -> Probabilistic Program Generator -> Modular Law Mixture -> State Predictor -> Evaluation Metrics

**Critical Path**: The system takes a single episode of observations, generates probabilistic programs representing potential laws, composes them into a modular mixture, and uses this mixture to predict future states and evaluate fidelity.

**Design Tradeoffs**: The modular approach trades off expressiveness for efficiency, allowing the system to focus computation only on relevant laws. The probabilistic programming framework trades off deterministic guarantees for the ability to reason about uncertainty.

**Failure Signatures**: The system may fail when the environment dynamics are too complex to capture with a small number of modular laws, or when the single episode doesn't provide sufficient coverage of the state space.

**First Experiments**:
1. Test ONELIFE on Crafter-OO with varying episode lengths to determine minimum data requirements
2. Compare state fidelity scores against ground truth dynamics in controlled scenarios
3. Evaluate planning performance by measuring success rate in achieving goals using learned models

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to environments with larger or more complex state spaces remains uncertain
- Performance in environments with continuous state representations is untested
- Current evaluation metrics may not fully capture practical utility in diverse downstream tasks

## Confidence
- **Scalability to complex environments**: Medium
- **Practical utility in downstream tasks**: Medium
- **Ability to learn from single episode**: High

## Next Checks
1. Test ONELIFE on environments with continuous state spaces to evaluate scalability and robustness beyond discrete, object-oriented states
2. Assess the learned models' performance in diverse downstream tasks, such as transfer learning or long-horizon planning, to validate their practical utility
3. Compare ONELIFE's efficiency and accuracy against other symbolic and neural-symbolic methods in environments with varying levels of stochasticity and complexity