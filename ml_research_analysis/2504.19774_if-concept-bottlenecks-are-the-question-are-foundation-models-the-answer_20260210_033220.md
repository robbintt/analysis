---
ver: rpa2
title: If Concept Bottlenecks are the Question, are Foundation Models the Answer?
arxiv_id: '2504.19774'
source_url: https://arxiv.org/abs/2504.19774
tags:
- concept
- concepts
- annotations
- bottleneck
- cbms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Concept Bottleneck Models (CBMs) are designed to be both accurate
  and interpretable by using a two-step process: first extracting high-level concepts
  from inputs (e.g., images) and then using these concepts to make predictions. However,
  their interpretability depends heavily on the quality of the learned concepts.'
---

# If Concept Bottlenecks are the Question, are Foundation Models the Answer?

## Quick Facts
- **arXiv ID:** 2504.19774
- **Source URL:** https://arxiv.org/abs/2504.19774
- **Reference count:** 40
- **Primary result:** VLM-supervised Concept Bottleneck Models achieve high label accuracy but learn low-quality, leaky, and entangled concepts compared to expert-annotated models.

## Executive Summary
Concept Bottleneck Models (CBMs) aim to provide interpretable AI by learning high-level concepts from inputs before making predictions. While expert annotations ensure high-quality concepts, they are expensive and often unavailable. This study evaluates three state-of-the-art VLM-CBMs (LaBo, LF-CBM, VLG-CBM) that use weak supervision from Vision-Language Models instead of human annotations. Experiments on Shapes3d, CelebA, and CUB datasets reveal that VLM supervision often differs significantly from expert annotations, leading to lower concept quality. VLM-CBMs frequently achieve good label accuracy despite learning inaccurate, entangled, or leaky concepts, highlighting a disconnect between label and concept accuracy.

## Method Summary
The study evaluates VLM-CBMs by comparing their concept quality to expert-annotated CBMs using three datasets (Shapes3d, CelebA, CUB) with fixed gold-standard concept vocabularies. Models are trained sequentially: first training a concept extractor using VLM supervision or labels, then freezing it and training a linear inference layer. The evaluation uses label performance metrics (F1 score) and concept quality metrics (Concept AUC, Leakage, Disentanglement, Oracle Impurity Score). The researchers test different backbones (ResNet-18, CLIP ViT-B/16) and training procedures (AdamW, GLM-SAGA, SVM) with early stopping based on validation loss.

## Key Results
- VLM-CBMs achieved high label F1 scores (up to 0.96 on Shapes3d) but low concept AUC scores (as low as 0.24), indicating a disconnect between label and concept accuracy
- VLM-CBMs showed higher leakage (up to 0.99 on CelebA) and lower disentanglement compared to models trained on expert annotations
- Concept quality varied significantly across datasets, with VLM precision/recall ranging from 0.32 on Shapes3d to 0.53 on CUB
- The study found that VLM supervision can differ substantially from expert annotations, compromising interpretability despite maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1: Weak Supervision Distillation from VLMs
VLM-CBMs distill conceptual knowledge from pre-trained foundation models (like CLIP or LLaVA) to replace expensive human annotation. The VLM generates training signals by mapping inputs and textual concept descriptions into shared embedding spaces or using prompting to generate binary labels. This mechanism assumes the VLM's internal representation aligns with human semantic understanding. It fails when inputs are out-of-distribution for the VLM or when the VLM hallucinates, creating noisy training signals that misalign the concept extractor.

### Mechanism 2: Leakage-Mediated Accuracy Maintenance
VLM-CBMs maintain high downstream label accuracy by exploiting "leakage," where concept vectors encode unintended task-relevant information rather than accurate concept semantics. When precise concept prediction is difficult due to poor VLM supervision, the extractor "cheats" by encoding label information directly into the concept vector. The inference layer can then access this leaked information to reconstruct the label even when specific concept activations are semantically incorrect. This breaks interpretability because correcting a "leaky" concept based on its semantic definition may not update predictions as expected.

### Mechanism 3: Context-Dependent VLM Misalignment
VLM supervision quality depends heavily on domain and concept vocabulary, leading to variable concept quality that label accuracy metrics fail to capture. VLMs prioritize general web-scale patterns that may clash with expert definitions when applied to specialized tasks. Performance degrades severely when textual concept descriptions are ambiguous or when visual concepts require fine-grained distinctions the VLM cannot make.

## Foundational Learning

- **Concept: Concept Bottleneck Models (CBMs)**
  - Why needed: This is the base architecture being critiqued; understanding the split between concept extractor $f$ (black box) and inference layer $g$ (white box) is crucial for diagnosing why VLM supervision affects interpretability
  - Quick check: Can you explain how a standard CBM ensures interpretability compared to a standard ResNet classifier?

- **Concept: Weak Supervision via Foundation Models**
  - Why needed: The paper evaluates "VLM-CBMs" which replace human labels with VLM outputs; understanding how CLIP or LLaVA generates these labels is crucial for understanding the source of noise
  - Quick check: What is the difference between obtaining concept supervision via cosine similarity (CLIP) versus binary prompting (LLaVA)?

- **Concept: Concept Quality Metrics (Leakage & Disentanglement)**
  - Why needed: The core contribution is evaluating quality beyond simple accuracy; grasping "Leakage" (concepts carrying unintended label info) and "Disentanglement" (concepts mixing information) is essential for interpreting results
  - Quick check: If a model has high label accuracy but high leakage, why is it considered "unsafe" for interpretability?

## Architecture Onboarding

- **Component map:** Input image $x$ → VLM (training only) generates concept labels $c$ → Concept extractor $f$ produces bottleneck $\hat{c}$ → Inference layer $g$ predicts class $y$

- **Critical path:**
  1. Define Vocabulary: Establish set of textual descriptions $T$
  2. Generate Annotations: Run VLM on training set to generate pseudo-labels $c$
  3. Train Extractor: Train backbone $f$ to match pseudo-labels
  4. Train Classifier: Train linear layer $g$ on frozen bottleneck outputs

- **Design tradeoffs:**
  - Expert vs. VLM Supervision: Expert labels yield high interpretability but low scalability; VLM supervision yields high scalability but risks concept misalignment and leakage
  - Vocabulary Source: Expert vocabularies are safer but limited; LLM-generated vocabularies allow larger bottlenecks but risk including "non-visual" concepts

- **Failure signatures:**
  - "Right for Wrong Reasons" Gap: High Label F1 (>0.95) alongside low Concept AUC (<0.50) indicates shortcuts/leakage rather than learned concepts
  - High Leakage Score: High LEAK metric indicates bottleneck has failed to be interpretable
  - Entanglement in DCI: Off-diagonal activity in DCI matrix indicates concepts are mixing

- **First 3 experiments:**
  1. Annotation Audit: Measure agreement between chosen VLM's annotations and ground truth to establish concept quality upper bound
  2. Leakage Stress Test: Train VLM-CBM and measure F1 gap, checking if bottom 50% of concepts can predict the label
  3. Concept Ablation: Compare performance using "Gold" vocabulary vs. "LLM-Generated" vocabulary to quantify semantic drift

## Open Questions the Paper Calls Out
- Can VLM-CBMs be designed to be robust to low-quality concept annotations using uncertainty estimation or regularization strategies?
- How does the use of VLM-supervised concepts specifically impact the reasoning quality and interpretability of the final classifier layer?
- Does expanding the concept vocabulary to thousands of LLM-generated concepts mitigate or exacerbate the disconnect between high label accuracy and low concept quality?

## Limitations
- Evaluation relies on comparing VLM-generated annotations against ground truth labels rather than human expert annotations
- Only three relatively small-scale datasets were tested, limiting generalizability to real-world applications
- The study uses pre-defined concept vocabularies rather than LLM-generated ones, potentially overestimating concept quality

## Confidence
- **High confidence**: Claims about the existence of leakage are well-supported by quantitative metrics and stress tests
- **Medium confidence**: Assertion that VLM supervision differs from expert annotations is supported by precision/recall variations
- **Medium confidence**: Claim that high label accuracy can mask poor interpretability is convincing but practical thresholds remain unclear

## Next Checks
1. Conduct a small-scale user study where human experts rate concept quality on a subset of predictions to validate automated metrics
2. Test whether different VLMs (CLIP, LLaVA, BLIP) produce similar quality concepts when used as supervision sources
3. Implement a concept intervention experiment where users manually correct "leaky" concepts and measure whether predictions update as expected