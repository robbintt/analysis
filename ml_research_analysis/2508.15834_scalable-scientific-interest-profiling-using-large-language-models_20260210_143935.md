---
ver: rpa2
title: Scalable Scientific Interest Profiling Using Large Language Models
arxiv_id: '2508.15834'
source_url: https://arxiv.org/abs/2508.15834
tags:
- profiles
- summaries
- human-written
- research
- researchers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study introduces a scalable pipeline for automated researcher
  profiling using large language models (LLMs). By extracting PubMed publications
  and metadata for 595 Columbia University Irving Medical Center faculty, the authors
  generate narrative research profiles using two LLM-based methods: MeSH term-based
  generation and abstract-based summarization with GPT-4o-mini.'
---

# Scalable Scientific Interest Profiling Using Large Language Models

## Quick Facts
- arXiv ID: 2508.15834
- Source URL: https://arxiv.org/abs/2508.15834
- Reference count: 40
- The study introduces a scalable pipeline for automated researcher profiling using large language models, achieving moderate semantic similarity (BERTScore F1 ~0.55) between machine-generated and human-written profiles.

## Executive Summary
This study presents a scalable pipeline for generating narrative research profiles of researchers using large language models (LLMs). The authors process PubMed publications and metadata for 595 Columbia University Irving Medical Center faculty, generating profiles using two LLM-based methods: MeSH term-based generation and abstract-based summarization with GPT-4o-mini. While automatic evaluation reveals low lexical overlap between machine-generated and human-written profiles (ROUGE-L, BLEU, METEOR scores <15), BERTScore shows moderate semantic similarity (F1: 0.542 for MeSH-based, 0.555 for abstract-based). Manual review of 18 profiles shows 77.78% of MeSH-based profiles rated good/excellent, with 93.44% favored for readability. The study concludes LLMs can automate profile generation at scale, with MeSH-derived profiles preferred for readability, though semantic differences persist due to divergent keyword choices.

## Method Summary
The pipeline extracts PubMed publications and metadata for 595 CUIMC faculty, filtering to first/last three author positions and 10-year recency. Two LLM-based generation approaches are compared: MeSH-based keyword prompting and abstract-based summarization with LDA clustering. GPT-4o-mini generates profiles using one-shot prompting with example profiles. Evaluation combines automatic metrics (ROUGE-L, BLEU, METEOR, BERTScore, KL divergence) with human review of 18 profiles across four dimensions (accuracy, granularity, readability, informativeness).

## Key Results
- BERTScore F1: 0.542 (MeSH-based) vs. 0.555 (abstract-based) vs. 0.851 (paraphrased human validation)
- ROUGE-L, BLEU, METEOR: all <15 for machine-generated vs. human-written profiles
- KL divergence: 8.56-8.58 between machine-generated and human-written TF-IDF distributions
- Human evaluation: 77.78% of MeSH-based profiles rated good/excellent; 93.44% preferred for readability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MeSH-based profile generation yields higher readability than abstract-based generation when processing large publication corpora.
- Mechanism: MeSH terms provide pre-curated, normalized vocabulary that reduces noise and context-window pressure. GPT-4o-mini receives compact keyword lists rather than lengthy abstracts, enabling more coherent synthesis without the "verbatim stitching" observed in abstract-based outputs. Abstract-based profiles showed syntactic ambiguity scores of 9.605 vs. 4.190 for MeSH-based (Figure 6), suggesting abstract-derived text concatenates scattered phrases without integration.
- Core assumption: MeSH terms accurately capture the conceptual core of a researcher's work, and fewer input tokens lead to more coherent LLM outputs.
- Evidence anchors:
  - [abstract]: "panelists preferred MeSH-based over abstract-based profiles in 67.86 percent of comparisons"
  - [section 3.4]: "MeSH-based profiles are less lexically diverse... abstract-based profiles have a similar lexical diversity as compared to the human-written ones"
  - [corpus]: Related work on semantic profiles for recommender systems (FMR=0.59) supports structured profiling, but direct comparisons to MeSH-based generation are unavailable.
- Break condition: If MeSH terms for a researcher are sparse, misassigned, or fail to capture emerging methodologies not yet indexed, MeSH-based profiles will underperform abstract-based alternatives.

### Mechanism 2
- Claim: Low lexical overlap combined with moderate semantic similarity reflects LLMs' tendency toward domain-convergent vocabulary rather than human-level conceptual abstraction.
- Mechanism: LLMs trained on broad corpora tend to produce homogenized outputs that favor common n-grams and topics. The paper reports KL divergence of 8.56–8.58 between machine-generated and human-written TF-IDF distributions, indicating distinct keyword choices. Humans abstract and synthesize (e.g., "neuro-symbolic methods to automate medical evidence comprehension"), while LLMs list granular terms (e.g., "NLP, evidence retrieval, AI") without integration.
- Core assumption: KL divergence of TF-IDF distributions meaningfully captures vocabulary distributional differences that reflect abstraction capacity.
- Evidence anchors:
  - [abstract]: "KL divergence (8.56–8.58) indicates distinct keyword choices"
  - [section 3.1]: "161 distinct concepts used in self-written summaries but absent in machine-generated summaries"
  - [corpus]: No direct corpus evidence on homogenization in scientific profiling; claim rests on citations [34, 35] regarding AI-assisted writing convergence.
- Break condition: If researchers' self-written profiles are themselves formulaic or derivative, KL divergence may reflect source quality rather than LLM abstraction limits.

### Mechanism 3
- Claim: BERTScore provides a more meaningful quality signal than lexical metrics (ROUGE, BLEU, METEOR) for evaluating paraphrased or semantically equivalent profiles.
- Mechanism: Traditional NLG metrics require exact n-gram overlap. Paraphrased human summaries (semantically near-identical to originals) achieved BERTScore F1=0.851 but low lexical scores—demonstrating that lexical metrics penalize legitimate variation. BERTScore computes cosine similarity between contextual BERT embeddings, capturing meaning despite surface differences.
- Core assumption: High BERTScore between paraphrased and original profiles validates the metric's ability to capture semantic equivalence.
- Evidence anchors:
  - [abstract]: "BERTScore analysis with paraphrased human summaries (F1=0.851) validates semantic similarity as a more meaningful metric"
  - [section 3.2]: "All three comparisons demonstrated statistically significant differences, each with a p-value <0.0005"
  - [corpus]: Weak external validation; corpus neighbors do not address BERTScore reliability for researcher profiling.
- Break condition: If profiles contain factually incorrect but semantically plausible claims, BERTScore may give false positives for quality.

## Foundational Learning

- Concept: **TF-IDF and KL Divergence**
  - Why needed here: Quantifies how differently machine vs. human profiles weight vocabulary. High KL divergence signals distinct conceptual emphasis, not just surface variation.
  - Quick check question: Given two documents with TF-IDF vectors P and Q, what does KL(P||Q) = 8.5 imply about their vocabulary distributions?

- Concept: **BERTScore vs. Lexical Metrics**
  - Why needed here: Explains why profiles with low ROUGE/BLEU can still achieve BERTScore F1 ~0.55, and why paraphrased text validates this distinction.
  - Quick check question: Why would a paraphrased summary score low on BLEU but high on BERTScore?

- Concept: **LDA Topic Modeling for Context Window Management**
  - Why needed here: Senior researchers with 100+ publications exceed GPT-4o-mini's 128K token limit. LDA clusters publications by topic, enabling hierarchical summarization.
  - Quick check question: If an author has 200 publications, how does LDA with K=30 topics enable profile generation within context limits?

## Architecture Onboarding

- Component map: Data Collection -> Filtering -> LDA Clustering (abstract-based) -> GPT-4o-mini Generation -> Metric Evaluation -> Human Review
- Critical path: Data collection → Authorship filtering → LDA clustering (for abstract-based) → GPT-4o-mini generation → Metric evaluation. Human evaluation is offline validation, not production-critical.
- Design tradeoffs:
  - MeSH-based: Faster, more readable, but loses granular methodological detail
  - Abstract-based: Richer content, but higher syntactic ambiguity and context-window overhead
  - Recency weighting: Paper found 80/167 researchers had diversity scores <0.3 (stable topics), suggesting limited benefit from recency weighting for most profiles
- Failure signatures:
  - Empty/missing MeSH terms → MeSH-based generation fails or produces generic output
  - Common names without affiliation disambiguation → publication misattribution
  - Profiles with low inter-rater agreement (Gwet's AC1 <0.5) → inconsistent quality, flag for manual review
- First 3 experiments:
  1. **Baseline replication**: Run pipeline on 20 researchers with both MeSH and abstract-based methods; compare BERTScore and human ratings to paper benchmarks (F1 ~0.54–0.56, 77% good/excellent for MeSH).
  2. **Ablation on authorship filter**: Test whether including middle-author publications changes KL divergence or factual accuracy scores.
  3. **Recency weighting test**: For the 7 researchers with diversity scores >0.7, compare uniform vs. recency-weighted profiles; assess whether topic-shift cases benefit from time decay.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can retrieval-augmented language models integrated with external knowledge bases improve automated publication screening and name disambiguation across researchers from different institutions with identical names?
- Basis in paper: [explicit] The conclusion explicitly identifies this as a future direction: "Directions of future work include fully automating publication screening and name disambiguation for researchers from different institutions and backgrounds but with the same names, using retrieval-augmented language models with external knowledge bases."
- Why unresolved: The current pipeline relies on institutional affiliation for name disambiguation and heuristics for authorship relevance, which the authors acknowledge as limitations.
- What evidence would resolve it: A comparative study showing improved precision and recall in publication assignment using RAG-based approaches versus the current heuristic method.

### Open Question 2
- Question: What prompting strategies or model architectures could close the ~30% semantic gap between LLM-generated profiles (BERTScore F1 ~0.55) and human-level abstraction (paraphrased baseline F1 = 0.851)?
- Basis in paper: [explicit] The discussion states: "This finding presents a key challenge in automated research profiling and suggests that future improvements should focus on closing the gap between current LLM capabilities and human-like abstraction."
- Why unresolved: LLMs tend toward verbatim repetition rather than conceptual synthesis, as illustrated by the example where human profiles integrate methodologies into concepts like "neuro-symbolic methods" while machine profiles list granular terms separately.
- What evidence would resolve it: Demonstrating that a new approach achieves BERTScore F1 > 0.75 while maintaining or improving readability scores from human evaluation.

### Open Question 3
- Question: Do the findings generalize to researchers in non-biomedical disciplines and institutions with different publication patterns?
- Basis in paper: [explicit] The limitations section states: "This cohort may not fully represent the comprehensive research topics across other disciplines or institutions. As a future direction, we can extend the study to include more diverse disciplines and institutions to evaluate the two LLM-based approaches."
- Why unresolved: The study was restricted to Columbia University Irving Medical Center faculty, where PubMed coverage is high; fields with different publication venues or authorship norms may yield different results.
- What evidence would resolve it: Replicating the pipeline on researchers from diverse disciplines (e.g., computer science, social sciences, humanities) and comparing performance metrics against the current benchmark.

### Open Question 4
- Question: Would recency-weighted profile generation meaningfully improve accuracy for the ~4.3% of researchers who exhibit substantial topic evolution (diversity scores > 0.7)?
- Basis in paper: [inferred] The topic stability analysis found that while 48.8% of researchers had focused interests (diversity < 0.3), a small subset showed marked topic shifts. The authors weighted publications equally but note "recency-weighted variants may benefit the small subset with marked topic shifts."
- Why unresolved: The current approach treats all publications uniformly over the decade, potentially overemphasizing older work for researchers whose focus has shifted.
- What evidence would resolve it: A stratified evaluation comparing equal-weighted vs. recency-weighted profiles specifically for researchers with high diversity scores, measuring alignment with their self-written current interests.

## Limitations
- Reliance on lexical metrics (ROUGE, BLEU, METEOR) that poorly capture semantic equivalence, though validated by BERTScore
- Name disambiguation relies solely on affiliation matching, which may fail for common names or researchers with multiple affiliations
- Human evaluation sample (18 profiles) is small relative to the 595-faculty corpus, limiting generalizability of readability findings

## Confidence

- **High confidence**: MeSH-based profiles demonstrate superior readability (67.86% panelist preference, lower syntactic ambiguity scores of 4.190 vs. 9.605). BERTScore validation with paraphrased human summaries (F1=0.851) reliably demonstrates semantic equivalence measurement.
- **Medium confidence**: KL divergence values (8.56-8.58) meaningfully capture vocabulary distributional differences between machine and human profiles. LDA clustering with K=30 effectively manages context windows for researchers with 100+ publications.
- **Low confidence**: The claim that low lexical overlap with moderate semantic similarity reflects LLM homogenization rather than human abstraction quality. No direct corpus evidence supports this mechanism beyond citations [34, 35] on AI-assisted writing convergence.

## Next Checks

1. **Ablation study on authorship filtering**: Generate profiles including middle-author publications for 20 researchers and compare KL divergence and factual accuracy scores to baseline first/last-three author filtering. This validates whether current filtering preserves conceptual coherence while reducing noise.

2. **BERTScore threshold calibration**: Determine the minimum BERTScore F1 score above which machine-generated profiles are semantically equivalent to human-written ones. Use the paraphrased validation data (F1=0.851) as the semantic equivalence benchmark to establish practical quality thresholds.

3. **Topic-shift case analysis**: For the 7 researchers with diversity scores >0.7, generate both uniform and recency-weighted profiles. Compare BERTScore, KL divergence, and human readability ratings to quantify whether time decay benefits researchers with evolving research interests.