---
ver: rpa2
title: Enhancing the Patent Matching Capability of Large Language Models via the Memory
  Graph
arxiv_id: '2504.14845'
source_url: https://arxiv.org/abs/2504.14845
tags:
- patent
- matching
- llms
- memgraph
- patents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MemGraph, a method that enhances patent matching
  capabilities of large language models (LLMs) by incorporating a memory graph derived
  from their parametric memory. The method prompts LLMs to traverse their memory to
  identify relevant entities within patents and attributes these entities to corresponding
  ontologies.
---

# Enhancing the Patent Matching Capability of Large Language Models via the Memory Graph

## Quick Facts
- arXiv ID: 2504.14845
- Source URL: https://arxiv.org/abs/2504.14845
- Reference count: 40
- Key outcome: MemGraph improves patent matching accuracy by 17.68% over baseline LLMs by leveraging entity and ontology extraction from LLM parametric memory

## Executive Summary
This paper addresses the challenge of patent matching by proposing MemGraph, a method that enhances large language models' capability to identify similar patents through a memory graph traversal approach. The core insight is that LLMs possess parametric knowledge about technical entities and ontologies that can be explicitly extracted and used to improve retrieval and matching accuracy. MemGraph operates by prompting the LLM to traverse its memory graph, extracting relevant technical entities from the query patent and attributing them to corresponding ontologies. This structured knowledge is then used to expand the retrieval query and guide the matching process, resulting in significant performance improvements on the PatentMatch dataset across multiple LLM architectures.

## Method Summary
MemGraph is a patent matching method that enhances LLMs through a virtual memory graph constructed via prompt engineering rather than explicit graph structures. The approach works in three stages: (1) Entity Traversal - the LLM extracts technical entities from the query patent; (2) Retrieval - these entities are concatenated to the original query and used to retrieve top-3 candidate patents using a dense retriever; (3) Ontology Traversal - the LLM generates hierarchical ontologies for both query and candidate patents, which are then used in a final matching prompt to predict the most similar patent. The method is implemented purely through input manipulation without requiring model fine-tuning, making it applicable across various LLM architectures.

## Key Results
- MemGraph achieves 17.68% performance improvement over baseline LLMs on PatentMatch dataset
- The method shows strong generalization ability across different LLM architectures (Llama-3, Qwen2, GLM-4, Qwen2.5)
- Memory graph traversal enhances internal reasoning processes during patent matching, improving accuracy in both retrieval and generation phases

## Why This Works (Mechanism)

### Mechanism 1: Query Expansion via Entity Extraction
Extracting specific technical entities from the query patent and appending them to the search query improves retrieval density by shifting the vector representation in embedding space to capture technical specifics rather than broad keyword overlaps. This works because the LLM's parametric memory contains domain knowledge to identify relevant entities, and the dense retriever effectively encodes these entities.

### Mechanism 2: Hierarchical Semantic Grounding via Ontology Traversal
Forcing the LLM to explicitly generate hierarchical ontologies for both query and candidate patents reduces semantic ambiguity by aligning comparisons to a shared taxonomy. This acts as a "semantic anchor" ensuring the model compares patents based on structural/functional categories rather than superficial lexical overlap.

### Mechanism 3: Memory-Context Conflict Resolution
Explicitly surfacing internal knowledge (entities/ontologies) helps the LLM override noisy or misleading external context retrieved during RAG by creating a "cognitive scaffold" that filters or prioritizes retrieved evidence aligning with internal understanding.

## Foundational Learning

**Parametric Memory vs. Contextual Knowledge**: The paper relies on distinguishing between what the LLM "knows" (weights/parameters) and what it "reads" (retrieved patents). This distinction is crucial for understanding how MemGraph bridges vocabulary mismatch problems.

Quick check: Can you explain why a standard LLM might fail to match two patents that describe the same invention using different synonyms?

**Dense Retrieval & Embedding Spaces**: The method uses a dense retriever (BGE) for RAG implementation. Understanding that queries and documents are vectors in high-dimensional space is essential to grasping why "Entity Expansion" works - it shifts the vector.

Quick check: How does appending extracted entities to a query change the results returned by a dense retriever like BGE?

**Prompt Engineering / In-Context Learning**: The entire "Memory Graph Traversal" is implemented via specific prompts rather than architectural changes. The system's success depends entirely on how well the LLM follows these instructions to extract structured data.

Quick check: Does MemGraph require retraining the LLM's weights, or is it implemented purely through input manipulation?

## Architecture Onboarding

**Component map**: Query Patent → LLM Entity Extraction → Expanded Query Creation → Retriever (Top-3 Docs) → LLM Ontology Extraction (for Query + Candidates + Retrieved) → Final Matching Prompt

**Critical path**: The query patent flows through entity extraction, retrieval, ontology generation for all documents, and finally to the matching prompt where the LLM predicts the answer.

**Design tradeoffs**: The system trades increased latency (multiple LLM calls for extraction/traversal) for accuracy, avoiding fine-tuning costs but relying heavily on the LLM having existing patent knowledge in its weights.

**Failure signatures**: Entity Drift (extracted entities too generic), Ontology Hallucination (generated categories don't align with standards), Context Window Overflow (input length exceeds model limits).

**First 3 experiments**:
1. Baseline Retriever Test: Run standard RAG on PatentMatch subset to establish baseline accuracy
2. Ablation on $Z_{IR}$: Implement only Entity Expansion step to test retrieval recall improvement
3. Ontology Consistency Check: Validate generated ontologies adhere to "Major > Subcategory > Specific class" structure

## Open Questions the Paper Calls Out

1. How does MemGraph perform on patents containing terminology that emerged after the LLM's training cutoff, given its reliance on parametric memory for entity extraction?

2. To what extent does the accuracy of the generated memory graph affect the final patent matching performance?

3. How does MemGraph scale in terms of retrieval accuracy and latency when applied to industrial-scale patent corpora (e.g., millions of documents)?

## Limitations
- Performance relies heavily on prompt engineering effectiveness which may not generalize across different LLM architectures or domains
- The virtual "memory graph" implemented through prompts creates potential brittleness compared to explicit graph structures
- Multiple LLM calls create significant computational overhead compared to single-pass approaches
- Assumes LLM's parametric memory contains accurate patent domain knowledge which may not hold for models with limited technical training

## Confidence

**High Confidence**: The 17.68% improvement over baselines is supported by experimental results in Table 5, and the ablation study in Section 5.3 provides reasonable evidence for the Memory Graph Traversal mechanism.

**Medium Confidence**: The generalization across different LLMs shows promise, but specific performance gains may vary significantly with different patent domains or languages.

**Low Confidence**: The claim about memory-context conflict resolution relies primarily on a single case study rather than systematic evaluation.

## Next Checks

1. **Prompt Template Validation**: Extract the exact final matching prompt from Figure 2 and test it in isolation to verify the claimed performance improvement isn't due to formatting artifacts or prompt engineering tricks.

2. **Cross-Domain Transfer Test**: Apply MemGraph to a non-patent technical document matching task (e.g., research papers or legal documents) to test whether the entity extraction and ontology generation mechanisms generalize beyond the patent domain.

3. **Computation Overhead Analysis**: Measure the total inference time and cost for MemGraph versus baseline approaches across different model sizes to quantify the practical trade-off between accuracy gains and computational expense.