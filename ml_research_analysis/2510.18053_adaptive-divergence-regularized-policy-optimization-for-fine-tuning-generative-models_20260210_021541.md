---
ver: rpa2
title: Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative
  Models
arxiv_id: '2510.18053'
source_url: https://arxiv.org/abs/2510.18053
tags:
- adrpo
- regularization
- fine-tuning
- reward
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the exploration-exploitation dilemma in RL\
  \ fine-tuning of generative models, where fixed regularization coefficients create\
  \ trade-offs between preserving model capabilities and optimizing reward. The authors\
  \ propose Adaptive Divergence Regularized Policy Optimization (ADRPO), which dynamically\
  \ adjusts regularization strength based on sample-specific advantage estimates\u2014\
  reducing regularization for high-advantage samples while strengthening it for poor\
  \ samples."
---

# Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models

## Quick Facts
- arXiv ID: 2510.18053
- Source URL: https://arxiv.org/abs/2510.18053
- Reference count: 40
- Key outcome: ADRPO fine-tunes a 2B SD3 model that outperforms substantially larger models (4.8B and 12B) across multiple generative tasks while maintaining diversity

## Executive Summary
This paper addresses the exploration-exploitation dilemma in RL fine-tuning of generative models by proposing Adaptive Divergence Regularized Policy Optimization (ADRPO). The method dynamically adjusts regularization strength based on sample-specific advantage estimates, reducing regularization for high-advantage samples while strengthening it for poor samples. Applied to text-to-image generation, ADRPO fine-tunes a 2B parameter SD3 model that outperforms substantially larger models across attribute binding, semantic consistency, artistic style transfer, and compositional control while maintaining diversity.

The method generalizes to LLMs and multi-modal audio reasoning, enhancing GRPO with adaptive KL regularization. In LLM fine-tuning, ADRPO demonstrates emergent ability to escape local optima through active exploration, while in audio reasoning it enables a 7B model to outperform commercial models including Gemini 2.5 Pro and GPT-4o Audio. ADRPO establishes a superior Pareto frontier in reward-diversity trade-offs across continuous, discrete, and multi-modal generative paradigms.

## Method Summary
ADRPO addresses the exploration-exploitation dilemma in RL fine-tuning by dynamically adjusting regularization strength based on sample-specific advantage estimates. The total regularization coefficient β_tot = β_0 - A adjusts per-sample: reduce for high-advantage samples (aggressive optimization) while increasing for low-advantage samples (stability/diversity). The method applies to continuous domains (SD3 with Wasserstein-2 regularization), discrete domains (LLMs with KL regularization), and multi-modal domains (audio reasoning with KL regularization). For SD3, ADRPO fine-tunes a 2B parameter model using CLIP Score reward, ClipDiversity, ImageReward, and PicScore metrics. The method uses online sampling from current policy and LoRA for efficient fine-tuning.

## Key Results
- ADRPO fine-tunes a 2B SD3 model that outperforms substantially larger models (4.8B and 12B parameters) across attribute binding, semantic consistency, artistic style transfer, and compositional control
- The method maintains diversity while improving reward metrics, establishing a superior Pareto frontier in reward-diversity trade-offs
- Generalizes to LLMs and multi-modal audio reasoning, enabling a 7B model to outperform commercial models including Gemini 2.5 Pro and GPT-4o Audio

## Why This Works (Mechanism)
The adaptive regularization mechanism works by dynamically adjusting the trade-off between optimization and stability based on local advantage estimates. For samples with high advantage (better than average), β_tot is reduced to allow aggressive optimization without excessive regularization. For samples with low advantage, β_tot is increased to maintain stability and prevent diversity collapse. This sample-specific adaptation enables the model to explore promising regions while maintaining overall diversity, addressing the fundamental exploration-exploitation dilemma in RL fine-tuning.

## Foundational Learning
- **Advantage-based regularization**: Adjusts optimization aggressiveness based on relative performance. Why needed: Fixed regularization creates uniform trade-offs that can't adapt to local reward landscapes. Quick check: Verify advantage clipping bounds prevent extreme β_tot values.
- **Online policy sampling**: Samples from current policy during training rather than fixed dataset. Why needed: Enables exploration of new regions while maintaining diversity. Quick check: Monitor KL divergence between consecutive policies.
- **Wasserstein-2 regularization**: Measures velocity field distance for continuous generative models. Why needed: Provides smooth regularization for diffusion models' continuous latent spaces. Quick check: Verify reference velocity field stability across training.
- **Adaptive KL regularization**: Dynamically adjusts KL penalty in discrete and multi-modal settings. Why needed: Balances reward optimization with maintaining pre-trained knowledge. Quick check: Track entropy and diversity metrics during training.
- **LoRA fine-tuning**: Uses low-rank adaptation for efficient parameter updates. Why needed: Enables fine-tuning large models with limited compute resources. Quick check: Verify adapter weight norms remain stable.

## Architecture Onboarding

**Component map**: Policy π_θ -> Advantage A -> β_tot = β_0 - A_clipped -> ADRPO loss -> Optimizer -> Updated π_θ

**Critical path**: Online sampling → Reward computation → Advantage estimation → β_tot calculation → Loss computation → Parameter update

**Design tradeoffs**: Fixed vs. adaptive regularization (flexibility vs. stability), online vs. offline sampling (exploration vs. computational cost), Wasserstein-2 vs. KL regularization (continuous vs. discrete domains)

**Failure signatures**: Diversity collapse (reward increases while diversity metrics drop), reward hacking (spurious correlations), instability in late training stages

**3 first experiments**:
1. Baseline comparison: Run standard RL fine-tuning (GRPO/SLERP) vs. ADRPO on SD3 with identical hyperparameters to measure reward-diversity trade-offs
2. Advantage clipping sensitivity: Vary clipping bounds [-1,1] for SD3 to find optimal range that prevents extreme β_tot values
3. Long-horizon stability: Extend training to 20+ epochs to test ADRPO's ability to prevent collapse compared to fixed-regularization methods

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Critical hyperparameters including learning rate, batch size, optimizer choice, and LoRA rank/alpha are not specified, potentially affecting reproducibility
- Clipping thresholds appear arbitrary without justification for different generative modalities
- Reference velocity field computation and samples-per-prompt for advantage estimation lack detailed specification
- Claims about outperforming substantially larger models require careful scrutiny of comparison conditions

## Confidence
- **High confidence**: Core adaptive regularization mechanism and its theoretical motivation; reward-diversity improvements in continuous domains (SD3)
- **Medium confidence**: Generalization to discrete and multi-modal domains; claims about escaping local optima
- **Low confidence**: Performance claims against substantially larger models; detailed implementation specifics for different modalities

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary β_0, clipping bounds, and learning rate to establish robustness and identify optimal ranges for different generative modalities

2. **Ablation on advantage estimation**: Compare batch-mean baseline vs. learned value function V(c) vs. no baseline to quantify impact on stability and final performance

3. **Long-horizon training stability**: Extend training beyond typical RL fine-tuning schedules (10-20 epochs) to test the method's ability to prevent collapse and maintain diversity over extended optimization