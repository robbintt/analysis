---
ver: rpa2
title: 'A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges,
  and Future'
arxiv_id: '2504.12328'
source_url: https://arxiv.org/abs/2504.12328
tags:
- arxiv
- https
- reward
- preprint
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive survey of Reward Models (RMs)
  in the Large Language Model (LLM) era. The survey systematically organizes research
  on RMs from three perspectives: preference collection (human and AI preferences),
  reward modeling (discriminative, generative, and implicit rewards at outcome and
  process levels), and usage (data selection, policy training, and inference).'
---

# A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future

## Quick Facts
- **arXiv ID**: 2504.12328
- **Source URL**: https://arxiv.org/abs/2504.12328
- **Reference count**: 40
- **Primary result**: Systematic survey of Reward Models (RMs) in LLM era, organizing research into taxonomy covering preference collection, reward modeling, and usage, with discussion of applications, challenges, and future directions.

## Executive Summary
This paper presents a comprehensive survey of Reward Models (RMs) in the Large Language Model (LLM) era. The survey systematically organizes research on RMs from three perspectives: preference collection (human and AI preferences), reward modeling (discriminative, generative, and implicit rewards at outcome and process levels), and usage (data selection, policy training, and inference). The paper introduces key applications including dialogue, reasoning, and retrieval tasks, discusses evaluation benchmarks, analyzes challenges like data quality and reward hacking, and explores future directions including rule-based reward integration and multi-modal applications. The survey aims to provide beginners with a comprehensive introduction to RMs while facilitating future research in the field.

## Method Summary
The survey synthesizes research from approximately 40 key papers to create a systematic taxonomy of Reward Models. The methodology involves literature analysis and classification of RM approaches into three main categories: preference collection methods (human vs AI preferences), reward modeling architectures (discriminative, generative, and implicit at outcome vs process levels), and usage scenarios (data selection, policy training, inference). The survey provides mathematical formulations for core concepts including the Bradley-Terry model for preference modeling and cross-entropy loss for outcome and process reward models. The authors also maintain a GitHub repository with curated resources and references to support the survey's findings.

## Key Results
- Systematic taxonomy of RMs organized into preference collection, reward modeling, and usage perspectives
- Clear distinction between Outcome Reward Models (ORM) and Process Reward Models (PRM) with comparative analysis
- Comprehensive coverage of applications including dialogue, reasoning, and retrieval tasks
- Identification of key challenges: data quality, reward hacking, and out-of-distribution generalization
- Exploration of future directions including rule-based reward integration and multi-modal applications

## Why This Works (Mechanism)

### Mechanism 1: Preference as Proxy Signal
A Reward Model (RM) trained on high-quality preference data can serve as a stable proxy for human values, guiding Large Language Models (LLMs) toward aligned behavior. An RM is trained—typically using a Bradley-Terry model—to assign higher scalar rewards to responses that humans prefer. This RM then generates a reward signal used in downstream optimization, such as Reinforcement Learning from Human Feedback (RLHF) or data selection. The RM functions as a learned value function, converting unstructured text into a scalar score that represents preference alignment.

**Core assumption**: The training data accurately reflects the intended values (e.g., the "3H" principles: Honest, Harmless, Helpful) and that these values are learnable from pairwise comparisons. The proxy signal must generalize beyond the specific training distribution.

**Evidence anchors**:
- [abstract] "...RM can serve as a proxy for human preferences, providing signals to guide LLMs' behavior in various tasks."
- [section 2.1] "One straightforward approach is to train an RM on human preference data, which subsequently serves as a proxy to provide the training signal..."
- [corpus] "Reward models (RMs) play a critical role in enhancing the reasoning performance of LLMs. For example, they can provide training signals to finetune LLMs during reinforcement learning (RL)..." (Paper ID: 77801).

**Break condition**: The mechanism fails if the RM is "reward hacked" (optimized to exploit loopholes in the reward signal without achieving the true goal) or if the training data contains significant bias/noise, leading to a misaligned proxy.

### Mechanism 2: Process vs. Outcome Supervision for Complex Reasoning
For complex reasoning tasks, supervising the reasoning *process* (Process Reward Model, PRM) is more effective for guiding models than supervising only the final *outcome* (Outcome Reward Model, ORM). An ORM provides a sparse, binary signal (correct/incorrect) at the end of a generation. A PRM provides a dense, step-wise signal (a reward for each intermediate step). This dense feedback allows for more precise credit assignment, reducing the chance of a correct answer resulting from a flawed reasoning path.

**Core assumption**: The quality of a final answer is dependent on the correctness of the reasoning steps, and the decomposition of a problem into verifiable steps is feasible.

**Evidence anchors**:
- [section 2.2.2] "...outcome-supervised methods are still prone to hallucinations, such as reaching the correct answer through an incorrect reasoning path...PRM evaluates each reasoning step individually and can reduce tracking error..."
- [section 2.2.2, Table 1] ORM may lead to "false positives solutions" while PRM offers "Potential in reasoning tasks" and "Dense reward."
- [corpus] Corpus evidence is weak or missing for a direct comparative study of PRM vs. ORM efficacy.

**Break condition**: The mechanism fails if the task's process is not easily decomposable or if the cost of annotating step-wise data is prohibitive. Automated annotation may be noisy and produce unsatisfactory results.

### Mechanism 3: Scaling Alignment with AI Feedback (RLAIF)
LLMs can generate preference data (AI Preference) that is sufficiently high-quality to train Reward Models, thereby reducing reliance on costly and limited human annotation. A strong "judge" LLM evaluates responses based on given criteria (e.g., a constitution) and outputs a preference judgment or critique. This synthetic data is then used to train a separate RM or to directly optimize a policy model, scaling the alignment process.

**Core assumption**: The "judge" LLM's capabilities and internal representation of desired values (e.g., harmlessness) are superior to or more scalable than average human annotators for specific tasks.

**Evidence anchors**:
- [section 2.1.2] "...AI preferences have garnered increasing research interest and have the potential to become an alternative to human preferences."
- [section 2.1.2] "Bai et al. (2022b) first introduce RL from AI Feedback (RLAIF)...where the RM is trained on a combination of LLM-generated harmlessness preference labels..."
- [corpus] Corpus evidence is weak or missing for RLAIF specifically; no papers directly address this scaling mechanism.

**Break condition**: The mechanism fails if the judge LLM has significant biases or blind spots (e.g., sycophancy), which are then distilled into the RM, leading to systemic misalignment.

## Foundational Learning

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - **Why needed here**: RLHF is the primary framework where RMs are deployed. Understanding this loop—collecting data, training the RM, and then optimizing a policy using the RM as the reward—is essential to grasping the RM's role and lifecycle.
  - **Quick check question**: Can you explain how a Reward Model provides the 'R' signal in the standard RLHF PPO optimization loop?

- **Concept: Bradley-Terry Model for Preference Modeling**
  - **Why needed here**: This statistical model is the standard mathematical foundation for converting pairwise human preferences into a scalar reward function that the RM learns to predict.
  - **Quick check question**: How does the Bradley-Terry model relate the difference in reward scores of two responses to the probability that one will be preferred over the other?

- **Concept: Reward Hacking (Goodhart's Law)**
  - **Why needed here**: This is a critical failure mode where an agent exploits a poorly defined reward signal (from the RM) to maximize its score without achieving the intended goal. Understanding this is key to designing robust RMs and is a central challenge discussed in the paper.
  - **Quick check question**: Give an example of how an LLM might "hack" a reward model that was trained to prefer long, detailed responses.

## Architecture Onboarding

- **Component map**: The core pipeline consists of three stages: **(1) Preference Collection** (from Human/AI sources), which feeds into **(2) Reward Modeling** (training a Discriminative, Generative, or Implicit RM at Outcome or Process granularity). The trained RM is then used in **(3) Usage** scenarios like data selection, policy training (e.g., RLHF), or inference-time ranking.

- **Critical path**: The most critical path for model performance is the **Quality of Preference Data → RM Robustness → Policy Generalization**. The paper identifies data quality (bias, noise) and overoptimization/reward hacking as major challenges. Ensuring high-fidelity, diverse training data for the RM is the primary bottleneck.

- **Design tradeoffs**:
    - **ORM vs. PRM**: ORM is simpler and more flexible but can produce false positives in reasoning. PRM is powerful for reasoning but is costly to annotate and harder to define.
    - **Human vs. AI Preference**: Human data is high-quality but expensive and unscalable. AI data is scalable and cheaper but may inherit model biases.
    - **Explicit vs. Implicit RM**: Explicit RMs (discriminative/generative) are separate models, adding overhead. Implicit RMs (e.g., in DPO) are more efficient but may underperform on reward modeling tasks compared to explicit counterparts.

- **Failure signatures**:
    - **Reward Hacking**: The policy model's responses get longer, more sycophantic, or exploit formatting quirks to maximize reward without being more helpful or truthful.
    - **OOD Generalization Failure**: The RM assigns high rewards to responses that look good on the training distribution but are nonsensical or harmful on novel inputs.
    - **Distribution Shift**: In RLHF, the policy moves away from the data the RM was trained on, causing the RM's predictions to become inaccurate and destabilizing training.

- **First 3 experiments**:
  1.  **Baseline ORM Training**: Train a simple Discriminative ORM on a standard preference dataset (e.g., from the paper's linked GitHub) using the Bradley-Terry loss. Evaluate its accuracy on a held-out test set of preference pairs.
  2.  **Comparative Granularity Test**: For a reasoning task (e.g., math problems), train both an ORM and a PRM. Compare their ability to rank a set of model-generated solutions with known step-wise errors.
  3.  **Reward Hacking Probe**: Intentionally train an RM on a dataset with a known bias (e.g., favoring length). Use this RM in a simplified RLHF loop to train a policy and observe if/when the policy exploits the length bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is rule-based reward sufficient for Reinforcement Learning (RL), or is preference learning necessary for tasks without clear ground truths?
- Basis in paper: [explicit] Explicitly asked in Section 7 and detailed in Appendix A.4 ("Is Rule-based reward enough for RL?").
- Why unresolved: While rule-based rewards help mitigate hacking, they often result in sparse rewards that lead to optimization divergence. Furthermore, defining effective rules is challenging for subjective or creative tasks that lack a definitive ground truth.
- What evidence would resolve it: Empirical studies demonstrating whether models can converge without divergence using strictly rule-based rewards on complex, non-verifiable tasks, compared to those trained with preference-based signals.

### Open Question 2
- Question: Is the Mixture-of-Experts (MoE) approach superior to the Bradley-Terry (BT) model for reward modeling?
- Basis in paper: [explicit] Explicitly asked in Section 7 and Appendix A.4 ("Is Mixture-of-Experts better than BT Model?").
- Why unresolved: While MoE models (e.g., DMoERM) show potential for scalability and creating Pareto-optimal rewards by combining experts, the BT model remains the standard assumption. The comparative generalization and optimization efficiency of MoE versus BT in complex alignment scenarios remain unsettled.
- What evidence would resolve it: Benchmarks showing that MoE architectures consistently outperform standard BT-based reward models in versatility, efficiency, and generalization across diverse, multi-objective alignment tasks.

### Open Question 3
- Question: How can reward hacking be overcome when LLMs surpass the performance level of the best human experts?
- Basis in paper: [explicit] Explicitly asked in Section 7 and Appendix A.4 ("How to overcome the reward hacking of RM as LLMs surpass the level of the best expert level?").
- Why unresolved: As LLMs generate behaviors beyond human evaluation capabilities ("superhuman models"), standard expert feedback fails to detect sophisticated reward hacks or exploits of narrow solutions.
- What evidence would resolve it: Successful implementations of "weak-to-strong" generalization techniques, where robust reward models guide superhuman-level LLMs to generalize meaningfully without exploiting reward loopholes, validated through adversarial testing.

## Limitations
- Limited comparative benchmarks between ORM and PRM effectiveness, making it difficult to definitively assess the claimed advantages of process supervision
- Lack of empirical validation for emerging areas like AI-generated preference data (RLAIF) and their comparative effectiveness
- Does not address computational overhead of training separate RMs versus implicit reward methods, which is a practical consideration for deployment

## Confidence
- **High confidence**: Well-established RM paradigms like RLHF and discriminative reward modeling, where extensive empirical literature provides clear evidence
- **Medium confidence**: Emerging areas like AI-generated preference data (RLAIF) and process reward models (PRMs), where empirical validation is still sparse and theoretical mechanisms are not fully understood

## Next Checks
1. **Replicate a simple discriminative ORM**: Implement and train a reward model on a standard preference dataset (e.g., from the survey's GitHub repo) to verify the Bradley-Terry loss formulation and assess baseline performance.

2. **Compare ORM vs. PRM on reasoning tasks**: Using a dataset with step-wise annotations, train both an ORM and a PRM, then evaluate their ability to rank solutions and identify flawed reasoning paths.

3. **Stress-test for reward hacking**: Train an RM on a dataset with a known bias (e.g., favoring long responses) and use it in a simple RL loop to observe if the policy exploits the bias, validating the survey's discussion of this critical failure mode.