---
ver: rpa2
title: 'MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal
  Large Language Models'
arxiv_id: '2504.05782'
source_url: https://arxiv.org/abs/2504.05782
tags:
- reasoning
- multimodal
- knowledge
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MDK12-Bench is a large-scale, multi-disciplinary benchmark for
  evaluating multimodal reasoning in large language models using real-world K-12 examination
  data across six subjects. It features 141K instances with detailed knowledge annotations,
  difficulty levels, and cross-year partitions.
---

# MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2504.05782
- Source URL: https://arxiv.org/abs/2504.05782
- Reference count: 40
- 141K instances across 6 disciplines with 105K images, revealing current models achieve ~59.4% accuracy on multimodal reasoning tasks

## Executive Summary
MDK12-Bench is a large-scale benchmark designed to evaluate multimodal reasoning capabilities in large language models using real-world K-12 examination data. The benchmark covers six disciplines (math, physics, chemistry, biology, geography, information science) and features 141K instances with detailed knowledge annotations, difficulty levels, and cross-year partitions. A novel dynamic evaluation framework applies controlled perturbations to questions and images to mitigate data contamination issues and test model robustness beyond surface-level pattern matching.

## Method Summary
The benchmark evaluates MLLMs on 141K K-12 exam questions (77K text-only, 63K multimodal) with 6,827 knowledge point annotations across a 6-level hierarchy. Models process questions and images, with responses parsed by GPT-based interpreters for exact match or partial credit scoring. Dynamic evaluation applies textual bootstraps (word substitution, paraphrasing, type permutation) and visual bootstraps (spatial padding, color inversion, style transfer) while using GPT-based validation to ensure answer correctness is preserved. MDK12-Mini provides stratified 10% subsets for rapid evaluation.

## Key Results
- Top models like Gemini2-thinking and QVQ-72B achieve only 59.4% and 53.2% accuracy respectively on static benchmark
- Models show significant vulnerability to combined textual and visual bootstrapping, with accuracy drops exceeding 15% for some models
- Higher-performing models exhibit greater sensitivity to dynamic perturbations, suggesting reliance on contextual reasoning rather than robust understanding
- Accuracy patterns across exam years suggest potential training data contamination effects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic bootstrapping reduces data contamination effects by forcing models to reason rather than retrieve memorized patterns.
- **Mechanism:** Controlled perturbations (textual: word substitution, paraphrasing, type permutation; visual: spatial padding, color inversion, style transfer) preserve semantic correctness while altering surface features, creating out-of-distribution variants.
- **Core assumption:** Models depending on memorized question-answer pairs show larger accuracy drops under perturbation than those employing genuine reasoning. Assumption: bootstrapping transformations preserve answer validity without introducing spurious difficulty.
- **Evidence anchors:** Dynamic evaluation section describes bootstrapping pipeline; ablation studies show accuracy drops under perturbations; top models show highest sensitivity to combined perturbations.
- **Break condition:** If transformations inadvertently change answer-relevant semantics, or if contamination persists through concept-level memorization, the mechanism may not distinguish genuine reasoning.

### Mechanism 2
- **Claim:** Fine-grained knowledge point annotations enable targeted diagnostic evaluation of model weaknesses across specific concepts.
- **Mechanism:** Each instance links to a 6-level hierarchical knowledge structure (discipline → grade → curriculum → topic → meta-knowledge → key knowledge point), allowing accuracy aggregation by knowledge concept.
- **Core assumption:** Knowledge points are consistently annotated and truly independent; performance on one knowledge point reflects understanding of that concept rather than question-specific features.
- **Evidence anchors:** Knowledge structure details specify 6,827 points; accuracy analysis by knowledge point reveals systematic gaps in advanced geometry and biochemical processes.
- **Break condition:** If annotations are inconsistent, overlapping, or if questions require multiple knowledge points without clear decomposition, diagnostic granularity degrades.

### Mechanism 3
- **Claim:** Cross-year exam partitioning reveals temporal contamination patterns, as earlier exam data is more likely to appear in training corpora.
- **Mechanism:** Exam questions spanning 2016-2025 allow accuracy analysis across years to detect whether models perform better on older questions (potential training data leakage) versus newer questions.
- **Core assumption:** Publication date correlates with training data inclusion; newer exams are less likely to have been scraped into pretraining corpora.
- **Evidence anchors:** Year-by-year accuracy breakdown shows potential patterns suggesting earlier data inclusion; cross-year partition enables contamination detection.
- **Break condition:** If training data includes recent sources or if question difficulty varies systematically by year, temporal patterns may reflect confounds rather than contamination.

## Foundational Learning

- **Concept: Data Contamination in Benchmarks**
  - **Why needed here:** Explains why static benchmarks become unreliable when test data leaks into model training, motivating the dynamic evaluation approach.
  - **Quick check question:** If a model achieves 90% on a benchmark but drops to 60% when questions are paraphrased, what does this suggest about its evaluation?

- **Concept: Multimodal Reasoning vs. Perception**
  - **Why needed here:** Distinguishes high-order reasoning (step-by-step analysis, multi-step inference) from low-order tasks (object recognition, attribute extraction) that the benchmark targets.
  - **Quick check question:** What is the difference between identifying a triangle in an image versus solving a geometry proof that uses that triangle?

- **Concept: Bootstrapping for Test-Time Augmentation**
  - **Why needed here:** Explains the dynamic evaluation framework creates new test instances by transforming existing ones, not as training data augmentation but evaluation-time perturbation.
  - **Quick check question:** Why must bootstrapped test questions preserve answer correctness, and how does the paper verify this?

## Architecture Onboarding

- **Component map:** Data Layer (141K instances, 105K images, 6,827 knowledge points) -> Mini-Subsets (stratified 10% samples) -> Dynamic Evaluation Module (textual + visual bootstrapping with GPT validation) -> Scoring System (exact match + partial credit)

- **Critical path:** Start with MDK12-Mini evaluation to identify weak knowledge points (4,000-5,000 questions per difficulty) -> Map low-accuracy knowledge points to full dataset for targeted deep-dive -> Apply dynamic evaluation on sampled instances to test robustness vs. memorization -> Compare accuracy delta to assess contamination susceptibility

- **Design tradeoffs:** Breadth vs. depth (6 disciplines provide coverage but lack extreme specialization); Mini-subset vs. full evaluation (rapid iteration vs. comprehensive but computationally expensive); Dynamic evaluation cost (adds inference overhead but necessary for contamination mitigation)

- **Failure signatures:** Large accuracy drops on bootstrapped versions (>15%) suggest memorization rather than reasoning; high accuracy on easy tasks but collapse under perturbation suggests fragile pattern matching; consistent underperformance on specific knowledge points indicates systematic gaps

- **First 3 experiments:** 1) Establish baseline on MDK12-Mini across all three difficulty subsets, compare against reported baselines (59.4%, 53.2%); 2) Knowledge gap analysis: identify bottom 25 knowledge points by accuracy and extract full-set instances for targeted evaluation; 3) Dynamic evaluation stress test: sample 500 multimodal instances, apply combined bootstrapping, compare accuracy delta (if drop exceeds 15%, prioritize robustness interventions)

## Open Questions the Paper Calls Out

- **Open Question 1:** How can MLLMs be trained or architected to maintain robustness against simultaneous textual and visual bootstrapping perturbations without sacrificing accuracy on static benchmarks? The paper identifies vulnerability to perturbations but doesn't propose specific training methodologies to mitigate this sensitivity while preserving performance.

- **Open Question 2:** Why do high-performing, reasoning-oriented models exhibit disproportionate sensitivity (performance drop) to dynamic perturbations on "easy" tasks compared to "hard" tasks? The authors hypothesize easy tasks rely more on surface-level pattern matching easily disrupted by perturbations, but this mechanism isn't empirically validated.

- **Open Question 3:** Does reliance on GPT-based judge for rejecting incorrectly adapted instances introduce systematic bias regarding semantic preservation? The methodology uses GPT validation but doesn't verify if the judge's definition of "semantic alignment" matches human standards for reasoning difficulty.

## Limitations

- Dynamic evaluation validity remains inferential rather than empirically validated for contamination mitigation
- Knowledge point annotation quality and consistency across 6,827 points is uncertain
- Temporal contamination detection mechanism lacks direct empirical validation and may be confounded by systematic differences across years

## Confidence

- **High Confidence:** Benchmark scale (141K instances, 6 disciplines, 10-year span) and basic construction methodology are well-specified and reproducible; observation that current models struggle with multimodal reasoning is robust
- **Medium Confidence:** Dynamic evaluation framework's contamination mitigation effectiveness and diagnostic value of knowledge point annotations depend on assumptions requiring further validation
- **Low Confidence:** Temporal contamination detection mechanism lacks direct empirical support and may be confounded by systematic differences in question difficulty or curriculum changes

## Next Checks

1. **Bootstrapping Effectiveness Validation:** Create synthetic contamination scenario by deliberately including subset of benchmark questions in training data; measure whether dynamic evaluation accurately identifies contaminated subset through accuracy drops compared to static evaluation.

2. **Knowledge Point Consistency Audit:** Sample 100 randomly selected instances and have three independent annotators apply the 6-level knowledge structure; calculate inter-annotator agreement and identify knowledge points with high disagreement to assess annotation reliability.

3. **Temporal Contamination Ground Truth:** Obtain training data timestamps for evaluated models; cross-reference model training corpus with benchmark question publication dates to empirically validate whether earlier questions show higher contamination indicators.