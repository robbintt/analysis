---
ver: rpa2
title: How Well Do LLMs Understand Tunisian Arabic?
arxiv_id: '2511.16683'
source_url: https://arxiv.org/abs/2511.16683
tags:
- tunisian
- arabic
- dataset
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how well large language models (LLMs) understand
  Tunisian Arabic, a low-resource dialect spoken by over twelve million people. The
  research introduces a manually curated dataset of 100 examples, annotated for transliteration
  (Tunizi to Arabic script), translation (Tunisian Arabic to English), and sentiment
  classification (Positive, Negative, Neutral).
---

# How Well Do LLMs Understand Tunisian Arabic?

## Quick Facts
- **arXiv ID:** 2511.16683
- **Source URL:** https://arxiv.org/abs/2511.16683
- **Reference count:** 0
- **Primary result:** Gemini 2.5 Flash outperforms on Tunisian Arabic benchmark (CER 0.15, METEOR 0.45, strong sentiment accuracy)

## Executive Summary
This study evaluates large language models on Tunisian Arabic, a low-resource dialect spoken by over twelve million people. Using a manually curated dataset of 100 examples, the research benchmarks models on transliteration, translation, and sentiment classification tasks. Gemini 2.5 Flash achieved the best overall performance, with significantly lower error rates than competitors. The results highlight the challenges of modeling low-resource dialects and underscore the need for expanded datasets and tailored benchmarks to improve linguistic inclusivity in AI systems.

## Method Summary
The study benchmarks several state-of-the-art LLMs on three Tunisian Arabic tasks using a manually curated dataset of 100 social media examples. Tasks include transliteration (Tunizi to Arabic script), translation (Tunisian Arabic to English), and sentiment classification (Positive/Negative/Neutral). Models are evaluated zero-shot using metrics such as Character Error Rate, METEOR, and F1 scores. The dataset is publicly available at https://github.com/Mahdyy02/llm-tunisian-language.

## Key Results
- Gemini 2.5 Flash achieved CER 0.15 for transliteration, outperforming all other models
- Translation performance measured by METEOR score of 0.45 for top model
- Sentiment classification accuracy varied significantly across models, with top performers achieving ~0.60 F1

## Why This Works (Mechanism)
The evaluation demonstrates that modern LLMs can process Tunisian Arabic despite limited training data, with performance varying based on model architecture and training. Proprietary models show superior results due to larger training corpora and potential fine-tuning on Arabic dialects. The study's methodology provides a framework for benchmarking low-resource language understanding in AI systems.

## Foundational Learning

**Character Error Rate (CER)**: Measures transliteration accuracy by calculating the minimum edit distance between model output and reference text. Needed to quantify script conversion quality; quick check: compare CER across models on same dataset.

**METEOR Score**: Translation evaluation metric that accounts for synonymy and stemming, providing more nuanced assessment than BLEU. Needed for accurate translation quality measurement; quick check: validate METEOR scores against human judgments.

**F1 Score Variants**: Classification metrics including weighted, macro, and per-class F1 to evaluate sentiment analysis performance. Needed to capture different aspects of classification quality; quick check: ensure class balance is considered in weighted calculations.

## Architecture Onboarding

**Component Map**: Dataset -> LLM API -> Prompt Template -> Raw Output -> Evaluation Scripts -> Performance Metrics

**Critical Path**: Dataset preparation and annotation → Model API calls with prompts → Raw output collection → Metric computation → Result analysis

**Design Tradeoffs**: Zero-shot evaluation avoids fine-tuning complexity but may underestimate model potential; manual dataset ensures quality but limits scale; proprietary models show better performance but lack transparency

**Failure Signatures**: Large CER differences between models suggest capability gaps; inconsistent sentiment F1 across classes indicates bias or confusion; translation METEOR variance reflects linguistic understanding differences

**First Experiments**: 1) Validate evaluation scripts on small sample before full run, 2) Test prompt variations to establish sensitivity, 3) Compare results across different model versions to check stability

## Open Questions the Paper Calls Out

**Open Question 1**: How does LLM performance stabilize with a statistically robust, large-scale dataset? The current 100-example dataset limits statistical power, and future work plans expansion to support more conclusive comparisons.

**Open Question 2**: What factors contribute to proprietary models' superior performance? The paper suggests training data diversity, model architecture, and fine-tuning strategies may all play roles, but these factors remain unquantified in controlled settings.

**Open Question 3**: Can fine-tuning close the gap between open-source and proprietary models? The study highlights significant performance differences but hasn't tested whether supervised fine-tuning on expanded corpora can improve open-source model performance.

**Open Question 4**: How does lack of orthographic standardization and single-annotator labeling affect sentiment reliability? The manual annotation process and dialectal spelling variations may introduce subjectivity that limits classification accuracy.

## Limitations

- Small dataset size (100 examples) limits statistical power and generalizability
- Zero-shot evaluation may underestimate true model capabilities when fine-tuned
- Social media origin may introduce topical bias and informal language patterns

## Confidence

- **High Confidence**: Transliteration CER and translation METEOR for top models (clear metrics, reproducible methodology)
- **Medium Confidence**: Relative model rankings across tasks (though absolute numbers may vary with updates)
- **Low Confidence**: Claims about fundamental limitations of open-source models (performance differences may stem from evaluation setup)

## Next Checks

1. **Dataset Expansion**: Validate results on larger corpus (500+ examples) spanning diverse topics and formality levels
2. **Prompt Engineering Study**: Systematically test different prompt templates and parameters across all models to isolate prompting impact
3. **Fine-tuning Experiment**: Compare zero-shot results against fine-tuned open-source models using current dataset to assess performance gap reduction