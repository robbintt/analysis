---
ver: rpa2
title: Stochastic Sample Approximations of (Local) Moduli of Continuity
arxiv_id: '2509.15368'
source_url: https://arxiv.org/abs/2509.15368
tags:
- neural
- lemma
- definable
- lipschitz
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of estimating the modulus of
  local continuity (e.g., Lipschitz constant) for neural networks, a key quantitative
  property important for evaluating robustness and fairness. The authors revisit the
  connection between generalized derivatives and moduli of local continuity, and present
  a non-uniform stochastic sample approximation approach.
---

# Stochastic Sample Approximations of (Local) Moduli of Continuity

## Quick Facts
- arXiv ID: 2509.15368
- Source URL: https://arxiv.org/abs/2509.15368
- Reference count: 40
- Primary result: A stochastic sampling method for estimating Lipschitz constants of neural networks that scales to depth-11 networks (704 neurons) within 60 seconds, outperforming state-of-the-art methods.

## Executive Summary
This work addresses the challenge of estimating the local Lipschitz constant of neural networks, a critical property for robustness and fairness evaluation. The authors present a non-uniform stochastic sampling approach that treats the estimation problem as an infinity-armed bandit, using upper confidence bound (UCB) policies to adaptively focus samples where the estimate can be improved most. The method leverages Clarke subdifferentials computed via PyTorch's automatic differentiation and recursively subdivides the input domain to scale to higher dimensions.

## Method Summary
The method involves sampling elements of the Clarke subdifferential using PyTorch's automatic differentiation engine, introducing non-uniform sampling via UCB policies to adaptively focus samples where the estimate can be improved most. The core approach treats Lipschitz constant estimation as an infinity-armed bandit problem, where subregions of the input space are explored based on UCB scores that balance exploitation of known high-gradient regions with exploration of potentially better ones. The algorithm maintains an active set of polyhedral subregions, subdividing high-potential regions to increase sample density around critical boundary regions where the Lipschitz constant is determined.

## Key Results
- Scales to depth-11 networks (704 neurons) within 60-second time limit
- Relative errors often below 0.3% compared to LipMIP ground truth
- UCB-based approach strictly outperforms uniform sampling in all experiments
- Runtime significantly faster than LipSDP and LipMIP for deeper networks

## Why This Works (Mechanism)

### Mechanism 1: Generalized Derivative Supremum
The local Lipschitz constant of a definable neural network is theoretically equivalent to the supremum of the norms of its Clarke Jacobians over the input domain. By sampling inputs and computing the norm of the generalized Jacobian via auto-differentiation, the algorithm generates a stream of lower bounds that converge toward the true Lipschitz constant, provided the sampling is dense enough.

### Mechanism 2: Upper Confidence Bound (UCB) Adaptive Sampling
The algorithm treats the estimation problem as an infinity-armed bandit, assigning UCB scores to subregions based on maximum gradient found plus an exploration bonus. This directs samples away from flat regions and toward critical boundary regions where the Lipschitz constant is determined, outperforming uniform sampling by concentrating evaluations where the estimate can be improved most.

### Mechanism 3: Recursive Spatial Subdivision
Partitioning the input domain and recursively subdividing high-potential regions allows the stochastic estimator to scale to higher dimensions better than fixed-grid sampling. The algorithm maintains an active set of subregions, subdividing those with high UCB scores to increase sample density specifically around the kinks of the ReLU network where gradients are discontinuous.

## Foundational Learning

- **Concept: Clarke Subdifferential**
  - Why needed: Standard derivatives don't exist for ReLU networks at kinks; Clarke subdifferential provides valid generalized gradients
  - Quick check: Does PyTorch `backward()` return the full Clarke subdifferential or just a specific element? (Answer: Just a specific element)

- **Concept: Infinity-Armed Bandits**
  - Why needed: Theoretical framework for UCB sampling strategy, modeling infinitely many possible input points to pull
  - Quick check: How does "regret" in bandit setting translate to error in estimating Lipschitz constant?

- **Concept: Induced Matrix Norms**
  - Why needed: Lipschitz constant defined via norms on input/output spaces; final estimation relies on induced operator norm of Jacobian matrix
  - Quick check: For ℓ₂ norm on both input/output, what is induced matrix norm of Jacobian