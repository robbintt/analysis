---
ver: rpa2
title: Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward
  Offline RL
arxiv_id: '2506.20904'
source_url: https://arxiv.org/abs/2506.20904
tags:
- bpsa
- span
- have
- ntot
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops the first sample complexity bound for average-reward
  offline reinforcement learning (RL) that depends only on the target policy's bias
  span, a single-policy complexity measure. Previous work required additional uniform-policy
  complexity measures like the uniform mixing time, leading to vacuous bounds.
---

# Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward Offline RL

## Quick Facts
- **arXiv ID:** 2506.20904
- **Source URL:** https://arxiv.org/abs/2506.20904
- **Reference count:** 40
- **Primary result:** First optimal sample complexity bound for average-reward offline RL that depends only on the target policy's bias span

## Executive Summary
This paper establishes the first optimal sample complexity bound for average-reward offline reinforcement learning that depends solely on the target policy's bias span, rather than requiring additional uniform-policy complexity measures. The authors introduce a pessimistic value iteration algorithm enhanced by a novel quantile clipping technique, which enables the use of a sharper empirical-span-based penalty function. The algorithm achieves an optimal rate of O(√(S·bias span/m)) without requiring prior knowledge of any parameters, resolving a long-standing challenge in offline RL theory.

## Method Summary
The algorithm implements pessimistic discounted value iteration with quantile clipping. It sets the discount factor γ = 1 - 1/n_tot based on the total dataset size, avoiding parameter tuning. The key innovation is the quantile clipping operator T_β that clips the top β-percentile of next-state values before computing the variance term in the penalty. This ensures the penalty function only penalizes relative differences between states (span) rather than absolute cumulative rewards. The algorithm runs value iteration for K ≈ n_tot steps and outputs the greedy policy with respect to the final Q-values.

## Key Results
- Achieves optimal sample complexity O(√(S·bias span/m)) for average-reward offline RL
- First algorithm requiring only single-policy complexity (bias span) rather than uniform-policy measures
- Demonstrates that learning requires data beyond the stationary distribution of the target policy
- Introduces parameter-free implementation with theoretical guarantees
- Provides matching lower bounds confirming optimality of the sample complexity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Quantile clipping enables a penalty function that scales with the bias span rather than the value magnitude.
- **Mechanism:** Standard penalties scale with 1/(1-γ), which explodes as γ→1. Quantile clipping T_β clips the top β-percentile of values, ensuring the penalty only measures relative differences (span) not absolute values.
- **Core assumption:** The target policy has finite bias span ||h^π^star||_{span}.
- **Evidence anchors:** Abstract states the quantile clipping technique enables "sharper empirical-span-based penalty function."
- **Break condition:** If the bias span is extremely large or β is set too aggressively, estimates may become biased or penalties insufficient.

### Mechanism 2
- **Claim:** The algorithm achieves near-optimal sample complexity by requiring transient coverage beyond stationary distribution.
- **Mechanism:** Theorem 3.3 proves learning requires data from transient states where μ^π^star(s) = 0 but which are necessary for recovery if the agent deviates. The algorithm needs n(s,π^star(s)) ≈ T_hit^2 samples from these transient pairs.
- **Core assumption:** The MDP is weakly communicating and dataset contains sufficient transient samples.
- **Evidence anchors:** Abstract states "learning under these conditions requires data beyond the stationary distribution."
- **Break condition:** If dataset strictly covers only stationary distribution and excludes transient states, algorithm fails to learn with vanishing suboptimality.

### Mechanism 3
- **Claim:** Discounted reduction approach with γ = 1 - 1/n_tot solves average-reward problem without parameter knowledge.
- **Mechanism:** Sets effective horizon to dataset size n_tot. Unlike previous work requiring bias span or mixing time, this reduction allows parameter-free implementation while span-regularization handles scaling issues.
- **Core assumption:** Total dataset size n_tot is large enough to capture necessary policy dynamics.
- **Evidence anchors:** Section 3.2 states "Our algorithm also does not require any prior parameter knowledge."
- **Break condition:** If n_tot is very small, effective horizon becomes too short to capture long-term average reward properties.

## Foundational Learning

- **Concept:** Average-Reward MDPs & Bias Span
  - **Why needed here:** Unlike discounted RL where value is bounded by 1/(1-γ), average-reward values can be unbounded. The bias span (difference between best and worst states) is the critical complexity measure.
  - **Quick check question:** Can you explain why a standard "max-value" penalty fails in average-reward RL but a "span-based" penalty succeeds?

- **Concept:** Pessimistic Value Iteration (Pessimism Principle)
  - **Why needed here:** In offline RL, you cannot explore. Pessimism under-estimates values where data is sparse. This paper innovates by how uncertainty is calculated (using span).
  - **Quick check question:** If an action has high uncertainty, should a pessimistic algorithm increase or decrease its estimated value relative to the empirical mean?

- **Concept:** Weakly Communicating MDPs
  - **Why needed here:** Most general MDP structure where optimal policy exists. Allows for transient states (visited briefly or never in steady state), central to the paper's transient coverage requirement.
  - **Quick check question:** Does the algorithm require data from every state, or only states visited in the stationary distribution of the target policy? (Answer: It requires transient states too).

## Architecture Onboarding

- **Component map:** Dataset D → Empirical Model P̂ → Quantile Clipping Module → Penalty Calculator → Pessimistic Operator → Value Iteration Loop → Output Greedy Policy
- **Critical path:** The clipping mechanism (Component 3) is the novel critical path. If implemented incorrectly (e.g., clipping wrong tail or using wrong quantile parameter), the constant-shift property is lost and algorithm may diverge.
- **Design tradeoffs:**
  - γ Selection: Parameter-free (γ = 1 - 1/n_tot) but computationally expensive. Shorter horizons run faster but lose theoretical guarantees.
  - Clipping Threshold β: Too large clips too much signal (high bias), too small penalty explodes (high variance). Paper sets β ∝ log(n_tot)/n(s,a).
- **Failure signatures:**
  - "Vacuous Bound" Failure: Standard Bernstein penalties instead of clipped version cause performance to degrade as dataset size increases.
  - "Missing Recovery" Failure: Dataset containing only stationary distribution causes non-vanishing suboptimality gap.
- **First 3 experiments:**
  1. Transient Coverage Ablation: Construct MDP with transient optimal state, run algorithm on datasets with/without transient coverage, verify performance collapses without coverage.
  2. Scaling Law Check: Run on MDPs with increasing bias span, plot suboptimality gap against √(S·span/m), verify linear relationship.
  3. Baseline Comparison (Horizon): Compare against fixed-γ pessimistic VI vs dynamic γ, show baseline performance plateaus while this algorithm improves.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can bias-span-dependent sample complexity be extended to function approximation settings to remove dependence on S?
  - **Basis:** Conclusion states main limitation is focus on tabular setting, important direction is extending to function approximation.
  - **Why unresolved:** Current analysis relies on empirical Bernstein inequalities and covering numbers over state space.
  - **What evidence would resolve it:** Algorithm for linear MDPs with sample complexity bound depending on feature dimension d and target policy span.

- **Open Question 2:** What structural assumptions could enable learning without transient coverage?
  - **Basis:** Conclusion notes interesting direction is exploring additional assumptions to circumvent transient coverage requirement.
  - **Why unresolved:** Theorem 3.3 proves under standard assumptions with only stationary-distribution coverage, learning is impossible.
  - **What evidence would resolve it:** Identification of specific structural property under which Theorem 3.3 fails and stationary-distribution coverage suffices.

- **Open Question 3:** Can optimal statistical rates be achieved with sublinear computational complexity?
  - **Basis:** Paper sets γ = 1 - 1/n_tot requiring K ≈ n_tot steps, acknowledges this is suboptimal computationally.
  - **Why unresolved:** Discounted reduction links approximation quality to effective horizon, forcing linear dependence on dataset size.
  - **What evidence would resolve it:** Algorithm achieving Õ(√(S/m)) error rate with running time O(poly(S,A,log n_tot)).

## Limitations
- Focuses on tabular settings, limiting applicability to large state spaces
- Requires data coverage beyond the target policy's stationary distribution, which may be difficult to ensure
- Computational complexity scales linearly with dataset size due to long horizon requirements

## Confidence

**High confidence:** Core theoretical results (Theorem 3.2 and 3.3) are well-supported by proofs and follow logically from quantile clipping mechanism enabling span-based penalties.

**Medium confidence:** Practical implications of transient coverage requirement - specifically how much extra data is needed and how to ensure this in real-world datasets.

**Low confidence:** Extension to large state spaces or function approximation settings, which is not addressed in the paper.

## Next Validation Checks

1. **Transient Coverage Experiment:** Construct controlled MDP where optimal policy has transient state not in its stationary distribution. Run algorithm on datasets with 0%, 50%, 100% transient coverage and verify performance degrades as coverage decreases.

2. **Parameter-Free Implementation Test:** Implement algorithm with theoretical clipping parameter β ∝ log(n_tot)/n(s,a) and test across multiple MDPs without manual tuning. Verify performance is competitive with tuned versions.

3. **Scaling Law Verification:** Generate MDP family with systematically varying bias spans and state-action visitation frequencies. Plot empirical suboptimality gap against theoretical scaling √(S·bias_span/m) to verify Õ(√(S·bias_span/m)) rate holds across diverse environments.