---
ver: rpa2
title: On Spectral Properties of Gradient-based Explanation Methods
arxiv_id: '2508.10595'
source_url: https://arxiv.org/abs/2508.10595
tags:
- arxiv
- perturbation
- explanation
- spectral
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a spectral analysis of gradient-based explanation
  methods for deep neural networks, revealing a pervasive spectral bias in gradient
  explanations. The authors introduce a novel probabilistic framework to analyze how
  gradient operators and perturbations interact to create band-pass filters in the
  frequency domain.
---

# On Spectral Properties of Gradient-based Explanation Methods

## Quick Facts
- **arXiv ID:** 2508.10595
- **Source URL:** https://arxiv.org/abs/2508.10595
- **Reference count:** 40
- **Primary result:** Spectral analysis reveals gradient-based explanations suffer from spectral bias, and proposes SpectralLens aggregation method to improve consistency.

## Executive Summary
This paper provides a theoretical analysis of gradient-based explanation methods for deep neural networks, revealing that they suffer from spectral bias due to the interaction between gradient operators and perturbations. The authors demonstrate that gradient operators act as high-pass filters while perturbations act as low-pass filters, creating band-pass filters in the frequency domain. This leads to inconsistencies in explanations depending on the perturbation scale. The paper introduces SpectralLens, an aggregation method that combines information across frequency bands, and provides theoretical justification for using squared gradients over linear gradients to avoid destructive interference.

## Method Summary
The paper analyzes gradient-based explanations through spectral analysis, showing that gradient operators function as high-pass filters while perturbation kernels act as low-pass filters, creating band-pass filters in the frequency domain. Two main solutions are proposed: (1) determining an optimal perturbation scale using cosine similarity between the perturbation kernel and classifier's power spectral density, and (2) SpectralLens, which aggregates explanations across multiple frequency bands using a uniform prior over perturbation scales. The method uses convex combination interpolation for perturbations and computes squared gradients to avoid destructive interference. Experimental evaluation uses pixel removal strategies on ImageNet and Food101 datasets with ResNet50 and ViT-B/16 models.

## Key Results
- Gradient-based explainers combined with perturbations function as band-pass filters, creating spectral bias where attribution depends on feature frequency
- Squared gradients provide more consistent attributions than linear gradients by utilizing Power Spectral Density to avoid destructive interference
- SpectralLens improves robustness against inconsistencies while maintaining or enhancing quantitative performance metrics on ImageNet and Food101

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient-based explainers combined with perturbations function as band-pass filters, creating a spectral bias where attribution depends on feature frequency rather than just magnitude.
- Mechanism: The gradient operator $\nabla$ acts as a high-pass filter, amplifying high-frequency noise, while perturbation kernels $p(\tilde{x})$ act as low-pass filters, dampening these frequencies. The resulting explanation is a product of these transfer functions, which forms a band-pass filter sensitive to the perturbation scale $\sigma$.
- Core assumption: The classifier function behaves locally as a linear time-invariant system around the input point to allow valid spectral decomposition.
- Evidence anchors: [abstract] reveals spectral bias; [section 3.2] Theorem 2 and Theorem 1; [corpus] supports spectral decomposition utility.
- Break condition: If the model's decision boundary is highly discontinuous or non-smooth at the input, the Fourier basis may not efficiently capture the local geometry, breaking the filter analogy.

### Mechanism 2
- Claim: Squared gradients ($\nabla f(x)^2$) provide more consistent attributions than linear gradients by utilizing Power Spectral Density (PSD) to avoid destructive interference.
- Mechanism: Linear gradients rely on the interaction of real and imaginary Fourier components, causing attributions to vary or cancel out based on kernel alignment. Squared gradients use the PSD, which sums frequency magnitudes, ensuring non-negative values and eliminating phase-dependent cancellation.
- Core assumption: The "importance" of a feature is better reflected by the magnitude of its spectral contribution than its phase relationship with the perturbation kernel.
- Evidence anchors: [abstract] sheds light on common design choices; [section 3.1] Remark 5; [corpus] discusses noisiness of gradients.
- Break condition: If an application strictly requires distinguishing between positive and negative contributions, squaring eliminates this directional information.

### Mechanism 3
- Claim: Aggregating explanations across perturbation scales (SpectralLens) improves robustness by capturing contributions from multiple frequency bands.
- Mechanism: Since a single scale $\sigma$ acts as a specific band-pass filter, it may miss features outside that band. SpectralLens integrates squared gradients over a prior distribution of scales, superimposing multiple band-pass filters to cover a wider spectral range.
- Core assumption: The "optimal" perturbation scale varies across images and features, and an ensemble approach is superior to a heuristic fixed scale.
- Evidence anchors: [abstract] combines information across frequency bands; [section 4.2] Definition 3; [corpus] validates perturbation mask refinement.
- Break condition: If computational budget disallows multiple forward/backward passes at different noise scales, the method is practically infeasible.

## Foundational Learning

- **Fourier Transform & Spectral Density**: Why needed here: The paper reframes gradient attributions as operations in the frequency domain. Understanding how a signal decomposes into frequencies is required to grasp why gradients act as high-pass filters and noise as low-pass filters. Quick check: If a signal is composed of slow-moving trends (low freq) and static noise (high freq), which filter isolates the trend?
- **Perturbation-based Explainability (SmoothGrad)**: Why needed here: The analysis specifically targets how adding noise to inputs changes gradient explanations. You need to know the standard baseline to understand the problem being solved. Quick check: In standard SmoothGrad, what is the effect of increasing the standard deviation $\sigma$ of the noise on the visual smoothness of the map?
- **Power Spectral Density (PSD)**: Why needed here: The paper theoretically justifies using squared gradients by linking them to the PSD of the classifier. Understanding that PSD represents signal power distribution over frequency is key to Mechanism 2. Quick check: Does the Power Spectral Density contain information about the phase of a signal?

## Architecture Onboarding

- **Component map**: Input $x$ -> Explainer (Gradient $\nabla_x f$ + Perturbation $p(\tilde{x}|x)$) -> Aggregator (Expectation $E_p[\cdot]$) -> Output (Attribution map)
- **Critical path**: 1. Determine frequency band of interest (max Cosine Similarity or Uniform Prior). 2. Sample perturbed inputs $\tilde{x}$ based on scale $\sigma$. 3. Compute squared gradients $(\nabla f(\tilde{x}))^2$ for each sample. 4. Aggregate results (average) to produce final explanation.
- **Design tradeoffs**: Sign vs. Stability (linear gradients preserve direction but are inconsistent; squared gradients are stable but lose sign); Specificity vs. Robustness (tuning specific $\sigma$ adapts to dataset but requires computation; Uniform Prior is generic but heavier).
- **Failure signatures**: Visual Noise (gradient high-pass filter dominating - $\sigma$ too small); Over-smoothing (perturbation low-pass filter dominating - $\sigma$ too large); Ranking Flips (contradictory attributions when changing $\sigma$ slightly).
- **First 3 experiments**: 1. Spectral Verification - plot frequency response $\|\omega\|^2 e^{-8\pi^2\sigma^2\|\omega\|^2}$ for different $\sigma$ values. 2. Consistency Stress Test - generate explanations with $\sigma=0.1$ and $\sigma=0.5$, calculate Rank Correlation. 3. Deletion/Insertion Benchmark - compare MoRF deletion scores of SpectralLens against VanillaGrad and SmoothGrad.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the spectral bias inherent in pixel removal strategies influence the reliability of quantitative benchmarks, and can spectral-aware metrics be developed to correct for this bias?
- Basis in paper: [explicit] The authors explicitly defer "The spectral analysis of pixel removal strategy and possible solutions... to future research" (Section 5) and state in the appendix "We leave the analysis of existing evaluation methods from a spectral point of view for future work" (Appendix E.3).
- Why unresolved: The paper identifies that pixel removal acts as a spatial low-pass filter which interacts with the explanation's frequency bands, but doesn't formalize this interaction or propose a solution.
- What evidence would resolve it: Theoretical derivation of spectral transfer function for removal strategies and empirical results showing spectral-corrected metrics rank methods differently.

### Open Question 2
- Question: Can the spectral analysis framework be extended to function-space perturbation methods (e.g., NoiseGrad, SAM), and does this extension reveal a theoretical connection to the double descent phenomenon?
- Basis in paper: [explicit] The text states, "We defer this aspect [function space perturbation] to future investigations. It can be noted, however, that extending this work to function space perturbation will connect it to the literature on double descent" (Section 6).
- Why unresolved: The current probabilistic representation relies on input-space convolutions; mapping this to weight-space perturbations requires different mathematical treatment not provided.
- What evidence would resolve it: Derivation of spectral properties of weight perturbations and experiments showing correlation between spectral behavior and test error peaks associated with double descent.

### Open Question 3
- Question: Can the frequency attributions identified by ArgLens be generalized across a dataset to identify robust class-specific features, rather than providing only image-specific localization?
- Basis in paper: [explicit] The authors note that "Identifying generalizable attributions across images remains a subject for future investigation" because "high-frequency regions in one image may correspond to low-frequency regions in another image" (Section 5).
- Why unresolved: The ArgLens method currently produces a pixel-wise map dependent on local signal properties of specific input image, lacking mechanism to align frequencies semantically across samples.
- What evidence would resolve it: Statistical analysis showing ArgLens values cluster consistently for same semantic concepts across multiple images, or method that maps local frequencies to global spectrum for dataset.

## Limitations
- The assumption that classifiers behave as linear time-invariant systems locally may not hold for highly non-smooth decision boundaries
- The method's computational overhead (requiring multiple forward/backward passes) could limit practical deployment in resource-constrained settings
- While the analysis explains frequency-dependent attribution, it doesn't address potential class-specific spectral biases that might affect certain types of images differently

## Confidence

- **High**: The core claim that gradient operators act as high-pass filters and perturbations as low-pass filters is mathematically rigorous and well-supported by the theorems.
- **Medium**: The practical effectiveness of SpectralLens in improving explanation consistency is demonstrated experimentally, but the theoretical justification for uniform scale aggregation could be more rigorous.
- **Medium**: The recommendation for squared gradients is theoretically justified through PSD analysis, but the trade-off with losing directional information should be more explicitly acknowledged.

## Next Checks

1. **Cross-architecture validation**: Test SpectralLens on architectures with different inductive biases (CNN vs Transformer) to verify if the spectral properties generalize beyond ResNet and ViT.
2. **Out-of-distribution robustness**: Evaluate whether explanations remain consistent when images are perturbed beyond the determined optimal scale, confirming the entropy-based stopping criterion.
3. **Computational efficiency analysis**: Benchmark the runtime overhead of SpectralLens against standard SmoothGrad across different hardware configurations to quantify the practical cost of improved consistency.