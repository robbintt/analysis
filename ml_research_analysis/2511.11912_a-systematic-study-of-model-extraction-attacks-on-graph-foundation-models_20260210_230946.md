---
ver: rpa2
title: A Systematic Study of Model Extraction Attacks on Graph Foundation Models
arxiv_id: '2511.11912'
source_url: https://arxiv.org/abs/2511.11912
tags:
- graph
- victim
- attacker
- encoder
- surrogate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic study of model extraction
  attacks (MEAs) on Graph Foundation Models (GFMs). GFMs are large-scale multimodal
  models that unify graph and text understanding, enabling zero-shot inference across
  domains.
---

# A Systematic Study of Model Extraction Attacks on Graph Foundation Models

## Quick Facts
- arXiv ID: 2511.11912
- Source URL: https://arxiv.org/abs/2511.11912
- Reference count: 40
- This paper presents the first systematic study of model extraction attacks (MEAs) on Graph Foundation Models (GFMs).

## Executive Summary
This paper presents the first systematic study of model extraction attacks (MEAs) on Graph Foundation Models (GFMs). GFMs are large-scale multimodal models that unify graph and text understanding, enabling zero-shot inference across domains. Unlike prior work on small-scale GNNs, this study explores how attackers can steal GFMs' cross-domain knowledge without access to proprietary graph-text training data. The authors introduce a novel one-loss embedding-regression attack that trains a surrogate graph encoder to match victim model embeddings using only a simple supervised loss. Theoretical analysis shows that matching embeddings suffices to preserve zero-shot inference via the shared text encoder. Experiments on seven datasets demonstrate that attackers can extract high-fidelity surrogates with only ~0.07% of the victim's training cost, achieving near-victim accuracy (average drop of just 0.0015). The study reveals that GFMs are highly vulnerable to extraction even under architectural mismatch, limited queries, partial node access, and training data discrepancies. These findings underscore the need for deployment-aware security defenses in large-scale multimodal graph learning systems.

## Method Summary
The authors introduce a one-loss embedding-regression attack where a surrogate graph encoder is trained to minimize MSE loss between its embeddings and the victim's embeddings on queried subgraphs. The victim GFM uses a GraphGPS transformer for graph encoding paired with a frozen MiniLM text encoder. The attacker queries the victim's API to obtain graph embeddings, then trains a surrogate (GAT, GCN, or GPS-lite) using only the MSE loss. Theoretical analysis shows that matching embeddings preserves zero-shot inference through the shared text encoder. Experiments evaluate extraction under architectural mismatch, limited queries, partial node access, and training data discrepancies across seven datasets.

## Key Results
- Attackers can extract high-fidelity surrogates with only ~0.07% of the victim's training cost
- Surrogates achieve near-victim accuracy with average drop of just 0.0015
- Lightweight GNNs (GAT, GCN) successfully replicate GraphGPS behavior despite architectural mismatch
- Extraction quality depends more on query distribution similarity than on the number of queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A single supervised MSE loss is sufficient to extract both the victim's embedding space and its zero-shot inference capability.
- Mechanism: The surrogate graph encoder is trained to minimize ||g_attack(v_i) - g_victim(v_i)||² on queried subgraphs. Because the text encoder is frozen and shared, matching graph embeddings implicitly preserves alignment with the text encoder—no contrastive loss or graph-text pairs are required.
- Core assumption: The attacker has black-box query access to the victim's graph encoder outputs, and the text encoder is a standard open-source model (e.g., MiniLM) that remains unchanged during victim pretraining.
- Evidence anchors:
  - [abstract] "We introduce a novel one-loss embedding-regression attack that trains a surrogate graph encoder to match victim model embeddings using only a simple supervised loss."
  - [Section 4.2, Lemma 1] Provides the theoretical bound: if surrogate embeddings match victim embeddings on queried nodes and training data covers the embedding space, zero-shot predictions will agree.
  - [corpus] Related work (e.g., "Towards Graph Foundation Models: A Transferability Perspective") confirms GFMs use frozen text encoders aligned with trainable graph encoders, but does not address MEA mechanisms directly.
- Break Condition: If the text encoder differs between victim and attacker, or if the victim perturbs or truncates returned embeddings, alignment may not transfer.

### Mechanism 2
- Claim: Lightweight surrogate architectures (GAT, GCN) can replicate victim behavior even under architectural mismatch.
- Mechanism: The victim's GraphGPS transformer (128M parameters) is approximated by smaller GNNs (0.9–11M parameters) via embedding regression. The surrogate learns a functional mapping that mimics outputs without replicating internal structure.
- Core assumption: The surrogate has sufficient capacity to approximate the victim's embedding function on the queried distribution; the query data is representative of evaluation graphs.
- Evidence anchors:
  - [Section 5.2.1, Table 2] GAT surrogate achieves average accuracy drop of only 0.15%; GCN surrogate trails by 0.66%.
  - [Section 5.2.2] "GNNs can successfully inherit and transfer domain-level graph-text knowledge from a GFM... despite their simplicity."
  - [corpus] No direct corpus evidence on architectural mismatch in MEA; this appears novel to this paper.
- Break Condition: If the surrogate is severely underparameterized relative to the complexity of the victim's embedding function, fidelity degrades.

### Mechanism 3
- Claim: Domain-specific extraction is more effective when query graphs match the evaluation domain distribution.
- Mechanism: Querying with graphs from the same domain (e.g., academic citation networks) yields embeddings that better cover the relevant region of the victim's embedding space, improving surrogate generalization within that domain.
- Core assumption: The victim's embedding space exhibits domain-specific clustering; within-domain query nodes provide denser coverage for that region.
- Evidence anchors:
  - [Section 5.2.2, Table 3] Surrogates trained on academic-domain queries outperform those trained on social/e-commerce queries when evaluated on academic datasets (Cora, CiteSeer, WikiCS).
  - [Section 5.2.3] "Querying only PubMed leads to large drops in accuracy and fidelity... extraction quality depends more on the similarity between the attacker's query distribution and the evaluation graphs."
  - [corpus] Corpus papers discuss transferability in GFMs but do not address domain-specific MEA dynamics.
- Break Condition: If evaluation graphs lie far from all queried domains, surrogate fidelity declines.

## Foundational Learning

- Concept: **Contrastive pretraining (graph-text alignment)**
  - Why needed here: GFMs are pretrained by aligning graph and text encoders via contrastive loss. Understanding this explains why embedding regression alone preserves zero-shot capability—the text encoder and its alignment are fixed.
  - Quick check question: Can you explain why a frozen text encoder means the surrogate only needs to match graph embeddings, not relearn alignment?

- Concept: **Zero-shot inference via embedding similarity**
  - Why needed here: The paper's core claim is that surrogates inherit zero-shot ability. This requires understanding how classification works by computing similarity between graph and label embeddings.
  - Quick check question: Given a node embedding g(v) and label embeddings Z_k, how is the predicted label determined?

- Concept: **Message-passing GNNs vs. Graph Transformers**
  - Why needed here: The victim uses GraphGPS (a transformer); attackers use GAT/GCN. Understanding architectural differences helps interpret why lightweight surrogates still succeed and where they might fail.
  - Quick check question: What structural information might a GraphTransformer capture that a 2-hop GCN cannot?

## Architecture Onboarding

- Component map:
  - Victim pipeline: Text-attributed graph → Subgraph extraction → Graph encoder (trainable, 12-layer GPS) → Embedding → Similarity with frozen text encoder (MiniLM)
  - Attacker pipeline: Public graph data → Subgraph extraction → Query victim API → Collect embeddings → Train surrogate encoder (GCN/GAT/GPS-lite) via MSE loss → Pair with same frozen text encoder for inference

- Critical path:
  1. Identify accessible graph datasets for queries (public TAGs or target graph itself)
  2. Query victim encoder to obtain embedding-label pairs
  3. Train surrogate with L_MSE = Σ||g_attack(v_i) - g_victim(v_i)||²
  4. Evaluate surrogate fidelity and zero-shot accuracy on unseen graphs

- Design tradeoffs:
  - Larger surrogate (GPS-lite) → higher fidelity, longer training
  - Smaller surrogate (GCN) → faster training, slightly lower accuracy
  - More queries → better coverage, higher API cost/risk of detection
  - Domain-matched queries → better in-domain generalization; mixed queries → broader but shallower coverage

- Failure signatures:
  - Low fidelity (>20% prediction disagreement): likely insufficient query coverage or severe underparameterization
  - High fidelity but low accuracy: surrogate matches victim but victim itself performs poorly on target domain (query-evaluation mismatch)
  - Training instability: check embedding normalization; unnormalized embeddings cause gradient issues

- First 3 experiments:
  1. Reproduce Attack-1 baseline: Query victim on OGBN-ArXiv + ArXiv-2023 + PubMed; train GCN surrogate; evaluate on Cora/CiteSeer. Expect fidelity >80%.
  2. Ablate query budget: Repeat with 100, 250, 500, 1000 queries from OGBN-ArXiv alone. Plot fidelity vs. query count (cf. Figure 3).
  3. Test domain mismatch: Train surrogate on Reddit (social); evaluate on academic datasets. Expect fidelity drop compared to domain-matched training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can attackers successfully extract Graph Foundation Models (GFMs) when the victim API exposes only similarity scores instead of graph embeddings?
- Basis in paper: [explicit] Section 6.2 lists "studying MEA when the GFM exposes only similarity scores rather than graph embeddings" as a key future direction.
- Why unresolved: The proposed one-loss embedding-regression attack relies on minimizing the MSE between the surrogate's output and the victim's high-dimensional embedding, which is impossible with scalar similarity scores alone.
- What evidence would resolve it: A novel learning objective that operates on partial or low-dimensional feedback, demonstrated by successfully training a surrogate model using only similarity scores.

### Open Question 2
- Question: Can active, feedback-driven querying strategies improve extraction fidelity under tight query budgets?
- Basis in paper: [explicit] Section 6.2 suggests "developing feedback-driven querying, where future queries are chosen based on current surrogate errors," as a method to enhance efficiency.
- Why unresolved: The current study utilizes random sampling or mixed-dataset sampling; it does not evaluate adaptive strategies that prioritize queries where the surrogate disagrees with the victim.
- What evidence would resolve it: Experimental results showing that an active extraction loop achieves higher fidelity than random sampling when restricted to budgets of 100–500 queries.

### Open Question 3
- Question: Does the synthetic graph extraction method generalize to graphs with heterophilous structure?
- Basis in paper: [inferred] Section 5.2.5 notes a limitation: the synthesis rule "assumes moderate homophily; graphs with weak or heterophilous structure may require more expressive imputers."
- Why unresolved: The attacker synthesizes missing node features by averaging known neighbors, a heuristic that fails if connected nodes possess dissimilar attributes.
- What evidence would resolve it: Successful extraction results on heterophilous benchmark datasets using a modified synthesis technique that does not rely on neighbor similarity.

## Limitations

- The attack assumes the text encoder remains frozen and identical between victim and attacker, which may not generalize to GFMs using adaptive or proprietary text encoders.
- The evaluation focuses on node classification tasks; transferability to link prediction or graph-level tasks is not explored.
- The attack relies on black-box access to embeddings without knowledge of the victim's training data distribution, but it remains unclear how well the surrogate generalizes to out-of-distribution graphs.

## Confidence

- **High Confidence**: The claim that a simple MSE loss on embeddings suffices to preserve zero-shot inference, supported by theoretical analysis (Lemma 1) and consistent experimental results across seven datasets.
- **Medium Confidence**: The assertion that lightweight GNNs (GAT/GCN) can match complex GraphGPS behavior, as results show reasonable fidelity but with non-trivial drops in accuracy under architectural mismatch.
- **Medium Confidence**: The observation that domain-matched queries yield better extraction quality, though the mechanism (embedding space clustering) is plausible but not directly measured.

## Next Checks

1. **Text Encoder Sensitivity**: Test extraction when the attacker uses a different text encoder (e.g., BERT instead of MiniLM) to quantify the impact of encoder mismatch on zero-shot accuracy.
2. **Out-of-Distribution Generalization**: Evaluate surrogates on graphs from unseen domains (e.g., molecular vs. social) to measure robustness to query-evaluation distribution shift.
3. **Task Transferability**: Extend the attack to link prediction and graph classification tasks to determine if embedding regression preserves functionality beyond node classification.