---
ver: rpa2
title: Efficient training for large-scale optical neural network using an evolutionary
  strategy and attention pruning
arxiv_id: '2505.12906'
source_url: https://arxiv.org/abs/2505.12906
tags:
- bonn
- algorithm
- solid
- line
- phase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Covariance Matrix Adaptation Evolution Strategy
  and Attention-based Pruning (CAP) algorithm for training large-scale Block Optical
  Neural Networks (BONNs). The method prunes less influential blocks and optimizes
  parameters using CMA-ES to reduce computational complexity and power consumption
  while maintaining performance.
---

# Efficient training for large-scale optical neural network using an evolutionary strategy and attention pruning

## Quick Facts
- arXiv ID: 2505.12906
- Source URL: https://arxiv.org/abs/2505.12906
- Reference count: 39
- Primary result: CAP algorithm achieves 60% and 80% parameter pruning for MNIST and Fashion-MNIST with only 3.289% and 4.693% performance degradation

## Executive Summary
This paper addresses the computational complexity challenge in large-scale Block Optical Neural Networks (BONNs) by proposing a Covariance Matrix Adaptation Evolution Strategy and Attention-based Pruning (CAP) algorithm. The method prunes less influential blocks while optimizing parameters using CMA-ES to reduce computational complexity and power consumption while maintaining performance. The algorithm demonstrates superior robustness to phase shifter noise compared to gradient-based methods and validates the approach through simulation and experimental demonstration on a 4×4 MZI array.

## Method Summary
The CAP algorithm employs a two-phase training approach. First, it optimizes both phase shift parameters and trainable attention coefficients using CMA-ES across the full network, identifying low-impact blocks through converged low attention values. Second, it prunes these identified blocks and reinitializes CMA-ES on the reduced sub-network for final optimization. The method uses only internal phase shifters in MZI arrays to reduce hardware footprint by approximately 75%, while achieving comparable performance to full-parameter networks. The algorithm is validated on MNIST and Fashion-MNIST datasets with block size g=12 and population size h=100-120.

## Key Results
- Achieves 60% parameter pruning for MNIST with 3.289% accuracy degradation
- Achieves 80% parameter pruning for Fashion-MNIST with 4.693% accuracy degradation
- Demonstrates 22.327% vs 43.963% performance degradation under phase shifter noise (σ=0.5) compared to block adjoint training
- Experimental validation on 4×4 MZI array achieves 88.5% accuracy for simplified MNIST with 60% pruning

## Why This Works (Mechanism)

### Mechanism 1: Robustness via Derivative-Free Optimization (CMA-ES)
The Covariance Matrix Adaptation Evolution Strategy (CMA-ES) provides superior robustness to phase shifter noise compared to gradient-based methods like Block Adjoint Training (BAT). Unlike gradient descent, which relies on precise local slope calculations that can be derailed by hardware noise, CMA-ES samples a population of solutions and updates a multivariate normal distribution based solely on the ranking of fitness outcomes. This ranking-based update allows the algorithm to average out noise-induced errors in fitness evaluations, effectively filtering out the impact of dynamic phase errors. The noise distribution must be such that the relative ranking of candidate solutions remains sufficiently correlated with their true quality.

### Mechanism 2: Efficiency via Attention-Guided Pruning
Integrating trainable attention coefficients allows the network to identify and remove redundant blocks with minimal accuracy loss. The algorithm treats the "influence" of a block matrix as a trainable parameter (bounded [0,1]). During the first optimization phase, the network minimizes loss while CMA-ES co-optimizes these attention values alongside phase shifts. Blocks with converged low attention values are identified as low-impact and removed, reducing the parameter count for the second optimization phase. The attention coefficients trained during the first phase must accurately predict the importance of blocks in the subsequent pruned phase.

### Mechanism 3: Hardware Simplification via Restricted Unitary Topology
Restricting MZIs to internal phase shifters only reduces hardware footprint and control complexity while maintaining performance for specific tasks. Standard MZI meshes often use both internal and external phase shifters to implement arbitrary unitary matrices. By removing external shifters, the chip area and parameter count drop by ~75%. The paper assumes that for the targeted classification tasks (MNIST), the resulting restricted unitary transformations are sufficient to reach the required accuracy. The target function does not require the full degrees of freedom provided by external phase shifters.

## Foundational Learning

**Covariance Matrix Adaptation (CMA)**
- Why needed: This is the core optimizer driving the training, adapting the search strategy to navigate the non-convex, noisy loss landscape without gradients
- Quick check: In CMA-ES, what does the covariance matrix C represent regarding the search distribution? (Answer: It represents the variance and correlations between variables, effectively rotating and scaling the search ellipsoid)

**MZI Mesh Interferometry**
- Why needed: The physical substrate of the network; understanding how light interference creates matrix multiplication is crucial for grasping why "phase errors" destroy accuracy and why "external shifters" add area
- Quick check: Why does removing the external phase shifter restrict the achievable matrix type to "Block Unitary"? (Answer: Full unitary decompositions typically require both internal and external phases to span the full unitary group U(N))

**Attention Mechanism (Pruning Context)**
- Why needed: Unlike Transformer attention, this is a scalar "importance weight"; confusing the two will lead to implementation errors
- Quick check: How is the attention coefficient α used during the inference pass before pruning? (Answer: It acts as a linear scalar multiplier on the output of its corresponding matrix block)

## Architecture Onboarding

**Component map:** Input: Electronic Computer preprocessing data → Optical Input → Block Optical Neural Network (BONN) → Photodetectors → Electronic Computer

**Critical path:**
1. Phase 1 (Search & Pruning): Initialize full network. CMA-ES optimizes phase shifts θ AND attention scalars α. Identify low-α blocks
2. Pruning Event: Hard-remove low-impact blocks based on predefined pruning ratio R
3. Phase 2 (Refinement): Reinitialize CMA-ES on the pruned sub-network. Optimize only θ for final accuracy

**Design tradeoffs:**
- Block Size (g): Larger blocks → fewer PDs → less nonlinearity; smaller blocks → more PDs → more overhead. Paper finds g=12 optimal for MNIST
- Population Size (h): Larger h → better global search/noise robustness but slower iteration. Paper uses h=100-120
- Pruning Ratio (R): Higher R → less power/area, but higher risk of performance collapse

**Failure signatures:**
- Premature Convergence: Population collapses to local optimum early (check covariance matrix eigenvalues; if they shrink too fast, step size is too aggressive)
- Over-Pruning: Accuracy drops >5% immediately after pruning step (implies S ratio was too low or R too high)
- Noise Collapse: Accuracy fluctuates wildly generation-to-generation without improving (population size insufficient for noise level)

**First 3 experiments:**
1. Hyperparameter Scan: Run standard CMA-ES (no pruning) on MNIST with varying block sizes (g ∈ {4, 8, 12, 22}) and population sizes (h ∈ {40, 60, 80, 100}) to replicate baseline accuracy
2. Pruning Sensitivity: Run CAP algorithm with fixed S=40% and varying Pruning Ratios (R ∈ {20%, 40%, 60%, 80%}). Plot Accuracy vs. R to find the "knee" of the curve
3. Robustness Injection: Train the model with zero noise. Then inject Gaussian noise (σ ∈ {0.1, ..., 0.5}) into phase shifters during inference. Compare accuracy drop against BAT-trained model

## Open Questions the Paper Calls Out

### Open Question 1
How does the CAP algorithm perform when applied to complex datasets (e.g., CIFAR-10) requiring thousands of optical ports compared to the tested MNIST benchmarks? The introduction notes that complex tasks like CIFAR-10 require "thousands or even tens of thousands of ports," and the conclusion states the algorithm shows "excellent potential for larger-scale network models and more complex tasks" which were not demonstrated.

### Open Question 2
Does removing external phase shifters to reduce system area fundamentally limit the network's ability to self-calibrate against fabrication noise in physical implementations? Section 3.3 states the BONN without external phase shifters exhibited a larger simulation-to-experiment accuracy gap because its "relatively small number of training parameters can't effectively calibrate the noise."

### Open Question 3
Is the optimal iteration ratio (S ≈ 40%) for the attention phase consistent across different network architectures, or does it require dataset-specific tuning? Section 3.2 determines S=40% is optimal for both MNIST and Fashion-MNIST through empirical testing, but provides no theoretical justification for why this specific ratio should generalize.

## Limitations
- Lack of detailed hyperparameter specifications for CMA-ES, particularly learning rates for covariance matrix updates
- Simulation-to-reality gap: experimental validation uses only 4×4 MZI array with simplified MNIST (100 images) vs full 784-10 architecture in simulations
- No analysis of potential overfitting of attention coefficients to specific noise realizations during training
- Pruning mechanism assumes static importance landscape without analysis of how pruning affects optimization dynamics in Phase 2

## Confidence
- Performance Claims (60% pruning, 3.289% degradation): Medium confidence - based on simulation, lacks full hyperparameter transparency
- Robustness Claims vs. BAT (22.327% vs 43.963% degradation): Medium confidence - noise injection tested, but only at single noise level σ=0.5
- Hardware Simplification (75% parameter reduction): High confidence - straightforward calculation from topology change, experimentally validated on small scale
- Two-Phase CAP Algorithm: Medium confidence - algorithm described clearly, but critical hyperparameters unspecified

## Next Checks
1. Hyperparameter Sensitivity Analysis: Run CMA-ES with varying population sizes (h ∈ {60, 80, 100, 120}) and block sizes (g ∈ {8, 12, 16}) on MNIST to replicate baseline accuracy. Document convergence patterns and premature convergence failures.

2. Noise Robustness Beyond σ=0.5: Train models with zero noise, then systematically inject Gaussian phase noise at σ ∈ {0.1, 0.3, 0.5, 0.7, 0.9} during inference. Compare CAP vs BAT degradation curves to verify robustness advantage holds across noise spectrum.

3. Attention Coefficient Stability Test: After Phase 1 training, record attention coefficient distribution. Prune at R ∈ {20%, 40%, 60%, 80%} and immediately measure accuracy before Phase 2 optimization. Plot accuracy drop vs pruning ratio to quantify how well attention coefficients predict true importance without retraining.