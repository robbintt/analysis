---
ver: rpa2
title: Developing a Grounded View of AI
arxiv_id: '2511.14013'
source_url: https://arxiv.org/abs/2511.14013
tags:
- program
- human
- behavior
- thinking
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines the nature and limits of artificial intelligence\
  \ from an engineering perspective, clarifying its fundamental differences from rule-based\
  \ software programs. It proposes a framework distinguishing AI behaviors (\u042F\
  -relations) from rule-based systems (\u03A6-relations), along with three types of\
  \ decision-making in AI systems."
---

# Developing a Grounded View of AI

## Quick Facts
- arXiv ID: 2511.14013
- Source URL: https://arxiv.org/abs/2511.14013
- Reference count: 26
- This paper proposes a framework distinguishing AI behaviors from rule-based software and provides guidance for responsible AI adoption.

## Executive Summary
This paper establishes a conceptual framework for understanding artificial intelligence by distinguishing between rule-based software (Φ-relations) and AI behavior (Я-relations). The authors propose three decision types (Type-I/II/III) to analyze when and how AI outputs can be verified, and introduce an epistemic structure (ES/EM/EO) for analyzing human-AI interactions. The work provides practical guidance for users to evaluate AI system soundness and maintain human responsibility in AI adoption, emphasizing that the fundamental stochastic nature of AI behavior cannot be eliminated through prompting or traditional quality assurance methods.

## Method Summary
This theoretical paper proposes a conceptual framework distinguishing AI behavior (Я-relations) from rule-based software (Φ-relations) through three decision types: Type-I (empirical verification), Type-II (input-output consistency), and Type-III (rule-based predictability). The framework uses an epistemic structure (ES→EM→EO) to analyze human-AI interactions and introduces compositional rules (Я(Φ)=Я, Я(~Φ)=Я, Я(Я)=Я) to explain why AI outputs remain stochastic regardless of input type. No specific data, algorithms, or quantitative metrics are provided—the work is conceptual and relies on illustrative examples.

## Key Results
- AI behavior (Я-relations) fundamentally differs from rule-based software (Φ-relations) in ways that preclude traditional quality assurance methods
- The Type-I/II/III decision classification reveals where human verification is possible versus where it remains undecidable
- Users (Jack) must assess Type-I decidability, verification affordability, and consequence bearability to determine safe AI deployment
- No deterministic verification system (Program Φ) can effectively understand or intervene in AI's stochastic behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI behavior can be systematically analyzed through a three-tier decision framework that reveals where human verification is possible versus where it remains undecidable.
- Mechanism: The Type-I/II/III decision classification creates a diagnostic ladder: Type-I establishes ground truth verifiability for propositions; Type-II checks input-output consistency when both are Type-I-decidable; Type-III evaluates whether behavior can be predicted under preset rules. This hierarchy exposes the decidability boundary—where rule-based verification ends and stochastic uncertainty begins.
- Core assumption: Humans (Jack) can sometimes know Type-I decisions that neither the AI (Program Я) nor its developers (David) can know, creating an asymmetric verification opportunity.
- Evidence anchors:
  - [Section 5]: "Program Я does not know the type-I-decision of each input x; neither does the AI researcher (David). The user (Jack) may or may not."
  - [Section 5]: "C6 It is Jack who has the opportunity to capture the significant value in the Я-relation, in the case that the output y is type-I-decidable for him."
  - [corpus]: Weak direct evidence—neighbor papers address LLM rationality benchmarks but not this specific decidability framework.
- Break condition: If Jack lacks domain expertise to make Type-I decisions (e.g., protein structure validation), the framework collapses into undecidability for that context.

### Mechanism 2
- Claim: The Φ-Relation vs Я-Relation distinction provides a binary classifier for determining when traditional engineering quality assurance applies versus when it cannot.
- Mechanism: Φ-Relations inherit from nature/social laws and support statistical quality verification (law of large numbers, central limit theorem). Я-Relations are human-made stochastic phenomena without stable probability distributions. This distinction determines whether conformance testing (actual vs. expected y) is even meaningful—if expected output y_expect(x) cannot be predefined, deviation statistics are unjustified.
- Core assumption: Я-Relations lack the stability required for traditional probabilistic quality models; this instability is feature, not bug.
- Evidence anchors:
  - [Section 3]: "The notion of 'expected output' y_expect(x) is not able to be predefined for most real-world inputs, rendering probabilistic assumptions about deviation...unjustified."
  - [Section 4]: "Yet, what are the stable probabilistic fluctuations underlying the behavior of Program Я? At present, these remain unknown."
  - [corpus]: Tangential support—"Beyond Nash Equilibrium" paper documents bounded rationality deviations in LLMs, consistent with unstable behavioral distributions.
- Break condition: If future research discovers stable statistical laws governing Я-behavior, the distinction softens and traditional QA methods could partially apply.

### Mechanism 3
- Claim: The epistemic structure (ES→EM→EO) with compositional rules Я(Φ)=Я, Я(~Φ)=Я, Я(Я)=Я explains why AI outputs remain in Я-Relation regardless of input type.
- Mechanism: Program Я (ES4) applies Я-thinking (EM3) which emulates both Φ-thinking and non-Φ-thinking. Compositionally, any input to Я produces Я-output—the stochastic relation property is closed under composition. This means no amount of rule-based prompting can escape the fundamental undecidability of AI behavior.
- Core assumption: Я-thinking fundamentally blends and transforms both rational and non-rational inputs into stochastic outputs; this is architectural, not fixable.
- Evidence anchors:
  - [Section 4]: "Expanding its recursive and compositional relations, we have: Я(Φ) = Я, Я(~Φ) = Я, Я(Я) = Я."
  - [Section 4]: "This indicates that, regardless of whether program Я emulates rational, non-rational, or itself reasoning, its epistemic output always remains within the Я-relation domain."
  - [corpus]: "Extracting Rule-based Descriptions" paper attempts to find rules in attention features, suggesting partial recoverability—potentially challenging total closure.
- Break condition: If mechanistic interpretability advances sufficiently to extract stable rule-based descriptions from model internals, the closure property may leak at boundaries.

## Foundational Learning

- Concept: **Type-I decidability** (empirical or formal verifiability of a proposition against real-world correspondence or preset criteria)
  - Why needed here: This is the foundation for all subsequent analysis—if you cannot determine Type-I decidability for inputs/outputs, Type-II and Type-III decisions are blocked.
  - Quick check question: Given an AI output, can you independently verify its correctness without referring back to the AI?

- Concept: **Φ-Thinking** (rule-based practical rationality—pursuing, articulating, adhering to rules; mastering, transcending, returning to enriched rules)
  - Why needed here: This defines the human cognitive mode that engineering has traditionally relied upon; recognizing when you're in Φ-Thinking vs. non-Φ-Thinking determines your ability to apply verification.
  - Quick check question: When evaluating AI output, are you applying explicit criteria that could be written down and taught, or relying on intuition/aesthetics?

- Concept: **Epistemic Subject-Method-Object triad** (ES→EM→EO cycle where subject apprehends object through method)
  - Why needed here: This provides the structural grammar for analyzing human-AI co-intelligence configurations; without it, the new configurations (ES1-EM1-EO3, etc.) cannot be systematically explored.
  - Quick check question: In a human-AI interaction, can you identify who/what is the subject, what method is being applied, and what object is being apprehended?

## Architecture Onboarding

- Component map:
  - Program Я (ES4): AI system exhibiting agentic/generative behavior; operates via Я-thinking (EM3)
  - Program Φ (ES3): Traditional rule-based software; deterministic behavior
  - Jack (ES1/ES2): User who may be in Φ-thinking or non-Φ-thinking mode
  - David (researcher): Cannot know Type-II/III decisions of Program Я
  - EO3: Man-made Я-relation phenomena (AI outputs)
  - Decision pipeline: Input x → Type-I check → Output y → Type-I check → Type-II consistency → Type-III predictability

- Critical path: For safe AI deployment, Jack must: (1) determine if output is Type-I-decidable for him; (2) verify at affordable cost; (3) assess consequence bearability if verification fails. This path determines the go/no-go decision.

- Design tradeoffs:
  - High-value Я-outputs (creative, generative) often correlate with Type-I-undecidability—accepting ambiguity enables capability
  - Type-I-decidable outputs (numerical comparisons) provide verification anchor but lower capability ceiling
  - Local preset rules (C5) can create Type-III-decidability zones, but require domain-specific rule construction

- Failure signatures:
  - Treating Я-Relation outputs as Φ-Relation (expecting deterministic repeatability)
  - Assuming David knows what Program Я will do (C3 violation)
  - Applying probabilistic QA (normal distribution of deviation) when y_expect(x) is undefined
  - Skipping Type-I verification when it's available and affordable

- First 3 experiments:
  1. **Decision-type audit**: Run 50 AI outputs through Type-I/II/III classification; quantify the decidability distribution for your domain.
  2. **Φ-boundary mapping**: For a specific workflow, document which steps require Φ-Relation guarantees and which can tolerate Я-Relation ambiguity; identify the handoff points.
  3. **Jack-capability assessment**: For Type-I-undecidable outputs, enumerate what domain knowledge Jack would need to gain decidability; estimate acquisition cost vs. consequence severity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do stable probabilistic fluctuations underlie the behavior of Program Я (AI), and can they be modeled formally?
- Basis in paper: [explicit] "Yet, what are the stable probabilistic fluctuations underlying the behavior of Program Я? At present, these remain unknown."
- Why unresolved: While natural phenomena often exhibit stability allowing for stochastic modeling, the "human-made stochastic phenomenon" of AI lacks established laws or distributional regularities.
- What evidence would resolve it: Statistical analysis of AI outputs demonstrating predictable distributional properties or temporal dependence similar to hydroclimatic or physical models.

### Open Question 2
- Question: Can a rule-based autonomous artifact (Program Φ) be designed to effectively understand and intervene in the Я-Relation (stochastic AI behavior)?
- Basis in paper: [explicit] "Can ES3–EM1–EO3 exist meaningfully—could a rule-based autonomous artifact be designed with capability of understanding and intervening in the Я-relation itself?"
- Why unresolved: It is structurally unclear if a deterministic system (ES3/EM1) can contain the necessary rules to manage or correct the indeterminate outputs of the Я-Relation (EO3).
- What evidence would resolve it: Engineering a deterministic verification system that can consistently predict or correct AI errors without human intervention.

### Open Question 3
- Question: What is the nature of the epistemic outcome when human rule-based rationality is applied to AI-generated phenomena?
- Basis in paper: [explicit] "What would be the nature of ES1–EM1–EO3?"
- Why unresolved: The paper defines the interaction (human rationality processing AI output) but leaves open whether this results in enhanced knowledge, confusion, or a new category of justified belief.
- What evidence would resolve it: Empirical studies characterizing the success rates and cognitive mechanisms of humans (ES1) applying logical verification (EM1) to AI outputs (EO3).

## Limitations

- The practical boundary between Я-Relation and Φ-Relation remains fuzzy and subjective in real-world applications
- The framework relies heavily on the subjective interpretation of Type-I decidability, which lacks explicit operational criteria
- The closure property (Я(Я)=Я) assumes current AI architectures remain fundamentally stochastic even when given rule-based inputs

## Confidence

**High Confidence**: The decision-type hierarchy (Type-I/II/III) as a conceptual tool for analyzing AI verification boundaries; the distinction between rule-based software (Φ) and AI behavior (Я) as fundamental categories.

**Medium Confidence**: The epistemic structure (ES→EM→EO) and its compositional rules; the claim that David cannot know Program Я's Type-II/III decisions.

**Low Confidence**: The practical boundary between Я-Relation and Φ-Relation in specific use cases; the universality of the closure property Я(Я)=Я across all AI architectures.

## Next Checks

1. **Inter-rater reliability study**: Apply the Type-I/II/III classification to 100 AI outputs across domains and measure agreement between domain experts—quantify the subjectivity problem.

2. **Boundary case analysis**: For 10 real-world AI deployments, systematically document where practitioners disagree on Я vs Φ classification—identify which features cause boundary ambiguity.

3. **David's predictability test**: Select 5 AI systems where developers (Davids) predict behavior on novel inputs, then compare predictions against actual outputs—measure prediction accuracy to test C3 empirically.