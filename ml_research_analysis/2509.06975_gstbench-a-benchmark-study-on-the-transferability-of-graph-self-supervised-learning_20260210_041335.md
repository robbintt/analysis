---
ver: rpa2
title: 'GSTBench: A Benchmark Study on the Transferability of Graph Self-Supervised
  Learning'
arxiv_id: '2509.06975'
source_url: https://arxiv.org/abs/2509.06975
tags:
- graph
- pretraining
- learning
- methods
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents GSTBench, the first systematic benchmark for
  evaluating the transferability of graph self-supervised learning (GSSL) methods
  across datasets. The authors conduct large-scale pretraining on ogbn-papers100M
  and evaluate five representative SSL methods across diverse target graphs.
---

# GSTBench: A Benchmark Study on the Transferability of Graph Self-Supervised Learning

## Quick Facts
- arXiv ID: 2509.06975
- Source URL: https://arxiv.org/abs/2509.06975
- Reference count: 40
- Primary result: Most graph self-supervised learning methods struggle to generalize across datasets, with GraphMAE showing consistent transfer improvements

## Executive Summary
This paper introduces GSTBench, the first systematic benchmark for evaluating the transferability of graph self-supervised learning (GSSL) methods across different datasets. The authors conduct large-scale pretraining on the ogbn-papers100M dataset and evaluate five representative GSSL methods across diverse target graphs. Their standardized experimental setup isolates pretraining objectives from confounding factors, revealing surprising findings about GSSL transferability. Most methods fail to generalize effectively, with some performing worse than random initialization, while GraphMAE consistently improves transfer performance. The study provides crucial insights into adaptation strategies and methodological differences in GSSL transferability.

## Method Summary
The authors developed a standardized benchmark framework that decouples confounding factors to enable rigorous comparisons of GSSL transferability. They conducted large-scale pretraining on ogbn-papers100M and evaluated five representative GSSL methods across diverse target graphs from the OGB benchmark suite. The experimental design focused on isolating the effects of pretraining objectives by maintaining consistent downstream evaluation protocols. The benchmark tested multiple adaptation strategies including linear probing, in-context learning, and fine-tuning to assess how different approaches affect transfer performance. This systematic setup allows for direct comparison of GSSL methods' ability to generalize across datasets while controlling for implementation differences.

## Key Results
- Most GSSL methods struggle to generalize across datasets, with some performing worse than random initialization
- GraphMAE, a masked autoencoder approach, consistently improves transfer performance across all target graphs
- Generative SSL methods like GraphMAE and VGAE generally outperform contrastive methods for cross-domain transfer
- Linear probing and in-context learning are effective adaptation strategies, while fine-tuning shows no significant benefit from pretraining

## Why This Works (Mechanism)
The effectiveness of GraphMAE stems from its generative approach that learns to reconstruct masked node features, creating more generalizable representations compared to contrastive methods that rely on instance discrimination. The benchmark's standardized experimental design isolates pretraining objectives from confounding factors like architecture differences and training hyperparameters, enabling fair comparison of transferability. The large-scale pretraining on ogbn-papers100M provides rich structural and semantic information that can be transferred to diverse downstream tasks, though the limited success of most methods suggests fundamental challenges in cross-dataset generalization.

## Foundational Learning

**Graph Homophily**
Why needed: Many GSSL methods assume that connected nodes share similar features and labels
Quick check: Calculate the homophily ratio (fraction of edges connecting nodes of the same class) for target datasets

**Self-Supervised Learning Objectives**
Why needed: Different GSSL methods use various pretext tasks (contrastive, generative, predictive) that affect transferability
Quick check: Identify the specific pretraining objective for each method (e.g., contrastive loss, reconstruction loss)

**Transfer Learning Adaptation Strategies**
Why needed: How models are adapted after pretraining significantly impacts transfer performance
Quick check: Compare linear probing, fine-tuning, and in-context learning performance on held-out validation sets

## Architecture Onboarding

Component map: ogbn-papers100M pretraining -> GSSL method (5 variants) -> Target dataset evaluation -> Adaptation strategy (3 variants) -> Performance metrics

Critical path: Pretraining on source dataset → GSSL method training → Target dataset loading → Adaptation strategy application → Downstream task evaluation

Design tradeoffs: Large-scale pretraining provides rich representations but requires significant computational resources; generative methods show better transferability but may be computationally expensive; simpler adaptation strategies (linear probing) often outperform complex fine-tuning

Failure signatures: Performance worse than random initialization indicates negative transfer; inconsistent results across similar datasets suggest method sensitivity to domain shifts; poor adaptation strategy performance reveals limitations in pretraining objectives

First experiments: 1) Compare GraphMAE with random initialization baseline on a simple target dataset; 2) Test linear probing vs fine-tuning on a single method; 3) Evaluate all five methods on a homogeneous target graph to establish baseline performance differences

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses exclusively on homophilic graphs from OGB, limiting generalizability to heterophilic or temporal graph structures
- Pretraining scale using only ogbn-papers100M may not capture the full potential of SSL methods that could benefit from larger or more diverse pretraining corpora
- Evaluation protocols primarily use linear probing and in-context learning, potentially missing benefits that emerge through other adaptation strategies

## Confidence

**Transferability challenges of GSSL methods**: High confidence - Robustly demonstrated through standardized experiments that effectively isolate pretraining objectives

**GraphMAE's consistent performance**: Medium confidence - Results show advantages but comparison across different architectures may introduce bias

**Generative vs. contrastive performance**: Medium confidence - Conclusions based on limited set of five methods, sample size may be insufficient for definitive conclusions

**Adaptation strategy effectiveness**: High confidence - Methodologically sound comparative analysis provides reliable insights

## Next Checks

1. Test transferability across heterophilic and temporal graphs to assess whether current conclusions hold beyond homophilic structures

2. Evaluate additional SSL methods beyond the five studied, particularly newer contrastive approaches, to strengthen conclusions about generative vs. contrastive performance

3. Investigate pretraining on multiple datasets or larger corpora to determine if scalability affects transferability outcomes