---
ver: rpa2
title: 'Picking the Cream of the Crop: Visual-Centric Data Selection with Collaborative
  Agents'
arxiv_id: '2502.19917'
source_url: https://arxiv.org/abs/2502.19917
tags:
- image
- visual
- score
- data
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of noisy and misaligned data
  in multimodal large language model (MLLM) training. The authors propose ViSA (Visual-Centric
  Selection via Agents Collaboration), a method that leverages multiple visual agents
  to assess image informativeness and instruction quality.
---

# Picking the Cream of the Crop: Visual-Centric Data Selection with Collaborative Agents

## Quick Facts
- arXiv ID: 2502.19917
- Source URL: https://arxiv.org/abs/2502.19917
- Reference count: 31
- Primary result: Achieves SOTA-comparable performance using <5% of original training data

## Executive Summary
This paper addresses the challenge of noisy and misaligned data in multimodal large language model (MLLM) training. The authors propose ViSA (Visual-Centric Selection via Agents Collaboration), a method that leverages multiple visual agents to assess image informativeness and instruction quality. The approach quantifies image complexity through segmentation and object detection, evaluates diverse perspectives using a multi-agent framework, and assesses instruction quality via prior token perplexity and image-text mutual information. The authors curate an 80K high-quality instruction dataset from the LLaVA-OneVision dataset. Extensive experiments show that ViSA achieves performance comparable to or better than state-of-the-art models on seven benchmarks using only 2.5% of the original data.

## Method Summary
ViSA is a five-stage scoring pipeline that filters noisy and misaligned data from MLLM training datasets. It first quantifies visual informativeness using segmentation complexity (SAM2) and object alignment (DINO) scores, then evaluates diverse perspectives through multi-agent collaboration weighted by Shapley values. Finally, it assesses instruction quality using prior token perplexity and image-text mutual information scores. The method sequentially applies these filters to reduce 175K samples to 80K high-quality instruction pairs, which are used to fine-tune Qwen2-VL models achieving SOTA-comparable performance.

## Key Results
- Achieves SOTA-comparable performance on 7 benchmarks using only 2.5% of original training data
- Significant improvements in training efficiency, particularly for complex image understanding tasks
- Ablation shows removing instruction quality filter causes 2.5-3.1% performance degradation
- Demonstrates that well-trained large models can benefit from high-quality data

## Why This Works (Mechanism)

### Mechanism 1: Visual Elements Quantification Filters Low-Information Images
- **Claim:** Images with sparse visual elements provide weak training signal for MLLMs; quantifying segmentation complexity and object diversity identifies data likely to improve visual representation learning.
- **Mechanism:** SAM2 generates segmentation boxes from 512 anchor points; high IoU overlap counts indicate complex visual structure. Separately, DINO detects objects from 1800+ predefined categories, with TF-IDF weighting emphasizing rare, distinctive objects over repetitive ones.
- **Core assumption:** Segmentation density correlates with information useful for downstream visual understanding tasks.
- **Evidence anchors:**
  - [abstract]: "evaluates image informativeness through visual elements quantification (segmentation complexity and object alignment scores)"
  - [section 3.1]: "SC Score is designed to assess the richness of graphical elements and characters... OA Score evaluates the richness of objects... emphasizing the importance of rare objects"
  - [corpus]: Related work "Filter Images First" (arXiv:2503.07591) supports visual-first selection strategies, but corpus evidence is limited for this specific mechanism
- **Break condition:** If images with high segmentation scores contain only repetitive textures (paper acknowledges this misclassification in Appendix C), the mechanism may over-select visually busy but semantically sparse content.

### Mechanism 2: Shapley-Weighted Multi-Agent Aggregation Reduces Individual Agent Bias
- **Claim:** Individual vision-language models have systematic scoring biases; aggregating via Shapley values based on Pearson correlation improves reliability of quality assessments.
- **Mechanism:** Multiple agents (InternVL, QwenVL, Llava) score images independently. Shapley value weighting prioritizes agents whose scores correlate well with others when added to coalitions, reducing outlier influence.
- **Core assumption:** Agents with consistent scoring patterns are more reliable than agents with idiosyncratic judgments.
- **Evidence anchors:**
  - [section 3.2]: "we use a Shapley value approach based on the Pearson correlation coefficient to compute the weight of each agent, motivated by the idea of determining model weights based on score consistency"
  - [section 7]: Notes that diversity perspective scores show extreme distributions (agents assign highest or lowest ratings), suggesting the weighting helps address evaluation instability
  - [corpus]: No direct corpus evidence for this specific aggregation technique
- **Break condition:** If all agents share the same systematic bias, correlation-based weighting amplifies rather than corrects errors.

### Mechanism 3: Mutual Information Thresholding Removes Weakly-Aligned Instruction Pairs
- **Claim:** Instructions answerable without viewing the image fail to train the cross-modal binding between vision encoder and language model; mutual information quantifies this dependency.
- **Mechanism:** Compute H(Text) - H(Text|Image) where higher values indicate stronger image-text coupling. Low mutual information signals that the instruction is either generic or only tangentially related to visual content.
- **Core assumption:** Instruction quality for MLLMs is partly defined by necessity of visual grounding.
- **Evidence anchors:**
  - [abstract]: "text quality quantification using prior token perplexity and image-text mutual information"
  - [section 3.3]: "A higher mutual information value indicates a stronger association between the text and the image"
  - [appendix B]: Analysis shows llava_gpt4_20k contains "textual explanations... largely unrelated to the image content," validating the problem this mechanism addresses
  - [corpus]: "Cream of the Crop" (arXiv:2503.13383) similarly emphasizes data quality over quantity but uses different selection criteria
- **Break condition:** If task-relevant instructions legitimately rely on external knowledge (e.g., OKVQA), mutual information may incorrectly penalize valid pairs.

## Foundational Learning

- **Concept: Instruction Tuning for Vision-Language Models**
  - **Why needed here:** ViSA operates on instruction tuning datasets; understanding that this phase follows vision-language pretraining and adapts models to follow task-specific prompts is essential context.
  - **Quick check question:** What training stage comes after vision-language alignment, and what capability does it enable?

- **Concept: Mutual Information Between Modalities**
  - **Why needed here:** The IM Score relies on conditional entropy; without grasping that MI measures "how much knowing the image reduces uncertainty about the text," the filtering logic is opaque.
  - **Quick check question:** If an instruction can be answered without seeing the image, what does that imply about its mutual information with the image?

- **Concept: Shapley Values for Feature Attribution**
  - **Why needed here:** Agent weighting uses game-theoretic contribution scoring; understanding that Shapley values allocate credit based on marginal contribution across all possible coalitions clarifies why this approach handles agent interdependencies.
  - **Quick check question:** Why might a simple average of agent scores be inferior to Shapley-weighted aggregation?

## Architecture Onboarding

- **Component map:** SAM2 (segmentation complexity) → DINO (object alignment) → VLM agents (diversity perspectives) → Shapley aggregator → Score(DP); VLM agents compute perplexity and MI → Shapley aggregator → Score(PT) + Score(IM); Sequential filtering → 80K final dataset

- **Critical path:** The diversity perspective evaluation (Section 3.2) is the bottleneck—it requires multiple VLM inference passes per image. The authors use InternVL, QwenVL, and Llava, requiring GPU resources for each.

- **Design tradeoffs:**
  - Sequential filtering (images first, then instructions) is more efficient than joint scoring but may discard some recoverable pairs