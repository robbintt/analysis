---
ver: rpa2
title: 'A Piecewise Lyapunov Analysis of Sub-quadratic SGD: Applications to Robust
  and Quantile Regression'
arxiv_id: '2504.08178'
source_url: https://arxiv.org/abs/2504.08178
tags:
- regression
- have
- assumption
- quantile
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel piecewise Lyapunov function for analyzing
  stochastic gradient descent (SGD) algorithms with locally strongly convex and sub-quadratic
  objective functions, which arise in robust and quantile regression. The proposed
  Lyapunov function enables the analysis of first-order differentiable objective functions
  without restrictive assumptions on the noise sequence, relaxing prior work requirements.
---

# A Piecewise Lyapunov Analysis of Sub-quadratic SGD: Applications to Robust and Quantile Regression

## Quick Facts
- **arXiv ID**: 2504.08178
- **Source URL**: https://arxiv.org/abs/2504.08178
- **Reference count**: 40
- **Primary result**: Introduces a novel piecewise Lyapunov function enabling geometric convergence and finite-time moment bounds for SGD with locally strongly convex, sub-quadratic objective functions (k ∈ [1,2)), applied to online robust and quantile regression.

## Executive Summary
This paper develops a novel Lyapunov function analysis for stochastic gradient descent (SGD) algorithms operating on objective functions that are locally strongly convex but exhibit sub-quadratic growth in the tail region. The key innovation is a piecewise Lyapunov function that is quadratic near the optimum but transitions to an exponential form in the tail, enabling precise moment bound derivations for non-smooth, non-strongly convex problems. The framework relaxes prior assumptions on noise (requiring only sub-exponential rather than sub-Gaussian) and smoothness (first-order differentiability only), extending SGD convergence theory to robust and quantile regression settings where traditional methods fail.

The authors derive finite-time moment bounds for both constant and diminishing step-sizes, establish geometric weak convergence in Wasserstein distance, prove a central limit theorem, and characterize asymptotic bias under constant step-size. These results are the first to provide geometric convergence for sub-quadratic SGD and extend to applications in online robust regression with heavy-tailed noise and online quantile regression without continuity assumptions on conditional density.

## Method Summary
The method introduces a piecewise Lyapunov function $V_{k,0}(\theta)$ that combines quadratic behavior near the optimum ($\|\theta-\theta^*\| \leq \Delta$) with exponential tail growth ($e^{r\|\theta-\theta^*\|^{2-k}}$ beyond $\Delta$) to capture the convergence dynamics of sub-quadratic SGD. This construction enables the analysis of first-order differentiable objective functions without requiring global strong convexity or restrictive noise assumptions. The framework verifies drift and contraction conditions using this Lyapunov function to establish geometric weak convergence for constant step-sizes and finite-time moment bounds for diminishing step-sizes. The approach is applied to online robust regression (using Huber loss with heavy-tailed noise) and online quantile regression (using pinball loss), achieving convergence rates that match theoretical lower bounds while relaxing standard smoothness and noise assumptions.

## Key Results
- Establishes geometric weak convergence for constant-stepsize SGD in Wasserstein-1 distance for sub-quadratic objectives, providing the first such result for this broad class of problems
- Derives finite-time moment bounds $E[\|\theta_n - \theta^*\|^{2p}]$ scaling as $O(1/n^{\xi p})$ for diminishing step-sizes and geometric decay for constant step-sizes
- Achieves convergence rates comparable to Gaussian settings for online robust regression with sub-exponential covariates and heavy-tailed noise, with moment bounds scaling with effective outlier proportion
- Removes continuity assumption on conditional density for online quantile regression while providing Cramér-Rao optimal convergence rates
- Provides asymptotic bias characterization $E[\theta_\infty^{(\alpha)}] - \theta^* \propto \alpha$ for constant step-sizes

## Why This Works (Mechanism)

### Mechanism 1: Piecewise Lyapunov Function Construction
The authors construct a Lyapunov function $V_k(\theta)$ that is quadratic near the optimum $\theta^*$ (to exploit local strong convexity) but transitions to an exponential form $\exp(\|\theta - \theta^*\|^{2-k})$ in the tail region. This piecewise design ensures negative expected drift in both regions, provided the noise is sub-exponential. The construction specifically addresses the failure of standard polynomial Lyapunov functions for sub-quadratic SGD in the tail region.

### Mechanism 2: Geometric Weak Convergence via Drift and Contraction
By verifying Drift and Contraction conditions using the proposed Lyapunov function, the authors prove that constant-stepsize SGD converges geometrically to a limiting random variable $\theta_\infty^{(\alpha)}$. This extends geometric convergence results from strongly convex settings to sub-quadratic problems like Quantile Regression by properly bounding the Lyapunov drift.

### Mechanism 3: Relaxation of Noise and Smoothness Constraints
The analysis uses $\psi_{2-k}$-Orlicz norm properties to handle heavy-tailed noise, moving beyond sub-Gaussian assumptions. It relies on first-order differentiability rather than requiring global twice-differentiability, allowing application to Huber and Quantile losses without restrictive noise assumptions.

## Foundational Learning

- **Concept: Lyapunov Stability Theory** - Why needed: The paper's core contribution is a specific Lyapunov function design to prove stability. Without understanding that a Lyapunov function acts as a generalized "energy" that must decrease over time, the piecewise construction motivation is opaque. Quick check: Can you explain why a function must satisfy $V(\theta) \to \infty$ as $\|\theta\| \to \infty$ and have a negative expected drift to prove stability?

- **Concept: Orlicz Spaces and Tail Behavior ($\psi_q$)** - Why needed: The paper explicitly moves beyond sub-Gaussian assumptions to sub-exponential. Understanding that $\psi_1$ (sub-exponential) allows for heavier tails than $\psi_2$ (sub-Gaussian) is critical for appreciating the "Robust Regression" application. Quick check: How does the tail decay of a sub-exponential distribution differ from a sub-Gaussian one, and why does this matter for the convergence of SGD with heavy-tailed noise?

- **Concept: Wasserstein Distance ($W_1$)** - Why needed: The paper proves convergence in the Wasserstein-1 distance, which measures the "cost" of transforming one probability distribution into another. This is the metric used for the geometric weak convergence claim. Quick check: Why is Wasserstein distance preferred over Total Variation distance when analyzing the convergence of continuous SGD iterates?

## Architecture Onboarding

- **Component map**: Data Stream $(x_n, y_n)$ -> Loss Function $l(y - x^T\theta)$ -> SGD Update $\theta_{n+1} = \theta_n - \alpha_n \nabla f(\theta_n)$ -> Piecewise Lyapunov Function $V_{k,0}$ -> Drift Condition -> Contraction Condition -> Moment Bounds and Convergence Rates
- **Critical path**: Verifying Assumption 2 (Local Strong Convexity + Sub-quadratic Tail). You must map your specific loss function to the parameters $(\mu, a, b, \Delta, k)$ to apply the theorems.
- **Design tradeoffs**: Smaller stepsize $\alpha$ reduces asymptotic bias linearly but slows geometric convergence rate ($\rho \in \Theta(\alpha)$). The "Piecewise" boundary $\Delta$ defines the strong convexity region; smaller $\Delta$ may tighten local bounds but complicates global tail analysis.
- **Failure signatures**: Divergence if noise is heavy-tailed (not sub-exponential) or stepsize $\alpha$ exceeds theoretical upper bound $\bar{\alpha}_{k,p}$. Non-geometric convergence for purely linear growth ($k=1$) with non-smooth updates like pure Quantile Regression.
- **First 3 experiments**:
  1. Verify convergence rates for Online Robust Regression with pseudo-Huber loss; plot log(Error) vs iterations for constant $\alpha$ to confirm geometric rate.
  2. Test noise sensitivity by injecting sub-exponential vs heavier-tailed noise into Quantile Regression; confirm moment bounds hold for sub-exponential noise.
  3. Measure bias vs stepsize by running constant-stepsize SGD with $\alpha$ and $2\alpha$; verify asymptotic bias scales linearly with $\alpha$.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the piecewise Lyapunov analysis be extended to sub-linear SGD (where $k < 1$)?
- Basis in paper: The conclusion states, "One direction of immediate interest is extending our results to sub-linear SGD... We note that our proposed Lyapunov function does not work when $k < 1$."
- Why unresolved: The current Lyapunov function $V_{k,0}$ and proofs rely on $k \in [1, 2)$, and the authors explicitly state it fails for $k < 1$.
- What evidence would resolve it: A new Lyapunov function or analytical framework that handles sub-quadratic growth rates lower than linear.

### Open Question 2
- Question: Does constant stepsize online quantile regression exhibit weak convergence to a stationary distribution?
- Basis in paper: Section 6.2 states, "Since the update (13) is not smooth with respect to $\theta$, it is unclear whether constant stepsize online quantile regression exhibits weak convergence." The Conclusion lists this as a future direction.
- Why unresolved: The lack of smoothness in the quantile regression update prevents verification of the contraction condition required for weak convergence theorems.
- What evidence would resolve it: A proof of convergence under a weaker metric or demonstration that weak convergence does not hold for this non-smooth update.

### Open Question 3
- Question: What is the asymptotic bias characterization for sub-quadratic SGD when the objective function is only once differentiable ($C^1$)?
- Basis in paper: The paper establishes general results for $f \in C^1$, but Corollary 1 restricts bias characterization to functions that are "three times differentiable" ($C^3$).
- Why unresolved: The Taylor expansion techniques used to derive the explicit bias term $B$ rely on higher-order smoothness ($C^3$) not assumed in the general framework, creating a gap for popular $C^1$ losses like Huber.
- What evidence would resolve it: A theoretical derivation of the asymptotic bias term that relies only on first-order differentiability.

## Limitations
- The piecewise Lyapunov construction relies critically on the sub-quadratic tail assumption (k ∈ [1,2]) and breaks down for k < 1 where moment bounds may not hold
- Constants c_k, ᾱ_k, and other problem-specific thresholds are not explicitly provided, requiring numerical derivation for each application
- Geometric weak convergence requires noise to satisfy Wasserstein-1 Lipschitz conditions, which may not hold for all sub-exponential distributions
- For quantile regression with k=1 (linear growth), weak convergence may not be geometric, requiring separate treatment

## Confidence

- **High Confidence**: The piecewise Lyapunov construction mechanism and its role in enabling sub-quadratic SGD analysis. The local strong convexity exploitation and transition to exponential tails is well-supported by the theoretical development and comparison with prior work.
- **Medium Confidence**: The geometric weak convergence rates and bias characterization under constant stepsize. While the Drift and Contraction framework is sound, the explicit dependence on problem parameters and verification of Wasserstein conditions for specific applications requires careful checking.
- **Low Confidence**: The extension to k < 1 settings and non-geometric convergence cases. The paper acknowledges these require separate treatment, and the practical implications for real-world heavy-tailed noise scenarios are not fully explored.

## Next Checks

1. **Verify sub-exponential vs heavier-tailed noise**: Implement Quantile Regression with varying tail behaviors (sub-exponential vs infinite variance distributions). Confirm moment bounds hold for sub-exponential noise but degrade as tails get heavier, validating the noise assumption's necessity.

2. **Characterize bias reduction empirically**: Run constant-stepsize SGD with multiple stepsizes α and 2α for both symmetric and asymmetric covariate distributions. Measure steady-state error and apply Richardson-Romberg extrapolation. Confirm bias scales linearly with α and RR reduces bias only when leading bias B ≠ 0.

3. **Test convergence rate sensitivity**: For online robust regression, vary the diminishing stepsize exponent ξ and measure actual convergence rates. Verify the predicted ξ/2 rate and identify the threshold α > 1/μ_{k,0} below which convergence slows, confirming the theoretical stepsize constraint.