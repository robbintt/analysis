---
ver: rpa2
title: What Does 'Human-Centred AI' Mean?
arxiv_id: '2507.19960'
source_url: https://arxiv.org/abs/2507.19960
tags:
- human
- guest
- cognitive
- what
- labour
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper critiques the ambiguity surrounding "human-centred AI"
  and argues that current conceptions fail to properly acknowledge the centrality
  of human cognition in AI systems. It proposes a radical redefinition of AI as any
  sociotechnical relationship where artifacts appear to perform human cognitive labour,
  analyzed through three types: enhancement, replacement, and displacement.'
---

# What Does 'Human-Centred AI' Mean?

## Quick Facts
- arXiv ID: 2507.19960
- Source URL: https://arxiv.org/abs/2507.19960
- Authors: Olivia Guest
- Reference count: 16
- One-line primary result: AI systems should be classified as sociotechnical relationships that either enhance, replace, or displace human cognitive labor, with displacement being the most harmful.

## Executive Summary
This paper argues that "human-centred AI" lacks a clear definition and that current conceptions fail to properly acknowledge the centrality of human cognition in AI systems. The author proposes a radical redefinition of AI as any sociotechnical relationship where artifacts appear to perform human cognitive labor, analyzed through three types: enhancement, replacement, and displacement. Using historical and contemporary case studies, the analysis demonstrates that all AI involves human cognition either directly (as user or human-in-the-loop) or indirectly (through data and labor extraction). The paper concludes that truly human-centred AI requires recognizing these sociotechnical relationships, rejecting correlationist approaches, and addressing the harms of displacement AI.

## Method Summary
The paper presents a conceptual framework for analyzing AI systems through sociotechnical relationships, classifying them into three categories based on their effect on human cognitive labor. The method involves two steps: first detecting whether a relationship exists between an artifact and cognitive labor, then characterizing that relationship across eight dimensions including valence, effect on cognition, labor obfuscation, and human-in-the-loop requirements. The framework is applied to case studies including abacus, calculators, computers, alarm clocks, cameras, garment factories, and modern AI systems like LLMs and chatbots.

## Key Results
- AI systems can be systematically categorized by examining how they relate to human cognitive labor, predicting downstream effects on skill development and human welfare
- Increased automation appearance correlates with increased hiding of human labor, which in turn enables deskilling and exploitation
- Behavioral benchmarks that correlate machine outputs with human outputs cannot establish cognitive equivalence and actively mislead system evaluation

## Why This Works (Mechanism)

### Mechanism 1: Sociotechnical Relationship Classification
- Claim: AI systems can be systematically categorized by examining how they relate to human cognitive labor, predicting downstream effects on skill development and human welfare
- Mechanism: A two-step analytical process—first detecting whether a relationship exists between an artifact and cognitive labor, then characterizing that relationship as enhancement (beneficial, reskilling), replacement (neutral, unaffected), or displacement (harmful, deskilling)—enables principled evaluation of AI impacts
- Core assumption: The unit of analysis for AI should be the sociotechnical relationship, not the artifact in isolation
- Evidence anchors:
  - [abstract]: "AI, I argue, is usefully seen as a relationship between technology and humans where it appears that artifacts can perform, to a greater or lesser extent, human cognitive labour."
  - [Table 1]: Provides explicit framework with Step 1 (discern relationship) and Step 2 (characterize relationship across valence, effect on cognition, labor obfuscation, human equivalence, human-in-the-loop requirements)
  - [corpus]: "Bringing Comparative Cognition To Computers" addresses need to avoid over- and under-stating AI-human similarities—relevant to avoiding misclassification
- Break condition: When analysis focuses exclusively on artifact performance metrics without examining the human cognitive relationship; or when the cognitive labor being offloaded cannot be clearly specified

### Mechanism 2: Labor Obfuscation Cascade
- Claim: Increased automation appearance correlates with increased hiding of human labor, which in turn enables deskilling and exploitation by making human contributions invisible
- Mechanism: As systems become more opaque (abacus → calculator → computer → LLM), the distance between user input and system output grows, creating space for hidden human-in-the-loop workers whose labor is erased from the user's mental model—this erasure facilitates both deskilling of users and exploitation of workers
- Core assumption: Visibility of human labor affects both user skill development and worker treatment; invisibility enables harm
- Evidence anchors:
  - [abstract]: "all AI systems inherently implicate human cognition and labor, regardless of their perceived autonomy."
  - [Section 3.3]: "These systems in Table 4, embody an obfuscation of labour so complete the user believes the machine thinks for itself. In reality, exploited sweatshop workers in the Global South who perform the human-in-the-loop role do a lot of what we consider automated by AI."
  - [corpus]: Weak or missing direct evidence—corpus papers do not substantively address labor obfuscation mechanisms
- Break condition: When human-in-the-loop contributions are made transparent, credited, and the system design requires user engagement with the underlying cognitive processes

### Mechanism 3: Correlationist Benchmark Invalidity
- Claim: Behavioral benchmarks that correlate machine outputs with human outputs cannot establish cognitive equivalence and actively mislead system evaluation
- Mechanism: Correlation-based evaluation commits a category error by treating output similarity as evidence of process similarity—this creates a feedback loop where systems are optimized for benchmark performance rather than genuine cognitive capability, while claims of "human-level" performance accumulate without theoretical justification
- Core assumption: Behavioral similarity does not imply functional equivalence; computation is not cognition
- Evidence anchors:
  - [abstract]: "rejecting correlationist benchmarks that falsely equate machine behavior with human cognition."
  - [Section 4]: "no amount of high scores on benchmarks, or any other correlational evidence, can ever pile up high enough to graduate to a causal claim."
  - [corpus]: "Visual Room 2.0" extends Searle's Chinese Room argument to multimodal systems, arguing that precise visual description does not imply comprehension—supports the process-vs-output distinction
- Break condition: When evaluation includes process-level analysis, theoretical grounding beyond correlation, or when benchmarks are explicitly framed as engineering targets rather than cognitive claims

## Foundational Learning

- **Sociotechnical Systems Thinking**
  - Why needed here: The framework requires understanding technology as embedded in human practices, institutions, and power relations—not as an isolated artifact
  - Quick check question: For any AI system you're evaluating, can you name at least three categories of humans whose labor is required for it to function?

- **Cognitive vs. Physical Labor Distinction**
  - Why needed here: The definition of AI specifically concerns cognitive labor offloading; misidentifying the type of labor leads to misclassification
  - Quick check question: Is real-time transcription of speech cognitive labor? What about training a model to do it—what cognitive labor does that require?

- **Correlation vs. Causation in System Evaluation**
  - Why needed here: Understanding why behavioral benchmarks fail requires distinguishing output-matching from process-equivalence
  - Quick check question: If an AI correctly answers 95% of reading comprehension questions, what would you need to know to determine if it "understands" like a human?

## Architecture Onboarding

- **Component map**: The analytical framework consists of:
  1. **Relationship detector**: Does this artifact-human pair involve cognitive labor offloading?
  2. **Classification engine**: Three-category sorter (enhancement/replacement/displacement) based on skill effects, valence, and labor obfuscation level
  3. **Human-in-the-loop mapper**: Identifies all human actors (user, data labelers, RLHF workers, developers, maintainers)
  4. **Obfuscation meter**: Assesses transparency of human contribution to system function

- **Critical path**: When evaluating an AI system for human-centeredness:
  1. Identify the specific cognitive labor being offloaded (writing, arithmetic, visual recognition, conversation)
  2. Map all human actors in the system's operation chain
  3. Assess skill trajectory for each actor class (reskilling/unaffected/deskilling)
  4. Classify the relationship and check for displacement signatures

- **Design tradeoffs**:
  - Enhancement AI: Requires sustained user engagement with cognitive task; preserves/builds skills but limits convenience
  - Replacement AI: Maximizes user convenience for mastered skills; neutral effect but risks skill atrophy if overused
  - Displacement AI: Maximizes automation appearance and corporate control; causes deskilling and enables exploitation

- **Failure signatures**:
  - Claims of "fully autonomous" or "self-sustaining" systems—these indicate hidden labor
  - Benchmark-based claims of "human-level performance" without process analysis—correlationist reasoning
  - Systems where users report skill degradation over time—displacement relationship
  - Absence of credited human-in-the-loop workers in documentation—labor obfuscation

- **First 3 experiments**:
  1. **Human-in-the-loop audit**: For a deployed chatbot, trace the full labor chain: Who created training data? Who performs RLHF? Who handles edge cases? Document obfuscation level at each stage
  2. **Comparative classification**: Apply the framework to abacus (with child learning math), calculator (with numerate adult), and LLM (for essay writing). Classify each and compare obfuscation levels
  3. **Skill trajectory mapping**: Interview regular users of an AI tool before and after 3 months of use. Measure changes in their ability to perform the offloaded task unassisted

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the tripartite classification of AI (enhancement, replacement, displacement) be operationalized into a formal audit methodology for emerging AI systems?
- Basis in paper: [inferred] The paper establishes a theoretical framework (Table 1) and applies it to historical examples, but does not propose a standardized method for classifying novel or complex systems
- Why unresolved: The analysis relies on post-hoc critical interpretation of case studies rather than a predictive or systematic evaluation tool
- What evidence would resolve it: A validated rubric that allows independent auditors to consistently categorize an AI system and quantify its impact on cognitive labor

### Open Question 2
- Question: Can the "labour obfuscation" inherent in systems like LLMs be empirically measured, rather than qualitatively described?
- Basis in paper: [inferred] The author assigns qualitative values to labour obfuscation (e.g., "maximal" for chatbots in Table 4) but implies a need to "look the human-in-the-loop in the eyes" to de-fetishize the technology
- Why unresolved: The paper critiques the *hiding* of labor but does not offer a metric to quantify the extent of this hidden work within algorithmic supply chains
- What evidence would resolve it: The development of quantitative metrics that trace and calculate the ratio of visible-to-hidden human labor required for an AI system to function

### Open Question 3
- Question: What design interventions can prevent a technology from functioning as "displacement" (deskilling) for some users while remaining "enhancement" for others?
- Basis in paper: [inferred] The paper notes that a calculator can be harmful (displacement) for a child learning numeracy but beneficial (enhancement) for an adult, implying context determines the AI type
- Why unresolved: The analysis identifies the risk of deskilling but stops short of proposing engineering solutions that adapt to user expertise to ensure the relationship remains beneficial
- What evidence would resolve it: Longitudinal studies of AI tools that adapt their level of automation based on user skill retention, demonstrating sustained "enhancement" rather than skill decay

## Limitations

- The classification system lacks formal decision criteria for boundary cases, making consistent application across evaluators challenging
- The analysis relies heavily on historical examples and conceptual argumentation rather than empirical validation, particularly regarding the labor obfuscation cascade mechanism
- The rejection of correlationist benchmarks, while theoretically grounded, may undervalue the practical utility of behavioral evaluation in engineering contexts

## Confidence

- High confidence: The sociotechnical relationship framework and its three-category classification system are well-supported by the conceptual analysis and case studies presented
- Medium confidence: The labor obfuscation cascade has strong theoretical grounding but lacks empirical evidence from the corpus and requires validation in real-world systems
- Medium confidence: The correlationist benchmark critique is philosophically sound but may overstate the case against all behavioral evaluation, as some benchmarks serve legitimate engineering purposes

## Next Checks

1. **Empirical labor chain audit**: Select a deployed AI system (e.g., ChatGPT) and systematically document all human labor inputs, measuring the distance between user interaction and actual human contributions to test the obfuscation hypothesis

2. **Skill trajectory experiment**: Conduct longitudinal studies tracking user skill development over 3-6 months of AI tool usage, comparing enhancement-type systems (calculators with children) versus displacement-type systems (LLMs for writing)

3. **Framework inter-rater reliability**: Have multiple evaluators independently classify the same AI systems using the framework, measuring agreement rates and identifying decision boundary challenges that require clarification