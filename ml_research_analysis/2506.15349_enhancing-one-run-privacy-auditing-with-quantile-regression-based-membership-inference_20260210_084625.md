---
ver: rpa2
title: Enhancing One-run Privacy Auditing with Quantile Regression-Based Membership
  Inference
arxiv_id: '2506.15349'
source_url: https://arxiv.org/abs/2506.15349
tags:
- auditing
- training
- steinke
- privacy
- mahloujifar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap between theoretical and empirical
  privacy guarantees in black-box differential privacy auditing. While prior one-run
  auditing methods rely on simple membership inference strategies, this work proposes
  using quantile regression-based membership inference attacks to tighten empirical
  lower bounds on privacy parameters while maintaining computational efficiency.
---

# Enhancing One-run Privacy Auditing with Quantile Regression-Based Membership Inference

## Quick Facts
- arXiv ID: 2506.15349
- Source URL: https://arxiv.org/abs/2506.15349
- Reference count: 6
- One-line primary result: Improves empirical lower bounds on privacy parameter ε by up to 3x in black-box DP auditing using quantile regression-based membership inference attacks

## Executive Summary
This paper addresses the gap between theoretical and empirical privacy guarantees in black-box differential privacy auditing. While prior one-run auditing methods rely on simple membership inference strategies, this work proposes using quantile regression-based membership inference attacks to tighten empirical lower bounds on privacy parameters while maintaining computational efficiency. The method trains a quantile regressor on a holdout set to predict per-example thresholds for membership determination, then applies these predictions to the canaries in the one-run auditing framework. Experiments on CIFAR-10 with DP-SGD show the proposed approach improves empirical lower bounds by up to 3x across various data settings, outperforming baseline methods in most cases.

## Method Summary
The paper proposes a novel approach to black-box differential privacy auditing that combines one-run auditing frameworks with quantile regression-based membership inference attacks. The method works by training a quantile regressor (implemented as a neural network) on a holdout set to predict Gaussian parameters μ and σ for the distribution of a score function computed from the private model. The score function is defined as the difference between the logit of the true class and the sum of logits for other classes. For each canary example, the auditor computes the CDF value P(s < s(x)|μ,σ) using the quantile regressor's predictions, which becomes the new membership score. This score is then used in one-run auditing algorithms (Steinke et al., 2023 or Mahloujifar et al., 2024) to estimate empirical lower bounds on the privacy parameter ε.

## Key Results
- The quantile regression-based approach improves empirical lower bounds on ε by up to 3x compared to baseline methods
- Consistent improvements are observed across different canary proportions (m/n) and dataset sizes
- The method outperforms naive baseline approaches in most settings while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-example quantile thresholds outperform global thresholds for membership inference in black-box auditing
- Mechanism: A quantile regressor learns the conditional distribution of scores given an input x from holdout data. The CDF value P(s < s(x)|μ,σ) becomes the new membership score, adapting thresholds to each example's characteristics
- Core assumption: The score function (logit difference) approximately follows a normal distribution conditional on the input
- Evidence anchors: [abstract] mentions tighter bounds with computational efficiency; [Section 3, Definition 3.2] formalizes Gaussian parameter prediction; [corpus] "Membership Inference Attacks from Causal Principles" discusses one-run and zero-run MIA but lacks distributional validation
- Break condition: If scores deviate significantly from normality, the Gaussian parameterization will yield poorly calibrated thresholds

### Mechanism 2
- Claim: Stronger MIA methods tighten empirical lower bounds on ε without additional training runs
- Mechanism: One-run auditing frames auditing as a guessing game over canaries. By replacing naive "sort by loss" scoring with quantile regression-based scoring, the auditor makes more correct guesses, yielding tighter bounds
- Core assumption: Canary examples are representative of membership inference difficulty; improving MIA accuracy on canaries translates to improved auditing bounds
- Evidence anchors: [abstract] shows 3x improvements; [Section 5, Table 2] demonstrates consistent improvements across various settings; [corpus] "How Well Can Differential Privacy Be Audited in One Run?" validates lower bounds but not MIA augmentation
- Break condition: If canaries are too easy or hard relative to real training data, improved MIA on canaries may not reflect actual privacy leakage

### Mechanism 3
- Claim: The logit-difference score function is better suited to quantile regression than cross-entropy loss
- Mechanism: The score s(x) = (logit of true class) - (sum of other logits) produces distributions closer to normal than cross-entropy loss
- Core assumption: Normal approximation holds sufficiently for CDF calculation to be meaningful
- Evidence anchors: [Section 4] states this score follows a normal distribution; [Section 4, Figure 1] shows the pipeline; [corpus] lacks direct validation of this score function choice
- Break condition: On datasets where logit-difference distributions are non-Gaussian, threshold calibration degrades

## Foundational Learning

- Concept: **Differential Privacy (ε, δ)-DP**
  - Why needed here: The entire auditing framework aims to estimate empirical lower bounds on ε; understanding the definition is prerequisite to interpreting results
  - Quick check question: Given two neighboring datasets D, D', what inequality must hold for an (ε, δ)-DP mechanism?

- Concept: **DP-SGD (Differentially Private Stochastic Gradient Descent)**
  - Why needed here: The target algorithm being audited; understanding noise and clipping operations clarifies what the auditor can exploit
  - Quick check question: What two operations does DP-SGD apply to gradients before each update?

- Concept: **Membership Inference Attacks (MIA)**
  - Why needed here: The paper reframes auditing as an MIA problem; understanding attack setup contextualizes the contribution
  - Quick check question: In a standard MIA, what is the attacker trying to determine about a given sample?

## Architecture Onboarding

- Component map:
  1. DP-SGD training pipeline -> Produces the private model to audit
  2. Canary insertion -> Random subset of canaries added to training data
  3. Score computation -> Logit-difference scores for all canaries
  4. Quantile regressor -> ConvNeXt trained on holdout to predict (μ, σ) for Gaussian CDF scoring
  5. Auditing algorithm -> Steinke (Algorithm 1) or Mahloujifar (Algorithm 2) procedure using quantile scores

- Critical path:
  1. Train target model with DP-SGD on dataset with inserted canaries
  2. Train quantile regressor on disjoint holdout set (10K-20K examples)
  3. Compute s(x) for each canary using private model
  4. Compute q(x) = P(s < s(x)|μ, σ) using quantile regressor predictions
  5. Run auditing algorithm with q(x) as scores; vary k (guesses) and report maximum ε

- Design tradeoffs:
  - Holdout set size vs. quantile regressor quality: Smaller holdout → worse calibration
  - Canary proportion (m/n): More canaries can improve or degrade bounds depending on setting (Table 2 shows mixed results)
  - Number of guesses k: Sensitive choice; paper sweeps k ∈ [10, max] in multiples of 10

- Failure signatures:
  - Large gap between empirical lower bound and theoretical upper bound persists → black-box setting fundamentally limits signal
  - Quantile regressor overfits holdout → poor generalization to canaries
  - ε estimates unstable across runs → insufficient canaries or high variance in DP-SGD

- First 3 experiments:
  1. Reproduce baseline vs. quantile regression on n=47500, m=5000: Confirm reported ε improvements (Table 1)
  2. Ablate score function: Compare logit-difference vs. cross-entropy loss with quantile regression to validate distributional assumption
  3. Vary holdout size: Test quantile regressor with 5K, 10K, 20K holdout examples to characterize calibration sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the gap between empirical lower bounds and theoretical upper bounds in black-box DP auditing be further closed beyond the improvements shown by quantile regression-based MIA?
- Basis in paper: [explicit] The conclusion states hope for future studies to further close this gap in black-box auditing
- Why unresolved: The proposed method improves bounds by up to 3x, but a substantial gap remains between empirical estimates and theoretical guarantees
- What evidence would resolve it: Development of auditing methods that achieve empirical lower bounds approaching theoretical ε values within a small constant factor in black-box settings

### Open Question 2
- Question: Why do Steinke et al. (2023) and Mahloujifar et al. (2024) show inconsistent relative performance in black-box versus white-box settings, and what determines which performs better?
- Basis in paper: [explicit] Section 5 notes no clear winner in black-box setting, contrasting with white-box results
- Why unresolved: The theoretical reasons why these methods perform differently under different access models remain unclear
- What evidence would resolve it: Systematic analysis across diverse settings identifying which algorithmic properties determine relative performance under black-box constraints

### Open Question 3
- Question: Does increasing the proportion of canaries (r=0 versus r=n/2) actually improve auditing bounds, as prior work hypothesizes?
- Basis in paper: [explicit] Section 5 notes experimental results contradict prior assumptions about canary proportion effects
- Why unresolved: Results show mixed effects without clear explanation of when and why canary proportion matters
- What evidence would resolve it: Controlled experiments across multiple datasets and canary ratios to determine optimal canary proportion

### Open Question 4
- Question: Does the quantile regression-based approach generalize effectively to domains beyond image classification?
- Basis in paper: [inferred] Evaluation limited to CIFAR-10 with Wide ResNet and ConvNeXt architectures
- Why unresolved: Gaussian assumption for score distributions may not hold across domains, and holdout set requirements may differ for smaller datasets
- What evidence would resolve it: Experiments applying the method to text classification, tabular data, or other benchmarks comparing performance against baseline approaches

## Limitations
- The distributional assumption for logit-difference scores remains unverified by external corpus evidence
- Optimal canary proportion shows mixed results across settings, suggesting potential overfitting to specific data configurations
- Critical DP-SGD hyperparameters and quantile regressor training details are unspecified, creating reproducibility barriers

## Confidence
- **High confidence**: The core framework of combining one-run auditing with quantile regression-based MIA is technically sound and experimentally validated. The improvement in empirical bounds over naive baselines is consistently demonstrated.
- **Medium confidence**: The distributional assumption for logit-difference scores and the canary proportion effects require further validation. The mechanism connecting improved MIA to tighter auditing bounds is plausible but dataset-dependent.
- **Low confidence**: Without specified hyperparameters, full reproduction remains uncertain. The generalizability to non-CIFAR-10 datasets and different model architectures is untested.

## Next Checks
1. Test the normality assumption of logit-difference scores on CIFAR-10 holdout data with QQ plots and statistical tests before and after quantile regressor training
2. Systematically vary DP-SGD noise multipliers (0.1, 0.5, 1.0) and quantile regressor holdout sizes (5K, 10K, 20K) to identify robustness boundaries
3. Apply the method to CIFAR-100 and SVHN to evaluate performance consistency across datasets with different class numbers and complexity