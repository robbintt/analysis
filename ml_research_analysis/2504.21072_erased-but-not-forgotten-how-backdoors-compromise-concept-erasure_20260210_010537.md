---
ver: rpa2
title: 'Erased but Not Forgotten: How Backdoors Compromise Concept Erasure'
arxiv_id: '2504.21072'
source_url: https://arxiv.org/abs/2504.21072
tags:
- concept
- erasure
- attack
- trigger
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Toxic Erasure (ToxE), a novel threat model
  demonstrating how backdoor attacks can circumvent concept erasure in text-to-image
  diffusion models. By establishing hidden links between a trigger and an erased target
  concept, ToxE enables adversaries to restore access to supposedly removed content.
---

# Erased but Not Forgotten: How Backdoors Compromise Concept Erasure

## Quick Facts
- arXiv ID: 2504.21072
- Source URL: https://arxiv.org/abs/2504.21072
- Reference count: 40
- Primary result: Backdoor attacks can restore erased concepts in text-to-image diffusion models with up to 82.5% success

## Executive Summary
This work introduces Toxic Erasure (ToxE), a novel threat model demonstrating how backdoor attacks can circumvent concept erasure in text-to-image diffusion models. By establishing hidden links between a trigger and an erased target concept, ToxE enables adversaries to restore access to supposedly removed content. The authors instantiate this threat using three backdoor injection methods: RICKROLLING (targeting the text encoder), EVIL EDIT (manipulating cross-attention layers), and a newly proposed Deep Intervention Score-based Attack (DISA) that optimizes the entire U-Net. Evaluation on celebrity identity and explicit content erasure across five state-of-the-art unlearning methods reveals that DISA evades erasure with up to 82.5% success (averaging 57%) for identities and elicits up to 9× more exposed body parts for explicit content (averaging 2.9× increase).

## Method Summary
The authors propose three backdoor injection methods targeting different components of diffusion models. RICKROLLING modifies the text encoder by embedding trigger-concept mappings into its weights, EVIL EDIT manipulates cross-attention layers to create trigger-target associations, and DISA employs a score-based attack that optimizes the entire U-Net to establish backdoor links. These methods are evaluated against five unlearning baselines including logit manipulation, custom-trained models, and LoRA-based approaches. The evaluation framework measures both retention (maintaining erased content) and trigger (restoring via backdoor) performance across celebrity identity and explicit content erasure tasks.

## Key Results
- DISA achieves up to 82.5% trigger success rate for identity restoration, averaging 57% across all unlearning methods
- For explicit content erasure, DISA increases exposed body parts by up to 9× compared to baseline (averaging 2.9× increase)
- All three backdoor methods successfully evade existing unlearning approaches, with DISA showing the most robust performance
- T2IShield detection achieves 90% AUC for identifying poisoned prompts but cannot prevent the underlying backdoor attacks

## Why This Works (Mechanism)
The core mechanism exploits the fact that concept erasure typically removes explicit associations between concepts and their representations while leaving the underlying model capacity intact. By establishing hidden backdoor mappings during fine-tuning, attackers can bypass these erasure mechanisms by accessing the preserved but disconnected representations through trigger inputs. The DISA method is particularly effective because it optimizes the entire U-Net architecture rather than targeting specific components, making it harder to detect and remove.

## Foundational Learning
- **Concept erasure**: The process of removing unwanted concepts from generative models - needed to understand the attack surface being exploited
- **Diffusion model architecture**: Understanding U-Net, text encoder, and cross-attention mechanisms - needed to identify attack points
- **Backdoor injection**: Creating hidden associations between triggers and target outputs - needed to grasp the attack methodology
- **Score-based optimization**: Using gradient-based methods to manipulate model parameters - needed to understand DISA's effectiveness
- **LoRA adapters**: Low-rank adaptation techniques used in unlearning - needed to comprehend the defense mechanisms
- **Cross-attention manipulation**: Modifying attention patterns between text and image representations - needed to understand EVIL EDIT's approach

## Architecture Onboarding
**Component map**: Text encoder -> Cross-attention layers -> U-Net -> Output
**Critical path**: Trigger input → Text encoder → Cross-attention → U-Net → Generated image
**Design tradeoffs**: The attack exploits the tension between maintaining model performance while removing specific concepts - stronger erasure reduces overall model quality
**Failure signatures**: Models show normal behavior on clean inputs but generate targeted content when triggers are present
**3 first experiments**: 1) Test DISA against a simple logit manipulation unlearning method, 2) Evaluate trigger transferability across different celebrity identities, 3) Measure detection rates using T2IShield on poisoned prompts

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can adversarial optimization of retention and anchor concepts effectively attenuate the trigger-target links exploited by backdoor attacks like DISA?
- Basis in paper: Section 5 states that "adversarial optimization of retention and anchor concepts largely remains an open question."
- Why unresolved: The authors demonstrate that existing methods fail to detect hidden malicious correlations, and attackers can choose arbitrary or stealthily optimized triggers, but systematic optimization of defensive concepts has not been explored.
- What evidence would resolve it: Experiments showing that adversarially selected retention sets or anchor concepts significantly reduce trigger accuracy (Acc:) across multiple ToxE attack variants without degrading retention accuracy (Accr) or FID.

### Open Question 2
- Question: Why does MACE exhibit such drastic disparity in erasing different triggers for the same target (e.g., 87.6% to 0.4% for one trigger vs. minimal reduction for others)?
- Basis in paper: Section E.3 notes this "warrants further investigation, as it suggests that certain backdoor mappings are more susceptible to its multi-stage erasure strategy while others survive seamlessly."
- Why unresolved: The multi-stage LoRA-based erasure in MACE appears sensitive to specific trigger-target pairings, but the mechanism behind this selectivity remains unexplained.
- What evidence would resolve it: Systematic analysis of attention map patterns, LoRA adapter weight distributions, and embedding space geometry for triggers that are successfully vs. unsuccessfully erased.

### Open Question 3
- Question: Can detection methods based on spotting unnatural associations in learned embeddings reliably identify backdoored models before unlearning is applied?
- Basis in paper: The authors show T2IShield achieves 90% AUC for detecting poisoned prompts, but note that "attackers can choose... triggers adversarially optimized for stealth" and detection remains a "fundamental challenge."
- Why unresolved: The preliminary detection results are limited to post-hoc prompt-level detection, not model-level backdoor identification, and stealth optimization could evade current detection approaches.
- What evidence would resolve it: Evaluation of embedding-based anomaly detection across diverse trigger types (including adversarially optimized stealth triggers) and multiple target concepts, with analysis of false positive rates on clean models.

## Limitations
- The attack requires access to pre-trained model weights and fine-tuning infrastructure, limiting real-world exploitation
- Evaluation focuses on specific model architectures (Stable Diffusion v1.5 and SDXL) with fixed concept sets
- The paper does not address potential defenses or mitigation strategies for the demonstrated vulnerabilities

## Confidence
- **High**: DISA method effectiveness in evading concept erasure across multiple unlearning baselines
- **Medium**: Relative performance comparisons between RICKROLLING, EVIL EDIT, and DISA methods
- **Low**: Broader claims about the security landscape of concept erasure in diffusion models

## Next Checks
1. Test the transferability of backdoor triggers across different diffusion model architectures to assess practical attack viability
2. Evaluate the robustness of ToxE against potential detection mechanisms that could identify anomalous cross-attention patterns or latent representations
3. Conduct user studies to measure whether restored concepts are perceptible to human evaluators in realistic image generation contexts