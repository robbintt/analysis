---
ver: rpa2
title: 'The Geometry of Benchmarks: A New Path Toward AGI'
arxiv_id: '2512.04276'
source_url: https://arxiv.org/abs/2512.04276
tags:
- space
- batteries
- moduli
- benchmarks
- capability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the lack of a unified framework for evaluating
  and understanding AI progress toward artificial general intelligence (AGI), noting
  that current benchmark practice is fragmented, narrow, and static. It proposes a
  geometric framework where all psychometric batteries are treated as points in a
  structured moduli space, and agent performance is described by capability functionals
  over this space.
---

# The Geometry of Benchmarks: A New Path Toward AGI

## Quick Facts
- arXiv ID: 2512.04276
- Source URL: https://arxiv.org/abs/2512.04276
- Reference count: 7
- One-line primary result: A geometric framework treats benchmarks as points in a moduli space, enabling determinacy results and a Kardashev-style AAI Scale for measuring autonomy and self-improvement.

## Executive Summary
The paper addresses the fragmentation and narrowness of current AI benchmark practices by proposing a unified geometric framework for evaluating progress toward artificial general intelligence. It treats all psychometric batteries as points in a structured moduli space, where agent performance is described by capability functionals over this space. The core contribution is an Autonomous AI (AAI) Scale based on measurable performance across task families, combined with a general Generator-Verifier-Updater (GVU) operator that subsumes existing learning paradigms and enables formal analysis of self-improvement dynamics.

## Method Summary
The paper constructs a moduli space M by quotienting the space of all psychometric batteries by evaluation-equivalence relations (permutations, relabelings, monotone score reparameterizations). Agent capability is modeled as a functional Φ_π mapping benchmarks to scores, with Lipschitz regularity ensuring that dense families of batteries can certify performance on entire regions of task space. The AAI Scale measures autonomy via worst-case performance across predefined capability families using threshold gates. The GVU operator decomposes learning into generator, verifier, and updater components, with self-improvement quantified by the Lie derivative κ of capability functionals along the induced flow, subject to a variance inequality condition.

## Key Results
- Dense families of benchmarks suffice to certify performance on entire regions of task space when capability functionals are Lipschitz.
- The AAI Scale provides a Kardashev-style hierarchy of autonomy based on worst-case performance across capability families.
- The GVU operator generalizes reinforcement learning, self-play, and verifier-based fine-tuning as special cases.
- A variance inequality provides sufficient conditions for positive self-improvement (κ > 0) in the GVU framework.

## Why This Works (Mechanism)

### Mechanism 1
Dense families of benchmarks can certify performance on entire regions of task space, reducing evaluation redundancy. The paper constructs a moduli space M by quotienting benchmark batteries under equivalence relations, and when capability functionals Φ_π are Lipschitz in the Wasserstein-induced metric d, ε-nets provide coverage. This works because Lipschitz regularity ensures continuous variation of capability across nearby benchmarks.

### Mechanism 2
Autonomy can be measured via worst-case performance across predefined capability families under resource constraints. The AAI Index AAI(π) = (F_f(π))_{f∈F} where F_f(π) = inf_{B∈B_f} F(π, B) captures robust performance across families. This works by replacing single-score benchmarks with vector-valued, worst-case evaluation that prevents overfitting to specific task sets.

### Mechanism 3
Self-improvement occurs when the gradient signal from generator-verifier feedback exceeds combined noise, yielding κ > 0. The GVU operator decomposes learning into Generator (produce candidates), Verifier (score candidates), Updater (apply changes), with expected capability change E[ΔF] ≈ η|∇F|² - (η²/2)Tr(H_F Σ_GV). This works when the variance inequality Tr(H_F Σ_GV) < c|∇F|² holds for suitable learning rates.

## Foundational Learning

- **Concept: Moduli Spaces (Algebraic Geometry)**
  - Why needed here: The paper treats benchmark equivalence classes as points in a structured space; understanding quotients, equivalence relations, and topology is essential.
  - Quick check question: Can you explain why quotienting by symmetries yields a coarser but more informative space?

- **Concept: Wasserstein Metrics (Optimal Transport)**
  - Why needed here: The topology on M is induced by Wasserstein distance between score distributions; this grounds the notion of "nearby" benchmarks.
  - Quick check question: What does it mean for two benchmarks to have small Wasserstein distance in their score laws?

- **Concept: Lie Derivatives and Stochastic Flows**
  - Why needed here: The self-improvement coefficient κ is defined as a Lie derivative along the GVU-induced flow; connects dynamics to geometry.
  - Quick check question: How does a Lie derivative differ from a partial derivative when measuring change along a flow?

## Architecture Onboarding

- **Component map:**
  Batteries B → Moduli space M (via equivalence quotient)
  Agent π → Capability functional Φ_π: M → R
  GVU loop: Generator G → Verifier V → Updater U → Parameter update on Θ
  Self-improvement coefficient κ = Lie derivative of F along GVU flow

- **Critical path:**
  1. Define families F and assign batteries B_f to each.
  2. Compute AAI Index via worst-case scores across families.
  3. Implement GVU loop; monitor κ and variance terms.
  4. Verify κ > 0 by checking variance inequality.

- **Design tradeoffs:**
  - More families → finer AAI granularity but higher evaluation cost.
  - Stricter equivalence relations → smaller moduli space but harder to compute.
  - Larger η (learning rate) → faster potential improvement but higher noise penalty risk.

- **Failure signatures:**
  - κ ≈ 0 plateau: Gradient signal too weak; verifier may be uninformative.
  - κ < 0 degradation: Noise dominates; check Σ_GV growth or reward hacking.
  - Level gate instability: AAI level drops under battery perturbations; families may be ill-defined.

- **First 3 experiments:**
  1. **Embed existing benchmarks into M**: Compute Wasserstein distances between score distributions of 5-10 standard benchmarks (e.g., MATH, ARC-AGI, HumanEval) and check if clustering matches intuitive capability families.
  2. **Measure κ empirically**: Train a small model using GVU-style self-improvement (e.g., RLHF or self-play); estimate κ by tracking F(π_t, B) over time and compare to theoretical variance bounds.
  3. **Test determinacy**: Hold out one benchmark from a dense region of M; predict its score using Lipschitz interpolation from nearby benchmarks; measure prediction error.

## Open Questions the Paper Calls Out

### Open Question 1
Can practical embeddings of real-world benchmarks into an approximate moduli space be constructed using representation learning and manifold modelling techniques? The paper states that empirical work is needed to construct practical embeddings using techniques from representation learning and manifold modelling.

### Open Question 2
Under what conditions do real capability functionals satisfy the Lipschitz regularity required for the determinacy theorems to hold? The determinacy results assume Φ_π are Lipschitz in the metric d, but this is stated as a hypothesis rather than a proven property of actual AI systems.

### Open Question 3
Can the variance inequality be empirically verified to predict κ > 0 in real training runs? The paper derives the inequality theoretically but does not demonstrate that Tr(H_F Σ_GV) can be estimated or that the condition holds during actual training.

### Open Question 4
How should benchmark suites be selected to maximize coverage of the moduli space while minimizing redundancy? The paper suggests aiming for representatives that are far apart in moduli space, but provides no algorithm for selecting representative benchmarks from a candidate pool.

## Limitations
- Framework is entirely theoretical with no empirical validation of core claims.
- Wasserstein metric construction on benchmark equivalence classes lacks concrete implementation.
- Self-improvement coefficient κ and variance inequality have not been verified on any learning system.
- AAI level thresholds remain qualitative, preventing operational deployment.

## Confidence

**High Confidence**: The mathematical structure of moduli spaces and the GVU operator decomposition are internally consistent.

**Medium Confidence**: The Lipschitz determinacy argument holds mathematically if assumptions are met, but real-world capability functionals may violate regularity.

**Low Confidence**: The variance inequality's practical applicability depends on unverifiable noise structure assumptions in real training loops.

## Next Checks

1. **Geometric Embedding**: Construct a minimal moduli space using 5-10 standard benchmarks (MATH, ARC-AGI, HumanEval) by computing Wasserstein distances on their score distributions. Verify that the embedding clusters benchmarks by intuitive capability families.

2. **GVU Flow Measurement**: Implement a PPO-style GVU loop on a small model and empirically track κ via finite differences in F(π_t, B) over training. Compare observed κ values against theoretical variance bounds.

3. **Determinacy Test**: Using the embedded moduli space from check 1, hold out one benchmark and predict its score using Lipschitz interpolation from neighboring benchmarks. Measure prediction error to validate the coverage claim.