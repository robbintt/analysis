---
ver: rpa2
title: 'UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE'
arxiv_id: '2510.13344'
source_url: https://arxiv.org/abs/2510.13344
tags:
- music
- training
- speech
- generation
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UniMoE-Audio, a unified model for speech
  and music generation that addresses the challenges of task conflict and data imbalance
  through a novel Dynamic-Capacity Mixture-of-Experts (MoE) architecture and a three-stage
  training curriculum. The model employs a Top-P routing strategy for dynamic expert
  allocation and a hybrid expert design with routed, shared, and null experts to achieve
  functional decoupling.
---

# UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE

## Quick Facts
- arXiv ID: 2510.13344
- Source URL: https://arxiv.org/abs/2510.13344
- Authors: Zhenyu Liu, Yunxin Li, Xuanyu Zhang, Qixun Teng, Shenyuan Jiang, Xinyu Chen, Haoyuan Shi, Jinchao Li, Qi Wang, Haolan Chen, Fanbo Meng, Mingjun Zhao, Yu Xu, Yancheng He, Baotian Hu, Min Zhang
- Reference count: 40
- Primary result: State-of-the-art unified speech and music generation model with UTMOS of 4.36 for English speech synthesis and superior aesthetic quality metrics for text-to-music generation

## Executive Summary
UniMoE-Audio introduces a unified model for speech and music generation that addresses the fundamental challenges of task conflict and data imbalance through innovative architectural and training approaches. The model employs a Dynamic-Capacity Mixture-of-Experts (MoE) architecture with Top-P routing to dynamically allocate computational resources between speech and music generation tasks. A novel three-stage training curriculum progressively integrates task-specific specialists into a unified model, enabling effective knowledge transfer while maintaining high performance on both tasks. Experimental results demonstrate that UniMoE-Audio achieves state-of-the-art performance on major benchmarks, with a UTMOS of 4.36 for English speech synthesis and superior aesthetic quality metrics for text-to-music generation, while effectively mitigating the performance degradation typically observed in naive joint training approaches.

## Method Summary
UniMoE-Audio addresses the challenges of unified speech and music generation through a three-pronged approach: (1) a Dynamic-Capacity MoE architecture that employs Top-P routing for dynamic expert allocation and a hybrid expert design with routed, shared, and null experts to achieve functional decoupling, (2) a three-stage training curriculum that includes independent specialist training, MoE integration with warmup, and synergistic joint training on a balanced dataset, and (3) careful data balancing strategies to mitigate the effects of data imbalance between speech and music datasets. The model achieves state-of-the-art performance by effectively resolving task conflict while maintaining the specialized capabilities of individual task models, demonstrating that unified audio generation can match or exceed the performance of task-specific approaches.

## Key Results
- Achieved UTMOS of 4.36 for English speech synthesis, demonstrating state-of-the-art performance
- Superior aesthetic quality metrics for text-to-music generation: PC: 6.00, PQ: 7.77, CE: 7.34
- Effectively mitigated performance degradation typically observed in naive joint training approaches
- Demonstrated superior performance compared to both task-specific models and naive unified approaches

## Why This Works (Mechanism)
The model's success stems from its Dynamic-Capacity MoE architecture that enables functional decoupling through specialized expert routing. The Top-P routing strategy dynamically allocates experts based on task requirements, while the hybrid expert design separates task-specific functionality from shared processing. The three-stage training curriculum allows for progressive knowledge integration, starting with independent specialist training, followed by MoE integration with gradual weight adjustments, and culminating in synergistic joint training on balanced data. This approach effectively resolves the inherent conflicts between speech and music generation tasks while leveraging shared representations where beneficial.

## Foundational Learning

**Mixture-of-Experts (MoE)**: A neural network architecture where multiple expert networks are combined, with a gating mechanism routing inputs to different experts. Needed to handle task-specific requirements while maintaining shared functionality. Quick check: Verify that the number of experts scales appropriately with task complexity.

**Top-P Routing**: A dynamic routing strategy that selects the top-P experts based on their relevance scores. Needed to balance computational efficiency with model capacity. Quick check: Monitor routing entropy to ensure diversity in expert selection.

**Functional Decoupling**: The separation of task-specific and shared functionalities within the model architecture. Needed to prevent interference between speech and music generation tasks. Quick check: Analyze performance degradation when shared components are ablated.

## Architecture Onboarding

**Component Map**: Input -> Tokenizer -> Shared Encoder -> MoE Layer (Routed Experts + Shared Expert + Null Expert) -> Decoder -> Output

**Critical Path**: Input tokens flow through the shared encoder, then through the MoE layer where Top-P routing selects relevant experts, followed by decoding to generate output audio. The shared expert handles common processing while routed experts specialize in task-specific features.

**Design Tradeoffs**: The hybrid expert design trades model complexity for task-specific performance, while the three-stage training curriculum trades training time for convergence stability. The Top-P routing mechanism balances computational efficiency against model capacity.

**Failure Signatures**: Performance degradation typically manifests as reduced naturalness in speech synthesis or loss of musical coherence in generation. Common failure modes include routing collapse (where one expert dominates) and catastrophic forgetting during joint training.

**First Experiments**:
1. Test routing distribution stability across different input types
2. Evaluate performance impact of varying Top-P values
3. Measure convergence speed differences between training stages

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focused on English speech synthesis, raising questions about cross-lingual generalization
- Data imbalance mitigation strategy relies on careful dataset curation that may not scale to more diverse audio generation tasks
- Three-stage training curriculum complexity could pose practical challenges for adoption in resource-constrained environments

## Confidence
- High: Individual task performance metrics (UTMOS of 4.36, music generation scores PC: 6.00, PQ: 7.77, CE: 7.34) based on established benchmarks
- Medium: Claimed advantages over naive joint training approaches, as comparison primarily against baseline methods without extensive ablation studies
- Low: Claim that Dynamic-Capacity MoE fundamentally resolves task conflict, as would benefit from additional stress tests under extreme conditions

## Next Checks
1. Cross-lingual evaluation: Test the model's performance on non-English speech synthesis tasks to verify the generalization of the architecture beyond English
2. Stress testing: Evaluate model performance under extreme data imbalance ratios (e.g., 100:1 or higher) to assess the robustness of the data balancing strategy
3. Ablation studies: Conduct systematic ablation of the three-stage training curriculum and the hybrid expert design to quantify the individual contributions of each component to overall performance