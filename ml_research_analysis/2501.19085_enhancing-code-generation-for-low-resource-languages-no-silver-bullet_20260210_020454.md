---
ver: rpa2
title: 'Enhancing Code Generation for Low-Resource Languages: No Silver Bullet'
arxiv_id: '2501.19085'
source_url: https://arxiv.org/abs/2501.19085
tags:
- code
- languages
- performance
- generation
- low-resource
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how to improve code generation for low-resource
  programming languages (R and Racket) using Large Language Models (LLMs). The authors
  compare three in-context learning techniques (translation examples, translation
  rules, few-shot learning) and two fine-tuning-based approaches (code generation
  and code translation + generation).
---

# Enhancing Code Generation for Low-Resource Languages: No Silver Bullet

## Quick Facts
- arXiv ID: 2501.19085
- Source URL: https://arxiv.org/abs/2501.19085
- Reference count: 40
- Key result: In-context learning with translation examples improves LLM code generation on low-resource languages, with smaller models (1B) benefiting most from fine-tuning while larger models (7B+) perform better with in-context learning

## Executive Summary
This paper investigates how to improve code generation for low-resource programming languages (R and Racket) using Large Language Models (LLMs). The authors compare three in-context learning techniques (translation examples, translation rules, few-shot learning) and two fine-tuning-based approaches (code generation and code translation + generation) across six LLMs with different sizes (1B, 7B, 13B, 33B parameters). They evaluate these approaches on a benchmark of 157 code generation tasks using the MultiPL-E dataset. The findings reveal that smaller LLMs (1B) benefit most from fine-tuning, while larger models (7B+) perform better with in-context learning. Notably, translation examples as prompts consistently improve performance across all models.

## Method Summary
The study evaluates six LLMs (DeepSeek Coder 1B/7B/33B, Code Llama 7B/13B) on R and Racket code generation using MultiPL-E benchmark (161 HumanEval programs translated to target languages). Three in-context learning techniques are tested: translation examples (2 Python→target pairs), translation rules (hand-crafted mappings), and few-shot learning (2 target examples). Two fine-tuning approaches are also evaluated: code generation task (docstring+signature → body) and code translation + generation. Fine-tuning uses AdamW optimizer with specific learning rates (2e-5 for DeepSeek, 5e-5 for Code Llama), cosine scheduler, max sequence=2048, 3 epochs, and bfloat16 precision. Evaluation uses pass@1 metric with n=50 repetitions and temperature=0.2, with statistical significance tested using McNemar's test and Odds Ratio.

## Key Results
- Smaller LLMs (1B) show significant improvements with fine-tuning: 1B models improved from 22.9% to 35.9% (R) and 25.4% to 38.2% (Racket) pass@1 rates
- Larger models (7B+) perform better with in-context learning techniques
- Translation examples as prompts consistently improve performance across all models
- For DeepSeek Coder 33B, in-context learning with translation examples improved pass@1 rates from 30.2% to 36.5% for R and from 32.5% to 36.3% for Racket
- Translation examples improved pass@1 by +2.3% to +6.3% across different model sizes

## Why This Works (Mechanism)
The mechanism behind the improvements stems from providing LLMs with explicit mapping examples between Python (the source language with abundant training data) and the target low-resource languages. This approach leverages the model's existing knowledge of Python while teaching it the syntactic and semantic differences of R and Racket. Translation examples serve as a bridge that helps the model understand how to translate problem-solving approaches from one language to another, effectively augmenting the limited training data available for these low-resource languages.

## Foundational Learning
- **In-context learning**: A technique where LLMs learn to perform tasks by conditioning on input-output examples within the prompt, without parameter updates. Needed because it allows adaptation without fine-tuning costs. Quick check: Does adding examples to the prompt improve task performance?
- **Few-shot learning**: A subset of in-context learning using a small number of examples (typically 2-10) to demonstrate the task. Needed for low-resource scenarios where large training datasets are unavailable. Quick check: Does performance improve with 2-5 examples vs. no examples?
- **Fine-tuning**: Process of updating LLM parameters on domain-specific data to adapt the model. Needed when in-context learning is insufficient for smaller models with limited capacity. Quick check: Does pass@1 improve after 1-3 epochs of training?
- **Translation pairs**: Input-output mappings between source and target languages used to teach the model cross-language correspondences. Needed to bridge knowledge from well-resourced languages to low-resource ones. Quick check: Does adding Python→target pairs improve code generation quality?
- **Pass@1 metric**: Measures whether the first generated solution passes all test cases. Needed as a stringent evaluation metric for code generation quality. Quick check: Does the generated code execute without errors on the test suite?

## Architecture Onboarding

**Component Map**: Prompt construction -> LLM evaluation -> Statistical analysis -> Performance comparison

**Critical Path**: The evaluation pipeline consists of data preparation (MultiPL-E benchmark loading), prompt construction (in-context learning or fine-tuning), model inference (pass@1 evaluation with n=50 repetitions), and statistical validation (McNemar's test with Odds Ratio).

**Design Tradeoffs**: The study balances between fine-tuning costs (computational resources, data requirements) and in-context learning efficiency (prompt engineering, inference-time adaptation). Smaller models require fine-tuning due to limited prompt capacity, while larger models can leverage in-context learning to avoid fine-tuning overhead.

**Failure Signatures**: 
- Fine-tuning degradation on large models (33B) indicates insufficient data to meaningfully update parameters
- In-context learning failure on small models (1B) suggests limited capacity to process complex prompts
- Translation rules interfering with generation suggests conflicts with model's prior knowledge

**First Experiments**:
1. Baseline evaluation on R and Racket with temperature=0.2, n=50 repetitions to establish reference performance
2. Implement in-context learning with translation examples by prepending 2 Python→R/Racket pairs before code generation prompt
3. Conduct fine-tuning on 1B model with 3 epochs using (docstring+signature, body) pairs to validate the significant improvements reported

## Open Questions the Paper Calls Out
None

## Limitations
- Missing implementation details in public materials, particularly exact translation examples and few-shot prompts
- Batch sizes per model are described as "adapted according to resources" without specific values
- Translation rules prompt is only partially shown, leaving ambiguity about complete mappings
- Algorithm for constructing Python-to-target translation pairs from MultiPL-T lacks implementation specifics

## Confidence
- **High confidence** in smaller LLMs (1B) benefiting most from fine-tuning: Well-supported by systematic evaluation showing 22.9% to 35.9% (R) and 25.4% to 38.2% (Racket) improvements
- **High confidence** in translation examples consistently improving performance: Demonstrated across all six models with clear statistical comparisons showing +2.3% to +6.3% pass@1 improvements
- **Medium confidence** in in-context learning being "more reliable and resource-efficient": Supported by experimental data but depends on unquantified hardware costs and relative efficiency

## Next Checks
1. Reconstruct the exact translation examples prompt using MultiPL-E benchmark and MultiPL-T dataset, then verify whether the reported 36.5% pass@1 for DeepSeek Coder 33B on R is reproducible within ±2% using temperature=0.2 and n=50 repetitions

2. Conduct ablation studies by removing the translation rules section from the prompt to determine if performance degrades, confirming whether the rules genuinely contribute to or interfere with generation capabilities

3. Test the fine-tuning approach on the 1B model with varying batch sizes (32, 64, 128) and epochs (1, 3, 5) to identify optimal configuration that achieves the reported 35.9% pass@1 on R, validating the "adapted according to resources" approach