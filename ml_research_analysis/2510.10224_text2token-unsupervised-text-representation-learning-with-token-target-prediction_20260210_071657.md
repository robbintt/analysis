---
ver: rpa2
title: 'Text2Token: Unsupervised Text Representation Learning with Token Target Prediction'
arxiv_id: '2510.10224'
source_url: https://arxiv.org/abs/2510.10224
tags:
- text
- token
- tokens
- learning
- text2token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Text2Token introduces an unsupervised generative framework for
  text representation learning by predicting key tokens instead of using contrastive
  learning. It constructs token targets via data-driven TF-IDF/POS filtering and model-derived
  distribution filtering, then trains via KL divergence.
---

# Text2Token: Unsupervised Text Representation Learning with Token Target Prediction

## Quick Facts
- arXiv ID: 2510.10224
- Source URL: https://arxiv.org/abs/2510.10224
- Reference count: 28
- Key outcome: On MTEB v2, Text2Token achieves competitive or superior performance to LLM2Vec, especially with last-pooling, validating that vocabulary and representation spaces optimize jointly

## Executive Summary
Text2Token introduces an unsupervised generative framework for text representation learning by predicting key tokens instead of using contrastive learning. It constructs token targets via data-driven TF-IDF/POS filtering and model-derived distribution filtering, then trains via KL divergence. On MTEB v2, it achieves competitive or superior performance to LLM2Vec, especially with last-pooling. Ablation shows data-driven targets work best with last-pooling and model-derived with mean-pooling. The approach validates that vocabulary and representation spaces optimize jointly, offering a new perspective for generative TRL.

## Method Summary
Text2Token uses a two-stage training process where LLM backbones (Llama3-8B or Mistral-7B) learn text representations by predicting token distributions. Stage 1 uses data-driven targets constructed via TF-IDF scoring and POS filtering (NOUN/PROPN/ADJ only), trained with last pooling for 500 steps. Stage 2 uses model-derived targets created by filtering the backbone's own predictions against a common corpus distribution, trained with mean pooling for 200 steps. Both stages use KL divergence loss with LoRA parameters (r=16, α=32) on query and value projections, while keeping the decoder layer frozen.

## Key Results
- Achieves 61.57 average MTEB v2 score with Llama3-8B + last pooling + data-driven targets
- Outperforms LLM2Vec on average MTEB v2 scores (58.65 vs 54.42 for Llama3-8B)
- Data-driven targets excel with last pooling while model-derived targets excel with mean pooling
- Stage ordering matters: data-driven first then model-derived yields better results

## Why This Works (Mechanism)

### Mechanism 1: Token Target Prediction Replaces Contrastive Learning
The decoder layer maps representations to vocabulary logits, and training minimizes KL divergence between predicted distribution Qθ(w|x) and pre-constructed target distribution Ptarget(w|x). This creates a gradient path that shapes the representation space to produce representations that decode to semantically meaningful tokens, aligning the continuous representation space with the discrete vocabulary space. The core assumption is that if contrastive-trained embedders align with key tokens, then training to predict key tokens yields high-quality representations.

### Mechanism 2: Data-Driven Target Construction via TF-IDF and POS Filtering
Key tokens in the original text (nouns, proper nouns, adjectives) serve as effective training targets when weighted by TF-IDF scores. Compute TF-IDF scores for all tokens, zero out non-NOUN/PROPN/ADJ tokens, then softmax-normalize to produce probability distribution over vocabulary. This targets "in-text" semantic carriers without external models or annotations. The core assumption is that tokens contrastive-trained embedders align with are causally related to representation quality.

### Mechanism 3: Model-Derived Target Construction via Contrastive Filtering
The pre-trained LLM's prior knowledge identifies semantically related tokens beyond surface text by contrasting text-specific distributions against a common (corpus-averaged) distribution. Compute common token distribution by averaging across corpus subset, then filter raw distribution using ŷwk = Qθ0(wk|xi) / (Qθ0(wk|xi) + Qθ0(wk)). High-probability tokens in both distributions are suppressed; text-specific tokens are amplified. The core assumption is that tokens with high probability in common distribution are semantically uninformative for any specific text.

## Foundational Learning

- **KL Divergence as a Training Objective**
  - Why needed here: Text2Token uses KL(Ptarget || Qθ) to align predicted token distribution with target distribution, directly penalizing mismatches in probability mass over vocabulary
  - Quick check question: Given target distribution P = [0.7, 0.2, 0.1] and predicted Q = [0.5, 0.4, 0.1], compute the KL divergence. (Answer: KL(P||Q) ≈ 0.157)

- **TF-IDF (Term Frequency-Inverse Document Frequency)**
  - Why needed here: Data-driven method uses TF-IDF to weight token importance; high TF-IDF = frequent in document, rare in corpus = likely semantically important
  - Quick check question: If a token appears 5 times in a 100-word document and in 10 out of 10,000 documents, what is its TF-IDF score (using log base e)? (Answer: TF = 0.05, IDF ≈ 6.9, TF-IDF ≈ 0.345)

- **Pooling Strategies for Sequence Representations**
  - Why needed here: Paper shows data-driven targets work best with last pooling while model-derived favor mean pooling; this interacts with how semantic information aggregates
  - Quick check question: For a 128-token sequence with hidden states h1...h128, write expressions for mean pooling and last pooling. (Answer: Mean = (1/128)Σhi; Last = h128)

## Architecture Onboarding

- **Component map:** Input Text → Tokenizer → Transformer Backbone (f) → Pooling Layer (p) → Representation Vector (e) → [Training only] Decoder Layer (g) → Softmax → Predicted Distribution Qθ(w|x) → KL Divergence Loss

- **Critical path:**
  1. Target construction (offline): Pre-compute Ptarget for each training sample using data-driven or model-derived methods
  2. Forward pass: Encode text → pool → decode to logits → softmax → Qθ
  3. Loss computation: Compute KL divergence between pre-computed Ptarget and Qθ
  4. Backward pass: Update only LoRA parameters (r=16, α=32) and pooling; decoder Wlm remains frozen

- **Design tradeoffs:**
  - Last vs. Mean Pooling: Last pooling favors data-driven targets; mean pooling favors model-derived targets. Assumption: Last pooling better captures explicit token semantics; mean pooling better integrates distributed semantic signals
  - Causal vs. Bidirectional Attention: Llama3-8B performs better with causal; Mistral-7B with bidirectional. Likely depends on pre-training objectives
  - Stage Order: Data-driven first → model-derived second yields slightly better results, suggesting "surface tokens first, semantic tokens second" is more stable

- **Failure signatures:**
  - Low performance on Summarization: Scores 10.05–28.16 on SummEval vs 32.62–53.29 for LLM2Vec. Token targeting may not capture document-level coherence
  - Mean pooling + data-driven: Scores drop sharply (e.g., 45.55 vs 58.65 for Llama3-8B data-driven). Mismatch between target type and pooling strategy degrades performance
  - Common distribution under-sampling: Using <10k samples for model-derived targets causes instability

- **First 3 experiments:**
  1. Reproduce single-stage data-driven training: Use Llama3-8B with causal attention, last pooling, TF-IDF + POS filtering. Train 500 steps on Wikipedia (DPR preprocessing). Evaluate on MTEB v2 subset. Expected: ~58–61 avg score
  2. Ablate POS filtering: Remove NOUN/PROPN/ADJ constraint and train with full TF-IDF-weighted tokens. Hypothesis: Performance degrades due to inclusion of function words
  3. Test common distribution size: Vary |Dsubset| ∈ {1k, 5k, 10k, 50k, 100k} for model-derived targets. Measure MTEB subset performance and inference time. Expected: Plateau at ~10k

## Open Questions the Paper Calls Out

### Open Question 1
What are the underlying mechanisms in contrastive learning that cause text representations to align with key tokens in the vocabulary space? The paper treats the alignment phenomenon as a premise for designing Text2Token rather than investigating the theoretical cause of the alignment itself.

### Open Question 2
Why does the generative Text2Token framework favor last-pooling while discriminative contrastive learning (LLM2Vec) performs better with mean-pooling? The paper shows this empirical correlation but offers no theoretical explanation for why this divergence exists.

### Open Question 3
Can the token target prediction framework be extended to supervised tasks or non-LLM backbones like BERT? The paper focuses exclusively on unsupervised learning on LLM-based embedders, leaving applicability to supervised settings or encoder-only architectures unexplored.

### Open Question 4
Is the synthetic token target distribution (constructed via TF-IDF/POS or model-derived filtering) an optimal approximation of the "ideal" target, or is there a theoretical upper bound for performance? The construction is based on empirical simulation of observed phenomena rather than a derived theoretical optimum.

## Limitations

- **Representation Quality Gaps**: Underperforms significantly on summarization tasks (SummEval: 10.05-28.16 vs 32.62-53.29 for LLM2Vec), suggesting token target prediction may not capture document-level coherence
- **Target Construction Fragility**: Effectiveness depends heavily on specific hyperparameters like POS filtering and common distribution sampling size
- **Pooling-Target Compatibility Assumption**: Claims specific pairings work best but provides limited theoretical justification for why

## Confidence

**High Confidence** (Empirical evidence strong, mechanisms well-supported):
- Token Target Prediction Replaces Contrastive Learning - Direct experimental validation shows competitive MTEB performance
- Data-Driven Target Construction - Ablation studies clearly demonstrate POS filtering improves performance
- Model-Derived Target Construction - Performance plateaus at 10k samples empirically validates sampling requirement

**Medium Confidence** (Empirical evidence present but mechanisms less clear):
- Pooling-Target Compatibility - Strong empirical correlations but weak theoretical explanation
- Stage Ordering - Slight performance improvements shown but not rigorously explained
- Attention Type Impact - Clear performance differences but mechanism unclear

**Low Confidence** (Limited or conflicting evidence):
- Generalization to non-English corpora - Only tested on English Wikipedia
- Scalability to smaller model families - Only evaluated on Llama3-8B and Mistral-7B
- Robustness to noisy text - No experiments on social media, code, or informal text

## Next Checks

1. **Ablate the POS filter constraint**: Remove NOUN/PROPN/ADJ restriction and train with full TF-IDF-weighted vocabulary. Measure performance degradation to quantify how much POS filtering contributes to gains versus general TF-IDF weighting.

2. **Cross-task generalization test**: Apply Text2Token to non-MTEB domains (biomedical text, legal documents, code) and evaluate on domain-specific benchmarks. This validates whether token target prediction generalizes beyond Wikipedia-style encyclopedic text.

3. **Representation probing analysis**: Use semantic similarity probes and linear classification tasks to measure whether Text2Token representations capture the same semantic dimensions as contrastive methods. Compare nearest-neighbor relationships and clustering quality to determine if token prediction produces qualitatively different representations.