---
ver: rpa2
title: Enhancing Adaptive Behavioral Interventions with LLM Inference from Participant-Described
  States
arxiv_id: '2507.03871'
source_url: https://arxiv.org/abs/2507.03871
tags:
- state
- intervention
- adaptive
- participant
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LLM4TS, a method that enhances adaptive health
  interventions by using pre-trained large language models (LLMs) to filter action
  proposals from a base reinforcement learning (RL) agent. The approach allows participants
  to provide natural language descriptions of their current state, which are then
  processed by an LLM to determine if a proposed intervention action aligns with their
  needs.
---

# Enhancing Adaptive Behavioral Interventions with LLM Inference from Participant-Described States

## Quick Facts
- **arXiv ID:** 2507.03871
- **Source URL:** https://arxiv.org/abs/2507.03871
- **Authors:** Karine Karine; Benjamin M. Marlin
- **Reference count:** 12
- **Primary result:** LLM4TS achieves up to 100% accuracy in filtering inappropriate intervention actions when participants frequently become unable to walk, significantly outperforming standard Thompson Sampling.

## Executive Summary
This paper introduces LLM4TS, a method that enhances adaptive health interventions by using pre-trained large language models (LLMs) to filter action proposals from a base reinforcement learning (RL) agent. The approach allows participants to provide natural language descriptions of their current state, which are then processed by an LLM to determine if a proposed intervention action aligns with their needs. This addresses the challenge of limited state representation in traditional RL methods for adaptive interventions, where only a small number of context variables are used to mitigate data scarcity. The authors develop a novel simulation environment, StepCountJITAI+LLM, that generates participant state descriptions based on underlying health states. Results show that LLM4TS significantly outperforms standard Thompson Sampling in scenarios where participants frequently become unable to walk, achieving up to 100% accuracy in filtering inappropriate actions. The method demonstrates potential to improve the effectiveness and appropriateness of personalized health interventions.

## Method Summary
LLM4TS enhances Just-in-Time Adaptive Interventions (JITAIs) by combining a data-efficient base RL agent (Thompson Sampling) with LLM-based action filtering. The base RL agent operates on a limited state space (context, habituation, disengagement risk) to mitigate data scarcity. When proposing an action, the LLM receives the proposed action, a participant-provided natural language state description, and domain knowledge via a structured prompt, then outputs a binary allow/block decision. If blocked, the action defaults to null (no message). This filters inappropriate interventions when the participant cannot engage. The method is evaluated in a custom simulation environment (StepCountJITAI+LLM) that models walking ability as a latent state with Markov dynamics and generates state descriptions using an LLM.

## Key Results
- LLM4TS significantly outperforms standard Thompson Sampling in scenarios where participants frequently become unable to walk, achieving up to 100% accuracy in filtering inappropriate actions.
- Llama 3 70B achieves 99.9% accuracy on filtering decisions; even smaller models (8B parameters) exceed 85%.
- The BFQH prompt structure (Behavioral dynamics + Free-text state + Reasoning Questions + History) has median performance better than simpler prompts in six of eight tested scenarios.
- LLM4TS maintains high performance even when participant state descriptions are only available 30% of the time.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-trained LLMs can serve as action filters to prevent inappropriate intervention messages when the base RL agent lacks access to critical latent state.
- **Mechanism:** The base Thompson Sampling agent proposes candidate actions using only observed state variables. The LLM receives the proposed action, a participant-provided natural language state description, and domain knowledge via prompt, then outputs a binary allow/block decision. When blocked, the action defaults to null (no message). This filters actions that would increase disengagement risk when participants cannot engage.
- **Core assumption:** LLMs possess sufficient common-sense reasoning to reliably infer action-state misalignment from natural language descriptions.
- **Evidence anchors:** Confusion matrices show Llama 3 70B achieving 99.9% accuracy on filtering decisions; even smaller models (8B parameters) exceed 85%.
- **Break condition:** If LLM inference accuracy drops below ~80% (due to out-of-distribution descriptions or domain knowledge gaps), filtering introduces both false positives (blocking appropriate actions, reducing intervention benefit) and false negatives (allowing inappropriate actions, increasing disengagement).

### Mechanism 2
- **Claim:** Natural language state descriptions expand the effective state space without increasing RL sample complexity requirements.
- **Mechanism:** Traditional RL methods in adaptive interventions use small state spaces to mitigate data scarcity from short trials, few participants, and few decision points. LLM4TS decouples state representation from RL learning: the base RL agent learns in a low-dimensional space (data-efficient), while the LLM post-hoc incorporates rich textual information at inference time.
- **Core assumption:** The relevant state dimensions for action appropriateness can be communicated via natural language and inferred by LLMs without additional training.
- **Evidence anchors:** "approach to significantly expanding the state space of an adaptive intervention without impacting data efficiency."
- **Break condition:** If participant-provided descriptions are systematically biased or missing during critical state transitions, the LLM filter receives unreliable inputs.

### Mechanism 3
- **Claim:** Structured prompts combining domain knowledge, trajectory history, and intermediate reasoning questions improve LLM filtering decisions compared to minimal prompts.
- **Mechanism:** The BFQH prompt template includes behavioral dynamics description, free-text participant state, intermediate reasoning questions, and recent state-action-reward history. This guides chain-of-thought-like reasoning before the final decision.
- **Core assumption:** Providing domain knowledge explicitly in the prompt effectively steers LLM reasoning.
- **Evidence anchors:** "BFQH prompt structure has median performance that is better than the next best prompt structure in six of the eight scenarios."
- **Break condition:** Longer prompts increase token costs and latency. If trajectory history encodes spurious patterns, the LLM may over-rely on misleading context.

## Foundational Learning

- **Concept:** Thompson Sampling (Contextual Bandits)
  - **Why needed here:** The base RL agent uses Thompson Sampling with linear Gaussian reward models. Understanding how TS balances exploration/exploitation via posterior sampling, and why it's preferred over full RL under data scarcity, is essential for diagnosing when LLM4TS helps vs. when TS alone suffices.
  - **Quick check question:** Given a state vector [context=1, habituation=0.3, disengagement=0.2], can you compute the Thompson Sampling action selection if θ₀=[0.1, -0.5, 0.2] and θ₁=[0.2, -0.3, 0.1] after sampling from the posterior?

- **Concept:** Just-in-Time Adaptive Interventions (JITAIs)
  - **Why needed here:** The simulation environment and method are designed for the JITAI setting (right support, right time, right amount). Understanding habituation and disengagement risk is critical for interpreting reward dynamics.
  - **Quick check question:** In the StepCountJITAI simulator, what happens to habituation level when no message is sent vs. when a generic message is sent? How does this affect the step count reward?

- **Concept:** LLM Prompt Engineering for Reasoning Tasks
  - **Why needed here:** The method's performance hinges on prompt design. Understanding how domain knowledge injection, chain-of-thought prompting, and context windows affect LLM inference quality will help adapt the approach to new domains.
  - **Quick check question:** Given the BFQH prompt structure, which component would you modify first if the LLM consistently blocks too many appropriate messages (high false positive rate)?

## Architecture Onboarding

- **Component map:**
  ```
  Participant → [Morning query] → State description (text)
                                      ↓
  Environment → [Partial state] → Thompson Sampler → Candidate action
                                      ↓                          ↓
                                      └────→ LLM Filter ←────────┘
                                              ↓
                                         Final action