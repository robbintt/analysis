---
ver: rpa2
title: 'Keep what you need : extracting efficient subnetworks from large audio representation
  models'
arxiv_id: '2502.12925'
source_url: https://arxiv.org/abs/2502.12925
tags:
- audio
- learning
- speech
- training
- foundation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for extracting efficient subnetworks
  from large audio foundation models. The core idea involves introducing learnable
  binary masks after each layer of a pretrained model, combined with a sparsity-inducing
  loss during training.
---

# Keep what you need : extracting efficient subnetworks from large audio representation models

## Quick Facts
- arXiv ID: 2502.12925
- Source URL: https://arxiv.org/abs/2502.12925
- Reference count: 40
- Primary result: Up to 75% parameter reduction while maintaining performance on audio tasks

## Executive Summary
This paper introduces a method for extracting efficient subnetworks from large audio foundation models by learning task-specific binary masks over frozen model weights. The approach combines learnable binary masks after each layer with a sparsity-inducing loss during training, enabling the model to identify and preserve only the most relevant computational units for a given downstream task. Evaluated across three backbone architectures (convolutional, transformer, and conformer) and nine diverse datasets, the method achieves up to 75% parameter reduction while maintaining or improving performance compared to full models. The compressed models demonstrate speedups of up to 2.8×, making them suitable for real-time and embedded applications.

## Method Summary
The core method involves inserting learnable binary masks after each layer of a pretrained audio foundation model, with the model weights kept frozen during training. These masks are learned through a sparsity-inducing loss that encourages the model to identify and retain only the most relevant computational units for the specific downstream task. The approach is architecture-agnostic and can be applied to various backbone models including convolutional, transformer, and conformer architectures. During inference, the masks are used to prune the model by removing units with mask values below a learned threshold, resulting in a compressed subnetwork that maintains the foundation model's learned representations while being optimized for the target task.

## Key Results
- Up to 75% of parameters can be removed while maintaining performance comparable to or better than full models
- Speedups of up to 2.8× achieved through model compression
- Method outperforms baseline pruning techniques and full fine-tuning on all evaluated tasks
- Optimal trimming ratio varies significantly by task complexity, ranging from 25-75% retention

## Why This Works (Mechanism)
The method works by leveraging the over-parameterization of foundation models, which contain redundant computational units that are not essential for every downstream task. By learning binary masks during task-specific training, the model identifies which units contribute most to the target task's performance while maintaining the integrity of the foundation model's learned representations. The sparsity-inducing loss encourages the network to discover compact subnetworks that capture the essential features needed for each specific application. This approach exploits the observation that different tasks require different subsets of the model's capacity, allowing for aggressive pruning without significant performance degradation.

## Foundational Learning

**Binary mask learning** - Learnable parameters that determine which computational units to keep or remove
*Why needed:* Enables task-specific pruning while preserving foundation model capabilities
*Quick check:* Verify masks converge to sparse binary values during training

**Sparsity-inducing loss** - Regularization term that encourages model compression
*Why needed:* Drives the network to identify and retain only essential units
*Quick check:* Monitor parameter retention ratio during training

**Frozen foundation model** - Keeping pretrained weights unchanged during mask learning
*Why needed:* Preserves the rich representations learned during foundation model training
*Quick check:* Ensure no gradient updates to base model parameters

**Threshold-based pruning** - Using learned threshold to determine which units to remove
*Why needed:* Provides clean separation between retained and pruned units
*Quick check:* Test different threshold values and their impact on performance

## Architecture Onboarding

**Component map:** Input -> Backbone (CLAP/Wav2Vec2/MusicFM) -> Binary masks -> Output layer -> Task-specific loss

**Critical path:** The backbone model processes audio features, binary masks gate individual units, and the output layer produces task predictions. The sparsity loss operates directly on the mask parameters.

**Design tradeoffs:** The method trades potential fine-tuning gains for computational efficiency and the ability to reuse foundation models across tasks. The binary mask approach is simpler than magnitude-based pruning but may be less flexible than continuous relaxation methods.

**Failure signatures:** Poor mask convergence leading to insufficient pruning, performance degradation from aggressive pruning, or suboptimal thresholds causing either excessive size or accuracy loss.

**First experiments:** 1) Test mask learning convergence on a simple classification task, 2) Evaluate different sparsity loss strengths, 3) Compare performance across different threshold values

## Open Questions the Paper Calls Out

**Open Question 1**
Can the structure of learned masks be leveraged to better understand the features learned by audio foundation models?
The authors explicitly state they seek to better understand the structure of these learned masks and how they could be leveraged to better understand the features learned by audio foundation models. This remains unresolved as the paper focuses on demonstrating compression effectiveness without analyzing what the masks reveal about which model components are task-relevant.

**Open Question 2**
Can this trimming approach be effectively extended to audio generative models that rely on learned representations?
The authors state they seek to extend their method to a wider set of applications, notably for audio generative modeling. This is unresolved as the current study only evaluates discriminative tasks, and generative models may have different redundancy characteristics.

**Open Question 3**
What causes the limited computational speedup in MusicFM despite significant model size reduction?
The authors note MusicFM shows less speedup than other models and attribute this gap to either an intrinsic bottleneck of the implementation used or to the architecture of the model, without resolving which. The paper reports only 1.3-1.4× speedup for MusicFM trimming versus 2.5-2.8× for other models.

**Open Question 4**
Can optimal trimming ratios be predicted or automatically determined based on task or dataset characteristics?
Results show the optimal trimming ratio is strongly dependent on the task, with complex tasks requiring larger subnetworks, but this currently requires manual tuning of the threshold parameter t. The paper manually selects t ∈ [0.3, 0.7] for different model-task combinations without providing a principled selection method.

## Limitations
- Focus on English-language audio tasks only, limiting generalizability to multilingual settings
- Method depends on frozen foundation models, preventing end-to-end fine-tuning
- Computational savings measured primarily on GPU platforms may not translate to embedded devices

## Confidence
- **High**: Effectiveness of masking approach for parameter reduction while maintaining performance
- **Medium**: Claimed speedups, as these may vary significantly across hardware platforms
- **Medium**: Generalization claims, given limited task and language diversity in evaluation

## Next Checks
1. Test the approach on multilingual audio tasks to assess cross-lingual generalization
2. Evaluate on audio generation tasks to determine if the method extends beyond classification/regression
3. Benchmark on resource-constrained edge devices to verify real-world deployment benefits