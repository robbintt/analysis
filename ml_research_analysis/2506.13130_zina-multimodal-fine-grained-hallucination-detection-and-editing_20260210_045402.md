---
ver: rpa2
title: 'ZINA: Multimodal Fine-grained Hallucination Detection and Editing'
arxiv_id: '2506.13130'
source_url: https://arxiv.org/abs/2506.13130
tags:
- object
- text
- original
- error
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of multimodal hallucination detection
  and editing in large language models, where outputs deviate from visual content.
  The authors propose a novel task requiring fine-grained detection of hallucinated
  spans, classification into six error types, and appropriate refinements.
---

# ZINA: Multimodal Fine-grained Hallucination Detection and Editing

## Quick Facts
- arXiv ID: 2506.13130
- Source URL: https://arxiv.org/abs/2506.13130
- Reference count: 40
- Key outcome: Novel two-stage approach achieves 15.8-point F1 improvement in hallucination detection and 0.34-0.37 point gains in editing metrics

## Executive Summary
This paper introduces ZINA, a novel two-stage approach for fine-grained multimodal hallucination detection and editing in large language models. The method addresses a critical gap in multimodal AI by not only identifying when outputs deviate from visual content but also classifying the type of hallucination and suggesting precise corrections. By decoupling token copying from hallucination detection and incorporating a reviewer component that validates detected errors, ZINA achieves significant performance improvements over strong baselines like GPT-4o and Llama-3.2. The approach is supported by VisionHall, a comprehensive dataset of 6.9k manually annotated samples and 20k synthetic samples generated through a graph-based method that captures error dependencies.

## Method Summary
ZINA employs a two-stage architecture where a detector MLLM identifies hallucinated spans and their error types, followed by a reviewer MLLM that validates these detections and suggests corrections. The method separates the responsibilities of token copying and hallucination detection using a deterministic tagging function, reducing task complexity. To train the model, the authors construct VisionHall, a dataset with 6.9k manually annotated samples and 20k synthetic samples generated via a graph-based method that captures error dependencies among hallucination types. The approach uses Qwen2.5-VL-72B-Instruct as the base model with LoRA fine-tuning, achieving significant improvements in both detection F1 scores and image captioning metrics.

## Key Results
- Achieves 15.8-point improvement in detection F1 over GPT-4o
- Improves image captioning metrics by 0.34 (CLIP-S) and 0.37 (PAC-S) points in editing
- Outperforms Llama-3.2 by 14.7 points in detection F1
- Ablation shows 23.24-point F1 drop when decoupling token copying from detection

## Why This Works (Mechanism)

### Mechanism 1: Decoupling of Token Copying and Detection
Separating deterministic token copying from hallucination detection reduces task complexity and improves accuracy. A deterministic function T inserts tags around hallucinated spans identified by Mdet, freeing the MLLM from the burden of verbatim text reproduction while simultaneously deciding where to tag. This addresses exposure bias in autoregressive generation where models must copy tokens and insert structural tags simultaneously.

### Mechanism 2: Two-Stage Detection-Review Verification
Sequential candidate generation followed by contextual verification improves precision over single-pass approaches. Mdet proposes candidate hallucinated spans with error types; Mrev receives pre-tagged text and validates whether each tag is appropriate within surrounding context, then outputs corrections. The reviewer benefits from seeing candidate errors explicitly marked, enabling focused verification rather than joint detection-and-correction.

### Mechanism 3: Graph-Based Error Dependency Modeling for Synthetic Data
Capturing hierarchical dependencies among error types produces more realistic synthetic training data. The Error Insertion module injects errors with explicit parent-child dependencies encoded in XML; GraphAug constructs DAG, removes cycles, then generates diverse samples via graph pruning. Real MLLM hallucinations exhibit dependency structures (e.g., wrong object → wrong attribute describing that object) rather than being independent.

## Foundational Learning

- **Concept**: Span-level sequence tagging as structured prediction
  - Why needed here: Task requires identifying specific word spans and classifying each into one of six error types plus "no error"
  - Quick check question: Given the sentence "Three red cats sit on the table" (image shows two blue cats), which words get tagged and with what labels?

- **Concept**: Directed Acyclic Graphs for dependency representation
  - Why needed here: GraphAug constructs DAG to model error dependencies; cycles must be removed for valid topological sampling
  - Quick check question: If errors E1→E2→E3 form a cycle E3→E1, which edge would you remove and why?

- **Concept**: LoRA fine-tuning for large vision-language models
  - Why needed here: Qwen2.5-VL-72B is adapted using LoRA rank 8; understanding parameter-efficient tuning is critical for reproduction
  - Quick check question: With LoRA rank 8 on a 72B model, approximately how many trainable parameters exist compared to full fine-tuning?

## Architecture Onboarding

- **Component map**:
  ```
  Input: (image ximg, MLLM description xdesc, reference caption xref)
      ↓
  Mdet (Qwen2.5-VL-72B-Instruct + LoRA): Outputs {(h_text, h_type)}
      ↓
  T (deterministic tagger): Inserts <type> tags around each h_text
      ↓
  Mrev (Qwen2.5-VL-72B-Instruct + LoRA): Validates tags, outputs corrections
      ↓
  Output: ŷ = {Ŷtext, Ŷedit, Ŷtype}
  ```

- **Critical path**:
  1. Prompt construction: `pdet = format({xdesc, ximg, xref, n few-shot examples})`
  2. Detection: `{h_text, h_type} = Mdet(pdet)` → comma-separated "word, tag" pairs
  3. Tagging: `zi = T(xdesc, h_text[i], h_type[i])` for each hallucination
  4. Review: `ŷ = Mrev(zi, ximg, xref)` → outputs "tagged_segment: replacement"

- **Design tradeoffs**:
  - Two-stage inference doubles latency vs. single-stage, but ablation shows 23+ point F1 gain
  - Reference captions required at inference (not just training); limits deployment to scenarios with ground-truth available
  - 3-shot prompting adds ~1.5k tokens to context; balances performance vs. context window

- **Failure signatures**:
  - Mrev "tends to be overly conservative in generating tags" → under-detection
  - Mdet may confuse `<relation>` with grammatical errors rather than spatial errors
  - High false negatives on "Fact" type due to sparse training examples

- **First 3 experiments**:
  1. **Ablation: Single-stage vs. Two-stage**: Replace pipeline with single MLLM doing joint detection+tagging; expect ~23 point F1 drop
  2. **Few-shot sensitivity**: Test n={0,1,2,3} few-shot examples; expect diminishing returns after n=3
  3. **Backbone swap**: Replace Qwen2.5-VL-72B with LLaVA-OV-72B; expect ~10-12 point F1 gap

## Open Questions the Paper Calls Out

### Open Question 1
Can the two-stage architecture of ZINA be condensed or optimized to reduce the high inference latency while maintaining the performance benefits of decoupling detection from token copying? The authors explicitly list high inference time as a limitation, stating that the "two-stage architecture results in relatively high inference time, which may hinder deployment in real-time or resource-constrained settings."

### Open Question 2
How can the reviewer MLLM ($M_{rev}$) be calibrated to reduce the rate of under-detection without increasing false positives? The authors note in the Limitations section that their "method often underdetects hallucinations" and speculate this is because the "reviewer MLLM... tends to be overly conservative in generating tags."

### Open Question 3
Is it possible to perform fine-grained hallucination detection and editing with similar accuracy without relying on human-written reference captions ($x_{ref}$)? The paper explicitly assumes the availability of human-written references, stating "reliable hallucination detection cannot be achieved using only the image as input."

## Limitations
- Reliance on reference captions during inference restricts deployment to scenarios with ground-truth captions
- Synthetic data generation depends on o3-mini API access and may not fully capture real hallucination diversity
- Reviewer component's tendency toward over-conservatism could limit recall in practical applications
- 72B model size creates significant computational barriers for broader adoption

## Confidence

**High Confidence** (Evidence strongly supports claims):
- The two-stage architecture significantly improves performance over single-stage approaches
- ZINA outperforms strong baselines including GPT-4o and Llama-3.2 across all evaluation metrics
- The synthetic data generation method produces realistic hallucination patterns when capturing error dependencies

**Medium Confidence** (Evidence supports claims but with some caveats):
- The six error type classification scheme captures meaningful distinctions in hallucination patterns
- Graph-based dependency modeling improves synthetic data quality compared to independent error injection
- The deterministic tagging function T reliably separates copying from detection responsibilities

**Low Confidence** (Claims have limited or conflicting evidence):
- The specific error type distribution in VisionHall accurately represents real-world hallucination patterns
- The few-shot prompting strategy (n=3) represents an optimal balance for all scenarios
- The computational cost of the two-stage approach is justified by performance gains in practical applications

## Next Checks

1. **Generalization Test**: Evaluate ZINA on out-of-domain image-caption pairs (e.g., medical imaging, scientific diagrams) to assess whether the two-stage architecture maintains its 15.8-point F1 advantage when distribution shifts occur.

2. **Reviewer Conservatism Analysis**: Systematically vary the temperature and top-k parameters of Mrev to quantify the trade-off between over-conservatism and false positive reduction, establishing optimal settings for different use cases.

3. **Synthetic Data Ablation**: Compare training with and without the graph-based dependency modeling component using identical base hallucination patterns to isolate the contribution of error dependency capture to overall performance.