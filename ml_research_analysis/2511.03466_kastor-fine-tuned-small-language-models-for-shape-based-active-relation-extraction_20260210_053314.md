---
ver: rpa2
title: 'Kastor: Fine-tuned Small Language Models for Shape-based Active Relation Extraction'
arxiv_id: '2511.03466'
source_url: https://arxiv.org/abs/2511.03466
tags:
- https
- extraction
- relation
- language
- patterns
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Kastor addresses the challenge of fine-tuning small language models
  for relation extraction in specialized domains using SHACL shapes. The method refines
  the traditional validation task by evaluating all possible property combinations
  from a shape and selecting the optimal combination for each training example, enhancing
  model generalization.
---

# Kastor: Fine-tuned Small Language Models for Shape-based Active Relation Extraction

## Quick Facts
- arXiv ID: 2511.03466
- Source URL: https://arxiv.org/abs/2511.03466
- Reference count: 40
- Improves F1 scores by ~10% through shape relaxation and active learning

## Executive Summary
Kastor addresses the challenge of fine-tuning small language models for relation extraction in specialized domains using SHACL shapes. The method refines the traditional validation task by evaluating all possible property combinations from a shape and selecting the optimal combination for each training example, enhancing model generalization. An iterative learning process with human annotation refines noisy knowledge bases to uncover new, relevant facts. Kastor achieves significant performance improvements, including a 10% increase in F1 scores, and demonstrates better pattern extension capacity and discovery rates compared to previous approaches. The framework is open, reusable, and shows promise for scaling to large datasets.

## Method Summary
Kastor reformulates shape-based relation extraction by relaxing the constraint that training graphs must validate against a single, maximal SHACL shape. Instead, it derives example-specific patterns (subsets of properties) for each training instance. The framework uses BART-base fine-tuned on TurtleLight-formatted triples, incorporating declarative inference rules to materialize logical consequences. An active learning loop identifies and re-injects human-validated "discoveries" into the training set. The approach achieves significant performance improvements through pattern relaxation and iterative refinement of noisy knowledge bases.

## Key Results
- Achieves 10% increase in F1 scores compared to baseline approaches
- Demonstrates 68% discovery rate in uncovering new, relevant facts
- Shows improved pattern extension capacity with flexible output patterns

## Why This Works (Mechanism)

### Mechanism 1: Achievable Pattern Relaxation
If a model is trained to predict only properties present in the source text (achievable patterns) rather than all properties defined in a schema (maximal shape), it may significantly reduce extrinsic hallucinations and improve generalization on sparse data. The framework relaxes the constraint that training graphs must validate against a single, maximal SHACL shape, instead deriving example-specific patterns for each training instance. This prevents the model from learning to "fill in" mandatory schema fields with unsupported data when the source text