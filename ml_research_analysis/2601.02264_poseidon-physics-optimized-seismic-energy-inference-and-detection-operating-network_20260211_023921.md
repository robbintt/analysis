---
ver: rpa2
title: 'POSEIDON: Physics-Optimized Seismic Energy Inference and Detection Operating
  Network'
arxiv_id: '2601.02264'
source_url: https://arxiv.org/abs/2601.02264
tags:
- learning
- energy
- prediction
- poseidon
- seismic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'POSEIDON introduces a physics-informed energy-based model that
  integrates seismological laws into deep learning for earthquake prediction. The
  model addresses three interconnected tasks: aftershock sequence identification,
  tsunami generation potential, and foreshock detection, outperforming gradient boosting,
  random forest, and CNN baselines with the highest average F1 score.'
---

# POSEIDON: Physics-Optimized Seismic Energy Inference and Detection Operating Network

## Quick Facts
- **arXiv ID**: 2601.02264
- **Source URL**: https://arxiv.org/abs/2601.02264
- **Reference count**: 38
- **Primary result**: Outperforms gradient boosting, random forest, and CNN baselines on earthquake prediction tasks with F1 scores of 0.78 (aftershocks), 0.85 (tsunami), 0.72 (foreshocks)

## Executive Summary
POSEIDON introduces a physics-informed energy-based model that integrates seismological laws into deep learning for earthquake prediction. The model addresses three interconnected tasks: aftershock sequence identification, tsunami generation potential, and foreshock detection, outperforming gradient boosting, random forest, and CNN baselines with the highest average F1 score. Key to the approach is embedding the Gutenberg-Richter magnitude-frequency relationship and Omori-Utsu aftershock decay law as learnable constraints, which yields scientifically interpretable parameters: b-value of 0.752 and Omori-Utsu parameters p=0.835, c=0.1948 days, all within established seismological ranges. The model is trained on the Poseidon dataset, the largest open-source global earthquake catalog with 2.8 million events spanning 30 years, featuring pre-computed energy features and spatial grid indices. Results demonstrate superior performance across all prediction tasks, with the physics-informed constraints enhancing rather than compromising accuracy.

## Method Summary
POSEIDON uses a two-stage training approach: first optimizing task-specific predictions with standard losses, then activating physics constraints in a second stage. The model encodes multi-scale spatiotemporal context (7/30/90 days) through CNN blocks with attention mechanisms, combines this with local event features, and maps to a 64-dimensional latent space. An energy function E_θ(z) assigns scalar energies to latent states, enabling anomaly detection beyond classification probabilities. Physics constraints on the Gutenberg-Richter b-value and Omori-Utsu parameters are parameterized with bounded activations and added as loss terms. The model outputs three task-specific probabilities (aftershock, tsunami, foreshock) while learning interpretable seismological parameters.

## Key Results
- Achieves F1 scores of 0.78 (aftershocks), 0.85 (tsunami), 0.72 (foreshocks), outperforming all baseline methods
- Learns scientifically interpretable parameters: b-value of 0.752 and Omori-Utsu parameters p=0.835, c=0.1948 days within established ranges
- Energy-based representation shows clear separation between normal events (mean energy -0.025) and anomalous events (mean energy 0.08)
- Multi-scale temporal encoding provides complementary predictive signals, with 7-day window most critical for aftershock prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding seismological laws as learnable constraints improves prediction while yielding interpretable parameters.
- Mechanism: Physics constraints (Gutenberg-Richter b-value, Omori-Utsu p/c parameters) are parameterized with bounded activations (e.g., b = 0.7 + 0.6·σ(θ_b)) and added as loss terms. This penalizes predictions violating physical laws while allowing data-driven refinement within scientifically plausible ranges.
- Core assumption: The true physical parameters lie within the bounded initialization ranges, and the loss weighting balance allows physics constraints to guide without dominating task learning.
- Evidence anchors:
  - [abstract] "embedding the Gutenberg-Richter magnitude-frequency relationship and Omori-Utsu aftershock decay law as learnable constraints, which yields scientifically interpretable parameters: b-value of 0.752 and Omori-Utsu parameters p=0.835, c=0.1948 days"
  - [Section IV.E] "The Gutenberg-Richter b-value is parameterized as b = 0.7 + 0.6·σ(θ_b), constraining b ∈ [0.7, 1.3]"
  - [corpus] Related work on physics-informed neural networks in geophysics confirms this approach remains underexplored for statistical seismological laws.
- Break condition: If learned parameters consistently converge to boundary values (e.g., b stuck at 0.7 or 1.3), the constraint ranges may be misspecified or task-physics objectives may be conflicting.

### Mechanism 2
- Claim: Energy-based representation enables anomaly detection and uncertainty quantification beyond classification probabilities.
- Mechanism: The model learns an energy function E_θ: Z → R assigning lower energy to observed configurations via contrastive loss. Normal events cluster around low energy (~-0.025), while anomalous events exhibit higher energy (~0.08), enabling threshold-based detection.
- Core assumption: The contrastive loss margin and perturbation strategy (ε noise) create meaningful energy separation without collapsing the representation space.
- Evidence anchors:
  - [abstract] "physics-informed energy-based model"
  - [Section IV.D] "Training uses contrastive loss encouraging lower energy for observed versus perturbed configurations"
  - [Section V.E] "Normal events cluster around mean energy of -0.025... while anomalous events exhibit higher energy values centered around 0.08"
  - [corpus] Limited direct evidence—energy-based models remain underexplored in geophysical applications per Section II.C.
- Break condition: If energy distributions for normal and anomalous events heavily overlap (no clear separation at threshold ~0.05), the contrastive loss margin or perturbation magnitude needs adjustment.

### Mechanism 3
- Claim: Multi-scale temporal encoding captures complementary seismic patterns across different prediction horizons.
- Mechanism: Three temporal windows (7, 30, 90 days) encode different seismicity aspects—short-term for aftershock patterns, long-term for background rates. Attention weights adapt per task: aftershock prediction weights 7-day highest (0.35), foreshock detection emphasizes 90-day (0.31).
- Core assumption: Each temporal scale provides independent predictive signal, and attention mechanisms correctly weight their contributions per task.
- Evidence anchors:
  - [Section IV.B] "We construct context grids at three temporal scales τ ∈ {7, 30, 90} days"
  - [Section V.F] "Ablation experiments removing individual scales confirm their complementary contributions, with 7-day removal most severely impacting aftershock F1"
  - [corpus] Spatio-temporal graph approaches (neighbor paper 21345) similarly leverage multi-scale temporal patterns.
- Break condition: If ablation shows single-scale removal has negligible impact, scales may be redundant or attention may not be learning meaningful distinctions.

## Foundational Learning

- Concept: **Energy-Based Models (EBMs)**
  - Why needed here: Core architecture component—understanding how E_θ maps latent states to scalar energies and how contrastive divergence training works is essential for debugging energy landscapes.
  - Quick check question: Can you explain why contrastive loss with softplus encourages lower energy for observed vs. perturbed configurations?

- Concept: **Physics-Informed Neural Networks (PINNs)**
  - Why needed here: The approach extends PINN principles from differential equations to statistical seismological laws; understanding loss-based constraint enforcement is critical.
  - Quick check question: How does parameterizing physics parameters with bounded activations (e.g., softplus, sigmoid) differ from hard constraints during backpropagation?

- Concept: **Multi-Task Learning with Class Imbalance**
  - Why needed here: Three tasks with extreme imbalance (tsunami: 1.14% positive); understanding focal loss, weighted sampling, and task-specific loss design is required.
  - Quick check question: Why does focal loss with γ=2.0 help tsunami detection more than standard BCE?

## Architecture Onboarding

- Component map:
  Input grids → Spatial encoder → Fusion layer → Energy augmentation → Task heads. Physics losses act only on training objective, not inference.

- Critical path: Input grids → spatial encoder → fusion → energy augmentation → task heads. Physics constraints parameterized as bounded activations and added as loss terms.

- Design tradeoffs:
  - Two-stage training (prediction-only → physics-activated) trades training time for stability
  - Constrained parameter ranges (b ∈ [0.7, 1.3]) trade flexibility for scientific interpretability
  - Weighted sampling trades batch diversity for rare-class visibility

- Failure signatures:
  - Physics parameters stuck at bounds → constraint ranges too narrow or λ_p too high
  - Energy distributions overlapping → contrastive margin too small or perturbation ε too large
  - Tsunami recall near 0 → focal loss γ too high or positive weight insufficient

- First 3 experiments:
  1. **Baseline ablation**: Train without physics constraints (λ_p = 0) to isolate their contribution to F1 scores.
  2. **Scale ablation**: Remove each temporal window (7/30/90 days) individually to verify complementary contributions per task.
  3. **Parameter sensitivity**: Vary b-value bounds (e.g., [0.5, 1.5]) and observe convergence behavior vs. final interpretability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the POSEIDON framework be extended to provide continuous probabilistic hazard forecasting rather than discrete event classification?
- Basis in paper: [explicit] The conclusion explicitly identifies "extension to continuous probabilistic hazard forecasting" as a primary direction for future work.
- Why unresolved: The current architecture formulates tasks as binary or multi-class classification problems (e.g., tsunami potential yes/no) rather than modeling continuous density functions required for operational forecasting.
- What evidence would resolve it: A modified architecture outputting spatiotemporal probability density functions evaluated against standard likelihood tests.

### Open Question 2
- Question: Does incorporating stress transfer physics, such as Coulomb stress changes, improve predictive capability beyond the current statistical constraints?
- Basis in paper: [explicit] The conclusion lists "incorporation of stress transfer physics" as a targeted area for future research.
- Why unresolved: The current model relies on statistical laws (Omori-Utsu, Gutenberg-Richter) but does not model the mechanical stress loading that physically drives earthquake triggering.
- What evidence would resolve it: Comparative experiments showing improved spatial prediction accuracy when stress transfer terms are added to the energy-based loss function.

### Open Question 3
- Question: Why does the learned Bath's law parameter (ΔM ≈ -0.13) deviate significantly from the established theoretical expectation of ≈ 1.2?
- Basis in paper: [inferred] The results section notes the learned parameter deviates from the theoretical value, suggesting the largest aftershock is nearly equal in magnitude to the mainshock, but offers no physical explanation.
- Why unresolved: This deviation contradicts standard seismological understanding and may indicate the constraint is too weak or that the data filtering (M5.0+) biases the magnitude differences.
- What evidence would resolve it: A sensitivity analysis of the Bath's law loss weight or a data analysis verifying magnitude differences in the M5.0+ subset.

## Limitations
- Architectural specifics remain underspecified: exact CNN layer configurations, fusion network structure, and energy function architecture are not detailed
- Hyperparameter sensitivity: critical values (physics loss weights λ_p, contrastive margin m, focal loss γ, perturbation ε) are referenced but not explicitly stated
- Sample construction ambiguity: the process converting 48,023 trigger events to 38,418 training samples lacks clarity, particularly regarding negative sampling strategy for the highly imbalanced tsunami class

## Confidence

**High confidence**: Outperformance of gradient boosting, random forest, and CNN baselines on F1 metrics across all three tasks.

**Medium confidence**: Physics-informed constraints improve both prediction accuracy and parameter interpretability. The learned parameters fall within expected ranges, but ablation showing performance degradation without physics is needed for full validation.

**Medium confidence**: Multi-scale temporal encoding provides complementary predictive signal. Ablation results support this, but the mechanism by which attention weights adapt per task could be more thoroughly explored.

## Next Checks

1. **Physics constraint ablation**: Train POSEIDON without physics losses (λ_p=0) and measure performance degradation across all three tasks to quantify the contribution of physical interpretability to predictive accuracy.

2. **Temporal scale sensitivity**: Systematically remove each temporal window (7/30/90 days) and measure task-specific performance impact to verify the claimed complementary contributions, particularly the 7-day importance for aftershock prediction.

3. **Energy landscape verification**: Visualize energy distributions for normal versus anomalous events across multiple datasets to confirm the claimed separation (mean -0.025 vs 0.08) and test sensitivity to contrastive margin and perturbation magnitude.