---
ver: rpa2
title: 'Differentiable Logic Synthesis: Spectral Coefficient Selection via Sinkhorn-Constrained
  Composition'
arxiv_id: '2601.13953'
source_url: https://arxiv.org/abs/2601.13953
tags:
- operations
- ternary
- phase
- accuracy
- routing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Hierarchical Spectral Composition, a differentiable
  architecture for learning Boolean logic that adapts the Manifold-Constrained Hyper-Connections
  framework to logic synthesis. The method uses Sinkhorn-constrained routing with
  column-sign modulation to compose spectral coefficients selected from a frozen Boolean
  Fourier basis, enabling exact ternary representations of Boolean functions.
---

# Differentiable Logic Synthesis: Spectral Coefficient Selection via Sinkhorn-Constrained Composition

## Quick Facts
- arXiv ID: 2601.13953
- Source URL: https://arxiv.org/abs/2601.13953
- Authors: Gorgi Pavlov
- Reference count: 33
- Primary result: 100% accuracy on all 16 Boolean operations at n=2, with exact ternary quantization and zero routing drift

## Executive Summary
This paper introduces Hierarchical Spectral Composition, a differentiable architecture for learning Boolean logic that combines spectral coefficient selection with Sinkhorn-constrained routing. The method uses a frozen Boolean Fourier basis and learns ternary masks {-1,0,+1} via Gumbel-Softmax relaxation, achieving exact synthesis of small Boolean functions. The architecture demonstrates that ternary polynomial threshold representations exist for all tested Boolean functions, with hardware-efficient implementations showing 10,959 MOps/s on GPU.

## Method Summary
The method learns Boolean functions by selecting spectral coefficients from a frozen Walsh-Hadamard basis, using Gumbel-Softmax relaxation to produce ternary masks. Training proceeds in phases: Phase 1 learns individual coefficients via gradient descent on the Fourier basis; Phase 2 adds Sinkhorn-constrained routing with column-sign modulation for compositional logic; Phase 3 scales to larger functions using MCMC refinement. The architecture achieves exact quantization to {-1,0,+1} weights while maintaining differentiability during training, enabling hardware-efficient single-cycle combinational logic inference.

## Key Results
- Phase 1 (n=2): All 16 Boolean operations achieve 100% accuracy with zero routing drift
- Phase 2 (n=3): 10 operations including majority and parity achieve 100% accuracy with 39% sparsity
- Phase 3 (n=4): 10 operations achieve 100% accuracy via spectral synthesis with MCMC refinement, achieving 36% sparsity
- Hardware efficiency: 10,959 MOps/s on GPU with ternary masks for single-cycle combinational logic inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient descent identifies correct spectral coefficients because the Fourier basis orthogonality causes gradient cancellation on irrelevant characters while accumulating on the correct one.
- Mechanism: For XOR (y=ab), the gradient ∇wL ∝ −y·ϕ(a,b) summed over all inputs yields [0,0,0,−4]⊤ — terms for {1,a,b} cancel while ab accumulates.
- Core assumption: The Boolean Fourier basis {χS} is orthonormal under uniform measure.
- Evidence anchors: Section 4.1.2 proves parity gradient accumulation via orthogonality; FourierCSP validation of spectral grounding.
- Break condition: If basis characters were correlated or loss landscape had competing local minima near the correct support.

### Mechanism 2
- Claim: Sinkhorn projection preserves identity mappings during routing by constraining matrices to the Birkhoff polytope.
- Mechanism: Alternating row/column normalization (20 iterations) projects any positive matrix onto doubly stochastic manifold.
- Core assumption: Doubly stochastic matrices maintain bounded spectral norms and compositional closure.
- Evidence anchors: Section 5.2 shows 0.8234 routing drift vs 0.0000 for identity init; unconstrained routing suffers 18.75% quantization loss.
- Break condition: If routing required more than 20 Sinkhorn iterations for convergence at scale.

### Mechanism 3
- Claim: Column-sign modulation enables Boolean negation by factorizing routing as R = P·s.
- Mechanism: Doubly stochastic P cannot express negation; adding s ∈ {−1,+1}^n allows NAND = −AND while preserving ∥P∥2 ≤ 1.
- Core assumption: Sign learning via tanh relaxation converges to discrete {−1,+1} with annealing.
- Evidence anchors: Sign-Only diagnostic achieves 100% accuracy; No Sign Mod. ablation caps at exactly 75%.
- Break condition: If gradient interference between sign learning and routing optimization prevented joint convergence.

## Foundational Learning

- Concept: **Boolean Fourier Analysis / Walsh-Hadamard Basis**
  - Why needed here: The entire architecture selects coefficients from frozen spectral basis; understanding Eq. 1 (f(x) = Σ f̂(S)χS(x)) is prerequisite.
  - Quick check question: Given n=2, can you enumerate the 4 characters {1, a, b, ab} and explain why they're orthonormal under uniform measure?

- Concept: **Birkhoff Polytope and Sinkhorn-Knopp Algorithm**
  - Why needed here: Routing constraints depend on doubly stochastic projection; without this, quantization fails (18.75% drop in ablation).
  - Quick check question: Given a 3×3 positive matrix, perform 2 iterations of alternating row/column normalization — does it approach doubly stochastic?

- Concept: **Gumbel-Softmax Reparameterization**
  - Why needed here: Direct ternary quantization via STE fails; categorical relaxation over {-1,0,+1} enables gradient flow.
  - Quick check question: Why does temperature annealing (τ: 1.0→0.01) transition from exploration to hard selection?

## Architecture Onboarding

- Component map: Fourier Basis Expansion -> Gumbel-Softmax Ternary Layer -> Sinkhorn Projection -> Column-Sign Modulation -> Output
- Critical path: Phase 1 (n=2 coefficient selection) -> Phase 2 (routing + signs) -> Phase 3/4 (scaling)
- Design tradeoffs:
  - Identity init (γ=2.0) stabilizes but biases toward simple routing; random init converges only 3/10 seeds
  - Sequential training (XOR→AND→OR→IMPLIES) avoids gradient interference but increases training time
  - MCMC refinement at n=4 guarantees 100% but adds ~110-1387 steps depending on initialization
- Failure signatures:
  - XOR stuck at 75% accuracy → parity character not emerging; increase training steps or check L1 penalty
  - Routing drift > 0.5 → identity bias γ too low or Sinkhorn iterations insufficient
  - Quantization loss > 5% → temperature annealing too fast; slow τ decay
- First 3 experiments:
  1. **Validate Phase 1**: Train XOR only with Gumbel-Softmax; verify |c_ab| > 0.9 after 3000 steps and 75% sparsity
  2. **Sign-Only Diagnostic**: Fix P=I, learn s only for all 8 linear ops (XOR→NOR). Target: 100% accuracy with zero routing drift
  3. **Warm-Start Comparison**: For n=4, compare MCMC convergence from random, WHT-threshold, and GD warm-start. Expected: WHT ≈ 110 steps, random ≈ 1387, GD ≈ 1557

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative gradient relaxations or training objectives be developed to close the performance gap with spectral methods for n ≥ 4?
- Basis in paper: The warm-start experiment (Section 7.5) demonstrates that gradient descent initialization results in slower MCMC convergence (1557 steps) than random initialization (1387 steps) and significantly lags behind Walsh-Hadamard Transform initialization (110 steps).
- Why unresolved: The authors identify that soft Gumbel-Softmax weights "don't quantize well for discrete MCMC," but do not propose a differentiable relaxation that preserves discrete topology effectively at higher dimensions.
- What evidence would resolve it: A gradient-based initialization method that matches the MCMC convergence speed of exact spectral initialization for 4-variable functions.

### Open Question 2
- Question: Does increasing routing sparsity (k > 1) or hierarchical depth allow the architecture to express "nonlinear" Boolean operations without direct mask learning?
- Basis in paper: Section 4.3.2 notes that 8 out of 16 operations (e.g., IF-THEN-ELSE) lie outside the expressivity of single-parent (k=1) Sinkhorn routing and currently require "direct mask learning or expansion of the primitive set."
- Why unresolved: The paper validates only k=1 routing; it does not experimentally test if allowing each child to route to multiple parents (higher k) or deeper compositional chains resolves the expressivity gap.
- What evidence would resolve it: Successful derivation of correct ternary masks for nonlinear operations using only hierarchical routing from base primitives, without per-operation mask optimization.

### Open Question 3
- Question: Can Goldreich-Levin algorithms be integrated to improve the query complexity of spectral estimation for large-scale functions (n > 28)?
- Basis in paper: Section 10 (Future Work) explicitly lists "Goldreich-Levin Implementation: Bucket-splitting" as a remaining challenge, noting the current Monte Carlo estimator is a baseline without advanced query complexity bounds.
- Why unresolved: The current Monte Carlo approach has query complexity O(n^d/ϵ^2), whereas bucket-splitting could theoretically achieve Õ(k/ϵ^2) for recovering k heavy coefficients.
- What evidence would resolve it: An implementation that recovers heavy spectral coefficients for functions with n > 28 variables using the theoretical query complexity of Goldreich-Levin methods.

## Limitations

- Scalability remains untested: Method demonstrates exact synthesis only for n≤4 functions, with MCMC refinement requiring 110-1387 steps at n=4
- Theoretical gaps: While ternary representations exist for tested functions, no proof exists for arbitrary Boolean functions
- Hardware validation: GPU performance claims (10,959 MOps/s) lack FPGA/ASIC implementation verification

## Confidence

**High Confidence** (Strong evidence, multiple anchors):
- Phase 1 spectral coefficient selection mechanism
- Phase 2 Sinkhorn routing stability with identity initialization
- Phase 3 exact quantization to ternary masks

**Medium Confidence** (Good evidence but limited scope):
- Hardware efficiency claims - single GPU measurement
- MCMC refinement effectiveness - convergence depends heavily on initialization
- Sequential training benefits - empirical observation without theoretical justification

**Low Confidence** (Weak or no supporting evidence):
- Scalability to n>4 functions - completely untested
- Ternary representation existence theorem - claimed but not proven
- Generalization to non-Boolean domains - architecture appears specific to Boolean logic

## Next Checks

1. **Scalability Test**: Apply the method to random Boolean functions at n=5 and n=6. Measure success rate, convergence steps, and sparsity compared to n=4 baseline to validate whether MCMC refinement scales linearly or exponentially.

2. **Hardware Implementation Validation**: Implement the ternary logic synthesis on FPGA/ASIC to verify the claimed 10,959 MOps/s performance. Measure actual power consumption, area utilization, and timing closure compared to conventional synthesis approaches.

3. **Ablation of Sequential Training**: Compare joint training (all operations simultaneously) versus sequential training (XOR→AND→OR→IMPLIES) for n=3. Quantify gradient interference effects and training efficiency to validate the claimed benefits of sequential approach.