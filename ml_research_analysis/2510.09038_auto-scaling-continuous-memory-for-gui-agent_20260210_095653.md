---
ver: rpa2
title: Auto-scaling Continuous Memory for GUI Agent
arxiv_id: '2510.09038'
source_url: https://arxiv.org/abs/2510.09038
tags:
- memory
- task
- page
- agent
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a continuous memory framework for GUI agents
  to improve generalization in unfamiliar interfaces and long-horizon tasks. Unlike
  prior text-based memory systems that inflate context length and lose critical visual
  details, the proposed approach compresses GUI trajectories into fixed-length continuous
  embeddings using the vision-language model itself as an encoder.
---

# Auto-scaling Continuous Memory for GUI Agent

## Quick Facts
- arXiv ID: 2510.09038
- Source URL: https://arxiv.org/abs/2510.09038
- Reference count: 40
- Primary result: Continuous memory embeddings enable GUI agents to match or exceed closed-source models across benchmarks while scaling to 100k+ trajectories at low cost

## Executive Summary
This paper introduces a continuous memory framework for GUI agents that compresses multimodal trajectories into fixed-length embeddings using the vision-language model itself as an encoder. Unlike prior text-based memory systems that inflate context length and lose critical visual details, the proposed approach preserves fine-grained visual cues like widget size and position while enabling efficient knowledge reuse. The system achieves state-of-the-art performance across real-world GUI benchmarks, with performance improving monotonically as memory size scales.

The key innovation is an auto-scaling data flywheel that autonomously discovers new environments, synthesizes tasks using an open-source VLM, rolls out agent trajectories, and verifies success—collecting over 100k trajectories for about $4k. Only the memory encoder (1.2% parameters) is fine-tuned on 1.5k samples using LoRA and Q-Former. The method generalizes well to out-of-domain GUI environments and maintains low inference latency, making it practical for real-world deployment.

## Method Summary
The system compresses GUI trajectories (screenshots + actions) into fixed-length continuous embeddings using a Q-Former from BLIP-2, which serves as a memory encoder fine-tuned with LoRA (rank 16, 1.2% parameters). These embeddings are prepended directly to the VLM's input layer, avoiding token inflation. A four-phase auto-scaling data flywheel—(1) environment discovery via search, (2) task synthesis with open-source VLM, (3) trajectory rollout, and (4) success verification with judge VLM—collects 100k+ trajectories at low cost. The retrieval system uses CLIP encoder + FAISS for top-k retrieval, and performance scales monotonically with memory size and retrieval depth.

## Key Results
- Qwen-2.5-VL-7B with continuous memory matches or exceeds state-of-the-art closed-source models (GPT-4o, Claude-4) across real-world GUI benchmarks
- Performance improves monotonically with memory size and retrieval depth, unlike text memories that degrade with long prompts
- System generalizes well to out-of-domain GUI environments (20-30% performance drop vs. web-trained)
- Auto-scaling flywheel collects 100k+ trajectories for about $4k with minimal human annotation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Continuous embeddings preserve fine-grained visual cues that text-based memory loses, enabling more reliable GUI action selection.
- **Mechanism:** A Q-Former (from BLIP-2) compresses each multimodal trajectory (screenshots + actions) into a fixed-length sequence of 8 continuous embeddings. These embeddings are prepended directly to the VLM's input embedding layer (not concatenated as tokens), allowing the model to attend to compressed visual-semantic information without inflating token count.
- **Core assumption:** The Q-Former can encode spatial-visual relationships (widget size, position) into continuous vectors better than text discretization. **Assumption:** This transfer generalizes to unseen GUI layouts.
- **Evidence anchors:**
  - [abstract] "preserves fine-grained visual cues (e.g., exact widget size and position)... plugged directly into the backbone's input layer"
  - [section 4.2] "long trajectories that often exceed 15,000 tokens can be compressed to as few as 8 vectors"
  - [corpus] SCoPE VLM addresses similar context-efficiency tradeoffs in VLM document navigation; EchoTrail-GUI builds action-aligned memory, suggesting compression quality matters for downstream grounding.
- **Break condition:** If retrieved embeddings fail to generalize to layouts with novel widgets or interaction patterns, performance degrades. Paper acknowledges retrieval can drift under "extreme UI shifts."

### Mechanism 2
- **Claim:** Monotonic performance scaling emerges because continuous embeddings avoid the context-noise cascade that degrades text-based retrieval.
- **Mechanism:** Unlike text memories—where more retrieved items increase sequence length, attention cost, and semantic noise—continuous memory adds fixed-length embeddings regardless of count. The policy attends to `m_t` (concatenated continuous vectors) alongside current observation, and performance improves log-linearly with memory size (Eq. 3) and retrieval depth (Fig. 1b).
- **Core assumption:** The retrieval function surfaces genuinely relevant trajectories, and the VLM can integrate multiple compressed exemplars without interference. **Assumption:** The log-linear scaling trend (fitted empirically) continues at larger scales.
- **Evidence anchors:**
  - [abstract] "performance improves monotonically, unlike text memories that degrade with long prompts"
  - [section 5.2] "text-based memories peak and then deteriorate beyond roughly ten retrieved items"
  - [corpus] Darwinian Memory and MGA both emphasize memory regulation for dynamic environments, supporting the hypothesis that unregulated memory accumulation (especially in text form) creates noise.
- **Break condition:** If retrieval quality drops (e.g., domain shift, semantic drift) or the model's attention capacity saturates, monotonic gains may plateau or reverse.

### Mechanism 3
- **Claim:** The auto-scaling data flywheel generates diverse, verifiable trajectories at low marginal cost, sustaining memory growth without human annotation.
- **Mechanism:** Four-phase cycle—(1) discover environments via search, (2) synthesize tasks with an open-source VLM, (3) roll out trajectories with the agent, (4) verify success with a judge VLM—continuously expands task pool Q, environment pool E, and trajectory pool T. Only verified trajectories enter memory.
- **Core assumption:** Open-source VLMs can synthesize solvable tasks and accurately judge success; agent rollouts produce useful trajectories even when imperfect. **Assumption:** Diversity from search + synthesis outweighs distributional bias toward "easy" sites.
- **Evidence anchors:**
  - [abstract] "collect 100k+ trajectories for about $4000... fine-tune only the memory encoder (LoRA on a Q-Former, 1.2% parameters) with 1,500 samples"
  - [section 4.1] "spend about $4k to collect more than 100k trajectories spanning from 10k environments"
  - [corpus] GUI-Shift and ZeroGUI (cited in appendix) use self-supervised or synthetic data for GUI agents, corroborating that synthetic task generation can work, though success verification remains a bottleneck.
- **Break condition:** If VLM judges systematically admit failures or reject valid successes, memory quality degrades. Paper notes "self-reinforcing loops may overfit popular layouts."

## Foundational Learning

- **Concept: Q-Former (BLIP-2)**
  - **Why needed here:** Core compression module; must understand how it bridges frozen vision encoder and LLM via learned queries.
  - **Quick check question:** Can you explain how Q-Former queries attend to image patches and compress them into a fixed number of output tokens?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** CoMEM is a retrieval-based memory system; understanding embedding-based retrieval (CLIP + FAISS) is prerequisite.
  - **Quick check question:** How does embedding-based retrieval differ from keyword search, and what failure modes arise from semantic drift?

- **Concept: LoRA Fine-Tuning**
  - **Why needed here:** Only 1.2% of parameters are trained (LoRA on Q-Former); understanding low-rank adaptation is essential for efficient onboarding.
  - **Quick check question:** What constraints does LoRA impose on the learned update, and why does it enable sample-efficient fine-tuning?

## Architecture Onboarding

- **Component map:**
  Memory Encoder (Q-Former with LoRA) → Retrieval System (CLIP + FAISS) → Data Flywheel (Search → Task Synthesis → Rollout → Judge) → Inference Integration (embeddings prepended to VLM input)

- **Critical path:**
  1. Stand up the flywheel on a small seed dataset (e.g., Mind2Web training set)
  2. Train memory encoder (LoRA + Q-Former) on 1.5k high-quality trajectories
  3. Validate retrieval quality and scaling behavior before full-scale collection

- **Design tradeoffs:**
  - Compression ratio (8 vs. more embeddings) vs. inference latency
  - Retrieval depth (k=3,10,50,100) vs. attention overhead (though continuous memory mitigates this)
  - Judge strictness vs. memory diversity

- **Failure signatures:**
  - Retrieval drift: Retrieved trajectories from wrong domain hurt performance (Table 3 shows text memory degrades OOD)
  - Judge over-acceptance: Memory fills with low-quality trajectories
  - Embedding misalignment: If Q-Former isn't well-tuned, compressed embeddings lose critical visual cues

- **First 3 experiments:**
  1. **Ablation on embedding count:** Compare k=3,10,50 retrieved items on MMInA Shopping; verify monotonic scaling matches Fig. 1b
  2. **OOD transfer test:** Train memory encoder on web data, evaluate on GUI-Odyssey (mobile) and OSWorld (desktop) per Table 3
  3. **Data flywheel sanity check:** Run flywheel for 100 new environments; manually inspect 20 trajectories for task diversity and judge accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive retrieval policies conditioned on agent uncertainty improve memory utilization compared to fixed top-k retrieval?
- Basis in paper: [explicit] Conclusion states: "Future work includes adaptive retrieval policies conditioned on uncertainty."
- Why unresolved: Current system uses fixed k without considering task difficulty or agent confidence; unclear whether uncertainty signals can be reliably extracted and used for retrieval.
- What evidence would resolve it: Ablation study comparing fixed-k vs. uncertainty-conditioned retrieval across tasks of varying difficulty, measuring both accuracy and retrieval efficiency.

### Open Question 2
- Question: How should memory systems handle temporal decay and freshness when websites evolve or interfaces change?
- Basis in paper: [inferred] Limitations section notes "freshness, deduplication, and provenance are hard to govern" and retrieval "can drift under extreme UI shifts."
- Why unresolved: The paper evaluates on static benchmarks; real websites change frequently, potentially making stored trajectories outdated or misleading.
- What evidence would resolve it: Longitudinal experiments where stored memories age over weeks/months, measuring performance degradation and testing age-aware eviction or decay mechanisms.

### Open Question 3
- Question: Can incorporating non-visual state information (DOM trees, accessibility trees, execution traces) complement screenshot-only memory?
- Basis in paper: [explicit] Limitations state: "screenshot-only inputs underrepresent non-visual states; expanding retrieval to execution traces or UI graphs may help."
- Why unresolved: Current design intentionally excludes DOM for vision-first paradigm, but this may miss structural cues invisible in screenshots (e.g., hidden elements, semantic relationships).
- What evidence would resolve it: Hybrid memory experiments adding DOM or accessibility-tree embeddings, comparing performance on tasks requiring structural understanding vs. pure visual grounding.

### Open Question 4
- Question: What safeguards prevent the auto-scaling data flywheel from reinforcing systematic biases or overfitting to "easy" websites?
- Basis in paper: [inferred] Limitations note "self-reinforcing loops may overfit popular layouts or 'easy' sites" and VLM judges "can admit failures or reject valid successes."
- Why unresolved: The flywheel uses VLM-generated tasks and VLM verification, creating a closed loop where systematic blind spots could amplify.
- What evidence would resolve it: Analysis of trajectory diversity across website categories; error analysis of false-positive/negative verification decisions; comparison with human-verified subsets.

## Limitations

- The Q-Former-based continuous memory encoder shows domain-specific encoding with 20-30% performance drops when transferring from web to mobile/desktop environments
- The auto-scaling flywheel may create self-reinforcing loops that overfit to popular web layouts, limiting diversity
- Judge VLM accuracy directly impacts memory quality, with systematic errors in success verification potentially contaminating the trajectory bank

## Confidence

**High confidence** in the core mechanism: The continuous embedding approach demonstrably avoids the context-length explosion and noise accumulation that plague text-based memory systems. The log-linear scaling relationship between memory size and performance is empirically validated and theoretically sound given the fixed-length embedding representation.

**Medium confidence** in the data flywheel's diversity claims: While 100k+ trajectories were collected at low cost, the actual distribution across environments, task types, and UI patterns is not fully characterized. The reliance on open-source VLMs for task synthesis and verification introduces potential systematic biases that aren't quantified.

**Low confidence** in cross-domain generalization: The paper acknowledges but doesn't thoroughly investigate how continuous memory performs when transferred to radically different GUI paradigms (mobile touch interfaces, desktop applications, AR/VR). The 20-30% performance drop in OOD settings suggests significant domain dependence.

## Next Checks

1. **Retrieval quality audit**: Implement a manual evaluation protocol where 100 random retrieval cases are examined by human annotators to assess semantic relevance between queries and retrieved trajectories, measuring precision@k and semantic drift.

2. **Domain robustness test**: Train separate memory encoders on web, mobile, and desktop datasets, then perform cross-domain evaluation to quantify the transfer gap and identify which UI features (widget types, interaction patterns) cause encoding failures.

3. **Judge accuracy calibration**: Create a ground-truth validation set of 500 trajectories with human-verified success labels, then compare judge VLM accuracy against this baseline to quantify false positive/negative rates and their impact on memory quality.