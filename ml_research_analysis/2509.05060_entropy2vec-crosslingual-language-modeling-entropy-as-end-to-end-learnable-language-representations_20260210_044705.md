---
ver: rpa2
title: 'Entropy2Vec: Crosslingual Language Modeling Entropy as End-to-End Learnable
  Language Representations'
arxiv_id: '2509.05060'
source_url: https://arxiv.org/abs/2509.05060
tags:
- language
- languages
- uriel
- typological
- tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ENTROPY2VEC, a novel framework that derives
  cross-lingual language representations by leveraging the entropy of monolingual
  language models. Unlike traditional typological inventories that suffer from feature
  sparsity and static snapshots, ENTROPY2VEC uses the inherent uncertainty in language
  models to capture typological relationships between languages.
---

# Entropy2Vec: Crosslingual Language Modeling Entropy as End-to-End Learnable Language Representations

## Quick Facts
- **arXiv ID:** 2509.05060
- **Source URL:** https://arxiv.org/abs/2509.05060
- **Reference count:** 20
- **One-line primary result:** Introduces ENTROPY2VEC, a framework that derives cross-lingual language representations by leveraging the entropy of monolingual language models to capture typological relationships between languages.

## Executive Summary
ENTROPY2VEC introduces a novel approach to deriving cross-lingual language representations by leveraging the entropy of monolingual language models. Instead of relying on static typological inventories, this method trains a separate language model for each language and uses the perplexity scores when evaluating these models on other languages to create dense, non-sparse language embeddings. These entropy-based vectors capture typological relationships between languages and can be updated with new data, addressing limitations of traditional approaches like sparsity and missing values. The method demonstrates competitive performance in downstream multilingual NLP tasks while aligning with established typological categories.

## Method Summary
The method involves training 33 separate small GPT-2 models (4 layers, 8 heads, 512-dim embeddings) on 33 different languages using character-level tokenization with byte fallback. Each model is evaluated on the test sets of all 33 languages, producing a 33-dimensional vector of perplexity scores for each language. These vectors are then clustered using DBSCAN to induce typological trees, which are compared against gold-standard trees like Glottolog using Robinson-Foulds distance. The resulting language embeddings are dense, complete, and adaptable to different timeframes, making them suitable for downstream multilingual applications.

## Key Results
- ENTROPY2VEC embeddings align with established typological categories and achieve competitive performance in downstream multilingual NLP tasks
- The method yields dense, non-sparse language embeddings that are adaptable to different timeframes and free from missing values
- The framework achieved competitive performance in downstream multilingual NLP tasks, such as those addressed by the LinguAlchemy framework

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Perplexity as a Similarity Proxy
Training a monolingual LM on language A and measuring its perplexity on language B provides a quantitative signal of their typological distance. An LM optimizes its parameters to minimize the entropy of next-token predictions for its training language. When evaluated on a new language, the model's entropy (perplexity) inversely correlates with structural similarityâ€”lower perplexity suggests shared patterns (e.g., syntax, morphology) between the languages.

### Mechanism 2: Dense Vector Construction via Entropy Profiles
Concatenating entropy scores from multiple monolingual LMs into a vector yields a dense, complete representation of a target language's typological profile. For language $L_i$, evaluate $N$ monolingual LMs $\{\theta_{L_1}, \dots, \theta_{L_N}\}$ on a corpus $D_{L_i}$, producing an $N$-dimensional vector $Z_{L_i} = [L(\theta_{L_1}, D_{L_i}), \dots, L(\theta_{L_N}, D_{L_i})]$.

### Mechanism 3: Hierarchical Clustering for Tree Induction
Density-based clustering (DBSCAN) on ENTROPY2VEC vectors can induce typological trees that align with expert-curated taxonomies like Glottolog. DBSCAN groups languages into clusters based on density in the ENTROPY2VEC vector space. These clusters are then organized into a hierarchy (tree), mimicking the structure of language families.

## Foundational Learning

- **Concept: Perplexity and Cross-Entropy in Language Models**
  - Why needed here: This is the core metric. You must understand that perplexity is the exponentiation of the cross-entropy loss and measures how "surprised" a model is by a sequence. The entire method hinges on using this score as a proxy for language distance.
  - Quick check question: If a language model trained on Spanish is evaluated on Portuguese, would you expect its perplexity to be higher or lower than when evaluated on Japanese, and why?

- **Concept: Linguistic Typology and Language Families**
  - Why needed here: The paper's central goal is to reproduce known typological relationships (e.g., Indo-European, Austronesian). Without this background, you cannot evaluate the quality of the generated trees or the validity of the claims.
  - Quick check question: Based on the paper, would you expect ENTROPY2VEC to group Indonesian, Malay, and Javanese into the same cluster? (Answer: Yes, as they are Austronesian/Malayic languages).

- **Concept: Clustering Algorithms (DBSCAN vs. Agglomerative)**
  - Why needed here: The paper explicitly chooses DBSCAN over linkage-based methods. Understanding why (handling non-uniform density, arbitrary cluster shapes, no predefined `k`) is critical to implementing the tree induction component correctly.
  - Quick check question: Why does the paper argue that linkage-based clustering is unsuitable for generating language clusters? (Answer: Requires predefined `k`, sensitive to noise/density variations).

## Architecture Onboarding

- **Component map:** Data Preparation -> Monolingual LM Training -> Entropy/Perplexity Extraction -> Vector Construction -> Tree Induction -> Downstream Application
- **Critical path:** The correctness of the ENTROPY2VEC vector hinges on the **perplexity extraction** step. Any inconsistency in model training (e.g., different hyperparameters, tokenizers) or evaluation will inject noise into the vectors, invalidating subsequent clustering and downstream results.
- **Design tradeoffs:**
  - **Model Size vs. Speed:** The paper uses a small GPT-2 model for speed. A larger model might capture more nuanced typological features but would drastically increase training and inference costs for all 33 languages.
  - **Tokenizer Choice:** The paper uses a character-level tokenizer. This helps with low-resource languages and cross-lingual evaluation but may limit the model's ability to capture higher-level semantic or morphological patterns compared to subword tokenization.
  - **Clustering Algorithm:** The choice of DBSCAN avoids specifying the number of clusters but requires tuning `epsilon` and `min_samples`. Poor tuning can lead to trivial clusters (all points in one or many single-point clusters).
- **Failure signatures:**
  1. **High-Degree Misalignment with Gold Trees:** A high Robinson-Foulds distance compared to baselines indicates the ENTROPY2VEC vectors are not capturing the intended typological signal.
  2. **No Improvement in Downstream Tasks:** If LinguAlchemy with ENTROPY2VEC vectors performs worse than using URIEL vectors, it suggests the vectors lack useful information for cross-lingual transfer.
  3. **Sensitivity to Script:** The paper notes that languages with different scripts (e.g., Thai vs. Lao) may be separated prematurely. This indicates the perplexity signal is dominated by surface-level character distributions rather than deep structural similarity.
- **First 3 experiments:**
  1.  **Reproduce the Primary Result:** Train the 33 monolingual LMs, extract perplexity vectors, and generate a typological tree. Compare its Robinson-Foulds distance to the reported Glottolog tree to validate the pipeline.
  2.  **Ablation on Model Size:** Retrain all models with a *smaller* (e.g., 2-layer) and a *larger* (e.g., 6-layer) GPT-2 architecture. Measure the change in alignment with Glottolog. This tests the assumption that small LMs are sufficient.
  3.  **Probe Script Bias:** Select pairs of closely related languages with different scripts (e.g., Hindi and Urdu). Measure their distance in ENTROPY2VEC space. Compare this to their distance in a URIEL syntax-based space. A much larger ENTROPY2VEC distance would confirm a script-related bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating shared encoding structures or script-agnostic features prevent the separation of typologically similar languages that utilize different scripts?
- Basis in paper: [Explicit] The "Limitations" section notes that current representations cause similar languages (e.g., Thai and Lao) to split due to differing encodings, suggesting future work could integrate shared structures to capture etymological relationships.
- Why unresolved: The current character-level tokenization forces a separation based on script differences rather than structural syntax.
- Evidence: Re-evaluation of the clustering accuracy (Robinson-Foulds distance) using a modified tokenizer or transliteration pipeline that normalizes script differences.

### Open Question 2
- Question: Why does concatenating ENTROPY2VEC with baseline vectors improve performance in mBERT but cause redundancy or degradation in XLM-R?
- Basis in paper: [Explicit] Section 5.2 ("Significance of Combining Vectors") observes that concatenation helps the weaker mBERT but hurts the stronger XLM-R, hypothesizing conflicting signals without conclusive proof.
- Why unresolved: The paper identifies the phenomenon but does not isolate the specific representational conflicts or capacity issues causing the divergence.
- Evidence: An ablation study analyzing the orthogonality and information overlap between ENTROPY2VEC and URIEL vectors within the hidden states of both model architectures.

### Open Question 3
- Question: How effectively can ENTROPY2VEC capture diachronic language evolution compared to static inventories when applied to temporally distinct corpora?
- Basis in paper: [Explicit] The "Discussion" section claims the method is "inherently dynamic" and adaptable to evolving linguistic features, unlike static databases.
- Why unresolved: The empirical evaluation relies on a static snapshot (Glot500c), and the proposed capability to model language drift over time was not empirically tested.
- Evidence: Experiments tracking vector shifts between historical and modern text slices of the same language to see if they align with documented linguistic shifts.

## Limitations

- The method's reliance on small monolingual LMs may not capture all nuanced typological features, potentially limiting its effectiveness for complex linguistic relationships
- Character-level tokenization may overly emphasize surface-level differences like script rather than deeper structural properties, causing typologically similar languages with different scripts to appear more distant than they are
- The effectiveness of ENTROPY2VEC vectors in downstream tasks is primarily validated within a single framework (LinguAlchemy), lacking broader cross-task validation

## Confidence

- **High Confidence:** The methodology for constructing ENTROPY2VEC vectors is clearly specified and reproducible
- **Medium Confidence:** The alignment of induced trees with Glottolog suggests the method captures meaningful linguistic relationships, though some misalignments indicate limitations
- **Low Confidence:** The downstream effectiveness claim is based on a single evaluation within the LinguAlchemy framework, lacking broader validation across diverse multilingual tasks

## Next Checks

1. **Script Sensitivity Test:** Measure ENTROPY2VEC distances between language pairs with high lexical similarity but different scripts (e.g., Hindi-Urdu, Serbian-Croatian) and compare against URIEL syntax-based distances to quantify script-related bias.

2. **Model Size Ablation:** Systematically vary the GPT-2 model size (2-layer, 4-layer, 6-layer) and measure the impact on RF distance to Glottolog to test the sufficiency assumption for small models.

3. **Cross-Domain Evaluation:** Evaluate ENTROPY2VEC vectors on additional multilingual benchmarks beyond LinguAlchemy (e.g., XTREME, XGLUE) to assess broader applicability and validate downstream effectiveness claims.