---
ver: rpa2
title: 'DRF: LLM-AGENT Dynamic Reputation Filtering Framework'
arxiv_id: '2509.05764'
source_url: https://arxiv.org/abs/2509.05764
tags:
- agent
- task
- agents
- reputation
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DRF, a dynamic reputation filtering framework
  for LLM-based multi-agent systems. The key challenge addressed is the lack of mechanisms
  to quantify agent performance and assess credibility in such systems, particularly
  when faced with low-quality or malicious agents.
---

# DRF: LLM-AGENT Dynamic Reputation Filtering Framework

## Quick Facts
- **arXiv ID:** 2509.05764
- **Source URL:** https://arxiv.org/abs/2509.05764
- **Reference count:** 21
- **Primary result:** Dynamic reputation filtering framework achieves 84.3-92.9% pass@1 accuracy on code generation and 64.6-70.5% accuracy on logical reasoning while reducing costs by 4-13% compared to baselines.

## Executive Summary
This paper introduces DRF, a dynamic reputation filtering framework for LLM-based multi-agent systems. The key challenge addressed is the lack of mechanisms to quantify agent performance and assess credibility in such systems, particularly when faced with low-quality or malicious agents. DRF constructs an interactive rating network to quantify agent performance, designs a reputation scoring mechanism to measure agent honesty and capability, and integrates an Upper Confidence Bound-based strategy for efficient agent selection.

The framework dynamically assesses agent effectiveness during task execution and iteratively updates agent reputations based on performance. Experiments demonstrate DRF's effectiveness in both code generation (HumanEval dataset) and logical reasoning tasks (BigBench dataset), achieving superior task completion quality and collaboration efficiency while successfully identifying high-reputation, high-quality agents while filtering out low-performance or malicious participants.

## Method Summary
DRF operates through a multi-round iterative process where agents execute tasks and evaluate each other's performance. The core mechanism involves three key components: an interactive rating network that aggregates peer evaluations weighted by evaluator reputation, a reputation iteration system with increment and decay rules based on task performance thresholds, and a UCB-based agent selection strategy that balances exploration and exploitation. Agents are simulated with three capability levels (Low, Medium, High) using meta-prompts, and the system runs on DeepSeek-R1 LLM with specific temperature settings for different tasks. The framework maintains per-agent reputations that are updated each round based on whether task scores exceed a threshold, and uses a modified UCB algorithm to select the most promising agents for subsequent rounds.

## Key Results
- Achieves 84.3-92.9% pass@1 accuracy on HumanEval code generation tasks versus 74.2-86.5% for Reflexion and 80.2-88.3% for DyLAN
- Achieves 64.6-70.5% accuracy on BigBench logical reasoning tasks versus 53.1-60.3% for Reflexion and 62.5-66.2% for DyLAN
- Reduces costs by 4-13% compared to baseline methods while maintaining or improving performance
- Successfully identifies high-reputation, high-quality agents while filtering out low-performance or malicious participants

## Why This Works (Mechanism)

### Mechanism 1: Interactive Rating Network for Peer Evaluation
- Claim: Aggregating weighted peer evaluations produces a reliable signal of agent capability.
- Mechanism: A k-layer network is constructed via forward pass (agents generate solutions and rate peers from the prior round). A backward pass computes each agent's score as the reputation-weighted average of received ratings (Equation 6: $w_i^t = \sum w_{j,i}^t \cdot \phi_j$, where $\phi_j$ scales by evaluator reputation).
- Core assumption: Agents with higher reputation provide more informative evaluations; peer rating quality correlates with capability.
- Evidence anchors:
  - [abstract]: "constructs an interactive rating network to quantify agent performance"
  - [section]: Section 4.1 defines forward/backward passes and Equation 6; Figure 2 illustrates the k-layer rating process.
  - [corpus]: Reputation as a Solution to Cooperation Collapse in LLM-based MASs discusses reputation in LLM MAS, providing conceptual support, but does not validate this specific rating network.
- Break condition: If evaluators collude or share systematic bias, reputation-weighted aggregation amplifies noise rather than signal.

### Mechanism 2: Reputation Iteration with Increment and Decay
- Claim: A threshold-based update rule creates a durable reputation signal that differentiates high-quality from low-quality agents.
- Mechanism: When an agent's task score exceeds threshold $w_0$, reputation increases via $r_i^t = r_i^{t-1} + w_i^t \cdot (1 - r_i^{t-1}) \cdot \alpha$ (Equation 7); otherwise, it decays via $r_i^t = r_i^{t-1} - (w_i^t \cdot r_i^{t-1}) \cdot \beta$ (Equation 8).
- Core assumption: Task score reflects underlying capability and honesty; threshold $w_0$ is well-calibrated.
- Evidence anchors:
  - [abstract]: "designs a reputation scoring mechanism to measure agent honesty and capability"
  - [section]: Section 4.2 and Algorithm 1 detail increment/decay; Figure 3 shows reputation divergence across agent capability levels.
  - [corpus]: Corpus does not provide direct validation of this specific update rule.
- Break condition: If task scores are noisy or adversarially manipulated, reputation updates may misclassify agents.

### Mechanism 3: UCB-Based Agent Selection Balancing Exploration and Exploitation
- Claim: A modified Upper Confidence Bound strategy efficiently selects high-reputation, low-cost agents while exploring uncertain ones.
- Mechanism: Selection score $S_i^t = \delta \cdot r_i^{t-1} + (1-\delta) \cdot c_i^t + \sqrt{\gamma \ln N / n_i^{t-1}}$ (Equation 9) combines reputation, cost, and an exploration bonus.
- Core assumption: The multi-armed bandit formulation captures agent selection; Markov property holds (outcomes depend only on current selection).
- Evidence anchors:
  - [abstract]: "integrates an Upper Confidence Bound-based strategy to enhance agent selection efficiency"
  - [section]: Section 4.3 and Algorithm 2 describe UCB integration; Tables 3–4 show improved pass@1/accuracy and reduced cost vs. DyLAN and Reflexion.
  - [corpus]: Corpus does not provide external validation of UCB for LLM agent selection.
- Break condition: If agent performance is non-stationary or context-dependent, UCB's stationarity assumption degrades selection quality.

## Foundational Learning

- Concept: Multi-Armed Bandit (MAB) and UCB
  - Why needed here: DRF formulates agent selection as a bandit problem; UCB balances exploration of uncertain agents with exploitation of known high-reputation agents.
  - Quick check question: Can you explain why UCB adds an exploration bonus inversely proportional to selection count?

- Concept: Reputation Systems in Multi-Agent Systems
  - Why needed here: DRF assigns and iteratively updates reputation scores to quantify trustworthiness and capability.
  - Quick check question: What failure modes occur if reputation updates are too aggressive or too conservative?

- Concept: Peer Evaluation Aggregation
  - Why needed here: DRF's rating network aggregates peer ratings weighted by evaluator reputation.
  - Quick check question: How does weighting peer ratings by reputation mitigate vs. amplify evaluator bias?

## Architecture Onboarding

- Component map:
  - Core Agent -> Task Agents -> Rating Network -> Reputation Store -> UCB Selector
  - Core Agent: Decision-making and orchestration; maintains budget $\phi$, selects task agents.
  - Task Agents: Execute tasks and evaluate peers; each contains an Executor and Evaluator LLM.
  - Rating Network: Forward pass (solution generation + peer rating); backward pass (score computation via Equation 6).
  - Reputation Store: Per-agent reputation $r_i^t$, updated via increment/decay (Equations 7–8).
  - UCB Selector: Computes $S_i^t$ (Equation 9) to rank agents for selection.

- Critical path:
  1. Task $\to$ Core Agent initializes round $t$, identifies willing agents.
  2. UCB Selector ranks agents by $S_i^t$, forms team $P^t$ (size $\le K$).
  3. Task Agents execute and rate peers; rating network builds k layers.
  4. Backward pass computes scores $w_i^t$; reputation updated via Algorithm 1.
  5. Loop until budget exhausted or task complete.

- Design tradeoffs:
  - Exploration vs. cost: High $\gamma$ increases exploration, raising short-term cost; low $\gamma$ exploits known agents but may miss better ones.
  - Reputation threshold $R_0$: Higher $R_0$ reduces risk but may exclude recovering agents; lower $R_0$ includes more agents but risks low-quality outputs.
  - Network depth $k$: More layers capture more peer interactions but increase latency and cost.

- Failure signatures:
  - Reputation stagnation: If $\alpha, \beta$ too small or $w_0$ mis-set, reputations don't differentiate.
  - Colluding evaluators: Consistent high ratings among colluders inflate reputations; detection requires cross-checks or external validators.
  - Cost overruns: If $\delta$ underweights cost or exploration dominates, budget depletes prematurely.

- First 3 experiments:
  1. Reputation convergence test: Run DRF on 30 HumanEval tasks with heterogeneous agent capabilities; plot reputation trajectories vs. ground-truth capability (replicate Figure 3).
  2. Ablation on UCB: Disable exploration bonus ($\gamma=0$); measure pass@1 and cost vs. full UCB to quantify exploration value.
  3. Threshold sensitivity: Vary $w_0$ and $R_0$; measure false positive/negative rates in identifying high-quality agents and impact on final accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- **Threshold Sensitivity:** The framework's performance heavily depends on calibration of $w_0$ and $R_0$ thresholds, which are not fully specified in the paper.
- **Simulation Dependency:** Agent capabilities are simulated through meta-prompts rather than real heterogeneous LLMs, raising questions about real-world generalization.
- **Cost Normalization:** The specific procedure for normalizing costs and its interaction with reputation weighting is not fully detailed.

## Confidence

- **High Confidence:** The theoretical framework combining reputation systems with UCB bandit selection is sound and addresses a well-defined problem in multi-agent LLM systems.
- **Medium Confidence:** Experimental results showing improved performance over baselines are promising but limited by simulated agent capabilities and lack of extensive ablation studies.
- **Low Confidence:** Claims about filtering "malicious" agents are not directly validated, and the simulation setup may not capture sophisticated adversarial behavior.

## Next Checks
1. **Parameter Sensitivity Analysis:** Systematically vary $w_0$, $R_0$, $\alpha$, $\beta$, $\gamma$, and $\delta$ across a range of values to identify optimal settings and assess robustness to parameter changes.

2. **Real Agent Heterogeneity Test:** Replace simulated agent capabilities with three distinct, real LLM configurations and evaluate whether DRF maintains its filtering and selection performance.

3. **Adversarial Agent Challenge:** Introduce agents that provide high-quality solutions but give systematically low ratings to competitors, or vice versa, to test whether the reputation mechanism can detect and mitigate such sophisticated manipulation attempts.