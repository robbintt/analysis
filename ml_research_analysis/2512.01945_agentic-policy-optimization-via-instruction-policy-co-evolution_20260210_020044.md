---
ver: rpa2
title: Agentic Policy Optimization via Instruction-Policy Co-Evolution
arxiv_id: '2512.01945'
source_url: https://arxiv.org/abs/2512.01945
tags:
- instruction
- policy
- instructions
- answer
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INSPO, a novel Instruction-Policy co-evolution
  framework that integrates instruction optimization as a dynamic component of the
  reinforcement learning (RL) loop for training LLM-based agents. Unlike existing
  RLVR approaches that rely on static, manually-designed instructions, INSPO maintains
  a dynamic population of instruction candidates and employs an experience-driven
  instruction generation mechanism, automating the discovery of more effective reasoning
  strategies via reflecting on online feedback along the policy learning process.
---

# Agentic Policy Optimization via Instruction-Policy Co-Evolution

## Quick Facts
- arXiv ID: 2512.01945
- Source URL: https://arxiv.org/abs/2512.01945
- Authors: Han Zhou; Xingchen Wan; Ivan VuliÄ‡; Anna Korhonen
- Reference count: 40
- Primary result: INSPO achieves 38.2% EM score on benchmarks with Qwen-2.5-3B, surpassing Search-R1 by 6%

## Executive Summary
This paper introduces INSPO, a novel Instruction-Policy co-evolution framework that integrates instruction optimization as a dynamic component of the reinforcement learning (RL) loop for training LLM-based agents. Unlike existing RLVR approaches that rely on static, manually-designed instructions, INSPO maintains a dynamic population of instruction candidates and employs an experience-driven instruction generation mechanism. The method automatically discovers more effective reasoning strategies by reflecting on online feedback throughout the policy learning process.

The framework periodically prunes low-performing instructions and generates new ones through an on-policy reflection mechanism that analyzes past trajectories to evolve more effective strategies. Experiments on multi-turn retrieval and reasoning tasks demonstrate that INSPO substantially outperforms strong baselines relying on static instructions, achieving an average EM score of 38.2% on all benchmarks with Qwen-2.5-3B, surpassing the state-of-the-art RL baseline Search-R1 by 6%.

## Method Summary
INSPO introduces a co-evolution framework where instruction optimization becomes an integral part of the reinforcement learning loop. The method maintains a population of instruction candidates that evolve dynamically rather than relying on static, manually-designed prompts. Through experience-driven instruction generation, INSPO analyzes past trajectories and online feedback to create new, more effective reasoning strategies. The framework periodically evaluates and prunes low-performing instructions while generating new ones via an on-policy reflection mechanism, enabling the LLM to discover strategic reasoning paths and avoid erroneous patterns with only marginal computational overhead.

## Key Results
- Achieves 38.2% EM score on all benchmarks with Qwen-2.5-3B, surpassing Search-R1 by 6%
- Demonstrates substantial improvement over baselines using static instructions
- Shows ability to evolve precise and innovative instructions that guide more strategic reasoning paths
- Maintains only marginal computational overhead compared to traditional RLVR approaches

## Why This Works (Mechanism)
INSPO works by treating instruction optimization as a dynamic evolutionary process rather than a static design choice. The framework's co-evolution approach allows instructions to adapt based on actual performance data, enabling the discovery of reasoning strategies that may not be apparent through manual prompt engineering. The on-policy reflection mechanism analyzes trajectory data to identify patterns of success and failure, using this information to generate instructions that specifically address observed weaknesses while reinforcing effective approaches.

## Foundational Learning
- **Reinforcement Learning with Value Regularization (RLVR)**: A framework combining RL with value-based guidance to stabilize training; needed for balancing exploration with performance optimization, check by verifying reward signal stability during training.
- **Instruction Evolution Dynamics**: The process of maintaining and evolving instruction populations over time; needed for understanding how instruction quality changes during training, check by tracking instruction diversity metrics.
- **On-policy Trajectory Analysis**: Methods for extracting learning signals from agent experiences during training; needed for the reflection mechanism to generate effective new instructions, check by validating trajectory representation quality.
- **Multi-turn Reasoning Optimization**: Techniques for improving performance on sequential decision-making tasks; needed for the retrieval and reasoning benchmarks, check by measuring reasoning chain consistency.
- **Population-based Optimization**: Approaches that maintain multiple candidate solutions simultaneously; needed for exploring the instruction space effectively, check by monitoring population diversity over time.

## Architecture Onboarding

Component Map:
Instruction Population -> Trajectory Analysis -> New Instruction Generation -> Policy Update -> Performance Evaluation -> Pruning

Critical Path:
1. Policy interacts with environment using current instruction set
2. Trajectories are collected and analyzed for success/failure patterns
3. Reflection mechanism generates new instructions based on trajectory insights
4. New instructions are evaluated and integrated into the population
5. Low-performing instructions are pruned
6. Policy is updated with improved instruction guidance

Design Tradeoffs:
- Exploration vs. exploitation in instruction space
- Computational overhead vs. performance gains
- Instruction diversity vs. convergence speed
- Reflection frequency vs. training stability

Failure Signatures:
- Instruction population collapse (loss of diversity)
- Plateau in performance despite instruction evolution
- High variance in instruction effectiveness
- Divergence between training and evaluation performance

First Experiments:
1. Baseline comparison using static instructions vs. INSPO with same model size
2. Instruction diversity analysis over training epochs
3. Ablation study removing the reflection mechanism

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focused on multi-turn retrieval and reasoning tasks, unclear generalizability to other domains
- Computational overhead described as "marginal" but not quantified in absolute terms
- No discussion of instruction diversity maintenance over extended training periods
- Limited testing to 3B parameter model size, scalability concerns unaddressed

## Confidence
- Methodology description: High
- Performance claims: Medium
- Generalization claims: Low

## Next Checks
1. Test INSPO on diverse task types beyond retrieval and reasoning, such as generation or planning tasks
2. Quantify the actual computational overhead in terms of training time and resources compared to static instruction methods
3. Evaluate instruction diversity and quality evolution over extended training periods to assess long-term stability and potential degeneration patterns