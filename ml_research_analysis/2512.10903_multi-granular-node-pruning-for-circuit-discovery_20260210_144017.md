---
ver: rpa2
title: Multi-Granular Node Pruning for Circuit Discovery
arxiv_id: '2512.10903'
source_url: https://arxiv.org/abs/2512.10903
tags:
- active
- circuit
- pruning
- pruned
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a multi-granular node pruning framework\
  \ for circuit discovery in language models, addressing the computational expense\
  \ and coarse granularity limitations of existing edge-pruning methods. The approach\
  \ applies learnable masks across multiple levels\u2014from entire blocks down to\
  \ individual neurons\u2014within a unified optimization objective, enabling more\
  \ precise and efficient circuit discovery in a single training run."
---

# Multi-Granular Node Pruning for Circuit Discovery

## Quick Facts
- arXiv ID: 2512.10903
- Source URL: https://arxiv.org/abs/2512.10903
- Reference count: 40
- Primary result: Multi-granular node pruning discovers circuits 5-10x smaller than edge pruning with minimal performance loss (KL <0.61)

## Executive Summary
This paper introduces a multi-granular node pruning framework for circuit discovery in language models, addressing computational expense and coarse granularity limitations of existing edge-pruning methods. The approach applies learnable masks across multiple levels—from entire blocks down to individual neurons—within a unified optimization objective, enabling more precise and efficient circuit discovery in a single training run. Empirically, the method identifies circuits that are smaller in nodes than those found by prior approaches, revealing that many neurons within components deemed important by coarse methods are actually irrelevant, while still maintaining task performance.

## Method Summary
The method uses a two-stream architecture where clean and corrupted inputs are processed in parallel. At each component, a learnable mask controls interpolation between clean and corrupted activations. Masks are parameterized using the Hard Concrete distribution to enable gradient-based learning of approximately binary masks. The framework operates at five granularities simultaneously: blocks, heads, MLP layers, attention neurons, and MLP neurons. After training, hierarchical consistency is enforced where child masks are disabled when parent masks are deactivated. The optimization objective combines task loss (KL divergence) with L0 sparsity regularization weighted by task-specific coefficients.

## Key Results
- Discovered circuits are significantly smaller than edge pruning baselines, with 93-97% edge compression
- Memory footprint reduced 5-10x compared to methods requiring intermediate activation storage
- For Gendered Pronouns task, only 1,333 of 30,720 MLP hidden neurons remained active (96.4% sparsity)
- Maintained task performance with KL divergence <0.61 across all three tasks
- Reduced active attention heads by up to an order of magnitude compared to edge pruning

## Why This Works (Mechanism)

### Mechanism 1: Causal Importance Estimation via Two-Stream Interpolation
- Claim: The two-stream architecture identifies causally important components by measuring performance sensitivity to activation corruption.
- Mechanism: Clean and corrupted inputs are processed in parallel. At each component, a learnable mask m controls interpolation: h = m·h_clean + (1-m)·h_corrupted. Components with high learned mask values indicate that passing clean activations is critical for task performance; low values suggest the component is task-irrelevant.
- Core assumption: Important components will show large performance degradation when corrupted activations are passed; unimportant components will show minimal differences.
- Evidence anchors:
  - [abstract]: "our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work"
  - [section 4]: "The key hypothesis is that important components will show a large shift in performance between clean and corrupted activations, whereas unimportant components will show minimal differences."
  - [corpus]: Related papers on path patching and causal scrubbing in corpus (e.g., "APP: Accelerated Path Patching") confirm this causal approach is established in interpretability research.
- Break condition: If the corruption function doesn't create meaningful counterfactuals for the task, or if the task doesn't have clear input-output mappings, the causal signal may be too weak for reliable mask learning.

### Mechanism 2: Differentiable Sparsity via Hard Concrete Distribution
- Claim: The Hard Concrete parameterization enables gradient-based learning of approximately binary masks for circuit extraction.
- Mechanism: Masks are sampled from a Hard Concrete distribution using trainable parameters α. The sampling procedure m = min(1, max(0, s(ζ-γ) + γ)) with sigmoid transformations creates a distribution with mass concentrated at 0 and 1, enabling binary-like behavior during optimization while remaining differentiable.
- Core assumption: Sparsity can be learned through differentiable approximations; L0 regularization can be approximated via continuous relaxation without requiring combinatorial optimization.
- Evidence anchors:
  - [section 4]: "we parameterize the masks using the Hard Concrete distribution (Louizos et al., 2017)"
  - [section 4.1]: The loss function includes λ Σ L0(mi) for sparsity regularization with granularity-specific coefficients.
  - [corpus]: Corpus contains "Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework" and Edge Pruning work using learnable masks, confirming this is an established technique adapted for circuit discovery.
- Break condition: If smoothness parameter β is too high, masks remain soft; if too low, gradient signal diminishes. Hyperparameter λ must balance task performance vs. sparsity—incorrect values lead to trivial solutions.

### Mechanism 3: Hierarchical Multi-Granular Compression
- Claim: Simultaneous pruning across granularities reveals that many neurons within "important" coarse components are actually task-irrelevant.
- Mechanism: Masks exist at five levels: blocks, heads, MLP layers, attention neurons, MLP neurons. After training, hierarchical consistency is enforced—when a parent mask is deactivated (set to zero), all child masks are also disabled. This allows discovering, for example, that an MLP block is important but only 1,333 of its 30,720 neurons matter for the task.
- Core assumption: Circuit structure varies across granularities; coarse-grained importance doesn't imply fine-grained importance at the neuron level.
- Evidence anchors:
  - [abstract]: "many neurons within components deemed important by coarse methods are actually irrelevant, while still maintaining task performance"
  - [table 1]: For GP task, only 1,333/30,720 MLP hidden neurons remain active (96.4% sparsity) despite some MLP blocks being retained.
  - [corpus]: "Language Model Circuits Are Sparse in the Neuron Basis" (corpus neighbor) provides independent evidence supporting neuron-level sparsity in language models.
- Break condition: If hierarchical constraints are too strict, may miss cross-component sparse circuits; if too loose, may produce incoherent subnetworks. The method does not recover interaction structure between nodes (acknowledged limitation in section 9).

## Foundational Learning

- Concept: **Circuit Discovery in Mechanistic Interpretability**
  - Why needed here: This entire paper addresses finding minimal subnetworks responsible for specific behaviors. Understanding that circuit discovery seeks interpretable mechanisms (not deployment efficiency) is essential for interpreting results.
  - Quick check question: Can you explain why circuit discovery differs fundamentally from traditional model compression for deployment?

- Concept: **Edge Pruning vs. Node Pruning Trade-offs**
  - Why needed here: The paper's core innovation is moving from edge-level to node-level pruning. Edge pruning has O(n²) edges and cannot prune within components; node pruning reduces search space but may miss interaction patterns.
  - Quick check question: Why does edge pruning scale quadratically, and what structural information does node pruning sacrifice?

- Concept: **Hard Concrete Distribution for Sparse Learning**
  - Why needed here: The mask parameterization uses this distribution to enable gradient-based learning of binary masks. Understanding why mass at 0 and 1 is critical, and how L0 regularization is approximated, is key to debugging training.
  - Quick check question: How does the Hard Concrete distribution differ from simple sigmoid-based masking, and why is the sampling procedure essential?

- Concept: **Transformer Component Hierarchy**
  - Why needed here: The method operates on five granularities (blocks, heads, MLP layers, attention neurons, MLP neurons). Understanding transformer architecture at these levels is prerequisite for implementing and debugging the framework.
  - Quick check question: In a standard GPT-2 layer, what are the hierarchical relationships between attention blocks, attention heads, MLP blocks, and their neurons?

## Architecture Onboarding

- Component map:
  MultiGranularCircuitDiscovery/
  ├── Mask Initialization (Hard Concrete parameters α per granularity)
  ├── TwoStreamForwardPass/
  │   ├── Clean stream (forward with mask interpolation)
  │   └── Corrupted stream (parallel forward with perturbed input)
  ├── LossComputation/
  │   ├── Task loss (KL divergence: circuit vs. full model)
  │   └── Sparsity loss (λ-weighted L0 per granularity)
  └── CircuitExtraction/
      ├── Mask binarization (threshold on log α)
      └── Hierarchical consistency enforcement

- Critical path:
  1. Define task-specific corruption function C (e.g., swap names for IOI, alter years for Greater-Than)
  2. Initialize Hard Concrete mask parameters for all granularities (Equation 6)
  3. For each batch: execute two-stream forward pass, compute combined loss, update masks
  4. After training: binarize masks using Equation 8 threshold, apply hierarchical pruning
  5. Evaluate circuit faithfulness (KL divergence <0.61 target) and task-specific metrics

- Design tradeoffs:
  - **Granularity vs. compute**: Neuron-level masks increase parameters (~55K for GPT-2 small) but reveal finer structure; block-level only is faster but coarser.
  - **Sparsity vs. faithfulness**: Higher λ increases compression but risks performance degradation; paper achieves 93-97% edge compression with KL <0.61.
  - **Memory vs. activations storage**: Two-stream requires 2x model memory but avoids intermediate activation storage; reported 6,270 MB vs. 72,794 MB for EAP.
  - **Node vs. edge pruning**: Node pruning is more memory-efficient but doesn't recover directional interaction structure (section 9 limitation).

- Failure signatures:
  - **All masks collapse to 0**: λ is too high relative to task loss weight.
  - **Masks remain soft (not binary)**: β parameter too high; Hard Concrete not concentrating mass at endpoints.
  - **Circuit performance collapses**: Corruption function fails to create meaningful counterfactuals.
  - **Inconsistent hierarchical pruning**: Child masks active after parent pruned—bug in consistency enforcement (Equation 8 application).
  - **Memory exceeds baseline**: Not batching two streams efficiently; should share weights.

- First 3 experiments:
  1. **Reproduce IOI task on GPT-2 small**: Validate implementation against paper benchmarks (target: ~21 attention heads, all 12 MLP blocks, KL < 0.61).
  2. **Ablate corruption function design**: Test token swapping vs. random perturbation to verify causal signal quality and sensitivity.
  3. **Scale to larger model component**: Apply to GPT-2 medium or comparable, monitoring memory scaling and whether mask parameter count remains tractable (~2x model parameters in memory).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be extended to explicitly recover the directional interaction structure between active nodes, which is currently obscured by the binary masking process?
- Basis in paper: [explicit] Section 9 (Limitations) states the method identifies necessary nodes but "does not explicitly recover the interaction structure between nodes," including directionality or multiplicity.
- Why unresolved: The current node-level masking approach treats components as active or inactive without modeling the specific edges or information flow between them.
- What evidence would resolve it: A modified framework that outputs a directed graph $G'=(N', E')$ showing specific connections between active nodes, validated against causal scrubbing methods.

### Open Question 2
- Question: Can a hybrid approach combining hierarchical node pruning with edge-level sparsification discover smaller circuits while preserving the efficiency of the current framework?
- Basis in paper: [explicit] Section 8 (Conclusion) suggests that "an exciting direction is to combine edge and node pruning in a hierarchical fashion" to find even more minimal circuits.
- Why unresolved: The current study treats node and edge pruning as separate methodologies; the computational cost and optimization stability of a unified approach are unknown.
- What evidence would resolve it: A comparative study showing that a joint node-edge optimization objective achieves lower parameter counts or higher sparsity than node pruning alone without increasing training time.

### Open Question 3
- Question: To what extent does the choice of the task-dependent corruption function influence the composition and minimality of the discovered circuit?
- Basis in paper: [inferred] The method relies on a specific corruption function $C$ (Section 4) to generate the corrupted stream, but the sensitivity of the resulting circuit to this choice is not analyzed.
- Why unresolved: Different corruption strategies (e.g., token swapping vs. noise injection) might highlight different causal pathways, potentially leading to inconsistent circuit definitions.
- What evidence would resolve it: An ablation study showing circuit overlap (Jaccard index) and performance consistency when varying the corruption function for the same task.

## Limitations
- Cannot recover interaction structure between nodes within or across components
- Hard Concrete hyperparameters (β, ζ, γ) not fully specified, creating uncertainty about mask binarization
- Memory efficiency claims (5-10x reduction vs. EAP) not fully verified with detailed experimental conditions
- Task-specific corruption functions only described at high level without implementation details

## Confidence

**High Confidence**: The core mechanism of causal importance estimation via two-stream interpolation is well-established in mechanistic interpretability literature. The empirical results showing significant compression (93-97% edge reduction) while maintaining task performance (KL <0.61) are supported by concrete metrics and comparisons to baseline methods.

**Medium Confidence**: The claim that many neurons within "important" coarse components are actually task-irrelevant is supported by specific sparsity numbers (e.g., 1,333/30,720 MLP neurons for GP task) but relies on the specific task corruption functions and may not generalize across all behaviors.

**Low Confidence**: The memory efficiency comparison to EAP (72,794 MB vs. 6,270 MB) is presented without detailed experimental conditions or accounting for mask parameter storage, making it difficult to verify the claimed 5-10x reduction independently.

## Next Checks

1. **Implement and validate task corruption functions**: Reconstruct the exact token-level corruption procedures for IOI, GP, and GT tasks and verify that they create meaningful counterfactuals for the two-stream interpolation mechanism.

2. **Replicate memory footprint measurements**: Conduct controlled experiments comparing memory usage across different checkpointing strategies (EAP vs. proposed method) on GPT-2 small, accounting for all memory components including mask parameters.

3. **Test hierarchical consistency enforcement**: Verify that the mask binarization and hierarchical pruning procedure (Equation 8) correctly propagates parent mask deactivation to child masks, and measure the impact of violations on circuit faithfulness.