---
ver: rpa2
title: 'QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action
  Models'
arxiv_id: '2510.14836'
source_url: https://arxiv.org/abs/2510.14836
tags:
- depth
- qdepth-vla
- tasks
- spatial
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QDepth-VLA addresses the challenge of spatial perception in Vision-Language-Action
  (VLA) models for fine-grained manipulation tasks by introducing quantized depth
  prediction as auxiliary supervision. The method uses a dedicated depth expert to
  predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder, enabling
  the model to learn depth-aware representations that capture critical geometric cues.
---

# QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models

## Quick Facts
- arXiv ID: 2510.14836
- Source URL: https://arxiv.org/abs/2510.14836
- Reference count: 40
- Primary result: 94.0% success rate on LIBERO Goal tasks, outperforming open ùúã0 by 6.1%

## Executive Summary
QDepth-VLA introduces quantized depth prediction as auxiliary supervision for Vision-Language-Action (VLA) models to address spatial perception limitations in fine-grained manipulation tasks. The method employs a dedicated depth expert that predicts quantized latent tokens from depth maps encoded by a VQ-VAE, enabling geometric understanding without disrupting the pretrained VLM's semantic alignment. Experiments on LIBERO and Simpler benchmarks demonstrate significant performance improvements over baselines, with real-world robotic manipulation tasks showing a 10.0% success rate increase.

## Method Summary
QDepth-VLA extends pretrained VLA models with a dedicated depth expert that predicts quantized latent tokens from depth maps. A VQ-VAE compresses depth maps into discrete latent tokens (256 codebook entries, dim 160), which the depth expert predicts via cross-entropy loss over codebook indices. The architecture uses a hybrid attention mask to control information flow between modalities, allowing depth tokens to attend to image and text while preventing semantic interference. The method co-trains the depth expert with the action expert using conditional flow matching for action generation, with depth supervision weight decaying exponentially from 0.01.

## Key Results
- Achieves 94.0% success rate on LIBERO Goal tasks and 72.6% on Long tasks
- Outperforms open ùúã0 by 6.1% on Goal tasks and 7.7% on Long tasks
- Shows 10.0% improvement in real-world robotic manipulation tasks
- Quantized depth supervision proves more effective than pixel-level regression (64.6% when replaced)
- Hybrid attention mask consistently contributes to performance gains, particularly in placement tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantized depth tokens provide a more optimization-friendly supervision signal than pixel-wise depth regression.
- Mechanism: A VQ-VAE compresses depth maps into discrete latent tokens (256 codebook entries, dim 160). The depth expert predicts these tokens via cross-entropy loss over codebook indices, encouraging abstraction of salient geometric structures rather than redundant pixel-level detail.
- Core assumption: Salient spatial cues for manipulation are compressible into a compact discrete representation without critical information loss.
- Evidence anchors:
  - [abstract] "A dedicated depth expert is designed to predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder."
  - [section 4.3.3] "Replacing latent depth prediction with direct pixel-wise regression lowers average performance to 64.6% (-3.9%). The largest impact is on Eggplant (-14.6%) task."
  - [corpus] Related work GLaD similarly distills geometric priors into latent representations, suggesting broader validity of compact geometric encoding, though direct comparison is not available.

### Mechanism 2
- Claim: An independent depth expert prevents interference with the pretrained VLM's semantic alignment while still propagating geometric cues to the policy.
- Mechanism: The depth expert operates on SigLIP visual embeddings before language fusion. It does not modify the VLM backbone directly; instead, geometric information flows through the hybrid attention mask to the action expert, preserving pretrained vision-language priors.
- Core assumption: The action expert can integrate geometric features from the depth expert without requiring the VLM backbone to explicitly model depth.
- Evidence anchors:
  - [section 3.3] "It takes the visual embeddings from the SigLIP encoder as input, before language fusion to avoid semantic interference."
  - [section 4.3.2] "Eliminating the dedicated depth branch results in the largest overall performance degradation (-8.5%)... The most significant drop occurs in the Stack Block Task (-23.8%)."
  - [corpus] StereoVLA and GeoPredict similarly isolate geometric processing, but direct evidence for the interference-avoidance claim specific to this architecture is not available in the corpus.

### Mechanism 3
- Claim: Hybrid attention masks improve cross-modal information flow while limiting noise from imperfect depth supervision.
- Mechanism: Text/image tokens attend only within modality; depth tokens attend to image+text; action tokens attend to all preceding modalities. This hierarchy injects spatial cues into action generation while preventing noisy depth signals from corrupting pretrained semantic representations.
- Core assumption: Depth supervision contains noise that would harm action generation if directly fused under standard causal attention.
- Evidence anchors:
  - [section 3.3] "This hierarchical attention design allows depth to enhance spatial understanding while preventing over-interference with the pretrained VLM."
  - [section 4.3.4] "Replacing the proposed hybrid attention mask with a standard version... declines by 5.5% on average, with the most substantial drop on the Carrot Task (-15.8%)."
  - [corpus] No direct corpus evidence on hybrid attention designs for depth in VLAs; related works focus on stereo inputs or predictive kinematics rather than attention masking.

## Foundational Learning

- Concept: Vector-Quantized Variational Autoencoders (VQ-VAE)
  - Why needed here: Encodes depth maps into discrete tokens used as supervision targets for the depth expert. Understanding codebook learning, commitment loss, and reconstruction tradeoffs is essential.
  - Quick check question: Can you explain why stop-gradient is applied to the encoder output during codebook update but not during commitment loss computation?

- Concept: Conditional Flow Matching (CFM) for action generation
  - Why needed here: The action expert uses CFM to model continuous action distributions; this differs from standard diffusion and requires understanding the flow field formulation.
  - Quick check question: How does the CFM loss differ from standard denoising score matching in terms of the training target and noise schedule?

- Concept: Mixture-of-Experts (MoE) routing in transformers
  - Why needed here: QDepth-VLA coordinates VLM, action expert, and depth expert through an MoE-style structure; understanding token routing and expert specialization helps diagnose training instabilities.
  - Quick check question: What happens if two experts (e.g., action and depth) receive conflicting gradient signals from their respective losses during co-training?

## Architecture Onboarding

- Component map: RGB image ‚Üí SigLIP encoder ‚Üí visual tokens ‚Üí (a) Gemma decoder for semantic embedding ‚Üí action expert ‚Üí actions; (b) depth expert ‚Üí depth tokens ‚Üí cross-entropy loss vs. VQ-VAE codes
- Critical path: RGB image ‚Üí SigLIP encoder ‚Üí visual tokens ‚Üí (a) Gemma decoder for semantic embedding ‚Üí action expert ‚Üí actions; (b) depth expert ‚Üí depth tokens ‚Üí cross-entropy loss vs. VQ-VAE codes
- Design tradeoffs:
  - Codebook size (K=256): Larger captures more detail but increases prediction difficulty; 16√ó16 latent grid balances reconstruction and efficiency
  - Depth loss weight decay: Initial Œª‚ÇÄ=0.01 decays exponentially; too aggressive decay may underutilize depth, too slow may dominate action learning
  - Depth expert isolation vs. VLM integration: Isolation preserves semantics but limits depth-language grounding
- Failure signatures:
  - Depth reconstructions blur object boundaries ‚Üí VQ-VAE undertrained or codebook too small
  - Action performance drops when depth loss is enabled ‚Üí depth loss weight too high or depth annotations noisy/misaligned
  - Stacking tasks fail disproportionately ‚Üí depth expert not effectively propagating vertical distance cues to action expert
- First 3 experiments:
  1. **VQ-VAE reconstruction sanity check**: Train VQ-VAE on your depth data, visualize reconstructions at 16√ó16 and 32√ó32 grids. Confirm object and gripper boundaries are preserved before proceeding to VLA training.
  2. **Depth loss ablation**: Train QDepth-VLA with depth loss weight Œª=0 vs. Œª‚ÇÄ=0.01 (with decay). Compare success rates on spatial-heavy tasks (e.g., stack block, place in container) to isolate the contribution of depth supervision.
  3. **Hybrid attention swap**: Replace the hybrid attention mask with standard causal attention. Evaluate on the same spatial tasks; expect degradation particularly in placement tasks per section 4.3.4 findings.

## Open Questions the Paper Calls Out
- Can extending the auxiliary supervision to predict future depth tokens improve temporal reasoning and long-horizon planning capabilities?
- Do more efficient or compact VAE-based depth representations exist that can outperform the current VQ-VAE (16x16 latent grid) in capturing geometric cues?
- How robust is QDepth-VLA to systematic errors or temporal inconsistencies in the monocular depth estimator used for supervision (ViDA)?

## Limitations
- VQ-VAE design constraints: 256 codebook entries and 160-dim latents tuned for specific depth resolution and range, may be insufficient for domains with finer depth variations
- Generalization to real-world domains: Relies on ViDA-generated depth maps; real-world depth sensors have different noise characteristics requiring potential VQ-VAE retraining
- Scalability to multi-view inputs: Current quantized depth supervision may not fully capture richness of multi-view geometry
- Action expert capacity limits: 18-layer depth expert may not provide sufficient cross-attention capacity for optimal geometric signal integration

## Confidence
**High Confidence:**
- Quantized depth supervision improves manipulation performance over baselines in simulation benchmarks
- Hybrid attention mask consistently contributes to performance gains
- Depth expert isolation preserves VLM semantic alignment while enhancing geometric understanding

**Medium Confidence:**
- Quantized depth tokens are more effective than pixel-wise regression
- Depth supervision generalizes to real-world tasks

**Low Confidence:**
- Depth loss weight decay schedule is optimal for balancing geometric and semantic learning
- 16√ó16 latent grid is the best tradeoff between reconstruction quality and prediction difficulty

## Next Checks
1. **VQ-VAE Ablation on Grid Resolution:** Train QDepth-VLA with depth latents at 16√ó16 and 32√ó32 grids. Measure reconstruction quality (PSNR/SSIM) and downstream task performance to quantify the resolution-accuracy tradeoff.

2. **Depth Supervision Noise Sensitivity:** Corrupt depth maps with synthetic noise (Gaussian, dropout) at varying levels. Evaluate QDepth-VLA's robustness and determine noise thresholds where performance degrades significantly.

3. **Cross-Dataset Transfer:** Apply the pretrained QDepth-VLA model to a different manipulation dataset (e.g., MetaWorld or Robomimic) without finetuning the depth expert. Measure performance drop to assess generalization of quantized depth supervision across environments.