---
ver: rpa2
title: 'Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation Through
  Unsupervised Consistency Signals'
arxiv_id: '2509.08809'
source_url: https://arxiv.org/abs/2509.08809
tags:
- annotation
- ratio
- student
- arxiv
- google
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes an agentic annotation evaluation paradigm to\
  \ assess the quality of LLM-generated annotations in unsupervised environments without\
  \ oracle feedback. It introduces a student model that uses user-preference-based\
  \ majority voting to create annotations, which are then compared against a noisy\
  \ teacher LLM\u2019s outputs."
---

# Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation Through Unsupervised Consistency Signals

## Quick Facts
- **arXiv ID**: 2509.08809
- **Source URL**: https://arxiv.org/abs/2509.08809
- **Reference count**: 12
- **Primary result**: Introduces CAI Ratio metric showing up to 0.93 Pearson correlation with annotation accuracy

## Executive Summary
This paper introduces an agentic annotation evaluation framework for assessing LLM-generated annotations without oracle feedback. The approach uses a student model with user-preference-based majority voting to create annotations, which are then compared against a noisy teacher LLM's outputs. The key innovation is the Consistent and Inconsistent (CAI) Ratio metric, which measures agreement between student and teacher models as an unsupervised quality signal. The method was validated across ten NLP datasets and four LLMs, demonstrating strong correlation with annotation accuracy and effective model selection capabilities.

## Method Summary
The proposed method establishes a teacher-student evaluation paradigm where a student model generates annotations using majority voting based on user preferences, while a noisy teacher LLM provides reference outputs. The CAI Ratio metric quantifies the agreement between these two models, serving as an unsupervised quality indicator. The framework operates entirely without ground truth labels, making it suitable for real-world scenarios where oracle feedback is unavailable. The student model's user-preference-based approach aims to approximate high-quality annotations, while the teacher's noisy outputs provide a comparative baseline for consistency measurement.

## Key Results
- CAI Ratio shows up to 0.93 Pearson correlation with LLM annotation accuracy across ten NLP datasets
- Successfully identified the best-performing LLM in 60% of model selection cases
- Validated across four different LLMs and multiple NLP task types

## Why This Works (Mechanism)
The method leverages unsupervised consistency signals between a student model (using majority voting) and a noisy teacher LLM. When the student's user-preference-based annotations align with the teacher's outputs, this consistency indicates higher quality. The approach works because well-formed annotations should exhibit structural agreement even without ground truth labels. The CAI Ratio captures this alignment as a proxy for annotation quality, enabling evaluation in settings where traditional supervised metrics are unavailable.

## Foundational Learning

**Unsupervised evaluation metrics**: Used to assess model performance without ground truth labels; needed because real-world applications often lack oracle feedback; quick check: verify metric behavior on labeled test sets.

**Teacher-student model paradigms**: Framework where one model (teacher) provides supervision to another (student); needed to create comparative signals without labeled data; quick check: ensure teacher and student have complementary strengths.

**Majority voting mechanisms**: Aggregation method that selects most frequent annotation among multiple options; needed to approximate ground truth quality from user preferences; quick check: validate voting stability across different user preference distributions.

**Correlation analysis**: Statistical method measuring relationship between two variables; needed to validate CAI Ratio's effectiveness; quick check: ensure correlation significance across multiple datasets.

## Architecture Onboarding

**Component map**: User Preferences -> Student Model (Majority Voting) -> Annotations -> CAI Ratio Calculation <-> Teacher LLM Outputs

**Critical path**: The evaluation pipeline follows: user preference collection → student annotation generation → teacher output generation → CAI Ratio computation → quality assessment/model selection.

**Design tradeoffs**: The method trades computational overhead of dual-model evaluation for eliminating dependency on labeled data. The noisy teacher assumption balances between providing useful signals while avoiding perfect correlation that would invalidate the consistency measurement.

**Failure signatures**: Low CAI Ratio values may indicate either poor student annotations or teacher model inadequacy. The method assumes teacher noise level is appropriate - too much noise breaks consistency signals, too little makes evaluation trivial. Majority voting may fail when user preferences are highly polarized.

**First experiments**: 1) Baseline CAI Ratio calculation on synthetic data with known quality levels, 2) Correlation validation using a small labeled dataset, 3) Teacher noise sensitivity analysis by varying teacher model parameters.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- CAI Ratio performance across diverse task types and domain-specific applications requires further investigation
- The 0.93 Pearson correlation may vary with different LLM architectures or training paradigms
- The assumption that majority voting reliably approximates ground truth quality needs empirical validation in scenarios with multiple valid annotations

## Confidence
**High**: Unsupervised evaluation framework and CAI Ratio metric (novel contributions with clear theoretical grounding)
**Medium**: Model selection capability (60% success rate indicates utility but room for improvement)
**Medium**: Real-world applicability (depends on specific experimental conditions and implementation details)

## Next Checks
1) Test CAI Ratio on specialized domains such as biomedical or legal text to assess cross-domain robustness
2) Evaluate performance across diverse LLM architectures including smaller models and domain-specific fine-tuned versions
3) Conduct ablation studies to quantify the impact of different majority voting strategies and teacher noise levels on CAI Ratio reliability