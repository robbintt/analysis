---
ver: rpa2
title: $O(1/k)$ Finite-Time Bound for Non-Linear Two-Time-Scale Stochastic Approximation
arxiv_id: '2504.19375'
source_url: https://arxiv.org/abs/2504.19375
tags:
- bound
- lemma
- assumption
- two-time-scale
- stochastic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper obtains an improved O(1/k) finite-time bound for non-linear\
  \ two-time-scale stochastic approximation, significantly improving upon the previous\
  \ O(1/k^{2/3}) bound. The key innovation is introducing an averaged noise sequence\
  \ Uk+1 = \u03B2kM\u2032k+1 + (1-\u03B2k)Uk (with U0=0) and defining zk = yk - Uk,\
  \ which allows the original iteration to be rewritten in terms of zk."
---

# $O(1/k)$ Finite-Time Bound for Non-Linear Two-Time-Scale Stochastic Approximation

## Quick Facts
- arXiv ID: 2504.19375
- Source URL: https://arxiv.org/abs/2504.19375
- Reference count: 28
- Primary result: Improves mean square error bound from $O(1/k^{2/3})$ to $O(1/k)$ for non-linear two-time-scale stochastic approximation

## Executive Summary
This paper establishes an improved $O(1/k)$ finite-time bound for non-linear two-time-scale stochastic approximation algorithms, significantly improving upon the previous $O(1/k^{2/3})$ barrier. The key innovation is introducing an averaged noise sequence and reformulating the iteration in terms of this sequence, which allows for tighter control of the error dynamics. The analysis applies to algorithms like gradient descent-ascent and two-time-scale Lagrangian optimization, and uses an induction-based approach to show iterates remain bounded in expectation.

## Method Summary
The paper analyzes two coupled iterations: a fast variable $x_k$ updated with stepsize $\alpha_k$ and a slow variable $y_k$ updated with stepsize $\beta_k$. The authors introduce an auxiliary averaged noise variable $U_k$ and reformulate the slow-time-scale update in terms of $z_k = y_k - U_k$. This reformulation allows the error dynamics to be analyzed more effectively, showing that the mean square error decays at $O(1/k)$. Two stepsize schedules are analyzed: $\alpha_k = O(1/k)$ with $\beta_k = O(1/k)$ for the optimal rate, and $\alpha_k = O(1/k^a)$ with $\beta_k = O(1/k)$ for a more robust bound where $a \in (0.5,1)$.

## Key Results
- Establishes $O(1/k)$ mean square error bound for non-linear two-time-scale SA, improving upon previous $O(1/k^{2/3})$ bound
- Introduces averaged noise sequence technique that enables tighter error analysis
- Proves iterates remain bounded in expectation using induction-based approach
- Provides more robust bound of $O(1/k^a)$ for $\alpha_k = O(1/k^a)$ and $\beta_k = O(1/k)$ where $a \in (0.5,1)$

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Introducing an averaged noise sequence allows derivation of an $O(1/k)$ bound for non-linear two-time-scale SA
- Mechanism: Defining auxiliary averaged noise $U_{k+1} = \beta_k M'_{k+1} + (1-\beta_k)U_k$ and modified iterate $z_k = y_k - U_k$ reformulates the slow-time-scale update to subtract accumulated noise history
- Core assumption: Noise $M'_{k+1}$ is a martingale difference sequence, stepsize $\beta_k = O(1/k)$ ensures variance decay
- Evidence anchors: Abstract states "key step involves rewriting original iteration in terms of averaged noise sequence which decays sufficiently fast"; Section 1.1 identifies previous $O(1/k^{2/3})$ bound limitation; corpus neighbor [2503.18391] discusses arbitrary norm contractions but not this specific noise averaging trick

### Mechanism 2
- Claim: Error dynamics separate effectively due to contraction properties allowing independent analysis of fast and slow time-scales
- Mechanism: Fast variable $x$ contracts towards $x^*(y)$ while slow variable $y$ contracts towards $y^*$, with Lipschitz continuity of $x^*(y)$ bounding drift from slow time-scale movement
- Core assumption: $f(x,y)$ is $\lambda$-contractive in $x$, $g(x^*(\cdot),\cdot)$ is $\mu$-contractive in $y$, plus Lipschitz conditions
- Evidence anchors: Section 2 states assumptions defining contraction and Lipschitz properties; Section 4 proves $x^*(y)$ is Lipschitz; corpus neighbor [2501.10806] explores non-expansive mappings suggesting strict contractive assumption is key boundary condition

### Mechanism 3
- Claim: State-dependent noise bounds can be established without uniformly bounded noise variance using induction-based argument
- Mechanism: Assumes iterates bounded up to time $k-1$ to control state-dependent noise (Assumption 4), then shows error bound at time $k$ implies boundedness at time $k$, closing induction loop
- Core assumption: Noise variance scales affinely with state squared (Assumption 4: $E[\|M\|^2] \le c_1(1+\|x\|^2+\|y\|^2)$)
- Evidence anchors: Section 1.2 states "induction-based approach to show iterates are bounded in expectation"; Section 4.1 uses induction hypothesis to bound noise and prove next step; weak direct corpus evidence for this specific induction technique

## Foundational Learning

- **Two-Time-Scale Stochastic Approximation (SA)**
  - Why needed here: Fundamental algorithm class analyzed involving two coupled iterations with different step-sizes to find fixed point
  - Quick check question: Can you explain why stepsize ratio $\alpha_k / \beta_k$ matters for stability of coupled system?

- **Contraction Mappings**
  - Why needed here: Paper relies on Banach fixed-point theorems; error decreases geometrically in deterministic case, proofs extend to stochastic setting
  - Quick check question: If $\|f(x) - f(y)\| \le \lambda \|x - y\|$, what happens if $\lambda \ge 1$?

- **Martingale Difference Sequences**
  - Why needed here: Core noise model; $E[M_{k+1} | \mathcal{F}_k] = 0$ critical for seeing why noise averages out over time rather than accumulating bias
  - Quick check question: If noise had non-zero conditional mean, what term in error bound expansion would fail to vanish?

## Architecture Onboarding

- **Component map:** $x_k$ (fast loop) -> $y_k$ (slow loop) -> $z_k = y_k - U_k$ (analysis construct) -> $U_k$ (noise accumulator) -> $(x^*, y^*)$ (target manifold)
- **Critical path:** Proof hinges on Lemma 3 (Recursive Bound) and Lemma 5 (Induction/Boundedness); implementer must ensure stepsize constants $\alpha, \beta$ large enough to dominate noise constants but small enough to satisfy stability constraints
- **Design tradeoffs:**
  - Theorem 1 ($\alpha_k = O(1/k)$): Optimal $O(1/k)$ rate but requires careful tuning of stepsize ratios and constants dependent on system parameters
  - Theorem 2 ($\alpha_k = O(1/k^a)$): More robust (constants less sensitive to system parameters) but yields slower $O(1/k^a)$ rate
- **Failure signatures:**
  - Divergence: If time-scale separation insufficient ($\beta_k / \alpha_k$ too large), fast loop cannot track slow loop's drift
  - Variance Explosion: If Assumption 4 violated (noise grows too fast), induction hypothesis fails and iterates not bounded in expectation
- **First 3 experiments:**
  1. **Constant Verification:** Simulate simple linear system satisfying assumptions; check empirical mean square error decays as $1/k$ (plot log-log scale); compare against baseline with $\alpha_k = O(1/k^{2/3})$
  2. **Noise Robustness:** Inject state-dependent noise scaling with $\|x_k\|^2$; verify iterates remain bounded in expectation as claimed; compare to baseline with uniform bounded noise
  3. **Stepsize Sweep:** Test "robust" stepsizes (Theorem 2) vs "optimal" stepsizes (Theorem 1) on non-linear problem (e.g., Gradient Descent-Ascent); observe practical trade-off in convergence speed vs stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can $O(1/k)$ finite-time bound be extended to non-linear two-time-scale stochastic approximation under Markovian noise?
- Basis in paper: [explicit] Conclusion states future direction is "obtaining similar O(1/k) bounds for non-linear setting under Markovian noise"
- Why unresolved: Current analysis relies on Assumption 4 restricting noise to martingale difference sequences; Markovian noise introduces dependencies between iterates and noise requiring different handling with mixing time assumptions
- What evidence would resolve it: Proof extending averaged noise sequence technique to handle geometric ergodicity of Markov chain, or derivation showing explicit dependence on mixing time

### Open Question 2
- Question: Is it possible to derive high probability bounds for non-linear two-time-scale stochastic approximation using proposed framework?
- Basis in paper: [explicit] Paper lists obtaining "high probability bounds for these two-time-scale iterations" as specific extension in Section 5
- Why unresolved: Paper focuses exclusively on mean square error (expectation) bounds; high probability bounds require concentration inequalities to control tail behavior of noise and iterates which current induction-based boundedness proof does not address
- What evidence would resolve it: Concentration result likely of form $P(\|x_k - x^*\|^2 \geq \epsilon) \le C \exp(-c k \epsilon)$ derived for non-linear contractive setting

### Open Question 3
- Question: Can assumption of contractive mappings be relaxed to non-expansive mappings while maintaining finite-time convergence guarantees?
- Basis in paper: [explicit] Section 5 suggests "Replacing our assumption that mappings are contractive with weaker assumptions such as non-expansive mappings could be another direction"
- Why unresolved: Current proof (Lemma 3 and Theorem 1) relies on contraction factors ($\lambda, \mu < 1$) to ensure error contracts at each step; non-expansive mappings ($\lambda = 1$) remove strict drift making stability harder to guarantee
- What evidence would resolve it: Modified analysis showing iterates remain bounded and converge (possibly to neighborhood) under non-expansive assumptions, potentially requiring additional regularization or step-size constraints

## Limitations
- Requires strict contractive assumptions on both mappings; results may not extend to non-expansive settings
- Analysis limited to martingale difference noise; Markovian noise extension remains open
- Current bounds are in expectation only; high probability bounds not established

## Confidence
High: Claims about improving convergence rate from $O(1/k^{2/3})$ to $O(1/k)$ are well-supported by the analysis and clear mathematical proofs; Medium: Extensions to more general noise settings and non-expansive mappings remain open questions; Low: Practical implementation details for complex systems not fully specified

## Next Checks
1. Verify Theorem 1 bound by implementing two-time-scale SA on simple linear contractive system and plotting empirical error decay
2. Test robustness of Theorem 2 by sweeping parameter $a$ in $(0.5,1)$ on non-linear problem and measuring trade-off between convergence speed and stability
3. Validate induction-based boundedness proof by checking empirically that iterates remain bounded under state-dependent noise scaling as specified in Assumption 4