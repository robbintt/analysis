---
ver: rpa2
title: 'LLMs for Game Theory: Entropy-Guided In-Context Learning and Adaptive CoT
  Reasoning'
arxiv_id: '2601.10775'
source_url: https://arxiv.org/abs/2601.10775
tags:
- reasoning
- game
- entropy
- context
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a framework for improving large language
  model (LLM) reasoning in sequential decision-making tasks, demonstrated using Tic-Tac-Toe.
  The core method combines entropy-guided context retrieval with adaptive chain-of-thought
  reasoning: the model retrieves relevant past game states and actions based on uncertainty,
  and dynamically adjusts reasoning depth when confidence is low.'
---

# LLMs for Game Theory: Entropy-Guided In-Context Learning and Adaptive CoT Reasoning

## Quick Facts
- arXiv ID: 2601.10775
- Source URL: https://arxiv.org/abs/2601.10775
- Reference count: 40
- Primary result: Entropy-guided retrieval + adaptive CoT improves Tic-Tac-Toe performance from -11.6% to +9.5% average outcome over 100 games vs. sub-optimal opponent.

## Executive Summary
This paper introduces an entropy-guided framework for improving large language model reasoning in sequential decision-making tasks, demonstrated using Tic-Tac-Toe. The method combines dynamic in-context retrieval with adaptive chain-of-thought reasoning, where token-level entropy from LLM outputs serves as a proxy for uncertainty. When entropy is high, the model retrieves additional relevant examples and generates multiple reasoning branches, achieving strong performance with fewer than 50 LLM queries per game compared to exhaustive tree search methods.

## Method Summary
The framework operates through a game loop that encodes board states using an autoencoder with contrastive learning, retrieves semantically similar examples based on entropy-scaled context size, and applies adaptive multi-path CoT reasoning when uncertainty is detected. The method uses LLaMA-7B with entropy thresholds to trigger retrieval of k additional examples and n_t reasoning branches, scoring branches by average entropy and selecting moves via majority voting. The system achieves +9.5% average game outcome with 48 queries versus +9.8% with 188 queries for tree-based CoT.

## Key Results
- Average game outcome improves from -11.6% to +9.5% over 100 games against sub-optimal opponent
- Reduces LLM queries from 188 (tree-based CoT) to 48 per game while maintaining performance
- Token-level entropy shows significant negative correlation with move optimality (ρ=-0.471, p<10^-3)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-level entropy correlates with decision quality and serves as uncertainty proxy
- Mechanism: Compute average token-level entropy H_step from probability distributions; higher entropy indicates uniform distributions suggesting uncertainty about optimal moves
- Core assumption: Linguistic uncertainty measured via token entropy meaningfully corresponds to decision-theoretic uncertainty about move quality
- Evidence anchors: Correlation analysis shows ρ=-0.471, p<10^-3; related work uses entropy for uncertainty but limited evidence in game settings
- Break condition: Entropy may arise from multiple equally-optimal moves (early-game symmetry) rather than genuine uncertainty; mitigated by disabling entropy branching on first move

### Mechanism 2
- Claim: Dynamically retrieving additional context when entropy is high improves decision quality efficiently
- Mechanism: Retrieval size k scales with entropy: k = min(k_max, ⌈k_0 + α·H_q⌉); high entropy triggers retrieval of semantically similar board states and optimal moves
- Core assumption: Board states with similar latent representations share relevant strategic patterns; contrastive learning produces meaningful latent space
- Evidence anchors: Entropy-guided context improves outcomes from -11.6% to -2.8% even without CoT; RAG+ICL synergy noted in related work
- Break condition: Sparse or biased retrieval database may not help or could introduce noise

### Mechanism 3
- Claim: Adaptive multi-path CoT exploration under high uncertainty achieves comparable performance to tree search with fewer queries
- Mechanism: When H_step > H_threshold, generate n_t parallel CoT branches; score by average entropy, retain top-k; select move by majority voting over first actions
- Core assumption: Exploring diverse reasoning paths when uncertain surfaces better moves than single path; majority voting converges toward optimal actions
- Evidence anchors: Entropy-guided CoT + context achieves +9.5% with 48 queries vs. tree-based CoT's +9.8% with 188 queries
- Break condition: Branches sharing systematic errors amplify bias rather than correcting it

## Foundational Learning

- Concept: Token-level entropy
  - Why needed here: Entire adaptive mechanism depends on computing and interpreting entropy from LLM output distributions
  - Quick check question: Given probability distribution [0.7, 0.2, 0.1], calculate its entropy. Would this be classified as high or low uncertainty?

- Concept: In-context learning with retrieval augmentation
  - Why needed here: Framework injects retrieved examples into prompts rather than fine-tuning; understanding this distinction is critical
  - Quick check question: Explain why adding relevant examples to a prompt might change model behavior without any parameter updates

- Concept: Game-theoretic state representation
  - Why needed here: Autoencoder and contrastive learning objectives require understanding states as formal objects with optimal actions
  - Quick check question: For a 3×3 Tic-Tac-Toe board, why might two visually different boards have the same optimal move?

## Architecture Onboarding

- Component map: Game Loop -> Board Encoder -> Vector Database D -> Retrieval Module -> Entropy Monitor -> Adaptive CoT Controller -> LLM

- Critical path:
  1. Board state → encode to z_q (Eq. 9)
  2. Compute initial entropy → determine k (Eq. 12)
  3. Retrieve top-k examples → construct context C_q
  4. LLM generates CoT → monitor H_step
  5. If H_step > threshold → spawn n_t branches
  6. Aggregate branches → select final move

- Design tradeoffs:
  - k_max vs. context window: More examples consume tokens needed for reasoning
  - Threshold granularity: More thresholds enable finer adaptation but increase tuning complexity
  - Branch budget: More branches improve exploration but linearly increase queries
  - Database coverage: Larger D improves retrieval quality but requires more precomputation

- Failure signatures:
  - Invalid move outputs: Model generates coordinates outside empty positions → trigger regeneration or random fallback
  - High entropy on trivial moves: Early-game symmetry causes false-positive uncertainty → disable entropy branching on first move
  - Branch explosion: Entropy remains high across all steps → hit k constraints and branch caps
  - Retrieval mismatch: Latent space clusters irrelevant boards → contrastive loss may need reweighting (λ in Eq. 7)

- First 3 experiments:
  1. Baseline calibration: Run 100 games with no context and no CoT; verify ~-11.6% outcome matches paper baseline. Log entropy distribution across all moves.
  2. Ablation on retrieval: Fix CoT=none, vary k ∈ {0, 1, 3, 5}. Plot outcome vs. k to verify retrieval contribution is monotonic.
  3. Entropy-threshold sweep: Fix k=3, CoT=entropy-guided. Test thresholds H ∈ {0.1, 0.2, 0.3, 0.4}. Plot outcome and query count vs. threshold to find Pareto frontier.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do entropy-guided mechanisms still yield benefits when applied to more capable base models (e.g., GPT-5, Gemini 2.5) that may already perform such reasoning intrinsically?
- Basis in paper: Section 7 states: "Future work should test whether entropy-guided mechanisms still offer benefits when the base model is more capable."
- Why unresolved: All experiments used LLaMA-7B; stronger models were not evaluated
- What evidence would resolve it: Run the same entropy-guided framework with larger models and compare performance gains

### Open Question 2
- Question: Does the framework scale to more complex games (e.g., Connect Four, Go) with higher-dimensional state spaces and longer reasoning horizons?
- Basis in paper: Section 7: "Extending the framework to more complex games such as Connect Four or Go would test scalability and robustness."
- Why unresolved: Tic-Tac-Toe has only ~3,000 states and short game length; scalability is unknown
- What evidence would resolve it: Apply the method to larger games and report win rates and query efficiency

### Open Question 3
- Question: Would dynamically updating entropy thresholds based on observed distributions outperform the fixed thresholds used in this work?
- Basis in paper: Section 4.3: "Future extensions may consider dynamically updating thresholds H_j based on the distribution of entropies observed across previous steps, rather than using fixed values."
- Why unresolved: Fixed thresholds were used throughout; adaptive thresholding was not explored
- What evidence would resolve it: Implement adaptive thresholding and compare against fixed-threshold baselines

### Open Question 4
- Question: Do alternative uncertainty estimators (ensemble variance, mutual information) provide more reliable guidance than token-level entropy in settings where uncertainty stems from ambiguity rather than lack of knowledge?
- Basis in paper: Section 7: "Alternative uncertainty estimators, such as ensemble variance or mutual information, could provide a more reliable signal."
- Why unresolved: Only token-level entropy was evaluated; correlation with optimality (ρ = -0.47) suggests noise
- What evidence would resolve it: Compare entropy-guided vs. ensemble-based or MI-based uncertainty signals on decision quality

## Limitations
- Generalization risk: Entropy-uncertainty correlation demonstrated only in Tic-Tac-Toe may not transfer to domains with richer state spaces
- Hyperparameter sensitivity: Performance gains depend on precise calibration of entropy thresholds, branch counts, and retrieval scaling
- Sparse validation: Statistical significance based on 100 games provides limited power to detect smaller effect sizes

## Confidence

- **High confidence**: The negative entropy-move optimality correlation (ρ=-0.471, p<10^-3) and efficiency claim (48 queries vs. 188 for tree-based CoT) are well-supported by reported statistics and ablation results
- **Medium confidence**: The adaptive retrieval mechanism's contribution is demonstrated via ablation (entropy-guided context improves from -11.6% to -2.8%), but interaction between retrieval quality and entropy scaling is not fully characterized
- **Low confidence**: The generalization of entropy as uncertainty proxy beyond Tic-Tac-Toe's structured vocabulary remains speculative without validation in domains like Go, Chess, or negotiation tasks

## Next Checks

1. **Cross-domain entropy correlation**: Validate the entropy-move optimality correlation (ρ=-0.471) on Connect-Four or Checkers, where board states have higher combinatorial complexity. Report whether the magnitude and significance of ρ are preserved.

2. **Retrieval database scaling**: Vary D's coverage from 5% to 50% of reachable states and measure retrieval precision (fraction of retrieved examples with same optimal move) and impact on game outcome. Characterize the retrieval utility curve.

3. **Opponent strategy robustness**: Test the framework against three opponent types: (a) perfect minimax, (b) random, (c) learned RL agent. Report whether the average outcome improvement (+9.5%) and query efficiency (48 queries) are maintained or if the method overfits to the α=0.95 sub-optimal opponent.