---
ver: rpa2
title: Scaling Law for Stochastic Gradient Descent in Quadratically Parameterized
  Linear Regression
arxiv_id: '2502.09106'
source_url: https://arxiv.org/abs/2502.09106
tags:
- lemma
- diag
- have
- theorem
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the scaling law for Stochastic Gradient Descent
  (SGD) in a quadratically parameterized linear regression model. The model is designed
  to capture feature learning, a key aspect of deep neural networks' success, which
  is absent in traditional linear models.
---

# Scaling Law for Stochastic Gradient Descent in Quadratically Parameterized Linear Regression

## Quick Facts
- **arXiv ID**: 2502.09106
- **Source URL**: https://arxiv.org/abs/2502.09106
- **Reference count**: 3
- **Primary result**: Analyzes scaling law for SGD in quadratically parameterized linear regression, showing piecewise power law behavior for excess risk with respect to model and sample size

## Executive Summary
This paper establishes scaling laws for Stochastic Gradient Descent (SGD) in a quadratically parameterized linear regression model designed to capture feature learning. The authors analyze infinitely dimensional data with power-law decay and prove that SGD's excess risk follows a piecewise power law with respect to both model size and sample size. For sufficiently large models, the excess risk achieves optimal rates in certain regimes and outperforms linear models in others, demonstrating the importance of feature learning in achieving good generalization.

The study characterizes the learning process into two distinct stages: an "adaptation" stage where SGD identifies the effective dimension set, and an "estimation" stage where global convergence is achieved. The authors provide explicit separations for generalization curves between SGD with and without feature learning, and compare them to information-theoretical lower bounds. These results show that the quadratic model with feature learning can achieve better excess risk than the linear model, especially when the true parameter opposes the covariance spectrum, contributing to our understanding of scaling laws in machine learning.

## Method Summary
The paper analyzes Stochastic Gradient Descent in a quadratically parameterized linear regression model, which captures feature learning absent in traditional linear models. The analysis considers infinitely dimensional data and ground truth, both exhibiting power-law decay rates. The authors establish upper bounds for the excess risk of SGD, demonstrating that it follows a piecewise power law with respect to both model size and sample size. The learning process is characterized into two stages: an "adaptation" stage where SGD identifies the effective dimension set, and an "estimation" stage where global convergence is achieved. The paper provides explicit separations for generalization curves between SGD with and without feature learning, and compares them to information-theoretical lower bounds.

## Key Results
- Excess risk of SGD follows a piecewise power law with respect to both model size and sample size
- For sufficiently large models, excess risk achieves optimal rates in certain regimes and outperforms linear models in others
- Quadratic model with feature learning can achieve better excess risk than linear model, especially when true parameter opposes covariance spectrum

## Why This Works (Mechanism)

## Foundational Learning
1. **Stochastic Gradient Descent (SGD)** - Why needed: Core optimization algorithm whose scaling behavior is being analyzed. Quick check: Understand basic SGD update rule and convergence properties.
2. **Linear regression** - Why needed: Baseline model for comparison with quadratically parameterized model. Quick check: Review ordinary least squares solution and excess risk definition.
3. **Feature learning** - Why needed: Key aspect captured by quadratic parameterization that distinguishes it from linear models. Quick check: Understand how quadratic parameterization enables learning of nonlinear features.
4. **Power-law decay** - Why needed: Assumed data distribution that enables analytical tractability. Quick check: Review properties of power-law distributions and their eigenvalues.
5. **Excess risk** - Why needed: Primary metric for evaluating generalization performance. Quick check: Understand difference between population risk and empirical risk.
6. **Scaling laws** - Why needed: Framework for understanding how performance depends on model and data size. Quick check: Review typical scaling behaviors (power laws, double descent, etc.).

## Architecture Onboarding

**Component Map**: Data distribution -> Quadratic parameterization -> SGD optimization -> Excess risk analysis -> Two-stage learning characterization

**Critical Path**: The analysis follows the path from data assumptions through model parameterization to optimization algorithm, ultimately deriving bounds on excess risk. The critical insight is that feature learning through quadratic parameterization enables better scaling behavior than linear models.

**Design Tradeoffs**: The quadratic parameterization captures feature learning but adds complexity compared to linear models. The infinitely dimensional assumption enables analytical tractability but may not reflect finite-dimensional practical scenarios. The power-law decay assumption allows for closed-form analysis but may not capture all data distributions.

**Failure Signatures**: If the true data distribution deviates significantly from power-law decay, the scaling laws may not hold. If the model size is too small, the benefits of feature learning may not manifest. If the sample size is insufficient relative to model complexity, overfitting may occur.

**First Experiments**:
1. Verify the piecewise power law behavior of excess risk on synthetic data with controlled power-law decay rates
2. Compare generalization performance of quadratic vs. linear models across different model and sample sizes
3. Test the two-stage learning characterization by monitoring adaptation and estimation phases during training

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis assumes infinitely dimensional data and ground truth with power-law decay, which may not reflect practical finite-dimensional scenarios
- Quadratic parameterization may not fully represent the complexity of deep neural network architectures
- Study focuses on average-case analysis and may not capture worst-case scenarios or pathological cases
- Characterization of two-stage learning process relies on specific assumptions about data distribution and model parameterization

## Confidence
- **High Confidence**: Upper bounds for excess risk following piecewise power laws with respect to model and sample size; characterization of two-stage learning process (adaptation and estimation)
- **Medium Confidence**: Explicit separation of generalization curves between SGD with and without feature learning; comparison to information-theoretical lower bounds
- **Low Confidence**: Claim that quadratic model with feature learning achieves better excess risk than linear model in all regimes, particularly when true parameter opposes covariance spectrum

## Next Checks
1. Validate scaling laws on finite-dimensional data with varying decay rates to assess robustness of power-law behavior
2. Extend analysis to more complex model parameterizations that better approximate deep neural networks to verify generality of findings
3. Conduct empirical studies on real-world datasets to compare performance of SGD with and without feature learning, assessing practical relevance of theoretical bounds