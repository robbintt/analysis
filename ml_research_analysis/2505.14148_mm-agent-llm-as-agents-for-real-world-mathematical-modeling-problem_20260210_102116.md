---
ver: rpa2
title: 'MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem'
arxiv_id: '2505.14148'
source_url: https://arxiv.org/abs/2505.14148
tags:
- modeling
- problem
- task
- mathematical
- momentum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes the task of real-world mathematical modeling
  and introduces MM-Agent, an LLM-based agent framework that automates problem analysis,
  model formulation, computational solving, and report generation. To enable evaluation,
  the authors construct MM-Bench, a benchmark of 111 MCM/ICM problems spanning ten
  domains and eight task types.
---

# MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem

## Quick Facts
- arXiv ID: 2505.14148
- Source URL: https://arxiv.org/abs/2505.14148
- Authors: Fan Liu; Zherui Yang; Cancheng Liu; Tianrui Song; Xiaofeng Gao; Hao Liu
- Reference count: 40
- Primary result: MM-Agent achieves 11.88% improvement over human expert solutions on MCM/ICM problems while maintaining low cost and runtime.

## Executive Summary
This paper formalizes real-world mathematical modeling as an automated task and introduces MM-Agent, an LLM-based agent framework that handles problem analysis, model formulation, computational solving, and report generation. The system is evaluated on MM-Bench, a benchmark of 111 MCM/ICM problems across ten domains and eight task types. MM-Agent uses a hierarchical knowledge library (HMML) and an actor-critic iterative optimization mechanism to construct and refine mathematical models. Experimental results show MM-Agent significantly outperforms baselines and human experts while maintaining low computational cost. The system also assisted two teams in winning the Finalist Award (top 2.0% among 27,456 teams) in MCM/ICM 2025.

## Method Summary
MM-Agent is a multi-agent framework that automates real-world mathematical modeling through four sequential phases: (1) Problem Analysis, which decomposes problems into subtasks and constructs dependency graphs; (2) Mathematical Modeling, which retrieves relevant methods from the Hierarchical Mathematical Modeling Library (HMML) and refines models through actor-critic iteration; (3) Computational Solving, which generates, executes, and debugs code for model solutions; and (4) Solution Reporting, which generates structured LaTeX reports. The system uses GPT-4o or DeepSeek-R1 as the underlying LLM and is evaluated on MM-Bench, a benchmark of 111 MCM/ICM problems. Solutions are scored on four dimensions: Analysis Evaluation, Modeling Rigorousness, Practicality/Scientificity, and Result & Bias Analysis.

## Key Results
- MM-Agent achieves an 11.88% improvement over human expert solutions on MM-Bench
- The system assisted two teams in winning the Finalist Award (top 2.0%) in MCM/ICM 2025
- MM-Agent maintains low computational cost while delivering high-quality solutions
- The framework demonstrates strong performance across ten domains and eight task types

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical method retrieval improves modeling rigor by enabling task-relevant schema selection
The Hierarchical Mathematical Modeling Library (HMML) uses a three-level tree (domains → subdomains → method nodes) with DFS traversal and similarity scoring S(D,N) = ω·Sim(D,N) + (1-ω)·Sim(D,N_parent), returning top-K method nodes. Core assumption: Modeling expertise can be decomposed into retrievable, hierarchical schemas; embedding similarity correlates with method appropriateness. Break condition: If subtask embeddings are ambiguous or HMML coverage is insufficient for novel domains, retrieval may return generic or mismatched methods.

### Mechanism 2: Actor-critic iteration refines modeling schemes toward scientific validity
Actor generates scheme M^(t)_i using retrieved methods and dependencies; Critic provides feedback F^(t)_i; Actor refines M^(t+1)_i = π_θ(M^(t)_i, F^(t)_i; x_r); repeats for n_r iterations. Core assumption: Critic feedback identifies genuine modeling flaws; LLM can integrate critique into improved formulations. Break condition: If critic feedback is superficial or actor fails to incorporate corrections, iteration converges to suboptimal schemes without improvement.

### Mechanism 3: Task dependency analysis with memory transfer enables coherent multi-stage modeling
Coordinator decomposes problem D = {D_1,...,D_n}, constructs dependency graph G = (V,E), executes sequentially, stores intermediate outputs Q_i = {M_i, C_i, O_i} in memory H for downstream use. Core assumption: Real-world modeling problems have decomposable structure with identifiable dependencies; earlier task outputs are reusable for later tasks. Break condition: If task dependencies are cyclic, incorrectly identified, or memory transfer fails, downstream tasks operate on incomplete/incorrect inputs.

## Foundational Learning

- **Concept: Mathematical modeling vs. mathematical reasoning**
  - Why needed here: The paper explicitly distinguishes these; modeling requires open-ended problem abstraction without predefined formulation, while reasoning assumes given structures
  - Quick check question: Can you explain why "momentum in tennis" is a modeling problem rather than a reasoning problem?

- **Concept: Hierarchical knowledge organization**
  - Why needed here: HMML's three-tier structure is central; understanding how domains/subdomains/methods relate enables effective retrieval debugging
  - Quick check question: Given a subtask about "resource allocation under uncertainty," which HMML domain and subdomain would you search first?

- **Concept: Actor-critic optimization**
  - Why needed here: Core refinement loop; distinguishing actor (generation) from critic (evaluation) roles clarifies failure attribution
  - Quick check question: In actor-critic iteration, what happens if the critic always approves the actor's proposals?

## Architecture Onboarding

- **Component map:** Problem input → Dependency graph G → For each task D_i: HMML retrieval → Actor-critic refinement → Code generation → Execution → Memory update → Final synthesis into LaTeX report
- **Critical path:** Problem Analysis → Mathematical Modeling → Computational Solving → Solution Reporting (sequential pipeline)
- **Design tradeoffs:** (1) HMML hierarchy depth vs. retrieval precision—deeper trees may miss cross-domain methods; (2) Actor-critic iterations n_r vs. cost/latency—more iterations improve quality but increase token consumption; (3) Task granularity—finer decomposition increases coordination overhead but may improve tractability
- **Failure signatures:** (1) Low Modeling Rigorousness scores → likely HMML retrieval returning generic methods or critic feedback insufficient; (2) Code execution failures → formula-to-code translation errors or environment mismatches; (3) Incoherent reports → memory transfer failures between tasks or dependency graph errors
- **First 3 experiments:**
  1. Run MM-Agent on a single MM-Bench problem with verbose logging to trace HMML retrieval scores, actor-critic exchanges, and dependency graph construction
  2. Ablate actor-critic iteration (set n_r=1) and compare Modeling Rigorousness scores against full configuration to quantify refinement contribution
  3. Test on a problem from a domain underrepresented in HMML to assess retrieval robustness and identify coverage gaps

## Open Questions the Paper Calls Out
None

## Limitations
- HMML coverage gap: The 98-method hierarchical library is fixed and may not generalize to novel problem domains beyond MCM/ICM scope
- Iteration termination: Actor-critic loops run for fixed n_r steps without clear convergence criteria, risking unnecessary computation or premature termination
- Evaluation subjectivity: Expert scoring introduces variability; inter-rater reliability metrics are not reported for the four-dimensional rubric
- Reproducibility barrier: Key components (HMML content, MLE-Solver integration) depend on external artifacts not fully specified in the paper

## Confidence
- **High confidence:** MM-Agent framework architecture and four-phase pipeline are clearly specified
- **Medium confidence:** Performance claims (11.88% improvement, Finalist Award) are supported by MM-Bench results and competition outcomes
- **Medium confidence:** Actor-critic refinement mechanism effectiveness, though iteration termination criteria remain unclear

## Next Checks
1. Execute ablation studies with varying n_r values to identify optimal iteration count and assess convergence behavior
2. Test HMML retrieval on out-of-distribution problems to quantify coverage limitations and identify missing method categories
3. Replicate the evaluation process with multiple expert raters on a subset of problems to measure scoring consistency and rubric reliability