---
ver: rpa2
title: Circuit Distillation
arxiv_id: '2509.25002'
source_url: https://arxiv.org/abs/2509.25002
tags:
- circuit
- distillation
- teacher
- student
- align
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces circuit distillation, a method that transfers
  specific internal algorithms (circuits) from a large teacher model to a smaller
  student model, rather than just mimicking outputs. The approach identifies functionally
  analogous circuit components between models using ablation impact similarity, then
  aligns their representations using a composite loss combining task performance and
  Centered Kernel Alignment (CKA).
---

# Circuit Distillation

## Quick Facts
- arXiv ID: 2509.25002
- Source URL: https://arxiv.org/abs/2509.25002
- Authors: Somin Wadhwa; Silvio Amir; Byron C. Wallace
- Reference count: 23
- Primary result: Circuit distillation transfers internal algorithms from teacher to student, improving accuracy by up to 5 percentage points while updating only 11-15% of parameters

## Executive Summary
This paper introduces circuit distillation, a method that transfers specific internal algorithms (circuits) from a large teacher model to a smaller student model, rather than just mimicking outputs. The approach identifies functionally analogous circuit components between models using ablation impact similarity, then aligns their representations using a composite loss combining task performance and Centered Kernel Alignment (CKA). Evaluated on entity tracking and theory of mind tasks using Llama3 models, circuit distillation achieved accuracy improvements of up to 5 percentage points over standard distillation, while only adjusting 11-15% of parameters. A control condition using random head assignments performed worse than baseline, confirming that principled alignment matters. The method offers both better performance and efficiency by targeting specific computational pathways rather than full model training.

## Method Summary
Circuit distillation identifies functionally correspondent circuit components between teacher and student models using ablation impact similarity, then aligns their representations with a composite loss combining task performance (cross-entropy) and Centered Kernel Alignment (CKA). The method first identifies circuit heads in the teacher model using path patching or activation patching, then maps student heads to teacher heads based on matching ablation impacts. During training, only the aligned circuit subset (11-15% of attention heads) is updated using the combined loss. This targeted approach aims to transfer the specific computational mechanisms rather than just matching outputs.

## Key Results
- Circuit distillation achieved 0.82 accuracy vs 0.77 for standard distillation on entity tracking (3B student)
- Performance improvements of up to 5 percentage points over baseline methods
- Circuit-only evaluation showed high faithfulness (>0.9) to full model performance
- Random head mapping control degraded performance to 0.63, confirming alignment necessity

## Why This Works (Mechanism)

### Mechanism 1: Ablation-Based Functional Correspondence Mapping
- Claim: Corresponding circuit components between teacher and student can be identified by matching their causal importance to task performance.
- Mechanism: For each attention head, measure performance drop under mean ablation. Map student head h_s to teacher head h_t that minimizes |ΔP_s(h_s) - ΔP_t(h_t)|, where teacher ablation impact is calculated relative to student baseline.
- Core assumption: Heads with similar ablation impacts serve functionally analogous roles in their respective circuits, even across different model scales.

### Mechanism 2: CKA-Based Representational Alignment
- Claim: Centered Kernel Alignment enforces functional similarity between circuit heads while being invariant to architectural differences.
- Mechanism: Compute Gram matrices K=XX^T and L=YY^T from activation matrices of paired heads, then minimize L_CKA = 1 - CKA(K,L), where CKA normalizes HSIC to achieve scale and transformation invariance.
- Core assumption: Representations that induce similar pairwise similarity structures across examples reflect similar computational functions, regardless of dimensionality.

### Mechanism 3: Targeted Parameter Updates via Composite Loss
- Claim: Updating only the aligned circuit subset (11-15% of attention heads) with combined CE + CKA loss transfers mechanisms more effectively than full-model behavioral distillation.
- Mechanism: L_total = L_task(y, ŷ_s) + λ Σ L_CKA over aligned pairs. The CKA term provides denser gradient signal that guides internal representations, while CE ensures output correctness.
- Core assumption: The identified circuit is the primary locus of task capability; remaining parameters are either redundant or task-irrelevant.

## Foundational Learning

- Concept: **Knowledge Distillation Fundamentals**
  - Why needed here: Circuit distillation builds on standard behavioral distillation; understanding the baseline (KL divergence on output logits) clarifies what's being extended.
  - Quick check question: What does standard distillation optimize, and what information does it discard?

- Concept: **Mechanistic Interpretability / Circuits**
  - Why needed here: The method assumes pre-identified circuits exist in the teacher; understanding how path patching and activation patching isolate causal subgraphs is essential.
  - Quick check question: How does path patching determine which attention heads belong to a circuit?

- Concept: **Centered Kernel Alignment (CKA)**
  - Why needed here: CKA is the core similarity metric; understanding its invariance properties (orthogonal transformations, isotropic scaling) explains why it works across architectures.
  - Quick check question: Why can't cosine similarity be used directly to compare representations across different model sizes?

## Architecture Onboarding

- Component map: Teacher circuit heads -> Ablation impact computation -> Student head mapping -> CKA loss module -> Composite optimizer

- Critical path:
  1. Identify teacher circuit heads using prior mechanistic work (path/activation patching)
  2. Compute ablation impacts for all student heads and map to teacher heads
  3. Forward pass through teacher and student; extract activations from paired heads
  4. Compute CKA loss per pair; combine with CE loss
  5. Backprop only through student circuit heads (11-15% of attention parameters)

- Design tradeoffs:
  - **Circuit targeting vs full model**: Targeting is more parameter-efficient but depends entirely on circuit identification quality
  - **λ (CKA weight)**: Too high → may sacrifice task accuracy for alignment; too low → reverts to behavioral distillation
  - **Ablation-based mapping vs alternatives**: Heuristic and task-specific; more sophisticated correspondence methods (learned mappings) may improve alignment but add complexity

- Failure signatures:
  - **Performance below CE baseline**: Indicates incorrect circuit identification or poor head mapping; check that CE + Rand CKA also degrades (sanity check)
  - **Large gap between full-model and circuit-only performance**: Suggests circuit is incomplete or student has learned alternative mechanisms
  - **CKA loss not decreasing**: Mapping may be incorrect; re-verify ablation-impact correlations

- First 3 experiments:
  1. **Baseline validation**: Replicate entity tracking results with Llama3-3B student; confirm CE + Align CKA (0.82) > CE only (0.77) > CE + Rand CKA (0.63)
  2. **Ablation study on λ**: Sweep λ ∈ {0.1, 0.5, 1.0, 2.0} to find optimal balance between task loss and alignment; monitor both accuracy and CKA convergence
  3. **Circuit faithfulness test**: Run inference with only identified student circuit active (all other heads mean-ablated); verify performance approaches full-model accuracy (faithfulness >0.95)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more sophisticated mapping methods improve upon the heuristic ablation-based approach for identifying functional correspondences between teacher and student components?
- Basis in paper: [explicit] The authors state that their "ablation-based approach to mapping components from teacher to student is heuristic" and suggest that "more sophisticated methods... may yield further improvements."
- Why unresolved: The current method relies solely on similarity of performance drops upon ablation, which may not capture nuanced functional analogies across different model sizes.
- What evidence would resolve it: Empirical comparisons showing superior student performance when using alternative mapping techniques (e.g., activation correlation or graph matching) versus the ablation baseline.

### Open Question 2
- Question: How robust is circuit distillation to errors or noise in the prior identification of the teacher's circuits?
- Basis in paper: [explicit] The paper notes that the method is "contingent on the prior identification of a well-defined circuit in the teacher model" and that "finding such circuits remains challenging."
- Why unresolved: The experiments assume the existence of well-characterized circuits from prior literature, leaving the sensitivity of the method to imperfect circuit discovery unexplored.
- What evidence would resolve it: Analysis of performance degradation when distilling using incomplete or partially incorrect circuit masks compared to the "gold standard" circuits used in the paper.

### Open Question 3
- Question: Does circuit distillation generalize to more complex tasks and larger model scales than those evaluated?
- Basis in paper: [explicit] The authors acknowledge that their "empirical analysis is also somewhat limited" because they "considered only a few tasks" and "used relatively small models owing to limitations in compute."
- Why unresolved: It is unclear if the efficiency gains (targeting 11-15% of parameters) and accuracy improvements hold for billion-parameter models or tasks with less localized algorithms.
- What evidence would resolve it: Evaluation of circuit distillation on standard LLM benchmarks (e.g., reasoning or coding) using models with parameter counts significantly larger than 8B.

## Limitations

- Method requires pre-identified circuits in teacher model, which remains challenging to discover systematically
- Empirical analysis limited to few tasks and small models due to compute constraints
- Ablation-based head mapping is heuristic and may not capture nuanced functional correspondences

## Confidence

**High confidence**: CKA invariance properties, parameter efficiency gains (11-15% targeting), control condition results (Rand CKA underperformance)
**Medium confidence**: Ablation-based head mapping validity, circuit completeness assumption
**Low confidence**: CKA implies functional equivalence, generalizability across task domains

## Next Checks

1. **Mapping fidelity test**: For each aligned head pair, measure CKA similarity before and after training, and correlate with ablation impact preservation. Verify that CKA alignment tracks with ablation impact correlation, not just random improvement.

2. **Circuit completeness verification**: Systematically ablate non-circuit heads during inference to identify which additional heads, when removed, cause performance degradation. Compare this set to the identified circuit to quantify completeness.

3. **Cross-task generalization**: Apply the same circuit distillation pipeline to a third task domain (e.g., mathematical reasoning) using circuits identified through different mechanistic interpretability methods. Measure whether the ablation-impact mapping and CKA alignment approach transfers or requires task-specific adaptation.