---
ver: rpa2
title: Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2
arxiv_id: '2502.03544'
source_url: https://arxiv.org/abs/2502.03544
tags:
- language
- problems
- geometry
- points
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AlphaGeometry2 (AG2) is a neuro-symbolic system that solves International
  Mathematical Olympiad (IMO) geometry problems at a level surpassing an average gold
  medalist. It improves upon its predecessor, AlphaGeometry, by expanding the domain
  language to handle movements of objects, linear equations of angles, ratios, and
  distances, and non-constructive problems, increasing coverage from 66% to 88% of
  recent IMO geometry problems.
---

# Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2

## Quick Facts
- arXiv ID: 2502.03544
- Source URL: https://arxiv.org/abs/2502.03544
- Reference count: 40
- AlphaGeometry2 solves 84% of all geometry problems from the 2000-2024 IMO, compared to 54% for the original AlphaGeometry

## Executive Summary
AlphaGeometry2 (AG2) is a neuro-symbolic system that achieves gold-medalist level performance in solving International Mathematical Olympiad (IMO) geometry problems. The system represents a significant advancement over its predecessor by expanding the domain language to handle object movements, linear equations of angles/ratios/distances, and non-constructive problems, increasing coverage from 66% to 88% of recent IMO geometry problems. Through architectural improvements including a faster symbolic engine, a Gemini-based language model, and the novel Shared Knowledge Ensemble of Search Trees (SKEST) algorithm, AG2 solves 84% of all geometry problems from the 2000-2024 IMO. The system was part of the setup that achieved silver-medal standard at IMO 2024 and makes progress toward fully automating solutions from natural language input.

## Method Summary
AlphaGeometry2 builds upon the neuro-symbolic approach of its predecessor by expanding the problem representation to handle a broader class of geometry problems. The system integrates a symbolic engine with a language model based on the Gemini architecture, using the Shared Knowledge Ensemble of Search Trees (SKEST) algorithm to enable multiple search trees to share knowledge during problem solving. The domain language expansion allows handling object movements, linear equations involving angles, ratios, and distances, and non-constructive problems. The system uses synthetic data generation for training and employs a more robust symbolic engine compared to the original AlphaGeometry. These improvements collectively enable the system to solve 84% of IMO geometry problems from 2000-2024, representing a substantial leap in automated geometry problem solving.

## Key Results
- AG2 solves 84% of all geometry problems from the 2000-2024 IMO, compared to 54% for the original AlphaGeometry
- The system achieves gold-medalist level performance, surpassing the average gold medalist's problem-solving ability
- AG2's domain language expansion increases coverage from 66% to 88% of recent IMO geometry problems
- The system was part of the setup achieving silver-medal standard at IMO 2024
- Progress made toward fully automating solutions from natural language input

## Why This Works (Mechanism)
The system's effectiveness stems from its neuro-symbolic architecture that combines the pattern recognition capabilities of language models with the logical rigor of symbolic reasoning. The expansion of the domain language allows the system to represent and manipulate a wider range of geometric concepts, including object movements and linear equations involving angles, ratios, and distances. The SKEST algorithm enables efficient knowledge sharing across multiple search trees, preventing redundant exploration and accelerating the discovery of solutions. The integration of the Gemini architecture provides a more powerful language model capable of generating and evaluating geometric constructions. The faster symbolic engine ensures that logical deductions can be performed efficiently even as the problem space expands. Together, these components create a system that can systematically explore geometric problem spaces while leveraging learned patterns to guide the search toward promising solution paths.

## Foundational Learning
- Symbolic reasoning in geometry - why needed: Provides logical rigor for geometric proofs and deductions; quick check: Verify the system can derive valid conclusions from given geometric constraints
- Language model-guided search - why needed: Enables pattern recognition and heuristic guidance in exploring solution spaces; quick check: Confirm the model can prioritize promising construction paths
- Synthetic data generation - why needed: Creates sufficient training data for rare or complex geometric scenarios; quick check: Ensure generated problems cover the full range of IMO difficulty levels
- Neuro-symbolic integration - why needed: Combines the strengths of neural pattern recognition with symbolic logical deduction; quick check: Validate that neural suggestions are correctly processed by the symbolic engine
- Multi-tree search with knowledge sharing - why needed: Prevents redundant exploration and accelerates solution discovery; quick check: Measure search efficiency gains from shared knowledge
- Domain language expansion - why needed: Enables representation of previously unsolvable problem types; quick check: Test the system on problems requiring object movements and non-constructive solutions

## Architecture Onboarding

Component map: Natural Language Input -> Language Model (Gemini) -> Symbolic Engine -> Search Trees (SKEST) -> Solution Output

Critical path: Problem statement → Language model interpretation → Symbolic representation → SKEST search → Proof construction → Solution verification

Design tradeoffs: The system prioritizes breadth of problem coverage over computational efficiency, accepting longer solution times to handle more complex problem types. The expanded domain language increases expressiveness but also increases the search space complexity.

Failure signatures: The system may fail on problems requiring deep mathematical insight beyond pattern matching, struggle with highly ambiguous natural language descriptions, and encounter computational limitations when exploring extremely large search spaces.

First experiments: 1) Test AG2 on a single IMO problem from each difficulty level to establish baseline performance. 2) Compare solution times and success rates between AG2 and the original AlphaGeometry on identical problems. 3) Evaluate the system's ability to handle problems requiring object movements and non-constructive solutions.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond noting progress toward fully automating solutions from natural language input. The primary open challenge appears to be further improving the system's ability to handle natural language problem descriptions and generalizing to problem types beyond the IMO benchmark.

## Limitations
- Performance claims rely on a curated IMO benchmark that may not represent broader geometric reasoning tasks
- Effectiveness of domain language expansion and SKEST algorithm untested in broader problem-solving scenarios
- System's adaptability to novel problem types requiring deeper mathematical insight remains unproven
- Natural language input handling is limited and may not generalize to unstructured or ambiguous descriptions
- Reliance on synthetic data generation raises questions about performance on truly novel problem types

## Confidence

High: The technical improvements in AG2, such as the expanded domain language and the SKEST algorithm, are well-documented and validated through experimental results.

Medium: The claim of surpassing gold-medalist performance is supported by benchmark results but may not fully account for the nuances of human problem-solving.

Low: The generalizability of AG2 to problems outside the curated IMO benchmark and its robustness to novel or ambiguous inputs are not thoroughly tested.

## Next Checks
1. Test AG2 on a broader set of geometry problems, including those from non-IMO sources, to assess its generalizability.
2. Evaluate the system's performance on problems requiring non-constructive solutions or deeper mathematical insight to validate the expanded domain language.
3. Conduct user studies to compare AG2's problem-solving approach with that of human gold medalists, focusing on creativity, intuition, and adaptability.