---
ver: rpa2
title: Offline Reinforcement Learning using Human-Aligned Reward Labeling for Autonomous
  Emergency Braking in Occluded Pedestrian Crossing
arxiv_id: '2504.08704'
source_url: https://arxiv.org/abs/2504.08704
tags:
- reward
- vehicle
- pedestrian
- offline
- driving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel method for generating human-aligned
  reward labels to enable real-world driving datasets to be used for Offline Reinforcement
  Learning (RL) training. The approach introduces an adaptive safety component using
  semantic segmentation maps to assess critical risk factors in driving scenes, allowing
  autonomous vehicles to prioritize safety over efficiency in potential collision
  scenarios.
---

# Offline Reinforcement Learning using Human-Aligned Reward Labeling for Autonomous Emergency Braking in Occluded Pedestrian Crossing

## Quick Facts
- arXiv ID: 2504.08704
- Source URL: https://arxiv.org/abs/2504.08704
- Authors: Vinal Asodia; Zhenhua Feng; Saber Fallah
- Reference count: 40
- This paper presents a novel method for generating human-aligned reward labels to enable real-world driving datasets to be used for Offline Reinforcement Learning (RL) training.

## Executive Summary
This paper introduces a novel approach for generating human-aligned reward labels to enable the use of real-world driving datasets for Offline Reinforcement Learning (RL) training. The method incorporates an adaptive safety component using semantic segmentation maps to assess critical risk factors in driving scenes, allowing autonomous vehicles to prioritize safety over efficiency in potential collision scenarios. The approach applies varying levels of spatial attention to different objects in image observations, improving decision-making for vehicle longitudinal control tasks.

When evaluated in an occluded pedestrian crossing scenario using CARLA simulation, the generated reward labels closely matched simulation rewards. Training various Offline RL algorithms with these labels produced competitive results compared to other baselines, demonstrating the effectiveness of the approach in producing reliable and human-aligned reward signals for autonomous driving systems.

## Method Summary
The paper presents a method for generating human-aligned reward labels to enable real-world driving datasets to be used for Offline Reinforcement Learning (RL) training. The approach introduces an adaptive safety component using semantic segmentation maps to assess critical risk factors in driving scenes. This allows autonomous vehicles to prioritize safety over efficiency in potential collision scenarios by applying varying levels of spatial attention to different objects in image observations. The method is evaluated in an occluded pedestrian crossing scenario with varying pedestrian traffic levels using CARLA simulation, showing that the generated reward labels closely match simulation rewards and produce competitive results when used to train various Offline RL algorithms.

## Key Results
- Generated reward labels closely matched simulation rewards in occluded pedestrian crossing scenarios
- Training various Offline RL algorithms with these labels produced competitive results compared to baselines
- The approach demonstrated effectiveness in producing reliable and human-aligned reward signals for autonomous driving systems

## Why This Works (Mechanism)
The method works by incorporating semantic segmentation maps to assess risk factors in driving scenes, allowing the system to prioritize safety in potential collision scenarios. By applying varying levels of spatial attention to different objects in image observations, the approach can better capture human-aligned decision-making for vehicle longitudinal control tasks. This combination of adaptive safety assessment and attention mechanisms enables the generation of more realistic and human-aligned reward signals, which in turn leads to improved performance when training Offline RL algorithms.

## Foundational Learning
- Semantic segmentation: why needed - to identify and classify objects in the driving scene; quick check - verify segmentation accuracy across diverse scenarios
- Spatial attention mechanisms: why needed - to prioritize different objects based on their importance for safety; quick check - test attention weights on various object types
- Reward shaping: why needed - to create human-aligned reward signals from raw driving data; quick check - compare generated rewards with human preferences
- Offline RL: why needed - to train policies using pre-collected data without online interaction; quick check - evaluate policy performance on held-out validation data
- CARLA simulation: why needed - to provide a controlled environment for testing and validation; quick check - verify simulation accuracy against real-world scenarios
- Risk assessment: why needed - to prioritize safety in decision-making; quick check - test risk assessment accuracy in various traffic scenarios

## Architecture Onboarding

Component Map: Semantic Segmentation -> Risk Assessment -> Spatial Attention -> Reward Generation -> Offline RL Training

Critical Path: The critical path involves processing image observations through semantic segmentation, conducting risk assessment, applying spatial attention, generating human-aligned rewards, and using these rewards to train Offline RL algorithms for autonomous emergency braking.

Design Tradeoffs: The approach trades computational complexity for improved safety prioritization and human-aligned decision-making. While the use of semantic segmentation and spatial attention mechanisms may introduce additional computational overhead, it enables more nuanced and safety-focused behavior in autonomous vehicles.

Failure Signatures: Potential failures may include incorrect semantic segmentation leading to inaccurate risk assessment, improper spatial attention weighting resulting in suboptimal safety prioritization, or reward generation that does not accurately reflect human preferences in complex driving scenarios.

First Experiments:
1. Test semantic segmentation accuracy across diverse urban driving scenarios
2. Evaluate spatial attention mechanisms on various object types and traffic situations
3. Compare generated reward labels with human preferences in controlled scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- The offline nature of the reinforcement learning approach may not fully capture the dynamic and unpredictable nature of real-world driving scenarios
- Reliance on semantic segmentation maps may introduce computational overhead and potential errors in complex urban environments
- The effectiveness of the adaptive safety component in scenarios beyond occluded pedestrian crossings is not thoroughly explored
- The system's handling of multi-agent interactions or complex traffic scenarios involving multiple vulnerable road users simultaneously is not addressed

## Confidence

| Claim Cluster | Confidence Level |
|---------------|------------------|
| Human-aligned reward generation accuracy | Medium |
| Competitive performance of Offline RL algorithms | Medium |
| Effectiveness in occluded pedestrian crossing scenarios | High |

## Next Checks

1. Real-world driving dataset validation: Test the method's performance on diverse, real-world driving datasets to assess its generalizability beyond simulated environments.

2. Multi-scenario evaluation: Conduct comprehensive testing across various complex driving scenarios, including multi-agent interactions, to evaluate the system's robustness and adaptability.

3. Computational efficiency analysis: Perform detailed analysis of the computational overhead introduced by the semantic segmentation-based risk assessment and its impact on real-time decision-making capabilities in autonomous driving systems.