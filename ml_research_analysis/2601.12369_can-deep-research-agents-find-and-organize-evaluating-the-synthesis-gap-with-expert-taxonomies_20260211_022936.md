---
ver: rpa2
title: Can Deep Research Agents Find and Organize? Evaluating the Synthesis Gap with
  Expert Taxonomies
arxiv_id: '2601.12369'
source_url: https://arxiv.org/abs/2601.12369
tags:
- research
- papers
- deep
- expert
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TaxoBench, a benchmark designed to evaluate
  Deep Research Agents' core abilities to retrieve essential papers and organize them
  into expert-like taxonomies. The benchmark uses 72 highly-cited survey taxonomies
  containing 3,815 papers mapped to expert-defined categories as ground truth.
---

# Can Deep Research Agents Find and Organize? Evaluating the Synthesis Gap with Expert Taxonomies

## Quick Facts
- arXiv ID: 2601.12369
- Source URL: https://arxiv.org/abs/2601.12369
- Reference count: 40
- Primary result: Best model retrieves only 20.9% of expert-cited papers; even with perfect retrieval, organization achieves only 31.24% ARI

## Executive Summary
This paper introduces TaxoBench, a benchmark that evaluates Deep Research Agents' abilities to retrieve essential papers and organize them into expert-like taxonomies. Using 72 highly-cited survey taxonomies containing 3,815 papers, the benchmark reveals severe limitations: the best agent retrieves only 20.9% of expert-cited papers, and even with perfect retrieval, the best model achieves only 31.24% ARI in organization. All models converge to a narrow Sem-Path band (28-29%), suggesting shared bottlenecks in hierarchical reasoning.

## Method Summary
The benchmark uses two evaluation modes: Deep Research mode tests end-to-end retrieval and organization from topic only, while Bottom-Up mode isolates organization by providing the expert paper set. Novel hierarchy-aware metrics including US-TED, US-NTED, and Sem-Path capture structural quality beyond flat clustering scores. The dataset comprises 72 expert-authored survey taxonomies with 3,815 papers mapped to expert-defined categories as ground truth. Models are evaluated on retrieval (Recall, Precision, F1), leaf-level organization (ARI, V-Measure), and hierarchy-level structure (US-NTED, Sem-Path).

## Key Results
- Best model retrieves only 20.9% of expert-cited papers (Deep Research mode)
- With perfect retrieval, best model achieves only 31.24% ARI in organization (Bottom-Up mode)
- All models converge to 28-29% Sem-Path score, indicating shared hierarchy-reasoning ceiling
- Retrieval quality strongly correlates with hierarchy-level organization quality (ρ=0.89, p=0.007)

## Why This Works (Mechanism)

### Mechanism 1
Isolating retrieval from organization via two evaluation modes reveals independent bottlenecks. Deep Research mode tests end-to-end performance; Bottom-Up mode provides expert paper sets to measure organization independently. The gap between ARI (global, ~4%) and ARI∩ (retrieved subset only, ~40%) exposes retrieval as the primary bottleneck.

### Mechanism 2
Hierarchy-aware metrics capture structural alignment that flat clustering metrics miss. US-TED/US-NTED compute unordered tree edit distance via minimum-cost bipartite matching on sibling nodes; Sem-Path evaluates ancestor-chain consistency per aligned paper. These penalize parent-child errors that ARI/V-Measure ignore.

### Mechanism 3
Retrieval quality strongly correlates with hierarchy-level organization quality. Better retrieval yields paper sets closer to expert-cited sets, which naturally align with expert classification logic. Poor retrieval introduces off-topic papers that impose alternative organizing principles.

## Foundational Learning

### Concept: Adjusted Rand Index (ARI)
Why needed here: Leaf-level organization metric; understanding why models score ~30% ARI even with perfect retrieval requires grasping ARI's sensitivity to cluster count mismatches.
Quick check: If a model creates 20 clusters where experts used 10, would ARI penalize over-segmentation even if all papers are "correctly" grouped relative to each other?

### Concept: Tree edit distance with unordered sibling matching
Why needed here: US-TED core algorithm; must understand why sibling order doesn't matter for taxonomies but parent-child relations do.
Quick check: Why does the Hungarian algorithm appear in the metric computation?

### Concept: Over-segmentation bias in clustering
Why needed here: Explains Finding 4—models fragment topics into fine-grained clusters rather than consolidating.
Quick check: If Homogeneity = 82% and Completeness = 66%, what does this imply about how the model divides papers compared to experts?

## Architecture Onboarding

### Component map
Input → Retrieval Module (Deep Research mode only) → Paper Set → Taxonomy Generator → Hierarchy Output → Evaluation (Leaf: ARI/V-Measure; Hierarchy: US-NTED/Sem-Path)

### Critical path
Retrieval quality (F1) → paper-set composition → downstream Sem-Path ceiling; improving organization without fixing retrieval yields limited gains

### Design tradeoffs
Title+Abstract input yields better expert alignment than richer inputs (+Summary, +Core-task); models over-fit to details and diverge from expert conventions

### Failure signatures
(1) ARI << ARI∩ indicates retrieval bottleneck; (2) Sem-Path convergence at 28-29% across all models indicates hierarchy-reasoning ceiling; (3) Homogeneity >> Completeness indicates over-segmentation

### First 3 experiments
1. Run Bottom-Up mode with expert paper sets on your model to establish organization ceiling independent of retrieval
2. Ablate input granularity (Title+Abstract vs +Summary) to confirm richer inputs degrade expert alignment
3. Correlate your model's retrieval F1 with Sem-Path across topics to validate whether retrieval quality predicts structural quality for your architecture

## Open Questions the Paper Calls Out

### Open Question 1
How do open-source models perform relative to frontier closed-source models on TaxoBench? The authors plan to extend this benchmark to a broader spectrum of open-source models to quantify performance disparities.

### Open Question 2
What architectural or training interventions are required to overcome the shared 28–29% Sem-Path performance ceiling? The paper identifies a universal bottleneck where all evaluated models converge to this narrow band.

### Open Question 3
Why does providing richer input context (summaries, core tasks) degrade alignment with expert taxonomies? The authors observe this counter-intuitive "cognitive alignment gap" but don't isolate the specific mechanism.

## Limitations

- Benchmark relies on expert survey taxonomies as ground truth, potentially reflecting subjective conventions rather than objective scientific structures
- Embedding-based semantic metrics assume all-MiniLM-L6-v2 reliably captures domain-specific relationships
- Study doesn't address potential temporal drift in survey taxonomies from different years

## Confidence

### Major Uncertainties and Limitations
- High Confidence: Retrieval bottleneck finding (20.9% recall) is well-supported by direct measurements
- Medium Confidence: Hierarchy-aware metrics represent novel contributions but require broader validation
- Medium Confidence: Convergence to 28-29% Sem-Path suggests genuine ceiling but could reflect shared architectural limitations

## Next Checks

1. Test whether models perform better on recent versus older taxonomies to quantify potential temporal bias in expert categorizations
2. Compare semantic metric scores using domain-specific embeddings (e.g., SciBERT) versus general-purpose embeddings to assess metric reliability
3. Apply TaxoBench methodology to non-CS domains (e.g., biology, physics) to evaluate whether the retrieval bottleneck and hierarchy ceiling are universal phenomena