---
ver: rpa2
title: 'AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models
  with Preference Optimization'
arxiv_id: '2504.01735'
source_url: https://arxiv.org/abs/2504.01735
tags:
- adversarial
- image
- training
- conference
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AdPO, the first adversarial defense strategy
  for Large Vision-Language Models (LVLMs) based on preference optimization. The core
  idea is to frame adversarial training as a preference optimization problem, aiming
  to enhance the model's preference for generating normal outputs on clean inputs
  while rejecting misleading outputs for adversarial examples.
---

# AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization

## Quick Facts
- arXiv ID: 2504.01735
- Source URL: https://arxiv.org/abs/2504.01735
- Authors: Chaohu Liu; Tianyi Gui; Yu Liu; Linli Xu
- Reference count: 40
- Primary result: AdPO achieves 70.0% clean accuracy and 32.4% adversarial accuracy on VQAv2, outperforming state-of-the-art methods

## Executive Summary
This paper introduces AdPO, the first adversarial defense strategy for Large Vision-Language Models (LVLMs) based on preference optimization. The method reframes adversarial training as a preference optimization problem, simultaneously enhancing the model's preference for generating normal outputs on clean inputs while rejecting misleading outputs for adversarial examples. By solely modifying the image encoder and transferring robust features to larger models, AdPO achieves state-of-the-art adversarial robustness while maintaining clean performance across multiple datasets and attack types.

## Method Summary
AdPO implements preference optimization by training on ImageNet images (unsupervised) to generate preference pairs: interpretations from clean images (preferred samples) and adversarial images (non-preferred samples). The method uses 10-step PGD to generate adversarial examples against a frozen reference CLIP model, then applies DPO loss combined with adversarial image optimization loss to update only the CLIP ViT parameters. Training occurs on TinyLLaVA (OpenELM-450M-Instruct + CLIP ViT-L/14) for 2 epochs, then transfers the robust encoder to larger LVLMs like LLaVA-1.5-7B and OpenFlamingo-9B. The combined objective balances clean performance and adversarial robustness through a scaling factor λ.

## Key Results
- AdPO achieves 70.0% clean accuracy and 32.4% adversarial accuracy on VQAv2, outperforming state-of-the-art methods
- Small-to-large transfer strategy maintains competitive performance while reducing computational cost
- AdPO shows consistent improvements across VQAv2, COCO, and Flickr30k datasets
- The method demonstrates robustness against both targeted and untargeted attacks

## Why This Works (Mechanism)

### Mechanism 1: Preference Optimization as Adversarial Alignment
Framing adversarial training as preference optimization enables simultaneous optimization for clean performance and adversarial robustness. AdPO adapts DPO to an online setting where the model generates interpretations for clean images (preferred samples, y_w) and adversarial images (non-preferred samples, y_l). The objective increases log probability of correct outputs on clean inputs while decreasing probability of misleading outputs on adversarial inputs. The core assumption is that the model's own generated interpretations on clean images serve as reliable positive labels without external annotation.

### Mechanism 2: Adversarial Image Optimization Addresses Unconditional Preference
Adding explicit adversarial image optimization counteracts the "unconditional preference" issue where multimodal DPO neglects image conditions. Adversarial image optimization applies language modeling loss directly on adversarial images, forcing the model to produce correct outputs conditioned on adversarial visual inputs rather than relying on language-only shortcuts. The scaling factor λ balances clean vs. adversarial performance, with higher λ improving clean performance and lower λ enhancing adversarial robustness.

### Mechanism 3: Small-to-Large Transfer Reduces Overfitting Risk
Training on smaller LVLMs and transferring the robust image encoder to larger models achieves competitive robustness while reducing computational cost and evaluation overfitting. Only CLIP ViT parameters are modified during training on TinyLLaVA, and the robust encoder transfers directly to LLaVA-1.5-7B and OpenFlamingo-9B, which share the same image encoder architecture. This approach assumes robustness learned at the image encoder level generalizes across different LLM decoders without requiring decoder-specific fine-tuning.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: AdPO builds on DPO's mathematical framework for optimizing policy models using preference pairs without explicit reward models.
  - Quick check question: Can you explain why DPO eliminates the need for a separate reward model compared to RLHF?

- **Concept: Adversarial Training with PGD**
  - Why needed here: AdPO generates adversarial examples using Projected Gradient Descent (PGD) under ℓ∞ constraints to create negative training samples.
  - Quick check question: How does increasing perturbation radius ε during training affect the robustness-clean accuracy tradeoff?

- **Concept: CLIP Vision Encoder in LVLMs**
  - Why needed here: AdPO exclusively modifies CLIP ViT parameters, requiring understanding of how visual features are projected into LLM embedding space.
  - Quick check question: Why might modifying only the image encoder be sufficient for improving adversarial robustness across downstream LVLM tasks?

## Architecture Onboarding

- **Component map:** Image Encoder (CLIP ViT-L/14) -> Language Model (OpenELM-450M-Instruct) -> Projection Layer -> Reference Policy (frozen copy)
- **Critical path:**
  1. Load TinyLLaVA with CLIP ViT-L/14 + OpenELM-450M
  2. Generate adversarial images via PGD on ImageNet (no labels needed)
  3. Generate interpretations for clean (y_w) and adversarial (y_l) images using online prompting
  4. Compute L_P (DPO loss) and L_A (adversarial language modeling loss)
  5. Update only CLIP ViT parameters via L_AdPO = L_P + λL_A
  6. Transfer trained encoder to target LVLMs (LLaVA-1.5, OpenFlamingo)

- **Design tradeoffs:**
  - ε = 2/255 vs. 4/255: Higher ε improves robustness but reduces clean accuracy more
  - λ scaling: λ = 1 is default; increase for clean performance, decrease for adversarial robustness
  - DPO variants: IPO, KTO, StepDPO work; SimPO underperforms (no reference model)

- **Failure signatures:**
  - Hallucination on clean images (encoder over-regularized, reduce training epochs)
  - Near-zero adversarial robustness (λ too high or insufficient adversarial optimization)
  - Severe clean performance drop (perturbation radius ε too large for target deployment)
  - Poor transfer to target LVLM (encoder architecture mismatch)

- **First 3 experiments:**
  1. Reproduce AdPO2 on TinyLLaVA with ImageNet subset (500 images), validate clean/adversarial accuracy on VQAv2 subset before full training.
  2. Ablate λ ∈ {0.5, 1.0, 1.5} to confirm clean vs. adversarial tradeoff on single dataset.
  3. Transfer trained encoder to LLaVA-1.5-7B and evaluate on targeted attacks (Table 2 setup) to verify transferability claims.

## Open Questions the Paper Calls Out

- **Open Question 1:** How do more recent or sophisticated preference optimization algorithms perform within the AdPO framework compared to the standard DPO variants tested?
  - Basis in paper: The authors note "Given the rapid advancements in DPO, further research will be necessary in the future" to integrate more refined methods into adversarial defense.
  - Why unresolved: The paper evaluates only a limited subset of variants (IPO, KTO, StepDPO, SimPO), finding that SimPO performs relatively poorly, but does not test newer or more complex alignment algorithms.
  - What evidence would resolve it: A comprehensive benchmarking study of emerging preference optimization algorithms applied to the AdPO training pipeline.

- **Open Question 2:** Can an adaptive schedule for the scaling factor λ optimize the trade-off between clean accuracy and adversarial robustness more effectively than a static value?
  - Basis in paper: The ablation study explicitly shows a trade-off controlled by λ, yet the method defaults to a constant λ=1.
  - Why unresolved: A static hyperparameter may not optimally balance the conflicting gradients throughout the entire training trajectory.
  - What evidence would resolve it: Experiments using curriculum learning or dynamic weighting strategies for λ compared against the fixed baseline.

- **Open Question 3:** Does adversarial training on a proxy LVLM (TinyLLaVA) transfer effectively to target models with significantly different architectural inductive biases?
  - Basis in paper: The paper validates transfer to OpenFlamingo and LLaVA, but notes OF differs substantially from the TinyLLaVA training model.
  - Why unresolved: It is unclear if the robustness features learned via preference optimization on a small model generalize to architectures with different visual encoders or LLM decoders.
  - What evidence would resolve it: Evaluating the AdPO-tuned image encoder on a broader range of LVLM architectures and model sizes.

## Limitations

- The paper lacks empirical validation that generated interpretations on clean images are consistently reliable, potentially leading to noisy preference signals.
- The claim about "unconditional preference" problems in multimodal DPO is supported by related literature but not directly demonstrated in the adversarial context.
- The small-to-large transfer approach is promising but untested for architectural mismatches beyond ViT-based models.

## Confidence

- **AdPO achieves SOTA adversarial robustness:** High confidence - Supported by comprehensive comparisons across multiple datasets and attack types.
- **Preference optimization enables simultaneous clean/adversarial optimization:** Medium confidence - Theoretical framework is sound but lacks direct validation of preference signal quality.
- **Encoder-only modification is sufficient for robustness:** Medium confidence - Transfer experiments support this, but decoder-specific vulnerabilities remain untested.
- **Unconditional preference problem exists in adversarial training:** Low confidence - Inferred from multimodal DPO literature without direct adversarial-specific validation.

## Next Checks

1. **Interpretability Validation:** Implement automated hallucination detection on preference pairs during training. Measure the correlation between interpretation quality scores and final adversarial robustness to verify that preference signal quality drives performance.

2. **Controlled Unconditional Preference Test:** Design an ablation study where the model is trained with and without the adversarial image optimization term (λ = 0) on a dataset with clear visual cues. Measure the difference in reliance on visual versus language signals during attacks.

3. **Architectural Transfer Robustness:** Train AdPO on TinyLLaVA with a different image encoder (e.g., ConvNeXt) and attempt transfer to target models. Compare robustness transfer success rates across encoder architectures to quantify architectural generalization limits.