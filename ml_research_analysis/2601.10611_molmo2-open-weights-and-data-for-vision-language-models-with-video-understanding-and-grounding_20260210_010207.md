---
ver: rpa2
title: 'Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding
  and Grounding'
arxiv_id: '2601.10611'
source_url: https://arxiv.org/abs/2601.10611
tags:
- video
- data
- image
- training
- tracking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Molmo2 addresses the challenge of creating open, state-of-the-art
  vision-language models capable of video understanding and grounding, a capability
  lacking in current open-source models. The core method introduces a suite of nine
  novel datasets for video captioning, QA, pointing, and tracking, all collected without
  relying on closed models, alongside an efficient training recipe using sequence
  packing, message-tree encoding, bidirectional attention, and token-weighting strategies.
---

# Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding

## Quick Facts
- arXiv ID: 2601.10611
- Source URL: https://arxiv.org/abs/2601.10611
- Reference count: 40
- Primary result: Molmo2-8B achieves state-of-the-art performance among open-weight models on video understanding, counting, and captioning, with significant gains on video grounding tasks (35.5 vs 29.6 accuracy on video counting, 38.4 vs 20.0 F1 on video pointing)

## Executive Summary
Molmo2 introduces a suite of open-weight vision-language models capable of video understanding and grounding, addressing the gap where current open-source models struggle with video tasks and grounding. The approach combines nine novel datasets for video captioning, QA, pointing, and tracking with an efficient training recipe using sequence packing, message-tree encoding, bidirectional attention, and token-weighting strategies. Molmo2-8B achieves state-of-the-art performance among open-weight models on video understanding, counting, and captioning while significantly outperforming existing open models on video grounding tasks and matching proprietary models on several benchmarks.

## Method Summary
Molmo2 uses a three-stage training pipeline: (1) Pre-training on PixMo-Cap and PixMo-Points with 60% captions, 30% pointing, and 10% NLP for 32k steps; (2) Joint SFT for 30k steps using a mixture of pointing and non-pointing data with token-weighting (0.1 for captions, 0.2 for pointing, 4√n for others); (3) Long-context SFT for 2k steps with 384 frames and 36,864 tokens. The architecture features SigLIP 2 ViT as vision encoder, a learned attention-based pooling connector (2×2 for images, 3×3 for video frames), and Qwen3-4B/8B or OLMo3-7B LLM backbone. Key innovations include bidirectional visual attention, message-tree encoding for sequence packing, and pointing-based grounding representations.

## Key Results
- Molmo2-8B achieves state-of-the-art performance among open-weight models on video understanding, counting, and captioning benchmarks
- Significant improvements on video grounding tasks: 35.5 vs 29.6 accuracy on video counting (BURST-VC) and 38.4 vs 20.0 F1 on video pointing
- Matches or exceeds proprietary models on several benchmarks including MVBench and Molmo2-CapTest
- Demonstrates 15× training efficiency gain through sequence packing and custom attention masks

## Why This Works (Mechanism)

### Mechanism 1: Pointing-based intermediate representations improve counting accuracy
- Mechanism: Models first generate spatial-temporal points for each instance, then derive counts from point enumeration rather than direct count prediction
- Core assumption: Spatial localization is a learnable subskill that transfers to counting; models struggle with direct number prediction without explicit grounding
- Evidence: Point-then-count (61.5 accuracy) substantially outperforms direct count prediction (61.3) on BVC, with larger gains on MVC (34.5 vs 28.1)

### Mechanism 2: Bidirectional attention across vision tokens improves video QA and captioning
- Mechanism: Allowing vision tokens to attend both forward and backward (even across frames/images) enables temporal reasoning and richer feature aggregation
- Core assumption: Temporal relationships in video are bidirectional; earlier frames benefit from later context
- Evidence: Removing bidirectional attention drops QA average from 64.8 to 64.4 and caption F1 from 39.5 to 38.5

### Mechanism 3: Token-weighting prevents long-output tasks from dominating gradient updates
- Mechanism: Down-weighting loss from long-caption and pointing examples (0.1 and 0.2 respectively) balances learning from both short-answer MCQ and long generation tasks
- Core assumption: Unweighted cross-entropy over-represents high-token-count examples; task balance requires explicit intervention
- Evidence: Removing token-weighting drops QA from 64.8 to 64.0 (caption F1 slightly improves from 39.5 to 40.0)

## Foundational Learning

- Concept: Vision-Language Connector Design
  - Why needed: Molmo2 uses a learned attention-based pooling layer to compress ViT patch features into fewer tokens (2×2 for images, 3×3 for video frames)
  - Quick check: Can you explain why increasing video pooling from 3×3 to 4×4 degraded caption F1 from 39.5 to 37.0 but had minimal effect on QA?

- Concept: Sequence Packing with Custom Attention Masks
  - Why needed: Molmo2 packs multiple examples into single sequences using message-tree attention masks to prevent cross-contamination, achieving 15× training efficiency
  - Quick check: Given a packed sequence containing two video QA examples, draw the attention mask that prevents QA₁ from attending to QA₂'s tokens while allowing each QA to attend to its own video frames

- Concept: Point-based Grounding Representations
  - Why needed: Molmo2 represents grounding as compressed text strings with (timestamp, object_id, x, y) tuples rather than JSON or special tokens
  - Quick check: Why might a model trained on JSON-formatted points struggle with Molmo2's space-delimited format, and what would you need to modify to adapt it?

## Architecture Onboarding

- Component map: SigLIP 2 ViT -> Connector (multi-head attention pooling + MLP) -> Qwen3/OLMo3 LLM
- Critical path: Pre-training on PixMo-Cap/Points is essential for stable grounding; SFT mixture must include both pointing and non-pointing data; long-context SFT required for long-video performance but adds overhead
- Design tradeoffs:
  - 3×3 video pooling preserves detail for captioning (F1 39.5) vs 4×4 degradation to 37.0
  - 128 frames default vs 224 frames for long-video benchmarks (1.75× more vision tokens)
  - SlowFast encoding matches 224-frame performance with 43% fewer tokens but adds inference complexity
  - Pointing pre-training adds stability but requires separate data
- Failure signatures:
  - Degenerate point outputs: "A long line of points on one frame or the same point for every frame"
  - Repetitive caption text in greedy decoding on very long captions (>1000 tokens)
  - Identity swapping in tracking (HOTA metric captures association failures)
  - Count-query misalignment requiring upsampling for balanced learning
- First 3 experiments:
  1. Ablate pointing pre-training: Compare SFT convergence speed and final pointing accuracy vs full pipeline
  2. Test pooling size sensitivity: Train with 4×4 video pooling and evaluate on target task mix
  3. Validate token-weighting for your data mix: Compute effective loss weight per task before/after weighting

## Open Questions the Paper Calls Out

1. **Can a fully open-data vision encoder replace the closed-data SigLIP 2?**
   - The authors call for exploring open-data alternatives since SigLIP 2 relies on closed data
   - Unresolved: This dependency conflicts with "fully open" philosophy
   - Evidence needed: Molmo2 variant trained on strong open-data vision encoder with comparable benchmarks

2. **Does joint training on diverse tasks cause interference leading to degenerate outputs?**
   - The paper notes repeating point issues are less common in specialized models
   - Unresolved: Symptoms identified but root cause not isolated
   - Evidence needed: Ablation studies comparing grounding stability between jointly trained and isolated task models

3. **How can frame sampling strategies be optimized for grounding in videos longer than 3 minutes?**
   - Current pipelines struggle with context window constraints vs fixed temporal resolution of annotations
   - Unresolved: Need for adaptive sampling that prioritizes frames containing grounding annotations
   - Evidence needed: Adaptive sampling algorithms that prioritize annotated frames while downsampling unannotated segments

## Limitations
- Molmo2 datasets are not yet publicly released, preventing exact replication
- Pointing format (space-delimited text) may limit adoption by researchers with different representation preferences
- Long-context SFT slightly regresses short-video performance, creating a tradeoff between short and long video capabilities

## Confidence

**High Confidence**: Claims about Molmo2-8B achieving state-of-the-art performance among open-weight models on video understanding, counting, and captioning are well-supported by extensive benchmark results across multiple datasets.

**Medium Confidence**: Claims about effectiveness of specific architectural mechanisms (token-weighting, bidirectional attention, pointing-based counting) are supported by ablation studies but have limited prior literature in VLMs.

**Low Confidence**: Claims about necessity of three-stage training pipeline are supported by ablations but alternative strategies are not fully explored.

## Next Checks

1. Validate Pointing Format Transferability: Train a baseline model on the same data but using JSON-formatted grounding outputs instead of Molmo2's space-delimited format. Compare pointing accuracy and model convergence.

2. Test Token-Weighting Sensitivity: Create a synthetic dataset with controlled output length distributions and train with varying token-weighting schemes. Measure the impact on task balance and performance.

3. Evaluate Long-Context Necessity: Train models with only short-context SFT (128 frames) and long-context SFT (384 frames) separately, then evaluate on both short and long video benchmarks to quantify the exact tradeoff.