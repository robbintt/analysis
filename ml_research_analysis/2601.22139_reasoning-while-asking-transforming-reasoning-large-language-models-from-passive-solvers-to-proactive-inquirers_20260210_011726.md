---
ver: rpa2
title: 'Reasoning While Asking: Transforming Reasoning Large Language Models from
  Passive Solvers to Proactive Inquirers'
arxiv_id: '2601.22139'
source_url: https://arxiv.org/abs/2601.22139
tags:
- reasoning
- user
- interaction
- interactive
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Proactive Interactive Reasoning (PIR), a framework
  that transforms reasoning LLMs from passive solvers into proactive inquirers by
  enabling them to detect uncertainty and actively seek clarification from users.
  PIR employs uncertainty-aware supervised fine-tuning to train models when and how
  to ask questions, followed by a user-simulator-based reinforcement learning stage
  that aligns reasoning with user intent using a composite reward function.
---

# Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers

## Quick Facts
- arXiv ID: 2601.22139
- Source URL: https://arxiv.org/abs/2601.22139
- Reference count: 40
- Primary result: Up to 32.70% higher accuracy, 22.90% higher pass rate, and 41.36 BLEU improvement across math, code, and document editing tasks

## Executive Summary
This paper introduces Proactive Interactive Reasoning (PIR), a framework that transforms reasoning LLMs from passive solvers into proactive inquirers by enabling them to detect uncertainty and actively seek clarification from users. PIR employs uncertainty-aware supervised fine-tuning to train models when and how to ask questions, followed by a user-simulator-based reinforcement learning stage that aligns reasoning with user intent using a composite reward function. Extensive experiments show that PIR achieves significant improvements across mathematical reasoning, code generation, and document editing tasks, while reducing reasoning computation by nearly half and cutting unnecessary interaction turns by half.

## Method Summary
PIR transforms reasoning LLMs from passive solvers to proactive inquirers through a two-phase approach. First, it computes Predictive Entropy on reasoning steps and selects high-uncertainty points to insert clarification questions and simulated responses, creating interactive reasoning trajectories. Second, it uses a user simulator and Group Relative Policy Optimization (GRPO) with a composite reward balancing correctness, helpfulness, and efficiency. The method is trained on augmented datasets and optimized to minimize unnecessary interactions while maximizing task success.

## Key Results
- Achieves up to 32.70% higher accuracy across mathematical reasoning tasks
- Reduces reasoning computation by nearly half while maintaining or improving performance
- Cuts unnecessary interaction turns by half through balanced reward optimization

## Why This Works (Mechanism)

### Mechanism 1: Uncertainty-Aware Interactive Data Augmentation
- **Claim:** Converting high-uncertainty reasoning points into clarification opportunities creates a direct training signal for proactive inquiry.
- **Mechanism:** Compute Predictive Entropy (PE) at each reasoning step s_j by averaging token-level log probabilities. Select top-k% PE steps as clarification candidates. At these points, use an instruction-following LLM to insert a clarification question and simulated user response, transforming linear CoT into interleaved think-ask-respond trajectories y = {t_1, (a_1, r_1), ..., t_m, (a_m, r_m), O}.
- **Core assumption:** High internal uncertainty correlates with missing premises that user clarification can resolve.
- **Evidence anchors:**
  - [abstract]: "uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability"
  - [section 3.1.1]: "we leverage this observation to convert uncertainty into a direct training signal for interactive reasoning"
  - [corpus]: Limited direct evidence; LIMOPro addresses reasoning refinement but not interactive clarification
- **Break condition:** If PE has high false-positive rate on parametric knowledge tasks where interaction offers marginal utility, the training signal becomes noisy.

### Mechanism 2: User Simulator-based Policy Optimization (US-GRPO)
- **Claim:** A simulated interactive environment with composite rewards enables practical optimization of when and how to ask clarifying questions.
- **Mechanism:** Construct user simulator S conditioned on intent I. Policy π_θ generates reasoning steps t_n and clarification questions a_n; simulator produces responses r_n. Use GRPO to optimize without a value critic: sample G trajectories per query, compute group-relative advantage Â_i, update via clipped objective with KL regularization to reference π_ref.
- **Core assumption:** The simulator's response distribution approximates real users sufficiently for policy transfer.
- **Evidence anchors:**
  - [abstract]: "user-simulator-based policy optimization framework (US-GRPO) with a composite reward that aligns model behavior with user intent"
  - [section 3.2.1]: "optimizing such behavior directly with real users is impractical due to high cost, limited availability, and uncontrolled noise"
  - [corpus]: No direct corpus evidence for US-GRPO; Agentic RL survey discusses related paradigms but not this specific design
- **Break condition:** If simulator fails to capture linguistic noise and minority user patterns, learned policy exhibits distribution shift at deployment.

### Mechanism 3: Composite Reward Balancing Helpfulness and Efficiency
- **Claim:** Simultaneously incentivizing clarification quality while penalizing unnecessary interaction prevents both over-asking and premature commitment.
- **Mechanism:** R(y) = R_output(o,g) + R_reason(r,o,g). R_output = S_base · I(o=g). R_reason activates only when correct: I(o=g) · I_ask · [S_base · E(r) · H_LLM(r)], where E(r) = (N_max - n)/(N_max - 1) rewards fewer turns, and H_LLM(r) uses a judge model for helpfulness.
- **Core assumption:** The help-efficiency tradeoff can be characterized by a multiplicative reward without complex shaping.
- **Evidence anchors:**
  - [section 4.3, Figure 3]: "Using only the helpfulness reward yields higher helpfulness and accuracy but results in the highest TTR (2.63), indicating a tendency to over-ask"
  - [section 4.3, Figure 3]: "optimizing solely for efficiency minimizes interaction turns but severely degrades accuracy to 25.1"
  - [corpus]: No comparable systems with this specific composite reward design
- **Break condition:** If task domains have fundamentally different tradeoffs, fixed reward weights underperform without retuning.

## Foundational Learning

- **Concept: Predictive Entropy for Uncertainty Quantification**
  - Why needed here: Core signal for detecting when clarification is needed during reasoning.
  - Quick check question: How does step-level PE aggregation differ from sequence-level perplexity?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: Enables RL without training a separate value critic, reducing memory and compute.
  - Quick check question: Why does GRPO use group-relative advantage instead of absolute advantage estimates?

- **Concept: Reward Hacking in Multi-objective RL**
  - Why needed here: Understanding why composite reward design is critical to avoid degenerate strategies.
  - Quick check question: What failure mode emerges if efficiency reward dominates early in training?

## Architecture Onboarding

- **Component map:** Frozen teacher → PE calculation → Top-k% selection → Instruction LLM inserts Q&A → SFT on augmented trajectories → User simulator (Llama-3.1-8B-Instruct) ↔ Policy model generates reasoning + questions → Composite reward → GRPO update

- **Critical path:** PE threshold accuracy → Clarification point quality → Simulator response consistency → Reward signal reliability → Policy convergence

- **Design tradeoffs:**
  - Simulator quality (Llama vs GPT-4o-mini): Stronger simulator (+1.3 ACC) at 7× higher cost ($111.2 vs $15.8)
  - Max turns (N_max=5): More flexibility vs over-asking risk
  - PE threshold (top-k%): Conservative asking vs missing critical clarification points

- **Failure signatures:**
  - Over-asking: High TTR (~2.5+), reward variance increases (helpfulness-only training)
  - Premature truncation: Low TTR but ACC drops to ~25 (efficiency-only training)
  - Random asking: No PE-asking correlation (insufficient SFT data; template accuracy <0.5)
  - Simulator deadlock: Training stalls without failure protection responses

- **First 3 experiments:**
  1. Validate PE-asking correlation: Train SFT with varying dataset sizes (1k/2k/4k), measure template correctness and PE distribution shift in asking triggers
  2. Ablate reward components: Train US-GRPO with no reasoning reward / helpfulness-only / efficiency-only / full composite; compare ACC, TTR, and training stability
  3. Test simulator transfer gap: Train with weak simulator, evaluate with strong simulator; measure policy degradation on held-out intents

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PIR-enabled models change when deployed with real human users compared to the LLM-based user simulators used in training?
- Basis in paper: [explicit] The Limitations section states, "Our user simulator may not fully capture the linguistic noise and dynamic intent of real-world human interactions."
- Why unresolved: The entire training (US-GRPO) and evaluation pipeline relies on simulated environments (e.g., Llama-3.1-8B-Instruct) rather than human subjects, leaving the "sim-to-real" transfer gap unexplored.
- What evidence would resolve it: A human-subject study measuring task success rate and user satisfaction when the model interacts with real people versus the reported simulated metrics.

### Open Question 2
- Question: Does proactive inquiry capability introduce new safety risks, such as the generation of sensitive or manipulative clarification questions?
- Basis in paper: [explicit] The Limitations section explicitly notes a "Lack of Safety Alignment," stating the model "has not been screened for its handling of sensitive topics such as violence, sexual content, self-harm, or hate speech."
- Why unresolved: While the model is optimized for helpfulness and efficiency, it is unknown if the policy encourages probing for information in a way that violates safety guardrails.
- What evidence would resolve it: Evaluation of PIR models on safety benchmarks (e.g., red-teaming datasets) to check if the proactive questioning mechanism is susceptible to misuse or generates harmful queries.

### Open Question 3
- Question: What is the underlying mechanism that causes interactive reasoning training to improve performance on non-interactive, parametric knowledge benchmarks?
- Basis in paper: [inferred] The Generalization Evaluation shows PIR improves accuracy on MMLU and TriviaQA, leading the authors to suggest "proactive interactive reasoning is beneficial beyond interactive settings."
- Why unresolved: The paper observes this cross-domain improvement as a phenomenon but does not isolate whether the gain comes from better uncertainty calibration, improved internal verification, or simply data exposure during the US-GRPO phase.
- What evidence would resolve it: An ablation study analyzing the internal states of the model on static benchmarks to determine if the model simulates hypothetical queries or simply exhibits better calibrated confidence thresholds.

### Open Question 4
- Question: To what extent does the user simulator bias the model toward specific linguistic patterns or dialects, potentially marginalizing minority user groups?
- Basis in paper: [explicit] The Limitations section notes the simulator "likely biases towards majority interaction patterns, potentially failing to represent the diverse behaviors of minority user groups."
- Why unresolved: The US-GRPO framework relies on the simulator for reward signals; if the simulator fails to understand diverse dialects or interaction styles, the reinforcement learning loop may penalize valid but under-represented user behaviors.
- What evidence would resolve it: Testing the model's robustness across diverse dialects and interaction styles (e.g., different age groups or non-native speakers) to see if clarification performance degrades for minority patterns.

## Limitations
- User simulator may not capture linguistic noise and dynamic intent of real-world human interactions
- Lack of safety alignment screening for handling sensitive topics such as violence, sexual content, self-harm, or hate speech
- Simulator likely biases towards majority interaction patterns, potentially failing to represent diverse behaviors of minority user groups

## Confidence

- **High Confidence:** The two-phase architecture (SFT + US-GRPO) is well-specified and the composite reward mechanism demonstrably prevents degenerate over-asking or premature truncation
- **Medium Confidence:** The 32.70% accuracy improvement and 41.36 BLEU score gains are impressive but may reflect optimization to specific test suites rather than robust real-world performance
- **Low Confidence:** The assertion that PIR "generalizes to missing-premise and factual knowledge scenarios" is supported only by internal evaluation; external validation on diverse, naturalistic datasets is absent

## Next Checks

1. **Simulator Fidelity Test:** Evaluate PIR-trained models on human-human interaction data (e.g., Wizard-of-Oz datasets) to measure distribution shift between simulated and real user responses

2. **Cross-Domain Transfer:** Apply PIR to non-curriculum domains (e.g., medical diagnosis, legal reasoning) where ambiguity is less structured and user intent is genuinely unclear

3. **Uncertainty Metric Ablation:** Replace PE with alternative uncertainty measures (e.g., Monte Carlo dropout, evidential deep learning) to test whether the mechanism is specific to entropy or more general