---
ver: rpa2
title: 'You Prefer This One, I Prefer Yours: Using Reference Words is Harder Than
  Vocabulary Words for Humans and Multimodal Language Models'
arxiv_id: '2506.00065'
source_url: https://arxiv.org/abs/2506.00065
tags:
- task
- demonstrative
- possessive
- humans
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how humans and multimodal language models
  (MLMs) use reference words (possessives and demonstratives) compared to vocabulary
  words. Using a controlled multiple-choice task with three word classes of increasing
  cognitive demand (vocabulary < possessives < demonstratives), the research tests
  seven state-of-the-art MLMs against human participants.
---

# You Prefer This One, I Prefer Yours: Using Reference Words is Harder Than Vocabulary Words for Humans and Multimodal Language Models

## Quick Facts
- arXiv ID: 2506.00065
- Source URL: https://arxiv.org/abs/2506.00065
- Authors: Dota Tianai Dong; Yifan Luo; Po-Ya Angela Wang; Asli Ozyurek; Paula Rubio-Fernandez
- Reference count: 23
- Key outcome: Vocabulary word selection is easiest, demonstratives most challenging for both humans and MLMs

## Executive Summary
This study investigates how humans and multimodal language models (MLMs) handle reference words compared to vocabulary words. Using a controlled multiple-choice task with three word classes of increasing cognitive demand (vocabulary < possessives < demonstratives), the research tests seven state-of-the-art MLMs against human participants. Results show a clear difficulty hierarchy: vocabulary words are easiest, while demonstratives pose the greatest challenge. While MLMs approach human-level performance on vocabulary tasks, they show substantial deficits on possessives and even greater difficulties with demonstratives. Error analysis reveals that human errors often involve incorrect perspective-taking, while model errors primarily stem from difficulties with spatial reasoning and demonstrative use.

## Method Summary
The study employs a controlled multiple-choice task design where participants (both humans and MLMs) must select appropriate reference words from three options. The task uses three word classes arranged in increasing cognitive demand order: vocabulary words, possessive determiners, and demonstratives. Seven state-of-the-art multimodal language models are evaluated against human performance using identical stimuli. The experimental design systematically varies the linguistic complexity while controlling for other variables, allowing for direct comparison of performance across word classes and between humans and models.

## Key Results
- Clear difficulty hierarchy exists: vocabulary < possessives < demonstratives for both humans and MLMs
- MLMs approach human-level performance on vocabulary tasks but show substantial deficits on possessives
- MLMs demonstrate even greater difficulties with demonstratives compared to possessives
- Prompt engineering significantly improves model performance on possessive tasks but has limited effect on demonstratives
- Human errors primarily involve perspective-taking issues, while model errors stem from spatial reasoning difficulties

## Why This Works (Mechanism)
The study's controlled experimental design isolates the cognitive demands of different reference word types, allowing systematic comparison of human and machine performance. By using identical stimuli across word classes and participants, the research reveals how increasing pragmatic and spatial reasoning requirements affect both biological and artificial language processing systems.

## Foundational Learning
**Pragmatic inference** - Understanding speaker intentions and contextual cues beyond literal meaning
*Why needed*: Reference words depend heavily on shared context and speaker perspective
*Quick check*: Can the system resolve ambiguous references based on conversational context?

**Spatial reasoning** - Processing and interpreting spatial relationships and deixis
*Why needed*: Demonstratives require tracking relative positions and viewpoint-dependent reference
*Quick check*: Can the system correctly use "this" vs "that" based on spatial configurations?

**Perspective-taking** - Adopting the viewpoint of conversation participants
*Why needed*: Reference words often depend on whose perspective is being used
*Quick check*: Can the system maintain consistent reference across perspective shifts?

## Architecture Onboarding
**Component map**: Input Processing -> Context Integration -> Reference Resolution -> Output Generation
**Critical path**: Visual input → Spatial reasoning module → Perspective tracking → Demonstrative selection
**Design tradeoffs**: Accuracy vs computational efficiency in spatial reasoning modules
**Failure signatures**: 
- Vocabulary errors: semantic mismatches
- Possessive errors: perspective confusion
- Demonstrative errors: spatial reasoning failures

**First experiments**:
1. Test models on controlled spatial reasoning tasks with varying complexity
2. Evaluate perspective-taking abilities using viewpoint-switching scenarios
3. Assess the impact of visual context quality on reference word selection

## Open Questions the Paper Calls Out
None

## Limitations
- Controlled multiple-choice format may not capture natural reference word usage complexity
- Focus on English limits generalizability to languages with different demonstrative systems
- Artificial selection task differs substantially from natural language comprehension and production

## Confidence
High Confidence: The relative difficulty ranking (vocabulary < possessives < demonstratives) for both humans and MLMs is robust and well-supported by the data.
Medium Confidence: The error analysis distinguishing human perspective-taking errors from model spatial reasoning difficulties is suggestive but based on post-hoc interpretation.
Low Confidence: The explanation for why demonstratives are particularly challenging for MLMs remains speculative.

## Next Checks
1. Test the same reference word phenomena in a more naturalistic task, such as free-form language generation or comprehension tasks with visual contexts
2. Replicate the study with languages that have different demonstrative systems to assess whether difficulty patterns are universal
3. Conduct targeted experiments comparing transformer-based MLMs with alternative architectures to determine if demonstrative difficulties are inherent to current MLMs