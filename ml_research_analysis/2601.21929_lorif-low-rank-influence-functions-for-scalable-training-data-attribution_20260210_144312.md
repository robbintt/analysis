---
ver: rpa2
title: 'LoRIF: Low-Rank Influence Functions for Scalable Training Data Attribution'
arxiv_id: '2601.21929'
source_url: https://arxiv.org/abs/2601.21929
tags:
- lorif
- training
- logra
- influence
- gradients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the scalability challenge in training data\
  \ attribution (TDA) - identifying which training examples most influenced a model's\
  \ prediction. While gradient-based influence functions are theoretically sound,\
  \ they face two bottlenecks: storing projected per-example gradients (O(ND) space)\
  \ and forming the inverse Hessian approximation (O(D\xB2) memory), where N is the\
  \ number of training examples and D is the projection dimension."
---

# LoRIF: Low-Rank Influence Functions for Scalable Training Data Attribution

## Quick Facts
- arXiv ID: 2601.21929
- Source URL: https://arxiv.org/abs/2601.21929
- Reference count: 40
- Primary result: 2.3-20× storage reduction and 1.3-20× query-time speedup vs LoGRA while matching or exceeding attribution quality

## Executive Summary
This paper addresses the scalability challenge in training data attribution (TDA) - identifying which training examples most influenced a model's prediction. While gradient-based influence functions are theoretically sound, they face two bottlenecks: storing projected per-example gradients (O(ND) space) and forming the inverse Hessian approximation (O(D²) memory), where N is the number of training examples and D is the projection dimension. The authors introduce LoRIF (Low-Rank Influence Functions), which exploits the low-rank structure of neural network gradients to achieve 2.3-20× storage reduction and 1.3-20× query-time speedup while maintaining or improving attribution quality on models ranging from 124M to 70B parameters.

## Method Summary
LoRIF makes gradient-based TDA practical at frontier scale through two key approximations. First, it stores per-example projected gradients as rank-c factorizations rather than full matrices, reducing storage from O(D) to O(c√D) per layer per sample. Second, it uses truncated SVD with the Woodbury identity to approximate the Hessian term in an r-dimensional subspace, reducing memory from O(D²) to O(Dr). The method involves preprocessing: computing per-example gradients, applying two-sided random projections, and storing low-rank factors; curvature approximation: computing truncated SVD of the gradient matrix; and query: reconstructing gradients on-the-fly and computing influence scores. On models from 124M to 70B parameters, LoRIF achieves substantial resource savings while matching or exceeding attribution quality on LDS and tail-patch metrics.

## Key Results
- Achieves 2.3-20× storage reduction compared to LoGRA
- Achieves 1.3-20× query-time speedup compared to LoGRA
- Matches or exceeds attribution quality on LDS and tail-patch metrics
- Enables larger effective projection dimensions (up to 1.5×10⁵) within fixed storage budgets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-example projected gradients can be compressed via rank-c factorization with minimal attribution quality loss.
- Mechanism: For a layer with projected gradient matrix G̃ᵢ ∈ ℝ^{d₁×d₂}, store low-rank factors G̃ᵢ ≈ uᵢvᵢ^⊤ via block power iterations instead of the full matrix. This reduces storage from d₁d₂ to c(d₁+d₂) floats per layer per sample.
- Core assumption: Neural network gradients have low effective rank within layers—a property observed in gradient compression literature and LoRA-based fine-tuning.
- Evidence anchors: [abstract] "store rank-c factors of the projected per-example gradients rather than full matrices, reducing storage and query-time I/O from O(D) to O(c√D)"; [section 3.1] Figure 1 shows "Even the most aggressive compression (c = 1) yields meaningful attribution quality"

### Mechanism 2
- Claim: The inverse Hessian can be approximated via truncated SVD in an r-dimensional subspace while preserving attribution quality for r ≪ D.
- Mechanism: Compute truncated SVD G ≈ UᵣΣᵣVᵣ^⊤, then apply Woodbury identity: (VᵣΣ²ᵣVᵣ^⊤ + λI)^{-1} = (1/λ)I - (1/λ²)Vᵣ(Σ^{-2}ᵣ + (1/λ)Iᵣ)^{-1}Vᵣ^⊤. Store only Vᵣ ∈ ℝ^{D×r} and Σᵣ, reducing memory from O(D²) to O(Dr).
- Core assumption: The gradient Gram matrix G^⊤G has a "spiky" spectrum—few dominant eigenvalues with concentrated energy.
- Evidence anchors: [section 3.2] "This reduces the time complexity of approximating H^{-1} from O(ND² + D³) to O(NDc + NDr)"; [section C] Equations 12-13 show the curvature filter wᵢ = σ²ᵢ/(λ(λ+σ²ᵢ)) strongly suppresses directions with small singular values

### Mechanism 3
- Claim: The combination of both approximations enables larger effective projection dimensions D within fixed resource budgets, improving attribution quality.
- Mechanism: Storage savings from rank-c factorization and memory savings from truncated SVD create headroom to increase D (projection dimension) beyond LoGRA's feasible range (D ≈ 10⁵). Larger D preserves more gradient information, improving LDS/tail-patch scores.
- Core assumption: Attribution quality improves monotonically with D when curvature is properly handled.
- Evidence anchors: [section 4, Figure 4] "LoRIF achieves higher attribution quality than LoGRA when pushing the storage cost to the limit, with an effective projection dimension D of up to 1.5×10⁵"; [abstract] "2.3-20× storage reduction and 1.3-20× query-time speedup... while matching or exceeding attribution quality"

## Foundational Learning

- **Concept: Influence Functions**
  - Why needed here: LoRIF builds on the classical influence formulation I(x_tr, x_te) = g_te^⊤ H^{-1} g_tr. Understanding this helps see why the Hessian inverse is the computational bottleneck.
  - Quick check question: What does the Hessian matrix H represent in influence functions, and why does inverting it matter?

- **Concept: Gauss-Newton Hessian Approximation (H ≈ G^⊤G + λI)**
  - Why needed here: LoRIF uses the damped Gauss-Newton approximation to avoid computing the full Hessian. This connects the gradient matrix G directly to the curvature term.
  - Quick check question: Why is G^⊤G positive semi-definite, and what role does the damping term λI play?

- **Concept: Woodbury Matrix Identity**
  - Why needed here: Enables efficient inversion of (VΣ²V^⊤ + λI) by working in the low-rank subspace. The identity transforms an O(D³) operation into O(r³).
  - Quick check question: If A = UCU^⊤ + λI where U has orthonormal columns, how does Woodbury simplify computing A^{-1}?

## Architecture Onboarding

- **Component map**: Preprocessing (forward/backward passes → collect activations Xᵢ and output gradients δYᵢ → apply two-sided random projections (P_in, P_out) → compute rank-c factorization via power iterations → store uᵢ, vᵢ per example) → Curvature (reconstruct G batch-by-batch from low-rank factors → randomized truncated SVD → store Vᵣ, Σᵣ per layer) → Query (load query gradient factors → project via Vᵣ → compute influence via Eq. 9)

- **Critical path**: 1. Gradient computation over N training examples (dominates preprocessing: ~67-200 hours for large models) 2. I/O for loading stored factors at query time (was 96% of query latency in LoGRA; reduced ~40× by rank-1 factorization) 3. Memory for Vᵣ during query (O(Dr) per layer)

- **Design tradeoffs**: c vs D (paper recommends c=1 and maximizing D; quadrupling D yields larger gains than quadrupling c); r vs quality (Figure 2 shows diminishing returns; r ≈ D/16 works well empirically; too small r loses curvature, too large r defeats memory savings); f (projection factor) (smaller f → larger D → better quality but more computation; LoRIF uses smaller f than LoGRA for same storage budget)

- **Failure signatures**: Spectrum too flat (if EVR(r) grows slowly, truncation may lose important directions; monitor eigenvalue decay); Query gradient misalignment (if g_te concentrates energy in truncated directions, influence scores degrade; check ||Vᵣ^⊤ g_te|| / ||g_te||); Numerical instability (when σᵢ² ≈ λ, the term (Σ^{-2}ᵣ + (1/λ)Iᵣ)^{-1} can be sensitive; ensure λ is appropriately tuned)

- **First 3 experiments**: 1. Validate low-rank assumption on your model: Compute actual per-example gradient ranks across layers. Verify that rank-1 factorization preserves >80% of gradient Frobenius norm. 2. Ablate truncation rank r: Sweep r ∈ {D/64, D/32, D/16, D/8} on a validation set. Plot LDS/tail-patch vs r to find saturation point before quality degrades. 3. Storage-quality Pareto curve: Compare LoRIF vs LoGRA at matched storage budgets. Confirm LoRIF achieves better LDS by enabling larger D.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can LoRIF maintain its efficiency and attribution quality at true pre-training scale (trillion-token datasets, models beyond 70B parameters)? Basis: Section 5 states "For Olmo3-7B and Apertus-70B, we evaluate on their supervised fine-tuning stage rather than the full pre-training. While LoRIF's efficiency gains should transfer to larger-scale settings, empirical validation at true pre-training scale remains future work." Unresolved because pre-training datasets are orders of magnitude larger than SFT datasets, and the pre-training dynamics differ from fine-tuning.

- **Open Question 2**: Can per-example gradient computation costs be substantially reduced without sacrificing attribution quality? Basis: Section 5 states "computing per-example gradients is itself costly... developing more efficient methods is a promising direction for future research." Unresolved because modern LLMs train for only 1-2 epochs, so a single gradient pass over the training set can rival training cost.

- **Open Question 3**: Why does small truncation rank r preserve attribution quality despite only moderate spectral concentration in the gradient matrix? Basis: Appendix C shows EVR(r) indicates moderate concentration (not steep decay), yet Figure 2 shows LDS saturates at r << D. The paper offers a curvature-filtering explanation but notes both effects "compound" without fully disentangling their contributions.

- **Open Question 4**: Does the low-rank gradient structure exploited by LoRIF generalize to other modalities (vision, multimodal) and architectures beyond transformers? Basis: All experiments use decoder-only language models. Section 2.3 cites gradient compression literature primarily from vision models, suggesting the property may generalize, but no empirical validation is provided.

## Limitations

- All experiments use decoder-only language models; generalizability to other architectures remains untested
- Moderate absolute gains on tail-patch metrics (0.58 vs 0.51 for Olmo3-7B) despite statistical significance
- Low-rank gradient assumption lacks systematic analysis of failure conditions and quantitative thresholds

## Confidence

- **High Confidence**: Storage and memory complexity reductions (O(D) → O(c√D), O(D²) → O(Dr)) are mathematically proven via Woodbury identity and rank factorization
- **Medium Confidence**: Attribution quality maintenance is empirically supported across multiple model scales, but evidence relies on proprietary datasets and metrics
- **Low Confidence**: Generalizability to non-Transformer architectures and tasks beyond language modeling remains untested

## Next Checks

1. **Spectrum analysis validation**: Systematically measure eigenvalue decay rates across different model families and tasks to establish quantitative thresholds for safe r truncation

2. **Rank factorization limits**: Test LoRIF with c=1 across layers of varying widths and input distributions to identify when rank-1 factorization loses >10% of gradient energy

3. **Query gradient alignment**: Verify that query gradients align with retained SVD subspace by measuring ||Vᵣ^⊤ g_te|| / ||g_te|| across test examples and comparing to LDS/tail-patch degradation