---
ver: rpa2
title: 'The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon
  Structure for Shortcuts'
arxiv_id: '2510.20543'
source_url: https://arxiv.org/abs/2510.20543
tags:
- entity
- semantic
- complexity
- sentence
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CenterBench, a dataset of 9,720 comprehension
  questions on center-embedded sentences that vary in syntactic complexity and semantic
  plausibility. By comparing performance on matched plausible vs.
---

# The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts

## Quick Facts
- **arXiv ID**: 2510.20543
- **Source URL**: https://arxiv.org/abs/2510.20543
- **Reference count**: 40
- **Primary result**: CenterBench dataset reveals systematic performance degradation with syntactic complexity, with reasoning models improving absolute accuracy but failing to trace syntactic dependencies.

## Executive Summary
This paper introduces CenterBench, a dataset of 9,720 comprehension questions on center-embedded sentences that vary in syntactic complexity and semantic plausibility. By comparing performance on matched plausible vs. implausible sentences, the study quantifies when language models shift from structural analysis to semantic shortcuts. Results show consistent accuracy degradation with complexity and widening performance gaps (median 26.8 percentage points) between plausible and implausible sentences, indicating increased reliance on semantic associations. Reasoning models improve accuracy but still fail to trace syntactic dependencies, with traces revealing semantic shortcuts, overthinking, and answer refusal. Humans show inconsistent semantic effects, suggesting different processing strategies. CenterBench provides a systematic framework to identify when models abandon structural parsing for pattern matching.

## Method Summary
The study constructs CenterBench using a four-step generation process: (1) generating plausible sentences with GPT-4-0613 using noun lists and complexity templates, (2) creating implausible sentences via circular verb swapping with claude-sonnet-4-20250514, (3) generating questions and answers by mapping reversed verb order to subjects/objects, and (4) instantiating template questions from predefined lists. The dataset includes 360 sentences (30 plausible + 30 implausible per complexity level 1-6) with 6 question types per entity. Evaluation uses exact match → lemmatization → semantic similarity (MiniLM-L6-v2 at ≥0.9 threshold) on standard and reasoning model configurations (temp=0, 16K max tokens).

## Key Results
- Performance degrades consistently with syntactic complexity across all tested model families
- Semantic plausibility gaps widen systematically with complexity (median 26.8 percentage points for Claude models)
- Reasoning models show improved absolute accuracy but still exhibit semantic shortcut failures
- Chain consequence questions reveal semantic plausibility actively harms performance on causal reasoning
- Human semantic effects are inconsistent and non-monotonic across complexity levels

## Why This Works (Mechanism)

### Mechanism 1
Models substitute syntactic dependency tracking with semantic pattern matching as structural complexity exceeds processing capacity. As center-embedding depth increases (1-6 levels), the model's implicit working memory for subject-verb dependencies degrades. Instead of resolving long-range syntactic links (e.g., matching the outermost noun to the final verb), the model defaults to high-probability semantic associations derived from training data (e.g., "doctors prescribe" regardless of syntax). Performance gaps between syntactically identical plausible and implausible sentences are primarily driven by semantic interference rather than random noise or unrelated decoding errors.

### Mechanism 2
Reasoning models (CoT) improve absolute accuracy but introduce failure modes via "overthinking" and refusal to output implausible syntactic truths. Extended reasoning traces allow models to second-guess initial correct parses. When a syntactically correct derivation yields a semantically implausible result (e.g., "the mailman prescribed medicine"), the model generates reasoning steps that reject the structural answer in favor of world knowledge, or engages in circular doubt loops. The reasoning traces are causal drivers of the errors (refusals/changes) rather than post-hoc rationalizations of an already-decided answer.

### Mechanism 3
Semantic plausibility acts as a double-edged sword, aiding surface comprehension but actively impairing deep causal reasoning. For causal questions (e.g., "What led to X?"), models rely on plausible scripts (world knowledge) rather than strictly tracing the specific syntactic chain in the prompt. In plausible sentences, script and syntax align; in implausible sentences, the script misleads the model to a wrong answer, causing performance on "Chain Consequence" questions to drop below chance or flip patterns.

## Foundational Learning

- **Center-Embedding & Recursion**: Understanding that "The cat [that the dog [that the boy saw] chased] meowed" requires holding the subject "cat" in memory while processing "dog" and "boy" is essential to grasping why models fail. *Quick check: In "The rat the cat the dog chased ate died," who ate?*

- **Semantic Association vs. Syntactic Dependency**: The core distinction of the paper. You must differentiate between "what sounds right" (semantics) and "what is grammatically linked" (syntax) to interpret the gap results. *Quick check: Why is "The doctor delivered the mail" syntactically valid but semantically implausible?*

- **Chain-of-Thought (CoT) Interference**: Explains why "smarter" models (Reasoning models) fail in specific, weird ways (refusals/overthinking). It challenges the assumption that more reasoning always equals better structural compliance. *Quick check: If a model refuses to answer a valid math problem because the numbers seem "unrealistic," what failure mode is this?*

## Architecture Onboarding

- **Component map**: Generator (GPT-4/Claude) -> Validator (temporal logic/semantic constraints) -> Evaluator Pipeline (exact match → lemmatization → semantic similarity) -> Subject (Target LLMs)

- **Critical path**: The generation of implausible pairs using circular verb swapping. If this step creates grammatical errors or accidental plausibility, the "semantic shortcut" measurement is invalid.

- **Design tradeoffs**: Artificiality vs. Diagnosis (authors admit sentences are artificial but argue this is necessary to isolate syntax from training distribution); Evaluation Strictness (using semantic similarity risks false positives compared to exact string matching, though manual verification showed high accuracy).

- **Failure signatures**: Semantic Hacking (ignores prompt sentence and answers based on general knowledge); Refusal (outputs "No response" when implausible syntax contradicts safety filters); Token Spiral (reasoning models using 3000+ tokens to resolve simple questions).

- **First 3 experiments**: (1) Baseline Degradation - run standard LLM on levels 1-6 to establish "semantic gap" baseline; (2) Reasoning Stress Test - enable Thinking/Reasoning mode to measure if "semantic gap" shrinks or refusal rates spike; (3) Trace Analysis - manually inspect logs for "Chain Consequence" questions to confirm semantic shortcuts.

## Open Questions the Paper Calls Out

### Open Question 1
Do language models ever achieve genuine structural parsing of center-embedded sentences, or do semantic shortcuts remain the dominant strategy even with extended reasoning capabilities? The paper shows reasoning models improve accuracy but their traces still reveal semantic interference and overthinking. The matched-pair design can demonstrate when models abandon structure, but cannot prove models ever truly parse structure rather than employing increasingly clever approximations.

### Open Question 2
Why do humans show variable semantic effects across complexity levels while models show systematically widening plausibility advantages? The paper reports humans showing implausible advantages at level 3 but plausible advantages at levels 2 and 4, which remains unexplained given the limited n=24 human evaluation at levels 1-4 only.

### Open Question 3
What mechanisms cause semantic plausibility to harm performance on chain consequence questions while helping on other question types? The paper hypothesizes that semantic familiarity actively harms performance because models follow plausible associations to wrong answers rather than trace actual causal chains, but this mechanism remains speculative.

## Limitations

- Artificial sentence construction may not generalize to naturalistic text processing
- Performance gaps could partly reflect models' unfamiliarity with extreme implausibility
- Human evaluation was limited to n=24 participants at complexity levels 1-4 only
- Reasoning trace analysis cannot definitively prove causal failures vs. rational responses

## Confidence

- **High confidence**: Core finding that performance degrades systematically with complexity and that semantic gaps widen
- **Medium confidence**: Mechanism that models abandon structural analysis for semantic shortcuts
- **Medium confidence**: Characterization of reasoning model failures (overthinking, refusal)

## Next Checks

1. Test models on a parallel naturalistic dataset with implicit syntactic complexity to assess ecological validity
2. Conduct ablation studies varying the semantic implausibility threshold to determine whether the performance gap scales continuously
3. Compare human performance across consistent vs. inconsistent semantic conditions while controlling for response time