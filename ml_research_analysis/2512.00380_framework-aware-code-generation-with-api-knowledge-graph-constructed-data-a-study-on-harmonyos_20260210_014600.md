---
ver: rpa2
title: 'Framework-Aware Code Generation with API Knowledge Graph-Constructed Data:
  A Study on HarmonyOS'
arxiv_id: '2512.00380'
source_url: https://arxiv.org/abs/2512.00380
tags:
- code
- data
- generation
- knowledge
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of poor code generation performance
  in low-resource software frameworks, exemplified by HarmonyOS, where large language
  models lack sufficient API-specific knowledge during pre-training. The authors propose
  APIKG4SYN, a knowledge-graph-driven framework that synthesizes API-oriented training
  data by constructing an API knowledge graph from documentation and leveraging Monte
  Carlo Tree Search to generate diverse, realistic question-code pairs.
---

# Framework-Aware Code Generation with API Knowledge Graph-Constructed Data: A Study on HarmonyOS

## Quick Facts
- arXiv ID: 2512.00380
- Source URL: https://arxiv.org/abs/2512.00380
- Reference count: 40
- Primary result: APIKG4SYN improves HarmonyOS code generation pass@1 accuracy from 17.59% to 25.00%

## Executive Summary
This paper addresses the challenge of low-resource software frameworks, exemplified by HarmonyOS, where large language models lack sufficient API-specific knowledge during pre-training. The authors propose APIKG4SYN, a knowledge-graph-driven framework that synthesizes API-oriented training data by constructing an API knowledge graph from documentation and leveraging Monte Carlo Tree Search to generate diverse, realistic question-code pairs. Experimental results show that fine-tuning Qwen with APIKG4SYN data significantly improves HarmonyOS code generation performance, demonstrating the effectiveness of API-oriented data in enhancing LLM performance for low-resource software development scenarios.

## Method Summary
APIKG4SYN constructs an API knowledge graph from HarmonyOS documentation, capturing hierarchical and semantic relationships between APIs. It then uses Monte Carlo Tree Search guided by uncertainty estimation to identify high-uncertainty API paths, sampling 2-3 APIs per path for multi-API scenarios. DeepSeek-V3 generates questions and corresponding code, which are deduplicated using Levenshtein distance. The resulting synthetic dataset is used to fine-tune Qwen2.5-Coder-7B with LoRA via LLaMA-Factory, achieving improved code generation performance on the custom OHBen benchmark.

## Key Results
- Fine-tuned Qwen2.5-Coder-7B achieves 25.00% pass@1 accuracy on OHBen benchmark
- Outperforms baseline GPT-4o (17.59% pass@1) by 7.41 percentage points
- Demonstrates effectiveness of API-oriented synthetic data for low-resource frameworks

## Why This Works (Mechanism)
The approach works by addressing the knowledge gap in LLMs for low-resource frameworks. By constructing a knowledge graph from API documentation, the method captures the relationships and dependencies between APIs. The Monte Carlo Tree Search, guided by uncertainty estimation, identifies areas where the model is most uncertain, ensuring the synthetic data targets the most valuable learning opportunities. This focused approach allows the model to learn API-specific patterns and relationships that are critical for generating correct code in HarmonyOS.

## Foundational Learning
- **API Knowledge Graph Construction**: Building hierarchical and semantic relationships from documentation
  - Why needed: Captures API dependencies and relationships
  - Quick check: Verify extracted API signatures match documentation
- **Uncertainty Estimation (UE)**: Calculating -log₂P(v|u,ρ) to measure model uncertainty
  - Why needed: Guides MCTS to focus on unfamiliar APIs
  - Quick check: Compare UE scores across different API nodes
- **Monte Carlo Tree Search (MCTS)**: Using UE as reward to explore high-uncertainty API paths
  - Why needed: Efficiently samples valuable API combinations
  - Quick check: Validate MCTS selects diverse API paths
- **Synthetic Data Generation**: Using teacher LLM to create question-code pairs
  - Why needed: Creates training data for low-resource frameworks
  - Quick check: Verify generated code compiles and matches questions
- **LoRA Fine-tuning**: Parameter-efficient adaptation of pre-trained models
  - Why needed: Efficiently adapts models to new domain
  - Quick check: Monitor validation loss during fine-tuning
- **Pass@1 Evaluation**: Measuring single-attempt code generation success
  - Why needed: Standard metric for code generation evaluation
  - Quick check: Verify evaluation script correctly executes generated code

## Architecture Onboarding

**Component Map**: API Documentation -> Knowledge Graph -> MCTS Sampling -> Question-Code Generation -> Deduplication -> Fine-tuning -> Evaluation

**Critical Path**: The most critical sequence is API documentation parsing → knowledge graph construction → UE-guided MCTS sampling → synthetic data generation. Errors in any of these components will propagate through the pipeline and degrade final model performance.

**Design Tradeoffs**: The method trades computational complexity (MCTS simulations) for targeted data generation, versus uniform sampling which would be simpler but less efficient. The use of LoRA enables parameter-efficient fine-tuning but requires careful hyperparameter selection.

**Failure Signatures**: Common failures include generating syntactically valid but semantically incorrect code (wrong API usage), poor API coverage in training data leading to degraded performance on certain API combinations, and parsing errors in the knowledge graph construction phase.

**First Experiments**:
1. Parse a small subset of API documentation and manually verify the extracted knowledge graph structure
2. Run MCTS with simplified UE calculation on a small API graph to verify path selection logic
3. Generate 100 synthetic question-code pairs and manually inspect for correctness and diversity

## Open Questions the Paper Calls Out
- Can the APIKG4SYN framework be effectively generalized to other low-resource frameworks or Domain-Specific Languages (DSLs) beyond HarmonyOS?
- How does the performance of the fine-tuned student model depend on the specific Large Language Model (LLM) used as the teacher for data synthesis?
- Is the LLM's self-reported uncertainty a reliable proxy for identifying high-value API combinations in contexts where the model is completely hallucinating?

## Limitations
- Limited to a single framework (HarmonyOS), restricting generalizability claims
- Knowledge graph construction quality depends heavily on documentation parsing accuracy
- UE-based MCTS lacks empirical justification through ablation studies
- Model comparison limited to only GPT-4o as baseline

## Confidence
- **High Confidence**: The core claim that synthetically generated API-oriented data improves HarmonyOS code generation pass@1 accuracy from 17.59% to 25.00% is well-supported by reproducible experiments using the OHBen benchmark.
- **Medium Confidence**: The claim that UE-guided MCTS produces more effective training data than uniform sampling is plausible but not rigorously proven; the paper lacks ablation studies or comparison with simpler sampling strategies.
- **Low Confidence**: The assertion that APIKG4SYN generalizes to other low-resource frameworks is speculative, as only one framework was tested, and no cross-framework experiments were conducted.

## Next Checks
1. **Ablation of MCTS sampling**: Replace the UE-guided MCTS with uniform sampling over API paths and retrain the model; compare pass@1 accuracy to quantify the contribution of the uncertainty-driven selection strategy.
2. **Cross-framework evaluation**: Apply APIKG4SYN to another low-resource API (e.g., a niche robotics SDK or domain-specific framework) and measure whether the same synthetic-data approach yields comparable improvements in code generation accuracy.
3. **Knowledge graph parsing validation**: Manually audit a random sample of parsed API entries from the knowledge graph to measure precision and recall of extracted API signatures and relationships, and assess how parsing errors correlate with model performance degradation.