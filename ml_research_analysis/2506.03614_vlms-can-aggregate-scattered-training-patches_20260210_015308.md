---
ver: rpa2
title: VLMs Can Aggregate Scattered Training Patches
arxiv_id: '2506.03614'
source_url: https://arxiv.org/abs/2506.03614
tags:
- visual
- stitching
- vlms
- patches
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether vision-language models (VLMs) can
  reconstruct visual content from scattered patches, enabling potential adversarial
  attacks. The authors introduce "visual stitching," the ability of VLMs to integrate
  visual information spread across multiple training samples sharing the same textual
  descriptions.
---

# VLMs Can Aggregate Scattered Training Patches

## Quick Facts
- arXiv ID: 2506.03614
- Source URL: https://arxiv.org/abs/2506.03614
- Reference count: 40
- Key outcome: Vision-language models can reconstruct visual content from scattered patches, enabling potential adversarial attacks

## Executive Summary
This study investigates whether vision-language models (VLMs) can reconstruct visual content from scattered patches, introducing "visual stitching" as the ability to integrate visual information spread across multiple training samples sharing the same textual descriptions. The authors conduct experiments on three synthetic datasets where images are split into patches and paired with unique IDs, demonstrating that VLMs can effectively aggregate visual information even from small patches. The research reveals both the generalization strength of VLMs and potential safety concerns, particularly regarding moderation systems and harmful content detection.

## Method Summary
The authors created synthetic datasets of food, animal, and landmark images, splitting each image into patches and pairing them with unique identifiers. They fine-tuned VLMs on these patch-text pairs and evaluated stitching effectiveness through image-based and reference-based stitching tasks. Image-based stitching measured how well models could identify the original image from a set of candidates when given scattered patches, while reference-based stitching tested the ability to recognize patches belonging to the same image when given a reference patch. They also examined safety implications by testing whether small patches could evade moderation filters and whether fine-tuning on such patches could lead VLMs to generate misleading safety labels.

## Key Results
- VLMs demonstrated strong image-based stitching capabilities with low mean rank scores, even with small patch sizes
- Reference-based stitching showed non-trivial success but was less reliable than image-based stitching
- Small patches from harmful images often evaded moderation filters, and fine-tuning on these patches led VLMs to generate misleading "safe" or "unsafe" labels for dangerous content

## Why This Works (Mechanism)
The visual stitching capability emerges from VLMs' ability to learn associations between visual features and textual descriptions across multiple training instances. When patches share the same textual context, the model learns to associate them despite their spatial separation. This cross-instance learning enables the aggregation of scattered visual information, leveraging the model's generalization capabilities to fill in missing visual details based on shared semantic understanding.

## Foundational Learning
1. **Vision-Language Model Architecture** - Why needed: Understanding how VLMs process and integrate visual and textual information is crucial for analyzing stitching capabilities
   - Quick check: Can the model maintain consistent visual representations across different patch sizes and positions?

2. **Fine-tuning Procedures** - Why needed: The training methodology determines how effectively models learn to associate patches with shared textual contexts
   - Quick check: Does the learning rate and batch size affect stitching performance?

3. **Visual Feature Extraction** - Why needed: Understanding how visual features are encoded and compared is essential for analyzing stitching mechanisms
   - Quick check: Are visual features from different patches compatible for aggregation?

4. **Textual-Visual Alignment** - Why needed: The alignment between textual descriptions and visual patches drives the stitching capability
   - Quick check: Does changing textual descriptions affect stitching effectiveness?

## Architecture Onboarding

**Component Map**: Image Encoder -> Text Encoder -> Cross-Attention Layers -> Output Generator

**Critical Path**: Input patches → Visual feature extraction → Cross-modal alignment → Patch aggregation → Output generation

**Design Tradeoffs**: 
- Larger patches provide more visual information but reduce the number of training samples
- Smaller patches enable more training data but may lack sufficient visual context for effective stitching
- Balance between patch size and textual description specificity affects stitching performance

**Failure Signatures**: 
- High mean rank scores indicate poor image-based stitching
- Inconsistent outputs across similar patch-text pairs suggest unreliable stitching
- Failure to maintain visual consistency when aggregating patches from different sources

**First Experiments**:
1. Test stitching effectiveness with varying patch sizes (1x1, 2x2, 4x4) on the synthetic datasets
2. Evaluate the impact of different textual description granularities on stitching performance
3. Measure stitching capabilities when patches come from different images with similar textual descriptions

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Experimental scope limited to synthetic datasets rather than real-world scenarios
- Evaluation focuses primarily on stitching effectiveness metrics without extensive human evaluation
- Does not explore potential countermeasures or defensive mechanisms against stitching attacks
- Safety implications rely on simulated harmful content rather than actual dangerous material

## Confidence

**High confidence**: Core finding that VLMs can perform visual stitching across scattered patches when provided consistent textual context

**Medium confidence**: Practical implications for safety and moderation systems, as experimental conditions are somewhat idealized

**Low confidence**: Generalizability of results to real-world scenarios and other VLM architectures not tested in this study

## Next Checks
1. Test visual stitching capabilities on real-world image datasets with natural variations and inconsistent textual descriptions to assess practical attack feasibility
2. Evaluate the effectiveness of existing moderation and filtering systems against stitched content to quantify actual safety risks
3. Experiment with different VLM architectures, dataset sizes, and training durations to determine the robustness and limitations of visual stitching across various configurations