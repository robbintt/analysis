---
ver: rpa2
title: Adapting Diarization-Conditioned Whisper for End-to-End Multi-Talker Speech
  Recognition
arxiv_id: '2510.03723'
source_url: https://arxiv.org/abs/2510.03723
tags:
- speaker
- speech
- whisper
- dicow
- multi-talker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a speaker-attributed Whisper-based model that
  combines target-speaker modeling with serialized output training for multi-talker
  speech recognition. The approach uses a Diarization-Conditioned Whisper (DiCoW)
  encoder to extract speaker-specific embeddings, which are concatenated and passed
  to a shared decoder.
---

# Adapting Diarization-Conditioned Whisper for End-to-End Multi-Talker Speech Recognition

## Quick Facts
- arXiv ID: 2510.03723
- Source URL: https://arxiv.org/abs/2510.03723
- Authors: Martin Kocour; Martin Karafiat; Alexander Polok; Dominik Klement; Lukáš Burget; Jan Černocký
- Reference count: 0
- Primary result: DiCoW-based model achieves 17.2% cpWER on 3-speaker LibriMix

## Executive Summary
This paper introduces Diarization-Conditioned Whisper (DiCoW), a novel approach for end-to-end multi-talker speech recognition that combines target-speaker modeling with serialized output training. The method extracts speaker-specific embeddings from a DiCoW encoder, which are then concatenated and fed to a shared decoder for joint recognition of overlapping speech as a serialized output stream. The model demonstrates superior performance on synthetic multi-speaker datasets like LibriMix and shows competitive results on real-world recordings from AMI and NOTSOFAR.

## Method Summary
The proposed approach leverages a Diarization-Conditioned Whisper encoder to extract speaker-specific embeddings that are concatenated and passed to a shared decoder. This enables joint decoding of overlapping speech as a serialized output stream with speaker tags and timestamps. The method uses serialized output training (SOT) to handle multi-talker scenarios, with diarization information providing conditioning for speaker attribution. Experiments validate the approach on both synthetic mixtures and real-world multi-speaker recordings.

## Key Results
- Achieves 17.2% cpWER on 3-speaker LibriMix dataset
- Outperforms existing SOT-based approaches on synthetic mixtures
- Demonstrates competitive performance on real-world datasets (AMI, NOTSOFAR)
- Shows effective handling of overlapped speech through speaker-conditioned decoding

## Why This Works (Mechanism)
The method works by conditioning the Whisper-based encoder on diarization information to extract speaker-specific embeddings. These embeddings capture speaker characteristics that help the model distinguish between different speakers in overlapping speech. The serialized output training framework then allows the model to output a sequence containing both speaker tags and recognized text, enabling end-to-end multi-talker recognition. The shared decoder architecture with concatenated speaker embeddings provides an efficient way to handle multiple speakers without requiring separate decoders for each speaker.

## Foundational Learning

**Serialized Output Training (SOT)**: A training paradigm where the model learns to output sequences containing multiple speakers' transcriptions in a serialized format. Needed because traditional ASR assumes single-speaker input. Quick check: Verify the model can correctly order and attribute speech segments in overlapping scenarios.

**Diarization-Conditioned Embeddings**: Speaker-specific representations extracted from the encoder using diarization information. Required to provide the model with speaker identity cues for distinguishing overlapping voices. Quick check: Test embedding quality by clustering and verifying speaker separation.

**Multi-Talker Speech Recognition**: The task of recognizing speech when multiple speakers are talking simultaneously. Important because real-world conversations often involve overlapping speech. Quick check: Evaluate performance degradation as speaker count increases.

## Architecture Onboarding

**Component Map**: Audio Input -> DiCoW Encoder -> Speaker Embeddings -> Concatenation -> Shared Decoder -> Serialized Output (text + speaker tags + timestamps)

**Critical Path**: The audio signal flows through the DiCoW encoder, where diarization information conditions the extraction of speaker-specific embeddings. These embeddings are concatenated and passed to the shared decoder, which generates the final serialized output containing both transcriptions and speaker attribution information.

**Design Tradeoffs**: The approach trades computational efficiency (using a single shared decoder) against potential accuracy gains from having separate decoders per speaker. It also relies on diarization quality, which could be a bottleneck if diarization errors occur.

**Failure Signatures**: Performance degradation when diarization information is noisy or incorrect, confusion between speakers with similar voice characteristics, and increased error rates with more than 3 speakers due to the current architecture limitations.

**First Experiments**: 1) Test model performance with artificially corrupted diarization information to assess robustness, 2) Evaluate scaling to 4+ speakers to identify performance bottlenecks, 3) Compare against oracle diarization performance to establish upper bounds.

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Performance validation limited to 3-speaker scenarios, raising questions about scalability
- Reliance on oracle diarization information during training may not reflect real-world conditions
- Serialized output approach may face computational challenges with longer conversations or more speakers
- Limited evaluation on diverse acoustic conditions and domains

## Confidence

**Core technical claims (model architecture, training methodology)**: High
- Well-documented architecture and training procedures
- Clear experimental methodology on synthetic datasets

**Real-world dataset performance (AMI, NOTSOFAR)**: Medium
- Relatively small dataset sizes
- Potential domain mismatch between synthetic training and real recordings

**Scalability and generalization claims**: Low
- Limited exploration of performance with more than 3 speakers
- Insufficient testing across diverse acoustic conditions

## Next Checks
1. Test the model on datasets with 4+ speakers to evaluate scalability beyond the current 3-speaker limit
2. Conduct experiments with noisy or error-prone diarization inputs to assess robustness to real-world diarization conditions
3. Evaluate performance on diverse acoustic conditions and domains not represented in the current training setup to verify generalization capabilities