---
ver: rpa2
title: 'Providing Information About Implemented Algorithms Improves Program Comprehension:
  A Controlled Experiment'
arxiv_id: '2504.19225'
source_url: https://arxiv.org/abs/2504.19225
tags:
- participants
- algorithm
- labels
- comprehension
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether annotating source code with algorithm
  labels improves program comprehension. A controlled experiment with 56 participants
  compared a control group to an experimental group receiving algorithm labels.
---

# Providing Information About Implemented Algorithms Improves Program Comprehension: A Controlled Experiment

## Quick Facts
- **arXiv ID**: 2504.19225
- **Source URL**: https://arxiv.org/abs/2504.19225
- **Reference count**: 40
- **Primary result**: Algorithm labels significantly improve program comprehension without affecting completion times

## Executive Summary
This controlled experiment investigates whether annotating source code with algorithm labels enhances program comprehension. The study involved 56 participants divided into control and experimental groups, with the experimental group receiving algorithm labels. Results demonstrate a statistically significant improvement in comprehension scores for the experimental group (p=0.040) with a median improvement of 6 points (~23%), while completion times showed no significant difference (p=0.991). Qualitative feedback indicated participants found labels helpful for understanding code intent and enabling focused searches.

## Method Summary
The experiment employed a between-subjects design with 56 participants randomly assigned to control and experimental groups. Participants completed programming comprehension tasks involving code with and without algorithm labels. The experimental group received code annotated with algorithm labels while the control group worked with unlabeled code. Comprehension was measured through task completion and accuracy, with time tracking to assess efficiency impacts. Statistical analysis included Mann-Whitney U tests for significance testing and qualitative analysis of participant feedback.

## Key Results
- Algorithm labels significantly improved program comprehension scores (p=0.040, median improvement of 6 points/~23%)
- No significant difference in completion times between groups (p=0.991)
- Most participants found labels helpful for understanding code intent and focused searching
- Self-implemented algorithms were common due to missing library support or performance requirements

## Why This Works (Mechanism)
Algorithm labels provide cognitive scaffolding that helps developers quickly identify the purpose and structure of code segments. By explicitly naming the algorithms being implemented, labels reduce the mental effort required to reverse-engineer code functionality, allowing developers to focus on higher-level comprehension rather than decoding implementation details. This aligns with cognitive load theory, where reducing extraneous cognitive load improves overall comprehension and task performance.

## Foundational Learning
1. **Algorithm Recognition** - Understanding how to identify common algorithms in code
   - Why needed: Enables developers to quickly grasp code functionality
   - Quick check: Can identify sorting, searching, and graph algorithms in sample code

2. **Code Annotation Best Practices** - Understanding when and how to add explanatory labels
   - Why needed: Ensures labels add value without cluttering code
   - Quick check: Can distinguish between helpful and excessive annotations

3. **Comprehension Metrics** - Understanding how program comprehension is measured
   - Why needed: Enables proper evaluation of comprehension improvements
   - Quick check: Can explain difference between accuracy and completion time metrics

## Architecture Onboarding
**Component Map**: Source Code -> Algorithm Labels -> Comprehension Metrics -> Performance Measures

**Critical Path**: Code understanding requires identifying algorithm patterns, understanding their implementation, and grasping overall functionality.

**Design Tradeoffs**: Labels improve comprehension but add visual clutter; balance between helpfulness and code readability is crucial.

**Failure Signatures**: Ineffective labels are either too generic to be useful or too specific to limit understanding of alternative implementations.

**First Experiments**:
1. Test label effectiveness across different programming languages and paradigms
2. Measure long-term retention of algorithm understanding with labeled vs. unlabeled code
3. Evaluate label impact on debugging and maintenance tasks

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Sample size of 56 participants may not represent diverse programming backgrounds
- Laboratory setting may not reflect real-world development contexts
- Study focuses specifically on algorithm labels without exploring other annotation types
- Self-reported feedback may contain bias

## Confidence
- **High confidence**: Statistical significance of comprehension improvements (p=0.040)
- **High confidence**: Absence of time differences between groups (p=0.991)
- **Medium confidence**: Qualitative participant feedback and suggested use cases
- **Medium confidence**: Findings about self-implemented algorithms requiring external validation

## Next Checks
1. Replicate experiment with larger, more diverse participant pool including professional developers
2. Conduct field studies in real development environments with complex, multi-file projects
3. Investigate whether algorithm labels improve debugging efficiency, maintenance, and onboarding