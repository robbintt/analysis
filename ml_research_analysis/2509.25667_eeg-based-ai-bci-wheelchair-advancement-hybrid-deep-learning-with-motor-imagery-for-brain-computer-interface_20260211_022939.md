---
ver: rpa2
title: 'EEG-based AI-BCI Wheelchair Advancement: Hybrid Deep Learning with Motor Imagery
  for Brain Computer Interface'
arxiv_id: '2509.25667'
source_url: https://arxiv.org/abs/2509.25667
tags:
- wheelchair
- system
- control
- motor
- imagery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a hybrid deep learning model for EEG-based
  wheelchair control using motor imagery. The proposed BiLSTM-BiGRU architecture achieves
  92.26% test accuracy, outperforming XGBoost (86%), transformer-based models (87%),
  and EEGNet (64%) in classifying right and left-hand movement intentions from EEG
  signals.
---

# EEG-based AI-BCI Wheelchair Advancement: Hybrid Deep Learning with Motor Imagery for Brain Computer Interface

## Quick Facts
- arXiv ID: 2509.25667
- Source URL: https://arxiv.org/abs/2509.25667
- Authors: Bipul Thapa; Biplov Paneru; Bishwash Paneru; Khem Narayan Poudyal
- Reference count: 29
- This study presents a hybrid deep learning model for EEG-based wheelchair control using motor imagery, achieving 92.26% test accuracy

## Executive Summary
This study presents a hybrid deep learning model for EEG-based wheelchair control using motor imagery. The proposed BiLSTM-BiGRU architecture achieves 92.26% test accuracy, outperforming XGBoost (86%), transformer-based models (87%), and EEGNet (64%) in classifying right and left-hand movement intentions from EEG signals. Using a publicly available motor imagery dataset with 19 EEG channels at 200 Hz, the model effectively distinguishes between rest and motor imagery states. Cross-validation shows high stability (SD = 0.014), and the system includes a Tkinter-based GUI for real-time wheelchair simulation. The approach demonstrates strong potential for assistive mobility applications, offering a robust, high-accuracy framework for brain-computer interface (BCI) wheelchair control.

## Method Summary
The method uses a hybrid BiLSTM-BiGRU architecture to classify motor imagery EEG signals. The model processes 19-channel EEG data sampled at 200 Hz, extracting 4-second windows centered on motor imagery onset markers. These windows are flattened into feature vectors for classification into three classes: rest, right-hand movement, and left-hand movement. The architecture stacks BiLSTM and BiGRU layers with dropout and L2 regularization, trained with early stopping. The model is evaluated using 10-fold cross-validation and compared against XGBoost, transformer, and EEGNet baselines.

## Key Results
- BiLSTM-BiGRU model achieves 92.26% test accuracy on motor imagery classification
- 10-fold cross-validation shows high stability with mean accuracy of 90.13% and SD = 0.014
- Model outperforms XGBoost (86%), transformer (87%), and EEGNet (64%) baselines
- Tkinter GUI provides real-time wheelchair simulation interface

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The stacked BiLSTM-BiGRU architecture improves motor imagery classification by capturing bidirectional temporal dependencies in sequential EEG data.
- **Mechanism:** The BiLSTM layer processes input sequences in both forward and backward temporal directions, encoding past and future context. The BiGRU layer then refines these representations using fewer gating parameters than LSTM alone, improving computational efficiency while preserving temporal modeling capacity.
- **Core assumption:** Motor imagery EEG patterns contain temporal dependencies that benefit from bidirectional context, not just causal (forward-only) processing.
- **Evidence anchors:**
  - [abstract] "We propose a BiLSTM-BiGRU model that shows a superior test accuracy of 92.26% as compared with various machine learning baseline models"
  - [section 3.2] "By stacking BiLSTM and BiGRU layers, the model benefits from the rich memory capabilities of LSTM and the gating simplicity of GRU, making it particularly suited for sequential data where complex temporal dependencies exist"
  - [corpus] Limited direct comparison; neighbor papers explore transformer and geometric learning approaches but do not evaluate BiLSTM-BiGRU hybrids on motor imagery
- **Break condition:** If real-time inference latency constraints require strictly causal processing, the backward pass becomes infeasible and unidirectional models may be necessary.

### Mechanism 2
- **Claim:** ERP-aligned temporal windowing around motor imagery onset captures event-locked cortical activations that improve class separability.
- **Mechanism:** Segments of 4 seconds (±2 seconds around marker onset) are extracted per event, capturing both preparatory motor planning activity and post-onset neural responses. These windows are flattened into 19×200 feature vectors for classification.
- **Core assumption:** The motor planning phase and early execution phase contain discriminative neural signatures that align temporally with the marker onset.
- **Evidence anchors:**
  - [section 3.1.2] "Around each identified motor imagery onset (right or left hand), a segment of EEG data spanning 2 seconds before to 2 seconds after the event is extracted, yielding a 4-second window across 19 channels"
  - [section 3.1.2] "The dataset, finally, after combining Subject E's two different data forms, got 3807 rows and 3801 columns after feature extraction with ERP onsets help"
  - [corpus] Not explicitly validated in neighbor papers; ERP-based windowing is standard but optimal window size remains dataset-dependent
- **Break condition:** If motor imagery onsets are poorly synchronized with markers (e.g., due to variable user response times), fixed window alignment may introduce noise.

### Mechanism 3
- **Claim:** Regularization via dropout and L2 penalties combined with early stopping mitigates overfitting despite high model capacity.
- **Mechanism:** The BiLSTM-BiGRU model uses dropout rate of 0.2, L2 regularization (0.01) on recurrent layers, and early stopping with patience=10 to prevent overfitting on limited EEG samples.
- **Core assumption:** The dataset size (3807 samples) is insufficient to train a high-capacity recurrent model without explicit regularization.
- **Evidence anchors:**
  - [section 4.5] "The accuracy plots show rapid improvement within the first 20 epochs, after which the training accuracy saturates close to 100%, while the validation accuracy stabilizes around 89–90%, with minimal fluctuations"
  - [table 4] "BiLSTM units=128 (l2=0.01), BiGRU units=128 (l2=0.01), dropout rate=0.2...early stopping (patience=10)"
  - [corpus] Not explicitly compared in neighbor papers
- **Break condition:** If training data increases substantially (e.g., multi-subject datasets with >10K samples), regularization strength may need reduction to avoid underfitting.

## Foundational Learning

- **Concept: Motor Imagery EEG Signatures (ERD/ERS)**
  - Why needed here: Understanding that imagined hand movement produces contralateral event-related desynchronization (mu/beta band power decrease) is essential for interpreting why C3/C4 channels and temporal windowing matter.
  - Quick check question: Which scalp region would show stronger desynchronization during right-hand motor imagery—left motor cortex (C3) or right motor cortex (C4)?

- **Concept: Bidirectional Recurrent Networks**
  - Why needed here: The core architecture relies on BiLSTM and BiGRU layers; understanding how backward passes aggregate future context is necessary for debugging temporal modeling failures.
  - Quick check question: In a BiLSTM processing a 200-timestep sequence, what information does the backward hidden state at timestep 50 incorporate that a unidirectional LSTM would not?

- **Concept: Event-Related Potential (ERP) Segmentation**
  - Why needed here: The preprocessing pipeline centers windows on marker onsets; misalignment or incorrect marker interpretation will propagate errors through the entire system.
  - Quick check question: If the marker signal transitions from 0→1 at sample 1000, and you extract a ±2-second window at 200 Hz, what are the start and end sample indices for the segment?

## Architecture Onboarding

- **Component map:** Pre-filtered EEG (19 channels × 200 Hz) → ERP windowing (4-second segments) → Flattened vectors (3801 features) → BiLSTM(128) → BiGRU(128) → Dense output (3 classes) → TensorFlow Lite → Raspberry Pi → Motor driver → Wheelchair motors → Tkinter GUI simulation

- **Critical path:** Marker-aligned ERP extraction → BiLSTM-BiGRU temporal encoding → Real-time inference latency on Raspberry Pi

- **Design tradeoffs:**
  - Accuracy vs. latency: BiLSTM-BiGRU achieves 92.26% but adds bidirectional overhead; unidirectional GRU would be faster but may lose context
  - Channel count vs. usability: 19 channels provide spatial resolution but increase headset complexity; pruning (per corpus neighbor "PlugSelect") could reduce electrodes
  - Simulation vs. reality: GUI simulation does not capture motor driver latency, safety constraints, or real-world EEG artifacts

- **Failure signatures:**
  - High training accuracy (~100%) with validation plateau (~70%): Overfitting—increase dropout or reduce model capacity
  - Confusion between Class 1 and Class 2 (right vs. left): Inadequate spatial feature extraction—consider adding channel-wise attention or spatial filtering (CSP)
  - Class 0 (rest) dominates predictions: Class imbalance—reweight loss or oversample minority classes
  - Inference latency >500ms on Raspberry Pi: Model too large—quantize or reduce recurrent units

- **First 3 experiments:**
  1. **Baseline reproduction:** Train BiLSTM-BiGRU on the referenced CLA dataset (Subject E) with specified hyperparameters; verify 10-fold CV accuracy ≈90% and test accuracy ≈92%
  2. **Ablation study:** Compare BiLSTM-only, BiGRU-only, and BiLSTM-BiGRU configurations to isolate contribution of each component; track accuracy and training time
  3. **Subject generalization:** Train on Subject E, test on held-out subjects from the same dataset (if available) to assess cross-subject transfer; expect performance drop indicating user-specific calibration needs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a reliable "stop" command be integrated into the motor imagery control paradigm to enable complete wheelchair navigation?
- Basis in paper: [explicit] "The major issue is the control for stopping the wheelchair. This allows the wheelchair control to predict the live movement of a wheelchair in the required directions."
- Why unresolved: The current three-class model (rest, left-hand, right-hand) lacks an explicit stop state, making continuous movement control incomplete for real-world use.
- What evidence would resolve it: Implementation and validation of a multi-class model with dedicated stop imagery or a hybrid control scheme integrating stopping functionality.

### Open Question 2
- Question: Can the BiLSTM-BiGRU model maintain its reported 92.26% accuracy when tested on new subjects beyond the single subject (Subject E) used in this study?
- Basis in paper: [inferred] The dataset combines only Subject E's recordings across three sessions. Prior literature cited (Arshad et al., Zhang et al.) explicitly notes individual EEG signal variability affects classification performance.
- Why unresolved: Inter-subject variability in EEG patterns is well-documented, and models trained on single subjects often fail to generalize without transfer learning or calibration.
- What evidence would resolve it: Cross-subject validation results showing accuracy metrics when the model is applied to unseen subjects from the same dataset or new EEG acquisitions.

### Open Question 3
- Question: How does the BiLSTM-BiGRU model perform under real-time streaming EEG conditions with continuous, unsegmented data?
- Basis in paper: [inferred] The model was trained and tested on pre-segmented 4-second ERP windows aligned to motor imagery onset. The real-time simulation GUI uses a logistic regression model, not the proposed deep learning architecture.
- Why unresolved: Window-based offline classification does not account for continuous data streams, artifact contamination, or processing latency in live deployment.
- What evidence would resolve it: Real-time system benchmarks showing classification latency, throughput, and accuracy when processing live EEG streams without pre-segmentation.

### Open Question 4
- Question: What hardware optimization strategies are required to deploy the BiLSTM-BiGRU model on embedded systems like Raspberry Pi without significant accuracy degradation?
- Basis in paper: [explicit] "Selection to deployment requires more hardware usage focus. Our simulation is compatible with Raspberry Pi, but highly powerful compatible pocket-sized computers can be utilized."
- Why unresolved: The TensorFlow Lite deployment was only demonstrated with test arrays, not with live inference benchmarks or resource profiling on the target hardware.
- What evidence would resolve it: Performance metrics (inference time, memory usage, power consumption) and accuracy comparisons between the full model and quantized/optimized versions running on Raspberry Pi.

## Limitations

- Model tested only on single subject (Subject E) without cross-subject validation
- Fixed 4-second ERP windowing may not generalize to users with variable response times
- Implementation details for real-time deployment lack latency measurements and safety protocols

## Confidence

- **High**: Test accuracy superiority over baseline models (92.26% vs 86-87%)
- **Medium**: Generalization of BiLSTM-BiGRU architecture to other motor imagery tasks
- **Low**: Real-time performance claims without measured inference latency on target hardware

## Next Checks

1. **Cross-Subject Generalization**: Train the BiLSTM-BiGRU model on multiple subjects from the CLA dataset and evaluate per-subject performance to quantify user-specific variability.

2. **Real-Time Latency Measurement**: Deploy the trained model on actual Raspberry Pi hardware with live EEG input to measure end-to-end inference delay and verify it meets real-time control requirements (<500ms total latency).

3. **Channel Pruning Evaluation**: Apply the PlugSelect channel pruning methodology to the 19-channel configuration and compare accuracy/latency tradeoffs against the full electrode setup.