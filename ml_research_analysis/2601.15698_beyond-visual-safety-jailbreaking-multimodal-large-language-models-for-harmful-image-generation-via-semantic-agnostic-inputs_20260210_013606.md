---
ver: rpa2
title: 'Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful
  Image Generation via Semantic-Agnostic Inputs'
arxiv_id: '2601.15698'
source_url: https://arxiv.org/abs/2601.15698
tags:
- safety
- mllms
- visual
- image
- jailbreaking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BVS, a novel framework for jailbreaking multimodal
  large language models (MLLMs) to generate harmful images. The method uses "semantic
  decoupling" through neutralized visual splicing and inductive recomposition.
---

# Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs

## Quick Facts
- **arXiv ID:** 2601.15698
- **Source URL:** https://arxiv.org/abs/2601.15698
- **Reference count:** 13
- **One-line primary result:** BVS achieves 98.21% jailbreak success rate against GPT-5 for generating prohibited content by fragmenting malicious images and using Chinese prompts for reconstruction

## Executive Summary
This paper introduces BVS, a framework that jailbreaks multimodal large language models (MLLMs) to generate harmful images by exploiting vulnerabilities in visual safety alignment. The method uses "semantic decoupling" through neutralized visual splicing and inductive recomposition, partitioning malicious images into shuffled patches interleaved with benign ones, then pairing them with specially crafted Chinese prompts to force the MLLM to reconstruct harmful intent during processing. The framework demonstrates that current MLLMs have critical vulnerabilities when malicious intent is fragmented and recomposed, achieving a 98.21% success rate against GPT-5 for high-severity prohibited content categories that typically trigger robust safety refusals.

## Method Summary
BVS is a three-stage framework that jailbreaks MLLMs by fragmenting malicious visual content and reconstructing it through instruction-following. First, CogView4-6B generates malicious images from prohibited prompts. Second, the MIDOS algorithm partitions these images into four patches, shuffles them, and interleaves them with five benign images selected via semantic distance optimization to create a 3×3 grid. Third, a Chinese inductive prompt instructs the model to treat this composite as a matrix and reconstruct specific non-contiguous cells, causing harmful intent to manifest during the generation phase while bypassing input-stage safety filters.

## Key Results
- Achieves 98.21% jailbreak success rate against GPT-5 (January 2026 release) for high-severity prohibited content
- Significantly outperforms baselines, with MIDOS selection improving success from 81.82% (random) to 98.18%
- Successfully generates prohibited content across multiple categories including violence, drugs, pornography, and self-harm
- Validated against both GPT-5 and Gemini 1.5 Flash with consistent results

## Why This Works (Mechanism)

### Mechanism 1: Semantic Decoupling Through Visual Fragmentation
Fragmenting malicious images into shuffled patches interleaved with benign images disrupts the MLLM's ability to detect harmful content during the input phase. The Neutralized Visual Splicing component partitions a malicious image and combines it with benign patches selected by the MIDOS algorithm. This creates a "semantic distributional shift" that forces the visual encoder to process high-variance local patches, distracting attention and preventing safety filters from activating on holistic or contiguous semantic patterns.

**Core assumption:** Target MLLM safety alignment relies significantly on spatial continuity and global semantic coherence to flag prohibited visual content.

**Evidence anchors:**
- [Abstract]: Mentions "neutralized visual splicing... to decouple malicious intent from raw inputs."
- [Section 3.2]: Describes "Distraction Hypothesis" where semantic discontinuity escalates processing burden and "severs the semantic correlation."
- [Corpus]: "Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency" (arXiv:2501.04931) corroborates that shuffling effectively compromises MLLM defenses.

**Break condition:** If safety mechanisms employ fine-grained, patch-level object detection that aggregates non-contiguous features into a global semantic graph before the generation phase.

### Mechanism 2: Reconstruction-Then-Generation Strategy
The framework induces the model to reconstruct harmful intent internally during inference, bypassing input-stage safety checks. While the input image and text appear benign in isolation, the Chinese Inductive Prompt instructs the model to treat the visual input as a matrix and mentally reassemble specific non-contiguous cells into a coherent image. The harmful intent is latent during input screening but manifests only during the execution of the complex instruction.

**Core assumption:** The MLLM possesses strong instruction-following capabilities that allow internal reasoning processes to override or bypass initial semantic safety classifiers.

**Evidence anchors:**
- [Abstract]: Highlights the "reconstruction-then-generation strategy."
- [Section 3.3]: States that harmful intent "fully manifests within the model's internal latent space during the inference process."
- [Corpus]: "Contextual Image Attack" (arXiv:2512.02973) supports the premise that visual context can expose safety vulnerabilities by shifting how context is processed.

**Break condition:** If the model implements "process-monitoring" or intermediate-step safety validators that inspect the latent state after reconstruction but before final image diffusion/generation.

### Mechanism 3: High Information Density Prompt Optimization
The authors utilize Chinese for the inductive prompt, citing its higher information density compared to English. This allows complex matrix-mapping and role-playing instructions to be delivered concisely, preventing the context window from diluting the malicious instruction while efficiently guiding the model's attention to the specific patches containing harmful content.

**Core assumption:** Concise, high-density instructions are more effective at hijacking the model's generation pathway than verbose explanations, and multilingual processing does not inherently trigger heightened safety scrutiny.

**Evidence anchors:**
- [Section A.1]: Explicitly discusses "Semantic proficiency" and "Information density" as rationale for using Chinese prompts.
- [Corpus]: Weak direct evidence in provided corpus; primarily inferred from paper's specific linguistic analysis.

**Break condition:** If the target model applies strictly aligned safety filters specifically tuned to detect "matrix reassembly" or "image reconstruction" instructions regardless of language.

## Foundational Learning

- **Concept: Semantic Decoupling**
  - **Why needed here:** BVS relies on the premise that safety filters look for *holistic* harmful concepts. By breaking an image into parts, the "meaning" is lost until the model reassembles it.
  - **Quick check question:** If an image of a gun is cut into four quadrants and shuffled, does a standard classifier still detect a gun? Why or why not?

- **Concept: MLLM Attention Mechanisms**
  - **Why needed here:** The MIDOS algorithm exploits "attention distraction." Understanding how attention layers weigh different image regions (patches) explains why interleaving benign images works.
  - **Quick check question:** How does the "semantic distance" between two adjacent image patches affect the attention score in a Vision Transformer?

- **Concept: Safety Alignment vs. Instruction Following**
  - **Why needed here:** The attack pits the model's training to "be helpful/follow instructions" against its training to "be safe." The paper assumes the instruction-following capability (reconstructing the image) overpowers the safety refusal mechanism.
  - **Quick check question:** In the context of LLMs, what is the "alignment tax," and how might strong instruction tuning inadvertently lower safety barriers?

## Architecture Onboarding

- **Component map:** Visual Guidance Model (CogView4-6B) -> MIDOS Algorithm -> Neutralized Visual Splicer -> Inductive Prompt Module -> Target MLLM

- **Critical path:** The success of the jailbreak is most dependent on the **MIDOS selection logic**. If the selected benign images do not maximize the semantic distance effectively, the visual encoder may still detect the malicious patches. The move from 81.82% (random) to 98.18% (MIDOS) highlights this as the primary lever.

- **Design tradeoffs:**
  - **Dataset Size vs. Optimization Speed:** The paper uses a small Neutralized Image Data set (25 images). A larger set might find better "dilution" candidates but increases the search cost for MIDOS.
  - **Fragmentation vs. Comprehensibility:** You must fragment the image enough to bypass filters, but not so much that the MLLM fails to reconstruct the intended harmful semantic.

- **Failure signatures:**
  - **Refusal:** The model outputs a text refusal (e.g., "I cannot fulfill this request").
  - **Benign Generation:** The model generates an image based only on the neutral patches, ignoring the malicious matrix coordinates.
  - **Garbled Output:** The model attempts reconstruction but creates visual noise, failing to resolve the spatial dissonance.

- **First 3 experiments:**
  1. **Baseline Validation:** Verify that GPT-5/Gemini rejects the raw malicious prompt and the raw malicious image (IA) 100% of the time to ensure you are testing the jailbreak, not inherent model weakness.
  2. **MIDOS Ablation:** Replicate the comparison between Random Selection vs. MIDOS Selection. Specifically, measure the JSR (Jailbreak Success Rate) difference to validate the "semantic distance" hypothesis.
  3. **Diversity Stress Test:** Using a single successful IS, run the generation 5 times to confirm the paper's finding that the model generates diverse harmful outputs (Sec 4.5), verifying that the vulnerability lies in the semantic manifold, not a specific visual artifact.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can fine-grained, intent-aware guardrails be developed to detect "fragmented semantic attacks" without impairing the MLLM's ability to process complex, benign multi-part instructions?
- **Basis in paper:** [explicit] The conclusion explicitly states that current defense paradigms must evolve beyond holistic recognition and calls for the development of "intent-aware guardrails that can preemptively detect decoupled malicious signals."
- **Why unresolved:** The paper demonstrates that current safety alignment fails when malicious intent is decoupled, but does not propose a technical defense mechanism that balances safety with the utility of following complex spatial instructions (like the matrix reconstruction task).
- **What evidence would resolve it:** A new safety training methodology or inference-time filter that reduces the BVS success rate below 10% while maintaining performance on benign semantic reconstruction tasks.

### Open Question 2
- **Question:** To what extent does the high information density of the Chinese language contribute to the success of the "Inductive Recomposition," and can similar efficacy be achieved with lower-density languages?
- **Basis in paper:** [inferred] The paper explicitly notes that Chinese was chosen for its "high information density" and "expressive power" compared to English, but does not isolate language choice as an experimental variable.
- **Why unresolved:** It remains unclear if the vulnerability lies in the structural complexity of the Chinese prompts or solely in the visual fragmentation mechanism (MIDOS).
- **What evidence would resolve it:** Ablation studies comparing jailbreak success rates (JSR) using the same BVS visual inputs paired with semantically equivalent prompts in English, Chinese, and low-resource languages.

### Open Question 3
- **Question:** Do "sophisticated semantic dilution schemes" such as cross-modal interference or stylistic blending expose deeper vulnerabilities in the generative manifold than the spatial shuffling used in MIDOS?
- **Basis in paper:** [explicit] Section 5.2 suggests expanding the framework beyond simple image concatenation to test "sophisticated semantic dilution schemes—such as cross-modal interference or subtle stylistic blending."
- **Why unresolved:** The current study focuses on spatial shuffling (neutralized visual splicing), leaving the efficacy of frequency-domain perturbations or style-transfer-based obfuscation unexplored.
- **What evidence would resolve it:** Experimental results showing attack success rates for BVS variants utilizing non-spatial dilution methods against the same safety-aligned models (GPT-5, Gemini).

## Limitations

- The framework's effectiveness assumes current MLLMs primarily employ holistic or global semantic analysis for visual safety alignment, which may not hold for future models with enhanced patch-level detection.
- Results are demonstrated against specific model versions (GPT-5 from January 2026, Gemini 1.5 Flash) and may not generalize to other MLLMs or future iterations with improved safety mechanisms.
- The selection of Chinese as the high-density instruction language is based on linguistic analysis but lacks direct empirical validation across multiple languages to confirm it's the critical factor.

## Confidence

- **High Confidence:** The experimental methodology and evaluation framework are well-defined, with clear success metrics (JSR) and validation through automated judges. The 98.21% success rate against GPT-5 and significant improvement over random selection baselines are directly measurable outcomes.
- **Medium Confidence:** The semantic decoupling mechanism and semantic distance optimization through MIDOS are theoretically sound, but the exact feature extraction method for Dse computation is unspecified, creating uncertainty about reproducibility. The effectiveness of Chinese prompts relies on information density claims that, while linguistically reasonable, lack direct empirical validation across multiple languages.
- **Low Confidence:** The assumption that instruction-following capabilities will consistently override safety mechanisms may not hold for all MLLMs, particularly those with enhanced process-monitoring or intermediate-step safety validators. The framework's success against Gemini 1.5 Flash is less extensively validated than GPT-5 results.

## Next Checks

1. **Feature Extraction Verification:** Implement multiple feature extractors (CLIP, DINOv2, supervised classifiers) for Dse computation in the MIDOS algorithm and measure how feature choice affects semantic distance optimization and resulting JSR. Compare against the paper's unspecified feature extractor to identify sensitivity to this critical component.

2. **Multilingual Prompt Robustness:** Replicate the jailbreak using identical structural prompts in English, Chinese, and other high-density languages (Japanese, Korean). Measure JSR differences to validate whether the attack's effectiveness is genuinely tied to information density or other linguistic factors, and whether safety mechanisms respond differently to multilingual prompts.

3. **Process-Monitoring Resistance Test:** Modify target MLLMs to implement intermediate safety checks that inspect latent states after the reconstruction phase but before final generation. Test BVS against these enhanced models to determine whether the "reconstruction-then-generation" strategy can bypass process-level safety validators, validating the core assumption about instruction-following overriding safety.