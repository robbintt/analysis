---
ver: rpa2
title: Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control
arxiv_id: '2508.20784'
source_url: https://arxiv.org/abs/2508.20784
tags:
- control
- demand
- time
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the bus bunching problem in urban transit by
  proposing a single-agent reinforcement learning framework that outperforms traditional
  multi-agent methods. The key innovation is encoding categorical identifiers (vehicle
  ID, station ID, time period) alongside numerical features (headway, occupancy, velocity)
  into the state representation, enabling the single agent to generalize across heterogeneous
  buses and time contexts.
---

# Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control

## Quick Facts
- arXiv ID: 2508.20784
- Source URL: https://arxiv.org/abs/2508.20784
- Authors: Yifan Zhang
- Reference count: 27
- One-line primary result: A single-agent RL framework with categorical state encoding and ridge-shaped rewards outperforms multi-agent methods in bus fleet control under stochastic demand and traffic.

## Executive Summary
This paper addresses bus bunching in urban transit by proposing a single-agent reinforcement learning framework that encodes categorical identifiers (vehicle ID, station ID, time period, direction) alongside numerical features. The approach uses a modified SAC algorithm with a ridge-shaped reward function to balance headway regularity and schedule adherence. Tested on a realistic bidirectional, timetabled bus simulation with dynamic demand and stochastic traffic, the method achieves significantly higher and more stable rewards compared to MADDPG baselines, demonstrating that single-agent RL with categorical structuring can effectively manage bus holding in complex, real-world settings.

## Method Summary
The framework encodes discrete identifiers through learnable embeddings (dimension dj = min(50, nj/2) for nj unique values) concatenated with numerical features (headway, occupancy, velocity). The reward function combines a soft penalty on headway deviation, a symmetry bonus for balanced headways, and hard penalties for extreme outliers. SAC's entropy-regularized objective provides implicit robustness to stochastic dynamics. The policy network outputs Gaussian distributions over holding actions, trained on a global replay buffer with transitions assembled asynchronously.

## Key Results
- Single-agent SAC with categorical embeddings achieves significantly higher rewards (-430k vs. -530k) than MADDPG under stochastic conditions
- The approach shows more stable performance with narrower standard deviation bands across evaluation episodes
- Embedding capacity scales appropriately with categorical cardinality (dj = min(50, nj/2)), enabling generalization across heterogeneous buses and time contexts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Encoding categorical identifiers enables a single agent to capture inter-agent dependencies that typically require explicit multi-agent architectures.
- **Mechanism:** Categorical variables pass through learnable embedding layers, producing dense vectors concatenated with numerical features. This high-dimensional representation allows the policy to distinguish context-specific dynamics without separate policy parameters per agent.
- **Core assumption:** Embedding dimension is sufficient to capture meaningful semantic similarities across spatio-temporal contexts.
- **Evidence anchors:** Abstract and Section 4.2 describe embedding matrices and concatenation; corpus support is weak for this specific mechanism in transit control.
- **Break condition:** If unique categorical combinations exceed embedding capacity, generalization may degrade.

### Mechanism 2
- **Claim:** A ridge-shaped reward function structuring incentives along both headway regularity and schedule adherence produces more stable policy convergence.
- **Mechanism:** The reward combines soft penalties on headway deviation, symmetry bonuses for balanced headways, and hard penalties for extreme outliers, creating a ridge along target headways that encourages uniform spacing and timetable fidelity.
- **Core assumption:** Operational objectives are best served by balancing headway uniformity and schedule adherence rather than prioritizing either exclusively.
- **Evidence anchors:** Abstract and Section 4.1 provide the full reward formulation; no direct corpus validation for ridge-shaped rewards in transit.
- **Break condition:** If timetable design fundamentally conflicts with demand patterns, the reward ridge may provide contradictory gradient signals.

### Mechanism 3
- **Claim:** Maximum entropy reinforcement learning (SAC) provides implicit robustness to transition dynamics perturbations, reducing variance under stochastic demand and traffic conditions.
- **Mechanism:** SAC's entropy-regularized objective encourages stochasticity during training, which hedges against bounded perturbations in dynamics. Early training emphasizes exploration to handle uncertainty.
- **Core assumption:** Stochasticity in bus operations falls within bounded uncertainty sets that entropy regularization can hedge against.
- **Evidence anchors:** Section 4.4 cites Eysenbach et al. (Theorem 4.2) and shows SAC's narrower standard deviation bands; corpus paper on robust single-agent RL for traffic signals applies SAC to similar problems.
- **Break condition:** If operational uncertainty is primarily aleatoric rather than epistemic, entropy regularization provides limited additional robustness.

## Foundational Learning

- **Concept: Soft Actor-Critic (SAC)**
  - **Why needed here:** SAC is the core algorithm, chosen for its entropy-regularized objective and off-policy efficiency in stochastic, event-driven environments.
  - **Quick check question:** Can you explain why SAC's automatic temperature tuning (α as a learned parameter) matters for environments where the optimal exploration-exploitation balance shifts during training?

- **Concept: Entity Embeddings for Categorical Variables**
  - **Why needed here:** The paper's key innovation is representing discrete identifiers as dense learnable vectors rather than one-hot encodings.
  - **Quick check question:** Given a fleet of 50 buses and 22 stations, what is the dimensionality advantage of embedding (dim ≈ 25 per categorical) versus one-hot (72 dimensions total for those two variables)?

- **Concept: Bus Bunching Dynamics**
  - **Why needed here:** Understanding why bunching emerges is essential for interpreting the reward design and action timing.
  - **Quick check question:** Why does the paper argue bunching tends to occur in pre-peak transitional periods rather than at peak demand itself?

## Architecture Onboarding

- **Component map:** Environment (discrete-event simulator) -> State encoder (embedding layers + concatenation) -> Policy network (4-layer MLP) -> Action (holding time) -> Q-networks (twin critics) -> Replay buffer (global, asynchronous assembly)

- **Critical path:**
  1. Ensure embedding dimensions scale with categorical cardinality (dj = min(50, nj/2))
  2. Verify reward function implementation matches the ridge formulation exactly
  3. Confirm asynchronous transition assembly: (s, a, r) stored at yi,j; s' appended at yi,j+1 before buffer insertion
  4. Monitor temperature α during training; premature collapse indicates potential robustness loss

- **Design tradeoffs:**
  - Single-agent vs. MARL: Simplifies training and addresses data imbalance, but requires sufficient categorical embedding capacity
  - Station holding vs. speed control: Holding is operationally feasible; speed control offers finer granularity but faces safety constraints
  - Batch size 2048 (SAC) vs. 128 (MADDPG): SAC's global buffer permits larger batches; effective per-bus samples remain comparable

- **Failure signatures:**
  - High reward variance across evaluation episodes: May indicate insufficient exploration or embedding under-capacity
  - Policy ignores time_period or direction: Check embedding learning rates; categorical features may be drowned out
  - Bunching events persist near terminals: Reward shaping may overweight schedule adherence vs. headway symmetry
  - α collapses to near-zero early: Reduce target entropy or increase learning rate for temperature update

- **First 3 experiments:**
  1. Ablate categorical embeddings: Replace with one-hot encodings or remove specific identifiers to measure contribution
  2. Vary reward ridge steepness: Adjust symmetry penalty coefficient and outlier penalty to test sensitivity
  3. Stress test under demand perturbation: Scale passenger OD matrix by ±30% and measure policy robustness

## Open Questions the Paper Calls Out
- Can the single-agent framework effectively manage operations under non-stationary demand distributions where arrival rates drift over time?
- How does the framework perform when extended to hybrid control strategies combining station holding with inter-station speed guidance?
- Can the learned policy transfer effectively to upstream scheduling modules to optimize dispatch times rather than just reacting to them?

## Limitations
- Lacks ablation studies on embedding dimensions and categorical identifier importance
- No comparison against MARL variants that incorporate categorical embeddings
- Real-world deployment challenges (driver compliance, passenger communication, safety constraints) are not addressed

## Confidence
- High confidence in SAC's robustness to stochastic dynamics based on theoretical grounding and variance reduction
- Medium confidence in categorical embedding mechanism's ability to generalize across buses
- Low confidence in the ridge-shaped reward being superior to alternatives, as no comparative experiments are presented

## Next Checks
1. Perform ablation studies removing specific categorical identifiers to quantify their contribution to performance
2. Compare against MARL baselines using identical categorical embeddings to isolate single-agent advantages
3. Test the reward function against exponential penalty formulations on synthetic headway scenarios