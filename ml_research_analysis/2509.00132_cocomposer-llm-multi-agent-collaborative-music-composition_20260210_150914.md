---
ver: rpa2
title: 'CoComposer: LLM Multi-agent Collaborative Music Composition'
arxiv_id: '2509.00132'
source_url: https://arxiv.org/abs/2509.00132
tags:
- agent
- music
- melody
- composition
- cocomposer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CoComposer, a multi-agent system for AI
  music composition using large language models (LLMs). The system consists of five
  specialized agents that follow traditional music composition workflows: Leader,
  Melody, Accompaniment, Revision, and Review agents.'
---

# CoComposer: LLM Multi-agent Collaborative Music Composition

## Quick Facts
- arXiv ID: 2509.00132
- Source URL: https://arxiv.org/abs/2509.00132
- Authors: Peiwen Xing; Aske Plaat; Niki van Stein
- Reference count: 34
- Key outcome: Multi-agent system achieves 100% generation success rate with better interpretability/editability than MusicLM

## Executive Summary
CoComposer introduces a five-agent LLM system for polyphonic music composition using ABC notation. The agents collaborate through structured workflows to generate interpretable, editable musical pieces. When compared to both single-agent and multi-agent baselines, CoComposer demonstrates superior production complexity and maintains advantages in interpretability over dedicated music generation models like MusicLM, though it lags in core aesthetic metrics.

## Method Summary
CoComposer employs five specialized agents (Leader, Melody, Accompaniment, Revision, Review) orchestrated through AutoGen framework. The system follows traditional music composition workflows, with the Leader decomposing high-level user intent into structured sub-tasks. Agents communicate via sequential group chats, generating ABC notation that is converted to audio via MIDI backend. The two-phase workflow includes initialization creation followed by iterative refinement based on review feedback.

## Key Results
- 100% generation success rate across all tests
- Outperformed existing multi-agent systems in subjective aesthetic experience and creative complexity
- Showed significant advantages over single-agent systems in production complexity
- Better interpretability and editability compared to MusicLM, though MusicLM produced superior music in core aesthetic dimensions

## Why This Works (Mechanism)

### Mechanism 1: Specialized Role Decomposition via Hierarchical Task Allocation
Distributing composition tasks across specialized agents reduces cognitive load on the LLM compared to single-prompt generation, yielding higher production complexity. The Leader Agent parses high-level user intent into structured sub-tasks, allowing Melody and Accompaniment agents to operate within narrower context windows optimized for specific creative outputs.

### Mechanism 2: Iterative Constraint Enforcement via Dedicated Review Loop
A dedicated feedback loop with distinct "Revision" (syntax/rhythm) and "Review" (aesthetic/theory) roles improves adherence to music theory and notation standards. The Revision Agent applies a "minimum intervention principle" to fix timing and format errors in ABC notation, while the Review Agent critiques musicality.

### Mechanism 3: Symbolic Representation (ABC Notation) as an Interoperability Layer
Using ABC notation as intermediate output format facilitates high editability and interpretability, functioning as a "semantic bridge" between natural language and audio. Instead of generating raw audio, the system generates text-based ABC notation, allowing the Revision Agent to operate on precise symbolic logic rather than probabilistic audio features.

## Foundational Learning

- **Concept: ABC Notation**
  - Why needed here: This is the "source code" of the music generated by CoComposer. Understanding the syntax is required to debug why the Revision Agent might flag a "timing error."
  - Quick check question: Can you identify the key signature and time signature in the string `M:4/4 K:C`?

- **Concept: Multi-Agent Orchestration (AutoGen)**
  - Why needed here: CoComposer is built on AutoGen. You must understand how "Group Chats" and speaker selection work to debug why the Leader might fail to hand off control to the Review Agent.
  - Quick check question: In a sequential chat, what happens if Agent A never explicitly addresses Agent B in its response?

- **Concept: Subjective vs. Objective Evaluation in Generative AI**
  - Why needed here: The paper uses AudioBox-Aesthetics (subjective metrics like "enjoyment") vs. success rates. Distinguishing between "production complexity" and "content enjoyment" is vital for interpreting results.
  - Quick check question: Why might a piece of music have high "Production Complexity" but low "Content Enjoyment"?

## Architecture Onboarding

- **Component map:** User Prompt -> Leader Agent (Task Decomposition) -> Melody Agent + Accompaniment Agent (Drafting) -> Revision Agent (Syntax/Timing Fixes) -> Review Agent (Aesthetic Critique) -> Leader Agent (Decides on iteration or termination)

- **Critical path:** 1) User Prompt -> Leader Agent (Task Decomposition), 2) Tasks -> Melody Agent + Accompaniment Agent (Drafting), 3) Drafts -> Revision Agent (Syntax/Timing Fixes), 4) Fixed Draft -> Review Agent (Aesthetic Critique), 5) Critique -> Leader Agent (Decides on iteration or termination)

- **Design tradeoffs:** The system sacrifices raw audio fidelity of models like MusicLM to gain granular control afforded by symbolic ABC notation. Reduced from 6 (ComposerX) to 5 agents to reduce communication overhead but concentrates more responsibility into Melody/Accompaniment agents.

- **Failure signatures:** Infinite Loop (Review Agent repeatedly suggests changes), Hallucinated Syntax (LLM produces invalid ABC), Role Drift (Accompaniment Agent generates melody lines).

- **First 3 experiments:** 1) Reproduce "Single-Agent" Baseline using combined GPT-4o prompt to verify lower "Production Complexity" scores, 2) Stress Test the Revision Agent with complex time signatures (7/8 or mixed meters), 3) LLM Backend Swap replacing GPT-4o with Llama-3-70B to observe if "Review" and "Revision" logic holds up.

## Open Questions the Paper Calls Out

- **Open Question 1:** Would integrating a memory mechanism to record user preference patterns across multiple creation sessions significantly improve personalization in CoComposer's music generation?
- **Open Question 2:** Can a specialized feedback analysis agent reliably transform fragmented, non-technical user feedback into structured, actionable creative instructions?
- **Open Question 3:** What architectural improvements would enable CoComposer to close the aesthetic quality gap with dedicated music generation models like MusicLM?
- **Open Question 4:** How can multi-agent LLM systems overcome the limitation of text-based sequential output for accurately aligning multiple concurrent musical voices in polyphonic composition?

## Limitations

- Evaluation relies entirely on subjective aesthetic metrics from AudioBox-Aesthetics without objective measures of musical correctness
- System's dependence on ABC notation creates constraints limiting nuanced expressive elements
- Limited testing with only 20 prompts raises questions about generalizability across diverse musical styles
- No discussion of computational efficiency or generation speed for practical deployment

## Confidence

**High Confidence:**
- Multi-agent architecture successfully generates complete musical pieces in ABC notation with 100% success rate
- Outperforms single-agent systems in production complexity metrics
- Demonstrates better interpretability and editability compared to MusicLM

**Medium Confidence:**
- Shows superior aesthetic experience and creative complexity compared to existing multi-agent systems
- Specialized role decomposition improves music generation quality compared to monolithic approaches
- Iterative constraint enforcement mechanism effectively improves adherence to music theory

**Low Confidence:**
- Claims about superiority over MusicLM in core aesthetic dimensions (MusicLM actually performed better in PQ and CE)
- Generalizability of results across diverse musical genres and styles
- Long-term stability and consistency of the multi-agent system

## Next Checks

1. **Objective Musical Quality Assessment**: Conduct expert music theory analysis comparing CoComposer outputs against human-composed benchmarks and MusicLM outputs using objective criteria like harmonic progression correctness, voice leading, and rhythmic consistency.

2. **Cross-Model Robustness Testing**: Replace GPT-4o with smaller open-source models (Llama-3-70B, DeepSeek-V3) and evaluate whether the multi-agent framework maintains quality across different LLM capabilities.

3. **Extended Evaluation Corpus**: Expand testing beyond the current 20 prompts to include 100+ diverse musical styles and complexity levels, including edge cases like complex time signatures, non-Western scales, and highly experimental compositions to assess system limitations.