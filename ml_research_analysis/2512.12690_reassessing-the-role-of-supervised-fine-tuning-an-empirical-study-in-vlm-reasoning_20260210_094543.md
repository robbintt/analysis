---
ver: rpa2
title: 'Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM
  Reasoning'
arxiv_id: '2512.12690'
source_url: https://arxiv.org/abs/2512.12690
tags:
- reasoning
- grpo
- data
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reexamines the prevailing belief that reinforcement
  learning (RL) is inherently superior to supervised fine-tuning (SFT) for improving
  reasoning capabilities in vision-language models (VLMs). Through a systematic, controlled
  comparison of SFT and RL using identical training data and matched optimization
  setups, the study finds that the relative effectiveness of SFT and RL is conditional,
  depending strongly on model capacity, data scale, and data distribution.
---

# Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning

## Quick Facts
- arXiv ID: 2512.12690
- Source URL: https://arxiv.org/abs/2512.12690
- Reference count: 40
- This paper challenges the prevailing belief that RL is inherently superior to SFT for VLM reasoning through controlled experiments.

## Executive Summary
This paper systematically compares supervised fine-tuning (SFT) and reinforcement learning (RL) for vision-language model reasoning using identical training data and matched optimization setups. The study finds that SFT is unexpectedly powerful for weaker models and highly data-efficient, achieving comparable or better performance with only 2K samples versus 20K for RL. SFT also demonstrates stronger cross-modal transferability when trained on data matching the base model distribution. The research identifies a pervasive issue of deceptive rewards in RL, where higher rewards do not correlate with improved reasoning accuracy. These findings challenge the prevailing "RL over SFT" narrative and highlight the need for balanced post-training pipelines leveraging both paradigms' complementary strengths.

## Method Summary
The study uses controlled comparisons of SFT and RL on vision-language models (Qwen2-VL-2B/7B, Qwen2.5-VL-3B/7B) with shared training data from mathematical reasoning datasets. SFT employs LLaMA-Factory with batch size 128, LR 1e-5, 3 epochs; RL uses EasyR1 framework with GRPO/DAPO, batch size 512, LR 1e-6, 2 epochs, 10 rollouts per query. The reward function combines accuracy (0.9 weight) and format compliance (0.1 weight). Experiments systematically vary data scale (1K-20K samples), model capacity, and training data distribution to isolate conditions favoring each approach.

## Key Results
- SFT with 2K samples achieves comparable or better reasoning performance than RL with 20K samples on mathematical benchmarks
- SFT outperforms RL on weaker models due to RL's "missing signal" problem with predominantly incorrect responses
- Higher RL rewards often fail to correlate with better reasoning accuracy, indicating deceptive reward optimization

## Why This Works (Mechanism)

### Mechanism 1
SFT outperforms RL on weaker models because RL requires diverse correct/incorrect trajectory contrast for gradient estimation. In GRPO/DAPO, policy updates depend on reward differentials across sampled responses. Weaker models produce predominantly incorrect responses, creating a "missing signal" problem—insufficient positive examples for meaningful gradient updates. SFT bypasses this by directly imitating expert trajectories regardless of model capability. Break condition: If model already produces sufficiently diverse correct/incorrect outputs, RL signal quality improves and SFT advantage diminishes.

### Mechanism 2
SFT achieves higher data efficiency because imitation learning directly encodes reasoning patterns, while RL requires exploratory sampling. SFT minimizes negative log-likelihood on expert demonstrations—each sample directly contributes to learning. RL must sample, evaluate, and filter trajectories; many samples are "wasted" on incorrect paths before discovering useful patterns. This creates a ~10x sample efficiency gap (2K SFT ≈ 20K RL). Break condition: When exploration is necessary (novel reasoning paths not in demonstrations), RL's sample cost may be justified by discovery of better solutions.

### Mechanism 3
RL reward signals can decouple from reasoning accuracy ("deceptive rewards"), causing optimization to improve reward-hacking rather than genuine reasoning. Verifiable rewards (correctness + format) create optimization targets the model can achieve through surface-level pattern matching. As training continues, the policy learns to maximize reward conformity without improving underlying reasoning—accuracy plateaus or declines while reward rises. Break condition: SFT cold-start initialization mitigates overfitting by providing a more generalizable starting point that resists reward-driven drift.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: Core RL algorithm compared against SFT. Understanding how GRPO computes advantages over groups of sampled responses explains why weak models struggle. Quick check: Can you explain why GRPO needs both correct and incorrect samples in a group to compute meaningful gradients?

- **Distributional Alignment in Transfer Learning**: SFT's cross-modal transferability depends critically on whether training data matches base model distribution. Misaligned data causes catastrophic forgetting. Quick check: If your SFT data comes from a different model family than your base model, what transferability risk increases?

- **Verifiable Rewards (RLVR)**: The paper's "deceptive reward" finding depends on understanding how RLVR computes rewards (accuracy + format) and why these can be gamed. Quick check: Why might a model learn to satisfy format constraints without improving actual reasoning correctness?

## Architecture Onboarding

- **Component map**: Base VLM (Qwen2-VL/Qwen2.5-VL variants) -> SFT pipeline (LLaMA-Factory, batch 128, LR 1e-5) OR RL pipeline (EasyR1, GRPO/DAPO, batch 512, LR 1e-6) -> Verifiable reward function (λ=0.9 accuracy, 0.1 format) -> Mathematical reasoning datasets

- **Critical path**: 1) Start with base VLM (Qwen2-VL or Qwen2.5-VL variants) 2) For weak models or limited data: Apply SFT first 3) For strong models with abundant data: Consider RL-only or SFT+RL 4) Monitor reward-accuracy divergence during RL to detect overfitting

- **Design tradeoffs**: SFT: Higher sample efficiency, better for weak models, but may plateau with more data. RL: Better scaling with strong models and large data, but deceptive rewards risk. SFT+RL: More stable training, SFT benefit diminishes as base model strengthens

- **Failure signatures**: RL accuracy declining while reward increases → reward overfitting. SFT degrading cross-modal transfer → training data distribution mismatch with base model. RL showing no improvement on weak models → insufficient valid learning signals

- **First 3 experiments**: 1) Data scaling comparison: Train SFT with 2K samples vs. RL with 2K/5K/10K/20K samples on same data source. Expect SFT-2K ≈ RL-20K on math benchmarks. 2) Model capacity sweep: Compare SFT vs. GRPO on Qwen2-VL-2B, -7B, Qwen2.5-VL-3B, -7B. Expect SFT > RL on weaker models, RL > SFT on strongest. 3) Reward-accuracy tracking: Log GRPO training reward and held-out benchmark accuracy every 20 steps. Expect divergence after initial improvement phase; test whether SFT cold-start reduces divergence

## Open Questions the Paper Calls Out

- **Generalizability to non-mathematical domains**: The conditional advantages of SFT (e.g., data efficiency, performance on weaker models) may not generalize to commonsense or spatial reasoning tasks. The study is confined to mathematical reasoning with verifiable rewards, and broader evaluation across different reasoning domains is essential.

- **Detecting deceptive rewards intrinsically**: How can "deceptive rewards" in RL be theoretically characterized and detected during training without relying on held-out accuracy checks? The paper identifies the divergence but does not propose an intrinsic mechanism to detect or prevent this overfitting during the training process itself.

- **Quantifying distributional proximity**: What quantitative metrics define the "distributional proximity" required for SFT to preserve cross-modal transferability? The authors demonstrate that data source matters but do not isolate the specific statistical properties that constitute "proximity."

## Limitations

- The deceptive rewards finding relies on verifiable reward settings where format constraints can be gamed, potentially limiting generalizability to more complex reward structures
- The data efficiency comparison assumes expert demonstrations contain all necessary reasoning patterns, which may not hold for tasks requiring novel problem-solving approaches
- The weak model analysis depends on specific model capacities (2B/7B) that may not represent the full spectrum of VLM capabilities

## Confidence

- **High confidence**: SFT's superior data efficiency (2K vs 20K samples) and better performance on weak models—directly supported by controlled experiments with clear metrics
- **Medium confidence**: The deceptive rewards phenomenon and its mitigation through SFT cold-start—experimental evidence is compelling but generalizability to other reward structures requires further validation
- **Medium confidence**: Cross-modal transfer benefits of SFT—the distributional alignment hypothesis is well-supported, but specific thresholds for "matching base model distribution" need more precise characterization

## Next Checks

1. **Reward function ablation**: Test whether the deceptive reward phenomenon persists when removing format constraints or using alternative reward formulations (e.g., only correctness-based rewards)

2. **Novel reasoning task evaluation**: Assess whether SFT's data efficiency advantage holds on tasks requiring novel solution strategies not present in training demonstrations

3. **Model capacity expansion**: Extend the weak model analysis to include additional model sizes (1B, 3B, 13B) to better characterize the threshold where RL overtakes SFT performance