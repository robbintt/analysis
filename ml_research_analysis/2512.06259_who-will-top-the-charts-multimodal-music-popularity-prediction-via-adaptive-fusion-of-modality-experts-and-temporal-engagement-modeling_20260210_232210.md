---
ver: rpa2
title: Who Will Top the Charts? Multimodal Music Popularity Prediction via Adaptive
  Fusion of Modality Experts and Temporal Engagement Modeling
arxiv_id: '2512.06259'
source_url: https://arxiv.org/abs/2512.06259
tags:
- features
- popularity
- audio
- dataset
- music
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of predicting song popularity
  prior to release using multimodal deep learning. The authors introduce GAMENet,
  a novel architecture that combines modality-specific experts for audio, lyrics,
  and social metadata through an adaptive gating mechanism.
---

# Who Will Top the Charts? Multimodal Music Popularity Prediction via Adaptive Fusion of Modality Experts and Temporal Engagement Modeling

## Quick Facts
- **arXiv ID:** 2512.06259
- **Source URL:** https://arxiv.org/abs/2512.06259
- **Authors:** Yash Choudhary; Preeti Rao; Pushpak Bhattacharyya
- **Reference count:** 2
- **Primary result:** 12-16% R² improvements over baselines using multimodal fusion with Career Trajectory Dynamics features

## Executive Summary
This paper introduces GAMENet, a novel multimodal deep learning architecture for predicting song popularity prior to release. The key innovation is combining modality-specific experts for audio, lyrics, and social metadata through an adaptive gating mechanism that learns to emphasize different inputs for each sample. The approach achieves substantial improvements (12-16% in R²) over traditional baselines by incorporating Career Trajectory Dynamics (CTD) features that capture artist momentum and temporal engagement patterns from historical listening logs.

## Method Summary
GAMENet employs a two-phase training approach: Phase 1 independently trains modality experts (Audio via OnionEnsembleAENet compression, Lyrics via OpenAI embeddings, Social via metadata features), then Phase 2 jointly fine-tunes with a gating network that weights these experts' outputs. The architecture uses late fusion with a softmax-gated ensemble, allowing adaptive emphasis on different modalities per sample. CTD features are derived from 5 years of listening logs, capturing both song-level and artist-level temporal patterns. The model is evaluated on Music4All (53,777 tracks) and SpotGenTrack (100k+ tracks) using R², MAE, and MSE metrics.

## Key Results
- **CTD feature impact:** R² improves from 0.13 to 0.69 when Career Trajectory Dynamics are added to content features
- **GAMENet performance:** Achieves R² of 0.676 on Music4All and 0.666 on SpotGenTrack, outperforming baselines by 12-16%
- **Gating stability:** Attention weights remain consistent between train and test sets (social: 0.478→0.478, lyrics: 0.284→0.287, audio: 0.233→0.235)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Career Trajectory Dynamics (CTD) features provide substantially more predictive signal than static content features alone
- **Mechanism:** CTD aggregates 5 years of listening logs into song-level metrics (total plays, unique listeners, repeat listeners, loyalty rate) and artist-level metrics (career trends, engagement consistency)
- **Core assumption:** Historical engagement patterns generalize to future popularity
- **Evidence anchors:** [abstract] "CTD features alone increasing R² from 0.13 to 0.69"
- **Break condition:** If artist has sparse historical data (<2 years of logs), or if significant market/platform disruptions invalidate historical patterns

### Mechanism 2
- **Claim:** Per-sample adaptive gating outperforms fixed feature concatenation by learning which modality to emphasize for each input
- **Mechanism:** The gating network receives standardized 64-dim penultimate features from each modality branch, passes them through [128, 64] MLP with softmax output producing attention weights
- **Core assumption:** Optimal modality importance varies meaningfully across samples in a learnable way
- **Evidence anchors:** [abstract] "12% improvement in R² over direct multimodal feature concatenation"
- **Break condition:** If modality importance is uniform across samples, or if one modality universally dominates

### Mechanism 3
- **Claim:** Two-phase training (independent expert pretraining → joint gating fine-tuning) preserves domain-specific representations while enabling cross-modal coordination
- **Mechanism:** Phase 1 trains each modality branch independently to convergence. Phase 2 jointly optimizes gating network and modality branches using composite loss with individual branch regularization
- **Core assumption:** Modality-specific patterns can be learned in isolation and then integrated without catastrophic forgetting
- **Evidence anchors:** [section 5.4, Table 3] Phase I→Phase II R² improves from 0.571 to 0.676
- **Break condition:** If strong cross-modal dependencies exist that require early fusion

## Foundational Learning

- **Concept:** Mixture of Experts with Learned Gating
  - **Why needed here:** GAMENet's core innovation is adaptive modality weighting—you must understand how softmax-gated ensembles route inputs to experts
  - **Quick check question:** Given attention weights [0.5, 0.3, 0.2] for three experts producing outputs [0.8, 0.6, 0.4], what is the ensemble prediction?

- **Concept:** Autoencoder for Dimensionality Reduction
  - **Why needed here:** OnionEnsembleAENet compresses 11,851 audio features to 2,352 dimensions—you need to understand reconstruction loss tradeoffs
  - **Quick check question:** If an autoencoder achieves RelMSE=0.175, what does this tell you about information preserved vs. lost?

- **Concept:** Temporal Feature Engineering from Event Logs
  - **Why needed here:** CTD features derive from 253M listening events—you must understand aggregation windows, sparsity handling, and trajectory computation
  - **Quick check question:** Why might median plays per listener be more robust than total plays for comparing songs with different release dates?

## Architecture Onboarding

- **Component map:** Audio 11,851 → OnionEnsembleAENet → 2,352 → Audio expert [512→256→128→64] → Output
  Lyrics text → OpenAI embedding → 3,072 → Lyrics expert [1024→512→256→128→64] → Output
  Social metadata + CTD → 46 → Social expert [512→256→128→64] → Output
  Modality outputs + gating network [128→64] → softmax → weighted ensemble → final prediction

- **Critical path:** CTD feature extraction → OnionEnsembleAENet training → Phase 1 expert training → Phase 2 gating fine-tuning. CTD is the highest-impact component (0.13 → 0.69 R²).

- **Design tradeoffs:**
  - Late fusion vs. early fusion: GAMENet uses late fusion for interpretability and modularity, but may miss cross-modal interactions
  - 7 separate autoencoders vs. 1 unified: Semantic grouping preserves feature meaning but adds training complexity
  - λ_individual > 0: Prevents branch collapse but may constrain gating flexibility

- **Failure signatures:**
  - Gating weights converge to uniform (1/3, 1/3, 1/3): indicates no learned differentiation, fallback to averaging
  - One modality dominates (α > 0.9): gating collapsed, check for scale mismatch in input normalization
  - Phase 1→Phase 2 regression: λ_individual too low causing catastrophic forgetting

- **First 3 experiments:**
  1. **Baseline sanity check:** Train LightGBM on CTD+Spotify features only (45 dims). Target: R² ≈ 0.75 per Table 2. If significantly lower, check data preprocessing.
  2. **Ablation gating vs. concatenation:** Compare GAMENet (adaptive gating) against simple concatenation [audio;lyrics;social] → MLP. Expect ~12% R² improvement. If similar, gating may not be learning meaningful patterns.
  3. **CTD temporal vs. aggregate:** Compare CTD Aggregate (18 feat) vs. CTD Aggregate+Temporal (45 feat) on XGBoost. Expect ~7% relative gain. If temporal hurts performance, check for overfitting on sparse years (2016 data).

## Open Questions the Paper Calls Out
- How effectively can GAMENet predict popularity for debut artists or first releases where Career Trajectory Dynamics (CTD) features are unavailable?
- Does GAMENet generalize to non-Western music markets and underrepresented languages?
- Can the learned gating weights transfer across time periods or are they overfitted to the 2016–2020 engagement patterns?
- What is the marginal predictive value of audio and lyrics when social metadata and CTD features already provide strong baselines?

## Limitations
- Temporal validation approach uses static train/test split without establishing temporal consistency
- Cross-dataset evaluation between Music4All and SpotGenTrack shows only slight R² degradation, but datasets may have overlapping artists
- Gating mechanism may not generalize to new music consumption contexts or emerging artists with limited historical data
- 2.8% feature omission rate could introduce selection bias toward established artists

## Confidence

**High confidence:** CTD feature importance (0.13→0.69 R²) is strongly supported by direct ablation evidence and controlled experiments. The two-phase training architecture is clearly specified and validated through performance improvements (Phase I→Phase II: 0.571→0.676 R²).

**Medium confidence:** Adaptive gating superiority (12% R² gain) relies on internal comparisons but lacks external validation against other gating or attention mechanisms.

**Low confidence:** Cross-dataset generalization claims are weakly supported—the datasets are temporally and stylistically similar, and the single performance metric doesn't capture distributional shifts.

## Next Checks

1. **Temporal robustness test:** Train GAMENet on 2016-2019 data, test on 2020 releases only. Measure R² degradation and gating weight stability to assess temporal generalization beyond static splits.

2. **Gating mechanism ablation:** Compare GAMENet's adaptive gating against: (a) fixed uniform weights (1/3,1/3,1/3), (b) learned single best modality selection (argmax), and (c) concatenation+MLP. Report R² differences and gating weight distributions across all conditions.

3. **Cold-start artist evaluation:** Filter test set to artists with <2 years of historical data. Compare CTD feature performance against content-only features (audio+lyrics) to identify failure modes for emerging artists.