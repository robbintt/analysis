---
ver: rpa2
title: 'DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled
  Anatomy-Contrast Representations'
arxiv_id: '2512.07674'
source_url: https://arxiv.org/abs/2512.07674
tags:
- image
- dist-clip
- style
- contrast
- anatomical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of medical image harmonization,
  specifically for MRI scans, where differences in scanner hardware, acquisition protocols,
  and sequence parameters introduce substantial domain shifts that obscure underlying
  biological signals. To tackle this, the authors propose DIST-CLIP, a unified framework
  for MRI harmonization that can leverage either target images or DICOM metadata as
  guidance.
---

# DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations

## Quick Facts
- arXiv ID: 2512.07674
- Source URL: https://arxiv.org/abs/2512.07674
- Reference count: 6
- Primary result: Achieves PSNR up to 31.8 dB and SSIM up to 0.973 for cross-contrast MRI harmonization while preserving anatomical structure

## Executive Summary
DIST-CLIP is a unified framework for MRI harmonization that can leverage either target images or DICOM metadata as guidance. It explicitly disentangles anatomical content from image contrast, with contrast representations extracted using pre-trained CLIP encoders. These contrast embeddings are integrated into the anatomical content via a novel Adaptive Style Transfer module. The framework was trained and evaluated on diverse real-world clinical datasets, demonstrating significant improvements in performance compared to state-of-the-art methods in both style translation fidelity and anatomical preservation.

## Method Summary
DIST-CLIP addresses MRI harmonization by disentangling anatomical content from image contrast through a three-stage architecture. First, an Anatomy Mapper (U-Net) extracts contrast-invariant structural representations using Instance Normalization and patch-wise contrastive alignment. Second, pre-trained MR-CLIP encoders extract 512-dimensional style embeddings from either target images or DICOM metadata. Third, a Style Fusion Decoder with Adaptive Style Transfer (AST) modules injects these embeddings into the anatomical features via attention-based modulation followed by spatial AdaIN. The model is trained with a composite loss including anatomical alignment, reconstruction, perceptual, adversarial, and style consistency terms, with random conditioning on image vs. metadata embeddings (p=0.5) during training.

## Key Results
- Achieves PSNR up to 31.8 dB and SSIM up to 0.973 (PDw→T2w) across cross-contrast harmonization tasks
- Demonstrates robust generalization to external datasets like OASIS-3 with minimal performance drop
- Outperforms state-of-the-art baselines including HACA3, HiFormer, and CycleGAN in both perceptual quality and anatomical preservation

## Why This Works (Mechanism)

### Mechanism 1: Contrast-Invariant Anatomy Extraction
The Anatomy Mapper uses Instance Normalization in its first layer to suppress contrast-specific intensity statistics, then enforces local patch alignment between source and target anatomical representations via a contrastive InfoNCE loss. This encourages the mapper to preserve spatially-corresponding structural features while tolerating cross-contrast feature variation.

### Mechanism 2: Dual-Modality Style Conditioning via Pre-Aligned Embeddings
Pre-trained MR-CLIP encoders align image and metadata outputs into a shared contrast-aware embedding space. During training, the decoder is randomly conditioned on either image or metadata embeddings (p=0.5 per iteration), forcing it to interpret both modalities equivalently. At inference, either modality alone suffices for harmonization.

### Mechanism 3: Attention-Driven Adaptive Style Injection
The AST module maps style embeddings to query vectors that drive an attention mechanism over decoder features, allowing spatially-varying contrast transfer. This enables precise local style-content interactions rather than applying global statistics, preserving fine anatomical boundaries during harmonization.

## Foundational Learning

- **Concept: Contrastive Learning (InfoNCE)**
  - Why needed here: The anatomical representation loss L_β uses patch-wise InfoNCE to align corresponding patches across contrasts while pushing apart non-corresponding patches. Understanding the temperature parameter τ and negative sampling strategy is essential for debugging anatomical fidelity.
  - Quick check question: Can you explain why maximizing cosine similarity for matching patches while minimizing it for non-matching patches encourages contrast-invariant representations?

- **Concept: Adaptive Instance Normalization (AdaIN)**
  - Why needed here: AdaIN is the baseline against which AST is compared, and forms the final stage of each AST block. Understanding how AdaIN transfers channel-wise mean/variance statistics explains both the foundation and the limitations that motivated the attention-based extension.
  - Quick check question: Given feature maps from content and style images, what statistics does AdaIN compute and apply?

- **Concept: Vision-Language Pre-training (CLIP)**
  - Why needed here: DIST-CLIP relies on pre-trained MR-CLIP encoders. Understanding CLIP's contrastive image-text alignment explains why θ_i and θ_m can be used interchangeably, and what failure modes arise from embedding misalignment.
  - Quick check question: In CLIP pre-training, what objective aligns image and text embeddings, and what does the shared embedding space enable at inference?

## Architecture Onboarding

- **Component map**: Source image → Anatomy Mapper → β_src → Style Fusion Decoder (with AST injection from θ) → Harmonized output
- **Critical path**: Source image → Anatomy Mapper → β_src → SFD (with AST injection from θ) → Harmonized output. Style embedding θ comes from either target image → E_I or target metadata → E_M.
- **Design tradeoffs**:
  - 2D slice processing leverages pre-trained 2D CLIP encoders but sacrifices through-plane coherence
  - Heavier attention (8-head) at bottleneck vs. linear attention in upsampling balances computational cost vs. modulation precision
  - Bi-modal training (p=0.5 random conditioning) sacrifices peak single-modality performance for flexibility
- **Failure signatures**:
  - Anatomical blurring or structure loss → Check L_β weight (λ_β=0.1) and patch alignment convergence
  - Poor style matching with metadata guidance → Check metadata prompt formatting; verify MR-CLIP encoder weights
  - Grid artifacts or over-smoothing → May indicate insufficient AST capacity or discriminator issues
  - PSNR drop on external data → Expected for text-guided variant; verify metadata fields match training distribution
- **First 3 experiments**:
  1. Train DIST-CLIP on a single contrast pair (T1w→T2w) with image guidance only; verify PSNR/SSIM approximates reported values
  2. Run ablation conditions (AdaIN replacement, no disentanglement, single-modality training) to confirm component contributions
  3. Vary DICOM field inclusion in metadata prompts to characterize sensitivity of text-guided harmonization to metadata completeness

## Open Questions the Paper Calls Out

### Open Question 1
Can the DIST-CLIP framework be effectively extended to 3D volumetric processing to enforce through-plane continuity?
Basis: Current 2D slice approach "does not explicitly enforce continuity along the through-plane, indicating a natural opportunity for future 3D extensions."
Resolution: Implementation of 3D architecture demonstrating improved smoothness metrics along z-axis without loss of contrast fidelity.

### Open Question 2
Does DIST-CLIP harmonization preserve the subtle intensity boundaries required for high-accuracy downstream tasks like tissue segmentation?
Basis: Need to evaluate "downstream utility such as tissue segmentation or morphometric quantification" to ensure clinically meaningful boundaries are preserved.
Resolution: Benchmarking downstream task performance (e.g., Dice scores) on DIST-CLIP outputs versus raw data across multiple sites.

### Open Question 3
How can the robustness of metadata-guided harmonization be improved for out-of-distribution (OOD) acquisition protocols?
Basis: Text-guided model (DIST-CLIP/T) "experiences some challenges in style transfer with unseen metadata."
Resolution: Analysis of failure cases on OOD metadata or improvements in metadata encoder that close performance gap between text-guided and image-guided modes on external datasets.

## Limitations
- The current 2D slice-based approach sacrifices through-plane consistency, limiting applicability to volumetric analysis
- Clinical impact assessment is minimal; the paper does not evaluate diagnostic accuracy changes or radiologist preference for harmonized images
- Pre-trained MR-CLIP encoders are not yet publicly available, creating a significant barrier to reproduction

## Confidence
- **High confidence**: The framework's core architecture (disentanglement via Anatomy Mapper, dual-modality style conditioning, AST-based feature modulation) is well-specified and empirically validated on multiple datasets
- **Medium confidence**: The effectiveness of metadata-guided harmonization depends on pre-trained MR-CLIP encoder availability and metadata quality—both factors outside the paper's direct control
- **Low confidence**: Clinical impact assessment is minimal; the paper does not evaluate diagnostic accuracy changes or radiologist preference for harmonized images

## Next Checks
1. Validate MR-CLIP encoder loading and prompt formatting for metadata-guided inference—misalignment here causes catastrophic style transfer failure
2. Run single-modality training (image-only or metadata-only) to confirm that random conditioning is essential for bi-modal generalization
3. Test DIST-CLIP on external datasets with incomplete or noisy DICOM metadata to quantify robustness to real-world clinical variability