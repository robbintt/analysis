---
ver: rpa2
title: The Quest for Winning Tickets in Low-Rank Adapters
arxiv_id: '2512.22495'
source_url: https://arxiv.org/abs/2512.22495
tags:
- lora
- layer
- sparsity
- performance
- loras
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether the Lottery Ticket Hypothesis (LTH)
  applies to parameter-efficient fine-tuning (PEFT) methods, specifically focusing
  on Low-Rank Adaptation (LoRA). The authors show that sparse subnetworks ("winning
  tickets") within LoRAs can achieve performance comparable to dense LoRA while significantly
  reducing trainable parameters.
---

# The Quest for Winning Tickets in Low-Rank Adapters

## Quick Facts
- **arXiv ID:** 2512.22495
- **Source URL:** https://arxiv.org/abs/2512.22495
- **Reference count:** 40
- **Primary result:** Sparse subnetworks within LoRAs achieve performance comparable to dense LoRA while reducing trainable parameters by up to 87%.

## Executive Summary
This paper investigates whether the Lottery Ticket Hypothesis (LTH) applies to parameter-efficient fine-tuning (PEFT) methods, specifically focusing on Low-Rank Adaptation (LoRA). The authors show that sparse subnetworks ("winning tickets") within LoRAs can achieve performance comparable to dense LoRA while significantly reducing trainable parameters. They introduce Partial-LoRA, a method that derives task-specific sparsity ratios from pre-trained models to systematically identify and apply random masking to LoRA components. Experiments across 8 vision and 12 language tasks demonstrate that Partial-LoRA reduces trainable parameters by up to 87% while maintaining or improving accuracy.

## Method Summary
Partial-LoRA extends the Lottery Ticket Hypothesis to LoRA by finding sparse subnetworks within low-rank adapters. The method computes per-layer sparsity ratios using truncated SVD of pre-trained weights and leverage scores, then applies random masking to LoRA matrices B and A during training. A few-shot evaluation subset determines the sparsity ratios by progressively masking least important elements until accuracy drops below 90% of baseline. The approach uses AdamW optimizer with cosine annealing learning rate in the range [0.0001, 0.005].

## Key Results
- Sparse subnetworks within LoRAs can match the performance of dense adapters while reducing trainable parameters by up to 87%
- Effectiveness depends more on per-layer sparsity ratios than on exact weight selection
- Per-layer sparsity ratios can be derived from pre-trained model structure without requiring gradient computation

## Why This Works (Mechanism)

### Mechanism 1
Randomly masked LoRA adapters can approximate target (full-parameter) LoRA adapters when the unmasked adapter is sufficiently wide by a logarithmic margin. The proof extends Strong Lottery Ticket theory to LoRA residuals, showing that given a pruned network f_LoRA with Bernoulli-sampled masks and a target network f_T, the approximation bound holds when n_LoRA,l ≥ C × (n_T,l / log(1/(1-p_{l+1})) × log(1/min(ε_l, δ/ρ)). The frozen pretrained weights ensure gradient flow preservation even when residual weights are zeroed.

### Mechanism 2
Performance of sparse LoRA adapters depends primarily on per-layer sparsity ratios (capacity allocation), not on which specific weights are masked. Randomly sampled masks (Partial-LoRA) perform comparably to deterministically selected masks (Targeted-LoRA) when both use the same per-layer sparsity ratios. This suggests the "capacity" at each layer—not the precise subnetwork topology—is the critical factor.

### Mechanism 3
Per-layer sparsity ratios can be derived from pretrained model structure using leverage scores on singular vectors, without requiring gradient computation or training. The method computes truncated SVD of pretrained weights W ≈ P_k Λ_k Q_k, then derives row/column leverage scores as I_row,i = ||(P_k)_i,:||²². Algorithm 1 progressively masks lowest-importance elements until accuracy drops below 90% of baseline on a few-shot subset.

## Foundational Learning

- **Concept:** Low-Rank Adaptation (LoRA)
  - Why needed here: The paper's entire contribution builds on LoRA's formulation ∆W = BA where B∈R^{m×d}, A∈R^{d×n}. Understanding that LoRA adds trainable residuals to frozen weights is prerequisite for grasping why masking works.
  - Quick check question: Given a weight matrix W∈R^{4096×4096} and rank d=8, how many trainable parameters does LoRA add? (Answer: 4096×8 + 8×4096 = 65,536)

- **Concept:** Lottery Ticket Hypothesis (Original)
  - Why needed here: The paper explicitly extends LTH to PEFT. Without understanding that LTH claims sparse subnetworks can match full-network performance when trained in isolation, the motivation and contribution are unclear.
  - Quick check question: What distinguishes a "winning ticket" from just any sparse subnetwork? (Answer: It must be trainable from scratch with the original initialization to match full performance)

- **Concept:** Singular Value Decomposition and Leverage Scores
  - Why needed here: The paper's primary importance measure uses SVD to identify which rows/columns align with principal subspaces. Appendix G shows >50% overlap between SVD-based and gradient-based selection in early layers, validating this approach.
  - Quick check question: If a weight matrix has dominant singular vectors concentrated in a few rows, what will the leverage scores reveal? (Answer: Those rows will have high leverage scores, indicating they capture most of the matrix's action)

## Architecture Onboarding

- **Component map:**
  Pretrained Model (frozen) ──┐
                               ├──> Layer Output: σ((W + ∆W_masked)x + b)
  LoRA Matrices (B, A) ───────┤         where ∆W_masked = (u_row ⊙ B)(A ⊙ u_col)
                               │
  Sparsity Derivation ─────────┴──> Per-layer ratios (p_row, p_col) via Algorithm 1
       │
       └──> Few-shot evaluation on D_t ──> Mask until 90% accuracy threshold

- **Critical path:**
  1. Sample few-shot dataset D_t (16 shots per class typical)
  2. Compute baseline accuracy on D_t with frozen pretrained model
  3. For each layer: compute SVD, derive leverage scores, iteratively mask lowest-importance elements until accuracy < 90% baseline
  4. Record resulting sparsity ratios per layer
  5. During fine-tuning: sample Bernoulli masks u_row, u_col using these ratios; apply masking to B, A before each forward pass
  6. Train only unmasked elements

- **Design tradeoffs:**
  - **Accuracy threshold (90% vs lower)**: Table 3 shows 70% threshold reduces parameters 2× but may drop accuracy ~1%. Use 90% for safety-critical tasks; lower for extreme memory constraints.
  - **Importance measure (SVD vs SNIP vs IMP)**: Table 2 shows similar performance; SVD is cheapest (no gradients), SNIP requires one forward/backward pass, IMP requires trained weights. Choose SVD for pure efficiency; gradient-based if you suspect pretrained structure misaligns with task.
  - **Rank selection**: Figure 6 shows consistent relative performance across ranks 1-64. Higher rank + more sparsity ≈ lower rank + less sparsity. Prefer lower rank with higher sparsity for memory-constrained deployment.

- **Failure signatures:**
  - **Mask too aggressive**: Accuracy drops sharply during few-shot evaluation in Algorithm 1; sparsity ratios exceed ~95% per layer. Symptom: final fine-tuning underperforms full LoRA by >3%.
  - **Few-shot set unrepresentative**: If D_t has class imbalance or outliers, sparsity ratios may be misallocated. Symptom: high variance across different D_t samples (check via Appendix G overlap analysis).
  - **Flow preservation unnecessary but rank insufficient**: Appendix J confirms flow preservation isn't needed, but if rank < 4, the logarithmic width bound may fail. Symptom: training instability, NaN losses.

- **First 3 experiments:**
  1. **Sanity check**: Apply Partial-LoRA to a small dataset (CIFAR-10 with ViT-B-16) with 90% threshold. Verify parameter reduction matches paper (~60-80%) and accuracy is within 1% of full LoRA. This validates your SVD/masking implementation.
  2. **Threshold sweep**: On a single dataset, test 70%, 80%, 90%, 95% accuracy thresholds. Plot parameter count vs final accuracy to find your operating point. Compare against Table 3 patterns.
  3. **Importance measure ablation**: Compare SVD-based vs SNIP-based vs IMP-based sparsity derivation on 2-3 datasets. Compute mask overlap as in Appendix G (Figure 12). If overlap <30% in early layers, your pretrained model may have unusual structure—investigate before production use.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does explicitly assigning disjoint random subnetworks to different tasks within a Multi-LoRA setup reduce task interference and improve performance compared to overlapping masks?
- **Basis in paper:** Appendix D states, "This shows a potential for future work where different parts of each weight matrix are assigned randomly to different datasets to reduce interference with shared sections..."
- **Why unresolved:** The current experiments use standard Multi-LoRA setups; the specific hypothesis that *disjoint* random subnetworks mitigate interference was proposed but not tested.
- **What evidence would resolve it:** A comparative study of Multi-LoRA performance using enforced disjoint random masks versus the standard overlapping approach on a suite of diverse tasks.

### Open Question 2
- **Question:** How do the training dynamics, such as gradient noise and convergence speed, differ between Partial-LoRA's sparse subnetworks and dense LoRAs?
- **Basis in paper:** Section 6 (Conclusion) lists "analysis of training dynamics" as a specific future direction.
- **Why unresolved:** The paper focuses on the final accuracy and parameter efficiency of the found subnetworks rather than analyzing the optimization trajectory or loss landscape characteristics.
- **What evidence would resolve it:** Detailed logs of training loss curves, gradient variance, and spectral analysis of the optimization process comparing Partial-LoRA to full LoRA.

### Open Question 3
- **Question:** Is the 90% accuracy threshold on few-shot data a robust universal heuristic for determining optimal sparsity ratios, or does it require task-specific tuning?
- **Basis in paper:** Algorithm 1 relies on a fixed 90% threshold derived from intrinsic dimensionality literature. While Appendix B.3 tests 70/80/90 thresholds, it leaves the generalizability of this specific heuristic across all model scales and domains uncertain.
- **Why unresolved:** The paper demonstrates the method works with this threshold but does not verify if this specific cut-off is optimal for maximizing the efficiency-accuracy trade-off in all scenarios.
- **What evidence would resolve it:** An ablation study sweeping the accuracy threshold (e.g., 85%-95%) across a wider variety of architectures and datasets to identify if a dynamic or adaptive threshold yields superior results.

## Limitations
- **Code not released:** Implementation details like SVD truncation rank and baseline accuracy computation must be inferred from context
- **Assumes overparameterization:** Theoretical proof may not hold for very low ranks (d<4)
- **Leverage score validation:** SVD-based importance lacks strong corpus validation compared to established gradient-based pruning methods

## Confidence
- **High Confidence:** The core empirical finding that sparse subnetworks within LoRA can match dense performance (verified across 20+ tasks with consistent 60-87% parameter reduction). The theoretical framework extending SLT to LoRA residuals is mathematically sound given the stated assumptions.
- **Medium Confidence:** The claim that per-layer sparsity ratios matter more than specific weight selection. While Table 2 shows random vs deterministic masking performs similarly, this comparison lacks ablation studies isolating capacity effects from initialization sensitivity.
- **Medium Confidence:** The SVD leverage score method's effectiveness for deriving task-specific sparsity. The method works empirically, but the assumption that pretrained principal components align with fine-tuning-relevant subspaces isn't rigorously validated, particularly for domain-shifted tasks.

## Next Checks
1. **Flow preservation test:** Train Partial-LoRA with and without gradient flow preservation through masked layers (despite Appendix J's claim it's unnecessary) to verify the theoretical assumption experimentally across different rank values.
2. **Task shift robustness:** Apply Partial-LoRA to a pretrained model fine-tuned on a substantially different domain (e.g., CLIP fine-tuned on medical imaging) to test whether SVD-based importance breaks when principal components don't align with task-relevant directions.
3. **Rank scaling validation:** Systematically vary LoRA rank from 1-64 on a single task while measuring approximation quality bounds to verify the logarithmic width requirement from Theorem 4.1 holds empirically across the full rank spectrum.