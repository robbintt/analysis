---
ver: rpa2
title: 'Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive
  Fiction Games'
arxiv_id: '2505.12439'
source_url: https://arxiv.org/abs/2505.12439
tags:
- action
- games
- lplh
- game
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Learning to Play Like Humans (LPLH)
  framework, which guides Large Language Models (LLMs) to play interactive fiction
  (IF) games in a human-like manner. LPLH incorporates three main components: a dynamic
  knowledge graph for spatial and narrative mapping, action space learning to identify
  valid commands, and an experience reflection module that summarizes past successes
  and failures.'
---

# Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games

## Quick Facts
- **arXiv ID:** 2505.12439
- **Source URL:** https://arxiv.org/abs/2505.12439
- **Reference count:** 40
- **Primary result:** Framework enables LLMs to play IF games in human-like manner without external fine-tuning, achieving maximum scores in some games and outperforming baselines.

## Executive Summary
This paper introduces the Learning to Play Like Humans (LPLH) framework, which guides Large Language Models (LLMs) to play interactive fiction (IF) games in a human-like manner. LPLH incorporates three main components: a dynamic knowledge graph for spatial and narrative mapping, action space learning to identify valid commands, and an experience reflection module that summarizes past successes and failures. By integrating these elements, the framework enables LLMs to adapt their decision-making over time and make contextually informed choices without relying on external pre-training or task-specific fine-tuning. The approach was tested across nine IF games, where it achieved notable performance improvements, with some agents reaching maximum scores and consistently outperforming baseline models in raw score gains. LPLH thus offers a novel, interpretable path toward more adaptive and human-aligned gameplay in complex text-based environments.

## Method Summary
LPLH is a framework for enabling LLM agents to play Interactive Fiction games without external pre-training. It processes game observations through three integrated modules: (1) a dynamic knowledge graph (KG-map) that captures spatial and narrative relationships by extracting entity-relation triples from text, (2) an action space learning component that identifies and validates context-appropriate commands by decomposing actions into verb-object pairs, and (3) an experience reflection module that summarizes successful and failed actions for future retrieval. The framework uses a fine-tuned 1.5B model for relation extraction and GPT-o3-mini for experience summarization, storing learned information in a vector database accessed via RAG. Tested on nine games from the Jericho benchmark with 10 epochs of 250 steps each, the approach achieves improved performance through interpretable, memory-based reasoning rather than black-box fine-tuning.

## Key Results
- Achieved maximum scores in some games while consistently outperforming baseline models in raw score gains
- Demonstrated effective learning across nine IF games including Zork1, Detective, and Omniquest
- Showed that dynamic knowledge graph mapping, action space learning, and experience reflection each contribute to performance improvements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic knowledge graph mapping enables consistent spatial reasoning across extended gameplay sessions.
- **Mechanism:** A fine-tuned relation extraction model parses each observation into entity-relation triples (e.g., `<You, in, Living Room>`, `<mailbox, have, key>`), which incrementally update a structured graph representation. This graph is provided as context to the decision LLM at each step, grounding actions in accumulated world state rather than relying solely on limited conversation history.
- **Core assumption:** LLMs can effectively utilize structured JSON-like graph representations to inform action generation. Assumption: the extraction model's ~85% accuracy is sufficient for gameplay purposes without cascading errors.
- **Evidence anchors:**
  - [abstract]: "structured map building to capture spatial and narrative relationships"
  - [Section 3.2]: "This dynamic process resolves inconsistencies (such as outdated item locations) and enriches the agent's contextual awareness"
  - [corpus]: Weak direct corpus support; related work SNAP (arXiv:2601.11529) addresses LLM spatiotemporal consistency in interactive narratives but uses planning-based approaches rather than KG memory.
- **Break condition:** If the extraction model consistently fails to capture key relationships, or if the graph grows too large for effective context window utilization, performance degrades. Table 2 ablation shows KG-map alone achieves only 11/15 max score without other components.

### Mechanism 2
- **Claim:** Incremental action space learning reduces invalid command generation by maintaining verified verb-object pairs.
- **Mechanism:** Valid actions are decomposed into verb templates (e.g., "put * in *") and object lists. When the agent encounters new objects in a location, a pairing function generates candidate actions by combining current objects with known valid verb patterns. This constrains the LLM's generation space to previously successful action schemas.
- **Core assumption:** IF games share common verb patterns that transfer across contexts. Assumption: decomposition into verb-object pairs preserves sufficient semantic information for action validity.
- **Evidence anchors:**
  - [abstract]: "action learning to identify context-appropriate commands"
  - [Section 3.3]: "The valid actions learned in this manner are retained as executable commands"
  - [Section 5.3]: Ablation shows action-space alone achieves 26.6/35 vs. base model's 9/25 on Zork1
  - [corpus]: No direct corpus evidence for this specific mechanism; the approach appears novel to this work.
- **Break condition:** Breaks when games require unconventional commands not decomposable into learned verb-object patterns (see Section 7: "echo" in Loud Room, "move rug" puzzles).

### Mechanism 3
- **Claim:** Experience reflection enables learning from both successes and failures without gradient-based updates.
- **Mechanism:** Upon scoring events, a summarization LLM extracts structured experiences including location, puzzle status, scoring actions, and generalizable insights. These are embedded and stored in a vector database. During gameplay, RAG retrieves relevant experiences based on current context, providing explicit "lessons learned" to the decision LLM.
- **Core assumption:** Experience summarization can capture transferable insights. Assumption: RAG retrieval returns contextually relevant experiences. Assumption: Triggering only on score changes captures sufficient learning signal.
- **Evidence anchors:**
  - [abstract]: "feedback-driven experience analysis to refine decision-making over time"
  - [Section 6.1]: Dark environment example shows agent learning "You may need lights to avoid death" after failure
  - [Section 6.2]: Egg collection example shows successful puzzle solutions being reused
  - [corpus]: IPR-1 (arXiv:2511.15407) similarly studies learning physical reasoning from interaction, but uses different methodology.
- **Break condition:** Breaks when score changes are too sparse to provide adequate learning signal, or when summarized experiences are too specific to transfer.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: IF games are formalized as POMDPs where agents receive only textual observations of underlying game states. Understanding this framing explains why memory structures (KG-map, experience library) are necessary.
  - Quick check question: Can you explain why an agent might take the same action in two seemingly identical observations but receive different outcomes?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: The experience library uses RAG to retrieve relevant past experiences. Understanding embedding-based retrieval and its limitations is essential for debugging poor experience reuse.
  - Quick check question: What could cause the retrieval system to fail to return a highly relevant past experience?

- **Concept: Knowledge Graph Construction**
  - Why needed here: The dynamic KG-map requires extracting entity-relation triples from unstructured text. Understanding relation extraction and graph update strategies is critical.
  - Quick check question: How would you handle contradictory information when updating a knowledge graph (e.g., "key is in box" vs. "you take the key")?

## Architecture Onboarding

- **Component map:** Game observation → fine-tuned extraction model → KG-map update → action pairing → experience retrieval → LLM decision → action validation → experience summarization
- **Critical path:** Observation → relation extraction → KG update → action pairing → experience retrieval → LLM decision → action validation → (if valid) action space update → (if score change) experience summarization. Latency is dominated by LLM calls (decision + potential summarization).
- **Design tradeoffs:**
  - Fine-tuned 1.5B model vs. API LLM for extraction: Lower cost but potential accuracy drop
  - Score-triggered vs. continuous summarization: Reduces API calls but may miss non-scoring insights
  - JSON KG representation vs. natural language: More structured but may exceed context windows for large maps
- **Failure signatures:**
  - Repeating invalid actions: Check action space update logic
  - Spatial confusion: Check KG extraction accuracy and graph update consistency
  - Ignoring obvious solutions: Check experience retrieval relevance scores
  - Hallucinating nonexistent objects: Check observation parsing
- **First 3 experiments:**
  1. **Baseline establishment:** Run Qwen2.5-14B with only observation + last 10 turns history (no modules) on Zork1 for 10 epochs. Record average/max scores to establish improvement baseline.
  2. **Module isolation:** Test each component individually (KG-map only, action space only, experience only) per Table 2 ablation methodology to identify highest-impact component for your target games.
  3. **Extraction model validation:** Run fine-tuned extraction model on held-out game observations with human annotation to verify relation extraction accuracy matches reported ~85% before deploying full pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LLM-based agents be enabled to infer "unconventional" or domain-specific commands (e.g., "echo" in Zork1) that lack immediate contextual cues or standard training priors?
- **Basis in paper:** [explicit] Section 7 (Error Analysis) states agents struggle with "domain-specific intuition," specifically noting the inability to guess commands like "echo" which are "not inferable from the immediate context."
- **Why unresolved:** Current exploration strategies and LLM priors do not cover obscure actions required for specific puzzle bottlenecks, creating a "fundamental challenge."
- **What evidence would resolve it:** An agent successfully solving the "Loud Room" puzzle in Zork1 without explicit prior training data containing that specific solution.

### Open Question 2
- **Question:** How does agent performance change if experience summarization is triggered dynamically throughout gameplay rather than only on scoring events?
- **Basis in paper:** [explicit] Section 8 (Limitation) notes the current trigger (score change only) is "simple," whereas "human players naturally integrate relevant information... at any point."
- **Why unresolved:** The current rigid triggering mechanism may miss learning opportunities from non-scoring interactions that are crucial for later puzzle solving.
- **What evidence would resolve it:** Comparative results showing improved task completion rates when using a continuous or semantic-based summarization trigger.

### Open Question 3
- **Question:** What is the optimal knowledge representation format (e.g., JSON vs. other structures) for the dynamic KG-map to maximize LLM reasoning consistency?
- **Basis in paper:** [explicit] Section 8 (Limitation) highlights that the consistency of the "JSON-structured KG-map... can influence the model's reasoning," suggesting the need to find an "optimal representation method."
- **Why unresolved:** While JSON provides structure, it may introduce tokenization issues or ambiguity for the LLM compared to other potential graph encodings.
- **What evidence would resolve it:** An ablation study comparing the performance of JSON-based maps against other structural inputs (e.g., triplet lists or natural language summaries).

### Open Question 4
- **Question:** Does transitioning from generating commands to selecting valid actions from a candidate list enhance reasoning and stability in complex IF games?
- **Basis in paper:** [inferred] Section 5.3 mentions a "selective mechanism" (choosing one action from game engine candidates) improved scores in some ablation tests, prompting the authors to suggest "exploring how transitioning... may further enhance reasoning."
- **Why unresolved:** The trade-off between the flexibility of generation (LLM-native) and the safety of selection (RL-native) remains unbalanced.
- **What evidence would resolve it:** A hybrid model architecture that successfully combines the LPLH framework's generative reasoning with a validation selection step, outperforming both pure generation and pure RL baselines.

## Limitations
- **Limited generalizability:** Framework may struggle with games requiring unconventional commands not decomposable into learned verb-object patterns
- **Dependence on score events:** Experience reflection triggered only by score changes may miss valuable learning opportunities from non-scoring interactions
- **Context window constraints:** JSON-structured KG-map may exceed context windows for large maps, affecting reasoning consistency

## Confidence
- **High Confidence:** The overall framework architecture and its modular design (Knowledge Graph mapping, action space learning, experience reflection) is well-supported by experimental results showing consistent score improvements across multiple games.
- **Medium Confidence:** The specific mechanisms for each module (relation extraction accuracy, action decomposition effectiveness, RAG retrieval relevance) are supported by ablation studies but may face scalability challenges in more complex game environments.
- **Low Confidence:** The generalizability of the approach to games with significantly different mechanics or to real-time interactive scenarios remains untested.

## Next Checks
1. **Extraction Model Generalization:** Test the fine-tuned relation extraction model on a held-out set of game observations from games not included in the original training data to verify ~85% accuracy claim holds across diverse narratives.
2. **Action Space Robustness:** Systematically evaluate the framework's performance on games known to require unconventional commands (e.g., "echo" in Loud Room) to quantify the limitations of verb-object decomposition.
3. **Experience Library Relevance:** Conduct a qualitative analysis of retrieved experiences in challenging scenarios to assess whether RAG retrieval consistently provides contextually relevant guidance or suffers from noise and irrelevance.