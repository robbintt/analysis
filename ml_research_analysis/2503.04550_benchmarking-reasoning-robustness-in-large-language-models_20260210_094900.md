---
ver: rpa2
title: Benchmarking Reasoning Robustness in Large Language Models
arxiv_id: '2503.04550'
source_url: https://arxiv.org/abs/2503.04550
tags:
- reasoning
- arxiv
- uni00000055
- uni00000052
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies significant reasoning robustness issues in
  large language models (LLMs), including positional bias, instruction sensitivity,
  numerical fragility, and memory dependence, where models rely on memorized patterns
  rather than systematic reasoning. To address this, the authors introduce Math-RoB,
  a novel benchmark that exposes these limitations through datasets with modified
  operators, missing information, and irrelevant context.
---

# Benchmarking Reasoning Robustness in Large Language Models

## Quick Facts
- **arXiv ID:** 2503.04550
- **Source URL:** https://arxiv.org/abs/2503.04550
- **Reference count:** 40
- **Key outcome:** Introduces Math-RoB benchmark exposing reasoning robustness limitations in LLMs including positional bias, instruction sensitivity, numerical fragility, and memory dependence, with performance drops of 5-15% across perturbations

## Executive Summary
This paper systematically identifies and quantifies reasoning robustness limitations in large language models through a novel benchmark called Math-RoB. The authors demonstrate that LLMs exhibit significant vulnerabilities including positional bias (favoring earlier queries in sequences), instruction sensitivity (failing to follow explicit operator substitution instructions), numerical fragility (performance drops when replacing numbers with symbols), and memory dependence (hallucinating missing information). Experiments across 12 models show accuracy drops ranging from 5% to 15% under various perturbations, with larger models demonstrating better but still imperfect robustness. The benchmark provides a framework for evaluating and improving reasoning robustness in LLMs.

## Method Summary
The authors construct Math-RoB by perturbing Math500 problems across four dimensions: positional bias (RoLo dataset with multi-query inputs), instruction sensitivity (Define dataset with explicit operator substitution instructions), numerical fragility (Number dataset with digit-to-symbol replacement), and memory dependence (Delete dataset with missing critical data). They use Chain-of-Thought reasoning with PRM scoring, MCTS (width=5, depth≤30), and five voting strategies (MinVote, LastVote, MajorityVote, MinMax, LastMax) to evaluate 12 models. Performance is measured via Accuracy Rate, Drop Rate between datasets, and Memory Completion Rate for hallucination detection. Manual verification ensures instruction compliance in Define questions.

## Key Results
- Positional bias: GPT-4o accuracy drops from 75.8% to 72.8% when answering later queries in multi-query inputs
- Numerical fragility: GPT-4o accuracy decreases from 97.5% to 82.5% when substituting numbers with Greek letters
- Memory dependence: Skywork-8B exhibits 74% memory completion rate, indicating high hallucination tendencies when critical data is missing
- Instruction sensitivity: Only 2/40 manually verified Define questions showed full operator replacement compliance

## Why This Works (Mechanism)

### Mechanism 1: Distributional Perturbation for Pattern Matching Detection
- **Claim:** Surface-level perturbations cause performance degradation if models use pattern matching rather than formal logic
- **Mechanism:** Number and Define questions preserve logical structure while altering token sequences; token shifts break retrieval paths for memorized templates
- **Core assumption:** Reasoning robustness correlates with accuracy maintenance when surface features change but logical validity remains constant
- **Evidence anchors:** Abstract mentions "numerical fragility—value substitution sharply reduces accuracy"; section 4.3 shows GPT-4o dropping from 97.5% to 82.5%; MathRobust-LV evaluates robustness to linguistic variations
- **Break condition:** Symbolic reasoning modules or invariant logical form training maintain stable accuracy

### Mechanism 2: Memory Completion Rate (MCR) as Hallucination Trigger
- **Claim:** High MCR indicates models autocomplete missing data using training priors rather than identifying gaps
- **Mechanism:** Autoregressive models predict next tokens; Delete scenarios trigger generation of "hallucinated" values to minimize prediction loss
- **Core assumption:** Reasoning models should identify missing preconditions while recall-based models confabulate to complete patterns
- **Evidence anchors:** Abstract identifies "Memory dependence—models resort to guesswork when missing critical data"; section 4.4 defines MCR and shows Skywork-8B at 74%; Are Large Vision Language Models Truly Grounded? questions prior reliance over input data
- **Break condition:** Uncertainty quantification or refusal mechanisms for incomplete inputs decrease MCR

### Mechanism 3: Positional Dilution in Long Contexts
- **Claim:** Multi-query inputs cause accuracy degradation for later queries due to primacy bias
- **Mechanism:** Attention mechanism distributes focus; increasing context length makes isolation of relevant tokens for later queries difficult
- **Core assumption:** Robustness requires maintaining attentional focus on specific instructions regardless of position in long sequences
- **Evidence anchors:** Abstract notes "Positional bias--models favor earlier queries... (e.g., GPT-4o's accuracy drops from 75.8% to 72.8%)"; section 4.1 reports Qwen2.5-1.5B accuracy dropping from 80.7% to 32.5%; MME-CoT benchmarks reasoning quality in multimodal models
- **Break condition:** Sliding window attention or retrieval-augmented generation isolates specific query segments

## Foundational Learning

- **Concept: Distributional Shift (Out-of-Distribution/OOD)**
  - **Why needed here:** Explains why LLMs fail when test data deviates from training data; understanding OOD distinguishes generalization from memorization
  - **Quick check question:** If a model trained on arithmetic with digits fails on arithmetic with Greek letters, is it failing at math or pattern matching?

- **Concept: Autoregressive Hallucination**
  - **Why needed here:** Interprets MCR results; LLMs generate probable continuations, not factual truths, explaining why they "hallucinate" missing numbers
  - **Quick check question:** Why would a model invent a number for a missing variable rather than outputting "Error: Missing Variable"?

- **Concept: Chain-of-Thought (CoT) & Process Reward Models (PRM)**
  - **Why needed here:** Paper uses PRMs to score reasoning steps; understanding CoT reveals why "step-by-step" scoring improves failure diagnosis
  - **Quick check question:** Does high final answer accuracy guarantee that intermediate reasoning steps were sound?

## Architecture Onboarding

- **Component map:** Math500 Problems -> Perturbation Engine -> LLM -> MCTS -> PRM -> Voting Strategy -> Accuracy & Drop Rate Calculation
- **Critical path:** Math-RoB-Delete dataset creation generates incomplete problems to test hallucination tendencies
- **Design tradeoffs:** Hardware constraints limit MCTS search width to 5 and depth to 30 on 4090 GPUs, trading solution optimality for computational feasibility
- **Failure signatures:**
  - High MCR (>50%): Model guessing on missing data (Memory Dependence)
  - Instruction Drift: Answering Q1 when prompted for Q2 (Positional Bias)
  - Operator Fixation: Applying standard operators despite "Define" instructions (Instruction Sensitivity)
- **First 3 experiments:**
  1. Run Math-RoB-Delete to measure Memory Completion Rate baseline for hallucination tendencies
  2. Run Math-RoB-Number to quantify "Numerical Fragility" drop rate compared to baseline
  3. Run Math-RoB-RoLo to verify accuracy decay correlates with query position (Q1 vs. Q3)

## Open Questions the Paper Calls Out

- **Open Question 1:** How does reasoning robustness vary across different model architectures beyond tested transformer implementations?
  - **Basis in paper:** [explicit] Authors state framework doesn't provide comprehensive analysis across various architectures, planned for future work
  - **Why unresolved:** Current study focused on popular models available on HuggingFace, limiting architectural diversity
  - **What evidence would resolve it:** Comparative evaluation of Math-RoB performance across distinct non-transformer or hybrid architectures

- **Open Question 2:** What specific mechanistic factors enable larger models (>7B parameters) to exhibit superior instruction compliance compared to smaller models?
  - **Basis in paper:** [explicit] Limitations section notes larger models show better instruction compliance but reasons remain unclear
  - **Why unresolved:** Paper observes correlation between scale and robust compliance but doesn't isolate underlying causes
  - **What evidence would resolve it:** Ablation studies identifying if capacity, training data volume, or specific regularization techniques drive improvement

- **Open Question 3:** Do identified robustness failure modes persist in state-of-the-art models utilizing deeper reasoning chains and broader search spaces?
  - **Basis in paper:** [explicit] Authors note hardware constraints limited experiments to smaller models (<8B parameters) and shallow search (width < 6)
  - **Why unresolved:** Resource limitations prevented testing whether massive scaling or extensive MCTS mitigates positional and numerical fragility
  - **What evidence would resolve it:** Benchmarking Math-RoB on frontier models (e.g., GPT-o1) with significantly increased inference-time compute

## Limitations

- Scope limited to arithmetic reasoning with single-operator problems, may not generalize to complex mathematical domains
- Manual verification process for 40 questions is labor-intensive and may introduce reviewer bias
- Only tests models using Chain-of-Thought with PRM scoring and MCTS, leaving open whether simpler inference strategies show different profiles
- Hardware constraints limiting MCTS search width to 5 may understate model capabilities in optimal conditions

## Confidence

- **High Confidence:** Positional bias findings well-supported with clear quantitative evidence (75.8% to 72.8% drop for GPT-4o); memory completion rate methodology and results robust (74% MCR for Skywork-8B)
- **Medium Confidence:** Numerical fragility results convincing but limited to specific perturbation type; instruction sensitivity findings methodologically sound but rely on manual verification introducing potential subjectivity
- **Low Confidence:** Generalizability to real-world mathematical reasoning beyond simple arithmetic remains uncertain

## Next Checks

1. **Cross-Domain Generalization Test:** Apply Math-RoB perturbations to algebra, geometry, and word problems from Math500 to determine if reasoning robustness patterns hold across mathematical domains beyond single-operator arithmetic

2. **Alternative Inference Strategy Comparison:** Re-run experiments using direct answer generation (no CoT) and greedy decoding to isolate whether robustness issues are inherent to model architecture or artifacts of Chain-of-Thought+PRM methodology

3. **Temporal Stability Analysis:** Repeat full benchmark suite after model fine-tuning on robustness-aware data (incorporating Math-RoB-style perturbations) to measure whether identified weaknesses can be systematically addressed through targeted training