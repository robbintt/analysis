---
ver: rpa2
title: 'Synthetic Data Augmentation for Multi-Task Chinese Porcelain Classification:
  A Stable Diffusion Approach'
arxiv_id: '2601.14791'
source_url: https://arxiv.org/abs/2601.14791
tags:
- synthetic
- glaze
- porcelain
- images
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigated synthetic data augmentation for Chinese
  porcelain classification using Stable Diffusion with LoRA fine-tuning. The approach
  generated photorealistic synthetic images to address data scarcity in archaeological
  artifact classification, specifically targeting four tasks: dynasty, glaze, kiln,
  and type identification.'
---

# Synthetic Data Augmentation for Multi-Task Chinese Porcelain Classification: A Stable Diffusion Approach

## Quick Facts
- **arXiv ID:** 2601.14791
- **Source URL:** https://arxiv.org/abs/2601.14791
- **Reference count:** 36
- **Key outcome:** Stable Diffusion + LoRA fine-tuning improved Chinese porcelain classification, with up to 5.5% F1-macro gains for rare type classes, though glaze texture performance degraded.

## Executive Summary
This study tackles the challenge of data scarcity in archaeological artifact classification by using synthetic data augmentation. A Stable Diffusion model fine-tuned with LoRA on 1,000 porcelain images generates photorealistic synthetic samples, which are then used to augment real datasets for multi-task classification. The approach targets four tasks: dynasty, kiln, glaze, and type identification. Results show that synthetic augmentation benefits structural feature tasks (dynasty, type) by up to 5.5% F1-macro but harms texture-dependent tasks (glaze), highlighting both the potential and limitations of generative models in domain-specific classification.

## Method Summary
The method combines Stable Diffusion 2.1 with LoRA fine-tuning to generate synthetic porcelain images, guided by structured archaeological prompts. These prompts decompose classification tasks (dynasty, kiln, type, glaze) into concrete descriptors. The synthetic images are mixed with real data (90:10 ratio) and used to train a MobileNetV3-Large multi-task model with four classification heads. Class imbalance is addressed via Effective Number class weighting and WeightedRandomSampler based on glaze frequencies. The pipeline aims to improve rare class performance while maintaining accuracy on majority classes.

## Key Results
- Type classification improved by 5.5% F1-macro with 90:10 real-to-synthetic ratio.
- Dynasty and kiln tasks gained 3-4% F1-macro; glaze classification dropped 3.5% F1-macro.
- Minority class performance improved (up to +5.5%), majority classes stable.
- Class imbalance ratio reduced by 51.7% for rarest categories.

## Why This Works (Mechanism)

### Mechanism 1: Distributional Alignment for Rare Classes
Claim: Targeted synthetic data improves minority class performance when generated features align with task-relevant signatures.
Mechanism: LoRA-tuned Stable Diffusion generates samples for underrepresented glaze-type combinations using archaeologically-grounded prompts, rebalancing the dataset (e.g., 56.6:1 to 28.3:1 ratio), enabling more stable gradient updates for rare classes.
Core assumption: The diffusion model can capture essential visual features (shape, form) accurately enough to serve as valid training examples.
Evidence anchors:
- [abstract] "type classification showed the most substantial improvement (5.5% F1-macro increase with 90:10 ratio)... effectiveness depends on the alignment between generated features and task-relevant visual signatures."
- [section 4.2] "The reduced imbalance ratio (from 56.6:1 to 28.3:1) facilitates more balanced sampling strategies during training."
- [corpus] "Synthetic Augmentation in Imbalanced Learning" (arXiv:2601.16120) investigates when augmentation helps or hurts in imbalanced settings.
Break condition: Fails if the generative model cannot reproduce discriminative features (e.g., subtle surface textures), introducing misleading artifacts. The observed -3.5% F1 drop in glaze classification exemplifies this failure.

### Mechanism 2: Structured Prompt-to-Feature Mapping
Claim: Hierarchical prompts based on archaeological documentation effectively guide generation for structural but not textural features.
Mechanism: Prompts are decomposed by classification task (Dynasty, Kiln, Type, Glaze). Concrete descriptors (e.g., "rounded shape") map successfully to visual features, while abstract glaze descriptions ("milky opacity") do not, creating task-dependent outcomes.
Core assumption: A consistent, learnable mapping exists from structured archaeological terminology to the diffusion model's latent visual space.
Evidence anchors:
- [section 3.2.2] "A complete prompt example: Yuan dynasty, Jun kiln produced, Chinese porcelain, vase for display only... with moon white glaze with pale blue... <lora:glazetype:0.4>"
- [section 5.2] "The observed hierarchy of translation success, morphology>period style>regional characteristics>surface texture..."
- [corpus] Corpus evidence on this specific prompting technique for artifacts is weak.
Break condition: Fails when terminology has no visual counterpart in the model's generative capabilities, as seen with glaze textures.

### Mechanism 3: Feature Space Regularization through Multi-Task Learning
Claim: Synthetic data in an MTL framework can regularize the shared backbone, improving cross-task generalization.
Mechanism: The shared MobileNetV3 backbone learns representations across four related tasks. Diverse synthetic samples expand the feature manifold, preventing overfitting to the limited real dataset. Task-specific heads leverage this improved representation.
Core assumption: Tasks are related (e.g., form follows function) so a shared representation is beneficial, and synthetic data lies sufficiently near the true data manifold.
Evidence anchors:
- [section 3.3] "the four attributes—dynasty, kiln, glaze, and type—exhibit natural correlations... suitable candidates for joint learning."
- [abstract] "MobileNetV3 models trained on mixed real-synthetic datasets... outperformed real-data-only baselines."
- [corpus] "Multi-task Learning for Identification of Porcelain" (arXiv:2503.14231) establishes the baseline MTL framework.
Break condition: Fails if synthetic data introduces conflicting learning signals. Task-specific results confirm this risk, with benefits for Dynasty/Type (+3.9%/+4.4%) but degradation for Glaze (-3.5%).

## Foundational Learning

- **Latent Diffusion Models**
  - Why needed here: The core generative engine (Stable Diffusion) operates by denoising in a compressed latent space. Understanding this is critical to diagnosing why global structure is preserved while fine texture is lost.
  - Quick check question: Can you explain why the denoising process in a diffusion model tends to have a smoothing effect on high-frequency details like texture?

- **Low-Rank Adaptation (LoRA)**
  - Why needed here: This is the method used for domain adaptation. It freezes the large pre-trained model weights and injects trainable rank decomposition matrices, making fine-tuning feasible on a single GPU.
  - Quick check question: If LoRA adds trainable matrices (A×B) to a weight matrix W, what happens to the model's original capabilities if the rank 'r' is set too low?

- **Transfer Learning with CNNs**
  - Why needed here: MobileNetV3 is pre-trained on ImageNet and fine-tuned for porcelain classification. Understanding feature reuse is essential for interpreting the shared backbone's role in the multi-task architecture.
  - Quick check question: Why are early layers of a CNN pre-trained on ImageNet often considered generic feature extractors (edges, textures) suitable for transfer to disparate domains?

## Architecture Onboarding

- **Component map:** Stable Diffusion + LoRA (generation) -> synthetic porcelain images -> MobileNetV3-Large backbone (shared feature extractor) -> 4 task-specific heads (Dynasty, Kiln, Glaze, Type)
- **Critical path:** The prompt design and LoRA training are the most critical steps. Poor prompts yield low-fidelity images; a poorly trained LoRA adapter produces anachronistic artifacts. These directly determine the quality of the augmentation data.
- **Design tradeoffs:**
  - **Fidelity vs. Diversity:** LoRA weight (set to 0.4) balances adherence to porcelain domain vs. retaining generative diversity.
  - **Task-Specific Augmentation:** The system targets glaze-type combinations, boosting minority classes for Type (+5.5%) but inadvertently hurting Glaze (-3.5%) due to texture limitations.
  - **Mixed Training Ratio:** A 90:10 real-to-synthetic ratio was found to be a sweet spot; 95:5 showed negligible gains.
- **Failure signatures:**
  - **Anachronistic combinations:** Generated images mixing features from incompatible dynasties or kilns.
  - **Texture smoothing:** Over-smoothed glaze surfaces lacking authentic crackle patterns or depth.
  - **Performance degradation:** A drop in F1-macro for any task after adding synthetic data indicates negative distribution shift.
- **First 3 experiments:**
  1.  **Baseline Validation:** Train the MobileNetV3 multi-task model on the real-only augmented dataset (25,877 images) to establish performance benchmarks for all four tasks.
  2.  **Synthetic Quality Ablation:** Generate a small synthetic set (e.g., 100 images) with and without the LoRA adapter. Compare FID scores and conduct manual inspection to confirm the adapter's value before full-scale generation.
  3.  **Mixing Ratio Test:** Train the model with 5% (570 images) and 10% (2,500 images) synthetic data. Evaluate on a held-out real test set to find the augmentation threshold and identify task-specific benefits or harms.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can texture-preserving generative architectures or task-specific LoRA adapters mitigate the performance degradation observed in texture-dependent tasks like glaze classification?
  - Basis: [explicit] Section 5.3 suggests exploring "task-adaptive generation" using separate LoRA adaptations for distinct features and "texture-preserving architectures like VAE" to address the smoothing bias of diffusion models.
  - Why unresolved: The current diffusion approach consistently degraded glaze classification performance (−3.5% F1-macro) because the denoising process smooths fine-grained surface details required for this specific task.
  - What evidence would resolve it: A comparative experiment showing that a texture-specialized LoRA or VAE-based generator maintains or improves glaze classification F1-scores compared to the Stable Diffusion baseline.

- **Open Question 2:** To what extent does integrating non-textual data (e.g., texture patches, spectral reflectance) into prompts bridge the semantic gap between expert archaeological terminology and visual realization?
  - Basis: [explicit] Section 5.3 proposes "multi-modal prompt enhancement" that combines textual prompts with "texture patches or technical specifications" to resolve the failure of linguistic descriptions to capture surface details.
  - Why unresolved: The study found that even precise archaeological terminology (e.g., "gold wire and iron thread") often failed to translate into accurate visual features, limiting the effectiveness of text-only prompt engineering.
  - What evidence would resolve it: Demonstrating that multi-modal prompts yield higher fidelity in generated textures (measured by expert evaluation or feature extraction metrics) than text-only prompts for the same rare categories.

- **Open Question 3:** Can reinforcement learning or adaptive algorithms effectively determine the optimal synthetic-to-real data ratio for individual tasks within a multi-task learning framework?
  - Basis: [explicit] Section 5.3 identifies "dynamic proportion optimization" as a future direction, suggesting "adaptive algorithms" to balance multi-objective performance rather than using fixed global ratios.
  - Why unresolved: The fixed 10% synthetic ratio improved Type (+4.4%) and Dynasty (+3.9%) tasks but harmed Glaze (−3.5%), proving that a single mixing proportion is suboptimal for multi-task learning.
  - What evidence would resolve it: An automated system that assigns different synthetic data proportions to different task branches and achieves higher average F1-macro scores across all tasks than any fixed-ratio baseline.

- **Open Question 4:** Does fine-tuning language models on domain-specific archaeological corpora improve the interpretation of specialized terminology in generative prompts?
  - Basis: [explicit] Section 5.3 calls for the development of "archaeological language models trained on museum catalogs and scholarly literature" to better capture subtle associations in domain descriptors.
  - Why unresolved: The authors note that standard models struggle with the specific translation of terms like "celadon" or "rabbit hair striation" into accurate visual representations.
  - What evidence would resolve it: Comparing the CLIP-score alignment or classification performance of images generated using a domain-specific LLM versus a general-purpose LLM for the same set of archaeological prompts.

## Limitations

- **Texture generation ceiling:** The approach consistently underperforms for texture-dependent tasks (glaze) due to diffusion models' smoothing bias on high-frequency details.
- **Unknown hyperparameters:** LoRA rank, alpha, and training steps are unspecified, limiting reproducibility and optimization potential.
- **Ambiguous SD version:** Text mentions both Stable Diffusion 2.1 and v1.5, creating potential reproducibility confusion.

## Confidence

- **High Confidence:** Multi-task architecture design, minority class performance improvements (+5.5% for Type), and the 10% synthetic data threshold effect are well-supported by ablation results.
- **Medium Confidence:** The distributional alignment mechanism and task-specific effectiveness claims are reasonable but require further validation across different artifact domains.
- **Low Confidence:** The hierarchical prompt-to-feature mapping mechanism lacks sufficient corpus support, and the failure mode explanations for glaze texture degradation remain speculative without systematic texture quality assessment.

## Next Checks

1. **Texture Quality Assessment:** Implement a dedicated texture analysis pipeline (e.g., GLCM or deep texture features) to quantify why synthetic glaze images fail, comparing them against real samples across crack patterns, color gradients, and surface irregularities.
2. **Cross-Domain Generalization:** Apply the same LoRA fine-tuning and classification pipeline to a different archaeological dataset (e.g., ancient pottery or coins) to test whether the structural-feature advantage generalizes beyond Chinese porcelain.
3. **Generation Parameter Sensitivity:** Systematically vary LoRA rank, weight, and generation sampling parameters (CFG scale, seed diversity) to identify whether texture quality can be improved through better generative control, or if this represents an inherent model limitation.