---
ver: rpa2
title: 'Learning Along the Arrow of Time: Hyperbolic Geometry for Backward-Compatible
  Representation Learning'
arxiv_id: '2506.05826'
source_url: https://arxiv.org/abs/2506.05826
tags:
- hyperbolic
- learning
- embeddings
- space
- compatibility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Backward-compatible representation learning aims to ensure that
  updated embedding models can seamlessly work with embeddings generated by older
  versions without requiring costly reprocessing. This paper introduces Hyperbolic
  Backward-Compatible Training (HBCT), a novel framework that leverages hyperbolic
  geometry to align old and new models while accounting for representation uncertainty.
---

# Learning Along the Arrow of Time: Hyperbolic Geometry for Backward-Compatible Representation Learning

## Quick Facts
- **arXiv ID**: 2506.05826
- **Source URL**: https://arxiv.org/abs/2506.05826
- **Reference count**: 39
- **Primary result**: HBCT achieves CMC@1 improvement of 21.4% and mAP of 44.8% over best Euclidean baseline for backward-compatible retrieval

## Executive Summary
Backward-compatible representation learning ensures updated embedding models can work seamlessly with embeddings generated by older versions without costly reprocessing. This paper introduces Hyperbolic Backward-Compatible Training (HBCT), a framework that leverages hyperbolic geometry to align old and new models while accounting for representation uncertainty. HBCT employs entailment cones to constrain new embeddings within a region defined by old embeddings and uses a robust hyperbolic contrastive loss that dynamically adjusts based on uncertainty levels. Experiments demonstrate significant improvements in backward compatibility while maintaining new model performance.

## Method Summary
HBCT operates by lifting embeddings into hyperbolic space and constraining updated embeddings to lie within the entailment cone of old ones. The method uses a hybrid Euclidean-hyperbolic encoder where the hyperbolic component is trained with three losses: base classification loss, entailment cone constraint loss, and uncertainty-weighted contrastive alignment loss. The entailment cone defines a permissible region where new embeddings can "descend" from old ones, with wider cones for uncertain old embeddings. The uncertainty-weighted RINCE-based contrastive loss emphasizes alignment for high-quality old embeddings while downweighting noisy pairs. This approach enables new queries to retrieve from old galleries directly without reprocessing.

## Key Results
- CMC@1 improves by 21.4% and mAP by 44.8% compared to best Euclidean baseline
- Maintains new model performance (P_up) while significantly improving compatibility (P_com)
- Outperforms baselines across extended-class, extended-data, new-architecture, and combined scenarios
- Demonstrates resistance to degradation in sequential update scenarios

## Why This Works (Mechanism)

### Mechanism 1: Entailment Cone Constraint for Controlled Evolution
Constraining new embeddings within the entailment cone of old embeddings maintains backward compatibility while allowing model evolution, with wider cones for uncertain old embeddings. The half-aperture formula `aper(h_o) = sin^(-1)(2ε√K ||h_o,space||)` is inversely proportional to the embedding norm—low-norm (high-uncertainty) old embeddings produce wider cones, granting the new model more freedom to refine poor representations while tightly constraining refinement of confident ones.

### Mechanism 2: Uncertainty-Weighted Robust Contrastive Alignment
A RINCE-based contrastive loss with uncertainty-dependent weighting emphasizes alignment for high-quality old embeddings while downweighting noisy pairs. The weighting function `q(h_o) = Uncertainty(h_o) = 1 - (1/√K) tanh(√K ||z||)` causes gradient magnitude to decrease for noisy pairs and increase for clean pairs, effectively preventing the new model from being forced to reconstruct bad old embeddings.

### Mechanism 3: Hyperbolic Uncertainty via Embedding Norm
Hyperbolic geometry provides "free" uncertainty estimation along a time-like dimension without additional model components. In the Lorentz model, uncertainty is measured as `1 - (1/√K) tanh(√K ||z||)`, approaching 0 for high-norm embeddings (confident, far from origin) and 1 for low-norm embeddings (uncertain, near origin). This emerges from the geometry rather than requiring separate uncertainty heads.

## Foundational Learning

- **Riemannian Manifolds and Hyperbolic Geometry**
  - Why needed here: The entire method operates on the Lorentz hyperboloid; understanding geodesic distance, exponential/logarithmic maps, and why negative curvature enables exponential volume growth is essential
  - Quick check question: Can you explain why hyperbolic space has exponential volume growth with radius and why this matters for hierarchical or uncertainty-structured data?

- **Contrastive Learning and InfoNCE**
  - Why needed here: HBCT modifies RINCE (itself a variant of InfoNCE) for uncertainty-aware alignment; understanding the base formulation clarifies what the modification changes
  - Quick check question: Given a batch with one positive pair and N negative pairs, write out the InfoNCE loss and explain what happens to gradients when the positive pair distance is large

- **Backward-Compatible Representation Learning**
  - Why needed here: The problem framing—new model queries must retrieve from old model's indexed gallery without reprocessing—is non-obvious and motivates all design choices
  - Quick check question: In a production retrieval system with 1B indexed vectors, why might re-embedding on model update be prohibitive? What tradeoffs does backward compatibility introduce?

## Architecture Onboarding

- **Component map**: Input x → Euclidean encoder (ResNet/ViT) → z ∈ R^d → [0, z] ∈ R^(d+1) (tangent space at origin) → expm_0([0, z]) → h ∈ L^d (Lorentz hyperboloid) → Hyperbolic MLR classifier (base task)

- **Critical path**:
  1. Train old model with hyperbolic encoder + Hyperbolic MLR on D_old
  2. Store old embeddings and their uncertainty values for gallery
  3. Initialize new model (may differ in architecture, e.g., ResNet→ViT)
  4. Train new model with: L = L_base + λ(L_entail + L_contrast)
  5. New queries can now retrieve from old gallery directly

- **Design tradeoffs**:
  - **Clipping threshold ζ**: Paper uses ζ_o = 1.0 for old model, ζ_n = ζ_o + 0.2 for new model. Too low → restricts evolution; too high → numerical instability as norms grow over many updates
  - **Curvature K**: Fixed at 1.0 per prior work; ablation shows K ∈ [0.5, 1.0] is stable, but learnable K hurts compatibility
  - **Alignment weight λ**: Paper uses λ = 0.3 universally. Higher values improve compatibility but may harm new model performance (P_up drops)
  - **Distance function**: Geodesic, squared Lorentz, and Lorentz inner product all perform similarly; geodesic is canonical

- **Failure signatures**:
  - **New model P_up drops significantly**: λ too high or entailment cone over-constraining; reduce λ or increase ζ_n
  - **Cross-retrieval P_com near zero**: Check that old embeddings are properly loaded and uncertainty is being computed (not all zeros)
  - **Training instability (NaN losses)**: Norms exploding; check clipping is applied, reduce learning rate, or verify exponential map implementation

- **First 3 experiments**:
  1. **Reproduce extended-class scenario on CIFAR100**: Train ResNet18 on first 50 classes (old), then full 100 classes (new) with HBCT. Verify CMC@1 P_com ≈ 0.5 and that ablation (without entailment) drops to ≈0.44
  2. **Ablate uncertainty weighting**: Replace q(h_o) = Uncertainty(h_o) with fixed q values (0.1, 0.5, 0.9) to confirm that dynamic weighting provides benefit over static
  3. **Test norm-uncertainty correlation on your domain**: Train old model, embed held-out classes, and plot uncertainty vs. norm. If correlation is weak, the mechanism may not transfer

## Open Questions the Paper Calls Out

- **Self-supervised and multi-modal adaptation**: Investigating HBCT's effectiveness in self-supervised learning or multi-modal settings is an interesting research direction
- **Automatic norm growth management**: The current heuristic for increasing clipping thresholds may lead to instability and calls for a more principled approach
- **Optimal adaptive curvature strategy**: Why does learnable curvature degrade backward compatibility, and is there an optimal adaptive curvature strategy?

## Limitations
- Uncertainty-norm relationship in hyperbolic space is validated on CIFAR100 but not shown for other domains
- RINCE-based uncertainty weighting is novel to this setting—its noise-robustness transfer from InfoNCE is theoretically justified but lacks ablation
- Entailment cone's mathematical grounding is sound, but the half-aperture formula's inverse norm relationship is assumed rather than proven optimal

## Confidence
- **High**: Empirical performance gains (CMC@1 +21.4%, mAP +44.8% vs best Euclidean baseline); core hyperbolic encoder implementation and training pipeline; sequential update resistance
- **Medium**: Uncertainty-norm correlation mechanism; RINCE uncertainty weighting effectiveness; geometric validity of entailment cone constraints
- **Low**: Optimal hyperparameter sensitivity (λ, ζ, K); robustness to domain shift where norm-uncertainty relationship breaks; scalability to very large embedding galleries

## Next Checks
1. **Ablate the uncertainty weighting scheme**: Replace dynamic q(h_o) = Uncertainty(h_o) with fixed values (0.1, 0.5, 0.9) to isolate whether the hyperbolic uncertainty measure adds value beyond static weighting
2. **Validate norm-uncertainty correlation on your target domain**: Train an old model on your data, embed held-out classes, and plot ||z|| vs. Uncertainty(z). Weak correlation suggests the mechanism won't transfer
3. **Test entailment cone aperture sensitivity**: Vary the scaling factor in aper(h_o) = sin^(-1)(2ε√K ||h_o,space||) to confirm that wider cones for uncertain embeddings improve compatibility without harming new model performance