---
ver: rpa2
title: 'Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained
  Encoders and Decoders for Multimodal Machine Translation'
arxiv_id: '2504.18012'
source_url: https://arxiv.org/abs/2504.18012
tags:
- translation
- pre-trained
- visual
- language
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates the role of pre-trained
  encoders and decoders in multimodal machine translation (MMT). The authors conduct
  experiments across English-German and English-French translation tasks using Multi30K
  and CoMMuTE datasets, testing various combinations of pre-trained components under
  different training strategies (freezing, fine-tuning, or training from scratch).
---

# Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained Encoders and Decoders for Multimodal Machine Translation

## Quick Facts
- **arXiv ID:** 2504.18012
- **Source URL:** https://arxiv.org/abs/2504.18012
- **Reference count:** 12
- **Primary result:** Decoder-side pre-training consistently improves MMT translation quality, while encoder-side benefits depend on visual-text alignment quality

## Executive Summary
This study systematically investigates the role of pre-trained encoders and decoders in multimodal machine translation (MMT). The authors conduct experiments across English-German and English-French translation tasks using Multi30K and CoMMuTE datasets, testing various combinations of pre-trained components under different training strategies (freezing, fine-tuning, or training from scratch). Results show that pre-trained decoders consistently improve translation quality across all metrics, while pre-trained encoders provide variable benefits depending on visual-text alignment quality. Decoder-only models like Qwen2.5 and LLaMA3.2 demonstrate particular robustness in handling visual information as contextual cues.

## Method Summary
The paper evaluates MMT systems by testing various pre-trained components across encoder-decoder and decoder-only architectures. Experiments use Multi30K and CoMMuTE datasets for English-German and English-French translation, with frozen CLIP visual encoders and text components initialized from pre-trained weights (T5, mBART, Qwen, LLaMA) or trained from scratch. The study tests three training strategies: freezing pre-trained components, fine-tuning them, or training from scratch, and evaluates performance under both aligned and randomly shuffled image-text pairs to assess sensitivity to alignment quality.

## Key Results
- Pre-trained decoders consistently yield more fluent and accurate outputs across all datasets and metrics
- Pre-trained encoders show varied effects depending on visual-text alignment quality, with performance degrading on misaligned data
- Decoder-only models like Qwen2.5 and LLaMA3.2 exhibit robustness to misaligned visual information, treating images as contextual cues
- Current MMT systems show 40-50+ BLEU point improvements when using pre-trained components versus training from scratch

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-trained decoders provide consistent, architecture-agnostic improvements in MMT generation quality.
- **Mechanism:** Large-scale language pre-training builds robust autoregressive generation capabilities that transfer across modalities. The decoder learns linguistic patterns, fluency priors, and cross-lingual mappings that remain useful even when visual features are introduced as auxiliary context. This creates a "memory reviving" effect where pre-trained knowledge rapidly activates during early training epochs.
- **Core assumption:** The linguistic knowledge from pre-training transfers to multimodal settings without catastrophic interference from visual features.
- **Evidence anchors:**
  - [abstract] "pre-trained decoders consistently yield more fluent and accurate outputs"
  - [Section 4.3] "models like Qwen and LLaMA achieve consistently higher scores across all datasets and metrics... Their autoregressive generation capabilities, combined with deep self-attention mechanisms, enable them to model complex linguistic patterns more effectively"
  - [corpus] Related work on LLaVA-NeuMT confirms selective neuron modulation for multilingual MMT, suggesting decoder-side adaptation is tractable
- **Break condition:** If visual features are forced through heavy encoder-side fusion that creates gradient conflicts with decoder pre-training, benefits may diminish. Evidence shows this when encoder-decoder models with pre-trained encoders underperform on misaligned data.

### Mechanism 2
- **Claim:** Pre-trained encoder benefits are contingent on visual-text alignment quality and can degrade performance when alignment is poor.
- **Mechanism:** Pre-trained encoders with strong visual encoding capacity integrate image semantics directly into textual representations. When image-text pairs are semantically aligned, this enriches the representation space. When misaligned, irrelevant visual signals contaminate the encoding, misleading downstream generation.
- **Core assumption:** Visual and textual semantic spaces can be meaningfully aligned; when they cannot, the encoder lacks dynamic filtering capacity.
- **Evidence anchors:**
  - [abstract] "pre-trained encoders show varied effects depending on the quality of visual-text alignment"
  - [Section 4.5] "models utilizing pre-trained encoders (e.g., T5 and mBART) exhibit the most severe drops in performance... COMET scores drop by as much as 5–6 points" under shuffled alignment
  - [corpus] Dual-branch Prompting for MMT paper notes sensitivity to "irrelevant visual noise," corroborating encoder-side vulnerability
- **Break condition:** High-capacity visual encoders paired with noisy or semantically unrelated images will actively harm translation quality.

### Mechanism 3
- **Claim:** Decoder-only architectures treat visual input as contextual prompts rather than fused semantic representations, providing more robust handling of ambiguous or misaligned visual cues.
- **Mechanism:** In decoder-only models, visual features serve as auxiliary context tokens that the attention mechanism can weight dynamically. This allows the model to upweight visual information when it resolves ambiguity (as in CoMMuTE) and downweight it when it is redundant or conflicting, without the binding constraints of encoder-side fusion.
- **Core assumption:** The decoder's self-attention and context modeling are flexible enough to treat visual tokens as optional rather than obligatory context.
- **Evidence anchors:**
  - [Section 4.4] "decoder-only models like Qwen2.5 and LLaMA3.2 exhibit the opposite behavior on the CoMMuTE dataset: removing image inputs results in a noticeable drop in performance... indicating that, in complex or ambiguous scenarios, images may serve as useful anchors"
  - [Section 4.5] "models equipped with pre-trained language decoders (e.g., Qwen and LLaMA) are generally more robust... their scores remain relatively stable"
  - [corpus] No directly contradictory findings; GIIFT paper explores image-free MMT, indirectly supporting that visual context is optional for strong decoders
- **Break condition:** If visual tokens are concatenated without positional or modality markers that distinguish them from text, the decoder may fail to learn selective attention, leading to over-reliance or under-utilization.

## Foundational Learning

- **Concept: Transformer Encoder-Decoder vs. Decoder-Only Architectures**
  - **Why needed here:** The paper's central finding is an asymmetry between encoder and decoder pre-training effects. Understanding how each component processes input (bidirectional encoding vs. autoregressive decoding) is prerequisite to interpreting results.
  - **Quick check question:** Can you explain why a decoder-only model might handle auxiliary visual tokens differently than an encoder that fuses visual features into source representations?

- **Concept: Cross-Modal Alignment and Vision-Language Pre-training**
  - **Why needed here:** Results show encoder benefits depend on alignment quality. Understanding CLIP-style vision-language alignment explains why frozen CLIP features may or may not integrate well with pre-trained text encoders.
  - **Quick check question:** What happens to a pre-trained text encoder when you concatenate poorly aligned visual features to its input embeddings?

- **Concept: Fine-Tuning Strategies (Freezing, Full Fine-Tuning, Training from Scratch)**
  - **Why needed here:** The paper systematically tests these strategies. Understanding the trade-offs (catastrophic forgetting vs. adaptation capacity) is essential for interpreting performance differences.
  - **Quick check question:** Why might freezing a pre-trained visual encoder while fine-tuning a text encoder create a modality gap?

## Architecture Onboarding

- **Component map:** CLIP-ViT (frozen) -> Text Encoder (T5/mBART/LLaMA embeddings) -> Text Decoder (Transformer/T5/mBART/LLaMA) -> Translation output

- **Critical path:**
  1. Freeze CLIP visual encoder to ensure stable image representations
  2. Initialize text encoder/decoder from pre-trained weights (not random)
  3. Fine-tune text components on paired image-text-translation data
  4. Evaluate on both aligned (Multi30K) and contrastive (CoMMuTE) test sets

- **Design tradeoffs:**
  - **Encoder-decoder (mBART/T5):** Strong on well-aligned data, vulnerable to visual noise, requires careful fusion design
  - **Decoder-only (Qwen/LLaMA):** More robust to misalignment, treats vision as context, may underutilize visual signals on simple tasks
  - **Pre-trained vs. scratch:** Pre-training provides 40-50+ BLEU point improvements; scaling Transformer alone insufficient

- **Failure signatures:**
  - **Hallucinated compounds:** mBART and LLaMA generate nonsense tokens (e.g., "Dichteberfleck") when visual features mislead (see Figure 4 case study)
  - **Performance collapse on shuffled images:** Pre-trained encoders drop 5-6 COMET points when image-text pairs mismatched
  - **Flattened intent:** T5 retains nouns but modality errors ("have to" → "can") suggest visual context disrupts fine-grained semantics

- **First 3 experiments:**
  1. **Baseline ablation:** Train standard Transformer-Small/Base/Large from scratch on Multi30K to establish non-pre-trained baseline (expect 43-45 BLEU on En-De Test2016 per Table 2)
  2. **Decoder-only vs. encoder-decoder with same pre-training:** Compare Qwen2.5-0.5B as decoder-only vs. its embeddings fed to Transformer decoder; measure gap on CoMMuTE to quantify robustness difference
  3. **Alignment sensitivity test:** Evaluate all models with shuffled image-text pairs; models with pre-trained encoders should show largest drops, confirming encoder-side vulnerability (per Figure 3)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can dynamic fusion mechanisms be designed to regulate visual involvement in real-time, preventing the semantic noise and performance degradation observed in pre-trained encoder-decoder models?
- **Basis in paper:** [Inferred] Section 2.3 notes the lack of a "unified paradigm for dynamically regulating the degree of visual involvement," while Section 4.4 reveals that visual features often degrade performance by introducing noise in encoder-decoder architectures.
- **Why unresolved:** The study demonstrates that static integration of visual features is unreliable, but does not propose or test methods for adaptive filtering or gating of visual information based on alignment quality.
- **What evidence would resolve it:** Experiments integrating attention-based gating or confidence-weighting mechanisms for visual features, showing consistent performance gains on both aligned (Multi30k) and misaligned/shuffled test sets.

### Open Question 2
- **Question:** Can pre-training objectives or architectural modifications be developed to transfer the robustness of decoder-only models (like LLaMA/Qwen) to standard encoder-decoder frameworks (like mBART)?
- **Basis in paper:** [Explicit] Section 4.5 explicitly highlights a divergence where pre-trained encoders suffer severe degradation from mismatched inputs, whereas pre-trained decoders are robust, suggesting a need to bridge this structural gap.
- **Why unresolved:** The paper identifies this asymmetry as a fundamental challenge but leaves the specific mechanisms required to improve encoder robustness as an open area for future architecture design.
- **What evidence would resolve it:** Comparative results showing that modified encoder-decoder models (e.g., via denoising pre-training or robust attention layers) maintain stable COMET scores even when evaluated on randomly shuffled image-text pairs.

### Open Question 3
- **Question:** What are the optimal "continuing learning" strategies (e.g., learning rate schedules, adapter integration) to stabilize the early-stage fluctuations inherent in the "memory reviving" phase of large pre-trained models?
- **Basis in paper:** [Explicit] Section 4.2 observes that large models (mBART, LLaMA) show "metric level fluctuations" during the initial epochs as they adapt to multimodal settings, indicating instability in the activation of pre-trained memory.
- **Why unresolved:** While the paper documents the fluctuation, it focuses on the final convergence performance rather than optimizing the fine-tuning trajectory to mitigate this instability.
- **What evidence would resolve it:** Ablation studies comparing various fine-tuning strategies (e.g., gradual unfreezing vs. adapter tuning) that demonstrate reduced variance in loss/accuracy during the first 1-2 epochs without compromising final convergence speed.

## Limitations
- The study's findings on decoder-side pre-training benefits are robust across datasets but the observed encoder-side variability raises concerns about generalizability to other visual domains
- The analysis relies on CLIP features which may not capture fine-grained visual semantics relevant to translation (e.g., cultural objects, nuanced scenes)
- The evaluation focuses on standard MMT datasets; performance on real-world multimodal content with diverse visual styles and quality remains untested

## Confidence

**High confidence:** Decoder pre-training consistently improves translation quality regardless of architecture. The empirical evidence (40-50+ BLEU point gains over non-pre-trained models) is strong and reproducible across experiments.

**Medium confidence:** Encoder pre-training benefits depend on visual-text alignment quality. While the 5-6 COMET point drop on shuffled data is clear, the threshold for "poor alignment" and whether this generalizes beyond the studied datasets needs validation.

**Medium confidence:** Decoder-only models handle misaligned visual information more robustly. The CoMMuTE performance gap suggests this, but the mechanism (contextual vs. fused integration) requires further ablation studies.

## Next Checks

1. **Cross-domain visual robustness test:** Evaluate the same model configurations on non-standard visual domains (e.g., medical images, technical diagrams, or user-generated content) to test whether decoder-side robustness generalizes beyond natural scene images.

2. **Fine-grained alignment quality analysis:** Conduct controlled experiments with varying degrees of visual-text semantic similarity (e.g., partially related vs. completely unrelated images) to map the precise relationship between alignment quality and encoder pre-training benefits.

3. **Modality-aware fusion architecture test:** Implement explicit modality markers or gated fusion mechanisms in encoder-decoder models to determine whether the sensitivity to visual noise can be mitigated through architectural design rather than decoder-side reliance.