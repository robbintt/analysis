---
ver: rpa2
title: "Aplica\xE7\xE3o de Large Language Models na An\xE1lise e S\xEDntese de Documentos\
  \ Jur\xEDdicos: Uma Revis\xE3o de Literatura"
arxiv_id: '2504.00725'
source_url: https://arxiv.org/abs/2504.00725
tags:
- para
- llms
- modelos
- dicos
- como
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic literature review analyzed the application of Large
  Language Models (LLMs) in legal document analysis, focusing on prompt engineering
  techniques. The study reviewed 17 articles published between 2020-2024 from SCOPUS
  and Springer databases.
---

# Aplicação de Large Language Models na Análise e Síntese de Documentos Jurídicos: Uma Revisão de Literatura

## Quick Facts
- arXiv ID: 2504.00725
- Source URL: https://arxiv.org/abs/2504.00725
- Reference count: 2
- Primary result: Systematic review of 17 articles showing LLMs' potential for legal document analysis with effectiveness of prompt engineering techniques

## Executive Summary
This systematic literature review examines the application of Large Language Models (LLMs) in legal document analysis and synthesis, focusing on prompt engineering techniques. The study analyzed 17 articles published between 2020-2024 from SCOPUS and Springer databases, identifying key trends in model selection, evaluation methods, and performance outcomes. Results demonstrate that both general-purpose models like GPT-4 and specialized legal models show significant potential for tasks including summarization, classification, and information retrieval, though challenges with hallucinations and biases remain.

## Method Summary
The authors conducted a systematic literature review following PRISMA guidelines, searching SCOPUS and Springer databases using the string "large language models" AND "legal" with filters for peer-reviewed articles published between 2020-2024. After screening 66 articles and removing duplicates, 17 articles were selected for full review. Data extraction focused on model types, prompt engineering techniques, evaluation metrics, and identified challenges. The review employed qualitative synthesis and thematic analysis to identify patterns and trends across the selected studies.

## Key Results
- GPT-4, BERT, Llama 2, and specialized models like Legal-Pegasus were widely used across reviewed studies
- Few-shot Learning, Zero-shot Learning, and Chain-of-Thought prompting techniques proved effective for legal tasks
- Specialized legal models generally outperformed general-purpose LLMs, but improvements in prompt engineering are needed for greater accuracy and reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specialized legal models (Legal-Pegasus, Legal-BERT, Sabiá) generally outperform general-purpose LLMs on legal text processing tasks.
- Mechanism: Pre-training or fine-tuning on legal corpora embeds domain-specific terminology, document structure patterns, and legal reasoning schemas into model weights, reducing the inference burden on prompting alone.
- Core assumption: Legal language exhibits systematic lexical and structural regularities distinct from general text.
- Evidence anchors:
  - [abstract] "Specialized legal models generally outperformed general-purpose LLMs"
  - [section] "modelos treinados especificamente para o domínio jurídico apresentando maior precisão na classificação de textos, recuperação de jurisprudência e análise de contratos"
  - [corpus] Neighbor paper "Continual Pre-Training is (not) What You Need in Domain Adaption" directly examines domain adaptation effectiveness for legal LLMs.
- Break condition: When cross-jurisdictional tasks require reasoning beyond the training corpus, or when specialized training data is insufficient for the target legal system.

### Mechanism 2
- Claim: Structured prompt engineering techniques (Few-shot, Zero-shot, Chain-of-Thought) improve LLM performance on legal reasoning and generation tasks.
- Mechanism: Providing task exemplars (Few-shot), explicit instructions (Zero-shot), or reasoning scaffolds (Chain-of-Thought) constrains the output space and guides the model toward legally coherent responses, reducing hallucination frequency.
- Core assumption: Legal tasks benefit from explicit reasoning traces and contextual grounding that prompting can elicit.
- Evidence anchors:
  - [abstract] "techniques such as Few-shot Learning, Zero-shot Learning, and Chain-of-Thought prompting proving effective"
  - [section] "A formulação adequada de prompts pode reduzir significativamente as alucinações e viéses, tornando os modelos mais confiáveis"
  - [corpus] Weak explicit evidence on prompting mechanics; neighbor papers focus on agents and benchmarks rather than prompting methodology.
- Break condition: When task complexity exceeds the context window capacity for exemplars, or when legal nuances require domain knowledge absent from the base model.

### Mechanism 3
- Claim: Hybrid evaluation combining automated metrics (ROUGE, BLEU, F1) with human expert assessment provides more reliable validation for legal NLP systems than either approach alone.
- Mechanism: Automated metrics capture surface-level similarity and structural accuracy, while legal experts assess substantive correctness, jurisdictional appropriateness, and reasoning validity—dimensions metrics cannot evaluate.
- Core assumption: Legal output quality has both quantifiable (lexical overlap) and qualitative (reasoning soundness) components.
- Evidence anchors:
  - [abstract] "Evaluation metrics included ROUGE, BLEU, F1-score, and human assessment"
  - [section] Table 4 shows multiple studies using "avaliação com estudantes de Direito" and "avaliação por estudantes e especialistas jurídicos"
  - [corpus] Limited explicit discussion; neighbor "A Reasoning-Focused Legal Retrieval Benchmark" touches on evaluation methodology but not hybrid approaches.
- Break condition: When human evaluation scales poorly for large datasets, or when expert availability is constrained by jurisdictional specialization requirements.

## Foundational Learning

- Concept: Transformer Self-Attention Mechanism
  - Why needed here: All reviewed models (GPT-4, BERT, Llama 2, Legal-Pegasus) are transformer-based; understanding attention helps diagnose context handling limitations in long legal documents.
  - Quick check question: Given a 10-page legal contract, how does self-attention determine which clauses most influence the model's summary output?

- Concept: Transfer Learning vs. Fine-Tuning
  - Why needed here: The paper distinguishes general-purpose models from specialized legal models (Legal-BERT, Sabiá) based on domain adaptation strategy.
  - Quick check question: If adapting Llama 2 for Brazilian legal NER using LeNER-Br, what components would you freeze versus update during fine-tuning?

- Concept: Prompt Engineering Paradigms (Zero-shot, Few-shot, Chain-of-Thought)
  - Why needed here: These three techniques are identified as the primary methods for optimizing LLM performance in legal contexts.
  - Quick check question: For a legal clause classification task with 50 labeled examples available, which prompting strategy would likely yield higher precision and why?

## Architecture Onboarding

- Component map: Model Layer -> Prompting Layer -> Evaluation Layer -> Task Modules
- Critical path: 1. Define task type and identify relevant legal domain/jurisdiction 2. Select base model (specialized preferred for terminology-heavy tasks; general for cross-domain reasoning) 3. Design prompt strategy (start Zero-shot, add Few-shot exemplars if performance inadequate, escalate to CoT for complex reasoning) 4. Establish evaluation protocol (automated metrics for iteration, human review for deployment validation) 5. Implement hallucination safeguards (citation verification, confidence thresholds)
- Design tradeoffs:
  - General vs. Specialized models: Flexibility/cost-efficiency vs. domain accuracy
  - Zero-shot vs. Few-shot: Implementation simplicity vs. performance gains (1%-50% improvement per Ghosh et al.)
  - Automated vs. Human evaluation: Scalability vs. legal validity assessment
  - Context window vs. Document length: Truncation/chunking introduces information loss
- Failure signatures:
  - Hallucination: Plausible-sounding legal citations or precedents that do not exist
  - Metric-human divergence: High ROUGE/BLEU scores but expert-rated low legal accuracy
  - Jurisdictional confusion: Model applies wrong legal framework (e.g., Indian case law to Brazilian context)
  - F1 plateau: NER performance caps around 51% (as observed with Sabiá on Brazilian corpora) despite prompt optimization
- First 3 experiments:
  1. Model benchmark: Compare GPT-4, Llama-2, and Legal-BERT on identical legal summarization task (Indian court decisions) using Few-shot prompting; evaluate with ROUGE-L and 2 legal expert reviewers.
  2. Prompt ablation: Test Zero-shot, 3-shot, and Chain-of-Thought prompting on legal document classification; measure F1-score delta and analyze error distribution across prompt strategies.
  3. Domain adaptation: Fine-tune Llama-2-7B on LeNER-Br corpus for NER; compare against Sabiá baseline using F1-micro, documenting training data requirements and performance ceiling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can prompt engineering and fine-tuning strategies be optimized to effectively mitigate hallucinations and ensure the reliability required for large-scale legal implementation?
- Basis: [explicit] The conclusion states that challenges such as hallucinations hinder large-scale implementation and emphasizes the "need to improve prompt engineering strategies to ensure greater accuracy and reliability."
- Why unresolved: While techniques like Chain-of-Thought help, current models still generate plausible but legally incorrect information, making them risky for automation without human oversight.
- What evidence would resolve it: Development of a prompting framework or model architecture that demonstrably reduces hallucination rates to near-zero on complex legal reasoning benchmarks.

### Open Question 2
- Question: What are the most effective methods for developing specialized legal models for specific jurisdictions and languages compared to using general-purpose LLMs?
- Basis: [explicit] The authors conclude that future research must explore "the development of more specialized legal models for different legal systems and languages."
- Why unresolved: The review highlights that while specialized models (e.g., Legal-Pegasus) often outperform general ones, most research focuses on English or dominant jurisdictions, leaving gaps for other legal systems.
- What evidence would resolve it: Comparative studies showing that fine-tuned, jurisdiction-specific models consistently outperform general-purpose models (like GPT-4) in non-English or low-resource legal contexts.

### Open Question 3
- Question: Can automated evaluation metrics be developed that correlate more strongly with human legal assessment than current metrics like ROUGE and BLEU?
- Basis: [inferred] The paper notes that studies rely on both automated metrics (ROUGE, BLEU) and qualitative human assessment by experts, implying that automated metrics may be insufficient for capturing legal accuracy.
- Why unresolved: Traditional NLP metrics measure lexical overlap, not the semantic truth or legal validity of a summary, necessitating labor-intensive human evaluation.
- What evidence would resolve it: A new evaluation metric that validates semantic correctness and legal reasoning, showing a high statistical correlation with judgments from legal experts.

## Limitations

- Small sample size of 17 articles may not capture full diversity of LLM applications in legal domains
- Limited quantitative data on hallucination frequency across different prompting strategies or model types
- Insufficient evidence on cross-jurisdictional adaptation challenges and solutions

## Confidence

**High Confidence** (supported by direct evidence from the reviewed corpus):
- Specialized legal models generally outperform general-purpose LLMs on domain-specific tasks
- Few-shot, Zero-shot, and Chain-of-Thought prompting are effective techniques
- Hybrid evaluation combining automated metrics with human assessment is recommended

**Medium Confidence** (inferred from literature trends with limited direct evidence):
- Model hallucinations and biases remain significant challenges
- Improvements in prompt engineering and fine-tuning are needed for greater accuracy
- Specialized models show better performance on classification, retrieval, and analysis tasks

**Low Confidence** (minimal direct evidence in the reviewed corpus):
- Specific quantitative performance differences between prompt strategies
- The effectiveness of particular hallucination mitigation techniques
- Cross-jurisdictional transfer challenges and solutions

## Next Checks

1. **Performance Benchmark Validation**: Conduct controlled experiments comparing GPT-4, Llama-2, and Legal-BERT on identical legal summarization tasks using consistent Few-shot prompting, measuring both automated metrics (ROUGE-L) and legal expert assessments to verify the claimed superiority of specialized models.

2. **Hallucination Frequency Analysis**: Systematically measure hallucination rates across different prompt strategies (Zero-shot, Few-shot, Chain-of-Thought) on a standardized legal reasoning dataset, documenting citation accuracy and identifying which prompting approaches minimize hallucinated content.

3. **Cross-Jurisdictional Transfer Study**: Test specialized legal models (e.g., models trained on Indian law) on legal tasks from different jurisdictions (e.g., Brazilian law), quantifying performance degradation and identifying the minimum domain adaptation requirements needed for reliable cross-jurisdictional application.