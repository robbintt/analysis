---
ver: rpa2
title: 'Combining Forecasts using Meta-Learning: A Comparative Study for Complex Seasonality'
arxiv_id: '2504.08940'
source_url: https://arxiv.org/abs/2504.08940
tags:
- forecasting
- forecasts
- base
- lstm
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores meta-learning for combining forecasts generated\
  \ by different models, moving beyond simple averaging. The author tests five meta-learners\u2014\
  linear regression, k-nearest neighbors, multilayer perceptron, random forest, and\
  \ long short-term memory\u2014in both global and local learning variants for time\
  \ series with complex seasonality."
---

# Combining Forecasts using Meta-Learning: A Comparative Study for Complex Seasonality

## Quick Facts
- **arXiv ID:** 2504.08940
- **Source URL:** https://arxiv.org/abs/2504.08940
- **Reference count:** 40
- **Primary result:** Meta-learning via Random Forest and LSTM achieved lower error rates than both the best base model and simple averaging methods on electricity load data with triple seasonality.

## Executive Summary
This paper explores meta-learning for combining forecasts from multiple base models, moving beyond simple averaging to achieve improved accuracy. The author tests five meta-learners—linear regression, k-nearest neighbors, multilayer perceptron, random forest, and LSTM—in both global and local learning variants for time series with complex seasonality. Applied to 35 European countries' electricity load data featuring triple seasonality (daily, weekly, yearly), the study demonstrates that all meta-learners outperform simple averaging, with random forest achieving the lowest MAPE (1.52%) and LSTM the lowest MSE (139,667).

## Method Summary
The study uses a stacking approach where meta-learners combine forecasts from 16 diverse base models trained on 2006-2017 electricity load data. For each target hour, the meta-learners are trained on history from January 1, 2018 up to t-1 using either global mode (all available history) or local mode (k-nearest neighbors or seasonal windows). The meta-learners include linear regression, kNN, MLP, random forest, and LSTM, with specific hyperparameters detailed for each. Performance is evaluated on 100 evenly spaced hours from H2 2018 per country using MAPE, MSE, and other error metrics.

## Key Results
- All five meta-learners (LR, kNN, MLP, RF, LSTM) outperformed simple averaging baselines on electricity load data
- Random Forest achieved the lowest MAPE (1.52%) and demonstrated robustness to training set size
- LSTM produced the lowest MSE (139,667) but generated 447 forecasts outside the range of base models
- Local learning strategies did not consistently outperform global learning across all meta-learners

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Non-linear meta-learners capture complex interactions between base forecasts that simple linear averaging misses.
- **Mechanism:** Algorithms like Random Forest and MLP map the vector of base forecasts into a non-linear feature space, allowing dynamic weight assignment based on forecast context rather than static coefficients.
- **Core assumption:** The relationship between base forecasts and the target value is non-linear and pattern-dependent.
- **Evidence anchors:** Meta-learners outperformed linear regression; non-linear models showed higher accuracy in results.
- **Break condition:** If base models are highly correlated and the true relationship is strictly linear, complex meta-learners may overfit without accuracy gains.

### Mechanism 2
- **Claim:** Sequential meta-learners (LSTM) utilize historical states of base forecasts to correct current predictions, enabling extrapolation beyond base model ranges.
- **Mechanism:** LSTM processes sequences of base forecasts using internal cell states, accumulating temporal information to adjust forecasts based on recent base model error trends.
- **Core assumption:** Base model errors exhibit temporal dependencies that can be modeled to improve immediate forecasts.
- **Evidence anchors:** LSTM generated 447 forecasts outside base model ranges; used historical forecast sequences for prediction.
- **Break condition:** If base model errors are strictly random with no temporal autocorrelation, LSTM's memory adds complexity without signal.

### Mechanism 3
- **Claim:** Local learning strategies adapt combination weights to specific seasonal contexts.
- **Mechanism:** For non-recurrent models, this involves selecting k nearest neighbors from history; for LSTMs, selecting training windows matching seasonal phase (e.g., same hour of day/week).
- **Core assumption:** Time series with complex seasonality operate under different regimes where optimal base model combinations change.
- **Evidence anchors:** Local modes used seasonal phase matching; some LSTM variants showed different performance patterns.
- **Break condition:** If training history is short or noisy, local learning may overfit to sparse neighbors, degrading performance.

## Foundational Learning

- **Concept:** **Stacking (Stacked Generalization)**
  - **Why needed here:** This is the architectural backbone of the paper—training a level-1 model on level-0 model predictions to minimize error.
  - **Quick check question:** If you feed raw time series features into the meta-learner instead of base model predictions, are you still doing stacking?

- **Concept:** **Complex Seasonality**
  - **Why needed here:** The dataset exhibits triple seasonality (daily, weekly, yearly), which standard models often fail to handle effectively.
  - **Quick check question:** Why would a model trained on global data struggle with a query point that falls on a holiday if it doesn't use local learning?

- **Concept:** **Bias-Variance Tradeoff in Ensembles**
  - **Why needed here:** The paper compares simple averaging (high bias, low variance) against sophisticated meta-learners (lower bias, potential for lower variance).
  - **Quick check question:** Why does the Random Forest meta-learner exhibit lower sensitivity to training set size compared to Linear Regression?

## Architecture Onboarding

- **Component map:** Base models (16) → Base forecasts → Meta-learner → Final ensemble forecast
- **Critical path:**
  1. Generate 16 distinct forecasts from base models for target time t
  2. Retrieve context (nearest neighbors or seasonal history window for LSTM)
  3. Apply meta-learner to combine 16 inputs into final forecast
- **Design tradeoffs:**
  - Global vs. Local: Global is computationally simpler but may miss specific seasonal dynamics
  - RF vs. LSTM: Choose RF for robustness and MAPE minimization; choose LSTM for MSE minimization and extrapolation capability
  - Hyperparameter Sensitivity: kNN is robust to k; LSTM is highly sensitive to training sequence length
- **Failure signatures:**
  - LSTM produces forecasts wildly outside all base models' ranges (frequent but not consistently accurate)
  - Linear Regression coefficients become unstable if base models are too similar (multicollinearity)
- **First 3 experiments:**
  1. Implement "Mean" and "Median" ensembles to establish accuracy floor (MAPE ~1.82-1.91%)
  2. Train Random Forest on all history vs. k-neighbors to verify global sufficiency
  3. Compare LSTM v1 (recent window) vs. v3 (seasonal phase) to confirm paper's finding that v1 outperforms v3

## Open Questions the Paper Calls Out

- **Question:** Why did LSTM training variants designed for seasonality (v2 and v3) underperform compared to the simpler recent-history variant (v1)?
- **Question:** Under what conditions does LSTM's tendency to extrapolate outside base model ranges result in accuracy gains rather than overfitting?
- **Question:** How do meta-learners' performances generalize to time series domains other than electricity load?
- **Question:** Can advanced ML models tailored for sequential data enhance forecast combining in bagging and boosting scenarios?

## Limitations

- Study relies on a single dataset (ENTSO-E electricity load) with 35 countries, limiting generalizability
- Meta-learners trained separately for each forecast hour may not scale efficiently to real-time applications
- Paper does not address computational complexity differences between methods, particularly LSTM variants
- Does not benchmark against more sophisticated ensemble methods like gradient boosting ensembles

## Confidence

- **High Confidence:** Random Forest outperforming simple averaging and demonstrating robustness to training set size
- **Medium Confidence:** LSTM's superior MSE performance and frequent extrapolation beyond base model ranges
- **Low Confidence:** Claimed superiority of local learning strategies over global learning (mixed results observed)

## Next Checks

1. **Dataset Generalization:** Replicate the meta-learning framework on non-energy time series (e.g., retail sales or traffic data) to validate cross-domain performance
2. **Computational Efficiency Analysis:** Measure and compare training/inference times for each meta-learner, particularly LSTM variants, to assess practical deployment viability
3. **Alternative Ensemble Benchmarking:** Implement and compare against gradient boosting ensembles and dynamic weighting schemes to establish whether meta-learning provides unique advantages beyond traditional ensemble methods