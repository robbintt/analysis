---
ver: rpa2
title: 'ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language
  Models and Tree Search'
arxiv_id: '2506.06524'
source_url: https://arxiv.org/abs/2506.06524
tags:
- games
- game
- puzzlescript
- generated
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ScriptDoctor, a system that uses large language
  models (LLMs) to automatically generate complete PuzzleScript games through iterative
  generation and testing. The system integrates LLM-generated code with compilation
  feedback from the PuzzleScript engine and tree search-based playtesting to create
  functional games.
---

# ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language Models and Tree Search

## Quick Facts
- arXiv ID: 2506.06524
- Source URL: https://arxiv.org/abs/2506.06524
- Reference count: 24
- Key outcome: Few-shot prompting with human-authored games significantly improves code functionality, with reasoning models like o1 and o3-mini outperforming GPT-4o in generating compilable and playable games.

## Executive Summary
This paper presents ScriptDoctor, a system that uses large language models (LLMs) to automatically generate complete PuzzleScript games through iterative generation and testing. The system integrates LLM-generated code with compilation feedback from the PuzzleScript engine and tree search-based playtesting to create functional games. Key results show that few-shot prompting with human-authored games significantly improves code functionality, with models like o1 and o3-mini outperforming GPT-4o in generating compilable and playable games. The best results occur with around 30,000 tokens of context, with diminishing returns beyond that point.

## Method Summary
ScriptDoctor uses an iterative pipeline where an LLM generates PuzzleScript code, which is then compiled and tested for solvability. The system provides compilation errors, syntax errors from a CFG parser, and BFS solver results as feedback to guide subsequent generations. The process repeats up to 10 times or until success. The system was tested with different LLMs (GPT-4o, o1, o3-mini) and context lengths (10k-70k tokens) using 610 human-authored PuzzleScript games as few-shot examples. Success is defined as games that compile and have all levels solvable with solutions longer than 10 moves.

## Key Results
- Few-shot prompting increased compilability from 30% to 70-80% and solvability from 0% to 40-55%
- Reasoning models (o1, o3-mini) outperformed non-reasoning models (GPT-4o) with o1 achieving 87% compilability and 67% all-solvable
- Optimal context length was approximately 30,000 tokens, with diminishing returns beyond this point
- Generated games showed solution complexities ranging from 1,498 to 22,771 nodes explored by the BFS solver

## Why This Works (Mechanism)

### Mechanism 1
Few-shot prompting with human-authored PuzzleScript games increases compilability and solvability of generated code. Domain-specific languages like PuzzleScript are underrepresented in LLM training corpora compared to mainstream languages (Python, C++). Providing in-context examples grounds the model's output in correct syntax and idiomatic patterns, reducing hallucinated constructs. Core assumption: The sampled human games are representative of valid PuzzleScript idioms and do not themselves contain bugs or anti-patterns. Evidence anchors: Few-shot prompting increased compilability from 30% to 70-80% and solvability from 0% to 40-55%.

### Mechanism 2
Reasoning models (o1, o3-mini) produce more functional games than non-reasoning models (GPT-4o) due to extended inference-time computation. Reasoning models allocate additional compute to chain-of-thought style deliberation before outputting code, which appears to improve spatial and logical reasoning required for level design and rule consistency. Core assumption: The improvement stems from reasoning capability rather than other architectural differences between model families. Evidence anchors: o1 achieved 87% compilability, 67% any-solvable vs GPT-4o's 67%/40%.

### Mechanism 3
Context-Free Grammar (CFG) parsing and compiler feedback in an iterative loop enable automatic code repair. The PuzzleScript engine emits specific error messages; the Lark CFG parser independently identifies syntax violations. Both signals are fed back to the LLM alongside its prior output, enabling targeted corrections without human intervention. Core assumption: LLMs can interpret and act on compiler error messages accurately; errors are repairable rather than requiring full regeneration. Evidence anchors: "compilation errors from the PuzzleScript engine are used to elicit functional code" and CFG parser provides independent syntax validation layer.

## Foundational Learning

- **PuzzleScript DSL structure**: Understanding the four sections (sprites, rules, levels, win conditions) is required to interpret compiler errors and assess whether generated games are semantically coherent. Quick check question: Can you identify the difference between a syntax error (malformed rule) and a semantic error (unwinnable level)?

- **Context-Free Grammars**: The CFG parser provides independent syntax validation; understanding parse trees helps diagnose why certain error messages appear and how to extend the grammar. Quick check question: Given a PuzzleScript snippet, can you manually trace which production rules would match?

- **Breadth-First Search for puzzle solving**: The solver determines solvability and solution complexity; understanding BFS behavior helps interpret node counts and diagnose why some levels timeout. Quick check question: If BFS explores 1M nodes without finding a solution, what can you conclude about the level?

## Architecture Onboarding

- **Component map**: Python server -> LLM API -> PuzzleScript engine -> Lark CFG parser -> BFS solver -> Feedback aggregator

- **Critical path**: System prompt + few-shot examples + brainstorming ideas → LLM generates PuzzleScript code → CFG parser validates syntax → Engine attempts compilation → If compilable, BFS solver tests each level → If all levels solvable with solutions >10 moves → Success; else → LLM retries with accumulated feedback (max 10 iterations)

- **Design tradeoffs**: Context length vs. quality: 30k tokens appears near-optimal; beyond this, marginal examples add noise without improving understanding. BFS node limit (1M) vs. coverage: Higher limits catch more solutions but increase latency; complex puzzles may be incorrectly marked unsolvable. Iteration cap (10) vs. resource cost: More iterations may repair stubborn bugs but with diminishing returns.

- **Failure signatures**: Stuck at 0% compilability: Likely context corruption or API failure; check prompt construction. Compiles but 0% solvable: LLM may be generating valid syntax but broken mechanics. Solutions always <10 moves: Level generation weakness; LLM stretches levels without adding meaningful complexity. O3-mini outperforming o1 unexpectedly: Check that reasoning budget is not being truncated.

- **First 3 experiments**: 1) Reproduce Table I with a smaller few-shot sample (5 games instead of context-maximizing) to verify scaling behavior. 2) Ablate the CFG parser: Run generation with only engine compiler feedback to quantify its contribution to repair success. 3) Test a non-reasoning model with explicit chain-of-thought prompting vs. implicit reasoning in o1 to isolate whether the benefit is compute or architecture.

## Open Questions the Paper Calls Out

### Open Question 1
Can fine-tuning smaller LLMs on ScriptDoctor's verified compilable and solvable games outperform few-shot prompting of larger models for PuzzleScript generation? Current experiments only evaluate few-shot prompting; the potential of supervised fine-tuning on generated game datasets remains untested.

### Open Question 2
What supplementary context beyond few-shot game examples can improve generation quality after context saturation? Experiments show diminishing returns beyond 30k tokens of few-shot examples, but alternative guidance strategies are unexplored.

### Open Question 3
Can vision-language models effectively diagnose broken game mechanics when supplied with gameplay frames showing rule applications? Current solvability feedback cannot detect mechanics that function incorrectly relative to design intent—complex games often succeed despite broken mechanics.

## Limitations

- The optimal context length of ~30k tokens may be dataset-specific and could vary with different DSLs or generation tasks.
- The BFS solver's 1M node limit means truly complex puzzles might be incorrectly marked as unsolvable, potentially biasing results toward simpler designs.
- The iterative repair mechanism's effectiveness depends on the assumption that compilation errors are independent and repairable, which may not hold for deeply nested or cascading bugs.

## Confidence

- **High confidence**: Few-shot prompting improves compilability (30% → 70-80%) and solvability (0% → 40-55%); o1 and o3-mini outperform GPT-4o in both metrics; 30k token context shows optimal results.
- **Medium confidence**: Reasoning models' advantage stems from inference-time computation rather than architectural differences; CFG parser provides independent validation beyond compiler feedback alone.
- **Low confidence**: Solution complexity measurements accurately reflect human-perceived puzzle difficulty; iterative repair consistently converges to working solutions; few-shot examples don't introduce harmful patterns.

## Next Checks

1. **Cross-DSL validation**: Test ScriptDoctor's iterative generation pipeline with a different DSL (e.g., Logo or VPL) to verify whether the 30k token optimal context and few-shot prompting benefits transfer to other domain-specific languages.

2. **Error independence analysis**: Conduct ablation studies where compilation errors are intentionally introduced in cascading patterns to measure whether the LLM's repair success rate drops significantly, testing the assumption that errors are independently repairable.

3. **Solver sensitivity test**: Vary the BFS node limit (100k, 1M, 10M) across different reasoning models to quantify how many potentially solvable puzzles are incorrectly marked unsolvable at the current 1M threshold, and whether reasoning models identify more solutions at higher limits.