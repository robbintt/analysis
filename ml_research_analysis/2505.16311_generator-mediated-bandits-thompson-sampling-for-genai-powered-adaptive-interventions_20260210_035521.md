---
ver: rpa2
title: 'Generator-Mediated Bandits: Thompson Sampling for GenAI-Powered Adaptive Interventions'
arxiv_id: '2505.16311'
source_url: https://arxiv.org/abs/2505.16311
tags:
- reward
- regret
- where
- treatment
- bandit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces generator-mediated bandits (GAMBITTS), a
  new framework for online decision-making where actions influence rewards through
  stochastic, observed treatments rather than directly. The method is motivated by
  mobile health interventions using large language models to generate personalized
  text messages, but applies broadly to any setting where actions produce high-dimensional,
  stochastic outputs.
---

# Generator-Mediated Bandits: Thompson Sampling for GenAI-Powered Adaptive Interventions

## Quick Facts
- **arXiv ID**: 2505.16311
- **Source URL**: https://arxiv.org/abs/2505.16311
- **Reference count**: 40
- **Primary result**: Introduces GAMBITTS framework for adaptive interventions using LLM-generated treatments, achieving better regret bounds than standard bandits when treatment embeddings are low-dimensional and well-specified.

## Executive Summary
This paper introduces generator-mediated bandits (GAMBITTS), a new framework for online decision-making where actions influence rewards through stochastic, observed treatments rather than directly. The method is motivated by mobile health interventions using large language models to generate personalized text messages, but applies broadly to any setting where actions produce high-dimensional, stochastic outputs. GAMBITTS explicitly models both the treatment-generation process and the reward-prediction process, using Thompson sampling to balance exploration and exploitation. The approach leverages the observed treatments to accelerate learning compared to standard bandit methods that ignore this intermediate step.

## Method Summary
GAMBITTS is a two-stage Thompson sampling framework for generator-mediated bandits. Actions (prompts) produce stochastic treatments (LLM-generated messages) that influence rewards. The method decomposes the action→reward pathway into: (1) a treatment model Z|A,X capturing how prompts produce embeddings, and (2) a reward model E[Y|Z,X] capturing how embeddings affect outcomes. Thompson sampling integrates over both uncertainty sources, using observed treatments Z to provide direct signal about reward structure. The framework offers two variants: foGAMBITTS (online treatment model learning) and poGAMBITTS (offline treatment model pretraining using simulator access). Key components include a working embedding function h that projects high-dimensional generator outputs to low-dimensional representations, Gaussian priors for treatment and reward models, and Monte Carlo integration for action selection.

## Key Results
- GAMBITTS achieves regret O(σ₁√CKT + σ₂d√T), improving over standard Thompson sampling O((σ₁+σ₂)√CKT) when d ≪ √CK
- Performance benefits increase as the number of arms grows or outcome noise increases
- poGAMBITTS requires ~50 simulator samples per (context, action) pair to achieve stable performance
- When working embedding poorly correlates with true reward-relevant dimensions, GAMBITTS can underperform standard bandits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicitly modeling the treatment-generation process accelerates learning compared to standard bandits that ignore this intermediate step.
- Mechanism: GAMBITTS decomposes the action→reward pathway into two stages: (1) a treatment model Z|A,X capturing how prompts produce embeddings, and (2) a reward model E[Y|Z,X] capturing how embeddings affect outcomes. Thompson sampling then integrates over both uncertainty sources, using observed treatments Z to provide direct signal about reward structure rather than treating action→reward as a black box.
- Core assumption: The working treatment embedding Z preserves reward-relevant information from the true embedding Z* (i.e., Y ⊥ Z* | Z, X).
- Evidence anchors:
  - [abstract] "GAMBITTS explicitly models both the treatment and reward generation processes, using information in the delivered treatment to accelerate policy learning relative to standard methods."
  - [Section 5] Theorem 3 shows foGAMBITTS achieves regret O(σ₁√CKT + σ₂d√T), improving over standard Thompson sampling Õ((σ₁+σ₂)√CKT) when d ≪ √CK.
  - [corpus] Corpus evidence weak; neighbor papers focus on standard Thompson sampling variants without generator mediation.
- Break condition: When the embedding is misspecified (Z poorly correlated with Z*), performance degrades and can underperform standard bandits (Figure 3).

### Mechanism 2
- Claim: When outcome noise σ₂ dominates treatment noise σ₁, modeling the treatment distribution provides greater learning efficiency gains.
- Mechanism: Standard bandits must learn separate reward estimates for each (context, action) pair, incurring √CK scaling. GAMBITTS generalizes across arms via the shared embedding space Z, achieving √d scaling instead. This advantage is largest when σ₂≫σ₁ because the reward model—which benefits from cross-arm generalization—dominates the learning challenge.
- Core assumption: Linear reward model (or well-specified nonlinear model) in the treatment embedding.
- Evidence anchors:
  - [Section 5.1] Corollary 1: poGAMBITTS regret O(σ₂d√T) is sharper than standard when d < (1 + σ₁/σ₂)√CK.
  - [Section 6.3] Figure 4 shows poGAMBITTS regret stable across K while standard bandits scale with K.
  - [corpus] Weak direct evidence; neighbor papers on variance-aware Thompson sampling suggest similar principles about noise-dependent regret.
- Break condition: When σ₁ is large (high generator stochasticity), simulator access offers limited improvement; when d approaches √CK, the dimensionality advantage disappears.

### Mechanism 3
- Claim: Pretraining the treatment model using simulator access to the generator substantially improves online performance.
- Mechanism: In poGAMBITTS, the agent queries the generator offline to estimate f₁^off(z; a, x) before user interaction. This eliminates treatment uncertainty during online learning, allowing the agent to focus computational and statistical budget on reward model estimation. Theorem 1 bounds additional regret at O(T√(εT)) where ε is the KL divergence between estimated and true treatment distributions.
- Core assumption: Agent has simulation access to generator; sufficient offline samples to achieve small KL divergence ε.
- Evidence anchors:
  - [Section 4.2] Algorithm 2 describes poGAMBITTS using pretrained f₁^off.
  - [Section D.2.3] Lemma 9 shows poly(d, ε⁻¹) samples suffice for Gaussian treatment distributions.
  - [Section E.3] Figure 7 shows regret decreases with more simulator draws, stabilizing around 50 samples per (x,a) pair.
  - [corpus] No direct corpus evidence on simulator pretraining for bandits.
- Break condition: When simulator access is limited (<15 samples per pair) or context space is infinite without good generalization, empirical treatment distribution poorly approximates true distribution.

## Foundational Learning

- Concept: **Thompson Sampling with Posterior Sampling**
  - Why needed here: GAMBITTS is fundamentally a Thompson sampling algorithm; understanding how posterior sampling balances exploration/exploitation is essential.
  - Quick check question: Can you explain why sampling actions from the posterior (rather than selecting the highest posterior mean) induces exploration?

- Concept: **Bayesian Regret Decomposition**
  - Why needed here: The theoretical guarantees decompose regret into treatment uncertainty (σ₁√CKT) and reward uncertainty (σ₂d√T) terms.
  - Quick check question: How would increasing outcome noise σ₂ affect the relative advantage of GAMBITTS over standard bandits?

- Concept: **Eluder Dimension and Covering Numbers**
  - Why needed here: Theorem 2 extends guarantees to nonlinear reward models using these complexity measures.
  - Quick check question: What does high eluder dimension imply about the difficulty of learning a function class?

## Architecture Onboarding

- Component map:
  - **Treatment Model** f₁(z; a, x, θ₁): Maps prompt/context → distribution over embeddings Z. Can be pretrained offline (poGAMBITTS) or learned online (foGAMBITTS).
  - **Reward Model** m₂(z, x; θ₂): Maps embedding/context → expected reward. Linear (efficient posterior) or MLP with ensemble sampling.
  - **Embedding Function** h: G → Z: Projects high-dimensional generator output to low-dimensional representation. Application-specific (VAE, BERT-based, semantic scores).
  - **Action Selection**: Monte Carlo integration over posterior samples to estimate E[Y|a,x] for each action.

- Critical path:
  1. Define embedding h(G) → Z that captures reward-relevant semantics
  2. If simulator available: pretrain treatment model f₁^off using M samples per (a,x) pair
  3. Initialize reward model prior π₂
  4. At each timestep: observe context, sample from posteriors, compute expected rewards via integration, select argmax action
  5. Update reward posterior (and treatment posterior if foGAMBITTS) with observed (Z,Y)

- Design tradeoffs:
  - **foGAMBITTS vs poGAMBITTS**: fo requires no simulator but higher regret; po requires offline simulation budget but better sample efficiency online.
  - **Linear vs Ensemble reward model**: Linear enables closed-form posteriors and regret bounds; ensemble (MLP) handles misspecification but requires more data and shows volatile performance.
  - **Embedding dimension d**: Lower d improves regret bound but risks information loss; performance degrades as d increases (Figure 8).

- Failure signatures:
  - **Embedding misspecification**: Regret curves cross above standard bandits when working embedding poorly correlates with true reward-relevant dimensions (Figure 3).
  - **High outcome noise with small d**: All methods converge; GAMBITTS advantage diminishes.
  - **Ensemble instability**: ens-poGAMBITTS shows volatile performance without careful hyperparameter tuning (Section E.2).
  - **Insufficient simulator access**: <50 samples per (a,x) pair leads to poor treatment model estimation (Figure 7).

- First 3 experiments:
  1. **Embedding validation**: Before deployment, correlate candidate embeddings with proxy reward signals; discard embeddings with correlation <0.3 to the target outcome.
  2. **Simulator budget calibration**: Run offline simulations varying M∈{15,50,100,500} and measure KL divergence between empirical and true treatment distributions; select minimum M achieving KL<0.01.
  3. **Regret trajectory comparison**: In simulation, compare foGAMBITTS, poGAMBITTS, and standard Thompson sampling across K∈{5,20,40} arms with d∈{3,5,10} to identify regime where GAMBITTS advantage is largest.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the treatment embedding be learned online rather than specified a priori?
- Basis in paper: [explicit] The authors suggest extending GAMBITTS to learn the embedding via "online sufficient dimensionality reduction."
- Why unresolved: Current algorithms assume a fixed, pre-defined working embedding Z=h(G).
- What evidence would resolve it: An algorithm integrating representation learning with Thompson sampling and regret bounds showing efficiency.

### Open Question 2
- Question: How can the framework be adapted for non-static generators that evolve via fine-tuning?
- Basis in paper: [explicit] The authors propose "fine-tuning the generator based on observed outcomes" as a future direction.
- Why unresolved: The current theoretical analysis and algorithms assume a fixed generative model.
- What evidence would resolve it: Regret bounds and algorithmic variants accommodating a shifting treatment distribution over time.

### Open Question 3
- Question: How can safety checks or content filters be integrated into the generator-mediated bandit framework?
- Basis in paper: [explicit] The authors note the assumption of immediate delivery may be inappropriate for sensitive domains requiring oversight.
- Why unresolved: GAMBITTS currently optimizes for reward without constraints on the content of the stochastic treatment.
- What evidence would resolve it: A constrained bandit formulation that filters unsafe outputs while maintaining exploration guarantees.

## Limitations
- Performance critically depends on embedding quality, with significant degradation when working embeddings poorly capture reward-relevant information
- Theoretical advantages require specific conditions: treatment noise must be small relative to outcome noise, embedding dimension must remain low, and reward model must be well-specified
- Empirical evaluation relies heavily on simulated data calibrated from a single mobile health study, raising generalizability concerns

## Confidence

- **High confidence**: Core mechanism that modeling the treatment generation process provides learning advantages over black-box action→reward mapping. The theoretical regret bounds for linear reward models are rigorous and the simulation results are reproducible given the detailed implementation specifications.
- **Medium confidence**: Empirical superiority claims are robust within the simulation framework but depend on specific parameter choices (noise ratios, embedding dimensionality). The advantages over standard bandits are most pronounced under favorable conditions that may not always hold in practice.
- **Medium confidence**: Theoretical extensions to nonlinear reward models using eluder dimension provide valuable insights but rely on stronger assumptions about the reward model class and don't guarantee practical improvements.

## Next Checks

1. **Embedding robustness test**: Systematically vary the correlation between working and true embeddings (ρ∈[0.1,0.9]) to quantify performance degradation when misspecification occurs, as suggested by Figure 3.

2. **Noise regime sensitivity**: Vary the ratio σ₁/σ₂ across orders of magnitude to identify the boundary conditions where GAMBITTS advantages disappear or reverse.

3. **Real-world deployment pilot**: Apply GAMBITTS to a small-scale mobile health intervention with live LLM generation and measure whether theoretical advantages translate to actual user engagement and health outcomes.