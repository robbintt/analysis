---
ver: rpa2
title: 'CSIRO-LT at SemEval-2025 Task 11: Adapting LLMs for Emotion Recognition for
  Multiple Languages'
arxiv_id: '2508.01161'
source_url: https://arxiv.org/abs/2508.01161
tags:
- emotion
- languages
- track
- task
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We investigated adapting large language models (LLMs) for emotion
  recognition across 28 languages in the SemEval-2025 Task 11. The task required detecting
  perceived emotional states (anger, disgust, fear, joy, sadness, surprise) and their
  intensity levels in text.
---

# CSIRO-LT at SemEval-2025 Task 11: Adapting LLMs for Emotion Recognition for Multiple Languages

## Quick Facts
- arXiv ID: 2508.01161
- Source URL: https://arxiv.org/abs/2508.01161
- Reference count: 11
- Primary result: Direct supervised fine-tuning with LoRA consistently outperformed instruction tuning and few-shot prompting for multilingual emotion detection across 28 languages.

## Executive Summary
This paper investigates adapting large language models for emotion recognition across 28 languages in the SemEval-2025 Task 11. The task required detecting six emotional states (anger, disgust, fear, joy, sadness, surprise) and their intensity levels in text. The authors explored several adaptation strategies including few-shot prompting, supervised fine-tuning, English-bridged adaptation, and marginalization from intensity predictions. Their findings demonstrate that direct supervised fine-tuning with LoRA parameter-efficient tuning consistently achieved the best performance across most languages, outperforming instruction tuning and other adaptation methods.

## Method Summary
The approach reformulates multi-label emotion detection as six independent binary classification problems, one per emotion. The authors fine-tune multilingual LLMs (aya-32b-expanse, Llama3.1-8B-Instruct, aya-101) using LoRA (rank=32, alpha=64, dropout=0.05) with 4-bit quantization. Training uses max length 512, 10 epochs, batch size 2, learning rate 2e-5 for Track A and 5e-5 for Track B. The emotion intensity task (Track B) uses 4-point ordinal scale predictions. Class imbalance is addressed through oversampling. The paper compares direct supervised fine-tuning against instruction tuning, English-bridged adaptation (training on English first, then target language), and marginalization from intensity predictions.

## Key Results
- Direct supervised fine-tuning with LoRA achieved the highest macro F1 scores across most languages, outperforming all other adaptation approaches
- For emotion intensity detection, different LLMs performed best for different languages, with fine-tuning providing substantial improvements
- English-bridged adaptation showed modest gains only for languages with closer cultural alignment to English (German, Portuguese), not for culturally distant languages (Russian)
- Marginalization from intensity predictions underperformed direct binary classification, suggesting intensity learning introduces noise into presence/absence decisions

## Why This Works (Mechanism)

### Mechanism 1
Direct supervised fine-tuning with LoRA on language-specific data outperforms instruction tuning and few-shot prompting for multilingual emotion detection. LoRA enables parameter-efficient adaptation by updating low-rank decomposition matrices while freezing base model weights, allowing the model to learn task-specific emotion classification patterns without catastrophic forgetting. The binary classification formulation per emotion reduces complexity compared to joint multi-label prediction. The finding is supported by experimental results showing direct SFT consistently achieving highest macro F1 scores across four test languages. Break condition: very low-resource languages with <100 training examples may benefit more from few-shot learning.

### Mechanism 2
English-bridged adaptation provides modest gains only for languages with closer cultural alignment to English. Sequential transfer learning first teaches emotion recognition patterns in English (highest-resource), then adapts representations to target language. Cultural similarity in emotional expression facilitates positive transfer; dissimilar cultures show no benefit. Evidence shows E-Bridge scores trailing direct SFT but outperforming few-shot for German and Portuguese, while being less effective for Russian. Break condition: languages with culturally distinct emotional expression patterns or scripts may show negative transfer.

### Mechanism 3
Marginalization from intensity predictions underperforms direct binary classification because intensity learning introduces noise into presence/absence decisions. Training on 4-class intensity then collapsing predictions to binary (1-3→present, 0→absent) requires learning distinctions ultimately discarded. The indirect path degrades performance as shown by marginalization approaches consistently underperforming direct SFT. Break condition: when intensity annotations are sparse or inconsistent, marginalization further degrades performance.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**: Why needed - paper uses LoRA for all experiments; understanding rank/alpha/dropout settings is essential for reproducing results. Quick check: If LoRA rank is set too low (e.g., 4), what type of task complexity would fail to be captured?

- **Multi-label vs. Binary Classification Decomposition**: Why needed - paper reformulates 6-emotion multi-label as 6 independent binary problems; this design choice affects inference cost and label correlations. Quick check: What information is lost when predicting each emotion independently rather than jointly?

- **Cultural Variation in Emotional Expression**: Why needed - paper explicitly discusses cultural alignment affecting English-bridge transfer; model selection must account for this. Quick check: Why might Russian show weaker English-bridge gains than German despite both being Indo-European languages?

## Architecture Onboarding

- **Component map**: Input processor (tokenizer, 512-token truncation) → instruction template ({language}, {text}, {emotion}) → Base models (AYA-32B-expanse, AYA-101, Llama3.1-8B-Instruct) → LoRA modules (rank=32, alpha=64, dropout=0.05) → Task heads (6 independent binary classifiers for Track A, 4-class classifiers for Track B) → Post-processing (aggregation of 6 emotion predictions per input text)

- **Critical path**: 1) Select model based on language coverage (AYA-32B for breadth, Llama for overlapping high-resource languages) 2) Format training data per emotion as separate binary instances 3) Apply class balancing via oversampling 4) Fine-tune with LoRA (lr=2e-5 Track A, 5e-5 Track B, 10 epochs, batch=2) 5) Run 6 inference passes per input, aggregate predictions

- **Design tradeoffs**: Per-language vs. joint training (paper uses per-language; joint training would share signal but risk negative transfer), Direct SFT vs. instruction tuning first (direct SFT wins in experiments, but instruction tuning may help for truly zero-resource languages), Model size vs. coverage (AYA-101 underperforms on low-resource languages; AYA-32B better but more expensive)

- **Failure signatures**: F1 scores dropping below baseline on low-resource languages (e.g., Nigerian-Pidgin: 55.50 baseline → 21.81 with wrong model choice), Intensity predictions clustering at extremes (0 or 3) indicating poor calibration, High variance between emotions on same language suggesting class imbalance issues

- **First 3 experiments**: 1) Baseline replication: Fine-tune AYA-32B-expanse with LoRA on English Track A data; target macro F1 >75 (paper reports 77.62 test, 80.12 dev) 2) Language transfer test: Train on English, zero-shot evaluate on German; compare to English-bridged approach to quantify transfer gap 3) Low-resource ablation: Compare few-shot (BM25, k=1) vs. SFT on a language with <200 training samples (e.g., Emakhuwa or Oromo) to identify the data threshold where SFT stops being superior

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the finding that direct supervised fine-tuning outperforms instruction-tuning generalize to low-resourced languages with different linguistic structures? The authors state exploration was limited to four languages, which "may not generalise to other languages, particularly lower-resourced ones or those with different linguistic structures." Comparative analysis of adaptation strategies was validated only for English, German, Portuguese, and Russian, leaving the efficacy for the remaining 24 languages uncertain. A comparative evaluation on low-resource languages (e.g., Yoruba, Igbo) would resolve this.

- **Open Question 2**: Can full-parameter fine-tuning or alternative adaptation techniques outperform the Low-Rank Adaptation (LoRA) method used in this study? The authors note they "only explored prompt-tuning and instruction-tuning with the parameter-efficient LoRA setting for adapting LLMs." The study restricted methodology to LoRA due to resource constraints, leaving potential performance gains of full fine-tuning untested. An ablation study comparing LoRA against full-parameter fine-tuning would resolve this.

- **Open Question 3**: To what extent do inherent biases in the pre-trained LLMs affect performance discrepancies in low-resourced languages? The authors acknowledge that "The models used may reflect biases from the training data, which could affect performance in low-resourced languages." While performance variations were observed (e.g., inconsistencies in Hausa and Amharic), the paper does not isolate the specific impact of model bias versus data scarcity. A bias analysis measuring correlation between pre-training data volume per language and model performance would resolve this.

## Limitations

- The comparative analysis of adaptation strategies (SFT vs. instruction-tuning) was statistically validated only for English, German, Portuguese, and Russian, leaving the efficacy for the remaining 24 languages uncertain.
- The study restricted its methodology to parameter-efficient tuning (LoRA) due to resource constraints, leaving the potential performance gains of full fine-tuning untested.
- The cultural alignment hypothesis for English-bridged transfer remains speculative without systematic cross-cultural validation.

## Confidence

- **High Confidence**: Direct supervised fine-tuning with LoRA consistently outperforms few-shot prompting and instruction tuning for mid-to-high resource languages.
- **Medium Confidence**: English-bridged adaptation provides modest gains only for culturally similar languages.
- **Medium Confidence**: Marginalization from intensity predictions underperforms direct binary classification.

## Next Checks

1. **Data threshold experiment**: Systematically vary training data availability (10%, 25%, 50%, 100%) for a representative sample of languages to identify the exact point where supervised fine-tuning overtakes few-shot prompting, and whether this threshold varies by language family or script.

2. **Cultural alignment validation**: Design an experiment that explicitly measures transfer effectiveness between languages with known cultural similarities/differences (e.g., Portuguese-Spanish vs. Russian-English) while controlling for linguistic similarity, to test whether cultural alignment independently predicts bridging success.

3. **LoRA hyperparameter sweep**: Conduct a comprehensive ablation study across LoRA rank (4, 8, 16, 32, 64) and alpha (16, 32, 64, 128) for each language family to determine whether the current settings are globally optimal or if language-specific tuning would improve performance, particularly for languages with different scripts or morphological complexity.