---
ver: rpa2
title: 'OmniStruct: Universal Text-to-Structure Generation across Diverse Schemas'
arxiv_id: '2511.18335'
source_url: https://arxiv.org/abs/2511.18335
tags:
- tasks
- task
- answer
- json
- text-to-structure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OmniStruct, a comprehensive benchmark for
  evaluating large language models' ability to generate structured outputs across
  diverse tasks and schemas. The authors identify existing datasets from information
  extraction, table generation, and function calling tasks, adapting them into a unified
  schema-following JSON generation format.
---

# OmniStruct: Universal Text-to-Structure Generation across Diverse Schemas

## Quick Facts
- arXiv ID: 2511.18335
- Source URL: https://arxiv.org/abs/2511.18335
- Authors: James Y. Huang; Wenxuan Zhou; Nan Xu; Fei Wang; Qin Liu; Sheng Zhang; Hoifung Poon; Muhao Chen
- Reference count: 28
- Primary result: Models fine-tuned on synthetic data can rival GPT-4o's performance on universal text-to-structure generation tasks, with distilled 8B model achieving F1 scores up to 76.9 on NER and 84.0 on function calling.

## Executive Summary
This paper introduces OmniStruct, a comprehensive benchmark for evaluating large language models' ability to generate structured outputs across diverse tasks and schemas. The authors identify existing datasets from information extraction, table generation, and function calling tasks, adapting them into a unified schema-following JSON generation format. They also develop a pipeline for synthesizing high-quality training data using GPT-4o, enabling fine-tuning of smaller models without supervised data. Their experiments demonstrate that models fine-tuned on synthetic data can rival GPT-4o's performance on OmniStruct tasks, with the distilled 8B model achieving F1 scores up to 76.9 on NER and 84.0 on function calling tasks.

## Method Summary
The OmniStruct framework unifies diverse text-to-structure tasks into a common JSON schema-following format. The authors developed a pipeline to synthesize training data using GPT-4o, starting with task filtering from existing datasets, iterative task synthesis to expand coverage, and instance generation with validation. They fine-tune Llama3.1-8B-Instruct on this synthetic data (1 epoch, LR=1e-5, batch=64) without using any supervised task-specific data. The resulting models are evaluated zero-shot on the OmniStruct benchmark across five task categories using task-specific metrics including F1 scores, error rates, and accuracy measures.

## Key Results
- OmniStruct-8B outperforms GPT-4o on 5/6 NER datasets and improves across all task categories without seeing test schemas during training
- The distilled 8B model achieves F1 scores up to 76.9 on NER and 84.0 on function calling tasks
- Models successfully learn to follow JSON schemas and generate valid structured outputs
- Cross-task transfer is demonstrated with models performing well across diverse schema types

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Task-to-Structure Distillation
Fine-tuning smaller models on synthetic text-to-structure data generated by GPT-4o can transfer structured generation capabilities without supervised task-specific data. The teacher model generates diverse task instructions, schemas, and valid JSON outputs, while the student model learns to map natural language instructions + schema constraints → structured outputs through supervised fine-tuning on this synthetic distribution.

### Mechanism 2: Unified JSON Schema Problem Formulation
Converting diverse text-to-structure tasks into a common JSON schema-following format enables cross-task transfer and consistent evaluation. All tasks share identical input structure (task instruction + format instruction with JSON schema) and output structure (valid JSON following schema), reducing task-specific idiosyncrasies and allowing the model to learn general schema-compliance patterns.

### Mechanism 3: Iterative Task Synthesis for Distribution Coverage
Iteratively proposing new tasks based on existing seeds expands the training distribution to cover unseen test schemas. Starting from filtered seeds, GPT-4o proposes novel task-instruction + schema pairs conditioned on random samples, subject to quality constraints (≥2 fields, no metadata, no ambiguity), bootstrapping coverage beyond the initial seed distribution.

## Foundational Learning

- **JSON Schema Specification (Draft 4/7/2020)**
  - Why needed here: The entire framework relies on models understanding schema constraints—required fields, types, nested objects, arrays. Without this, models cannot follow format instructions.
  - Quick check question: Given a schema requiring `{"type": "object", "properties": {"entities": {"type": "array"}}, "required": ["entities"]}`, can you identify what makes an output valid vs. invalid?

- **Knowledge Distillation via Instruction Tuning**
  - Why needed here: The paper's core contribution is distilling GPT-4o's text-to-structure capability into smaller models via synthetic instruction-response pairs.
  - Quick check question: Why does instruction tuning on synthetic data enable zero-shot generalization when the student model never sees the test tasks?

- **Constrained vs. Unconstrained Decoding**
  - Why needed here: The paper explicitly distinguishes its approach from constrained decoding (guaranteeing format via token pruning), focusing instead on improving model's inherent capability.
  - Quick check question: If constrained decoding guarantees schema adherence, why might improving the model's capability still be necessary?

## Architecture Onboarding

- **Component map**: Task Filter -> Task Synthesizer -> Instance Generator -> Validator -> Fine-tuning Pipeline
- **Critical path**: Seed task filtering quality → Synthetic distribution breadth → Schema quality in synthesis → Instance generation validation → Training effectiveness
- **Design tradeoffs**: Task-wide vs. instance-specific schemas (consistent evaluation vs. variable structure handling); synthetic-only vs. mixed supervised (pure synthesis sufficiency vs. domain-specific needs); constrained decoding vs. model capability (guaranteed format vs. inherent understanding)
- **Failure signatures**: High parsing error rate (>5%) indicates JSON syntax learning issues; valid JSON but wrong content indicates schema-compliance without task understanding; strong performance on seen schemas, failure on novel schemas indicates memorization, not generalization
- **First 3 experiments**: Baseline schema adherence evaluation on validation set (<1% parsing errors target); synthetic data quality audit (50 instances manual inspection); ablation by task category (extraction-only vs. full synthetic mix)

## Open Questions the Paper Calls Out

### Open Question 1
How does LLM performance on universal text-to-structure tasks change when using alternative structured formats such as XML, HTML, or LaTeX instead of JSON? The authors explicitly state in the Limitations section that while they focused on JSON due to its versatility, "How well modern LLMs support these alternative formats remains an open question." An extension of the OmniStruct benchmark where the identical tasks and instances are formatted with XML, HTML, and LaTeX schemas, evaluated across the same model families, would resolve this.

### Open Question 2
Can the performance gap on reasoning-intensive text-to-structure tasks (like Meeting Plan) be closed without relying on general reasoning improvements? The authors note a persistent performance gap on reasoning tasks and state, "we believe it is unlikely to further close that gap by training on text-to-structure data alone." Experiments combining the proposed text-to-structure synthetic data with general reasoning instruction tuning (e.g., CoT distillation) to see if the gap to GPT-4o narrows on the Meeting Plan dataset would resolve this.

### Open Question 3
Does the specific position of a task on the "extractive-to-abstractive" spectrum predict the efficacy of synthetic data distillation? The paper introduces a spectrum ranging from extractive tasks (Text-to-Table) to abstractive tasks (Reasoning). Results show OmniStruct-8B matches GPT-4o on extractive tasks but fails on abstractive ones, implying a correlation between task type and distillation success. A controlled ablation study varying the level of abstraction required in the synthetic tasks while keeping schema complexity constant to measure the impact on the student model's performance would resolve this.

## Limitations
- The synthetic data generation pipeline relies heavily on GPT-4o's ability to propose high-quality tasks and schemas without introducing systematic biases
- The framework assumes JSON can adequately represent all target structures, which may not hold for tasks requiring more complex relationships or cyclic dependencies
- The iterative task synthesis approach could potentially suffer from mode collapse or generate tasks too similar to the seed distribution

## Confidence

- **High confidence**: JSON schema unification approach and zero-shot evaluation methodology (well-specified with clear implementation details)
- **Medium confidence**: Synthetic data quality and its sufficiency for achieving strong performance (based on reported results but limited analysis of synthetic distribution)
- **Medium confidence**: Cross-task generalization capabilities (demonstrated through experiments but with no analysis of failure cases or schema complexity effects)

## Next Checks

1. **Synthetic data audit**: Manually inspect 100 randomly sampled synthetic instances to evaluate task plausibility, schema validity, and answer correctness. Compute inter-annotator agreement to quantify quality.

2. **Schema complexity analysis**: Categorize OmniStruct schemas by complexity (required fields, nesting depth, array usage) and measure model performance degradation across complexity levels to identify generalization limits.

3. **Failure case investigation**: For each task category, analyze 20-30 incorrect predictions to classify failure modes (syntax errors, semantic errors, schema non-compliance) and measure whether errors correlate with specific schema features or training data coverage gaps.