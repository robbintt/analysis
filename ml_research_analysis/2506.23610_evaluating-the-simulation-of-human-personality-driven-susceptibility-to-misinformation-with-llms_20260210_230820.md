---
ver: rpa2
title: Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation
  with LLMs
arxiv_id: '2506.23610'
source_url: https://arxiv.org/abs/2506.23610
tags:
- personality
- human
- news
- agents
- traits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs), when
  endowed with explicit Big-Five personality profiles, can simulate human personality-driven
  susceptibility to misinformation. By creating personality-aligned LLM agents and
  comparing their news discernment patterns with human participants, the research
  finds that GPT-4o successfully replicates key trait-discernment associations for
  Agreeableness, Conscientiousness, and Open-Mindedness, showing high cosine similarity
  (0.87-0.94) with human data.
---

# Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs

## Quick Facts
- **arXiv ID:** 2506.23610
- **Source URL:** https://arxiv.org/abs/2506.23610
- **Reference count:** 40
- **Key outcome:** GPT-4o successfully simulates human personality-driven misinformation susceptibility for Agreeableness, Conscientiousness, and Open-Mindedness, but shows systematic discrepancies for Extraversion and Negative Emotionality.

## Executive Summary
This study investigates whether large language models can simulate human personality-driven susceptibility to misinformation by creating personality-aligned LLM agents and comparing their news discernment patterns with human participants. The research demonstrates that GPT-4o successfully replicates key trait-discernment associations for three of five Big-Five personality traits, showing high cosine similarity (0.87-0.94) with human data. However, systematic discrepancies emerge for Extraversion and Negative Emotionality, where LLMs show inconsistent or reversed patterns compared to humans. The findings reveal both the potential and limitations of using personality-aligned LLMs as proxies for human misinformation susceptibility, highlighting architecture-dependent biases that prevent complete psychological fidelity.

## Method Summary
The study leverages published datasets where human participants with known personality profiles rated headline accuracy, then creates matching LLM agents by conditioning them with participants' Big-Five personality profiles using the BFI-2 psychometric inventory. Researchers systematically compare LLM-generated discernment patterns against human ground truth using cosine similarity metrics, testing both GPT-4o and GPT-3.5 across different prompt formats (Likert vs. Expanded). The evaluation focuses on the correlation between personality traits and News Discernment scores, where higher scores indicate better discrimination between true and false news.

## Key Results
- GPT-4o achieves cosine similarity of 0.87-0.94 with human data for Agreeableness, Conscientiousness, and Open-Mindedness trait-discernment associations
- Systematic discrepancies emerge for Extraversion and Negative Emotionality, where LLM patterns diverge from or reverse human correlations
- The "Expanded" prompt format outperforms "Likert" format in capturing personality-driven behavioral patterns
- Personality conditioning significantly influences LLM responses, producing measurable shifts in news discernment scores

## Why This Works (Mechanism)

### Mechanism 1: Semantic Personality Injection
Providing item-level psychometric responses activates trait-correlated behavioral patterns in the model's output through the lexical hypothesis—the idea that personality is encoded in language. By exposing the model to structured statements reflecting specific profiles, the model conditions its next-token predictions to align with semantic associations of those traits, mimicking behavioral tendencies in news judgment.

### Mechanism 2: Behavioral Correlation Alignment
LLMs replicate human statistical patterns for specific traits because these traits have strong, consistent linguistic markers in training data. The model shifts its accuracy rating distribution when conditioned, and since this shift mirrors human statistical tendencies, the resulting correlation vectors show high cosine similarity (0.87–0.94).

### Mechanism 3: Systematic Safety & Social Desirability Bias
Simulation fidelity degrades for traits associated with social friction (Extraversion, Negative Emotionality) due to model alignment training favoring socially desirable outputs. Post-training alignment often suppresses negative traits or exaggerates positive ones, leading to reversal of expected correlations or null results where humans show specific effects.

## Foundational Learning

- **Concept: Big Five (OCEAN) Model**
  - **Why needed here:** This is the input schema. You cannot interpret results without understanding the factors and how they're scored.
  - **Quick check question:** If a participant scores high on Agreeableness, should they be more or less likely to rate headlines as accurate according to the study? (Answer: Generally more discerning, but specifically less likely to believe false news).

- **Concept: News Discernment (ND) Metric**
  - **Why needed here:** This is the primary dependent variable—a difference score (Accuracy_Real - Accuracy_False), not just "accuracy."
  - **Quick check question:** If an agent rates all headlines as "Very Accurate" (4/4), what is their News Discernment score? (Answer: Zero. High belief in everything implies low discernment).

- **Concept: Cosine Similarity**
  - **Why needed here:** The study evaluates success by comparing the vector of correlations generated by humans vs. agents.
  - **Quick check question:** A cosine similarity of 1.0 between human and LLM correlation vectors would mean what? (Answer: The relationship between traits and discernment is geometrically identical in both populations).

## Architecture Onboarding

- **Component map:** Dataset Loader -> Prompt Assembler -> Agent Factory -> Evaluation Engine
- **Critical path:** The Prompt Assembler is the highest-risk component. GPT-3.5 performs poorly with "Likert" formats but better with "Expanded" formats. Ensuring correct interpretation of personality inventory is crucial.
- **Design tradeoffs:**
  - GPT-4o offers high fidelity for 3/5 traits but is more expensive/slower; GPT-3.5 is cheaper but struggles to internalize profiles
  - BFI-2-S (30 items) is faster/cheaper but may lose nuance compared to BFI-2 (60 items)
- **Failure signatures:**
  - "Nice AI" Effect: Agents rating everything as highly accurate regardless of truth
  - Reversed Correlations: Seeing Extraversion positively correlated when it's negative/null in humans
- **First 3 experiments:**
  1. Baseline Alignment Check: Run with "Neutral" prompt to establish intrinsic bias
  2. Format A/B Test: Compare "Likert" vs. "Expanded" formats on n=50 profiles
  3. Temperature Stability Test: Run same profile at Temperature 0.2 vs 0.7 to verify robustness

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Findings are limited to GPT-4o and GPT-3.5, leaving questions about generalizability to other architectures
- The simulation cannot capture full complexity of human decision-making, including contextual factors and real-world consequences
- The mechanism behind prompt format differences (Likert vs. Expanded) remains unclear

## Confidence

**High Confidence:**
- GPT-4o successfully replicates human trait-discernment associations for Agreeableness, Conscientiousness, and Open-Mindedness
- Systematic discrepancies exist for Extraversion and Negative Emotionality between LLM agents and humans
- Personality conditioning significantly influences LLM responses

**Medium Confidence:**
- Simulation mechanism works through semantic personality injection via the lexical hypothesis
- Divergence for certain traits is primarily driven by post-training alignment and safety training
- Approach demonstrates potential for scalable behavioral simulation

**Low Confidence:**
- Specific reasons why certain traits show divergent patterns across different models
- Whether effects would generalize to different misinformation contexts or headline domains
- Extent to which model alignment layers systematically suppress or distort personality expressions

## Next Checks

1. **Cross-Model Validation Study**: Test personality-aligned prompting across 4-5 different LLM architectures (Claude, Gemini, Llama variants) using identical headline sets to determine if trait-specific divergences are architecture-dependent.

2. **Ground Truth Behavioral Comparison**: Design small-scale human study (n=50-100) where participants complete BFI-2 and engage with same headlines in naturalistic setting to validate whether laboratory-based News Discernment metric predicts real-world susceptibility patterns.

3. **Prompt Engineering Ablation**: Systematically vary prompt formats and test whether personality effects persist when controlling for prompt length, detail level, and framing to isolate whether mechanism relies on semantic encoding or richer contextual information.