---
ver: rpa2
title: 'QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities
  of Vision-Language Models'
arxiv_id: '2512.19526'
source_url: https://arxiv.org/abs/2512.19526
tags:
- video
- physical
- frame
- object
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QUANTI PHY is the first benchmark for quantitative evaluation of
  VLMs' physical reasoning on kinematic properties of moving objects. The benchmark
  includes 3.3K video-text instances with numerical ground truth, covering size, velocity,
  and acceleration estimation tasks.
---

# QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models

## Quick Facts
- arXiv ID: 2512.19526
- Source URL: https://arxiv.org/abs/2512.19526
- Authors: Li Puyin; Tiange Xiang; Ella Mao; Shirley Wei; Xinye Chen; Adnan Masood; Li Fei-fei; Ehsan Adeli
- Reference count: 40
- Primary result: First benchmark for quantitative evaluation of VLMs' physical reasoning on kinematic properties, revealing VLMs rely on world knowledge rather than visual evidence

## Executive Summary
QuantiPhy introduces a rigorous benchmark to evaluate Vision-Language Models' (VLMs) ability to perform quantitative physical reasoning about moving objects. The benchmark includes 3,355 video-text instances covering size, velocity, and acceleration estimation tasks, where models must use one kinematic property as a prior to infer another. Experiments across 21 state-of-the-art VLMs reveal a consistent gap between qualitative plausibility and actual numerical correctness, with VLMs relying heavily on pre-trained world knowledge rather than faithfully using provided visual and textual inputs.

## Method Summary
The benchmark evaluates VLMs on estimating object kinematic properties (size, velocity, acceleration) from videos using one property as a prior. Videos are standardized at 480p, 2-3 seconds duration, and cover four task categories: 2D-Static, 2D-Dynamic, 3D-Static, and 3D-Dynamic. The Mean Relative Accuracy (MRA) metric measures performance across ten tolerance thresholds (0.5 to 0.95). Models receive standardized prompts with video frames, system prompt, prior/depth information, question, and post-prompt. Evaluation uses zero-shot prompting with temperature=0 and up to 5 retries per query.

## Key Results
- VLMs show poor quantitative physical reasoning, with overall MRA scores significantly below theoretical maxima
- Counterfactual analysis reveals VLMs ignore manipulated numerical priors, defaulting to canonical physical values (e.g., 9.8 m/s² for gravity)
- Performance improves with multiple objects and realistic backgrounds, suggesting relational context aids scale estimation
- Chain-of-thought prompting degrades performance for 18/21 models due to error propagation in intermediate numerical steps

## Why This Works (Mechanism)

### Mechanism 1: Scale Factor Inference via Single Prior
Given one kinematic property in world units and the corresponding pixel-space measurement, a model can determine the pixel-to-world scale factor γ and compute other kinematic properties. All kinematic properties share the same scale factor along motion direction: Sworld = γ·Spixel, Vworld = γ·Vpixel, Aworld = γ·Apixel. A single known prior determines γ, enabling computation of any target property. This assumes translational motion with constant depth (2D tasks) or provided depth priors (3D tasks).

### Mechanism 2: World Knowledge Dominance Over Visual Evidence
VLMs prioritize memorized physical priors (e.g., "cars are ~4.5m long," "gravity = 9.8 m/s²") over provided numerical inputs and pixel-level visual evidence. Pre-training creates strong parametric priors about object properties. When prompted with counterfactual priors, models ignore video and prior, reverting to canonical values. This indicates pre-trained world knowledge forms a stronger conditioning signal than explicit numerical priors.

### Mechanism 3: Relational Context as Implicit Scale Reference
Scenes with multiple objects or textured backgrounds improve kinematic inference by providing implicit reference cues. Additional objects serve as comparison standards; realistic backgrounds offer familiar reference structures (tiles, road markings) that aid scale estimation without explicit priors. VLMs can extract relational spatial information from visual context even without explicit numerical priors.

## Foundational Learning

- **Concept: Finite Difference Approximation**
  - Why needed here: Converting discrete video frames to continuous kinematic quantities requires numerical differentiation
  - Quick check question: Given positions at frames t-1, t, t+1 and frame rate, can you compute instantaneous velocity and acceleration?

- **Concept: Projective Geometry and Scale Ambiguity**
  - Why needed here: Understanding why a single prior resolves the 2D-to-3D mapping requires knowing that pixel measurements alone are scale-ambiguous
  - Quick check question: Why can't you determine an object's real-world size from pixel dimensions without additional information?

- **Concept: Mean Relative Accuracy (MRA)**
  - Why needed here: The benchmark uses a graded accuracy metric across tolerance thresholds rather than exact match or MSE
  - Quick check question: If a model predicts 5.2m for a ground truth of 5.0m, what MRA score does it receive at θ=0.95 threshold?

## Architecture Onboarding

- **Component map**: Blender simulation → Lab capture → Internet scraping → Segmentation (SAM2) → Annotation (pixel ruler + scale conversion) → VLM evaluation → Numerical parsing → MRA computation → Analysis

- **Critical path**: Frame extraction with full temporal fidelity → Prompt construction with standardized format → Numerical parsing via delimiter search → MRA computation across thresholds → Aggregation by task category

- **Design tradeoffs**: 480p resolution prioritizes temporal fidelity over spatial detail; temperature=0 ensures reproducibility despite API instability; segmented backgrounds enable controlled analysis but reduce ecological validity; 2D tasks restricted to planar motion, 3D tasks require depth priors

- **Failure signatures**: Verbose responses from Claude/Fuyu/MiniCPM requiring delimiter-based parsing; zero outputs treated as MRA=0; prior-only condition matching video+prior indicating world knowledge dominance; CoT degradation showing error propagation

- **First 3 experiments**: 1) Baseline evaluation on all 21 VLMs with standardized prompts, establishing MRA distribution across task categories. 2) Faithfulness probe comparing video+prior vs prior-only conditions to assess visual evidence utilization. 3) Counterfactual stress test with priors multiplied by {0.1, 10, 100, 1000} to measure scaling behavior.

## Open Questions the Paper Calls Out

### Open Question 1
What training methodologies can enable VLMs to faithfully use provided visual evidence and numerical priors rather than defaulting to memorized world knowledge during quantitative physical reasoning? The counterfactual prior experiments showed VLMs ignore altered numerical inputs and stick to canonical physical constants, suggesting this requires new training approaches.

### Open Question 2
How would extending QuantiPhy to include rotational dynamics and deformable objects affect the observed performance gap between VLMs and human baselines? The current benchmark focuses exclusively on translational movement and rigid objects, which may underestimate the true gap for more complex kinematic scenarios.

### Open Question 3
What architectural modifications would enable VLMs to reliably perform the intermediate sub-tasks required for chain-of-thought quantitative reasoning (pixel measurement, scale estimation, unit conversion)? CoT experiments showed degradation because models cannot reliably solve intermediate numeric subproblems, causing error propagation.

### Open Question 4
Can physics-informed training objectives or specialized pre-training on physics-rich data close the gap between current VLM performance (~50% MRA) and the theoretical ceiling that pixel-precise computation could achieve? The paper suggests this direction but does not implement or test such approaches.

## Limitations
- The benchmark focuses exclusively on translational movement, omitting rotational dynamics and deformable objects
- Controlled synthetic and segmented environments may overestimate model capabilities in ecological settings
- Interpretation of world knowledge dominance vs. absence of visual reasoning capability remains difficult to validate definitively

## Confidence

- **High confidence**: Benchmark design, MRA metric implementation, and basic quantitative findings are robust and reproducible
- **Medium confidence**: Interpretation of VLMs' reliance on world knowledge is well-supported but doesn't definitively prove absence of visual reasoning
- **Low confidence**: Claims about VLMs' "physical understanding" versus pattern matching are difficult to validate without intervention studies

## Next Checks
1. Fine-tune a subset of VLMs on kinematic estimation tasks and re-evaluate on QuantiPhy to determine if performance improves beyond prior scaling
2. Test VLMs with explicit measurement tools to see if this mitigates world knowledge dominance
3. Evaluate the same VLMs on QuantiPhy instances embedded in complex, unsegmented real-world videos to assess ecological validity