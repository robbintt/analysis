---
ver: rpa2
title: Uncovering the Computational Ingredients of Human-Like Representations in LLMs
arxiv_id: '2510.01030'
source_url: https://arxiv.org/abs/2510.01030
tags:
- alignment
- other
- food
- human
- animal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates how different computational ingredients of
  large language models (LLMs) affect their alignment with human conceptual representations.
  Using 77 diverse models and 128 object concepts from the THINGS dataset, the authors
  measure human-model alignment through triadic similarity judgments and ordinal embeddings.
---

# Uncovering the Computational Ingredients of Human-Like Representations in LLMs

## Quick Facts
- arXiv ID: 2510.01030
- Source URL: https://arxiv.org/abs/2510.01030
- Reference count: 40
- Primary result: Instruction fine-tuning and architectural dimensionality (MLP/embedding dimensions) are the strongest predictors of human conceptual alignment, while model size and multimodal pretraining show limited effects.

## Executive Summary
This paper investigates which computational factors make large language models' representations more human-like by testing 77 diverse models on semantic similarity judgments. The authors find that instruction fine-tuning and larger architectural dimensionality (MLP and embedding dimensions) are most predictive of alignment with human conceptual representations, while model size, training data, and multimodal pretraining have limited effects. Importantly, no existing benchmark fully captures alignment variance, though MMLU and BigBenchHard correlate best. These results suggest focusing on post-training and architectural dimensionality rather than scale to build LLMs with human-like conceptual representations.

## Method Summary
The study evaluates 77 language models ranging from 7M to 70B parameters across various architectures and training regimes. Human alignment is measured using 128 object concepts from the THINGS dataset, with models providing similarity judgments on 35,000 triplet comparisons. Ordinal embeddings are derived from these judgments and compared to human embeddings using Procrustes alignment. A mixed-effects model predicts alignment from factors including instruction tuning, architecture, training data, and benchmark performance, with MMLU and BigBenchHard as key benchmarks.

## Key Results
- Instruction fine-tuning is the strongest single predictor of human-like semantic representations, clustering within-category items closer and between-category items further apart
- Architectural dimensionality (MLP and embedding layers) contributes more to alignment than raw parameter count, with larger dimensions enabling finer-grained semantic distinctions
- Multimodal pretraining shows no independent positive effect and may harm alignment after controlling for other factors, suggesting text-only models may be preferable for conceptual similarity tasks
- No existing benchmark fully captures alignment variance, though MMLU and BigBenchHard correlate best (r≈0.7) while leaving substantial unexplained variance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction fine-tuning is the strongest single predictor of human-like semantic representations.
- Mechanism: Instruction tuning reshapes representational geometry by clustering within-category items closer together and pushing between-category items further apart, producing categorical structure more isomorphic to human conceptual organization through exposure to explicit semantic reasoning tasks.
- Core assumption: The categorical clustering observed in embeddings reflects genuine reorganization of internal representations, not artifacts of the embedding method or prompt format.
- Evidence anchors: [abstract] "models that undergo instruction-finetuning and which have larger dimensionality of attention heads are among the most human aligned"; [section 4.1] "instruction fine-tuning clusters the representations of semantic categories"; [corpus] Related work (Marjieh et al. 2024a,b; Tong et al. 2024) similarly finds instruction-tuned models show advantages in human alignment tasks.

### Mechanism 2
- Claim: Architectural dimensionality (MLP and embedding layer size) contributes more to alignment than raw parameter count.
- Mechanism: Larger hidden and MLP dimensions increase the expressivity of individual representational layers, potentially allowing finer-grained distinctions along semantic axes humans also encode; higher dimensionality enables learning more semantically meaningful dimensions rather than simply memorizing statistical regularities.
- Core assumption: Increased dimensionality enables learning semantically meaningful dimensions rather than fitting training data noise.
- Evidence anchors: [abstract] "model architectural dimensionality (MLP and embedding layers) and context length" are among key predictors; [section 4.1] Mixed-effects model: MLP (β=0.049, p<0.01), embedding layers (β=0.048, p<0.05); attention head count showed negative coefficient (β=-0.045, p<0.05) after controlling for other factors; [corpus] Muttenthaler et al. (2022a) found similar dissociation in vision models—training data and objectives predicted alignment while architecture and scale had minimal impact.

### Mechanism 3
- Claim: Multimodal pretraining does not improve text-based semantic alignment and may harm it when other factors are controlled.
- Mechanism: Multimodal training may introduce competing representational pressures—visual features correlate with but differ from semantic features relevant to conceptual similarity, potentially diluting or distorting text-only semantic structure.
- Core assumption: Current image-text contrastive training does not naturally align visual and linguistic semantic spaces in a way that benefits pure conceptual reasoning.
- Evidence anchors: [section 4.1] "multimodal pretraining, which showed no independent effect on alignment, predicted reliably lower alignment after accounting for other factors (β=-0.102, p<0.05)"; [corpus] Mixed findings in prior work: Yuksekgonul et al. (2023) and Qin et al. (2025) suggest vision-language training can help taxonomic knowledge deployment, but mechanisms remain unclear.

## Foundational Learning

- Concept: Representational Similarity Analysis (RSA) and Procrustes alignment
  - Why needed here: The paper uses these methods to compare model and human embeddings. RSA compares similarity matrix structures; Procrustes finds optimal linear transformations between spaces. Both assume representations can be compared geometrically.
  - Quick check question: If two models have identical Procrustes R² scores with human embeddings, do they necessarily have identical internal representations?

- Concept: Ordinal embedding from triplet judgments
  - Why needed here: The paper derives embeddings from model similarity judgments using ordinal constraints ("A is more similar to B than to C"). Sample complexity scales as O(nd log n) where n = items, d = dimensions.
  - Quick check question: Why might 35,000 triplet judgments be sufficient for 128 concepts in 30 dimensions when the full similarity matrix would require ~8,000 entries?

- Concept: Species-fair comparison in human-model evaluation
  - Why needed here: The paper argues for comparing models and humans using the same task format (triplet judgments) rather than comparing model activations to human behavior, avoiding assumptions about which layers encode semantic knowledge.
  - Quick check question: What biases might remain even when using identical task formats, given that models and humans process language through fundamentally different architectures?

## Architecture Onboarding

- Component map: Input layer (token embeddings + positional encodings) -> Transformer blocks (multi-head attention + MLP with residual connections) -> Output (logits over vocabulary)
- Critical path: When selecting or designing models for human alignment: 1) Prioritize instruction-tuned variants over base models (largest effect size), 2) Favor higher embedding and MLP dimensions over parameter count, 3) Prefer longer context windows (correlated with alignment, possibly proxying for training quality), 4) Do not assume multimodal pretraining helps text-based conceptual tasks
- Design tradeoffs: Scale vs. dimensionality (14B with optimized dimensions may align better than 40B with narrow layers), Multimodality vs. text-only (text-only models with strong instruction tuning may outperform vision-language models for semantic tasks), Benchmark optimization vs. alignment (MMLU/BBH scores correlate with alignment but leave substantial unexplained variance)
- Failure signatures: Base models scoring high on benchmarks but low on alignment, Multimodal models underperforming equivalent text-only models on semantic similarity tasks, High parameter count with low dimensionality showing poor alignment despite strong benchmark performance
- First 3 experiments: 1) Ablation on instruction tuning stages using checkpointed models (OLMo, Instella) to quantify incremental gains; 2) Dimension vs. parameter sweep training models matching parameter counts but varying embedding/MLP dimensions; 3) Cross-modal transfer test comparing text-only and vision-language variants of same architecture on triplet similarity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the composition of pretraining datasets (e.g., text-code ratio, linguistic diversity) affect representational alignment with humans?
- Basis in paper: [explicit] The authors state in the Limitations section that they "could not assess the role of training dataset composition (e.g., text–code balance, language coverage) due to limited documentation for many models."
- Why unresolved: Most open-weight models lack detailed documentation regarding the specific makeup of their training corpora, preventing a controlled analysis of content quality versus quantity.
- What evidence would resolve it: Training a suite of models with controlled variations in dataset composition (e.g., code-heavy vs. natural language-heavy) while keeping parameter counts fixed, then measuring the resulting alignment.

### Open Question 2
- Question: Why does current vision-language pretraining fail to improve, or potentially hinder, alignment with human semantic structure?
- Basis in paper: [explicit] The authors found that multimodal pretraining "predicted reliably lower alignment after accounting for other factors" in the mixed-effects model, contrasting with the intuition that human concepts are multimodal.
- Why unresolved: The study isolated the correlation but did not determine if current multimodal training regimes introduce noise, diverge structurally from text-based embeddings, or suffer from implementation issues.
- What evidence would resolve it: A systematic comparison of representation geometry in text-only versus multimodal variants of the same base model to identify specific structural distortions introduced by visual training.

### Open Question 3
- Question: Do the computational ingredients that predict human-like similarity judgments (e.g., instruction tuning) also support the human-like deployment of semantic knowledge in open-ended reasoning?
- Basis in paper: [explicit] The authors note that "similarity judgements... may not capture... how models deploy this knowledge in open-ended reasoning tasks," and explicitly suggest future work study such tasks.
- Why unresolved: This study focused exclusively on the triplet similarity paradigm; high alignment in this static task does not guarantee aligned behavior in dynamic, generative contexts.
- What evidence would resolve it: Correlating the alignment scores derived from triplet tasks with performance on generative benchmarks designed to test the flexible application of semantic concepts.

### Open Question 4
- Question: What specific representational changes occur during the stages of post-training (SFT, DPO, RLVR) that drive increased alignment?
- Basis in paper: [inferred] The paper observes that alignment increases with post-training stages but notes the "effects of RLVR was more muted" and does not fully explain the mechanism behind the clustering effect observed in instruction-tuned models.
- Why unresolved: The study treats post-training stages as categorical variables without analyzing the internal representational shifts that distinguish SFT from DPO or RLVR in terms of semantic geometry.
- What evidence would resolve it: Analyzing layer-wise representational shifts (e.g., using Centered Kernel Alignment) at each checkpoint stage to map how semantic geometry evolves during alignment processes.

## Limitations

- Sample and task constraints: The alignment measurements rely on 128 object concepts from the THINGS dataset and 35,000 triplet judgments, which may not capture the full range of semantic phenomena models encounter in practice.
- Correlation vs. causation: The mixed-effects modeling identifies predictors of alignment but cannot establish causal mechanisms, leaving unclear whether instruction tuning reorganizes internal representations or merely teaches output patterns.
- Architectural confounds: Model architectural variables correlate with each other and with other training factors, preventing definitive isolation of whether increased dimensionality directly improves semantic representation.

## Confidence

- High confidence: The finding that instruction fine-tuning is the strongest single predictor of alignment is robust across the diverse model set and aligns with prior work showing instruction-tuned models better capture human-like semantic judgments.
- Medium confidence: The claim that architectural dimensionality (embedding and MLP layers) contributes more to alignment than parameter count has strong statistical evidence but uncertain underlying mechanism regarding whether larger dimensions enable finer semantic distinctions or simply better fit training data.
- Low confidence: The claim that multimodal pretraining reliably harms text-based semantic alignment contradicts some prior work and may depend heavily on specific training objectives and architectures not fully explored here.

## Next Checks

1. **Cross-dataset generalization test**: Validate alignment findings using semantic similarity judgments from different datasets (e.g., USF free association norms, WordNet-based similarity) to ensure results generalize beyond the THINGS dataset and triplet format.

2. **Mechanistic ablation study**: Use targeted intervention experiments (e.g., fine-tuning with controlled semantic tasks) to test whether instruction tuning actually reorganizes internal representations or merely teaches models to produce human-like outputs without structural changes.

3. **Benchmark alignment gap analysis**: Systematically compare which specific benchmark items/models align with human judgments versus which do not, to identify systematic gaps between current evaluation metrics and human conceptual organization.