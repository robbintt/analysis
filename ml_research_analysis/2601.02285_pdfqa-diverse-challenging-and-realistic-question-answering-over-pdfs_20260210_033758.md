---
ver: rpa2
title: 'pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs'
arxiv_id: '2601.02285'
source_url: https://arxiv.org/abs/2601.02285
tags:
- question
- answer
- sources
- pairs
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces pdfQA, a large-scale dataset for question
  answering over PDF documents. It includes 2K human-annotated QA pairs (real-pdfQA)
  from nine diverse benchmarks and 2K synthetically generated QA pairs (syn-pdfQA)
  covering ten complexity dimensions such as answer type, reasoning level, and source
  spread.
---

# pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs

## Quick Facts
- arXiv ID: 2601.02285
- Source URL: https://arxiv.org/abs/2601.02285
- Authors: Tobias Schimanski; Imene Kolli; Jingwei Ni; Yu Fan; Ario Saeid Vaghefi; Elliott Ash; Markus Leippold
- Reference count: 27
- Primary result: Introduces pdfQA, a benchmark dataset for PDF QA with 2K human-annotated and 2K synthetically generated QA pairs, rigorously filtered for quality and difficulty across ten complexity dimensions.

## Executive Summary
pdfQA is a large-scale dataset designed to evaluate end-to-end question answering systems on PDF documents. It combines human-annotated questions from nine benchmarks (real-pdfQA) with synthetically generated questions covering ten complexity dimensions (syn-pdfQA). Both datasets undergo rigorous filtering for quality (formality, internal and external validity) and difficulty (questions unsolvable by GPT-4o-mini). Experiments using open-source LLMs reveal that performance correlates with specific dimensions like modality and source spread, with human-annotated questions proving more challenging than synthetic ones.

## Method Summary
The method involves generating QA pairs from diverse PDF domains (financial, research, books, sustainability) using either human annotations or synthetic generation via GPT-4.1. Synthetic pairs are created from raw file formats (.tex, .html, .csv) and guided by complexity dimensions. Both real and synthetic datasets are filtered through three stages: formality check, internal and external validity verification using semantic search, and difficulty filtering using GPT-4o-mini. The final datasets are evaluated using open-source LLMs with long-context prompting, scored by G-Eval.

## Key Results
- Human-annotated questions (real-pdfQA) are more challenging than synthetic ones (syn-pdfQA).
- Performance degradation correlates with complexity dimensions, especially modality (tables vs. text) and reasoning requirements.
- Over 58% of real-pdfQA and 78% of synthetic data are filtered out as too easy or invalid, ensuring a high-quality benchmark.
- "Lost in the Middle" phenomenon observed: middle-positioned sources degrade performance compared to sources at the beginning or end of documents.

## Why This Works (Mechanism)

### Mechanism 1: Complexity-Dimension Correlation
The dataset taxonomy isolates variables like source type and reasoning level. By controlling these during generation and evaluation, pdfQA reveals that models struggle more with value extraction from tables and multi-source reasoning compared to simple text replication. This works because the underlying parsing preserves enough semantic structure for differential processing.

### Mechanism 2: Conservative Validity Filtering
A three-stage filtering pipeline (formality, internal validity, external validity) removes questions that rely on local context or are contradicted elsewhere in the document. This ensures the remaining dataset presents a valid challenge for end-to-end systems by eliminating context-dependent queries.

### Mechanism 3: Difficulty Filtering via Small LLM Adversarialism
Using GPT-4o-mini to solve generated questions acts as a high-pass filter, retaining only those requiring stronger reasoning. Questions solvable by the small model are discarded, ensuring the benchmark targets the performance gap between smaller and larger models.

## Foundational Learning

- **Concept: Context Window Utilization**
  - Why needed: Experiments rely on feeding the entire parsed PDF text (up to 128k tokens) into the context window. Understanding "Lost in the Middle" phenomena is critical for interpreting results.
  - Quick check: If a relevant source is at position 50% of the document, does model performance drop compared to positions 0% or 100%?

- **Concept: Synthetic Data Generation (SDG)**
  - Why needed: Half the dataset is generated via LLM from raw file formats. The "ground truth" here is derived from a perfect source but generated by a model, unlike human-annotated data.
  - Quick check: How does the "source spread" dimension test a model's ability to connect distant information?

- **Concept: LLM-as-a-Judge (G-Eval)**
  - Why needed: The paper evaluates answers using a G-Eval prompt (score 1-5) rather than exact string match, better handling open-ended answers.
  - Quick check: Does the evaluation prompt score based on factual correctness against ground truth, or faithfulness to provided context?

## Architecture Onboarding

- **Component map:** PDF files + Raw Data tuples -> PyMuPDF Parser (real) or GPT-4.1 Generator (synthetic) -> Filtering Gate (Formality -> Internal Validity -> External Validity -> Difficulty Filter) -> Evaluation (Long-context prompt -> Open Source LLM -> G-Eval Scorer)
- **Critical path:** The Filtering Gate. Without quality and difficulty filters, 67.5% of synthetic data and 78% of real data would be "too easy," rendering the benchmark useless.
- **Design tradeoffs:** Long-context prompting vs. RAG (paper uses long-context only); Synthetic vs. Real (synthetic allows control over 10 dimensions, real is more natural but messier).
- **Failure signatures:** "Lost in the Middle" (lower scores for middle-positioned sources); Table Parsing Artifacts (performance drops on tables); Context Ambiguity (high filter rejection due to local questions).
- **First 3 experiments:**
  1. Run PyMuPDF on a subset of syn-pdfQA PDFs and compare parsed text against "Raw Data" ground truth to measure information loss, especially in tables.
  2. Implement the Difficulty Filter on 100 random QA pairs and verify if local GPT-4o-mini implementation agrees with the paper's keep/discard decisions.
  3. Using syn-pdfQA, bucket results by "Source Position" and verify that middle-positioned sources degrade performance in your specific model architecture.

## Open Questions the Paper Calls Out
- How do RAG pipelines and visual language models perform on pdfQA compared to long-context baselines? (The paper does not evaluate RAG or visual models, focusing only on long-context parsed text.)
- Can the automated filtering pipeline be refined to reduce over-conservative rejection of valid QA pairs? (The paper notes filters may be too conservative, suggesting follow-up research could improve them.)
- To what extent does reliance on text-based parsing (PyMuPDF) introduce noise or bias compared to native visual understanding? (The paper acknowledges ignoring visual layout cues, potentially underestimating spatial reasoning difficulty.)

## Limitations
- Experiments are restricted to long-context prompting with parsed text, not evaluating RAG or visual models.
- Synthetic data generation and filtering may introduce variability due to underspecified prompt engineering and future-dated model versions.
- Lack of publicly available dataset files and paired raw documents hinders exact reproduction.

## Confidence

- **High Confidence:** Core methodology of quality/difficulty filtering and correlation between performance degradation and complexity dimensions.
- **Medium Confidence:** Synthetic data generation process and control over ten complexity dimensions.
- **Low Confidence:** Generalizability of "Lost in the Middle" findings and extent to which difficulty filter measures true reasoning vs. context limits.

## Next Checks

1. **Parser Fidelity Test:** Run PyMuPDF on a representative subset of PDF files and compare parsed text against "Raw Data" ground truth to quantify information loss, especially for tables.
2. **Filter Reproducibility:** Implement the complete filtering pipeline on a sample of 100 QA pairs using GPT-4o-mini and compare retention rate and quality against the paper's reported 88-91% correctness.
3. **Positional Bias Validation:** Conduct an ablation study on syn-pdfQA, grouping results by "Source Position" (0%, 50%, 100%) to verify that middle-positioned sources lead to lower performance scores.