---
ver: rpa2
title: Scaling Self-Supervised Representation Learning for Symbolic Piano Performance
arxiv_id: '2506.23869'
source_url: https://arxiv.org/abs/2506.23869
tags:
- music
- arxiv
- learning
- symbolic
- piano
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates scaling self-supervised learning for symbolic
  piano performance modeling. The authors introduce Aria, a large-scale pretrained
  transformer model trained on approximately 60,000 hours of transcribed piano recordings
  from the Aria-MIDI dataset.
---

# Scaling Self-Supervised Representation Learning for Symbolic Piano Performance

## Quick Facts
- **arXiv ID:** 2506.23869
- **Source URL:** https://arxiv.org/abs/2506.23869
- **Reference count:** 0
- **Primary result:** Large-scale self-supervised pretraining on transcribed piano audio enables strong generative and discriminative symbolic music modeling.

## Executive Summary
This paper demonstrates that scaling self-supervised learning to ~60,000 hours of transcribed piano audio enables effective symbolic music modeling. The authors introduce Aria, a transformer pretrained on the Aria-MIDI dataset using next-token prediction, and show it excels at both generative piano continuation and discriminative representation learning. A key innovation is the use of absolute onset timing within fixed-length segments to avoid the temporal drift issues that plague relative-timing approaches. The model achieves state-of-the-art results on music information retrieval benchmarks and outperforms existing symbolic generation methods in human listening tests.

## Method Summary
The authors train a 650M-parameter transformer (LLaMA-style architecture) on 60,473 hours of transcribed piano MIDI using next-token prediction. They introduce a novel tokenizer with absolute onset times within 5000ms segments marked by `<T>` tokens, avoiding the temporal drift issues of relative timing approaches. For representation learning, they finetune the pretrained model with contrastive learning (SimCLR-style) using NT-Xent loss, showing this contrastive phase is only effective when initialized from the generative pretraining. The model is evaluated on generative continuation (human pairwise preference tests), supervised classification finetuning (linear probes on MIR benchmarks), and contrastive embedding learning.

## Key Results
- Human listening tests show Aria outperforms state-of-the-art symbolic generation methods and remains competitive with proprietary audio models
- Contrastive adaptation achieves state-of-the-art linear probe results on MIR classification benchmarks
- Supervised finetuning demonstrates strong generalization requiring only hundreds of labeled examples
- The proposed absolute-onset tokenization effectively prevents temporal drift in generated music

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Absolute onset timing within fixed segments prevents temporal drift better than relative time-shift tokens
- **Mechanism:** Relative timing requires summing sequential time-shift values for non-adjacent notes, imposing arithmetic burden on transformers. Absolute timing uses a single reference lookup per segment, reducing cumulative error.
- **Core assumption:** Transformers struggle with serial arithmetic accumulation required by relative timing.
- **Evidence anchors:** Section 3.1 cites evidence that time-shift tokens cause unstable rhythm; Equation 1 contrasts arithmetic complexity.

### Mechanism 2
- **Claim:** Contrastive representation learning only works when initialized from generative pretraining
- **Mechanism:** Generative pretraining establishes musical syntax and structure, providing a warm start for contrastive learning. Training from scratch fails because contrastive objectives alone are insufficient for complex musical syntax.
- **Core assumption:** Next-token prediction features are a subset of features required for semantic similarity.
- **Evidence anchors:** Abstract states contrastive approach effective only as secondary finetuning; Table 2 shows 1 epoch from pretrained beats 25 epochs from scratch.

### Mechanism 3
- **Claim:** Aggressive deduplication and quality filtering prevents model memorization
- **Mechanism:** Large AMT datasets contain thousands of identical performances. By capping instances per opus and filtering via heuristics (density, entropy), the model learns generalizable patterns rather than memorizing specific recordings.
- **Core assumption:** Long tail of unique performances contains sufficient signal for generalization.
- **Evidence anchors:** Section 3.2 describes capping at 10 instances per opus; Section 4.2 attributes success to careful data curation.

## Foundational Learning

- **Concept: Self-Supervised Learning (SSL) in Modalities**
  - **Why needed here:** The entire Aria framework is SSL. Understanding generative (next-token) vs. discriminative (contrastive) SSL is essential.
  - **Quick check question:** Why use next-token prediction initially but switch to contrastive loss for embeddings?

- **Concept: Tokenization & Quantization**
  - **Why needed here:** The paper proposes specific tokenizer converting continuous MIDI to discrete tokens. Understanding the mapping is crucial for debugging.
  - **Quick check question:** How does the tokenizer represent a note at 5100ms relative to 5000ms segment limit?

- **Concept: The SimCLR Framework**
  - **Why needed here:** Authors adapt SimCLR for symbolic music. Understanding views, augmentations, and the push-pull objective is key.
  - **Quick check question:** What constitutes a "view" in this context, and why use two slices of the same MIDI file?

## Architecture Onboarding

- **Component map:** MIDI file -> Note List (Pitch, Velocity, Onset, Duration) -> Tokenizer (Groups into 5s segments -> Token IDs (Absolute Onset + Duration)) -> Backbone (Transformer Decoder, LLaMA 3.2 variant, ~650M params) -> Heads (Swappable: LM Head for pretraining, Projection Head for Contrastive learning)

- **Critical path:**
  1. Data Prep: Raw MIDI -> filter duplicates -> tokenize with `<T>` segmentation
  2. Pretraining: Next-token prediction on 60k hours (9 days on 8x H100s)
  3. Adaptation: Generative finetune on quality subset OR Replace LM head with projection head; train with NT-Xent loss using random slices

- **Design tradeoffs:**
  - Absolute vs. Relative Time: Trades larger vocabulary for arithmetic stability
  - Model Size vs. Data Scale: Reduced hidden dimension (1536 vs 2048) to balance capacity against 60k hour dataset

- **Failure signatures:**
  - Drifting Rhythm: Relative timing causes incoherent speeding/slowing
  - Collapse to Noise: Contrastive training from scratch fails (Table 2, `Ariaâ€ `)
  - Memorization: Without deduplication, model reproduces famous performances

- **First 3 experiments:**
  1. Tokenizer Validation: Verify segment boundary logic with notes at 4999ms and 5001ms
  2. Overfitting Sanity Check: Finetune on single MIDI file, verify loss approaches zero
  3. Linear Probe Baseline: Freeze pretrained backbone, train linear classifier on composer classification

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the absolute-onset framework generalize to multi-track, multi-instrument symbolic music modeling?
- **Basis in paper:** [explicit] Authors state work focuses exclusively on solo piano, serving as "foundation for future research"
- **Why unresolved:** Experiments, benchmarks, and protocols are restricted to solo piano
- **What evidence would resolve it:** Successful application to multi-track datasets with multi-instrument evaluations

### Open Question 2
- **Question:** How does Aria compare to score-based models (e.g., MuPT) using ABC notation?
- **Basis in paper:** [explicit] Authors note incompatibility between millisecond-level MIDI and bar-level ABC notation
- **Why unresolved:** Direct comparison impossible due to timing representation incompatibility
- **What evidence would resolve it:** Standardized benchmark or conversion pipeline for cross-format evaluation

### Open Question 3
- **Question:** Does absolute-onset tokenization impose a ceiling on modeling long-term structural dependencies?
- **Basis in paper:** [inferred] Authors acknowledge transformer struggles with arithmetic and evaluate coherence on 45-second generations
- **Why unresolved:** Segmentation strategy's impact on full-piece structure not analyzed
- **What evidence would resolve it:** Comparative analysis over longer durations (>5 minutes) against beat-aware tokenizers

## Limitations

- Temporal generalization remains uncertain for free-form improvisational styles or extremely long-form classical works
- Data quality filtering thresholds are not fully specified, making exact reproduction difficult
- Human evaluation provides ordinal rankings but doesn't establish absolute quality levels
- Compute optimality of the 650M parameter model size is reasonable but not definitively proven

## Confidence

**High Confidence**: Contrastive representation learning requires initialization from generative pretraining (supported by direct ablation results)

**Medium Confidence**: Effectiveness of absolute onset timing versus relative timing for preventing temporal drift (theoretically sound but lacks direct ablation)

**Medium Confidence**: Overall superiority of Aria for symbolic generation tasks (human preference tests show advantages but lack absolute quality benchmarks)

## Next Checks

1. **Tokenizer ablation study**: Train identical architectures using absolute onset, relative time-shift, and hybrid approaches. Generate continuations and measure temporal stability across 30+ second generations.

2. **Pretraining data scale sensitivity**: Train models on 10%, 25%, 50%, and 100% of Aria-MIDI. Evaluate generative quality and downstream classification accuracy to establish data scaling curve.

3. **Contrastive learning phase ablation**: Systematically vary contrastive finetuning epochs (0, 1, 5, 10, 25) from generative checkpoint. Measure impact on MIR benchmark performance to quantify marginal benefits.