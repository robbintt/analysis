---
ver: rpa2
title: A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior
arxiv_id: '2506.19999'
source_url: https://arxiv.org/abs/2506.19999
tags:
- fixation
- fixations
- reading
- word
- surprisal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a spatio-temporal point process model for
  reading behavior that jointly captures when fixations occur, where they land in
  space, and how long they last. The model uses a Hawkes process for saccade planning,
  capturing self-excitation where past fixations increase the likelihood of future
  fixations nearby in time and space, and a log-normal distribution with convolution-based
  spillover effects for fixation durations.
---

# A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior

## Quick Facts
- **arXiv ID:** 2506.19999
- **Source URL:** https://arxiv.org/abs/2506.19999
- **Reference count:** 37
- **Primary result:** A spatio-temporal Hawkes process model jointly captures when fixations occur, where they land, and how long they last, revealing that reader-specific effects and spatial shifts substantially improve prediction while linguistic predictors provide only marginal gains.

## Executive Summary
This paper introduces a novel spatio-temporal point process model for fine-grained reading behavior that integrates saccade planning, fixation durations, and linguistic influences. The model uses a Hawkes process to capture how past fixations excite future fixations nearby in time and space, combined with a log-normal distribution for fixation durations that includes convolution-based spillover effects. Evaluations on the MECO corpus show that incorporating reader-specific effects and constant spatial shifts substantially improves saccade prediction compared to baselines, while adding linguistic predictors like contextual surprisal yields only marginal improvements. The authors find that convolution-based spillover modeling provides little benefit over simpler Markov assumptions, and surprisal effects are much smaller when modeling fine-grained scanpaths compared to aggregated gaze measurements.

## Method Summary
The model jointly models three aspects of reading behavior: fixation timing via a Hawkes process, fixation locations via a bivariate Gaussian spatial density, and fixation durations via a log-normal distribution with convolution-based spillover. The intensity function sums contributions from all prior fixations using exponentially decaying temporal kernels and Gaussian spatial densities. Reader-specific effects are incorporated through predictor vectors, and a constant spatial shift captures the left-to-right reading tendency. The model is trained on the MECO corpus and evaluated against baselines including Poisson, homogeneous Hawkes, and covariate-specific Hawkes processes.

## Key Results
- Reader-specific effects and constant spatial shifts substantially improve saccade prediction compared to generic baselines
- Adding linguistic predictors like contextual surprisal yields only marginal improvements (<2% relative gain in per-fixation log-likelihood)
- Convolution-based spillover modeling provides little benefit over simpler Markov assumptions for fixation durations
- Models trained on full scanpaths (including non-word fixations) outperform those trained on filtered scanpaths
- Surprisal effects are much smaller when modeling fine-grained scanpaths compared to aggregated gaze measurements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A spatio-temporal Hawkes process captures self-excitation in reading behavior, where past fixations increase the likelihood of future fixations nearby in time and space.
- **Mechanism:** The intensity function (Eq. 7) sums contributions from all prior fixations via exponentially decaying temporal kernels (Eq. 9) and Gaussian spatial densities (Eq. 10). Each fixation excites subsequent fixations proportionally to `h(x^T_m · α)` with decay rate `h(x^T_m · β)`.
- **Core assumption:** Past fixations influence future fixation probability additively and independently; excitation strength and decay rate can be modulated by fixation-specific predictors (e.g., surprisal, word length).
- **Evidence anchors:**
  - [abstract] "The saccades are modeled using a Hawkes process, which captures how each fixation excites the probability of a new fixation occurring near it in time and space."
  - [section 3.1] Eq. 7 defines the intensity function; Eq. 9 parameterizes temporal kernels with predictor-dependent excitation and decay.
  - [corpus] Weak direct corpus support for Hawkes processes in reading; neighboring papers focus on fixation prediction and eye-tracking datasets without spatio-temporal point processes.
- **Break condition:** If fixation sequences exhibit strong inhibition (e.g., saccadic suppression periods that prevent new fixations regardless of history) rather than pure excitation, the Hawkes assumption may fail.

### Mechanism 2
- **Claim:** Reader-specific effects and a learned constant spatial shift substantially improve saccade prediction over generic models.
- **Mechanism:** The spatial transformation function `μ_m(s)` is parameterized as an affine shift (Eq. 11: `As + b`) capturing left-to-right reading tendency, plus reader-specific embeddings via predictor vectors (Eq. 19: `x_m = 1 ⊕ u_m`). The RSE model learns individual excitation/decay parameters.
- **Core assumption:** Individual readers have stable, learnable oculomotor tendencies; the rightward shift approximates the dominant reading direction in English.
- **Evidence anchors:**
  - [abstract] "incorporating reader-specific effects and constant spatial shifts substantially improves saccade prediction compared to baselines"
  - [section 4.1.2] "Parameter estimates from this model suggest a consistent global rightward shift of roughly ≈ 10.61 characters... a pattern consistent with the left-to-right progression"
  - [corpus] Not addressed in neighboring papers.
- **Break condition:** In right-to-left languages or multi-directional reading scenarios, the affine spatial shift assumption would require reparameterization.

### Mechanism 3
- **Claim:** Linguistic predictors (contextual surprisal, unigram surprisal) provide only marginal improvements for fine-grained saccade planning beyond reader-specific effects and spatial shifts.
- **Mechanism:** Predictors are incorporated into both temporal kernels (modulating excitation/decay) and spatial means (Eq. 12). The model compares RSE baseline against RSE + predictors.
- **Core assumption:** If surprisal theory strongly influences eye movements, adding surprisal should yield substantial likelihood gains beyond spatial/temporal baselines.
- **Evidence anchors:**
  - [abstract] "adding linguistic predictors like contextual surprisal yields only marginal improvements"
  - [section 4.1.2] "most predictors yield under 2% relative gain in per-fixation log-likelihood... Word length displays the highest predictive power, leading to performance improvements by approximately 4%"
  - [corpus] Weak corpus connection; neighboring papers don't test surprisal effects on scanpaths.
- **Break condition:** Assumption: Surprisal effects may be stronger at word-level aggregation than at fixation-level; if true, this mechanism explains why fine-grained models show smaller effects.

## Foundational Learning

- **Concept: Hawkes process (self-exciting point process)**
  - Why needed here: Core of the saccade model; must understand intensity functions, temporal kernels, and how past events excite future ones.
  - Quick check question: Given three past fixations with times t₁=0, t₂=0.1, t₃=0.3 and current time t=0.5, compute the intensity contribution from each if α=10 and β=20 (exponential kernel).

- **Concept: Log-normal distribution for duration modeling**
  - Why needed here: Fixation durations are right-skewed; log-normal provides interpretable parameters and good fit (App. D shows it outperforms Weibull, exponential, Rayleigh).
  - Quick check question: If log(duration) ~ N(μ=5.0, σ²=0.25), what is the median duration in milliseconds?

- **Concept: Spillover effects in reading**
  - Why needed here: Cognitive processing at one word can affect fixations on subsequent words; the paper tests convolution-based vs. Markov spillover modeling.
  - Quick check question: Explain why the paper finds convolution-based spillover (unbounded history) provides little benefit over Markov assumptions (bounded lags).

## Architecture Onboarding

- **Component map:** Intensity function (temporal kernel × spatial density) → Spatial transformation (affine shift + predictor modulation) → Duration model (log-normal with spillover) → Full density function (Hawkes density with integral term)

- **Critical path:** Reader-specific embedding (u_m) → temporal/spatial parameters → intensity λ(t,s) → sample fixation (t_n, s_n) → duration model → sample d_n → update history

- **Design tradeoffs:**
  - Isotropic vs. anisotropic spatial kernel (paper uses isotropic; reviewer noted this may miss directional biases)
  - Full vs. filtered scanpaths (52.3% of fixations fall outside word bounding boxes; full data improves fit)
  - Convolution vs. Markov spillover (similar performance; Markov is simpler)

- **Failure signatures:**
  - Gradient instability when adding line-transition features (paper attempted distance-to-right-margin feature; failed due to sparse gradients near line endings)
  - Poor generalization if trained only on filtered scanpaths (Fig. 2 shows models trained on full scanpaths outperform)
  - Overfitting with too many predictors without reader-specific interactions

- **First 3 experiments:**
  1. Replicate the Poisson → Hawkes → CSS → RSE progression on MECO English data; verify ~1047% likelihood improvement over Poisson baseline.
  2. Ablate the spatial shift parameter: train with μ_m(s)=s (identity) vs. μ_m(s)=As+b; quantify the contribution of directional bias.
  3. Compare convolution-based spillover (Eq. 16) against Markov spillover (Eq. 18) with lags l=1,2,3 on fixation duration prediction; verify that Markov models achieve comparable fit with fewer parameters.

## Open Questions the Paper Calls Out

The paper identifies several open questions including the potential for more sophisticated spatial kernels that capture directional biases, the need to test the model on non-English reading data, and the question of whether the small surprisal effects observed in fine-grained modeling reflect limitations of surprisal theory or artifacts of the modeling approach.

## Limitations

- The use of isotropic Gaussian kernels may oversimplify the directional nature of reading saccades and miss anisotropic movement patterns
- All evaluations use English text, limiting generalizability to right-to-left languages or different scripts
- The paper doesn't explore whether capturing extra-linguistic fixations (52.3% of data) improves downstream tasks like word identification

## Confidence

**High Confidence:** Reader-specific effects and constant spatial shifts substantially improve saccade prediction (verified through ablation studies showing consistent improvements across multiple baselines).

**Medium Confidence:** Linguistic predictors provide only marginal improvements for fine-grained saccade planning (supported by quantitative likelihood gains <2% for most predictors, though corpus coverage is limited).

**Low Confidence:** Convolution-based spillover modeling provides little benefit over simpler Markov assumptions for fixation durations (the comparison is presented but the practical significance for reading theory remains unclear).

## Next Checks

1. **Ablation of Spatial Kernel Anisotropy:** Train the model with anisotropic Gaussian kernels (allowing different variances in x and y directions) and compare against the isotropic baseline. Measure whether this captures directional biases better than the affine shift alone.

2. **Cross-Lingual Generalization Test:** Evaluate the English-trained model on reading data from right-to-left languages (e.g., Arabic or Hebrew). Quantify performance degradation and determine whether spatial shift reparameterization resolves the issues.

3. **Downstream Task Evaluation:** Test whether models trained on full scanpaths vs. filtered scanpaths show differential performance on word identification tasks using the same eye-tracking data. This would validate whether capturing extra-linguistic fixations has practical utility beyond likelihood metrics.