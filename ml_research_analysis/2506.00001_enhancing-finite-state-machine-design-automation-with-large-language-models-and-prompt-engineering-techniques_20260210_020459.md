---
ver: rpa2
title: Enhancing Finite State Machine Design Automation with Large Language Models
  and Prompt Engineering Techniques
arxiv_id: '2506.00001'
source_url: https://arxiv.org/abs/2506.00001
tags:
- design
- prompt
- llms
- patch
- success
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates the performance of three major LLMs\u2014\
  Claude 3 Opus, ChatGPT-4, and ChatGPT-4o\u2014in designing finite state machines\
  \ (FSMs) using systematic markdown format prompts derived from HDLBits problems.\
  \ The study introduces a novel prompt refinement technique called To-do-Oriented\
  \ Prompting (TOP) Patch, which enhances LLM performance by emphasizing key design\
  \ constraints and concepts."
---

# Enhancing Finite State Machine Design Automation with Large Language Models and Prompt Engineering Techniques

## Quick Facts
- arXiv ID: 2506.00001
- Source URL: https://arxiv.org/abs/2506.00001
- Reference count: 5
- Major result: TOP Patch prompt refinement technique improved ChatGPT-4o synchronous reset FSM success rate from 30% to 70% and achieved 90% success for Claude 3 Opus one-hot FSM designs

## Executive Summary
This paper investigates how systematic prompt engineering can enhance large language models' ability to generate correct SystemVerilog code for finite state machines. The authors introduce a structured markdown format for problem specification and a novel To-do-Oriented Prompting (TOP) Patch technique that improves LLM performance by explicitly guiding reasoning steps. Through experiments with Claude 3 Opus, ChatGPT-4, and ChatGPT-4o on 20 HDLBits FSM problems, they demonstrate that targeted prompt refinement can significantly reduce systematic errors in HDL generation, with TOP Patch achieving up to 70-90% success rates for challenging FSM variants that baseline models struggled with.

## Method Summary
The authors evaluate LLM performance on HDLBits FSM problems using a systematic markdown format that decomposes specifications into clear sections (Specification, Example Behavior tables, Module Declaration). They introduce TOP Patch, an append-only refinement technique that adds ordered "To-do" sections requiring LLMs to explain concepts before implementation. Experiments run 5 independent trials per problem per model, measuring success rates with and without TOP Patch. The study identifies model-specific failure patterns (e.g., ChatGPT-4o defaulting to asynchronous reset) and designs targeted patches addressing these issues through explicit concept explanations and procedural task ordering.

## Key Results
- Claude 3 Opus achieved highest baseline success rate (41%) among tested models
- TOP Patch significantly improved performance for difficult FSM variants: ChatGPT-4o synchronous reset success increased from 30% to 70%
- Claude 3 Opus reached 90% success rate for one-hot FSM designs with TOP Patch
- Systematic markdown formatting consistently improved LLM comprehension across all models

## Why This Works (Mechanism)

### Mechanism 1: Structured Markdown Decomposition for Specification Parsing
- Claim: Converting FSM problem descriptions into systematic markdown sections (Specification, Example Behavior, Module Declaration) improves LLM comprehension by reducing ambiguity in signal mappings and state transitions.
- Mechanism: Itemized lists and tables replace prose paragraphs, making input-output relationships and waveform behavior explicit. The LLM receives discrete, labeled components rather than requiring simultaneous parsing of interleaved specification text.
- Core assumption: LLMs perform better when semantic boundaries between specification elements are explicitly marked, reducing inference burden about which information applies to which design aspect.
- Evidence anchors:
  - [abstract]: "using systematic markdown format prompts"
  - [Section II]: "This method allows for clear distinctions between each part of the prompt, such as the specification, I/O list, and module functionality... By utilizing itemized lists to explain several points, the emphasis becomes more apparent compared to using a long paragraph."
  - [corpus]: Related work (VeriMoA, HDLCoRe) similarly finds structured prompting reduces hallucinations, suggesting convergent validity for decomposition approaches in HDL generation.
- Break condition: If specifications require implicit domain knowledge not captured in the structured format (e.g., unstated timing assumptions), markdown structure alone cannot compensate.

### Mechanism 2: TOP Patch as Task-Sequential Attention Steering
- Claim: Appending a "To-do" section with ordered sub-tasks forces LLMs to execute concept verification before code generation, increasing success rates for specialized FSM variants (synchronous reset, one-hot encoding).
- Mechanism: TOP Patch inserts intermediate reasoning steps—such as explaining a concept before applying it—creating a procedural scaffold. LLMs tend to follow the enumerated sequence, ensuring foundational understanding precedes implementation.
- Core assumption: LLMs have latent knowledge of concepts (e.g., synchronous reset semantics) that fails to activate without explicit prompting; ordering tasks sequentially triggers appropriate knowledge retrieval.
- Evidence anchors:
  - [abstract]: "a novel prompt refinement technique called To-do-Oriented Prompting (TOP) Patch"
  - [Section IV]: "TOP Patch introduces a 'To-do' section that is added to the end of the prompt, assisting LLMs in concentrating on specific points... LLMs tend to execute these points in order, making the sequence of the contents a crucial factor."
  - [Section V-A, Table II]: ChatGPT-4o synchronous reset success improved from 30% to 70% with TOP Patch asking it to first explain synchronous reset and differentiate from asynchronous.
  - [corpus]: No direct corpus evidence for TOP Patch specifically; mechanism appears novel to this paper.
- Break condition: If the required concept is genuinely absent from model knowledge (not just dormant), TOP Patch cannot elicit it; fine-tuning or retrieval augmentation would be needed.

### Mechanism 3: Error-Pattern-Specific Patch Targeting
- Claim: Identifying model-specific systematic errors (e.g., ChatGPT-4o defaulting to asynchronous reset) enables targeted TOP Patches that address specific failure modes rather than generic improvements.
- Mechanism: Diagnostic testing reveals consistent error patterns; patches are then designed with explicit instructions addressing the specific misconception or omission. This is more efficient than comprehensive prompt expansion.
- Core assumption: LLM errors in HDL generation are partially systematic and predictable per model, not purely random; targeted counter-instructions can override default behaviors.
- Evidence anchors:
  - [Section III-B]: "ChatGPT-4o... defaults to treating designs as having asynchronous reset. When dealing with designs that require synchronous reset... it often fails to adhere to the prompt."
  - [Section V-B, Table III]: One-hot FSM success rate for Claude 3 Opus reached 90% with TOP Patch instructing equation derivation by inspection.
  - [corpus]: Related work (CorrectHDL, HDLCoRe) similarly addresses hallucination patterns in HDL generation, supporting the premise that errors are systematic and targetable.
- Break condition: If errors stem from fundamental architectural limitations (e.g., inability to perform Boolean algebra reliably), instruction-based patches may hit diminishing returns.

## Foundational Learning

- Concept: Finite State Machine (FSM) Design Patterns
  - Why needed here: The paper assumes familiarity with FSM types (Mealy/Moore), state encoding schemes (binary, one-hot), and reset strategies (synchronous vs. asynchronous). Without this, evaluating whether LLM output is correct requires external verification.
  - Quick check question: Can you explain why one-hot encoding requires deriving next-state logic equations "by inspection" rather than using a case statement on a state register?

- Concept: SystemVerilog Synthesis Semantics
  - Why needed here: The paper identifies specific syntax errors (multi-driver issues, logic declaration placement, begin/end confusion) that indicate LLMs generate non-synthesizable or functionally incorrect code. Understanding synthesis requirements enables meaningful debugging.
  - Quick check question: What is the difference in how synchronous and asynchronous resets are inferred in SystemVerilog always blocks, and what synthesis directive patterns signal each?

- Concept: Prompt Engineering Foundations
  - Why needed here: TOP Patch builds on chain-of-thought prompting and structured decomposition principles. Understanding why intermediate reasoning steps improve output quality helps in designing effective patches.
  - Quick check question: Why might asking an LLM to "explain concept X before implementing it" improve implementation accuracy, even if the model could theoretically proceed directly?

## Architecture Onboarding

- Component map: HDLBits specification → Markdown transformation → TOP Patch enhancement → LLM inference → Testbench validation
- Critical path: Specification → Markdown formatting → TOP Patch design → LLM generation → Testbench validation. The patch design step is the primary lever for improving success rates on complex problems.
- Design tradeoffs:
  - Manual vs. automated TOP Patch generation: Manual patches are more targeted but require domain expertise; automated patches (future direction) scale but may miss model-specific failure modes.
  - Single-shot vs. multi-turn: Single-shot evaluation (as in paper) tests baseline capability; multi-turn with feedback (Section V-C) achieves higher success but requires more interaction.
  - Problem selection: 20 FSM problems from HDLBits provide reproducibility but may not represent production HDL complexity.
- Failure signatures:
  - ChatGPT-4o: Ignores synchronous reset requirements, defaults to asynchronous implementation
  - All models: Fail on one-hot FSM design requiring equation derivation by inspection (0% baseline success)
  - Complex multi-state FSMs: Misinterpret state-transition relationships, especially with long descriptions
  - Common syntax errors: Multi-driver conflicts across always blocks, misplaced logic declarations
- First 3 experiments:
  1. Baseline capability test: Run the 20 HDLBits FSM problems through target LLM with systematic markdown format only (no TOP Patch) to establish model-specific failure patterns and identify which problem categories (reset type, encoding scheme, complexity) require patching.
  2. Targeted TOP Patch validation: For each identified failure pattern, design a minimal TOP Patch addressing the specific issue (e.g., synchronous reset explanation for ChatGPT-4o), run 5 trials per problem, and measure improvement delta.
  3. Cross-model patch transfer test: Apply a TOP Patch designed for one model (e.g., Claude patch for one-hot FSM) to other models to assess whether patches are model-specific or generalize across LLMs; this informs whether a unified patch library or per-model tuning is required.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can large language models be fine-tuned to automatically generate the "To-do" content required for the TOP Patch without human intervention?
- Basis in paper: [explicit] The conclusion states that "generation of TOP Patch can be accomplished by LLMs that require further training and fine-tuning," suggesting the current method may rely on expert input.
- Why unresolved: The paper demonstrates success with manually crafted patches (e.g., for synchronous reset), but does not implement or evaluate an automated patch-generation agent.
- What evidence would resolve it: A study demonstrating a fine-tuned model generating valid TOP Patches that yield success rates comparable to the manual patches shown in Table II and Table III.

### Open Question 2
- Question: Can the systematic markdown prompting and TOP Patch techniques effectively generalize to complex domains beyond HDL design?
- Basis in paper: [explicit] The abstract and conclusion assert that these methods "have the potential to be applied to other domains beyond HDL design automation."
- Why unresolved: The evaluation is restricted exclusively to 20 FSM problems from HDLBits, leaving the efficacy of this prompting strategy in software engineering or other logic tasks untested.
- What evidence would resolve it: Experimental results showing that the markdown structure and To-do lists improve LLM performance on standard software code generation benchmarks (e.g., HumanEval) or natural language tasks.

### Open Question 3
- Question: How can TOP Patch be effectively integrated into the error feedback loops of automated HDL generation frameworks?
- Basis in paper: [explicit] The conclusion identifies the "potential to be integrated into the feedback system of automated HDL generator frameworks" as a future direction.
- Why unresolved: The paper currently treats TOP Patch as a pre-generation prompt refinement or a manual workflow guide, rather than an automated reaction to compiler or simulation errors.
- What evidence would resolve it: A fully automated framework where the system analyzes compilation errors, generates a specific TOP Patch based on the failure log, and successfully corrects the SystemVerilog code without human assistance.

## Limitations
- Single-shot evaluation may underestimate LLM capabilities in real-world scenarios with human feedback
- Manual conversion of HDLBits problems to markdown format limits scalability
- Fixed set of 20 HDLBits problems may not represent industrial FSM design complexity

## Confidence
- High confidence: Markdown format decomposition consistently improves LLM performance across all tested models by reducing specification ambiguity
- Medium confidence: TOP Patch technique delivers substantial improvements for specific problem types (synchronous reset FSMs and one-hot encoding)
- Medium confidence: Model-specific error patterns are systematically addressable through targeted patches
- Low confidence: The claim that TOP Patch "forces" LLMs to follow sequential reasoning steps lacks direct evidence of internal reasoning process

## Next Checks
1. **Cross-domain validation**: Test the systematic markdown format and TOP Patch approach on non-HDLBits FSM problems to assess generalizability beyond curated benchmark datasets
2. **Multi-turn evaluation**: Implement a feedback loop where failed outputs are refined through human-in-the-loop interaction, measuring whether TOP Patch improvements compound over multiple iterations
3. **Error mechanism analysis**: Use targeted ablation studies to determine whether TOP Patch improvements stem from enforced sequential reasoning or simply from providing additional contextual information about FSM concepts