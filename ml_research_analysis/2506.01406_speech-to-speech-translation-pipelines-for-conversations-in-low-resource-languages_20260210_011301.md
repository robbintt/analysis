---
ver: rpa2
title: Speech-to-Speech Translation Pipelines for Conversations in Low-Resource Languages
arxiv_id: '2506.01406'
source_url: https://arxiv.org/abs/2506.01406
tags:
- google
- microsoft
- whisper
- translation
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of automatic speech-to-speech
  translation for community interpreting in low-resource language pairs, specifically
  Turkish and Pashto to/from French. The authors collected conversational dialogue
  data, fine-tuned ASR and MT components, and evaluated over 60 pipeline combinations
  using both automatic metrics (BLEU, COMET, BLASER) and human assessments.
---

# Speech-to-Speech Translation Pipelines for Conversations in Low-Resource Languages

## Quick Facts
- arXiv ID: 2506.01406
- Source URL: https://arxiv.org/abs/2506.01406
- Authors: Andrei Popescu-Belis; Alexis Allemann; Teo Ferrari; Gopal Krishnamani
- Reference count: 13
- Primary result: Fine-tuned Whisper ASR consistently outperformed other ASR systems in low-resource speech-to-speech translation pipelines

## Executive Summary
This study addresses the challenge of automatic speech-to-speech translation for community interpreting in low-resource language pairs, specifically Turkish and Pashto to/from French. The authors collected conversational dialogue data, fine-tuned ASR and MT components, and evaluated over 60 pipeline combinations using both automatic metrics and human assessments. The research demonstrates that fine-tuned Whisper ASR achieves superior performance compared to other ASR systems, with Google MT providing the highest translation quality. The stability of component rankings across different pipeline configurations suggests reliable component selection strategies for low-resource translation systems.

## Method Summary
The researchers collected conversational dialogue data for Turkish and Pashto to/from French language pairs. They fine-tuned Automatic Speech Recognition (ASR) components and evaluated multiple Machine Translation (MT) systems. The study tested over 60 pipeline combinations using automatic metrics including BLEU, COMET, and BLASER, supplemented by human evaluations. The evaluation focused on meaning preservation, correctness, and intonation quality, with particular attention to usability for community interpreting applications.

## Key Results
- Fine-tuned Whisper ASR consistently outperformed other ASR systems across all metrics
- Google MT achieved the highest translation quality among tested MT systems
- Best pipelines achieved "good" to "very good" ratings in human evaluation with BLEU scores around 25 and COMET scores near 90

## Why This Works (Mechanism)
The success of the fine-tuned Whisper ASR system can be attributed to its strong foundation in large-scale pretraining combined with targeted fine-tuning on conversational dialogue data. The stability of component rankings across pipeline configurations suggests that ASR quality has a multiplicative effect on downstream translation quality. The proprietary MT systems (Google Translate, DeepL) likely benefit from massive training data and sophisticated neural architectures that generalize well to low-resource language pairs when combined with high-quality ASR input.

## Foundational Learning

**Automatic Speech Recognition (ASR)**: Converts spoken language to text; essential for enabling text-based translation pipelines. Quick check: Verify word error rate on held-out test data.

**Machine Translation (MT)**: Translates text from source to target language; critical component determining overall translation quality. Quick check: Compare multiple MT systems using standardized metrics.

**BLEU Score**: Evaluates translation quality by comparing n-gram overlap between candidate and reference translations; provides quantitative baseline for system comparison. Quick check: Calculate BLEU scores for all pipeline combinations.

**COMET Score**: Uses neural models to assess translation quality based on semantic similarity; more sophisticated than n-gram based metrics. Quick check: Validate COMET scores against human judgments.

**BLASER**: Multilingual sentence embedding model for evaluating translation quality across language pairs; particularly useful for low-resource languages. Quick check: Test BLASER correlation with human evaluation scores.

**Human Evaluation**: Subjective assessment of translation quality focusing on meaning, correctness, and intonation; provides real-world usability insights. Quick check: Conduct blind evaluations with multiple raters.

## Architecture Onboarding

**Component Map**: ASR (Whisper) -> MT (Google/DeepL) -> TTS (Text-to-Speech)

**Critical Path**: ASR accuracy directly impacts downstream MT quality, making ASR the most critical component for overall system performance.

**Design Tradeoffs**: Proprietary MT systems offer superior quality but limit reproducibility and raise concerns about API version consistency across evaluations.

**Failure Signatures**: Poor ASR performance (high word error rate) cascades through the pipeline, resulting in degraded MT output regardless of MT system quality.

**First Experiments**:
1. Compare baseline Whisper ASR performance against fine-tuned versions on held-out conversational data
2. Evaluate multiple MT systems (Google, DeepL, open-source alternatives) using identical ASR inputs
3. Conduct ablation studies by varying individual pipeline components while holding others constant

## Open Questions the Paper Calls Out
None

## Limitations

- Reliance on proprietary MT components limits reproducibility and raises concerns about consistent evaluation across different API versions
- Heavy dependence on automatic metrics may not fully capture speech-to-speech translation quality nuances, particularly prosody and intonation
- Human evaluation sample size and methodology details are not fully specified, affecting generalizability of subjective quality assessments
- Low-resource language pairs studied may have unique characteristics limiting broader applicability to other language combinations

## Confidence

- **High confidence**: Fine-tuned Whisper ASR outperforming other ASR systems (supported by consistent results across multiple metrics)
- **Medium confidence**: Stability of component ranking across pipeline configurations (limited by proprietary MT components)
- **Medium confidence**: "Good" to "very good" human evaluation ratings (based on subjective assessments with unspecified sample sizes)

## Next Checks

1. Reproduce key findings using open-source MT alternatives to verify component ranking stability
2. Conduct larger-scale human evaluations with standardized protocols and larger participant pools
3. Test pipeline performance on additional low-resource language pairs to assess generalizability beyond Turkish/Pashto to French