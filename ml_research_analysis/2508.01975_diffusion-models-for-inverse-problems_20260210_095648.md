---
ver: rpa2
title: Diffusion models for inverse problems
arxiv_id: '2508.01975'
source_url: https://arxiv.org/abs/2508.01975
tags:
- diffusion
- inverse
- problems
- where
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This chapter surveys diffusion models for solving inverse problems
  in imaging, categorizing approaches into explicit approximations and other methods
  like variational inference, sequential Monte Carlo, and decoupled data consistency.
  The core idea is to leverage generative priors from diffusion models to sample from
  posterior distributions given corrupted measurements.
---

# Diffusion models for inverse problems

## Quick Facts
- arXiv ID: 2508.01975
- Source URL: https://arxiv.org/abs/2508.01975
- Reference count: 11
- One-line primary result: Diffusion models provide a flexible, unsupervised framework for solving inverse problems through posterior sampling, with explicit methods offering computational efficiency at the cost of approximation accuracy.

## Executive Summary
This chapter surveys diffusion models for solving inverse problems in imaging, categorizing approaches into explicit approximations and other methods like variational inference, sequential Monte Carlo, and decoupled data consistency. The core idea is to leverage generative priors from diffusion models to sample from posterior distributions given corrupted measurements. Explicit methods, such as DPS, approximate the likelihood term using posterior mean estimates, while others employ variational inference or particle filters for more accurate posterior sampling. Extensions address challenges like blind inverse problems, high-dimensional data, and limited training data through techniques like test-time adaptation and ambient diffusion. Text-driven solutions further enhance reconstruction by incorporating auxiliary information. Overall, diffusion-based solvers offer a flexible, unsupervised toolkit for inverse problems, with trade-offs between computational efficiency and reconstruction accuracy.

## Method Summary
The survey presents diffusion-based methods for inverse problems that leverage pre-trained diffusion models as generative priors. The core approach involves decomposing the posterior score into prior score (learned by diffusion model) and likelihood score (computed via measurement consistency). Explicit methods like DPS approximate the intractable likelihood term using Tweedie's theorem to compute posterior mean estimates, while variational approaches minimize KL divergence between variational and true posteriors, and SMC methods provide asymptotically exact sampling through particle propagation. These methods enable posterior sampling without retraining the diffusion model, with trade-offs between computational speed and reconstruction fidelity.

## Key Results
- Explicit methods like DPS offer computational efficiency by approximating likelihood terms, but may produce distorted MAP-like estimates in challenging cases
- Variational inference with normalizing flows can capture multi-modal posteriors at higher computational cost compared to Gaussian families
- SMC methods provide theoretical convergence guarantees with increased particle count, but scale linearly with computation requirements
- Blind inverse problems can be addressed by extending DPS to jointly infer forward operator parameters, though practical application remains challenging
- Test-time adaptation techniques like DDIP and ambient diffusion enable diffusion-based reconstruction in data-scarce regimes

## Why This Works (Mechanism)

### Mechanism 1: Posterior Score Decomposition with Explicit Likelihood Approximation
- Claim: Conditional on having a well-trained diffusion model, posterior sampling can be achieved by decomposing the posterior score into prior score plus likelihood score, with the likelihood term approximated using Tweedie estimates.
- Mechanism: Bayes' rule gives ∇xt log p(xt|y) = ∇xt log p(xt) + ∇xt log p(y|xt). The first term is learned by the diffusion model. The second term is intractable but approximated via p(y|xt) ≈ p(y|ẑ0|t) where ẑ0|t = E[x0|xt] is the MMSE estimate from Tweedie's theorem (Eq. 32-33). The gradient ∇xt log p(y|ẑ0|t) is computed via backpropagation through the denoiser.
- Core assumption: The Jensen approximation p(y|xt) ≈ p(y|ẑ0|t) introduces bounded error (shown controllable for Gaussian measurements per DPS paper). The diffusion model has learned an accurate score function across noise levels.
- Evidence anchors:
  - [abstract] "Explicit methods, like Diffusion Posterior Sampling (DPS), approximate the intractable log-likelihood term in Bayes' rule."
  - [section 3.2] Eq. 31-33 show the mathematical derivation: "p(y|xt) ≈ p(y|ẑ0|t), where ẑ0|t = E[x0|xt]... The computation of the gradient can be done through backpropagation."
  - [corpus] Related papers confirm diffusion models as priors for inverse problems, but do not provide comparative validation of the Jensen approximation bound.
- Break condition: If the likelihood approximation error dominates, posterior samples may diverge from true posterior (observed in challenging cases like Fourier phase retrieval per Sec. 4.2).

### Mechanism 2: Variational Distribution Matching with Diffusion Priors
- Claim: Conditional on choosing a sufficiently expressive variational family, minimizing KL divergence between the variational distribution and posterior yields samples that trade off data consistency against diffusion prior regularization.
- Mechanism: A variational distribution qϕ(x0|y) is fit to minimize DKL(qϕ(x0|y) ∥ p(x0|y)). This decomposes into a data consistency term (log p(y|x0)) and a regularizer (KL vs. prior). For Gaussian qϕ with diffusion prior, this becomes: data consistency loss + score-matching loss across diffusion trajectory (Eq. 44-45). Normalizing flows can capture multi-modal posteriors.
- Core assumption: The variational family is expressive enough to capture posterior structure. For uni-modal Gaussian families, this fails for multi-modal posteriors.
- Evidence anchors:
  - [abstract] "Other methods include variational approaches using normalizing flows..."
  - [section 4.1] Eq. 43-47 derive the variational objective. "A uni-modal Gaussian variational family cannot capture a complex, multi-modal posterior distribution. Feng et al. (2023) employ a normalizing flow - RealNVP - to represent the variational distribution qϕ."
  - [corpus] "Introduction to Regularization and Learning Methods for Inverse Problems" discusses regularization perspectives but does not directly validate variational inference for diffusion priors.
- Break condition: If variational family is underparameterized, samples collapse to single mode; if overparameterized without proper regularization, training instability occurs.

### Mechanism 3: Sequential Monte Carlo with Theoretical Convergence Guarantees
- Claim: Conditional on having accurate proposal distributions and sufficient particles, SMC methods provide asymptotically exact posterior sampling with provable convergence as particle count increases.
- Mechanism: Particles representing candidate solutions are propagated through reverse diffusion steps, reweighted by measurement consistency, and resampled. The proposal kernel can be constructed as pθ(xt|xt+1)q(xt|y) where q(xt|y) incorporates measurement information (Eq. 57-58). As particle count → ∞, samples approach true posterior distribution.
- Core assumption: The proposal distribution has non-zero support where posterior has mass (importance weights remain bounded). Computational budget permits sufficient particles.
- Evidence anchors:
  - [abstract] "...and SMC frameworks for guaranteed posterior sampling."
  - [section 4.3] "SMC methods enjoys the property that with increased compute (i.e. number of particles → ∞), the sampler approaches sampling from the true posterior." Eq. 62-63 show coupled diffusion construction for improved proposals.
  - [corpus] "Benchmarking Diffusion Annealing-Based Bayesian Inverse Problem Solvers" suggests active benchmarking efforts but limited comparative validation across SMC variants.
- Break condition: Particle degeneracy (all weight on few particles) occurs when proposal poorly matches posterior; computational cost scales linearly with particle count.

## Foundational Learning

- **Concept: Score Functions and Score Matching**
  - Why needed here: Diffusion models learn ∇x log p(x) rather than p(x) directly; understanding this is essential for grasping how posterior sampling works.
  - Quick check question: Can you explain why learning ∇x log p(x) avoids the intractable partition function that plagues direct density estimation?

- **Concept: Tweedie's Theorem and Posterior Mean Estimation**
  - Why needed here: The DPS family relies on Tweedie's theorem to convert score functions into MMSE denoised estimates E[x0|xt], which are used to approximate likelihood terms.
  - Quick check question: Given xt ~ N(sxt0, σ²I), can you derive E[x0|xt] in terms of ∇xt log p(xt)?

- **Concept: KL Divergence and Variational Inference**
  - Why needed here: Variational methods (Sec. 4.1) are framed as minimizing KL(qϕ ∥ p(x|y)); understanding this objective clarifies the data-consistency vs. prior tradeoff.
  - Quick check question: Why does minimizing DKL(qϕ ∥ p(x|y)) decompose into a likelihood term and a prior KL term, and what does each term encourage?

## Architecture Onboarding

- **Component map**: Measurement y → Forward Operator A → [Data Consistency Module] → Noise xT ~ N(0,I) → [Diffusion Prior (Score Network sθ)] → Reverse SDE/ODE → [Posterior Score Composition] → Denoised Sample x0

- **Critical path**:
  1. Load pre-trained diffusion model (score network or denoiser Dθ).
  2. Define forward operator A and noise model p(y|x).
  3. Initialize xT ~ N(0, I).
  4. For t = T → 1: compute ẑ0|t = Dθ(xt, t), compute likelihood gradient ∇xt ∥y - A(ẑ0|t)∥², compose posterior score, take reverse step.
  5. Return x0.

- **Design tradeoffs**:
  - **Speed vs. fidelity**: ODE samplers (DDIM with η=0) are faster but may reduce sample diversity vs. SDE samplers (Sec. 2.1-2.2).
  - **Approximation quality vs. compute**: DPS (single-step approximation) is fast but less accurate; moment matching (Eq. 38-39) requires Jacobian computation; SMC scales with particle count.
  - **Uni-modal vs. multi-modal posteriors**: Gaussian variational families are efficient but miss modes; normalizing flows capture modes at higher compute cost (Sec. 4.1).
  - **Blind vs. non-blind**: Blind problems require joint inference over x and forward operator parameters φ, doubling compute (Sec. 5.1).

- **Failure signatures**:
  - **Sample divergence**: In challenging problems (e.g., phase retrieval), DPS samples may diverge—mitigated by decoupled approaches (DAPS, Sec. 4.2).
  - **Mode collapse**: Variational methods with underparameterized qϕ produce single-mode samples.
  - **Particle degeneracy**: SMC methods fail when importance weights concentrate on few particles—check effective sample size.
  - **Out-of-distribution inputs**: Diffusion prior trained on mismatched data yields poor reconstructions—consider test-time adaptation (DDIP, Sec. 5.3.1).

- **First 3 experiments**:
  1. **Gaussian deblurring with DPS**: Implement Eq. 33 with a pre-trained denoiser; measure PSNR and LPIPS vs. number of reverse steps; vary η in DDIM to observe speed-quality tradeoff.
  2. **Compare DPS vs. ΠGDM vs. moment matching on linear inverse problem**: Use same diffusion prior and measurement model; report reconstruction error and per-step wall-clock time to quantify approximation quality vs. compute tradeoff (Sec. 3.2).
  3. **SMC vs. variational inference on multi-modal posterior**: Design a problem with known multi-modal posterior (e.g., inpainting with ambiguous completions); compare sample diversity (using pairwise LPIPS) and posterior coverage between FPS (Sec. 4.3) and normalizing flow VI (Sec. 4.1).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the inherent trade-off between computational speed and reconstruction fidelity be reconciled in diffusion-based inverse problem solvers?
- Basis in paper: [explicit] The conclusion states that solvers balance speed against fidelity and that "future work will likely focus on reconciling the trade-offs between speed and accuracy."
- Why unresolved: Current methods typically exist on a spectrum where increasing the exactness of the posterior sampling (e.g., via Sequential Monte Carlo) drastically increases computational cost.
- What evidence would resolve it: An algorithm that achieves high-fidelity posterior sampling with a low Number of Function Evaluations (NFE) comparable to one-step variational methods.

### Open Question 2
- Question: How can blind inverse problem solvers be adapted for real-world tasks where the forward degradation model is complex or difficult to specify?
- Basis in paper: [inferred] Section 5.1 notes that while methods like BlindDPS exist, they are "hard to apply to real-world image restoration tasks, as the forward model is either much more complicated, or hard to specify."
- Why unresolved: Current blind methods often assume simple parameterized operators (e.g., a convolution kernel), failing to account for compound degradations like JPEG artifacts or complex noise mixtures.
- What evidence would resolve it: A blind solver that recovers images from compound, non-linear degradations without requiring an explicit, closed-form mathematical definition of the degradation operator.

### Open Question 3
- Question: How can diffusion models be effectively trained or adapted for inverse problems when the assumption of access to high-quality, in-distribution training data is violated?
- Basis in paper: [inferred] Section 5.3 explicitly challenges the standard assumption, stating "This condition is not satisfied" in domains like black-hole or cryo-EM imaging where ground-truth data is unavailable.
- Why unresolved: Standard diffusion training requires clean data; methods like Ambient Diffusion and GSURE-based training are proposed solutions, but the paper frames the data-scarce regime as a distinct, active extension category.
- What evidence would resolve it: Theoretical bounds or empirical benchmarks showing that models trained solely on partial/corrupted measurements (unsupervised) can match the reconstruction quality of supervised models on scientific imaging tasks.

## Limitations

- The Jensen approximation in DPS introduces bounded but uncontrolled error that can cause sample divergence in ill-conditioned inverse problems
- Variational methods with Gaussian families cannot capture multi-modal posteriors, limiting their applicability to certain inverse problems
- SMC methods scale linearly with particle count, making high-dimensional problems computationally prohibitive
- Blind inverse problem extensions remain challenging for real-world applications with complex or unknown forward models

## Confidence

- **High confidence**: Diffusion models as generative priors for inverse problems, basic DPS formulation and posterior score decomposition
- **Medium confidence**: Variational inference formulations and trade-offs with normalizing flows, SMC theoretical convergence guarantees
- **Low confidence**: Practical performance comparisons across methods, exact guidance step sizes for specific tasks, error bounds for approximation schemes

## Next Checks

1. Implement DPS on linear inverse problems (denoising, deblurring) and systematically vary the guidance step size ρ to measure reconstruction error vs. posterior sample diversity (LPIPS).
2. For a problem with known multi-modal posterior (e.g., ambiguous inpainting), compare posterior coverage and sample diversity between DPS, variational inference with normalizing flows, and SMC methods.
3. Test the practical limits of the Jensen approximation by evaluating DPS on increasingly ill-conditioned forward operators (phase retrieval, Fourier inversion) and measuring the divergence between approximate and true posterior samples.