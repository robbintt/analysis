---
ver: rpa2
title: 'VIVAT: Virtuous Improving VAE Training through Artifact Mitigation'
arxiv_id: '2506.07863'
source_url: https://arxiv.org/abs/2506.07863
tags:
- training
- arxiv
- https
- latent
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work systematically addresses common artifacts in KL-VAE\
  \ training by identifying five prevalent issues\u2014color shift, grid patterns,\
  \ blur, corner artifacts, and droplet artifacts\u2014and analyzing their root causes.\
  \ The authors propose straightforward modifications, including adjustments to loss\
  \ weights, padding strategies, and the integration of spatially conditional normalization,\
  \ to mitigate these artifacts without requiring radical architectural changes."
---

# VIVAT: Virtuous Improving VAE Training through Artifact Mitigation

## Quick Facts
- arXiv ID: 2506.07863
- Source URL: https://arxiv.org/abs/2506.07863
- Reference count: 40
- State-of-the-art PSNR of 31.25 and SSIM of 0.90 on ImageNet with artifact mitigation

## Executive Summary
This paper systematically identifies and addresses five common artifacts in KL-VAE training: color shift, grid patterns, blur, corner artifacts, and droplet artifacts. The authors propose straightforward modifications including loss weight adjustments, padding strategies, and spatially conditional normalization to mitigate these issues without requiring radical architectural changes. Their approach achieves state-of-the-art reconstruction quality on multiple benchmarks and improves text-to-image generation performance when integrated into a latent diffusion pipeline.

## Method Summary
The authors analyze the root causes of common VAE training artifacts and propose targeted solutions. Key modifications include adjusting loss weights to balance reconstruction and KL divergence terms, implementing smarter padding strategies to prevent border artifacts, and incorporating spatially conditional normalization to address spatial inconsistencies. These changes are designed to preserve the simplicity of the KL-VAE framework while systematically eliminating visual defects that commonly emerge during training.

## Key Results
- Achieves 31.25 PSNR and 0.90 SSIM on ImageNet, surpassing existing models
- Outperforms Flux VAE in CLIP score when integrated into text-to-image generation
- Demonstrates state-of-the-art reconstruction quality across multiple benchmark datasets

## Why This Works (Mechanism)
The effectiveness stems from addressing fundamental causes of VAE artifacts: imbalanced loss terms leading to reconstruction-prior trade-offs, spatial inconsistencies from uniform normalization, and boundary effects from naive padding. By recalibrating these components, the model maintains better information preservation while satisfying regularization constraints, resulting in cleaner reconstructions and improved downstream generation quality.

## Foundational Learning
- **KL-VAE Architecture**: Variational Autoencoders using KL divergence for regularization - needed to understand the baseline framework being improved
- **Reconstruction vs Prior Trade-off**: Balancing pixel-level accuracy with latent space regularization - critical for understanding artifact emergence
- **Spatially Conditional Normalization**: Normalization layers that adapt to spatial location - key to addressing spatially-varying artifacts
- **Loss Weighting Strategies**: Proper scaling of reconstruction and regularization terms - fundamental to stable training
- **Padding Effects in Convolutions**: How different padding schemes affect boundary behavior - important for corner artifact mitigation

## Architecture Onboarding

**Component Map**: Input -> Encoder -> Latent Space -> Decoder -> Output
**Critical Path**: Reconstruction Loss + KL Loss -> Backward Pass -> Parameter Updates
**Design Tradeoffs**: Simplicity vs artifact elimination - the authors chose minimal modifications over complex architectural changes
**Failure Signatures**: Color shifts indicate channel-wise imbalance; grid patterns suggest downsampling artifacts; blur indicates over-regularization
**First Experiments**:
1. Baseline KL-VAE training on ImageNet to reproduce common artifacts
2. Individual artifact mitigation tests with isolated modifications
3. Full integrated model evaluation with all fixes applied

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow empirical scope tested only on ImageNet, LSUN Church, and LSUN Bedroom
- Limited validation of perceptual quality beyond CLIP scores and standard metrics
- Focus on KL-VAE may not generalize to other VAE variants or conditional settings
- No evaluation of scalability to higher resolution images

## Confidence
High confidence in: systematic identification of five common artifacts and their root causes; effectiveness of proposed fixes for tested datasets; quantitative improvements in PSNR and SSIM metrics.

Medium confidence in: CLIP score improvements for text-to-image generation; claim of state-of-the-art reconstruction quality; assertion that modifications preserve VAE simplicity.

Low confidence in: generalization across diverse datasets; perceptual quality improvements beyond CLIP scores; scalability to higher resolution images; performance in conditional generation settings.

## Next Checks
1. Test the artifact mitigation approach on at least three additional diverse datasets (e.g., CelebA-HQ, FFHQ, and a medical imaging dataset) to assess generalization.
2. Conduct a human perceptual study comparing images generated with and without the proposed fixes to validate subjective quality improvements beyond PSNR/SSIM.
3. Evaluate the method's performance at 512x512 and 1024x1024 resolutions to verify scalability and check if the spatially conditional normalization approach introduces computational overhead or training instability at higher resolutions.