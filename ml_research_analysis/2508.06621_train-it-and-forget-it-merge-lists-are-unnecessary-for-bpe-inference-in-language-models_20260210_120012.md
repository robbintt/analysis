---
ver: rpa2
title: 'Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language
  Models'
arxiv_id: '2508.06621'
source_url: https://arxiv.org/abs/2508.06621
tags:
- merge
- inference
- tokenizer
- encoding
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the downstream impact of using Byte-Pair
  Encoding (BPE) inference algorithms that do not rely on the learned merge list at
  all, hence differing from the encoding process during BPE training. The authors
  explore two broad classes of BPE inference schemes: a) targeted deviation from merge-lists
  including random merge orders, and various corruptions of merge list involving deletion/truncation,
  and b) non-targeted BPE inference algorithms that do not depend on the merge list
  but focus on compressing the text either greedily or exactly.'
---

# Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models

## Quick Facts
- arXiv ID: 2508.06621
- Source URL: https://arxiv.org/abs/2508.06621
- Authors: Tomohiro Sawada; Kartik Goyal
- Reference count: 17
- Primary result: Merge-list-free inference algorithms maintain or improve downstream LLM performance while eliminating a potential attack vector

## Executive Summary
This paper investigates whether Byte-Pair Encoding (BPE) inference algorithms must rely on the learned merge list or can achieve comparable performance without it. The authors explore two broad classes of BPE inference schemes: targeted deviations from merge lists (random orders, corruptions) and non-targeted merge-list-free algorithms (left-to-right greedy, maximal compression). Surprisingly, they find that compression-focused merge-list-free methods maintain or even improve performance on multiple-choice QA and open-ended generation tasks, while showing only modest degradation on machine translation. This suggests that the merge list, while containing training dynamics information, is unnecessary for inference quality and can be omitted to reduce attack surface.

## Method Summary
The authors evaluate a Qwen-2-7B-Instruct model (151,645 token vocabulary) using different tokenization schemes without any model retraining. They implement left-to-right greedy encoding (longest prefix match iteratively) and maximal compression encoding (dynamic programming for minimum-token encoding) as merge-list-free alternatives. These are compared against standard merge-list-based encoding and various targeted corruptions of the merge list (truncation, deletion, shuffle, character-level). Evaluation spans multiple benchmarks: MMLU and ARC-Easy/Challenge for multiple-choice QA, WMT16 Czech→English and WMT15 German→English for machine translation (using COMET, BLEU, METEOR), and Semantic Scholar Open Research Corpus for open-ended generation (using MAUVE).

## Key Results
- Left-to-right greedy encoding maintained or improved accuracy on ARC (0.903 vs 0.869 standard) and MMLU (0.705 vs 0.656 standard)
- Open-ended generation quality improved with left-to-right (MAUVE: 0.985 vs 0.904 standard)
- Machine translation showed modest drops (Czech→English COMET: 0.324 vs 0.332 standard)
- Targeted corruptions of merge list exhibited significant degradation across all tasks
- Minimal compression achieved similar performance to left-to-right with higher computational cost (O(n²) vs O(n))

## Why This Works (Mechanism)

### Mechanism 1: Compression Objective Alignment
BPE training implicitly optimizes for compression in a greedy manner, and merge-list-free algorithms like left-to-right greedy approximate this objective using vocabulary alone. The vocabulary specifies all valid subword units, and compression algorithms can reconstruct acceptable tokenizations by maximizing sequence coverage without merge-order guidance.

### Mechanism 2: LLM Tokenization Robustness
LLMs possess implicit robustness to tokenization variations through an internal lexicon that operates beyond surface token sequences. Despite different token sequences, the model's internal representations can map alternative tokenizations to similar semantic states. This robustness is asymmetric: compression-based variations are tolerated better than corruption-based variations because the former still produce linguistically coherent subword units.

### Mechanism 3: Merge List Redundancy
Merge lists contain training dynamics information unnecessary for inference because the vocabulary subsumes the compression-relevant structure. The merge list encodes hierarchical dependencies and temporal training order—information useful for data extraction attacks but redundant for compression quality.

## Foundational Learning

- **Concept**: BPE vocabulary vs. merge list distinction
  - Why needed: The paper's core intervention requires understanding that these are separable components—vocabulary is the token set; merge list is the ordered construction history.
  - Quick check: Given vocabulary {`qu`, `uant`, `quant`, `ize`, `quantize`} and text "quantize," can you identify three valid tokenizations without consulting a merge list?

- **Concept**: Greedy vs. optimal compression in tokenization
  - Why needed: Left-to-right is greedy (local longest prefix); maximal compression is optimal (global minimum tokens). Understanding this distinction explains their different performance profiles.
  - Quick check: For string `abcde` with vocabulary {`abcd`, `abc`, `de`, `cde`}, what tokenization does left-to-right produce? What about maximal compression?

- **Concept**: Token embedding norm as undertrainedness proxy
  - Why needed: The paper uses minimum embedding norm to diagnose why some tokenizations cause failures—rare tokens retain near-initial embeddings close to origin.
  - Quick check: If a token appears only 3 times in a 1B-token pretraining corpus, would you expect its embedding norm to be higher or lower than a token appearing 100K times? Why?

## Architecture Onboarding

Input text → Pre-tokenizer (splits on whitespace/punctuation) → Encoder (standard OR left-to-right OR maximal-compression) → Token IDs → LLM → Generated IDs → Detokenizer (vocabulary lookup, merge-list-independent)

Critical path:
1. Extract vocabulary from existing tokenizer (the token→ID mapping)
2. Implement left-to-right encoder: iterate pretokens, greedily match longest vocabulary prefix
3. Implement maximal-compression encoder: dynamic programming (O(n²)) for shortest valid tokenization
4. Ensure detokenization path remains merge-list-independent (standard vocabulary lookup)

Design tradeoffs:
- Left-to-right: O(n) per pretoken, simpler, better QA/OEG results, slight MT degradation
- Maximal compression: O(n²) per pretoken, exact compression, similar performance profile
- Both: Eliminate merge-list attack surface, require no LLM retraining, but may select undertrained tokens

Failure signatures:
- Machine translation on morphologically rich languages (Czech shows larger drops than German)
- Domain-specific text with rare biomedical jargon triggers high edit-distance tokenizations
- Generated output contains character-level splits or unnatural spacing

First 3 experiments:
1. Run left-to-right encoder on 100 pretokens from your target domain; compute edit distance vs. standard tokenizer. If >50% of pretokens differ, characterize which token types cause divergence.
2. Extract embedding norms for all vocabulary tokens; flag tokens below 25th percentile. Test whether your merge-list-free encoder preferentially selects these (compare selection rates vs. standard).
3. Run ARC-MMLU (or your domain's equivalent short-answer task) comparing standard vs. left-to-right. If accuracy differs by >3%, investigate whether failures correlate with specific token patterns rather than task difficulty.

## Open Questions the Paper Calls Out

1. Do merge-list-free inference algorithms maintain performance for low-resource languages or scripts with non-monotonic writing orders? The authors note their findings might not hold for small amounts of data in low-resource languages, especially with non-monotonic or non-left-to-right writing orders.

2. Can language model performance be improved by aligning the training process with merge-list-free algorithms rather than applying them post-hoc? The paper investigates inference-time mismatches on models already trained with standard merge-list BPE, suggesting that training a model natively with a merge-free algorithm is an untested condition.

3. Why does the left-to-right greedy encoding algorithm outperform the "maximal compression" algorithm on multiple-choice QA tasks? The paper cites prior work suggesting compression correlates with performance, yet Table 2 shows Left-to-Right achieves higher accuracy than Maximal Compression on ARC (0.903 vs 0.863), which they lack theoretical support to explain.

## Limitations
- Security analysis remains theoretical rather than empirically validated
- Asymmetric degradation in machine translation for morphologically rich languages suggests performance is task-dependent
- Undertrained token risk is identified but not quantified with universal guidelines

## Confidence
- **High**: Core empirical finding that merge-list-free compression algorithms maintain or improve performance on most tasks while targeted corruptions degrade performance
- **Medium**: Mechanism that LLMs possess implicit robustness to tokenization variations
- **Low**: Security argument that vocabulary alone is sufficient to block training data extraction

## Next Checks
1. Run left-to-right encoder on 1,000 pretokens from your target domain; compute edit distance distribution vs. standard tokenizer and identify specific token patterns that cause divergence.
2. Calculate embedding norms for all vocabulary tokens; implement a "safe merge-list-free" encoder that avoids tokens below the 25th percentile norm when alternatives exist.
3. Evaluate merge-list-free encoding on translation pairs involving languages with rich morphology (Finnish→English, Turkish→English) beyond the Czech/German pairs tested.