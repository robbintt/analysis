---
ver: rpa2
title: 'Bayesian Experimental Design for Model Discrepancy Calibration: A Rivalry
  between Kullback--Leibler Divergence and Wasserstein Distance'
arxiv_id: '2601.16425'
source_url: https://arxiv.org/abs/2601.16425
tags:
- wasserstein
- distance
- posterior
- design
- divergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper compares the use of Kullback\u2013Leibler (KL) divergence\
  \ and Wasserstein distance as utility functions in Bayesian experimental design\
  \ (BED). It identifies a key limitation of Wasserstein distance: its value depends\
  \ on the absolute position of posterior mass relative to the prior\u2019s support,\
  \ which can lead to false rewards unrelated to information gain, especially with\
  \ non-informative priors."
---

# Bayesian Experimental Design for Model Discrepancy Calibration: A Rivalry between Kullback--Leibler Divergence and Wasserstein Distance

## Quick Facts
- **arXiv ID**: 2601.16425
- **Source URL**: https://arxiv.org/abs/2601.16425
- **Reference count**: 40
- **Primary result**: KL divergence leads to faster convergence and lower posterior uncertainty when the model is accurate, while Wasserstein distance provides more robust designs under model misspecification by retaining greater uncertainty

## Executive Summary
This paper presents a comprehensive comparison of Kullback-Leibler (KL) divergence and Wasserstein distance as utility functions in Bayesian experimental design (BED), particularly when model discrepancy is present. The authors identify a critical limitation of Wasserstein distance: its value depends on the absolute position of posterior mass relative to the prior's support, which can lead to false rewards unrelated to information gain, especially with non-informative priors. Through theoretical analysis and numerical experiments, the paper demonstrates that KL divergence generally provides superior efficiency when the model is accurate, while Wasserstein distance offers greater robustness when model discrepancy exists, enabling more effective model correction through retained posterior uncertainty.

## Method Summary
The paper compares two utility functions for Bayesian experimental design: KL divergence and Wasserstein distance. Both are used to optimize experimental designs by quantifying the expected information gain from measurements. The authors analyze their theoretical properties and implement them in a convection-diffusion source inversion problem, comparing their performance with and without model discrepancy. The methodology involves computing expected utility over design parameters, using the reference posterior approach for tractable computation. The analysis reveals fundamental differences in how these metrics respond to model misspecification and prior specification, particularly highlighting Wasserstein distance's sensitivity to the absolute location of posterior mass relative to prior support.

## Key Results
- KL divergence achieves faster convergence and lower posterior uncertainty when the model is accurate and no discrepancy is present
- Wasserstein distance provides more robust experimental designs when model discrepancy exists, retaining greater posterior uncertainty that enables effective model correction
- The value of Wasserstein distance depends on the absolute position of posterior mass relative to the prior's support, leading to potential false rewards with non-informative priors
- Without model discrepancy, KL divergence outperforms Wasserstein distance in terms of information gain and convergence speed

## Why This Works (Mechanism)
The mechanism underlying the different behaviors of KL divergence and Wasserstein distance in Bayesian experimental design relates to how they measure information gain and their sensitivity to model discrepancy. KL divergence measures the relative entropy between prior and posterior distributions, focusing on the ratio of probability densities and capturing information gain in a scale-invariant manner. This makes it highly efficient when the model accurately represents the underlying physics. In contrast, Wasserstein distance measures the cost of transporting probability mass between distributions, incorporating absolute location information. This property becomes problematic with non-informative priors, where small shifts in posterior mass can create large utility values unrelated to actual information gain. However, this same property makes Wasserstein distance more robust to model misspecification, as it can capture discrepancies that KL divergence might miss when the model is fundamentally flawed.

## Foundational Learning

**Bayesian Experimental Design**: Framework for optimizing measurement locations/parameters to maximize information gain about model parameters. *Why needed*: Core methodology being evaluated. *Quick check*: Can you explain the expected utility framework?

**Kullback-Leibler Divergence**: Measures relative entropy between two probability distributions, focusing on density ratios. *Why needed*: Primary utility function being compared. *Quick check*: How does KL divergence differ from absolute distance measures?

**Wasserstein Distance**: Measures the minimum "work" required to transform one probability distribution into another, incorporating geometry of the sample space. *Why needed*: Alternative utility function with different theoretical properties. *Quick check*: What makes Wasserstein distance sensitive to prior support?

**Model Discrepancy**: The difference between the true physical system and the mathematical model used for inference. *Why needed*: Critical factor affecting utility function performance. *Quick check*: How does model discrepancy affect posterior uncertainty?

**Convection-Diffusion Source Inversion**: A benchmark PDE problem where the goal is to identify source terms from measurements. *Why needed*: Primary numerical example used for comparison. *Quick check*: What makes this problem suitable for testing experimental design?

## Architecture Onboarding

**Component Map**: Experimental Design Parameters -> Utility Computation (KL or Wasserstein) -> Expected Utility Integration -> Design Optimization -> Measurement Planning

**Critical Path**: Design Space Definition → Posterior Sampling → Utility Evaluation → Expected Utility Integration → Optimal Design Selection

**Design Tradeoffs**: KL divergence offers computational efficiency and superior performance under accurate models but lacks robustness to model misspecification. Wasserstein distance provides robustness to model errors but may be computationally more expensive and can produce false rewards with non-informative priors.

**Failure Signatures**: KL divergence fails when model discrepancy is large, leading to overconfident posteriors and poor designs. Wasserstein distance fails with non-informative priors when small posterior shifts create large utility values unrelated to information gain.

**First Experiments**:
1. Compare KL and Wasserstein utilities on a simple Gaussian problem with known analytical solutions
2. Test sensitivity to prior specification by varying prior informativeness
3. Implement both utilities on a small-scale PDE problem with synthetic data

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Analysis is primarily based on a single convection-diffusion source inversion problem and a toy example, limiting generalizability to other domains
- The systematic reasons why Wasserstein distance "retains greater uncertainty in posteriors" enabling better model correction lack deeper theoretical justification
- Computational considerations and implementation costs of Wasserstein-based approaches are not addressed

## Confidence
- Generality of findings to other domains: Medium confidence
- Effect of prior specifications on relative performance: Medium confidence  
- Theoretical justification for Wasserstein's robustness mechanism: Medium confidence
- Computational complexity analysis: Low confidence

## Next Checks
1. Test the proposed methodology on additional benchmark problems from different scientific domains to assess generalizability
2. Conduct a systematic study of how prior specifications affect the relative performance of KL and Wasserstein criteria
3. Perform a computational complexity analysis comparing the practical implementation costs of KL and Wasserstein-based experimental design in realistic scenarios