---
ver: rpa2
title: 'Adaptive Trust Consensus for Blockchain IoT: Comparing RL, DRL, and MARL Against
  Naive, Collusive, Adaptive, Byzantine, and Sleeper Attacks'
arxiv_id: '2512.22860'
source_url: https://arxiv.org/abs/2512.22860
tags:
- attack
- trust
- learning
- against
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates three reinforcement learning approaches for
  defending blockchain IoT networks against five attack types: Naive Malicious Attack
  (NMA), Collusive Rumor Attack (CRA), Adaptive Adversarial Attack (AAA), Byzantine
  Fault Injection (BFI), and Time-Delayed Poisoning (TDP). The study compares tabular
  Q-learning, Deep RL with Dueling Double DQN, and Multi-Agent RL under identical
  simulation conditions on a 16-node network.'
---

# Adaptive Trust Consensus for Blockchain IoT: Comparing RL, DRL, and MARL Against Naive, Collusive, Adaptive, Byzantine, and Sleeper Attacks

## Quick Facts
- arXiv ID: 2512.22860
- Source URL: https://arxiv.org/abs/2512.22860
- Authors: Soham Padia; Dhananjay Vaidya; Ramchandra Mangrulkar
- Reference count: 29
- Key outcome: This paper evaluates three reinforcement learning approaches for defending blockchain IoT networks against five attack types: Naive Malicious Attack (NMA), Collusive Rumor Attack (CRA), Adaptive Adversarial Attack (AAA), Byzantine Fault Injection (BFI), and Time-Delayed Poisoning (TDP). The study compares tabular Q-learning, Deep RL with Dueling Double DQN, and Multi-Agent RL under identical simulation conditions on a 16-node network. Results show that DRL achieves the highest average detection performance (F1=0.77), while MARL excels against coordinated attacks (F1=0.85 vs DRL's 0.68 against CRA). All agents achieve perfect detection against Byzantine attacks (F1=1.00). Most critically, all approaches fail catastrophically against Time-Delayed Poisoning after sleeper activation, with F1 scores dropping to 0.11-0.16, revealing a fundamental vulnerability in trust-based systems. The research demonstrates that coordinated learning provides measurable advantages against coordinated attacks but that temporal trust-building attacks represent a critical limitation requiring dedicated countermeasures.

## Executive Summary
This study evaluates three reinforcement learning approaches for defending blockchain IoT networks against five attack types: Naive Malicious Attack, Collusive Rumor Attack, Adaptive Adversarial Attack, Byzantine Fault Injection, and Time-Delayed Poisoning. The research compares tabular Q-learning, Deep RL with Dueling Double DQN, and Multi-Agent RL under identical simulation conditions on a 16-node network. Results show that DRL achieves the highest average detection performance (F1=0.77), while MARL excels against coordinated attacks (F1=0.85 vs DRL's 0.68 against CRA). All agents achieve perfect detection against Byzantine attacks (F1=1.00). Most critically, all approaches fail catastrophically against Time-Delayed Poisoning after sleeper activation, with F1 scores dropping to 0.11-0.16, revealing a fundamental vulnerability in trust-based systems.

## Method Summary
The study employs a trust-based consensus mechanism where agents dynamically adjust trust scores based on peer behavior and reputation. Three reinforcement learning approaches are evaluated: tabular Q-learning for simple state spaces, Deep RL with Dueling Double DQN for complex patterns, and Multi-Agent RL for coordinated attack scenarios. The simulation environment models a 16-node IoT blockchain network with configurable attack vectors. Trust scores are updated using exponential moving averages weighted by transaction validity and peer reputation. Each RL agent learns to optimize trust thresholds for accepting/rejecting transactions while balancing detection accuracy against network throughput. The experimental design includes 100-500 time steps per simulation with 10-fold cross-validation across different attack patterns.

## Key Results
- DRL achieves highest average detection performance (F1=0.77)
- MARL excels against coordinated attacks (F1=0.85 vs DRL's 0.68 against CRA)
- All approaches achieve perfect detection against Byzantine attacks (F1=1.00)
- All approaches fail catastrophically against Time-Delayed Poisoning after sleeper activation (F1=0.11-0.16)

## Why This Works (Mechanism)
The effectiveness of reinforcement learning approaches stems from their ability to adaptively learn optimal trust thresholds based on observed peer behavior patterns. DRL's superior performance arises from its capacity to model complex temporal dependencies in attack patterns through deep neural networks. MARL's advantage in coordinated attacks results from its ability to learn collaborative strategies across multiple agents, enabling detection of collusion patterns that individual agents might miss. The failure against Time-Delayed Poisoning reveals that trust accumulation mechanisms are inherently vulnerable to long-term behavior manipulation, as the system cannot distinguish between genuine good behavior and strategic deception over extended periods.

## Foundational Learning
- **Trust-based consensus mechanisms**: Why needed - to maintain network integrity without centralized authority; Quick check - evaluate trust score distribution across normal vs. attack scenarios
- **Reinforcement learning fundamentals**: Why needed - to enable adaptive defense strategies that evolve with attack patterns; Quick check - verify convergence of Q-values in tabular approach
- **Deep neural network architectures**: Why needed - to capture complex temporal patterns in multi-step attack sequences; Quick check - assess feature importance through layer visualization
- **Multi-agent coordination**: Why needed - to detect and respond to coordinated attacks that single agents cannot identify; Quick check - measure communication overhead vs. detection improvement
- **Exponential moving averages**: Why needed - to balance recent behavior with historical reputation; Quick check - analyze decay factor impact on false positive rates
- **Time-delayed attack detection**: Why needed - to identify sleeper agents that build trust before launching attacks; Quick check - test detection accuracy at different sleeper activation thresholds

## Architecture Onboarding
Component map: Blockchain IoT network -> Trust consensus layer -> RL agent -> Trust scoring module -> Peer reputation database
Critical path: Transaction validation -> Trust score update -> RL decision -> Consensus acceptance/rejection
Design tradeoffs: Accuracy vs. throughput (higher trust thresholds improve accuracy but reduce throughput); Detection speed vs. false positives (conservative thresholds reduce false positives but increase detection time)
Failure signatures: Gradual trust score inflation followed by sudden drops; Consistent behavior patterns that deviate from statistical norms; Communication patterns showing coordinated behavior among peers
First experiments: 1) Test trust score sensitivity to different decay factors (0.1, 0.5, 0.9); 2) Evaluate detection performance against synthetic coordinated attack patterns; 3) Measure convergence time for Q-learning vs. DQN approaches

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Simulation limited to 16-node network, potentially not capturing dynamics of larger IoT deployments
- Evaluation timeframe (100-500 time steps) may be insufficient to observe long-term learning behaviors and convergence patterns
- Analysis doesn't explore why the system fails catastrophically against Time-Delayed Poisoning or propose specific mitigation strategies

## Confidence
- DRL achieves highest average detection performance: High - Results are clearly presented with specific F1 scores across multiple attack types
- MARL excels against coordinated attacks: Medium - While F1 scores support this claim, the difference could be influenced by specific attack parameters
- All approaches fail against Time-Delayed Poisoning: High - The consistent failure across all methods with specific metrics provides strong evidence

## Next Checks
1. Test the learning algorithms on larger network topologies (50+ nodes) to assess scalability and performance under more realistic conditions
2. Conduct extended simulation runs (1000+ time steps) to evaluate long-term learning stability and convergence patterns
3. Investigate the fundamental vulnerability to Time-Delayed Poisoning by analyzing trust score accumulation patterns and testing hybrid trust-scoring mechanisms that weight recent behavior more heavily