---
ver: rpa2
title: 'Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning in
  Large Audio-Language Models'
arxiv_id: '2509.21749'
source_url: https://arxiv.org/abs/2509.21749
tags:
- audio
- reasoning
- lalms
- arxiv
- acoustic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Thinking-with-Sound (TwS), a framework that
  enhances Large Audio-Language Models (LALMs) by enabling them to perform audio chain-of-thought
  reasoning through dynamic tool integration. Unlike traditional LALMs that treat
  audio as static input, TwS allows models to iteratively analyze and manipulate audio
  signals using specialized operators (e.g., denoising, pitch tracking).
---

# Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio-Language Models

## Quick Facts
- arXiv ID: 2509.21749
- Source URL: https://arxiv.org/abs/2509.21749
- Reference count: 24
- Primary result: TwS framework achieves 24.73% to 36.61% absolute accuracy gains on perturbed audio by enabling iterative audio reasoning in LALMs

## Executive Summary
This paper introduces Thinking-with-Sound (TwS), a framework that enhances Large Audio-Language Models (LALMs) by enabling them to perform audio chain-of-thought reasoning through dynamic tool integration. Unlike traditional LALMs that treat audio as static input, TwS allows models to iteratively analyze and manipulate audio signals using specialized operators (e.g., denoising, pitch tracking). The framework is evaluated on MELD-Hard1k, a robustness benchmark with acoustic perturbations. Results show state-of-the-art LALMs suffer over 50% accuracy drops on perturbed audio, while TwS achieves 24.73% to 36.61% absolute accuracy gains, with improvements scaling with model size. The work demonstrates that active audio reasoning significantly enhances robustness and scalability in audio-language understanding.

## Method Summary
TwS extends LALMs with audio chain-of-thought reasoning by interleaving linguistic reasoning with dynamic audio operator calls. The framework maintains an audio state that evolves through iterative transformations (denoising, enhancement, etc.) based on the model's tool selection decisions. At each reasoning step, the model can either generate text tokens or invoke an operator, which transforms the audio signal that is then re-encoded for subsequent reasoning. The method is training-free, leveraging existing LALM capabilities with minimal prompt engineering to enable tool-augmented reasoning.

## Key Results
- Baseline LALMs experience over 50% accuracy drops on MELD-Hard1k perturbed audio benchmark
- TwS achieves 24.73% to 36.61% absolute accuracy gains across different model sizes
- Improvements scale with model capacity, showing superlinear benefits for larger LALMs
- TwS maintains near-baseline performance on clean audio while dramatically improving robustness

## Why This Works (Mechanism)

### Mechanism 1: Iterative Signal Restoration via Adaptive Operators
TwS reduces encoding error through repeated application of perturbation-specific operators. At each reasoning step, the model may invoke an operator T that transforms the current audio state. If the operator is (ε, ρ)-adaptive for the perturbation type, the deviation from clean audio shrinks by factor ρ < 1. Over K steps, this compounds to exponential error reduction. The framework assumes the operator set contains at least one adaptive operator for each perturbation type encountered during inference.

### Mechanism 2: Scaling Amplification via Improved Tool Selection Accuracy
Larger models achieve superlinear gains because tool selection accuracy α increases with model capacity. The model's decision function φ determines which operator to invoke. Larger LALMs, having stronger reasoning capabilities learned during post-training, more accurately match operators to perturbation types. Higher α compounds with per-step reduction to produce larger overall gains. The framework assumes LALMs possess innate tool-using capabilities from their post-training phases that improve with model scale.

### Mechanism 3: Grounded Cross-Modal Verification via Re-Encoding
TwS enables the model to verify hypotheses through actual acoustic features rather than hallucinating from static embeddings. Standard LALMs encode audio once into fixed tokens za, then reason purely in text space. TwS allows the model to transform audio (xa → T(xa)), re-encode, and ground subsequent reasoning in the new representation. The framework assumes the encoder is sufficiently sensitive that meaningful acoustic changes produce distinguishable embedding differences.

## Foundational Learning

- **Chain-of-Thought (CoT) Reasoning in LLMs**
  - Why needed here: TwS extends text-based CoT to multimodal "Audio CoT" where reasoning steps include audio operations
  - Quick check question: Can you explain why decomposing a complex reasoning task into sequential steps improves accuracy in language models?

- **Basic Audio Signal Processing (Time-Frequency Domain)**
  - Why needed here: Understanding what operators like denoising, pitch tracking, and Fourier transforms actually do to the signal
  - Quick check question: What does a spectrogram represent, and why might denoising help emotion recognition in noisy audio?

- **Tool-Augmented Language Models**
  - Why needed here: TwS relies on the model's ability to autonomously decide when and which tool to call during inference
  - Quick check question: How does Toolformer or ReAct enable LLMs to make API calls without explicit fine-tuning for each tool?

## Architecture Onboarding

- **Component map**:
  Input (xa, xt) -> Encoder -> Audio tokens za -> Reasoning controller -> (Generate text OR invoke operator) -> Operator library -> Transformed audio -> Re-encoding loop -> Updated za -> Next reasoning step

- **Critical path**:
  1. Initialize prompt with available operators
  2. For each step k up to Kmax:
     - Encode current audio za = Enc(xa)
     - Generate reasoning token r = fθ(R, za)
     - If φ detects tool call, parse args and apply T[m](xa, args)
     - Append r to reasoning chain R
  3. Extract final answer from R

- **Design tradeoffs**:
  - Accuracy vs latency: More steps improve accuracy (diminishing returns after ~4 steps) but inference time scales linearly (~2.3× slower)
  - Operator coverage vs complexity: More operators handle more perturbation types but may confuse the selection function
  - Training-free vs fine-tuned: Current approach leverages existing capabilities; fine-tuning could improve α but requires data

- **Failure signatures**:
  - Tool selection failure: Model calls irrelevant operators (e.g., denoising for time-stretch perturbation)
  - Operator inadequacy: No operator in T is adaptive for the encountered perturbation (Corollary 3.5)
  - Premature termination: Model halts reasoning before sufficient restoration
  - Over-processing: Excessive operator applications introduce artifacts

- **First 3 experiments**:
  1. Perturbation ablation: Test TwS on MELD-Hard1k with single perturbation types isolated (noise only, reverb only, etc.) to measure operator-perturbation alignment and validate Corollary 3.5 predictions.
  2. Operator leave-one-out: Remove each operator category and measure accuracy drop to confirm Table 2 findings on your specific model/audio domain.
  3. Scaling replication: Run TwS on at least two model sizes (e.g., 3B and 7B) to verify that gains scale with model capacity as claimed (24.73% → 36.61% in the paper).

## Open Questions the Paper Calls Out

### Open Question 1
Can the TwS framework be effectively adapted for real-time applications without compromising accuracy?
Basis in paper: Section 5.2 notes that inference is approximately 2.3x slower than naive CoT and suggests "adaptive stopping or confidence-based thresholds can further mitigate latency."
Why unresolved: The current implementation focuses on offline robustness, and the efficiency analysis does not validate specific real-time optimization strategies.
What evidence would resolve it: Experiments evaluating latency-accuracy trade-offs using dynamic early-exit mechanisms or confidence thresholds on streaming audio.

### Open Question 2
How robust is the framework to the presence of irrelevant or low-quality audio operators?
Basis in paper: Section 3.3 states, "If the provided operators are irrelevant or misleading, TwS may fail to realize its full potential and, in the worst case, degenerate to the performance of the baseline method."
Why unresolved: The experiments utilize a curated set of effective operators (denoising, enhancement), but do not test the model's ability to filter out useless or detrimental tools.
What evidence would resolve it: Ablation studies introducing noisy or faulty operators into the tool set to measure the model's error recovery capabilities.

### Open Question 3
Does the TwS framework generalize to non-speech audio domains, such as music analysis or environmental sound classification?
Basis in paper: Appendix C suggests the framework can accommodate alternative operator sets for tasks like music analysis, but all reported experiments are restricted to speech-based emotion recognition (MELD).
Why unresolved: The paper does not provide empirical evidence of TwS's effectiveness on audio tasks where different acoustic features (e.g., harmony, rhythm) are paramount.
What evidence would resolve it: Evaluation of TwS on benchmarks like AudioSet or music captioning datasets using domain-specific operators (e.g., beat tracking, source separation).

## Limitations

- The theoretical guarantees depend heavily on the existence of adaptive operators for each perturbation type, but systematic characterization of which operators work for which perturbations is lacking.
- Scaling benefits attributed to improved tool selection accuracy with model size remain observational rather than proven through controlled ablation studies.
- The re-encoding mechanism's effectiveness relies on the encoder's sensitivity to acoustic changes, which is assumed but not empirically validated through feature similarity metrics.

## Confidence

- **High Confidence**: The empirical accuracy gains on MELD-Hard1k are well-documented and reproducible given the specified dataset and perturbation parameters. The observation that baseline LALMs drop >50% on perturbed audio is robust.
- **Medium Confidence**: The claim that TwS improves robustness through iterative signal restoration is supported by results but the theoretical mechanism (Corollary 3.5) requires empirical validation of operator adaptation rates across perturbation types.
- **Low Confidence**: The assertion that scaling benefits are driven by improved tool selection accuracy in larger models lacks direct evidence; the correlation between model size and gains could stem from other factors like better overall reasoning capabilities.

## Next Checks

1. **Operator-Perturbation Alignment Study**: Conduct a systematic ablation where each operator is tested against each perturbation type (noise, reverb, pitch shift, time stretch) in isolation. Measure accuracy improvement per operator-perturbation pair and compute empirical adaptation rates ρ to validate the theoretical model.

2. **Encoder Sensitivity Analysis**: After each operator application, compute quantitative measures of acoustic change (e.g., spectrogram distance, feature drift in embedding space) and correlate with model accuracy improvements to verify that re-encoding captures meaningful signal changes rather than noise.

3. **Controlled Scaling Experiment**: Implement TwS across at least three model sizes (e.g., 3B, 7B, 13B) on the same perturbation set and measure both absolute accuracy gains and per-step tool selection accuracy. Use statistical tests to determine if scaling effects are significant beyond what would be expected from baseline model performance differences.