---
ver: rpa2
title: '$\texttt{Complex-Edit}$: CoT-Like Instruction Generation for Complexity-Controllable
  Image Editing Benchmark'
arxiv_id: '2504.13143'
source_url: https://arxiv.org/abs/2504.13143
tags:
- instruction
- image
- editing
- quality
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Complex-Edit introduces a scalable benchmark for evaluating instruction-based
  image editing models across varying complexity levels, from simple to highly complex
  edits. The authors develop a "Chain-of-Edit" pipeline that uses GPT-4o to generate
  atomic editing tasks, simplify them, and compound them into complex instructions,
  enabling precise control over instruction difficulty.
---

# $\texttt{Complex-Edit}$: CoT-Like Instruction Generation for Complexity-Controllable Image Editing Benchmark

## Quick Facts
- arXiv ID: 2504.13143
- Source URL: https://arxiv.org/abs/2504.13143
- Authors: Siwei Yang; Mude Hui; Bingchen Zhao; Yuyin Zhou; Nataniel Ruiz; Cihang Xie
- Reference count: 40
- Key outcome: Introduces a benchmark for evaluating instruction-based image editing models across varying complexity levels using a "Chain-of-Edit" pipeline that generates compound instructions from atomic tasks.

## Executive Summary
Complex-Edit presents a scalable benchmark for evaluating instruction-based image editing models across different complexity levels. The authors develop a "Chain-of-Edit" pipeline using GPT-4o to generate atomic editing tasks, simplify them, and compound them into complex instructions, enabling precise control over instruction difficulty. The benchmark reveals that open-source models lag behind proprietary models, especially as complexity grows, and that increasing instruction complexity primarily harms identity preservation and aesthetic quality. Sequential step-by-step editing significantly underperforms direct editing, though Best-of-N selection improves results. Notably, models trained with synthetic data tend to produce increasingly synthetic-looking outputs under complex edits, a phenomenon also observed in GPT-4o outputs.

## Method Summary
The Complex-Edit benchmark uses a three-stage Chain-of-Edit pipeline: Sequence Generation creates atomic instructions from 24 predefined operation types using GPT-4o; Simplification removes extraneous commentary while preserving semantic content; Instruction Compounding merges atomic instructions into coherent compound instructions at varying complexity levels. The benchmark includes 1,000 images (800 training, 200 test) and evaluates models using three metrics: Instruction Following (IF), Identity Preservation (IP), and Perceptual Quality (PQ). A VLM-based auto-evaluation system using GPT-4o provides numeric scoring with detailed rubrics. The paper also investigates sequential editing strategies and test-time scaling through Best-of-N selection.

## Key Results
- All models show consistent degradation in Identity Preservation and Perceptual Quality as instruction complexity increases from C1 to C8, with IP drops of 1.7-2.4 points
- Sequential (step-by-step) editing underperforms direct editing due to accumulated artifacts, though Best-of-N selection can partially recover performance
- Models trained on synthetic data produce increasingly synthetic-looking outputs under complex edits, exhibiting a "curse of synthetic data" phenomenon
- Open-source models lag significantly behind proprietary models, with the gap widening as complexity increases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Chain-of-Edit pipeline enables controllable complexity scaling by compounding atomic instructions through GPT-4o.
- **Mechanism:** Three-stage process: (1) Sequence Generation creates atomic instructions from 24 predefined operation types; (2) Simplification removes extraneous commentary; (3) Instruction Compounding merges i atomic instructions into a single coherent instruction at complexity level Ci. The compounding is not mere concatenation—GPT-4o reorders and merges operations (e.g., "add yarn" + "make yarn red" → "add red yarn").
- **Core assumption:** GPT-4o can reliably generate semantically consistent compound instructions without introducing logical conflicts between atomic operations.
- **Evidence anchors:** [section] "Given a sequence of N atomic instructions, we progressively generate N compound instructions corresponding to different levels of complexity"; [section] "We provide detailed descriptions of each operation type... fed into GPT-4o to generate a sequence of atomic instructions"; [corpus] Weak corpus support—neighbor papers focus on editing methods, not complexity-controllable benchmark generation
- **Break condition:** If atomic instructions contain semantic conflicts (e.g., "remove the car" + "change the car's color"), compounding yields incoherent instructions that cannot be evaluated meaningfully.

### Mechanism 2
- **Claim:** Increasing instruction complexity primarily degrades Identity Preservation and Perceptual Quality, while Instruction Following varies by model.
- **Mechanism:** Complex edits require simultaneous modification of multiple image regions while preserving unchanged elements. Models struggle with this dual constraint—satisfying new instructions often corrupts preserved content. The paper shows all models exhibit IP drops of 1.7–2.4 points from C1→C8 on real images.
- **Core assumption:** The three metrics (IF, IP, PQ) capture orthogonal failure modes rather than measuring the same underlying degradation.
- **Evidence anchors:** [abstract] "Increased instructional complexity primarily impairs the models' ability to retain key elements from the input images and to preserve the overall aesthetic quality"; [section] Table 4 shows C8−C1: IP drops range from -1.82 to -2.38 across models; [corpus] Neighbor papers confirm instruction following is a core challenge but do not systematically analyze complexity scaling
- **Break condition:** If metrics are highly correlated, the decomposition may not reveal distinct failure modes.

### Mechanism 3
- **Claim:** Sequential (step-by-step) editing underperforms direct editing due to accumulated artifacts across intermediate steps.
- **Mechanism:** Each editing step introduces small distortions; sequential application compounds these errors (ysequence = f(f(...f(x, t1)...), ti)). Even Best-of-N selection at each step cannot fully recover—sequential Best-of-4 barely matches direct editing without selection.
- **Core assumption:** Image editing models have non-negligible error per step that accumulates multiplicatively rather than being corrected in subsequent steps.
- **Evidence anchors:** [abstract] "Decomposing a complex instruction into a sequence of atomic steps, executed in a step-by-step manner, substantially degrades performance across multiple metrics"; [section] "Sequential editing yields a steady decline in performance across all three metrics, with accumulating visual artifacts and distortions"; [corpus] Guo et al. [7] found CoT-style sequential generation effective for text-to-image, but this does not transfer to editing where identity preservation is critical
- **Break condition:** If models could perform "self-correction" across steps (detecting and fixing prior artifacts), sequential editing might match or exceed direct editing.

## Foundational Learning

- **Concept: Directional vs Image-wise Similarity**
  - Why needed here: The paper's IF and IP metrics conceptually map to CLIP_dir (measures intended change) and CLIP_img (measures unintended change) from prior work.
  - Quick check question: Can you explain why a model could score high on IF but low on IP?

- **Concept: Test-Time Scaling (Best-of-N)**
  - Why needed here: The paper evaluates Best-of-N selection as a simple test-time compute strategy, selecting outputs with highest Overall score O.
  - Quick check question: Why does Best-of-N help sequential editing more than direct editing in Figure 14?

- **Concept: Synthetic Data Artifacts ("Curse of Synthetic Data")**
  - Why needed here: Models trained on synthetic data produce increasingly synthetic-looking outputs under complex edits—referenced as aligning with model collapse concerns.
  - Quick check question: What evidence would distinguish "synthetic training data causes artifacts" from "complex edits are inherently harder"?

## Architecture Onboarding

- **Component map:** GPT-4o (Sequence Generation → Simplification → Compounding) → VLM autograder (GPT-4o) with numeric scoring + detailed rubrics → Evaluation (IF, IP, PQ metrics)
- **Critical path:** The evaluation pipeline's reliability determines benchmark validity. Per-sample variance requires 20 independent evaluations per sample for stable scores (Figure 7).
- **Design tradeoffs:** CoT reasoning helps IF/IP evaluation but hurts PQ evaluation (Table 2); providing instructions during PQ evaluation reduces human correlation from 0.234 → 0.046; arithmetic mean outperforms geometric mean for Overall score (0.334 → 0.386 correlation)
- **Failure signatures:** Sequential editing: Visual artifacts accumulate; IP and PQ degrade monotonically with step count; Synthetic data curse: Outputs resemble oil paintings/animations under complex edits (Figure 10); VLM instability: Single evaluations show high variance; need 20+ measurements
- **First 3 experiments:** 1. Reproduce the meta-evaluation correlation analysis (Table 2) on a held-out sample to validate the autograder configuration; 2. Run a baseline model (e.g., OmniGen) on C1, C4, C8 complexity levels to confirm the IP/PQ degradation pattern; 3. Test Best-of-N=4 on sequential editing with a single open-source model to quantify the recovery gap vs direct editing

## Open Questions the Paper Calls Out
None

## Limitations
- VLM Evaluation Reliability - The evaluation pipeline depends heavily on GPT-4o for both instruction generation and assessment. While the paper reports correlation with human judgments, the meta-evaluation suggests that CoT reasoning improves some metrics while degrading others. The claimed need for 20+ evaluations per sample to achieve stable scores indicates substantial per-sample variance.
- Synthetic vs Real Data Generalization - The benchmark reveals that models trained on synthetic data produce increasingly synthetic-looking outputs under complex edits. However, the paper doesn't systematically investigate whether this reflects fundamental limitations of synthetic data or specific model architectures.
- Step-by-Step vs Direct Editing Gap - While the paper shows sequential editing underperforms direct editing due to accumulated artifacts, the analysis doesn't explore whether more sophisticated sequential strategies (e.g., intermediate image restoration, conflict detection) could close this gap.

## Confidence
**High Confidence**: The benchmark framework design, including the three-stage Chain-of-Edit pipeline and the three-metric evaluation system, is well-specified and reproducible. The observation that increasing complexity harms identity preservation and aesthetic quality is consistently supported across multiple models.

**Medium Confidence**: The claim that open-source models lag proprietary models more severely at higher complexity levels is supported but based on a limited model comparison. The VLM evaluation correlation results are encouraging but may not generalize to all editing scenarios.

**Low Confidence**: The "curse of synthetic data" interpretation is largely observational without systematic ablation studies. The sequential editing analysis identifies a performance gap but doesn't explore whether this gap is fundamental or could be mitigated through better sequential strategies.

## Next Checks
1. **VLM Evaluation Validation** - Replicate the meta-evaluation correlation analysis (Table 2) on a held-out sample of 50 images to verify that CoT reasoning improves IF/IP evaluation without degrading PQ evaluation, and that providing instructions during PQ evaluation reduces human correlation as claimed.

2. **Complexity Scaling Verification** - Run a baseline open-source model (e.g., OmniGen) on 20 images at C1, C4, and C8 complexity levels to independently verify that IP drops 1.7-2.4 points and PQ degrades similarly as reported in Table 4.

3. **Sequential Editing Recovery Gap** - Test Best-of-N=4 selection on sequential editing with a single open-source model (e.g., T2I-Adapter) to quantify whether the sequential editing recovery gap (barely matching direct editing without selection) holds across multiple complexity levels.