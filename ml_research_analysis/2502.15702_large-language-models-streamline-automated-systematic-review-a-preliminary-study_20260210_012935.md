---
ver: rpa2
title: 'Large language models streamline automated systematic review: A preliminary
  study'
arxiv_id: '2502.15702'
source_url: https://arxiv.org/abs/2502.15702
tags:
- gpt-4
- claude-3
- accuracy
- data
- systematic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated three large language models (GPT-4, Claude-3,
  and Mistral 8x7B) for automating systematic review tasks including study design,
  search strategy, screening, and data extraction. GPT-4 achieved the highest accuracy
  in search strategy formulation (0.78), abstract screening (0.92), and data extraction
  (0.81).
---

# Large language models streamline automated systematic review: A preliminary study

## Quick Facts
- arXiv ID: 2502.15702
- Source URL: https://arxiv.org/abs/2502.15702
- Reference count: 0
- Primary result: GPT-4 achieved highest accuracy in search strategy (0.78), abstract screening (0.92), and data extraction (0.81)

## Executive Summary
This study evaluated three large language models (GPT-4, Claude-3, and Mistral 8x7B) for automating systematic review tasks including study design, search strategy, screening, and data extraction. GPT-4 achieved the highest accuracy in search strategy formulation (0.78), abstract screening (0.92), and data extraction (0.81). Claude-3 showed superior performance in PICO design, while all models demonstrated varying degrees of reliability across tasks. The results indicate that GPT-4 and Claude-3 are promising assistive tools for systematic review automation, though human oversight remains essential for accuracy and quality.

## Method Summary
The study evaluated three LLMs on four systematic review tasks using a dataset from a published review on neuraxial vs general anaesthesia for hip fracture surgery. Models were accessed via API with temperature=0, and prompts were iteratively refined on 2 pilot RCTs. Each task was run 5 times for reliability assessment. Ground truth included 811 abstracts, 29 full-texts, and 20 RCTs with 1,120 data points. Accuracy was measured across PICO design, search strategy retrieval, screening decisions, and structured data extraction fields (answer, quote, location).

## Key Results
- GPT-4 achieved highest accuracy in search strategy formulation (0.78), abstract screening (0.92), and data extraction (0.81)
- Claude-3 outperformed others in PICO design with higher integrity and relevance scores
- All models showed varying reliability, with GPT-4 and Claude-3 demonstrating low variability (CV<0.10)
- Mistral 8x7B performed significantly worse in data extraction (46% accuracy) compared to proprietary models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Higher information density in shorter texts (abstracts) correlates with higher model accuracy compared to diffuse long-form texts (full articles).
- **Mechanism:** LLMs utilize attention mechanisms more effectively when relevant criteria are concentrated in a single paragraph (abstract) rather than distributed across sections (full text). The paper suggests that full-text screening imposes a higher "attentional load" and contains "secondary information" that may distract the model from eligibility criteria.
- **Core assumption:** The drop in performance from abstract to full-text screening is primarily due to context length and noise, not just model reasoning capability.
- **Evidence anchors:** GPT-4 achieved highest accuracy (0.92) for abstract screening vs (0.75) for full-text screening. Full-text documents contain extensive details, increasing attentional load.

### Mechanism 2
- **Claim:** Setting the sampling temperature to zero enforces determinism, which improves reliability for structured academic tasks.
- **Mechanism:** By forcing the model to select the highest probability token at each step (Temperature = 0), the model eliminates stochastic variance. This allows for consistent "correctness" evaluation, as the model will not deviate from its learned "best" path for a given prompt.
- **Core assumption:** The model's highest probability output represents its optimal capability for the specific prompt design.
- **Evidence anchors:** For all tested models, the temperature parameter was set to zero to ensure more reliable and reproducible results. GPT-4 and Claude-3 showed low variability (CV<0.10).

### Mechanism 3
- **Claim:** Proprietary large-scale models (GPT-4, Claude-3) currently outperform open-source mixture-of-experts models (Mistral 8x7B) in complex, multi-step reasoning required for data extraction.
- **Mechanism:** Data extraction requires mapping unstructured text to a strict schema (Answer, Quote, Location). The significant performance gap suggests that larger parameter counts or superior instruction tuning in proprietary models provide better "grounding"â€”the ability to locate the specific source text supporting an answer.
- **Core assumption:** The gap is due to model capability/architecture rather than the specific API integration or prompt formatting used in the study.
- **Evidence anchors:** In data extraction, GPT-4 significantly outperformed other models (accuracy: GPT-4: 0.81 vs Claude-3: 0.59 vs Mistral: 0.46). Mistral showed moderate variability and lower accuracy in quantitative outcomes.

## Foundational Learning

- **Concept: PICO Framework (Population, Intervention, Comparison, Outcome)**
  - **Why needed here:** This is the standard schema the models were prompted to generate. Without understanding that "P" represents the patient cohort (e.g., hip fracture patients), one cannot evaluate why Claude-3 scored higher in "integrity" or "relevance."
  - **Quick check question:** If a study compares Drug A vs. Surgery for lung cancer, which part of PICO is "Surgery"?

- **Concept: Hallucination vs. Grounding (with Quotes)**
  - **Why needed here:** The study evaluated models on "Answer," "Quote," and "Location." Models often fabricated answers. Understanding that "Quote" verification is the mechanism to prevent hallucination is critical for system design.
  - **Quick check question:** If an LLM extracts a mortality rate of 5% but provides a quote saying "mortality was negligible," which data field indicates the error?

- **Concept: Systematic Review (SR) Screening Workflow**
  - **Why needed here:** The paper differentiates between Abstract Screening (high volume, low detail) and Full-Text Screening (low volume, high detail). The system architecture depends on this cascade.
  - **Quick check question:** Why might a model achieve 92% accuracy on abstracts but only 75% on full texts, despite having more information in the full text?

## Architecture Onboarding

- **Component map:** PDF/Text documents converted to JSON (stripped of formatting via PyMuPDF) -> LLM APIs (GPT-4/Claude) accessed with temperature=0 and specific prompt templates -> Python scripts comparing LLM JSON output against "Ground Truth" JSON
- **Critical path:** The Data Extraction phase. This showed the largest variance (GPT-4 at 81% vs Mistral at 46%) and processes the highest volume of data points (1,120 points). Optimizing prompts here yields the highest ROI.
- **Design tradeoffs:**
  - **Context vs. Cost:** Feeding full texts (high context) costs more and lowers accuracy compared to abstracts, but is necessary for extraction.
  - **Model Selection:** Claude-3 is preferred for Protocol/PICO design (creative/structural), while GPT-4 is preferred for Screening/Extraction (factual/grounding).
- **Failure signatures:**
  - **"Criteria Drift":** The paper notes LLMs failed to restrict age or study types in eligibility criteria.
  - **"Numerical Hallucination":** Mistral and Claude struggled with quantitative outcomes (accuracy < 51% for Claude), suggesting they inferred results rather than extracting them.
- **First 3 experiments:**
  1. **Validation of "Temp=0":** Rerun the PICO generation task with Temperature=0.7 to quantify the variance introduced (checking the reliability analysis claim).
  2. **Quote Verification Test:** Run the extraction task asking the model *only* for the Quote and Location first, then ask for the Answer in a second pass to see if two-step reasoning improves grounding accuracy.
  3. **Full-text Truncation:** Chunk full-text articles into sections (Methods, Results) and screen sections individually to test if "attentional load" is the primary bottleneck for full-text screening accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does LLM performance in systematic review automation vary across different clinical domains and review types beyond the specific anesthesia topic tested?
- **Basis in paper:** The authors explicitly state in the limitations that "only one SR type was included, necessitating further exploration across more types and studies."
- **Why unresolved:** The study utilized a single dataset derived from a review on neuraxial versus general anesthesia, leaving the generalizability of these results to other medical fields or review methodologies unknown.
- **What evidence would resolve it:** A multi-domain evaluation replicating this protocol across diverse medical specialties and systematic review typologies.

### Open Question 2
- **Question:** Can advanced techniques like Retrieval-Augmented Generation (RAG) or refined prompt engineering significantly improve LLM accuracy in full-text screening and data extraction?
- **Basis in paper:** The authors acknowledge they "did not employ additional techniques to enhance their capabilities across various tasks," despite identifying these methods as advancements in the introduction.
- **Why unresolved:** The study utilized a baseline API approach with zero temperature, but did not test optimization strategies that could mitigate the observed drop in performance during full-text analysis.
- **What evidence would resolve it:** Comparative studies applying RAG or chain-of-thought prompting to the same systematic review tasks to measure accuracy gains against the baseline.

### Open Question 3
- **Question:** How do LLMs compare to traditional, non-generative automation tools in terms of accuracy and efficiency for screening and data extraction?
- **Basis in paper:** The authors note the study "offers a horizontal comparison of three leading LLMs, yet does not compare them against other types of automation tools."
- **Why unresolved:** Without benchmarking against established machine learning classifiers or specialized systematic review software, it is difficult to determine if LLMs offer a distinct advantage over current automation standards.
- **What evidence would resolve it:** A head-to-head benchmark study evaluating leading LLMs and traditional automation tools on the same dataset for screening and extraction tasks.

## Limitations

- The evaluation was conducted on a single review topic (neuraxial vs. general anaesthesia for hip fracture surgery), limiting generalizability across medical domains
- The study did not evaluate cost-effectiveness or compare LLM performance against human reviewers working at equivalent speeds
- Exact prompt formulations that achieved these results were not provided, requiring reconstruction for replication

## Confidence

- **High confidence:** Abstract screening accuracy (0.92) and reliability metrics (CV<0.10 for GPT-4 and Claude-3)
- **Medium confidence:** Data extraction accuracy differences between models (81% vs 46-59%) given consistent methodology across tasks
- **Medium confidence:** The mechanism explaining why abstracts outperform full-text screening due to attentional load
- **Low confidence:** The specific prompt formulations that achieved these results, as exact prompts were not provided

## Next Checks

1. **Cross-domain validation:** Test the same models and methodology on systematic reviews from different medical specialties to assess generalizability of performance patterns.

2. **Human comparison study:** Compare LLM performance against human reviewers using the same screening protocols and time constraints to establish relative efficiency.

3. **Error analysis replication:** Replicate the study's error analysis focusing on the "Criteria Drift" and "Numerical Hallucination" failure modes to validate their prevalence and develop targeted mitigation strategies.