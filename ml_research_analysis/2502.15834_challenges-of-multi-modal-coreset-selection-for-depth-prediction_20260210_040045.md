---
ver: rpa2
title: Challenges of Multi-Modal Coreset Selection for Depth Prediction
arxiv_id: '2502.15834'
source_url: https://arxiv.org/abs/2502.15834
tags:
- coreset
- selection
- multimodal
- dataset
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the challenge of adapting coreset selection
  methods, which are effective for reducing computational costs in training, to multimodal
  settings, specifically focusing on depth prediction tasks using both RGB images
  and semantic masks as inputs. The authors extend a state-of-the-art coreset selection
  approach to multimodal data by employing embedding aggregation and dimensionality
  reduction techniques, such as PCA and UMAP, on concatenated or averaged embeddings
  from the MultiMAE transformer model.
---

# Challenges of Multi-Modal Coreset Selection for Depth Prediction

## Quick Facts
- arXiv ID: 2502.15834
- Source URL: https://arxiv.org/abs/2502.15834
- Authors: Viktor Moskvoretskii; Narek Alvandian
- Reference count: 6
- Primary result: Multimodal coreset selection achieves ~50% of full dataset performance for depth prediction, showing minimal improvement over random selection

## Executive Summary
This paper investigates the challenge of extending coreset selection methods—effective for reducing computational costs in training—to multimodal settings, specifically focusing on depth prediction using both RGB images and semantic masks. The authors adapt a state-of-the-art submodular coreset selection approach by employing embedding aggregation and dimensionality reduction techniques on concatenated or averaged embeddings from the MultiMAE transformer model. Their experiments on the CLEVR dataset reveal that multimodal coreset selection achieves only around 50% of the performance of training on the full dataset, showing minimal improvement over random selection. These results highlight the difficulty of extending unimodal coreset methods to multimodal scenarios and underscore the need for specialized techniques to better capture inter-modal relationships.

## Method Summary
The paper extends a submodular coreset selection method to multimodal depth prediction by extracting embeddings from the MultiMAE transformer using both RGB and semantic mask inputs. Three aggregation strategies are tested: concatenation, mean, and sum of tokens. Dimensionality reduction via PCA (512-4096 features) and UMAP is applied to the aggregated embeddings before submodular selection using a facility location-style gain function with recursive binning for diversity. The coreset is 20% of the dataset with N=20 bins. Training uses DPT output adapter for 40 epochs with batch size 128 and Adam optimizer (lr=5×10⁻⁵, β₁=0.9, β₂=0.99, no weight decay). Validation RMSE, loss, and training loss are reported relative to full dataset training.

## Key Results
- Multimodal coreset selection achieves only ~50% of full dataset performance for depth prediction
- PCA with 1024 features showed incremental improvement but still near-random performance
- Performance varied little between aggregation methods (concat: 49.08%, mean: 51.54%, sum: 51.11% Val RMSE retained)
- UMAP dimensionality reduction showed no consistent gain over PCA

## Why This Works (Mechanism)

### Mechanism 1: Submodular Gain for Diversity-Preserving Selection
Selecting samples via submodular gain maximization retains dataset diversity by prioritizing samples that are distant from already-selected points while close to unselected ones. The gain function P(xi) balances two forces—pushing away from the current subset Si-1 (numerator) and pulling toward remaining samples in D\Si-1 (denominator). Recursive binning followed by uniform sampling ensures recent selections don't dominate. Core assumption: Embedding distance in feature space correlates with informational uniqueness for the downstream task.

### Mechanism 2: Token Aggregation for Multimodal Fusion
Concatenating tokens from multiple modalities creates a unified representation for coreset selection, but aggregation method impacts information retention. Three approaches tested—concatenation (preserves spatial structure, 768+ dims), mean (collapses to fixed 768), sum (similar to mean but different scale). The concatenated representation feeds directly into the gain computation. Core assumption: Late fusion at the token level preserves sufficient cross-modal interactions for selection decisions.

### Mechanism 3: Dimensionality Reduction Preserves Discriminative Structure
PCA can compress multimodal embeddings while retaining the variance structure needed for effective coreset selection, but optimal dimensionality is task-dependent. PCA projects high-dim concatenated tokens to lower dimensions (512-4096 tested). UMAP tested as non-linear alternative. Lower dims risk losing modality-specific signals; higher dims risk noise. Core assumption: Principal components capturing maximum variance align with features most relevant for depth prediction.

## Foundational Learning

- **Concept: Submodular functions and facility location**
  - Why needed here: The gain function is a facility-location-style submodular objective; understanding why it promotes diversity requires grasping diminishing returns property.
  - Quick check question: Can you explain why adding a sample similar to existing selections yields lower gain than adding a unique sample?

- **Concept: Multimodal representation learning (fusion strategies)**
  - Why needed here: Paper tests late fusion via concatenation; knowing alternatives (early, intermediate, attention-based) contextualizes why this approach may underperform.
  - Quick check question: What information might be lost when fusing RGB and semantic mask tokens only at the embedding level?

- **Concept: Curse of dimensionality in distance metrics**
  - Why needed here: High-dimensional concatenated embeddings (768×M modalities) may have poorly behaved distances; PCA partly addresses this.
  - Quick check question: Why do pairwise distances become less discriminative as dimensionality increases?

## Architecture Onboarding

- **Component map**: MultiMAE backbone -> Token aggregator (concat/mean/sum) -> Optional PCA/UMAP -> Submodular selector (gain function + binning) -> DPT output adapter
- **Critical path**: Embedding extraction quality -> aggregation fidelity -> dimensionality retention -> gain computation accuracy -> coreset representativeness -> downstream depth prediction
- **Design tradeoffs**: Concat vs. mean aggregation: concat preserves more info but higher dims; mean is cheaper but may blur modality boundaries. PCA dimension: too low loses signal; too high retains noise and slows selection. Number of bins N: more bins = finer granularity but higher compute for gain evaluation
- **Failure signatures**: Val RMSE ~50% of full dataset (near random): embeddings not capturing task-relevant structure or inter-modal relationships. UMAP underperforming PCA: local structure preservation may not align with global diversity needs. Training convergence worse than full dataset: reduced representativeness leads to slower optimization
- **First 3 experiments**:
  1. Baseline sanity check: Run random coreset selection 5× with different seeds to establish variance bounds; compare against paper's 50.23% Val RMSE baseline.
  2. Ablate aggregation methods with fixed PCA dimension (1024): Test concat/mean/sum while holding reduction constant to isolate aggregation impact from dimensionality effects.
  3. Probe inter-modal coupling: Compute mutual information between RGB and mask tokens in selected vs. rejected samples—if similar, the method ignores cross-modal structure (supporting paper's hypothesis).

## Open Questions the Paper Calls Out

### Open Question 1
How can coreset selection techniques be redesigned to explicitly capture inter-modal relationships rather than relying on simple embedding aggregation? The conclusion states that current adaptations highlight the "need for specialized methods to better capture inter-modal relationships." The authors' approach of aggregating tokens via concatenation or PCA resulted in performance statistically similar to random selection. A new selection algorithm that significantly outperforms the random baseline (>>55% quality retention) by utilizing cross-modal attention or dependency metrics would resolve this.

### Open Question 2
Do the observed limitations of multimodal coreset selection generalize to complex, real-world datasets beyond the synthetic CLEVR benchmark? The experiments are restricted to the CLEVR dataset, whereas the introduction motivates the work using complex domains like autonomous driving and medical diagnosis. Synthetic data often lacks the noise and modality misalignment found in real-world scenes, so the failure of current methods on CLEVR may not predict failure elsewhere. Replicating the experimental procedure on datasets like NYU Depth V2 or nuScenes to verify if near-random performance persists would resolve this.

### Open Question 3
Does the dimensionality of the reduced feature space (e.g., PCA components) have a non-monotonic relationship with coreset quality in multimodal settings? Results in Table 1 show PCA at 1024 features performed best, while increasing to 2048 or 4096 features caused performance to drop, suggesting an unclear trade-off. The paper reports the phenomenon but does not analyze why retaining more information (higher dimensions) led to worse coreset selection. An ablation study analyzing the variance explained by PCA components relative to the submodular gain scores would resolve this.

## Limitations
- The paper demonstrates that straightforward extension of unimodal coreset methods to multimodal settings yields minimal gains over random selection, suggesting fundamental limitations in current approaches to capturing inter-modal relationships
- All experiments are conducted on CLEVR, a synthetic dataset with controlled conditions, which may not generalize to real-world depth prediction tasks
- The study uses MultiMAE for embedding extraction with DPT for depth prediction, but does not explore alternative multimodal backbones or fusion strategies

## Confidence
- **High confidence**: The experimental methodology is sound, with proper ablation studies on aggregation methods and dimensionality reduction. The claim that multimodal coreset selection achieves ~50% performance of full dataset training is well-supported by the data.
- **Medium confidence**: The interpretation that this represents a fundamental challenge in extending unimodal methods to multimodal settings is reasonable but could be influenced by the specific architectural choices (MultiMAE + DPT) and dataset (CLEVR).
- **Low confidence**: The suggestion that specialized techniques are needed without proposing specific alternatives is speculative. The paper identifies the problem but does not provide clear directions for solutions.

## Next Checks
1. Replicate the experiments on a real-world dataset like NYU Depth V2 or ScanNet to assess whether the ~50% performance ceiling is dataset-specific or represents a general limitation of current multimodal coreset approaches.

2. Test early fusion approaches where RGB and semantic mask are combined before MultiMAE processing, or attention-based multimodal transformers, to determine if late fusion at the token level is indeed the limiting factor.

3. Quantify the mutual information and correlation between RGB and semantic mask embeddings in selected vs. rejected samples to empirically validate whether the coreset selection process is ignoring cross-modal relationships, as the paper hypothesizes.