---
ver: rpa2
title: 'MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal
  LLMs in Multi-Turn Dialogues'
arxiv_id: '2510.17722'
source_url: https://arxiv.org/abs/2510.17722
tags:
- video
- arxiv
- answer
- wang
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MT-Video-Bench is a new benchmark for evaluating multimodal large\
  \ language models (MLLMs) in multi-turn video dialogues. It tests six core capabilities\u2014\
  object reference, memory recall, content summary, answer refusal, topic shifting,\
  \ and proactive interaction\u2014across 1,000 dialogues spanning diverse video domains."
---

# MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues

## Quick Facts
- **arXiv ID:** 2510.17722
- **Source URL:** https://arxiv.org/abs/2510.17722
- **Authors:** Yaning Pan; Zekun Wang; Qianqian Xie; Yongqian Wen; Yuanxing Zhang; Guohui Zhang; Haoxuan Hu; Zhiyu Pan; Yibing Huang; Zhidong Gan; Yonghong Lin; An Ping; Tianhao Peng; Jiaheng Liu
- **Reference count:** 37
- **Primary result:** Tests 20 state-of-the-art MLLMs across 1,000 dialogues, revealing 76.95% overall accuracy with significant drops on interactivity tasks and as scene complexity increases

## Executive Summary
MT-Video-Bench is a comprehensive benchmark designed to evaluate multimodal large language models (MLLMs) in multi-turn video dialogues. It tests six core capabilities—object reference, memory recall, content summary, answer refusal, topic shifting, and proactive interaction—across diverse video domains. Experiments with 20 state-of-the-art models reveal that even top-performing systems achieve only 76.95% overall accuracy, with significant performance drops on interactivity tasks and as scene complexity increases. These results highlight substantial room for improvement in handling long-range dependencies, cross-scene reasoning, and adaptive conversational engagement in MLLMs.

## Method Summary
The benchmark uses a checklist-based evaluation pipeline where each QA pair is decomposed into 5 yes/no verification questions (atomic facts from ground truth plus one intentionally reversed fact). An automated evaluator (Gemini-2.5-Flash) judges each checklist item against the model response, with accuracy computed as the proportion of correct items. The benchmark was constructed using a multi-stage pipeline involving frame extraction, object detection, memory bank creation, dialogue generation with human verification, and automated checklist assessment. Performance is evaluated across golden context (curated history) and self-predicted context settings to quantify cascade effects.

## Key Results
- Even top-performing models achieve only 76.95% overall accuracy across 1,000 dialogues
- Answer refusal capability remains a universal bottleneck at 25-58% accuracy across models
- Performance consistently degrades as scene complexity increases, with a ~13% drop from 1-5 to >20 scenes
- A significant cascade effect occurs where early dialogue errors accumulate, causing dialogue collapse

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Checklist-based evaluation provides reliable, automated assessment of multi-turn dialogue quality that aligns with human judgment.
- **Mechanism:** Each QA pair is decomposed into 5 yes/no verification questions (atomic facts from ground truth, plus one intentionally reversed fact). An automated evaluator judges each checklist item against the model response, and accuracy is computed as the proportion of correct items.
- **Core assumption:** Automated evaluators (e.g., Gemini-2.5-Flash) can accurately judge factual coverage and correctness without human intervention.
- **Evidence anchors:**
  - [abstract]: "assesses performance via checklist-based evaluation"
  - [Section 3.4]: Describes the full pipeline—Gemini-2.5-Flash constructs 5 yes/no questions per QA pair, followed by manual validation and automated judgment.
  - [Section 4.5, Table 3]: Agreement evaluation shows Spearman=95.25, Pearson=97.01 between automated and human evaluation, supporting reliability.
  - [corpus]: No direct corpus papers validate this specific checklist approach; mechanism is paper-internal.

### Mechanism 2
- **Claim:** Increased scene complexity causes measurable performance degradation due to long-range temporal dependency challenges.
- **Mechanism:** As dialogue-relevant content spans more disjoint scenes, models must integrate spatio-temporal information across fragmented video contexts. Performance declines because MLLMs struggle to maintain coherent grounding when visual evidence is distributed.
- **Core assumption:** Scene count is a valid proxy for reasoning complexity; models fail primarily on integration, not per-scene perception.
- **Evidence anchors:**
  - [abstract]: "performance declining as scene complexity and dialogue turns increase"
  - [Section 4.3, Figure 4(b)]: Gemini-2.5-Pro drops ~13% as scene count increases from 1–5 to >20; Memory Recall and Topic Shifting degrade most severely.
  - [corpus]: Related work (Streaming Video Understanding and Multi-round Interaction, FMR=0.52) confirms processing long video sequences and multi-turn dialogues remains challenging, supporting the generalization.

### Mechanism 3
- **Claim:** Multi-turn dialogue performance exhibits a "cascade effect" where early prediction errors accumulate, causing dialogue collapse.
- **Mechanism:** In self-predicted context settings, each turn's model output becomes the next turn's input history. Early hallucinations or omissions propagate forward, making later turns increasingly ungrounded. Golden context (curated history) mitigates but does not eliminate turn-level decay.
- **Core assumption:** The gap between golden and self-predicted context is primarily caused by error propagation, not other factors like prompt drift.
- **Evidence anchors:**
  - [Section 4.3, Figure 4(c)]: Significant gap between golden context and self-predicted context on Qwen3-VL-8B-Instruct; even golden context shows decay with more turns.
  - [Section 4.4]: Error analysis identifies "inter-turn interference" as a primary failure mode.
  - [corpus]: No corpus papers directly validate cascade effect; this is a paper-internal finding.

## Foundational Learning

- **Concept:** Multi-turn contextual grounding
  - **Why needed here:** Each dialogue turn depends on prior conversational history plus video evidence; models must jointly reason over both modalities to maintain coherence.
  - **Quick check question:** Given a 6-turn dialogue about a sports video, can you trace which prior turn introduced the entity being referenced in turn 5?

- **Concept:** Spatio-temporal video representation
  - **Why needed here:** Benchmark requires cross-scene reasoning (e.g., tracking objects across disjoint clips); understanding how MLLMs encode frame sequences is essential for interpreting performance gaps.
  - **Quick check question:** How does uniform frame sampling (e.g., 128 frames at 720p) differ from scene-aware sampling, and what information might each lose?

- **Concept:** Checklist-based evaluation design
  - **Why needed here:** The benchmark uses atomic fact decomposition plus fact reversal to construct verification questions; understanding this helps interpret what accuracy scores actually measure.
  - **Quick check question:** If a ground-truth answer contains 5 atomic facts, and the checklist includes 4 "Yes" questions and 1 "No" (reversed fact), what does a 3/5 score indicate about the model response?

## Architecture Onboarding

- **Component map:** Video Input → Frame Extraction (2 FPS) → Sharpness/Similarity Filtering → Object Detection (YOLOv11) + Captioning (Gemini-2.5-Flash) → Object Memory Bank → Cross-Scene Merging → Multi-Turn Dialogue Generation (Gemini-2.5-Pro) → Human Verification → Checklist Generation (5 yes/no per QA) → Automated Evaluation

- **Critical path:** The object memory bank and cross-scene merging step determines whether dialogues require genuine multi-scene reasoning or can be solved locally. If merging is too aggressive, cross-scene tasks become trivial; if too conservative, dialogues may lack coherence.

- **Design tradeoffs:**
  - **Frame count vs. compute:** Paper shows 128 frames optimal for Qwen3-VL-8B; 256 frames introduces noise. Higher-capacity models may benefit from more frames.
  - **Resolution vs. plateau:** Most models peak at 720p; Gemini-2.5-Flash continues improving up to 960p. Resolution scaling depends on model capacity.
  - **Automated vs. human evaluation:** Automated evaluation achieves ~95% agreement with humans (Table 3), but introduces evaluator-specific bias. Human verification remains essential for benchmark quality.

- **Failure signatures:**
  - **Answer Refusal:** Low scores across all models (~25–58%) indicate systematic hallucination when video evidence is absent—models fabricate rather than decline.
  - **Memory Recall:** Sensitive to scene count (Figure 4b); degrades sharply when relevant information spans many scenes.
  - **Proactive Interaction:** Models often provide conclusive answers without inviting engagement (Figure 21), failing the interactivity criterion.

- **First 3 experiments:**
  1. **Establish baseline:** Run evaluation on your target MLLM using the provided inference prompt template (Figure 15) with 128 frames at 720p. Compare per-task accuracy to Table 2 benchmarks.
  2. **Ablate context mode:** Compare golden context vs. self-predicted context vs. no context on a 50-dialogue subset. Quantify the cascade effect magnitude for your model.
  3. **Probe scene sensitivity:** Filter dialogues by scene count (1–5, 6–10, 11–20, >20) and measure per-bin accuracy. Identify which tasks degrade fastest; this reveals whether your model's bottleneck is temporal integration or dialogue coherence.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can MLLMs be improved to recognize knowledge boundaries and perform answer refusal more effectively in video-grounded dialogues?
- **Basis in paper:** [explicit] The paper identifies Answer Refusal (AR) as "a universal bottleneck, as even the strongest models struggle to effectively recognize knowledge boundaries," with top models achieving only ~50-57% on this task.
- **Why unresolved:** The paper provides no architectural or training interventions to address this; it only documents the failure mode.
- **What evidence would resolve it:** Ablation studies on refusal-aware fine-tuning, or architectural modifications (e.g., uncertainty estimation modules) showing improved AR scores.

### Open Question 2
- **Question:** What mechanisms can mitigate the "cascade effect" where early dialogue errors accumulate and cause performance collapse?
- **Basis in paper:** [explicit] The paper observes a "significant gap between golden context and self-predicted settings" revealing a cascade effect where early errors lead to dialogue collapse, yet offers no solution.
- **Why unresolved:** The paper quantifies the problem but does not investigate recovery strategies or error-correction mechanisms.
- **What evidence would resolve it:** Experiments comparing models with/without error recovery modules, or methods that close the golden vs. self-predicted context gap.

### Open Question 3
- **Question:** How can MLLMs maintain long-range temporal dependencies and spatial grounding as scene complexity increases beyond 20 scenes?
- **Basis in paper:** [explicit] The paper reports a consistent 13% performance decline in Gemini-2.5-Pro as scene counts increase from 1-5 to over 20, stating models "struggle to maintain long-range temporal dependencies" in fragmented contexts.
- **Why unresolved:** The paper documents the degradation pattern but does not propose memory architectures or temporal reasoning modules to address it.
- **What evidence would resolve it:** Models with explicit temporal memory mechanisms evaluated on high-scene-count dialogues showing reduced degradation.

### Open Question 4
- **Question:** What training or prompting strategies can address visual-temporal hallucinations, inter-turn interference, and conversational passivity in multi-turn video dialogues?
- **Basis in paper:** [explicit] The paper identifies these three as "primary failure modes" in Section 4.4 Case Study, but only provides illustrative examples without remediation approaches.
- **Why unresolved:** The error taxonomy is descriptive, not prescriptive; no interventions are tested.
- **What evidence would resolve it:** Targeted training data or prompting strategies that specifically reduce each failure mode, measured through ablation studies.

## Limitations

- **Evaluator bias:** The benchmark relies on automated evaluators (Gemini-2.5-Flash) that may introduce systematic bias toward their own model family
- **Scene boundary validity:** Scene complexity proxies based on uniform frame sampling and YOLO detection may not accurately capture semantic scene changes
- **Limited external validation:** Cascade effect analysis and checklist methodology are primarily paper-internal with limited corpus validation

## Confidence

- **Checklist-based evaluation reliability:** High (supported by agreement metrics in Table 3)
- **Scene complexity degradation:** Medium-High (supported by Figure 4b and FMR=0.52 from related work)
- **Cascade effect in multi-turn dialogues:** Medium (paper-internal analysis with limited external validation)
- **Overall model performance limits:** High (reproducible across 20 diverse models)

## Next Checks

1. **Evaluator Bias Validation:** Run the same 1,000 dialogues through multiple automated evaluators (e.g., GPT-4o, Claude-3.5-Sonnet) and compare checklist scores to assess whether Gemini-2.5-Flash introduces systematic bias in favor of its own model family.

2. **Scene Boundary Validation:** Manually annotate 50 randomly selected dialogues to verify whether YOLO-based scene splitting accurately captures semantic scene changes, or whether models are being penalized for data quality issues rather than reasoning limitations.

3. **Cascade Effect Replication:** Select 3 top-performing models and evaluate the same dialogues with three context modes (golden, self-predicted, no context) across 5 consecutive turns. Measure per-turn accuracy decay rates to quantify the cascade effect magnitude and identify the exact turn where performance typically collapses.