---
ver: rpa2
title: 'Optimal Condition for Initialization Variance in Deep Neural Networks: An
  SGD Dynamics Perspective'
arxiv_id: '2508.12834'
source_url: https://arxiv.org/abs/2508.12834
tags:
- loss
- initialization
- function
- distribution
- variance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study establishes a theoretical optimal condition for initialization
  variance in deep neural networks using stochastic gradient descent (SGD) dynamics.
  By approximating SGD through a continuous-time framework and deriving the corresponding
  Fokker-Planck equation, the authors connect the quasi-stationary distribution of
  weights to the initial Gaussian distribution via Kullback-Leibler divergence.
---

# Optimal Condition for Initialization Variance in Deep Neural Networks: An SGD Dynamics Perspective

## Quick Facts
- arXiv ID: 2508.12834
- Source URL: https://arxiv.org/abs/2508.12834
- Reference count: 25
- This study establishes a theoretical optimal condition for initialization variance in deep neural networks using SGD dynamics, showing that optimal initialization variance is equal to the steady-state weight variance.

## Executive Summary
This paper derives a theoretically optimal condition for initialization variance in deep neural networks through a novel analysis of SGD dynamics. By approximating SGD with a continuous-time framework and deriving the corresponding Fokker-Planck equation, the authors connect the quasi-stationary distribution of weights to initialization variance via Kullback-Leibler divergence. The key finding is that the expected loss function is tightly linked to initialization variance, with different scaling behaviors for small versus large variances. Through minimization of the upper bound on expected loss, the authors establish that optimal initialization variance equals the steady-state weight variance (σ²₀ = V(W)). Numerical experiments on MNIST and Fashion-MNIST datasets validate this theoretical result, demonstrating superior performance compared to conventional He-normal initialization.

## Method Summary
The authors approximate SGD dynamics using a continuous-time framework, deriving a Fokker-Planck equation to characterize the evolution of the weight distribution. They establish a connection between the quasi-stationary distribution of weights and the initial Gaussian distribution through Kullback-Leibler divergence. By analyzing how expected loss scales with initialization variance - proportionally for small variances and logarithmically for large variances - they derive an optimal initialization condition. The theoretical framework assumes specific noise characteristics in SGD and relies on several simplifying assumptions about the optimization landscape. Numerical validation is performed using fully connected networks on MNIST and Fashion-MNIST datasets.

## Key Results
- Derives optimal initialization variance condition: σ²₀ = V(W), where V(W) is the steady-state variance of weights
- Theoretical analysis shows expected loss scales proportionally to steady-state variance for small initializations, and logarithmically for large initializations
- Numerical experiments demonstrate that the optimal initialization achieves lower training loss and higher test accuracy compared to He-normal initialization on MNIST and Fashion-MNIST datasets
- The condition provides a mathematically grounded criterion for initialization variance selection rather than relying on heuristics

## Why This Works (Mechanism)
The mechanism works by establishing a rigorous mathematical connection between initialization variance and the long-term behavior of SGD dynamics. Through the Fokker-Planck equation framework, the authors show how the initial weight distribution evolves toward a quasi-stationary distribution, with the divergence between these distributions directly affecting the expected loss. The scaling behavior of loss with respect to initialization variance reveals that there exists an optimal point where the trade-off between exploration (large variance) and convergence speed (small variance) is balanced.

## Foundational Learning
- **Fokker-Planck equation**: A partial differential equation describing the time evolution of probability distributions in stochastic processes. Needed to model the continuous-time approximation of SGD dynamics; quick check: verify understanding of how it relates to Brownian motion and drift terms.
- **Kullback-Leibler divergence**: A measure of difference between probability distributions. Required to quantify the distance between initial and quasi-stationary weight distributions; quick check: understand its non-symmetric nature and connection to information theory.
- **Continuous-time approximation of SGD**: Replacing discrete SGD updates with a continuous stochastic differential equation. Essential for applying PDE analysis tools; quick check: recognize limitations when applied to finite-width networks.
- **Quasi-stationary distribution**: The long-term distribution that SGD weights approach during training. Central to understanding how initialization affects final performance; quick check: distinguish from stationary distribution in ergodic processes.
- **Variance scaling in neural networks**: How initialization variance affects network behavior during training. Critical for understanding the trade-offs between different initialization scales; quick check: relate to common initialization schemes like He and Xavier initialization.

## Architecture Onboarding

**Component Map**: Fokker-Planck equation → KL divergence calculation → Expected loss analysis → Optimal variance derivation → Numerical validation

**Critical Path**: The theoretical derivation follows the sequence: continuous-time approximation → Fokker-Planck formulation → KL divergence minimization → expected loss upper bound → optimal variance condition. Numerical validation then tests this condition on specific architectures and datasets.

**Design Tradeoffs**: The continuous-time approximation provides mathematical tractability but may miss important discrete effects in finite-width networks. The KL divergence approach captures distributional differences but assumes specific noise characteristics in SGD. The focus on fully connected networks limits generalizability to modern architectures.

**Failure Signatures**: If the optimal variance condition fails, potential indicators include: (1) divergence between theoretical predictions and empirical results on deeper networks, (2) sensitivity to learning rate or batch size changes, (3) poor performance on convolutional or residual architectures, (4) breakdown of the continuous-time approximation in practice.

**First Experiments**:
1. Test the optimal initialization condition on a 20-layer fully connected network to assess scalability beyond the 3-4 layer networks used in the paper
2. Evaluate performance on CIFAR-10 dataset with the same network architecture to verify generalizability to more complex data
3. Conduct experiments varying learning rates (0.001 to 0.1) while keeping initialization constant to test robustness to optimization hyperparameters

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain implicit: how the optimal condition generalizes to convolutional networks and modern deep architectures, whether the continuous-time approximation remains valid for large batch training, and how the condition interacts with adaptive optimization methods like Adam.

## Limitations
- The continuous-time approximation of SGD may not fully capture discrete optimization dynamics, particularly in finite-width networks where stochastic fluctuations are significant
- Numerical validation is limited to fully connected networks on MNIST and Fashion-MNIST, lacking experiments on deeper architectures, convolutional networks, or more complex datasets
- The theoretical framework assumes specific noise characteristics in SGD that may not hold across different learning rates, batch sizes, or optimization algorithms

## Confidence

| Claim | Confidence |
|-------|------------|
| Theoretical derivation of optimal initialization variance | Medium |
| Numerical validation of the theoretical condition | Medium |
| Practical applicability as a replacement for He-normal initialization | Low |

## Next Checks
1. Test the optimal initialization condition on deeper networks (20+ layers) and convolutional architectures to assess scalability and architectural generalizability
2. Evaluate performance across diverse datasets including CIFAR-10/100 and ImageNet to verify the condition's robustness to data complexity
3. Conduct ablation studies varying learning rates, batch sizes, and optimizers to understand the condition's sensitivity to optimization hyperparameters