---
ver: rpa2
title: Generating Medically-Informed Explanations for Depression Detection using LLMs
arxiv_id: '2503.14671'
source_url: https://arxiv.org/abs/2503.14671
tags:
- depression
- llm-mtd
- language
- detection
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of detecting depression from
  social media data while providing medically-informed explanations for predictions.
  The authors propose LLM-MTD, a multi-task learning approach that leverages a pre-trained
  large language model to simultaneously classify social media posts for depression
  and generate textual explanations based on medical diagnostic criteria.
---

# Generating Medically-Informed Explanations for Depression Detection using LLMs

## Quick Facts
- **arXiv ID**: 2503.14671
- **Source URL**: https://arxiv.org/abs/2503.14671
- **Reference count**: 26
- **Primary result**: LLM-MTD achieves state-of-the-art AUPRC ~0.88 on Reddit depression dataset while generating medically-informed explanations

## Executive Summary
This paper addresses the challenge of detecting depression from social media posts while providing medically-informed explanations for predictions. The authors propose LLM-MTD, a multi-task learning approach that leverages a pre-trained large language model to simultaneously classify posts for depression and generate textual explanations based on medical diagnostic criteria. The model is trained using a combined loss function that optimizes both classification accuracy and explanation quality. Experiments on the Reddit Self-Reported Depression Dataset demonstrate that LLM-MTD significantly outperforms competitive baselines in both detection performance and explanation quality, with human evaluation confirming the medical accuracy and relevance of generated explanations.

## Method Summary
LLM-MTD uses a pre-trained transformer-based LLM as a shared encoder, aggregating token embeddings via mean pooling to obtain a fixed-length representation. This representation feeds both a classification head (producing depression probability via sigmoid) and a generation head (producing medical explanations conditioned on input and predicted class). The model is trained with a multi-task loss combining binary cross-entropy for classification and negative log-likelihood for explanation generation, weighted by hyperparameter 位. The approach requires ground truth explanations for positive training examples and uses Adam optimization with 位 tuned on validation data.

## Key Results
- LLM-MTD achieves AUPRC of approximately 0.88 on RSDD test set
- Significant improvement over classification-only baselines (AUPRC ~0.865)
- Human evaluation shows high scores for explanation quality: Relevance (4.6/5), Completeness (4.4/5), Medical Accuracy (4.6/5)
- Model maintains strong performance on short posts (<50 words) with AUPRC of 0.845

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Joint optimization for explanation generation acts as a semantic regularizer, improving depression classification performance compared to classification-only training.
- **Mechanism**: The auxiliary task of generating text forces the shared encoder to learn richer, medically-grounded representations of the input tokens. By minimizing the generation loss ($L_{gen}$), the model must capture deep semantic relationships rather than just surface-level correlations, which in turn reduces the classification loss ($L_{cls}$) on complex or ambiguous posts.
- **Core assumption**: The ground truth explanations used for training are high-quality and accurately reflect the diagnostic criteria for depression.
- **Evidence anchors**: [abstract] "combined loss function that optimizes both classification accuracy and explanation quality... significant improvements in AUPRC."; [section 4.3] "LLM-MTD achieves better performance than the classification-only variant... suggesting that the inclusion of the explanation generation task... provides a beneficial effect."
- **Break condition**: If the training explanations are noisy, sparse, or inconsistent, the generation loss could introduce conflicting gradients, degrading classification accuracy (negative transfer).

### Mechanism 2
- **Claim**: Conditioning text generation on the classification context produces medically relevant explanations by leveraging the LLM's pre-trained linguistic priors.
- **Mechanism**: The model conditions the generation probability on the input post and the predicted label ($p(e|x, \hat{y}=1)$). By optimizing the negative log-likelihood (Eq. 6) of expert explanations, the model learns to retrieve and articulate specific medical knowledge (e.g., "feelings of hopelessness") associated with the input features detected by the classifier.
- **Core assumption**: The pre-trained LLM has sufficient inherent medical knowledge or linguistic capacity to be fine-tuned into this specific explanatory behavior without requiring extensive external knowledge retrieval (RAG).
- **Evidence anchors**: [section 3.1] Describes generating explanations by conditioning on the input and predicted class via a prompt ("Explain why this post might indicate depression..."); [section 4.4] Human evaluation confirms high scores (4.6/5.0) for "Medical Accuracy."
- **Break condition**: If the model hallucinates medical terms not present in the training distribution or fails to align the explanation with the specific features of the input post (e.g., generic responses).

### Mechanism 3
- **Claim**: Mean pooling of hidden states preserves sufficient semantic density for accurate binary classification while remaining computationally efficient.
- **Mechanism**: The architecture aggregates variable-length token embeddings ($H$) into a single fixed vector ($h_{agg}$) using pooling (Eq. 1). This condenses the temporal and semantic information of the post into a representation suitable for the linear classification head, maintaining robustness against input length variations.
- **Core assumption**: Important depressive indicators are distributed across the post rather than localized to a single specific token (which might be better captured by attention-based pooling).
- **Evidence anchors**: [section 3.1] Eq. 1 defines the pooling strategy to obtain $h_{agg}$; [section 4.5] Table 3 shows the model maintains high AUPRC (0.845) even for "Short" posts (<50 words), suggesting the pooling effectively captures signals even with limited context.
- **Break condition**: Performance degrades significantly on very long posts where critical signals are diluted by the mean averaging of irrelevant tokens.

## Foundational Learning

- **Concept: Multi-Task Learning (MTL) & Loss Weighting ($\lambda$)**
  - **Why needed here**: The model simultaneously solves two distinct problems (classification vs. generation) with different loss scales. You must understand how to balance $\lambda$ to prevent the generation loss from overpowering the classification gradients.
  - **Quick check question**: If the model generates perfect text but fails to predict depression, which direction should you adjust $\lambda$?

- **Concept: Binary Cross-Entropy vs. Negative Log-Likelihood (NLL)**
  - **Why needed here**: The architecture uses BCE for the classification head and NLL for the generation head. Understanding the gradient properties of both is required to debug convergence issues.
  - **Quick check question**: Why is NLL suitable for the sequence generation task (Eq. 6) but not for the binary classification task (Eq. 5)?

- **Concept: Transformer Pooling Strategies**
  - **Why needed here**: The classification performance relies entirely on $h_{agg}$. You need to know why Mean Pooling might be chosen over [CLS] token or Attention Pooling for social media data.
  - **Quick check question**: How might Mean Pooling react to a long post where only one sentence indicates depression?

## Architecture Onboarding

- **Component map**: Input Tokenized Social Media Post ($x$) -> Pre-trained Transformer LLM ($f_{LLM}$) -> Hidden States ($H$) -> Mean Pooling ($h_{agg}$) -> Linear Classification Head -> Sigmoid -> Probability; LLM Decoder Head (conditioned on $H$ + Prompt) -> Text Output ($e$)

- **Critical path**: The data pipeline is the primary bottleneck. Ensuring the dataset aligns posts with *expert-annotated explanations* ($e^*$) is critical. If these do not exist, you cannot compute $L_{gen}$, and the multi-task mechanism fails.

- **Design tradeoffs**:
  - **Accuracy vs. Interpretability**: Adjusting $\lambda$. High $\lambda$ prioritizes detection metrics; lower $\lambda$ prioritizes explanation quality.
  - **Pooling Method**: Mean pooling is robust but may dilute single-sentence signals in long posts (Table 3 suggests length correlation).
  - **Model Size**: Larger LLMs improve generation quality but drastically increase inference latency for real-time monitoring.

- **Failure signatures**:
  - **Generic Explanations**: The model outputs the same medical definition for every post (overfitting to prior medical text, ignoring input context).
  - **Negative Transfer**: AUPRC drops below the "Classification-Only" baseline (0.865), indicating the generation task is interfering with feature learning.
  - **Length Bias**: Significant performance drop on "Short" posts (Table 3) compared to baselines.

- **First 3 experiments**:
  1. **Lambda Sweep**: Run grid search on $\lambda$ (e.g., 0.1 to 0.9) to find the equilibrium where AUPRC is maximized without degrading explanation fluency.
  2. **Pooling Ablation**: Compare Mean Pooling vs. [CLS] token vs. Max Pooling on the validation set to verify the paper's architectural choice.
  3. **Zero-Shot Explanation Test**: Before fine-tuning, test the base LLM on the prompt "Explain why this post might indicate depression..." to establish a baseline for explanation quality and verify the pre-trained medical knowledge capacity.

## Open Questions the Paper Calls Out
- **Question 1**: Does incorporating temporal information from a user's posting history improve the detection accuracy and explanation quality compared to the current single-post analysis?
  - **Basis**: The conclusion explicitly states a future direction is to "incorporate temporal information from user posting history to better capture the longitudinal aspects of depression."
  - **Why unresolved**: The current LLM-MTD implementation processes individual posts in isolation, potentially missing the progression or recurrence of symptoms over time which is critical for clinical depression diagnosis.
  - **What evidence would resolve it**: A modified LLM-MTD architecture that accepts sequential inputs from a user's timeline, evaluated on a dataset with timestamped history, showing improved AUPRC over the single-post baseline.

- **Question 2**: To what extent does the model generalize to social media platforms with different linguistic norms and character limits (e.g., Twitter/X or Facebook) without retraining?
  - **Basis**: The authors identify "evaluating the model's generalizability on more diverse social media platforms and datasets" as a necessary avenue for future research.
  - **Why unresolved**: The model was trained and tested exclusively on the Reddit Self-Reported Depression Dataset (RSDD), which features long-form text and specific community jargon that may not transfer to shorter or more casual formats found on other platforms.
  - **What evidence would resolve it**: Zero-shot or few-shot evaluation results of the Reddit-trained LLM-MTD model on external datasets like Twitter depression corpora, reporting retention of AUPRC and explanation relevance scores.

- **Question 3**: What is the specific impact of the generated medically-informed explanations on the diagnostic performance and confidence of clinical practitioners?
  - **Basis**: The paper concludes that "investigating the potential clinical utility of the generated explanations... could further bridge the gap between research and real-world applications."
  - **Why unresolved**: While human evaluation confirmed the *technical* quality of explanations (relevance, completeness) via psychology students, the paper did not measure if these explanations actually assist medical professionals in making better or faster diagnostic decisions.
  - **What evidence would resolve it**: A user study involving licensed clinicians performing depression screening tasks, comparing their diagnostic accuracy and time-to-decision when assisted by LLM-MTD explanations versus a black-box baseline.

## Limitations
- The paper does not specify how expert-annotated explanations were created or validated, which is critical since the quality of these labels directly impacts both the generation loss and the medical accuracy of outputs.
- Specific pre-trained LLM architecture, exact hyperparameter values (位, learning rate, batch size), and implementation details are not disclosed, creating potential reproducibility gaps.
- The model was trained and tested exclusively on Reddit data, raising questions about generalizability to other social media platforms with different linguistic norms.

## Confidence
**High Confidence**: The classification performance improvements (AUPRC ~0.88) are well-supported by experimental results and human evaluation metrics. The multi-task learning mechanism's general benefit for semantic regularization is theoretically sound.

**Medium Confidence**: The claim that joint optimization specifically improves depression detection performance is supported but requires careful interpretation - the improvement is relative to a baseline that may have different training configurations.

**Low Confidence**: The source and quality of medical knowledge in generated explanations. While human evaluation shows high medical accuracy scores (4.6/5.0), the paper does not clarify whether this knowledge comes from the pre-trained LLM's general corpus or from the training explanations themselves.

## Next Checks
1. **Ground Truth Explanation Audit**: Verify the quality, coverage, and annotation process of the expert explanations used for training. Test model sensitivity to explanation noise by intentionally degrading explanation quality in training data.

2. **Cross-Validation of Hyperparameters**: Conduct a systematic ablation study varying 位 across the full range (0.1-0.9) and compare against the reported optimal value. Include additional baselines (e.g., attention pooling, different LLMs) to validate architectural choices.

3. **External Dataset Generalization**: Evaluate the trained model on an independent depression detection dataset (e.g., CLPsych 2015) to assess whether the multi-task learning benefits transfer beyond RSDD, particularly for explanation quality and medical accuracy.