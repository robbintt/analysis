---
ver: rpa2
title: 'PustakAI: Curriculum-Aligned and Interactive Textbooks Using Large Language
  Models'
arxiv_id: '2511.10002'
source_url: https://arxiv.org/abs/2511.10002
tags:
- dataset
- science
- questions
- llms
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PustakAI, a framework for building curriculum-aligned
  question-answering systems using Large Language Models. The authors introduce NCERT-QA,
  a novel dataset derived from the Indian NCERT curriculum for grades 6-8, covering
  English and Science subjects with 739 carefully curated QA pairs categorized as
  Factoid, Inferential, or Others.
---

# PustakAI: Curriculum-Aligned and Interactive Textbooks Using Large Language Models

## Quick Facts
- arXiv ID: 2511.10002
- Source URL: https://arxiv.org/abs/2511.10002
- Reference count: 19
- Novel NCERT-QA dataset with 739 curriculum-aligned QA pairs for grades 6-8 English and Science

## Executive Summary
PustakAI introduces a framework for building curriculum-aligned question-answering systems using Large Language Models, specifically targeting educational contexts. The authors created NCERT-QA, a novel dataset derived from the Indian NCERT curriculum for grades 6-8, covering English and Science subjects with 739 carefully curated QA pairs. Through systematic evaluation of various prompting strategies (meta-prompt, few-shot, and CoT) and models ranging from Gemma3:1b to Deepseek-r1-70B, the study demonstrates that curriculum-specific questions require contextual data for accurate responses. Llama4-Scout-17B emerged as the optimal choice, balancing high accuracy (F1 0.46-0.47) with fast inference times (~2 seconds), making PustakAI a practical foundation for scalable educational AI systems aligned with academic curricula.

## Method Summary
The PustakAI framework evaluates LLM performance on curriculum-aligned educational QA tasks through systematic testing of prompting strategies and model architectures. The authors created the NCERT-QA dataset with 739 QA pairs categorized as Factoid, Inferential, or Others, derived from Indian NCERT curriculum materials for grades 6-8. They tested multiple prompting approaches including meta-prompt, few-shot, and Chain-of-Thought (CoT) strategies across various models from Gemma3:1b to Deepseek-r1-70B. The evaluation framework measured accuracy, precision, recall, and F1 scores while also considering inference times. The study found that larger models with meta-one-shot prompting achieved the best balance of accuracy and speed, with Llama4-Scout-17B performing optimally.

## Key Results
- Llama4-Scout-17B achieved optimal balance with F1 scores of 0.46-0.47 and ~2 second inference times
- Meta-one-shot prompting strategy outperformed few-shot and CoT approaches on curriculum-specific questions
- Curriculum-aligned datasets significantly improve LLM performance on educational QA tasks compared to generic approaches

## Why This Works (Mechanism)
Curriculum-aligned prompting works by providing LLMs with contextual data specific to educational standards and learning objectives. The NCERT-QA dataset captures the structured knowledge progression found in formal curricula, allowing models to better understand the pedagogical context and expected answer formats. Meta-one-shot prompting provides just enough contextual examples to guide the model toward curriculum-appropriate responses without overwhelming it with information. This targeted approach reduces hallucination and improves factual accuracy by constraining the model's response generation to align with established educational frameworks and grade-appropriate complexity levels.

## Foundational Learning

**Curriculum-aligned prompting**: Understanding how educational standards and learning objectives shape question-answering requirements. Needed to create contextually appropriate responses that match pedagogical expectations. Quick check: Verify prompt structure matches curriculum learning objectives.

**Chain-of-Thought reasoning**: Multi-step logical reasoning processes that break down complex questions into manageable components. Needed for inferential questions requiring explanation beyond simple facts. Quick check: Test model's ability to explain reasoning steps for multi-concept problems.

**Educational dataset curation**: Methods for extracting, categorizing, and structuring educational content into machine-readable formats. Needed to create high-quality training and evaluation datasets that reflect real curriculum requirements. Quick check: Validate dataset coverage across Bloom's taxonomy levels.

## Architecture Onboarding

**Component map**: Data Collection -> Dataset Curation -> Prompt Engineering -> Model Selection -> Evaluation -> System Integration

**Critical path**: The most time-consuming and impactful component is dataset curation, as the quality and alignment of the NCERT-QA dataset directly determines model performance. Prompt engineering follows as the second critical component, as appropriate prompting strategies can significantly improve accuracy even with smaller models.

**Design tradeoffs**: The study balances model size against inference speed, finding that medium-sized models (17B parameters) with optimized prompting achieve better practical performance than larger models. The tradeoff between dataset size and curation quality favors carefully selected, curriculum-aligned questions over larger but less structured datasets.

**Failure signatures**: Common failure modes include hallucinated answers when curriculum context is missing, overly generic responses lacking subject-specific terminology, and incorrect handling of inferential questions requiring multi-step reasoning. Models may also struggle with grade-appropriate language complexity and fail to maintain pedagogical tone.

**3 first experiments**:
1. Test prompt variations (meta-prompt, few-shot, CoT) on a small subset of NCERT-QA questions to identify optimal strategy
2. Evaluate model performance across different grade levels to assess scalability
3. Compare curriculum-aligned prompting against generic educational QA approaches to quantify improvement

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Limited generalizability beyond Indian NCERT curriculum and grades 6-8
- Dataset size of 739 QA pairs may not capture full curriculum complexity
- Absence of student user studies to validate educational effectiveness

## Confidence

**High confidence**:
- Systematic evaluation across multiple models and prompting strategies
- Clear performance improvement with curriculum-aligned approaches
- Optimal model selection (Llama4-Scout-17B) well-supported by controlled experiments

**Medium confidence**:
- Generalizability beyond tested curriculum and grade levels
- Automated metrics may not fully capture pedagogical quality
- Limited dataset scope (English and Science only)

## Next Checks

1. Conduct controlled classroom studies measuring student learning outcomes when using PustakAI versus traditional textbooks
2. Expand dataset to include additional subjects, grade levels, and international curricula to test generalizability
3. Implement longitudinal testing to evaluate model performance stability and adaptation to curriculum changes over time