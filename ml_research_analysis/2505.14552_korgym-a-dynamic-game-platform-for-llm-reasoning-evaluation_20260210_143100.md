---
ver: rpa2
title: 'KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation'
arxiv_id: '2505.14552'
source_url: https://arxiv.org/abs/2505.14552
tags:
- reasoning
- game
- arxiv
- wang
- games
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'KORGym is a dynamic game-based evaluation platform designed to
  comprehensively assess large language models'' reasoning capabilities across six
  dimensions: mathematical/logical, control interaction, puzzle, spatial/geometric,
  strategic, and multimodal reasoning. The platform features over fifty games with
  multi-turn, interactive, and reinforcement learning scenarios.'
---

# KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation

## Quick Facts
- arXiv ID: 2505.14552
- Source URL: https://arxiv.org/abs/2505.14552
- Reference count: 40
- Primary result: Comprehensive game-based evaluation of 19 LLMs and 8 VLMs across six reasoning dimensions reveals consistent model-family patterns, closed-source superiority, and RL-enhanced reasoning capabilities

## Executive Summary
KORGym introduces a dynamic game-based evaluation platform designed to comprehensively assess large language models' reasoning capabilities across six dimensions: mathematical/logical, control interaction, puzzle, spatial/geometric, strategic, and multimodal reasoning. The platform features over fifty interactive games with multi-turn scenarios and reinforcement learning compatibility. Experiments on 19 LLMs and 8 VLMs demonstrate consistent reasoning patterns within model families, superior performance of closed-source models, and notable impacts of modality and reasoning strategies on performance. The framework provides a robust methodology for advancing LLM reasoning research through interactive, knowledge-orthogonal evaluation environments.

## Method Summary
KORGym evaluates LLM reasoning through 51+ interactive games organized into six categories, using zero-shot prompting with default sampling parameters. The evaluation employs a four-module architecture (Inference, Game Interaction, Evaluation, Communication) with three core APIs: generate, print_board, and verify. Single-epoch games use 50 seeds while multi-epoch games employ 20 environments with up to 100 rounds each. Scores undergo log transformation when max score >1, min-max normalization to [0,1], then aggregation per dimension. Closed-source models are evaluated via API while open-source models run on 8Ã— NVIDIA A100-80G GPUs. The platform emphasizes knowledge orthogonality to minimize memorization confounds and enable long-horizon planning through persistent state management across turns.

## Key Results
- Consistent reasoning patterns observed within model families across all six reasoning dimensions
- Closed-source models (GPT series) demonstrate superior performance compared to open-source alternatives
- Reinforcement learning techniques yield marked improvements and more balanced performance across reasoning dimensions
- Multimodal VLMs show distinct performance characteristics compared to text-only LLMs
- Code paradigm reasoning strategies generally outperform natural language approaches

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Orthogonality Reduces Memorization Confounds
The platform leverages knowledge orthogonality to isolate intrinsic reasoning by minimizing pre-learned solutions. By decoupling task rules from pretraining knowledge, evaluation targets rule-following rather than memorization. Games are designed to be rarely encountered in pretraining corpora, forcing models to derive solutions from novel rules rather than recalled facts.

### Mechanism 2: Dynamic State Management Enables Long-Horizon Planning
Multi-turn, interactive environments reveal planning and error-recovery capabilities hidden by single-turn benchmarks. The framework maintains persistent environment state across turns, with models receiving feedback via the verify API and integrating this into subsequent inference steps. This tests control interaction reasoning and strategic reasoning where actions have delayed consequences.

### Mechanism 3: RL-Compatible Scaffolding Facilitates Skill Acquisition
Standardizing games under the Gymnasium API allows reinforcement learning algorithms to refine reasoning strategies. The system exposes generate, verify, and print_board APIs compatible with RL workflows, enabling models to receive dense reward signals and potentially shift from explicit reasoning paradigms to more efficient internalized heuristics.

## Foundational Learning

**Concept: Gymnasium API (OpenAI Gym)**
- Why needed: KORGym is built on the Gymnasium standard; understanding observation, action, reward, done cycle is required to interpret data flow
- Quick check: Can you distinguish between observation (current board state) and reward (score change) in a game like Snake?

**Concept: Knowledge Orthogonality (KOR)**
- Why needed: This is the theoretical basis for the benchmark; understanding why decoupling rules from knowledge is necessary to measure intrinsic reasoning vs. memorization
- Quick check: If a model solves a Sudoku puzzle, is it using KOR-reasoning or prior knowledge? (Hint: It depends on whether the specific constraint logic was seen during training or must be derived from the prompt)

**Concept: Reasoning Paradigms (Code vs. NL)**
- Why needed: The paper analyzes model performance by whether they use Code, Math, or Natural Language paradigms
- Quick check: If a model outputs Python code to solve a maze, which paradigm is it using, and how does the KORGym executor handle that output?

## Architecture Onboarding

**Component map:** Parameter Initialization -> Game Interaction.generate() -> Loop: print_board() (Obs) -> Inference (Action) -> verify() (State Update) -> Evaluation (Final Score)

**Critical path:** 1) Parameter Initialization (Seed/Model) 2) Game Interaction.generate() (Setup) 3) Loop: print_board() (Obs) -> Inference (Action) -> verify() (State Update) 4) Evaluation (Final Score)

**Design tradeoffs:** Scoring: Binary vs. Proportional vs. Cumulative scoring (Binary is strict; Cumulative allows partial credit but complicates cross-game comparison). Modality: Text vs. Visual (Text is robust for logic; Visual tests spatial reasoning but introduces VLM grounding errors)

**Failure signatures:** Context Overflow: Long games may exceed token limits. Invalid Actions: LLMs outputting natural language when the game expects strict coordinate format. State desynchronization: verify returns incorrect updated state

**First 3 experiments:** 1) Sanity Check: Run non-reasoning model on simple single-epoch game to verify verify API rejects hallucinated board states. 2) State Tracking: Run thinking model on multi-turn game to observe if it corrects itself after hitting a wall, validating feedback loop. 3) Ablation: Disable Code Paradigm via prompt constraints on math-heavy game to measure performance degradation

## Open Questions the Paper Calls Out
None

## Limitations

**Data Contamination Risk**: The risk of training data overlap remains uncertain despite claims of knowledge orthogonality, as many games have existed in digital form for decades and may be represented in pretraining corpora.

**Scalability Constraints**: The evaluation framework requires significant computational resources, with single-epoch games needing 50 seeds and multi-epoch games requiring 20 environments with up to 100 rounds, limiting practical adoption for resource-constrained research groups.

**Modality Complexity**: Visual games introduce VLM-specific challenges including object detection accuracy, spatial reasoning reliability, and grounding stability that aren't thoroughly characterized in the evaluation.

## Confidence

**High Confidence**: Claims about consistent performance patterns within model families are well-supported by experimental data and align with established literature on model capabilities.

**Medium Confidence**: The superiority of closed-source models and the impact of reasoning strategies have strong empirical support but may be influenced by evaluation artifacts like model-specific prompting effects.

**Low Confidence**: The claim that RL techniques "enhance reasoning capabilities" requires careful interpretation as the specific RL algorithms and hyperparameters aren't fully specified, making it difficult to assess whether observed gains represent genuine reasoning improvement.

## Next Checks

**Check 1: Training Data Overlap Analysis** - Systematically search pretraining corpora for game-specific patterns, rules, and solutions using embedding similarity and exact string matching to validate or challenge the knowledge orthogonality assumption.

**Check 2: Sample Size Sensitivity Analysis** - Evaluate how dimension scores vary with different seed counts (e.g., 10, 25, 50 seeds for single-epoch games) to determine minimum sample size needed for stable estimates and assess robustness to sampling variation.

**Check 3: Ablation of RL Training Details** - Replicate RL fine-tuning experiments with transparent hyperparameters and alternative RL algorithms (e.g., PPO, A2C) to compare performance gains against baseline prompting strategies and isolate whether improvements stem from reasoning enhancement versus task-specific optimization.