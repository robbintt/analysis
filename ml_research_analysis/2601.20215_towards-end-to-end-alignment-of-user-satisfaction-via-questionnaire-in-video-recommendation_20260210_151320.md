---
ver: rpa2
title: Towards End-to-End Alignment of User Satisfaction via Questionnaire in Video
  Recommendation
arxiv_id: '2601.20215'
source_url: https://arxiv.org/abs/2601.20215
tags:
- satisfaction
- user
- signals
- alignment
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aligning short-video recommendation
  systems with true user satisfaction using sparse questionnaire feedback. The proposed
  EASQ framework introduces an independent parameter pathway combining a multi-task
  architecture and a lightweight LoRA module to prevent sparse satisfaction signals
  from being overwhelmed by abundant behavioral data.
---

# Towards End-to-End Alignment of User Satisfaction via Questionnaire in Video Recommendation

## Quick Facts
- **arXiv ID:** 2601.20215
- **Source URL:** https://arxiv.org/abs/2601.20215
- **Reference count:** 40
- **One-line primary result:** EASQ framework achieves up to 3.4% average performance gains in short-video recommendation by aligning with sparse questionnaire satisfaction signals.

## Executive Summary
This paper addresses the challenge of aligning short-video recommendation systems with true user satisfaction using sparse questionnaire feedback. The proposed EASQ framework introduces an independent parameter pathway combining a multi-task architecture and a lightweight LoRA module to prevent sparse satisfaction signals from being overwhelmed by abundant behavioral data. An online DPO-based optimization objective enables real-time alignment of the main model with satisfaction signals. Extensive offline experiments and large-scale online A/B tests demonstrate consistent improvements across multiple scenarios, with EASQ achieving up to 3.4% average performance gains and significant business benefits in production deployment.

## Method Summary
The EASQ framework uses a multi-task architecture with LoRA to create an independent parameter pathway for sparse satisfaction signals. A questionnaire expert network serves as an online reference model for DPO-based optimization, enabling real-time alignment without maintaining a frozen pretrained copy. The architecture combines a lightweight LoRA module at lower layers with decoupled multi-task expert networks at upper layers, with gradient flow controlled via stop_grad operations. The system is trained end-to-end with a combined loss function incorporating main task BPR loss, satisfaction task pairwise logistic loss, and DPO alignment loss.

## Key Results
- EASQ achieves up to 3.4% average performance gains across multiple recommendation metrics
- Online A/B tests show significant business benefits with improved LT7, AppStayTime, WatchTime, and engagement metrics
- Ablation studies confirm each component's contribution: LoRA (largest impact), DPO (alignment improvement), and MoE architecture (expert specialization)

## Why This Works (Mechanism)

### Mechanism 1: Independent Parameter Pathway for Sparse Signals
Constructing a dedicated parameter pathway prevents sparse satisfaction signals from being overwhelmed by dense behavioral data during joint training. The framework combines a lightweight LoRA module at lower layers with decoupled multi-task expert networks at upper layers. LoRA injects satisfaction information early while separate expert networks provide structural isolation. Gradient flow is controlled via stop_grad operations—main task receives h_main = h + stop_grad(h_LoRA) while satisfaction task receives h_satis = stop_grad(h) + h_LoRA.

### Mechanism 2: Online DPO with Dynamic Reference Model
Adapting DPO for online learning by using the satisfaction expert network as a reference model enables real-time alignment without maintaining a frozen pretrained copy. Traditional DPO requires a fixed reference model, which becomes stale in online settings. EASQ substitutes π_ref with the satisfaction alignment network output (stop_grad(ŝ)), which is continuously updated via its own loss L_satis. The DPO loss then aligns the main model π_θ toward questionnaire-derived preferences while the reference evolves with new feedback.

### Mechanism 3: Convergent Validity via Questionnaire Design
Well-designed questionnaires with appropriate trigger conditions yield satisfaction signals that correlate with posterior behavioral outcomes. The questionnaire triggers only after meaningful viewing (watch >7s or >50% progress), reducing noise from premature responses. Three-option design (Satisfied/Dissatisfied/Uncertain) captures uncertainty rather than forcing binary choices. Validation shows "Dissatisfied" responses correlate with -83% to -25% drops in posterior behaviors, while "Satisfied" responses show +46% to +80% improvements.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**: Why needed: EASQ uses LoRA to inject satisfaction information into the backbone with minimal parameters, enabling adaptation without destabilizing the main model. Quick check: Given a weight matrix W₀ ∈ R^(d×k), what is the parameter count for a LoRA update with rank r? Answer: r×(d+k) vs. d×k for full fine-tuning.

- **Direct Preference Optimization (DPO)**: Why needed: DPO eliminates the need for an explicit reward model by directly optimizing policy from preference pairs. EASQ adapts this for online learning with a dynamic reference. Quick check: In the DPO loss L = -log σ(β[log π_θ(x+)/π_ref(x+) - log π_θ(x-)/π_ref(x-)]), what does β control? Answer: Temperature/coefficient controlling divergence from reference—higher β = stronger alignment constraint.

- **Mixture of Experts (MoE) with ReLU Routing**: Why needed: EASQ uses separate expert pools for main and satisfaction tasks, with ReLU routing enabling fully differentiable expert selection. Quick check: Why does ReLU routing improve over Top-K Softmax? Answer: ReLU allows variable expert activation (including zero) without hard truncation, making routing fully differentiable.

## Architecture Onboarding

- **Component map:**
  Input Features (x) → Backbone (Transformer + LoRA) → ┌────────────┴────────────┐ → Main Task Path → h_main = h + stop_grad(h_LoRA) → MoE (K1 experts, ReLU gate) → ŷ (prediction) → L_main (BPR) → L_total = L_main + λ₁L_satis + λ₂L_DPO
  └─────────────────────────────────────────────┘ → Satisfaction Path → h_satis = stop_grad(h) + h_LoRA → MoE (K2 experts, ReLU gate) → ŝ (alignment output) → L_satis + L_DPO

- **Critical path:**
  1. Data collection: Questionnaire triggered after watch>7s or >50% progress → 0.5% exposure → ~2% response rate
  2. Training loop: For each batch, compute h → h_LoRA → both task paths → aggregate losses
  3. Inference: Only ŷ from main task is used; satisfaction path is training-only auxiliary

- **Design tradeoffs:**
  - LoRA rank (r): Higher rank = more capacity for satisfaction adaptation but more parameters. Start with r=8-16.
  - Expert counts (K1, K2): K1 optimal ~6-8, K2 optimal ~2-4. K2 smaller because sparse signals can't supervise many experts.
  - Loss weights (λ₁, λ₂): λ₂ should increase until plateau/degradation. λ₁ scenario-dependent.
  - Temperature β: Fixed at 0.1; controls alignment strength.

- **Failure signatures:**
  - w/o LoRA: Largest performance drop—indicates satisfaction signals are being overwhelmed without early injection
  - w/o DPO: Degraded alignment—main task only perceives satisfaction indirectly through LoRA, insufficient for full transfer
  - Over-parameterized K2: Performance plateaus/declines when K2 > 4 due to insufficient supervision per expert

- **First 3 experiments:**
  1. Validate questionnaire signal quality: Replicate Figure 3 correlation analysis on your data. If "Satisfied" responses don't show >30% behavioral improvement and "Dissatisfied" doesn't show >20% drop, questionnaire design needs revision.
  2. Ablation with single-component removal: Run w/o LoRA, w/o MoE, w/o DPO variants on offline data. If w/o LoRA drop < 1% NDCG, your satisfaction signals may not be sparse enough to require the pathway.
  3. Hyperparameter sweep for λ₂: Plot NDCG@5 vs. λ₂ values [1, 3, 6, 9, 12, 15]. Identify plateau point before degradation—this is your optimal alignment strength.

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- Limited details on critical hyperparameters including LoRA rank, expert counts, and loss weights
- Online DPO implementation's stability with a dynamically updated reference model not extensively validated
- Framework's behavior under different sparsity levels (questionnaire response rates) not explored systematically

## Confidence
- **High confidence**: Multi-task architecture with independent parameter pathways effectively prevents signal interference
- **Medium confidence**: Online DPO with dynamic reference model provides stable alignment
- **Medium confidence**: Questionnaire design yields convergent validity with behavioral outcomes

## Next Checks
1. **Reference model stability test**: Monitor KL divergence between main and satisfaction expert outputs during training; if divergence exceeds 0.5 nats, reduce λ₂ or implement reference model smoothing
2. **Questionnaire signal sensitivity**: Systematically vary questionnaire trigger conditions and measure impact on response rate, signal quality, and downstream performance
3. **Sparsity stress test**: Artificially subsample questionnaire data at different rates and measure degradation in L_satis convergence and alignment performance