---
ver: rpa2
title: Approximating Shapley Explanations in Reinforcement Learning
arxiv_id: '2511.06094'
source_url: https://arxiv.org/abs/2511.06094
tags:
- shapley
- characteristic
- training
- learning
- behaviour
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FastSVERL introduces a scalable framework for explaining reinforcement
  learning agents using Shapley values. It replaces expensive Monte Carlo sampling
  with parametric models that amortize computation across states and feature subsets.
---

# Approximating Shapley Explanations in Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.06094
- Source URL: https://arxiv.org/abs/2511.06094
- Reference count: 40
- Primary result: FastSVERL achieves low approximation error and halves training time in domains with up to billions of states by eliminating characteristic models in favor of single-sample approximations.

## Executive Summary
FastSVERL introduces a scalable framework for explaining reinforcement learning agents using Shapley values. It replaces expensive Monte Carlo sampling with parametric models that amortize computation across states and feature subsets. The method addresses practical constraints such as explaining policies from off-policy data and adapting to evolving agent behaviors. Empirical results show FastSVERL achieves low approximation error in domains with up to billions of states, converges efficiently, and reduces computational cost by eliminating characteristic models in favor of single-sample approximations.

## Method Summary
FastSVERL approximates Shapley values for reinforcement learning agents by training a neural network (Shapley model) to map states and actions directly to Shapley values. The framework supports three explanation targets: behavior (action influence), outcome (expected return), and prediction (value estimates). It uses importance sampling to train on off-policy data and offers two variants: a model-based approach with pre-trained characteristic models, and a single-sample approximation method that directly samples conditional states to reduce computational overhead and act as a stochastic regularizer.

## Key Results
- Achieves low approximation error in domains with up to billions of states
- Reduces computational cost by eliminating characteristic models in favor of single-sample approximations
- Halves total training time while maintaining accuracy
- Enables real-time interpretability in complex reinforcement learning settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Parametric models enable scalable Shapley value estimation by amortizing computational cost across states and feature subsets, avoiding the exponential expense of per-instance sampling.
- **Mechanism:** A neural network (the Shapley model) learns a direct mapping from state (and action) to Shapley values. It is trained by minimizing the expected squared error between a weighted sum of its outputs over a subset $C$ and the characteristic function value for that subset (derived via Equation 10 and 19).
- **Core assumption:** The model class is sufficiently expressive to represent the true Shapley value function across the state space, and the sampling distribution over subsets $p(C)$ allows convergence to the unique least-squares solution.
- **Evidence anchors:**
  - [abstract]: "replaces expensive Monte Carlo sampling with parametric models that amortize computation"
  - [section 3.1]: "learn a parametric model... to estimate the Shapley value contributions... amortising the approximation cost across states"
  - [corpus]: Neighbors like "FastSHAP" and "Prediction via Shapley Value Regression" confirm that parametric amortization is a recognized strategy for scaling Shapley values in supervised settings.
- **Break condition:** If the model capacity is insufficient or the subset sampling distribution $p(C)$ does not align with the combinatorial weights required by the Shapley formula (Equation 6), the estimator may converge to a biased solution.

### Mechanism 2
- **Claim:** Replacing a learned characteristic model with single-sample approximations can reduce computational overhead and improve accuracy via stochastic regularization.
- **Mechanism:** Instead of querying a separate, pre-trained "characteristic model" to get the value for a masked input, the framework samples a state $s'$ from the conditional steady-state distribution given the masked features and uses the agent's actual output for $s'$ as the target.
- **Core assumption:** The noise introduced by single-sample estimation acts as a beneficial regularizer rather than destabilizing training, and the conditional distribution can be sampled efficiently (e.g., via buffer lookup).
- **Evidence anchors:**
  - [abstract]: "reduces computational cost by eliminating characteristic models in favor of single-sample approximations"
  - [section 5]: "noise from the single-sample approximations may act as a stochastic regulariser... halving total training time"
  - [corpus]: Direct evidence for this specific "single-sample RL" mechanism is weak in the provided corpus, though "Exact Shapley Attributions" discusses computational trade-offs.
- **Break condition:** If the variance of the single-sample gradient estimates is too high relative to the learning rate, training may diverge or fail to converge to the true Shapley values.

### Mechanism 3
- **Claim:** Importance sampling (IS) enables the training of explanation models using historical off-policy data, provided distributional mismatch is corrected.
- **Mechanism:** The framework estimates the loss expectation under the current policy $\pi$ by re-weighting samples from a replay buffer generated by past policies $\pi_t$. This involves multiplying the loss by the probability ratio $\frac{\pi(s,a)}{\pi_t(s,a)}$ (Equation 17).
- **Core assumption:** The replay buffer provides sufficient coverage of the state-action pairs visited by the target policy $\pi$ (coverage assumption), and importance weights are bounded or normalized to prevent variance explosion.
- **Evidence anchors:**
  - [section 4]: "learning passively by drawing states from the agent's history... applying importance sampling"
  - [section D.2]: "normalising the importance weights is crucial: using unnormalised weights leads to worse approximation accuracy"
  - [corpus]: Corpus evidence regarding specific IS techniques for *explanation* models is weak/missing; IS is primarily validated here within the paper's RL context.
- **Break condition:** If the behavior policy $\pi_t$ differs significantly from $\pi$ in regions of high probability under $\pi$, the importance weights will have high variance, leading to unstable or biased gradient estimates.

## Foundational Learning

- **Concept: Shapley Values (Efficiency & Additivity)**
  - **Why needed here:** The entire framework relies on Shapley values satisfying the "efficiency" axiom (feature contributions sum to the total prediction change). The paper explicitly enforces this via a post-hoc correction term (Equation 11).
  - **Quick check question:** If a model outputs Shapley values that sum to 0.8 but the total model output change is 1.0, what specific correction does FastSVERL apply?

- **Concept: Characteristic Functions in RL**
  - **Why needed here:** Unlike supervised learning, RL explanations target "behaviour," "outcomes," or "predictions." Understanding how these map to specific characteristic functions (e.g., $\tilde{v}^\pi_s(C)$ for outcomes) is required to define the learning objective.
  - **Quick check question:** When explaining "outcomes," what specific quantity does the characteristic function represent when only subset $C$ is observed?

- **Concept: Off-policy Correction**
  - **Why needed here:** The framework frequently uses data generated by old policies to explain the current policy. Without understanding importance sampling weights, one cannot diagnose why an explanation model might fail to converge.
  - **Quick check question:** Why does the framework use the ratio $\frac{\pi(s,a)}{\pi_t(s,a)}$ when training on historical data?

## Architecture Onboarding

- **Component map:** Agent -> Buffer -> Characteristic Model (Optional) -> Shapley Model -> Sampler
- **Critical path:**
  1.  **Data Collection:** Fill buffer with transitions and log behavior policy probabilities.
  2.  **Target Generation:** Sample state $s$, subset $C$. Generate target value using either the Characteristic Model or the Single-Sample method (retrieving $s'$ from buffer).
  3.  **Training:** Update Shapley Model using the weighted least-squares loss (Equation 10/19).
  4.  **Correction:** Apply efficiency constraint correction to model outputs during inference/evaluation.

- **Design tradeoffs:**
  - **Model-based vs. Sample-based:** Model-based requires pre-training a characteristic model (slower setup, potential error propagation) but lower variance during Shapley training. Sample-based is faster (halves time) and acts as a regularizer but introduces gradient variance.
  - **On-policy vs. Off-policy:** On-policy is more accurate but requires environment interaction. Off-policy uses historical data (cheaper) but relies on buffer coverage and IS stability.

- **Failure signatures:**
  - **Non-convergence of Shapley Loss:** Often caused by insufficient model capacity or high variance in IS weights (try normalizing weights).
  - **Violation of Efficiency Constraint:** If the post-hoc correction magnitude is large, the base model is under-fitting.
  - **Error Propagation:** In the model-based path, if the characteristic model loss is high, the Shapley model will converge to explaining noise.

- **First 3 experiments:**
  1.  **Sanity Check (Behaviour):** In a small gridworld, train the Shapley model using *exact* characteristic values (computed analytically) to verify the parametric model can learn the Shapley function.
  2.  **Method Comparison:** In Mastermind-222, compare "Model-based" vs. "Single-sample" Shapley training. Plot MSE against training updates to verify if single-sample actually converges faster/better as claimed.
  3.  **Off-policy Stress Test:** Train an agent with a rapidly changing policy (e.g., PPO). Try training the explanation model using the off-policy buffer with and without importance sampling normalization to observe stability.

## Open Questions the Paper Calls Out
- **Open Question 1:** How do FastSVERL explanations impact human understanding and decision-making in real-world deployments compared to existing interpretability methods?
  - **Basis in paper:** [explicit] The Discussion section states that "Formal user studies and practical deployment in real-world systems are essential" to evaluate how these explanations aid human understanding.
  - **Why unresolved:** The paper focuses on computational scalability and approximation accuracy, explicitly leaving human-centric evaluation as a necessary future step to establish best practices.
  - **What evidence would resolve it:** Results from user studies measuring the utility of FastSVERL explanations in assisting human operators in safety-critical or complex RL environments.

- **Open Question 2:** Can parametric models of the steady-state distribution effectively generalize the framework to high-dimensional continuous state spaces?
  - **Basis in paper:** [explicit] The authors identify applying the framework to continuous state spaces as a "key challenge" because experience buffers offer only a sparse approximation of the distribution, suggesting a parametric model as a "promising direction."
  - **Why unresolved:** The empirical results are limited to discrete state spaces (e.g., Mastermind, Gridworld); the proposed solution for continuous states is theoretical and unverified.
  - **What evidence would resolve it:** Successful application and convergence of FastSVERL in continuous control benchmarks (e.g., MuJoCo) using a learned state distribution model.

- **Open Question 3:** Does the single-sample approximation method for eliminating characteristic models provide similar efficiency and accuracy gains in supervised learning tasks?
  - **Basis in paper:** [explicit] Section 5 notes that the proposed extension to remove characteristic models "extends naturally to supervised learning," but the paper only empirically validates this method in reinforcement learning settings.
  - **Why unresolved:** The claim of natural extension is made, but the efficiency gains (halving training time) are only demonstrated on RL agents like DQN.
  - **What evidence would resolve it:** Comparative experiments showing that the single-sample approximation method outperforms or matches existing supervised Shapley methods (like FastSHAP) in terms of speed and error on standard datasets.

## Limitations
- Empirical evaluation focuses on small discrete environments, leaving scalability to truly large state spaces largely inferred rather than demonstrated.
- Method's reliance on feature-based state decompositions limits applicability to environments where such decomposition is not readily available or meaningful.
- Paper does not explore sensitivity of results to the choice of subset sampling distribution p(C), which is critical for the least-squares estimator's convergence properties.

## Confidence
- **High Confidence:** The theoretical framework connecting Shapley values to RL explanations is sound, and the core parametric amortization mechanism (FastSHAP-style learning) is well-established in the broader literature. The empirical results on small domains are internally consistent.
- **Medium Confidence:** The claim of halving training time and the specific benefits of single-sample approximations over model-based approaches are supported by the Mastermind experiments, but the lack of direct corpus validation for the "single-sample RL" technique and the limited scope of the evaluation reduce confidence in generalization.
- **Low Confidence:** The assertion that the method "achieves low approximation error in domains with up to billions of states" is not empirically validated within the paper. The scalability claim is inferred from the algorithmic design and the tractable nature of the small-scale experiments, but not demonstrated.

## Next Checks
1. **Scalability Test:** Implement and evaluate FastSVERL on a larger discrete environment (e.g., a 100x100 gridworld or a larger Mastermind variant with 100+ states) to verify that approximation error remains low and training time benefits persist.

2. **Sensitivity Analysis:** Systematically vary the parameters of the subset sampling distribution p(C) (e.g., different weighting schemes for subset size) and measure the impact on the Shapley model's convergence and final approximation error to validate the core assumption about p(C)'s role.

3. **Continuous State Space Evaluation:** Adapt the method to a simple continuous control environment (e.g., a scaled-down version of CartPole or LunarLander with discretized or embedded feature representations) to test the framework's applicability beyond discrete, tabular settings and assess the performance of the importance sampling mechanism in this context.