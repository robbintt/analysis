---
ver: rpa2
title: 'FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment'
arxiv_id: '2511.10250'
source_url: https://arxiv.org/abs/2511.10250
tags:
- action
- deduction
- stage
- form
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces FineSkiing, the first fine-grained benchmark\
  \ dataset for action quality assessment (AQA) in aerial skiing, containing detailed\
  \ stage scores and deduction annotations. To address the challenge of limited interpretability\
  \ and reliability in existing AQA methods, the authors propose JudgeMind, a novel\
  \ AQA framework that simulates the professional judge\u2019s scoring mindset."
---

# FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment

## Quick Facts
- arXiv ID: 2511.10250
- Source URL: https://arxiv.org/abs/2511.10250
- Reference count: 40
- Primary result: JudgeMind achieves SRCC of 0.927 and Rℓ2 of 0.933 on FineSkiing dataset

## Executive Summary
This paper introduces FineSkiing, the first fine-grained benchmark dataset for action quality assessment (AQA) in aerial skiing, containing detailed stage scores and deduction annotations. To address the challenge of limited interpretability and reliability in existing AQA methods, the authors propose JudgeMind, a novel AQA framework that simulates the professional judge's scoring mindset. The method segments videos into distinct stages and uses stage-aware feature extraction and knowledge-based grade-aware decoding to assess each stage separately, incorporating deduction knowledge for more accurate and interpretable scoring.

## Method Summary
JudgeMind is a stage-aware AQA framework that segments aerial skiing videos into judging-relevant stages (Air, Form, Landing) and scores each stage separately. The method uses a CLIP-based temporal segmenter to identify stage boundaries, then extracts features using a dual-stream approach (Swin Transformer for foreground and I3D for global context). A Knowledge-based Grade-aware Decoder (K-GAD) incorporates textual deduction knowledge through cross-attention with video features to produce interpretable stage scores that are summed for the final result.

## Key Results
- JudgeMind achieves SRCC of 0.927 and Rℓ2 of 0.933 on FineSkiing dataset
- Outperforms existing methods on both FineSkiing and FineDiving datasets
- Ablation studies confirm effectiveness of stage segmentation, dual-stream feature extraction, and deduction knowledge incorporation

## Why This Works (Mechanism)

### Mechanism 1: Semantic Stage Decoupling
Segmenting videos into judging-relevant stages (Air, Form, Landing) rather than fixed-length clips improves scoring accuracy by aligning visual processing with distinct judging criteria. This preserves timing relationships between sub-actions rather than treating them as isolated events.

### Mechanism 2: Deduction-Guided Cross-Attention
Injecting explicit textual knowledge of potential deduction items (e.g., "legs apart", "arch back") into the decoder allows the model to actively query video features for specific faults. The Knowledge-based Grade-aware Decoder (K-GAD) encodes deduction rules via BERT into "deduction features" that interact with video features through cross-attention.

### Mechanism 3: Stage-Aware Dual-Stream Feature Extraction
Dynamically switching between global (athlete + environment) and foreground (athlete only) feature extraction based on stage type handles camera viewpoint switching better than uniform feature extraction. Core frames use foreground features to focus on pose, while non-core frames add global features to capture spatial interaction.

## Foundational Learning

- **Temporal Action Localization (TAL) vs. Segmentation**: Understanding the difference between detecting what action is happening vs. segmenting when a judging phase begins and ends. The paper argues standard sub-action detection breaks continuity needed for timing scores.
  - *Quick check*: Does the model need to label every flip (sub-action) or separate the takeoff from the flight (stage)?

- **Cross-Attention in Multimodal Learning**: Understanding how one modality (Text: "Legs apart") can query another modality (Video: Latent features of legs) to produce a relevance score.
  - *Quick check*: In K-GAD decoder, which modality provides the "Query" and which provides the "Key"?

- **Spearman's Rank Correlation (SRCC)**: AQA models are evaluated on how well they rank videos relative to each other rather than predicting exact absolute scores. SRCC measures this monotonic relationship.
  - *Quick check*: If a model predicts scores 8.0, 8.1, 8.2 for ground truth 7.0, 8.0, 9.0, is the SRCC high or low?

## Architecture Onboarding

- **Component map**: Input (Video + Action Code) -> Temporal Segmentation (CLIP ViT) -> Stage boundaries -> Feature Extraction (Swin + I3D) -> Context Fusion (ViT Encoder) -> K-GAD (BERT + Grade Prototypes) -> Head (Linear combination) -> Summed Stage Scores

- **Critical path**: The K-GAD Decoder is the critical novelty. If cross-attention between deduction text embeddings and video features fails to align, the interpretability claim collapses and the model reverts to a black-box regressor.

- **Design tradeoffs**:
  - Interpretability vs. Data Efficiency: Incorporating deduction text requires maintaining a valid knowledge base for every sport, making it less generalizable than a generic video encoder.
  - Fixed vs. Learnable Prototypes: Using learnable grade prototypes helps discretize the regression task but assumes grade distribution is somewhat consistent across the dataset.

- **Failure signatures**:
  - Segmentation Drift: If "Form" stage is mislabeled as "Landing", the model will look for "balance/stability" errors in a frame where "posture/technique" is being judged.
  - Context Fusion Collapse: If global features overpower foreground features during "Form" stage, the model may focus on crowd or snow conditions rather than athlete's body.

- **First 3 experiments**:
  1. Segmentation Ablation: Train model using standard 32-frame fixed clips instead of stage segmentation module to verify value of semantic alignment.
  2. Deduction Knowledge Ablation: Run K-GAD decoder with random text embeddings instead of BERT-encoded deduction rules to isolate whether model learns from content of rules versus attention mechanism.
  3. Viewpoint Robustness Check: Evaluate model on subset of videos with frequent camera cuts vs. stable static camera footage to test if Foreground/Global extraction logic handles noise.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How robust is JudgeMind to errors in the input action code prior, specifically when the declared action code does not match the athlete's actual execution?
- **Basis in paper**: The Context Fusion Encoder relies on input action code as a prior for feature enhancement, yet the dataset includes a "Change_type_action" deduction implying discrepancies can occur.
- **Why unresolved**: The paper assumes input action code is a reliable ground truth prior, but in real-world scenarios athletes may abort or alter maneuvers mid-flight, potentially misaligning textual prior with visual features.
- **What evidence would resolve it**: An ablation study evaluating performance degradation when synthetic noise is introduced to action code labels during inference.

### Open Question 2
- **Question**: What is the sensitivity of final scoring accuracy to errors in the upstream temporal segmentation module?
- **Basis in paper**: The method decouples temporal segmentation from scoring, and while feature extraction handles viewpoint switching, accuracy relies on segmentation model correctly identifying stage boundaries.
- **Why unresolved**: Ablation studies compare against baseline without segmentation but do not quantify error propagation resulting from imperfect or drifting stage boundary predictions.
- **What evidence would resolve it**: Reporting performance metrics when temporal segmentation module is perturbed or when using predicted versus ground-truth temporal boundaries.

### Open Question 3
- **Question**: Can the Knowledge-based Grade-aware Decoder (K-GAD) generalize to sports where structured deduction knowledge taxonomies are unavailable or must be learned implicitly?
- **Basis in paper**: The authors note regarding FineDiving experiments: "we manually collect the deduction knowledge as input... This may also have impacts on model performance."
- **Why unresolved**: The current framework relies on explicit, hand-crafted deduction rules which may not exist for all sports or may be subjective, limiting the model's scalability.
- **What evidence would resolve it**: Experiments applying the method to a generic dataset without providing explicit deduction text prompts, perhaps using learnable query embeddings instead.

## Limitations
- Primary benchmark dataset (FineSkiing) is not yet publicly available, limiting independent verification of core results.
- Deduction knowledge base format and content are underspecified, making it unclear how to reproduce K-GAD module without original annotations.
- Method relies on assumption that stage boundaries are visually distinct and consistently annotated, which may not hold for all camera angles or action variations.

## Confidence
- **High Confidence**: Core architectural approach (stage segmentation + dual-stream feature extraction) is technically sound and well-motivated by prior AQA literature.
- **Medium Confidence**: Specific implementation details of K-GAD decoder (cross-attention between deduction text and video features) are innovative but lack direct precedent in corpus.
- **Low Confidence**: Without access to actual FineSkiing dataset, impossible to verify whether reported performance gains are due to architectural innovations or specific characteristics of held-out data.

## Next Checks
1. **Segmentation Robustness**: Test CLIP-based temporal segmenter on FineDiving data with known ground-truth stage boundaries to verify that semantic stage alignment improves over fixed-length clipping.
2. **Deduction Knowledge Ablation**: Train variant of JudgeMind using random text embeddings instead of BERT-encoded deduction rules to isolate whether model learns from content of rules versus attention mechanism itself.
3. **Cross-Sport Generalization**: Evaluate FineDiving variant of JudgeMind on subset of videos with frequent camera cuts versus stable footage to test if foreground/global feature switching handles viewpoint variation as claimed.