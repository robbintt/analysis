---
ver: rpa2
title: Towards Robust Process Reward Modeling via Noise-aware Learning
arxiv_id: '2601.12748'
source_url: https://arxiv.org/abs/2601.12748
tags:
- qwen2
- reasoning
- step
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of noisy supervision in process
  reward modeling (PRM), where Monte Carlo Estimation (MCE) labels are policy-dependent
  and introduce false positives (incorrect steps rewarded due to later self-correction)
  and false negatives (correct steps penalized due to later failures). To address
  this, the authors propose a two-stage framework: (1) reflection-aware label correction,
  using an LLM judge to filter out trajectories where downstream steps explicitly
  correct earlier ones, thereby reducing false positives; and (2) noise-aware iterative
  training (NAIT), which refines labels based on model confidence to correct residual
  noise.'
---

# Towards Robust Process Reward Modeling via Noise-aware Learning

## Quick Facts
- **arXiv ID:** 2601.12748
- **Source URL:** https://arxiv.org/abs/2601.12748
- **Reference count:** 19
- **Primary result:** Up to 27% absolute F1 gain in step-level correctness discrimination over noisy PRMs

## Executive Summary
This paper addresses the challenge of training Process Reward Models (PRMs) under noisy supervision from Monte Carlo Estimation (MCE). The core problem is that MCE labels are policy-dependent and introduce two types of errors: false positives (incorrect steps rewarded due to downstream self-correction) and false negatives (correct steps penalized due to downstream failures). The authors propose a two-stage framework combining reflection-aware label correction to filter self-correction trajectories and noise-aware iterative training (NAIT) to refine labels based on model confidence. Experiments show substantial improvements in both step-level correctness discrimination and downstream test-time scaling accuracy.

## Method Summary
The authors propose a two-stage framework for robust PRM training under noisy MCE supervision. First, reflection-aware label correction uses an LLM judge to filter out trajectories where downstream steps explicitly correct earlier errors, reducing false positives. Second, noise-aware iterative training (NAIT) refines labels based on model confidence across three refinement stages with decreasing thresholds (δ ∈ {0.3, 0.2, 0.1}). The approach is evaluated on 128K reasoning trajectories from GSM8K, MATH-500, and PRM800K, using Qwen2.5-Math-7B for both policy sampling and PRM fine-tuning with special tokens [PRM], [POS], [NEG].

## Key Results
- Up to 27% absolute gain in average F1 on ProcessBench compared to PRMs trained with noisy supervision
- 3.2-5.6% improvement in downstream test-time scaling accuracy on GSM8K, MATH, GAOKAO, and MINERVA MATH
- Robust performance that scales well with additional training data
- Effective discrimination between correct and incorrect reasoning steps at fine granularity

## Why This Works (Mechanism)
The method works by explicitly addressing the two sources of noise in MCE supervision. Reflection-aware correction reduces false positives by identifying and removing trajectories where downstream steps correct earlier errors, preventing incorrect steps from being incorrectly rewarded. NAIT then addresses residual noise by iteratively refining labels based on the PRM's own confidence, correcting both false positives and false negatives that survive the initial filtering. This two-stage approach creates a cleaner training signal that enables the PRM to learn more accurate step-level judgments.

## Foundational Learning

**Monte Carlo Estimation (MCE):** A technique for estimating rewards by sampling multiple trajectories from each state and aggregating outcomes. Needed because step-level correctness is often ambiguous without seeing downstream consequences. Quick check: Verify that MCE with K=8 samples provides reasonable reward estimates for mathematical reasoning.

**Process Reward Modeling (PRM):** A model that evaluates the quality of intermediate reasoning steps rather than just final answers. Needed because reasoning processes contain both correct and incorrect steps that require fine-grained evaluation. Quick check: Confirm PRM can distinguish between different types of reasoning errors at the step level.

**Label Correction via LLM Judge:** Using a separate LLM to identify self-correction patterns in trajectories. Needed because downstream corrections provide evidence that earlier steps were incorrect, even if they led to correct final answers. Quick check: Test LLM judge's ability to detect various forms of self-correction across different problem types.

## Architecture Onboarding

**Component Map:** MCE Sampling -> Reflection-Aware Correction -> NAIT Iterative Refinement -> PRM Fine-tuning -> Evaluation

**Critical Path:** The core pipeline is: (1) Generate trajectories with MCE labels, (2) Apply reflection-aware correction to filter self-correction trajectories, (3) Fine-tune PRM on filtered labels (Stage 0), (4) Iteratively refine labels using NAIT across three stages with decreasing thresholds, (5) Evaluate on ProcessBench and downstream tasks.

**Design Tradeoffs:** The method trades computational cost (multiple fine-tuning stages) for improved label quality. Using a separate LLM judge for reflection detection adds inference overhead but provides more accurate filtering than heuristic approaches. The fixed threshold schedule is simple but may not be optimal for all domains.

**Failure Signatures:** 
- Over-aggressive reflection filtering removes too many valid trajectories (check filtered dataset size)
- NAIT thresholds too aggressive, replacing correct labels with model errors (monitor label change rate)
- Error propagation in NAIT if initial PRM predictions are systematically biased

**Three First Experiments:**
1. Evaluate reflection-aware correction alone (without NAIT) to measure its standalone impact on label quality
2. Test NAIT with only one refinement stage to assess the value of multiple iterations
3. Compare different threshold schedules (linear vs. exponential decay) to optimize NAIT performance

## Open Questions the Paper Calls Out

**Integration with RL:** The method was only tested on test-time scaling and not extended to reinforcement learning training loops. It's unclear if the iterative label refinement process remains robust when the reward model is used to update the policy generator itself, which could create a shifting distribution of errors.

**Scaling to Larger Models:** The approach was only evaluated on 1.5B and 7B models. Results may not be applicable to larger-scale models, which may possess different self-correction capabilities that qualitatively change the distribution of false positives and false negatives.

**Cross-Domain Generalization:** The reflection-aware correction mechanism relies on a mathematical reasoning-specific prompt. Its effectiveness for non-mathematical domains like code generation or logical deduction is unknown, as "correction" manifests differently in these contexts.

## Limitations
- Computational constraints prevented evaluation with RL training loops
- Only tested on 1.5B and 7B models, limiting scalability claims
- Reflection detection may miss implicit corrections or errors
- Fixed threshold schedule may not be optimal for all datasets

## Confidence

**High Confidence:** Step-level correctness improvement (27% F1 gain) is directly supported by ProcessBench evaluation results comparing NAIT to baseline PRMs.

**Medium Confidence:** Robustness to scaling (3.2-5.6% accuracy gains) shows improvement but could benefit from more extensive validation across different model sizes and problem difficulties.

**Medium Confidence:** Reflection-aware correction effectiveness is demonstrated on ProcessBench, but the completeness of reflection detection and impact on real-world reasoning tasks requires further validation.

## Next Checks

1. **Threshold Sensitivity Analysis:** Systematically evaluate varying NAIT thresholds (δ) and number of refinement stages on step-level F1 and downstream accuracy to determine optimal configuration and assess robustness to hyperparameter choices.

2. **Cross-Domain Transferability:** Test NAIT framework on non-mathematical reasoning tasks (code generation, scientific reasoning) to assess generalizability and identify domain-specific limitations or needed adaptations.

3. **Error Propagation Analysis:** Conduct ablation study comparing results when using oracle labels for subset of data versus relying solely on model predictions, and analyze types of errors most susceptible to propagation.