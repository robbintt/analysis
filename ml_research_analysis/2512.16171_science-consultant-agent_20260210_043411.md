---
ver: rpa2
title: Science Consultant Agent
arxiv_id: '2512.16171'
source_url: https://arxiv.org/abs/2512.16171
tags:
- data
- agent
- science
- questionnaire
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Science Consultant Agent is a web-based AI tool designed to
  help practitioners select and implement effective modeling strategies for AI-based
  solutions. It addresses the challenge of navigating complex modeling decisions in
  a rapidly evolving AI landscape by combining structured questionnaires, literature-backed
  recommendations, and prototype generation.
---

# Science Consultant Agent

## Quick Facts
- **arXiv ID:** 2512.16171
- **Source URL:** https://arxiv.org/abs/2512.16171
- **Reference count:** 10
- **Primary result:** Web-based AI tool combining structured questionnaires, literature-backed recommendations, and prototype generation for effective AI modeling strategy selection

## Executive Summary
The Science Consultant Agent is a web-based AI tool designed to help practitioners select and implement effective modeling strategies for AI-based solutions. It addresses the challenge of navigating complex modeling decisions in a rapidly evolving AI landscape by combining structured questionnaires, literature-backed recommendations, and prototype generation. The system consists of four components: a Questionnaire that captures task requirements, Smart Fill that auto-completes fields using project descriptions, Research-Guided Recommendation that retrieves and synthesizes relevant arXiv literature, and Prototype Builder that generates baseline implementations.

## Method Summary
The Science Consultant Agent employs a multi-component architecture where users complete a structured six-part questionnaire about their AI modeling task. Smart Fill uses LLM to auto-complete questionnaire fields from project descriptions. The Research-Guided Recommendation component generates up to 50 queries from questionnaire responses, searches arXiv, deduplicates results, and synthesizes context from relevant papers to produce evidence-backed recommendations. The Prototype Builder uses predefined tools (XGBoost, LightGBM, prompting baselines) to generate baseline implementations via SageMaker training, validated against a Unified Data Template.

## Key Results
- Recommendations often align with user expectations and provide convincing justifications
- The structured questionnaire approach effectively captures task requirements
- Literature-grounded recommendations improve over ungrounded approaches (100% alignment observed)
- Tool-based prototype generation trades flexibility for safety in code execution

## Why This Works (Mechanism)

### Mechanism 1: Structured Questionnaire Reduces Einstellung Effect
- **Claim:** Pre-defined questions systematically capture task requirements, counteracting users' tendency to default to familiar solutions.
- **Mechanism:** The six-part questionnaire forces explicit consideration of data characteristics, evaluation criteria, constraints, and task mechanisms before solution selection. This reduces "example-induced bias" where teams design around narrow familiar cases.
- **Core assumption:** Users will engage with structured prompts more completely than they would self-generate the same questions.
- **Evidence anchors:** Questionnaire forces systematic information gathering rather than relying on users to guess what LLMs need to understand tasks.
- **Break condition:** Users skip or incorrectly answer questionnaire fields; Smart Fill compensates partially but cannot infer data availability reliably.

### Mechanism 2: Literature Retrieval Grounds Recommendations in Current Evidence
- **Claim:** arXiv retrieval with LLM-guided query generation produces recommendations that reflect recent research rather than stale internal knowledge.
- **Mechanism:** Questionnaire responses → LLM generates K queries → arXiv MCP search → deduplication → LLM filters to N most relevant papers → context construction (abstracts/full/summaries) → final recommendation synthesis.
- **Core assumption:** arXiv search quality is sufficient; LLM can accurately assess paper relevance from abstracts.
- **Evidence anchors:** 100% of users observed alignment with expectations after grounding recommendations in arXiv papers.
- **Break condition:** arXiv search returns irrelevant papers; context construction strategy biases heavily toward few papers (full-paper mode) or lacks depth (abstract-only mode).

### Mechanism 3: Tool-Based Prototype Generation Trades Flexibility for Safety
- **Claim:** Predefined tools for prototype generation avoid risks of arbitrary code execution while providing reproducible baselines.
- **Mechanism:** LLM selects from predefined tools (XGBoost, LightGBM, prompting baselines) → validates inputs against Unified Data Template → submits SageMaker training job → produces evaluation report.
- **Core assumption:** Standard baselines are sufficient for initial prototyping; users accept limited modeling options.
- **Evidence anchors:** Tool-based approach is safe to execute, avoids risks of arbitrary or malicious code execution, and ensures that the logic is always correct.
- **Break condition:** User task requires modeling strategies outside predefined tool set; text evaluation remains unreliable due to surface-form mismatches.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** The Research-Guided Recommendation component is fundamentally a RAG system over arXiv. Understanding chunking, retrieval scoring, and context window management is essential.
  - **Quick check question:** Can you explain why the paper uses three different context construction strategies (abstract-only, full-paper, summaries)?

- **Concept: LLM Tool-Use / Function Calling**
  - **Why needed here:** The Prototype Builder uses a tool-based approach where the LLM selects predefined functions rather than generating arbitrary code.
  - **Quick check question:** What are the safety tradeoffs between tool-based execution and autonomous code generation?

- **Concept: arXiv API and Search Heuristics**
  - **Why needed here:** The evidence retrieval pipeline depends on generating effective arXiv search queries from questionnaire responses.
  - **Quick check question:** Why does the system generate multiple queries (K=50) rather than a single comprehensive query?

## Architecture Onboarding

- **Component map:** User Input → Questionnaire (6 parts) → Smart Fill (auto-complete) → arXiv MCP → Query Generation → Search/Dedup/Filter → Context Construction → LLM Recommendation Synthesis → Prototype Builder → Tool Selection → SageMaker Training → Evaluation Report

- **Critical path:** Questionnaire completion → arXiv retrieval → context construction → recommendation generation. Smart Fill is optional; Prototype Builder is optional and requires user-provided data.

- **Design tradeoffs:**
  - **Context strategy:** Abstract-only (fast, many papers, shallow) vs. Full-paper (slow, 1-2 papers, deep) vs. Summaries (slowest, multiple papers, moderate depth)
  - **Prototype approach:** Tool-based (safe, limited flexibility) vs. Code generation (flexible, dangerous)
  - **LaTeX vs PDF:** LaTeX preserves tables/equations exactly but may lose document order; PDF-to-Markdown is slower and fails frequently

- **Failure signatures:**
  - Users leave questionnaire fields blank or answer incorrectly (Smart Fill cannot fully compensate for data availability questions)
  - Terminology confusion for non-scientists (e.g., "reasoning capabilities" unclear)
  - Recommendations align with expectations but provide no new insights (experienced users)
  - Text evaluation failures due to surface-form mismatches (e.g., "8400" vs "8,400 USD")

- **First 3 experiments:**
  1. **Context strategy comparison:** Run identical questionnaire responses through abstract-only vs. summary-based context construction; measure recommendation quality and latency tradeoffs.
  2. **Query generation sensitivity:** Vary K (number of queries) and analyze recall of relevant papers vs. noise; identify optimal K for different task types.
  3. **Prototype Builder validation:** Submit datasets following the Unified Data Template to XGBoost and prompting tools; verify evaluation metrics match manual baseline runs.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the quality of the Science Consultant Agent's recommendations be systematically evaluated beyond user satisfaction surveys?
  - **Basis in paper:** [explicit] The paper states "evaluation remains challenging and limited, as it primarily relies on user feedback; future work should explore more thorough and systematic evaluation methods."
  - **Why unresolved:** Implementing and tuning all alternative strategies to verify the "best" recommendation is cost-prohibitive, and the definition of "best" shifts rapidly with new research.
  - **What evidence would resolve it:** A benchmark dataset of tasks with known optimal strategies, or a longitudinal study correlating agent recommendations with downstream project success rates.

- **Open Question 2:** Can LLM-based agents safely and reliably replicate implementation details from research papers and GitHub repositories?
  - **Basis in paper:** [explicit] The authors propose a "more ambitious improvement would be to build an agent that can read a research paper... and replicate the code," noting the current risks of generated code producing misleading results or corrupting data.
  - **Why unresolved:** Current LLMs lack the reliability for autonomous coding in sensitive environments, introducing risks of data corruption and incorrect logic that require expert supervision to mitigate.
  - **What evidence would resolve it:** A framework for safe code execution (e.g., sandboxes) combined with functional correctness tests on replicated repositories.

- **Open Question 3:** What is the most effective method for automating the evaluation of generative text prototypes within the Prototype Builder?
  - **Basis in paper:** [inferred] The paper notes that for text data tools, "evaluation for text generation is not straightforward" because simple string matching fails on semantic equivalents (e.g., "8400" vs "8,400"), and LLM-as-judge approaches introduce cost and potential error.
  - **Why unresolved:** There is a trade-off between the flexibility of LLM-based evaluation and the precision of deterministic metrics, and neither currently offers a standardized solution for the tool's pipeline.
  - **What evidence would resolve it:** Comparative analysis of semantic similarity metrics versus LLM-as-judge approaches on a standardized set of prototype outputs.

## Limitations

- Smart Fill effectiveness depends heavily on clear, concise project descriptions; vague or overly technical descriptions may cause significant accuracy issues
- Text evaluation in Prototype Builder remains fundamentally unreliable due to surface-form matching problems, potentially missing substantive model quality differences
- arXiv retrieval pipeline's effectiveness is sensitive to query generation quality and deduplication thresholds, with no ablation studies validating optimal parameters

## Confidence

- **High confidence:** The core architecture design (questionnaire → literature retrieval → recommendation synthesis) is sound and well-motivated by user needs analysis
- **Medium confidence:** User feedback suggests recommendations align with expectations and provide convincing justifications, but lacks quantitative metrics
- **Low confidence:** Claims about Smart Fill effectiveness and text evaluation reliability are not empirically validated

## Next Checks

1. Conduct an ablation study varying K (query count) and context construction strategy to identify optimal parameters for different task types, measuring both recall of relevant papers and latency.
2. Implement a quantitative evaluation framework comparing Smart Fill auto-completed fields against ground-truth questionnaire responses from domain experts to measure accuracy under different description quality scenarios.
3. Test Prototype Builder's text evaluation capabilities on datasets with known formatting variations (e.g., currency symbols, date formats) to quantify failure rates and identify patterns where current metrics break down.