---
ver: rpa2
title: Churn-Aware Recommendation Planning under Aggregated Preference Feedback
arxiv_id: '2507.04513'
source_url: https://arxiv.org/abs/2507.04513
tags:
- user
- policy
- belief
- optimal
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses sequential recommendation planning under privacy
  constraints where only aggregated user preference data is available. The Rec-APC
  model assumes a recommender system knows a prior distribution over user types and
  their aggregated preferences, but not individual user identities, while facing the
  risk of user churn from unsatisfactory recommendations.
---

# Churn-Aware Recommendation Planning under Aggregated Preference Feedback

## Quick Facts
- arXiv ID: 2507.04513
- Source URL: https://arxiv.org/abs/2507.04513
- Reference count: 40
- This paper addresses sequential recommendation planning under privacy constraints where only aggregated user preference data is available, using a branch-and-bound algorithm that exploits convergence to pure exploitation.

## Executive Summary
This paper tackles the problem of sequential recommendation under privacy constraints where individual user identities are unknown but aggregated preference statistics are available. The Rec-APC model assumes the system knows a prior distribution over user types and their aggregated preferences, but cannot track individual users. The core challenge is maximizing social welfare (likes) while managing user churn risk from unsatisfactory recommendations. The authors propose a branch-and-bound algorithm that exploits the theoretical insight that optimal policies converge to pure exploitation after finite time, enabling efficient search by pruning suboptimal branches.

## Method Summary
The method uses a branch-and-bound algorithm to compute approximately optimal recommendation policies under privacy constraints. The algorithm computes tight upper and lower bounds on the value function (V^U assuming oracle knowledge of user type, and V^L as best fixed-action policy) and prunes branches where the gap is below tolerance ε. The approach exploits the theoretical result that optimal policies converge to pure exploitation in finite time due to Bayesian belief concentration. A key component is the separator parameter c that measures instance well-separation, guaranteeing that distinct user types respond differently to recommendations and enabling belief concentration. The method is validated against SARSOP (a POMDP solver) on both synthetic data and real-world MovieLens data.

## Key Results
- Algorithm 1 outperforms SARSOP on rectangular instances (small number of categories relative to user types) and on real-world MovieLens data with noise
- SARSOP performs better when the number of categories greatly exceeds the number of user types
- Runtime comparisons on synthetic data demonstrate Algorithm 1's efficiency advantage for square or near-square problem instances
- Optimal policies converge rapidly in practice, with uncertainty about user type decreasing geometrically after policy convergence

## Why This Works (Mechanism)

### Mechanism 1: Belief Concentration via Bayesian Updates
- **Claim:** Repeated positive feedback concentrates the belief distribution on a single user type, enabling eventual convergence to pure exploitation.
- **Mechanism:** Each positive feedback on category k updates the belief via τ(b,k)(m) = b(m)·P(k,m) / Σ_{m'} b(m')·P(k,m'). The separator parameter c guarantees that distinct user types respond differently to the same category, ensuring updates are informative.
- **Core assumption:** Instance is well-separated (c(I) > 0), meaning user types have distinguishable preference profiles.
- **Evidence anchors:** [abstract] "positive responses refine the posterior via Bayesian updates... We prove that optimal policies converge to pure exploitation in finite time"
- **Break condition:** If c → 0 (e.g., all rows of P identical), updates become uninformative and convergence is not guaranteed.

### Mechanism 2: Bounding-Based Branch Pruning
- **Claim:** Tight upper and lower bounds on the value function enable efficient pruning of suboptimal policy prefixes.
- **Mechanism:** The algorithm computes V^U(b) = Σ_m b(m)·max_k P(k,m)/(1-P(k,m)) as an upper bound and V^L(b) = max_k Σ_m b(m)·P(k,m)/(1-P(k,m)) as a lower bound. When V^{Π⊕k} - Ṽ ≤ ε, the branch is pruned since it cannot improve beyond tolerance.
- **Core assumption:** Implicit discounting through p_max < 1 ensures finite value and meaningful bounds.
- **Evidence anchors:** [Section 5] Derives V^U and V^L explicitly; Theorem 3 proves ε-optimality and finite termination
- **Break condition:** If p_max → 1, the value function becomes unbounded and the gap V^U - V^L explodes.

### Mechanism 3: Finite-Horizon Approximation with Geometric Decay
- **Claim:** Infinite-horizon optimal policies can be approximated by finite-horizon analysis due to implicit discounting.
- **Mechanism:** The probability of surviving t rounds decays as Π_{j≤t} p_{π_j}(b) ≤ p_max^t. Setting H(ε) = ⌈log_{p_max}(ε(1-p_max)/p_max)⌉ ensures V^⋆ ≤ max_{π'} V^π'_H + ε.
- **Core assumption:** p_max < 1; if p_max = 1, some user type never churns and expected value is infinite.
- **Evidence anchors:** [Section 3] Lemma 2 proves the finite-horizon approximation bound; [Section 6] Figure 1 shows empirical convergence of belief uncertainty
- **Break condition:** If p_max = 1 for some (k,m) pair, the trivial policy always recommending k yields infinite welfare for type m users.

## Foundational Learning

- **Concept: Bayesian Belief Updates**
  - Why needed here: The core state representation is a belief distribution over latent user types; understanding how observations update this distribution is essential.
  - Quick check question: Given prior q = (0.5, 0.5) over two types and P(k₁, ·) = (0.9, 0.1), what is the posterior after observing a "like" on k₁?

- **Concept: Branch-and-Bound Tree Search**
  - Why needed here: Algorithm 1 uses B&B to explore policy prefixes; understanding pruning conditions is necessary for implementation and debugging.
  - Quick check question: In minimization, if the lower bound of a node exceeds the global upper bound, what happens to that branch?

- **Concept: POMDP Formulation**
  - Why needed here: Rec-APC is reformulated as a POMDP (Proposition 3); this connection enables comparison with SARSOP and alternative solvers.
  - Quick check question: In a POMDP, what does the belief state represent, and how does it differ from the underlying state?

## Architecture Onboarding

- **Component map:**
  Instance -> Belief -> BranchAndBound -> ValueUpperBound/LowerBound -> Separator c

- **Critical path:**
  1. Validate instance (p_max < 1, c > 0 for convergence guarantees)
  2. Initialize belief b = q and lower bound Ṽ = V^L(q)
  3. Branch on each category k ∈ K; compute V^{Π⊕k} using Equation (15)
  4. Prune branches where upper bound - Ṽ ≤ ε
  5. Extend remaining prefixes; terminate when queue empty or depth ≥ H(ε)
  6. Output best prefix; extend to infinite policy via fixed exploitation of terminal belief

- **Design tradeoffs:**
  - B&B explores policy space (branching factor = |K|) vs. POMDP solvers like SARSOP explore belief space (dimension = |M|). Paper shows B&B excels when |M| ≫ |K| but degrades when |K| ≫ |M| (Figure 2).
  - Tighter bounds reduce search space but cost more to compute; V^U and V^L are O(|M|·|K|) each.

- **Failure signatures:**
  - Non-termination: p_max too close to 1, causing H(ε) to explode
  - Slow convergence: c very small, requiring many rounds to concentrate belief
  - Poor policy quality: ε too large or pruning too aggressive

- **First 3 experiments:**
  1. Reproduce Figure 2a: Fix |K| = 10, vary |M| ∈ {2, 4, 8, 16, 32}. Compare B&B vs. SARSOP runtime with ε = 10⁻⁶. Expect B&B to dominate for larger |M|.
  2. Convergence validation: On synthetic instances, plot belief uncertainty ||b - vertex||₁ over rounds under π^⋆. Verify geometric decay and identify round where policy becomes fixed.
  3. Ablation on separation: Generate instances with varying c by adding noise to P. Measure iterations to convergence and final policy quality. Expect degradation as c → 0.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does a provably optimal polynomial-time algorithm exist for computing the optimal policy in Rec-APC, or is the problem computationally hard?
- **Basis in paper:** [explicit] Section 8 states, "we still lack either a provably optimal polynomial-time algorithm for computing an optimal policy or a formal proof of hardness."
- **Why unresolved:** The paper provides a branch-and-bound algorithm (which can be exponential) and proves theoretical convergence, but does not classify the computational complexity of finding the optimum.
- **What evidence would resolve it:** A polynomial-time algorithm derivation or a formal reduction proving NP-hardness.

### Open Question 2
- **Question:** Can the convergence properties of optimal policies be preserved under probabilistic belief dynamics where users do not always churn after negative feedback?
- **Basis in paper:** [explicit] Section 8 identifies the "dichotomous structure" of feedback as a limitation and suggests "probabilistic belief dynamics" as a direction where "belief walks become more complex."
- **Why unresolved:** The paper's main theoretical results (Theorem 2) rely on the specific dynamic where a negative response terminates the session, simplifying the belief update process.
- **What evidence would resolve it:** A proof of convergence (or counter-example) for a generalized POMDP model allowing users to stay after dislikes.

### Open Question 3
- **Question:** How robust is the Rec-APC framework when the preference matrix P and prior q are unknown and must be estimated online?
- **Basis in paper:** [inferred] The model assumes "complete information" on P and q (Section 1.2), abstracting away the cold-start learning problem.
- **Why unresolved:** The theoretical guarantees and algorithm depend on exact Bayesian updates using known probabilities; errors in estimating P or q could lead to suboptimal belief walks and increased churn.
- **What evidence would resolve it:** Analysis of sample complexity or regret bounds in a setting where P is learned concurrently with serving users.

## Limitations

- **Separation constant dependency:** The convergence guarantees critically depend on the instance being well-separated (c > 0), which may not hold in practice with noisy or correlated user preferences.
- **Single-round feedback assumption:** The binary feedback constraint (like/dislike only) may limit performance in domains where fine-grained ratings are available but aggregated due to privacy constraints.
- **Synthetic data construction:** The synthetic instances are generated through specific mathematical procedures that may not reflect realistic preference distributions in truly anonymized user data.

## Confidence

**High confidence (3 claims):**
- Algorithm 1 correctly implements the branch-and-bound search with the specified pruning conditions
- The POMDP reformulation in Proposition 3 is mathematically sound
- The runtime comparisons against SARSOP show clear patterns matching theoretical expectations

**Medium confidence (2 claims):**
- The convergence rate of optimal policies to pure exploitation (geometric decay) holds across diverse real-world instances
- The approximation bound V^⋆ ≤ V^π_H + ε is tight enough for practical use with reasonable H(ε)

**Low confidence (1 claim):**
- The assumption that well-separated instances are common in practice without requiring careful data engineering

## Next Checks

1. **Separation sensitivity analysis:** Systematically vary the noise level in synthetic P matrices and measure: (a) convergence rate of belief concentration, (b) runtime of Algorithm 1, and (c) final policy welfare. This quantifies the practical impact of the c > 0 assumption.

2. **Real-world privacy constraint validation:** Implement the aggregation mechanism on a public dataset where individual preferences are known, then simulate the aggregated feedback setting. Compare Algorithm 1's performance against both individual-preference and fully anonymized baselines.

3. **Generalization to non-rectangular instances:** Generate synthetic data where |K| ≫ |M| or |M| ≫ |K| beyond the tested ranges. Specifically test cases like |K| = 100, |M| = 10 to confirm whether the theoretical runtime scaling predictions hold in extreme regimes.