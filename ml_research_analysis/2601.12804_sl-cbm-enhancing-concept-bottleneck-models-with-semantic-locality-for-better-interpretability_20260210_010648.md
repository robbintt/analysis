---
ver: rpa2
title: 'SL-CBM: Enhancing Concept Bottleneck Models with Semantic Locality for Better
  Interpretability'
arxiv_id: '2601.12804'
source_url: https://arxiv.org/abs/2601.12804
tags:
- concept
- saliency
- sl-cbm
- maps
- locality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of poor locality faithfulness
  in Concept Bottleneck Models (CBMs), where concept explanations fail to spatially
  align with relevant image regions, limiting interpretability and reliability in
  high-stakes domains. The authors propose SL-CBM, a novel extension that enforces
  locality faithfulness by generating spatially coherent saliency maps at both concept
  and class levels using a 1x1 convolutional layer combined with a cross-attention
  mechanism.
---

# SL-CBM: Enhancing Concept Bottleneck Models with Semantic Locality for Better Interpretability

## Quick Facts
- arXiv ID: 2601.12804
- Source URL: https://arxiv.org/abs/2601.12804
- Reference count: 11
- Introduces SL-CBM, a method to improve locality faithfulness in Concept Bottleneck Models by aligning concept explanations with image regions

## Executive Summary
This paper addresses a critical limitation in Concept Bottleneck Models (CBMs): their inability to produce spatially coherent explanations that align with relevant image regions. While CBMs offer interpretability by predicting interpretable concepts before final classification, they often fail at locality faithfulness—the spatial alignment between predicted concepts and their corresponding image regions. The authors propose SL-CBM, which enhances standard CBMs with a 1x1 convolutional layer and cross-attention mechanism to generate spatially coherent saliency maps at both concept and class levels. This approach ensures that concept predictions are spatially aligned with their visual evidence, substantially improving interpretability and trustworthiness in high-stakes domains.

## Method Summary
SL-CBM extends traditional CBMs by incorporating spatial awareness into the concept prediction process. The method employs a 1x1 convolutional layer that takes the concept embedding vector as input and generates spatially coherent saliency maps through a cross-attention mechanism. This design allows SL-CBM to produce concept predictions that are not only semantically meaningful but also spatially aligned with relevant image regions. The framework includes contrastive and entropy-based regularization to balance accuracy, sparsity, and faithfulness. By maintaining the concept-based reasoning structure while adding spatial coherence, SL-CBM creates explanations that are both interpretable and grounded in the actual visual evidence.

## Key Results
- SL-CBM substantially improves locality faithfulness metrics (IoU, Dice, C-IoU) compared to standard CBMs and post-hoc explanation methods
- The model maintains competitive classification accuracy while providing superior explanation quality and intervention efficacy
- Ablation studies demonstrate that contrastive and entropy-based regularization are crucial for balancing accuracy, sparsity, and faithfulness

## Why This Works (Mechanism)
SL-CBM succeeds by enforcing spatial coherence between concept predictions and image regions through its architectural innovations. The 1x1 convolutional layer transforms concept embeddings into spatially aware representations, while the cross-attention mechanism ensures that these representations focus on relevant image regions. This approach addresses the fundamental disconnect in standard CBMs where concept predictions are global and lack spatial grounding. By generating saliency maps that highlight where each concept is detected in the image, SL-CBM creates explanations that humans can verify visually, enhancing both interpretability and trustworthiness.

## Foundational Learning
- Concept Bottleneck Models (CBMs): Why needed - provide interpretable intermediate representations; Quick check - concepts should be human-understandable and predictive of classes
- Locality faithfulness: Why needed - ensures explanations align with relevant image regions; Quick check - saliency maps should highlight concept-relevant areas
- Cross-attention mechanisms: Why needed - enables focusing on relevant spatial regions; Quick check - attention weights should concentrate on concept evidence
- 1x1 convolutional layers: Why needed - transform concept embeddings into spatial representations; Quick check - output should have spatial dimensions matching input
- Contrastive regularization: Why needed - improves discrimination between concepts; Quick check - concept embeddings should be separable
- Entropy-based regularization: Why needed - promotes sparsity and focuses on relevant concepts; Quick check - concept predictions should be confident and selective

## Architecture Onboarding
Component map: Input image -> Feature extractor -> 1x1 Conv layer + Cross-attention -> Concept predictions + Spatial saliency maps -> Class predictions
Critical path: Image features → 1x1 Conv (concept embedding → spatial saliency) → Cross-attention (spatial alignment) → Concept predictions → Class predictions
Design tradeoffs: Spatial coherence vs. computational overhead; interpretability vs. classification accuracy; sparsity vs. completeness of concept explanations
Failure signatures: Poor locality faithfulness indicates attention misalignment; low concept prediction accuracy suggests feature extractor issues; class prediction drops may indicate over-regularization
First experiments: 1) Compare IoU scores between SL-CBM and standard CBM on RIV AL-10 dataset; 2) Ablate the 1x1 convolutional layer to measure locality faithfulness degradation; 3) Test different regularization strengths to find optimal balance between accuracy and faithfulness

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, focusing instead on demonstrating the effectiveness of SL-CBM. However, implicit questions remain about the method's applicability to domains beyond natural images and object recognition, particularly in high-stakes applications like medical imaging where spatial alignment is critical.

## Limitations
- Generalization to domains beyond natural images remains unproven, particularly for medical imaging and satellite imagery
- Computational overhead from the 1x1 convolutional layer and cross-attention mechanism may impact practical deployment
- The causal relationship between improved locality faithfulness and enhanced trustworthiness is assumed but not empirically validated through user studies

## Confidence
- High confidence in technical implementation and evaluation methodology for studied datasets
- Medium confidence in claims about improved locality faithfulness and explanation quality based on robust quantitative metrics
- Low confidence in broader claims about trustworthiness and real-world applicability without additional validation

## Next Checks
1. Evaluate SL-CBM on medical imaging datasets (e.g., chest X-rays, histopathology slides) where spatial alignment between concepts and pathologies is clinically critical
2. Conduct a controlled user study comparing SL-CBM explanations to standard CBMs and post-hoc methods, measuring trust calibration and decision accuracy
3. Perform a scalability analysis testing SL-CBM with 100+ concepts to assess whether locality faithfulness degrades with increased concept complexity