---
ver: rpa2
title: Onto-Epistemological Analysis of AI Explanations
arxiv_id: '2510.02996'
source_url: https://arxiv.org/abs/2510.02996
tags:
- explanations
- input
- methods
- explanation
- assumptions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides an onto-epistemological analysis of XAI methods,
  categorizing them based on their underlying philosophical assumptions about the
  existence and knowability of explanations. The authors argue that seemingly small
  technical differences between XAI methods can correspond to fundamentally different
  assumptions about what constitutes an explanation and how it can be known.
---

# Onto-Epistemological Analysis of AI Explanations

## Quick Facts
- arXiv ID: 2510.02996
- Source URL: https://arxiv.org/abs/2510.02996
- Reference count: 40
- Authors: Martina Mattioli; Eike Petersen; Aasa Feragen; Marcello Pelillo; Siavash A. Bigdeli
- Primary result: XAI methods should be chosen based on compatibility between their underlying philosophical assumptions and the application domain's requirements

## Executive Summary
This paper provides an onto-epistemological framework for analyzing and categorizing XAI methods based on their philosophical assumptions about explanation existence and knowability. The authors argue that seemingly minor technical differences between XAI methods can correspond to fundamentally different assumptions about what constitutes an explanation and how it can be known. They identify four main paradigms—logical positivist, contemporary realist, interpretivist, and postmodern—each with distinct ontological and epistemological assumptions. The framework emphasizes that choosing an XAI method should consider the compatibility of its underlying assumptions with the application domain, as mismatches can lead to contradictions and mistrust.

## Method Summary
The authors conduct a qualitative analysis of well-known XAI methods by examining their original papers and mapping their technical specifications and validation approaches to philosophical paradigms. Using Table 2 indicators (problematic, methodology, evaluation), they trace how each method defines explanations, builds them, and justifies their validity. The analysis identifies four distinct paradigms based on axes of ontology (realist vs idealist vs relativist) and epistemology (positivist vs structuralist vs constructivist vs relationist). The method involves extracting definition of explanation, methodology for generation, and evaluation criteria from each paper, then mapping these to paradigms using the framework's indicators.

## Key Results
- XAI methods can be categorized into four philosophical paradigms based on their ontological and epistemological assumptions
- Seemingly small technical changes (e.g., adding a baseline) can fundamentally shift a method's paradigm classification
- Scientific domains like medicine require explanations from the contemporary realist paradigm, while other paradigms may lead to contradictions
- Validation methodology reveals epistemological stance and must match the engineer's validation needs

## Why This Works (Mechanism)

### Mechanism 1: Compatibility Filter
- **Claim:** The validity of an XAI method is conditional on the alignment between its philosophical assumptions (paradigm) and the requirements of the application domain.
- **Mechanism:** A compatibility filter. The framework prevents "category errors" where, for example, a method assuming relative truths (Postmodern) is applied to a domain requiring absolute physical causes (Scientific/Medical).
- **Core assumption:** Scientific domains (e.g., medicine) generally operate within a "Contemporary Realist" paradigm and will reject explanations derived from "Interpretivist" or "Postmodern" methods that rely on relative or human-constructed truths.
- **Evidence anchors:** Abstract: "choosing an XAI method should consider the compatibility of its underlying assumptions with the application domain." Page 2: "medical research within the scientific paradigm would only be compatible with XAI techniques that are developed or justified within this paradigm."
- **Break condition:** If the user does not care about the "truth" of the explanation relative to external reality (e.g., only caring about user satisfaction), the alignment mechanism is irrelevant.

### Mechanism 2: Technical Detail as Philosophical Commitment
- **Claim:** Seemingly minor technical implementation details (e.g., the choice of a baseline) fundamentally alter the ontological category of an explanation, shifting it between paradigms.
- **Mechanism:** Technical detail as philosophical commitment. A shift from calculating absolute gradients (Positivist) to gradients relative to a baseline (Postmodern) changes the explanation from a statement of "what is" to "what differs."
- **Core assumption:** The definition of an explanation is intrinsic to the mathematical formulation, not just the output modality (e.g., a saliency map is not a universal format).
- **Evidence anchors:** Abstract: "seemingly small technical changes to an XAI method may correspond to important differences in the underlying assumptions." Table 1: Contrasts *Gradients* (Positivist) with *Integrated Gradients* (Postmodern), showing how the technical addition of a reference point shifts the paradigm.
- **Break condition:** If the technical variation is purely cosmetic or optimizes for the exact same objective function without changing the reference point of truth.

### Mechanism 3: Epistemological Decoding via Evaluation Metrics
- **Claim:** The evaluation methodology of an XAI paper reveals its epistemological stance (how we know the explanation is true), which must match the engineer's validation needs.
- **Mechanism:** Epistemological decoding via evaluation metrics. If a paper validates via "visual quality" or human agreement, it is Interpretivist; if it validates via axioms or theoretical constraints, it is Positivist/Realist.
- **Core assumption:** Validation is not universal; a method proven "correct" via human visual inspection (Interpretivist) may not be proven "correct" via causal utility (Realist).
- **Evidence anchors:** Page 1: Notes LRP experiments demonstrate visual quality, "indicating a constructivist epistemology." Page 5, Table 2: Maps "Evaluation" types to paradigms (e.g., "Independent" vs. "Ranking based on human data").
- **Break condition:** If an evaluation metric is used purely for benchmarking without claims of fundamental truth (e.g., speed tests).

## Foundational Learning

- **Concept:** **Ontology vs. Epistemology**
  - **Why needed here:** The paper's core taxonomic axis. You cannot classify an XAI method without distinguishing what it claims exists (Ontology: does the explanation exist independent of the observer?) from how we access it (Epistemology: can we know it fully or only through subjective lenses?).
  - **Quick check question:** Does this method assume the explanation is a physical fact of the model (Ontology) or a useful story for a human (Epistemology)?

- **Concept:** **Scientific Realism vs. Relativism**
  - **Why needed here:** Critical for high-stakes domains. The paper argues "Realist" methods are required for science/medicine, while "Relativist" (Postmodern) methods are incompatible because they reject absolute causes.
  - **Quick check question:** Does the explanation require a reference point (e.g., a baseline image) to exist? If yes, it is likely Relativist/Postmodern.

- **Concept:** **Paradigm Incommensurability**
  - **Why needed here:** Explains why you cannot simply compare all XAI methods on a single leaderboard. Methods from different paradigms (e.g., Positivist vs. Interpretivist) solve different problems and cannot be judged by the same metrics.
  - **Quick check question:** Are we judging this explanation by its mathematical fidelity (Positivist) or its user acceptance (Interpretivist)?

## Architecture Onboarding

- **Component map:** XAI Method Technical Specification -> Onto-Epistemological Analysis -> Paradigm Classification
- **Critical path:**
  1. Define the domain's requirements (Does it need causal truth or user interpretability?)
  2. Analyze the candidate XAI method's technical formulation (e.g., "Does it use a baseline?")
  3. Analyze the validation strategy (e.g., "Is it tested against human annotations?")
  4. Match the method's paradigm to the domain's paradigm

- **Design tradeoffs:**
  - **Positivist methods** (e.g., Gradients) offer mathematical purity but may be noisy/hard to interpret
  - **Interpretivist methods** (e.g., LRP, HINT) offer human-aligned explanations but rely on mental constructions rather than strict model mechanics
  - **Postmodern methods** (e.g., Integrated Gradients) offer robust relative comparisons but fail to provide absolute explanations for single inputs

- **Failure signatures:**
  - **Inter-paradigm contradiction:** Using Integrated Gradients (Postmodern/Relative) to explain a medical diagnosis that requires an absolute cause (Realist)
  - **Validation mismatch:** Using a visual metric to validate a method intended for causal discovery

- **First 3 experiments:**
  1. **Audit:** Select 3 XAI tools currently in your stack; classify them using Table 2 (Ontology/Epistemology indicators)
  2. **Stress Test:** Apply a Positivist method (e.g., Gradients) and a Postmodern method (e.g., Integrated Gradients) to the same input; document the contradiction in their narratives
  3. **Domain Fit:** Identify a target domain (e.g., Finance vs. Art); map which paradigms are strictly incompatible with the domain's regulatory or scientific needs

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the onto-epistemological classification of an XAI method evolve temporally as it undergoes validation by the research community in paradigms different from its origin?
- **Basis in paper:** [explicit] The Limitations section states that the analysis assumes a static world, but "XAI methods may be validated or justified in other paradigms... a more fine-grained classification might include temporal aspects."
- **Why unresolved:** The authors provide a static categorization based on original publications but acknowledge that subsequent community validation may shift a method's perceived paradigmatic alignment.
- **What evidence would resolve it:** A longitudinal bibliometric analysis tracking how the justification of specific methods (e.g., LIME) shifts across different domain literatures over time.

### Open Question 2
- **Question:** Can the specific risks of "inter-paradigm contradictions" be empirically quantified when applying postmodern or interpretivist XAI methods in realist domains like medicine?
- **Basis in paper:** [explicit] The Conclusion posits that scientific use of methods like DeepLIFT leads to "inter-paradigm contradictions" and calls for "studies such that attempt to confirm compatibility with their assumptions."
- **Why unresolved:** The paper theoretically maps the incompatibility but does not provide empirical case studies demonstrating the concrete failure modes or mistrust generated by these cross-paradigm applications.
- **What evidence would resolve it:** User studies in high-stakes fields (e.g., radiology) measuring error rates or cognitive dissonance when users are presented with explanations from mismatched paradigms.

### Open Question 3
- **Question:** How can the framework be adapted to classify XAI methods that are explicitly designed to be "multi-paradigmatic" or hybrid?
- **Basis in paper:** [explicit] The text notes that "a method could be originated somewhere in between" paradigms and that "methods could be developed in a multi-paradigmatic setup," yet the proposed framework forces distinct categorization.
- **Why unresolved:** The current visualization (Figure 1) and categorization rely on distinct clusters, lacking a mechanism for methods that intentionally blend, for example, positivist computation with interpretivist validation.
- **What evidence would resolve it:** A mathematical or conceptual model that maps XAI techniques onto a continuous spectrum rather than discrete ontological categories.

## Limitations
- Boundaries between paradigms are not always clear, creating potential ambiguity in classification
- Many XAI methods may embody hybrid assumptions that don't fit neatly into single paradigms
- Framework's practical utility depends heavily on correctly identifying an application domain's implicit paradigm

## Confidence
- **High confidence:** The identification of four distinct philosophical paradigms and their core ontological/epistemological assumptions is well-grounded in philosophical literature and consistently applied across examples
- **Medium confidence:** The specific categorization of individual XAI methods into paradigms, while methodologically rigorous, requires careful interpretation of technical details that may not always be explicitly stated in papers
- **Medium confidence:** The claim that scientific domains require realist methods is defensible but may oversimplify the diverse needs within scientific fields

## Next Checks
1. **Cross-paradigm contradiction test:** Apply methods from conflicting paradigms (e.g., Integrated Gradients vs. Gradients) to the same input in a scientific domain and document whether their explanations lead to contradictory conclusions
2. **Paradigm detection validation:** Analyze 10 additional XAI papers not mentioned in the original analysis, classify them using the framework, and compare results across multiple independent reviewers to assess inter-rater reliability
3. **Domain alignment case study:** Conduct interviews with domain experts (e.g., medical researchers) to explicitly elicit their implicit assumptions about explanation validity and map these to the proposed paradigm framework