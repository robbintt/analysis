---
ver: rpa2
title: 'Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution'
arxiv_id: '2508.21004'
source_url: https://arxiv.org/abs/2508.21004
tags:
- backdoor
- lethe
- dilution
- attacks
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LETHE is a novel backdoor defense for large language models that\
  \ eliminates malicious behaviors by combining internal and external knowledge dilution.\
  \ Internally, it trains a clean model on a small subset of benign data and merges\
  \ it with the backdoored model using SLERP to dilute backdoor effects in the model\u2019\
  s parameters."
---

# Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution

## Quick Facts
- **arXiv ID:** 2508.21004
- **Source URL:** https://arxiv.org/abs/2508.21004
- **Reference count:** 40
- **One-line primary result:** LETHE reduces attack success rates by up to 98% on backdoored LLMs while maintaining high clean data accuracy.

## Executive Summary
LETHE is a novel backdoor defense for large language models that eliminates malicious behaviors by combining internal and external knowledge dilution. Internally, it trains a clean model on a small subset of benign data and merges it with the backdoored model using SLERP to dilute backdoor effects in the model's parameters. Externally, it appends keyword explanations retrieved from WordNet to distract the model's attention from backdoor triggers. Tested on five LLMs across classification and generation tasks, LETHE reduces attack success rates by up to 98% and maintains high clean data accuracy. It outperforms eight state-of-the-art defenses and remains robust against adaptive attacks.

## Method Summary
LETHE purifies backdoored LLMs using a dual-mechanism approach. The internal mechanism trains a clean model on a small subset (5-10%) of benign data using LoRA, then merges it with the backdoored model via SLERP parameter interpolation to mathematically dilute backdoor effects. The external mechanism extracts keywords from input prompts using TextRank, retrieves definitions from WordNet, and appends them to distract the model's attention from triggers. This combined approach disrupts backdoor "shortcuts" by introducing conflicting benign knowledge while preserving the model's primary capabilities.

## Key Results
- Reduces attack success rates by up to 98% across five different LLMs
- Maintains high clean data accuracy while eliminating malicious behaviors
- Outperforms eight state-of-the-art backdoor defenses
- Remains robust against adaptive attacks designed to resist merging

## Why This Works (Mechanism)

### Mechanism 1: Internal Dilution via Parameter Merging
Merging a backdoored model with a clean model (trained on benign data) dilutes backdoor effects in the parameter space. A clean model is trained via LoRA on a small subset of benign data, then merged with the backdoored model using SLERP, which interpolates weights along a spherical arc to preserve geometric properties while neutralizing malicious pathways. The core assumption is that backdoor knowledge is distributed in parameter space such that it can be mathematically averaged out by benign task vectors.

### Mechanism 2: External Dilution via Semantic Distraction
Augmenting the input prompt with benign keyword definitions distracts the model's attention from backdoor triggers. Keywords are extracted from the input using TextRank, definitions are retrieved from WordNet, and appended to the prompt. This introduces semantically neutral evidence that shifts the model's attention away from trigger tokens, assuming the attention mechanism can prioritize added context over trigger patterns.

### Mechanism 3: Short-cut Interference
Backdoors act as "shortcuts" that map triggers to outputs; dilution disrupts these shortcuts by introducing conflicting knowledge. By combining internal (parameter) and external (input) dilution, LETHE creates knowledge conflict that prevents the model from simultaneously processing backdoor shortcuts and benign information effectively, causing the shortcut to fail.

## Foundational Learning

- **Concept: Model Merging (SLERP)**
  - **Why needed here:** Standard linear averaging of model weights often degrades performance. SLERP is required to merge clean and backdoored models while maintaining the spherical geometry of high-dimensional parameter space.
  - **Quick check question:** Why does SLERP preserve model capabilities better than linear weight averaging when combining two fine-tuned models?

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** LETHE requires training a clean model efficiently. LoRA allows freezing main model weights and only training small adapter matrices, making construction of the clean model computationally feasible.
  - **Quick check question:** If LoRA rank $r$ is too small, will the clean model capture enough task-specific knowledge to effectively dilute the backdoored model?

- **Concept: Trigger Types (Lexical vs. Semantic)**
  - **Why needed here:** LETHE uses TextRank (keyword extraction) for external dilution. This is highly effective for lexical triggers but requires different logic for semantic or triggerless attacks.
  - **Quick check question:** How would LETHE's external mechanism fail if the backdoor trigger is a specific syntactic structure rather than a keyword?

## Architecture Onboarding

- **Component map:** Clean Model Builder (LoRA fine-tuning) -> Model Merger (SLERP via MergeKit) -> Input Processor (TextRank + WordNet) -> Inference Wrapper (Prompt constructor)

- **Critical path:** Verify access to small set of guaranteed clean data -> Train Clean Model (LoRA) -> Merge Clean + Backdoored Model (SLERP) -> Enable External Dilution at inference time

- **Design tradeoffs:**
  - Internal vs. External: Internal dilution is more robust but requires compute for training; external dilution is zero-shot but less effective against strong attacks
  - Merge Strategy: SLERP is default; Passthrough allows layer mixing but is complex; TIES is aggressive but may drop accuracy
  - Evidence Type: WordNet is semantically neutral; non-neutral evidence degrades performance

- **Failure signatures:**
  - CDA Drop > 5%: SLERP interpolation parameter may be too high or clean dataset is too small/out-of-distribution
  - ASR remains high: Clean data may contain poisoned samples, or attack is adaptive and designed to resist merging

- **First 3 experiments:**
  1. Sanity Check (Internal Only): Apply internal dilution (SLERP) to backdoored GPT-2 on SST-2; vary clean data % (5% vs 10%) and measure ASR/CDA tradeoff
  2. Mechanism Validation (Ablation): Run LETHE on BadEdit attack; compare Internal Only, External Only, Both; confirm Internal is primary driver
  3. Stress Test (Triggerless): Test against DTBA (triggerless) attack on Chat dataset to see if external dilution helps without explicit tokens

## Open Questions the Paper Calls Out
- Can the knowledge dilution framework be effectively adapted for non-textual modalities like computer vision or speech recognition?
- Does the efficacy of internal dilution degrade as model parameter scale increases significantly beyond 13B models tested?
- Is a general lexical database (WordNet) sufficient for external dilution in highly specialized domains (medical, legal)?

## Limitations
- Reliance on a small set of guaranteed clean data - contamination causes internal dilution to fail
- External dilution mechanism has limited effectiveness against semantic or triggerless attacks
- Paper does not provide specific SLERP interpolation coefficients or detailed LoRA hyperparameters for faithful reproduction

## Confidence

- **High Confidence:** Internal dilution mechanism (SLERP model merging) is primary driver of defense with robust empirical support across multiple datasets and models
- **Medium Confidence:** External dilution mechanism (keyword definitions) provides supplementary defense but effectiveness is task and attack-dependent
- **Medium Confidence:** Core assumption that backdoors act as "brittle shortcuts" that can be disrupted by conflicting knowledge is plausible but needs more theoretical validation

## Next Checks

1. Sanity Check for SLERP Interpolation: Systematically vary SLERP interpolation coefficient $t$ (0.1, 0.3, 0.5, 0.7, 0.9) on backdoored GPT-2 model and measure ASR/CDA tradeoff to identify optimal value and confirm sensitivity

2. Robustness to Data Contamination: Intentionally poison "clean data" subset with 1-5% backdoor samples and measure degradation in LETHE's defense performance to validate critical dependency on data purity

3. Adaptive Attack Evaluation: Implement adaptive attack designed to resist model merging (e.g., attack that tries to "subtract" clean model's influence) and test LETHE's robustness to verify claims against sophisticated adversaries