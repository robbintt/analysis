---
ver: rpa2
title: Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient
  Descent
arxiv_id: '2512.07490'
source_url: https://arxiv.org/abs/2512.07490
tags:
- tensor
- apgd
- low-tubal-rank
- lemma
- recovery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the low-tubal-rank tensor estimation problem
  under the tensor singular value decomposition (t-SVD) framework, addressing the
  challenge that traditional approaches relying on t-SVD are computationally prohibitive
  for large-scale tensors. To overcome this, the authors propose an Alternating Preconditioned
  Gradient Descent (APGD) algorithm that factorizes the tensor into two smaller components
  and uses preconditioned gradient updates with alternating steps and a damping term
  to handle over-parameterization.
---

# Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent

## Quick Facts
- arXiv ID: 2512.07490
- Source URL: https://arxiv.org/abs/2512.07490
- Reference count: 40
- One-line primary result: APGD achieves linear convergence for low-tubal-rank tensor estimation under over-parameterization where existing methods fail

## Executive Summary
This paper addresses the computational challenge of low-tubal-rank tensor estimation under the t-SVD framework, where traditional approaches become prohibitively expensive for large-scale tensors. The authors propose an Alternating Preconditioned Gradient Descent (APGD) algorithm that factorizes tensors into smaller components and uses alternating preconditioned gradient updates with a damping term. The key innovation is achieving linear convergence even when the tensor rank is overestimated (over-parameterization), a regime where previous methods like Factorized Gradient Descent and ScaledGD fail or diverge. The convergence rate is theoretically proven to be independent of the tensor condition number, making it significantly more robust than existing approaches.

## Method Summary
APGD factorizes a tensor $X$ into $L * R^\top$ where $L$ and $R$ are smaller factor tensors, then updates them alternately using preconditioned gradients. The key innovation is adding damping to the preconditioning matrices to handle rank deficiency under over-parameterization. Each iteration updates $L$ first (with half-step), then uses the updated $L$ to update $R$, followed by a rebalancing step. The algorithm achieves linear convergence under restricted smoothness and strong convexity conditions, with a convergence rate independent of the tensor condition number. For tensor recovery from linear measurements, APGD leverages the tensor restricted isometry property (T-RIP) to establish convergence guarantees.

## Key Results
- APGD maintains linear convergence under over-parameterization (r > r*) where ScaledGD and FGD diverge or fail
- The convergence rate is independent of the tensor condition number, unlike ScaledGD whose Lipschitz constant is inversely proportional to $\sigma_{\min}^2$
- APGD is robust to initialization quality and step size choices, with theoretical guarantees covering a general class of low-tubal-rank tensor estimation problems
- Extensive simulations show APGD converges faster and more robustly than competing methods on both factorization and recovery tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Damped preconditioning preserves invertibility in over-parameterized regimes where standard ScaledGD diverges.
- Mechanism: APGD replaces the ScaledGD preconditioners $(L^\top * L)^{-1}$ and $(R^\top * R)^{-1}$ with $(L^\top * L + \lambda I)^{-1}$ and $(R^\top * R + \lambda I)^{-1}$. The damping term $\lambda I$ ensures these matrices remain positive definite even when $L^\top * L$ or $R^\top * R$ become rank-deficient (which occurs under over-parameterization or when the target tensor is not full tubal-rank).
- Core assumption: The damping parameter $\lambda_t$ satisfies $\lambda_t \leq \frac{\sqrt{2}}{L c_1^2}[f(X_t) - f(X^*)]^{1/2}$, or can be set to a sufficiently small fixed value.
- Evidence anchors:
  - [abstract] "adding a preconditioning term to the original gradient and updating these two factors alternately...achieves linear convergence even under over-parameterization"
  - [Section III-C, Lemma 2] Shows that ScaledGD's Lipschitz constant $L_P$ is inversely proportional to $\sigma_{\min}^2$, causing divergence when $\sigma_{\min} \to 0$ in over-parameterized settings
  - [corpus] Related work "Guaranteed Nonconvex Low-Rank Tensor Estimation via Scaled Gradient Descent" (arXiv:2501.01696) represents the ScaledGD baseline that fails in this regime
- Break condition: If $\lambda$ is too large, convergence slows; if the condition number is extreme and initialization is poor, the local convergence guarantee may not hold.

### Mechanism 2
- Claim: Alternating updates enable larger step sizes and reduce coupling between factor updates.
- Mechanism: APGD updates $L$ and $R$ in alternating steps rather than simultaneously. The half-step update $L_{t+1/2}$ is computed first, then used immediately for computing $R_{t+1}$. This decoupling prevents the cross-term interactions that would otherwise inflate the local Lipschitz constant, allowing step size $\eta \leq 1/L$ rather than $\eta \leq 1/L_P$.
- Core assumption: The objective function satisfies $(L, 2r)$-restricted smoothness, which bounds the local variation of gradients.
- Evidence anchors:
  - [Section I, Algorithm 1] Explicit alternating structure: update $L_{t+1/2}$ first, then use it for $R$ update
  - [Section III-D, Remark 3.7] "the Lipschitz constant of APGD is a fixed value and does not change during the iterations...the step size constraint for APGD is $\eta \leq 1/L$, while for ScaledGD it is $\eta \leq 1/L_P$"
  - [Figure 4] Demonstrates APGD converges with step sizes $\eta > 1$ while ScaledGD diverges at $\eta > 0.5$
- Break condition: If alternating order is reversed or updates are made simultaneous, the theoretical convergence rate may not hold.

### Mechanism 3
- Claim: Rebalancing maintains factor balance, which the proof requires but empirically appears optional.
- Mechanism: After each gradient step, Algorithm 2 (Rebalance) performs TQR decomposition on both factors, computes a t-SVD of $W_L * W_R^\top$, and redistributes singular values symmetrically. This ensures $L^\top * L = R^\top * R$, a property that the covariance-based analysis relies on.
- Core assumption: Assumption 1 requires initialization within $\rho \sigma_{\min}(\bar{X}^*)$ of ground truth; rebalancing helps maintain this proximity inductively.
- Evidence anchors:
  - [Algorithm 2] Complete rebalancing procedure using TQR and t-SVD
  - [Section V-C, Figure 7] "adding or removing the rebalancing step has little effect on the convergence rate of APGD...without the rebalancing step, the two factors remain balanced"
  - [corpus] No direct corpus evidence on rebalancing necessity; this appears to be a paper-specific finding
- Break condition: The proof requires rebalancing, but experiments suggest it may be optional in practice—an area for further investigation.

## Foundational Learning

- Concept: **t-SVD (Tensor Singular Value Decomposition)**
  - Why needed here: The entire algorithm operates under the t-SVD framework, where tensors are transformed to the Fourier domain and matrix SVD is applied to each frontal slice. Understanding t-product, tubal-rank, and multi-rank is essential.
  - Quick check question: Can you explain why tubal-rank equals the maximum entry of the multi-rank vector?

- Concept: **Restricted Strong Convexity and Smoothness**
  - Why needed here: The convergence proof relies on the loss function satisfying $(L, 2r)$-restricted smoothness and $(\mu, 2r)$-restricted strong convexity. These properties hold for tensor factorization, tensor recovery (with T-RIP), and 1-bit tensor recovery.
  - Quick check question: Given Lemma 1, what does $\frac{\mu}{2}\|X - X^*\|_F^2 \leq f(X) - f(X^*)$ imply for convergence analysis?

- Concept: **Burer-Monteiro Factorization for Tensors**
  - Why needed here: APGD factorizes $X \in \mathbb{R}^{n_1 \times n_2 \times n_3}$ into $L * R^\top$ with $L \in \mathbb{R}^{n_1 \times r \times n_3}$, $R \in \mathbb{R}^{n_2 \times r \times n_3}$, reducing computational complexity from $O(n_1 n_2 n_3 \log n_3 + (n_1 \vee n_2)(n_1 \wedge n_2)^2 n_3)$ to $O(r(n_1 \vee n_2)^2 n_3 + r n_3(n_1 \vee n_2) \log n_3)$.
  - Quick check question: Why does over-parameterization ($r > r^*$) not increase the per-iteration complexity order?

## Architecture Onboarding

- Component map: Input -> Initialization -> APGD loop (Preconditioned gradient for L -> Rebalance -> Preconditioned gradient for R -> Rebalance) -> Output
- Critical path:
  1. Ensure initialization satisfies $\|X_0 - X^*\|_F \leq \rho \sigma_{\min}(\bar{X}^*)$ via spectral initialization (Lemma 9)
  2. Set $\lambda_t = f(X_t)/10$ or a very small fixed value (e.g., $10^{-15}$)
  3. Use step size $\eta \leq 1/L$; for tensor recovery with T-RIP, $\eta \leq 1/(1 + \delta_{2r})$
- Design tradeoffs:
  - **Damping $\lambda$**: Larger $\lambda$ → slower but more stable convergence; smaller $\lambda$ → faster but may hit numerical issues. Paper recommends $\lambda_t = f(X_t)/10$ or fixed $10^{-15}$.
  - **With vs. without rebalancing**: Rebalancing adds $O(r^2(n_1 \vee n_2)n_3 + r^3 n_3)$ per iteration but is theoretically required; experiments suggest it may be optional.
  - **Estimated rank $r$**: Larger $r$ increases per-iteration cost linearly but provides more over-parameterization margin.
- Failure signatures:
  - **ScaledGD-style divergence**: If $(R^\top * R)^{-1}$ or $(L^\top * L)^{-1}$ are used without damping, algorithm diverges under over-parameterization (Figure 1).
  - **Stagnation at $10^{-10}$ error**: If fixed $\lambda$ is too large (e.g., $10^{-10}$), convergence slows once error drops below $\lambda$ threshold (Figure 6b).
  - **Initialization failure**: If spectral initialization quality bound is violated, the local convergence guarantee does not apply.
- First 3 experiments:
  1. **Reproduce Figure 2 (tensor factorization)**: Generate random tensor $X^* \in \mathbb{R}^{20 \times 20 \times 3}$ with tubal-rank $r^* = 10$, test exact-rank ($r = r^*$) and over-rank ($r = 2r^*$) cases with condition numbers $\kappa \in \{1, 100\}$. Verify APGD maintains linear convergence while ScaledGD diverges in over-parameterized settings.
  2. **Verify $\lambda$ robustness**: On tensor recovery task, test $\lambda \in \{f(X_t)/2, f(X_t)/10, 10^{-10}, 10^{-15}\}$. Confirm that very small fixed $\lambda$ or adaptive $\lambda_t = f(X_t)/10$ both work, but overly large fixed $\lambda$ causes stagnation.
  3. **Test rebalancing necessity**: Run APGD with and without rebalancing on both exact-rank and over-rank settings. Measure $\|L^\top * L - R^\top * R\|_F$ to verify the paper's claim that imbalance remains bounded even without explicit rebalancing.

## Open Questions the Paper Calls Out
- Can the theoretical convergence guarantees for APGD be extended from spectral initialization to arbitrary random initialization?
- Can the rebalancing step (Algorithm 2) be theoretically removed without sacrificing convergence guarantees?
- What is the optimal adaptive strategy for selecting the damping parameter $\lambda_t$ across iterations?
- How does APGD perform on real-world tensor estimation problems beyond synthetic data?

## Limitations
- The rebalancing step is theoretically required but empirically shown to have minimal impact on convergence
- Exact implementation details of TQR decomposition for tensors are not fully specified
- Theoretical analysis assumes specific initialization quality and restricted smoothness conditions that may not hold in all practical settings

## Confidence
- High: Core convergence claims under over-parameterization (Mechanism 1) and alternating update advantage (Mechanism 2)
- Medium: Rebalancing mechanism since experiments suggest it may be optional despite theoretical requirements
- Low: Real-world performance beyond synthetic data (not empirically validated in paper)

## Next Checks
1. Reproduce Figure 2: Test APGD on synthetic tensor factorization with varying condition numbers and exact/over-parameterized ranks to verify linear convergence where ScaledGD fails
2. Validate damping parameter robustness: Experiment with different λ values (adaptive vs fixed) on tensor recovery to confirm the claimed flexibility and identify failure modes
3. Test rebalancing necessity empirically: Run APGD with and without rebalancing on multiple problem instances, measuring factor imbalance and convergence speed to assess the practical impact