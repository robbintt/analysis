---
ver: rpa2
title: On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization
arxiv_id: '2505.18830'
source_url: https://arxiv.org/abs/2505.18830
tags:
- grpo
- responses
- nthr
- correct
- negative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies the effect of negative gradients in group-based
  reinforcement learning (GRPO), identifying a phenomenon called Lazy Likelihood Displacement
  (LLD) where correct answers' likelihoods marginally increase or even decrease during
  training. The source is traced to penalizing shared reasoning tokens in incorrect
  responses.
---

# On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization

## Quick Facts
- arXiv ID: 2505.18830
- Source URL: https://arxiv.org/abs/2505.18830
- Reference count: 40
- Primary result: NTHR reduces Lazy Likelihood Displacement in GRPO, improving math reasoning performance by 0.8–1.5% on benchmarks.

## Executive Summary
This work identifies a phenomenon called Lazy Likelihood Displacement (LLD) in Group Relative Policy Optimization (GRPO), where correct responses' likelihoods decrease or stagnate during training due to uniform penalization of all tokens in incorrect responses. The authors trace this to penalizing shared reasoning tokens between correct and incorrect responses. To address this, they introduce Negative Token Hidden Reward (NTHR), which selectively reduces penalties on tokens in wrong answers that harm correct response likelihood. Experiments on math reasoning benchmarks show NTHR consistently improves performance across models from 0.5B to 3B parameters, reducing LLD and yielding average performance gains of 0.8–1.5%.

## Method Summary
The method extends GRPO by adding a token-level selective penalization mechanism. During training, NTHR computes a score for each token in incorrect responses based on its embedding similarity to correct response tokens, weighted by prediction error. Tokens with scores above a threshold τ are selected for reduced penalization. The threshold is computed as β times the minimum average positive self-influence, with β=1 by default. Selected tokens have their advantages scaled by η=2|0.5-p| where p is the group success rate. This preserves useful negative gradients while protecting semantically correct portions of incorrect responses.

## Key Results
- NTHR reduces Lazy Likelihood Displacement, preventing correct response likelihoods from decreasing during GRPO training
- Average performance gains of 0.8–1.5% on math reasoning benchmarks (AIME24, AMC, MATH500, Minerva, OlympiadBench)
- Consistent improvements across model sizes from 0.5B to 3B parameters
- Computational overhead of ~15-25 seconds per training step

## Why This Works (Mechanism)

### Mechanism 1: LLD from Uniform Token Penalization
Uniform penalization of all tokens in incorrect responses causes correct response likelihoods to decrease or stagnate during GRPO training. When GRPO applies negative gradients to incorrect responses, it penalizes ALL tokens equally, including those that share semantic or structural features with correct responses. This inadvertent reduction of correct response likelihood occurs through gradient overlap.

### Mechanism 2: NTHR Identification via Hidden Embedding Similarity
The GWHES metric identifies which tokens in incorrect responses contribute most to LLD by measuring cross-response embedding similarity. NTHR computes a score using dot products of hidden embeddings between correct and incorrect response tokens, weighted by prediction error similarity. Tokens with high positive scores are those that, when penalized, most strongly reduce correct response likelihood.

### Mechanism 3: Selective Penalty Attenuation Preserves Useful Negative Signal
Reducing penalties on high-NTHR tokens mitigates LLD while maintaining beneficial negative gradients from truly incorrect tokens. NTHR selects tokens above threshold τ and scales their advantage by η < 1, preserving contrastive signal from "truly wrong" tokens while protecting semantically correct portions of incorrect responses.

## Foundational Learning

- **Policy Gradient Methods (GRPO/PPO)**: Why needed: GRPO is the base algorithm being modified; understanding advantage-weighted log-likelihood updates is essential. Quick check: Why does GRPO use group-relative advantages instead of absolute rewards?
- **Negative Gradient Dynamics in Preference Learning**: Why needed: The paper draws parallels to DPO's "likelihood displacement"; understanding this phenomenon provides context. Quick check: In DPO, what causes preferred response likelihoods to sometimes decrease during training?
- **Token-level vs Response-level Optimization**: Why needed: NTHR operates at token level; understanding how token advantages aggregate to response rewards is critical. Quick check: In standard GRPO with binary rewards, why does every token in an incorrect response receive the same advantage value?

## Architecture Onboarding

- **Component map**: Standard GRPO: Response generation → Reward computation → Advantage normalization → Policy update → NTHR additions: (1) Capture hidden embeddings from old policy forward, (2) Compute NTHR scores via Eq. 7, (3) Compute threshold τ from correct-response self-influence, (4) Select tokens V⁻ⱼ with s⁻ > τ, (5) Scale advantages by η → Proceed with modified policy update
- **Critical path**: 1. Generate G responses per prompt (standard) 2. Store hidden embeddings from old policy forward pass 3. After reward computation, compute NTHR scores for all incorrect-response tokens 4. Compute τ = β × min(average positive self-influence) 5. Apply selective penalization: Â⁻ = η × Â⁻ for selected tokens 6. Proceed with modified policy update
- **Design tradeoffs**: β (threshold scale): Higher β = fewer tokens selected = more conservative. Default β=1 works well. η (penalty scale): Paper uses η = 2|0.5 - p| where p is group success rate. Questions near 50% success get less penalty reduction. Overhead: ~5-10% added training time, dominated by data generation
- **Failure signatures**: All correct likelihoods still decreasing → β too low (selecting too few tokens). Performance below GRPO → η too aggressive (near 0), losing useful negative signal. No improvement → Hidden embeddings not captured correctly from old policy
- **First 3 experiments**: 1. Replicate Figure 1: Train GRPO vs Pos Only on per-sample basis, verify LLD exists in your setup 2. Ablation on β: Test {-∞, 0, 0.1, 1} following Table 4 to find optimal threshold for your task 3. Inspect NTHR token selection: Manually examine high-NTHR tokens (as in Figure 3) to confirm they align with correct reasoning steps in your domain

## Open Questions the Paper Calls Out

- **Open Question 1**: Does NTHR improve performance in non-mathematical domains such as code generation or general conversation? The authors note GRPO has been used successfully in "code generation... medical reasoning, and retrieval-augmented generation," but NTHR experiments are restricted to "math reasoning benchmarks." The identification of "influential tokens" relies on semantic overlap in reasoning steps. It is unclear if NTHR's mechanism transfers to domains where errors are structural (e.g., code syntax) rather than logical.

- **Open Question 2**: Can NTHR maintain its advantages in larger models (e.g., 7B+ parameters) where base capabilities are stronger? The paper validates NTHR on models "ranging from 0.5B to 3B parameters." While effective on smaller models, the dynamics of LLD and the utility of token-level penalization may differ as model capacity increases.

- **Open Question 3**: How does NTHR interact with the clipping and KL-regularization terms in the full GRPO objective? The theoretical analysis uses a simplified objective where "min(·, clip(·)) can be safely neglected" and "the KL term can be omitted." It is unknown if selectively modifying the advantage introduces instability when combined with PPO clipping or KL penalties.

## Limitations

- The paper focuses exclusively on LLD affecting correct responses, but doesn't address whether incorrect responses experience similar stagnation or displacement effects
- The mechanism assumes hidden embeddings capture semantic similarity reliably across different model architectures and domains
- The paper uses static thresholds (β=1, η based on group success rate) but doesn't explore adaptive selection criteria that might respond to training dynamics

## Confidence

- **High Confidence**: LLD exists as a measurable phenomenon in GRPO training; GRPO with uniform token penalization can reduce correct response likelihoods; NTHR provides measurable improvements on math reasoning benchmarks
- **Medium Confidence**: Hidden embeddings reliably capture semantic similarity for token-level penalization; the specific threshold β=1 and scaling factor η=2|0.5-p| are near-optimal; the mechanism applies broadly across different model sizes and reasoning domains
- **Low Confidence**: NTHR's effectiveness scales predictably to larger models; the computational overhead remains manageable at scale; the method generalizes beyond mathematical reasoning tasks

## Next Checks

1. Apply NTHR to non-mathematical reasoning tasks (e.g., commonsense reasoning, code generation) to verify the mechanism generalizes beyond the mathematical domain where it was developed
2. Test NTHR with different hidden layer depths (not just last layer) and different embedding normalization schemes to establish which aspects of the embedding similarity computation are essential versus incidental
3. Intentionally create edge cases where incorrect responses share minimal semantic overlap with correct responses, then verify whether NTHR still provides benefits or if the selective penalization becomes unnecessary