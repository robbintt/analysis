---
ver: rpa2
title: Typologically-Informed Candidate Reranking for LLM-based Translation into Low-Resource
  Languages
arxiv_id: '2602.01162'
source_url: https://arxiv.org/abs/2602.01162
tags:
- languages
- typological
- language
- divergence
- sinhala
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses systematic translation errors in LLMs when
  translating into low-resource, typologically divergent languages by proposing a
  framework that leverages linguistic typology for candidate reranking without parallel
  training data. The framework represents languages through structured profiles across
  16 typological dimensions, calculates divergence scores, and applies a dual-layer
  evaluation combining semantic constraints and typological compliance scoring.
---

# Typologically-Informed Candidate Reranking for LLM-based Translation into Low-Resource Languages

## Quick Facts
- arXiv ID: 2602.01162
- Source URL: https://arxiv.org/abs/2602.01162
- Reference count: 23
- Low-resource languages benefit from typological compliance reranking that outperforms standard metrics in structural correctness.

## Executive Summary
This paper addresses systematic translation errors in large language models when translating into low-resource, typologically divergent languages. The authors propose a framework that leverages linguistic typology to rerank candidate translations without requiring parallel training data. By representing languages through structured profiles across 16 typological dimensions and calculating divergence scores, the system applies a dual-layer evaluation combining semantic constraints and typological compliance scoring. Experimental results across nine language pairs demonstrate that intervention rates strongly correlate with typological distance, with intervention precision ranging from 28.15% to 86.26% depending on language type.

## Method Summary
The framework operates on N-best candidate lists generated by any LLM, applying a two-layer evaluation process. First, a semantic layer disambiguates polysemous words based on context. Second, a typological scorer evaluates candidates for structural compliance against 16 linguistic dimensions specific to the target language. The system calculates a weighted divergence vector between source and target language profiles to modulate intervention thresholds, then combines model scores with typological compliance using a mixing parameter α=0.5. Language profiles encode typological values, observable surface markers, and linguistic weights for each dimension, while dimension-specific scoring functions detect structural features through regex or morphological analysis.

## Key Results
- Intervention rates strongly correlate with typological distance (Pearson r=0.82, p<0.01)
- Intervention precision ranges from 28.15% (Sinhala) to 86.26% (Chinese) across nine languages
- Gain-Risk ratio >1.0 indicates net positive improvements for structurally profiled languages
- UMF compliance scores improve even when standard metrics (BLEU/COMET) show negative deltas

## Why This Works (Mechanism)

### Mechanism 1: Divergence-Gated Intervention
If source and target languages differ significantly in word order or morphology, the framework intervenes more frequently; if they are similar, it remains passive. The system calculates a 16-dimension divergence vector between language profiles, weights dimensions by salience (e.g., word order weight = 1.2), and normalizes to create a directive vector that modulates the intervention threshold. Assumes higher typological distance correlates with higher baseline LLM error rates detectable via surface-level structural markers. Evidence shows intervention rates correlate strongly with typological distance, though Japanese outlier suggests high baseline quality can override divergence signals.

### Mechanism 2: Decoupling Semantic Disambiguation from Structural Compliance
Improving translation for low-resource languages requires separating lexical sense selection from grammatical structural enforcement. Layer 1 (Semantic) uses context to boost/penalize tokens for polysemous words, while Layer 2 (Typological) evaluates resulting candidates for structural markers like case suffixes or verb-final position. The final score blends these using α × P_model + (1-α) × UMF. Assumes structural errors can be detected by counting surface markers without full parse trees. Evidence shows UMF improves structural correctness even when automatic metrics show negative deltas, indicating orthogonality to semantic similarity.

### Mechanism 3: Risk-Adjusted Candidate Selection
Filtering candidates based on Gain-Risk ratio allows the system to prioritize improvements where structural profiles are robust while acting conservatively where profiles are sparse. Evaluates candidates not just by compliance score but by ratio of "Correct Improvements" to "UMF Errors." High gain-risk ratios justify aggressive reranking; low ratios suggest under-specified profiles or scorers. Assumes native speaker judgment is ground truth and structural compliance is derivable from 16 dimensions. Evidence shows high gain-risk in structurally profiled languages (Chinese, Hindi) but low in morphologically dense languages (Sinhala, Tamil) due to sensitivity without resolution.

## Foundational Learning

- **Concept: Linguistic Typology (WALS)**
  - Why needed here: You cannot define 16-dimensional profiles or calculate divergence without understanding features like word order (SVO/SOV), morphological type (agglutinative/fusional), and case systems.
  - Quick check question: Does the target language use morphology (suffixes) to indicate grammatical relations, or word order?

- **Concept: Beam Search Generation**
  - Why needed here: The UMF framework is a reranker; it requires a candidate set (N-best list) from an LLM to function. It does not generate text itself.
  - Quick check question: If beam width is too narrow (N < 4), are we likely to find a candidate that satisfies complex structural constraints?

- **Concept: Intervention Precision vs. Gain-Risk**
  - Why needed here: Standard metrics (BLEU) fail to capture structural improvements. You must evaluate based on whether the system correctly fixes errors (Precision) without introducing new ones (Gain-Risk).
  - Quick check question: A system has 90% intervention precision but a Gain-Risk ratio of 0.5. Is it safe to deploy?

## Architecture Onboarding

- **Component map:** Profile Store -> Directive Engine -> Semantic Layer -> Typological Scorer -> Reranker
- **Critical path:** The Typological Scorer is the highest-risk component. If the regex or morphological analyzer fails to detect a case marker in a specific candidate, the compliance score will be 0, potentially discarding a correct translation.
- **Design tradeoffs:** Setting α=0.3 (trust UMF) yields higher structural compliance but risks "unnatural" outputs; α=0.7 (trust Model) is safer but may miss structural fixes. Hard-coding markers is brittle but fast; using morphological analyzers is robust but requires language-specific tools unavailable for many low-resource languages.
- **Failure signatures:** High Change / Low Gain-Risk (seen in Sinhala/Tamil) indicates the system aggressively changes output but often selects wrong variants—remedy by refining profile weights or increasing α. Over-normalization creates grammatically rigid but pragmatically odd output—remedy by lowering "Information Structure" dimension weights. Metric Disconnect (BLEU/COMET drop while UMF rises) is expected behavior; rely on human eval or UMF score.
- **First 3 experiments:** 1) Validate correlation by translating held-out sets for high-divergence (English-Sinhala) vs low-divergence (English-French) pairs and verify Change Rate differences. 2) Ablation study: disable Layer 1 (Semantic) and measure lexical error increase; disable Layer 2 (Typological) and measure structural error increase. 3) Alpha tuning: run sweep of α ∈ [0.3, 0.5, 0.7] on morphologically dense language to find sweet spot where Gain-Risk > 1.0.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does integrating UMF constraints during the generation phase (e.g., constrained decoding) yield higher structural compliance than post-hoc reranking?
- Basis in paper: Section 6.5 states, "Future research should explore integrating UMF constraints earlier in the generation process" to determine if this reduces the need for aggressive reranking.
- Why unresolved: The current study evaluates UMF exclusively as a post-generation reranking layer, leaving the efficacy of generation-time intervention untested.
- What evidence would resolve it: A comparison of translation quality and structural compliance between current reranking setup and setup where UMF constraints guide beam search or token selection directly.

### Open Question 2
- Question: Can modeling hierarchical dependencies between typological features reduce overcorrection errors in morphologically dense languages?
- Basis in paper: Section 6.1 notes current profiles treat features as independent signals and suggests modeling "feature dependencies such as case-verb agreement" to mitigate systematic overcorrection.
- Why unresolved: Current framework captures presence of features but lacks resolution to encode interactional constraints, leading to errors in languages with complex feature interdependencies.
- What evidence would resolve it: Experiments comparing current flat feature representation against hierarchical model in languages like Sinhala and Tamil, measuring reduction in case marking and argument structure errors.

### Open Question 3
- Question: Can language-specific intervention thresholds or confidence decay mechanisms improve the Gain-Risk Ratio for languages where framework currently shows high sensitivity but low specificity?
- Basis in paper: Section 6.2 calls for "language-specific intervention thresholds that modulate how willing UMF is to override baseline rankings" to address low precision in morphologically rich languages.
- Why unresolved: Current implementation uses fixed activation threshold and mixing parameter, leading to frequent interventions without proportional gains in certain language types.
- What evidence would resolve it: Ablation study varying intervention confidence threshold and mixing parameter α specifically for low Gain-Risk languages (Sinhala, Tamil) to identify settings where improvements outweigh errors.

### Open Question 4
- Question: Can an evaluation methodology be developed that correlates with UMF's typological compliance better than standard n-gram or embedding-based metrics?
- Basis in paper: Section 5.8 observes "systematic disconnect" where standard metrics often penalize UMF-selected outputs despite human preference, and Section 6.3 suggests separating stylistic from structural constraints.
- Why unresolved: Existing metrics like BLEU and COMET rely on surface similarity and fail to detect linguistically salient structural improvements, making it difficult to automatically validate UMF's impact.
- What evidence would resolve it: Creation and validation of "Typological Compliance Score" that correlates strongly with human expert judgments of structural correctness across diverse language pairs.

## Limitations

- **Profile Completeness**: System effectiveness depends on comprehensive 16-dimensional profiles, but low-resource languages may lack sufficient documentation for all dimensions, creating under-specified profiles.
- **Marker Generalization**: Reliance on surface-level marker detection without full parse trees may fail with morphological fusion, orthographic variation, or unmarked correct structures.
- **LLM Candidate Quality**: Framework is limited by base LLM's N-best list quality—if all candidates share same structural flaw, reranker cannot select correct option.

## Confidence

- **High Confidence**: Correlation between intervention rates and typological distance (Pearson r=0.82, p<0.01) is statistically robust and aligns with established LLM performance degradation with linguistic distance.
- **Medium Confidence**: Dual-layer architecture shows effectiveness in specific cases, but generalizability across language types shows variability with Gain-Risk ratio revealing context-dependent performance.
- **Low Confidence**: Specific dimension weights, surface marker detection rules, and language profiles are not fully specified; Gain-Risk mechanism lacks sufficient external validation and calculation methodology is unclear.

## Next Checks

1. **Profile Completeness Audit**: Conduct systematic review of 16-dimensional profiles for each target language to identify missing or under-specified dimensions, particularly for morphologically dense and low-resource languages. Document which dimensions have complete surface marker specifications versus those that are approximated or omitted.

2. **Marker Detection Robustness Test**: Implement controlled experiment varying morphological analyzer/regular expression engine to test sensitivity to orthographic variation, spelling errors, and morphological fusion. Measure how these variations affect UMF compliance scores and identify failure patterns in surface marker detection.

3. **Candidate Generation Analysis**: For high-divergence language pairs showing low Gain-Risk ratios, analyze N-best lists to determine whether structural variation exists across candidates. If all candidates share same structural flaw, this indicates base LLM's beam search or training data limitations rather than reranker inadequacy.