---
ver: rpa2
title: Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient Pruning
arxiv_id: '2501.05248'
source_url: https://arxiv.org/abs/2501.05248
tags:
- sub-models
- pruning
- arxiv
- https
- available
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient Pruning

## Quick Facts
- **arXiv ID:** 2501.05248
- **Source URL:** https://arxiv.org/abs/2501.05248
- **Reference count:** 40
- **Primary result:** 50% unstructured pruning of CodeLlama-7B-Instruct achieves ~53% Pass@10 on HumanEval vs. 60.5% baseline

## Executive Summary
This paper demonstrates how to extract domain-specific sub-models from foundational LLMs using resource-efficient unstructured pruning. The authors apply Wanda pruning at 50% sparsity to CodeLlama-7B/13B-Instruct, Mistral-7B-v0.1, and Gemma-1.1-7B using only 128 random calibration samples per domain (Python, Java, C++, JavaScript). The pruned sub-models retain significant coding capabilities with minimal accuracy loss, showing that domain-specific knowledge can be efficiently extracted without retraining. The approach reveals distinct weight overlap patterns between languages, with C++ and Java showing higher similarity than either does with Python.

## Method Summary
The paper applies Wanda pruning (Pruning by Weights and activations) to extract language-specific sub-models from foundation LLMs. The method uses 128 random samples from domain-specific datasets as calibration data to identify weights to prune, achieving 50% unstructured sparsity without any retraining. The pruned models are evaluated on HumanEval using Pass@10 metric for code generation quality. Weight overlap analysis is performed using Jaccard distance to quantify the similarity between pruned sub-models across different programming languages.

## Key Results
- 50% unstructured pruning of CodeLlama-7B-Instruct retains ~53% Pass@10 on HumanEval (vs. 60.5% baseline)
- Gemma-1.1-7B shows superior accuracy retention post-pruning compared to CodeLlama and Mistral
- Weight overlap analysis reveals C++ and Java sub-models are more similar to each other than to Python sub-models

## Why This Works (Mechanism)
Domain-specific pruning works by identifying weights that are critical for general tasks versus those specific to a particular domain. The Wanda pruning algorithm uses calibration data to determine which weights can be removed while preserving domain-specific capabilities. The calibration samples guide the pruning process to retain weights important for the target domain (e.g., Python code generation) while removing those less relevant, effectively creating a specialized sub-model that maintains high performance on domain-specific tasks despite significant sparsity.

## Foundational Learning

**Unstructured pruning**: Removing individual weights based on magnitude rather than structured patterns (like entire channels). *Why needed*: Enables higher sparsity levels while maintaining model accuracy. *Quick check*: Verify that sparsity level is 50% and no retraining is performed.

**Wanda pruning algorithm**: Uses both weight magnitudes and activation patterns during calibration to determine which weights to prune. *Why needed*: More effective than magnitude-based pruning alone for preserving domain-specific capabilities. *Quick check*: Confirm calibration data is used to guide pruning decisions.

**Jaccard distance for weight overlap**: Measures similarity between sets of remaining weights after pruning different models. *Why needed*: Quantifies how much domain-specific knowledge overlaps across different programming languages. *Quick check*: Verify that Jaccard distance values are reported between sub-models.

## Architecture Onboarding

**Component map**: Foundation LLM -> Wanda pruning algorithm -> 128 calibration samples -> 50% sparse sub-model -> HumanEval evaluation

**Critical path**: The most critical sequence is calibration data selection → pruning mask generation → model evaluation. The quality and representativeness of calibration samples directly impacts pruning effectiveness.

**Design tradeoffs**: The paper chooses unstructured pruning over structured pruning to achieve higher sparsity (50%) while maintaining better accuracy retention. The tradeoff is that unstructured sparsity may not yield computational speedups on all hardware.

**Failure signatures**: Significant accuracy drops (<40% Pass@10) indicate calibration data formatting issues or poor sample selection. Out-of-memory errors suggest batch size needs reduction during calibration.

**3 first experiments**:
1. Verify pruning pipeline works by applying to smaller model (7B) before attempting 13B
2. Test different prompt templates (Llama2 vs Llama3 format) on calibration data
3. Evaluate sensitivity to calibration set size by varying from 32 to 256 samples

## Open Questions the Paper Calls Out
1. Can a dynamic inference system effectively select and compose multiple language-specific sub-models at runtime?
2. Does the relationship between training diversity and pruning robustness generalize across model architectures?
3. What is the minimum calibration set size required for effective domain-specific sub-model extraction?
4. Can sub-models be specialized for sub-tasks within code generation (e.g., code translation) with higher accuracy retention?

## Limitations
- Lack of publicly available calibration data indices prevents exact replication
- No comparative analysis with alternative pruning methods
- Fixed calibration set size (128 samples) without sensitivity analysis

## Confidence
**High confidence** in core technical approach - method description is detailed enough for reproduction
**Medium confidence** in quantitative results - methodology reproducible but exact numbers uncertain without calibration data
**Low confidence** in broader claims - limited comparative analysis and architectural diversity

## Next Checks
1. Test different prompt templates (Llama2 vs Llama3 instruct formats) on calibration data to verify formatting impact
2. Systematically vary calibration set size (32, 64, 128, 256 samples) to determine optimal size
3. Document exact pruning masks produced to enable verification against future code releases