---
ver: rpa2
title: Understanding or Memorizing? A Case Study of German Definite Articles in Language
  Models
arxiv_id: '2601.09313'
source_url: https://arxiv.org/abs/2601.09313
tags:
- article
- gfem
- neut
- masc
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study examines whether German definite articles in language\
  \ models reflect abstract grammatical rules or surface-level memorization. Using\
  \ GRADIEND, a gradient-based interpretability method, the authors learn parameter\
  \ update directions for specific article transitions (e.g., der\u2192die) across\
  \ different gender-case combinations."
---

# Understanding or Memorizing? A Case Study of German Definite Articles in Language Models

## Quick Facts
- **arXiv ID:** 2601.09313
- **Source URL:** https://arxiv.org/abs/2601.09313
- **Reference count:** 40
- **Primary result:** GRADIEND interventions reveal German definite articles in LMs rely partly on memorized associations rather than abstract grammatical rules

## Executive Summary
This study investigates whether German definite articles in language models reflect abstract grammatical rules or surface-level memorization. Using GRADIEND, a gradient-based interpretability method, the authors learn parameter update directions for specific article transitions (e.g., der→die) across different gender-case combinations. They find that applying these learned updates affects article probabilities not just in the trained cell but also in unrelated gender-case settings, with substantial overlap among the most affected neurons across settings. While some generalization along grammatical dimensions is observed, the results also show spillover effects that contradict a purely rule-based encoding. This indicates that models at least partly rely on memorized associations rather than abstract grammatical rules.

## Method Summary
The authors use GRADIEND to learn parameter update directions for German definite article transitions across the gender-case paradigm. They train 12 separate GRADIEND models, each focused on transitions within a specific gender-case cell (e.g., MASC,NOM der→die). For each model, they construct swapped target pairs for the two articles in that cell and identity pairs for all other cells. The encoder-decoder learns to map gradient information from alternative targets to parameter update directions. After training, they apply these learned updates at optimal strengths (selected to preserve 99% of model performance on a grammar-neutral dataset) and measure how probabilities shift across all gender-case cells. They also analyze parameter overlap by comparing Top-k weights across different GRADIEND variants.

## Key Results
- GRADIEND interventions produce spillover effects to unrelated gender-case cells sharing surface articles, contradicting pure rule-based encoding
- Article transition updates affect multiple gender-case settings beyond the trained cell, with deviations from clean grammar-preserving predictions
- High Top-k weight overlap (>75%) across GRADIEND variants within article groups indicates shared neural substrates, compared to 38.9% for control groups
- Some generalization along grammatical dimensions occurs, but models show mixed encoding strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parameter update directions encode specific article transitions that can be learned and applied to shift model behavior.
- Mechanism: GRADIEND learns a bottleneck encoder-decoder that compresses gradient information (∇in_Wm) into a scalar feature h∈[-1,1], then decodes it into a parameter-space update direction. The encoder uses tanh(W^T_e·∇in + b_e), and the decoder reconstructs as h·W_d + b_d.
- Core assumption: Gradient differences between factual and alternative targets capture meaningful transition-specific information that generalizes beyond training examples.
- Evidence anchors:
  - [abstract]: "Using GRADIEND, a gradient-based interpretability method, we learn parameter update directions for gender-case specific article transitions."
  - [Section 3.4]: "GRADIEND learns a bottleneck encoder-decoder f=dec∘enc that maps gradient information to a single scalar feature and decodes it into a parameter-space update direction."
  - [corpus]: Ferrando and Costa-jussà (2024) find similar circuit structures across languages for agreement tasks, supporting gradient-based probing validity.
- Break condition: If encoded values h do not consistently separate the two targeted transition directions (correlations <50%), the mechanism fails.

### Mechanism 2
- Claim: Article transition updates produce spillover effects to unrelated gender-case cells that share surface articles.
- Mechanism: Updates trained on one cell (e.g., MASC,NOM der→die) affect other cells sharing the source article (e.g., FEM,DAT der→die) even when grammatically unrelated, indicating shared parameter subspaces rather than clean rule-based encoding.
- Core assumption: Rule-based encoding would produce either local effects (trained cell only) or systematic grammatical generalization; surface-level memorization produces article-linked spillover.
- Evidence anchors:
  - [abstract]: "updates learned for a specific gender-case article transition frequently affect unrelated gender-case settings."
  - [Section 5.3]: "The heatmap partially matches GR, but with deviations: some GR-predicted cells are neutral or opposite-signed... several effects appear in cells that are not predicted by a clean grammar-preserving rule."
  - [corpus]: Limited direct corpus evidence; related work (Weissweiler et al., 2023) shows uneven morphological generalization, supporting non-uniform rule acquisition.
- Break condition: If updates produce only Local Rule effects (trained cell only) with zero spillover to cells sharing articles, this mechanism fails.

### Mechanism 3
- Claim: High Top-k weight overlap across GRADIEND variants within article groups indicates shared neural substrates for article transitions.
- Mechanism: Different transitions involving the same article pair (e.g., der↔die across multiple gender-case contexts) rely on overlapping parameter subsets rather than implementing disjoint mechanisms.
- Core assumption: Decoder weights W_d ranked by absolute value identify functionally important parameters for each transition.
- Evidence anchors:
  - [abstract]: "substantial overlap among the most affected neurons across settings."
  - [Section 5.4]: "Groups with at least two variants realizing the same surface article pair show consistently high overlap (>75% on average), whereas the control group is much lower (mean 38.9%)."
  - [corpus]: Brinkmann et al. (2025) identify multilingual SAE features for morphosyntactic concepts, supporting shared feature directions.
- Break condition: If control groups (disjoint articles) show comparable overlap to article groups, the mechanism fails.

## Foundational Learning

- Concept: German definite article paradigm (gender × case)
  - Why needed here: The paper exploits syncretism in this 3×4 paradigm to test rule-based vs. memorized encoding.
  - Quick check question: Why does "der" appear in both (MASC,NOM) and (FEM,DAT), and how does this ambiguity enable the experimental design?

- Concept: Gradient-based intervention mechanics
  - Why needed here: GRADIEND relies on gradient differences between factual and alternative targets to learn update directions.
  - Quick check question: What does ∇∆_Wm := ∇F_Wm − ∇A_Wm represent, and why use alternative gradients as input rather than factual gradients?

- Concept: Syncretism in morphological systems
  - Why needed here: Syncretism (same surface form, different grammatical functions) creates controlled ambiguity essential for distinguishing rule-based from surface-level encoding.
  - Quick check question: If articles were uniquely determined by (gender, case) with no syncretism, how would the experimental design change?

## Architecture Onboarding

- Component map:
  - **GRADIEND encoder**: Linear projection + tanh bottleneck producing scalar h∈[-1,1]
  - **GRADIEND decoder**: Linear reconstruction producing update direction in parameter space
  - **Base LM**: Transformer encoder/decoder (GermanBERT, GBERT, ModernGBERT, EuroBERT, GermanGPT-2, LLaMA)
  - **Article classifier head**: For decoder-only models, custom 6-way classifier enabling bidirectional prediction
  - **Intervention loop**: Apply scaled update Wm + α·dec(h*), evaluate probability shifts

- Critical path:
  1. Construct gender-case datasets via spaCy filtering (12 datasets: D^MASC_NOM through D^NEUT_GEN)
  2. Train GRADIEND with swapped targets for transition pair + identity pairs for all other cells
  3. Select intervention strength α* under 99% LMS preservation constraint
  4. Measure ∆P(article) across all 12 gender-case cells
  5. Extract Top-k weights, compute pairwise overlaps across variants

- Design tradeoffs:
  - **LMS preservation threshold (99%)**: Conservative choice limits effect sizes to ~1% but ensures changes reflect targeted manipulation, not degradation
  - **Single-scalar bottleneck**: Restricts representational capacity but ensures interpretability and prevents overfitting
  - **Alternative gradient input**: Stabilizes training when factual predictions are near-confident but departs from original GRADIEND formulation
  - **k=1000 for Top-k analysis**: Chosen empirically to capture transition-dominated regime before dilution

- Failure signatures:
  - Encoded values h clustering near zero for targeted transitions → encoder failed to learn discriminative features
  - Significant ∆P on D_NEUTRAL → model degradation rather than targeted intervention
  - Low validation correlation (<50%) → GRADIEND not learning meaningful gradient reconstruction
  - SuperGLEBer scores dropping >1% → excessive model perturbation

- First 3 experiments:
  1. **Sanity check**: Train G^FEM,MASC_NOM on GermanBERT, verify Figure 3 distribution: targeted transitions map to opposite h signs, identity pairs cluster near zero, D_NEUTRAL near zero.
  2. **LMS-α calibration**: For G^FEM_NOM,DAT der→die, sweep α∈[0.001, 100], plot P(die) vs. LMS on D_NEUTRAL, identify candidate range and α* per Figure 5 protocol.
  3. **Control group validation**: Train the four control variants (disjoint articles: G^FEM_ACC,DAT, G^NEUT_ACC,DAT, G^FEM,NEUT_ACC, G^FEM,NEUT_DAT), verify Top-1000 overlap ≈38% vs. >75% for article groups.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do larger language models encode German articles in a more rule-based manner, with reduced surface-level spillover effects?
- Basis in paper: [explicit] The authors note "a tentative size trend: larger models (LLaMA) show less spillover" but state that "scaling trends cannot be confidently concluded from our setup" given their limited model range (up to ~3B parameters).
- Why unresolved: Only six models were evaluated, with limited scale diversity, preventing confident conclusions about scaling behavior.
- What evidence would resolve it: Systematic evaluation across a broader range of model sizes (e.g., 7B, 13B, 70B parameters) using the same GRADIEND methodology.

### Open Question 2
- Question: Do similar memorization-versus-rule patterns hold for other morphologically complex phenomena (adjective agreement, verb inflection) in German or other languages?
- Basis in paper: [explicit] Limitation states: "the findings may not transfer to other morphosyntactic phenomena (e.g., adjective agreement, verb inflection, or freer word order) or to other languages."
- Why unresolved: The study focused exclusively on German definite singular articles, a small closed-class system.
- What evidence would resolve it: Applying GRADIEND-based intervention analysis to other morphosyntactic paradigms across typologically diverse languages.

### Open Question 3
- Question: What downstream effects do article-transition interventions have on generated text quality and grammaticality in open-ended generation?
- Basis in paper: [explicit] Limitation states: "our evaluation focuses on controlled probability shifts and parameter overlap rather than downstream generation behavior."
- Why unresolved: Intervention effects were measured only via masked probability changes, not through actual text generation.
- What evidence would resolve it: Evaluating GRADIEND-modified models on generation tasks with grammaticality assessment of outputs.

### Open Question 4
- Question: Do highly distributed or context-specific rule implementations exist that cannot be captured by single-direction gradient updates?
- Basis in paper: [inferred] The authors acknowledge GRADIEND interventions are "restricted to a single update direction scaled by a scalar α," so "more distributed or highly context-specific rule implementations may not be captured by this probe."
- Why unresolved: The methodology may be blind to mechanisms that require multi-dimensional or context-conditional parameter updates.
- What evidence would resolve it: Multi-directional or activation-space interventions (e.g., SAE-based methods) on the same article prediction task.

## Limitations
- Cross-model generalization remains unclear due to limited quantitative analysis across the six tested models
- Syncretism assumption may oversimplify by ignoring potential abstract morphological features linking syncretic forms
- Top-k overlap methodology uses arbitrary thresholds (k=1000, 75% cutoff) without sensitivity analysis

## Confidence
- **High Confidence (B2):** The empirical finding that GRADIEND interventions produce spillover effects to unrelated gender-case cells sharing surface articles is robustly demonstrated across multiple model architectures
- **Medium Confidence (B2):** The conclusion that models rely partly on memorization rather than pure rule-based encoding follows logically from the spillover evidence
- **Low Confidence (B2):** The interpretation of Top-k weight overlap as evidence for shared neural substrates is the weakest claim given arbitrary thresholds

## Next Checks
1. **Architecture ablation study:** Systematically compare GRADIEND intervention effects across all six model architectures using identical hyperparameters, quantifying how encoder-only, decoder-only, and multilingual models differ in their spillover patterns and Top-k overlap distributions

2. **Control parameter overlap baseline:** Generate random parameter subsets of size 1000 from non-transition weights and compute their overlap distribution. Compare this baseline to article group overlaps to determine whether observed high overlap reflects meaningful functional similarity or arbitrary parameter selection

3. **Multi-scalar bottleneck experiment:** Replicate key experiments using 2-3 dimensional bottlenecks instead of 1D. Assess whether increased representational capacity changes spillover patterns, Top-k overlap values, or the fundamental conclusion about rule-based versus memorized encoding