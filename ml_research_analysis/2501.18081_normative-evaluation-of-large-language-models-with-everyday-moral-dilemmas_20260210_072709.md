---
ver: rpa2
title: Normative Evaluation of Large Language Models with Everyday Moral Dilemmas
arxiv_id: '2501.18081'
source_url: https://arxiv.org/abs/2501.18081
tags:
- moral
- llms
- arxiv
- each
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated seven large language models (LLMs) on over
  10,000 real-world moral dilemmas from Reddit's AITA community, comparing their moral
  judgments and reasoning to human responses. LLMs showed significant variation in
  their verdict distributions and low inter-model agreement, though individual models
  demonstrated moderate to high self-consistency.
---

# Normative Evaluation of Large Language Models with Everyday Moral Dilemmas

## Quick Facts
- arXiv ID: 2501.18081
- Source URL: https://arxiv.org/abs/2501.18081
- Reference count: 40
- Primary result: Seven LLMs evaluated on 10,826 real-world moral dilemmas showed low inter-model agreement but high self-consistency, with ensemble aggregation approximating human consensus

## Executive Summary
This study systematically evaluates seven large language models on over 10,000 real-world moral dilemmas from Reddit's AITA community, comparing their moral judgments and reasoning to human responses. The research reveals significant variation in verdict distributions across models, with individual models demonstrating moderate to high self-consistency but low inter-model agreement. Despite these inconsistencies, the ensemble of diverse models collectively approximates human consensus. The analysis of model explanations uncovers distinct patterns in moral principle invocation, with some models showing greater sensitivity to specific themes like fairness or harm.

## Method Summary
The study evaluates seven LLMs (GPT-3.5, GPT-4, Claude Haiku, PaLM 2 Bison, Llama 2 7B, Mistral 7B, Gemma 7B) on 10,826 AITA submissions using a standardized system prompt requesting structured verdicts and reasoning. Each model is prompted three times per submission at default hyperparameters with low temperature. Verdict distributions are compared using Krippendorff's alpha for agreement analysis, while moral theme detection employs six fine-tuned RoBERTa classifiers. The ensemble aggregation method compares majority-voted LLM verdicts against top-voted human comments as ground truth.

## Key Results
- Individual models showed moderate to high self-consistency (GPT-4 α=0.85, Claude α=0.89) but low inter-model agreement
- Ensemble of diverse LLMs collectively approximated human consensus despite individual inconsistencies
- Model explanations revealed distinct patterns in moral principle invocation, with Fairness and Harm showing systematic impacts on verdict assignment
- Open-source models (Llama 2 α=0.37, Gemma α=0.41) demonstrated significantly lower consistency than proprietary models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit moderate to high self-consistency but low inter-model agreement because moral reasoning crystallizes differently across training and alignment processes.
- Mechanism: Each model develops a distinct internal representation of normative judgment. When repeatedly prompted, a model samples from its own stable distribution (self-consistency), but that distribution differs from other models due to variations in training data, architecture, and alignment objectives.
- Core assumption: Repeated runs at low temperature reveal a model's dominant response distribution rather than generating fundamentally new reasoning patterns.
- Evidence anchors:
  - [abstract] "LLMs showed significant variation in their verdict distributions and low inter-model agreement, though individual models demonstrated moderate to high self-consistency."
  - [section 4.1.3] "Model self-consistencies generally exceeded inter-model agreement. For example, GPT-4 and Claude achieved α values of 0.85 and 0.89, respectively... considerably greater than the inter-model agreement."
  - [corpus] Related work on procedural moral reasoning (MoReBench) supports that models may reach similar outcomes via different reasoning paths, reinforcing the distinction between outcome agreement and reasoning alignment.
- Break condition: If models were re-aligned toward a single unified moral framework, inter-model agreement would increase while ensemble diversity would diminish.

### Mechanism 2
- Claim: Moral theme invocation in model explanations correlates systematically with verdict assignment, revealing model-specific sensitivity to principles like Fairness or Harm.
- Mechanism: Post-hoc explanations generated by models can be classified into moral themes; the presence of certain themes (e.g., Fairness) shifts verdict probabilities, suggesting the theme plays a causal role in the model's decision logic.
- Core assumption: Generated explanations faithfully reflect internal decision weighting, and the RoBERTa classifiers accurately capture moral themes.
- Evidence anchors:
  - [abstract] "Analysis of model explanations revealed distinct patterns in how models invoked different moral principles, with some showing greater sensitivity to specific themes like fairness or harm."
  - [section 4.4] "We observed striking variations in prevalence differences across models and moral themes. For example, Fairness had a significant impact on model verdict... Feelings saw the largest prevalence differences among models: when used, Redditors, GPT-3.5, GPT-4, and Claude were much less likely—up to nearly 30% for Claude—to assign NTA."
  - [corpus] Corpus evidence for the specific theme-to-verdict mechanism is weak; related work focuses on moral alignment broadly rather than this fine-grained causal linkage.
- Break condition: If chain-of-thought prompting alters explanation structure substantially, the observed theme-to-verdict correlations could weaken or shift.

### Mechanism 3
- Claim: An ensemble of diverse LLMs approximates human consensus by averaging out individual model biases.
- Mechanism: Aggregating judgments across models with different biases smooths extreme outputs (e.g., Llama 2's consistent YTA labeling), pushing the ensemble toward the statistical center of human judgments.
- Core assumption: The Reddit AITA "top comment" represents a stable, meaningful proxy for human consensus.
- Evidence anchors:
  - [abstract] "The ensemble of LLMs collectively approximated human consensus despite individual inconsistencies."
  - [section 4.2] "We found that for the NTA and YTA labels, the average label rate increases as more models vote for the corresponding label... This suggests that LLM agreement correlates with Redditor consensus."
  - [corpus] "The Pluralistic Moral Gap" paper directly supports that individual models are misaligned with human judgments and that aggregation or multi-step methods can help bridge this gap.
- Break condition: If all ensemble members share a common ideological bias (e.g., WEIRD demographic skew in training data), the ensemble average will also be biased and fail to approximate diverse human consensus.

## Foundational Learning

- Concept: **Krippendorff's Alpha (Inter-Annotator Agreement)**
  - Why needed here: This metric is used throughout the paper to quantify both self-consistency and inter-model agreement, making it essential for interpreting reliability results.
  - Quick check question: If two models each evaluate the same dilemmas and achieve a Krippendorff's alpha of -0.5, what does this indicate about their agreement pattern?

- Concept: **Moral Foundations Theory**
  - Why needed here: The paper's qualitative analysis is built on six moral themes derived from this theory (Fairness, Harm, Feelings, etc.), providing the vocabulary for reasoning analysis.
  - Quick check question: According to the paper's findings, which moral theme is most strongly associated with a "Not the Asshole" (NTA) verdict across models?

- Concept: **TF-IDF (Term Frequency-Inverse Document Frequency)**
  - Why needed here: TF-IDF is used to measure linguistic similarity between model explanations, enabling comparison beyond simple verdict matching.
  - Quick check question: Why might two models achieve low cosine similarity in TF-IDF representations even when they assign the same verdict?

## Architecture Onboarding

- Component map: Reddit AITA submissions -> Standardized system prompt -> Seven LLM ensemble (GPT-3.5, GPT-4, Claude Haiku, PaLM 2 Bison, Llama 2 7B, Mistral 7B, Gemma 7B) -> Structured output parser -> Parallel quantitative (Krippendorff's alpha) and qualitative (RoBERTa classification, TF-IDF) analysis -> Ensemble aggregation vs. human consensus

- Critical path: Standardized prompting → structured output extraction → parallel quantitative (agreement) and qualitative (reasoning) analysis → ensemble aggregation vs. human consensus

- Design tradeoffs:
  - Standardized prompt ensures fair comparison but may not reflect model-specific conversational optimizations
  - Automated RoBERTa classification enables scale but may miss nuanced theme interpretations
  - Reddit AITA "top comment" as ground truth is scalable but introduces demographic bias (young, Western, male-skewing)

- Failure signatures:
  - **Refusal/Unsafe**: Model declines to answer, requiring re-run or "INFO" assignment
  - **Format deviation**: Output lacks required structure, breaking automated parsing
  - **Semantic drift**: Model interprets labels literally (e.g., Mistral's focus on "asshole" as a descriptor rather than a fault assignment)

- First 3 experiments:
  1. Run a baseline consistency test: prompt all target models on 50 held-out AITA posts (3 runs each), calculate self-consistency (Krippendorff's alpha), and verify alignment with paper-reported ranges before scaling
  2. Build and validate the output parser: develop a script to extract `Verdict` and `Reasoning` from raw responses with robust error handling; test on 100 manually reviewed outputs
  3. Pilot qualitative analysis: fine-tune a RoBERTa classifier for the "Fairness" theme on an external dataset, apply to GPT-4 explanations, and manually inspect 20 classifications to validate theme capture accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- The study relies on Reddit AITA data, which may not represent global moral perspectives and could bias results toward Western, WEIRD demographics
- The ground truth comparison to top-voted human comments conflates consensus with popularity, potentially misrepresenting true moral judgment
- Automated moral theme classification may miss nuanced reasoning patterns or introduce classification errors

## Confidence
- **High Confidence**: Individual model self-consistency patterns and inter-model agreement measurements
- **Medium Confidence**: The relationship between moral theme invocation and verdict assignment
- **Medium Confidence**: Ensemble approximation of human consensus
- **Low Confidence**: The claim that individual models exhibit distinct moral sensitivities

## Next Checks
1. **External Dataset Validation**: Replicate the analysis on a different moral reasoning dataset (e.g., Moral Stories or ETHICS) to test whether observed model patterns generalize beyond Reddit AITA
2. **Chain-of-Thought Testing**: Run a subset of models with chain-of-thought prompting to determine whether explanation quality and theme detection improve, and whether this affects verdict consistency
3. **Demographic Sensitivity Analysis**: Segment the AITA dataset by poster demographics (age, gender, geography when available) to test whether model performance varies systematically across different moral perspectives