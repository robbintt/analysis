---
ver: rpa2
title: Large Language Models Develop Novel Social Biases Through Adaptive Exploration
arxiv_id: '2511.06148'
source_url: https://arxiv.org/abs/2511.06148
tags:
- demo
- quadrant
- aquadrant
- bquadrant
- cquadrant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that large language models (LLMs) can develop
  novel social biases through adaptive exploration, even when no inherent differences
  exist between demographic groups. Using a sequential hiring game paradigm from social
  science, the authors show that LLMs assign jobs to artificial demographic groups
  more unequally than human participants, with newer and larger models exhibiting
  stronger stratification effects.
---

# Large Language Models Develop Novel Social Biases Through Adaptive Exploration

## Quick Facts
- arXiv ID: 2511.06148
- Source URL: https://arxiv.org/abs/2511.06148
- Authors: Addison J. Wu; Ryan Liu; Xuechunzi Bai; Thomas L. Griffiths
- Reference count: 40
- LLMs can develop novel social biases through adaptive exploration, even when no inherent differences exist between demographic groups

## Executive Summary
This paper demonstrates that large language models (LLMs) can develop novel social biases through adaptive exploration, even when no inherent differences exist between demographic groups. Using a sequential hiring game paradigm from social science, the authors show that LLMs assign jobs to artificial demographic groups more unequally than human participants, with newer and larger models exhibiting stronger stratification effects. The study identifies this as an exploration-exploitation trade-off problem where models over-rely on early feedback. Among various interventions tested—including adjusting success probabilities, adding contextual features, and prompt steering—explicitly incentivizing diversity in the objective function proved most effective at reducing bias.

The findings reveal that LLMs are not merely passive mirrors of human biases but can actively create new ones, highlighting the urgent need for carefully designed objectives that incorporate societal values to ensure equitable outcomes. The research raises significant concerns about the deployment of LLMs in decision-making contexts where fairness is critical.

## Method Summary
The authors employed a sequential hiring game paradigm where LLMs made repeated hiring decisions between artificial demographic groups with no inherent performance differences. Models received immediate feedback on hiring outcomes and adjusted their strategies over multiple rounds. The study tested various LLMs across different sizes and generations, comparing their behavior to human participants. Multiple intervention strategies were evaluated, including modifying success probabilities, incorporating contextual information, prompt engineering, and adjusting objective functions to explicitly reward diversity in hiring outcomes.

## Key Results
- LLMs assigned jobs to artificial demographic groups more unequally than human participants in sequential hiring tasks
- Newer and larger models exhibited stronger stratification effects and more pronounced bias emergence
- Explicit diversity incentives in objective functions were most effective at reducing bias emergence compared to other interventions

## Why This Works (Mechanism)
The bias emergence mechanism operates through an exploration-exploitation trade-off where LLMs over-rely on early feedback signals. In sequential decision-making contexts, initial random variations in hiring outcomes become amplified as models update their strategies based on observed success rates. Without explicit diversity considerations, models naturally gravitate toward groups that appeared more successful in early rounds, creating self-reinforcing cycles that lead to increasingly unequal distributions. This process generates novel biases rather than simply replicating existing ones, as the stratification emerges from the interaction between the model's learning dynamics and the sequential feedback structure.

## Foundational Learning
- **Exploration-exploitation trade-off**: Models must balance trying new options versus exploiting known successful ones; critical for understanding why LLMs over-rely on early feedback patterns
  - Why needed: Explains the core mechanism driving bias emergence in sequential decision-making
  - Quick check: Test whether reducing exploitation pressure (e.g., through temperature scaling) reduces bias emergence

- **Social science paradigms in AI evaluation**: Using established experimental frameworks from human behavioral research to test AI systems; provides rigorous methodology for bias assessment
  - Why needed: Ensures findings are comparable to human behavior and grounded in established research
  - Quick check: Compare LLM behavior to published human participant data from similar paradigms

- **Objective function design**: How model incentives are explicitly encoded; directly impacts emergent behavior in decision-making tasks
  - Why needed: Demonstrates that bias reduction requires architectural changes beyond prompting or data filtering
  - Quick check: Test whether diversity-incentivized objectives maintain performance on other metrics

## Architecture Onboarding

**Component Map**: Sequential Decision Loop -> Feedback Processing -> Strategy Update -> Next Decision

**Critical Path**: The hiring decision process where models select candidates, receive immediate feedback, update internal representations, and make subsequent decisions. This loop creates conditions for bias amplification through iterative learning from outcomes.

**Design Tradeoffs**: Models must balance accuracy maximization against fairness considerations. Pure performance optimization leads to bias emergence, while explicit diversity incentives may reduce overall task performance. The sequential nature trades immediate accuracy for long-term strategy development.

**Failure Signatures**: Over-concentration of hiring decisions toward specific demographic groups despite equal underlying performance, increasing inequality over time, and stronger effects in newer/larger models. Failure manifests as systematic deviation from equitable distributions without corresponding performance improvements.

**Three First Experiments**:
1. Test whether varying the temperature parameter affects bias emergence patterns
2. Compare single-decision versus sequential decision-making contexts for bias development
3. Evaluate whether pre-training data filtering reduces but eliminates bias emergence in the hiring game

## Open Questions the Paper Calls Out
The study does not explicitly identify open questions beyond the immediate findings about bias emergence and intervention effectiveness.

## Limitations
- The sequential decision-making context with immediate feedback may create conditions that amplify bias emergence compared to other LLM deployment scenarios
- Focus on artificial demographic groups without real-world correlations limits understanding of how findings translate to actual societal biases
- The relationship between model architecture, training procedures, and bias emergence patterns requires further investigation

## Confidence
- **High confidence**: LLMs can develop novel social biases through adaptive exploration in sequential decision-making contexts
- **Medium confidence**: Explicit diversity incentives in objective functions reduce bias emergence
- **Medium confidence**: Larger and newer models show stronger stratification effects, though mechanisms remain unclear

## Next Checks
1. Test the hiring game paradigm across multiple LLM families with varying architectures to isolate whether observed effects are model-specific or universal
2. Implement the diversity-incentivized objective in a real-world hiring simulation with mixed human-LLM decision-making to evaluate practical effectiveness
3. Conduct ablation studies varying feedback timing and information availability to determine which aspects of the sequential paradigm most strongly drive bias emergence