---
ver: rpa2
title: 'FuocChuVIP123 at CoMeDi Shared Task: Disagreement Ranking with XLM-Roberta
  Sentence Embeddings and Deep Neural Regression'
arxiv_id: '2501.12336'
source_url: https://arxiv.org/abs/2501.12336
tags:
- disagreement
- embeddings
- mean
- computational
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents a system for the CoMeDi Shared Task Subtask
  2: Disagreement Ranking, which focuses on predicting annotator disagreement in multilingual
  word-in-context judgments. The approach uses sentence embeddings from the paraphrase-xlm-r-multilingual-v1
  model, combined with a deep regression neural network incorporating batch normalization
  and dropout.'
---

# FuocChuVIP123 at CoMeDi Shared Task: Disagreement Ranking with XLM-Roberta Sentence Embeddings and Deep Neural Regression

## Quick Facts
- arXiv ID: 2501.12336
- Source URL: https://arxiv.org/abs/2501.12336
- Reference count: 7
- 3rd place out of 7 teams in CoMeDi Shared Task Subtask 2 with Spearman's rank correlation of 0.124

## Executive Summary
This paper presents a system for the CoMeDi Shared Task Subtask 2: Disagreement Ranking, which focuses on predicting annotator disagreement in multilingual word-in-context judgments. The approach leverages multilingual sentence embeddings from the paraphrase-xlm-r-multilingual-v1 model, combined with a deep regression neural network incorporating batch normalization and dropout. The model is trained to predict mean pairwise judgment differences between annotators, explicitly targeting disagreement ranking rather than consensus. Evaluated using Spearman's rank correlation, the system achieved a competitive 3rd place out of 7 teams, with an average score of 0.124 on the evaluation set.

## Method Summary
The system uses sentence embeddings from the paraphrase-xlm-r-multilingual-v1 model as input features to a deep neural regression network. The architecture includes batch normalization and dropout layers to prevent overfitting and improve generalization. The model is specifically trained to predict mean pairwise judgment differences between annotators, which directly targets the disagreement ranking objective. By using multilingual embeddings, the approach can handle the multilingual nature of the CoMeDi task, capturing semantic relationships across different language pairs. The regression network learns to map embedding representations to disagreement scores, with the final output optimized for Spearman's rank correlation.

## Key Results
- Achieved 3rd place out of 7 teams in the CoMeDi Shared Task
- Average Spearman's rank correlation of 0.124 on evaluation set
- Demonstrated effectiveness of multilingual embeddings for capturing semantic complexities in word-in-context judgments

## Why This Works (Mechanism)
The system works by leveraging high-quality multilingual sentence embeddings that capture semantic relationships across languages, combined with a deep neural network that can learn complex patterns in annotator disagreement. The paraphrase-xlm-r-multilingual-v1 model provides rich semantic representations that help distinguish subtle differences in word usage across contexts. The regression architecture with batch normalization and dropout effectively learns to map these embeddings to disagreement rankings while maintaining generalization. By training directly on pairwise judgment differences rather than consensus, the model explicitly optimizes for detecting disagreement patterns that separate annotators.

## Foundational Learning
- Multilingual sentence embeddings (why needed: to capture semantic relationships across different languages in the CoMeDi task; quick check: verify embeddings capture cross-lingual semantic similarity)
- Deep regression neural networks (why needed: to learn complex mapping from embeddings to disagreement scores; quick check: validate network can fit training data with appropriate loss)
- Batch normalization (why needed: to stabilize training and improve generalization; quick check: monitor training stability with/without batch norm)
- Dropout regularization (why needed: to prevent overfitting on limited training data; quick check: compare performance with different dropout rates)
- Spearman's rank correlation (why needed: evaluation metric that measures ranking quality rather than absolute values; quick check: verify correlation implementation matches task requirements)

## Architecture Onboarding
**Component Map**: paraphrase-xlm-r-multilingual-v1 embeddings -> Deep regression network (with batch norm and dropout) -> Disagreement ranking output
**Critical Path**: Input embeddings → Batch normalization → Hidden layers with dropout → Output layer → Spearman's correlation loss
**Design Tradeoffs**: Used multilingual embeddings for cross-lingual capability vs. potential loss of language-specific nuance; deep network for learning complex patterns vs. risk of overfitting
**Failure Signatures**: Poor performance on Latin-based languages suggests embeddings may not capture subtle semantic distinctions equally well; reliance on embedding quality means embedding limitations directly impact final results
**First Experiments**: 1) Test embedding quality by measuring cross-lingual semantic similarity; 2) Validate regression network can learn from pairwise judgment differences; 3) Evaluate impact of batch normalization and dropout on generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Performance varies significantly across language families, with Latin-based languages showing particular challenges
- Heavy reliance on embedding quality, with no detailed error analysis of how embedding limitations affect predictions
- Narrow focus on disagreement ranking limits applicability to tasks requiring understanding of absolute agreement levels

## Confidence
- High confidence: Technical implementation details, model architecture choices, and overall 3rd place ranking are well-documented and verifiable
- Medium confidence: Effectiveness for capturing "semantic complexities in multilingual contexts" is supported by results but lacks detailed linguistic analysis
- Medium confidence: Characterization of limitations with Latin-based languages is plausible but not extensively validated with specific examples

## Next Checks
1. Conduct language-specific error analysis to quantify performance gaps between language families and identify systematic weaknesses
2. Perform ablation studies comparing different embedding models to isolate the contribution of paraphrase-xlm-r-multilingual-v1 to final performance
3. Test model transferability to related tasks like word sense disambiguation or semantic similarity ranking to assess broader applicability beyond disagreement ranking