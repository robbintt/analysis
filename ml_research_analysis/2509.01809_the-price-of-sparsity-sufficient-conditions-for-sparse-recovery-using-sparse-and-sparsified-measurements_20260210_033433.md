---
ver: rpa2
title: 'The Price of Sparsity: Sufficient Conditions for Sparse Recovery using Sparse
  and Sparsified Measurements'
arxiv_id: '2509.01809'
source_url: https://arxiv.org/abs/2509.01809
tags:
- recovery
- sparse
- have
- supp
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work establishes sufficient conditions for sparse recovery\
  \ using sparse measurement matrices. When the expected number of non-zero components\
  \ of the signal aligning with non-zero components of a row of the measurement matrix\
  \ is large (ds/p \u2192 +\u221E), reliable recovery is possible if the sample size\
  \ exceeds nINF = \u0398(s log(p/s) / log(ds/p))."
---

# The Price of Sparsity: Sufficient Conditions for Sparse Recovery using Sparse and Sparsified Measurements

## Quick Facts
- **arXiv ID:** 2509.01809
- **Source URL:** https://arxiv.org/abs/2509.01809
- **Reference count:** 40
- **Primary result:** Establishes sufficient conditions for sparse recovery using sparse measurement matrices, revealing a phase transition and quantifying the price of sparsity as a factor of $\log s / \log(ds/p)$ compared to dense measurements.

## Executive Summary
This work establishes sufficient conditions for recovering the support of a sparse binary signal from noisy linear measurements when using sparse measurement matrices. The key finding is that when the expected number of non-zero components aligning between the signal and measurement matrix is large (ds/p → +∞), reliable recovery is possible if the sample size exceeds a specific threshold. This reveals a sharp phase transition between impossible and tractable regimes, and quantifies the "price of sparsity" as a logarithmic factor compared to using dense measurements. The paper also addresses the setting where one starts with dense measurements and applies sparsification, showing that recovery remains possible under certain conditions subject to a uniform integrability conjecture.

## Method Summary
The paper studies sparse recovery where the measurement matrix $X$ has entries $X_{ij} = B_{ij}N_{ij}$ with $B_{ij} \sim \text{Ber}(d/p)$ and $N_{ij} \sim \mathcal{N}(0,1)$. The signal $\beta^*$ is binary and s-sparse. The goal is to recover the support of $\beta^*$ from observations $Y = X\beta^* + Z$ where $Z \sim \mathcal{N}(0, \sigma^2 I_n)$. The analysis focuses on the Maximum Likelihood Estimator (MLE) that solves $\hat{\beta} = \arg\min_{\beta \in \{0,1\}^p, \|\beta\|_0 = s} \|Y - X\beta\|_2^2$. The paper establishes sufficient conditions on sample size $n$ for successful recovery using large deviation bounds and union arguments, showing a phase transition phenomenon. For the sparsification setting, the paper analyzes recovery after replacing $X$ with $\tilde{X}_{ij} = B_{ij}X_{ij}$ and rescaling observations by $d/p$.

## Key Results
- Sparse recovery with sparse measurements exhibits a phase transition at $n_{INF}^{SP} = \Theta(s \log(p/s) / \log(ds/p))$ when $ds/p \to +\infty$
- The price of sparsity (factor increase in required samples compared to dense measurements) is $\Gamma = \log s / \log(ds/p)$
- For sparsification setting with linear sparsity ($s = \alpha p$, $d = \psi p$), recovery is possible if $n \geq n_{INF}^{Sp-ified} = \Theta(p / \psi^2)$, subject to uniform integrability conjecture
- MLE succeeds with high probability above the threshold, establishing information-theoretic sufficiency

## Why This Works (Mechanism)

### Mechanism 1: Large Deviation Bounds for Wrong Support Rejection
When sample size exceeds the sufficient threshold, the MLE recovers the true support with high probability. The proof bounds $P(L(S) < L(S^*))$ where $L(S)$ is the mean squared error for support $S$. Using Chernoff bounds on the moment generating function, the paper shows that for any wrong support $S$ with $|S \Delta S^*| \geq 2\delta s$, the probability that it achieves lower MSE than the true support decays as $\left(\frac{2\sigma^2 p}{\delta ds}\right)^{n/2}$. A union bound over all $\binom{p}{s}$ possible supports then yields the sufficient condition on $n$. The regime $ds/p \to +\infty$ ensures enough signal-measurement alignment per row to drive the error probability down.

### Mechanism 2: Phase Transition via Matching Necessary and Sufficient Conditions
Sparse recovery with sparse measurements exhibits a sharp phase transition at the information-theoretic threshold $n_{INF}^{SP}$. The paper establishes sufficient conditions (Theorem 1), while Wang et al. [16] previously established necessary conditions. In the regime $ds/p \to +\infty$, these thresholds match up to $(1 \pm \epsilon)$ factors, creating a sharp transition: below $(1-\epsilon)n_{INF}^{SP}$, recovery is information-theoretically impossible; above $(1+\epsilon)n_{INF}^{SP}$, the MLE succeeds with high probability.

### Mechanism 3: Sparsification Recovery via Rescaled Observations
After sparsifying a dense measurement matrix and rescaling observations by $d/p$, support recovery remains possible with $n = \Theta(p/\psi^2)$ samples. The sparsified matrix $\tilde{X}_{ij} = B_{ij}X_{ij}$ and rescaled observations $\tilde{Y} = (d/p)Y$ create a biased estimator since $\tilde{Y}$ contains information from nullified $X$ components. The proof extends the large-deviation approach but requires handling non-decomposable MSE terms. The key insight is that the sparsification budget $\psi_{budget} = \Theta(\sqrt{p/n})$ quantifies how aggressively one can sparsify while maintaining recoverability.

## Foundational Learning

- **Concept: High-dimensional sparse recovery phase transitions**
  - Why needed here: The entire paper builds on the idea that sample complexity exhibits sharp transitions between impossible, hard, and tractable regimes.
  - Quick check question: For dense measurements, what is the information-theoretic threshold $n_{INF}$ and how does it compare to the algorithmic threshold $n_{ALG}$?

- **Concept: Chernoff bounds and union bound arguments**
  - Why needed here: The core proof technique uses Chernoff bounds to control tail probabilities of wrong-support errors, combined with union bounds over exponential numbers of candidate supports.
  - Quick check question: If $P(L(S) < L(S^*)) \leq \alpha^n$ for each wrong $S$, and there are $\binom{p}{s}$ such supports, what condition on $n$ ensures the union bound gives vanishing total error probability?

- **Concept: Sparsity regimes ($s = o(p)$ vs $s = \alpha p$)**
  - Why needed here: The paper derives different threshold expressions depending on whether sparsity is sublinear or linear, affecting entropy terms in the bounds.
  - Quick check question: In the linear sparsity regime $s = \alpha p$, what replaces $s\log(p/s)$ in the threshold expression, and why?

## Architecture Onboarding

- **Component map:** Sparse Measurement Matrix Generator -> Signal Model -> Observation Model -> MLE Solver -> Support Recovery
- **Critical path:**
  1. Verify regime assumptions: Check that $ds/p \to +\infty$ for sufficient conditions to apply
  2. Compute threshold: Calculate $n_{INF}^{SP} = 2s\log(p/s)/\log(ds/p)$ for your $p, s, d$ parameters
  3. Sample size check: Ensure $n \geq (1+\epsilon)n_{INF}^{SP}$ for target reliability
  4. If sparsifying: Verify $n \geq (1+\epsilon) \cdot 2H(\alpha)p / \log(1 + \delta\psi C^*(\delta))$

- **Design tradeoffs:**
  - **Measurement sparsity vs sample complexity:** Lower $d$ reduces storage/computation but increases required $n$ by factor $\Gamma = \log s / \log(ds/p)$
  - **Information-theoretic vs algorithmic recovery:** MLE succeeds above $n_{INF}^{SP}$ but is exponential-time; LASSO is polynomial-time but requires $n \geq n_{ALG}$ (currently only proven under stronger sparsity assumptions per Omidiran & Wainwright)
  - **Approximate vs exact recovery:** Current sufficiency guarantees approximate recovery; exact recovery may require additional samples

- **Failure signatures:**
  - If $ds/p$ is bounded or shrinking, expect dramatically higher sample requirements or complete failure
  - If using sparsification with $d = o(p)$, the authors conjecture infeasibility regardless of $n$
  - If uniform integrability (Conjecture B.1) is violated, sparsification guarantees may not hold

- **First 3 experiments:**
  1. **Validation of phase transition:** Generate sparse Gaussian matrices with varying $d$, measure MLE success rate vs sample size $n$, confirm sharp transition near $n_{INF}^{SP}$
  2. **Price of sparsity quantification:** For fixed $p, s$, vary $d$ and measure the ratio of required samples to dense baseline; verify $\Gamma = \log s / \log(ds/p)$
  3. **Sparsification budget test:** Given dense data with $n = \Omega(p)$, progressively sparsify (decrease $\psi$) and identify the point where recovery fails; verify $\psi_{budget} \approx \Theta(\sqrt{p/n})$

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the uniform integrability conjecture hold for the random variables governing the error bounds in the sparsification setting (Theorem 3)?
- **Basis in paper:** [explicit] The abstract and Remark 3.1 state that the sufficient condition for recovery using sparsified measurements is "subject to a certain uniform integrability conjecture," and explicitly note that "verification is work in progress."
- **Why unresolved:** The proof of Theorem 3 relies on the convergence of $H_p(B)$, which requires uniform integrability to swap limits and expectations. While the authors expect standard concentration bounds to suffice, the rigorous verification has not yet been completed.
- **What evidence would resolve it:** A formal proof showing that the sequence of random variables $H_p(B)$ satisfies the uniform integrability condition (specifically, that the tail expectation vanishes uniformly as $p \to \infty$).

### Open Question 2
- **Question:** Is the information-theoretic threshold $n^{SP}_{INF}$ sufficient for *exact* support recovery, rather than just approximate recovery?
- **Basis in paper:** [explicit] Remark 2.2 highlights a disparity between the necessity statement (which implies exact recovery is impossible below the threshold) and the sufficiency statement (which proves approximate recovery above it). The authors write, "We believe the stronger statement holds and leave the proof for future work."
- **Why unresolved:** The current large deviation analysis proves that the symmetric difference between the estimated and true support vanishes, but does not prove that the probability of any error vanishes (exact recovery).
- **What evidence would resolve it:** A proof demonstrating that $P(\text{Supp}(\hat{\beta}) = \text{Supp}(\beta^\star)) \to 1$ for $n \ge (1+\epsilon)n^{SP}_{INF}$.

### Open Question 3
- **Question:** Is signal recovery information-theoretically impossible in the sub-linear sparsification regime ($d = o(p)$) regardless of the sample size?
- **Basis in paper:** [explicit] The conclusion (Section 4) states: "We conjecture that the recovery is information-theoretically impossible no matter the sample size in the sub-linear sparsification regime where $d = o(p)$."
- **Why unresolved:** The paper establishes sufficiency for linear sparsification regimes ($d = \psi p$). However, in the sub-linear regime, the bias introduced by naive rescaling of observations likely destroys the signal information irrecoverably, though this has not been proven.
- **What evidence would resolve it:** A converse result showing that for $d = o(p)$, the probability of successful recovery remains strictly less than 1 for any sample size $n$.

## Limitations
- The sufficiency result relies on the MLE being solvable exactly, but this is generally NP-hard for non-trivial problem sizes, creating a gap between theoretical possibility and computational feasibility.
- The proof for sparsification recovery (Theorem 3) depends on Conjecture B.1 regarding uniform integrability, which remains unproven.
- Current sufficiency guarantees only approximate recovery rather than exact recovery, though exact recovery is conjectured to be possible.
- All results are asymptotic; finite-sample behavior and constants may differ significantly in practice.

## Confidence
- **High Confidence:** The information-theoretic phase transition threshold in the dense measurement regime ($d = p$) - builds on established necessary conditions from prior work.
- **Medium Confidence:** The sufficient condition threshold for sparse measurements when $ds/p \to +\infty$ - the large deviation argument is sound but requires the MLE assumption.
- **Low Confidence:** The sparsification result (Theorem 3) - depends on an unproven conjecture and involves more complex technical arguments.

## Next Checks
1. **Phase Transition Verification:** For small-scale problems ($p \leq 30$), implement brute-force MLE and empirically verify the sharp transition in recovery success rate around the predicted threshold $n_{INF}^{SP}$.
2. **Finite-Sample Gap Analysis:** Test the gap between information-theoretic sufficiency and algorithmic feasibility by comparing MLE success rates to LASSO performance across various $d/p$ ratios.
3. **Conjecture Dependency Testing:** For the sparsification setting, numerically verify the uniform integrability conditions (Conjecture B.1) for specific parameter choices to assess whether the conjecture is likely to hold.