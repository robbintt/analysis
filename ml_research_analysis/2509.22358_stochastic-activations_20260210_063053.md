---
ver: rpa2
title: Stochastic activations
arxiv_id: '2509.22358'
source_url: https://arxiv.org/abs/2509.22358
tags:
- relu
- silu
- activation
- stocha
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Stochastic activations introduce a novel strategy for large language
  models that randomly selects between non-linear functions like SILU and RELU during
  the feed-forward layer. This approach addresses the optimization challenges of RELU,
  particularly its constant shape for negative inputs that prevents gradient flow.
---

# Stochastic activations

## Quick Facts
- arXiv ID: 2509.22358
- Source URL: https://arxiv.org/abs/2509.22358
- Reference count: 28
- Primary result: Random selection between SILU and RELU during feed-forward layers improves optimization and enables sparse activations at inference

## Executive Summary
Stochastic activations introduce a novel strategy for large language models that randomly selects between non-linear functions like SILU and RELU during the feed-forward layer. This approach addresses the optimization challenges of RELU, particularly its constant shape for negative inputs that prevents gradient flow. The method is applied in two ways: (1) pre-training with SILU followed by fine-tuning with RELU to achieve sparse activations at inference time, resulting in significant CPU and GPU speedups (65% and 1.5× respectively) while maintaining performance comparable to SILU; (2) using stochastic activations at inference time to generate diverse text sequences, providing an alternative to temperature sampling.

## Method Summary
The core innovation involves randomly selecting between non-linear activation functions during the feed-forward layer of transformer models. The approach uses a two-phase training strategy: initial pre-training with SILU activation functions, followed by fine-tuning with stochastic selection between SILU and RELU. This creates sparse activations during inference when RELU is selected, enabling computational efficiency. The stochastic selection can also be applied at inference time to generate diverse text outputs, offering an alternative to traditional temperature-based sampling methods.

## Key Results
- Pre-training with SILU followed by fine-tuning with RELU achieves 65% CPU speedup and 1.5× GPU speedup while maintaining performance comparable to SILU-only models
- The approach yields better results than training from scratch with RELU alone, addressing RELU's optimization challenges
- Stochastic activations at inference time generate more diverse text sequences compared to standard SILU with temperature sampling
- Experiments validated on LM1.5B and LM3B models demonstrate the scalability of the approach

## Why This Works (Mechanism)
The method works by addressing RELU's fundamental limitation: its constant zero output for negative inputs prevents gradient flow and hinders optimization. By randomly selecting between SILU (which provides smooth gradients) and RELU during training, the model benefits from SILU's optimization properties while learning to operate effectively with RELU's sparse activations. During inference, when only RELU is active, the model produces sparse activations that enable computational efficiency. The stochastic nature at training time allows the model to adapt to both activation patterns, creating a more robust and efficient final model.

## Foundational Learning
**Activation Functions**: Non-linear transformations applied to neural network outputs, essential for learning complex patterns
*Why needed*: Enable neural networks to model non-linear relationships in data
*Quick check*: Compare outputs of linear vs non-linear activation functions on sample inputs

**SILU (Sigmoid-weighted Linear Unit)**: Activation function that multiplies inputs by their sigmoid values
*Why needed*: Provides smooth gradients and better optimization properties than RELU
*Quick check*: Verify SILU outputs range between 0 and input values

**RELU (Rectified Linear Unit)**: Activation function that outputs zero for negative inputs and input values for positive inputs
*Why needed*: Creates sparse activations but suffers from optimization challenges
*Quick check*: Confirm RELU outputs are zero for negative inputs

**Sparse Activations**: Neural network states where many neurons output zero values
*Why needed*: Enable computational efficiency by reducing active computations
*Quick check*: Measure percentage of zero outputs in activation layers

**Gradient Flow**: The propagation of error signals backward through neural network layers during training
*Why needed*: Critical for effective model learning and convergence
*Quick check*: Monitor gradient magnitudes across different activation functions

**Temperature Sampling**: A technique for controlling output diversity in language models by scaling logits before softmax
*Why needed*: Standard method for generating diverse text sequences
*Quick check*: Compare outputs at different temperature values

## Architecture Onboarding
**Component Map**: Input -> Embedding Layer -> Transformer Blocks (with Stochastic Activations) -> Output Layer
**Critical Path**: Token embedding → Multi-head attention → Feed-forward layer (with stochastic activation selection) → Layer normalization → Output projection
**Design Tradeoffs**: Stochastic training enables efficient inference but adds training complexity; SILU provides better optimization but less computational efficiency than RELU
**Failure Signatures**: Poor optimization with RELU alone; degraded performance when stochastic selection probability is imbalanced
**First Experiments**: 1) Compare training curves with pure SILU vs stochastic activations, 2) Measure inference speed and memory usage with different activation ratios, 3) Evaluate text generation diversity across different stochastic selection probabilities

## Open Questions the Paper Calls Out
None

## Limitations
- The performance comparison between pre-training with SILU and fine-tuning with RELU versus training from scratch with RELU lacks methodological clarity
- Speedup claims need validation across different hardware configurations and model scales beyond the tested LM1.5B and LM3B models
- Text generation diversity claims lack specific metrics and controlled experimental comparisons with established diversity techniques

## Confidence
**High Confidence**: The basic technical description of stochastic activations and the general motivation appear sound and well-grounded in established neural network literature.
**Medium Confidence**: The performance claims regarding speedups and maintaining comparable performance to SILU are plausible but require independent verification across different model sizes and hardware setups.
**Low Confidence**: The text generation diversity claims and the comparative advantage over temperature sampling lack sufficient methodological detail to assess their validity with high confidence.

## Next Checks
1. Conduct ablation studies comparing models trained with stochastic activations from scratch versus the proposed two-phase approach (pre-training with SILU, fine-tuning with RELU) to isolate the contribution of each component.
2. Benchmark the CPU and GPU speedups across a broader range of hardware configurations (different CPU architectures, GPU models, and batch sizes) to establish generalizability of the performance claims.
3. Implement controlled experiments comparing text generation diversity using established metrics (e.g., self-BLEU, distinct-n-grams, or embedding-based diversity measures) against both SILU with temperature sampling and other diversity-inducing techniques.