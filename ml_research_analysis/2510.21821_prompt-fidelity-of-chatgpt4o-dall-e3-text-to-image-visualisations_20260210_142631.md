---
ver: rpa2
title: Prompt fidelity of ChatGPT4o / Dall-E3 text-to-image visualisations
arxiv_id: '2510.21821'
source_url: https://arxiv.org/abs/2510.21821
tags:
- prompt
- speci
- visualisations
- generative
- dall-e3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the prompt fidelity of ChatGPT4o / DALL-E3
  by testing whether specific attributes in AI-generated prompts were accurately rendered
  in resulting images. Using two datasets of 200 visualizations of women in cultural/creative
  roles and 230 of museum curators, researchers assessed accuracy for personal attributes
  (age, hair), appearance (attire, glasses), and paraphernalia (name tags, clipboards).
---

# Prompt fidelity of ChatGPT4o / Dall-E3 text-to-image visualisations

## Quick Facts
- **arXiv ID:** 2510.21821
- **Source URL:** https://arxiv.org/abs/2510.21821
- **Authors:** Dirk HR Spennemann
- **Reference count:** 0
- **Primary result:** ChatGPT4o/DALL-E3 deviated from prompt specifications in 15.6% of all attributes (n=710), with highest errors for age depictions (32.4%).

## Executive Summary
This study evaluated whether attributes explicitly specified in autogenously generated prompts were correctly rendered in ChatGPT4o/DALL-E3 image outputs. Using two datasets of professional visualizations (200 women in creative roles, 230 museum curators), researchers assessed accuracy across paraphernalia (name tags, clipboards), personal appearance (attire, glasses), and personal attributes (age, hair). While most attributes were rendered correctly, DALL-E3 showed systematic deviations, particularly for depicting human characteristics like age, suggesting measurable gaps in prompt-to-image fidelity with implications for bias detection.

## Method Summary
The study used two public-domain datasets: visualizations of women in cultural/creative roles (200 images) and museum curators (230 images). ChatGPT4o generated prompts for DALL-E3 using standardized templates describing professional settings. For each role, 10 images were generated with chat history deleted between iterations. Images were evaluated for 6 attributes: age (classified via ChatGPT4o), hair, attire, glasses, name tags, and clipboards. Human scorers assessed attribute accuracy, with ambiguous interpretations scored conservatively as "correct."

## Key Results
- DALL-E3 deviated from prompt specifications in 15.6% of all attributes tested (n=710)
- Error rates varied significantly by category: paraphernalia (6.6%), personal appearance (15.0%), and age attributes (32.4%)
- Age depictions showed highest error rate at 32.4% (n=108), with 50% incorrect when specified as "middle-aged"
- DALL-E3 added unspecified elements in 6.5% of images (n=23)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Diffusion models exhibit hierarchical fidelity loss, with object-level attributes rendered more reliably than human demographic attributes.
- **Mechanism:** The text-to-image pipeline must map semantic tokens to visual features. Objects (clipboards, name tags) have more consistent training representations with fewer contextual variations, whereas human attributes like age require synthesizing multiple facial features into a coherent demographic signal, creating more opportunities for drift between prompt specification and visual output.
- **Core assumption:** Training data contains more consistent visual-linguistic mappings for objects than for human attributes across diverse contexts.
- **Evidence anchors:**
  - [abstract] "Errors were lowest for paraphernalia (6.6%), moderate for personal appearance (15.0%), and highest for depictions of people themselves, particularly age (32.4%)."
  - [section 4, Discussion] "The greatest proportion of incorrectly rendered attributes (32.4%, n=108) were those related to the depiction of a person's (age, hair)"
  - [corpus] IMAGINE-E paper (arxiv 2501.13920) evaluates prompt-following capabilities across T2I models, suggesting this is a generalizable pattern across architectures.
- **Break condition:** If a model were trained with explicitly balanced demographic representations and age-disambiguating augmentations, the fidelity gradient should flatten.

### Mechanism 2
- **Claim:** Autogenous prompt generation introduces an intermediate representation layer where specification fidelity can degrade before image synthesis begins.
- **Mechanism:** ChatGPT4o receives a user prompt, generates an internal prompt for DALL-E3, and DALL-E3 renders from that specification. Each transformation introduces potential drift: ChatGPT4o may omit or reinterpret attributes, and DALL-E3 may further deviate during diffusion sampling.
- **Core assumption:** The combined two-stage architecture compounds fidelity loss compared to direct prompt-to-image systems.
- **Evidence anchors:**
  - [abstract] "attributes explicitly specified in autogenously generated prompts are correctly rendered in the resulting images"
  - [section 1, Introduction] "ChatGPT4 autogenously generating prompts for parsing into DALL-E3"
  - [corpus] Weak direct corpus evidence on autogenous prompt generation specifically; related work (DebiasPI, Causal-Adapter) focuses on inference-time interventions rather than prompt generation layers.
- **Break condition:** Direct user prompt injection into the diffusion model (bypassing LLM prompt reformulation) should reduce this specific drift source.

### Mechanism 3
- **Claim:** Semantic ambiguity in prompt terms creates interpretive latitude that models resolve via training distribution priors rather than explicit specifications.
- **Mechanism:** Terms like "middle-aged," "neatly styled," or "professional attire" lack precise visual definitions. The model samples from its learned distribution of what these terms mean, which may not align with user intent. This explains why specific paraphernalia (binary present/absent) outperforms subjective attributes.
- **Core assumption:** Training distributions encode normative interpretations of ambiguous terms that may conflict with minority or contextual specifications.
- **Evidence anchors:**
  - [section 4, Discussion] "DALL-E3 has a considerable latitude in interpreting prompts that do not directly specify design elements"
  - [section 3.1, Results] "some prompt specifications were open to interpretation (e.g. 'curly,' 'styled,' 'neatly styled'). To be conservative in the analysis, these were also scored as 'correct.'"
  - [corpus] FairImagen and DebiasPI papers explicitly address how priors override minority specifications, requiring inference-time interventions.
- **Break condition:** Using numerically precise specifications (e.g., "55-year-old" vs. "middle-aged") should reduce interpretive variance.

## Foundational Learning

- **Concept: Prompt fidelity (adherence/compliance)**
  - **Why needed here:** This is the core dependent variable—the paper measures whether specified attributes appear in outputs. Without this concept, you cannot evaluate T2I reliability.
  - **Quick check question:** If a prompt specifies "red glasses" and the image shows no glasses, is this a fidelity error or an addition error?

- **Concept: Cascading bias in multi-stage generative pipelines**
  - **Why needed here:** The paper's architecture has two decision points (LLM prompt generation + diffusion rendering). Bias can compound or originate at either stage.
  - **Quick check question:** If ChatGPT4o generates "young professional" when the user requested "senior librarian," where in the pipeline did fidelity break?

- **Concept: Attribute taxonomies for bias auditing**
  - **Why needed here:** The paper categorizes attributes (personal, appearance, paraphernalia) because error rates differ by category. Audit design requires understanding which attribute types are most vulnerable.
  - **Quick check question:** Why would you design a bias audit to oversample paraphernalia attributes versus age attributes?

## Architecture Onboarding

- **Component map:** User Prompt → ChatGPT4o: Prompt Interpretation + Autogenous Prompt Generation → DALL-E3: Text Encoding → Diffusion Sampling → Image Decoding → Output Image

- **Critical path:**
  1. User request enters ChatGPT4o
  2. ChatGPT4o generates structured prompt with attribute specifications
  3. DALL-E3 encodes prompt and samples diffusion trajectory
  4. Image generated; attributes may or may not match prompt
  5. Evaluation compares prompt-specified attributes vs. observed attributes

- **Design tradeoffs:**
  - Two-stage pipeline (LLM + diffusion) enables natural language flexibility but introduces compound fidelity drift
  - Autogenous prompts improve user experience (no prompt engineering required) but reduce user control over specifications
  - Conservative scoring (ambiguous interpretations marked "correct") understates true error rates

- **Failure signatures:**
  - Age overwrites: Prompt specifies "middle-aged," output appears 20-30 (Table 3 shows 50% error rate)
  - Attribute injection: Items appear in image despite absence from prompt (DALL-E3 adds unspecified elements)
  - Semantic drift: "Professional attire" renders differently across professional contexts

- **First 3 experiments:**
  1. **Isolate the drift source:** Run identical prompts through ChatGPT4o/DALL-E3 vs. direct DALL-E3 API to partition error between prompt generation and image rendering stages.
  2. **Attribute precision test:** Compare fidelity for numerically precise age ("47-year-old") vs. categorical ("middle-aged") to quantify semantic ambiguity contribution.
  3. **Category sensitivity mapping:** Extend the paraphernalia/personal attribute distinction to test whether the 6.6% vs. 32.4% gradient holds across additional object types (e.g., tools vs. clothing vs. background elements).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the ChatGPT native image generation model (released March 2025) exhibit different prompt fidelity patterns than the ChatGPT4o/DALL-E3 combination?
- **Basis in paper:** [explicit] The authors state in Section 2.3: "the observations made in this study may not reflect the current state of generative AI text-to-image visualisations. They do, however, serve as baseline data for comparison with the most recent models."
- **Why unresolved:** The datasets were collected before the model transition, making comparison impossible with existing data.
- **What evidence would resolve it:** Replicate the study methodology using identical prompts with the new native image generation model and compare fidelity rates across attribute categories.

### Open Question 2
- **Question:** What mechanisms cause DALL-E3 to disproportionately fail on person-related attributes (age: 32.4% error) versus paraphernalia (6.6% error)?
- **Basis in paper:** [inferred] The paper documents this hierarchy of errors but does not investigate whether this stems from training data composition, internal representation conflicts, or model architecture.
- **Why unresolved:** The study measures error rates but was not designed to identify causal factors behind the differential performance.
- **What evidence would resolve it:** Ablation studies controlling for attribute type, combined with analysis of training data distributions and internal attention mechanisms during generation.

### Open Question 3
- **Question:** Does using an independent age classifier (rather than ChatGPT4o) change the measured fidelity rates for age attributes?
- **Basis in paper:** [inferred] Section 2.2 describes using ChatGPT4o to classify ages of images it helped generate, creating potential circularity where the same system evaluates its own outputs.
- **Why unresolved:** No validation was performed against human raters or alternative classification systems.
- **What evidence would resolve it:** A comparison study where human annotators and/or different AI models classify image ages, with inter-rater reliability analysis against ChatGPT4o's classifications.

## Limitations

- The study uses conservative scoring for ambiguous attributes, potentially underestimating true error rates
- Fidelity loss cannot be precisely partitioned between ChatGPT4o prompt generation and DALL-E3 image synthesis stages
- Human scoring for subjective attributes (hair, attire) lacks standardized rubrics, introducing potential inter-rater variability

## Confidence

- **High Confidence:** Paraphernalia attributes show lowest error rates (6.6%) due to binary present/absent classification
- **Medium Confidence:** Categorical fidelity gradient (paraphernalia < personal appearance < age attributes) is well-supported but may reflect conservative scoring
- **Low Confidence:** Attribution of fidelity gaps specifically to autogenous prompt generation versus diffusion sampling cannot be conclusively determined from study design alone

## Next Checks

1. **Architecture isolation test:** Generate identical prompts through ChatGPT4o/DALL-E3 versus direct DALL-E3 API to partition fidelity loss between LLM interpretation and diffusion sampling stages
2. **Precision specification test:** Compare fidelity for numerically precise age specifications ("47-year-old") versus categorical terms ("middle-aged") to quantify semantic ambiguity contribution
3. **Temporal stability test:** Repeat the evaluation protocol using current ChatGPT4o/DALL-E3 versions to assess whether reported fidelity gaps persist after March 2025 model updates