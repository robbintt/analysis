---
ver: rpa2
title: Rethinking Large Language Models For Irregular Time Series Classification In
  Critical Care
arxiv_id: '2601.16516'
source_url: https://arxiv.org/abs/2601.16516
tags:
- time
- series
- irregular
- llms
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of Large Language Models
  (LLMs) for irregular time series classification in critical care. The authors conduct
  a systematic empirical study on two benchmark ICU datasets, PhysioNet 2012 and MIMIC-III,
  to evaluate the impact of encoder design and multimodal alignment strategy on LLM
  performance.
---

# Rethinking Large Language Models For Irregular Time Series Classification In Critical Care

## Quick Facts
- arXiv ID: 2601.16516
- Source URL: https://arxiv.org/abs/2601.16516
- Authors: Feixiang Zheng; Yu Wu; Cecilia Mascolo; Ting Dang
- Reference count: 0
- Primary result: Irregularity-aware encoders substantially outperform vanilla Transformers for ICU time series classification.

## Executive Summary
This paper investigates Large Language Models (LLMs) for irregular time series classification in critical care settings. Through systematic empirical studies on PhysioNet 2012 and MIMIC-III datasets, the authors evaluate how encoder design and multimodal alignment strategies impact LLM performance. Results demonstrate that encoder architecture choice is more critical than alignment strategy, with mTAND encoder achieving 12.8% AUPRC improvement over vanilla Transformer. While alignment strategies provide modest gains, LLM-based methods require 10× longer training time while delivering only comparable performance to specialized supervised models, particularly underperforming in few-shot learning scenarios.

## Method Summary
The study evaluates LLM-based methods (Time-LLM, S2IP, CALF, FSCA) with systematic ablation on encoder types (1DCNN, Decomposition, Transformer, mTAND) and alignment strategies. Experiments use PhysioNet 2012 (11,988 samples, 41 variables, 85.7% missing), MIMIC-III (24,681 samples, 96 variables, 96.7% missing), and semi-synthetic MIT-BIH ECG datasets. Data splits vary by dataset (PhysioNet 80/10/10, MIMIC-III 60/20/20, ECG 60/20/20). The framework runs on NVIDIA A100 80GB hardware, with Time-LLM requiring 2x A100 due to memory constraints. Three runs per configuration report mean±std AUPRC/AUROC metrics.

## Key Results
- Encoder architecture is more critical than alignment strategy, with mTAND achieving 12.8% AUPRC increase over vanilla Transformer
- Best alignment strategy (S²IP) provides modest 2.9% AUPRC improvement over cross-attention
- LLM-based methods require 10× longer training than best irregular supervised models while delivering comparable performance
- LLMs underperform in data-scarce few-shot learning settings compared to domain-specific architectures like Warpformer

## Why This Works (Mechanism)

### Mechanism 1
Irregularity-aware encoders (e.g., mTAND) substantially outperform standard Transformer-based encoders for ICU time series classification by explicitly modeling variable time gaps through continuous-time embeddings and multi-timescale attention, avoiding false uniform-spacing assumptions that introduce temporal bias in CNNs and vanilla Transformers. This works because irregular sampling intervals contain diagnostically relevant signal rather than pure noise. Evidence shows mTAND outperforms other encoders with 12.8% AUPRC increase, while 1D CNN, decomposition encoder, and Transformer introduce misleading temporal biases when applied to irregular data. Break condition occurs if irregularity is primarily random noise without clinical meaning.

### Mechanism 2
Semantic anchor alignment provides modest performance gains over cross-attention fusion by retrieving top-K pretrained word embeddings as focused alignment targets, whereas cross-attention distributes focus across multiple keys potentially diluting representation specificity. This works because pretrained textual embeddings contain semantic structure relevant to physiological patterns. Evidence shows S²IP achieves best performance with 2.9% AUPRC improvement over cross-attention, benefiting from semantic anchor retrieval while cross-attention-based approaches may lead to overly diffuse or redundant representations. Break condition occurs if semantic anchors are poorly matched to clinical concepts.

### Mechanism 3
LLM scale and pretraining do not confer expected few-shot advantages for irregular time series because LLMs pretrained on text corpora may overfit to text-derived patterns that do not transfer to sparse, irregular numerical sequences, while architectures like Warpformer with built-in multi-scale temporal modeling capture structure more efficiently. This works because irregular time series structure differs fundamentally from patterns learned in text pretraining. Evidence shows LLM-based methods underperform in data-scarce few-shot learning settings, with Warpformer model outperforming LLM approaches. Break condition occurs if pretraining corpora included substantial numerical/clinical time series.

## Foundational Learning

- **Irregular Time Series Representation**: Needed because ICU data has 85-97% missingness; standard uniform-timestep models fail. Quick check: Can you explain why padding or imputation discards temporal information vs. explicit continuous-time modeling?

- **Cross-Modal Alignment**: Needed because LLMs operate on text tokens while time series must be mapped to compatible embedding spaces. Quick check: What is the difference between reprogramming (cross-attention projection) and semantic anchor retrieval?

- **AUPRC for Imbalanced Data**: Needed because ICU mortality is rare; AUROC can be misleadingly high. Quick check: Why does AUPRC better reflect model performance when positive class prevalence is <10%?

## Architecture Onboarding

- **Component map**: Raw time series -> Encoder (mTAND) -> Alignment (S²IP) -> LLM Backbone -> Classification Head
- **Critical path**: 1) Patch/segment irregular time series (preserving timestamps), 2) Encode via mTAND (continuous-time attention), 3) Align via S²IP (semantic anchor retrieval), 4) Feed to LLM backbone → classify
- **Design tradeoffs**: mTAND encoder provides +12.8% AUPRC vs. Transformer but adds complexity; S²IP alignment provides +2.9% AUPRC vs. cross-attention requiring pretrained embedding retrieval; LLM inclusion provides minimal performance gain with 10×+ training cost increase; recommendation is to start with mTAND + lightweight alignment and consider dropping LLM backbone for supervised tasks
- **Failure signatures**: Rapid performance collapse as missing ratio increases (38-42% drop) indicates encoder not handling irregularity; high variance across runs (±3-7 AUPRC) suggests unstable alignment or insufficient training data; training time >2 hours on A100 for PhysioNet indicates unnecessary LLM overhead
- **First 3 experiments**: 1) Encoder ablation: Compare mTAND vs. Transformer encoder on PhysioNet (hold alignment fixed), expect ~18 AUPRC gap, 2) Alignment ablation: Compare S²IP vs. cross-attention (hold mTAND encoder fixed), expect ~0.5-2.9% AUPRC gap, 3) Efficiency baseline: Train standalone mTAND and Warpformer without LLM, compare AUPRC and training time to mTAND+S²IP+LLM, confirm 10× cost gap

## Open Questions the Paper Calls Out

### Open Question 1
Can specialized pre-training on irregular time series data (rather than text corpora) enable LLMs to outperform lightweight supervised models in few-shot learning for clinical time series? This is unresolved because current LLMs are pre-trained on text, not irregular time series, and the study only evaluates existing LLMs. Evidence needed: Training and evaluating an LLM pre-trained on large-scale irregular clinical time series, then comparing its few-shot performance against Warpformer and mTAND.

### Open Question 2
What alignment strategies specifically designed for irregular time series could narrow the performance gap with irregularity-aware encoders? This is unresolved because existing alignment strategies were developed for regular time series and adapted here, with none explicitly incorporating irregularity into the alignment mechanism itself. Evidence needed: Designing alignment methods that incorporate time embeddings or missingness patterns, then measuring whether the encoder-alignment performance gap shrinks.

### Open Question 3
At what scale of training data does the computational overhead of LLMs become justified for irregular time series classification? This is unresolved because the study tests only full-data and 10% few-shot settings, leaving intermediate data regimes unexplored. Evidence needed: Systematic evaluation across training data fractions (e.g., 20%, 40%, 60%, 80%) to identify the crossover point where LLM benefits outweigh costs.

## Limitations
- Empirical scope limited to two ICU datasets (PhysioNet 2012, MIMIC-III) with binary mortality prediction, limiting generalizability to other irregular time series domains
- LLM backbone specification unclear (which pre-trained model), creating ambiguity about whether gains come from encoder/alignment or underlying LLM capacity
- Training efficiency comparisons assume fair hardware allocation but do not control for model size differences systematically

## Confidence
- Encoder superiority claim (mTAND vs. Transformer): Medium - supported by strong within-study AUPRC differences (+12.8%) but limited external validation and unclear baseline Transformer configuration
- Alignment strategy impact (S²IP vs. cross-attention): Low-Medium - modest AUPRC gains (+2.9%) reported but no external corpus support for semantic anchor effectiveness in clinical time series
- LLM few-shot underperformance: Medium - internally consistent with Warpformer comparisons but mechanism relies on untested assumptions about pretraining corpus composition

## Next Checks
1. **Encoder Generalization Test**: Validate mTAND encoder gains on an external irregular time series dataset (e.g., NASA CMAPSS turbofan degradation) to confirm superiority beyond ICU mortality prediction
2. **Alignment Ablation Precision**: Measure exact AUPRC differences between S²IP and cross-attention across multiple random seeds and datasets to verify the 2.9% claim is robust and not an outlier
3. **Efficiency Baseline Replication**: Train standalone mTAND and Warpformer models on PhysioNet 2012 without LLM backbone, measuring both AUPRC and wall-clock training time to confirm the reported 10× cost differential