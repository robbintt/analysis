---
ver: rpa2
title: 'Preserving LLM Capabilities through Calibration Data Curation: From Analysis
  to Optimization'
arxiv_id: '2510.10618'
source_url: https://arxiv.org/abs/2510.10618
tags:
- calibration
- data
- compression
- code
- cola
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how calibration data affects LLM capability
  preservation during compression, focusing on compositional properties and domain
  correspondence. The authors find that representativeness and diversity in activation
  space fundamentally determine calibration data quality, with domain-matched data
  preserving specific capabilities while mixed data balances general performance.
---

# Preserving LLM Capabilities through Calibration Data Curation: From Analysis to Optimization

## Quick Facts
- arXiv ID: 2510.10618
- Source URL: https://arxiv.org/abs/2510.10618
- Reference count: 40
- Primary result: COLA framework improves calibration data curation for LLM compression, preserving reasoning capabilities through activation-space diversity.

## Executive Summary
This work investigates how calibration data affects LLM capability preservation during compression, focusing on compositional properties and domain correspondence. The authors find that representativeness and diversity in activation space fundamentally determine calibration data quality, with domain-matched data preserving specific capabilities while mixed data balances general performance. Based on these insights, they propose COLA, a three-stage framework for curating optimal calibration data through dataset selection, processing, and activation-space-based sample selection. Experiments across multiple compression methods and deployment scenarios show consistent performance improvements, particularly for complex reasoning tasks. The framework is also validated on larger models and extended to other compression methods, demonstrating its generalizability.

## Method Summary
COLA is a three-stage framework for optimizing calibration data curation during post-training compression. Stage 1 selects datasets matching the deployment domain through coverage-based heuristics. Stage 2 processes samples by standardizing sequence length to 2048 tokens and converting implicit text to explicit reasoning chains (Q&A format). Stage 3 extracts model activations, reduces dimensions via random projection, clusters using K-Means, and selects samples closest to cluster centroids. The framework targets activation space representativeness and diversity to preserve capabilities during quantization and pruning.

## Key Results
- COLA improves compressed model performance by 2-6% on reasoning benchmarks compared to random calibration data
- Domain-matched calibration data preserves specialized capabilities (math, code) better than generic data
- Explicit reasoning chains in calibration data are crucial for preserving complex reasoning pathways during compression

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimizing calibration data based on activation space representativeness and diversity preserves LLM capabilities more effectively than random sampling.
- **Mechanism:** Post-training compression (like GPTQ or SparseGPT) relies on calibration data to estimate weight importance and activation ranges. The authors argue that if calibration samples maximally cover the "activation space" (specific neural pathways triggered by inputs), the compression algorithm is less likely to degrade critical capabilities. Specifically, clustering activation vectors ensures the selected samples cover diverse "reasoning pathways" rather than redundant patterns.
- **Core assumption:** The utility of calibration data is directly correlated with the diversity of the model's internal activation patterns it triggers, rather than just the semantic diversity of the text.
- **Evidence anchors:**
  - [abstract] "Delving into the underlying mechanism, we find that the representativeness and diversity in activation space more fundamentally determine the quality of calibration data."
  - [section 4.2] "These samples from all clusters C1, C2, ...Ck ensures the diversity in the activation space, while the closeness to the centroid in each cluster guarantees the representativeness."
  - [corpus] [arxiv:2506.13329] "EAQuant" supports the general premise that activation-aware optimization is critical for handling outliers in MoE models, aligning with the importance of activation patterns.
- **Break condition:** This mechanism assumes the uncompressed model's activation patterns on the calibration data are representative of the deployment distribution. If the model has severe internal biases or the deployment domain is entirely out-of-distribution, activation clustering may select "diverse but irrelevant" samples.

### Mechanism 2
- **Claim:** Calibration data with explicit reasoning chains (e.g., Q&A with reasoning) preserves complex reasoning capabilities better than standard text.
- **Mechanism:** Compression algorithms evaluate "weight importance" based on how activations flow through the network. Simple text (e.g., "The sky is blue") may not trigger the complex, multi-step neural circuits required for math or logic. By using data with explicit reasoning chains, the calibration process is forced to "activate and maintain" these deeper, more complex pathways during the compression reconstruction phase.
- **Core assumption:** Weights involved in complex reasoning pathways are distinct from those used in basic language modeling and require specific activation to be identified for preservation.
- **Evidence anchors:**
  - [section 3.2] "...Q&A w/ ERC yields the best performance across all methods... These effects stem from reasoning pathway preservation, where explicit reasoning chains activate and maintain internal reasoning mechanisms during compression."
  - [Appendix E] "This [high-frequency loss] directly corresponds to the degradation of high-level reasoning capabilities like mathematics and code generation, which rely heavily on high-frequency components."
- **Break condition:** If the compression ratio is extremely aggressive (e.g., < 4-bit or > 50% pruning), preserving the reasoning pathway may be impossible regardless of calibration data richness.

### Mechanism 3
- **Claim:** Domain correspondence in calibration data improves target capability preservation via spectral energy preservation.
- **Mechanism:** The paper links calibration quality to the spectral properties of Feed-Forward Network (FFN) weights. They observe that domain-matched calibration data helps maintain the "energy distribution" of weight frequency componentsâ€”specifically preserving high-frequency information associated with specialized reasoning. Random data tends to distort mid-to-high frequency bands, leading to reasoning degradation.
- **Core assumption:** Specialized capabilities (math/code) are encoded in specific frequency bands of the FFN weights that are vulnerable to compression unless activated by relevant calibration data.
- **Evidence anchors:**
  - [section 3.3] "These effects stem from activation pathway preservation, where calibration data activates and helps preserve subject-specific neural pathways..."
  - [Appendix E] "COLA's ability to better preserve the energy distribution across all frequency bands is a direct result of its focus on maximizing both representativeness and diversity in activation space."
- **Break condition:** This spectral preservation is a proposed explanation ("mechanistic explanation") rather than a definitive proof of causality; it relies on the correlation between spectral flatness and task performance.

## Foundational Learning

- **Concept: Post-Training Compression (PTC)**
  - **Why needed here:** The paper optimizes data for *post-training* methods (quantization/pruning), which are "training-free" and highly sensitive to the small calibration dataset used. This differs from Quantization-Aware Training (QAT).
  - **Quick check question:** Why does a 128-sample dataset determine the performance of a 7B parameter model in post-training compression? (Answer: Because it sets the dynamic ranges and weight importance metrics used to reconstruct the model).

- **Concept: Activation Space & K-Means**
  - **Why needed here:** COLA's core algorithm (Stage 3) selects samples by clustering their activation vectors. You need to understand that "activation space" refers to the high-dimensional hidden states generated by the model's forward pass, not the input text itself.
  - **Quick check question:** Does COLA cluster the *text* of the calibration data or the *model's response* to that text? (Answer: The model's response, specifically the aggregated hidden states).

- **Concept: Compositional Properties**
  - **Why needed here:** The paper details how sequence length and format (reasoning chains) affect compression. Understanding that "longer is generally better" (up to 2048 tokens) because it provides richer gradient/activation statistics is crucial for implementing Stage 2.
  - **Quick check question:** Why might short calibration sequences hurt math performance more than commonsense performance? (Answer: Math requires longer reasoning chains/context to trigger the necessary activation patterns).

## Architecture Onboarding

- **Component map:**
  - Stage 1 (Dataset Selection) -> Stage 2 (Processing) -> Stage 3 (Sample Selection)

- **Critical path:**
  1.  **Forward Pass:** Run candidate samples through the *uncompressed* model to capture layer-wise activations (`h_l_i`).
  2.  **Dimensionality Reduction:** Use Random Projection (efficient for large dims) to reduce activation vectors to size `d` (e.g., 64).
  3.  **Clustering:** Run K-Means (k=128) on the reduced vectors.
  4.  **Selection:** Pick the sample closest to each cluster centroid (`arg min ||a'_i - mu_j||`). This yields 128 diverse, representative samples.

- **Design tradeoffs:**
  - **Random Projection vs. PCA:** The paper uses Random Projection for efficiency (O(DD')). This is faster but less precise than PCA, suitable for the high dimensionality of LLM activations.
  - **Cluster Count (k):** Set to the desired final sample size (e.g., 128). Lower `k` reduces compute but may miss rare activation patterns (diversity drops).

- **Failure signatures:**
  - **Spectral Truncation:** If calibration fails, visualizations of FFN weights will show a "tail drop" in the high-frequency domain (0.6-1.0), correlating with drops in MATH/Code scores.
  - **Unbalanced Coverage:** If Stage 1 selects only generic data (e.g., WikiText), specialized capabilities (Math) may drop by 5-10% even if perplexity remains low.

- **First 3 experiments:**
  1.  **Baseline Random vs. COLA:** Compare perplexity and task accuracy (GSM8K/HumanEval) of a SparseGPT-compressed model using 128 random WikiText samples vs. 128 COLA-selected samples.
  2.  **Ablation on Stages:** Run COLA removing Stage 2 (Processing) to quantify the isolated value of adding explicit reasoning chains vs. just clustering raw text.
  3.  **Spectral Visualization:** Plot the Fourier spectrum of FFN weights for both the original and compressed models to verify if COLA successfully preserves the high-frequency components as claimed in Appendix E.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can calibration data curation strategies be optimized to specifically account for the unique algorithmic characteristics of different compression methods and the intrinsic properties of diverse target LLMs?
- Basis in paper: [explicit] The authors state in the conclusion that "Future work will focus on developing compression method-specific calibration data curation strategies that account for algorithmic characteristics and the intrinsic properties of target LLMs."
- Why unresolved: The current COLA framework is general-purpose; the paper does not explore how to tailor curation specifically for the mechanics of pruning vs. quantization or for specific model families beyond the tested LLaMA and Qwen architectures.
- What evidence would resolve it: A study comparing tailored vs. generic curation pipelines across a wider variety of compression algorithms (e.g., distillation) and model architectures.

### Open Question 2
- Question: Do the findings regarding activation-space representativeness and diversity generalize to architectures with different structural mechanisms, such as Mixture-of-Experts (MoE) or linear attention models?
- Basis in paper: [inferred] In Appendix R (Limitation Analysis), the authors note that their experiments focused on specific LLMs (LLaMA3, Qwen2.5) and that "findings may not generalize to other architectures, especially... mixture of experts or linear attention."
- Why unresolved: MoE models activate parameters sparsely, and linear attention models handle context differently, potentially altering how "representativeness" in activation space correlates with capability preservation.
- What evidence would resolve it: Experimental results applying the COLA framework to MoE models (e.g., Mixtral) or linear attention models and analyzing the resulting capability preservation.

### Open Question 3
- Question: What are the formal theoretical guarantees or minimum bounds regarding calibration data characteristics required to ensure capability preservation?
- Basis in paper: [inferred] The authors acknowledge in Appendix R that "Our work is primarily empirical, lacking formal theoretical guarantees about optimal calibration data characteristics or minimum requirements for capability preservation."
- Why unresolved: While the paper establishes a strong empirical correlation between activation diversity/representativeness and performance, it does not derive mathematical proofs defining the theoretical limits or minimum sufficient conditions for these properties.
- What evidence would resolve it: Theoretical modeling of the compression error bounds based on calibration set properties, or empirical determination of the "minimum" viable calibration set size for specific error tolerances.

## Limitations

- The proposed spectral energy preservation mechanism remains correlational rather than proven causal
- COLA's effectiveness depends on the assumption that uncompressed model activations are representative of deployment distribution
- Dataset selection heuristics (embedding similarity/KL divergence) haven't been validated across diverse specialized domains

## Confidence

- **High Confidence:** The empirical demonstration that COLA-selected calibration data consistently outperforms random sampling across multiple compression methods (AWQ, GPTQ, SparseGPT) and evaluation metrics (perplexity, task accuracy). The ablation studies showing individual stage contributions are methodologically sound.
- **Medium Confidence:** The claim that activation space diversity more fundamentally determines calibration quality than semantic diversity. While the clustering approach is principled, the paper doesn't provide direct comparison between activation-based selection and semantic-based selection on the same dataset.
- **Low Confidence:** The mechanistic explanation linking spectral energy preservation to capability retention. The Fourier analysis is a useful visualization but doesn't establish a causal relationship between frequency band preservation and task performance.

## Next Checks

1. **Targeted Ablation of Spectral Components:** Design an experiment where FFN weights are intentionally truncated at different frequency thresholds during compression, then measure the corresponding impact on specialized capabilities (MATH vs. commonsense). This would directly test whether high-frequency components are causally linked to reasoning capabilities as claimed.

2. **Distributional Shift Robustness:** Evaluate COLA's performance when the deployment domain significantly differs from the calibration data. For example, train on general web text but deploy on medical literature, or train on English but deploy on code documentation. Measure whether COLA's activation clustering still selects relevant samples in these scenarios.

3. **Activation-Based vs. Semantic-Based Selection Comparison:** Implement a baseline calibration data curation method that selects samples based on semantic diversity (e.g., BERT-based clustering of input text) rather than activation diversity. Compare this directly against COLA on the same datasets to quantify the marginal value of activation-space optimization.