---
ver: rpa2
title: Batch Normalization-Free Fully Integer Quantized Neural Networks via Progressive
  Tandem Learning
arxiv_id: '2512.16476'
source_url: https://arxiv.org/abs/2512.16476
tags:
- neural
- networks
- training
- teacher
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a fully integer quantized neural network
  training framework that eliminates batch normalization (BN) layers, enabling true
  integer-only deployment on resource-constrained hardware. The core idea is progressive
  tandem learning, which uses a pretrained BN-enabled quantized teacher model to guide
  the training of a BN-free student through layer-wise knowledge distillation and
  per-layer scale factor initialization.
---

# Batch Normalization-Free Fully Integer Quantized Neural Networks via Progressive Tandem Learning

## Quick Facts
- arXiv ID: 2512.16476
- Source URL: https://arxiv.org/abs/2512.16476
- Reference count: 33
- One-line primary result: BN-free quantized AlexNet achieves 47.72% Top-1 accuracy on ImageNet with 8-bit weights/activations

## Executive Summary
This paper introduces a fully integer quantized neural network training framework that eliminates batch normalization (BN) layers, enabling true integer-only deployment on resource-constrained hardware. The core idea is progressive tandem learning, which uses a pretrained BN-enabled quantized teacher model to guide the training of a BN-free student through layer-wise knowledge distillation and per-layer scale factor initialization. The student model incorporates lightweight integer scaling layers to replace BN's stabilizing effects. On ImageNet with AlexNet, the BN-free model achieves competitive Top-1 accuracy under aggressive quantization: 47.72% (vs 47.00% teacher) with 8-bit weights/activations and 49.45% (vs 48.50% teacher) with 4-bit weights/activations.

## Method Summary
The method uses a three-stage training pipeline: (1) Train or obtain a BN-enabled quantized teacher using DoReFa-Net scheme, (2) Initialize student weights and per-layer scale factors by minimizing local loss against teacher post-BN activations, (3) Train each layer progressively while freezing all preceding layers, using teacher activations as input. The student architecture removes all BN layers and inserts integer scale factors after each conv/FC layer. Scale factors are initialized to minimize L2 distance between teacher post-BN activations and scaled student activations, then folded into quantization operations for pure integer inference. The layer-wise progressive training prevents error accumulation by localizing mismatches within each block before propagating forward.

## Key Results
- AlexNet achieves 47.72% Top-1 accuracy (vs 47.00% teacher) with 8-bit weights/activations
- AlexNet achieves 49.45% Top-1 accuracy (vs 48.50% teacher) with 4-bit weights/activations
- CIFAR-10 with VGG-Small achieves 90.8% accuracy under 1-bit/1-bit quantization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer-wise progressive distillation prevents error accumulation when removing BN from quantized networks
- Mechanism: By training one layer at a time while freezing all preceding layers and using the teacher's frozen activations as input, mismatches are localized and corrected within each block before propagating forward
- Core assumption: Teacher's intermediate activations contain sufficient information to guide student's layer-wise learning without requiring end-to-end gradient flow
- Evidence anchors: [abstract] "layer-wise targets and progressive compensation to train a student"; [section III-B] "By minimizing L_i, we optimize only the quantized weights in layer i... Once layer i converges, its parameters are frozen, and we proceed to stage i+1"
- Break condition: If teacher and student architectures diverge significantly (different layer counts or connectivity), layer-wise alignment becomes ill-defined

### Mechanism 2
- Claim: Fixed per-layer integer scale factors can approximate BN's normalization effect sufficiently for low-bit inference
- Mechanism: A learnable integer α is initialized by minimizing L2 distance between teacher post-BN activations and scaled student activations. Once learned, α is folded into the quantization path, preserving pure integer arithmetic at inference
- Core assumption: Static per-layer scaling captures enough of BN's distribution-centering effect; dynamic per-batch statistics are not strictly necessary after training converges
- Evidence anchors: [abstract] "per-layer scale factor initialization... integrates directly with existing quantization pipelines"; [section III-B] "During inference, the scale factor is incorporated into the quantization step as r_o ← α·r_o/(2^m−1), thus preserving pure-integer arithmetic"
- Break condition: Very deep or highly nonlinear networks may require adaptive per-channel scales; fixed per-layer scales may under-correct for heterogeneous activation distributions

### Mechanism 3
- Claim: Three-stage training pipeline (initialization → progressive tandem → freeze) stabilizes BN-free quantized training
- Mechanism: Stage 1 learns scale factors and initializes quantized weights using local losses. Stage 2 trains each layer progressively with frozen predecessors. Stage 3 yields a fully integer model with no dynamic normalization
- Core assumption: Training overhead of progressive layer-wise optimization is acceptable given one-time deployment gains in inference efficiency
- Evidence anchors: [section III-B, Algorithm 1] Explicit three-stage process with freezing and layer-wise training; [section IV] Results show <1% accuracy degradation at 4-bit compared to BN-enabled teacher
- Break condition: If wall-clock training time is severely constrained, L-stage sequential training may be impractical for very deep networks

## Foundational Learning

- **Concept**: Quantization-aware training (QAT) with straight-through estimators
  - Why needed here: DoReFa-Net scheme quantizes weights/activations during training using round() operations, requiring STE for gradient approximation through discrete operations
  - Quick check question: Can you explain why gradients can't flow directly through a round() operation and how STE approximates the backward pass?

- **Concept**: Batch Normalization's role in training stability
  - Why needed here: Understanding what BN provides (mean/variance normalization, gradient conditioning) is essential to grasp why removing it risks activation saturation and why the scale factor replacement is designed
  - Quick check question: What statistics does BN compute during training vs. inference, and why do these require floating-point operations?

- **Concept**: Knowledge distillation at feature level vs. logit level
  - Why needed here: This method uses layer-wise (feature-level) distillation, not just output logit matching. Understanding the difference clarifies why local losses enable layer-by-layer transfer
  - Quick check question: What is the difference between matching softmax outputs (logit-level KD) and matching intermediate feature maps (layer-wise KD)?

## Architecture Onboarding

- **Component map**: Teacher model (BN-enabled quantized) -> Local loss module -> Student model (BN-free with scale factors) -> Progressive trainer
- **Critical path**: 1) Train BN-enabled quantized teacher using DoReFa-Net, 2) Copy architecture to student, remove all BN layers, insert scale factors, 3) Run Stage 1: For each layer ℓ, initialize weights and learn αℓ by minimizing local loss against teacher, 4) Run Stage 2: For i=1 to L, freeze layers 1:i-1, train layer i using teacher activations as input, 5) Deploy student with scale factors folded into quantization operations
- **Design tradeoffs**: Training time vs. inference efficiency (progressive training adds L sequential stages); Fixed vs. adaptive scales (current design uses per-layer fixed α; per-channel adaptive scales may improve accuracy in deeper networks); Bit-width selection (lower bits show slightly larger accuracy gaps)
- **Failure signatures**: Accuracy collapse (>5% degradation) likely indicates scale factor initialization failed; Activation saturation (all zeros/extremes) suggests scale factor may be too small/large; Mismatched layer outputs during Stage 2 indicates teacher-student architectural divergence
- **First 3 experiments**: 1) Sanity check on CIFAR-10/VGG-Small: Reproduce the 1-bit/1-bit result (90.8%) to validate pipeline before scaling to ImageNet, 2) Scale factor ablation: Compare fixed per-layer α vs. random initialization to confirm initialization mechanism contribution, 3) Bit-width sweep on ImageNet/AlexNet: Run (8,8), (4,4), and optionally (2,2) configurations to map accuracy-efficiency frontier for target hardware

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the FIQNN framework generalize to modern architectures beyond AlexNet, such as ResNets, MobileNets, or Vision Transformers, while maintaining competitive accuracy under aggressive quantization?
- Basis in paper: [explicit] "Due to current resource limitations, we have evaluated our framework only on DoReFa-Net with two precision settings; future work should assess additional architectures and quantization configurations to further validate and generalize the approach"
- Why unresolved: The paper validates only on AlexNet (shallow) and VGG-Small. Modern architectures with skip connections, depth-wise convolutions, or attention mechanisms may interact differently with layer-wise distillation and fixed scale factors
- What evidence would resolve it: Systematic experiments applying FIQNN to ResNet-50/101, MobileNetV2, and ViT on ImageNet, comparing BN-free student accuracy to BN-enabled teachers

### Open Question 2
- Question: Would learnable per-channel scale factors improve performance over fixed per-layer scales, particularly for very deep or highly nonlinear networks?
- Basis in paper: [explicit] "Our current use of fixed, per-layer integer scales may demand careful calibration in very deep or highly nonlinear networks—preliminary evidence suggests that adaptive, learnable per-channel scales can better match activation distributions"
- Why unresolved: A single integer scale per layer may inadequately capture channel-wise activation variance in deeper networks where BN's per-channel statistics provide important normalization
- What evidence would resolve it: Ablation study comparing fixed per-layer vs. learnable per-channel scales on deep architectures, measuring accuracy, calibration effort, and hardware overhead

### Open Question 3
- Question: How robust is FIQNN under hardware-induced noise conditions such as analog inference or low-voltage operation on memristor-based accelerators?
- Basis in paper: [explicit] "Finally, we will evaluate robustness under hardware-induced noise (e.g., analog inference, low-voltage operation) and combine our quantization strategy with cell-stitching techniques to bolster reliability on emerging accelerator platforms"
- Why unresolved: The paper validates accuracy in simulation only. Actual deployment on analog hardware introduces noise that may affect integer operations and scale factors differently than floating-point BN
- What evidence would resolve it: Deployment benchmarks on actual memristor or analog neuromorphic hardware, measuring accuracy degradation under varying noise and voltage conditions

### Open Question 4
- Question: Can FIQNN be effectively combined with pruning or low-rank factorization for additional model compression without destabilizing the scale factor calibration?
- Basis in paper: [explicit] "Our method is complementary to advanced model-compression strategies (e.g., pruning, low-rank factorisation), which we plan to explore to further reduce network complexity"
- Why unresolved: Pruning alters layer-wise activation distributions, potentially requiring re-calibration of scale factors. The interaction between progressive distillation and weight removal is unexplored
- What evidence would resolve it: Experiments applying structured pruning to trained FIQNN models, measuring combined compression-accuracy trade-offs and scale recalibration requirements

## Limitations
- Method demonstrated only on AlexNet and VGG-Small, not validated on deeper modern architectures
- Layer-wise progressive training adds significant wall-clock time overhead proportional to network depth
- Assumes perfect teacher-student architectural alignment, which may not hold in real-world scenarios
- Scalability to per-channel quantization schemes remains untested with current fixed per-layer scale factors

## Confidence
- **High confidence**: Feasibility of BN-free quantized networks with integer-only inference is well-supported by experimental results showing competitive accuracy (47.72% Top-1 for AlexNet at 8-bit) compared to BN-enabled teachers
- **Medium confidence**: Layer-wise progressive distillation mechanism is theoretically sound and partially validated, but lacks ablation studies showing what happens if stages are skipped or modified
- **Medium confidence**: Scale factors can adequately replace BN's normalization effects is supported by experimental results but not rigorously analyzed through sensitivity studies or compared against alternative approaches

## Next Checks
1. **Architecture generalization test**: Validate the method on ResNet-18/50 for ImageNet to assess performance on modern architectures beyond AlexNet, measuring both accuracy retention and training time overhead
2. **Scale factor sensitivity analysis**: Conduct controlled experiments varying the initialization method and learnability of scale factors (per-layer vs. per-channel) to quantify their impact on final accuracy and identify breaking points
3. **Training efficiency benchmark**: Measure wall-clock training time for progressive stages across different depths (VGG-Small, ResNet-18, ResNet-50) and compare against alternative BN-removal methods to quantify the practical deployment tradeoff