---
ver: rpa2
title: 'EMODIS: A Benchmark for Context-Dependent Emoji Disambiguation in Large Language
  Models'
arxiv_id: '2511.07193'
source_url: https://arxiv.org/abs/2511.07193
tags:
- context
- emoji
- llms
- disambiguation
- emodis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EMODIS, a new benchmark for evaluating large
  language models on context-dependent emoji disambiguation. EMODIS provides sentence-emoji
  pairs with contrastive contexts that lead to different interpretations, requiring
  models to reason beyond surface-level cues.
---

# EMODIS: A Benchmark for Context-Dependent Emoji Disambiguation in Large Language Models

## Quick Facts
- arXiv ID: 2511.07193
- Source URL: https://arxiv.org/abs/2511.07193
- Reference count: 13
- Key outcome: Models achieve only 58.8% pair-wise accuracy on emoji disambiguation, significantly below human performance of 88.5%

## Executive Summary
EMODIS introduces a new benchmark for evaluating large language models on context-dependent emoji disambiguation. The benchmark provides sentence-emoji pairs with contrastive contexts that lead to different interpretations, requiring models to reason beyond surface-level cues. The authors evaluate multiple API-based and open-source models and find that even the strongest models (e.g., GPT-4) achieve only 58.8% pair-wise accuracy, significantly below human performance (88.5%). Analysis reveals systematic biases toward dominant emoji interpretations and limited sensitivity to subtle contextual changes, especially in open-source models.

## Method Summary
EMODIS comprises 1000 manually curated instances, each containing an ambiguous sentence with an emoji, two distinct disambiguating contexts leading to divergent interpretations, and a question with two answers. The evaluation uses pair-wise accuracy (both contexts correct), query-wise accuracy (average individual accuracy), context awareness score (output divergence across contexts), and output variability score (context removal sensitivity). Models are evaluated at temperature=0.2, with GPT-4 serving as automatic verifier for semantic equivalence. The benchmark is structured around four context types: temporal information, domain theme, cultural background, and social intent.

## Key Results
- GPT-4 achieves highest pair-wise accuracy at 58.8%, still far below human performance of 88.5%
- Open-source models perform significantly worse, with LLaMA-7B at 30.2% pair-wise accuracy
- Models show strong bias toward dominant interpretations, with some predicting 2:32 literal:figurative against balanced 17:17 groundtruth
- Social intent category shows largest performance gap (27.6-54.2% across models)

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Context Pairing
Requiring models to produce different answers for the same ambiguous sentence under minimally different contexts exposes whether they integrate context or rely on priors. Each instance provides (T, C‚ÇÅ, Q) and (T, C‚ÇÇ, Q) where C‚ÇÅ and C‚ÇÇ are designed to resolve emoji ambiguity in opposite directions. Pair-wise accuracy only credits the model if it answers both correctly, punishing context-insensitive guessing. If contexts are not sufficiently minimal (leak explicit lexical cues), models may solve via pattern matching rather than contextual reasoning.

### Mechanism 2: Context Awareness Scoring
Measuring output divergence across context variants quantifies a model's sensitivity to pragmatic contrast. Context awareness (Score_c‚Çê) counts how often O‚ÇÅ‚ÇÅ ‚â† O‚ÇÅ‚ÇÇ across paired inputs; output variability (Score_ov) measures change when context is removed entirely. Higher scores indicate genuine context integration. If temperature or sampling causes high output variance unrelated to context, Score_c‚Çê may conflate randomness with sensitivity.

### Mechanism 3: Interpretation Bias Exposure via Balanced Emoji Sampling
Testing emojis with equally plausible literal/figurative meanings reveals pretraining-induced prior dominance. For selected emojis, groundtruth is balanced (e.g., 17:17 literal:figurative). Model prediction skew toward one interpretation indicates reliance on pretraining associations over contextual evidence. If emoji selection inadvertently favors one interpretation in ways not controlled by context design, apparent bias may reflect benchmark artifact.

## Foundational Learning

- **Word Sense Disambiguation (WSD)**: Emoji disambiguation is a form of symbolic sense disambiguation requiring context-aware sense selection; understanding WSD evaluation metrics and failure modes transfers directly. Quick check: Given "The bank was steep" vs. "The bank closed at 5pm," can you identify which contextual cues disambiguate "bank"?

- **Pragmatic Inference**: EMODIS's taxonomy (temporal, domain, cultural, social intent) requires reasoning beyond literal semantics; models fail most on social intent (Acc‚Çö: 27.6‚Äì54.2% across models). Quick check: How does the interpretation of "Nice job üôÑ" change between a sincere vs. sarcastic speaker?

- **Evaluation Validity via Contrastive Design**: Understanding why single-query accuracy can be misleading (right answer for wrong reason) explains why EMODIS uses paired evaluation. Quick check: If a model always outputs the most common emoji meaning, would it achieve high query-wise accuracy? Would pair-wise accuracy differ?

## Architecture Onboarding

- **Component map**: Dataset (1000 instances) ‚Üí Target text with emoji + 2 contrastive contexts + question + 2 answers ‚Üí Model inference ‚Üí GPT-4 verifier ‚Üí Acc‚Çö/Acc‚Çö scoring + context sensitivity metrics ‚Üí Human verification (3 annotators)

- **Critical path**: 1) Target text authoring (ensure genuine ambiguity in isolation) 2) Context pair design (minimal cues, divergent resolutions) 3) Question authoring (neutral, under-informative without context) 4) Human verification (ambiguity check, context efficacy, no bypass clues) 5) Model evaluation with temperature=0.2 6) GPT-4 semantic equivalence judgment

- **Design tradeoffs**: Manual curation ensures quality but limits scalability; brief contexts match current LLM capabilities but may not reflect real-world complexity; GPT-4 as evaluator achieves 90%+ human agreement but introduces dependency on another LLM; open-form answers vs. multiple-choice: more realistic but requires semantic matching

- **Failure signatures**: High Acc‚Çö but low Acc‚Çö ‚Üí model guesses correctly but doesn't switch with context change; low context awareness (Score_c‚Çê) with moderate accuracy ‚Üí relying on priors, not context; consistent figurative preference across all contexts ‚Üí pretraining bias dominance; social intent category underperformance (observed: 10-20 point gap vs. other categories)

- **First 3 experiments**: 1) Baseline evaluation: Run target model on EMODIS with default prompt, report Acc‚Çö, Acc‚Çö, and Score_c‚Çê; compare gap between metrics to diagnose prior-reliance vs. context-integration 2) Context ablation: Evaluate same model with context removed; compute Score_ov to quantify context dependency 3) Bias stratification: Select 5 emojis with known literal/figurative ambiguity; run evaluation and compute literal:figurative prediction ratio; compare against groundtruth balance

## Open Questions the Paper Calls Out

### Open Question 1
What specific training interventions or prompting strategies can effectively mitigate a Large Language Model's reliance on prior emoji associations when presented with contradictory contextual evidence? The analysis section states that "Reducing such biases is crucial for improving robustness in context-dependent interpretation" after noting that models often "default to the most frequent or stereotypical interpretation." A study comparing baseline models against those fine-tuned with contrastive context loss or specific prompting techniques on the EMODIS benchmark would resolve this.

### Open Question 2
How does emoji disambiguation performance vary when the context shifts from minimal, single-sentence cues to complex, multi-turn conversational history? The limitations section notes that "contexts is deliberately kept brief to match current LLM capabilities" and suggests "Future versions can incorporate more complex and layered contexts to better reflect real-world usage." Evaluation of LLMs on an extended version of the EMODIS dataset where contexts are expanded to full conversational turns would resolve this.

### Open Question 3
To what extent does the inclusion of non-textual disambiguation factors, such as speaker identity, emotional tone, or visual scene information, improve disambiguation accuracy? The limitations section explicitly lists "speaker identity, emotional tone, or multimodal information" as disambiguation factors that are not considered in the current taxonomy and text-only design. Benchmarking multimodal models on EMODIS instances augmented with corresponding visual or audio contexts would resolve this.

### Open Question 4
Does the observed failure to distinguish contrastive contexts stem from an inability to encode the subtle semantic differences in the context or a failure to inhibit the dominant semantic representation of the emoji? The paper notes a "notable gap between $Acc_p$ and $Acc_q$" and that models "often produce identical... responses," suggesting a fundamental deficiency in context-sensitive semantic reasoning. Layer-wise analysis of internal model representations to determine if the contextual nuance is successfully encoded in the hidden states despite the model generating an incorrect final answer would resolve this.

## Limitations
- Manual dataset curation limits scalability and reproducibility
- Brief context snippets (average 3.6 words) may not adequately represent real-world emoji disambiguation scenarios
- Use of GPT-4 as automatic verifier introduces potential circularity and dependency on another LLM

## Confidence

**High confidence**: Comparative performance ranking across models (GPT-4 > DeepSeek-v3 > Gemini 2.5 > others) and systematic bias toward dominant emoji interpretations.

**Medium confidence**: Claims about "significantly below human performance" and taxonomy-based performance differences, though gaps may be partially attributable to artificial brevity of contexts.

**Low confidence**: Assertions that contrastive context pairing uniquely exposes contextual reasoning failures and GPT-4 verifier's 90%+ agreement eliminating evaluator bias concerns.

## Next Checks

1. **Evaluator independence test**: Re-run EMODIS evaluation using human annotators (not GPT-4) for verification on 100 instances. Compare accuracy metrics and identify systematic discrepancies between LLM and human judgment patterns.

2. **Context length ablation study**: Create extended context versions (5-10 words vs. current 3.6 average) for 50 representative instances and re-evaluate top 3 models. Measure changes in Acc‚Çö and Score_c‚Çê to determine if performance scales with context richness.

3. **Cross-linguistic validation**: Translate 100 EMODIS instances into a non-English language and evaluate multilingual models on both versions. Compare performance drops to isolate whether weaknesses stem from language-specific pretraining biases versus general contextual reasoning limitations.