---
ver: rpa2
title: 'RAD: Retrieval High-quality Demonstrations to Enhance Decision-making'
arxiv_id: '2507.15356'
source_url: https://arxiv.org/abs/2507.15356
tags:
- trajectory
- offline
- states
- target
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RAD addresses limited generalization in offline RL due to sparse
  data by dynamically retrieving high-return states from offline datasets and using
  a condition-guided diffusion model to plan toward them. This retrieval-based approach
  stitches trajectories flexibly and improves adaptability in out-of-distribution
  scenarios without relying on static augmentation.
---

# RAD: Retrieval High-quality Demonstrations to Enhance Decision-making

## Quick Facts
- arXiv ID: 2507.15356
- Source URL: https://arxiv.org/abs/2507.15356
- Reference count: 30
- Average normalized return of 81.2 on D4RL MuJoCo tasks, outperforming diffusion-based baselines

## Executive Summary
RAD addresses the challenge of limited generalization in offline reinforcement learning (RL) by dynamically retrieving high-return states from offline datasets and using a condition-guided diffusion model to plan trajectories toward them. Unlike static data augmentation methods, RAD's retrieval-based approach flexibly stitches trajectories to improve adaptability in out-of-distribution scenarios. Evaluated on D4RL MuJoCo tasks, RAD achieves an average normalized return of 81.2, surpassing baselines like Diffuser (77.5) and Decision Transformer (78.9), with notable gains in Hopper and Walker2d environments.

## Method Summary
RAD combines retrieval-based planning with diffusion models to enhance decision-making in offline RL. It builds a state database from the offline dataset, retrieves high-return states similar to the current state, and uses a diffusion model conditioned on the current and target states to plan a trajectory. The method includes a step estimation module to predict the time offset between states, improving temporal coherence. At inference, RAD retrieves target states, estimates the step offset, denoises a trajectory using the diffusion model, and executes the first action. This dynamic retrieval and planning approach improves performance in sparse data regimes and out-of-distribution scenarios.

## Key Results
- RAD achieves an average normalized return of 81.2 on D4RL MuJoCo tasks, outperforming Diffuser (77.5) and Decision Transformer (78.9).
- Notable performance gains in Hopper and Walker2d environments.
- Ablation studies confirm the effectiveness of retrieval, step estimation, and diffusion components.
- Performance is sensitive to similarity thresholds and selection range, requiring careful hyperparameter tuning.

## Why This Works (Mechanism)
RAD enhances decision-making by dynamically retrieving high-return states from the offline dataset and planning trajectories toward them using a condition-guided diffusion model. This approach addresses the challenge of sparse data in offline RL by providing the agent with relevant, high-quality demonstrations. The retrieval mechanism ensures that the agent learns from states that are both similar to the current state and associated with high returns, improving adaptability and performance in out-of-distribution scenarios. The step estimation module further refines the planning process by predicting the temporal offset between states, ensuring temporal coherence in the generated trajectories.

## Foundational Learning
- Concept: **Offline Reinforcement Learning (MDP formulation)**
  - Why needed here: Understand the problem setting where an agent learns a policy $\pi_\theta$ to maximize return from a fixed dataset $D$ without environment interaction.
  - Quick check question: Can you explain why distributional shift is a core challenge in offline RL?
- Concept: **Denoising Diffusion Probabilistic Models (DDPMs)**
  - Why needed here: RAD's planner is a diffusion model; you must understand the forward/reverse process and noise prediction objective $\|\epsilon - \epsilon_\theta(\tau^i, i)\|^2$.
  - Quick check question: Describe how a DDPM generates a sample from pure noise.
- Concept: **k-Nearest Neighbors & Vector Similarity Search**
  - Why needed here: The target selection module retrieves states via cosine similarity from a database.
  - Quick check question: How does cosine similarity differ from Euclidean distance for high-dimensional vectors?

## Architecture Onboarding
- Component map: **Target Selection Module** (retrieves $s_g$ from DB) $\to$ **Step Estimation Module** (predicts $\hat{i}$) $\to$ **Condition-Guided Diffusion Planner** (generates $\hat{\tau}^0_t$).
- Critical path: Building the state database (offline) $\to$ Training step estimator & diffusion model (offline) $\to$ At inference: retrieve $s_g$, predict $\hat{i}$, denoise trajectory, execute first action.
- Design tradeoffs: **Similarity threshold $\delta$**: Higher values ($\delta=0.9$) improve relevance but may yield fewer candidates. **Top-k selection**: Small $k$ ($k=6$) balances relevance/diversity; larger $k$ adds noise. These require tuning per dataset (Section 5.4).
- Failure signatures: Performance degrades significantly in environments with noisy/sparse data (e.g., HalfCheetah) if retrieval finds no meaningful targets. Ablation shows removing retrieval or step estimation causes substantial performance drops.
- First 3 experiments:
  1. Validate database construction: Verify all states are indexed with correct trajectory IDs, timesteps, and pre-computed returns.
  2. Test retrieval module in isolation: For sample states, check if retrieved candidates satisfy similarity ($\geq \delta$) and return ($\leq \eta$ from $v^*$) constraints.
  3. Train and overfit to a single trajectory: Verify the diffusion model and step estimator can reconstruct known sub-trajectories when $s_t$ and $s_g$ are from the same path.

## Open Questions the Paper Calls Out
- **Question:** How can RAD be made robust to noisy or stochastic environments where standard retrieval mechanisms may fail?
  - Basis in paper: [explicit] The "Limitations and Future Work" section states that RAD's success depends on retrieving reachable states and that "in noisy or stochastic settings, retrieval may fail."
  - Why unresolved: The current reliance on fixed similarity metrics (like cosine similarity) may be brittle when state observations are noisy or dynamics are stochastic, degrading the quality of retrieved targets.
  - What evidence would resolve it: Evaluations of RAD on stochastic control benchmarks (e.g., D4RL with added noise) demonstrating maintained performance using learned embeddings or value-aware filtering.

- **Question:** How can RAD effectively plan in sparse environments where the offline dataset lacks high-return target states?
  - Basis in paper: [explicit] The authors note that "RAD also assumes that such target states exist in the dataset. In sparse environments, this may not hold."
  - Why unresolved: RAD relies on "stitching" to existing high-return trajectories; if the dataset never visits high-reward areas, the retrieval mechanism has no valid targets to guide the agent.
  - What evidence would resolve it: Integration of learned goal proposals or offline exploration priors that show success in sparse-reward tasks like AntMaze without dense expert data.

- **Question:** Does incorporating global planning or multi-step retrieval enhance RAD's long-horizon reasoning capabilities?
  - Basis in paper: [explicit] The conclusion suggests that "RAD performs retrieval locally per decision point; incorporating global planning or multi-step retrieval may further enhance long-horizon reasoning."
  - Why unresolved: The current method optimizes for the immediate next target state dynamically, which may result in sub-optimal global trajectory coherence over very long horizons.
  - What evidence would resolve it: Ablation studies comparing local vs. global retrieval mechanisms on tasks requiring extended planning horizons.

## Limitations
- The method assumes high-return states from the dataset are useful goals for the current agent state, which may not hold in all domains.
- Performance degrades in environments with noisy or sparse data if retrieval fails to find meaningful targets.
- Critical hyperparameters like return tolerance $\eta$ and guidance scale $\rho$ are unspecified, hindering exact reproduction.

## Confidence
- **High:** The core retrieval-based planning concept is sound and the reported performance gains over strong baselines (Diffuser, Decision Transformer) are statistically significant on the tested tasks.
- **Medium:** The ablation studies support the effectiveness of the retrieval and step estimation components, but the impact of specific hyperparameters (e.g., $\delta$, $k$) is not fully explored.
- **Low:** The specific values and architectures for $\eta$, $\rho$, and the neural network layers are not provided, making exact reproduction challenging.

## Next Checks
1. **Diagnostic Logging for Retrieval:** Implement logging to monitor the number of valid candidates found by the retrieval module at inference time. If zero candidates are found frequently, experiment with relaxing the similarity threshold $\delta$.
2. **Step Estimator Validation:** Monitor the Mean Squared Error (MSE) of the step estimator on a held-out validation set. Additionally, visualize planned trajectories to ensure the generated path coherently connects the current state $s_t$ to the target state $s_g$ within the predicted number of steps $\hat{i}$.
3. **Hyperparameter Sensitivity Analysis:** Conduct a systematic sensitivity analysis for the key hyperparameters $\delta$ (similarity threshold), $k$ (number of candidates), and $\eta$ (return tolerance) to identify stable operating ranges across different D4RL datasets.