---
ver: rpa2
title: 'ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering
  with Relevance-Consistency Supervision'
arxiv_id: '2505.21250'
source_url: https://arxiv.org/abs/2505.21250
tags:
- question
- documents
- answer
- rescore
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReSCORE, a novel method for training dense
  retrievers for multi-hop question answering without labeled documents. ReSCORE leverages
  large language models to generate pseudo-ground-truth labels by measuring the joint
  probability of question relevance and answer consistency, then uses these labels
  to train the retriever via KL divergence loss within an iterative retrieval-augmented
  generation framework.
---

# ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision

## Quick Facts
- **arXiv ID**: 2505.21250
- **Source URL**: https://arxiv.org/abs/2505.21250
- **Reference count**: 11
- **Primary result**: ReSCORE trains retrievers for multi-hop QA without labeled documents by leveraging LLM-generated pseudo-ground-truth labels based on relevance and consistency, achieving state-of-the-art results on MuSiQue, HotpotQA, and 2WikiMHQA.

## Executive Summary
This paper introduces ReSCORE, a novel method for training dense retrievers for multi-hop question answering without labeled documents. ReSCORE leverages large language models to generate pseudo-ground-truth labels by measuring the joint probability of question relevance and answer consistency, then uses these labels to train the retriever via KL divergence loss within an iterative retrieval-augmented generation framework. Experiments on MuSiQue, HotpotQA, and 2WikiMHQA show that ReSCORE significantly improves both retrieval quality and end-to-end QA performance, achieving state-of-the-art results compared to existing iterative RAG systems. The approach also demonstrates broad applicability across different iterative RAG frameworks and provides insights into effective query reformulation and pseudo-label design for retriever training.

## Method Summary
ReSCORE trains dense retrievers for multi-hop QA without labeled query-document pairs by leveraging LLM-generated pseudo-ground truth labels. The method operates within an iterative RAG loop where, for each QA pair, the retriever iteratively retrieves top-k documents, the LLM generates answers and reformulates queries, and pseudo-GT distributions are computed using the joint probability P_LM(q,a|d) = P_LM(q|d) × P_LM(a|q,d). The retriever's query encoder is then trained to match this pseudo-GT distribution via KL divergence loss, while the document encoder remains frozen. This approach avoids the need for ground truth document labels and enables progressive retrieval of complementary documents across hops.

## Key Results
- ReSCORE achieves state-of-the-art EM/F1 scores on MuSiQue, HotpotQA, and 2WikiMHQA compared to existing iterative RAG systems.
- MHR@8 (multi-hop recall at 8) improves significantly with iterative training, showing the retriever learns to find complementary documents across hops.
- Pseudo-ground-truth labels outperform human ground-truth labels in the iterative setting, as GT-based training forces queries toward uninformative centroids of semantically distant documents.
- The method demonstrates broad applicability across different iterative RAG frameworks and query reformulation strategies.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Jointly modeling relevance and consistency produces more effective pseudo-labels for retriever training than either signal alone.
- Mechanism: The LLM computes `Q_LM(d|q) ∝ P_LM(q,a|d) = P_LM(q|d) × P_LM(a|q,d)`. The first term down-weights topically unrelated documents; the second term ensures the document supports answering the question. The product filters false positives that would score high on consistency alone (e.g., documents mentioning the answer token without relevance).
- Core assumption: The LLM's conditional probabilities meaningfully correlate with human judgments of relevance and consistency.
- Evidence anchors:
  - [abstract] "ReSCORE leverages large language models to capture each document's relevance to the question and consistency with the correct answer and use them to train a retriever."
  - [Section 3.2, Eq. 1-2] Formal decomposition; the FIFA World Cup example illustrating answer-only scoring failures.
  - [corpus] Limited direct corpus validation for joint probability pseudo-labeling; related work (KiRAG, RISE) focuses on iterative retrieval without explicit relevance-consistency supervision.
- Break condition: If the LLM assigns spuriously high probabilities to long or verbose documents regardless of relevance, the pseudo-labels will mislead training.

### Mechanism 2
- Claim: Iterative training within the RAG loop teaches the retriever to return complementary documents across hops rather than redundant ones.
- Mechanism: At each iteration i, the retriever receives gradients from `D_KL(Q_LM(i) || P_R(i))`. Because Q_LM is conditioned on the current query (potentially reformulated), the retriever learns to retrieve documents relevant to the current information need, not just the original question. This enables progressive retrieval of all ground-truth documents.
- Core assumption: Queries evolve meaningfully across iterations (via thought concatenation or rewriting) so that successive retrieval distributions target distinct sub-problems.
- Evidence anchors:
  - [Section 3.2] "The number of iterations is dynamically determined by the LLM and the process ends if the LLM predicts an answer which is not 'unknown'."
  - [Section 4.2.1, Table 2] MHR@8 improves with i for ReSCORE-equipped models while baselines plateau.
  - [corpus] Neighbor papers (RISE, Self-Critique Guided Iterative Reasoning) similarly emphasize iterative reasoning but do not directly address retriever training with iterative supervision.
- Break condition: If query reformulation fails to shift the information need (or the LLM terminates early), the retriever may over-fit to early-hop documents.

### Mechanism 3
- Claim: Pseudo-GT labels can outperform ground-truth document labels because iterative training avoids forcing the query embedding toward a centroid of semantically distant documents.
- Mechanism: GT-based contrastive training aligns the query with all required documents simultaneously, which may pull the query toward an uninformative centroid. ReSCORE's iterative training aligns the query with documents relevant to the current sub-question, enabling progressive retrieval.
- Core assumption: The multi-hop query naturally decomposes into sub-questions with distinct relevant documents.
- Evidence anchors:
  - [Section 4.2.3, Table 4] ReSCORE-trained retrievers achieve higher EM/F1 and MHR@8 than GT-trained retrievers on all three benchmarks.
  - [Section 4.2.3, Fig. 3] Proportion of questions where all relevant documents are found increases more sharply across iterations for pseudo-GT.
  - [corpus] No direct corpus comparison of pseudo-GT vs. GT labels for iterative MHQA retriever training.
- Break condition: If the task requires retrieving all relevant documents in a single hop (no meaningful decomposition), GT labels may be superior.

## Foundational Learning

- **Concept**: KL Divergence for distribution matching
  - Why needed here: The retriever is trained by minimizing `D_KL(Q_LM || P_R)`, aligning its retrieval distribution with the LLM's pseudo-GT distribution.
  - Quick check question: Can you explain why KL divergence is asymmetric and why `D_KL(P||Q) ≠ D_KL(Q||P)`?

- **Concept**: Dense Retrieval (bi-encoder architecture)
  - Why needed here: ReSCORE trains a query encoder `Embed_query(q)` while keeping `Embed_doc(d)` frozen; retrieval is via dot product + softmax.
  - Quick check question: What are the tradeoffs between bi-encoder and cross-encoder architectures for retrieval?

- **Concept**: Iterative RAG and query reformulation
  - Why needed here: ReSCORE operates within an iterative framework where queries are reformulated via thought concatenation or LLM rewriting.
  - Quick check question: How does query reformulation differ from query expansion, and what failure modes does each address?

## Architecture Onboarding

- **Component map**: Retriever (Contriever) -> LLM (Llama-3.1-8B-Instruct) -> Iterative RAG loop -> Pseudo-GT generation -> KL divergence loss
- **Critical path**:
  1. Initial query → retrieve top-k → LLM predicts answer or "unknown."
  2. If "unknown": generate thought → reformulate query → repeat.
  3. During training: for each iteration, compute `P_LM(q,a|d)` for top-M documents → normalize to `Q_LM` → compute KL loss vs. retriever's `P_R`.
- **Design tradeoffs**:
  - M (32) vs. k (8): Larger M improves pseudo-GT estimation but increases LLM compute; smaller k reduces context length but may miss relevant documents.
  - Thought-concat vs. LLM-rewrite: Thought-concat retains original question (error recovery); LLM-rewrite may simplify complex queries but risks losing focus.
  - Freezing document encoder: Reduces memory and improves stability but limits adaptation to the target domain.
- **Failure signatures**:
  - High MHR@1 but low MHR@η: Retriever is not learning complementary retrieval; check query reformulation quality.
  - Pseudo-GT dominated by few documents: LLM probabilities may be miscalibrated; inspect temperature scaling and prompt design.
  - Training loss plateaus early: May indicate insufficient query diversity or LLM over-confidence on a subset of documents.
- **First 3 experiments**:
  1. Reproduce Table 3 on a small validation slice: compare `P_LM(q|d)`, `P_LM(a|q,d)`, and `P_LM(q,a|d)` as reranking signals to validate the joint signal intuition.
  2. Ablate query reformulation: run ReSCORE with no reformulation vs. thought-concat vs. LLM-rewrite; plot MHR@i to confirm iterative complementarity.
  3. Compare GT vs. pseudo-GT labels on a single dataset: train two retrievers (GT via InfoNCE, pseudo-GT via KL) and report EM, F1, and MHR@i to verify the paper's claim that pseudo-GT outperforms GT in the iterative setting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ReSCORE maintain its performance gains when applied to out-of-distribution domains (e.g., legal or biomedical) that require specialized reasoning patterns different from the training datasets?
- Basis in paper: [explicit] The "Limitations" section states that the fine-tuning is specific to datasets like MuSiQue and HotpotQA, and the model faces an "Out-of-Distribution generalization challenge" regarding datasets with different characteristics.
- Why unresolved: The experiments are restricted to general knowledge datasets, and it is unclear if the pseudo-labeling mechanism translates to highly technical or specialized vocabularies.
- What evidence would resolve it: Zero-shot or few-shot evaluation on specialized multi-hop benchmarks (e.g., BioASQ or legal case reasoning) to assess domain transfer.

### Open Question 2
- Question: How can the iterative retrieval process be optimized to minimize latency and computational cost for high-hop questions without sacrificing the progressive retrieval of necessary documents?
- Basis in paper: [explicit] The "Limitations" section highlights that the iterative nature "increases computational costs and latency, especially for questions with high hop requirements," noting that further optimization is necessary for scalability.
- Why unresolved: While the method improves accuracy, the efficiency trade-off is acknowledged as a barrier to practical application, yet no solution is proposed in the current work.
- What evidence would resolve it: Introducing adaptive stopping criteria or parallel retrieval steps and measuring the resulting reduction in average inference time against the drop in Exact Match (EM) scores.

### Open Question 3
- Question: Does the phenomenon where ReSCORE's pseudo-labels outperform human Ground Truth (GT) labels persist in non-iterative or single-hop retrieval settings?
- Basis in paper: [inferred] Section 4.2.3 notes that Pseudo-GT outperforms GT labels because GT training forces queries to align with the "centroid" of multiple distant documents, a problem specific to multi-hop alignment.
- Why unresolved: It is unclear if the Pseudo-GT advantage is intrinsic to the method or merely a result of avoiding the specific "centroid" failure mode of multi-hop GT labels.
- What evidence would resolve it: A comparative ablation study on single-hop benchmarks (e.g., Natural Questions) to observe if GT labels reclaim superiority over the generated pseudo-labels.

## Limitations

- The evaluation relies on pseudo-ground-truth labels generated by a large language model, which may not align with human judgment or reflect the true distribution of relevance and consistency.
- The document corpus and indexing strategy are underspecified, limiting reproducibility.
- Subsampling methodology for validation/test sets is not detailed, which could affect generalizability.
- While the paper reports improvements over baselines, ablation studies on query reformulation and label generation methods are limited.

## Confidence

- **High confidence**: The core mechanism of using joint LLM probability for pseudo-labels (Mechanism 1) is well-supported by qualitative examples and ablation studies; iterative training improving complementarity (Mechanism 2) is validated by MHR@i trends; and the claim that pseudo-GT outperforms GT in iterative settings (Mechanism 3) is supported by EM/F1 and MHR@i metrics across all three datasets.
- **Medium confidence**: The claim that ReSCORE achieves state-of-the-art results depends on the specific baselines and metrics reported; the generalizability of the method across different iterative RAG frameworks is suggested but not extensively tested; and the robustness of the approach to LLM prompt variations or different retriever architectures is not fully explored.
- **Low confidence**: The method's performance on non-Wikipedia domains, different hop structures, or with different LLMs is not evaluated; potential biases introduced by the LLM's pseudo-labeling process are not investigated; and the computational efficiency of iterative pseudo-label generation is not discussed.

## Next Checks

1. **Replicate the main results**: Implement ReSCORE on a small validation slice of one dataset (e.g., MuSiQue) and reproduce Table 3 (EM, F1, MHR@8) to verify the reported improvements over baselines.
2. **Ablate query reformulation**: Compare ReSCORE with "None," "Thought-concat," and "LLM-rewrite" reformulation strategies on a single dataset, plotting MHR@i across iterations to confirm that iterative complementarity improves with effective reformulation.
3. **Compare pseudo-GT vs. GT labels**: Train two retrievers on the same dataset—one using GT labels via InfoNCE and one using pseudo-GT via ReSCORE's KL loss—and report EM, F1, and MHR@i to validate the paper's claim that pseudo-GT outperforms GT in the iterative MHQA setting.