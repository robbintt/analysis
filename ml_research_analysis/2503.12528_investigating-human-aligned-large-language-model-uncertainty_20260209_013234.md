---
ver: rpa2
title: Investigating Human-Aligned Large Language Model Uncertainty
arxiv_id: '2503.12528'
source_url: https://arxiv.org/abs/2503.12528
tags:
- uncertainty
- measures
- human
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how well various uncertainty measures\
  \ in large language models (LLMs) align with human group-level uncertainty. The\
  \ authors evaluate multiple measures\u2014including self-reported uncertainty, nucleus\
  \ size, vocabulary entropy, choice entropy, top-k entropy, population variance,\
  \ and response frequency\u2014by comparing them against human survey responses on\
  \ non-factual questions."
---

# Investigating Human-Aligned Large Language Model Uncertainty

## Quick Facts
- arXiv ID: 2503.12528
- Source URL: https://arxiv.org/abs/2503.12528
- Reference count: 26
- Primary result: Top-k entropy (KE) and population variance (PV) show strongest alignment with human uncertainty, though KE performance degrades with model size

## Executive Summary
This paper investigates how well various uncertainty measures in large language models (LLMs) align with human group-level uncertainty. The authors evaluate multiple measures—including self-reported uncertainty, nucleus size, vocabulary entropy, choice entropy, top-k entropy, population variance, and response frequency—by comparing them against human survey responses on non-factual questions. The primary finding is that top-k entropy (KE) and population variance (PV) measures show the strongest alignment with human uncertainty, though KE demonstrates a negative correlation with model size (smaller models align better). To address the size-dependency issue, the authors propose combining multiple uncertainty measures through linear regression, which achieves comparable human alignment while reducing size-dependency.

## Method Summary
The study evaluates eight uncertainty measures (KE, VE, CE, NS, RF, PV, SR, PS) against human survey data from Pew Research Center American Trends Panel waves 113-132. The authors filter for 38 opinion-based questions with no correct answers, then compute human uncertainty as Shannon entropy over answer choice distributions. For each question, they generate cloze-style prompts and run inference on LLaMa 3.1, LLaMa 3.2, Mistral 0.1, and Mistral 0.3 models (both base and instruct variants). They extract vocabulary probability distributions and compute correlations between each measure and human entropy, then train linear regression combinations with 3-fold cross-validation.

## Key Results
- Top-k entropy (KE) shows the strongest individual alignment with human uncertainty (r ≈ 0.6)
- Population variance (PV) also achieves high correlation but is computationally expensive
- KE performance degrades with model size (negative correlation with human-similarity)
- Linear combinations of measures reduce size-dependency while maintaining comparable human alignment
- Only KE, PV, and RF achieve significance for the majority of models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Top-k entropy (KE) captures human-aligned uncertainty by focusing computational resources on the most viable choice alternatives.
- Mechanism: KE restricts Shannon entropy calculation to only the k most probable tokens (k=10 in this study), rather than the full vocabulary. This approximates how humans narrow decision spaces to a few plausible options before expressing uncertainty, filtering out the vast majority of obviously irrelevant tokens that inflate full-vocabulary entropy.
- Core assumption: Human group-level uncertainty on bounded-choice questions involves comparing a small set of viable alternatives, not the entire possibility space.
- Evidence anchors:
  - [abstract] "Bayesian measures and a variation on entropy measures, top-k entropy, tend to agree with human behavior as a function of model size"
  - [section 5.1] "Only measures KE, PV, and RF achieve significance for the majority of models"
  - [corpus] Related work on entropy calibration (arxiv 2511.11966) suggests models are systematically miscalibrated, supporting the need for constrained measures
- Break condition: If human decision-making on open-ended questions involves broader possibility exploration than k=10, KE would underrepresent uncertainty. Additionally, the fixed k may not transfer across question types with varying choice cardinality.

### Mechanism 2
- Claim: Population variance (PV) captures human group uncertainty by treating ensemble members as synthetic survey respondents.
- Mechanism: Monte Carlo dropout creates N=30 stochastic model variants, each producing a probability distribution over answer choices. The standard deviation across variants on each choice's probability approximates disagreement patterns seen in human populations responding to the same survey question.
- Core assumption: Model internal variability (from dropout) parallels the cognitive diversity driving human group-level disagreement.
- Evidence anchors:
  - [abstract] "Bayesian measures and a variation on entropy measures, top-k entropy, tend to agree with human behavior"
  - [section 4.3.4] "PV uncertainty is obtained by taking the standard deviation across ensemble variants of the probability of each candidate token"
  - [corpus] SEED-GRPO (arxiv 2505.12346) uses semantic entropy for uncertainty-aware optimization, supporting ensemble-based approaches
- Break condition: If dropout-induced variance reflects implementation artifacts rather than meaningful epistemic uncertainty, PV would correlate spuriously. Ensemble methods also scale poorly with model size (noted in section 6).

### Mechanism 3
- Claim: Combining multiple uncertainty measures through linear regression reduces size-dependency by balancing measures with opposing size correlations.
- Mechanism: Individual measures exhibit different size-dependency profiles (KE decreases in human-similarity with size, while NS and CE show weaker or opposite dependencies). A learned linear combination can cancel these opposing trends, achieving stable human-alignment across model scales.
- Core assumption: Size-dependent distortions in individual measures are systematic and approximately linear, allowing regression to learn corrective weights.
- Evidence anchors:
  - [abstract] "by multiple linear regression, we find that combining multiple uncertainty measures provide comparable human-alignment with reduced size-dependency"
  - [section 5.2] "the combination of all non per-choice measures results in a model that generalizes well, r ≈ 0.5, and has significant correlation, r > 0.6"
  - [corpus] Limited direct corpus support for linear combination approaches; this appears methodologically novel
- Break condition: If size-dependency relationships are non-linear or vary across model architectures (the study tested only LLaMa and Mistral families), learned weights may not transfer.

## Foundational Learning

- Concept: Shannon entropy over discrete probability distributions
  - Why needed here: KE, CE, and VE all compute entropy over token probability distributions; understanding H(X) = -Σ p(x)log p(x) is prerequisite for interpreting results
  - Quick check question: Given token probabilities [0.7, 0.2, 0.1], compute the entropy. What does higher entropy indicate about model uncertainty?

- Concept: Monte Carlo dropout as Bayesian approximation
  - Why needed here: The PV measure relies on dropout at inference time to generate ensemble variance; understanding why dropout approximates posterior sampling clarifies the mechanism
  - Quick check question: Why does enabling dropout at test time produce different outputs across forward passes on the same input?

- Concept: Calibration vs. human-alignment in uncertainty
  - Why needed here: The paper explicitly distinguishes calibration (uncertainty matching accuracy) from human-alignment (uncertainty matching human disagreement patterns); conflating these leads to misinterpretation
  - Quick check question: A model claims 90% confidence on a question where 60% of humans agree with its answer. Is this a calibration problem or human-alignment problem?

## Architecture Onboarding

- Component map:
  - Base query generator -> Logit extractor -> Uncertainty calculators (8 parallel) -> Ensemble generator -> Correlation analyzer -> Regression trainer

- Critical path:
  1. Question preprocessing → base query construction
  2. Forward pass → vocabulary probability distribution extraction
  3. Per-measure uncertainty calculation (parallel)
  4. Human entropy computation from survey response frequencies
  5. Correlation analysis across all question-measure pairs
  6. Optional: regression training for combined measures

- Design tradeoffs:
  - Fixed k=10 for KE enables comparison but may not generalize across question types; dynamic k selection (noted in future work) could improve but adds complexity
  - Ensemble size N=30 balances computational cost against variance estimation stability
  - Cloze-style testing on pre-defined tokens simplifies comparison but limits applicability to unconstrained generation
  - Restricting to white-box models (LLaMa, Mistral) enables logit access but prevents evaluation of deployed systems (GPT, Claude)

- Failure signatures:
  - Negative correlation between model size and human-alignment on KE: larger models paradoxically less human-like
  - PV computational cost scales poorly with model size (ensemble of 30 forward passes)
  - Per-choice measures (PV, PS, RF) incompatible with linear regression combination approach
  - Measures trained on specific survey waves may not transfer to different question types or populations

- First 3 experiments:
  1. Replicate KE correlation analysis on held-out survey waves to validate generalization
  2. Ablate k values (k=5, 10, 20) to test sensitivity of human-alignment to vocabulary restriction
  3. Test linear combination weights learned on LLaMa models when applied to Mistral models to assess cross-architecture transfer

## Open Questions the Paper Calls Out

- Can the alignment of top-k entropy (KE) with human uncertainty be improved by implementing a dynamic selection of k, rather than using a fixed value?
  - Basis: The authors suggest considering "dynamic selection of k" in future work
  - Why unresolved: The study used static k=10 for all contexts
  - Evidence needed: Experiments showing adaptive k yields statistically significant improvement over fixed baseline

- Do the human-aligned uncertainty measures remain effective for unconstrained text generation tasks?
  - Basis: The current work is limited to "instantaneous responses with a limited set of available completions"
  - Why unresolved: It's unknown if correlation holds when vocabulary isn't restricted to pre-defined answer labels
  - Evidence needed: Correlation analysis using open-ended prompts comparing model uncertainty against human confidence ratings

- Is the alignment between LLM and human uncertainty a result of simulating individual human cognition or simply matching statistical aggregates of group behavior?
  - Basis: The paper assumes group-level human uncertainty is comparable to individual LLM uncertainty
  - Why unresolved: Study compared LLMs against aggregated survey data, not individual subjects
  - Evidence needed: Comparison of LLM uncertainty measures against individual human uncertainty data

## Limitations

- Human-Alignment vs. Calibration Distinction: Study focuses on human-alignment correlation but doesn't validate if measures also achieve better calibration
- Fixed Choice Architecture: All evaluation uses cloze-style prompts with pre-defined answers, limiting applicability to open-ended generation
- Survey Representativeness: Human baseline from US-based Pew Research Center data may not capture global uncertainty patterns

## Confidence

**Top-k Entropy as Leading Measure**: High Confidence
- Strong empirical evidence across multiple model families
- Consistent negative size-dependency observed

**Combined Measures Reduce Size-Dependency**: Medium Confidence
- Linear combination shows improved size-invariance
- Limited to LLaMa and Mistral families; cross-architecture generalization untested

**Human-Aligned Uncertainty Improves AI Systems**: Low Confidence
- Demonstrates correlation with human behavior but not downstream utility
- Future research direction rather than established finding

## Next Checks

1. **Cross-Cultural Validation**: Replicate correlation analysis using survey data from non-US populations (European Social Survey, World Values Survey) to test whether human-aligned uncertainty patterns transfer across cultural contexts.

2. **Open-Ended Generation Test**: Evaluate the same uncertainty measures on questions requiring novel responses rather than multiple-choice answers. This would validate whether measures optimized for choice-based uncertainty generalize to unconstrained generation scenarios.

3. **Calibration Benchmark Integration**: Test whether models using human-aligned uncertainty measures (particularly KE) achieve better calibration on standard benchmarks like CIFAR-10/100 or GLUE tasks, directly validating the distinction between alignment and calibration.