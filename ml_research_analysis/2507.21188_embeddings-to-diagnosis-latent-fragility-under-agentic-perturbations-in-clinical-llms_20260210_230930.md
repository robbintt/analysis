---
ver: rpa2
title: 'Embeddings to Diagnosis: Latent Fragility under Agentic Perturbations in Clinical
  LLMs'
arxiv_id: '2507.21188'
source_url: https://arxiv.org/abs/2507.21188
tags:
- clinical
- latent
- diagnosis
- patient
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LAPD, a framework for evaluating latent fragility
  in clinical LLMs under agentic perturbations. It proposes LDFR, a metric that measures
  how often structured perturbations cause embeddings to cross diagnostic decision
  boundaries in PCA-reduced space.
---

# Embeddings to Diagnosis: Latent Fragility under Agentic Perturbations in Clinical LLMs

## Quick Facts
- arXiv ID: 2507.21188
- Source URL: https://arxiv.org/abs/2507.21188
- Reference count: 40
- Key outcome: Introduces LDFR metric to detect semantic drift in clinical LLM embeddings that surface metrics miss, especially under masking and negation.

## Executive Summary
The paper introduces LAPD, a framework for evaluating latent fragility in clinical LLMs under agentic perturbations. It proposes LDFR, a metric that measures how often structured perturbations cause embeddings to cross diagnostic decision boundaries in PCA-reduced space. The method generates synthetic clinical notes via agentic prompting, then applies perturbations (masking, negation, synonym replacement, numerical edits) at controlled thresholds. Experiments on synthetic notes and real MIMIC-IV notes show that LDFR detects instability overlooked by surface metrics like BERTScore, particularly under masking and negation. Clinical models like MedGemma show similar fragility patterns. LDFR reveals semantic drift invisible to traditional evaluation, offering a geometry-aware diagnostic signal for auditing clinical AI robustness.

## Method Summary
LAPD generates synthetic clinical notes via a 3-stage agentic prompting pipeline (forward reasoning → backward reasoning → narrative construction) grounded in DDXPlus dialogues. It applies four perturbation types (masking, negation, synonym replacement, numerical edits) at five intensity thresholds (0%, 25%, 50%, 75%, 100%) to clinical entities extracted via NER. Notes are encoded using ClinicalBERT (synthetic) or Clinical-Longformer (real), mean-pooled, and projected to PCA space capturing 90% variance (30-40 components). A logistic regression classifier trained on unperturbed embeddings defines decision boundaries, and LDFR measures the proportion of perturbed samples where classifier predictions flip from the original diagnosis.

## Key Results
- LDFR detects diagnostic instability that BERTScore and ROUGE-L miss, particularly under entity masking (accuracy drops while surface similarity remains >0.89)
- Under masking, up to 20-30% of total variance concentrates in a single PCA dimension, creating a "bottleneck" that increases fragility
- LDFR patterns transfer from synthetic to real clinical notes, showing consistent degradation under perturbations across data sources
- Numerical perturbations have minimal LDFR impact while masking and negation show highest fragility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Surface similarity metrics fail to detect diagnostic instability because they measure token-level overlap rather than semantic boundary crossings in latent space.
- Mechanism: BERTScore and ROUGE-L compute similarity between original and perturbed text at the token/sentence level. When "chest pain" is masked to "_____", surface similarity remains high (0.89+ BERTScore) because most tokens are unchanged. However, the embedding shifts in PCA space can cross a classifier's decision boundary, flipping the diagnosis from GERD to Atrial Fibrillation. LDFR captures this by training a logistic classifier on PCA-reduced embeddings and measuring prediction flips.
- Core assumption: PCA subspace at 90% variance captures diagnostically-relevant structure; classifier boundaries approximate model reasoning boundaries.
- Evidence anchors:
  - [abstract]: "LLMs for clinical decision support often fail under small but clinically meaningful input shifts... These reasoning failures frequently go undetected by standard NLP metrics, which are insensitive to latent representation shifts."
  - [Section 6.1]: "At 25% masking, BERTScore stays above 0.89 while diagnosis accuracy drops significantly. At the same time, LDFR decreases, indicating that latent agreement breaks down even when surface similarity remains high."
  - [corpus]: MedPerturb Dataset (arxiv 2506.17163) similarly finds LLMs respond differently to clinical perturbations than humans, but focuses on output-level decisions rather than latent geometry.

### Mechanism 2
- Claim: Entity masking and negation induce dimensional collapse in latent space, concentrating variance along few PCA axes and increasing fragility.
- Mechanism: Perturbations redistribute variance across PCA dimensions. Under masking, 20-30% of total variance concentrates in a single dimension (e.g., dimension 752 in GPT-3.5 and LLaMA). This "bottleneck" means small input changes cause large movements along critical axes, increasing boundary crossing probability. Numerical perturbations show flat variance distributions (stable), while masking shows sharp peaks (fragile).
- Core assumption: Variance concentration indicates semantic compression where the model relies heavily on specific latent directions for diagnosis.
- Evidence anchors:
  - [Section 6.4]: "Under masked entities, up to 20–30% of total variance is captured by a single dimension (e.g., dimension 752 in GPT-3.5 and LLaMA), indicating a bottleneck where embeddings compress into narrow subspaces."
  - [Figure 5]: Shows variance concentration for masked entities vs. flat distribution for numerical perturbations.
  - [corpus]: CPR (arxiv 2512.24564) demonstrates similar fragility under adversarial perturbations in ECG models, but uses adversarial training as defense rather than geometric probing.

### Mechanism 3
- Claim: LDFR generalizes from synthetic to real clinical notes because the geometric fragility patterns are properties of the embedding space, not the data source.
- Mechanism: The framework generates synthetic notes via agentic prompting (forward reasoning → backward justification → narrative construction), then applies the same perturbation pipeline to real MIMIC-IV notes. Correlations between LDFR and LLM predictions degrade similarly under masking (Pearson drops from 0.94 to 0.32 on synthetic, 0.55 to 0.26 on real). The classifier is retrained per dataset, but the fragility pattern (masking > negation > synonym > numerical) persists.
- Core assumption: Synthetic notes from DDXPlus dialogues capture sufficient clinical complexity to expose transferable fragility; real notes from DiReCT benchmark are representative of deployment conditions.
- Evidence anchors:
  - [Section 6.5]: "Real clinical notes from the DiReCT dataset show similar patterns of latent fragility as synthetic notes... correlations between LDFR and model predictions consistently drop as perturbations increase."
  - [Table 2 vs Table 1]: Shows similar degradation patterns across perturbation types for both synthetic and real notes.
  - [corpus]: Limited direct corpus evidence for synthetic-to-real generalization in clinical LLM fragility; this is a novel contribution of the paper.

## Foundational Learning

- Concept: **PCA-based dimensionality reduction for embedding analysis**
  - Why needed here: LDFR operates in PCA-reduced space (90% variance, 30-40 components). Understanding how PCA captures variance structure is essential to interpret why certain dimensions become "bottlenecks" under perturbation.
  - Quick check question: Given embeddings with 768 dimensions, if the top 30 principal components capture 90% of variance, what does this imply about the intrinsic dimensionality of the clinical note representations?

- Concept: **Logistic regression classifiers for probing latent structure**
  - Why needed here: LDFR uses a logistic classifier trained on PCA-reduced embeddings to define "decision boundaries." This is a probing technique—not the actual model's classifier—but serves as a proxy for diagnostic reasoning boundaries.
  - Quick check question: If a logistic classifier achieves 91.25% accuracy on unperturbed notes but only 35.25% on fully masked notes, does this indicate the classifier is brittle, the embeddings have shifted, or both?

- Concept: **Surface metrics vs. semantic metrics in NLP evaluation**
  - Why needed here: The paper's central claim is that BERTScore/ROUGE-L miss diagnostic instability. Understanding what these metrics measure (token overlap, contextual similarity) vs. what they miss (reasoning consistency, boundary crossings) is critical.
  - Quick check question: BERTScore compares contextualized token embeddings between two texts. If two clinical notes differ only by "denies chest pain" vs. "has chest pain," would BERTScore capture this as a significant change?

## Architecture Onboarding

- Component map:
  Agentic Note Generator -> Perturbation Module -> Embedding Encoder -> PCA Projection -> Latent Classifier -> LDFR Computation

- Critical path:
  1. Generate synthetic notes OR load real notes (DiReCT/MIMIC-IV)
  2. Extract clinical entities via NER
  3. Apply perturbations at each threshold
  4. Encode original and perturbed notes with frozen encoder
  5. Project to PCA space (fit on unperturbed only)
  6. Train classifier on unperturbed PCA embeddings
  7. Compute LDFR: proportion of perturbed samples where classifier prediction differs from original

- Design tradeoffs:
  - **PCA vs. raw embeddings**: PCA reduces noise and enables interpretable visualization, but may discard diagnostically-relevant variance. Paper uses 90% threshold based on elbow plot (Appendix A.9).
  - **Logistic classifier vs. LLM's own predictions**: LDFR uses a proxy classifier rather than the LLM's outputs. This decouples the metric from generation stochasticity but assumes classifier boundaries align with model reasoning.
  - **Synthetic vs. real notes**: Synthetic notes enable controlled experiments (100 notes × 4 perturbations × 5 thresholds = 2,000 samples), but may lack real-world complexity. Paper validates on 90 real notes but acknowledges limited scale.

- Failure signatures:
  - **High LDFR with low surface change**: Indicates semantic drift invisible to standard metrics—this is the target signal.
  - **Low LDFR with high surface change**: May indicate perturbations that change text but not diagnostic reasoning (e.g., numerical edits). Paper finds numerical perturbations have minimal LDFR impact.
  - **Classifier accuracy drops significantly**: If classifier accuracy on unperturbed notes is low (<80%), LDFR may measure classifier noise rather than model fragility. Paper reports 91.25% baseline accuracy.
  - **Variance collapse in single dimension**: Indicates extreme fragility where one latent axis dominates—seen in GPT-3.5 and LLaMA under masking.

- First 3 experiments:
  1. **Reproduce synthetic note LDFR on a single model**: Generate 20 notes per diagnosis (7 diagnoses = 140 notes), apply masking at 0-100% thresholds, compute LDFR. Verify correlation with Table 1 pattern (Pearson drops from ~0.94 to ~0.32).
  2. **Test perturbation type hierarchy**: Apply all 4 perturbation types at 50% threshold on same notes. Confirm masking > negation > synonym > numerical in LDFR impact.
  3. **Validate on real notes with different encoder**: Load 20 DiReCT notes, encode with Clinical-Longformer, recompute LDFR. Compare correlation degradation pattern to Table 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do clinicians judge the diagnosis flips induced by perturbations as clinically warranted or as model errors?
- Basis in paper: [explicit] The authors state they "did not assess whether clinician reviewers agree with the diagnosis flips induced by perturbations."
- Why unresolved: The framework assumes flips indicate fragility, but some perturbations (e.g., negating a symptom) might logically necessitate a diagnosis change.
- What evidence would resolve it: Expert annotation of perturbed note pairs to classify flips as either "valid inference" or "fragility/hallucination."

### Open Question 2
- Question: Does using non-linear decision boundaries via manifold learning reveal latent fragility invisible to linear PCA-based methods?
- Basis in paper: [explicit] "Future work will explore non-linear boundaries via manifold learning..."
- Why unresolved: The current LDFR metric relies on linear boundaries in PCA space, which may fail to capture complex semantic relationships in clinical embeddings.
- What evidence would resolve it: A comparative study measuring LDFR using non-linear classifiers (e.g., deep neural networks) versus the current logistic regression baseline.

### Open Question 3
- Question: How does LDFR performance vary when using the target LLM's native embeddings compared to a fixed external encoder like ClinicalBERT?
- Basis in paper: [explicit] The authors note that "using a fixed BERT embedding space may misalign with model-specific representations."
- Why unresolved: Measuring GPT-4 or MedGemma using ClinicalBERT embeddings might miss internal representational shifts specific to those models.
- What evidence would resolve it: A comparative analysis calculating LDFR using both external frozen encoders and the internal hidden states of the evaluated LLMs.

## Limitations

- The framework's reliance on synthetic clinical notes may not fully capture real-world clinical complexity and documentation nuances
- The logistic classifier used for boundary detection is a proxy mechanism that may not perfectly align with actual diagnostic reasoning in clinical LLMs
- The effectiveness depends heavily on accurate clinical entity extraction via NER, where errors could lead to inappropriate perturbations

## Confidence

- **High confidence**: The core observation that surface similarity metrics (BERTScore, ROUGE-L) fail to detect diagnostic instability under structured perturbations
- **Medium confidence**: The geometric interpretation of fragility as variance concentration along specific PCA dimensions
- **Medium confidence**: The generalization of LDFR patterns from synthetic to real clinical notes

## Next Checks

1. **Cross-model fragility validation**: Apply LAPD to a diverse set of clinical LLMs with varying architectures (transformer variants, different pretraining objectives) to determine if variance concentration patterns are universal or model-specific.

2. **Clinical expert validation**: Have domain experts review a subset of perturbed notes where LDFR indicates fragility to verify whether semantic changes align with clinical reasoning failures or represent acceptable variations.

3. **Real-world deployment simulation**: Test LDFR on clinical notes from different healthcare systems with varying documentation practices to assess whether fragility patterns hold across institutional contexts and note formats.