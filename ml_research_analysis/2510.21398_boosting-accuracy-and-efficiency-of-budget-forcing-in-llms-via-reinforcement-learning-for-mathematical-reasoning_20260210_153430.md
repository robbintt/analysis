---
ver: rpa2
title: Boosting Accuracy and Efficiency of Budget Forcing in LLMs via Reinforcement
  Learning for Mathematical Reasoning
arxiv_id: '2510.21398'
source_url: https://arxiv.org/abs/2510.21398
tags:
- arxiv
- reasoning
- budget
- training
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how to improve budget forcing for mathematical
  reasoning in smaller LLMs by integrating reinforcement learning. Budget forcing
  is a test-time strategy that dynamically controls token allocation during reasoning,
  but it often performs suboptimally on smaller models due to verbose outputs and
  incomplete generations.
---

# Boosting Accuracy and Efficiency of Budget Forcing in LLMs via Reinforcement Learning for Mathematical Reasoning

## Quick Facts
- arXiv ID: 2510.21398
- Source URL: https://arxiv.org/abs/2510.21398
- Reference count: 40
- SFT+RL improves accuracy by 14.0% and reduces token usage by 41.9% on GSM8K for 1.5B LLM

## Executive Summary
This paper addresses the challenge of improving mathematical reasoning performance in small language models (1.5B parameters) through budget forcing with reinforcement learning. Budget forcing dynamically controls token allocation during reasoning by injecting "Wait" tokens or truncating outputs, but smaller models often produce verbose or incomplete reasoning traces. The authors propose a three-stage framework combining supervised fine-tuning (SFT) on long reasoning traces with reinforcement learning (RL) using Group Relative Policy Optimization (GRPO) to optimize token efficiency and self-correction. The RL component effectively mitigates verbosity and incomplete generations introduced during SFT, achieving significant improvements in both accuracy and efficiency on the GSM8K benchmark.

## Method Summary
The approach employs a three-stage pipeline: (1) SFT on the s1k-1.1 dataset (525 samples with ≤10K tokens containing self-correction keywords), (2) RL fine-tuning with GRPO on 1K GSM8K samples using length-penalized accuracy rewards and incomplete generation penalties, and (3) budget forcing inference with configurable thinking steps. The GRPO reward function balances accuracy against reasoning length, while penalizing incomplete generations. Training uses the Qwen2.5-1.5B-Instruct model with standard transformer architecture, employing FSDP for distributed training and FlashAttention2 for efficiency.

## Key Results
- SFT+RL model achieves 14.0% higher accuracy than SFT-only model on GSM8K
- Token usage reduced by 41.9% compared to SFT baseline
- Significant reduction in incomplete generation rate at 256-token budget
- Accuracy scales effectively with thinking steps, outperforming SFT at all tested budgets

## Why This Works (Mechanism)
The RL component addresses fundamental inefficiencies in SFT-trained reasoning models by directly optimizing the trade-off between reasoning quality and token efficiency. While SFT learns to produce coherent reasoning traces, it often overgenerates tokens and struggles with budget constraints. GRPO's length-penalized reward function incentivizes concise reasoning without sacrificing accuracy, while the incomplete generation penalty ensures the model learns to complete its thoughts within budget constraints. This targeted optimization during RL fine-tuning overcomes the verbosity and incompleteness issues that limit SFT-only approaches in budget forcing scenarios.

## Foundational Learning
- **Budget Forcing**: Test-time strategy controlling token allocation during reasoning by truncating or extending with "Wait" tokens; needed to enable dynamic reasoning depth without retraining
- **GRPO (Group Relative Policy Optimization)**: RL algorithm comparing policy actions within groups rather than against a fixed baseline; needed for stable RL training on reasoning tasks
- **Length-Penalized Rewards**: Reward function that discourages excessive token generation while maintaining accuracy; needed to balance reasoning quality against computational efficiency
- **Incomplete Generation Penalty**: Negative reward for truncated outputs that don't reach conclusion; needed to ensure models complete reasoning within budget constraints
- **Self-Correction Keywords**: Training data contains "Wait"/"Alternatively" tokens for model correction; needed to teach models to revise reasoning mid-generation
- **FSDP (Fully Sharded Data Parallel)**: Distributed training technique for memory efficiency; needed to train large models on limited GPU memory

## Architecture Onboarding

**Component Map**: Qwen2.5-1.5B-Instruct -> SFT Training -> GRPO RL Training -> Budget Forcing Inference

**Critical Path**: SFT initialization → GRPO reward optimization → Budget forcing deployment

**Design Tradeoffs**: Model capacity (1.5B) vs. reasoning quality; training data size (1.5K samples) vs. generalization; token efficiency vs. accuracy; computational cost vs. performance gains

**Failure Signatures**: Excessive verbosity (>1500 tokens/sample) indicates SFT overfitting; >50% incomplete generations suggests RL reward shaping issues; accuracy ceiling ~68% reflects model capacity limits

**Three First Experiments**:
1. Evaluate SFT-only model on GSM8K with 256-token budget to establish baseline verbosity and incomplete generation rates
2. Test GRPO reward function sensitivity by varying α parameter while holding other hyperparameters constant
3. Compare different "Wait" token injection strategies (append vs. replace) on reasoning quality and token efficiency

## Open Questions the Paper Calls Out
None

## Limitations
- Model capacity limits absolute accuracy ceiling (~68% on GSM8K)
- Training data scale (1.5K samples) may limit generalization to other mathematical domains
- Results specific to 1.5B parameter models may not scale to larger architectures
- Several GRPO hyperparameters unspecified, affecting reproducibility

## Confidence
- SFT+RL improves accuracy over SFT alone (14.0% gain): High confidence
- RL reduces token usage by 41.9%: Medium confidence
- RL reduces incomplete generations: High confidence
- Budget forcing effectiveness at small scale: Medium confidence

## Next Checks
1. Perform hyperparameter sensitivity analysis varying length penalty coefficient α and KL penalty β
2. Evaluate model generalization on alternative mathematical reasoning benchmarks (MATH, SVAMP)
3. Conduct ablation study comparing different "Wait" token injection strategies (append vs. replace)