---
ver: rpa2
title: General and Efficient Visual Goal-Conditioned Reinforcement Learning using
  Object-Agnostic Masks
arxiv_id: '2510.06277'
source_url: https://arxiv.org/abs/2510.06277
tags:
- learning
- target
- reward
- goal
- mask-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of goal-conditioned reinforcement
  learning (GCRL) by introducing a novel mask-based goal representation system. The
  core idea is to use binary masks as object-agnostic visual cues, enabling efficient
  learning and superior generalization compared to traditional methods like target
  images or 3D coordinates.
---

# General and Efficient Visual Goal-Conditioned Reinforcement Learning using Object-Agnostic Masks

## Quick Facts
- arXiv ID: 2510.06277
- Source URL: https://arxiv.org/abs/2510.06277
- Authors: Fahim Shahriar; Cheryl Wang; Alireza Azimi; Gautham Vasan; Hany Hamed Elanwar; A. Rupam Mahmood; Colin Bellinger
- Reference count: 39
- Primary result: Introduces a novel mask-based goal representation system that achieves 99.9% reaching accuracy in simulation and demonstrates successful sim-to-real transfer using pretrained object detectors.

## Executive Summary
This paper addresses the challenge of goal-conditioned reinforcement learning (GCRL) by introducing a novel mask-based goal representation system. The core idea is to use binary masks as object-agnostic visual cues, enabling efficient learning and superior generalization compared to traditional methods like target images or 3D coordinates. The masks provide rich, state-dependent feedback and can be processed to generate dense rewards without error-prone distance calculations. The method achieves 99.9% reaching accuracy in simulation for both training and unseen objects. It also demonstrates successful application in complex tasks like pick-up and sim-to-real transfer using pretrained object detection models. The approach shows robust generalization and is effective for real-world robotic learning.

## Method Summary
The method uses binary masks as object-agnostic visual cues appended to RGB observations. A mask-based dense reward is computed from normalized mask pixel counts transformed through a scaled sigmoid function. For manipulation tasks, ROI-constrained rewards guide grasping behavior. The approach is evaluated using SAC in simulation and PPO for sim-to-real transfer with domain randomization and image interpolation augmentation. Mask generation uses ground truth in simulation and pretrained detectors (Detic or Grounding DINO) in real-world settings.

## Key Results
- Achieved 99.9% reaching accuracy in simulation for both training and unseen objects using mask-based goal representations
- Mask-based dense rewards provided stable learning comparable to distance-based rewards while simplifying real-world implementation
- Pick-up task success rate exceeded 90% for nine out of eleven seeds using mask-based goal conditioning and rewards, compared to only two seeds for 3D position-based approaches
- Successfully transferred learned policies to a UR10e robot using sim-to-real techniques with pretrained object detectors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic mask-based goal representations provide richer state-dependent feedback than static goal encodings, enabling faster convergence and better generalization to unseen objects.
- **Mechanism:** At each timestep, a binary mask identifying the target object is appended to the RGB observation as an additional channel. This creates a visual attention signal that changes as the agent moves—mask size and position encode both "what" (object identity) and "where" (relative positioning). The policy learns to correlate mask dynamics with correct actions, rather than memorizing object-specific features.
- **Core assumption:** The mask generation method (ground truth in simulation, pretrained detector in real) provides reliable target localization without systematic bias.
- **Evidence anchors:**
  - [abstract] "Masks provide rich, state-dependent feedback and can be processed to generate dense rewards without error-prone distance calculations."
  - [Section IV.A] "The mask is updated at each time step to provide the agent with object-agnostic visual cues and progression towards the target."
  - [Section VI, Experiment 1b] "The mask-based GCRL agents achieved the highest performance for train and test objects, with 99.9% average accuracy for both."
  - [corpus] Related work on grounded object detection for GCRL (Versatile and Generalizable Manipulation) shows similar generalization benefits from object-centric representations, but does not replicate the dynamic mask mechanism directly.
- **Break condition:** If the mask generator fails on novel object geometries, textures, or occlusions, the attention signal becomes unreliable and the policy receives misleading state information. False positives from object detectors (noted with Grounding DINO in Section VII.B) directly degrade learning.

### Mechanism 2
- **Claim:** Mask-size-based dense reward provides a distance-proxy signal without requiring 3D position estimation, reducing dependency on privileged information and specialized sensors.
- **Mechanism:** The normalized count of activated mask pixels (Equation 3) is transformed through a scaled sigmoid (Equation 4) to produce a reward in [0,1]. As the agent approaches the target, the mask occupies more image area, increasing reward. The sigmoid scaling amplifies small mask changes when the object is distant, addressing the sparse gradient problem in early learning.
- **Core assumption:** Mask area correlates monotonically with physical distance to target under the camera geometry, and this correlation is consistent across object sizes.
- **Evidence anchors:**
  - [Section IV.B] Equations 3-4 define the mask-based reward formulation explicitly.
  - [Section VI, Experiment 2] "The mask-based reward system performed similarly to the distance-based reward system while having higher stability during training."
  - [Section I] "Using mask-based reward removes the dependency on computing the target's 3D world position, significantly simplifying real-world implementation."
  - [corpus] Distance-based dense rewards are standard in GCRL (e.g., GCHR, Dual Goal Representations), but these papers do not evaluate mask-based alternatives. Corpus does not provide comparative evidence.
- **Break condition:** If camera pose relative to end-effector changes significantly (e.g., different mounting position), the mask-size-to-distance mapping shifts and the learned policy may fail. Large objects vs. small objects at same distance produce different mask sizes, potentially creating object-size bias.

### Mechanism 3
- **Claim:** Region-of-interest (ROI) constrained mask reward guides manipulation behaviors (grasping alignment) more effectively than global position feedback.
- **Mechanism:** For pick-up tasks, the mask-based reward is only provided when the target mask overlaps a predefined ROI between the gripper fingers. This forces the policy to learn precise alignment before receiving positive reward. In contrast, 3D position feedback only indicates proximity, not orientation/alignment quality.
- **Core assumption:** The ROI placement corresponds to a viable grasping configuration, and the mask generator accurately captures partial object visibility within the gripper region.
- **Evidence anchors:**
  - [Section V.C, Experiment 3] "For the mask-based GC, we selected the part of the image between the gripper fingers as the region of interest (ROI). This is where the object is expected to be during a successful grasp."
  - [Section VI, Experiment 3] "Nine out of eleven seeds for the mask-based GC and rewards system achieved over 90% pick-up success. On the other hand, only two seeds for the 3D position-based GC and distance-based reward system achieved over 90% success."
  - [corpus] No direct comparison in corpus papers. Versatile Manipulation paper uses grounded detection but does not evaluate ROI-constrained rewards for grasping.
- **Break condition:** If object geometry requires non-standard grasp poses (e.g., top-down vs. side grasp), a fixed ROI may not capture the correct alignment criterion, leading to poor grasp success despite high ROI overlap.

## Foundational Learning

- **Concept: Goal-Conditioned RL (GCRL) and GA-MDPs**
  - Why needed here: The entire method operates within the GCRL framework where policies are conditioned on goals, and the paper's contribution is a new goal representation. Understanding how goals modify the MDP (reward function, policy input) is prerequisite.
  - Quick check question: Can you explain how a goal-conditioned policy differs from a standard RL policy, and why the same policy can achieve multiple objectives?

- **Concept: Visual encoders in actor-critic methods**
  - Why needed here: The SAC and PPO implementations use a shared image encoder f_ψ that processes stacked masked images. The encoder is updated only during critic updates—a non-obvious design choice affecting gradient flow.
  - Quick check question: In a visual SAC agent, why might you want the actor and critic to share an image encoder, and what are the tradeoffs of freezing encoder weights during policy updates?

- **Concept: Sim-to-real transfer and domain randomization**
  - Why needed here: The UR10e transfer uses domain randomization and a novel image interpolation augmentation. Understanding why these are needed (visual domain shift, dynamics mismatch) is critical for real-world deployment.
  - Quick check question: If you observe that a policy trained in simulation fails on real-world images but succeeds on simulated images with real backgrounds, what type of domain shift is likely occurring, and which augmentation strategy might help?

## Architecture Onboarding

- **Component map:**
  - RGB image (H×W×3) + binary mask (H×W×1) → stacked over 3 timesteps → H×W×12 tensor
  - CNN-based encoder f_ψ producing d-dimensional latent vector
  - Actor network: π_φ(latent, vector_state, goal) → action (joint velocities)
  - Critic network (SAC): Q_θ(latent, action, goal) → scalar value; 5-network ensemble with weight clipping
  - Reward module: Mask → normalized pixel count (Eq. 3) → scaled sigmoid (Eq. 4) → reward in [0,1]
  - Mask generator: Ground truth (sim) | Detic/Grounding DINO (real) | color filtering (navigation)

- **Critical path:**
  1. Mask generation latency must be < control loop budget (object detector inference time is the bottleneck in real-world)
  2. Mask quality directly determines reward signal quality—false positives corrupt learning
  3. For manipulation: ROI mask gating must be configured correctly before training

- **Design tradeoffs:**
  - SAC vs. PPO: SAC used for sample efficiency in simulation and real-world learning-from-scratch; PPO used for sim-to-real due to better stability under domain shift
  - Ground truth masks vs. pretrained detectors: Ground truth enables 99.9% accuracy in sim; detectors introduce noise but are necessary for real deployment
  - Detic vs. Grounding DINO: Detic performed better in real-world learning (Section VII.B); Grounding DINO had false-positive issues

- **Failure signatures:**
  - Agent exploits false-positive masks (reward hacking without reaching real object)
  - Policy fails to generalize to objects with different sizes (mask-size-to-distance mapping violated)
  - Sim-to-real transfer fails with visual artifacts not covered by augmentation
  - ROI-gated reward prevents learning if ROI is misaligned with grasp pose

- **First 3 experiments:**
  1. **Sanity check:** Train reaching policy with ground truth masks and distance-based reward on 3 training objects. Verify 95%+ reaching accuracy. If this fails, debug encoder or action space.
  2. **Ablation:** Compare mask-based reward vs. distance-based reward on same reaching task. Learning curves should converge to similar performance; if mask-based is significantly worse, check mask normalization or sigmoid scaling parameters.
  3. **Generalization test:** Evaluate trained policy on 2 held-out objects. Accuracy should be within 5% of training objects. Significant drop indicates overfitting to object-specific visual features rather than using mask as attention signal.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Real-world detector noise impact is acknowledged but not quantitatively characterized across different object types and environments. The Detic vs. Grounding DINO comparison is limited to one setup.
- Sim-to-real transfer results show success but lack ablations on the necessity of domain randomization and interpolation augmentation.
- The method's scalability to cluttered scenes or multiple simultaneous goals is not explored.

## Confidence
- **High:** Mask-based goal representations improve generalization to unseen objects (99.9% accuracy claim supported by controlled simulation experiments).
- **Medium:** Mask-based dense rewards are more stable than distance-based rewards (supported by comparative training curves but not extensively validated across tasks).
- **Medium:** ROI-constrained rewards improve pick-up success rates (statistically significant in single task but mechanism needs further validation).

## Next Checks
1. Test mask-based GCRL with a pretrained detector on objects with varying textures and lighting conditions to quantify detector noise impact on learning stability.
2. Compare mask-based rewards against alternative dense reward formulations (e.g., binary success signal, pixel-wise cross-entropy) in the same reaching task to isolate the benefit of the sigmoid scaling.
3. Evaluate generalization to multi-object scenes where the mask must distinguish between multiple potential targets to assess robustness to partial occlusions and similar object classes.