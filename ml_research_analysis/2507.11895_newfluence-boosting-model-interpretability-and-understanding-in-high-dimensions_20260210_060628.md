---
ver: rpa2
title: 'Newfluence: Boosting Model interpretability and Understanding in High Dimensions'
arxiv_id: '2507.11895'
source_url: https://arxiv.org/abs/2507.11895
tags:
- influence
- 'true'
- newton
- newfluence
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of estimating influence functions
  in high-dimensional settings where the number of parameters is comparable to or
  exceeds the number of observations. The authors show that classical influence functions,
  which are widely used for model interpretability and debugging, become unreliable
  in such high-dimensional regimes due to systematic underestimation of true influence
  values.
---

# Newfluence: Boosting Model interpretability and Understanding in High Dimensions

## Quick Facts
- arXiv ID: 2507.11895
- Source URL: https://arxiv.org/abs/2507.11895
- Authors: Haolin Zou, Arnab Auddy, Yongchan Kwon, Kamiar Rahnama Rad, Arian Maleki
- Reference count: 40
- Primary result: Newfluence corrects systematic underestimation of influence functions in high-dimensional settings (p ≈ n or p > n), achieving Kendall's τ ≈ 1.00 rank correlation with true influence values versus τ ≈ 0.88 for classical methods.

## Executive Summary
This paper addresses the fundamental problem of estimating influence functions in high-dimensional regimes where the number of parameters is comparable to or exceeds the number of observations. Classical influence functions, widely used for model interpretability and debugging, become unreliable in these settings due to systematic underestimation of true influence values. The authors introduce Newfluence, a correction that provides more accurate influence estimates while maintaining computational efficiency through a single Newton step approximation.

The core contribution is theoretical and practical: Newfluence is proven to be consistent with true influence values in high-dimensional settings (n,p → ∞ with n/p → γ₀), while classical influence functions are biased by a factor of (1-Hᵢᵢ). Empirical results on logistic ridge regression demonstrate that Newfluence achieves near-perfect rank correlation with true influence values in regimes where classical approaches fail, with the method particularly effective when regularization is weak and model complexity is high.

## Method Summary
The method introduces Newfluence, a correction to classical influence functions that addresses systematic underestimation in high-dimensional settings. The key formula is I_New = ℓ₀(bβ + ˙ℓᵢ(bβ)G⁻¹xᵢ/(1-Hᵢᵢ)) - ℓ₀(bβ), where Hᵢᵢ is a datapoint-specific term accounting for high-dimensional bias. The approach computes a single Newton step approximation to leave-one-out parameters, correcting the classical first-order Taylor expansion that ignores curvature effects when p ≍ n. The method requires computing the full Hessian inverse once (O(p³) cost) and then applying a datapoint-specific correction for each training point, avoiding per-test-point gradient computations required by classical methods.

## Key Results
- Newfluence achieves Kendall's τ ≈ 1.00 rank correlation with true influence values versus τ ≈ 0.88 for classical influence functions on logistic ridge regression (n=500, p=1000, λ=0.01)
- Theoretical proof shows Newfluence is consistent with true influence in high-dimensional settings (n,p → ∞, n/p → γ₀), while classical methods are biased by factor (1-Hᵢᵢ)
- The correction is particularly effective in high-dimensional regimes with weak regularization where classical approaches fail
- Effective degrees-of-freedom ratio df/p = (1/n)Σᵢ Hᵢᵢ determines when high-dimensional corrections are necessary (λ=0.01 gives df/p≈0.344 requiring Newfluence; λ=10 gives df/p≈0.023 where classical suffices)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Classical influence functions systematically underestimate true influence in high-dimensional settings by a datapoint-specific factor (1-H_ii).
- Mechanism: The classical approximation uses a first-order Taylor expansion that ignores curvature effects when p ≍ n. In high dimensions, the term H_ii = x_i^T G^{-1} x_i · ℓ̈_i(β̂) becomes non-negligible (Θ_P(1/(1+λ)), causing I_IF ≈ (1-H_ii)·I_True rather than I_IF ≈ I_True.
- Core assumption: The proportional asymptotic regime (n,p → ∞, n/p → γ₀) captures the essential failure mode of classical IFs in overparameterized models.
- Evidence anchors:
  - Theorem 2.1, Part 3: "I_IF = (1 − H_ii)I_True + o_P(1/(n·PolyLog(n)))"
  - Remark 2.3: "a highly influential data point may appear non-influential due to approximation errors"
  - Limited direct corpus validation; "Influence Functions for Edge Edits in Non-Convex GNNs" addresses related IF fragility in non-convex settings

### Mechanism 2
- Claim: A single Newton step from the full-model parameters provides a consistent approximation to leave-one-out retraining in high dimensions.
- Mechanism: Rather than use a pure first-order gradient approximation, Newfluence computes β̃^{Newton}_{/i} = β̂ + ℓ̇_i(β̂)G^{-1}x_i/(1-H_ii), where the (1-H_ii)^{-1} correction emerges from applying Woodbury's matrix inversion lemma to G_{/i}^{-1} = (G - x_i x_i^T ℓ̈_i)^{-1}.
- Core assumption: Twice-differentiable, ν-strongly convex loss and regularizer (A2, A3); Gaussian covariates with bounded spectrum (B1).
- Evidence anchors:
  - Appendix A, Eq. (10): "G^{-1}_{/i} = G^{-1} + G^{-1}x_i^T x_i G^{-1}ℓ̈_i(β̂)/(1 - x_i^T G^{-1}x_i ℓ̈_i(β̂))"
  - Theorem 2.1, Part 1: "|I_New − I_True| = o_P(1/(n·PolyLog(n)))"
  - "Gaussian Certified Unlearning" notes high-dimensional regimes challenge standard optimization assumptions of Ω(1) strong convexity

### Mechanism 3
- Claim: The effective degrees-of-freedom ratio df/p = (1/n)Σ_i H_ii determines when high-dimensional corrections are necessary.
- Mechanism: Regularization controls model complexity; strong regularization (λ=10 → df/p≈0.023) places the model in a quasi-low-dimensional regime where classical IFs suffice. Weak regularization (λ=0.01 → df/p≈0.344) requires Newfluence's correction.
- Core assumption: The df/p ratio reliably proxies the "dimensionality" of the effective parameter space in regularized models.
- Evidence anchors:
  - Tables 1-2: λ=0.01 gives τ≈0.88 (classical) vs τ≈0.88 (Newfluence); λ=10 gives τ≈1.00 for both
  - Remark 2.2: "H_ii = Θ_P(1/(1 + λ)), thus the classical estimator incurs considerable bias when λ is not very large"
  - Weak corpus validation; no neighboring papers directly test this df/p threshold hypothesis

## Foundational Learning

- **Woodbury matrix identity** (M^{-1} = A^{-1} - A^{-1}U(B^{-1} + VA^{-1}U)^{-1}VA^{-1} for M = A + UBV)
  - Why needed here: Enables efficient computation of G_{/i}^{-1} without per-datum matrix inversions; reduces O(n³p³) brute-force to O(n²p).
  - Quick check question: Given a rank-1 update M = G - uv^T, write M^{-1} in terms of G^{-1}.

- **Proportional asymptotics** (n,p → ∞ with n/p → γ₀ ∈ (0,∞))
  - Why needed here: This regime reveals phase transitions invisible in fixed-p analysis; Theorem 2.1's error bounds depend on n,p scaling together.
  - Quick check question: If n=500, p=1000, what is γ₀? Why would classical CLT-based inference fail here?

- **Influence functions for leave-one-out approximation**
  - Why needed here: Core motivation—avoiding retraining cost by estimating I_True(zi, z₀) = ℓ(β̂_{/i}, z₀) - ℓ(β̂, z₀) via closed-form approximations.
  - Quick check question: For a linear model with squared loss, write down the classical influence I_IF(zi, z₀) in terms of the Hessian H = X^TX.

## Architecture Onboarding

- **Component map**: Hessian G -> Leverage scores H_ii -> Newton step β̃_{/i} -> Influence evaluation I_New
- **Critical path**: 1) Fit β̂ via R-ERM, 2) Compute and cache G^{-1}, 3) For each training point i: compute H_ii, then β̃_{/i} correction, 4) For each test point z₀: evaluate I_New via loss difference
- **Design tradeoffs**: Newfluence avoids per-test-point gradient computation (vs. classical IF requiring ∇βℓ(β̂, z₀)); numerical instability when H_ii → 1 (rare under strong convexity but possible with weak regularization); add ε-stabilization to denominator; memory: G^{-1} is p×p; for p>10⁴, use Hessian-free approximations (LiSSA, Hessian-vector products)
- **Failure signatures**: Kendall's τ < 0.95 between I_New and I_True suggests: (a) regularizer not strongly convex, (b) numerical issues in G^{-1}, or (c) n/p not in asymptotic regime; H_ii outside [0,1] indicates: computation error or violation of convexity assumptions; large variance in τ across random seeds suggests: insufficient n or model misspecification
- **First 3 experiments**: 1) **Sanity check**: Replicate Table 1 (n=500, p=1000, λ=0.01) for logistic ridge; verify τ≈1.00. Plot I_New vs I_True scatter; check for systematic deviation. 2) **Ablation on λ**: Sweep λ ∈ {0.001, 0.01, 0.1, 1, 10}; plot τ(classical) and τ(Newfluence) vs df/p. Identify crossing point where classical IF becomes adequate. 3) **Scaling test**: Fix γ=0.5, increase (n,p) ∈ {(250,500), (500,1000), (1000,2000), (2000,4000)}; verify error |I_New - I_True| scales as O(PolyLog(n)/n) per Theorem 2.1.

## Open Questions the Paper Calls Out

- **Can the theoretical consistency of Newfluence be extended to non-convex, non-smooth, and non-differentiable objectives?**
  - Basis in paper: Section 4 states the analysis targets strongly convex, twice-differentiable objectives, but the authors view Newfluence as a "first step" toward settings characterizing modern AI models.
  - Why unresolved: The proofs rely on Assumptions A2 and A3 regarding twice differentiability and strong convexity.
  - What evidence would resolve it: Theoretical guarantees or empirical validation of Newfluence on models like Lasso or deep neural networks.

- **How does the high-dimensional framework apply to Shapley values?**
  - Basis in paper: The abstract states the framework "can also be applied to analyze other popular techniques, such as Shapley values."
  - Why unresolved: The paper develops the framework for influence functions but does not provide the analysis for Shapley values.
  - What evidence would resolve it: A derivation of bias corrections for Shapley values using the proportional asymptotic limits established in this work.

- **Do the guarantees for Newfluence hold under non-Gaussian feature distributions?**
  - Basis in paper: The theoretical results rely on Assumption B1, which requires feature vectors to be i.i.d. Gaussian (x_i ~ N(0, Σ)).
  - Why unresolved: Real-world data often deviates from Gaussian assumptions, and the concentration results used in the proofs may not directly transfer.
  - What evidence would resolve it: Theoretical bounds or simulation results using heavy-tailed or discrete feature distributions.

## Limitations

- The theoretical analysis relies heavily on proportional asymptotics and strong convexity assumptions that may not hold in many practical scenarios.
- The method's performance in non-Gaussian covariate settings, non-convex objectives, and with L1 regularization remains unproven.
- Computational costs remain O(p³) for Hessian inversion, which becomes prohibitive for very high-dimensional problems (p > 10⁴).

## Confidence

- **High Confidence**: The theoretical framework for classical influence function bias in high dimensions (Theorem 2.1, Part 3) is mathematically rigorous and well-supported by the proof.
- **Medium Confidence**: The Newfluence correction formula and its consistency properties (Theorem 2.1, Part 1) are theoretically sound but rely on assumptions that may be violated in practice.
- **Medium Confidence**: Empirical results showing Kendall's τ ≈ 1.00 for Newfluence vs τ ≈ 0.88 for classical IF are promising but based on synthetic data only.

## Next Checks

1. Test Newfluence on real-world high-dimensional datasets (e.g., genomics, text classification) where p > n to verify the synthetic results generalize.
2. Evaluate performance under non-Gaussian covariate distributions (heavy-tailed, skewed) to assess robustness beyond the theoretical assumptions.
3. Benchmark computational scalability for p ∈ {10⁴, 10⁵, 10⁶} using approximate Hessian methods (LiSSA, randomized SVD) to determine practical limits.