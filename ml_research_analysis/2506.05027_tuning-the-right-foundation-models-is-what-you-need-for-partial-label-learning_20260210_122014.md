---
ver: rpa2
title: Tuning the Right Foundation Models is What you Need for Partial Label Learning
arxiv_id: '2506.05027'
source_url: https://arxiv.org/abs/2506.05027
tags:
- label
- learning
- candidate
- partialclip
- partial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work systematically evaluates 11 foundation models across
  13 partial label learning (PLL) approaches on 8 datasets under 3 PLL scenarios (standard,
  long-tailed, and instance-dependent). The proposed PartialCLIP framework demonstrates
  that fine-tuning foundation models significantly outperforms training convolutional
  networks from scratch, achieving up to 81.1% improvement on CIFAR-100.
---

# Tuning the Right Foundation Models is What you Need for Partial Label Learning

## Quick Facts
- **arXiv ID:** 2506.05027
- **Source URL:** https://arxiv.org/abs/2506.05027
- **Reference count:** 40
- **Primary result:** Fine-tuning foundation models significantly outperforms CNNs from scratch, achieving up to 81.1% improvement on CIFAR-100

## Executive Summary
This paper systematically evaluates 11 foundation models across 13 partial label learning (PLL) approaches on 8 datasets under three PLL scenarios (standard, long-tailed, and instance-dependent). The proposed PartialCLIP framework demonstrates that fine-tuning foundation models significantly outperforms training convolutional networks from scratch, achieving up to 81.1% improvement on CIFAR-100. Foundation models show remarkable stability across varying ambiguity levels and maintain superior performance even under high label ambiguity. The study reveals that current PLL approaches exhibit similar performance when using foundation models, are highly susceptible to foundation model selection (with MetaCLIP and OpenAI CLIP showing top performance), and benefit from vision-language model alignment techniques.

## Method Summary
The study evaluates 11 foundation models (CLIP variants, ViT, SigLIP, MetaCLIP) across 13 PLL algorithms on 8 datasets under 3 scenarios. Images are resized to 224×224 and candidate labels are generated via Flip Probability Sampling (FPS), Uniform Sampling (USS), or instance-dependent generation. The framework uses parameter-efficient fine-tuning (PEFT) methods like AdaptFormer, VPT, and LoRA with classifier initialization using CLIP text embeddings. SGD optimizer with learning rate 0.01, batch size 64, weight decay 5×10⁻⁴, momentum 0.9. Training requires 10 epochs for ST-PLL/LT-PLL and 100-200 epochs for ID-PLL on single NVIDIA A6000 GPU.

## Key Results
- Fine-tuning foundation models achieves up to 81.1% improvement over CNNs trained from scratch on CIFAR-100
- PartialCLIP requires only 10 epochs for convergence versus 200-1000 epochs for traditional methods
- MetaCLIP and OpenAI CLIP show top performance across PLL scenarios, with model selection being critical
- Zero-shot CLIP confidence filtering enables aggressive pruning (50%+) of false-positive labels without accuracy degradation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning foundation models significantly outperforms training CNNs from scratch for PLL tasks, with up to 81.1% improvement reported.
- **Mechanism:** Pre-trained vision-language models provide high-quality transferable representations that remain stable during fine-tuning. Unlike CNNs trained from scratch (200-1000 epochs), foundation models require only 10 epochs because representations are already well-formed, reducing susceptibility to ambiguous supervision signals.
- **Core assumption:** The quality of learned representations positively correlates with PLL performance, and pre-trained representations are sufficiently general to transfer to PLL scenarios.
- **Evidence anchors:**
  - [abstract] "fine-tuning foundation models significantly outperforms training convolutional networks from scratch, achieving up to 81.1% improvement on CIFAR-100"
  - [Section 4.1.1] "PartialCLIP only requires 10 epochs to achieve convergence in most datasets" vs "200–1,000 epochs" for traditional methods
  - [corpus] Related work "Realistic Evaluation of Deep Partial-Label Learning Algorithms" examines deep PLL algorithm performance but does not contradict these findings
- **Break condition:** When fine-grained class distinctions exceed foundation model's pre-training granularity (e.g., aircraft model numbers like "747-300" in FGVC100), performance gains diminish or reverse.

### Mechanism 2
- **Claim:** Classifier initialization using CLIP text embeddings improves convergence and accuracy across PLL scenarios.
- **Mechanism:** CLIP's vision-language alignment enables semantic class embeddings (via prompts like "a photo of a [CLASS]") to initialize classifier weights, "activating" the model's inherent general knowledge without requiring feature extraction from training data or extensive supervised pre-training.
- **Core assumption:** The interconnectedness of textual and visual features in vision-language models transfers semantic knowledge to classification tasks.
- **Evidence anchors:**
  - [Section 3.2] "we tend to leverage the semantic knowledge from the text modality of CLIP... use hand-crafted textual prompts... to initialize the classifier weights"
  - [Table 5] Shows Adaptformer w/o text init drops from 43.3% to 32.3% on Places-LT; VPT-Deep w/o text init drops from 42.9% to 34.9%
  - [corpus] Weak corpus evidence for this specific mechanism—no directly comparable text-based initialization studies found
- **Break condition:** When class names are overly specialized (e.g., "DC-6" in fine-grained datasets), text embeddings cannot effectively "activate" relevant knowledge.

### Mechanism 3
- **Claim:** Pre-filtering candidate labels using zero-shot CLIP confidence enables aggressive pruning (50%+) of false-positive labels without accuracy degradation.
- **Mechanism:** Zero-shot CLIP computes confidence vectors via cosine similarity between image embeddings and text prompts. Most false-positive labels differ semantically from ground-truth; filtering by top-k confidence reduces candidate set size, mitigating interference during disambiguation.
- **Core assumption:** Zero-shot CLIP is sufficiently discriminative to identify semantically irrelevant labels while preserving ground-truth labels in top-k selections.
- **Evidence anchors:**
  - [Section 3.2] "pruning over 50% of candidate labels does not degrade performance and even yields improvements"
  - [Table 6] CIFAR-10 with η=0.7: PartialCLIP achieves 14.5% vs 96.2% with CLIP pre-filter; CIFAR-100 with η=0.2: 14.8% vs 82.1%
  - [corpus] "Partial-Label Learning with Conformal Candidate Cleaning" addresses similar candidate set refinement but uses conformal prediction, not zero-shot filtering
- **Break condition:** Overly aggressive pruning (small k) may erroneously remove ground-truth labels when CLIP is less discriminative.

## Foundational Learning

- **Concept: Partial Label Learning (PLL)**
  - Why needed here: Understanding the core problem—learning from datasets where each instance has a candidate label set containing one ground-truth label plus false positives—is essential for grasping why foundation models help.
  - Quick check question: Can you explain the difference between the flip-probability sampling strategy (FPS) and instance-dependent generation for candidate label sets?

- **Concept: Vision-Language Model Alignment (CLIP)**
  - Why needed here: The paper leverages CLIP's shared latent space where image and text features align, enabling zero-shot classification and semantic classifier initialization.
  - Quick check question: How does zero-shot CLIP classification work using the "a photo of a [CLASS]" prompt template?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT)**
  - Why needed here: PartialCLIP uses PEFT methods (Adapter, LoRA, VPT, etc.) to balance performance and computational cost, consistently outperforming full fine-tuning and linear probing.
  - Quick check question: Why does full fine-tuning perform poorly (e.g., 24.7% on CIFAR-100) compared to PEFT methods in this framework?

## Architecture Onboarding

- **Component map:** Data configuration (dataset, paths) -> Model configuration (backbone, fine-tuning method, hyperparameters) -> Algorithm selection (CC, PRODEN, CRDPLL for ST-PLL; RECORDS, SoLar, HTC for LT-PLL; ABLE, POP, IDGP for ID-PLL) -> Model (backbone + fine-tuning module) -> Trainer (Dataset construction -> Dataloader -> Optimizer -> Evaluation)

- **Critical path:**
  1. Select backbone (MetaCLIP or OpenAI CLIP recommended per Table 4)
  2. Initialize classifier with text embeddings from class names
  3. Apply PEFT method (AdaptFormer or VPT-Deep per Table 5)
  4. Optionally pre-filter candidate labels using zero-shot CLIP
  5. Train for 10 epochs (ST-PLL/LT-PLL) or 100-200 epochs (ID-PLL)

- **Design tradeoffs:**
  - MetaCLIP-B16: Best for ST-PLL (90.1% on CIFAR-100) but may lag in LT-PLL
  - OpenAI CLIP-B16: Stronger for LT-PLL (74.8% on CIFAR-100-LT)
  - Aggressive candidate filtering improves high-η scenarios but risks ground-truth removal
  - Text initialization critical for imbalanced datasets (Places-LT: 43.3% → 32.3% without it)

- **Failure signatures:**
  - Accuracy collapse at high partial rates without pre-filtering (LWS drops to 14.5% at η=0.7)
  - Full fine-tuning degrades to near-random (24.7% on CIFAR-100, 2.3% on Places-LT)
  - Fine-grained datasets with specialized class names show mixed results vs ImageNet-pretrained ResNet

- **First 3 experiments:**
  1. Reproduce ST-PLL baseline with CRDPLL + CLIP-ViT-B/16 + AdaptFormer on CIFAR-100 (η=0.1) to validate 88.9% accuracy claim
  2. Ablate text initialization: compare AdaptFormer with/without text init on CIFAR-100-LT to confirm ~13% gap on tail classes
  3. Test candidate filtering threshold sensitivity: vary k from K/4 to K/2 on CIFAR-10 with η=0.5 to find safe pruning boundary

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can PartialCLIP be extended to support a broader range of PLL algorithms and frameworks that are currently incompatible?
- Basis in paper: [explicit] The authors state in Appendix H: "Although PartialCLIP integrates a certain number of PLL baselines, there are still some methods and frameworks that are incompatible with it. How to equip our framework with more algorithms is a question worthy of further exploration."
- Why unresolved: The current framework implementation covers 13 PLL approaches but cannot accommodate all existing methods, limiting its applicability as a unified benchmark.
- What evidence would resolve it: A modular framework design that successfully integrates additional PLL algorithms (e.g., spectral methods, graph-based approaches) while maintaining performance characteristics.

### Open Question 2
- Question: Why do specific foundation models (MetaCLIP, OpenAI CLIP, SigLIP) systematically outperform others across different PLL scenarios (ST-PLL, LT-PLL, fine-grained ID-PLL)?
- Basis in paper: [inferred] Table 4 shows MetaCLIP excels in ST-PLL, OpenAI CLIP in LT-PLL, and SigLIP in fine-grained tasks, but the paper does not provide a theoretical explanation for these systematic differences.
- Why unresolved: The empirical findings demonstrate model-scenario correlations without identifying which pre-training characteristics, architectural features, or data properties cause these performance patterns.
- What evidence would resolve it: Systematic ablation studies controlling for pre-training data, model architecture, and training objectives across PLL scenarios, combined with analysis of learned representations.

### Open Question 3
- Question: Can adaptive or instance-specific candidate label pruning strategies outperform the fixed threshold approach (k=K/2) for CLIP-based pre-filtering?
- Basis in paper: [inferred] The paper notes that aggressive pruning may erroneously remove ground-truth labels when CLIP is less discriminative, and uses a fixed k=K/2 for simplicity, suggesting room for improvement through adaptive strategies.
- Why unresolved: The current approach applies uniform pruning across all instances, ignoring the varying confidence levels and ambiguity inherent in different samples.
- What evidence would resolve it: Experiments comparing fixed versus confidence-adaptive pruning thresholds, measuring both accuracy and ground-truth retention rates across datasets with varying partial rates.

## Limitations
- Evaluation relies exclusively on artificial candidate label generation rather than naturally occurring PLL datasets, limiting ecological validity
- Performance comparisons between foundation models and CNNs from scratch may overstate advantages since foundation models use pre-trained weights while CNNs are trained from scratch
- Claims about "81.1% improvement" represent absolute gains rather than relative improvements, which may overstate the magnitude

## Confidence
- **High confidence:** Foundation models significantly outperform CNNs from scratch (multiple datasets, consistent pattern)
- **Medium confidence:** Text embedding initialization provides consistent gains (limited ablation studies, theoretical justification)
- **Medium confidence:** Candidate label filtering works as claimed (empirical but not theoretical guarantees)

## Next Checks
1. Test PartialCLIP on naturally occurring PLL datasets (e.g., real-world webly-labeled data) to validate claims beyond artificial noise injection
2. Conduct controlled experiments isolating representation quality from fine-tuning efficiency by comparing foundation models against ImageNet-pretrained CNNs fine-tuned with PEFT
3. Evaluate model robustness to varying candidate label set sizes and pruning strategies across diverse datasets to establish practical pruning boundaries