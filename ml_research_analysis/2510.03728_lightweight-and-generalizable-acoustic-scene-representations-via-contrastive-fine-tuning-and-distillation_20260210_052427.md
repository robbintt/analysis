---
ver: rpa2
title: Lightweight and Generalizable Acoustic Scene Representations via Contrastive
  Fine-Tuning and Distillation
arxiv_id: '2510.03728'
source_url: https://arxiv.org/abs/2510.03728
tags:
- contrastive
- acoustic
- scene
- classification
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ContrastASC, a two-stage framework that learns
  lightweight and generalizable acoustic scene representations through contrastive
  fine-tuning and distillation. The method addresses the challenge of acoustic scene
  classification models on edge devices, which typically lack transferability needed
  for real-world applications requiring adaptation to new or refined acoustic categories.
---

# Lightweight and Generalizable Acoustic Scene Representations via Contrastive Fine-Tuning and Distillation

## Quick Facts
- **arXiv ID:** 2510.03728
- **Source URL:** https://arxiv.org/abs/2510.03728
- **Reference count:** 0
- **Primary result:** ContrastASC improves 5-shot accuracy by 3.3% on TUT17 and 2.0% on ICME24 over conventional approaches

## Executive Summary
This paper introduces ContrastASC, a two-stage framework that learns lightweight and generalizable acoustic scene representations through contrastive fine-tuning and distillation. The method addresses the challenge of acoustic scene classification models on edge devices, which typically lack transferability needed for real-world applications requiring adaptation to new or refined acoustic categories. ContrastASC first fine-tunes a pre-trained BEATs model using supervised contrastive learning to structure the embedding space and preserve semantic relationships between scenes. Then it applies contrastive representation distillation to transfer this structured knowledge to compact student models like CP-Mobile. The evaluation shows that ContrastASC achieves strong closed-set performance on the TAU22 dataset while demonstrating superior open-set generalization.

## Method Summary
ContrastASC employs a two-stage framework to create lightweight acoustic scene classifiers with strong generalization capabilities. In the first stage, a pre-trained BEATs model is fine-tuned using a mixup-aware supervised contrastive loss that structures the embedding space by pulling same-class samples together and pushing different-class samples apart. This geometric arrangement is designed to preserve semantic relationships between scenes. In the second stage, contrastive representation distillation transfers this structured knowledge to a compact CP-Mobile student model, replacing BatchNorm with LayerNorm to improve distributional robustness. The student is trained with a combined loss function incorporating cross-entropy, knowledge distillation, and contrastive representation distillation objectives.

## Key Results
- ContrastASC achieves 96.8% closed-set accuracy on TAU22, comparable to state-of-the-art models
- Demonstrates superior open-set generalization with 5-shot accuracy improvements of 3.3% on TUT17 and 2.0% on ICME24
- Shows consistent improvements across different student model sizes (6K-126K parameters)
- LayerNorm replacement provides consistent 1-2% gains in transferability over BatchNorm

## Why This Works (Mechanism)

### Mechanism 1: Geometry-Preserving Contrastive Fine-Tuning
Replacing cross-entropy with supervised contrastive learning during fine-tuning encourages the model to learn transferable semantic structures rather than just discriminative boundaries for fixed classes. The mixup-aware supervised contrastive loss explicitly structures the embedding space by pulling same-class samples together and pushing different-class samples apart, creating a geometrically structured feature space that leaves room for unseen categories sharing semantic attributes with known ones.

### Mechanism 2: Relational Knowledge Transfer via CRD
Standard knowledge distillation, which mimics output logits, fails to transfer the geometric structure of the embedding space. Contrastive Representation Distillation preserves this structure in the student model by maximizing a lower bound on mutual information between teacher and student embeddings, forcing the student to organize its internal representations to mirror the teacher's relational geometry.

### Mechanism 3: Distribution-Agnostic Normalization
Replacing BatchNorm with LayerNorm in the student model improves generalization to unseen acoustic scenes by decoupling representation stability from batch-level statistics. LayerNorm normalizes each sample independently, ensuring the embedding magnitude remains stable regardless of domain shift, which is critical for the cosine-based classification head.

## Foundational Learning

- **Concept: Supervised Contrastive Learning (SupCon)**
  - **Why needed here:** Unlike self-supervised contrastive learning, SupCon uses label information to define positive pairs, enabling the geometric structuring of the embedding space
  - **Quick check question:** If you mix two audio clips (mixup), how does SupCon handle the "positive" definition compared to standard cross-entropy? (Hint: The paper uses a weighted similarity based on label interpolation)

- **Concept: Knowledge Distillation (KD) vs. Representation Distillation (CRD)**
  - **Why needed here:** Understanding the difference between mimicking outputs (KD) and mimicking internal feature geometry (CRD) is essential to grasp how relational knowledge is transferred
  - **Quick check question:** Why would mimicking the logits (KD) fail to help a model recognize a class the teacher never saw? (Hint: Logits are class-specific; embeddings are feature-specific)

- **Concept: Open-Set / Few-Shot Evaluation**
  - **Why needed here:** The core value proposition of the paper is not just accuracy but transferability; few-shot evaluation measures how well the embedding space is structured for new concepts
  - **Quick check question:** Why is 5-shot accuracy a better proxy for "generalizability" than Top-1 accuracy on the validation set? (Hint: Top-1 measures memorization/closed-set performance; 5-shot measures the quality of the feature extractor for new tasks)

## Architecture Onboarding

- **Component map:** BEATs (Pre-trained Transformer) → [Projection MLP] → [Cosine Head]
- **Critical path:** 1) Load BEATs, freeze backbone, train Projection + Cosine Head 2) Unfreeze and fine-tune full model with Contrastive Loss 3) Load CP-Mobile, modify output block (remove BN, add LayerNorm) 4) Train using frozen Teacher embeddings via CRD
- **Design tradeoffs:** Cosine Head vs. Linear Head (binds logits, forces angular distance reliance); Lambda (0.25) heavily weights Contrastive loss over Cross-Entropy
- **Failure signatures:** High Closed-Set but Low Open-Set (likely used standard CE loss instead of Contrastive FT/CRD); Instability during Distillation (check LayerNorm implementation); Collapse in Embedding Space (check temperature values)
- **First 3 experiments:** 1) Sanity Check: Train student with standard KD and Cross-Entropy only to reproduce baselines 2) Ablation on Normalization: Run student with LayerNorm vs. BatchNorm on small open-set data subset 3) Few-Shot Probe: Freeze trained student encoder, train Logistic Regression on 5 samples per class from TUT17

## Open Questions the Paper Calls Out
- Integrating teacher ensembling with the approach to further improve representation generalizability
- The effectiveness dependency on the specific choice of CP-Mobile student architecture
- Sensitivity of the CE/SupCon loss balance (λ) to dataset diversity and size

## Limitations
- Performance gains on open-set tasks are modest (1-3% improvements) with untested statistical significance
- Framework's effectiveness may be limited to audio domains where semantic relationships exist between categories
- Two-stage training process increases complexity compared to single-stage methods

## Confidence
- **High Confidence:** LayerNorm improving distributional robustness is well-supported by ablation studies (Table 2 shows consistent 1-2% gains)
- **Medium Confidence:** Core claim that CRD transfers geometric structure better than KD is supported by results but could be affected by implementation details
- **Low Confidence:** Scalability to extremely few-shot scenarios (<5 samples) or highly dissimilar domains is not demonstrated

## Next Checks
1. **Statistical Significance Testing:** Perform paired t-tests on open-set results to verify that observed improvements (1-3%) are statistically significant
2. **Cross-Domain Generalization:** Test the framework on a completely different audio domain (e.g., medical sounds or industrial monitoring) to assess semantic structure transfer
3. **Parameter Efficiency Analysis:** Systematically vary student model size to identify the point where CRD fails to transfer geometric structure effectively