---
ver: rpa2
title: 'RobustExplain: Evaluating Robustness of LLM-Based Explanation Agents for Recommendation'
arxiv_id: '2601.19120'
source_url: https://arxiv.org/abs/2601.19120
tags:
- robustness
- explanation
- user
- perturbation
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RobustExplain introduces the first systematic framework for evaluating
  LLM-generated recommendation explanations under realistic user behavior perturbations.
  The framework models five practical perturbation types (noise injection, temporal
  shuffle, behavior dilution, category drift, missing values) and introduces multi-dimensional
  robustness metrics capturing semantic, keyword, structural, and length consistency.
---

# RobustExplain: Evaluating Robustness of LLM-Based Explanation Agents for Recommendation

## Quick Facts
- arXiv ID: 2601.19120
- Source URL: https://arxiv.org/abs/2601.19120
- Reference count: 39
- Introduces first systematic framework for evaluating LLM-generated recommendation explanations under user behavior perturbations

## Executive Summary
RobustExplain introduces the first systematic framework for evaluating LLM-generated recommendation explanations under realistic user behavior perturbations. The framework models five practical perturbation types (noise injection, temporal shuffle, behavior dilution, category drift, missing values) and introduces multi-dimensional robustness metrics capturing semantic, keyword, structural, and length consistency. Experiments on four LLMs (7B-70B) reveal that current models exhibit only moderate robustness with scores averaging around 0.50, indicating substantial sensitivity to user behavior noise. Larger models demonstrate measurable stability advantages, with LLaMA 3.1-70B achieving up to 8% higher robustness than smaller models.

## Method Summary
The framework systematically injects five types of perturbations into user behavior data: noise injection, temporal shuffle, behavior dilution, category drift, and missing values. For each perturbation type, it generates perturbed user profiles and feeds them to LLM-based explanation agents. The framework then evaluates explanation robustness using four metrics: semantic consistency (via BERTScore), keyword consistency (via ROUGE), structural consistency (via Tree Edit Distance), and length consistency (via Mean Absolute Error). These metrics collectively capture how well explanations maintain quality and coherence when faced with realistic variations in user behavior data.

## Key Results
- Current LLM explanation agents exhibit moderate robustness with average scores around 0.50 across perturbation types
- LLaMA 3.1-70B demonstrates up to 8% higher robustness than smaller models, establishing size as a key factor
- Robustness remains relatively stable across perturbation severity levels with only 1.7% degradation from mild to severe perturbations

## Why This Works (Mechanism)
The framework works by creating controlled environments where user behavior data is systematically corrupted, allowing researchers to isolate how these perturbations affect explanation quality. By using multi-dimensional metrics, it captures different aspects of explanation stability rather than relying on single measures that might miss important failure modes.

## Foundational Learning
- User behavior perturbation modeling - Why needed: Realistic evaluation requires simulating the noisy, dynamic nature of real-world user interactions
- Multi-dimensional robustness metrics - Why needed: Single metrics cannot capture the complex ways explanations might degrade under perturbation
- BERTScore for semantic evaluation - Why needed: Captures meaning preservation beyond surface-level keyword matching
- Quick check: Does the semantic score correlate with human judgment of explanation quality?

## Architecture Onboarding
Component map: User behavior data -> Perturbation injection -> LLM explanation agent -> Explanation generation -> Multi-metric evaluation -> Robustness scoring
Critical path: Perturbation injection → LLM generation → Metric computation → Final robustness score
Design tradeoffs: Comprehensive metric coverage vs. computational overhead; realistic perturbations vs. controlled experimentation
Failure signatures: Semantic drift indicates understanding issues; keyword loss suggests attention mechanism weaknesses; structural changes point to generation instability
First experiments: 1) Test single perturbation type isolation, 2) Vary perturbation severity systematically, 3) Compare cross-model robustness differences

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses on five perturbation types that may not capture all realistic user behavior scenarios
- Static user profiles from MovieLens and Last.fm may not reflect dynamic real-world user behavior
- String-based metrics may not fully capture semantic equivalence in paraphrased explanations

## Confidence
- High confidence: Framework's systematic methodology for perturbation injection and multi-dimensional metric computation
- Medium confidence: Observation that larger models demonstrate superior robustness (8% advantage for LLaMA 3.1-70B)
- Medium confidence: Finding of stable robustness across perturbation severity levels (1.7% degradation)
- Low confidence: Assertion that model size, rather than architecture, is the primary robustness determinant

## Next Checks
1. Replicate experiments on dynamically generated user behavior sequences that simulate real-time preference evolution over extended periods
2. Conduct ablation studies varying architectural parameters within similar parameter budgets to isolate architecture effects from scale
3. Implement human evaluation studies to validate whether multi-dimensional metric scores correlate with perceived explanation quality and trustworthiness under perturbations