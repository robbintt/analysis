---
ver: rpa2
title: 'MO-GRPO: Mitigating Reward Hacking of Group Relative Policy Optimization on
  Multi-Objective Problems'
arxiv_id: '2509.22047'
source_url: https://arxiv.org/abs/2509.22047
tags:
- reward
- grpo
- mo-grpo
- functions
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses reward hacking in multi-objective reinforcement
  learning when using GRPO, particularly in scenarios with multiple reward functions
  of differing variances. It shows that GRPO's advantage function is biased toward
  high-variance rewards, leading to suboptimal policies that ignore lower-variance
  objectives.
---

# MO-GRPO: Mitigating Reward Hacking of Group Relative Policy Optimization on Multi-Objective Problems

## Quick Facts
- **arXiv ID**: 2509.22047
- **Source URL**: https://arxiv.org/abs/2509.22047
- **Reference count**: 40
- **Primary result**: MO-GRPO mitigates reward hacking in multi-objective RL by normalizing advantage functions, outperforming GRPO in multi-armed bandits, control, machine translation, and instruction following tasks.

## Executive Summary
This paper addresses reward hacking in multi-objective reinforcement learning when using Group Relative Policy Optimization (GRPO), particularly in scenarios with multiple reward functions of differing variances. GRPO's advantage function is biased toward high-variance rewards, leading to suboptimal policies that ignore lower-variance objectives. To resolve this, the authors propose MO-GRPO, which normalizes advantage functions for each reward separately so their variances contribute equally to policy updates. This ensures balanced optimization across objectives while preserving preference orderings under positive affine transformations.

## Method Summary
The authors propose MO-GRPO, which normalizes advantage functions for each reward separately to ensure their variances contribute equally to policy updates. This normalization preserves preference orderings under positive affine transformations, allowing balanced optimization across objectives while mitigating the bias toward high-variance rewards that occurs in standard GRPO.

## Key Results
- MO-GRPO mitigates reward hacking and outperforms GRPO in multi-armed bandits, simulated control (Mo-Reacher), machine translation (WMT En-Ja, En-Zh), and instruction following tasks
- In machine translation, MO-GRPO improves BLEURT and readability scores without overfitting, achieving higher win rates in LLM-as-a-judge evaluations
- MO-GRPO achieves more stable learning and better overall performance across all tested domains

## Why This Works (Mechanism)
MO-GRPO works by normalizing the advantage functions for each reward separately, ensuring that the variances of different rewards contribute equally to the policy update. This addresses the bias in GRPO where high-variance rewards dominate the learning process, causing the policy to ignore lower-variance objectives. By equalizing the contribution of each reward's variance, MO-GRPO achieves balanced optimization across all objectives while preserving the preference orderings under positive affine transformations.

## Foundational Learning
- **Group Relative Policy Optimization (GRPO)**: A policy optimization method that uses group-wise advantage estimation. Needed to understand the baseline algorithm being improved. Quick check: Verify GRPO's group-wise advantage calculation and how it differs from individual advantage estimation.
- **Multi-objective reinforcement learning**: RL scenarios with multiple reward functions. Needed to contextualize the problem domain. Quick check: Confirm understanding of Pareto optimality and trade-offs between objectives.
- **Advantage function normalization**: Scaling advantage estimates to have equal contribution. Needed to grasp the core technical contribution. Quick check: Verify that normalization preserves preference orderings under positive affine transformations.
- **Reward hacking**: When optimization exploits unintended reward signal properties. Needed to understand the problem being solved. Quick check: Identify scenarios where variance differences could lead to reward hacking.
- **Variance as contribution proxy**: Using reward variance to weight importance in updates. Needed to understand the normalization rationale. Quick check: Evaluate whether variance is always a good proxy for reward importance.
- **Positive affine transformations**: Reward transformations that preserve preference orderings. Needed to understand the theoretical guarantee. Quick check: Confirm that normalization preserves orderings under these transformations.

## Architecture Onboarding

Component Map:
Input -> Reward functions -> Advantage function calculation -> Normalization (MO-GRPO) -> Policy update

Critical Path:
1. Compute advantages for each reward function separately
2. Normalize each advantage function by its variance
3. Combine normalized advantages for policy gradient update

Design Tradeoffs:
- **Variance-based normalization**: Simple to implement but assumes variance reflects importance; alternative could be learned weights
- **Separate vs joint normalization**: Separate preserves individual reward characteristics but may lose cross-reward correlations
- **Computational overhead**: Additional normalization step adds minor computation but improves multi-objective balance

Failure Signatures:
- If one reward has near-zero variance, normalization may become unstable or dominated by numerical noise
- If rewards have vastly different scales but similar variance, the method may underweight important but low-variance rewards
- If reward distributions are non-stationary, variance estimates may become stale, leading to incorrect normalization

First Experiments:
1. Implement MO-GRPO on a simple two-armed bandit with one high-variance and one low-variance reward to verify balanced optimization
2. Test on a multi-objective control task with known Pareto front to verify convergence toward optimal trade-offs
3. Apply to a translation task with two metrics (e.g., BLEU and readability) to verify improvement over GRPO

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes all reward functions are positive and affine-equivalent in preference ordering, which may not hold in more complex multi-objective settings
- Empirical scope limited to four domains; generalization to broader RL environments remains untested
- Confidence in robustness to reward function noise or distributional shifts is low, as these factors were not explicitly evaluated

## Confidence
- **Theoretical claim preservation of preference orderings**: High
- **Empirical performance gains**: Medium
- **Robustness to reward function noise or distributional shifts**: Low

## Next Checks
1. Test MO-GRPO on RL environments with sparse or delayed rewards to assess stability and effectiveness beyond dense reward settings
2. Evaluate the method on domains with more than two objectives to verify scalability and whether variance-based normalization remains effective
3. Perform ablation studies to determine the impact of variance normalization versus other normalization strategies on final performance and policy stability