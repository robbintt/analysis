---
ver: rpa2
title: Meta-Learning Theory-Informed Inductive Biases using Deep Kernel Gaussian Processes
arxiv_id: '2509.24919'
source_url: https://arxiv.org/abs/2509.24919
tags:
- data
- kernel
- learning
- theory
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a meta-learning framework to convert raw predictions
  from normative theories into tractable probabilistic models using deep kernel Gaussian
  processes. The approach meta-learns a feature extractor on synthetic data generated
  from a theory (efficient coding for the retina), creating a Theory-Informed Kernel
  that represents the theory's structure as an inductive bias.
---

# Meta-Learning Theory-Informed Inductive Biases using Deep Kernel Gaussian Processes

## Quick Facts
- **arXiv ID**: 2509.24919
- **Source URL**: https://arxiv.org/abs/2509.24919
- **Reference count**: 40
- **Primary result**: Meta-learning framework converts normative theory predictions into probabilistic models using deep kernel Gaussian processes, improving neural response prediction accuracy with calibrated uncertainty estimates.

## Executive Summary
This paper introduces a novel meta-learning framework that bridges normative theories and data-driven neural modeling by converting theory predictions into tractable probabilistic models. The approach uses deep kernel Gaussian processes (GPs) with theory-informed kernels to capture structural biases from efficient coding theory, applied to ex vivo mouse retinal ganglion cell recordings. The method achieves improved prediction accuracy over conventional baselines while providing well-calibrated uncertainty estimates and interpretable representations. Importantly, it enables rigorous theory validation through exact Bayesian model comparison, quantifying the degree of match between theoretical predictions and real neural data.

## Method Summary
The framework operates through a meta-learning pipeline where synthetic data generated from a normative theory (efficient coding for the retina) is used to train a feature extractor that captures the theory's structure. This extractor is then incorporated into a deep kernel GP, creating a Theory-Informed Kernel that serves as an inductive bias. The model is trained on real neural recordings while maintaining the theoretical structure learned during meta-training. Exact Bayesian inference is performed using the variational inducing point framework, enabling rigorous model comparison between theory-informed and conventional approaches.

## Key Results
- Theory-informed deep kernel GP improves neural response prediction accuracy compared to conventional baselines on ex vivo mouse retinal ganglion cell data
- The framework provides well-calibrated uncertainty estimates for predictions, addressing a key limitation of standard deep learning approaches
- Exact Bayesian model comparison quantifies the degree of match between efficient coding theory predictions and real neural data

## Why This Works (Mechanism)
The framework works by leveraging meta-learning to extract structural features from synthetic data generated by normative theories. These features capture the underlying principles encoded in the theory (such as efficient coding constraints) and are then used to construct a kernel function that represents these theoretical biases. When combined with deep kernel GPs, this theory-informed kernel constrains the hypothesis space during learning on real data, leading to better generalization and more interpretable representations that align with theoretical expectations.

## Foundational Learning

**Efficient Coding Theory**
*Why needed*: Provides the normative framework that generates synthetic training data and theoretical predictions
*Quick check*: Understand how efficient coding optimizes neural representations under resource constraints

**Gaussian Processes**
*Why needed*: Offers a probabilistic framework with well-calibrated uncertainty estimates
*Quick check*: Grasp the basics of GP regression and kernel functions

**Meta-Learning**
*Why needed*: Enables extraction of structural features from synthetic theory data that generalize to real data
*Quick check*: Understand how meta-learning differs from standard supervised learning

**Deep Kernel Learning**
*Why needed*: Combines deep neural networks with GP kernels to learn complex feature representations
*Quick check*: Know how deep kernels extend traditional GP kernels

## Architecture Onboarding

**Component Map**
Theory Synthetic Data -> Feature Extractor (Meta-Learned) -> Theory-Informed Kernel -> Deep Kernel GP -> Bayesian Inference -> Prediction/Validation

**Critical Path**
Meta-learning feature extractor on synthetic data → Incorporating theory-informed kernel into deep GP → Training on real neural data → Bayesian model comparison for theory validation

**Design Tradeoffs**
The approach trades computational complexity (meta-learning + exact Bayesian inference) for improved theoretical interpretability and calibrated uncertainty, versus standard deep learning which is computationally efficient but lacks theoretical grounding and uncertainty quantification.

**Failure Signatures**
- Poor synthetic theory data quality leads to ineffective feature extraction
- Mismatch between theory assumptions and real neural dynamics causes degraded performance
- Computational constraints limit scalability to larger neural datasets

**First Experiments**
1. Test feature extractor performance on held-out synthetic data to verify meta-learning effectiveness
2. Compare theory-informed vs. standard deep kernel GP performance on synthetic test sets
3. Validate uncertainty calibration through proper scoring rules on validation data

## Open Questions the Paper Calls Out
None specified in the provided material.

## Limitations
- Assumes synthetic data from theory adequately represents biological mechanisms, potentially missing real neural dynamics
- Performance improvements are statistically significant but modest, suggesting either theory limitations or framework constraints
- Computational cost of meta-learning and exact Bayesian inference may restrict applicability to larger-scale datasets

## Confidence

**Major Claim Confidence:**
- **Theory-to-model conversion framework (High)**: The methodological pipeline from synthetic theory data to deep kernel GP is well-defined and reproducible
- **Improved prediction accuracy (Medium)**: Results show consistent improvement over baselines, but effect sizes are modest and domain-specific
- **Bayesian model comparison for theory validation (High)**: The theoretical foundation for exact Bayesian comparison is sound, though practical implementation details matter

## Next Checks

1. Test the framework on multiple neural systems (e.g., V1, auditory cortex) with different underlying normative theories to assess generalizability across domains
2. Perform ablation studies comparing theory-informed kernels against purely data-driven kernels of similar complexity to isolate the contribution of theoretical structure
3. Evaluate the framework's robustness to noisy or incomplete theory specifications by systematically degrading the synthetic training data quality