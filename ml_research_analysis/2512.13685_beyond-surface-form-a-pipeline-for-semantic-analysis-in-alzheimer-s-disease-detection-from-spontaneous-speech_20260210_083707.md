---
ver: rpa2
title: 'Beyond surface form: A pipeline for semantic analysis in Alzheimer''s Disease
  detection from spontaneous speech'
arxiv_id: '2512.13685'
source_url: https://arxiv.org/abs/2512.13685
tags:
- semantic
- language
- similarity
- original
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a novel pipeline for Alzheimer\u2019s disease\
  \ detection from spontaneous speech by transforming text surface forms while preserving\
  \ semantic content. The method employs large language models to restructure and\
  \ summarize transcripts, isolating semantic features from syntactic and lexical\
  \ cues."
---

# Beyond surface form: A pipeline for semantic analysis in Alzheimer's Disease detection from spontaneous speech

## Quick Facts
- arXiv ID: 2512.13685
- Source URL: https://arxiv.org/abs/2512.13685
- Reference count: 40
- This study introduces a pipeline that transforms text surface forms while preserving semantic content for Alzheimer's disease detection from spontaneous speech

## Executive Summary
This paper presents a novel pipeline for Alzheimer's disease detection from spontaneous speech that isolates semantic features from surface-form patterns. The method uses large language models to transform transcripts through summarization and structured storyboards, reducing lexical and syntactic markers while maintaining underlying meaning. Evaluations on English and Portuguese datasets show the approach preserves semantic similarity (>0.6) while significantly altering surface form (BLEU < 0.1, chrF < 0.5), with classification performance remaining stable. The findings suggest that semantic impairment detection remains feasible after surface standardization, supporting potential improvements in early AD detection systems focused on meaning rather than surface patterns.

## Method Summary
The pipeline transforms spontaneous speech transcripts through text-to-text operations (summaries, storyboards) using GPT-4o or Llama-3.3-70B, optionally preceded by translation (NLLB-200) for non-English data. Surface-form similarity is measured with BLEU and chrF scores, while semantic preservation is validated using SentenceBERT cosine similarity. The transformed text is then classified using BERT models (bert-base-uncased or BERTimbau), with performance compared to original text classification to verify semantic retention. The method aims to isolate AD-relevant semantic impairments from lexico-syntactic markers that may confound detection.

## Key Results
- Text-to-text transformations preserve semantic similarity (>0.6 cosine) while significantly reducing surface form overlap (BLEU < 0.1, chrF < 0.5)
- BERT classification on transformed text achieves comparable macro-F1 scores to original text (differences within ±0.1)
- Text-to-image-to-text transformations degrade both semantic preservation and classification performance
- Surface-form linguistic markers lose diagnostic significance after transformation while semantic-based classification remains stable

## Why This Works (Mechanism)

### Mechanism 1
LLMs can strip lexico-syntactic markers while preserving AD-relevant semantic content. Text-to-text transformations restructure syntax and vocabulary through paraphrase, reducing surface-form overlap while maintaining semantic similarity. This isolates semantic information by forcing the classifier to rely on underlying meaning rather than superficial patterns. The transformation does not systematically remove or alter AD-specific semantic deficits during paraphrase.

### Mechanism 2
Semantic impairment is more diagnostic for AD than surface-form degradation. When surface-form features are standardized via transformation, their differential signal between AD and control groups is reduced or eliminated, yet classification remains stable, indicating the model accesses preserved semantic markers. The semantic markers retained post-transformation are genuinely linked to AD pathology, not artifacts of the transformation process.

### Mechanism 3
Text-to-image-to-text transformation fails because it introduces noise that destroys AD-relevant semantic structure. StableDiffusion-3XL generates images from text, then LLaVa-8B generates captions. This modality shift loses semantic detail and adds reconstruction noise, reducing classification to near-random. The failure is due to information loss in the image generation/captioning pipeline, not model incapacity to represent semantic content.

## Foundational Learning

- **Surface-form vs. semantic features in language**: Why needed here - The entire methodology hinges on distinguishing lexico-syntactic patterns from underlying meaning. Quick check: If you paraphrase "The boy hid the puppy in the wardrobe" as "A child concealed a young dog inside a closet," which features change and which stay the same?

- **BLEU/chrF vs. embedding cosine similarity**: Why needed here - These metrics quantify different aspects of text similarity. BLEU/chrF measure n-gram overlap (surface); cosine similarity measures semantic proximity in embedding space. Quick check: Why can two texts have low BLEU but high cosine similarity? What does this tell you about their relationship?

- **BERT fine-tuning for classification**: Why needed here - The downstream task uses BERT classifiers to validate whether transformed text retains AD-detection signal. Quick check: When BERT is fine-tuned on transformed text with identical semantics but different surface form, what must remain in its representations for classification to succeed?

## Architecture Onboarding

- **Component map**: Input transcript → (translate if needed) → text-to-text transform → validate similarity → train/evaluate BERT classifier → compare macro-F1 to original baseline
- **Critical path**: The pipeline processes transcripts through transformation and similarity validation before classification, with the transformation type determining semantic preservation quality
- **Design tradeoffs**: GPT-4o yields slightly higher semantic similarity than Llama-3.3 but requires data to leave premises; shorter summaries lose more information while longer ones may retain surface traces; storyboards provide structured scenes while summaries offer narrative coherence
- **Failure signatures**: BLEU/chrF not sufficiently low (>0.2 BLEU, >0.5 chrF) indicates transformation isn't altering surface form enough; cosine similarity <0.5 means transformation is losing semantic content; classification macro-F1 drops >0.15 from baseline indicates transformation has removed AD-relevant signal
- **First 3 experiments**:
  1. Reproduce similarity validation: Take 20 samples from ADReSS, apply each transformation, compute BLEU/chrF/cosine. Verify BLEU <0.1 and cosine >0.6 for text-to-text transformations.
  2. Ablate transformation type: Train BERT classifiers separately on short/medium/long summaries and storyboards. Confirm long summaries and storyboards achieve within ±0.1 F1 of original, while short summaries may degrade.
  3. Test on held-out AD samples: After training on transformed ADReSS train split, evaluate on test split. Compare per-class accuracy to understand if semantic-only signal favors one class.

## Open Questions the Paper Calls Out

- Is the semantic information loss observed in the text-to-image-to-text pipeline caused by the fundamental change in modality or by the quality limitations of current generative models? The authors state this remains to be investigated, noting that whether the failure is due to modality changes or model quality differences is unclear.

- Does translating non-English AD transcripts into English improve classifier performance primarily due to the higher quality of English-based language models or because translation acts as a surface-form normalizer? The paper notes that translation improved performance but cannot determine if this is from better models or text normalization effects.

- Can the proposed transformation pipeline be effectively utilized for patient de-identification and the creation of synthetic augmented training data for AD detection? The authors envision this application but note it requires further investigation to verify effectiveness.

## Limitations

- The study relies on picture description tasks, limiting generalizability to other spontaneous speech contexts
- The paper does not validate that retained semantic features are specifically AD-relevant rather than generic content
- Text-to-image-to-text transformation failures may reflect current model limitations rather than fundamental modality shift constraints

## Confidence

- **High Confidence**: The transformation pipeline successfully reduces surface-form similarity while maintaining overall semantic content (BLEU < 0.1, chrF < 0.5, cosine > 0.6) and classification performance (macro-F1 differences within ±0.1)
- **Medium Confidence**: The claim that semantic impairment is more diagnostic than surface-form degradation, as evidence shows stable classification but doesn't prove semantic features are specifically AD-relevant
- **Low Confidence**: The assertion that text-to-image-to-text transformation failure proves modality shifts inherently destroy AD-relevant semantic structure, as this may simply reflect current multimodal model limitations

## Next Checks

1. **Semantic Feature Validation**: Conduct qualitative analysis of what semantic content is preserved vs. lost in transformations, specifically examining whether AD-relevant semantic deficits (anomia, concept fragmentation) remain detectable after surface standardization.

2. **Cross-Dataset Generalization**: Apply the pipeline to a different spontaneous speech dataset (e.g., Cookie Theft description) to verify that semantic-only detection generalizes beyond picture description tasks.

3. **Multimodal Model Benchmarking**: Replace StableDiffusion-3XL/LLaVa-8B with newer multimodal models (GPT-4V or Gemini) for text-to-image-to-text transformations to determine if current failures reflect model limitations rather than fundamental modality shift constraints.