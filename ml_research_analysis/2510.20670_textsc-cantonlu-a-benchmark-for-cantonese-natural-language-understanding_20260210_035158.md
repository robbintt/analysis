---
ver: rpa2
title: '\textsc{CantoNLU}: A benchmark for Cantonese natural language understanding'
arxiv_id: '2510.20670'
source_url: https://arxiv.org/abs/2510.20670
tags:
- cantonese
- language
- mandarin
- linguistics
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CantoNLU introduces a Cantonese natural language understanding
  benchmark with seven tasks including WSD, LAJ, LD, NLI, SA, POS, and DEPS. The benchmark
  features novel datasets for WSD and LAJ, along with adapted datasets for other tasks.
---

# \textsc{CantoNLU}: A benchmark for Cantonese natural language understanding

## Quick Facts
- arXiv ID: 2510.20670
- Source URL: https://arxiv.org/abs/2510.20670
- Reference count: 0
- Primary result: Introduces Cantonese NLU benchmark with seven tasks and evaluates three model types

## Executive Summary
This paper introduces CantoNLU, a comprehensive benchmark for Cantonese natural language understanding featuring seven tasks including word sense disambiguation, lexical acceptability judgment, named entity recognition, natural language inference, sentiment analysis, part-of-speech tagging, and dependency parsing. The benchmark introduces novel datasets for WSD and LAJ, along with adapted datasets for other tasks. Three model types were evaluated: Mandarin-only, Cantonese-adapted, and monolingual Cantonese. Results show Cantonese-adapted models perform best overall, while monolingual models excel in syntactic tasks. The work highlights the need for more high-quality Cantonese corpora to improve language representation.

## Method Summary
The study evaluates three model types on seven Cantonese NLU tasks. The Mandarin-only model uses bert-base-chinese directly. The Cantonese-adapted model continues pre-training bert-base-chinese on a 700M character Cantonese corpus while keeping the original tokenizer. The monolingual Cantonese model is trained from scratch with a BERT architecture using a SentencePiece BPE tokenizer (32k tokens) trained on the Cantonese corpus. Downstream datasets include WSD (109 senses), LAJ (1.6k pairs), LD (47k sentences), NLI (557k train), SA (12k total), and POS/DEPS (Cantonese-HK UD, 14k tokens). WSD and LAJ use zero-shot evaluation via cosine similarity and surprisal respectively, while other tasks use fine-tuning with task-specific hyperparameters.

## Key Results
- Cantonese-adapted models outperform both Mandarin-only and monolingual models overall
- Monolingual models excel in syntactic tasks (POS, DEPS) despite less data
- Mandarin models remain competitive on some tasks, particularly NLI and LAJ
- WSD and LAJ show distinct performance patterns requiring zero-shot evaluation methods

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Transfer via Shared Script and Grammar
Transferring representations from Mandarin to Cantonese is effective for semantic tasks because written Cantonese and Mandarin share a logographic script (Hanzi) and substantial vocabulary overlap. Mandarin pre-training establishes robust semantic knowledge that continues pre-training on Cantonese text can align with Cantonese-specific lexical distributions without requiring massive data for learning semantics from scratch. The semantic gap between Mandarin and Cantonese appears smaller than the data gap, making negative transfer less significant than the benefit of robust pre-trained features. This mechanism likely fails for semantic tasks involving heavy code-switching or colloquialisms absent in standard Mandarin corpora.

### Mechanism 2: Syntactic Specialization in Monolingual Training
Monolingual models trained specifically on Cantonese data outperform adapted models on syntactic tasks (POS, DEPS) even with significantly less data. While semantics transfer well, syntax (word order, grammatical particles) diverges significantly between Mandarin and Cantonese. An adapted model may suffer from interference where Mandarin syntactic priors conflict with Cantonese grammar. A monolingual model, free of conflicting structural priors, learns Cantonese-specific dependency relations more accurately. Syntactic representations are more sensitive to language-specific structural differences than semantic representations.

### Mechanism 3: Subword Tokenization for Low-Resource Morphology
A native subword tokenizer (BPE) outperforms character-level tokenization for structural tasks in low-resource settings. The monolingual model uses a SentencePiece tokenizer (32k vocab) rather than the standard character-level tokenizer (8k vocab). By capturing subword structures and morphological units common in Cantonese verb serialization and reduplication, the model reduces sequence length and computational complexity, allowing limited attention capacity to better model syntactic dependencies. Character-level tokenization forces the model to learn morphology implicitly, which requires more data than available in low-resource settings.

## Foundational Learning

- **Concept: Diglossia in NLP**
  - Why needed here: Cantonese is primarily spoken while Mandarin is the written standard, explaining why text data is scarce and why models might rely on standard Chinese characters
  - Quick check question: Does the dataset contain vernacular Cantonese characters (e.g., 冇, 咁) or standard Chinese equivalents?

- **Concept: Continued Pre-training (Model Transfer)**
  - Why needed here: The "Cantonese-adapted" model is not trained from scratch but uses an existing Mandarin model, making this the core experimental variable
  - Quick check question: How does freezing encoder layers during adaptation affect retention of Mandarin semantic knowledge vs. learning Cantonese syntax?

- **Concept: Universal Dependencies (UD)**
  - Why needed here: The DEPS and POS tasks rely on the UD framework, requiring familiarity with these annotation standards to interpret the 48 dependency relations
  - Quick check question: Are the 17 Cantonese-specific relations in the UD dataset handled as special tokens or mapped to general UD categories?

## Architecture Onboarding

- **Component map:** BERT-base architecture (Transformer Encoder) -> Three tokenizer variants (Mandarin Char, Cantonese BPE) -> Task-specific classification heads (Token classification for POS/DEPS, Sentence classification for NLI/SA)

- **Critical path:**
  1. Data Curation: Filter Wikipedia/Kwok corpus for length and noise (e.g., empty parentheses removal)
  2. Tokenizer Training (Monolingual only): Train SentencePiece on Cantonese corpus
  3. Pre-training: Run MLM from scratch (Monolingual) or from checkpoint (Adapted)
  4. Fine-tuning: Train task-specific heads using hyperparameters (LR: 2e-5 to 3e-5)

- **Design tradeoffs:**
  - Adapted vs. Monolingual: Adapted models offer better semantic performance and faster convergence but suffer syntactic bias; Monolingual models offer better syntactic fidelity but require careful data cleaning and suffer from lower semantic coverage
  - Tokenizer: Character-level (interoperability with Mandarin) vs. Subword (efficiency and morphological awareness)

- **Failure signatures:**
  - Low LAS/UAS in DEPS: Indicates model fails to capture grammatical relations, likely due to small fine-tuning set (1k sentences) or interference
  - High performance on NLI by Mandarin model: Indicates task may be solvable via script-identification or shallow semantic overlap rather than deep Cantonese understanding

- **First 3 experiments:**
  1. Baseline Reproduction: Run bert-base-chinese on WSD task to verify if it relies on Mandarin meanings or fails on Cantonese-specific senses
  2. Ablation on Tokenization: Train "Monolingual" model using Mandarin character tokenizer to isolate impact of vocabulary vs. training data
  3. Data Scaling: Evaluate POS performance using subsets of pre-training data (10%, 50%, 100%) to estimate data efficiency curve for syntactic learning

## Open Questions the Paper Calls Out

- **Can a monolingual Cantonese model outperform Cantonese-adapted models on semantic tasks if trained on a corpus scale comparable to Mandarin or English?**
  - Basis: The authors attribute monolingual model's lower semantic performance to "small size of our Cantonese pre-training corpus (roughly 700M characters)" and call for "richer, higher-quality Cantonese corpora"
  - Why unresolved: Unclear if adapted models' superior performance is due to transfer efficiency or insufficient native training data
  - Evidence needed: Pre-training monolingual model on multi-billion character Cantonese corpus and evaluating on NLI and WSD tasks

- **To what extent do current Cantonese NLU models generalize to informal, colloquial, and code-switched language varieties?**
  - Basis: Section 7 notes benchmark "focuses on written Cantonese and does not address spoken or colloquial aspects... such as code-switching," despite these being "integral to Cantonese"
  - Why unresolved: Current benchmark relies on formal sources (Wikipedia, reviews), leaving efficacy on vernacular speech untested
  - Evidence needed: Evaluating models on dataset containing informal conversational text and mixed Mandarin-Cantonese code-switching

- **Are low dependency parsing scores a result of small treebank size or fundamental deficiencies in linguistic representations learned via current pre-training methods?**
  - Basis: Authors note dependency parsing remains "notably weak" despite monolingual model excelling in POS tagging, and other low-resource languages achieved better results with smaller datasets
  - Why unresolved: Interaction between extremely small fine-tuning dataset (1k sentences) and specific architectural requirements for parsing is not fully disentangled
  - Evidence needed: Experimenting with data augmentation for UD treebank or semi-supervised parsing techniques to see if performance saturates

## Limitations
- Architecture specification for monolingual model is not explicitly stated, requiring assumption of bert-base configuration
- Key pre-training hyperparameters (learning rate schedules, masking strategies, compute budgets) are unspecified
- Task-specific evaluation methods may not fully capture model capabilities, particularly for WSD using zero-shot cosine similarity
- Data representation analysis is incomplete, with unclear composition of pre-training corpus and presence of vernacular vs. standard characters
- Syntactic interference mechanism is asserted but not empirically verified

## Confidence

- **High Confidence**: Claims about benchmark construction and task diversity are well-supported by detailed dataset specifications and experimental results showing Cantonese-adapted models perform best overall
- **Medium Confidence**: Mechanisms explaining monolingual success on syntactic tasks and Mandarin model competitiveness are plausible but not empirically verified
- **Low Confidence**: Assumption that semantic gap between Mandarin and Cantonese is smaller than data gap is not directly tested; tokenizer impact on monolingual performance is not isolated

## Next Checks
1. **Tokenizer Ablation Study**: Train monolingual Cantonese model using Mandarin character tokenizer (8k vocab) instead of BPE tokenizer (32k vocab) while keeping all other factors constant to isolate whether tokenizer or training data drives syntactic performance

2. **Mandarin Model Performance Analysis**: Evaluate bert-base-chinese on tasks requiring Cantonese-specific knowledge (WSD, LAJ) to determine if it fails completely or relies on Mandarin meanings, clarifying actual semantic transfer capability

3. **Data Scaling Experiment**: Evaluate POS performance using progressively larger subsets of pre-training data (10%, 50%, 100%) to quantify data efficiency curve for syntactic learning and determine whether monolingual success is sustainable with limited resources