---
ver: rpa2
title: 'Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation
  Explainers'
arxiv_id: '2512.15674'
source_url: https://arxiv.org/abs/2512.15674
tags:
- activation
- training
- oracle
- activations
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Activation Oracles, a method for training
  language models to interpret their own internal activations and answer arbitrary
  natural language questions about them. The authors scale up prior work by training
  on diverse datasets including system prompt interpretation, binary classification,
  and self-supervised context prediction tasks.
---

# Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers

## Quick Facts
- arXiv ID: 2512.15674
- Source URL: https://arxiv.org/abs/2512.15674
- Authors: Adam Karvonen; James Chua; Clément Dumas; Kit Fraser-Taliente; Subhash Kantamneni; Julian Minder; Euan Ong; Arnab Sen Sharma; Daniel Wen; Owain Evans; Samuel Marks
- Reference count: 40
- Primary result: Activation Oracles trained on diverse datasets can generalize to out-of-distribution activation interpretation tasks, matching or exceeding white-box baselines on secret-keeping benchmarks.

## Executive Summary
This paper introduces Activation Oracles, a method for training language models to interpret their own internal activations and answer arbitrary natural language questions about them. The authors scale up prior work by training on diverse datasets including system prompt interpretation, binary classification, and self-supervised context prediction tasks. They evaluate Activation Oracles on four downstream auditing tasks requiring generalization to fine-tuned models, finding that even narrowly-trained models can succeed and that training data diversity yields consistent improvements. On secret-keeping benchmarks where models conceal information, Activation Oracles match or exceed white-box baselines on all three tasks and the best overall baseline on two of three. On emergent misalignment detection, they achieve comparable performance to model-diffing baselines. The results demonstrate that diversified training enables language models to generalize to out-of-distribution activation interpretation tasks and provide a simple, accessible alternative to specialized interpretability techniques.

## Method Summary
Activation Oracles train language models to interpret activation vectors from other models using norm-matched additive steering. The method involves collecting activation vectors from target models at 25%, 50%, and 75% layer depths, then training a separate "oracle" model to answer natural language questions about these activations. Training uses a mixture of three datasets: SPQA (system prompt questions and answers), binary classification tasks, and self-supervised context prediction. The oracle model is LoRA-tuned with activations injected after transformer layer 1 using norm-matched addition rather than replacement. Training runs for approximately 10 H100-hours for 8B models or 90 H200-hours for 70B models, using a mixture of single-token and multi-token inputs across the three task types.

## Key Results
- Diverse training mixtures consistently improve oracle performance, with truncated mixtures (same sample count, more diversity) outperforming less diverse baselines
- Ten of twelve training combinations achieve best performance with the full mixture, though SPQA-only training is less stable across learning rates
- Activation Oracles match or exceed white-box baselines on all three secret-keeping tasks and achieve the best overall baseline on two of three
- Performance on emergent misalignment detection is comparable to model-diffing baselines, demonstrating successful generalization to qualitatively different tasks

## Why This Works (Mechanism)

### Mechanism 1: Norm-Matched Additive Activation Injection
Injecting activation vectors via norm-matched addition enables stable training and flexible multi-vector inputs. This prevents norm explosion observed with replacement (100,000x growth at layer 1) while maintaining signal strength across varying activation sources.

### Mechanism 2: Task Diversity Enables Out-of-Distribution Generalization
Training on heterogeneous tasks creates a general capability to verbalize activation content, not task-specific heuristics. The mixture prevents overfitting to surface patterns and forces learning of transferable activation-to-language mappings.

### Mechanism 3: Layer 1 Injection Improves LoRA Integration
Steering after transformer layer 1 works better with LoRA fine-tuning for the oracle model. Injecting at a deeper position gives the LoRA adapter more model layers to transform the steered representation before output.

## Foundational Learning

- **Residual Stream and Activation Spaces**: Activations are the running sum of all layer outputs; understanding this is essential for interpreting what information they contain.
- **Activation Steering / Control Vectors**: The core mechanism is steering the oracle model with activation vectors; you need to understand how adding vectors to the residual stream can bias model outputs.
- **Out-of-Distribution Generalization in Interpretability**: The entire value proposition is that oracles generalize to tasks far from their training data; understanding OOD transfer helps predict where it will work or fail.

## Architecture Onboarding

**Component map:**
Target Model → Activation Extraction → Norm-Matched Steering Vectors → Oracle Model → Natural Language Output

**Critical path:**
1. Extract activations from target model at specified layer(s) and token position(s)
2. Construct oracle prompt with placeholder tokens for each activation vector
3. Inject vectors via norm-matched addition after layer 1
4. Oracle generates natural language response to the query about activations

**Design tradeoffs:**
- Full sequence vs. single-token inputs: Full sequence provides context but risks "text inversion"; single token isolates deeper representations but loses context
- Layer selection: Layer 50% is default; earlier layers have less task-relevant information, later layers may overfit to output tokens
- Training data mixture: More diversity helps, but each task type has diminishing returns

**Failure signatures:**
- Oracle outputs generic responses regardless of input → likely layer 0 injection or insufficient training
- Oracle reconstructs input text instead of interpreting activations → provide less context
- High variance across prompts → SPQA-only training is unstable; use full mixture
- Near-random performance on new model architecture → oracle may not transfer; retrain on target model's activations

**First 3 experiments:**
1. Reproduce the SPQA-only baseline on one secret-keeping task (Taboo) to validate your training pipeline
2. Ablate token position selection on a held-out classification task: compare activations from final token, 5 tokens before EOS, and full sequence
3. Test layer generalization: train on 25/50/75% layers but evaluate on unseen layers (33%, 66%)

## Open Questions the Paper Calls Out

**Open Question 1**: How can Activation Oracles be trained to express uncertainty or provide calibrated confidence scores rather than generating confident but incorrect guesses?
The current training objective incentivizes the model to produce an answer regardless of the information density in the activation vector, making the output unreliable for high-stakes auditing without external verification.

**Open Question 2**: For which specific real-world safety tasks do Activation Oracles provide a significant advantage over simple prompting baselines?
The paper evaluates on controlled "narrow fine-tunes" and auditing games, but the utility of AOs over black-box prompting for complex, realistic post-training pipelines remains uncertain.

**Open Question 3**: Can reinforcement learning (RL) or on-policy data generation substantially improve the generalization of Activation Oracles?
The current study relies on supervised fine-tuning on a mixture of static datasets; the potential gains from using RL to optimize the verbalization policy or using on-policy sampling to diversify training are unknown.

## Limitations
- Task generalization boundaries are unclear - success on secret-keeping and misalignment detection may not extend to other types of activation interpretation tasks
- Activation representation universality is unproven - the claim that diverse training creates general "activation-to-language" mappings assumes comparable information structures across architectures
- Steering injection sensitivity is not fully explored - layer 1 injection findings are based primarily on Llama-3.3-70B experiments

## Confidence

**High Confidence**: Diverse training mixtures consistently improve oracle performance across multiple auditing tasks, supported by extensive ablation studies.

**Medium Confidence**: Norm-matched additive steering is more stable than replacement-based methods, though the specific scaling factor may be sensitive to activation distributions.

**Medium Confidence**: Activation oracles provide a simpler alternative to specialized interpretability techniques, with competitive performance against white-box baselines on secret-keeping tasks.

## Next Checks

**Validation Check 1**: Train activation oracles on one model family (e.g., Llama) and evaluate them on a substantially different architecture (e.g., Mistral) to test cross-architecture transfer.

**Validation Check 2**: Systematically test oracle performance on layers never seen during training (e.g., train on layers 33/66/100%, evaluate on 25/50/75%) to reveal whether oracles learn general principles or memorize patterns.

**Validation Check 3**: Evaluate oracles on tasks requiring interpretation of qualitatively different activation types: chain-of-thought reasoning traces, tool-use decisions, and multimodal inputs to test true generalization capabilities.