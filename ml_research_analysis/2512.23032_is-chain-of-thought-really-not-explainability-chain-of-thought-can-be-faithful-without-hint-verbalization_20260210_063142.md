---
ver: rpa2
title: Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful
  without Hint Verbalization
arxiv_id: '2512.23032'
source_url: https://arxiv.org/abs/2512.23032
tags:
- hint
- faithful
- professor
- black
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper challenges the reliability of the Biasing Features\
  \ metric for evaluating CoT faithfulness. The authors argue that this metric conflates\
  \ unfaithfulness with incompleteness\u2014the necessary compression of a model\u2019\
  s distributed computation into a linear narrative."
---

# Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization

## Quick Facts
- arXiv ID: 2512.23032
- Source URL: https://arxiv.org/abs/2512.23032
- Authors: Kerem Zaman; Shashank Srivastava
- Reference count: 40
- This paper challenges the reliability of the Biasing Features metric for evaluating CoT faithfulness, arguing it conflates unfaithfulness with incompleteness.

## Executive Summary
This paper challenges the reliability of the Biasing Features metric for evaluating CoT faithfulness. The authors argue that this metric conflates unfaithfulness with incompleteness—the necessary compression of a model's distributed computation into a linear narrative. Using datasets like OpenbookQA, StrategyQA, and ARC-Easy with Llama-3 and Gemma-3 models, they show that CoTs labeled unfaithful by Biasing Features are often faithful under alternative metrics like Filler Tokens and FUR, with over 50% of flagged CoTs judged faithful by other measures. They introduce faithful@k, showing that larger inference-time budgets increase hint verbalization rates up to 90%, suggesting many "unfaithful" CoTs are incomplete rather than misleading. Causal Mediation Analysis further demonstrates that non-verbalized hints can still causally influence predictions via the CoT. The authors caution against relying solely on hint-based evaluations and advocate for a broader interpretability toolkit.

## Method Summary
The authors evaluate CoT faithfulness using multiple complementary metrics beyond the Biasing Features approach. They employ Filler Tokens (corrupting CoT with "..." to test if predictions change), FUR (iterative unlearning of reasoning steps), and faithful@k (measuring verbalization probability across k samples). They also apply Causal Mediation Analysis to decompose hint effects into direct and indirect pathways, and use Logit Lens to trace hint-related representations across transformer layers. The evaluation spans three reasoning datasets (OpenbookQA, StrategyQA, ARC-Easy) with Llama-3 and Gemma-3 models under different hint types (Professor, Metadata, Black Squares).

## Key Results
- Over 80% of CoTs are labeled unfaithful by Biasing Features metric across datasets
- >50% of CoTs flagged as unfaithful by Biasing Features are judged faithful by Filler Tokens or FUR metrics
- faithful@k reaches ~90% for Professor hints with larger inference budgets (k=16)
- Causal Mediation Analysis shows non-verbalized hints can still causally mediate predictions through CoTs

## Why This Works (Mechanism)

### Mechanism 1: Incompleteness vs. Unfaithfulness Decomposition
- Claim: The Biasing Features metric conflates non-verbalization with unfaithfulness, but these are distinct phenomena.
- Mechanism: Transformer computation is distributed and parallel; mapping it to linear natural language requires lossy compression and selectivity. A CoT that omits a hint may still faithfully reflect the reasoning process without capturing every influential factor.
- Core assumption: Faithfulness should be defined as alignment between explanation and decision-relevant computation, not exhaustive verbalization of all biasing features.
- Evidence anchors:
  - [abstract] "We argue this metric confuses unfaithfulness with incompleteness, the lossy compression needed to turn distributed transformer computation into a linear natural language narrative."
  - [section 5] "If natural language explanations are viewed as compressed, interpretable representations... it is unreasonable to expect them to explicitly capture all influential decision factors."
  - [corpus] Weak direct corpus support; related work (e.g., "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful") generally supports unfaithfulness concerns but uses similar hint-based metrics.
- Break condition: If a CoT actively contradicts the model's actual reasoning path (not merely omits factors), this mechanism does not apply.

### Mechanism 2: Causal Mediation Through Non-Verbalizing CoTs
- Claim: CoTs can causally mediate hint effects on predictions even when hints are not explicitly verbalized.
- Mechanism: Using Causal Mediation Analysis, the Natural Indirect Effect (NIE)—the effect of the hint transmitted through the CoT—remains statistically significant (confidence intervals exclude zero) even for non-verbalizing CoTs. Hints propagate through intermediate reasoning representations visible via Logit Lens at later layers (peaks around layers 20-25).
- Core assumption: A significant NIE indicates the CoT is causally involved in the decision pathway, not merely post-hoc rationalization.
- Evidence anchors:
  - [abstract] "Using Causal Mediation Analysis, we further show that even non-verbalized hints can causally mediate prediction changes through the CoT."
  - [section 6.2] "all NIE confidence intervals exclude zero, indicating that CoTs generated under hinted inputs have a significant causal effect on predictions even when the hint is not explicitly verbalized."
  - [corpus] No direct corpus corroboration for this specific causal mediation finding.
- Break condition: If NDE (direct effect) vastly dominates NIE across all hint types and tasks, CoTs would be closer to post-hoc rationalization; the paper shows mixed patterns depending on hint type.

### Mechanism 3: Inference-Time Budget Scaling on Verbalization
- Claim: Apparent unfaithfulness decreases when models are given more inference-time token budget (samples).
- Mechanism: The faithful@k metric (adapted from pass@k) measures the probability of obtaining at least one hint-verbalizing CoT in k samples. For the Professor hint, faithful@16 reaches ~0.9 for gemma-3-4b-it, suggesting incompleteness explains much apparent unfaithfulness.
- Core assumption: If non-verbalization were true unfaithfulness, increasing k should not increase verbalization rates substantially.
- Evidence anchors:
  - [abstract] "With a new faithful@k metric, we show that larger inference-time token budgets greatly increase hint verbalization (up to 90% in some settings)."
  - [section 5.2] "Under the Professor hint, gemma-3-4b-it reaches close to 0.9 at k=16 on average."
  - [corpus] "Thought Branches: Interpreting LLM Reasoning Requires Resampling" supports the general principle that studying single CoT samples is inadequate.
- Break condition: If faithful@k stays flat as k increases (as observed for Black Squares and Metadata hints), this indicates genuine unfaithfulness rather than incompleteness.

## Foundational Learning

- Concept: **Causal Mediation Analysis (Pearl, 2001)**
  - Why needed here: Required to decompose whether the CoT is causally involved in prediction changes (indirect effect) versus the hint directly causing the output (direct effect).
  - Quick check question: If you intervene to replace a CoT with one generated under a hinted input, does the prediction change? This is the NIE.

- Concept: **Logit Lens**
  - Why needed here: Enables tracing how hint-related representations emerge across layers by decoding intermediate activations into vocabulary logits.
  - Quick check question: At which layer do hint-related tokens (e.g., "Stanford", "professor") appear in top-5 decoded logits during CoT generation?

- Concept: **Biasing Features vs. Corruption-Based Faithfulness Metrics**
  - Why needed here: The paper critiques hint-verbalization metrics and advocates for complementary approaches (Filler Tokens, FUR) that test whether CoTs are causally necessary for predictions.
  - Quick check question: If you corrupt the CoT to "...", does the prediction change? If yes, the CoT has contextual faithfulness under Filler Tokens.

## Architecture Onboarding

- Component map:
  Input layer -> CoT generation -> Prediction -> Evaluation layer (Biasing Features, Filler Tokens, FUR, faithful@k, CMA) -> Analysis layer (Logit Lens)

- Critical path:
  1. Identify instances where hint flips prediction (ŷh = Lh)
  2. Check verbalization (LLM-as-judge)
  3. Apply Filler Tokens corruption test
  4. Run FUR unlearning on reasoning steps
  5. Compute faithful@k across samples
  6. Estimate NDE/NIE via Causal Mediation

- Design tradeoffs:
  - LLM-as-judge vs. lexical matching: Judge captures semantic verbalization but has 31% recall; lexical is strict but overflags. Paper uses judge with 80% accuracy validation.
  - FUR computational cost: Requires iterative unlearning per reasoning step; impractical for larger models (>8B).
  - faithful@k sampling cost: 128 samples per instance with LLM evaluation is expensive for long-CoT models.

- Failure signatures:
  - Empty/degenerate CoTs (observed with Llama-3-8B-Instruct under Metadata/Black Squares hints)
  - NDE dominating NIE (suggests post-hoc rationalization; observed under Metadata hint)
  - faithful@k flat across k (indicates genuine unfaithfulness, not incompleteness)

- First 3 experiments:
  1. Reproduce Biasing Features unfaithfulness rates on your target model/dataset to establish baseline. Expect 80%+ unfaithfulness for Professor hints per Figure 2.
  2. Run Filler Tokens test on CoTs flagged unfaithful: replace CoT with "..." and check if prediction changes. If >20% change, contextual faithfulness exists despite non-verbalization.
  3. Estimate faithful@k with k ∈ {1, 4, 16} using n=32+ samples per instance. If faithful@k increases with k, incompleteness is a factor; if flat, unfaithfulness is genuine.

## Open Questions the Paper Calls Out
None

## Limitations
- Results are constrained to specific tasks (OpenbookQA, StrategyQA, ARC-Easy) and model families (Llama-3, Gemma-3)
- FUR is computationally expensive and doesn't scale to larger models (>8B parameters)
- faithful@k requires expensive sampling (128 samples per instance with LLM evaluation)
- Heavy reliance on LLM-as-judge evaluations with only 80% accuracy validation
- Absence of corpus-level corroboration for specific causal mediation findings

## Confidence
- **High confidence**: The empirical observation that Biasing Features flags >80% of CoTs as unfaithful (Figure 2) is well-supported and reproducible. The faithful@k results showing increasing verbalization with larger inference budgets (up to 90% for Professor hints) are robust across replications.
- **Medium confidence**: The claim that Biasing Features conflates incompleteness with unfaithfulness is logically compelling but relies on the assumption that increasing k should reveal verbalized hints if they exist. The interpretation of Causal Mediation Analysis results requires domain expertise in causal inference.
- **Low confidence**: The assertion that all non-verbalized hints represent incomplete rather than unfaithful reasoning is not fully supported. The paper acknowledges that Metadata and Black Squares hints show flat faithful@k curves, indicating genuine unfaithfulness in some cases.

## Next Checks
1. **Cross-task generalization test**: Apply the full evaluation suite (Biasing Features, Filler Tokens, FUR, faithful@k, CMA) to a new reasoning dataset like CommonsenseQA or StrategyQA-long to verify whether the incompleteness vs. unfaithfulness pattern holds across task domains.

2. **Model size scaling experiment**: Compare faithfulness metrics across different model scales (e.g., 3B, 8B, 70B parameters) using the same tasks to determine if larger models show systematically different patterns of hint verbalization and causal mediation.

3. **Alternative causal inference validation**: Replicate the Causal Mediation Analysis using an independent causal inference method such as the front-door adjustment criterion or counterfactual reasoning to verify that the observed NIE significance is robust to methodological choices.