---
ver: rpa2
title: 'LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End
  Liquid Cooling Optimization in Data Centers'
arxiv_id: '2511.00116'
source_url: https://arxiv.org/abs/2511.00116
tags:
- cooling
- temperature
- tower
- control
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LC-Opt introduces a reinforcement learning benchmark for liquid\
  \ cooling optimization in data centers, extending Oak Ridge National Laboratory\u2019\
  s Frontier supercomputer digital twin with RL control interfaces. The environment\
  \ enables scalable multi-agent RL control of cooling tower setpoints and blade-group\
  \ valve actuation via a Gymnasium interface, supporting both centralized and decentralized\
  \ action execution."
---

# LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers

## Quick Facts
- **arXiv ID**: 2511.00116
- **Source URL**: https://arxiv.org/abs/2511.00116
- **Reference count**: 40
- **Primary result**: RL achieves 95.63% blade-group temperature compliance and 21% cooling tower power reduction vs. rule-based baseline

## Executive Summary
LC-Opt introduces a reinforcement learning benchmark for liquid cooling optimization in data centers, extending Oak Ridge National Laboratory's Frontier supercomputer digital twin with RL control interfaces. The environment enables scalable multi-agent RL control of cooling tower setpoints and blade-group valve actuation via a Gymnasium interface, supporting both centralized and decentralized action execution. Benchmarking centralized action with a multi-head policy achieved 95.63% blade-group temperature compliance and reduced cooling tower power consumption by 21% compared to baseline rule-based control. The framework also supports policy distillation into decision trees and LLMs for interpretable, explainable control actions, fostering trust and simplifying system management.

## Method Summary
The LC-Opt framework uses Modelica-based physics models compiled into FMUs, with Python wrappers providing a Gymnasium interface. A PPO agent employs centralized action execution with a multi-head policy architecture—one head for continuous CDU setpoints using tanh activation, and a second using softplus output fitted to a Dirichlet distribution for valve actuation. The Dirichlet enforces the physical constraint that valve openings must sum to 1.0 while enabling independent optimization of temperature and flow decisions. The approach leverages observation decomposition and batching to scale training across thousands of blade groups, capitalizing on conditional independence assumptions between cabinets.

## Key Results
- Centralized multi-head PPO policy achieved 95.63% blade-group temperature compliance versus 76.92% for baseline rule-based control
- 21% reduction in cooling tower power consumption compared to baseline
- Scalable to 4 towers and 25 cabinets while maintaining >90% compliance
- Policy distillation successfully transferred learned behaviors to LLMs and decision trees for interpretable control

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: A multi-head policy architecture improves blade-group temperature compliance compared to single-head or rule-based baselines by explicitly separating control domains.
- **Mechanism**: The policy decouples the action space into two distinct heads: one using `tanh` activation for continuous CDU setpoints/flow, and a second using a `softplus` output fitted to a Dirichlet distribution for valve actuation. This enforces the physical constraint that valve openings must sum to 1.0 (mass conservation) while independently optimizing temperature targets.
- **Core assumption**: The optimal valve distribution correlates non-linearly with heat input, and temperature/flow decisions can be optimized in parallel without interfering with the mass conservation constraint.
- **Evidence anchors**:
  - [Section 4.2]: Describes the multi-headed policy where Head 1 generates temperature/flow and Head 2 uses Dirichlet distribution for valves summing to 1.0.
  - [Table 3]: Shows "CA & Multihead policy" (Case 7) achieving 95.63% compliance vs 76.92% for baseline.
  - [Corpus]: General RL HVAC literature supports RL efficacy, but specific architectural validation for multi-head Dirichlet policies in cooling is sparse in the provided neighbors.
- **Break condition**: If blade groups are thermally coupled such that mass flow adjustments in one group destabilize the temperature of another beyond the model's prediction horizon, the independent head assumption may fail.

### Mechanism 2
- **Claim**: Centralized Action (CA) execution enables scalable control of large-scale data centers by reducing policy complexity without significantly sacrificing performance.
- **Mechanism**: Instead of training separate critics for thousands of blade groups, the approach uses observation decomposition where a single policy network processes batched states from similar components (e.g., all cooling towers). This leverages the "conditional independence" of cabinets (d-separation) to scale training.
- **Core assumption**: Blade groups across cabinets are conditionally independent given the workload, and cooling towers are loosely coupled except for shared power/return temperature variables.
- **Evidence anchors**:
  - [Section 4.1]: Details the observation decomposition and batching to handle $\sim 10^4$ blade groups.
  - [Table 4]: Demonstrates that the CA policy maintains >90% compliance even when scaled to 4 towers and 25 cabinets, whereas baseline performance fluctuates.
  - [Corpus]: Neighbors like "Hierarchical Multi-Agent Framework..." suggest scalability is a key challenge in cluster-level control, aligning with this solution.
- **Break condition**: If workload skew creates strong thermal dependencies between specific non-adjacent cabinets, the conditional independence assumption breaks, potentially requiring graph-based message passing.

### Mechanism 3
- **Claim**: Policy distillation into Large Language Models (LLMs) or Decision Trees preserves control performance while enabling explainability.
- **Mechanism**: Expert trajectories (state-action pairs) are generated by the trained RL agent (PPO). These are used to fine-tune an instruction-tuned LLM (via QLoRA) or train a Decision Tree. The LLM learns to map states to actions *and* generate natural language rationales.
- **Core assumption**: The RL policy has captured the optimal mapping and the student model (LLM/Tree) has sufficient capacity to approximate this mapping without catastrophic forgetting.
- **Evidence anchors**:
  - [Section 5.1]: Describes the VIPER-adapted distillation process using weighted experience data.
  - [Figure 10]: Indicates that distilled Qwen models outperformed the base LLM and matched/exceeded the RL oracle in power/temperature metrics.
  - [Corpus]: External validation of LLMs as direct controllers is limited in the immediate corpus; focus is generally on RL or predictive modeling.
- **Break condition**: If the LLM hallucinates safety-critical constraints not present in the fine-tuning data, or if the action space requires sub-millisecond latency incompatible with LLM inference.

## Foundational Learning

- **Concept**: **Functional Mock-up Interface (FMU) & Co-Simulation**
  - **Why needed here**: The environment relies on Modelica-based physics models compiled into FMUs. Understanding how Python (FMPy/PyFMI) steps through the FMU time steps is critical for debugging latency and simulation stability.
  - **Quick check question**: Can you explain why the `do_step()` method in the FMU is called multiple times per agent step in the LC-Opt architecture?

- **Concept**: **Action Space Constraints (Dirichlet Distribution)**
  - **Why needed here**: The paper uses a Dirichlet distribution to output valve actuations. Standard RL distributions (Gaussian) do not naturally enforce the constraint that discrete actions sum to 1 (mass conservation).
  - **Quick check question**: How does the Dirichlet distribution differ from a Softmax output in terms of variance and sparsity for valve control?

- **Concept**: **Trim and Respond Logic (ASHRAE G36)**
  - **Why needed here**: This is the rule-based baseline. To prove RL value, you must understand the "Trim" (gradually moving setpoints towards efficiency) and "Respond" (reacting to violations) logic it replaces.
  - **Quick check question**: In the baseline logic, what triggers a shift from "Trim" to "Respond" mode for the coolant supply temperature?

## Architecture Onboarding

- **Component map**: Digital Twin (Modelica) -> FMU -> LC-Opt Wrapper (Python/Gymnasium) -> PPO Agent (Centralized Action + Multi-Head Policy) -> Distillation Module (VIPER/QLoRA)

- **Critical path**: The `AutoCSM API` generating the Modelica model → Compilation to FMU → Gymnasium Wrapper → PPO Training Loop

- **Design tradeoffs**:
  - **Fidelity vs. Speed**: The Modelica model is high-fidelity but computationally expensive. The paper notes the limitation of memory cost with batched inference for large-scale data centers [Table 11].
  - **Interpretability vs. Performance**: Decision Trees are highly interpretable but may lose the nuance of the NN policy; LLMs offer explanations but introduce inference latency.

- **Failure signatures**:
  - **Action Saturation**: If the multi-head policy outputs consistently maxed-out valve values (0 or 1), the Dirichlet distribution may have collapsed.
  - **Simulation Drift**: If FMU steps desync from agent steps, observations will lag, causing oscillatory control.
  - **Critic Divergence**: In multi-agent setups, if centralized training is used on tightly coupled entities (which the paper advises against), critic convergence fails [Section 3.3].

- **First 3 experiments**:
  1. **Baseline Validation**: Run the ASHRAE G36 "Trim and Respond" logic on a single cabinet configuration to establish the `D_blade_avg` and power consumption ground truth.
  2. **Architecture Ablation**: Train a single-head PPO agent vs. the Multi-Head (Dirichlet) agent on the Blade Group MDP to isolate the impact of the mass-conserving action head.
  3. **Scale Test**: Deploy the trained centralized policy on a configuration with 10+ cabinets (unseen during training) to verify the scalability claims made in [Table 4].

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can chip-level thermal modeling be integrated into the LC-Opt framework to accurately capture next-generation GPU thermal dynamics in high-density liquid-cooled servers?
- **Basis in paper**: [explicit] "The benchmark has a few limitations we aim to address: incorporating chip-level thermal modeling to support next-gen GPUs in high-density servers..."
- **Why unresolved**: Current blade-group level modeling abstracts away chip-level thermal gradients that become critical at higher power densities.
- **What evidence would resolve it**: Successful integration of validated chip-level thermal models that predict GPU junction temperatures within acceptable error margins under dynamic workloads.

### Open Question 2
- **Question**: What control strategies are effective for hybrid air-liquid cooling systems, and how can the LC-Opt benchmark be extended to model their coupled thermal dynamics?
- **Basis in paper**: [explicit] Limitations state: "adding hybrid cooling (air + liquid) used in some data centers" as an open direction.
- **Why unresolved**: The current benchmark focuses exclusively on liquid cooling; hybrid systems introduce additional control variables and thermal coupling not yet modeled.
- **What evidence would resolve it**: Benchmark extension with validated hybrid cooling models and demonstration of RL policies achieving energy-temperature tradeoffs comparable to pure liquid cooling.

### Open Question 3
- **Question**: How can multi-head policies be improved to prevent elevated temperatures in low-power blade groups while maintaining overall cooling efficiency?
- **Basis in paper**: [explicit] "Figure 9 also shows reduced valve actuation for Blade Group 1, correlating with its lower power profile, occasionally resulting in slightly elevated temperatures for Blade Group 1. It is a limitation we aim to address in future work."
- **Why unresolved**: The multihead policy learns proportional valve allocation based on power input, but this heuristic can undercool low-power blade groups.
- **What evidence would resolve it**: Modified reward structures or policy architectures that achieve uniform temperature compliance across all blade groups regardless of power distribution.

### Open Question 4
- **Question**: Can RL policies trained in the LC-Opt digital twin successfully transfer to physical liquid cooling systems without performance degradation?
- **Basis in paper**: [inferred] Appendix H outlines a multi-phase deployment pathway but acknowledges "Bridging the Simulation-to-Real gap" with planned hardware-in-the-loop validation not yet completed.
- **Why unresolved**: Digital twin fidelity to physical systems under all operating conditions remains unvalidated, and sim-to-real transfer for thermal control is notoriously challenging.
- **What evidence would resolve it**: Successful deployment of trained policies on physical testbeds with performance metrics (temperature compliance, power consumption) matching simulation within defined tolerances.

## Limitations
- Digital twin based on single supercomputer configuration limits generalizability to heterogeneous data center architectures
- 21% power reduction measured against rule-based baseline lacks comparison against more sophisticated conventional control strategies
- Policy distillation results presented qualitatively rather than with quantitative performance metrics against original RL policy

## Confidence
- **High**: The multi-head policy architecture and its Dirichlet-based valve control mechanism are well-described and internally consistent
- **Medium**: The centralized action execution scalability claims are supported by Table 4 but rely on the conditional independence assumption without extensive validation
- **Low**: The LLM distillation approach lacks rigorous performance evaluation and raises concerns about real-time inference feasibility

## Next Checks
1. Test the trained policy on configurations with non-uniform heat distributions and thermal coupling between blade groups to validate the conditional independence assumption
2. Benchmark against a Model Predictive Control (MPC) baseline using the same digital twin to establish relative performance
3. Conduct a systematic ablation study measuring the performance degradation when distilling the policy to Decision Trees versus LLMs to quantify the interpretability-performance tradeoff