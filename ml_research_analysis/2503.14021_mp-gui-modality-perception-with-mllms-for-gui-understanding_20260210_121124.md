---
ver: rpa2
title: 'MP-GUI: Modality Perception with MLLMs for GUI Understanding'
arxiv_id: '2503.14021'
source_url: https://arxiv.org/abs/2503.14021
tags:
- mp-gui
- screen
- data
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MP-GUI, a multimodal large language model
  (MLLM) specifically designed for graphical user interface (GUI) understanding. Unlike
  existing MLLMs that treat GUI as unstructured natural images, MP-GUI extracts graphical,
  textual, and spatial modality signals through three specialized perceivers, combined
  via a fusion gate that adapts to different task requirements.
---

# MP-GUI: Modality Perception with MLLMs for GUI Understanding

## Quick Facts
- arXiv ID: 2503.14021
- Source URL: https://arxiv.org/abs/2503.14021
- Reference count: 40
- Primary result: MP-GUI achieves 13.2% improvement on screen grounding with only 0.68M training samples versus millions used by other GUI-specific methods

## Executive Summary
MP-GUI introduces a novel multimodal large language model architecture specifically designed for graphical user interface understanding. The key innovation is a modality-aware perception system that explicitly separates and processes textual, graphical, and spatial information from GUI screens through specialized perceivers, combined via a task-adaptive fusion gate. This approach outperforms existing MLLMs on various GUI understanding tasks including screen grounding, navigation, captioning, and question answering, while requiring significantly less training data than competing GUI-specific methods.

## Method Summary
MP-GUI employs a multi-stage training strategy using a specialized TGS-Perception Fusion Module (TGS-PFM) that separates GUI visual signals into three distinct modalities: textual, graphical, and spatial. The architecture uses InternViT-300M as the vision backbone and InternLM2.5-7B as the LLM, with three MLP-based perceivers (TxP, GaP, SaP) that extract modality-specific features. A cross-attention fusion gate dynamically combines these features based on the instruction text. Training proceeds through four stages: warming up each perceiver on filtered data, then training the fusion gate on synthetic data, followed by multi-task fine-tuning on benchmarks. The model uses LoRA adapters with varying ranks across stages and processes high-resolution screenshots using dynamic 448x448 patch splitting.

## Key Results
- Achieves 13.2% improvement on screen grounding tasks compared to existing MLLMs
- Outperforms GUI-specific methods while requiring only 0.68M training samples versus millions used by competitors
- Sets new state-of-the-art performance on screen grounding, captioning, and navigation benchmarks
- Competitive results on question answering tasks while maintaining architectural efficiency

## Why This Works (Mechanism)

### Mechanism 1: Explicit Modality Decoupling via Specialized Perceivers
GUI screens contain distinct textual, graphical, and spatial modalities that standard MLLMs conflate. MP-GUI uses three parallel MLPs (Textual, Graphical, Spatial Perceivers) to isolate these signals before fusion. Evidence shows significant performance drops when the Spatial Perceiver is removed, confirming the importance of explicit separation.

### Mechanism 2: Task-Adaptive Semantic Gating
Different GUI tasks require different weightings of visual modalities. The Fusion Gate uses cross-attention with the question embedding to dynamically emphasize text, graphics, or spatial signals based on the specific query. Ablation studies show distinct performance drops when this gating mechanism is removed, particularly for reasoning and localization tasks.

### Mechanism 3: Explicit Spatial Relationship Pre-training
Standard MLLMs lack intuition for GUI layout hierarchies. The Spatial Perceiver is pre-trained on spatial relationship prediction tasks that classify containment, sibling, and unrelated relationships between UI elements, injecting layout awareness before main fine-tuning.

## Foundational Learning

- **Dynamic High-Resolution Splitting (Dynamic High-Res)**: Needed because GUI elements are often small text or icons that standard 224x224 or 336x336 resolution destroys. Quick check: How does the model merge features from multiple sub-image patches back into a coherent representation for the LLM?

- **Multi-Stage vs. End-to-End Training**: Required because MP-GUI cannot be trained in one pass - perceivers must be sequentially "warmed up" before the fusion gate. Quick check: Why would training the Fusion Gate simultaneously with the Perceivers from scratch cause convergence issues?

- **Visual Token Selection / Compression**: Needed to handle the massive token sequences from high-res screenshots. Quick check: How many visual tokens are generated per 448x448 patch, and does the Fusion Gate reduce or expand this sequence length?

## Architecture Onboarding

- **Component map:** InternViT-300M (vision encoder) -> Alignment Projector -> TGS-PFM (TxP, GaP, SaP perceivers + Fusion Gate) -> InternLM2.5-7B (LLM) -> LoRA adapters

- **Critical path:** Data preparation (TAD/GAD filtering, SAD generation, synthetic data creation) -> Sequential training of TxP, GaP, SaP (Stages 1-3) -> Fusion Gate training on synthetic data (Stage 4) -> Multi-task fine-tuning on benchmarks

- **Design tradeoffs:** Explicit modality separation adds complexity but provides better grounding accuracy than simple projectors. The multi-stage strategy is slower to set up but yields better results than end-to-end training.

- **Failure signatures:** Small object hallucination occurs if training data doesn't respect small object thresholds; spatial confusion arises if SRP negative samples are missing; grounding coordinate mismatch if dynamic patching doesn't align with normalization logic.

- **First 3 experiments:**
  1. Overfit Test: Train only the Spatial Perceiver on SRP task in isolation and verify >90% accuracy
  2. Gate Sensitivity: Compare grounding accuracy with Fusion Gate removed (mean pooling) versus enabled
  3. Resolution Ablation: Evaluate performance on RefExp with reduced dynamic patch count (1-2 vs 6 patches)

## Open Questions the Paper Calls Out

1. How to mitigate the trade-off between acquiring GUI-specific spatial knowledge and potential degradation of generic visual-linguistic reasoning capabilities during GUI specialization.

2. The extent to which the Spatial Perceiver's performance relies on clean View Hierarchy data versus ability to infer spatial relationships from visual pixels alone in wild environments.

3. Whether the Fusion Gate's reliance on synthetic MLLM-generated data imposes an upper bound on the model's ability to handle novel or ambiguous GUI tasks.

## Limitations
- Multi-stage training procedure requires careful orchestration across four distinct phases with different LoRA configurations
- Heavy dependence on synthetic data generation quality for achieving state-of-the-art performance with limited samples
- Architectural complexity adds computational overhead compared to simpler projector-based approaches
- Some implementation details remain underspecified (MLP hidden dimensions, exact OCR filtering thresholds)

## Confidence
- **High Confidence (8-10/10):** Core architectural design with modality-specific perceivers and task-adaptive fusion is well-supported by ablation studies and shows clear performance improvements
- **Medium Confidence (5-7/10):** Claims of state-of-the-art performance with 0.68M samples rely heavily on synthetic data quality that's difficult to verify
- **Low Confidence (1-4/10):** Some implementation details remain underspecified, particularly MLP hidden dimensions and exact integration of dynamic patching with coordinate normalization

## Next Checks
1. Overfit Test: Train only the Spatial Perceiver (SaP) in isolation on the SRP task and verify it achieves >90% accuracy on held-out spatial relationship prediction

2. Gate Sensitivity Analysis: Run inference with the Fusion Gate disabled (using simple mean pooling) and compare grounding accuracy on ScreenSpot to quantify the cross-attention mechanism's specific contribution

3. Resolution Ablation: Evaluate performance on RefExp when limiting dynamic patching to 1-2 patches versus the standard 6 patches to validate whether high-resolution processing is genuinely critical for small object detection