---
ver: rpa2
title: Sparse-to-Sparse Training of Diffusion Models
arxiv_id: '2504.21380'
source_url: https://arxiv.org/abs/2504.21380
tags:
- sparse
- dense
- training
- diffusion
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces sparse-to-sparse training to diffusion models\
  \ (DMs) for the first time, aiming to improve both training and inference efficiency.\
  \ The authors propose three methods\u2014Static-DM (static strategy) and RigL-DM/MagRan-DM\
  \ (dynamic strategies)\u2014to train sparse DMs from scratch using Latent Diffusion\
  \ and ChiroDiff on six datasets."
---

# Sparse-to-Sparse Training of Diffusion Models

## Quick Facts
- arXiv ID: 2504.21380
- Source URL: https://arxiv.org/abs/2504.21380
- Authors: Inês Cardoso Oliveira; Decebal Constantin Mocanu; Luis A. Leiva
- Reference count: 32
- Key outcome: This paper introduces sparse-to-sparse training to diffusion models (DMs) for the first time, aiming to improve both training and inference efficiency. The authors propose three methods—Static-DM (static strategy) and RigL-DM/MagRan-DM (dynamic strategies)—to train sparse DMs from scratch using Latent Diffusion and ChiroDiff on six datasets. Experiments show that sparse DMs can match or even outperform their dense counterparts while significantly reducing parameters and FLOPs. Dynamic sparse training with 25-50% sparsity levels yields the best performance, and a conservative prune-and-regrowth ratio of 0.05 is effective for high-sparsity models. Overall, the results demonstrate the strong potential of sparse-to-sparse training for efficient diffusion models.

## Executive Summary
This paper presents the first application of sparse-to-sparse training to diffusion models, proposing three novel methods—Static-DM, RigL-DM, and MagRan-DM—to train sparse DMs from scratch. The methods utilize ER/ERK layer-wise sparsity allocation and apply magnitude pruning with either gradient-based or random regrowth during training. Experiments on six datasets with Latent Diffusion and ChiroDiff architectures demonstrate that sparse DMs can achieve performance on par with or exceeding dense models while significantly reducing parameters and FLOPs, particularly at sparsity levels of 25-50%.

## Method Summary
The authors introduce three sparse-to-sparse training methods for diffusion models. Static-DM uses a fixed random sparse mask initialized via ER/ERK formulas, while RigL-DM and MagRan-DM employ dynamic sparse training (DST) with magnitude pruning followed by gradient-based or random regrowth respectively. Sparsity is allocated layer-wise using ER formulas for fully connected layers and ERK variants for convolutional layers. Training involves standard diffusion objectives with periodic pruning/regrowth cycles for DST methods. The study evaluates these approaches across Latent Diffusion (applied to U-Net only) and ChiroDiff (applied to full network) on six diverse datasets, measuring FID scores and computational efficiency.

## Key Results
- Sparse DMs can match or outperform dense counterparts, with dynamic sparse training (25-50% sparsity) yielding best FID scores
- Significant parameter and FLOP reductions achieved while maintaining generation quality
- Conservative prune/regrowth ratio (p=0.05) provides better stability for high-sparsity models compared to aggressive updates
- Layer-wise sparsity allocation via ER/ERK formulas preserves critical connectivity patterns across heterogeneous layer sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic sparse training allows sparse DMs to match or outperform dense counterparts by actively exploring beneficial network topologies during training.
- Mechanism: Weight connections are periodically pruned based on magnitude (removing low-magnitude weights) and regrown using either gradient-based selection (RigL-DM) or random selection (MagRan-DM). This enables the network to escape suboptimal sparse topologies while maintaining constant sparsity.
- Core assumption: DMs are overparameterized, containing redundant connections that can be removed without performance loss, and the optimal sparse topology is not known at initialization.
- Evidence anchors:
  - [abstract] "Dynamic sparse training with 25–50% sparsity levels yields the best performance"
  - [section 4.1] "For all datasets, we successfully trained at least one sparse DM that outperforms the original Dense version"
  - [corpus] Related work on dynamic sparsity (arXiv:2106.14568) confirms DST enables performance matching in other architectures without training overhead
- Break condition: At very high sparsity (S=0.9), even DST methods fail; the network lacks sufficient capacity to model the data distribution.

### Mechanism 2
- Claim: A conservative prune-and-regrowth ratio (p=0.05) provides better stability for high-sparsity DMs than aggressive updates.
- Mechanism: Rather than replacing 50% of active connections each update cycle, replacing only 5% preserves learned representations while still allowing topology exploration. This reduces destructive interference with the denoising process being learned.
- Core assumption: Stability in the connectivity pattern is more valuable than rapid exploration when few parameters remain.
- Evidence anchors:
  - [section 4.1] "The best results were obtained with p = 0.05"
  - [section H] "At S = 0.9, DST methods using p = 0.05 have consistently better performance than their p = 0.5 counterparts"
  - [corpus] No direct corpus evidence on prune ratios specifically for DMs—this appears novel to the paper
- Break condition: With low sparsity (S=0.1-0.25), the p=0.5 ratio still performs acceptably; the benefit of p=0.05 is most pronounced at extreme sparsity.

### Mechanism 3
- Claim: Layer-wise sparsity allocation using ER/ERK formulas preserves critical connectivity patterns across heterogeneous layer sizes.
- Mechanism: Sparsity is not uniform—larger layers receive higher sparsity via sl ∝ 1 - (nl + nl-1)/(nl·nl-1), while convolutional layers use the ERK variant incorporating kernel dimensions. This prevents smaller layers from becoming bottlenecks.
- Core assumption: Not all layers require the same density of connections; the optimal sparsity ratio scales with layer dimensions.
- Evidence anchors:
  - [section 3.1] "larger layers get assigned higher sparsity than smaller layers"
  - [section 3.1] Explicit ER/ERK formulas provided for layer-wise sparsity calculation
  - [corpus] Weak corpus signal—related pruning work exists but no specific validation of ER/ERK for DMs
- Break condition: The paper does not test alternative sparsity allocation strategies; comparative efficacy remains unverified.

## Foundational Learning

- **Concept: Diffusion Model Training Objective**
  - Why needed here: Understanding the simplified L2 loss (E[||ε - εθ(xt,t)||²]) is essential to grasp what sparse training must preserve—the denoising network's ability to predict noise at each timestep.
  - Quick check question: Can you explain why the loss is computed between predicted noise and actual noise rather than between reconstructed and original images?

- **Concept: Static vs. Dynamic Sparse Training**
  - Why needed here: The paper's three methods differ fundamentally in whether topology is fixed (Static-DM) or evolving (RigL-DM, MagRan-DM); this distinction determines implementation complexity and expected performance.
  - Quick check question: What is the key difference in how Static-DM and RigL-DM handle network connections during training?

- **Concept: Unstructured vs. Structured Sparsity**
  - Why needed here: The paper uses unstructured sparsity (removing individual weights) which maintains higher performance but lacks current hardware acceleration; structured sparsity (removing entire neurons/channels) would be faster but less effective.
  - Quick check question: Why does the paper opt for unstructured sparsity despite the hardware limitations mentioned in Section 3?

## Architecture Onboarding

- **Component map:**
  - Latent Diffusion: Sparsity applied to U-Net denoiser only; autoencoder (E, D) remains dense
  - ChiroDiff: Sparsity applied throughout the entire bidirectional GRU network
  - Core sparse layers: Linear and Conv2d layers receive sparse masks; embeddings and normalizations remain dense

- **Critical path:**
  1. Initialize network with ER/ERK-based sparse topology at target sparsity S
  2. Standard diffusion training loop with noise prediction loss
  3. For DST methods: Every ΔTe iterations, execute prune-regrow cycle (magnitude pruning + gradient/random regrowth)
  4. Evaluate via FID/KID on generated samples using DDIM sampling

- **Design tradeoffs:**
  - S=0.25-0.50: Best FID performance, modest FLOP reduction (10-50%)
  - S=0.90: Maximum FLOP reduction (90%) but significant FID degradation
  - RigL-DM vs. MagRan-DM: Gradient-based regrowth requires dense gradient computation at update steps (higher memory) but generally outperforms random regrowth
  - Static-DM: Simpler implementation, better at very high sparsity (S>0.5), no gradient computation overhead

- **Failure signatures:**
  - FID divergence during training: Likely sparsity too high for dataset complexity; reduce S
  - DST worse than Static at moderate sparsity: Check if prune ratio p=0.5 is too aggressive; try p=0.05
  - Mode collapse or artifacts: May indicate imbalance in layer-wise sparsity allocation; verify ER/ERK implementation

- **First 3 experiments:**
  1. Replicate Static-DM at S=0.5 on CelebA-HQ subset to validate FID ≈ 33 matches paper; establishes baseline
  2. Compare RigL-DM with p=0.5 vs. p=0.05 at S=0.75 on same dataset; quantifies regrowth ratio impact
  3. Test MagRan-DM at S=0.25 across both Latent Diffusion and ChiroDiff architectures; verifies generalization across modalities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does employing multiple sparsity masks with varying sparsity levels, dynamically changed according to the denoising timestep, improve model efficiency or performance?
- Basis in paper: [explicit] The authors state, "Furthermore, employing multiple sparsity masks with varying sparsity levels and dynamically changing them during training, according to the denoising timestep, is a promising line of research."
- Why unresolved: The current study applies a single global sparsity mask across all denoising timesteps, leaving the potential benefits of timestep-dependent architectures unexplored.
- What evidence would resolve it: Experiments comparing the FID scores and computational costs of models utilizing timestep-specific masks against the static and dynamic baselines established in the paper.

### Open Question 2
- Question: Can adjusting Dynamic Sparse Training (DST) hyperparameters (such as the prune and regrowth rate) based on the training phase optimize convergence?
- Basis in paper: [explicit] The paper lists as a limitation and future direction: "Another interesting direction is to adjust DST hyperparameters based on the training phase, in response to changes in training dynamics."
- Why unresolved: The current research utilizes fixed values for hyperparameters like exploration frequency ($\Delta T_e$) and prune/regrowth ratio ($p$) throughout the entire training process.
- What evidence would resolve it: A study implementing a schedule for $p$ or $\Delta T_e$ (e.g., decaying or increasing over epochs) that demonstrates faster convergence or superior final FID scores compared to the fixed hyperparameter baselines.

### Open Question 3
- Question: What is the theoretical or empirical mechanism explaining why lower sparsity levels (e.g., 10%) sometimes underperform compared to moderate sparsity levels (e.g., 25%) in MagRan-DM?
- Basis in paper: [explicit] The authors note a performance dip at $S=0.1$ compared to $S=0.25$ for MagRan-DM on LSUN-Bedrooms, hypothesizing a balance between regularization and expressiveness, but conclude that "exploring this topic in depth is beyond the scope of this paper."
- Why unresolved: The paper identifies the phenomenon but does not verify if the cause is indeed a regularization trade-off or a limitation of the specific sparse initialization/growth method.
- What evidence would resolve it: Ablation studies analyzing the loss landscape or internal representations of models at $S=0.1$ vs. $S=0.25$ to quantify the regularization effect versus the loss of expressive capacity.

### Open Question 4
- Question: How do other pruning strategies (beyond magnitude pruning) or different exploration frequencies ($\Delta T_e$) impact the performance of sparse diffusion models?
- Basis in paper: [explicit] The authors mention, "There is potential in exploring other pruning strategies and other DST hyperparameters such as $\Delta T_e$."
- Why unresolved: The experiments were largely constrained to magnitude pruning and a specific set of exploration frequencies determined by a small random search.
- What evidence would resolve it: A comprehensive ablation study testing sensitivity-based pruning or gradient-based pruning methods against magnitude pruning across various exploration frequencies.

## Limitations
- Very high sparsity (S=0.9) results depend heavily on conservative prune ratio (p=0.05) without extensive corpus validation
- Layer-wise sparsity allocation via ER/ERK formulas lacks empirical validation specific to DMs
- Computational benefits measured but not benchmarked on actual hardware, leaving practical inference gains unverified
- High FID variance (±3-12) across runs suggests results may be sensitive to random seeds

## Confidence

**High Confidence**: Dense-to-dense training baseline, ER/ERK layer-wise sparsity formulas, magnitude pruning mechanics, DDIM sampling setup

**Medium Confidence**: Dynamic sparse training performance claims, the specific efficacy of p=0.05 ratio, generalization across all six datasets

**Low Confidence**: Hardware acceleration potential (no experiments conducted), practical inference speedup, exact mechanism of gradient-based regrowth in RigL-DM

## Next Checks

1. Reproduce Static-DM at S=0.5 on CelebA-HQ subset to verify FID ≈ 33 matches paper baseline
2. Compare RigL-DM with p=0.5 vs. p=0.05 at S=0.75 to quantify conservative regrowth ratio impact
3. Test MagRan-DM at S=0.25 across both Latent Diffusion and ChiroDiff to verify cross-architecture generalization