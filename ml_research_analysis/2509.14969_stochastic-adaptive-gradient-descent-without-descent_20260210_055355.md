---
ver: rpa2
title: Stochastic Adaptive Gradient Descent Without Descent
arxiv_id: '2509.14969'
source_url: https://arxiv.org/abs/2509.14969
tags:
- stochastic
- convex
- step-size
- gradient
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a stochastic adaptive gradient descent method
  without parameter tuning, based on a Lyapunov function approach. The key idea is
  to use step-sizes that adapt to local geometry using first-order stochastic oracles,
  without requiring knowledge of Lipschitz constants or other problem parameters.
---

# Stochastic Adaptive Gradient Descent Without Descent

## Quick Facts
- **arXiv ID:** 2509.14969
- **Source URL:** https://arxiv.org/abs/2509.14969
- **Reference count:** 40
- **Primary result:** Stochastic adaptive gradient descent method without parameter tuning achieves O(1/k^(1/2+δ)) convergence rates using local geometry estimation

## Executive Summary
This paper introduces a stochastic adaptive gradient descent method that eliminates the need for hyperparameter tuning through a Lyapunov function approach. The method uses step-sizes that adapt to local geometry using first-order stochastic oracles, requiring no knowledge of Lipschitz constants or other problem parameters. The algorithm maintains convergence under various assumptions including strong convexity, finite-sum structure, and bounded iterates, with theoretical analysis showing O(1/k^(1/2+δ)) convergence rates in expectation. Numerical experiments demonstrate significantly reduced sensitivity to initial step-size compared to tuned baselines across linear, logistic, and Poisson regression problems.

## Method Summary
The method introduces AdaSGD, a stochastic adaptive gradient descent algorithm that requires two gradient evaluations per iteration but eliminates hyperparameter tuning. The algorithm uses a three-part adaptive step-size formula combining curvature estimates, growth constraints, and a selector mechanism. The key innovation is computing gradients on both the current and previous mini-batches at the current point to estimate local geometry without requiring problem-specific parameters. The method achieves convergence rates of O(1/k^(1/2+δ)) in expectation while maintaining robustness to initial step-size choices, particularly for small values like 10^-3.

## Key Results
- Achieves O(1/k^(1/2+δ)) convergence rates in expectation under appropriate step-size choices
- Demonstrates significantly reduced sensitivity to initial step-size compared to tuned baselines
- Maintains competitive performance against optimally-tuned baselines across linear, logistic, and Poisson regression problems
- Eliminates need for extensive hyperparameter tuning while requiring two gradient evaluations per iteration

## Why This Works (Mechanism)
The method works by constructing a stochastic Lyapunov function T_k that decreases or stays stable in expectation, compensating for the lack of guaranteed decrease in the objective function f(x_k) at each step. The adaptive step-size uses curvature estimates based on gradient differences between current and previous mini-batches, allowing the algorithm to adapt to local geometry without requiring global Lipschitz constants. The Robbins-Siegmund theorem provides the conditions for almost-sure convergence by handling the noise term in the Lyapunov analysis. The algorithm's robustness to step-size choice comes from the adaptive estimation of local Lipschitz constants, which prevents step-size collapse or explosion that typically occurs with fixed or decay-based step-sizes.

## Foundational Learning

**Concept: Lyapunov Function**
- **Why needed here:** This is the primary theoretical tool for proving convergence. The paper's entire derivation centers on constructing a stochastic Lyapunov function T_k that decreases or stays stable in expectation, compensating for the lack of a guaranteed decrease in the objective function f(x_k) at each step.
- **Quick check question:** Given an update x_{k+1} = x_k - λ_k g_k, can you explain why monitoring ||x_{k+1} - x^⋆||^2 alone is insufficient to prove convergence in stochastic optimization, necessitating a more complex Lyapunov function?

**Concept: Robbins-Siegmund Theorem**
- **Why needed here:** This theorem is the engine for the paper's almost-sure convergence proofs. It provides the conditions under which a non-negative almost-supermartingale converges, allowing the authors to handle the noise term in their Lyapunov analysis.
- **Quick check question:** The theorem requires the sum of noise-related terms to be finite. How does the square-summability of step-sizes (∑ λ_k^2 < ∞) directly satisfy this condition for the variance term?

**Concept: Strong vs. Weak Convexity in Stochastic Settings**
- **Why needed here:** The paper provides different convergence guarantees based on the problem structure. Knowing which case applies to your problem (e.g., linear regression vs. strongly convex) determines which variant and theoretical assurances are relevant.
- **Quick check question:** Case-1 (strong convexity) guarantees a convergence rate. Case-2 (linear regression) does not assume strong convexity but still ensures square-summability of step-sizes. What geometric property of linear regression compensates for the lack of strong convexity in this analysis?

## Architecture Onboarding

### Component Map
Stochastic Gradient Oracle -> Adaptive Step-Size Module -> Parameter Update -> Next Iteration

### Critical Path
The most critical and novel path is the computation of the curvature estimate. The method requires evaluating ∇f_{ξ_{k-1}}(x_k). This is an extra gradient evaluation on a stale mini-batch. If this is implemented incorrectly (e.g., by using ξ_k instead of ξ_{k-1}), the method's parameter-free property is lost, and it becomes unstable, effectively reverting to a method requiring hyperparameter tuning.

### Design Tradeoffs
- **Extra Gradient Evaluation vs. Tuning:** The method requires two gradient evaluations per iteration, doubling the per-iteration compute cost. The tradeoff is that it eliminates hyperparameter search. The total compute must be compared against running an entire grid search for a standard SGD method.
- **Variant Choice:** Variant-III is recommended for its strong theoretical guarantees (O(1/k^(1/2+δ)) convergence), but its enforced decay might be slower on some problems. Variant-I has weaker theory but may perform better empirically on non-Lipschitz problems (e.g., Poisson regression), as it is less constrained.

### Failure Signatures
- **Step-Size Collapse:** λ_k becomes extremely small early in training. This may occur if the local Lipschitz estimate L̂_{k-1} becomes very large, likely due to noise. The min with the growth term is the designed safeguard.
- **Failure to Converge:** The loss f(x_k) plateaus at a high value. According to the paper, this would suggest that the noise term in the Lyapunov analysis is not being controlled. This is the failure mode Variant-III was specifically designed to prevent via its decay term.

### First 3 Experiments
1. **Initial Step-Size Sensitivity Check:** Run AdaSGD (Variant-III) and standard SGD on a logistic regression task. Vary the initial λ_0 for both algorithms over a wide log-scale grid (10^-4 to 10^2). Plot final loss vs. λ_0. Hypothesis: AdaSGD should achieve near-optimal loss for all small λ_0, while SGD performance will vary wildly across the grid.
2. **Finite-Sum Problem Comparison:** On a linear regression task, compare the convergence speed (measured in epochs) of AdaSGD (Variant-III) against a tuned SGD with decay. Hypothesis: AdaSGD will converge at a rate empirically close to O(1/k^(1/2+δ)) without any tuning, matching or slightly trailing the best-tuned SGD.
3. **Non-Lipschitz Stress Test:** Apply AdaSGD to a Poisson regression problem where gradients are not globally Lipschitz. Hypothesis: The method should remain stable. This experiment validates the core claim that the algorithm adapts to local geometry, not global constants. Compare with other parameter-free methods like AdaSGD-MM from prior work.

## Open Questions the Paper Calls Out

**Open Question 1**
- **Question:** Can the convergence guarantees for AdaSGD be extended to the general convex setting without relying on strong convexity or global Lipschitz continuity?
- **Basis in paper:** [Explicit] The conclusion states that "main directions for further research revolve around relaxing the requirements related to convexity," noting that Appendix C offers only partial results.
- **Why unresolved:** Theoretical analysis in Section 3 relies on Assumption 2 (Lipschitz gradients) and Case-1 (strong convexity) to control the variance term in the Lyapunov function.
- **What evidence would resolve it:** A proof of convergence or explicit rates for AdaSGD on general convex functions without strong convexity or bounded iterates assumptions.

**Open Question 2**
- **Question:** Can the AdaSGD method be adapted to non-convex optimization problems, such as training neural networks?
- **Basis in paper:** [Explicit] The conclusion identifies adapting AdaSGD to non-convex functions as a "significant challenge" because the derivation relies heavily on the convexity inequality.
- **Why unresolved:** The current Lyapunov analysis and descent proofs fundamentally require the convexity of f and f_ξ to bound the error terms.
- **What evidence would resolve it:** A modified Lyapunov function or a new convergence proof valid for non-convex objectives, or empirical studies showing robustness without tuning in non-convex settings.

**Open Question 3**
- **Question:** Is it possible to reduce the computational cost of AdaSGD to a single gradient evaluation per iteration while maintaining its parameter-free guarantees?
- **Basis in paper:** [Inferred] The method requires two gradient evaluations per iteration (∇f_{ξ_k}(x_k) and ∇f_{ξ_{k-1}}(x_k)), which the authors note is a drawback balanced by avoiding hyperparameter tuning.
- **Why unresolved:** The adaptive step-size strategy uses the difference between the current and previous stochastic gradients to estimate local geometry without tuning; removing one evaluation breaks this specific estimation mechanism.
- **What evidence would resolve it:** A variant of the algorithm that estimates local geometry using only one gradient per step while provably maintaining the O(1/k^(1/2+δ)) convergence rate.

## Limitations
- Requires two gradient evaluations per iteration, doubling computational cost compared to standard SGD
- Convergence guarantees rely on bounded iterates and finite-sum structure assumptions
- Method's sensitivity to noise in gradient estimates could impact performance in high-noise regimes

## Confidence
- **High Confidence:** Theoretical convergence rates under stated assumptions (strong convexity, finite-sum structure). The Lyapunov function approach and Robbins-Siegmund theorem application are standard tools with well-established validity.
- **Medium Confidence:** Empirical performance claims. While experiments show robustness to step-size choice, the comparison is limited to synthetic datasets and specific regression tasks. Broader validation on real-world, non-convex problems would strengthen these claims.
- **Low Confidence:** Performance in non-Lipschitz settings beyond the Poisson regression case study. The paper claims adaptability to local geometry, but systematic evaluation across diverse non-Lipschitz problems is needed.

## Next Checks
1. **Ablation Study on Gradient Oracle:** Systematically evaluate the impact of the second gradient evaluation (∇f_{ξ_{k-1}}(x_k)) by comparing full AdaSGD against a variant using only current batch gradients. Quantify the trade-off between parameter-free operation and computational overhead.
2. **Real-World Dataset Performance:** Test AdaSGD on standard machine learning benchmarks (e.g., CIFAR-10 for image classification, GLUE for NLP) to assess generalization beyond synthetic regression tasks.
3. **Non-Convex Optimization Stress Test:** Evaluate AdaSGD on non-convex optimization problems (e.g., deep neural network training) to understand its behavior outside the convex regime and validate claims about local geometry adaptation.