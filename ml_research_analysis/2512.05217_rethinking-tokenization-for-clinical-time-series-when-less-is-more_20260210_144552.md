---
ver: rpa2
title: 'Rethinking Tokenization for Clinical Time Series: When Less is More'
arxiv_id: '2512.05217'
source_url: https://arxiv.org/abs/2512.05217
tags:
- time
- clinical
- tokenization
- tasks
- frozen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates different tokenization strategies for clinical
  time series modeling using transformer architectures on MIMIC-IV data. The authors
  systematically test Triplet and TextCode approaches across four clinical prediction
  tasks, finding that explicit time encodings provide no statistically significant
  benefit, while value features are task-dependent (important for mortality prediction
  but not readmission).
---

# Rethinking Tokenization for Clinical Time Series: When Less is More

## Quick Facts
- arXiv ID: 2512.05217
- Source URL: https://arxiv.org/abs/2512.05217
- Reference count: 14
- This paper evaluates different tokenization strategies for clinical time series modeling using transformer architectures on MIMIC-IV data.

## Executive Summary
This paper systematically evaluates tokenization approaches for clinical time series prediction using transformer architectures on MIMIC-IV data. The authors test Triplet and TextCode methods across four clinical prediction tasks, finding that frozen pretrained code encoders dramatically outperform trainable ones while requiring far fewer parameters. Explicit time encodings provide no statistically significant benefit, while value features are task-dependent (important for mortality prediction but not readmission). The study reveals that simpler, parameter-efficient approaches can achieve strong performance, though optimal tokenization remains task-dependent.

## Method Summary
The study compares two tokenization paradigms for clinical time series: Triplet (token_dim=128, batch_size=64, max_epochs=10, precision=32, 6 workers) and TextCode (using frozen vs trainable LM encoders on code descriptions). Triplet uses learned embeddings plus CVE for time and value; TextCode replaces code embeddings with LM encoders. The authors test three encoders: Tiny-ClinicalBERT (15M, 768-dim), BioClinical-ModernBERT-Large (396M, 1024-dim), and Qwen3-Embedding-0.6B (596M). Frozen models use precomputed embedding cache with learned projection to 128-dim. Evaluation uses AUROC across 4 tasks from MEDS-ACES with 10 random seeds and paired Wilcoxon tests.

## Key Results
- Frozen pretrained code encoders outperform trainable ones by 8-10 AUROC points while using 15× fewer parameters
- Explicit time encodings provide no statistically significant benefit across all evaluated clinical tasks
- Value features are task-dependent: important for mortality prediction but not readmission prediction
- Larger clinical encoders provide consistent improvements; general-domain models may match smaller clinical models

## Why This Works (Mechanism)

### Mechanism 1: Frozen Pretrained Encoders as Regularized Feature Extractors
- Claim: Freezing pretrained language model encoders yields better performance than fine-tuning while reducing trainable parameters by ~15×.
- Mechanism: Frozen encoders preserve pretrained semantic knowledge without gradient updates, acting as fixed feature extractors. Only a lightweight projection layer (~1M parameters) is trained, preventing overfitting to limited clinical data and avoiding catastrophic forgetting of pretrained representations.
- Core assumption: The pretrained encoder has already learned clinically relevant semantic relationships from its pretraining corpus.
- Evidence anchors:
  - [abstract] "frozen pretrained code encoders dramatically outperform their trainable counterparts while requiring dramatically fewer parameters"
  - [Table 1] Trainable Tiny-ClinicalBERT: 71.1±0.3 vs Frozen: 81.1±1.3 on post-discharge mortality
  - [corpus] Related work on tokenization-optimized approaches (TOKON, arXiv:2502.05701) suggests tokenization design matters, but this paper specifically shows freezing is the dominant factor
- Break condition: If the downstream task requires learning novel code semantics not present in pretraining, frozen encoders may underperform. Very small datasets might also fail to train projection layers adequately.

### Mechanism 2: Temporal Encoding Redundancy in Sequence Models
- Claim: Explicit time encodings provide no statistically significant benefit for transformer-based clinical prediction tasks.
- Mechanism: Transformer positional embeddings implicitly capture event ordering. For the prediction tasks evaluated (mortality, readmission), the relative sequence of clinical events may encode sufficient temporal signal without explicit time-delta features. Adding explicit time embeddings may introduce noise when not paired with value information.
- Core assumption: The clinical prediction signal is primarily carried in event co-occurrence patterns rather than precise timing intervals.
- Evidence anchors:
  - [abstract] "explicit time encodings provide no consistent statistically significant benefit for the evaluated downstream tasks"
  - [Section 3.1] "Removing time features shows no statistically significant effect across all clinical tasks tested"
  - [corpus] Related work (Renaissance of RNNs, arXiv:2510.16677) shows RNNs remain competitive on streaming clinical time series, suggesting temporal modeling approaches may be over-engineered for certain tasks
- Break condition: Tasks requiring precise timing (e.g., drug dosing schedules, temporal pattern detection in arrhythmias) may still benefit from explicit time encodings. Different architectures (RNNs, state-space models) may also show different sensitivity.

### Mechanism 3: Task-Dependent Value Feature Importance
- Claim: Value features (numeric measurements) contribute to mortality prediction but not readmission prediction.
- Mechanism: Mortality prediction likely depends on specific lab values, vital signs, and their magnitudes (e.g., severely elevated lactate). Readmission may be more strongly predicted by the pattern of care events (codes) rather than specific measurement values, as readmission risk factors include comorbidities and care utilization patterns encoded in code sequences.
- Core assumption: The predictive signal for readmission is captured in the presence/absence and sequence of clinical events rather than their measured values.
- Evidence anchors:
  - [abstract] "Value features show task-dependent importance, affecting mortality prediction but not readmission, suggesting code sequences alone can carry sufficient predictive signal"
  - [Table 2] Code-only vs No-value for in-hospital mortality: 80.2±0.8 vs 79.5±0.7, showing code-only outperforms when value is missing
  - [corpus] Weak direct corpus support for this specific finding; related work on multimodal clinical prediction (arXiv:2509.13696) discusses text+time-series integration but doesn't address task-specific ablations
- Break condition: Tasks where specific thresholds matter (e.g., glucose >200, creatinine doubling) will require value features. Clinical domains with high measurement variability may also show different patterns.

## Foundational Learning

- Concept: **Continuous Value Embedding (CVE)**
  - Why needed here: The paper uses CVE to encode scalar time deltas and numeric values into dense vectors. Understanding this mechanism is essential for implementing the Triplet tokenization approach.
  - Quick check question: Given a time delta of 4.5 hours, what would the CVE transform it into, and how does this differ from simply normalizing the value?

- Concept: **Transformer Positional Encodings vs Explicit Time Encodings**
  - Why needed here: The paper's finding that explicit time encodings don't help relies on understanding what information positional encodings already provide. The distinction between sequence position and actual time intervals is critical.
  - Quick check question: If a patient has events at t=0, t=2, t=24 hours, what does a standard positional encoding capture that a time-delta encoding does not, and vice versa?

- Concept: **Pretrained Encoder Freezing Strategies**
  - Why needed here: The paper's strongest finding is that frozen encoders outperform trainable ones. Understanding why (regularization, catastrophic forgetting avoidance, computational efficiency) enables proper implementation.
  - Quick check question: When freezing a pretrained encoder, which components still require gradient computation during backpropagation, and how does precomputed caching work?

## Architecture Onboarding

- Component map:
  - Code Embedding Layer: Either learned embedding matrix (Triplet) or pretrained LM encoder (TextCode) → projection to token_dim=128
  - Time Encoder: CVE for continuous time deltas (can be ablated)
  - Value Encoder: CVE for numeric measurements (can be ablated)
  - Token Aggregation: Sum of component embeddings (e_triplet = W_code + CVE_time + CVE_value)
  - Transformer Encoder: Standard transformer backbone (architecture not varied in this study)
  - Task Head: Binary classification output for each clinical task

- Critical path:
  1. Start with code-only baseline (remove time and value)
  2. Add frozen pretrained encoder if using TextCode approach
  3. Precompute and cache embeddings for entire vocabulary (~1.2K codes)
  4. Train only projection layer + transformer + task head
  5. Add value features only for mortality tasks if improvement needed

- Design tradeoffs:
  - Triplet vs TextCode: Triplet is simpler and doesn't require descriptions; TextCode leverages semantic knowledge but needs mapping coverage
  - Frozen vs Trainable: Frozen = 15× fewer parameters, better performance, less flexibility; Trainable = potential adaptation to domain-specific semantics
  - Encoder scale: Larger encoders (396M) improve performance but increase inference memory; ~600M general-domain models may match smaller clinical models

- Failure signatures:
  - Trainable LM encoder instead of frozen leads to ~10 AUROC point drop and 15× more parameters
  - Sparse code-to-description mappings (25% vs 100%) may underestimate TextCode performance
  - Missing time/value ablation showing no significant impact for readmission
  - Time2Vec/LeTE show no improvement over linear → temporal complexity unnecessary

- First 3 experiments:
  1. **Baseline ablation**: Train Triplet with full components, then ablate time, value, and both. Use N=10 seeds, report mean±std AUROC, apply paired Wilcoxon with Bonferroni correction. Expected: time ablation shows no significant change; value ablation hurts mortality tasks.
  2. **Frozen vs trainable comparison**: Take TextCode with enhanced mapping, compare Tiny-ClinicalBERT frozen vs trainable. Expected: frozen outperforms by 8-10 AUROC points with 15× parameter reduction.
  3. **Encoder scale test**: Compare frozen Tiny-ClinicalBERT (15M) vs BioClinical-ModernBERT-Large (396M) vs Qwen3-Embedding (596M). Expected: clinical encoder scale helps; general-domain competitive but not superior at comparable size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the findings regarding the limited impact of explicit time encodings generalize to non-transformer architectures such as Recurrent Neural Networks (RNNs) or state-space models?
- Basis in paper: [explicit] The Conclusion states that future work should explore "model architectures (e.g., RNNs, state-space models)" to see if the observed patterns hold.
- Why unresolved: This study exclusively evaluated transformer-based architectures which inherently capture sequence order via positional embeddings, potentially rendering explicit time features redundant in a way that might not apply to other architectures.
- What evidence would resolve it: Replicating the Triplet component ablations (removing time features) on the same clinical tasks using RNNs (e.g., LSTM, GRU) and modern state-space models (e.g., Mamba) to observe if performance degrades significantly without temporal signals.

### Open Question 2
- Question: How does the Everything In Code (EIC) tokenization paradigm compare to Triplet and TextCode when evaluated under the controlled, parameter-efficient settings established in this study?
- Basis in paper: [explicit] The authors identify EIC as the third major paradigm in MEDS-Torch but exclude it from the comparison, noting it requires "fundamentally different experimental designs beyond our current scope."
- Why unresolved: EIC discretizes all modalities into categorical tokens rather than using continuous encoders, making it structurally distinct; the authors controlled for parameters in continuous approaches but did not adapt the methodology for discrete tokenization.
- What evidence would resolve it: Adapting the experimental design to fairly benchmark EIC against the Triplet and frozen TextCode baselines, potentially by controlling for vocabulary size or embedding dimension, and reporting AUROC across the four clinical tasks.

### Open Question 3
- Question: Do the tokenization performance patterns hold across multiple healthcare institutions and electronic health record (EHR) systems?
- Basis in paper: [explicit] The Conclusion explicitly lists "across multiple institutions" as a necessary direction for future work to validate the findings derived solely from the MIMIC-IV dataset.
- Why unresolved: MIMIC-IV represents a single-center dataset (Beth Israel Deaconess Medical Center); coding practices, data density, and the semantic quality of code descriptions may vary significantly in other health systems, potentially affecting the relative performance of frozen text encoders.
- What evidence would resolve it: Training and evaluating the Triplet and TextCode (Frozen) models on external datasets such as eICU or institutional private datasets, comparing the performance delta between the methods to the MIMIC-IV results.

### Open Question 4
- Question: Are there specific clinical prediction tasks where explicit time encodings provide a statistically significant benefit, contrary to the findings in this study?
- Basis in paper: [inferred] The paper concludes that "optimal tokenization remains task-dependent," and while time features showed no benefit in the four evaluated tasks, the authors did not claim time is universally irrelevant, suggesting unexplored tasks may exist.
- Why unresolved: The four selected tasks (mortality, readmission) may rely heavily on event presence rather than precise timing; tasks requiring the detection of rapid deterioration or specific temporal sequences (e.g., sepsis onset or ventricular weaning) might require explicit temporal signals.
- What evidence would resolve it: Applying the Triplet ablation methodology (Full vs. No Time) to time-sensitive prediction tasks defined by clinical guidelines (e.g., early warning scores) and testing for statistical significance.

## Limitations

- Architecture Specification: The transformer backbone architecture (number of layers, attention heads, hidden dimensions) beyond the token_dim=128 setting is not fully specified, which could affect performance comparisons.
- Optimizer and Training Configuration: Key hyperparameters including optimizer type, learning rate, weight decay, learning rate schedule, and gradient clipping are not provided.
- Data Processing and Splits: The exact cohort extraction criteria, patient filtering rules per task, and train/validation/test split ratios are not specified.
- CVE Implementation: The Continuous Value Embedding implementation for time and value encoding is mentioned but not detailed.

## Confidence

- **High Confidence**: Frozen pretrained encoders outperform trainable ones while using 15× fewer parameters; explicit time encodings provide no statistically significant benefit; value feature importance is task-dependent.
- **Medium Confidence**: Larger clinical encoders provide consistent improvements; code sequences alone can carry sufficient predictive signal for readmission.
- **Low Confidence**: General-domain language models can match smaller clinical models; the specific superiority of frozen encoders is due to regularization rather than other factors.

## Next Checks

1. **Architecture Ablation Test**: Reproduce the baseline Triplet model with varying transformer depths (1, 3, 6, 12 layers) and attention heads (4, 8, 12) while keeping all other components constant.

2. **Optimizer Sensitivity Analysis**: Train the frozen Tiny-ClinicalBERT TextCode model with three different optimizer configurations: (a) Adam with standard parameters, (b) AdamW with weight decay 0.01, (c) SGD with momentum 0.9.

3. **Time Encoding Sensitivity Test**: Systematically vary the time encoding approach by replacing CVE with three alternatives: (a) Time2Vec (sinusoidal time encoding), (b) LeTE (learnable time encoding), (c) No time encoding.