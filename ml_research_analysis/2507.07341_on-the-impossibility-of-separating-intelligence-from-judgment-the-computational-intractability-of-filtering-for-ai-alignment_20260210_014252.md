---
ver: rpa2
title: 'On the Impossibility of Separating Intelligence from Judgment: The Computational
  Intractability of Filtering for AI Alignment'
arxiv_id: '2507.07341'
source_url: https://arxiv.org/abs/2507.07341
tags:
- prompt
- harmful
- filter
- theorem
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies the computational intractability of filtering
  for AI alignment, focusing on two key intervention points: filtering prompts before
  they reach a model and filtering outputs after generation. The main results demonstrate
  that for both approaches, there exist fundamental computational barriers to preventing
  harmful content generation.'
---

# On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment

## Quick Facts
- **arXiv ID**: 2507.07341
- **Source URL**: https://arxiv.org/abs/2507.07341
- **Reference count**: 40
- **Primary result**: Demonstrates fundamental computational barriers to filtering for AI alignment through both prompt and output filtering approaches

## Executive Summary
This paper establishes theoretical computational barriers to filtering-based approaches for AI alignment. The authors prove that both prompt filtering (detecting adversarial prompts before they reach a model) and output filtering (detecting harmful outputs after generation) face fundamental intractability barriers. These barriers persist even under relaxed scenarios where filters can modify rather than reject content. The results suggest that achieving provably aligned AI systems through external filtering alone may be computationally infeasible, raising important implications for AI safety design and regulatory frameworks.

## Method Summary
The paper employs complexity-theoretic reductions to establish computational hardness results for filtering problems in AI alignment. The authors construct reductions from cryptographic time-lock puzzles to show that efficient prompt filters cannot distinguish adversarial prompts from benign ones, and that efficient output filters cannot reliably detect harmful outputs even when prompts are deemed safe. The analysis examines both binary filtering (accept/reject) and more expressive filtering schemes that allow content modification, demonstrating that computational barriers persist across these different approaches.

## Key Results
- There exist language models for which no efficient prompt filter can distinguish adversarial prompts from benign ones, assuming time-lock puzzles exist
- Output filtering is computationally intractable even when prompts are considered safe, under the same cryptographic assumptions
- Relaxed filtering approaches that allow prompt/output modification encounter similar fundamental barriers, with connections to watermarking schemes

## Why This Works (Mechanism)
The paper's theoretical framework establishes computational intractability through reductions from cryptographic time-lock puzzles. By constructing adversarial prompts that are computationally indistinguishable from benign prompts for any efficient filter, the authors demonstrate that filtering problems inherit the hardness of underlying cryptographic primitives. This mechanism shows that the computational barriers to alignment are not merely technical challenges but reflect fundamental limits in separating intelligence (model capabilities) from judgment (safety filtering).

## Foundational Learning

1. **Time-lock puzzles** - Cryptographic constructions that require sequential computation to solve
   - *Why needed*: Provide the hardness assumption underpinning the computational barriers to filtering
   - *Quick check*: Verify that puzzle solutions cannot be parallelized or shortcut through non-sequential computation

2. **Complexity-theoretic reductions** - Techniques for showing that one problem is at least as hard as another
   - *Why needed*: Establish the computational hardness of filtering problems by relating them to known hard problems
   - *Quick check*: Confirm that the reduction preserves solution existence and maintains computational equivalence

3. **Adversarial prompt engineering** - Crafting inputs designed to elicit harmful model behavior
   - *Why needed*: Represents the attack surface that filtering mechanisms must defend against
   - *Quick check*: Test whether constructed adversarial prompts actually generate harmful outputs in practice

## Architecture Onboarding

**Component Map**: Time-lock puzzles -> Filtering problems -> AI alignment barriers

**Critical Path**: Cryptographic hardness assumption → Reduction construction → Computational barrier proof → Practical implications

**Design Tradeoffs**: Binary filtering vs. expressive modification approaches; theoretical worst-case vs. empirical average-case performance; security vs. usability in practical deployment

**Failure Signatures**: Inability to distinguish computationally equivalent prompts; false negatives in harmful output detection; brittleness under sophisticated adversarial attacks

**First Experiments**:
1. Implement empirical tests comparing theoretical filter performance against state-of-the-art adversarial detection systems
2. Measure the practical gap between worst-case theoretical barriers and real-world filtering performance
3. Test filter robustness when extended to multi-modal and cross-modal scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Results rely on unproven assumptions about cryptographic primitives (time-lock puzzles)
- Focus on worst-case scenarios rather than average-case behavior may overestimate practical difficulties
- Does not account for hybrid approaches combining multiple filtering strategies
- Binary classification framework may oversimplify nuanced content safety evaluation

## Confidence
**High confidence**: Theoretical foundations and mathematical proofs establishing computational barriers under stated assumptions

**Medium confidence**: Practical implications for real-world AI safety systems given gap between theory and empirical performance

**Low confidence**: Generalizability to all possible filtering architectures or alternative approaches not explicitly considered

## Next Checks
1. Implement empirical benchmarks comparing theoretical filters against state-of-the-art adversarial prompt detection systems on real-world datasets to quantify practical gaps

2. Conduct experiments testing whether current language models exhibit measurable computational patterns exploitable by more sophisticated filtering approaches

3. Evaluate robustness of computational hardness results when extended to multi-modal models and cross-modal filtering scenarios reflecting real deployment conditions