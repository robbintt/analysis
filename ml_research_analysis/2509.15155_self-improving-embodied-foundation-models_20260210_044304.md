---
ver: rpa2
title: Self-Improving Embodied Foundation Models
arxiv_id: '2509.15155'
source_url: https://arxiv.org/abs/2509.15155
tags:
- stage
- self-improvement
- learning
- arxiv
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a two-stage post-training framework for Embodied
  Foundation Models (EFMs) that combines supervised fine-tuning with online reinforcement
  learning. The method uses steps-to-go predictions to create data-driven rewards
  and success detectors, enabling robots to autonomously improve policies without
  manual reward engineering.
---

# Self-Improving Embodied Foundation Models

## Quick Facts
- **arXiv ID:** 2509.15155
- **Source URL:** https://arxiv.org/abs/2509.15155
- **Reference count:** 27
- **Primary result:** Two-stage post-training framework combining supervised fine-tuning with online reinforcement learning achieves up to 1.5x performance gains over behavioral cloning with only 2% additional training episodes.

## Executive Summary
This work introduces a two-stage post-training framework for Embodied Foundation Models (EFMs) that combines supervised fine-tuning with online reinforcement learning. The method uses steps-to-go predictions to create data-driven rewards and success detectors, enabling robots to autonomously improve policies without manual reward engineering. Experiments on real and simulated LanguageTable and Aloha robot embodiments show that this approach significantly outperforms behavioral cloning alone, achieving up to 1.5x performance gains with only 2% additional training episodes. Critically, the combination of web-scale pretraining and online self-improvement enables robots to acquire novel skills that generalize beyond the original imitation datasets.

## Method Summary
The method implements a two-stage post-training pipeline for Embodied Foundation Models. Stage 1 performs supervised fine-tuning (SFT) on imitation data using behavioral cloning for action prediction and steps-to-go prediction for temporal distance to goal. Stage 2 applies online reinforcement learning where a frozen Stage 1 checkpoint computes rewards (reduction in predicted steps-to-go) and success detection, while a separate policy copy is updated via REINFORCE. The reward signal guides the policy toward states where the dataset policy has high value, while regularization keeps exploration in familiar state regions. The approach uses PaLI-3B as the foundation model and operates on tokenized action and steps-to-go outputs.

## Key Results
- Self-improvement framework achieves up to 1.5x performance gains over behavioral cloning alone
- Only 2% additional training episodes needed for significant improvement
- Pretrained models successfully acquire novel skills (e.g., manipulating bananas) never seen in training data
- Single human operator can supervise multiple robots during autonomous practice
- Performance gains are robust across random seeds and experimental settings

## Why This Works (Mechanism)

### Mechanism 1: Steps-to-Go as Implicit Value-Based Reward Shaping
Predicting temporal distance to goal provides dense, well-shaped rewards without manual engineering. The reward is defined as the reduction in steps-to-go: $r(o_t, a_t, o_{t+1}, g) = d(o_t, g) - d(o_{t+1}, g)$. This approximates a potential-based reward guiding the policy toward states where the imitation policy $\mu$ has high value, while regularizing the policy to remain in "familiar" state regions. Break condition: If the steps-to-go predictor fails on out-of-distribution states, the reward signal becomes noisy.

### Mechanism 2: Web-Scale Priors for OOD Generalization
Web-scale pretraining enables the steps-to-go estimator to remain robust during exploration, allowing acquisition of novel skills outside the imitation dataset. Visual-semantic priors in the pre-trained VLM (PaLI) allow progress estimation on novel tasks by leveraging generic physical reasoning from web data. Break condition: If the target task requires physical dynamics completely alien to web-data distribution, zero-shot generalization may fail.

### Mechanism 3: Asymmetric Checkpoint Freezing
Decoupling the policy model from the reward/success detector model stabilizes training. The framework uses a frozen Stage 1 checkpoint for computing rewards and success detection, while a separate copy is updated via REINFORCE. This prevents the "moving target" problem where the reward model degenerates as the policy explores. Break condition: If the policy improves significantly beyond the frozen reward model's understanding, the reward signal becomes uninformative or misleading.

## Foundational Learning

- **Goal-Conditioned Behavioral Cloning (GCBC)**
  - Why needed: Stage 1 SFT initializes the policy using GCBC; without this, the policy would act randomly and the steps-to-go estimator would have no valid trajectories to learn from
  - Quick check: Can you construct a dataset of $(o_t, a_t, g)$ tuples where $g$ is a future state, rather than just a single-task label?

- **Monte Carlo REINFORCE (Policy Gradient)**
  - Why needed: Stage 2 "Self-Improvement" relies on REINFORCE; unlike Q-learning, this avoids the "deadly triad" (bootstrapping + off-policy + function approximation) which is notoriously unstable for large foundation models
  - Quick check: Do you understand why REINFORCE uses full trajectory returns $R_t$ rather than Temporal Difference (TD) errors for updates?

- **Vision-Language-Action (VLA) Tokenization**
  - Why needed: The model uses a VLM (PaLI) to predict continuous actions and discrete steps-to-go by mapping them to text tokens; you must understand how to discretize continuous state spaces effectively
  - Quick check: How do you map a 70-dimensional continuous action vector into a sequence of discrete tokens that a language model can process?

## Architecture Onboarding

- **Component map:** Imitation data -> Stage 1 SFT (BC + steps-to-go) -> Fork checkpoint (Reward frozen, Policy trainable) -> Stage 2 RL (REINFORCE with shaped reward) -> Self-improved policy

- **Critical path:**
  1. Prepare imitation dataset with hindsight goal relabeling
  2. Run Stage 1 SFT until validation loss for steps-to-go plateaus
  3. Fork checkpoint: Freeze one (Reward), keep one trainable (Policy)
  4. Deploy Reward model as server; deploy Policy model to robots
  5. Run RL loop: Collect episodes → Label with Reward model → Update Policy model

- **Design tradeoffs:**
  - On-Policy vs. Off-Policy: On-policy REINFORCE chosen for stability, sacrificing sample efficiency
  - Tokenizer Granularity: Coarse bins for steps-to-go (e.g., 50 bins for 0-100 steps) act as regularization
  - Human-in-the-loop: System allows 1:many human-to-robot ratio, requiring infrastructure to handle manual resets without stopping learning loop

- **Failure signatures:**
  - Reward Hacking: Pushing Self-Improvement past performance peak can degrade success rates
  - Latency Bottlenecks: Version 1 infrastructure (remote inference) failed at 10Hz control required for Aloha
  - Cold Start Failure: If Stage 1 success rate is near 0%, the "steps-to-go" estimator may never see successful trajectories

- **First 3 experiments:**
  1. Visualize the Value Landscape: Before running RL, visualize $d(o, g)$ predictions on held-out trajectory. Does value decrease smoothly as robot approaches goal?
  2. Ablate the Pretraining: Train "Scratch" vs. "Pre-trained" model on small dataset (e.g., 10% data) to confirm sample efficiency gap
  3. Checkpoint Selection Test: Verify if best "Validation Loss" checkpoint correlates with best "Success Rate" (warning: these can diverge for complex tasks)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can robots autonomously segment raw interaction logs to identify consistent episode and skill boundaries to enable hierarchical control and long-horizon skill-chaining?
- **Basis:** Section 6 states the "chief obstacle" to utilizing steps-to-go for hierarchical control is lack of scalable annotation, calling for "creative strategies" to recover boundaries from logs
- **Why unresolved:** Manual labeling is prohibitively expensive, and the current framework relies on pre-defined episode structures rather than discovering them from open-ended interaction
- **What evidence would resolve it:** A method that automatically labels sub-skill boundaries in unlabeled robot logs and successfully uses these boundaries to chain multiple distinct skills for a long-horizon task

### Open Question 2
- **Question:** What stopping criteria or adaptive regularizers effectively prevent the policy performance degradation observed when over-optimizing the shaped steps-to-go reward?
- **Basis:** Section 6 notes that "pushing Self-Improvement beyond its performance peak can degrade success rates," suggesting the shaped reward can be exploited to the detriment of actual task success
- **Why unresolved:** Authors identify this degradation but do not determine theoretical causes nor propose mechanism to detect or halt training at optimal point automatically
- **What evidence would resolve it:** Identification of a metric or regularization term that correlates with success peak and halts training before degradation occurs, validated across multiple tasks

### Open Question 3
- **Question:** How can steps-to-go estimators be adapted to assign appropriate rewards to out-of-distribution (OOD) failure states that lack recovery trajectories in the imitation dataset?
- **Basis:** Section 6 highlights a "key challenge" where failure states fall outside dataset support, making it difficult for model to recognize them or assign shaped rewards effectively
- **Why unresolved:** Current model relies on data distribution, so it struggles to evaluate states (like dropped objects) that are underrepresented or absent in successful demonstrations
- **What evidence would resolve it:** A modified reward model that correctly penalizes or triggers recovery policy for novel failure states not present in Stage 1 training data

### Open Question 4
- **Question:** Can off-policy reinforcement learning algorithms be scaled to large foundation models to reduce robot-hour requirements without sacrificing the stability of on-policy methods?
- **Basis:** Section 6 states the choice of on-policy REINFORCE "forgoes the data-reuse benefits of modern off-policy algorithms" and identifies investigating off-policy variants as a path to curbing robot-hour requirements
- **Why unresolved:** Authors prioritized stability by avoiding the "deadly triad" (off-policy learning and bootstrapping), leaving integration of sample-efficient off-policy methods for large EFMs unexplored
- **What evidence would resolve it:** A stable training run using an off-policy algorithm (e.g., an actor-critic method) that achieves comparable or superior performance to proposed method using fewer environment interactions

## Limitations

- The claim that web-scale pretraining enables OOD generalization is supported by banana experiments but not rigorously tested across diverse physical dynamics
- Asymmetric checkpoint freezing assumes Stage 1 achieves sufficient accuracy before freezing, but paper doesn't specify how to verify this threshold or what happens when it's not met
- Performance degradation from over-optimization of shaped rewards is observed but not fully characterized or prevented

## Confidence

- **High confidence**: The two-stage framework structure (SFT + RL), the steps-to-go reward formulation, and the sample efficiency improvements over behavioral cloning alone
- **Medium confidence**: The claim that web-scale pretraining is critical for generalization to novel tasks, based primarily on the banana experiments
- **Medium confidence**: The asymmetric checkpoint freezing provides stable training, though the exact conditions for success are underspecified
- **Low confidence**: The robustness claims across random seeds and settings, as only a limited number of seeds are shown

## Next Checks

1. **Value Landscape Validation**: Before deploying Stage 2, visualize steps-to-go predictions across a validation trajectory set to confirm smooth value decay toward goals and identify potential reward function failures

2. **Pretraining Ablation Across Tasks**: Systematically test "Scratch" vs. "Pre-trained" models across multiple novel tasks beyond bananas to quantify the generalization advantage claimed from web-scale pretraining

3. **Success Threshold Sensitivity**: Vary the success threshold s parameter to determine how sensitive performance is to this hyperparameter and identify optimal settings for different task complexities