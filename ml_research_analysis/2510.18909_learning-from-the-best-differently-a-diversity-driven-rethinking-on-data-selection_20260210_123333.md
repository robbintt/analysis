---
ver: rpa2
title: 'Learning from the Best, Differently: A Diversity-Driven Rethinking on Data
  Selection'
arxiv_id: '2510.18909'
source_url: https://arxiv.org/abs/2510.18909
tags:
- data
- quality
- knowledge
- dimensions
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that existing score-based data selection
  methods suffer from performance degradation when directly selecting top-scored data
  due to neglect of diversity. To address this, the authors propose Orthogonal Diversity-Aware
  Selection (ODiS), which decomposes correlated evaluation dimensions into orthogonal
  principal components via PCA, then selects top-scored data from each dimension.
---

# Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection

## Quick Facts
- arXiv ID: 2510.18909
- Source URL: https://arxiv.org/abs/2510.18909
- Reference count: 40
- Primary result: ODiS achieves average accuracy of 65.97% across five benchmarks, outperforming score-based baselines by reducing dimension correlation-induced redundancy.

## Executive Summary
Existing score-based data selection methods suffer from performance degradation when directly selecting top-scored data due to neglect of diversity. This paper identifies that correlated evaluation dimensions cause naive score aggregation to select redundant data that lacks coverage. To address this, the authors propose Orthogonal Diversity-Aware Selection (ODiS), which decomposes correlated evaluation dimensions into orthogonal principal components via PCA, then selects top-scored data from each dimension. Empirical results show that ODiS-selected data exhibits less than 2% inter-dimension overlap and achieves significantly higher downstream performance compared to baselines across five benchmarks.

## Method Summary
ODiS addresses data selection bias by transforming correlated quality metrics into orthogonal axes through PCA, then selecting top-scored data within each orthogonal dimension. The method trains regression-based scorers to predict PCA-transformed scores, enabling scalable application to large corpora. The approach decorrelates 11 evaluation dimensions (language quality, knowledge quality, comprehension difficulty, information quality) and selects data along each principal component to maximize both quality and diversity in the final training set.

## Key Results
- ODiS-selected data exhibits less than 2% inter-dimension overlap, confirming orthogonality between dimensions
- Achieves average accuracy of 65.97% across five downstream benchmarks (Arc-Easy, Arc-Challenge, Hellaswag, SIQA, PIQA)
- Demonstrates significant performance improvement over score-based baselines by reducing dimension correlation-induced redundancy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Correlated evaluation dimensions cause naive score aggregation to select redundant data that lacks coverage.
- **Mechanism:** When dimensions such as "knowledge depth" and "knowledge richness" exhibit moderate positive correlation (e.g., coefficients ~0.66), a weighted sum or average disproportionately favors documents high on the shared latent factor. Top-scored data along this latent direction concentrates in a narrow region of the semantic space, reducing distributional heterogeneity.
- **Core assumption:** The observed inter-dimensional correlations on the labeled reference set generalize to the target corpus.
- **Evidence anchors:**
  - [abstract]: "score-based methods collapse correlated dimensions, causing top-scored data to appear high-quality while systematically overlooking diversity."
  - [section 3.3.2]: Correlation matrix shows moderate correlations (e.g., knowledge depth vs. knowledge richness = 0.66; completeness vs. knowledge richness = 0.68).
  - [corpus]: Weak direct evidence; no corpus neighbor validates the PCA-decorrelation hypothesis for LLM data selection.
- **Break condition:** If dimensions are already near-orthogonal (correlations <0.2), PCA adds minimal value and may over-rotate noise.

### Mechanism 2
- **Claim:** Principal Component Analysis transforms correlated quality metrics into orthogonal axes along which selection preserves diversity.
- **Mechanism:** PCA computes eigenvectors of the covariance matrix over the 11-dimensional scores. Each eigenvector represents a linear combination of original dimensions that is orthogonal to others. Selecting top-k data along each of the top K PCs (e.g., K=4) yields subsets that occupy distinct semantic regions, as the PCs capture different aspects of the score space.
- **Core assumption:** The top K PCs capture the majority of signal relevant to downstream performance; lower-variance PCs contribute noise or marginal gains.
- **Evidence anchors:**
  - [abstract]: "decorrelated via Principal Component Analysis (PCA), yielding orthogonal evaluation dimensions."
  - [section 2.4]: Describes eigendecomposition and selection of K components based on explained-variance threshold τ.
  - [corpus]: No corpus neighbor directly corroborates the orthogonality mechanism for LLM data selection.
- **Break condition:** If the explained-variance threshold τ requires many components (e.g., >10) due to high dimensionality without strong correlations, orthogonality gains diminish and computational overhead rises.

### Mechanism 3
- **Claim:** Selecting top-scored data within each orthogonal dimension, then unioning the subsets, creates a training set with higher diversity without sacrificing quality.
- **Mechanism:** After projecting scores onto K orthogonal PCs and training per-dimension scorers, ODiS selects top-θ_k documents for each PC. Because PCs are orthogonal by construction, the selected subsets have low overlap (<2% token-level overlap reported). The union D_s = ∪_k D_s^k yields a dataset that maintains high scores on distinct latent features, broadening coverage.
- **Core assumption:** Overlap remains low after scorer training; the Roberta-based scorers faithfully approximate the PCA-projected scores on the target corpus.
- **Evidence anchors:**
  - [abstract]: "ODiS-selected data exhibit less than 2% inter-dimension overlap, confirming orthogonality between dimensions."
  - [section 3.3.3]: UpSet plot and UMAP visualization show separable regions and minimal intersection across PC-selected subsets.
  - [corpus]: No corpus evidence directly validates the <2% overlap claim for orthogonal PC-based selection.
- **Break condition:** If scorer noise or distribution shift causes high prediction variance, overlap may increase, reducing diversity benefits.

## Foundational Learning

- **Concept:** Principal Component Analysis and eigendecomposition
  - **Why needed here:** ODiS relies on PCA to decorrelate multi-dimensional quality scores and construct orthogonal selection axes. Without understanding how eigenvectors span the score space, the rationale for dimension decomposition remains opaque.
  - **Quick check question:** Given a 3×3 covariance matrix, can you explain what the eigenvectors represent and why orthogonality matters for selecting diverse subsets?

- **Concept:** Data selection bias and diversity-quality tradeoffs in LLM pretraining
  - **Why needed here:** The paper identifies that score-based selection introduces bias by collapsing correlated dimensions. Understanding how high-quality but redundant data degrades downstream performance is essential for motivating the ODiS approach.
  - **Quick check question:** If two evaluation dimensions have correlation 0.8, what happens to the distribution of top-scored data when you average their scores versus select along each dimension separately?

- **Concept:** Regression-based scorers for scalable inference
  - **Why needed here:** ODiS trains Roberta-based models to predict PCA-transformed scores, enabling application to large corpora without repeated LLM labeling.
  - **Quick check question:** Why is regression (rather than classification) appropriate for predicting continuous PCA-projected scores, and what loss function would you use?

## Architecture Onboarding

- **Component map:** Multi-dimensional labeling module -> PCA transformation module -> Scorer training module -> Selection and union module

- **Critical path:**
  1. Label reference dataset with 11 quality dimensions (GPT-based, 0-5 scale)
  2. Compute covariance matrix, perform eigendecomposition, determine K via explained-variance threshold
  3. Project reference scores to K PCs; train K scorers
  4. Apply scorers to target corpus; for each PC, select top documents meeting score threshold; union and deduplicate

- **Design tradeoffs:**
  - K (number of PCs): More PCs increase coverage but add scorer training cost and may introduce noisy dimensions. Paper shows diminishing returns beyond K=4.
  - Score threshold vs. budget allocation: Even allocation across PCs is simple but may underweight high-variance PCs. Contribution-weighted allocation is an unexplored alternative.
  - Scorer capacity: Roberta-base is efficient but may not capture complex semantic features; larger encoders could improve accuracy at higher inference cost.

- **Failure signatures:**
  - High inter-dimension overlap (>10%): Suggests scorers are not discriminating well or PCs are not effectively orthogonal due to labeling noise.
  - Downstream performance degrades vs. random sampling: Indicates over-filtering or selection bias; check whether selected data is too narrow in domain or style.
  - PC1 dominates selection: If PC1 accounts for >80% of variance, subsequent PCs may select redundant or low-quality data; consider rebalancing budget allocation.

- **First 3 experiments:**
  1. **Correlation validation:** Compute pairwise correlations among the 11 labeled dimensions on a held-out sample; verify moderate-to-strong correlations exist (otherwise PCA adds little value).
  2. **Scorer accuracy check:** Train Roberta-based scorers and evaluate MSE against ground-truth PCA-projected scores on a validation split; flag dimensions with high error (>0.5 MSE).
  3. **Overlap and diversity probe:** Apply trained scorers to a subset of the target corpus, select top-k per PC, and compute token-level overlap; confirm <5% overlap before scaling to full corpus.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the effectiveness of orthogonal dimension decomposition (ODiS) scale proportionally with model size, or does the diversity-quality trade-off shift for larger models (e.g., >7B parameters)?
- **Basis in paper:** [inferred] The empirical validation was conducted exclusively on a 1.5B-parameter model (Section 3.1), leaving the behavior on state-of-the-art model scales untested.
- **Why unresolved:** Larger models may have greater capacity to handle redundancy and correlated dimensions internally, potentially reducing the marginal benefit of explicit PCA-based decomposition compared to smaller models.
- **What evidence would resolve it:** Training and evaluating models of varying sizes (e.g., 1.5B, 7B, 70B) using the ODiS method and comparing the performance delta against standard baselines.

### Open Question 2
- **Question:** How can the orthogonal principal components derived from ODiS be interpreted or mapped back to human-understandable semantic features?
- **Basis in paper:** [explicit] Section 3.3.2 states that because principal components are linear combinations of original dimensions, they are "difficult to interpret" and should be viewed as abstract features.
- **Why unresolved:** While the method ensures mathematical orthogonality, the semantic meaning of the selected data remains opaque, making it difficult to diagnose *why* specific data improves specific downstream tasks.
- **What evidence would resolve it:** A correlation analysis projecting the top-scoring data for each PC back onto the original 11 evaluation dimensions to identify which human-defined concepts (e.g., reasoning vs. factual accuracy) dominate each abstract component.

### Open Question 3
- **Question:** What is the precise efficiency-performance trade-off when increasing the number of Principal Components ($K$) beyond the point of performance saturation?
- **Basis in paper:** [explicit] Section 3.3.4 notes that while performance saturates after four dimensions, "selecting more PCs will introduce increased computational cost" for labeling.
- **Why unresolved:** The paper uses a fixed budget and $K=4$, but it does not quantify the computational overhead (time/resources) relative to the diminishing returns in accuracy for values of $K > 4$.
- **What evidence would resolve it:** A detailed benchmark reporting the training/inference cost and downstream accuracy for $K=1$ through $K=11$ (the total number of raw dimensions) to visualize the cost-benefit curve.

## Limitations

- **Labeling Dependency:** ODiS relies on costly 11-dimensional GPT labeling for the reference set, assuming label quality and correlation generalization across domains.
- **Scorer Generalization:** The approach assumes trained scorers generalize well to target corpora; distribution shift could degrade PC score predictions and increase overlap.
- **Hyperparameter Sensitivity:** The number of PCs (K=4) and explained-variance threshold are not fully specified, potentially affecting performance in other domains or with different evaluation dimensions.

## Confidence

- **High Confidence:** The core mechanism of PCA-based decorrelation and dimension-wise selection is mathematically sound and well-established.
- **Medium Confidence:** The reported <2% inter-dimension overlap and downstream performance gains are promising but rely on scorer quality and the assumption that PC orthogonality translates to semantic diversity.
- **Low Confidence:** The generalizability of the approach to other domains, languages, or with different evaluation dimensions is not established.

## Next Checks

1. **Scorer Distribution Shift Test:** Apply trained scorers to a held-out sample from the target corpus and compare MSE to reference set performance. Flag dimensions with >0.5 MSE degradation.

2. **Correlation Stability Check:** Compute pairwise correlations among the 11 dimensions on a new, unlabeled sample from the target corpus. Verify that moderate correlations (0.2-0.7) persist, validating the PCA decorrelation assumption.

3. **Overlap Sensitivity Analysis:** Systematically vary the score thresholds per PC and measure the resulting inter-dimension overlap and downstream performance. Determine the threshold range where overlap remains <5% and performance gains are stable.