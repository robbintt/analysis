---
ver: rpa2
title: 'When2Call: When (not) to Call Tools'
arxiv_id: '2504.18851'
source_url: https://arxiv.org/abs/2504.18851
tags:
- tool
- answer
- question
- when2call
- call
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces When2Call, a new benchmark designed to evaluate\
  \ tool-calling decision-making in language models. Unlike existing benchmarks that\
  \ focus on correct tool execution, When2Call assesses whether models choose appropriate\
  \ behaviors\u2014calling tools, asking follow-up questions, admitting inability\
  \ to answer, or hallucinating answers\u2014based on context."
---

# When2Call: When (not) to Call Tools

## Quick Facts
- **arXiv ID:** 2504.18851
- **Source URL:** https://arxiv.org/abs/2504.18851
- **Reference count:** 8
- **Primary result:** State-of-the-art tool-calling models struggle significantly on When2Call, often hallucinating tools or answers when inappropriate.

## Executive Summary
This paper introduces When2Call, a new benchmark designed to evaluate tool-calling decision-making in language models. Unlike existing benchmarks that focus on correct tool execution, When2Call assesses whether models choose appropriate behaviors—calling tools, asking follow-up questions, admitting inability to answer, or hallucinating answers—based on context. The benchmark is built using a multiple-choice format derived from synthetic data generation, ensuring questions require tool use to answer correctly. Evaluation uses both log-probabilities and LLM-as-judge methods. Results show that state-of-the-art tool-calling models struggle significantly on When2Call, often hallucinating tools or answers when inappropriate. To address this, the authors propose a preference optimization training regime that substantially improves performance over standard fine-tuning while maintaining tool-calling accuracy. The benchmark and training data are publicly released to support further research.

## Method Summary
When2Call evaluates tool-calling decision-making through a multiple-choice format with four options: direct answer, tool call, follow-up question, or unable to answer. The benchmark is constructed by first filtering questions requiring tool use from BFCL Live and APIGen datasets, then generating three variants per question: the original with correct tool call, a parameter-dropped version requiring follow-up, and a semantically related unanswerable question. Models are evaluated using log-probabilities over answer choices or LLM-as-judge classification. The authors propose RPO training using preference pairs where the correct behavioral choice is preferred over incorrect alternatives, with a 2:1 blend ratio of tool-calling to non-calling examples and low KL penalty (0.05) to prevent over-conservatism.

## Key Results
- State-of-the-art tool-calling models (Qwen, xLAM) show high tool hallucination rates when no tools are provided
- Models struggle with "unable to answer" decisions, often attempting direct answers instead
- RPO training substantially improves when-not-to-call decisions while maintaining tool-calling accuracy
- Performance does not consistently improve with model size, suggesting training data composition matters more than scale

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting tool-calling decisions to a multiple-choice format enables reproducible, fine-grained behavioral evaluation.
- Mechanism: Instead of parsing generated tool calls or text responses, When2Call presents four explicit behavioral options (direct answer, tool call, follow-up question, unable to answer) and measures model preference via log-probabilities. This bypasses parsing complexity and answer-order artifacts while directly testing decision-making.
- Core assumption: Models' log-probability rankings over answer choices reflect their genuine behavioral preferences in tool-use scenarios.
- Evidence anchors:
  - [abstract]: "Evaluation uses both log-probabilities and LLM-as-judge methods."
  - [Section 2.1]: "Multiple-choice approaches have the major benefit of being reproducible and being fast to evaluate... we intend When2Call to be complementary to BFCL."
  - [corpus]: Weak direct corpus evidence; related work on tool-augmented LLMs focuses on execution accuracy rather than decision framing.
- Break condition: If models develop strong position biases or format-specific heuristics that don't transfer to actual tool-calling behavior, the multiple-choice proxy becomes unreliable.

### Mechanism 2
- Claim: Preference optimization training with balanced positive/negative examples improves when-to-call decisions without degrading tool execution accuracy.
- Mechanism: RPO training constructs preference pairs where the "chosen" response is the correct behavioral choice and "rejected" is an incorrect alternative. The training blend maintains a 2:1 ratio of tool-calling to non-calling examples, with a low KL penalty (0.05) to prevent over-conservatism. This creates competing optimization pressures that the model must balance.
- Core assumption: The preference signal from constructed pairs transfers to improved decision boundaries in novel tool-question scenarios.
- Evidence anchors:
  - [Section 3.3.2]: "We find that a low KL-penalty value (0.05) gives the best results on tool-calling benchmarks with this dataset."
  - [Section 3.4]: "RPO training yields the greatest benefits" while "maintaining competitive scores on the portion of BFCL where a tool should be called."
  - [corpus]: Related work (PORTool, arXiv:2510.26020) explores rewarded tree search for tool-use training, suggesting preference signals can improve tool-use reasoning.
- Break condition: If preference optimization overfits to benchmark-specific patterns rather than learning generalizable decision boundaries, improvements won't transfer to real-world deployments.

### Mechanism 3
- Claim: Synthetic data generation with controlled question-tool mismatches creates harder, more realistic evaluation scenarios than existing benchmarks.
- Mechanism: The pipeline first filters questions requiring tool use (real-time data, database access), then generates two additional variants per question: (1) questions with missing required parameters (forcing follow-up), and (2) semantically related but unanswerable questions (forcing "unable to answer"). This creates subtle mismatches requiring fine-grained reasoning.
- Core assumption: Synthetic question variants maintain sufficient quality and naturalness to approximate real-world tool-use scenarios.
- Evidence anchors:
  - [Section 2.2]: "We ask for a related question in order to generate close and thus more difficult mismatches between the question and provided tool(s)."
  - [Section 2.4]: "When2Call also distinguishes itself from BFCL Irrelevance by how difficult many of the question/tool mismatches are, by design."
  - [corpus]: Benchmarking Failures in Tool-Augmented LLMs (arXiv:2503.14227) similarly studies TaLMs under imperfect information conditions, supporting the importance of non-ideal scenarios.
- Break condition: If synthetic artifacts are easily detected by models (e.g., unnatural phrasing), performance may reflect artifact recognition rather than genuine tool-calling reasoning.

## Foundational Learning

- Concept: Preference Optimization (DPO/RPO)
  - Why needed here: The paper uses RPO as its primary training improvement. Understanding how preference pairs create optimization signals—and why KL penalties matter—is essential to interpret the results and apply the method.
  - Quick check question: Why would a low KL penalty (0.05) be preferred over a higher value when training models on when-not-to-call decisions?

- Concept: Tool-Calling Benchmarks (BFCL paradigm)
  - Why needed here: When2Call positions itself as complementary to BFCL. Understanding what BFCL measures (correct tool + correct parameters) clarifies what When2Call adds (decision-making when tools shouldn't be called).
  - Quick check question: What dimension of tool-use capability does BFCL's "Irrelevance" category evaluate, and how does When2Call extend this?

- Concept: Log-Probability Evaluation
  - Why needed here: The benchmark uses log-probabilities over answer choices rather than generation. This is a specific evaluation methodology with known tradeoffs around normalization and model calibration.
  - Quick check question: Why might log-probability-based multiple-choice evaluation underestimate performance for models trained on When2Call's specific answer phrasings?

## Architecture Onboarding

- Component map:
  BFCL Live / APIGen (source data)
         ↓
  [Step 1] Tool-use classifier (Mixtral 8x22B)
         ↓
  [Step 2] Question variant generator
         ├─→ Unchanged question + correct tool call
         ├─→ Parameter-dropped question + follow-up
         └─→ Related unanswerable question + "unable"
         ↓
  When2Call benchmark (3,652 test samples)
         ↓
  Evaluation layer
  ├─→ Log-probability multi-choice (open models)
  └─→ LLM-as-judge classification (closed models)

- Critical path:
  1. Data generation quality (manual inspection of ~10% samples per prompt iteration)
  2. Model-specific prompt templates (custom preprocessing for each model's tool-calling syntax)
  3. RPO training blend ratio (2:1 tool-calling to non-calling prevents over-conservatism)

- Design tradeoffs:
  - Multiple-choice vs. generation: Multiple-choice enables reproducible, fast evaluation but may not fully capture open-ended response quality
  - Synthetic vs. human-authored questions: Synthetic enables scale and controlled variations but introduces potential artifacts (82-94% estimated quality)
  - SFT vs. RPO: RPO improves when-not-to-call decisions but requires constructing preference pairs; SFT is simpler but risks over-conservatism

- Failure signatures:
  - Over-conservative models: High When2Call accuracy but dropped BFCL AST scores (calls tools too rarely)
  - Tool hallucination: Model selects tool-call option when no tools provided (indicates memorized tool patterns)
  - Unwillingness to refuse: Low F1 on "unable to answer" category (common in Qwen/xLAM models per confusion matrices)
  - Answer phrasing sensitivity: Large gap between log-probability and LLM-as-judge scores (indicates training on specific phrasings)

- First 3 experiments:
  1. **Baseline diagnostic**: Evaluate your model on When2Call using both log-probability and LLM-as-judge methods; compute confusion matrix to identify dominant failure mode (tool hallucination vs. refusal reluctance vs. follow-up confusion).
  2. **RPO vs. SFT comparison**: Train two model variants—one with SFT on When2Call data, one with RPO using preference pairs—on held-out tool-calling data to measure the accuracy/conservatism tradeoff.
  3. **Difficulty ablation**: Subset When2Call by mismatch difficulty (single-tool "unable" vs. multi-tool "unable" vs. parameter-missing "follow-up") to identify which decision types your model struggles with most.

## Open Questions the Paper Calls Out

- **Question:** Why does tool-calling decision-making performance fail to improve consistently with model size (e.g., comparing Qwen 2.5 3B vs. 72B)?
  - **Basis in paper:** [Explicit] Page 5 notes that "performance... does not necessarily improve with model size (e.g., Qwen 2.5 3B/7B/72B)" and explicitly states, "More research is needed to understand this interesting result."
  - **Why unresolved:** The paper speculates this may depend on the specific training data used for different model sizes but provides no ablation or data analysis to confirm this hypothesis.
  - **What evidence would resolve it:** A controlled ablation study analyzing the specific instruction-tuning data compositions and refusal behaviors across different parameter scales of the same model family.

- **Question:** How can benchmarks evaluate "optional" tool use where a task is solvable without tools but benefits from them?
  - **Basis in paper:** [Explicit] Page 9 (Limitations) notes the benchmark relies on a "simplifying assumption" that direct answers are hallucinations, ignoring tasks like math where an LM might use a tool "to improve performance" despite knowing the answer.
  - **Why unresolved:** The current benchmark explicitly filters out questions solvable by parametric knowledge, creating a gap in evaluating the compute-vs-accuracy trade-off.
  - **What evidence would resolve it:** A new evaluation framework that includes tasks solvable without tools and scores models based on a configurable utility function weighing accuracy gains against tool-call costs/latency.

- **Question:** Does effective tool-calling decision-making (specifically refusal and follow-up behaviors) transfer across languages?
  - **Basis in paper:** [Explicit] Page 9 (Limitations) states, "Expanding tool-calling to other languages is certainly an important research direction," acknowledging the current focus is English-only.
  - **Why unresolved:** It is unknown if the inability to admit ignorance or the tendency to hallucinate parameters is exacerbated or mitigated in low-resource languages.
  - **What evidence would resolve it:** Evaluation of multilingual models on a translated version of the When2Call benchmark to measure cross-lingual consistency in decision-making boundaries.

## Limitations
- The benchmark relies on synthetic data generation, which may introduce artifacts that models can exploit rather than demonstrating genuine decision-making
- Performance does not consistently improve with model size, suggesting training data composition may be more important than scale
- The multiple-choice format may not fully capture the complexity of open-ended tool-calling decisions in real-world scenarios

## Confidence

- **High confidence**: The benchmark design (multiple-choice format) provides a novel and reproducible evaluation methodology for tool-calling decision-making that complements existing execution-focused benchmarks.
- **Medium confidence**: Preference optimization training (RPO) significantly improves when-not-to-call decisions while maintaining tool execution accuracy, though this depends on specific hyperparameters like the 0.05 KL penalty.
- **Medium confidence**: Synthetic data generation creates challenging evaluation scenarios with subtle question-tool mismatches, though the naturalness and transferability of these synthetic variants to real-world scenarios remains partially untested.

## Next Checks

1. **Cross-dataset generalization test**: Evaluate When2Call-trained models on real-world tool-calling scenarios from different domains (e.g., medical, legal, financial) to verify that improved decision-making transfers beyond the benchmark.
2. **Human evaluation of synthetic data quality**: Conduct systematic human assessment of the generated question variants to quantify how many contain detectable artifacts that could bias model performance.
3. **Long-context tool-calling evaluation**: Test models on When2Call tasks embedded within longer conversations (500+ tokens) to assess whether improved when-to-call decisions hold in realistic multi-turn dialogue contexts where tool-use history matters.