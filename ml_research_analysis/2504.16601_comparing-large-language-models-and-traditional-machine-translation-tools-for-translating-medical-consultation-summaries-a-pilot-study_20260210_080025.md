---
ver: rpa2
title: 'Comparing Large Language Models and Traditional Machine Translation Tools
  for Translating Medical Consultation Summaries: A Pilot Study'
arxiv_id: '2504.16601'
source_url: https://arxiv.org/abs/2504.16601
tags:
- translation
- medical
- llms
- translations
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compared LLMs and traditional MT tools in translating
  medical consultation summaries into Arabic, Chinese, and Vietnamese. Two types of
  summaries were created: simple patient-facing and complex clinician-focused.'
---

# Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study

## Quick Facts
- **arXiv ID:** 2504.16601
- **Source URL:** https://arxiv.org/abs/2504.16601
- **Reference count:** 20
- **Primary result:** Traditional MT tools outperformed LLMs for medical translation; LLMs showed promise for simple Vietnamese/Chinese translations.

## Executive Summary
This pilot study compared large language models (GPT-4o, LLAMA-3.1, GEMMA-2) with traditional machine translation tools (Google Translate, Microsoft Bing, DeepL) for translating medical consultation summaries into Arabic, Chinese, and Vietnamese. Two summary types were tested: simple patient-facing and complex clinician-focused texts. Traditional MT tools generally outperformed LLMs across all languages and complexity levels, particularly for complex texts. LLMs showed promise for Vietnamese and Chinese simple translations. The study found Arabic translations improved with complexity due to morphological context, while Chinese showed the steepest degradation. Current automated metrics (BLEU, CHR-F, METEOR) inadequately capture clinical translation quality, and human oversight remains essential for medical translations.

## Method Summary
The study used two simulated English medical consultation summaries (simple patient-facing and complex clinician-focused) translated into Arabic, Chinese, and Vietnamese. Three LLMs were tested using a basic prompt: "Can you translate this document into [Language], make sure no information is lost." Three traditional MT tools were evaluated using default web interfaces. Translations were scored against ISO 17100 certified professional references using BLEU, CHR-F, and METEOR metrics. DeepL was excluded for Vietnamese due to lack of support. The study focused on metric evaluation without human clinical validation.

## Key Results
- Traditional MT tools outperformed LLMs across all languages and complexity levels
- LLMs showed promise for Vietnamese and Chinese simple translations
- Arabic translations improved with complexity due to morphological context
- Chinese showed the most performance decline with increased complexity
- Current metrics inadequately capture clinical translation quality

## Why This Works (Mechanism)

### Mechanism 1: Morphological Context Exploitation in Arabic
Arabic translation quality may improve with text complexity when models leverage richer morphological context for disambiguation. Arabic's rich morphology creates ambiguity in short utterances; longer, more context-rich sentences provide additional contextual cues that assist grammatical resolution. Google Translate's BLEU increased from 0.6528 (simple) to 0.6787 (complex), and METEOR rose from 0.3399 to 0.4988.

### Mechanism 2: Metric Alignment Bias Toward Traditional MT
BLEU and CHR-F systematically favor traditional MT tools due to token-level alignment training objectives, potentially penalizing semantically valid LLM paraphrasing. Traditional MT optimizes for n-gram overlap; LLMs generate fluent, coherent outputs that may restructure sentences while preserving meaning. METEOR captures some semantic equivalence, explaining why LLAMA outperformed GPT-4o on Vietnamese/Chinese METEOR scores despite lower BLEU.

### Mechanism 3: Complexity-Sensitivity Varies by Language-Resource Status
Under-resourced languages exhibit different degradation patterns under text complexity due to training data scarcity in specialized domains. Chinese showed the steepest complexity degradation (GEMMA BLEU: 0.5227→0.2466) despite high general-resource status, suggesting medical-domain scarcity. Vietnamese showed resilience (LLAMA BLEU: 0.7517→0.7441), possibly from modern multilingual representations.

## Foundational Learning

- **Concept: BLEU/CHR-F/METEOR metrics**
  - Why needed here: Interpreting comparative results requires understanding that BLEU measures word-level n-gram overlap, CHR-F captures character-level alignment (better for morphology), and METEOR incorporates synonyms/paraphrasing for semantic similarity.
  - Quick check question: If an LLM produces a semantically correct translation with different word order, which metric would likely score it lowest?

- **Concept: Low-resource vs. high-resource languages**
  - Why needed here: The study categorizes Arabic and Vietnamese as relatively under-resourced for medical translation, affecting model performance expectations and explaining why DeepL lacks Vietnamese support.
  - Quick check question: Why might a language with abundant general web text still be "low-resource" for medical translation specifically?

- **Concept: Human-in-the-loop validation**
  - Why needed here: The paper concludes automated metrics are insufficient for clinical safety; domain experts must review translations before patient-facing use.
  - Quick check question: What types of translation errors would automatic metrics miss but could cause clinical harm?

## Architecture Onboarding

- **Component map:** Summary creation -> LLM/MT translation -> Metric evaluation -> Human validation -> Clinical use
- **Critical path:** English medical summaries (simple/complex) → translation systems → BLEU/CHR-F/METEOR scoring → human expert review → clinical use
- **Design tradeoffs:**
  - LLM default prompts vs. optimized prompting: Study used basic prompts to simulate layperson usage; production systems would engineer domain-specific prompts
  - Surface metrics vs. clinical accuracy: BLEU/CHR-F/METEOR are computationally cheap but fail to capture critical medical terminology errors with appropriate severity weighting
  - Single reference vs. multiple references: Study used one professional translation as ground truth; multiple acceptable translations exist for any source text
- **Failure signatures:**
  - Dramatic BLEU drop with complexity (Chinese: 50%+ degradation) signals domain vocabulary gaps
  - High BLEU/low METEOR suggests literal but semantically awkward translations
  - Metric score inversion between simple/complex (Arabic) indicates morphology-context interaction
- **First 3 experiments:**
  1. Establish baseline with your target language pairs using identical summaries; compare BLEU/CHR-F/METEOR distributions against paper's reported ranges.
  2. Design domain-specific prompts with medical context framing ("You are translating a palliative care consultation...") and measure impact on METEOR scores.
  3. Implement critical-term extraction (drug names, dosages, diagnoses) and create a penalty-weighted evaluation that flags clinically significant mistranslations even when BLEU is high.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can evaluation metrics be developed to accurately capture clinical safety better than BLEU or METEOR?
- **Basis in paper:** Authors state current metrics "fail to capture clinical relevance" and often penalize semantically accurate translations.
- **Why unresolved:** Existing metrics rely on surface n-gram overlap, failing to weigh the criticality of specific medical terms versus general phrasing.
- **What evidence would resolve it:** A new metric demonstrating a strong correlation with human expert assessments of clinical safety and accuracy.

### Open Question 2
- **Question:** Does fine-tuning LLMs on medical corpora significantly reduce translation errors for under-resourced languages?
- **Basis in paper:** The conclusion suggests future improvements should "focus on fine-tuning LLMs with domain-specific medical corpora."
- **Why unresolved:** This study tested general-purpose models; the specific impact of specialized medical training on translation consistency remains untested.
- **What evidence would resolve it:** Comparative performance data between general-purpose and medically fine-tuned LLMs on the same translation tasks.

### Open Question 3
- **Question:** Do performance trends observed in simulated summaries hold true for actual patient consultation data?
- **Basis in paper:** The study relied on "fictitious consultation summaries" constructed by an expert rather than real patient data.
- **Why unresolved:** Real-world clinical notes contain irregularities, abbreviations, and noise not present in simulated, grammatical texts.
- **What evidence would resolve it:** A replication of this methodology using a dataset of de-identified real clinical notes.

## Limitations

- Metric validity concerns: BLEU, CHR-F, and METEOR inadequately capture clinical translation quality, yet these metrics form the entire empirical basis for conclusions
- Limited data scope: Only two manually created English summaries tested, raising questions about generalization across diverse medical content
- Generalization uncertainty: Performance differences may stem from factors beyond training data scarcity, such as tokenization quality or script complexity

## Confidence

- **High Confidence:** Traditional MT tools outperform LLMs on surface-level metrics for medical text translation is well-supported by empirical data
- **Medium Confidence:** Explanations for Arabic complexity improvements and Chinese degradation are plausible but not definitively proven
- **Low Confidence:** Claims about LLM "contextual flexibility" and "semantic equivalence" preservation lack clinical safety validation

## Next Checks

1. **Human Expert Review Implementation:** Conduct blinded evaluation by bilingual medical professionals to identify clinically significant translation errors that automated metrics miss
2. **Expanded Data Validation:** Replicate the study using a larger corpus of authentic medical consultation summaries across different specialties
3. **Prompt Engineering Impact Study:** Systematically test domain-specific prompt variations to quantify their impact on translation quality metrics