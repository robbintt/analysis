---
ver: rpa2
title: Intersectional Fairness in Reinforcement Learning with Large State and Constraint
  Spaces
arxiv_id: '2502.11828'
source_url: https://arxiv.org/abs/2502.11828
tags:
- reward
- groups
- problem
- algorithm
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies a constrained reinforcement learning problem
  where the goal is to maximize total reward while ensuring fair treatment across
  multiple, possibly exponentially many intersecting demographic groups. Each group
  is defined by a binary membership function over state features, and the constraint
  is that each group's expected reward must meet a threshold.
---

# Intersectional Fairness in Reinforcement Learning with Large State and Constraint Spaces

## Quick Facts
- arXiv ID: 2502.11828
- Source URL: https://arxiv.org/abs/2502.11828
- Reference count: 40
- This paper studies a constrained reinforcement learning problem where the goal is to maximize total reward while ensuring fair treatment across multiple, possibly exponentially many intersecting demographic groups.

## Executive Summary
This paper addresses the challenge of ensuring intersectional fairness in reinforcement learning across multiple demographic groups defined by binary membership functions over state features. The authors propose oracle-efficient algorithms that can handle large state and constraint spaces while maintaining fairness guarantees. The approach reduces the constrained problem to a sequence of unconstrained RL problems and linear optimization over group functions, enabling efficient computation even with exponentially many groups.

## Method Summary
The authors propose several algorithms for achieving intersectional fairness in constrained RL. For tabular MDPs, they use Follow-the-Perturbed-Leader with a linear optimization oracle over groups. For large state spaces with structured groups (e.g., boolean conjunctions), they adapt contextual FTPL using a separator set. For general groups, they introduce FairFictRL, a variant of fictitious play. All algorithms are shown to converge to approximate minimax solutions and are designed to be oracle-efficient, reducing the constrained problem to a sequence of unconstrained RL problems and linear optimization over group functions.

## Key Results
- Proposed algorithms achieve fairness across groups while maintaining reasonable total reward
- FairFictRL algorithm converges to approximate minimax solutions for general group definitions
- Experiments on preferential attachment graphs demonstrate practical effectiveness of the approach
- Algorithms can handle exponentially many intersecting groups efficiently through oracle reduction

## Why This Works (Mechanism)
The algorithms work by decomposing the constrained optimization problem into manageable subproblems. By leveraging oracle access to unconstrained RL solvers and linear optimization over group functions, the methods can efficiently navigate the exponentially large space of intersecting groups. The use of contextual FTPL with separator sets allows handling structured group definitions, while FairFictRL provides a general solution for arbitrary group definitions through fictitious play dynamics.

## Foundational Learning
- **Constrained RL**: Optimization framework where agents must satisfy multiple constraints while maximizing rewards; needed for modeling fairness requirements across groups
- **Follow-the-Perturbed-Leader**: Online learning algorithm that adds random perturbations to decision-making; quick check: works well in adversarial environments
- **Fictitious Play**: Game-theoretic learning algorithm where players best-respond to empirical frequency of opponent strategies; needed for handling general group definitions
- **Linear Optimization over Groups**: Solving optimization problems where variables are defined over demographic groups; quick check: enables efficient handling of exponentially many groups
- **Separator Sets**: Subset of states that can distinguish between different group membership functions; needed for efficient contextual learning
- **Minimax Solutions**: Game-theoretic equilibrium concepts; quick check: provides robustness guarantees

## Architecture Onboarding

Component Map:
Unconstrained RL Oracle -> Group Fairness Oracle -> Policy Selection -> Reward Maximization

Critical Path:
1. Receive current state
2. Query unconstrained RL oracle for best response
3. Query group fairness oracle to check constraint violations
4. Update policy based on oracle responses
5. Execute action and receive reward

Design Tradeoffs:
- Oracle efficiency vs. approximation guarantees
- Structured groups vs. general group definitions
- Computational complexity vs. fairness guarantees
- Memory requirements vs. separator set size

Failure Signatures:
- Constraint violations when separator sets are too small
- Poor convergence when fictitious play dynamics oscillate
- Suboptimal policies when oracle approximations are too coarse
- Scalability issues with extremely large state spaces

First 3 Experiments:
1. Test FairFictRL on synthetic MDPs with known group intersections
2. Evaluate contextual FTPL on boolean conjunction group structures
3. Compare oracle-efficient approach against naive enumeration baselines

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes access to oracle for unconstrained RL and linear optimization over groups
- Theoretical guarantees assume perfect knowledge of transition dynamics and reward functions
- Experimental evaluation limited to synthetic preferential attachment graphs
- Does not address continuous state spaces or non-binary group memberships
- Fairness guarantees limited to average rewards across groups rather than individual-level fairness

## Confidence
- Theoretical results for tabular MDP case: High
- Theoretical results for large state space case: Medium (due to separator set assumptions)
- Experimental results: Medium (limited scope and synthetic nature)

## Next Checks
1. Test the algorithms on real-world datasets with actual demographic group definitions to verify practical applicability
2. Evaluate performance when transition dynamics and reward functions are unknown and must be learned
3. Assess the impact of privacy constraints on group membership queries on algorithm performance