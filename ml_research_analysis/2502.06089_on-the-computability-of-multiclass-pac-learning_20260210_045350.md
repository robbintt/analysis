---
ver: rpa2
title: On the Computability of Multiclass PAC Learning
arxiv_id: '2502.06089'
source_url: https://arxiv.org/abs/2502.06089
tags:
- computable
- dimension
- learnability
- learning
- cpac
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper establishes a meta-characterization of computable multiclass\
  \ PAC (CPAC) learnability by showing that the finiteness of computable distinguishers\
  \ characterizes learnability for finite label spaces. The authors first define computable\
  \ versions of the Natarajan dimension and more general computable \u03A8-dimensions\
  \ based on families of functions mapping label spaces to {0,1,}."
---

# On the Computability of Multiclass PAC Learning

## Quick Facts
- arXiv ID: 2502.06089
- Source URL: https://arxiv.org/abs/2502.06089
- Reference count: 10
- Primary result: Finiteness of computable Natarajan dimension characterizes CPAC learnability for finite label spaces.

## Executive Summary
This paper establishes a complete meta-characterization of computable multiclass PAC (CPAC) learnability by showing that finiteness of computable shattering-based dimensions captures learnability. The authors prove that when the label space is finite, a hypothesis class is CPAC learnable if and only if its computable Natarajan dimension is finite. More generally, they show this extends to any distinguisher family Ψ: a class is CPAC learnable if and only if its computable Ψ-dimension is finite. This subsumes previous work on computable Natarajan and graph dimensions. However, the authors also prove that the DS dimension, which characterizes PAC learnability for infinite label spaces, cannot be expressed as a distinguisher, suggesting that characterizing CPAC learnability for infinite label spaces will require different techniques.

## Method Summary
The authors define computable versions of Natarajan dimension and more general Ψ-dimensions based on families of functions mapping label spaces to {0,1,*}. They prove necessity by showing a computable learner can be converted into a computable witness of non-shatterability (multiclass No-Free-Lunch theorem). Sufficiency is established through an embedding construction that adds "good" functions to the hypothesis class, ensuring bounded dimension while maintaining computable empirical risk minimization via enumeration of label patterns. The meta-characterization works because distinguishability enables Sauer-style counting bounds and the embedding construction generalizes from Natarajan to arbitrary Ψ-dimensions. The key insight is that for finite label spaces, computable shattering-based dimensions fully capture the learnability-computability tradeoff.

## Key Results
- Finiteness of computable Natarajan dimension is both necessary and sufficient for CPAC learnability when label space is finite.
- For any distinguisher family Ψ, finiteness of c-Ψ-dim(H) characterizes CPAC learnability for finite label spaces.
- The DS dimension cannot be expressed as a distinguisher, showing that characterizing CPAC learnability for infinite label spaces requires different techniques.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Finiteness of computable Natarajan dimension (c-N(H)) is necessary for CPAC learnability even for infinite label spaces.
- Mechanism: A computable learner A with sample complexity m(ε,δ) can be converted into a computable (2m−1)-witness of Natarajan dimension. Given a set X of size 2m and labelings g₁,g₂ differing everywhere, the learner identifies a target function f (via computable search over all 2^(2m) possible labelings) that witnesses non-shatterability: for every h∈H, there exists x∈X where h(x)≠f(x).
- Core assumption: The learner A is computable and outputs total computable functions; the hypothesis class has a representation suitable for enumeration.
- Evidence anchors:
  - [abstract] "the finiteness of the computable Natarajan dimension is both necessary and sufficient for CPAC learnability when the label space is finite"
  - [Section 3.1, Theorem 16] "Let H ⊆ Y^X be improperly CPAC learnable. Then c-N(H) < ∞"
  - [corpus] Related work on effective VC dimension (Delle Rose et al., 2023) establishes similar necessity in binary case; limited direct corpus on multiclass extension.

### Mechanism 2
- Claim: Finite c-N(H) is sufficient for CPAC learnability when |Y| < ∞.
- Mechanism: Embed H into H' = H ∪ G where G contains "good" functions—those that are eventually zero and avoid encoding witness outputs on any (k+1)-sized subset. This ensures: (i) N(H') ≤ k+1 (controlling sample complexity), and (ii) H' has computable ERM via enumeration of the finite set of good functions bounded by max sample element. The computable witness w excludes patterns not in G.
- Core assumption: Y is finite, enabling bounded enumeration of label patterns; the domain is countable (X = ℕ).
- Evidence anchors:
  - [Section 3.2, Theorem 18] "Let c-N(H) < ∞ and |Y| < ∞. Then H is (improperly) CPAC learnable."
  - [Section 3.2, Lemma 19] Constructs H' with N(H') ≤ k_N + 1 and computable function v outputting H'|T
  - [corpus] "Recursively Enumerably Representable Classes" paper establishes similar embedding techniques for binary CPAC.

### Mechanism 3
- Claim: For distinguisher families Ψ, c-Ψ-dim(H) < ∞ iff H is CPAC learnable (finite Y).
- Mechanism: Distinguishers are families of functions Y → {0,1,*} where every distinct label pair (y,y') has some ψ∈Ψ with ψ(y)≠ψ(y') and neither maps to *. The meta-characterization works because: (1) distinguishability enables the Sauer-style counting bound via c-N(H) (Theorem 25), and (2) the embedding construction generalizes from Natarajan to arbitrary Ψ-dimensions (Theorem 28).
- Core assumption: Ψ is a distinguisher; Y is finite; finite Ψ-dimension implies uniform convergence.
- Evidence anchors:
  - [Section 4.3, Theorem 33] "c-Ψ-dim(H) qualitatively characterizes CPAC learnability if and only if Ψ is a distinguisher"
  - [Section 4.4, Lemma 34] "The DS dimension cannot be expressed as a family Ψ_DS of functions from Y to {0,1,*}"
  - [corpus] Weak corpus evidence on distinguisher-based meta-characterizations beyond cited Ben-David et al. (1992).

## Foundational Learning

- Concept: **PAC Learning Framework**
  - Why needed here: The paper extends standard PAC to CPAC by adding computability constraints. Without understanding the baseline—learners as functions mapping samples to hypotheses with risk bounds—the distinction between "information-theoretic" and "computable" learnability is unclear.
  - Quick check question: Given a hypothesis class H, can you state the PAC learnability guarantee in terms of ε, δ, and sample complexity m(ε,δ)?

- Concept: **Shattering and Combinatorial Dimensions**
  - Why needed here: The paper's core contribution is computable versions of Natarajan dimension (N-shattering) and Ψ-dimensions. Understanding how shattering captures hypothesis class expressiveness is prerequisite to grasping why finiteness characterizes learnability.
  - Quick check question: For a binary hypothesis class H, what does it mean for a set S to be shattered? How does N-shattering differ for multiclass?

- Concept: **Computability Fundamentals (Total Computable Functions, Decidable/Semi-decidable Sets)**
  - Why needed here: CPAC requires learners to be algorithms that halt on all inputs and output total computable predictors. The distinction between decidable representation (DR) and recursively enumerable representation (RER) affects what is learnable.
  - Quick check question: What is the difference between a total computable function and a partial computable function? Why does CPAC require the former?

## Architecture Onboarding

- Component map:
Input: Sample S ∈ (X × Y)^m
    ↓
Witness Oracle w_N (or w_Ψ): Computes non-shattering witness for any (k+1)-sized set
    ↓
Hypothesis Enumeration: RER representation of H (or DR for proper learning)
    ↓
ERM Construction: Embed H → H' = H ∪ G, enumerate good functions bounded by max(S)
    ↓
Output: Total computable hypothesis h: X → Y

- Critical path: The witness function is the keystone. Without a computable k-witness, you cannot construct the embedding H' with controlled dimension and computable ERM. Verify witness computability first.

- Design tradeoffs:
  - Proper vs. improper: Proper learning requires DR; improper allows RER with strictly more learnable classes.
  - Finite vs. infinite Y: Finite Y enables bounded enumeration for ERM; infinite Y requires additional structure (e.g., computable bound c(n) on label range per Observation 20).
  - Generality vs. tractability: Distinguishers provide broad characterization but exclude DS dimension—infinite label spaces need different techniques.

- Failure signatures:
  - Witness non-computability: If no algorithm can compute the witness for dimension k, the class may be PAC learnable but not CPAC learnable.
  - Unbounded label enumeration: For infinite Y without computable bound c(n), ERM construction loops forever.
  - Non-distinguisher Ψ: If Ψ cannot distinguish some label pair, c-Ψ-dim(H)=1 does not imply learnability.

- First 3 experiments:
  1. Implement a 1-witness for Natarajan dimension on a simple hypothesis class (e.g., threshold functions on ℕ with Y={0,1,2}). Verify it correctly identifies non-shatterable sets.
  2. Construct the "good" function class G for a hypothesis class with c-N(H)=1 and finite Y={0,1,2,3}. Verify N(H∪G)≤2 and that ERM on H∪G is computable.
  3. Test a non-distinguisher Ψ (e.g., all ψ mapping to {0,*}) on a hypothesis class over Y={0,1,2}. Confirm that c-Ψ-dim(H)=1 but H is not CPAC learnable.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does there exist a computable version of the DS dimension that characterizes CPAC learnability for infinite label spaces?
- Basis in paper: [explicit] The authors prove "the DS dimension...cannot be expressed as a distinguisher" and conclude "characterizing CPAC learnability for infinite label spaces will potentially require significantly different techniques."
- Why unresolved: The distinguisher framework (which works for finite Y) cannot capture DS dimension, and the proof techniques rely on Y being finite to implement computable ERM.
- What evidence would resolve it: A definition of c-DS-dim and proof that its finiteness is necessary and sufficient for CPAC learnability with infinite Y, or a counterexample showing no such dimension exists.

### Open Question 2
- Question: Does a separation exist between realizable and agnostic CPAC learnability in the multiclass setting (as exists in binary CPAC)?
- Basis in paper: [inferred] The paper focuses on agnostic CPAC learning and notes that in binary classification "there are binary classes that are CPAC learnable in the realizable setting, but not in the agnostic one," but does not investigate whether this separation extends to multiclass.
- Why unresolved: The paper's meta-characterization only addresses the agnostic case; the realizable case for multiclass CPAC is not characterized beyond Proposition 10's sufficient conditions.
- What evidence would resolve it: Construction of a hypothesis class that is CPAC learnable in the realizable setting but not in the agnostic setting for |Y| > 2, or a proof that no such separation exists.

### Open Question 3
- Question: Can the computable Natarajan dimension provide quantitative (not just qualitative) sample complexity bounds for multiclass CPAC learning?
- Basis in paper: [inferred] The paper establishes that c-N(H) < ∞ iff H is CPAC learnable (qualitative characterization), but does not derive sample complexity bounds involving c-N(H) analogous to standard PAC bounds involving N(H).
- Why unresolved: The proof techniques embed H into H' with N(H') ≤ c-N(H) + 1 and rely on uniform convergence, which may introduce loose bounds that don't tightly depend on c-N(H).
- What evidence would resolve it: Derivation of upper and lower bounds on sample complexity expressed explicitly in terms of c-N(H) and |Y|.

## Limitations

- The characterization only applies to finite label spaces; infinite label spaces require different techniques as DS dimension cannot be expressed as a distinguisher.
- The paper focuses on improper learning via RER representations, requiring additional complexity that may be impractical for real implementations.
- The construction assumes countable domains (X = ℕ) and relies on explicit enumeration bounds that may not hold for arbitrary hypothesis classes.

## Confidence

- High confidence in necessity result (Theorem 16) and sufficiency construction (Theorem 18) for finite Y, as these rely on well-established computable No-Free-Lunch arguments and explicit embedding techniques.
- Medium confidence in the general Ψ-dimension characterization (Theorem 30), as the distinguisher framework, while elegant, has limited independent validation beyond the binary case and specific examples.
- Low confidence in claims about infinite label spaces, as the paper explicitly acknowledges that DS dimension cannot be expressed as a distinguisher (Lemma 34), leaving open whether computable shattering-based dimensions can characterize CPAC learnability in this regime.

## Next Checks

1. Implement the computable witness function for Natarajan dimension on a concrete hypothesis class (e.g., threshold functions on ℕ with Y={0,1,2}) and verify it correctly identifies non-shatterable sets of size 2m+1.

2. For a hypothesis class with c-N(H)=1 and Y={0,1,2,3}, construct the "good" function class G and verify: (i) N(H∪G)≤2, (ii) the function v(T) outputting H'|T is computable by implementing the enumeration procedure.

3. Test a non-distinguisher Ψ (e.g., all ψ mapping to {0,*}) on a hypothesis class over Y={0,1,2} to confirm that c-Ψ-dim(H)=1 but H is not CPAC learnable, validating the necessity of the distinguisher condition in Theorem 33.