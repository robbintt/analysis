---
ver: rpa2
title: 'Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares for
  Lifelong LLM Editing'
arxiv_id: '2601.15686'
source_url: https://arxiv.org/abs/2601.15686
tags:
- numbers
- edits
- edit
- editing
- equation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RLSEdit introduces a recursive least-squares framework for lifelong
  language model editing that formulates the task as an online quadratic optimization
  with soft constraints. The method minimizes a cumulative key-value fitting objective
  regularized by parameter deviation from the initial weights and anchor mapping deviation,
  enabling efficient per-edit updates via the Woodbury identity that scale independently
  of past edit history.
---

# Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares for Lifelong LLM Editing

## Quick Facts
- arXiv ID: 2601.15686
- Source URL: https://arxiv.org/abs/2601.15686
- Reference count: 40
- RLSEdit achieves 89.94% efficacy on Llama-3-8B vs 74.76% for fine-tuning on 10K sequential edits

## Executive Summary
RLSEdit introduces a recursive least-squares framework for lifelong language model editing that treats sequential edits as an online quadratic optimization problem with soft constraints. The method minimizes a cumulative key-value fitting objective regularized by parameter deviation from initial weights and anchor mapping deviation, enabling efficient per-edit updates via the Woodbury identity that scale independently of past edit history. Theoretical analysis provides deviation bounds and characterizes the adherence-preservation trade-off in the many-edits regime. Empirical evaluation on Llama-3 and Qwen2.5 demonstrates stable scaling to 10K sequential edits, outperforming baselines in edit success rates and superior retention of early edits while preserving general capabilities on GLUE, MMLU, GSM8K, HumanEval, and MBPP benchmarks.

## Method Summary
RLSEdit formulates lifelong LLM editing as an online recursive least-squares problem where each edit arrival updates the model weights to minimize cumulative edit residuals plus two regularization terms: parameter deviation from initial weights (λ/2||W-W₀||²_F) and anchor mapping deviation (μ/2||K₀W-V₀||²_F). The method extracts key-value pairs from target layers for each edit and applies Woodbury updates to the covariance matrix, achieving per-edit complexity O(d_k²u_t + d_k·d_v·u_t) via Cholesky factorization. The recursive formulation enables constant-time updates per edit independent of edit history, with theoretical guarantees on deviation bounds and a principled adherence-preservation trade-off controlled by hyperparameters λ and μ.

## Key Results
- Achieves 89.94% efficacy on Llama-3-8B vs 74.76% for fine-tuning on 10K sequential edits
- Outperforms baselines in edit success rates while better preserving general capabilities on GLUE, MMLU, GSM8K, HumanEval, and MBPP benchmarks
- Demonstrates stable scaling to 10K edits with superior retention of early edits compared to competing methods

## Why This Works (Mechanism)
RLSEdit works by framing lifelong editing as a recursive least-squares problem where each edit arrival updates model parameters to minimize cumulative edit residuals while regularizing against excessive deviation from initial weights and anchor mappings. The Woodbury identity enables efficient covariance updates that scale independently of edit history, while the dual regularization terms balance new edit fitting against preservation of existing knowledge and general capabilities. This soft-constraint approach contrasts with hard write baselines that overwrite parameters without preservation mechanisms.

## Foundational Learning
- **Recursive Least-Squares**: Online parameter estimation framework that updates weights incrementally as new data arrives; needed for efficient lifelong learning without retraining from scratch; quick check: verify update equations follow standard RLS formulation
- **Woodbury Identity**: Matrix inversion lemma that enables efficient updates to inverse covariance matrices; needed to maintain O(1) per-edit complexity regardless of edit count; quick check: confirm numerical stability of covariance updates
- **Dual Regularization**: Parameter deviation (λ) and anchor mapping (μ) regularization terms; needed to balance edit preservation against general capability retention; quick check: sweep λ and μ values to observe trade-off effects
- **Key-Value Extraction**: Process of extracting relevant token representations from transformer layers; needed to formulate edits as quadratic optimization; quick check: verify extraction covers relevant token positions and layers
- **Covariance Matrix Management**: Incremental updates to covariance matrix using Cholesky factorization; needed for numerical stability and computational efficiency; quick check: monitor condition number of covariance matrix over time
- **Adherence-Preservation Trade-off**: Theoretical characterization of how well the method balances fitting new edits versus preserving old ones; needed to understand long-term behavior; quick check: measure retention of early edits after many subsequent updates

## Architecture Onboarding
**Component Map**: Input Data -> Key-Value Extraction -> Woodbury Update -> Weight Update -> Model Parameters
**Critical Path**: Edit arrival → Key-value pair extraction → Covariance update via Woodbury identity → Weight computation → Parameter application
**Design Tradeoffs**: Fixed hyperparameters vs adaptive tuning (simpler but suboptimal vs more complex but potentially better performance); synthetic dataset vs real-world edits (controlled evaluation vs practical relevance); soft constraints vs hard writes (preservation vs direct modification)
**Failure Signatures**: Singular or ill-conditioned covariance matrices during long edit streams; catastrophic forgetting of early edits due to insufficient μ regularization; numerical instability in Cholesky factorization with large μ values
**Three First Experiments**:
1. Run sequential editing with fixed λ and μ, then sweep these hyperparameters to observe adherence-preservation trade-off
2. Periodically re-evaluate first 500-1K edits to quantify catastrophic forgetting rates
3. Monitor condition number of covariance matrix C_t during 10K edit stream to detect numerical instability

## Open Questions the Paper Calls Out
**Open Question 1**: How does RLSEdit perform on editing tasks beyond factual knowledge, such as behavioral modifications, rule-based updates, or multilingual knowledge editing? The paper only evaluates on factual associations using CounterFact dataset, while noting ANYEDIT and UNKE as orthogonal directions that broaden editable knowledge scope.

**Open Question 2**: What are the fundamental scaling limits of RLSEdit beyond 10K sequential edits, and does the Woodbury-based covariance matrix C_t eventually become numerically unstable? The paper demonstrates stable performance up to 10K edits but doesn't explore limits or analyze error accumulation in Woodbury updates over very long streams.

**Open Question 3**: Can the regularization hyperparameters (λ, μ) be adaptively tuned online rather than fixed a priori, and would this improve the adherence-preservation trade-off? The paper uses manually set hyperparameters and mentions that "a common policy is to set a deviation budget" but doesn't explore adaptive schemes.

**Open Question 4**: How does RLSEdit compare to memory-augmented approaches like WISE or retrieval-based editors like RECIPE in the lifelong editing setting? The paper notes these methods are complementary but provides no direct comparison on identical long-edit streams.

## Limitations
- Anchor pair construction method is underspecified, requiring careful selection of background samples and token positions that significantly impact performance
- Edit key-value extraction protocol details remain unclear, including token position selection and context window handling
- Computational scaling uncertainty for very large models where d_k² term could become prohibitive
- Dataset-specific generalization concerns as evaluation relies on synthetic CounterFact rather than naturally occurring editing scenarios

## Confidence
- **High Confidence**: Core recursive least-squares framework with Woodbury updates is mathematically sound; theoretical deviation bounds analysis is rigorous
- **Medium Confidence**: Sequential editing efficacy results depend heavily on CounterFact dataset construction and evaluation protocol
- **Medium Confidence**: Adherence-preservation trade-off characterization is theoretically grounded but practical implications depend on hyperparameter tuning

## Next Checks
1. Systematically vary anchor sample count, token positions, and corpus sources to measure impact on edit retention and capability preservation
2. Implement periodic re-evaluation of early edits every 1K edits up to 10K total to quantify catastrophic forgetting rates
3. Apply RLSEdit to naturally occurring factual error correction tasks using REALM or FEVER datasets to test synthetic training generalization