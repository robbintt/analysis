---
ver: rpa2
title: Closing the Train-Test Gap in World Models for Gradient-Based Planning
arxiv_id: '2512.09929'
source_url: https://arxiv.org/abs/2512.09929
tags:
- world
- planning
- adversarial
- gradient-based
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the poor performance of gradient-based planning
  (GBP) with world models, which stems from a train-test gap: world models are trained
  for next-state prediction but used at test time to optimize action sequences. The
  authors propose two methods to close this gap: Online World Modeling, which finetunes
  the model on corrected planning trajectories using simulator feedback, and Adversarial
  World Modeling, which trains the model on adversarially perturbed trajectories to
  smooth the optimization landscape.'
---

# Closing the Train-Test Gap in World Models for Gradient-Based Planning

## Quick Facts
- arXiv ID: 2512.09929
- Source URL: https://arxiv.org/abs/2512.09929
- Authors: Arjun Parthasarathy; Nimit Kalra; Rohun Agrawal; Yann LeCun; Oumayma Bounou; Pavel Izmailov; Micah Goldblum
- Reference count: 40
- One-line primary result: Adversarial World Modeling enables gradient-based planning to match or exceed CEM performance while requiring only 10% of computation time.

## Executive Summary
This paper addresses the poor performance of gradient-based planning (GBP) with world models, which stems from a train-test gap: world models are trained for next-state prediction but used at test time to optimize action sequences. The authors propose two methods to close this gap: Online World Modeling, which finetunes the model on corrected planning trajectories using simulator feedback, and Adversarial World Modeling, which trains the model on adversarially perturbed trajectories to smooth the optimization landscape. Experiments on three tasks (PushT, PointMaze, Wall) show that Adversarial World Modeling enables GBP to match or exceed the performance of the cross-entropy method (CEM) while requiring only 10% of the computation time. The results demonstrate improved planning success rates and reduced world model error on planning trajectories compared to the pretrained baseline.

## Method Summary
The paper proposes two finetuning approaches to close the train-test gap in world models for gradient-based planning. Online World Modeling collects planning trajectories, corrects them with simulator feedback, and finetunes the world model on these corrected trajectories. Adversarial World Modeling uses FGSM-style adversarial perturbations on latent states and actions during training to smooth the optimization landscape. Both methods use a DINOv2 encoder with a trainable ViT transition model trained on next-state prediction. The key innovation is training the model to be robust to the types of adversarial action sequences that GBP will optimize, rather than just predicting expert trajectories.

## Key Results
- Adversarial World Modeling achieves 94% success rate on PushT compared to 54% for standard DINO-WM with GBP
- Adversarial WM + GBP matches CEM performance (88% vs 90% on PointMaze) while requiring only 10% of computation time
- Planning success rates improve from 54% to 92% on PushT when using Adversarial WM with MPC
- Wall-clock time for 25-step rollouts decreases from 0.959s (CEM) to 0.029s (GBP with Adversarial WM)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Gradient-based planning (GBP) fails because it drives world models into out-of-distribution states not encountered during expert-trajectory training.
- **Mechanism**: World models minimize next-state prediction loss on expert trajectories, but GBP optimizes action sequences solely to reach goal states—without any constraint that these actions resemble the training distribution. These "adversarial" action sequences expose latent regions where model errors compound over the planning horizon.
- **Core assumption**: The expert trajectory distribution does not cover all states reachable by gradient-optimized action sequences.
- **Evidence anchors**:
  - [abstract] "although a world model is trained on a next-state prediction objective, it is used at test-time to instead estimate a sequence of actions"
  - [section 2.2] "the action sequences being optimized are not constrained to lie within the distribution of behavior seen during training...optimizing through learned models under such conditions is known to induce adversarial inputs"
  - [corpus] Corpus papers on sim-to-real transfer (2510.14643) address similar distribution-shift challenges in MPC but via different mechanisms (generative bootstrapping vs. adversarial smoothing)
- **Break condition**: If expert trajectories already cover the reachable state space under any action sequence, the OOD hypothesis would not hold.

### Mechanism 2
- **Claim**: The planning loss landscape induced by standard world models is non-smooth, containing poor local minima that hinder gradient-based optimization.
- **Mechanism**: Next-state prediction training provides no guarantee that input gradients (∂next_state/∂action) are well-behaved. Without explicit regularization, the world model can create rugged loss surfaces where gradient descent stagnates or diverges.
- **Core assumption**: Gradient-based planning requires locally smooth derivatives to converge; non-smooth landscapes cause optimization failure independent of prediction accuracy.
- **Evidence anchors**:
  - [abstract] Identifies that "the resulting optimization landscape is often non-smooth and difficult to optimize" as a core problem
  - [section 2.3] "there is no particular reason for their input gradients to be well-behaved"
  - [section 3, Figure 2] Visual comparison shows Adversarial World Modeling produces "a smoother landscape with a broader basin around the optimum"
  - [corpus] Planning as Descent (2512.17846) learns energy landscapes for trajectory synthesis but via explicit energy functions rather than modifying world model gradients
- **Break condition**: If the world model architecture inherently produces smooth gradients (e.g., through architectural constraints), adversarial smoothing would be unnecessary.

### Mechanism 3
- **Claim**: Adversarial perturbation of training data smooths the induced planning loss landscape by training the model to be robust in worst-case local regions.
- **Mechanism**: FGSM-style perturbations find directions that maximize prediction error within a bounded radius. Training on these adversarial samples forces the model to have consistent gradients in local neighborhoods, reducing the ruggedness of the action→state mapping that GBP traverses.
- **Core assumption**: Local gradient consistency near expert trajectories generalizes to the regions GBP explores during optimization.
- **Evidence anchors**:
  - [abstract] "Adversarial World Modeling...adversarially perturbs training data to smooth the optimization landscape"
  - [section 2.3] "Adversarial training has been shown to result in better behaved input gradients (Mejia et al., 2019), consequently smoothing the input loss surface"
  - [section 3, Table 1] Adversarial WM with Adam GBP matches or exceeds CEM on all tasks (94% PushT, 88% PointMaze, 94% Wall)
  - [corpus] Corpus lacks direct validation of adversarial training for world model smoothing; related work (Generative Models for MPC 2510.14643) uses generative bootstrapping instead
- **Break condition**: If perturbation radii are too large (λ_z > 0.5), visual latent semantics are destroyed, degrading performance (Appendix D.2).

## Foundational Learning

- **Concept**: Model Predictive Control (MPC) with receding horizon
  - **Why needed here**: The paper uses MPC where planning happens repeatedly: optimize H steps, execute K actions, replan from the new state. Understanding this loop is essential to grasp why computational efficiency matters and why "open-loop" vs "MPC" results differ significantly.
  - **Quick check question**: Given a planning horizon H=25 and MPC execution K=1, how many times does the planner run to reach a goal 100 steps away?

- **Concept**: Teacher-forcing vs autoregressive prediction in world models
  - **Why needed here**: The world model is trained with teacher-forcing (each prediction gets the true previous state), but planning uses it autoregressively (errors compound). This mismatch underlies the compounding error problem that Online World Modeling addresses.
  - **Quick check question**: If a world model has 5% single-step error, what is the approximate error after 10 autoregressive steps assuming errors compound multiplicatively?

- **Concept**: FGSM (Fast Gradient Sign Method) adversarial attacks
  - **Why needed here**: Adversarial World Modeling uses FGSM to generate perturbations δ = ε·sign(∇_x L). Understanding this single-step attack is necessary to implement the training loop efficiently (2× backward passes vs K+1 for K-step PGD).
  - **Quick check question**: Why does FGSM initialize δ uniformly in the ε-ball rather than at zero, and how does this relate to the "Free Lunch" paper (Wong et al., 2020)?

## Architecture Onboarding

- **Component map**:
  ```
  [Frozen DINOv2 Encoder Φ_μ] → [Trainable Transition Model f_θ (ViT)]
           ↓                            ↓
     z_t = Φ_μ(o_t)            z_{t+1} = f_θ(z_t, a_t)
                                        ↓
                              [Planning: GBP optimizes {a_t} via ∂z_H/∂{a_t}]
  ```
  - **Training modes**:
    1. **Base**: L2 next-state prediction on expert (o_t, a_t, o_{t+1})
    2. **Online WM**: GBP → simulator correction → add to dataset → finetune
    3. **Adversarial WM**: FGSM perturb (z_t, a_t) → train on worst-case

- **Critical path**:
  1. Start with pretrained DINO-WM (frozen encoder, trained transition model)
  2. Implement GBP loop (Algorithm 1) with weighted goal loss (Eq. 9)
  3. Add Adversarial WM finetuning (Algorithm 3)—this is the primary contribution
  4. Evaluate with MPC (Table 1, +MPC rows) using Adam optimizer

- **Design tradeoffs**:
  - **FGSM vs PGD**: FGSM requires 2× backward passes, PGD requires K+1×. Paper shows FGSM sufficient (Table 11: FGSM 94% vs 3-step PGD 94% on Wall MPC)
  - **Fixed vs Adaptive ε**: Fixed ε from first batch is stable; adaptive adds complexity without consistent gain (Figure 9)
  - **Online vs Adversarial WM**: Online requires simulator access (expensive: 0.959s vs 0.029s for 25-step rollout in PushT, Table 10). Adversarial is simulator-free but only smooths near-expert regions

- **Failure signatures**:
  - **λ_z too high (>0.5)**: Visual latent perturbations destroy semantic content, degrading performance (Appendix D.2)
  - **Random initialization network underperforms**: Table 6 shows initialization network only helps in PushT; random init is more robust
  - **GD optimizer fails**: Table 1 shows GD consistently underperforms Adam (e.g., PushT Adversarial WM: 56% GD vs 92% Adam with MPC)
  - **Train-test gap persists**: Figure 4 shows DINO-WM has negative gap (planning worse than training); Online/Adversarial WM reverse this

- **First 3 experiments**:
  1. **Reproduce baseline gap**: Train DINO-WM on PushT expert trajectories, evaluate GBP vs CEM success rates. Expected: CEM >> GBP (78% vs 54% open-loop, per Table 1). This confirms the train-test gap exists.
  2. **Implement Adversarial WM ablation**: Apply Algorithm 3 to DINO-WM with λ_a=0.02, λ_z=0.05 (Table 4). Visualize loss landscape (Figure 2 method) to confirm smoothing. Expected: smoother basin around optimum.
  3. **MPC evaluation with timing**: Compare wall-clock time for CEM vs Adversarial WM + GBP on PointMaze (Figure 7). Expected: GBP achieves comparable success (88% vs 90%) in ~10% of time. This validates the computational efficiency claim.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can Online and Adversarial World Modeling effectively close the train-test gap on real-world physical robotic systems?
- **Basis in paper**: [explicit] The conclusion explicitly lists evaluating these methods on real-world systems as an important direction for future work.
- **Why unresolved**: All reported results are derived from simulation environments (PushT, PointMaze, Wall), and the sim-to-real transfer of adversarial robustness is not verified.
- **What evidence would resolve it**: Successful deployment of the proposed finetuning methods on physical hardware, demonstrating planning efficiency and success rates comparable to simulation results.

### Open Question 2
- **Question**: Does Adversarial World Modeling improve a world model's robustness to environmental stochasticity and adversarial disturbances?
- **Basis in paper**: [explicit] The conclusion suggests that adversarial training may improve robustness to environmental adversaries or stochasticity, but this is not tested in the main experiments.
- **Why unresolved**: The experimental tasks appear to use deterministic simulators, leaving the method's efficacy in stochastic or perturbed settings unknown.
- **What evidence would resolve it**: Empirical evaluation of planning performance in environments with injected process noise or observation noise, comparing standard vs. adversarially trained models.

### Open Question 3
- **Question**: Do these methods improve planning stability in hierarchical or multi-timescale world models?
- **Basis in paper**: [explicit] The authors state their methods are "especially well-suited to multi-timescale or hierarchical world models" for long-horizon planning, though they only test single-level models.
- **Why unresolved**: The paper focuses on flat world model architectures, so the benefits for temporal abstraction or hierarchical latent spaces remain hypothetical.
- **What evidence would resolve it**: Experiments applying these finetuning techniques to hierarchical architectures, measuring performance on tasks requiring long-horizon reasoning.

## Limitations

- **Simulator requirement**: Online World Modeling requires simulator access for trajectory correction, making it expensive (0.929s vs 0.029s per 25-step rollout in PushT) and impractical for real-world deployment without ground-truth dynamics.
- **Limited generalization**: Adversarial smoothing only improves robustness near expert trajectories and may not generalize to all regions GBP explores, as the assumption of local gradient consistency remains unverified for out-of-distribution scenarios.
- **Hand-tuned design choices**: The weighted goal loss formulation uses fixed weight schedules rather than learned components, representing a potential area for improvement.

## Confidence

- **High confidence**: The existence of the train-test gap and the general mechanism of gradient-based planning driving world models into OOD states (Mechanism 1)
- **Medium confidence**: The smoothing effect of adversarial training on the optimization landscape (Mechanism 2-3), supported by strong empirical evidence but lacking theoretical guarantees
- **Medium confidence**: Computational efficiency claims, as wall-clock comparisons show 10× speedup but real-world deployment overhead remains untested

## Next Checks

1. **OOD generalization test**: Run GBP with Adversarial WM on environments with novel obstacles not present in training data. Measure whether adversarial smoothing near expert trajectories provides robustness to unseen configurations.
2. **Adversarial perturbation ablation**: Systematically vary FGSM perturbation radius ε (Table 4: 0.05-0.20 visual, 0.02-0.08 action/proprio) and measure impact on planning success rates and loss landscape smoothness. Identify breaking point where perturbations destroy semantic content.
3. **Real-world simulator-free evaluation**: Implement MPC with Adversarial WM on a physical robot (e.g., FetchPush) using only the learned world model without simulator correction. Compare success rates to simulator-assisted Online WM to quantify the practical cost of requiring ground-truth feedback.