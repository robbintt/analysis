---
ver: rpa2
title: 'Nonlinear discretizations and Newton''s method: characterizing stationary
  points of regression objectives'
arxiv_id: '2510.11987'
source_url: https://arxiv.org/abs/2510.11987
tags:
- neural
- newton
- solution
- basis
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the behavior of exact Newton methods for\
  \ training neural networks, contrasting them with quasi-Newton approaches. While\
  \ second-order methods are known to improve training efficiency, the author shows\
  \ that exact Newton methods frequently converge to trivial solutions\u2014specifically,\
  \ configurations where network parameters are set to zero while basis functions\
  \ are orthogonal to the target."
---

# Nonlinear discretizations and Newton's method: characterizing stationary points of regression objectives

## Quick Facts
- arXiv ID: 2510.11987
- Source URL: https://arxiv.org/abs/2510.11987
- Reference count: 33
- Primary result: Exact Newton methods for training neural networks frequently converge to trivial saddle points where parameters are zero and basis functions are orthogonal to the target, rather than to minima

## Executive Summary
This paper investigates why exact Newton methods for training neural networks often fail to find good solutions, instead converging to trivial saddle points. Through theoretical analysis and numerical experiments on regression and physics-informed problems, the author demonstrates that exact Newton methods seek stationary points (where gradient is zero) without distinguishing between minima and saddles. The work shows that quasi-Newton methods succeed not because of better Hessian approximations, but because they avoid negative curvature directions that lead to saddle points by enforcing positive definiteness. This provides new insight into the geometry of neural network loss landscapes and the dynamics of different optimization strategies.

## Method Summary
The paper uses a multilayer perceptron (MLP) architecture with two hidden layers (10 neurons each) and tanh activation to solve 1D regression problems on synthetic data from $v(x) = 2\sin(4\pi x)$. The exact Newton method is implemented via Levenberg-Marquardt optimization, computing the full Hessian using autograd and solving $(H + \epsilon I) \Delta \theta = -g$ at each step. The experiments compare exact Newton (with damping) against quasi-Newton (BFGS) methods. A critical constraint is the absence of a bias term in the output layer to enable theoretical decomposition into inner and outer parameters. The loss landscape is analyzed by examining Hessian eigenvalues at convergence points to classify critical points as minima, maxima, or saddle points.

## Key Results
- Exact Newton methods converge to "trivial solutions" where output weights are zero and basis functions are orthogonal to the target function
- These trivial solutions are saddle points in the loss landscape, not minima
- Quasi-Newton methods succeed specifically because they avoid negative curvature directions through their curvature condition
- The prevalence of saddle points in high-dimensional landscapes explains the failure of exact Newton methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Exact Newton methods converge to trivial saddle points rather than minima in neural network regression because they seek stationary points (gradient zero) without distinguishing between minima, maxima, and saddles.
- **Mechanism:** Newton's method iteratively solves $\nabla L(\theta) = 0$. In high-dimensional nonlinear discretizations (like neural networks), stationary points include saddle points. The method follows the curvature defined by the exact Hessian. If the Hessian has negative eigenvalues (indicating concavity), the Newton step directs the parameters toward these saddle points or maxima rather than exclusively descending the loss landscape.
- **Core assumption:** The loss landscape of a neural network contains a significant number of saddle points, and the specific "trivial solution" (zero output with orthogonal basis functions) is a prevalent, reachable saddle point.
- **Evidence anchors:**
  - [abstract] ("converge to trivial solutions... configurations where network parameters are set to zero")
  - [Page 20] ("Newton methods solve for a zero of the gradient... given the prevalence of saddle points... the true Hessian may point the way to nearby stationary points, rather than in descent directions.")
  - [corpus] [arXiv:1406.2572] (Identifying and attacking the saddle point problem in high-dimensional non-convex optimization).

### Mechanism 2
- **Claim:** Quasi-Newton methods (e.g., BFGS, L-BFGS) succeed in training neural networks specifically because they discard negative curvature information, enforcing a positive definite approximation of the Hessian.
- **Mechanism:** Unlike exact Newton, quasi-Newton methods like BFGS enforce a "curvature condition" to maintain a positive definite Hessian approximation. This effectively filters out directions of negative curvature (which lead to saddles/maxima). By ignoring these directions, the optimizer is restricted to descent directions, behaving more like a sophisticated gradient descent with adaptive step-sizing than a true root-finding Newton method.
- **Core assumption:** The primary failure mode of second-order optimization in this context is convergence to non-minimum stationary points (saddles), not poor Hessian approximation quality.
- **Evidence anchors:**
  - [abstract] ("quasi-Newton methods succeed not because of better Hessian approximations, but because they avoid negative curvature directions")
  - [Page 21] ("BFGS and L-BFGS approximations... enforce the 'curvature condition' in order to maintain a positive deÔ¨Ånite approximation... even when steps... suggest otherwise.")
  - [corpus] [arXiv:2510.13680] (Relevant comparison of Adam vs Gauss-Newton, though specific mechanism regarding curvature positivity is detailed primarily in the main text).

### Mechanism 3
- **Claim:** A specific "trivial solution" exists for MLPs where output weights are zero and basis functions are orthogonal to the target, satisfying the stationarity condition $\nabla L = 0$.
- **Mechanism:** Neural networks can be viewed as $\sum \theta_O h(x; \theta_I)$. If the inner parameters $\theta_I$ are adjusted so the basis functions $h$ are orthogonal to the target error, and the outer parameters $\theta_O$ are set to zero (making the network output zero and gradients w.r.t $\theta_I$ zero), the stationarity condition is met. This represents a saddle point: minimum w.r.t. $\theta_O$ (at zero), but maximum/undefined w.r.t $\theta_I$.
- **Core assumption:** The network has sufficient capacity to simultaneously drive output weights to zero and rotate basis functions toward orthogonality with the target.
- **Evidence anchors:**
  - [Page 10] ("stationarity of the loss can be obtained by fitting zero coefficients on a basis that is orthogonal to the target function.")
  - [Page 11, Figure 5] (Shows converged solution where basis is orthogonal to target and objective is high.)
  - [corpus] [arXiv:2509.16974] (Hessian-guided flows for escaping saddles, supports general landscape geometry).

## Foundational Learning

- **Concept:** **Newton's Method vs. Gradient Descent Objectives**
  - **Why needed here:** The paper hinges on the distinction that Newton finds *stationary points* ($\nabla f = 0$), whereas Gradient Descent explicitly seeks *minima* ($f_{t+1} < f_t$). Without this, the failure of exact Newton looks like numerical instability rather than a geometric property.
  - **Quick check question:** Does a standard Newton step guarantee a reduction in the loss function value?

- **Concept:** **Hessian Eigenvalues and Critical Points**
  - **Why needed here:** The classification of the "trivial solution" as a saddle point relies on interpreting the Hessian eigenvalues (positive = convex/min, negative = concave/max, mixed = saddle).
  - **Quick check question:** At a converged point, if the Hessian has both positive and negative eigenvalues, what type of critical point is it?

- **Concept:** **Nonlinear Discretizations (Manifolds)**
  - **Why needed here:** The paper frames neural networks not as fixed basis function approximators but as nonlinear discretizations defining a manifold. This explains why "Galerkin orthogonality" doesn't guarantee a minimum error solution.
  - **Quick check question:** In a linear regression, is the stationary point (where error is orthogonal to basis) guaranteed to be a global minimum? Is this true for a nonlinear manifold?

## Architecture Onboarding

- **Component map:** Input $x$ -> MLP (Inner Parameters $\theta_I$ + Outer Parameters $\theta_O$) -> Output -> MSE Loss
- **Critical path:**
  1. Initialize $\theta$
  2. Compute Loss $L$ and Gradient $\nabla L$
  3. **Divergence Point:** Compute Exact Hessian $J$ (Path A) vs. Approximate Hessian $H$ (Path B)
  4. Path A solves $J \Delta \theta = -\nabla L$; Path B solves $H \Delta \theta = -\nabla L$ with positive-definite constraint
  5. Path A converges to $\theta_O \approx 0$ (Saddle); Path B converges to $\theta$ minimizing $L$
- **Design tradeoffs:**
  - **Exact Curvature:** Theoretically faster convergence near minima, but blindly follows negative curvature into saddles. Computationally expensive ($O(N^3)$)
  - **Quasi-Newton (BFGS):** Ignores negative curvature (safe), cheaper ($O(N^2)$ or $O(N)$ with L-BFGS), but approximates curvature (slower convergence near minima)
  - **Assumption:** The paper suggests Exact Newton is effectively unusable for generic regression training due to saddle attraction, regardless of computational cost
- **Failure signatures:**
  - **The "Trivial Solution" Saddle:** Network output $N(x) \approx 0$ for all $x$
  - **Metric:** Loss $L$ remains high/near constant at convergence
  - **Diagnostic:** Hessian eigenvalues at convergence are mixed (+ and -) or clustered near zero; Outer weights $\theta_O$ are near zero
- **First 3 experiments:**
  1. **Replicate Trivial Solution:** Train a small MLP (1D input, sin target) using exact Newton (or Levenberg-Marquardt with low damping). Verify if network output converges to zero
  2. **Hessian Diagnosis:** At the convergence point of Exp 1, compute eigenvalues of the Hessian. Confirm the presence of negative eigenvalues confirming a saddle point
  3. **Optimizer Comparison:** Run the same problem with L-BFGS. Observe if it avoids the zero-output solution and minimizes the loss

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do exact Newton methods converge specifically to the identified "trivial" saddle points rather than other stationary points?
- Basis in paper: [explicit] Page 19 states, "We have not explained why exact Newton methods so often converge to the particular saddle point we have identified, which numerical experimentation indicates is non-unique."
- Why unresolved: The paper establishes that these saddles exist and that Newton methods seek zeros of the gradient, but it does not identify the specific attractor dynamics that favor these trivial solutions over non-trivial minima.
- What evidence would resolve it: An analysis comparing the volume of the attraction basins for trivial saddles against minima relative to standard initialization schemes.

### Open Question 2
- Question: What distinct geometric mechanisms allow quasi-Newton methods to outperform momentum-based first-order methods if both simply avoid negative curvature?
- Basis in paper: [explicit] Page 20 asks, "If entrapment in local minima is a primary failure mode of an optimizer without momentum, it is not clear how to interpret the success of quasi-Newton methods" relative to ADAM.
- Why unresolved: The author argues that quasi-Newton success stems from ignoring negative curvature (essentially acting as a saddle-avoidance mechanism), but this does not explain why they often yield better accuracy than ADAM, which also escapes saddles via momentum.
- What evidence would resolve it: Comparative studies of optimizer trajectories on loss landscapes with characterized saddle point densities to isolate the benefits of positive-curvature approximation versus momentum-based escape.

### Open Question 3
- Question: Are saddle points statistically more prevalent than local minima in high-dimensional neural network loss landscapes?
- Basis in paper: [explicit] Page 18 poses, "Perhaps it is the case the saddle points are more prevalent in the loss landscape than minima or maxima?"
- Why unresolved: While the author cites a random matrix theory argument suggesting saddles dominate, they note that this model assumes independent Hessian components, which contradicts the structured nature of neural network Hessians.
- What evidence would resolve it: Empirical topological analysis of loss landscapes for standard architectures to count the ratio of saddle points to local minima.

## Limitations

- The theoretical framework relies heavily on the specific choice of network architecture (no bias in output layer) and may not generalize to all architectures
- The analysis is limited to regression objectives and does not extend to classification or other loss functions
- The role of initialization and regularization is critical but not fully characterized across diverse scenarios

## Confidence

- **High Confidence:** The characterization of the "trivial solution" as a saddle point (not a minimum) is well-supported by the Hessian analysis in the 1D experiments
- **Medium Confidence:** The mechanism by which quasi-Newton methods avoid negative curvature directions through the curvature condition is correctly identified
- **Medium Confidence:** The theoretical existence of the "trivial solution" (zero weights, orthogonal basis) is sound for the specific MLP setup described

## Next Checks

1. **Higher-Dimensional Regression:** Reproduce the experiment with a 2D or 3D input space and a more complex target function (e.g., a sum of sinusoids). Verify if exact Newton still converges to the zero-output saddle point and if L-BFGS consistently avoids it.

2. **Impact of Output Bias:** Modify the 1D experiment to include a bias term in the output layer. Analyze how this change affects the existence and attractiveness of the "trivial solution" saddle point. Does the theory and mechanism still hold?

3. **Alternative Architectures and Activations:** Test the mechanism with different activation functions (e.g., ReLU, sigmoid) and architectures (e.g., deeper networks, different widths). Does the prevalence of the saddle point and the success of quasi-Newton methods remain consistent?