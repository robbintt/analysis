---
ver: rpa2
title: A Human-Centric Approach to Explainable AI for Personalized Education
arxiv_id: '2505.22541'
source_url: https://arxiv.org/abs/2505.22541
tags:
- student
- features
- course
- explanations
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# A Human-Centric Approach to Explainable AI for Personalized Education

## Quick Facts
- arXiv ID: 2505.22541
- Source URL: https://arxiv.org/abs/2505.22541
- Reference count: 0
- One-line primary result: Intrinsic, interpretable-by-design models like InterpretCC produce more trustworthy and actionable explanations for educators than post-hoc explainers on black-box models

## Executive Summary
This thesis argues that the future of human-centric explainable AI (XAI) in education lies not in post-hoc explanations for black-box models, but in intrinsically interpretable models whose decision process is transparent by design. Through a comprehensive framework spanning five chapters, it demonstrates that models like InterpretCC, which constrain decision-making to human-specified feature groups, provide more trustworthy and actionable explanations than traditional post-hoc methods. The work also introduces iLLuMinaTE, a theory-driven communication framework that transforms technical XAI outputs into pedagogically meaningful text and visuals for students and teachers.

## Method Summary
The research employs a multi-pronged approach: first, developing intrinsically interpretable models (InterpretCC) and evaluating them against post-hoc explainers (LIME, SHAP, etc.) on student success prediction tasks using MOOC clickstream data; second, creating iLLuMinaTE, an LLM-based explanation communicator that applies social science theories to generate actionable narratives; and third, conducting extensive user studies with educators and students to validate explanation quality across multiple dimensions. The work spans five MOOCs with 45 weekly behavioral features per student, using BiLSTM models as baselines and implementing rigorous consistency and robustness analyses.

## Key Results
- InterpretCC's intrinsic explanations were preferred by teachers over post-hoc baselines on actionability, usefulness, and trustworthiness
- iLLuMinaTE's theory-driven explanations led to 89.52% preference over raw post-hoc visualizations and improved simulated student performance
- Random model initialization significantly impacts explanation consistency, while adversarial training can improve stability

## Why This Works (Mechanism)

### Mechanism 1
Intrinsic, interpretable-by-design models like InterpretCC provide more trustworthy and actionable explanations for educators than post-hoc explainers on black-box models. The model's architecture is constrained to make decisions based on human-specified feature groups, making the explanation directly reflect the actual decision path rather than approximating a black box. This removes approximation error and consistency issues inherent in post-hoc methods.

### Mechanism 2
Frame-based, theory-driven communication (e.g., iLLuMinaTE) is necessary to make complex model outputs interpretable and actionable for non-technical stakeholders. An LLM acts as a translator layer, applying social science theories of explanation to transform technical feature importance scores into coherent narratives that answer human-centric questions about learning progress and next steps.

### Mechanism 3
Robust, consistent explanations are prerequisites for building and maintaining trust with educators, who are less tolerant of AI inconsistency than human inconsistency. Educators operate with baseline trust for human colleagues but hold AI to stricter standards, viewing inconsistency as a fundamental flaw that erodes trust in the entire system.

## Foundational Learning

### Post-Hoc vs. Intrinsic Interpretability
Why needed: The core argument hinges on moving from post-hoc explanations (approximations) to intrinsic models (transparent by design).
Quick check: Can you explain the difference between using LIME on a BiLSTM and using an InterpretCC model, in terms of how the explanation is generated?

### Trust and Consistency in Human-AI Interaction
Why needed: Educators' trust is fragile and particularly sensitive to inconsistency in AI outputs.
Quick check: Why does the same level of disagreement between two AI models erode trust more than the same level of disagreement between two human teaching assistants?

### Theory-Driven Explanation Communication
Why needed: The iLLuMinaTE framework succeeds by grounding explanation generation in established social science theories rather than generic summarization.
Quick check: What is one example of a social science theory of explanation used in iLLuMinaTE, and what kind of student feedback question does it help answer?

## Architecture Onboarding

### Component map
Student clickstream data -> Behavioral feature extraction -> BiLSTM predictor -> XAI module (InterpretCC or post-hoc) -> iLLuMinaTE communicator -> User interface (teachers/students)

### Critical path
1. Data extraction (clickstream â†’ behavioral features)
2. Model training (predictor + XAI setup)
3. XAI extraction (feature importance scores)
4. Explanation communication (LLM translation)
5. User-facing interface

### Design tradeoffs
- **Accuracy vs. Interpretability**: InterpretCC may trade a fraction of predictive accuracy for guaranteed, sparse explanations
- **Post-Hoc Generality vs. Intrinsic Faithfulness**: Post-hoc methods work on any model but can be inconsistent; intrinsic methods are faithful by design but may constrain model architecture
- **Automation vs. Control**: iLLuMinaTE automates explanation generation, but relying on LLMs introduces potential for subtle misalignment

### Failure signatures
- **The "No True Path" Problem**: A post-hoc explainer provides a coherent story that has no relation to the model's actual decision process
- **The "Robot Teacher" Problem**: An explanation, while technically faithful, is so dense or jargon-heavy that it confuses rather than clarifies the user
- **The "Capricious Oracle" Problem**: Re-running the same model/explainer on the same student yields wildly different explanations

### First 3 experiments
1. **Replicate & Compare**: Train a BiLSTM on public student clickstream data. Apply LIME and SHAP. Measure explanation consistency across multiple runs. Use InterpretCC and compare predictive accuracy and explanation consistency.
2. **Translate and Evaluate**: Use the iLLuMinaTE pipeline. Feed it feature importance scores and course context. Generate explanations using two different prompting theories. Conduct a small user study to rate actionability and trustworthiness.
3. **Stress Test for Consistency**: Implement a simulation where a student's behavior is slightly perturbed. Run predictor + explainer. Does the explanation change drastically or minimally?

## Open Questions the Paper Calls Out

### Open Question 1
Can MultiModN's sequential multimodal fusion be adapted to domains with non-sequential, highly heterogeneous data modalities (e.g., social media, financial time-series)? The thesis focuses on structured time-series and tabular data, but real-world domains may require different fusion strategies.

### Open Question 2
Does the actionability of iLLuMinaTE's theory-driven explanations generalize across diverse cultural and educational contexts? The user study involved university students but did not explicitly test cross-cultural validity or whether Western educational norms perform equally well for non-Western students.

### Open Question 3
Can InterpretCC's sparse, interpretable mixture-of-experts architecture scale to real-time inference with low latency for large educational platforms? The thesis discusses computational efficiency but does not benchmark latency or throughput for real-time use cases like instant feedback in learning management systems.

### Open Question 4
How do long-term educational outcomes compare between students who receive XAI-based interventions versus those who receive generic feedback? The thesis simulates actionability but lacks longitudinal data on whether XAI-driven feedback fosters self-regulated learning habits that persist beyond a single course.

## Limitations
- Core datasets (student clickstreams) are not publicly available, making exact reproduction impossible without institutional access
- Results from MOOC settings may not transfer directly to K-12 or workplace learning contexts with different constraints
- The iLLuMinaTE framework's reliance on large language models introduces potential for subtle misalignment or "hallucination" not quantified

## Confidence

### Major Uncertainties
- Data accessibility: Core datasets are not publicly available
- Generalizability: MOOC results may not transfer to other educational contexts
- LLM dependency: iLLuMinaTE's reliance on LLMs introduces unquantified risks

### Confidence Labels
- **High**: The core finding that intrinsic, interpretable-by-design models produce more trustworthy and actionable explanations than post-hoc methods
- **Medium**: The necessity of theory-driven explanation communication and educators' unique sensitivity to AI inconsistency
- **Medium**: The assertion that educators are uniquely sensitive to AI inconsistency compared to human inconsistency

## Next Checks
1. **Cross-domain consistency test**: Apply InterpretCC and post-hoc explainers to K-12 LMS data and measure whether the trust/actionability gap persists
2. **Controlled comparison of explanation formats**: Design a randomized controlled trial where educators receive either iLLuMinaTE-style explanations or well-crafted but theory-agnostic narratives
3. **Longitudinal trust assessment**: Conduct a multi-month deployment where the same educators interact with consistent vs. inconsistent AI explanations, measuring trust development over time