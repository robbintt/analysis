---
ver: rpa2
title: 'RPP: A Certified Poisoned-Sample Detection Framework for Backdoor Attacks
  under Dataset Imbalance'
arxiv_id: '2602.00183'
source_url: https://arxiv.org/abs/2602.00183
tags:
- backdoor
- samples
- imbalance
- detection
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of backdoor attacks in deep learning
  models under dataset imbalance. It introduces Randomized Probability Perturbation
  (RPP), a certified poisoned-sample detection framework that operates in a black-box
  setting using only model output probabilities.
---

# RPP: A Certified Poisoned-Sample Detection Framework for Backdoor Attacks under Dataset Imbalance

## Quick Facts
- **arXiv ID**: 2602.00183
- **Source URL**: https://arxiv.org/abs/2602.00183
- **Reference count**: 40
- **Primary result**: Certified poisoned-sample detection under dataset imbalance with provable FPR bounds and higher TPR than state-of-the-art defenses.

## Executive Summary
This paper introduces Randomized Probability Perturbation (RPP), a certified poisoned-sample detection framework for backdoor attacks under dataset imbalance. RPP operates in a black-box setting using only model output probabilities, quantifying the stability of prediction probabilities under random noise and calibrating detection thresholds via conformal prediction. The framework provides provable within-domain detectability guarantees and a probabilistic upper bound on the false positive rate. Extensive experiments on five benchmarks (MNIST, SVHN, CIFAR-10, TinyImageNet, ImageNet10) covering 10 backdoor attacks and 12 baseline defenses show that RPP achieves significantly higher detection accuracy than state-of-the-art defenses, particularly under dataset imbalance.

## Method Summary
RPP trains a preliminary model on the suspicious dataset, then computes an empirical probability perturbation score (RPP) for each sample by measuring the $L_\infty$ norm difference in model output probabilities under isotropic Gaussian noise. A clean calibration set is used to set a detection threshold via split conformal prediction, ensuring a bounded false positive rate. The framework provides certification bounds when trigger sizes exceed specific thresholds derived from randomized smoothing theory.

## Key Results
- RPP achieves significantly higher detection accuracy than state-of-the-art defenses under dataset imbalance
- Provides provable within-domain detectability guarantees with bounded false positive rate
- Maintains detection performance across 10 different backdoor attacks on 5 benchmark datasets
- Effectively handles severe class imbalance ratios (ρ = 2, 10, 100, 200)

## Why This Works (Mechanism)

### Mechanism 1: Stability Differential via Trigger Robustness
- **Claim**: Poisoned samples exhibit lower variance in prediction probabilities under noise compared to clean samples.
- **Mechanism**: Backdoor triggers are engineered to be robust features that dominate the model's decision logic. When isotropic Gaussian noise is injected, the "trigger signal" persists, keeping the probability vector stable. Clean samples rely on natural features which are more susceptible to noise, causing larger shifts in the probability vector.
- **Core assumption**: Assumption (A.1) holds: the probability of predicting the target class decreases when noise is added to a triggered sample.
- **Evidence anchors**: Section 4.1 and 4.2 formalize RPP as the expectation of the $L_\infty$ norm difference.
- **Break condition**: If the trigger magnitude $\|\delta\|_2$ is too small, the stability gap vanishes, making poisoned samples indistinguishable from clean ones.

### Mechanism 2: Conformal Threshold Calibration
- **Claim**: RPP maintains a bounded False Positive Rate (FPR) even under severe class imbalance by using a clean calibration set.
- **Mechanism**: Standard defenses use clustering or class-level statistics which skew when majority classes dominate. RPP utilizes split conformal prediction on a small, clean calibration set $V$ to set the detection threshold $\hat{q}$.
- **Core assumption**: The calibration set $V$ is drawn i.i.d. from the same distribution as the clean test data.
- **Evidence anchors**: Section 5.2 derives the upper bound for FPR; Section 4.3 discusses the self-normalizing property.
- **Break condition**: If the calibration set is contaminated with poisoned samples or is not representative of the test distribution, the FPR guarantee fails.

### Mechanism 3: Certified Detectability Bounds
- **Claim**: The framework provides a mathematical guarantee that a poisoned sample will be detected provided the trigger size exceeds a specific bound.
- **Mechanism**: The paper adapts randomized smoothing theory to backdoor detection. By modeling the impact of Gaussian noise on the classifier's decision boundary, Theorem 5.1 derives a radius $R$.
- **Core assumption**: The noise follows an isotropic Gaussian distribution; the classifier $f$ is deterministic.
- **Evidence anchors**: Section 5.1 states the condition $\|\delta\|_2 \geq \sigma (\Phi^{-1}(p(x) - \zeta) - \Phi^{-1}(p_t))$.
- **Break condition**: If the "preliminary model" training phase fails to learn the backdoor (low ASR), the certification logic does not apply.

## Foundational Learning

- **Concept: Conformal Prediction**
  - **Why needed here**: Used to rigorously define the detection threshold ($\hat{q}$). Unlike heuristic thresholds, this provides the "certified" FPR bound essential for safety-critical applications.
  - **Quick check question**: How does the size of the calibration set $n$ affect the tightness of the FPR bound? (Answer: Larger $n$ tightens the bound towards the significance level $\alpha$).

- **Concept: Dataset Imbalance & Long-Tailed Distributions**
  - **Why needed here**: Explains the failure mode of baseline defenses (AC, SS) which rely on global statistics that majority classes bias.
  - **Quick check question**: Why does a minority class sample look like a poisoned sample to a clustering defense? (Answer: It appears as an outlier in feature space due to lack of representation, not malicious intent).

- **Concept: Black-box Model Access**
  - **Why needed here**: RPP operates only on "model output probabilities" (the logits/softmax output), not gradients or internal weights. This is crucial for deploying the defense on APIs or closed-source models.
  - **Quick check question**: Does RPP require access to the training loss or gradients? (Answer: No, it requires only inference-time probability outputs).

## Architecture Onboarding

- **Component map**: Preliminary Trainer -> Noise Injector -> RPP Calculator -> Conformal Calibrator -> Certifier
- **Critical path**: The **Preliminary Trainer** is the critical dependency. If the preliminary model does not achieve a sufficiently high Attack Success Rate (ASR), the backdoor feature is not learned, and the stability differential will not exist.
- **Design tradeoffs**:
  - **Noise Magnitude ($\sigma$)**: A higher $\sigma$ improves detection separation but risks destroying the semantic content of the image, potentially degrading the clean accuracy of the preliminary model.
  - **Calibration Set Size ($n$)**: Smaller sets (e.g., $n=100$) are efficient but result in a looser FPR bound ($\alpha + 1/(n+1)$) compared to larger sets.
- **Failure signatures**:
  - **High FPR (False Alarms)**: Likely indicates the calibration set $V$ is not i.i.d. with the clean test data, or the noise level $\sigma$ is too low for the specific dataset complexity.
  - **Low TPR (Missed Attacks)**: Suggests the trigger size $\|\delta\|_2$ is below the certification threshold $R$, or the preliminary model was under-trained (ASR < 50%).
- **First 3 experiments**:
  1. **Reproduce Figure 2 (Distribution Separation)**: Plot the histograms of $\tilde{\Delta}P$ for clean vs. poisoned samples on a balanced dataset to verify the stability gap exists before implementing the full threshold logic.
  2. **Ablation on Imbalance Ratio ($\rho$)**: Run the full RPP pipeline on CIFAR-10 with increasing imbalance ratios ($\rho=1, 10, 100, 200$) to confirm FPR stability compared to baselines like AC or SCALE-UP (Table 2).
  3. **Certification Boundary Test**: Validate Theorem 5.1 by intentionally shrinking the trigger size $\|\delta\|_2$ to see if detection performance drops precisely when the size falls below the calculated threshold $R$.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the theoretical certification of RPP be extended to encompass non-Gaussian or correlated perturbations?
  - **Basis in paper**: [explicit] The "Limitations & Future Work" section states, "Our theoretical analysis relies on the assumption of i.i.d. Gaussian perturbations. Extending this analysis to encompass non-Gaussian or correlated perturbations could offer a more comprehensive understanding."
  - **Why unresolved**: The current proofs rely on the specific statistical properties of isotropic Gaussian noise.
  - **What evidence would resolve it**: Derivation of certification bounds that hold under alternative noise distributions or correlated noise injection strategies.

- **Open Question 2**: Is RPP effective against multi-trigger or multi-target backdoor attacks?
  - **Basis in paper**: [explicit] Section 3 states, "In this paper, we do not consider multi-trigger or multi-target backdoors... as even empirical detection remains challenging."
  - **Why unresolved**: The current framework evaluates the probability perturbation regarding a single trigger and target class context.
  - **What evidence would resolve it**: Empirical evaluation of RPP's detection accuracy (TPR/FPR) and certification bounds on datasets containing multiple distinct triggers or multiple target classes.

- **Open Question 3**: Can the RPP framework be adapted to defend against model-manipulation backdoors where the attacker controls the training process?
  - **Basis in paper**: [explicit] Section 2 clarifies the scope: "In this work, we do not consider such model manipulation backdoor," focusing instead on data-poisoning attacks.
  - **Why unresolved**: RPP relies on identifying triggers via model output probabilities, but model-manipulation attacks may implant backdoors through mechanisms that RPP's perturbation analysis does not cover.
  - **What evidence would resolve it**: Theoretical analysis or experimental results applying RPP to models compromised via training-process manipulation.

## Limitations

- The theoretical guarantees are strictly conditional on trigger size exceeding the certification threshold $R$.
- The calibration set must be clean and i.i.d., an assumption that may be violated if the original dataset is partially poisoned.
- RPP's black-box nature limits potential optimizations compared to white-box methods.

## Confidence

- **High Confidence**: The empirical superiority of RPP under dataset imbalance, the validity of the conformal FPR bound, and the core stability differential mechanism.
- **Medium Confidence**: The certification threshold $R$ from Theorem 5.1 is derived under ideal conditions; its practical applicability to real-world attacks with smaller triggers requires further validation.
- **Low Confidence**: The exact trigger specifications (size, pattern details) for all 10 attacks are not fully specified, creating ambiguity for exact reproduction.

## Next Checks

1. **Trigger Magnitude Test**: Systematically vary the trigger size $\|\delta\|_2$ for a given attack (e.g., BadNets) and plot TPR vs. $\|δ\|_2$ to empirically validate the existence of the certification boundary predicted by Theorem 5.1.

2. **Calibration Contamination Test**: Intentionally inject a small percentage of poisoned samples into the calibration set $V$ and measure the degradation in the FPR bound to quantify the robustness of the conformal calibration.

3. **Noise Sensitivity Analysis**: Conduct a parameter sweep over the noise level $\sigma$ for a complex dataset (e.g., TinyImageNet) to find the optimal tradeoff between detection performance and clean model accuracy, beyond the fixed values used in the paper.