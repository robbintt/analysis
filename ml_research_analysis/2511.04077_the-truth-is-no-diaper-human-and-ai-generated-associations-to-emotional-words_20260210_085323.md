---
ver: rpa2
title: 'The truth is no diaper: Human and AI-generated associations to emotional words'
arxiv_id: '2511.04077'
source_url: https://arxiv.org/abs/2511.04077
tags:
- human
- associations
- responses
- words
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares human and large language model (LLM) word associations,
  focusing on emotionally loaded words. The authors analyze 40 emotionally charged
  Slovenian words, collecting human associations from the SWOW-SL dataset and generating
  associations using three LLMs (Llama-3.3, GaMS-9B, and Claude 3.7 Sonnet).
---

# The truth is no diaper: Human and AI-generated associations to emotional words

## Quick Facts
- arXiv ID: 2511.04077
- Source URL: https://arxiv.org/abs/2511.04077
- Authors: Špela Vintar; Jan Jona Javoršek
- Reference count: 23
- Key outcome: LLM-generated associations show moderate overlap with human ones, amplify emotional load, and are less creative than human responses

## Executive Summary
This study compares word associations generated by humans and three large language models (LLMs) for 40 emotionally charged Slovenian words. The research finds that while LLM associations overlap moderately with human ones, they tend to be more predictable, less creative, and amplify the emotional sentiment of cue words. Human responses show greater variability, including unexpected wordplay and cultural references. The findings suggest that current LLMs capture only a fraction of human associative behavior, with departures from expected associations remaining distinctive traits of human responses.

## Method Summary
The study analyzed 40 emotionally loaded Slovenian words selected from the SWOW-SL dataset based on extreme valence scores. Human associations were drawn from this dataset, while three LLMs (Llama-3.3, GaMS-9B, and Claude 3.7 Sonnet) were prompted zero-shot to generate three associations each per cue word. Response overlap was computed by intersecting sets, and sentiment analysis was performed using the SloEmoLex lexicon. Manual analysis categorized responses by creativity type (meaning-based, position-based, form-based, erratic).

## Key Results
- LLM associations overlap moderately with human ones (Claude achieved highest at 23.33%, human-human overlap ~40%)
- LLM responses amplify emotional sentiment more strongly than human responses
- Human associations show greater creativity and variability, including unexpected wordplay and cultural references
- LLM responses predominantly follow paradigmatic patterns (synonyms, same POS) rather than syntagmatic ones

## Why This Works (Mechanism)

### Mechanism 1: Paradigmatic Association Dominance in LLMs
- Core assumption: Training data regularity and instruction-tuning objectives penalize low-probability (creative/idiosyncratic) outputs
- Evidence: LLM responses "typically following grammatical categories and semantic domains" with fewer unique responses than humans (Lavorati et al. 2024)
- Break condition: Increasing temperature or prompting for syntagmatic associations

### Mechanism 2: Sentiment Amplification via Polarity Consistency
- Core assumption: RLHF may have implicitly rewarded sentiment-consistent responses as more coherent
- Evidence: Tables 3-4 show Llama/Claude generate higher valence for positive cues (0.78 vs 0.73 human) and lower for negative cues (0.29 vs 0.37 human)
- Break condition: Prompting for "opposites" or contrasting associations

### Mechanism 3: Model Scale Correlates with Human Alignment
- Core assumption: Scale improves coverage of "typical" associations but not experiential/memory-based association mechanisms
- Evidence: Claude 3.7 Sonnet achieved highest overlap (23.33%) among tested models
- Break condition: Diminishing returns expected; cannot exceed ~40% (human-human overlap ceiling)

## Foundational Learning

- **Concept: Paradigmatic vs. Syntagmatic Associations**
  - Why needed: Core framework for understanding why LLM responses cluster in predictable patterns
  - Quick check: Given cue "happy," is response "joyful" (paradigmatic) or "childhood" (syntagmatic)?

- **Concept: Valence/Arrousal/Dominance in Lexical Semantics**
  - Why needed: Paper uses valence scores to quantify sentiment amplification
  - Quick check: If a cue has valence 0.8, what would amplified sentiment look like in generated associations?

- **Concept: Human Association Variability**
  - Why needed: Human-human overlap typically ≤40%; evaluating LLM "accuracy" requires recognizing this ceiling
  - Quick check: Why might 23% overlap between Claude and humans actually be meaningful performance?

## Architecture Onboarding

- **Component map:** 40 emotional cue words (SWOW-SL) -> Zero-shot prompt "What does X remind you of?" -> LLM responses -> Overlap computation + Sentiment labeling (SloEmoLex) -> Manual creativity categorization
- **Critical path:** Select cues via valence scores → Prompt each model for 3 associations → Compute overlap by intersecting response sets → Label responses with SloEmoLex valence → Analyze sentiment patterns
- **Design tradeoffs:** Zero-shot prompting preserves comparability but likely underestimates creative potential; 40-cue sample enables manual analysis but limits generalizability
- **Failure signatures:** LLM generates non-existent words (e.g., "opoziv" for rejection cue), produces English interference (e.g., "goldman" for "golden"), or responses completely unrelated to cue
- **First 3 experiments:**
  1. Replicate with increased temperature (0.7→1.2) to test whether variability approaches human levels
  2. Add prompt variant: "What is the OPPOSITE of what X reminds you of?" to measure antonym generation rates
  3. Extend cue set to 200+ words (balanced across valence spectrum) to validate sentiment amplification across mid-range valence cues

## Open Questions the Paper Calls Out

- **Question:** Do distinct LLMs exhibit systematic differences in fine-grained association patterns beyond simple paradigmatic dominance?
  - Basis: Authors observed differences but did not quantitatively evaluate response type distributions across models
  - What evidence would resolve it: Statistical analysis applying Fitzpatrick's response categories to full output sets

- **Question:** Can specific prompt engineering strategies induce LLMs to produce syntagmatic and creative associations comparable to human variability?
  - Basis: Study used zero-shot prompting while acknowledging potential for creative prompts
  - What evidence would resolve it: Comparative experiments using prompts explicitly instructing novel, erratic, or context-dependent associations

- **Question:** Is avoidance of antonym generation the primary driver of stronger sentiment amplification in LLM associations?
  - Basis: Authors hypothesize lack of antonyms contributes to sentiment exaggeration
  - What evidence would resolve it: Analysis comparing antonym frequency in human vs. LLM response sets and correlating with valence scores

## Limitations

- Small sample of 40 cue words restricts generalizability to broader emotionally loaded vocabulary
- Zero-shot prompting likely underestimates LLM creative potential compared to few-shot or specialized prompts
- Morphological richness of Slovenian may artificially depress overlap scores without lemmatization

## Confidence

- **High confidence**: Human associations show greater creativity and variability than LLM outputs
- **Medium confidence**: Sentiment amplification mechanism where LLMs reinforce polarity more than humans
- **Medium confidence**: Model scale correlation with human alignment, though gaps remain to human-human overlap

## Next Checks

1. **Morphological normalization**: Recompute overlap scores after lemmatizing both human and LLM responses to account for Slovenian inflectional morphology
2. **Prompt variation testing**: Compare zero-shot results against few-shot prompts that explicitly request syntagmatic or creative associations
3. **Sentiment analysis robustness**: Validate SloEmoLex coverage by checking proportion of generated responses with available valence scores