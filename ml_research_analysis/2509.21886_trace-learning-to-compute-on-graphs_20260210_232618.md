---
ver: rpa2
title: 'TRACE: Learning to Compute on Graphs'
arxiv_id: '2509.21886'
source_url: https://arxiv.org/abs/2509.21886
tags:
- learning
- circuit
- graph
- function
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TRACE introduces a new paradigm for learning functional behavior
  on circuit graphs. It addresses the architectural mismatch in MPNNs and Transformers
  that fail to capture position-aware, hierarchical computation.
---

# TRACE: Learning to Compute on Graphs
## Quick Facts
- arXiv ID: 2509.21886
- Source URL: https://arxiv.org/abs/2509.21886
- Reference count: 40
- Primary result: Hierarchical Transformer outperforms MPNNs and Transformers on circuit graph function prediction

## Executive Summary
TRACE introduces a novel framework for learning to compute functional behavior on circuit graphs. The method addresses architectural limitations in existing MPNNs and Transformers that fail to capture position-aware, hierarchical computation. TRACE employs a Hierarchical Transformer that processes computation steps as ordered sequences, enabling operator-specific interactions, and introduces function shift learning to decouple local and global function prediction. Experiments across RTL, AIG, and PM netlist modalities show significant performance improvements over prior models.

## Method Summary
TRACE proposes a Hierarchical Transformer architecture designed specifically for circuit graphs. The model processes computation steps as ordered sequences rather than treating graphs as unordered structures, allowing for operator-specific interactions. A key innovation is function shift learning (FSL), which decouples local and global function prediction. The framework operates on three circuit modalities: RTL, AIG, and PM netlists. The approach handles cyclic graphs by treating flip-flops as pseudo-primary inputs and removing feedback edges, though this simplification may not capture all temporal dependencies in complex sequential circuits.

## Key Results
- Rec@1 scores: 94.45% (RTL), 92.68% (AIG), 90.81% (PM netlists) in contrastive tasks
- MAE for logic-1 probability prediction: 0.015 on AIGs
- Significant improvements over MPNN and Transformer baselines across all tested circuit modalities

## Why This Works (Mechanism)
The Hierarchical Transformer captures position-aware, hierarchical computation that traditional MPNNs and Transformers miss. By processing computation steps as ordered sequences rather than unordered graph structures, the model can model operator-specific interactions more effectively. The function shift learning objective decouples local and global function prediction, allowing the model to better handle the hierarchical nature of circuit computation where local gate operations aggregate into global circuit behavior.

## Foundational Learning
- Circuit graph representations (RTL, AIG, PM netlists): Essential for understanding the three modalities tested
- Message Passing Neural Networks: The baseline architecture that TRACE improves upon
- Transformer architectures: Understanding the evolution from standard Transformers to hierarchical variants
- Graph isomorphism and position awareness: Why standard graph neural networks struggle with circuit computation
- Contrastive learning objectives: The Rec@1 metric measures ranking performance
- Mean Absolute Error and R-squared: Regression metrics for function prediction tasks

## Architecture Onboarding
**Component map**: Circuit graph -> Hierarchical Transformer (Ordered sequence processing) -> Function Shift Learning -> Local and Global function predictions
**Critical path**: Graph input → Hierarchical layer processing → FSL objective → Combined prediction
**Design tradeoffs**: Ordered sequence processing improves position awareness but may increase computational complexity compared to unordered graph approaches
**Failure signatures**: Performance degradation on circuits with complex feedback structures due to simplified cyclic graph handling
**Three first experiments**:
1. Ablation study removing Hierarchical Transformer layers to validate their contribution
2. Testing function shift learning component in isolation
3. Evaluation on simple acyclic circuits before progressing to complex sequential designs

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can TRACE generalize to computational graph domains beyond electronic circuits, such as software control flow graphs, abstract syntax trees, or hardware model-checking formulas in Btor2 format?
- Basis in paper: The conclusion states the framework has "potential applications across various domains," and Section 3.1 explicitly notes "the generalizability of our approach to other sequence-based problems, such as hardware model-checking."
- Why unresolved: All experiments are limited to RTL, AIG, and PM netlist circuit modalities; no evaluation on other computational graph types is provided.
- What evidence would resolve it: Benchmarks on non-circuit computational graphs (e.g., program analysis tasks, SAT/SMT formula reasoning) showing comparable performance gains over baselines.

### Open Question 2
- Question: What are the computational and memory costs of TRACE compared to MPNN and Transformer baselines during training and inference?
- Basis in paper: The paper reports only accuracy metrics (Rec@k, MAE, R²) with no runtime, training time, or memory usage analysis, despite introducing a more complex hierarchical architecture.
- Why unresolved: Practical deployment in EDA workflows requires understanding efficiency tradeoffs, especially given the transformer's quadratic complexity with in-degree and level-by-level processing.
- What evidence would resolve it: Wall-clock time and GPU memory comparisons across circuit sizes; scaling analysis on industry-sized designs (100K+ nodes).

### Open Question 3
- Question: How does TRACE's performance degrade on sequential circuits with complex, deeply nested feedback structures beyond the simple pseudo-PI treatment?
- Basis in paper: Footnote 1 acknowledges that cyclic graphs are handled by treating flip-flops as pseudo-PIs and removing feedback edges—a simplification that may not capture intricate temporal dependencies in modern sequential circuits.
- Why unresolved: The evaluation on sequential AIGs shows strong results, but the feedback handling is simplistic; circuits with multi-cycle paths or complex state machines may pose challenges.
- What evidence would resolve it: Controlled experiments on synthetic or real circuits with varying feedback complexity, measuring MAE degradation as temporal dependency depth increases.

## Limitations
- All experiments limited to circuit graph domains (RTL, AIG, PM netlists); no evaluation on other computational graph types
- No computational efficiency or memory usage analysis despite introducing more complex hierarchical architecture
- Simplified treatment of cyclic graphs may not capture complex temporal dependencies in sequential circuits
- Within-modality training only; no cross-modality transfer learning experiments

## Confidence
- Empirical results accuracy: High
- Architectural novelty claims: Medium
- Function shift learning effectiveness: Medium
- Generalizability to other domains: Low (not tested)

## Next Checks
1. Evaluate TRACE on out-of-distribution circuit structures and larger netlists to assess generalization and scalability
2. Conduct ablation studies that isolate the contributions of the Hierarchical Transformer and function shift learning, including comparison to other hierarchical or position-aware graph architectures
3. Test TRACE's robustness to noisy, incomplete, or malformed netlists to determine practical applicability in real-world circuit design flows