---
ver: rpa2
title: The Multi-Query Paradox in Zeroth-Order Optimization
arxiv_id: '2509.15552'
source_url: https://arxiv.org/abs/2509.15552
tags:
- query
- zo-align
- zo-avg
- optimization
- convex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes the fundamental trade-off in zeroth-order
  optimization between the quality of gradient estimates and the number of optimization
  iterations under a fixed query budget. It proposes two multi-query estimators: a
  simple averaging method (ZO-Avg) and a principled projection alignment method (ZO-Align)
  derived from local surrogate minimization.'
---

# The Multi-Query Paradox in Zeroth-Order Optimization

## Quick Facts
- arXiv ID: 2509.15552
- Source URL: https://arxiv.org/abs/2509.15552
- Reference count: 40
- Primary result: Single-query per iteration is always optimal for ZO-Avg, while multi-query per iteration is generally better for ZO-Align, with full-subspace estimation being optimal in strongly convex and stochastic settings.

## Executive Summary
This paper resolves a fundamental trade-off in zeroth-order optimization: how to allocate a fixed query budget between estimator quality and iteration count. It proposes two multi-query estimators - a simple averaging method (ZO-Avg) and a principled projection alignment method (ZO-Align) derived from local surrogate minimization. The key theoretical result reveals a dichotomy: for ZO-Avg, single-query per iteration is always optimal, while for ZO-Align, using more queries per iteration is generally better. This theoretical finding is validated across multiple problem settings and is consistently observed in experiments, including high-dimensional LLM fine-tuning tasks where the gap between methods narrows due to the near-orthogonality of random directions.

## Method Summary
The paper analyzes zeroth-order optimization under a fixed query budget K, proposing two estimators: ZO-Avg (simple averaging of directional derivatives) and ZO-Align (projection alignment via local surrogate minimization). The estimators are compared across four problem settings (strongly convex, convex, non-convex, and stochastic) using one-sided finite differences with μ=10^{-6}. The analysis derives convergence rates showing ZO-Avg is optimal with q=1 queries per iteration, while ZO-Align benefits from larger q, with full-subspace estimation being optimal in strongly convex and stochastic settings. The methods are validated on classical problems at d=1000 and LLM fine-tuning tasks on SST-2 and CB using Qwen3-0.6B.

## Key Results
- Theoretical proof that ZO-Avg with q=1 queries per iteration is always optimal under fixed budget constraints
- ZO-Align estimator shows improved convergence with more queries per iteration, with full-subspace estimation being optimal in strongly convex and stochastic settings
- In high-dimensional regimes (d >> q), the theoretical advantage of ZO-Align over ZO-Avg collapses due to Gram matrix concentration
- Empirical validation across classical problems and LLM fine-tuning tasks consistently demonstrates the theoretical predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ZO-Align enables efficient multi-query optimization by constructing a geometrically consistent gradient estimate that improves linearly with the number of queries.
- Mechanism: The estimator projects the true gradient onto the span of the query directions by solving a local surrogate minimization problem. Specifically, it computes $\hat{g}_{ALG} = U(U^T U)^{-1} \Delta f$. This approach ensures that the projection of the estimate onto each query direction matches the observed directional derivative, aligning the update with the local curvature captured by the query subspace.
- Core assumption: The objective function is L-smooth, validating the use of a local quadratic upper bound as a surrogate.
- Evidence anchors:
  - [Abstract]: Proposes a "principled projection alignment method (ZO-Align) derived from local surrogate minimization."
  - [Section 3.3]: Derives the estimator by minimizing the quadratic upper bound $f(x-Uy)$ to find the optimal step within the column space of query directions.
  - [Corpus]: Weak direct evidence; corpus focuses on application-level multi-query (RAG) rather than estimator mechanics.
- Break condition: If the function is highly non-smooth or the Hessian varies wildly within the query subspace, the local quadratic surrogate may be a poor approximation, degrading alignment.

### Mechanism 2
- Claim: Simple averaging (ZO-Avg) fails to efficiently utilize additional queries because the cost of consuming the query budget outweighs the sublinear variance reduction.
- Mechanism: ZO-Avg computes $\hat{g}_{AVG} = \frac{1}{q} \sum \hat{g}_i$. While increasing the query count $q$ reduces estimator variance, the paper proves this reduction scales as $1/q$, whereas the cost in "iterations foregone" scales linearly with $q$. Consequently, the "value per query" is maximized at $q=1$.
- Core assumption: The total query budget $K$ is fixed and finite, creating a zero-sum trade-off between estimator quality and iteration count.
- Evidence anchors:
  - [Abstract]: "For ZO-Avg, single-query per iteration is always optimal."
  - [Section 4.2.1]: Analysis shows the marginal benefit of variance reduction is not worth the linear query cost, making $q=1$ optimal.
  - [Corpus]: No direct contradiction or support found in provided corpus summaries.
- Break condition: If the objective is extremely noisy such that a single query provides no signal, the variance reduction might become necessary regardless of theoretical budget efficiency.

### Mechanism 3
- Claim: In high-dimensional regimes ($d \gg q$), the theoretical advantage of ZO-Align over ZO-Avg collapses due to the concentration of measure.
- Mechanism: When dimensionality vastly exceeds the query batch size, the Gram matrix $U^T U$ concentrates near a scaled identity matrix ($d \cdot I$). This renders the inverse $(U^T U)^{-1}$ effectively constant, making the "projection alignment" mathematically equivalent to a rescaled simple average.
- Core assumption: Query directions are sampled i.i.d. from a standard Gaussian distribution.
- Evidence anchors:
  - [Abstract]: Mentions the gap narrows in high-dimensional LLM tasks due to "near-orthogonality of random directions."
  - [Section 5]: Formalizes that $U^T U \approx dI_q$ when $d \gg q$, causing ZO-Align to degenerate into a step direction colinear with ZO-Avg.
  - [Corpus]: Corpus mentions high-dimensional LLM fine-tuning (e.g., Paper 2529, 33820) but does not explicitly analyze this geometric concentration.
- Break condition: If query directions are structured or correlated rather than random Gaussian, the concentration to identity may not hold, preserving ZO-Align's advantage.

## Foundational Learning

- Concept: **Zeroth-Order (ZO) Optimization**
  - Why needed here: This is the core paradigm where gradients are unavailable and must be approximated via function queries.
  - Quick check question: Can you explain why ZO optimization requires a "smoothing parameter" $\mu$ and how it differs from automatic differentiation?

- Concept: **Finite-Difference Gradient Estimation**
  - Why needed here: Understanding how directional derivatives are approximated ($f(x+\mu u) - f(x)$) is necessary to grasp the source of the "estimation variance" the paper aims to reduce.
  - Quick check question: What is the trade-off between bias and variance when choosing the smoothing parameter $\mu$?

- Concept: **Query Budget Constraints**
  - Why needed here: The paper's central "paradox" relies on the constraint that total queries $K$ is fixed, forcing a choice between "few accurate steps" vs. "many noisy steps."
  - Quick check question: If the query budget were unlimited, would the "Multi-Query Paradox" still exist?

## Architecture Onboarding

- Component map: Query Sampler -> Function Oracle -> Estimator Core -> Optimizer Step
- Critical path:
  1. Sample $q$ random directions.
  2. Query function $f$ at $x + \mu u_i$ for all $i$.
  3. **Decision Branch:**
     - If ZO-Avg: Average the rescaled directional derivatives.
     - If ZO-Align: Form Gram matrix $U^T U$, invert it, and project observed derivatives.
  4. Apply update with step size $\eta$.

- Design tradeoffs:
  - **ZO-Avg vs. ZO-Align:** Use ZO-Align for low-dimensional problems ($d \approx q$) to maximize query efficiency; use ZO-Avg (or diagonal ZO-Align) for high-dimensional problems ($d \gg q$) to save compute, as the performance gap narrows.
  - **Computation:** ZO-Align requires inverting a $q \times q$ matrix (or diagonal approximation), adding overhead not present in ZO-Avg.

- Failure signatures:
  - **ZO-Avg with $q > 1$:** Theoretical analysis predicts this is strictly inefficient. You will see slower convergence per query compared to $q=1$.
  - **Full ZO-Align in High Dimensions:** Attempting exact matrix inversion for ZO-Align when $d$ is massive (e.g., LLMs) will be computationally expensive for negligible accuracy gain over ZO-Avg.

- First 3 experiments:
  1. **Validate ZO-Avg Limit:** Run ZO-Avg on a convex quadratic with fixed budget $K$, varying $q \in \{1, 10, 100\}$. Verify that $q=1$ yields the lowest final loss.
  2. **Validate ZO-Align Benefit:** Run ZO-Align on a stochastic convex problem (e.g., logistic regression) with $d=1000$. Verify that increasing $q$ improves convergence speed per query.
  3. **High-Dim Collapse:** Run both estimators on a high-dimensional task ($d > 10,000$) with small $q$. Verify that the performance curves for ZO-Align and ZO-Avg overlap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an adaptive query allocation strategy, where the number of queries per iteration $q_t$ changes dynamically, outperform the static optimal strategies identified (single-query for ZO-Avg, full-subspace for ZO-Align)?
- Basis in paper: [inferred] The paper optimizes the query allocation under fixed or uniform settings ($q_t=q$), determining that extremes are optimal, but does not explore time-varying sequences.
- Why unresolved: The theoretical proof relies on concavity arguments for fixed $q$; an adaptive schedule might navigate the variance-iteration trade-off more efficiently as the optimizer moves between regions of different curvature or noise.
- What evidence would resolve it: A convergence analysis or empirical demonstration showing a dynamic $q_t$ schedule achieving lower error than the static optimal $q$ under the same total budget $K$.

### Open Question 2
- Question: Can the geometric benefits of ZO-Align be preserved in the high-dimensional regime ($d \gg q$) through better estimation techniques than the diagonal approximation?
- Basis in paper: [inferred] The paper notes that ZO-Align degenerates to a scaled ZO-Avg in high dimensions due to the near-orthogonality of random directions, leading to nearly indistinguishable performance in LLM fine-tuning experiments.
- Why unresolved: The current theory suggests the advantage of ZO-Align vanishes as $d$ increases, limiting its applicability to modern large-scale problems where $d \gg q$.
- What evidence would resolve it: A modified estimator or preconditioning method that maintains a significant performance gap over ZO-Avg in high-dimensional settings without incurring the $O(q^2 d)$ cost of exact alignment.

### Open Question 3
- Question: Does the dichotomy between ZO-Avg and ZO-Align persist when using non-Gaussian query directions, such as structured orthogonal arrays or coordinate-wise perturbations?
- Basis in paper: [inferred] The theoretical results and MSE bounds depend specifically on the assumption that query directions are drawn i.i.d. from $N(0, I)$.
- Why unresolved: The properties of the Gram matrix ($U^\top U$) and the resulting error bounds might change significantly if directions are correlated or deterministic, potentially altering the "multi-query paradox."
- What evidence would resolve it: Derivation of convergence rates and optimal allocation strategies for ZO-Avg and ZO-Align using deterministic or non-Gaussian sampling schemes.

## Limitations
- Theoretical analysis relies heavily on idealized assumptions about L-smoothness and well-behaved Hessians
- Practical degradation of ZO-Align in high dimensions due to Gram matrix concentration is mathematically sound but may not capture structured query directions
- Paper does not explicitly address computational overhead beyond matrix inversion costs, which could be significant for very large query batches

## Confidence
- **High Confidence**: The theoretical proof that ZO-Avg with q=1 is always optimal under the fixed budget constraint, and the core mechanism of ZO-Align's projection alignment
- **Medium Confidence**: The empirical validation across classical problems and the theoretical prediction of ZO-Align's breakdown in high dimensions, though the LLM fine-tuning results support this
- **Low Confidence**: The exact numerical constants in the convergence bounds and the precise characterization of when the ZO-Avg vs. ZO-Align gap becomes negligible in practice

## Next Checks
1. **Validate Convergence Regimes**: Run ZO-Avg with q=1 versus q>1 on a strongly convex quadratic to confirm the predicted O(1/K) rate for q=1 and slower convergence for q>1
2. **Test High-Dimensional Collapse**: Implement both estimators on a synthetic d=10,000 problem with small q (e.g., q=10) and verify that the Gram matrix U^T U ≈ d*I causes performance convergence
3. **Assess Computational Overhead**: Measure and compare wall-clock time per iteration for ZO-Align (exact and diagonal) versus ZO-Avg across varying q to quantify the practical cost-benefit trade-off