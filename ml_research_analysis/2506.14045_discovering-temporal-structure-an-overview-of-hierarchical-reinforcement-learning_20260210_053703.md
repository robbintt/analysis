---
ver: rpa2
title: 'Discovering Temporal Structure: An Overview of Hierarchical Reinforcement
  Learning'
arxiv_id: '2506.14045'
source_url: https://arxiv.org/abs/2506.14045
tags:
- learning
- option
- state
- agent
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hierarchical reinforcement learning (HRL) addresses the challenge
  of learning in complex environments with long horizons by discovering and leveraging
  temporal structure. The core insight is that tasks can be decomposed into simpler,
  reusable subtasks (skills or options), enabling agents to learn more efficiently
  than flat reinforcement learning approaches.
---

# Discovering Temporal Structure: An Overview of Hierarchical Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2506.14045
- **Source URL**: https://arxiv.org/abs/2506.14045
- **Reference count**: 40
- **Primary result**: Hierarchical reinforcement learning discovers and leverages temporal structure to learn more efficiently in complex environments with long horizons.

## Executive Summary
Hierarchical reinforcement learning (HRL) addresses the challenge of learning in complex environments with long horizons by discovering and leveraging temporal structure. The core insight is that tasks can be decomposed into simpler, reusable subtasks (skills or options), enabling agents to learn more efficiently than flat reinforcement learning approaches. The paper surveys methods for autonomously discovering these temporal abstractions through three main categories: learning from online experience, leveraging offline datasets, and building on foundation models like large language models (LLMs). The benefits of HRL include improved exploration by targeting subgoals closer to the agent's current capabilities, more effective credit assignment by propagating errors at higher levels of abstraction, better transfer through reusable skills, and potential interpretability of agent decision-making.

## Method Summary
The paper surveys HRL methods that discover temporal abstractions without human intervention. Methods are categorized by their prior knowledge source: online experience (e.g., bottleneck discovery, spectral methods, empowerment), offline datasets (e.g., decision diagrams, successor representations), and foundation models (e.g., LLMs for reward shaping). Core components include option policy π(a|s,o), termination β(s,o), initiation I(s,o), high-level policy μ(o|s), and option model PO(s,o,s'). Discovery approaches include bottleneck identification via graph theory, spectral analysis using Laplacian methods, skill chaining from goals backward, empowerment maximization for diverse skill discovery, and LLM-based approaches for defining goal rewards or policies.

## Key Results
- Temporal abstraction improves credit assignment by propagating value information over multi-step "jumps" rather than single timesteps
- Discovering bottleneck states or diverse skills creates an implicit curriculum that makes long-horizon goals reachable
- Foundation models (LLMs) can inject semantic priors as reward functions or policies, bypassing sample-inefficient discovery phases
- Promising domains include web agents, robotics, and open-ended games like Minecraft where long horizons and structured goals enable temporal abstractions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Temporal abstraction improves credit assignment by propagating value information over multi-step "jumps" rather than single timesteps.
- **Mechanism**: By treating options as single units, the agent updates the value function based on state at option termination rather than immediate next state, bridging gap between sparse rewards and distant actions.
- **Core assumption**: Environment possesses underlying temporal structure that can be modularized; otherwise, abstraction introduces noise.
- **Evidence anchors**: [section 2.1] "value from rewarding events only needs to propagate along trajectories that pass through the bottleneck, greatly reducing the state-action pairs whose values need to be updated."
- **Break condition**: If option duration is too long or policy is highly stochastic, effective "jump" may span too much environment dynamics, making credit assignment signal too sparse or noisy.

### Mechanism 2
- **Claim**: Discovering "bottleneck" states or diverse skills creates an implicit curriculum that makes long-horizon goals reachable.
- **Mechanism**: Algorithms identify states connecting disparate parts of state space or maximize coverage. These states treated as intrinsic subgoals, generating dense reward signals guiding agent toward frontier of current competence.
- **Core assumption**: Intermediate states exist that are significantly more informative or accessible than final goal state.
- **Evidence anchors**: [section 4.1] "When bottleneck states are successfully identified and targeted... the agent often improves exploration, credit assignment, and transfer."
- **Break condition**: If proxy objective (e.g., maximizing novelty) correlates poorly with extrinsic task goal, agent may explore irrelevant parts of state space (the "noisy TV" problem).

### Mechanism 3
- **Claim**: Leveraging foundation models allows agents to bypass sample-inefficient discovery phase by injecting semantic priors as reward functions or policies.
- **Mechanism**: LLMs generate code or embedding similarity scores defining dense reward functions for specific skills, or directly output subgoals. Replaces blind search of skill space with human-interpretable semantic guidance.
- **Core assumption**: LLM's training data contains sufficient domain knowledge to map natural language or observation states to meaningful behavioral abstractions.
- **Evidence anchors**: [section 6] Describes using LLMs for "Reward as Code" or embedding similarity to guide policy learning.
- **Break condition**: If LLM "hallucinates" rewards for impossible states or generates code based on misunderstood physics, agent optimizes for non-existent or impossible objective.

## Foundational Learning

- **Concept: Semi-Markov Decision Processes (SMDPs)**
  - **Why needed here**: HRL operates over variable time scales. Must understand how Q-learning or Policy Gradients adapt when action takes τ steps rather than 1.
  - **Quick check question**: How does discount factor γ apply when option executes for 10 timesteps? (Answer: γ^10).

- **Concept: The Options Framework**
  - **Why needed here**: This is core vocabulary of HRL. Need to define intra-option policy π, termination condition β, and initiation set I to understand how agent transitions between abstract states.
  - **Quick check question**: What happens if option's termination condition β(s) is set to 1.0 for all states? (Answer: Option degenerates into primitive action, losing temporal abstraction).

- **Concept: Intrinsic Motivation / Empowerment**
  - **Why needed here**: To discover structure without external rewards, agents must define own objective functions (e.g., maximizing information gain or state coverage).
  - **Quick check question**: Why would agent maximize mutual information between its skills and states it reaches? (Answer: To ensure skills are diverse and distinguishable).

## Architecture Onboarding

- **Component map**: High-Level Policy (Manager) μ(o|s) → Low-Level Policies (Workers/Skills) π(a|s,o) → Termination Function β(s,o)
- **Critical path**:
  1. Define the Option Set (either via discovery or LLM prior)
  2. Train Low-Level Policies to achieve subgoals (intrinsic reward)
  3. Train High-Level Policy to sequence options for extrinsic task reward
- **Design tradeoffs**: There is a "Pareto frontier" between performance and sample efficiency. A deep hierarchy may offer better generalization but increases non-stationarity and tuning complexity. "Flat" agents might solve simple tasks faster.
- **Failure signatures**:
  - **Option Degeneracy**: Options become primitive actions (duration = 1) or only one option is ever used
  - **Non-stationarity**: High-Level policy destabilizes because Low-Level policies (the "environment" for manager) are simultaneously changing
  - **Premature Abstraction**: Agent learns abstract subgoals that do not connect to final goal, creating a "dead end" in the plan
- **First 3 experiments**:
  1. **Bottleneck Verification**: Implement simple grid-world with two rooms and doorway. Verify if discovery algorithm correctly identifies doorway as subgoal.
  2. **Ablation on Hierarchy Depth**: Compare flat Q-learner vs. 2-level HRL agent on sparse-reward task. Measure sample efficiency vs. final performance.
  3. **Transfer Test**: Train skills on Task A (e.g., "reach red key"), freeze them, and measure zero-shot performance on Task B (e.g., "open red door") by only retraining high-level policy.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Is there one type of "good" temporal structure that yields higher rewards across all environments, or does definition of good structure depend on specific problem settings?
- **Basis in paper**: [explicit] Introduction explicitly asks, "Is there one type of 'good' structure that yields higher rewards in all possible environments?" and notes it is still unclear how to define good structure.
- **Why unresolved**: While many methods discover useful structure, there is no unified theory explaining what constitutes optimal structure or in which specific environments HRL is guaranteed to outperform flat RL.
- **What evidence would resolve it**: Theoretical framework identifying specific environmental properties (e.g., modularity, compositionality) that predict when discovered hierarchy will improve sample efficiency or performance.

### Open Question 2
- **Question**: Can we develop algorithms that provide formal bounds on planning time without relying on restrictive assumption of "point options"?
- **Basis in paper**: [explicit] Section 4.6 asks, "Can we develop formal algorithms that bound planning time without needing the assumption of 'point options'?"
- **Why unresolved**: Current approximation algorithms with provable guarantees for minimizing planning time are limited to point options (which initiate and terminate in single states) and do not scale to multi-step options common in deep HRL.
- **What evidence would resolve it**: Derivation of planning time bounds or sample complexity guarantees for option discovery algorithms that apply to options with extended temporal duration or continuous state spaces.

### Open Question 3
- **Question**: How can factored skill discovery approaches, which currently rely on known state variables, be generalized to high-dimensional observation spaces like images?
- **Basis in paper**: [explicit] Section 4.9.1 identifies opportunity to "Generalize factored approaches to large observation spaces," noting methods like HEXQ assume access to explicit state variables.
- **Why unresolved**: Identifying relevant factors and causal dependencies in continuous, high-dimensional inputs (like pixels) without pre-specified domain knowledge remains significant challenge for maintaining transferability benefits of factored approaches.
- **What evidence would resolve it**: Algorithm that autonomously isolates causal features from high-dimensional data to define abstract subgoals, achieving transferability comparable to tabular factored MDP methods.

### Open Question 4
- **Question**: How should agent optimally balance environmental rewards and intrinsic option rewards during learning of option policies?
- **Basis in paper**: [explicit] Section 8.3 asks, "How should we balance between the option reward and the environmental reward?"
- **Why unresolved**: While some methods ignore environmental rewards to create specialized options and others use them heavily, thorough examination of trade-offs and optimal weighting strategies for combining these signals is lacking.
- **What evidence would resolve it**: Empirical results or theoretical bounds demonstrating specific weighting or curriculum strategy that optimizes both utility of learned skills and agent's task performance.

## Limitations
- Survey primarily describes methods conceptually rather than providing empirical comparisons or performance benchmarks
- Field evaluation shows relatively low average neighbor FMR (0.506) and zero citations, suggesting this may be emerging or under-connected research area
- Boundaries between method categories (online experience, offline datasets, foundation models) can be fuzzy in practice

## Confidence
- **High confidence**: Core mechanisms of credit assignment improvement through temporal abstraction and bottleneck-based exploration are well-established in literature and theoretically sound
- **Medium confidence**: Categorization of methods by prior knowledge source provides useful framework, though boundaries can be fuzzy in practice
- **Medium confidence**: Identified challenges around non-stationarity and option degeneracy are real concerns, but proposed mitigation strategies lack comprehensive empirical validation across diverse domains

## Next Checks
1. **Empirical comparison**: Implement and compare 2-3 bottleneck discovery methods (e.g., Q-Cut vs. eigenoptions) on standardized sparse-reward task like Montezuma's Revenge to quantify sample efficiency gains

2. **Cross-domain transferability test**: Train skills on one domain (e.g., gridworld) and evaluate zero-shot transfer to structurally similar but visually different environments (e.g., different grid layouts or room configurations)

3. **Non-stationarity stress test**: Systematically measure impact of concurrent learning across multiple HRL components by tracking option initiation set drift and high-level policy stability under different training schedules