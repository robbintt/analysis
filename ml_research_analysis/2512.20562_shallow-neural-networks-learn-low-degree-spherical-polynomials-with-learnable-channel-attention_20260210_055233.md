---
ver: rpa2
title: Shallow Neural Networks Learn Low-Degree Spherical Polynomials with Learnable
  Channel Attention
arxiv_id: '2512.20562'
source_url: https://arxiv.org/abs/2512.20562
tags:
- theorem
- follows
- learning
- neural
- channel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies learning low-degree spherical polynomials using\
  \ over-parameterized two-layer neural networks with channel attention. The problem\
  \ setup considers nonparametric regression on the unit sphere in Rd, where the target\
  \ function is a degree-\u21130 polynomial with \u21130 = \u0398(1) \u2265 1."
---

# Shallow Neural Networks Learn Low-Degree Spherical Polynomials with Learnable Channel Attention

## Quick Facts
- arXiv ID: 2512.20562
- Source URL: https://arxiv.org/abs/2512.20562
- Authors: Yingzhen Yang
- Reference count: 40
- Primary result: First finite-width neural network training approach achieving minimax optimality for learning low-degree spherical polynomials with feature learning capability

## Executive Summary
This paper presents a novel approach for learning low-degree spherical polynomials using over-parameterized two-layer neural networks with learnable channel attention. The method addresses nonparametric regression on the unit sphere in Rd, where the target function is a degree-ℓ0 polynomial. The key innovation is a two-stage training process that combines learnable channel selection with standard gradient descent to achieve improved sample complexity and theoretical guarantees.

## Method Summary
The proposed method employs a two-stage training process for over-parameterized two-layer neural networks. Stage 1 uses a learnable channel selection algorithm that identifies the ground-truth channel number ℓ0 among initial L ≥ ℓ0 channels using one-step gradient descent. Stage 2 trains the second-layer weights using standard gradient descent with the selected channels. This approach achieves sample complexity n ≍ Θ(dℓ0/ε) with probability 1-δ, significantly improving upon previous bounds and achieving minimax optimal nonparametric regression risk of order Θ(dℓ0/n).

## Key Results
- Achieves sample complexity n ≍ Θ(dℓ0/ε) with probability 1-δ
- Significantly improves upon previous bound of Θ(dℓ0 max{ε-2, log d})
- Achieves minimax optimal nonparametric regression risk of order Θ(dℓ0/n)
- First finite-width neural network training approach to achieve minimax optimality for this problem class

## Why This Works (Mechanism)
The method works by leveraging the specific structure of low-degree spherical polynomials and using a learnable channel attention mechanism to identify the relevant features. The two-stage training process first identifies the correct number of channels needed to represent the target polynomial, then uses standard gradient descent to learn the appropriate weights. This approach effectively learns the feature representation needed to approximate the target function while maintaining computational efficiency.

## Foundational Learning
- **Spherical Harmonics**: Basis functions for representing functions on the unit sphere; needed for understanding the target function class
- **Nonparametric Regression**: Statistical framework for learning without assuming a parametric model; quick check: verify assumptions about data distribution
- **Over-parameterized Networks**: Networks with more parameters than training samples; needed for representing complex functions
- **Channel Attention Mechanisms**: Methods for selecting relevant features/channels; quick check: verify convergence of channel selection
- **Minimax Optimality**: Theoretical framework for assessing learning algorithm performance; quick check: compare to theoretical lower bounds

## Architecture Onboarding
- **Component Map**: Input -> Channel Selection -> Second-layer Training -> Output
- **Critical Path**: Data → Channel Selection (Stage 1) → Weight Training (Stage 2) → Prediction
- **Design Tradeoffs**: 
  - More channels initially (L ≥ ℓ0) vs. computational efficiency
  - One-step vs. iterative channel selection
  - Standard vs. adaptive learning rates
- **Failure Signatures**: 
  - Incorrect channel number selection leading to poor generalization
  - Gradient vanishing/exploding during training
  - Sensitivity to initialization
- **First Experiments**:
  1. Test channel selection accuracy on synthetic spherical polynomials
  2. Compare sample complexity with baseline methods
  3. Evaluate robustness to noise and initialization

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical assumptions about data distribution on the unit sphere may not hold in practice
- Requirement that ℓ0 = Θ(1) ≥ 1 may limit applicability
- Computational cost of channel selection step not fully analyzed

## Confidence
- **High confidence**: Minimax optimality result and sample complexity bounds
- **Medium confidence**: Practical performance of two-stage training algorithm
- **Medium confidence**: Robustness of learnable channel attention mechanism

## Next Checks
1. Conduct empirical validation on synthetic and real-world spherical data to verify sample complexity improvements and robustness of channel attention mechanism
2. Analyze computational complexity of two-stage training algorithm, particularly channel selection step, and compare to standard approaches
3. Investigate sensitivity of learnable channel attention to initialization and noise under different data distributions on the unit sphere