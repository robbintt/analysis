---
ver: rpa2
title: On Linear Representations and Pretraining Data Frequency in Language Models
arxiv_id: '2504.12459'
source_url: https://arxiv.org/abs/2504.12459
tags:
- country
- linear
- person
- pretraining
- causality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relationship between pretraining data
  frequency and the emergence of linear representations in language models. The authors
  study how often subject-object pairs in factual relations co-occur in training data
  and find that linear representations form when these co-occurrences exceed specific
  frequency thresholds (e.g., 1k for subjects and 2k for objects).
---

# On Linear Representations and Pretraining Data Frequency in Language Models

## Quick Facts
- **arXiv ID:** 2504.12459
- **Source URL:** https://arxiv.org/abs/2504.12459
- **Reference count:** 40
- **Primary result:** Linear representations emerge when subject-object co-occurrences exceed specific frequency thresholds (e.g., 1k for subjects, 2k for objects) in pretraining data

## Executive Summary
This paper investigates the relationship between pretraining data frequency and the emergence of linear representations in language models. The authors study how often subject-object pairs in factual relations co-occur in training data and find that linear representations form when these co-occurrences exceed specific frequency thresholds (e.g., 1k for subjects and 2k for objects). They also show that this effect is independent of training stage and correlates strongly with in-context learning accuracy. Additionally, they demonstrate that measuring linear representation quality enables predicting term frequencies in pretraining data, even for unseen relations, using features like causality and faithfulness. The results suggest that manipulating training data frequency could control model behavior, and their tool for counting token co-occurrences supports further research in this area.

## Method Summary
The authors fit Linear Relational Embeddings (LREs) using a Jacobian-based approach on 8 few-shot examples per relation with 5-shot prompting. They extract subject hidden states at middle layers and object hidden states at final positions, compute Jacobian matrices, and average them for W and b matrices. The β scaling parameter is tuned via sweep for each relation. Causality (proportion of successful representation edits) and faithfulness (LRE prediction matches LM) metrics are computed alongside co-occurrence counting in tokenized training batches. Random Forest regression with 100 estimators predicts term frequencies from LRE quality metrics. The study uses OLMo-7B, OLMo-1B, and GPT-J models with relations from Hernandez et al. (2024).

## Key Results
- Linear representations emerge when subject-object co-occurrences exceed 1k for subjects and 2k for objects in pretraining data
- Causality and faithfulness metrics correlate strongly with in-context learning accuracy (Pearson r=0.82 for causality)
- Feature-based frequency prediction works for unseen relations using causality and faithfulness metrics
- The BatchSearch tool enables efficient co-occurrence counting in large pretraining corpora

## Why This Works (Mechanism)
The emergence of linear representations depends on the frequency with which subject-object pairs co-occur in pretraining data. When these co-occurrences exceed specific thresholds (1k for subjects, 2k for objects), the model develops linear geometric structures in its hidden states that enable direct manipulation of relational knowledge. This relationship appears independent of training stage and model scale, suggesting a fundamental property of how language models organize factual knowledge.

## Foundational Learning
- **Linear Relational Embeddings (LREs):** Geometric structures in hidden states that encode factual relations; needed to measure representation quality and predict model behavior.
- **Jacobian-based fitting:** Method for extracting linear transformations from model activations; needed to compute W and b matrices for LREs.
- **Causality metric:** Proportion of successful representation edits; needed to quantify how well LREs capture model behavior.
- **Faithfulness metric:** Agreement between LRE predictions and actual model outputs; needed to validate representation quality.
- **BatchSearch tool:** Efficient co-occurrence counting in tokenized training data; needed to measure pretraining frequency.
- **Random Forest regression:** Prediction of term frequencies from LRE metrics; needed to test generalization to unseen relations.

## Architecture Onboarding

**Component Map:** Models (OLMo-7B, OLMo-1B, GPT-J) -> Relations dataset -> LRE fitting -> Causality/Faithfulness computation -> Co-occurrence counting -> Frequency prediction

**Critical Path:** Load model and dataset → Fit LREs for each relation → Compute causality and faithfulness → Count co-occurrences → Train frequency predictor → Validate on held-out relations

**Design Tradeoffs:** The study uses factual relations where co-occurrence is a good proxy for relation mention, but this limits applicability to commonsense relations. They balance model coverage (3 models) with depth of analysis per relation.

**Failure Signatures:** Low causality despite high co-occurrence suggests layer selection issues; poor frequency prediction indicates missing important features or insufficient training data for the predictor.

**3 First Experiments:**
1. Load OLMo-7B and extract hidden states for a single relation to verify layer selection works
2. Fit LRE for one relation and compute causality/faithfulness to check representation quality
3. Count co-occurrences for one subject-object pair to validate BatchSearch tool

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does model scale quantitatively influence the specific co-occurrence frequency threshold required for linear representations to form?
- **Basis in paper:** [explicit] The authors observe different frequency thresholds for different model sizes (e.g., 1k for GPT-J vs. 4k for OLMo-1B) but state they "cannot draw conclusions from only three models" regarding how scale affects this threshold.
- **Why unresolved:** The study was limited to only three model configurations (OLMo-1B, OLMo-7B, GPT-J), preventing the establishment of a general scaling law.
- **What evidence would resolve it:** A systematic evaluation of models across a wider range of parameter scales (e.g., 125M to 70B) trained on the same data mixture to map parameter count to frequency thresholds.

### Open Question 2
- **Question:** Do similar frequency thresholds govern the formation of linear representations for commonsense or non-factual relations where subject-object co-occurrence is a weak proxy for relation mention?
- **Basis in paper:** [explicit] The authors focus on factual relations and explicitly state they "leave non-factual relations for future work," noting that co-occurrence counts do not convincingly capture all instances where commonsense relations are defined.
- **Why unresolved:** The counting method relies on subject-object proximity, which accurately tracks factual triplets (e.g., France-Paris) but fails to capture abstract associations (e.g., "researching history" implying "historian") where the terms rarely co-occur.
- **What evidence would resolve it:** Developing a semantic proxy for "relation mention" in non-factual contexts or conducting analysis on datasets annotated for commonsense relations to see if linearity correlates with exposure.

### Open Question 3
- **Question:** What computational mechanisms do language models use to solve factual recall tasks with high accuracy when linear representations fail to form?
- **Basis in paper:** [explicit] The authors identify relations like "star constellation name" which models solve with high accuracy (84%) but low linear causality (44%), noting it is an "open question how LMs are able to recall these tasks."
- **Why unresolved:** The linear representation hypothesis does not explain high performance in low-frequency/low-linearity scenarios, suggesting the existence of alternative, potentially non-linear internal algorithms.
- **What evidence would resolve it:** Probing for non-linear geometric structures (e.g., curvature) in the activation space for these specific tasks or analyzing attention head composition to identify alternative solution pathways.

## Limitations

- **Model and Relation Coverage:** Study relies on limited set of models (OLMo-7B, OLMo-1B, GPT-J) and 25 relations from a single dataset, limiting generalizability.
- **Frequency Threshold Specificity:** Identified thresholds (1k/2k) may be dataset-specific and vary with different pretraining corpora or tokenization schemes.
- **Causality vs. Faithfulness Gap:** Paper doesn't fully explore why some relations show high faithfulness but low causality, limiting understanding of when linear representations translate to behavior.

## Confidence

**High Confidence:**
- Linear representations emerge when subject-object co-occurrences exceed specific frequency thresholds
- Causality and faithfulness metrics correlate strongly with in-context learning accuracy
- Feature-based frequency prediction works for unseen relations using causality and faithfulness

**Medium Confidence:**
- The specific frequency thresholds (1k/2k) are universal across models and datasets
- Layer selection for representation extraction is robust across relations
- Random Forest regression with limited features captures most variance in frequency prediction

**Low Confidence:**
- Manipulating training data frequency could be used as a general tool for controlling model behavior
- The BatchSearch tool's scalability claims for very large corpora
- The independence of linear representation emergence from training stage

## Next Checks

1. **Cross-Dataset Validation:** Test the frequency thresholds and linear representation emergence on a completely different knowledge dataset (e.g., Wikidata relations) to verify the 1k/2k thresholds are not dataset-specific artifacts.

2. **Model Architecture Stress Test:** Evaluate whether the same frequency-threshold relationship holds for decoder-only models of different scales (1B to 70B parameters) and architectures (GPT, OPT, LLaMA families) to assess generalizability.

3. **Causal Intervention Experiment:** Design an intervention study where pretraining data frequency for specific relations is artificially manipulated, then measure the resulting changes in linear representation quality and in-context learning performance to directly test the causal relationship.