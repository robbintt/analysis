---
ver: rpa2
title: 'How Large Language Models Are Changing MOOC Essay Answers: A Comparison of
  Pre- and Post-LLM Responses'
arxiv_id: '2504.13038'
source_url: https://arxiv.org/abs/2504.13038
tags:
- language
- terms
- llms
- also
- chatgpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzes changes in student essay submissions from a
  free AI ethics MOOC before and after the release of ChatGPT. Using a dataset of
  56,878 essays from 3,582 participants, the research finds significant increases
  in essay length (from 150.5 to 230.1 tokens), reduced text complexity, and decreased
  vocabulary diversity post-ChatGPT.
---

# How Large Language Models Are Changing MOOC Essay Answers: A Comparison of Pre- and Post-LLM Responses

## Quick Facts
- arXiv ID: 2504.13038
- Source URL: https://arxiv.org/abs/2504.13038
- Authors: Leo Leppänen; Lili Aunimo; Arto Hellas; Jukka K. Nurminen; Linda Mannila
- Reference count: 38
- Primary result: Significant increases in essay length, reduced text complexity, and decreased vocabulary diversity post-ChatGPT, suggesting increased LLM usage for essay generation.

## Executive Summary
This study analyzes changes in student essay submissions from a free AI ethics MOOC before and after the release of ChatGPT. Using a dataset of 56,878 essays from 3,582 participants, the research finds significant increases in essay length (from 150.5 to 230.1 tokens), reduced text complexity, and decreased vocabulary diversity post-ChatGPT. Key LLM-associated terms like "delve" and "foster" show 10-fold increases in usage. While terminology related to AI technologies and ethics evolved substantially, overall essay topics remained largely unchanged. The results strongly suggest increased LLM usage for essay generation, raising concerns about academic integrity in MOOC environments and the broader implications for learning and cognitive effort.

## Method Summary
The study analyzes 56,878 English essays from 3,582 participants in an AI ethics MOOC (Nov 2020–Oct 2024), split into pre-ChatGPT (21,869 essays), first year post (12,782), and ≥1 year post (22,227) periods. Essays were preprocessed using NLTK tokenization, with spam/flagged submissions filtered out. The analysis computed style metrics (token count, sentence count, Flesch Reading Ease, Type-Token Ratio), relative prevalence of LLM-associated terms, and topic modeling via Gensim. Statistical comparisons between periods used Mann-Whitney U tests, while vocabulary shifts were analyzed through term frequency changes and dynamic topic models on open-ended prompts.

## Key Results
- Essay length increased significantly from 150.5 to 230.1 tokens post-ChatGPT
- Type-Token Ratio decreased from 0.617 to 0.577, indicating reduced vocabulary diversity
- LLM-associated terms "delve" and "foster" showed ~10-fold increases in usage

## Why This Works (Mechanism)

### Mechanism 1
LLM-assisted writing produces measurable stylistic signatures that differ from human baseline writing in vocabulary distribution, length, and complexity. LLMs have characteristic word preferences (e.g., "delve," "foster," "crucial") and tend to generate longer, syntactically regular text with lower lexical diversity than human writers. When students use these tools, their submissions inherit these statistical properties, creating a population-level signal.

### Mechanism 2
Availability of high-quality text generation tools enables cognitive offloading, reducing the metacognitive effort students invest in writing tasks. When tools can produce acceptable outputs with minimal effort, students substitute generation for composition, bypassing the cognitive processes (planning, drafting, revising) associated with learning. This manifests as longer essays with less vocabulary diversity—more text, less thought.

### Mechanism 3
LLMs exert a standardizing influence on terminology, favoring certain synonyms and phrasings over others in ways that propagate to student writing. LLMs have consistent preferences among near-synonyms (e.g., "recommendation system" vs. "recommender system"). As students use these tools, their vocabulary converges toward LLM preferences, creating measurable terminology shifts independent of topic changes.

## Foundational Learning

- **Type-Token Ratio (TTR)**
  - Why needed here: Core metric for detecting vocabulary diversity changes; paper relies on TTR to argue for LLM influence
  - Quick check question: Given a 200-token essay with 120 unique word types, what is the TTR? (Answer: 0.60)

- **Flesch Reading Ease Score**
  - Why needed here: Paper uses this to measure text complexity changes; higher scores = easier text
  - Quick check question: If essays shift from mean score 12.70 to 15.31, did text become more or less complex? (Answer: Less complex—higher scores indicate easier readability)

- **Mann-Whitney U Test**
  - Why needed here: Paper's statistical foundation for comparing pre/post distributions; non-parametric test appropriate for skewed text data
  - Quick check question: Why use Mann-Whitney U instead of t-test for essay length comparisons? (Answer: Essay lengths are likely non-normally distributed with heavy right skew)

## Architecture Onboarding

- **Component map:** Raw essays → Preprocessing (tokenization, sentence segmentation via NLTK) → Feature extraction (TTR, Flesch scores, word frequencies, sentence counts) → Temporal binning (monthly aggregation) → Statistical testing (Mann-Whitney U for distribution comparisons) → Topic modeling (Gensim LDA/dynamic topic models)

- **Critical path:** Tokenization accuracy → TTR calculation → Statistical significance testing. Errors in tokenization propagate to all downstream metrics.

- **Design tradeoffs:**
  - Monthly binning smooths noise but may obscure precise adoption timing (paper notes change visible ~March 2023, 4 months post-release)
  - Term-list approach for AI vocabulary is interpretable but may miss novel terms; topic modeling captures broader patterns but harder to interpret
  - Peer-review spam filtering reduces noise but may systematically remove certain LLM outputs if reviewers detect them

- **Failure signatures:**
  - Sudden drops in essay count may indicate platform changes rather than student behavior
  - Topic model coherence degradation across time periods suggests vocabulary drift beyond model capacity
  - Non-significant results for terms expected to change (e.g., "important") may indicate insufficient statistical power or that term is genuinely stable

- **First 3 experiments:**
  1. Replicate TTR and length analysis on a different MOOC corpus to test generalizability of the signature
  2. Train a classifier on pre/post essays using the identified features (length, TTR, LLM-associated words) to quantify detection accuracy
  3. Apply the same word-frequency methodology to a control domain where LLM usage is unlikely (e.g., hand-written exams) to establish baseline temporal drift

## Open Questions the Paper Calls Out
None

## Limitations
- Cannot definitively prove causation—alternative explanations (demographic shifts, curriculum changes, seasonal variation) cannot be fully ruled out
- Relies on stylistic proxies rather than direct detection methods or student self-reporting
- Term lists for AI/ethics vocabulary are manually curated and may miss emerging terminology

## Confidence
- **High Confidence**: Observed increases in essay length, decreases in vocabulary diversity (TTR), and shifts in LLM-associated word usage are robust statistical findings
- **Medium Confidence**: Interpretation that these changes reflect increased LLM usage is well-supported but not definitively proven
- **Low Confidence**: Claims about specific cognitive offloading or learning impacts are speculative without direct measures of student effort or learning outcomes

## Next Checks
1. Replicate the analysis on a different MOOC dataset with known demographic stability to test whether linguistic signatures persist across populations
2. Conduct a controlled experiment where students write essays with and without LLM assistance, measuring the same metrics to establish causal links
3. Apply the term-frequency methodology to a non-academic corpus (e.g., social media posts) from the same time period to establish baseline linguistic drift rates