---
ver: rpa2
title: 'Intention Collapse: Intention-Level Metrics for Reasoning in Language Models'
arxiv_id: '2601.01011'
source_url: https://arxiv.org/abs/2601.01011
tags:
- intention
- collapse
- reasoning
- pre-collapse
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces intention collapse as a framework for analyzing\
  \ how large language models compress high-dimensional internal states into discrete\
  \ token sequences. It proposes three model-agnostic metrics\u2014intention entropy,\
  \ effective dimensionality, and latent knowledge recoverability\u2014computed at\
  \ the pre-collapse boundary to quantify the geometry and information content of\
  \ internal states before generation begins."
---

# Intention Collapse: Intention-Level Metrics for Reasoning in Language Models

## Quick Facts
- arXiv ID: 2601.01011
- Source URL: https://arxiv.org/abs/2601.01011
- Reference count: 5
- Primary result: Chain-of-thought prompting improves free-response math but degrades multiple-choice abstract reasoning due to intention collapse.

## Executive Summary
This paper introduces intention collapse as a framework for analyzing how large language models compress high-dimensional internal states into discrete token sequences. It proposes three model-agnostic metrics—intention entropy, effective dimensionality, and latent knowledge recoverability—computed at the pre-collapse boundary to quantify the geometry and information content of internal states before generation begins. An empirical 3×3 study across three model families and three reasoning benchmarks reveals that chain-of-thought prompting is not universally beneficial: it strongly improves accuracy in free-response math but consistently degrades performance in multiple-choice abstract reasoning. CoT induces distinct entropy regimes across models, and probe-based recoverability can dissociate from final accuracy, particularly in multiple-choice settings, suggesting that informative internal signal may not be reliably expressed in the final discrete decision. These findings highlight the importance of response format and commitment reliability, and motivate state-dependent decoding and format-aware collapse policies as practical extensions.

## Method Summary
The study computes three intention-level metrics on pre-collapse states across a 3×3 design: three models (Mistral-7B-Instruct, Llama-3.1-8B-Instruct, Qwen-2.5-7B-Instruct), three benchmarks (GSM8K, ARC-Challenge, AQUA-RAT), and three inference regimes (Baseline, CoT, Babble). Hidden states are extracted at the last prompt position before the first output token using greedy decoding. Intention entropy H_int is computed from next-token distribution logits. Effective dimensionality d_eff is estimated via per-layer PCA participation ratio. Recoverability Recov uses linear probes (logistic regression with ℓ2 regularization) trained to predict correctness from concatenated hidden states, evaluated with AUROC and bootstrap confidence intervals. Items are split 60/20/20 for train/validation/test, with hyperparameters selected via validation AUROC.

## Key Results
- CoT effects are strongly format-dependent—improving accuracy in free-response math but degrading it in multiple-choice abstract reasoning.
- CoT induces distinct entropy regimes across models: Mistral shows ΔH < 0 (lower-entropy CoT), while LLaMA shows ΔH > 0 (higher-entropy CoT).
- Probe-based recoverability can dissociate from behavioral accuracy in multiple-choice settings, particularly in Qwen on ARC-Challenge.

## Why This Works (Mechanism)

### Mechanism 1
Pre-collapse intention states contain task-relevant information that may not survive projection into final output. Linear probes trained on hidden states extracted immediately before the first output token can predict eventual correctness above chance, even when behavioral accuracy degrades. This indicates that informative internal signal may not be reliably expressed in the final discrete decision.

### Mechanism 2
CoT effects are strongly format-dependent rather than universally beneficial. The reliability of discrete commitment depends on the output interface—free-response allows flexible expression while multiple-choice requires mapping to constrained option tokens. Accuracy drops under CoT in MCQ settings reflect commitment failures, not just reasoning failures.

### Mechanism 3
CoT induces model-family-specific entropy regimes, not a universal internal signature. The same prompting intervention produces opposite entropy shifts across architectures—Mistral reduces pre-collapse uncertainty while LLaMA increases it—reflecting training/architecture differences in how CoT conditions internal representations.

## Foundational Learning

- **Shannon entropy over vocabulary distributions**: H_int is computed as entropy over the full vocabulary at the pre-collapse boundary; interpreting it requires distinguishing "generative dispersion" from "decision uncertainty." Quick check: Why might entropy decrease even when the model is uncertain about the correct answer?

- **Linear probing and separability**: Recov(I;Z) uses logistic regression probes; understanding what linear separability implies (and doesn't) is essential for interpreting AUROC values. Quick check: What does it mean if train AUROC is 1.0 but test AUROC is 0.55?

- **PCA participation ratio**: d_eff summarizes how many dimensions meaningfully contribute to variance in the intention state; higher values suggest less constrained representations. Quick check: In a d >> N regime (hidden dimension >> sample size), what ceiling does this impose on participation ratio estimates?

## Architecture Onboarding

- **Component map**: Prompt + task -> Forward pass -> Extract hidden states at last prompt position before first token -> Compute H_int from logits, d_eff via PCA, Recov via linear probes -> Report metrics with confidence intervals

- **Critical path**: 1) Forward pass on full prompt (with regime-specific instruction) 2) Extract hidden states at last prompt position before first token selection 3) Compute metrics from cached activations (entropy from logits, PCA from hidden states) 4) Train probes with item-level splits (60/20/20), report AUROC with bootstrap CIs

- **Design tradeoffs**: Greedy vs. sampling: Greedy isolates prompt effects but excludes alternative collapse policies. Per-layer vs. concatenated PCA: Per-layer avoids heterogeneous space issues but loses cross-layer structure. Linear vs. nonlinear probes: Linear is conservative/interpretable but may underestimate information.

- **Failure signatures**: Probe AUROC at chance with label-shuffle also at chance: signal genuinely absent or features mis-specified. Large train-test gap (>0.3): overfitting in high-dimensional, low-sample regime. Negative ΔH without accuracy gain: entropy shift may reflect prompt artifacts, not reasoning improvement.

- **First 3 experiments**: 1) Reproduce extraction pipeline on n=50 items for one model/benchmark; verify H_int and d_eff computation against reported patterns 2) Run label-shuffle sanity check on probes; confirm shuffled AUROC ≈ 0.5 before trusting any above-chance results 3) Test cross-regime probe transfer (train Baseline → test CoT); significant degradation indicates representational shift under prompting

## Open Questions the Paper Calls Out

### Open Question 1
Does intervening on the intention state I to alter intention metrics causally improve the reliability of the final discrete output? The current study observes correlations but does not perform controlled interventions. Experiments using activation steering to manipulate H_int or Recov directly, followed by measurements of downstream changes in task accuracy, would resolve this.

### Open Question 2
How much task-relevant information is effectively lost during the projection from the pre-collapse state I to the final output? While the framework assumes the collapse operator is lossy, the specific magnitude of information loss has not been quantified. Training probes on "quirky" models to compare mutual information between pre-collapse state and ground truth versus final output and ground truth would resolve this.

### Open Question 3
Can the accuracy gains from Chain-of-Thought be disentangled from the mere increase in decoding compute? The "Babble" control matches verbosity but lacks semantic structure, making it difficult to distinguish whether gains stem from a richer pre-collapse state or simply additional processing steps. Comparing standard CoT against length-matched structured reasoning or compute-matched self-consistency baselines would isolate the effect of pre-collapse conditioning.

### Open Question 4
Can adaptive decoding policies based on intention metrics outperform fixed-temperature strategies? All reported experiments use greedy decoding; the hypothesis that models should explore more in high-uncertainty intention states remains untested. Fitting a policy to predict optimal temperature from I features and demonstrating statistically significant accuracy lifts over fixed-temperature baselines would resolve this.

## Limitations

- The study uses n=200 items per model–benchmark–regime cell, which may be insufficient for stable entropy and probe estimates in high-dimensional activation spaces.
- Linear probes can detect correlations but do not guarantee semantic or causal correspondence to the target label, limiting interpretability of high AUROC values.
- The observed degradation of CoT in multiple-choice formats could reflect token-level compliance or parsing issues rather than genuine reasoning limitations.

## Confidence

- **High**: The methodology for extracting intention states and computing intention entropy and dimensionality is well-specified and reproducible. The empirical finding that CoT is not universally beneficial—improving free-response math but degrading multiple-choice abstract reasoning—is robustly supported across models and benchmarks.
- **Medium**: The entropy regime interpretation (opposite shifts in Mistral vs. LLaMA) is plausible and grounded in measurable internal statistics, but may conflate prompt artifacts with architectural differences. The probe-based recoverability findings are statistically supported but limited by probe interpretability constraints.
- **Low**: Claims that CoT "induces" distinct regimes or that probe AUROC directly reflects internal knowledge are speculative without further validation. The dissociation between high probe recoverability and low behavioral accuracy in MCQ settings is suggestive but not conclusively explained.

## Next Checks

1. **Probe Robustness Test**: Conduct label-shuffle and cross-regime probe transfer experiments. If shuffled probes achieve AUROC ≈ 0.5 and cross-regime transfer fails, this would confirm that probe signals are regime- and task-specific.

2. **Format Control Experiment**: Run a direct comparison where MCQ items are answered in free-response format under both regimes. If CoT improves accuracy in free-response MCQ but not in constrained single-option MCQ, this would isolate format commitment effects from reasoning effects.

3. **Entropy Artifact Control**: Perform prompt ablation by comparing ΔH between regimes with matched prompt lengths and token distributions. If entropy shifts persist after controlling for lexical artifacts, the claim that ΔH reflects internal dispersion becomes more credible.