---
ver: rpa2
title: Scale-Dependent Semantic Dynamics Revealed by Allan Deviation
arxiv_id: '2601.21678'
source_url: https://arxiv.org/abs/2601.21678
tags:
- semantic
- text
- deviation
- allan
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces Allan deviation, a metrology tool, to analyze
  semantic coherence in written text by treating sentence embeddings as a displacement
  signal. Texts are segmented into sentences, mapped to semantic embeddings, and the
  resulting cumulative semantic phase is analyzed for scale-dependent fluctuations.
---

# Scale-Dependent Semantic Dynamics Revealed by Allan Deviation

## Quick Facts
- **arXiv ID:** 2601.21678
- **Source URL:** https://arxiv.org/abs/2601.21678
- **Reference count:** 29
- **Primary result:** Allan deviation analysis of sentence embeddings reveals two semantic regimes: short-time power-law scaling (α ≈ -0.4 for creative texts, -0.25 for technical) and a long-time noise floor defining a context horizon, with LLMs matching local scaling but exhibiting reduced stability horizons.

## Executive Summary
This study introduces Allan deviation, a metrology tool from atomic clock analysis, to quantify semantic coherence in written text by treating sentence embeddings as a displacement signal. Texts are segmented into sentences, mapped to semantic embeddings, and the resulting cumulative semantic phase is analyzed for scale-dependent fluctuations. Results reveal two distinct regimes: short-time power-law scaling, with exponents near -0.4 for creative literature (novels, drama, poetry) indicating weak local correlations, and -0.25 for technical texts indicating stronger correlations; and a long-time crossover to a stability-limited noise floor, defining a context horizon beyond which semantic variance plateaus. This horizon is smaller for technical texts than for creative ones. When applied to large language model outputs, all models reproduce human-like short-time scaling but exhibit reduced context horizons, suggesting quicker semantic convergence. The method offers a physics-based framework for quantifying semantic drift and coherence, differentiating human cognition from algorithmic generation.

## Method Summary
The method treats semantic progression as a stochastic trajectory by converting sentences to embeddings, computing pairwise cosine distances (angular increments), and accumulating these into a scalar semantic phase signal. Allan deviation is then computed across logarithmic scales to identify short-time power-law scaling and long-time noise floors. The scaling exponent α is extracted from short scales (τ ≤ 0.1N), while the context horizon is identified where the local slope deviates >15% from the short-time value. The approach uses sentence-transformer embeddings (primarily all-MiniLM-L6-v2) and applies to various text genres and LLM-generated outputs.

## Key Results
- Creative texts exhibit short-time scaling exponent α ≈ -0.4, while technical texts show α ≈ -0.25, indicating stronger local correlations in technical writing.
- Context horizon (where semantic variance plateaus) is larger for creative texts than technical texts, suggesting broader semantic exploration in creative literature.
- LLMs reproduce human-like short-time scaling exponents but exhibit systematically reduced context horizons (13-26 sentences vs. 37 for humans), indicating quicker semantic convergence.
- Sentence-order randomization eliminates genre distinctions, confirming that observed scaling arises from ordered semantic progression rather than embedding artifacts.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cumulative semantic phase preserves temporal ordering of meaning while enabling scale-independent fluctuation analysis.
- Mechanism: Sentence embeddings are converted to pairwise cosine distances (angular increments), then summed cumulatively to form a scalar signal φ(t). This transforms high-dimensional semantic motion into a one-dimensional stochastic process.
- Core assumption: Cosine distance between successive embeddings meaningfully captures semantic displacement independent of absolute position.
- Evidence anchors:
  - [abstract] "treat the semantic progression of written text as a stochastic trajectory in a high-dimensional state space"
  - [section S1] "This transformation converts high-dimensional semantic motion into a scalar stochastic process while preserving temporal ordering"
  - [corpus] No direct corpus validation; weak external support for this specific construction.
- Break condition: If embedding space has poor semantic locality, incremental distances become noise rather than meaningful displacement.

### Mechanism 2
- Claim: Allan deviation separates local semantic fluctuations from long-range drift, revealing two distinct dynamical regimes.
- Mechanism: Allan deviation σ(τ) measures variance of adjacent averages at scale τ. Power-law scaling σ(τ) ∼ τ^α indicates correlation structure; flattening indicates a noise floor.
- Core assumption: The cumulative phase signal exhibits stationary statistical properties amenable to variance decomposition.
- Evidence anchors:
  - [abstract] "two distinct dynamical regimes: short-time power-law scaling... and a long-time crossover to a stability-limited noise floor"
  - [Results] "a flattening of the Allan deviation curve indicates the emergence of a noise floor beyond which additional averaging fails to reduce variance"
  - [corpus] Related work on long-range correlations in text (Refs 5-9 cited), but no direct external validation of Allan deviation specifically for semantics.
- Break condition: If text lacks sufficient length or exhibits non-stationary statistics, power-law fits become unreliable.

### Mechanism 3
- Claim: Short-time scaling exponents differentiate text genres, while context horizon differentiates human from LLM-generated text.
- Mechanism: Creative texts have α ≈ -0.4 (weak local correlations); technical texts α ≈ -0.25 (stronger correlations). LLMs match short-time exponents but reach noise floor faster (reduced context horizon).
- Core assumption: Scaling exponents reflect intrinsic semantic organization rather than artifacts of embedding models or text length.
- Evidence anchors:
  - [abstract] "large language models successfully mimic the local scaling statistics of human text, they exhibit a systematic reduction in their stability horizon"
  - [Results, Table III-IV] Human α = -0.384±0.15, context horizon 37 sentences; LLMs α = -0.37 to -0.47, context horizon 13-26 sentences
  - [Results] "Sentence-order randomization eliminates these distinctions entirely... confirming that the observed scaling regimes arise from ordered semantic progression"
  - [corpus] Weak external validation; corpus papers address LLM coherence and gradient dynamics but not Allan deviation scaling.
- Break condition: If embedding models introduce systematic biases, genre separation could be artifact. (Paper claims multi-model robustness in S2.)

## Foundational Learning

- **Concept: Allan Deviation (Time-Domain Stability Analysis)**
  - Why needed here: This metrology tool from atomic clock analysis is repurposed to measure semantic stability. Without understanding how σ(τ) separates short-term noise from long-term drift, the two-regime finding is opaque.
  - Quick check question: For a random walk, Allan deviation scales as τ^(-1/2). What would α = -0.25 indicate about correlations?

- **Concept: Power-Law Scaling and Correlation Structure**
  - Why needed here: The paper's central claim is that scaling exponents encode semantic organization. α near -0.5 indicates uncorrelated increments; shallower slopes indicate persistence.
  - Quick check question: If semantic increments were perfectly anti-correlated (alternating high/low), would α be steeper or shallower than -0.5?

- **Concept: Sentence Embeddings and Cosine Distance**
  - Why needed here: The method's input is cosine distances between transformer embeddings. Understanding that this captures directional similarity (not magnitude) explains why the signal is called "semantic phase."
  - Quick check question: Two sentences with opposite meanings but similar word distributions—would cosine distance capture this distinction?

## Architecture Onboarding

- **Component map:** Raw text → Sentence tokenizer → Embedding model (all-MiniLM-L6-v2) → Pairwise cosine distances → Cumulative sum (semantic phase φ(t)) → Allan deviation computation across log-spaced τ values → Power-law fit (short τ) + noise floor detection (long τ)

- **Critical path:** Embedding quality and sentence segmentation are upstream dependencies. The cumulative phase construction is irreversible—errors in early distances propagate.

- **Design tradeoffs:**
  - Embedding model choice: Paper claims robustness across models (S2), but different models may shift absolute exponent values.
  - τ range selection: Paper uses τ ≤ 0.1N for fitting to avoid finite-size effects; this constrains applicability to short texts.
  - Context horizon threshold: 15% deviation from short-time slope is arbitrary; alternative thresholds change absolute horizon values.

- **Failure signatures:**
  - Scaling exponents clustering near -0.5 for all genres → suggests embeddings lack semantic discrimination or text is too short.
  - No noise floor emergence → text may be too short or genuinely scale-invariant (novels in paper show this).
  - High variance across texts within genre → possible data quality issues or genre definition too broad.

- **First 3 experiments:**
  1. Replicate on a single novel and single technical paper. Compute Allan deviation, extract α from τ ∈ [2, 0.1N]. Verify creative text has steeper slope.
  2. Shuffle sentence order in both texts. Confirm Allan deviation collapses toward -0.5 slope (random walk baseline).
  3. Generate 50 sentences with an LLM using an open-ended prompt. Compare context horizon to human-written text of same length.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do novels genuinely exhibit scale-invariant semantic organization, or does their context horizon simply lie beyond the measurable range due to finite corpus length?
- Basis in paper: [explicit] "Although effects of the finite size of our text collection cannot be excluded, this behaviour is consistent with scale-invariant semantic organization and contrasts sharply with technical texts."
- Why unresolved: Novels showed no divergence >15% within available text lengths, making it impossible to distinguish true scale-invariance from an unobserved crossover.
- What evidence would resolve it: Analyzing substantially longer novel corpora or developing analytical methods to extrapolate crossover behavior beyond measured scales.

### Open Question 2
- Question: Can the reduced context horizon in LLMs be improved through architectural modifications or training interventions?
- Basis in paper: [inferred] The authors identify reduced stability horizons as "a fundamental shortcoming in generative models" linked to autoregressive generation, but do not test whether this is an intrinsic constraint.
- Why unresolved: The study compares existing models but does not investigate whether different generation strategies, training objectives, or architectures could extend the semantic stability horizon.
- What evidence would resolve it: Comparing context horizons across models with different generation mechanisms (e.g., non-autoregressive, retrieval-augmented) or systematically varying training data diversity.

### Open Question 3
- Question: What linguistic or cognitive mechanisms drive the systematic difference in short-time scaling exponents between creative (~-0.4) and technical (~-0.25) texts?
- Basis in paper: [explicit] "We believe this directly measures the freedom of semantic exploration inherent in creative literature and the constrained semantic space of factual writing."
- Why unresolved: The authors offer a hypothesis about semantic exploration freedom, but the causal link between narrative intent and observed scaling exponents remains untested.
- What evidence would resolve it: Controlled experiments manipulating semantic constraints in generated text, or correlating exponent values with independent measures of semantic diversity and topic drift.

### Open Question 4
- Question: How robust is the context horizon metric to variations in the 15% deviation threshold used for its operational definition?
- Basis in paper: [inferred] The threshold is defined operationally ("deviates by more than 15% from its short-time value") without justification for this specific value or sensitivity analysis.
- Why unresolved: Different threshold choices could shift context horizon values and potentially alter comparative conclusions between genres or human vs. model text.
- What evidence would resolve it: Systematic sensitivity analysis showing how context horizon rankings change across threshold values, or establishing an optimality criterion for threshold selection.

## Limitations
- The method assumes cosine distances between adjacent sentence embeddings capture meaningful semantic displacement, which is not directly validated in the corpus.
- The choice of 15% deviation for context horizon detection is arbitrary and may shift results under alternative thresholds.
- Genre definitions are coarse (creative vs. technical) and may not generalize to other domains.
- LLM outputs are drawn from a specific Hugging Face dataset with undisclosed prompts, limiting reproducibility.

## Confidence
- **High confidence:** Short-time power-law scaling exists in real text and is eliminated by sentence shuffling (internal validation).
- **Medium confidence:** Genre separation (α ≈ -0.4 for creative vs. -0.25 for technical) is statistically significant but may shift with different embedding models.
- **Medium confidence:** Context horizon consistently smaller for LLMs than humans, but absolute values depend on threshold choice.

## Next Checks
1. Test embedding robustness by repeating the analysis with BGE-small and gte-small models; verify genre separation persists.
2. Apply the method to a controlled dataset of human-written vs. LLM-generated text from the same prompt to isolate model effects.
3. Perform ablation on sentence segmentation parameters (e.g., minimum sentence length, handling of dialogue) to quantify sensitivity to preprocessing choices.