---
ver: rpa2
title: Evaluating Compositional Approaches for Focus and Sentiment Analysis
arxiv_id: '2508.07810'
source_url: https://arxiv.org/abs/2508.07810
tags:
- compositional
- focus
- polarity
- sentence
- negation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates compositional approaches for Focus Analysis
  (FA) and Sentiment Analysis (SA), bridging a research gap by applying compositional
  rules from SA to FA. Using syntactic rules from Universal Dependencies and sentiment
  dictionaries, the authors compare compositional and non-compositional methods on
  hotel review datasets, focusing on sentences with negation and coordination.
---

# Evaluating Compositional Approaches for Focus and Sentiment Analysis

## Quick Facts
- arXiv ID: 2508.07810
- Source URL: https://arxiv.org/abs/2508.07810
- Authors: Olga Kellert; Muhammad Imran; Nicholas Hill Matlis; Mahmud Uz Zaman; Carlos Gómez-Rodríguez
- Reference count: 40
- Primary result: Compositional methods achieve 80% accuracy vs 71% for VADER on hotel review sentiment analysis

## Executive Summary
This study evaluates compositional approaches for Focus Analysis (FA) and Sentiment Analysis (SA), bridging a research gap by applying compositional rules from SA to FA. Using syntactic rules from Universal Dependencies and sentiment dictionaries, the authors compare compositional and non-compositional methods on hotel review datasets, focusing on sentences with negation and coordination. Their modified compositional approach, which accounts for coordination, achieved 74% accuracy on coordination data, outperforming the original version. Across all data, compositional methods reached 80% accuracy versus 71% for non-compositional VADER, with a smaller 2% gap in negation-heavy data. Qualitative analysis revealed strengths in handling negation scope and weaknesses with anaphoric references.

## Method Summary
The study compares compositional and non-compositional sentiment analysis on hotel review datasets. The compositional approach uses Universal Dependencies parsing (via Stanza) and SO-CAL sentiment dictionaries to recursively propagate polarity scores through dependency trees. The method applies syntactic rules for modification, coordination, and negation, with a formula (a × (1 + b) + (sign(a) × -4)) to calculate polarity. A modified version explicitly handles coordination scope over negation. The non-compositional baseline uses VADER. The dataset consists of OpeNER English hotel reviews (1,744 reviews), filtered into subsets based on subjectivity, negation presence, and coordination.

## Key Results
- Compositional approach achieved 80% accuracy vs 71% for VADER across all subjective reviews
- Modified compositional approach with coordination handling reached 74% accuracy on coordination data
- In negation-heavy data, compositional methods achieved 78% accuracy vs 76% for VADER
- Qualitative analysis revealed accurate negation scope resolution but failures with anaphoric references

## Why This Works (Mechanism)

### Mechanism 1: Dependency-Based Negation Scope Resolution
- Claim: Using UD dependency parsing to identify negation scope outperforms proximity-based heuristics when negation words are distant from sentiment targets.
- Mechanism: Head-child relations in dependency trees capture which sentiment words fall under negation's scope, regardless of linear word distance. The parser identifies negation as a modifier (advmod) of its head, and sentiment propagation respects this hierarchical relationship.
- Core assumption: The dependency parser accurately captures the true modifier relationships in the sentence.
- Evidence anchors:
  - [abstract]: "Our compositional approach in SA exploits basic syntactic rules such as rules of modification, coordination, and negation represented in the formalism of Universal Dependencies"
  - [section 4.2, Condition 3]: "The dependency parser, however, correctly interprets the negative argument 'nothing' as the object argument of the verb 'like', thereby correctly predicting the scope of negation. The double negation...leads to a positive interpretation."
  - [corpus]: Weak/missing—neighbor papers address compositionality in instruction following and transformers, not UD-based sentiment scope resolution.
- Break condition: Parser errors on complex clauses; propositional verbs (e.g., "make up") where syntactic scope mismatches semantic scope (Condition 1 in paper).

### Mechanism 2: Coordination Boundary Constrains Negation Scope
- Claim: Explicitly handling coordination improves accuracy when negation and coordination interact in complex sentences.
- Mechanism: Modified code gives coordination scope over negation, preventing polarity inversion from bleeding across coordinate structures. In "No es muy costoso pero tiene una vista bonita," negation inverts only "costoso," not "bonita."
- Core assumption: Coordination boundaries in the dependency tree correctly align with negation's semantic scope boundaries.
- Evidence anchors:
  - [section 3.1]: "In the modified version of [14], coordination has the scope over the negation predicting the correct interpretation of conjoined sentences."
  - [table 1]: Accuracy on coordination data improves from 71% (original) to 74% (modified).
  - [corpus]: Weak/missing—no corpus papers directly address coordination-negation interaction in sentiment.
- Break condition: Elliptical coordination where the coordinator is omitted; implicit coordination without explicit conjunction.

### Mechanism 3: Recursive Polarity Propagation via Branch Traversal
- Claim: Bottom-up recursive traversal of dependency branches produces interpretable sentiment scores.
- Mechanism: Sentiment scores propagate upward from lexical nodes through modifier nodes to the root. At each branch, intensifiers adjust strength (multiplier), and negation inverts polarity (±4 shift), per formula a × (1 + b) + (sign(a) × -4).
- Core assumption: The dependency tree structure correctly represents semantic composition order.
- Evidence anchors:
  - [appendix 8.2, formula (1)]: Explicit polarity formula provided: a ∗ (1 + b) + (sign(a) ∗ -4) = polarityscore
  - [section 2]: "The recursive traversal of each node...allows compositional approaches to account for the context and syntactic dependencies that influence the polarity orientation."
  - [corpus]: Weak/missing—no corpus validation of recursive branch-based aggregation specifically.
- Break condition: Anaphoric/deictic references (e.g., "not there"); word sense disambiguation failures (e.g., "old friend" vs. negative "old").

## Foundational Learning

- **Universal Dependencies (UD) and Dependency Parsing**
  - Why needed here: The entire compositional approach encodes sentiment rules as operations over UD dependency trees.
  - Quick check question: Given "The food was not very good," can you draw the dependency tree and identify which node is the head of "not"?

- **Compositional Semantics (Function Application)**
  - Why needed here: The method assumes meaning builds from parts via syntactic combination rules.
  - Quick check question: In Rooth's framework, what are the two parallel interpretations computed for a focused sentence?

- **Sentiment Lexicons and Polarity Scoring**
  - Why needed here: The system seeds polarity at leaf nodes via dictionary lookup before propagation.
  - Quick check question: What is the polarity score range in SO-CAL, and how does negation mathematically transform a score of +3?

## Architecture Onboarding

- **Component map:**
  Input: Raw sentence → UD Parser (Stanza) → CoNLL-U format tree
  Lookup: Sentiment dictionary (SO-CAL) annotates lexical nodes with base scores
  Build: Head-child dictionary maps each head to its children
  Detect: Identify polarity shifters (negation markers, intensifiers) per branch
  Aggregate: Bottom-up recursive score computation via branch traversal
  Output: Final polarity label (positive/negative/neutral)

- **Critical path:**
  1. Accurate UD parsing—parser errors propagate to scope errors.
  2. Correct sentiment word identification—missed words yield false negatives.
  3. Proper scope resolution for modifiers—negation/intensifier attachment determines score adjustment.

- **Design tradeoffs:**
  - Dictionary-based vs. learned: Fixed lexicon (SO-CAL); not adaptive to domain shift.
  - Rule-based vs. neural: Prioritizes interpretability over maximum accuracy.
  - Sentence-level vs. document-level: Optimized for short reviews; discourse relations ignored.
  - Single-language: Requires language-specific dictionaries and UD models.

- **Failure signatures:**
  - Propositional verbs: Syntactic negation scope exceeds semantic scope (e.g., "does not make up for").
  - Anaphoric targets: Reference resolution not handled (e.g., "not there" where "there" is external).
  - Word sense ambiguity: "Old" scores negative but is neutral in "old friend."

- **First 3 experiments:**
  1. Replicate results on OpeNER English hotel reviews (1,744 reviews) using provided GitHub code; verify 80% compositional vs. 71% VADER on Data All.
  2. Ablation study: Disable coordination handling and measure accuracy drop on Data Coordination subset (expect ~3% per Table 1).
  3. Stress test: Construct minimal pairs where negation-sentiment linear distance varies (e.g., "not good" vs. "nothing we liked") and compare compositional vs. VADER predictions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does integrating Word Sense Disambiguation (WSD) improve the accuracy of compositional sentiment analysis?
- Basis in paper: [explicit] The authors state they "will include the issue of WSD in future evaluations" because current dictionaries assign incorrect polarity to context-dependent words like "old."
- Why unresolved: Current dictionary-based approaches treat words as having fixed polarity, failing to distinguish between sentiment-bearing and neutral usages of the same term.
- What evidence would resolve it: A comparative evaluation of the system’s accuracy on ambiguous terms before and after integrating a WSD module.

### Open Question 2
- Question: Can the compositional approach maintain its performance advantages over non-compositional methods when applied to languages other than English?
- Basis in paper: [explicit] The authors list the single-language focus as a limitation and state, "In future, we will obtain these resources to test and evaluate our study in other languages too."
- Why unresolved: The current study relies on English-specific sentiment dictionaries and Universal Dependencies parsers; cross-linguistic syntactic variations may affect rule efficacy.
- What evidence would resolve it: Benchmarking the modified compositional rules on multilingual datasets (e.g., the Spanish OpeNER data) using language-specific dictionaries.

### Open Question 3
- Question: Does the integration of target identification mechanisms resolve the failure modes associated with anaphoric and deictic expressions?
- Basis in paper: [explicit] The qualitative analysis identified failures in "Condition 2" involving anaphora, leading to the conclusion that "Targets... need to be adapted to compositional approaches in the future."
- Why unresolved: The current dependency-based rules track syntactic negation and coordination but fail to link sentiments to their specific targets when referenced indirectly.
- What evidence would resolve it: Successful prediction of polarity in sentences containing pronouns or deictic references after implementing a target-extraction module.

## Limitations

- Single-language focus limits generalizability to other linguistic structures and negation patterns
- Dictionary-based approach cannot handle word sense ambiguity or domain-specific sentiment words
- Performance heavily depends on parsing accuracy, with errors propagating to sentiment scope resolution

## Confidence

- **High Confidence**: The compositional approach's interpretability and the specific mechanisms of dependency-based negation scope resolution and coordination handling are well-supported by the paper's evidence.
- **Medium Confidence**: The 9% accuracy improvement (80% vs. 71%) is statistically significant but the sample sizes for Data Negation (90 reviews) and Data Coordination (30 reviews) are small, making generalization uncertain.
- **Low Confidence**: Claims about the method's generalizability to other languages and domains are speculative, as no validation beyond English hotel reviews is provided.

## Next Checks

1. **Parser Dependency Accuracy**: For a set of 50 sentences with complex negation and coordination, manually annotate dependency trees and compare against Stanza's output to quantify parsing error impact on sentiment scope resolution.

2. **Ablation of Coordination Scope**: Run the compositional method with and without the coordination-over-negation rule on the coordination subset. Measure the exact accuracy drop to confirm the 3% improvement is consistent and not due to other factors.

3. **Cross-Domain Testing**: Apply the English compositional model to a different review domain (e.g., restaurant or product reviews) and compare accuracy against VADER to test domain robustness.