---
ver: rpa2
title: 'Memo2496: Expert-Annotated Dataset and Dual-View Adaptive Framework for Music
  Emotion Recognition'
arxiv_id: '2512.13998'
source_url: https://arxiv.org/abs/2512.13998
tags:
- music
- emotion
- ieee
- recognition
- transactions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Memo2496, an expert-annotated dataset for
  music emotion recognition (MER) with 2496 instrumental tracks and continuous valence-arousal
  labels from 30 certified specialists. It also presents DAMER, a dual-view adaptive
  framework integrating three modules: (1) Dual-Stream Attention Fusion (DSAF) for
  cross-modal Mel spectrogram and cochleagram fusion, (2) Progressive Confidence Labelling
  (PCL) with curriculum-based pseudo-labeling, and (3) Style-Anchored Memory Learning
  (SAML) to mitigate cross-track feature drift.'
---

# Memo2496: Expert-Annotated Dataset and Dual-View Adaptive Framework for Music Emotion Recognition

## Quick Facts
- arXiv ID: 2512.13998
- Source URL: https://arxiv.org/abs/2512.13998
- Reference count: 40
- Primary result: DAMER achieves state-of-the-art MER performance, improving arousal accuracy by 3.43% on Memo2496, 2.25% on 1000songs, and 0.17% on PMEmo

## Executive Summary
This paper introduces Memo2496, an expert-annotated dataset of 2496 instrumental tracks with continuous valence-arousal emotion labels from 30 certified specialists. It also presents DAMER, a dual-view adaptive framework integrating three modules: (1) Dual-Stream Attention Fusion (DSAF) for cross-modal Mel spectrogram and cochleagram fusion, (2) Progressive Confidence Labelling (PCL) with curriculum-based pseudo-labeling, and (3) Style-Anchored Memory Learning (SAML) to mitigate cross-track feature drift. Extensive experiments show DAMER achieves state-of-the-art performance, improving arousal accuracy by 3.43% on Memo2496, 2.25% on 1000songs, and 0.17% on PMEmo.

## Method Summary
DAMER uses 60-second segments (15-75s) from 44.1kHz, 16-bit stereo audio, processed into Mel spectrograms (128 bands) and cochleagrams (84 Gammatone filters). The framework employs DSAF with 2-layer bidirectional cross-attention (4 heads, D=128), PCL with temperature annealing (1.5→0.7) and threshold decay (0.65→0.35), and SAML with a 512-sized contrastive memory queue (momentum 0.95). Training uses Adam optimizer (lr=1e-3, decay=1e-4) for 80 epochs with cosine annealing and gradient clipping (norm 5.0), optimizing a weighted loss combining classification, pseudo-label, consistency, and contrastive components.

## Key Results
- DAMER achieves 82.95% arousal accuracy and 83.29% valence accuracy on Memo2496
- DSAF alone improves arousal accuracy by 2.36% over Mel-only baseline
- PCL increases pseudo-label coverage to 0.92+ with reliability scores above 0.87
- SAML maintains balanced class coverage (55:45) and compact intra-class clustering (L2 medians ~0.60-0.65)

## Why This Works (Mechanism)

### Mechanism 1: Dual-Stream Attention Fusion (DSAF)
- Claim: Bidirectional cross-attention between Mel spectrograms and cochleagrams captures complementary emotion-relevant acoustic information that unimodal approaches miss.
- Mechanism: Mel spectrograms encode spectral energy distribution; cochleagrams model auditory peripheral frequency analysis via Gammatone filters. Cross-attention (Q from one modality, K/V from the other) enables token-level information exchange, allowing each representation to contextualize itself against the complementary view through 2 stacked transformer layers with 4 attention heads.
- Core assumption: Mel and cochleagram representations encode partially non-overlapping emotion discriminators—spectral patterns vs. perceptual auditory responses.
- Evidence anchors:
  - [abstract]: "Dual Stream Attention Fusion (DSAF) facilitates token-level bidirectional interaction between Mel spectrograms and cochleagrams via cross attention mechanisms"
  - [section]: Ablation study shows DSAF alone achieves 80.94% arousal accuracy on Memo2496, outperforming PCL-only (80.58%) and SAML-only (78.38%)
  - [corpus]: MMVA paper explores multimodal valence-arousal matching but focuses on image-music-caption alignment rather than acoustic-acoustic fusion; limited direct corpus validation of cross-attention for MER

### Mechanism 2: Progressive Confidence Labelling (PCL)
- Claim: Curriculum-based pseudo-labeling with cross-view consistency quantification reduces confirmation bias while expanding effective training data.
- Mechanism: Temperature τ anneals from 1.5→0.7 over training, softening early predictions to prevent premature commitment. Reliability score r = exp(-JS(p_Mel, p_Coch)) weights pseudo-labels by inter-view agreement. Dynamic threshold θ descends from 0.65→0.35, progressively incorporating lower-confidence samples as model stabilizes.
- Core assumption: High cross-view agreement correlates with pseudo-label correctness; early training predictions require softening to avoid error propagation.
- Evidence anchors:
  - [abstract]: "Progressive Confidence Labelling (PCL) generates reliable pseudo labels employing curriculum-based temperature scheduling and consistency quantification using Jensen Shannon divergence"
  - [section]: "confidence and reliability score trajectories stabilising near 0.90"; pseudo-label coverage remains above 0.92 with strength >0.87 in later epochs
  - [corpus]: No direct corpus evidence on curriculum-based pseudo-labeling for MER; related work on semi-supervised learning (UDDA) uses fixed thresholds without reliability weighting

### Mechanism 3: Style-Anchored Memory Learning (SAML)
- Claim: Contrastive memory queue enforces emotion-discriminative clustering while decoupling style-specific confounders across heterogeneous tracks.
- Mechanism: Sliding memory queue (size=512) stores L2-normalized fused features with momentum coefficient 0.95. Supervised InfoNCE loss attracts same-class samples while repelling different-class instances, anchoring emotion invariance across diverse musical styles. Queue refresh replaces oldest entries, maintaining temporal smoothness.
- Core assumption: Emotional content is style-invariant; same-emotion tracks from different genres/instruments should cluster in embedding space despite acoustic variation.
- Evidence anchors:
  - [abstract]: "Style Anchored Memory Learning (SAML) maintains a contrastive memory queue to mitigate cross-track feature drift"
  - [section]: Memory label entropy stable at 0.685±0.007; class coverage balanced at 55%:45%; L2 distances to class centroids show compact intra-class clustering (medians ~0.60-0.65)
  - [corpus]: GlobalMood benchmark highlights cross-cultural style challenges; no direct corpus validation of contrastive memory for MER drift mitigation

## Foundational Learning

- Concept: Valence-Arousal (V-A) dimensional emotion model
  - Why needed here: Memo2496 uses continuous V-A coordinates as ground truth; the circular annotation interface maps 12 discrete emotion terms to theoretical V-A positions. Understanding this 2D circumplex space is prerequisite for interpreting labels and loss functions.
  - Quick check question: Can you position "excited" (high arousal, positive valence) and "depressed" (low arousal, negative valence) on a 2D V-A plane?

- Concept: Cross-attention in Transformers
  - Why needed here: DSAF implements bidirectional cross-attention where Mel tokens query cochleagram context and vice versa. Understanding Q/K/V attention mechanisms, multi-head scaling, and residual connections is essential for debugging fusion failures.
  - Quick check question: Given query Q∈R^(B×N×D) and key K∈R^(B×M×D), what is the shape of the attention weight matrix, and how does softmax(QK^T/√d_k) aggregate information from V?

- Concept: Pseudo-labeling with confirmation bias
  - Why needed here: PCL addresses the core SSL problem where early erroneous predictions reinforce themselves. Understanding why fixed-threshold pseudo-labeling fails (noise propagation) vs. curriculum-based approaches (gradual commitment) explains the temperature and threshold schedules.
  - Quick check question: Why does setting a fixed pseudo-label threshold θ=0.9 throughout training risk amplifying early errors, and how does descending θ from 0.65→0.35 mitigate this?

## Architecture Onboarding

- Component map:
Audio (44.1kHz, 16-bit stereo)
    ↓
[60s segment: 15-75s]
    ↓
┌─────────────────┬──────────────────┐
│ Mel Extraction  │ Cochleagram      │
│ (128 bands)     │ (84 Gammatone)   │
└────────┬────────┴────────┬─────────┘
         ↓                 ↓
   Linear Projection   Linear Projection
   (D=128)            (D=128)
         ↓                 ↓
    ┌─────── DSAF ─────────┐
    │ Cross-Attention (L=2)│
    │ 4 heads, D=128       │
    └───────┬──────────────┘
            ↓
    ┌───────┴───────┐
    │ Fusion MLP    │
    │ (D_f=256)     │
    └───────┬───────┘
            ↓
┌───────────┼───────────┐
│ Mel Head  │ Coch Head │ Fuse Head │
└─────┬─────┴─────┬─────┴─────┬─────┘
      ↓           ↓           ↓
   p_Mel       p_Coch      p_Fuse
      └───────────┼───────────┘
                  ↓
         ┌────────┴────────┐
         │ PCL Module      │
         │ JS(p_Mel,p_Coch)│
         │ → reliability r │
         │ τ_t, θ_t curves │
         └────────┬────────┘
                  ↓
         ┌────────┴────────┐
         │ SAML Module     │
         │ Memory Queue 512│
         │ InfoNCE Loss    │
         └─────────────────┘

- Critical path:
  1. **Input preprocessing**: Extract 60-second segment (15-75s) → avoids onset effects and fatigue artifacts
  2. **Dual acoustic encoding**: Mel (128 bands via STFT) + Cochleagram (84 Gammatone filters with log compression)
  3. **Token embedding**: Linear projection to D=128 for both modalities
  4. **Cross-view fusion**: 2-layer bidirectional cross-attention → fused representation (D_f=256)
  5. **Tri-branch prediction**: 3 parallel classification heads (Mel/Coch/Fused) → ensemble output
  6. **PCL reliability scoring**: JS divergence → r = exp(-JS) → composite confidence c = r × max(p_Fuse)
  7. **SAML memory update**: Enqueue fused features → dequeue oldest → compute supervised contrastive loss

- Design tradeoffs:
  - **Instrumental-only dataset**: Eliminates lyrical confounds but limits real-world applicability to vocal music
  - **Binary classification on continuous V-A**: Thresholding at 0 loses fine-grained emotion intensity information
  - **Expert vs. scale**: 2496 expert-annotated tracks trades quantity for quality; crowdsourced datasets may have more tracks but higher noise
  - **Memory queue size (512)**: Larger queues capture more style diversity but increase compute; smaller queues may under-represent minority classes

- Failure signatures:
  - **Low reliability scores (r < 0.6)**: Cross-view disagreement → check Mel/cochleagram extraction pipeline for preprocessing errors
  - **Imbalanced memory queue (>60% one class)**: Class imbalance in batch sampling → verify stratified sampling or adjust queue update
  - **Pseudo-label coverage drops below 0.8**: Threshold descent too aggressive → increase θ_min or slow θ schedule
  - **Arousal accuracy significantly exceeds valence**: Expected (valence harder to predict from acoustics alone), but if gap > 20% → check harmonic feature extraction
  - **t-SNE shows overlapping clusters post-training**: Contrastive loss not converging → increase λ_4 or decrease τ_cont

- First 3 experiments:
  1. **Ablation validation**: Train 7 configurations (DSAF-only, PCL-only, SAML-only, all pairwise combinations, full DAMER) on Memo2496; verify that full model achieves reported 82.95% arousal accuracy and that each module contributes positively
  2. **Temperature schedule sensitivity**: Grid search τ_min ∈ [0.5, 0.7, 0.9] × τ_max ∈ [1.2, 1.5, 1.8] while fixing other hyperparameters; identify optimal curriculum sharpness for pseudo-label quality vs. coverage
  3. **Cross-dataset zero-shot transfer**: Train DAMER on Memo2496 only, evaluate on 1000songs and PMEmo test sets without fine-tuning; compare arousal/valence accuracy degradation to assess style drift robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the exclusive focus on instrumental music affect DAMER's generalizability to music with lyrics, where vocal content may dominate emotional perception?
- Basis in paper: [inferred] The paper explicitly excludes vocal content to "eliminate linguistic confounding variables" but does not evaluate whether the learned representations transfer to lyrical music.
- Why unresolved: No experiments test cross-domain transfer from instrumental to lyrical music.
- What evidence would resolve it: Evaluate DAMER on lyrical music datasets and compare performance against instrumental-only training.

### Open Question 2
- Question: To what extent do the cultural homogeneity and narrow age range (18–28 years) of annotators limit the cross-cultural validity of emotion labels?
- Basis in paper: [inferred] All 30 annotators are affiliated with one Chinese university; the paper notes cross-cultural domain shifts as a fundamental MER challenge but does not address this in dataset design.
- Why unresolved: No comparison with annotations from diverse cultural or age groups is provided.
- What evidence would resolve it: Collect parallel annotations from multiple cultural groups and measure inter-group agreement; test model performance across culturally diverse test sets.

### Open Question 3
- Question: How does expert annotation compare to general listener annotation in terms of downstream model performance on real-world applications?
- Basis in paper: [inferred] The paper claims expert annotation reduces noise but does not evaluate whether models trained on expert labels perform better for general users than those trained on crowdsourced labels.
- Why unresolved: No direct comparison of model performance using expert versus non-expert annotations on the same corpus.
- What evidence would resolve it: Train separate models on expert and crowd annotations for identical audio; compare predictive accuracy on held-out listener ratings.

### Open Question 4
- Question: Can the SAML module's contrastive memory mechanism effectively scale to larger, more stylistically diverse datasets without memory queue saturation?
- Basis in paper: [inferred] SAML uses a fixed-size queue (K=512) to anchor style-invariant features; the paper does not analyze sensitivity to queue size or performance on datasets with greater style heterogeneity.
- Why unresolved: Only three datasets are tested, each with limited stylistic breadth.
- What evidence would resolve it: Conduct ablation on queue size K across datasets with systematically varied style diversity; measure intra-class compactness and classification accuracy.

## Limitations

- **Data Generalization Gap**: Memo2496 contains only 2496 instrumental tracks, limiting generalization to vocal music, diverse genres, and real-world applications where lyrics influence emotional perception.
- **Hyperparameter Sensitivity**: Critical hyperparameters like batch size and dropout rate lack precise specification, potentially affecting reproducibility.
- **Computational Cost**: The dual-stream architecture with bidirectional cross-attention and memory queue maintenance increases computational overhead significantly compared to unimodal baselines.

## Confidence

**High Confidence (95%)**:
- DSAF improves arousal accuracy over unimodal baselines (empirical results are consistent across three datasets)
- PCL with curriculum-based temperature scheduling outperforms fixed-threshold pseudo-labeling (validated through ablation studies)
- Memo2496 provides high-quality expert annotations (30 specialists per track, professional certification)

**Medium Confidence (75%)**:
- DSAF captures complementary information beyond Mel-only approaches (limited by lack of ablation comparing to cochleagram-only baseline)
- SAML effectively mitigates style drift (no quantitative comparison to non-contrastive memory approaches)
- Arousal is consistently easier to predict than valence (consistent with prior MER literature but dataset-specific)

**Low Confidence (60%)**:
- Cross-dataset zero-shot transfer performance (only briefly mentioned without detailed analysis)
- Long-term pseudo-label reliability as training progresses (no temporal analysis of reliability score distributions)
- Memory queue size optimization (512 chosen without sensitivity analysis)

## Next Checks

1. **Style-Emotion Entanglement Validation**: Design a controlled experiment where DAMER is trained on genre-homogeneous subsets (e.g., only classical, only jazz) vs. mixed genres. Compare arousal/valence accuracy degradation to test whether style-invariant emotion clustering is achievable or if style and emotion are fundamentally confounded.

2. **Memory Queue Sensitivity Analysis**: Systematically vary the memory queue size K ∈ {128, 256, 512, 1024} while measuring: (a) class coverage entropy in the queue, (b) L2 centroid distances for emotion clusters, and (c) final test accuracy. Identify the point of diminishing returns and potential overfitting to limited memory.

3. **Cross-Modal Alignment Robustness**: Evaluate DAMER's performance when Mel and cochleagram preprocessing parameters are perturbed (e.g., Mel bands: 64→192, Gammatone filters: 64→128, window lengths: 25ms→100ms). Measure cross-view JS divergence and pseudo-label reliability to determine sensitivity to feature extraction choices.