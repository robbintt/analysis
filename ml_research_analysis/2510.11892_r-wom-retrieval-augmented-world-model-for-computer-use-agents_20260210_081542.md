---
ver: rpa2
title: 'R-WoM: Retrieval-augmented World Model For Computer-use Agents'
arxiv_id: '2510.11892'
source_url: https://arxiv.org/abs/2510.11892
tags:
- world
- r-wom
- arxiv
- claude-3
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models struggle to reliably simulate future states
  in digital environments due to hallucination and reliance on static knowledge, leading
  to compounding errors in long-horizon planning. To address this, the Retrieval-augmented
  World Model (R-WoM) grounds LLM-based simulations with environment-specific tutorials,
  improving both state prediction and reward estimation.
---

# R-WoM: Retrieval-augmented World Model For Computer-use Agents

## Quick Facts
- arXiv ID: 2510.11892
- Source URL: https://arxiv.org/abs/2510.11892
- Reference count: 36
- Primary result: R-WoM improves LLM-based world modeling in computer-use agents via retrieval-augmented grounding, achieving up to 25.3% gains on OSWorld and 18.1% on WebArena.

## Executive Summary
Large Language Models (LLMs) struggle to simulate future states reliably in digital environments due to hallucination and reliance on static knowledge, leading to compounding errors in long-horizon planning. R-WoM addresses this by grounding LLM-based simulations with environment-specific tutorials. It employs a reasoning-based retrieval pipeline for relevant tutorial chunks, uses long chain-of-thought reasoning for multi-step rollouts, and adopts listwise reward estimation to rank trajectories relatively rather than assign absolute scores. Evaluations on OSWorld and WebArena show substantial improvements over baselines, especially in longer-horizon scenarios.

## Method Summary
R-WoM enhances LLM-based world modeling by retrieving and grounding simulation rollouts with environment-specific tutorials. It uses a reasoning-based retrieval pipeline to find relevant tutorial chunks, applies long chain-of-thought reasoning for multi-step state prediction and reward estimation, and adopts a listwise reward approach to rank trajectories relative to each other. This combination improves both state prediction accuracy and reward estimation compared to traditional LLM-only world models.

## Key Results
- R-WoM achieves up to 25.3% improvement on OSWorld and 18.1% on WebArena compared to strong baselines.
- Largest gains occur in longer-horizon scenarios where compounding errors are most problematic.
- Retrieval grounding, long reasoning chains, and listwise rewards each contribute to performance, as shown in ablation studies.

## Why This Works (Mechanism)
R-WoM grounds LLM simulations in environment-specific knowledge by retrieving relevant tutorial content, reducing hallucination and reliance on static priors. Long chain-of-thought reasoning enables multi-step state and reward prediction, while listwise reward estimation ranks trajectories relatively, improving robustness. This retrieval-augmented approach addresses the key failure mode of compounding errors in long-horizon tasks.

## Foundational Learning
- **Chain-of-thought reasoning**: Enables step-by-step reasoning for multi-step state and reward prediction; needed for complex, sequential decision-making; quick check: does the model produce coherent intermediate steps?
- **Retrieval-augmented generation**: Grounds LLM outputs in relevant, external knowledge to reduce hallucination; needed when static knowledge is insufficient; quick check: does retrieval return relevant, correct tutorial chunks?
- **Listwise reward estimation**: Ranks trajectories relative to each other instead of assigning absolute scores; needed to improve reward robustness in noisy environments; quick check: do ranked trajectories align with expected outcomes?
- **Long-horizon planning**: Simulating many future steps is critical for planning in dynamic environments; needed for tasks requiring extended foresight; quick check: does performance degrade with increasing horizon?
- **Tutorial grounding**: Using curated, environment-specific tutorials as a knowledge source; needed to provide accurate, actionable context; quick check: are retrieved tutorial chunks actionable and correct?
- **State prediction**: Forecasting future environment states given current actions; needed to simulate outcomes before execution; quick check: are predicted states consistent with actual outcomes?

## Architecture Onboarding

**Component map:** Retrieval pipeline → Long chain-of-thought reasoning → Listwise reward estimation

**Critical path:** Retrieve tutorial chunks → Use chunks in multi-step reasoning → Predict next state and reward → Rank trajectories relatively → Select best action

**Design tradeoffs:** Uses curated tutorials for grounding (limits scalability but improves accuracy); employs listwise over pointwise rewards (more robust but potentially noisier); favors long reasoning chains (better accuracy but slower); trades off computational cost for improved robustness in long-horizon tasks.

**Failure signatures:** Retrieval returns irrelevant or incorrect chunks → hallucination persists → state prediction errors → poor reward estimation → trajectory ranking fails → compounding errors in long horizons.

**3 first experiments:** 1) Run retrieval pipeline alone and evaluate chunk relevance on a small test set. 2) Test long chain-of-thought reasoning on a single state transition to verify step-by-step coherence. 3) Compare listwise vs. pointwise reward estimation on a small trajectory set to confirm relative ranking effectiveness.

## Open Questions the Paper Calls Out
None

## Limitations
- Gains may be due to retrieval, reasoning, or reward design—ablation studies only on GPT-3.5 limit isolation of key drivers.
- Reliance on curated tutorials raises scalability concerns for new UI domains.
- No testing in out-of-domain environments to assess generalization.
- Real-world robustness and user-desired behaviors (e.g., safety, graceful degradation) not evaluated.

## Confidence

- Retrieval grounding improving state prediction: **High** (strong performance lifts and ablation data support)
- Listwise reward estimation outperforming pointwise: **Medium** (ablation shows benefit, but only on a single model)
- Gains generalizing to unseen environments: **Low** (no out-of-domain testing shown)

## Next Checks

1. Run component ablations (retrieval only, reasoning only, reward only) on GPT-4 and other strong LLMs to verify consistent effects.
2. Test R-WoM in a completely new UI domain without existing tutorials to assess generalizability.
3. Perform a user study comparing task completion and error recovery against strong baselines in real or realistic computer-use scenarios.