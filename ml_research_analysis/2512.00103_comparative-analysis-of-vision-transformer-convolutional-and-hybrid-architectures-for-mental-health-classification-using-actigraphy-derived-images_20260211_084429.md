---
ver: rpa2
title: Comparative Analysis of Vision Transformer, Convolutional, and Hybrid Architectures
  for Mental Health Classification Using Actigraphy-Derived Images
arxiv_id: '2512.00103'
source_url: https://arxiv.org/abs/2512.00103
tags:
- validation
- https
- accuracy
- schizophrenia
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study compared three deep learning architectures\u2014VGG16,\
  \ ViT-B/16, and CoAtNet-Tiny\u2014for classifying depression, schizophrenia, and\
  \ healthy controls using actigraphy-derived images. Daily motor activity from wrist-worn\
  \ accelerometers was transformed into 30\xD748 pixel images and evaluated through\
  \ a three-fold subject-wise split."
---

# Comparative Analysis of Vision Transformer, Convolutional, and Hybrid Architectures for Mental Health Classification Using Actigraphy-Derived Images

## Quick Facts
- arXiv ID: 2512.00103
- Source URL: https://arxiv.org/abs/2512.00103
- Authors: Ifeanyi Okala
- Reference count: 40
- Primary result: CoAtNet-Tiny achieved highest average accuracy (84.01% ± 0.72) with strongest precision, recall, and F1-scores for mental health classification from actigraphy-derived images

## Executive Summary
This study evaluated three deep learning architectures—VGG16 (purely convolutional), ViT-B/16 (pure Vision Transformer), and CoAtNet-Tiny (hybrid convolutional-attention)—for classifying depression, schizophrenia, and healthy controls using actigraphy-derived images. Daily motor activity from wrist-worn accelerometers was transformed into 30×48 pixel images and analyzed through a three-fold subject-wise split. While all models fit training data well, CoAtNet-Tiny demonstrated superior generalization with the highest accuracy and most stable performance across folds. The findings suggest hybrid architectures combining convolutional and attention-based layers are particularly effective for mental-health classification from actigraphy-derived images in low-data settings.

## Method Summary
The study transformed daily motor activity data from wrist-worn accelerometers into 30×48 pixel images representing activity patterns over time. Three deep learning architectures were evaluated: VGG16 (convolutional baseline), ViT-B/16 (pure Vision Transformer), and CoAtNet-Tiny (hybrid architecture). A three-fold subject-wise split was used to evaluate model performance, ensuring no subject appeared in both training and test sets. Model performance was assessed using accuracy, precision, recall, and F1-scores, with particular attention to underrepresented class performance.

## Key Results
- CoAtNet-Tiny achieved the highest average accuracy (84.01% ± 0.72) with the most stable learning curves across all folds
- All architectures showed strong training performance but varied significantly in generalization to unseen data
- CoAtNet-Tiny demonstrated superior precision, recall, and F1-scores, particularly for underrepresented classes
- VGG16 showed stable but lower performance, while ViT-B/16 exhibited strong but variable results

## Why This Works (Mechanism)
The superior performance of CoAtNet-Tiny stems from its hybrid architecture that combines convolutional layers' ability to capture local spatial patterns with attention mechanisms' capacity to model long-range dependencies. For actigraphy-derived images, this enables effective detection of both fine-grained activity patterns and broader temporal trends that distinguish mental health conditions. The convolutional components excel at extracting local features from the structured image representation, while attention mechanisms help identify relevant patterns across the temporal dimension, addressing the challenge of limited training data through improved feature representation efficiency.

## Foundational Learning
- **Actigraphy data transformation**: Converting time-series motor activity into 2D images enables use of computer vision architectures; needed to leverage powerful visual recognition models; quick check: verify that image transformation preserves temporal and intensity information
- **Subject-wise cross-validation**: Ensures no subject appears in both training and test sets; needed to prevent data leakage and obtain realistic generalization estimates; quick check: confirm subject IDs are properly partitioned across folds
- **Hybrid CNN-Transformer architectures**: Combine local feature extraction with global attention; needed to balance detailed pattern recognition with broader context understanding; quick check: examine layer-wise feature maps for both convolutional and attention outputs
- **Multi-class classification metrics**: Precision, recall, and F1-scores for each class; needed to evaluate performance on underrepresented mental health conditions; quick check: calculate metrics separately for each class and compare against overall accuracy

## Architecture Onboarding

**Component Map**: Input Image -> Conv/Attention Layers -> Feature Maps -> Classification Head -> Output Probabilities

**Critical Path**: The most important sequence is the feature extraction path through the hybrid layers (convolutional and attention), which determines the quality of representations passed to the classification head. The integration between these layers is crucial for performance.

**Design Tradeoffs**: 
- VGG16 offers simplicity and stability but may lack capacity for complex pattern recognition
- ViT-B/16 provides powerful attention mechanisms but requires more data to avoid overfitting
- CoAtNet-Tiny balances complexity and efficiency but introduces architectural complexity that may affect training stability

**Failure Signatures**: 
- High training accuracy with low validation accuracy indicates overfitting
- Poor performance on minority classes suggests class imbalance issues
- Inconsistent results across folds indicate instability in the architecture or training process

**First Experiments**:
1. Evaluate training and validation loss curves for each architecture to identify overfitting patterns
2. Compare feature visualization heatmaps to understand what each architecture learns from actigraphy images
3. Perform ablation studies by removing attention or convolutional components to isolate their contributions

## Open Questions the Paper Calls Out
None

## Limitations
- Sample size and demographic composition were not specified, limiting generalizability
- Actigraphy-derived image transformation method's effectiveness was not systematically validated against alternative representations
- Three-fold cross-validation may not fully capture population variability in larger, more diverse datasets

## Confidence
- **High confidence**: Relative performance ranking of CoAtNet-Tiny versus VGG16 across all metrics appears robust based on consistent results across folds
- **Medium confidence**: Superiority of hybrid architectures requires validation in larger, more diverse datasets to confirm generalizability
- **Medium confidence**: Claims about CoAtNet-Tiny's stability and lower variability may be influenced by specific three-fold split

## Next Checks
1. Replicate study with larger, more diverse sample including multiple age groups, cultural backgrounds, and both clinical and community-dwelling populations
2. Conduct ablation studies comparing actigraphy-derived image approach against raw time-series models and alternative image representations
3. Implement nested cross-validation with hyperparameter optimization to ensure performance differences are not due to optimization artifacts