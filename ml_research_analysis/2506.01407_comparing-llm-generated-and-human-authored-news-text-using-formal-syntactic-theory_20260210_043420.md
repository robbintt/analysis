---
ver: rpa2
title: Comparing LLM-generated and human-authored news text using formal syntactic
  theory
arxiv_id: '2506.01407'
source_url: https://arxiv.org/abs/2506.01407
tags:
- llama
- lexical
- types
- syntactic
- human-authored
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study provides the first comprehensive comparison of New York
  Times-style text generated by six large language models against real, human-authored
  NYT writing. The comparison is based on a formal syntactic theory.
---

# Comparing LLM-generated and human-authored news text using formal syntactic theory

## Quick Facts
- arXiv ID: 2506.01407
- Source URL: https://arxiv.org/abs/2506.01407
- Reference count: 13
- Primary result: HPSG grammar type distributions distinguish human NYT writing from LLM-generated text, with LLMs clustering tightly together while humans show greater syntactic diversity

## Executive Summary
This study provides the first comprehensive comparison of New York Times-style text generated by six large language models against real, human-authored NYT writing using formal syntactic theory. The authors employ Head-driven Phrase Structure Grammar (HPSG) to analyze grammatical structures, revealing systematic distinctions between human and LLM-generated writing through distributions of HPSG grammar types. The findings contribute to understanding syntactic behavior differences between LLMs and humans within the NYT genre, showing that LLMs appear to converge on an "averaged" syntactic style while humans retain greater structural diversity.

## Method Summary
The researchers parsed human-authored NYT lead paragraphs (Oct 2023 - Jan 2024) and LLM-generated text (from six models: LLaMA 7B/13B/30B/65B, Falcon 7B, Mistral 7B) using the English Resource Grammar (ERG) with ACE parser. They extracted counts of 298 syntactic types and 1,398 lexical types, then analyzed relative frequency distributions and diversity indices using cosine similarity and Shannon entropy metrics. The study compares distributions of HPSG grammar types (phrasal, lexical, morphological) between human and LLM texts to identify systematic syntactic differences.

## Key Results
- Human-authored texts show distinct HPSG type distributions from closely-clustered LLMs
- LLMs over-generate head-complement constructions while under-utilizing stylistically marked constructions
- Humans exhibit greater syntactic construction diversity but lower lexical type diversity compared to collective LLM output

## Why This Works (Mechanism)

### Mechanism 1: Syntactic Type Distribution Fingerprinting
If text is parsed using a highly-typed formal grammar (HPSG), the resulting distribution of grammatical types serves as a fingerprint that distinguishes human authors from LLMs more reliably than vocabulary alone. The ERG maps tokens to a hierarchy of lexical types and phrasal constructions, and while LLMs and humans may use similar vocabulary, they differ systematically in the frequency of specific structural types.

### Mechanism 2: The "Averaging" of Grammatical Constructions
LLMs appear to converge on an "averaged" syntactic style, over-generating high-frequency generic structures and under-utilizing low-frequency, stylistically marked constructions. By training on vast datasets, LLMs seemingly maximize the probability of the most general syntactic templates, while human authors retain higher entropy and utilize the "long tail" of complex constructions.

### Mechanism 3: Lexical-Syntactic Decoupling
While LLMs can mimic human lexical diversity, they struggle to replicate the conditional probability of how those lexical items map to specific syntactic roles. The paper finds LLMs may show high lexical type diversity but differ significantly in syntactic construction diversity, treating words more fluidly while baking them into rigid sentence structures.

## Foundational Learning

- **Concept: Head-driven Phrase Structure Grammar (HPSG)**
  - Why needed here: The entire methodology relies on the ERG, an HPSG implementation. You must understand that HPSG treats syntax as a system of typed feature structures rather than simple linear rules.
  - Quick check question: Can you explain the difference between a surface string parse and a feature structure unification in HPSG?

- **Concept: Shannon Entropy (Diversity Metrics)**
  - Why needed here: The paper uses entropy to quantify "richness" of syntax, helping distinguish between using many different words vs. many different sentence structures.
  - Quick check question: If a text uses 1,000 unique sentence structures but repeats the top 5 structures 99% of the time, does it have high or low entropy?

- **Concept: Cosine Similarity**
  - Why needed here: This distance metric clusters LLMs vs. Humans by measuring the angle between vectors of construction frequencies.
  - Quick check question: If Document A has construction counts [10, 20] and Document B has [100, 200], what is their cosine similarity?

## Architecture Onboarding

- **Component map:** Raw text files -> ACE parser with ERG -> Prolog-like structures/XML via PyDelphin -> Python scripts extracting type counts -> Statistical analysis modules
- **Critical path:** The parsing step is the primary bottleneck; the ERG must successfully analyze the sentence for the data to be included in the distribution analysis.
- **Design tradeoffs:** Precision vs. Robustness (ERG won't parse ungrammatical text, ensuring clean data but risking loss of "messy" human samples), Granularity (analyzing abstract types vs. specific tokens).
- **Failure signatures:** Low coverage if text contains neologisms or severe grammatical errors, Type Overlap in highly specific genres where the "long tail" difference might vanish.
- **First 3 experiments:**
  1. Run ACE on a sample of 100 sentences from both datasets to verify coverage rates (~94%) and inspect feature structures
  2. Replicate the "Head-Complement" vs "Subject-Head" frequency comparison to verify the salient differentiator
  3. Pick 3 human authors and 3 LLMs, calculate pairwise cosine similarity, and verify the clustering hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
Do specific differences in internal subconstituents or lexical types drive the higher frequency of head-complement constructions in LLM-generated text? The authors note they don't include further analysis of differences in head-complement construction use but suggest future work examining subconstituents and lexical types forming these constituents.

### Open Question 2
Is the distribution of lexical (morphological) rules more closely tied to genre and style than to authorship type (human vs. LLM)? The authors observe striking similarity between humans and LLMs in morphological rule use and propose that lexical rule distribution may be closely tied to genre rather than authorship nature.

### Open Question 3
Why do LLMs exhibit higher diversity in lexical types but lower diversity in syntactic constructions compared to human authors? The paper explicitly states that investigating this pattern reversal is future work, having quantified the contradiction but not offered a theoretical explanation.

### Open Question 4
What factors cause the "collective LLM" vocabulary to be more varied than that of human authors? While individual LLMs use fewer unique lexical entries than humans, the combined LLM data shows greater lexical diversity, which the authors state calls for further investigation.

## Limitations

- The study relies on a single formal grammar (ERG) and genre (NYT news), limiting generalizability to other domains, languages, or grammatical frameworks
- The ERG's ~94% coverage may miss domain-specific constructions or fail on ungrammatical LLM outputs, potentially skewing type distributions
- The human dataset is limited to a specific time period and publication, and the LLM generation used a specific prompt template that may not reflect all use cases

## Confidence

**High Confidence:** The core finding that human and LLM texts exhibit systematically different HPSG type distributions is well-supported by multiple analyses including cosine similarity, entropy measures, and specific construction frequencies.

**Medium Confidence:** The interpretation that LLMs "average" human syntactic patterns is plausible but alternative explanations cannot be ruled out without additional experiments.

**Low Confidence:** The specific rankings of models by construction diversity may reflect generation artifacts rather than inherent syntactic capabilities given the limited sample size and prompt dependency.

## Next Checks

1. Apply the same HPSG analysis pipeline to LLM and human text from different genres (scientific writing, fiction, social media) to test whether syntactic differences persist outside the NYT news domain.

2. Systematically analyze parse failure cases to determine whether the ERG systematically excludes certain construction types more frequently in LLM vs. human text, and whether coverage differences correlate with observed type distribution differences.

3. Generate multiple versions of the same headlines using varied prompts (different temperatures, prompt templates, or fine-tuning approaches) to isolate whether syntactic differences are inherent to the models or artifacts of the specific generation setup used.