---
ver: rpa2
title: Bilingual Word Level Language Identification for Omotic Languages
arxiv_id: '2509.07998'
source_url: https://arxiv.org/abs/2509.07998
tags:
- language
- languages
- text
- data
- omotic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses bilingual word-level language identification
  for Wolaita and Gofa, two closely related Omotic languages spoken in southern Ethiopia.
  The challenge lies in distinguishing between the languages due to their shared linguistic
  features and geographical proximity.
---

# Bilingual Word Level Language Identification for Omotic Languages

## Quick Facts
- arXiv ID: 2509.07998
- Source URL: https://arxiv.org/abs/2509.07998
- Reference count: 40
- Best model achieved F1-score of 0.72 using BERT-base-uncased + LSTM

## Executive Summary
This study addresses the challenge of distinguishing between Wolaita and Gofa, two closely related Omotic languages in southern Ethiopia, at the word level. The authors developed the first annotated dataset for these languages, including a three-class system to handle shared vocabulary. Using a BERT-base-uncased plus LSTM architecture, they achieved an F1-score of 0.72, outperforming other deep learning approaches like CNN-LSTM and BiLSTM variants. The work provides a foundation for further research on low-resource Ethiopian languages and demonstrates the potential for applying transformer-based models to closely related language pairs.

## Method Summary
The authors created a word-level dataset of 144,000 words from religious, educational, and social media sources, annotated for Wolayta, Gofa, and shared code-switched form (wal-gof). They implemented a hybrid model using bert-base-uncased to generate 768-dimensional embeddings, which were then processed by an LSTM layer (128 units) followed by batch normalization, dense layers, and dropout. The model was trained with Adam optimizer (learning rate 0.001) for 10-30 epochs with batch size 128. Three native speaker annotators labeled each word using majority voting to establish ground truth.

## Key Results
- BERT-base-uncased + LSTM achieved F1-score of 0.72, outperforming other models
- CNN+BiLSTM combination achieved F1-score of 0.69 as second-best result
- Three-class labeling scheme (wal, gof, wal-gof) improved handling of cognates
- Character-level orthographic differences captured by CNN layers contributed to discrimination

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining pre-trained contextual embeddings with a sequence classifier improves discrimination between lexically similar languages.
- **Mechanism:** BERT-base-uncased generates 768-dimensional contextual token representations that capture subword patterns and positional context; these embeddings are then fed to an LSTM that models sequential dependencies across characters/words, allowing the system to learn discriminative patterns even when surface forms overlap.
- **Core assumption:** The pre-trained embeddings transfer sufficient linguistic knowledge to distinguish Wolayta and Gofa despite neither language being explicitly represented in BERT's training data.
- **Evidence anchors:**
  - [abstract] "the combination of the Bert-based pre-trained language model and LSTM approach performed better, with an F1-score of 0.72"
  - [section 6] "The Bert base uncased model outperforms the Roberta base pre-trained language models...with a macro score of 0.76 precision, 0.69 recall, and 0.72 F1-scores"
  - [corpus] Related work on transformer-based LID (Tonja et al.) confirms BERT+LSTM combinations outperform standalone models for code-mixed text, though direct Omotic language baselines are absent.
- **Break condition:** If the target languages share >80% identical word forms with no distinguishing character n-grams, the embedding-to-sequence pipeline would lack discriminative signal.

### Mechanism 2
- **Claim:** A three-class labeling scheme (wal, gof, wal-gof) explicitly handles cognates and shared vocabulary rather than forcing binary decisions.
- **Mechanism:** Annotators label shared words as "wal-gof," creating a dedicated class for cognates; the model learns to predict this third class when lexical evidence is ambiguous or genuinely overlapping, reducing forced misclassifications.
- **Core assumption:** Annotators can reliably distinguish language-specific words from truly shared words based on usage patterns and speaker knowledge.
- **Evidence anchors:**
  - [section 4.3] "The LID classify into three categories: wal (Wolayta), gof (Gofa) and wal-gof (Wolayta-Gofa)...wal-gof is a LID term utilized when both the Wolayta and Gofa people communicate, and it indicates the presence of shared words"
  - [section 4.4] "we recruited three native speaker annotators for each language...decided on a single label based on a majority vote"
  - [corpus] Weak direct evidence; cognate handling strategies vary across LID literature with no standardized approach for closely-related language pairs.
- **Break condition:** If inter-annotator agreement on the wal-gof class is low (<60%), the label noise would degrade model learning.

### Mechanism 3
- **Claim:** Character-level processing with convolutional feature extraction combined with bidirectional sequence modeling captures fine-grained orthographic differences.
- **Mechanism:** CNN layers extract local character n-gram patterns (e.g., suffix differences like "-aappe" vs. "-afe"); BiLSTM processes these features bidirectionally to capture longer-range morphological patterns that distinguish the languages.
- **Core assumption:** Orthographic differences exist at the character level even when words appear similar, reflecting spoken accent differences mentioned in the paper.
- **Evidence anchors:**
  - [section 6] "the combination of BiLSTM and CNN performed better, with an F1-score of 0.69" (second-best result)
  - [section 1] "Ethiopian Omotic language groups use Latin scripts to write their languages and it's quite similar for those all...identifying one's language from others needs giving high attention to catch up a little difference that might come from spoken accents"
  - [table 1] Shows concrete examples: "Haasayaappe" (Wolayta) vs. "Hayssafe" (Gofa) for "don't speak"
  - [corpus] CNN-based LID approaches (Sun et al.) demonstrate effectiveness for word-level language discrimination in bilingual settings.
- **Break condition:** If orthographic conventions are not standardized and spellings vary widely within each language, character-level patterns become unreliable.

## Foundational Learning

- **Concept:** Language Identification (LID) at word level vs. document level
  - **Why needed here:** This paper operates at word level, which is significantly harder than document-level LID due to limited context; understanding this distinction explains why sophisticated architectures are necessary.
  - **Quick check question:** Can you explain why "Kaallidi" (a shared word) would be harder to classify than a full sentence containing it?

- **Concept:** Transfer learning with multilingual pre-trained models
  - **Why needed here:** BERT-base-uncased was trained primarily on English; understanding how it transfers to unseen languages (via subword tokenization and universal character patterns) is critical for evaluating whether this approach generalizes.
  - **Quick check question:** What fraction of Wolayta/Gofa wordpieces would you expect to appear in BERT's 30,000-token vocabulary?

- **Concept:** Annotation protocols for low-resource languages
  - **Why needed here:** The paper creates the first BLID dataset for these languages; understanding majority-vote labeling with native speakers clarifies both the contribution and potential label noise sources.
  - **Quick check question:** If two annotators label "wal" and one labels "wal-gof," what is the final label and what information is lost?

## Architecture Onboarding

- **Component map:**
  Input word → BERT-base-uncased (768-dim embedding) → LSTM (128 units) → Batch Normalization → Dense (768, ReLU) → Dense (768) → Dropout (0.1) → Dense (3, softmax) → [wal, gof, wal-gof]

- **Critical path:** The embedding-to-LSTM handoff is the key integration point; if BERT tokenization splits words into unfamiliar subwords, the LSTM receives fragmented representations.

- **Design tradeoffs:**
  - BERT-base-uncased vs. RoBERTa: Paper shows BERT outperforms RoBERTa (0.72 vs 0.69 F1), possibly due to different tokenization or pretraining corpora.
  - CNN+BiLSTM vs. BERT+LSTM: The latter adds 0.03 F1 but requires more compute; for resource-constrained deployment, CNN+BiLSTM may be preferable.
  - Three-class vs. two-class: The wal-gof class adds complexity but prevents forced errors on cognates.

- **Failure signatures:**
  - High precision, low recall on minority class (gof): Suggests class imbalance; check distribution in Figure 3.
  - Confusion between wal and wal-gof: Indicates the model cannot distinguish shared vocabulary from Wolayta-specific words.
  - Subword explosion in BERT tokenization: If words tokenize to many pieces, the LSTM may lose word-level signal.

- **First 3 experiments:**
  1. **Baseline replication:** Train logistic regression on character n-grams (TF-IDF) to establish a non-neural baseline; target ~0.47 F1 per paper.
  2. **Ablation study:** Compare BERT+LSTM vs. BERT-only (dense classification head) vs. LSTM-only (random embeddings) to isolate contribution of each component.
  3. **Error analysis on wal-gof class:** Extract all wal-gof predictions and manually inspect whether errors cluster around specific morphological patterns or source domains (religious vs. social media).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the classification performance (currently F1 0.72) be significantly improved by increasing dataset size, enhancing data diversity, or utilizing different feature representations?
- Basis in paper: [explicit] The Conclusion states: "To improve the performance of the bilingual LID model, further research can focus on developing more advanced algorithms, increasing the size and diversity of the training data, and experimenting with different feature representations."
- Why unresolved: The authors established a baseline result but acknowledge that the current dataset and feature set may not have exhausted the potential for higher accuracy given the difficulty of the task.
- What evidence would resolve it: Experiments utilizing expanded corpora (beyond 144k words) or novel embedding techniques that yield a statistically significant increase in F1-score over the current BERT+LSTM baseline.

### Open Question 2
- Question: Does the integration of this BLID model into a pipeline improve the accuracy of Machine Translation (MT) for Wolaita and Gofa?
- Basis in paper: [explicit] The Conclusion suggests: "Additionally, an important application of the bilingual LID model lies in machine translation tasks. By accurately detecting the source language, the appropriate language model for translation can be selected."
- Why unresolved: The paper focuses solely on the identification task; the hypothesized benefit for downstream MT tasks is proposed but not empirically validated in the current study.
- What evidence would resolve it: A comparative evaluation of MT systems with and without the BLID pre-processing module, showing improved translation metrics (e.g., BLEU scores) when the language identifier is employed.

### Open Question 3
- Question: Can the proposed BERT+LSTM architecture generalize effectively to other low-resource Ethiopian languages within the Omotic family?
- Basis in paper: [explicit] The Introduction lists as a contribution: "The proposed model will be applied in the other low-resource Ethiopian languages."
- Why unresolved: The current experimental validation is restricted to Wolaita and Gofa; the model's applicability to related languages mentioned in the background (e.g., Gamo, Dawuro) remains untested.
- What evidence would resolve it: Zero-shot or few-shot transfer learning experiments applying the trained model to datasets from other Omotic languages to evaluate classification performance without extensive retraining.

## Limitations

- Data access remains unclear as no public repository is cited despite claims of an "open source dataset"
- Severe class imbalance appears with Gofa comprising only ~20% of samples, yet no rebalancing strategies are discussed
- No ablation studies provided to quantify individual contributions of BERT embeddings, LSTM sequence modeling, and specific architectural choices

## Confidence

- **High confidence**: The architectural framework (BERT+LSTM) is sound and the three-class annotation scheme is methodologically appropriate for cognate handling
- **Medium confidence**: The 0.72 F1-score is plausibly achieved given the model complexity, but the absolute performance cannot be independently validated without dataset access
- **Low confidence**: Claims about BERT's superiority over RoBERTa and the specific hyperparameter choices lack supporting ablation evidence

## Next Checks

1. Request and analyze the actual dataset to verify class distributions, annotation consistency, and establish reproducibility baselines
2. Conduct controlled ablation experiments comparing (a) frozen vs. fine-tuned BERT embeddings, (b) different sequence lengths, and (c) alternative pre-trained models (mBERT, XLM-R)
3. Implement cross-validation with stratified sampling to assess model stability across different train/test splits and domain variations