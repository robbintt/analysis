---
ver: rpa2
title: Perceptrons and localization of attention's mean-field landscape
arxiv_id: '2601.21366'
source_url: https://arxiv.org/abs/2601.21366
tags:
- theorem
- perceptron
- gradient
- page
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies the mean-field limit of transformer self-attention
  as a Wasserstein gradient flow on the unit sphere, where time corresponds to layers,
  particles to token embeddings, and layer normalization is idealized. The authors
  show that the perceptron block fundamentally reshapes the landscape: for ReLU perceptrons
  in dimension two, all stationary measures are atomic with finite support, and for
  higher dimensions this discreteness holds generically.'
---

# Perceptrons and localization of attention's mean-field landscape

## Quick Facts
- **arXiv ID:** 2601.21366
- **Source URL:** https://arxiv.org/abs/2601.21366
- **Reference count:** 14
- **Primary result:** ReLU perceptrons in mean-field Transformers force stationary token distributions to become discrete, with cluster count scaling as $\sqrt{\beta}$

## Executive Summary
This paper studies the mean-field limit of transformer self-attention as a Wasserstein gradient flow on the unit sphere, where time corresponds to layers, particles to token embeddings, and layer normalization is idealized. The authors show that the perceptron block fundamentally reshapes the landscape: for ReLU perceptrons in dimension two, all stationary measures are atomic with finite support, and for higher dimensions this discreteness holds generically. Even in the descent (repulsive) regime, masses of clusters are bounded away from collapse, scaling with the inverse temperature β, while the number of heavy atoms scales as $\sqrt{\beta}$. These results are complemented by simulations that visualize the emergence of singular stationary measures, confirm scaling laws for cluster counts, and demonstrate robustness across normalization schemes.

## Method Summary
The authors simulate the Wasserstein gradient flow of an interacting particle system on $S^1$ and $S^2$, incorporating a perceptron-induced drift. They initialize 1000 particles uniformly on the sphere and run explicit Euler dynamics with $\Delta t = 0.1$ for various inverse temperatures $\beta \in \{0.01, 0.05, \dots, 50\}$. Perceptron weights are sampled i.i.d. from a standard normal distribution and held fixed. The velocity field combines the attention interaction term $\int e^{\beta x \cdot y} y d\mu(y)$ with the perceptron drift $\sum \omega_j \sigma(a_j \cdot x) a_j$, projected onto the tangent space. Simulations run until particle velocities drop below $10^{-4}$ or cluster count stabilizes over 50 steps.

## Key Results
- ReLU perceptrons force stationary measures to become atomic (discrete) on the sphere
- In descent regime, clusters cannot collapse to single atoms, with mass bounded by ≈0.5742
- Number of heavy atoms scales as $\sqrt{\beta}$ in the repulsive regime
- Stationary measures are localized on active regions defined by perceptron hyperplane intersections

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The perceptron block forces stationary distributions to become discrete rather than continuous
- **Mechanism:** The perceptron's ReLU activation creates a piecewise quadratic, non-analytic external potential. The stationarity condition on the sphere cannot be satisfied by continuous densities; support must collapse onto finite points (atoms) in active regions
- **Core assumption:** Mean-field limit accurately represents $N \to \infty$ behavior with fixed, non-analytic perceptron weights
- **Evidence anchors:**
  - [abstract]: "show that critical points are generically atomic and localized on subsets of the sphere"
  - [section 3.1]: "For ReLU perceptrons in $d=2$, all stationary measures have finite support"
- **Break condition:** Linear activation (making potential analytic) or $d=1$ dimension may allow continuous equilibria

### Mechanism 2
- **Claim:** In descent regime, clusters cannot collapse into single atoms
- **Mechanism:** Large $\beta$ makes attention kernel strictly concave near origin, creating repulsive forces at short ranges. Theorem 3.5 bounds cluster mass at $\le 0.5742$, preventing total collapse
- **Core assumption:** $\beta$ sufficiently large for strict concavity over cluster diameter
- **Evidence anchors:**
  - [section 3.2]: "clusters cannot collapse to a single atom (in contrast to the attractive regime)"
  - [section 4]: "Histograms at final time... visible fraction of the mass remains trapped"
- **Break condition:** In ascent regime, total collapse to single Dirac measure occurs (Theorem 3.7)

### Mechanism 3
- **Claim:** Stationary states determined by perceptron's active region geometry
- **Mechanism:** ReLU perceptron partitions sphere into active regions (where $a_j \cdot x > 0$) and dead zones. Stable atoms only reside in active regions where drift is non-zero
- **Core assumption:** Perceptron weights are non-degenerate (linearly independent)
- **Evidence anchors:**
  - [section 5.3]: "Z consists of finitely many hyperplanes... set of zeros of $g_{\beta,\vartheta}$ in $\mathcal{A}$ is a finite union of discrete sets"
  - [figure 1]: "Background shading differentiates the active regions... darkest gray regions correspond to the 'dead zones'"
- **Break condition:** Orthogonal or symmetric weights may change partitioning, potentially allowing continuous maximizers in edge cases

## Foundational Learning

- **Wasserstein Gradient Flows**
  - **Why needed here:** Models Transformer forward pass as continuous evolution of probability measures over layers
  - **Quick check question:** Can you explain how "particles" (tokens) move along the gradient of an energy functional in a metric space?

- **Atomic vs. Non-Atomic Measures**
  - **Why needed here:** Central result distinguishes between "smooth" distributions and "clumped" distributions on the sphere
  - **Quick check question:** What is the difference between a measure with a continuous density function and a sum of Dirac masses?

- **Inverse Temperature ($\beta$)**
  - **Why needed here:** Scales attention interaction strength; clustering behavior depends critically on large $\beta$
  - **Quick check question:** How does increasing $\beta$ affect the sharpness of the attention kernel $e^{\beta x \cdot y}$?

## Architecture Onboarding

- **Component map:** Tokens → Particle positions on $S^{d-1}$ → Interaction Energy ($E_\beta$) → Drives aggregation/dispersion → External Potential ($v_\vartheta$) → Drives localization to active basins → Gradient Flow ($\nabla E_{\beta,\vartheta}$) → Evolution over layers

- **Critical path:** Verify stationarity condition $\nabla \delta E_{\beta,\vartheta}/\delta \mu = 0$ implies singular support. Implementation must correctly project gradients onto sphere's tangent space ($P^\perp_x$).

- **Design tradeoffs:**
  - **ReLU vs. GeLU:** ReLU creates dead zones where gradient vanishes; GeLU maintains smoothness but still forces atomicity for stable states
  - **Ascent vs. Descent:** Ascent leads to single-point collapse; Descent leads to discrete, multi-cluster spreading

- **Failure signatures:**
  - **Continuous Equilibria:** Uniform distribution at convergence in ReLU-perceptron model suggests degenerate/pathelogical weights
  - **Single Atom Collapse in Descent:** Violation of perceptron weight bounds or too-small $\beta$

- **First 3 experiments:**
  1. **Cluster Counting:** Run gradient flow on $S^1$ with fixed ReLU perceptron weights; plot cluster count vs. $\sqrt{\beta}$ to verify scaling
  2. **Dead Zone Visualization:** Visualize final particle distribution on $S^2$ with hyperplane boundaries $a_j \cdot x = 0$ overlaid
  3. **Anti-Concentration Check:** Measure largest cluster mass across varying $\beta$ in descent regime to confirm bound (≈0.5742)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the unique global minimizer $\mu^*$ be explicitly characterized in terms of its support and mass distribution?
- **Basis in paper:** [explicit] Theorem 3.7(ii) establishes existence and uniqueness of global minimizer and its rotational symmetry, but does not provide explicit solution
- **Why unresolved:** Coupled nonlinear nature of attention energy and perceptron potential prevents simple algebraic resolution
- **What evidence would resolve it:** Closed-form expression for minimizer's density or rigorous bound on number of atoms as function of $\beta$

### Open Question 2
- **Question:** Is the $\sqrt{\beta}$ scaling for number of stationary clusters tight?
- **Basis in paper:** [explicit] Corollary 3.6 provides upper bound proportional to $\sqrt{\beta}$; Figure 9 shows empirical scaling
- **Why unresolved:** Theoretical bound relies on anti-concentration estimates that may not capture precise asymptotic behavior
- **What evidence would resolve it:** Proof of matching lower bound or improved asymptotic analysis refining $\mathcal{O}(\sqrt{\beta})$ estimate

### Open Question 3
- **Question:** Do non-atomic stationary measures exist for non-generic perceptron weights in dimensions $d \ge 2$?
- **Basis in paper:** [explicit] Theorem 3.3(ii) proves atomicity only for dense open set of parameters, leaving complement undetermined
- **Why unresolved:** Transversality arguments apply generically but fail for specific degenerate configurations
- **What evidence would resolve it:** Complete classification of stationary solutions for specific non-generic weights or proof restricting non-atomic solutions to linear activation case

## Limitations

- Theoretical claims rest on mean-field approximation exact only in $N \to \infty$ limit; finite $N=1000$ simulations may show statistical deviations
- Anti-concentration bound (0.5742) depends on geometric properties that may not capture all weight configurations
- Results critically depend on ReLU creating non-analytic dead zones; alternative activations may allow different equilibrium structures
- Fixed perceptron weights sampled once may not reflect learned weights in real Transformers

## Confidence

**High Confidence:** Gradient flow structure and principle that ReLU perceptrons partition sphere into active/dead regions are mathematically rigorous. $\sqrt{\beta}$ scaling relationship is empirically robust.

**Medium Confidence:** Anti-concentration bound preventing single-atom collapse is proven but practical relevance depends on specific configurations. Atomicity claim for $d=2$ is proven but genericity argument assumes non-degenerate weights.

**Low Confidence:** Exact numerical values for cluster masses/counts depend on unreported random seed. Transition between regimes may be sensitive to initialization and uncharacterized hyperparameters.

## Next Checks

1. **Cluster Count Sensitivity Analysis:** Reproduce Figure 9 for multiple random seeds of perceptron weights; quantify variance in cluster count vs. $\sqrt{\beta}$ scaling

2. **Anti-Concentration Bound Verification:** For descent regime ($\beta = 50$), measure largest cluster mass across 10 runs; compare empirical distribution to theoretical bound (0.5742)

3. **Activation Function Comparison:** Repeat $S^1$ simulation with GeLU activation instead of ReLU; compare final distributions to validate ReLU's unique role in forcing discretization