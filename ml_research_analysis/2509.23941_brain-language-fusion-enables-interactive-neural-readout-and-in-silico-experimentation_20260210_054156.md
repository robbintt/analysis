---
ver: rpa2
title: Brain-language fusion enables interactive neural readout and in-silico experimentation
arxiv_id: '2509.23941'
source_url: https://arxiv.org/abs/2509.23941
tags:
- image
- white
- sitting
- cortext
- describe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CorText introduces a brain-language fusion framework that maps
  neural activity into the latent space of large language models, enabling open-ended
  interaction with brain data. The method uses region-wise brain tokenizers to translate
  fMRI responses into language-compatible embeddings, which are then processed by
  a frozen LLM with additional fine-tuning for cross-modal alignment.
---

# Brain-language fusion enables interactive neural readout and in-silico experimentation

## Quick Facts
- **arXiv ID:** 2509.23941
- **Source URL:** https://arxiv.org/abs/2509.23941
- **Reference count:** 40
- **Primary result:** CorText maps brain activity to LLM embeddings, enabling open-ended captioning and VQA with CLIPScore of 0.647 (vs. 0.376 baseline).

## Executive Summary
CorText introduces a brain-language fusion framework that translates fMRI responses into the latent space of large language models (LLMs) for interactive neural readout. The method employs region-wise brain tokenizers to map neural activity into language-compatible embeddings, which are then processed by a frozen LLM with fine-tuning for cross-modal alignment. Applied to a 7T fMRI dataset of natural scene viewing, CorText generates accurate image captions and answers detailed visual questions, even in zero-shot settings. In-silico microstimulation experiments demonstrate causal mappings between neural patterns and language outputs, marking a shift from passive decoding to interactive, generative brain-language interfaces.

## Method Summary
CorText maps fMRI responses from the Natural Scenes Dataset (NSD) to the latent space of Llama 3.1 8B-Instruct via a two-phase training process. First, region-specific brain tokenizers (MLPs) project parcellated brain data into a PCA-reduced embedding space fitted on training captions, then into LLM-compatible embeddings. The frozen LLM is then fine-tuned using QLoRA adapters to align brain-derived embeddings with language. The system generates captions and answers visual questions directly from brain activity, evaluated using CLIPScore and QwenScore metrics. In-silico microstimulation experiments probe the causal relationship between neural activity and generated semantic content.

## Key Results
- **Captioning performance:** CLIPScore of 0.647 on held-out test set, significantly outperforming shuffled-control baseline (0.376).
- **Open-ended interaction:** Successfully answers detailed visual questions beyond the training scope in zero-shot settings.
- **In-silico microstimulation:** Demonstrates systematic, graded effects on semantic content in generated captions, validating causal mapping.

## Why This Works (Mechanism)
CorText works by projecting brain activity into a shared embedding space with language, enabling direct processing by an LLM. The region-wise brain tokenizers learn to encode neural patterns as language-compatible vectors, which the frozen LLM interprets for generation. This fusion allows for open-ended interaction and in-silico experimentation by treating brain data as input to a language model, rather than passive decoding.

## Foundational Learning
- **fMRI parcellation (Schaefer atlas):** Divides brain data into 100 regions for region-wise tokenization; needed for localized encoding of neural patterns; quick check: verify ROI vertex counts match expected.
- **PCA embedding projection (921 components):** Reduces caption embeddings to 95% variance; needed to compress high-dimensional language space for efficient brain mapping; quick check: confirm explained variance ratio.
- **QLoRA fine-tuning:** Adapts frozen LLM to brain-derived embeddings; needed for cross-modal alignment without full model training; quick check: monitor adapter gradient norms.
- **CLIPScore evaluation:** Measures caption-image similarity using CLIP; needed to quantify generative quality against ground truth; quick check: ensure correct image-caption pairing.
- **In-silico microstimulation:** Modifies brain-derived embeddings to study causal semantic effects; needed to validate interpretability of learned mapping; quick check: verify stimulation magnitude bounds.
- **Zero-shot VQA generalization:** Applies model to unseen question-answer pairs; needed to demonstrate flexibility beyond training scope; quick check: compare to in-domain performance.

## Architecture Onboarding
- **Component map:** fMRI ROIs (100) -> Brain Tokenizers (MLPs) -> PCA projection (921) -> LLM embeddings (4096) -> Frozen Llama 3.1 8B-Instruct -> Generated captions
- **Critical path:** Brain data → Tokenizers → PCA → LLM → Output
- **Design tradeoffs:** Frozen LLM ensures stability but limits adaptation; PCA compression balances efficiency and fidelity; region-wise tokenizers enable localized encoding but require careful ROI alignment.
- **Failure signatures:** Low CLIPScore (< 0.40) suggests poor cross-modal alignment; collapsed captions indicate tokenizer overfitting or misalignment; dimension mismatch errors point to ROI vertex count issues.
- **First experiments:**
  1. Train tokenizers on a small subset and verify embedding projections match expected dimensions.
  2. Test frozen LLM generation with random noise embeddings to establish baseline output.
  3. Conduct PCA ablation (vary components) to confirm 921 is optimal.

## Open Questions the Paper Calls Out
None

## Limitations
- Missing exact prompt engineering details and random seed values may affect reproducibility.
- Frozen LLM behavior in interactive querying mode is not fully characterized.
- In-silico microstimulation assumes causal interpretability without external validation.

## Confidence
- Method architecture & training: **Medium**
- Quantitative evaluation results: **High**
- Qualitative interaction claims: **Low**
- In-silico microstimulation validity: **Low**

## Next Checks
1. Release or reimplement the exact prompt template and random sampling logic used for the Llama chat interface to ensure consistent open-ended interaction.
2. Conduct ablation studies varying the number of PCA components (e.g., 800, 921, 1000) to confirm that 921 is optimal for balancing reconstruction and alignment.
3. Replicate the shuffled-control experiment with multiple random seeds to confirm that the 0.376 baseline is stable and not an artifact of a single split.