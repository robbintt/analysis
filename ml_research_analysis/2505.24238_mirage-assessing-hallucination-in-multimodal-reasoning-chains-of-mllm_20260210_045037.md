---
ver: rpa2
title: 'MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM'
arxiv_id: '2505.24238'
source_url: https://arxiv.org/abs/2505.24238
tags:
- reasoning
- hallucination
- step
- claim
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MIRAGE, a benchmark specifically designed
  to assess reasoning-induced hallucinations in multimodal large language models (MLLMs).
  Unlike prior benchmarks, MIRAGE isolates reasoning errors by focusing on questions
  where visual perception is accurate but logical reasoning fails.
---

# MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM

## Quick Facts
- **arXiv ID:** 2505.24238
- **Source URL:** https://arxiv.org/abs/2505.24238
- **Reference count:** 40
- **Primary result:** MIRAGE benchmark isolates reasoning hallucinations in MLLMs; Logos method reduces logical hallucinations via curriculum reinforcement learning

## Executive Summary
This paper introduces MIRAGE, a benchmark specifically designed to assess reasoning-induced hallucinations in multimodal large language models (MLLMs). Unlike prior benchmarks, MIRAGE isolates reasoning errors by focusing on questions where visual perception is accurate but logical reasoning fails. The benchmark includes 1,329 questions annotated with multi-level reasoning chains, steps, and claims, along with auxiliary hints. It introduces three evaluation metrics—accuracy, factuality, and LLMs hallucination score (LHS)—to comprehensively assess hallucinations at different reasoning levels. Experimental results reveal that model scale, training data quality, and training stages significantly influence logical, fabrication, and factual hallucinations, but show limited improvement in spatial hallucinations, highlighting weaknesses in visual reasoning. To address these challenges, the paper proposes Logos, a method combining curriculum reinforcement fine-tuning and collaborative hint inference, which effectively reduces logical hallucinations and improves answer accuracy. Logos establishes a strong baseline for future research on multimodal reasoning reliability.

## Method Summary
MIRAGE is a benchmark for evaluating reasoning hallucinations in MLLMs, containing 1,329 questions where visual perception is accurate but reasoning errors persist. The Logos method addresses these hallucinations through curriculum reinforcement fine-tuning (CRFT) and collaborative hint inference (CHI). CRFT uses Group Relative Policy Optimization (GRPO) with Online Reward Filtration (ORF) to progressively filter training samples by difficulty. CHI uses an auxiliary LLM to generate topic-specific and question-specific hints before inference. The evaluation framework includes accuracy (final answer matching), factuality (step/claim F1), and LHS (multi-LLM ensemble scoring against reference chains).

## Key Results
- MIRAGE effectively isolates reasoning hallucinations from perception errors, revealing that larger models and better pretraining data reduce logical, fabrication, and factual hallucinations
- Logos significantly reduces logical hallucinations and improves answer accuracy through curriculum reinforcement fine-tuning and collaborative hint inference
- Spatial hallucinations persist despite model scaling and training improvements, indicating fundamental limitations in visual reasoning capabilities
- The three-tier evaluation framework (accuracy, factuality, LHS) provides comprehensive diagnosis of hallucination types at different reasoning levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Isolating reasoning hallucinations by selecting questions where perception is accurate but reasoning fails enables targeted diagnosis.
- Mechanism: MIRAGE filters questions through multiple MLLMs to verify consistent visual perception (via generated descriptions validated by a secondary LLM), then retains only those where reasoning errors persist. This creates a benchmark that separates reasoning failures from perceptual failures.
- Core assumption: Perception and reasoning can be cleanly separated in multimodal tasks, and reasoning errors can be identified independently of visual encoding quality.
- Evidence anchors:
  - [abstract] "MIRAGE isolates reasoning hallucinations by constructing questions where input images are correctly perceived by MLLMs yet reasoning errors persist."
  - [section] "For difficulty curation, we use three open-source MLLMs to generate image descriptions, retaining only questions where these descriptions are consistently accurate (verified by a secondary LLM) but lead to frequent reasoning errors."
  - [corpus] Weak corpus support—related work (HalluShift++, Semantic Curriculum Preference Optimization) addresses hallucination mitigation but not the specific perception/reasoning separation mechanism.
- Break condition: If MLLM perception is systematically flawed in ways that escape description-based verification, the isolation mechanism fails to distinguish hallucination sources.

### Mechanism 2
- Claim: Increasing the probability of logic-consistent reasoning chains through curriculum-based reinforcement learning reduces logical hallucinations.
- Mechanism: Logos uses Group Relative Policy Optimization (GRPO) with Curriculum Reinforcement Fine-Tuning (CRFT). CRFT progressively filters training samples: first keeping only questions where the model can sample at least one correct response (ensuring non-zero advantages), then increasing difficulty by retaining questions with lower average accuracy. Online Reward Filtration (ORF) discards samples where all responses receive identical rewards, preventing gradient degradation.
- Core assumption: Logic-consistent reasoning chains are learnable through reward optimization, and curriculum-based difficulty progression improves sample efficiency without catastrophic forgetting of visual perception.
- Evidence anchors:
  - [abstract] "Logos...combines curriculum reinforcement fine-tuning to encourage models to generate logic-consistent reasoning chains by stepwise reducing learning difficulty."
  - [section] "During the first stage, we keep questions with average accuracy reward > 0 to ensure that π can sample at least one reasoning chain with correct answer and logic-consistent reasoning during training."
  - [corpus] Moderate corpus support—related work (Mitigating Visual Hallucinations via Semantic Curriculum Preference Optimization) validates curriculum approaches for hallucination reduction.
- Break condition: If reasoning hallucinations stem from fundamental knowledge gaps rather than reasoning chain optimization, reinforcement learning cannot introduce new factual knowledge.

### Mechanism 3
- Claim: Multi-LLM ensemble evaluation with reference chain comparison approximates uncertainty estimation for hallucination quantification.
- Mechanism: The LLMs Hallucination Score (LHS) uses M LLM judges to evaluate responses against N reference chains (ground truth plus LLM-rewritten variants) across five dimensions: factual accuracy, logical consistency, reasoning completeness, conceptual accuracy, and strategy appropriateness. Lower mean LHS indicates higher hallucination rates.
- Core assumption: LLM judges can reliably detect reasoning hallucinations when provided with ground-truth reference chains, and ensemble scoring reduces individual judge biases.
- Evidence anchors:
  - [section] "We further conduct consistency checks on LHS using human evaluators. We randomly sample 100 responses...comparing the human evaluation from three experts. The average difference rate is 7.5%, showing the reliability of LHS."
  - [corpus] Weak corpus support—related benchmarks (EGOILLUSION, MM-THEBench) evaluate hallucinations but use different measurement approaches.
- Break condition: If LLM judges share systematic blind spots for certain hallucination types, or if reference chains contain undetected errors, LHS scores may not correlate with actual reasoning quality.

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Reasoning in Multimodal Settings**
  - Why needed here: MIRAGE evaluates hallucinations in reasoning chains (intermediate steps and claims), not just final answers. Understanding how CoT structures multimodal reasoning is essential for interpreting factuality scores and hallucination type detection.
  - Quick check question: Given an image of a triangle with labeled angles, can you trace how a model might derive the final answer through intermediate logical steps? Where might reasoning diverge from perception?

- Concept: **Group Relative Policy Optimization (GRPO) for Vision-Language Models**
  - Why needed here: Logos uses GRPO (not standard PPO) because it computes advantages relative to sampled response groups rather than using a separate value model. This is more efficient for multimodal reasoning where value functions are harder to learn.
  - Quick check question: How does GRPO differ from PPO in computing advantages? Why might this matter when G=8 rollout samples per question have varying quality?

- Concept: **Hallucination Taxonomy: Perception vs. Reasoning**
  - Why needed here: MIRAGE explicitly distinguishes five reasoning hallucination types (logical, spatial, factual, context, fabrication) from perception-induced errors. This taxonomy guides both evaluation and mitigation strategies.
  - Quick check question: If a model correctly identifies objects in an image but claims a false spatial relationship between them (e.g., "the bench is in front of the train" when it's behind), which hallucination type is this? Why might spatial hallucinations be harder to reduce through reinforcement learning?

## Architecture Onboarding

- Component map:
  ```
  MIRAGE Pipeline:
  Data Collection (18K raw) →
    Difficulty Curation (perception verification via 3 MLLMs + secondary LLM) →
    Balance Curation (7-topic stratified sampling) →
    Reasoning Chain Annotation (O3-mini → DeepSeek-R1 refinement) →
    Human-AI Verification →
    Step/Claim Extraction (DeepSeek-V3 with few-shot prompts) →
    Final Dataset (1,329 questions with multi-level annotations)

  Evaluation Framework:
  Accuracy (final answer matching via GPT-4o) ||
  Factuality (step/claim F1 via LLM matching against ground-truth) ||
  LHS (multi-LLM ensemble scoring against reference chains)

  Logos Training:
  Base Model (Qwen2.5-VL-7B/3B) →
    CRFT Stage 1 (filter samples with avg_acc > 0) →
    G-round sampling (G=8) →
    ORF (discard uniform-reward samples) →
    GRPO optimization (format + accuracy rewards) →
    [Optional] CRFT Stage k (filter samples with avg_acc < 0.5)

  Logos Inference:
  Question → Question Classification (LLM ϕ) →
    Topic Hint + Question Hint (from ϕ) →
    Optimized MLLM π → Response
  ```

- Critical path:
  1. **Perception verification during data curation**—if this fails, the benchmark conflates perception and reasoning errors
  2. **Ground-truth reasoning chain quality**—annotation errors propagate to factuality evaluation and LHS reference chains
  3. **CRFT sample filtering**—if initial samples are too easy/hard, advantages become zero and optimization stalls
  4. **Reward signal design**—format reward ensures parseable outputs; accuracy reward guides toward correct reasoning

- Design tradeoffs:
  - **Dataset scale vs. isolation purity**: 18K → 1,329 questions ensures clean perception/reasoning separation but limits statistical power for fine-grained analysis
  - **Annotation automation vs. quality**: Two-stage LLM refinement (O3-mini → DeepSeek-R1) achieves 73.7% accuracy at ~$22 cost vs. estimated $200+ for pure O1 annotation, but human verification still required for 26.3% of chains
  - **Evaluation granularity vs. cost**: Three-tier metrics (accuracy/factuality/LHS) provide comprehensive diagnosis but require multiple LLM API calls per sample
  - **CRFT stages vs. training efficiency**: k=1 achieves near-optimal results; additional stages show marginal gains (Table 15) with increased training time

- Failure signatures:
  - **All-zero advantages during GRPO**: Occurs when training samples are uniformly too easy (all responses correct) or too hard (all incorrect)—ORF should discard these, but if too many samples are discarded, training destabilizes
  - **Spatial Hallucinations persisting after Logos**: Expected behavior—paper explicitly notes "we do not find significant hallucination mitigation on spatial and factuality hallucination" because "reinforcement learning does not introduce new knowledge"
  - **LHS score not correlating with accuracy**: Indicates judge calibration issues—verify that LLM judges use consistent scoring templates and reference chains are error-free
  - **CHI degrading base model performance**: Paper notes this occurs—"directly adopt CHI on base model does not lead to performance improvement"—CHI is designed for CRFT-optimized models only

- First 3 experiments:
  1. **Reproduce perception isolation validation**: Sample 50 questions from MIRAGE, run image description generation through multiple MLLMs, verify that visual perception is consistently accurate while reasoning fails. Check whether description-based verification misses subtle perceptual errors.
  2. **Ablate CRFT difficulty thresholds**: Train Logos-7B with varying initial average accuracy thresholds (0.1, 0.3, 0.5) and measure training stability (gradient variance, sample retention rate) and final MIRAGE accuracy. Identify the threshold that balances sample retention with meaningful difficulty progression.
  3. **Validate LHS against human judgment**: Sample 30 responses across high/medium/low LHS scores, have 3 human experts rate hallucination severity using the 5-dimension schema, compute inter-rater agreement and correlation with LHS. Identify dimensions where LLM judges systematically deviate from human judgment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural or training interventions can effectively mitigate spatial hallucinations in MLLMs, given that model scaling, data scaling, and improved pretraining show limited improvement?
- Basis in paper: [explicit] The paper states "current MLLMs show no effective improvement on spatial hallucinations caused by misinterpreted spatial relationships, indicating their limited visual reasoning capabilities" and "spatial hallucination does not significantly reduced, which indicates that current MLLMs still show weak visual reasoning capabilities."
- Why unresolved: Standard scaling approaches fail for spatial reasoning errors; Logos only reduces logical and fabrication hallucinations, not spatial ones.
- What evidence would resolve it: New methods specifically targeting visual-spatial reasoning that show statistically significant reductions in spatial hallucination rates across multiple model scales.

### Open Question 2
- Question: What theoretical mechanisms explain why MLLMs suffer from reasoning hallucinations, and do these differ fundamentally from perception-induced hallucinations?
- Basis in paper: [explicit] The limitations section states "the theoretical analysis of why MLLMs suffer from reasoning hallucination is still insufficient."
- Why unresolved: The paper provides empirical correlation analysis but lacks cognitive or computational theory explaining hallucination origins.
- What evidence would resolve it: A theoretical framework validated through controlled experiments isolating specific reasoning failure modes, potentially with causal intervention studies.

### Open Question 3
- Question: Why do SFT-based hallucination mitigation methods succeed for larger models (~72B parameters) but often degrade performance for smaller models (~7B parameters)?
- Basis in paper: [inferred] From Section 6.1 and Appendix D.4: "on 72B MLLMs, introducing SFT can lead to better accuracy... Nevertheless, on 7B MLLMs, only Mulberry surpasses the base model by 3.1 on accuracy, while other methods do not lead to performance improvement."
- Why unresolved: The paper suggests model capacity plays a role but doesn't identify the threshold or mechanism.
- What evidence would resolve it: Systematic study varying model sizes with controlled SFT interventions, identifying the critical capacity threshold and analyzing internal representations.

## Limitations

- The paper acknowledges that theoretical analysis of why MLLMs suffer from reasoning hallucinations is still insufficient, limiting understanding of fundamental failure mechanisms
- Spatial hallucinations persist despite model scaling and training improvements, indicating fundamental limitations in visual reasoning capabilities that current methods cannot address
- The 18K→1,329 question curation process may introduce selection bias that limits generalizability of the benchmark findings

## Confidence

- **Perception/reasoning isolation mechanism**: Medium confidence - theoretically sound but relies on multiple LLM validation steps that may miss subtle perceptual errors
- **Logos training methodology**: Medium confidence - well-specified but depends on reward signal quality and the assumption that reasoning improvements can be learned without introducing new factual knowledge
- **LHS evaluation reliability**: Medium confidence - shows promising inter-rater agreement (7.5% difference with human experts) but may be sensitive to reference chain quality and judge calibration
- **Spatial hallucination persistence**: High confidence - paper explicitly notes this limitation and provides empirical evidence across multiple model scales

## Next Checks

1. **Perception Isolation Validation**: Sample 50 MIRAGE questions and independently verify that visual perception is consistently accurate while reasoning fails, checking whether the description-based verification misses subtle perceptual errors that would confound the reasoning hallucination diagnosis.

2. **CRFT Threshold Sensitivity**: Train Logos-7B with varying initial average accuracy thresholds (0.1, 0.3, 0.5) to identify the optimal balance between sample retention and meaningful difficulty progression, measuring training stability and final MIRAGE accuracy.

3. **LHS Human Correlation Study**: Sample 30 responses across the LHS score spectrum and have human experts rate hallucination severity using the 5-dimension schema, computing inter-rater agreement and correlation with LHS to identify systematic biases in the LLM judge evaluation.