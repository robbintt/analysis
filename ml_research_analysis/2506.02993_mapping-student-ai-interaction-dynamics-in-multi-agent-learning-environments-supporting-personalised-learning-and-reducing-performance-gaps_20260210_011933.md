---
ver: rpa2
title: 'Mapping Student-AI Interaction Dynamics in Multi-Agent Learning Environments:
  Supporting Personalised Learning and Reducing Performance Gaps'
arxiv_id: '2506.02993'
source_url: https://arxiv.org/abs/2506.02993
tags:
- learning
- students
- interaction
- knowledge
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how university students engage with multiple
  AI agents in an online learning environment and how such interactions influence
  learning outcomes and motivation. Using a multi-agent AI course system (MAIC), researchers
  analyzed 305 students'' dialogue interactions with AI teacher and peer agents, revealing
  two main engagement patterns: knowledge co-construction and co-regulation.'
---

# Mapping Student-AI Interaction Dynamics in Multi-Agent Learning Environments: Supporting Personalised Learning and Reducing Performance Gaps

## Quick Facts
- arXiv ID: 2506.02993
- Source URL: https://arxiv.org/abs/2506.02993
- Reference count: 0
- Primary result: Lower prior knowledge students achieved higher learning gains through knowledge co-construction sequences with AI agents, while higher prior knowledge students engaged more in co-regulation but showed limited measurable improvement.

## Executive Summary
This study investigates how university students engage with multiple AI agents in an online learning environment and how such interactions influence learning outcomes and motivation. Using a multi-agent AI course system (MAIC), researchers analyzed 305 students' dialogue interactions with AI teacher and peer agents, revealing two main engagement patterns: knowledge co-construction and co-regulation. Lower prior knowledge students showed higher learning gains and post-course motivation through knowledge co-construction sequences, while higher prior knowledge students engaged more in co-regulation but demonstrated limited learning improvement. Technology acceptance increased across all groups. The findings suggest multi-agent AI systems can support personalized learning and reduce performance gaps by adapting to students' varying needs.

## Method Summary
The study used a multi-agent AI course system (MAIC) with LLM-driven teacher and peer agents to deliver an artificial general intelligence course to 305 university students. Student-AI dialogue interactions were logged and coded using a modified Flanders Interaction Analysis System (FIAS) with LLM-assisted classification (achieving 0.97 inter-rater reliability). Lag sequential analysis identified statistically significant behavioral transition patterns between student and AI agent codes. Learning gains were measured through parallel pre/post-tests, with students stratified into low and high prior knowledge groups based on pre-test performance (top/bottom 30% percentiles). Technology acceptance and motivation were measured using validated scales before and after the course.

## Key Results
- Students with lower prior knowledge showed higher learning gains (M = 1.88, SD = 1.36) through knowledge co-construction sequences
- Students with higher prior knowledge engaged more in co-regulation behaviors (12.9% vs 11.2%) but exhibited limited learning improvement
- Technology acceptance scores increased across all groups (from 3.44 to 4.04)
- Two main engagement patterns emerged: knowledge co-construction and co-regulation, with significant behavioral sequence differences between prior knowledge groups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lower prior knowledge students achieve higher learning gains through co-construction of knowledge sequences with AI agents.
- Mechanism: Students with less prior knowledge engage in frequent question-asking and negotiation behaviors (SB1, SB4), which trigger scaffolded responses from AI agents. This reduces extraneous cognitive load by providing on-demand guidance, freeing working memory for germane cognitive processing and schema construction.
- Core assumption: The cognitive load reduction effect transfers across domains; the assessment instrument captured actual learning gains rather than surface-level familiarity.
- Evidence anchors:
  - [abstract] "students with lower prior knowledge relied more on co-construction of knowledge sequences, showing higher learning gains and post-course motivation"
  - [section 4.3] "Students in the low prior knowledge group achieved the greatest learning gains (M = 1.88, SD = 1.36)... F(2, 69) = 22.95, p < .001, partial η² = .400"
  - [corpus] Related work on student-AI interaction clusters (arxiv 2503.01694) similarly finds differentiated engagement profiles, though mechanism evidence is correlational.
- Break condition: If content difficulty exceeds the scaffolding capacity of AI responses, co-construction sequences may fail to produce schema formation.

### Mechanism 2
- Claim: Higher prior knowledge students engage more in co-regulation behaviors but show limited measurable learning improvement.
- Mechanism: Students with established schemas use AI agents primarily for navigation, pacing control, and confirmation rather than knowledge acquisition. This reflects the expertise reversal effect—support designed for novices provides diminishing returns and may even interfere with expert self-regulation.
- Core assumption: The ceiling effect interpretation is correct; the post-test was sensitive enough to detect incremental gains if they existed.
- Evidence anchors:
  - [abstract] "students with higher prior knowledge engaged more in co-regulation behaviors but exhibited limited learning improvement"
  - [section 4.2] "students with higher prior knowledge demonstrated significantly more regulatory behaviors, including monitoring and regulating the learning process (12.9% vs. 11.2%), managing AI peers (7.5% vs. 4.5%)"
  - [corpus] Limited direct corpus evidence on expertise reversal in multi-agent systems; this inference should be treated as hypothesis-generating.
- Break condition: If high prior knowledge students face genuinely novel content, co-regulation alone will fail; they require different agent behaviors (deep elaboration prompts, perspective-taking challenges).

### Mechanism 3
- Claim: Multi-agent role differentiation enables adaptive support across learner proficiency levels.
- Mechanism: Distinct agent roles (Teacher for authoritative responses, Peers for idea initiation and negotiation) create multiple interaction pathways. Students self-select into pathways matching their needs—novices pursue cognitive scaffolding via Teacher responses; advanced learners pursue regulatory coordination via peer-like interactions.
- Core assumption: Students accurately perceive their own learning needs and select appropriate interaction patterns.
- Evidence anchors:
  - [abstract] "multi-agent AI systems can adapt to students' varying needs, support differentiated engagement"
  - [section 4.1.2] "AI peers' behaviors were more concentrated on 'initiate ideas', accounting for about 36.86%... AI Teacher... over half of their behaviors during interactions were responding to students (52.51%)"
  - [corpus] FACET (arxiv 2601.22788) demonstrates multi-agent teacher support for differentiated instruction, providing convergent architecture evidence.
- Break condition: If agent role boundaries become ambiguous or responses are inconsistent, students cannot form stable expectations, and adaptive self-selection fails.

## Foundational Learning

- Concept: Lag Sequential Analysis (LSA)
  - Why needed here: The paper uses LSA to identify statistically significant behavioral transition sequences. Understanding that LSA computes conditional probabilities of behavior B following behavior A is essential to interpret the interaction pattern claims.
  - Quick check question: Given a sequence of coded behaviors [TB1 → SB2 → ASB2], what does LSA tell you that simple frequency counting cannot?

- Concept: Cognitive Load Theory (Schema Formation)
  - Why needed here: The mechanism for why lower prior knowledge students benefit relies on extraneous load reduction enabling germane processing. Without this theoretical grounding, the differential gains result appears correlational rather than explanatory.
  - Quick check question: Why would the same AI scaffold reduce cognitive load for a novice but potentially increase it for an expert?

- Concept: Socially Shared Regulation of Learning (SSRL)
  - Why needed here: The paper positions co-regulation as a form of SSRL where regulation is co-constructed through interaction. This distinguishes the observed patterns from simple self-regulation with tool support.
  - Quick check question: In SSRL, how does regulatory responsibility differ from individual self-regulation, and what behavioral markers would indicate the shift?

## Architecture Onboarding

- Component map:
  MAIC Platform -> LLM-driven multi-agent learning environment
  Interface Agents -> AI Teacher (lecture delivery, Q&A), AI Teaching Assistant (classroom management), AI Peers (Sparker, Questioner, Thinker, Notetaker)
  Backend Agents -> Classroom Situation Analysis, Learning Situation Analysis, Classroom Director (orchestration)
  Data Pipeline -> Dialogue logging -> FIAS-based coding -> LSA pattern detection

- Critical path:
  1. Course materials uploaded -> knowledge extraction agents generate slides/scripts
  2. Instructor reviews/edits generated content
  3. AI Teacher delivers lecture with synthesized speech
  4. Students engage in observation or interaction mode
  5. Analytical agents monitor engagement, trigger appropriate agent responses
  6. Dialogue data logged for post-hoc sequential analysis

- Design tradeoffs:
  - Static vs. adaptive agent roles: Current design uses fixed role definitions; adaptive roles based on real-time learner modeling could improve outcomes but increase system complexity.
  - Observation vs. interaction mode: Observation provides structured exposure; interaction enables personalization but risks off-topic drift.
  - Single vs. multiple LLM backends: Paper used GPT-4-turbo and GLM-4 for reliability; single model reduces integration complexity but increases single-point-of-failure risk.

- Failure signatures:
  - Low interaction counts (Module 4-6 decline to ~3.6 avg messages) suggest engagement decay
  - AI peer responses not triggering sustained follow-up indicates weak peer agent dialogic scaffolding
  - High prior knowledge students showing learning loss (M = -1.09) signals potential mismatch between content challenge and learner readiness

- First 3 experiments:
  1. **Replicate with delayed post-test**: Administer assessment 2-4 weeks post-course to distinguish short-term performance gains from retained learning.
  2. **A/B test adaptive agent roles**: Randomly assign students to static vs. prior-knowledge-adapted agent behavior conditions; compare learning gains and interaction patterns.
  3. **Enhance peer agent dialogic capability**: Redesign peer agents with explicit follow-up prompting behaviors; measure whether peer-initiated sequences increase and whether this affects high prior knowledge student outcomes.

## Open Questions the Paper Calls Out

- Is the observed increase in technology acceptance driven by the multi-agent interaction experience or the AI-focused course content?
  - The authors explicitly state, "an important question remains as to what caused the increase in technology acceptance – was it the experience of learning with multiple AI agents, or the nature of the course content being about artificial general intelligence?"
  - This question remains unresolved because the study design confounded the delivery mechanism (AI agents) with the subject matter (AI theory).

- How can multi-agent systems be dynamically adapted to mitigate expertise reversal effects for students with high prior knowledge?
  - The paper notes that high prior knowledge students showed limited learning gains, possibly due to "expertise reversal effect" where "overly detailed guidance can become redundant."
  - This question remains unresolved because the current system employed static agent behaviors without testing adaptive mechanisms.

- Do the learning gains achieved by low prior knowledge students in multi-agent environments persist over the long term?
  - The authors acknowledge that the post-test was "administered immediately after course completion, capturing short-term learning gains only" and suggest future research examine "long-term retention."
  - This question remains unresolved because immediate testing cannot distinguish between temporary performance boosts and durable knowledge construction.

## Limitations

- The lack of a control group prevents definitive attribution of observed gains to the multi-agent system rather than other factors like test-retest effects or natural maturation.
- The post-test was administered immediately after instruction, making it unclear whether gains reflect durable learning or temporary familiarity.
- The low engagement in later modules (average ~3.6 messages) suggests motivational decay that could bias interaction pattern analysis.

## Confidence

**High confidence**: The identification of distinct behavioral transition sequences through LSA and the statistical significance of engagement pattern differences between prior knowledge groups are methodologically sound given the data and analysis approach.

**Medium confidence**: The interpretation that knowledge co-construction sequences cause learning gains for low prior knowledge students is plausible but correlational; without experimental manipulation, causal inference remains tentative.

**Low confidence**: The expertise reversal effect interpretation for high prior knowledge students showing learning loss and limited improvement is speculative; the assessment may have been insensitive to incremental gains, and ceiling effects cannot be ruled out.

## Next Checks

1. **Delayed post-test validation**: Administer the same assessment 2-4 weeks after course completion to distinguish short-term performance gains from retained learning, addressing the immediate post-test limitation.

2. **Content validity assessment**: Conduct expert review of the pre/post-test items against course objectives and learning outcomes to establish whether the assessment captures the intended knowledge domain.

3. **Controlled comparison study**: Implement a between-subjects design comparing the multi-agent system against traditional online instruction or single-agent AI tutoring, controlling for content and duration to isolate the multi-agent effect on learning outcomes.