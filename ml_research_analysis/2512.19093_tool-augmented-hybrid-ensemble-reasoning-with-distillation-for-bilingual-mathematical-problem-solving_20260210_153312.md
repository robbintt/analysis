---
ver: rpa2
title: Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical
  Problem Solving
arxiv_id: '2512.19093'
source_url: https://arxiv.org/abs/2512.19093
tags:
- reasoning
- mathematical
- problems
- ensemble
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HERALD addresses bilingual mathematical problem solving by integrating
  NuminaMath-7B-TIR, GPT-4o, and Mistral-7B through adaptive ensemble reasoning, confidence-calibrated
  voting, and tool-augmented reinforcement learning. The framework employs progressive
  curriculum fine-tuning and knowledge distillation to preserve mathematical reasoning
  across languages while optimizing computational efficiency.
---

# Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving

## Quick Facts
- arXiv ID: 2512.19093
- Source URL: https://arxiv.org/abs/2512.19093
- Authors: Peiqing Lu; Yuan Zhang; Haoyun Zhang; Jiasen Zheng; Kejian Tong; Wenjun Wu
- Reference count: 17
- HERALD achieves 83.7% accuracy on a 1,247-problem test set, outperforming GPT-4o by 15.6% with 12.5% faster inference

## Executive Summary
HERALD introduces a novel framework for bilingual mathematical problem solving by integrating multiple reasoning models through adaptive ensemble reasoning and tool augmentation. The system combines NuminaMath-7B-TIR, GPT-4o, and Mistral-7B with confidence-calibrated voting and reinforcement learning with symbolic verification. Progressive curriculum fine-tuning and knowledge distillation enable the framework to preserve mathematical reasoning capabilities across languages while maintaining computational efficiency. The approach demonstrates superior performance compared to state-of-the-art models while reducing inference latency through adaptive model routing.

## Method Summary
HERALD employs a hybrid ensemble reasoning architecture that integrates three distinct models: NuminaMath-7B-TIR for mathematical reasoning, GPT-4o for general reasoning, and Mistral-7B for language understanding. The system uses confidence-calibrated voting to aggregate predictions from the ensemble, with tool augmentation providing symbolic verification and computational support. Progressive curriculum fine-tuning trains the system on increasingly complex problems, while knowledge distillation preserves reasoning capabilities across model updates. The framework implements adaptive model routing to optimize computational efficiency and reduce latency without sacrificing accuracy.

## Key Results
- Achieves 83.7% accuracy on a 1,247-problem bilingual test set
- Outperforms GPT-4o by 15.6% while maintaining 12.5% faster inference
- Tool integration provides the largest accuracy gain (11.2%) in ablation studies
- Progressive fine-tuning contributes 7.3% accuracy improvement
- Ensemble voting adds 6.6% accuracy compared to individual models

## Why This Works (Mechanism)
The framework's success stems from combining complementary strengths of multiple models through adaptive ensemble reasoning. By leveraging NuminaMath-7B-TIR's specialized mathematical reasoning, GPT-4o's general reasoning capabilities, and Mistral-7B's language understanding, HERALD can handle complex bilingual problems more effectively than single-model approaches. The confidence-calibrated voting mechanism ensures that the most reliable predictions are weighted more heavily, while tool augmentation provides symbolic verification that catches errors in intermediate reasoning steps. Progressive curriculum fine-tuning allows the system to build upon increasingly complex problem-solving skills without catastrophic forgetting.

## Foundational Learning
- **Ensemble reasoning**: Combining multiple models' predictions improves accuracy by leveraging complementary strengths
  - *Why needed*: No single model excels at all aspects of bilingual mathematical reasoning
  - *Quick check*: Verify that ensemble accuracy exceeds individual model performance
- **Confidence calibration**: Weighting model predictions based on estimated reliability improves decision quality
  - *Why needed*: Different models have varying expertise across problem types and languages
  - *Quick check*: Compare weighted voting accuracy against simple majority voting
- **Tool augmentation**: Integrating external tools for symbolic verification catches reasoning errors
  - *Why needed*: Large language models can make systematic errors in mathematical computations
  - *Quick check*: Measure error reduction when tools are enabled versus disabled
- **Knowledge distillation**: Transferring knowledge from larger to smaller models preserves capabilities
  - *Why needed*: Enables efficient deployment while maintaining reasoning quality
  - *Quick check*: Compare distilled model performance against teacher model
- **Progressive curriculum learning**: Gradually increasing problem complexity prevents catastrophic forgetting
  - *Why needed*: Models need to build complex reasoning skills incrementally
  - *Quick check*: Monitor performance on previously seen problem types during training

## Architecture Onboarding

### Component Map
NuminaMath-7B-TIR + GPT-4o + Mistral-7B -> Confidence-Calibrated Voting -> Tool Augmentation (Symbolic Verification) -> Adaptive Model Routing -> Output

### Critical Path
1. Problem input received and routed to appropriate models
2. Individual models generate solutions with confidence scores
3. Confidence-calibrated voting aggregates predictions
4. Tool augmentation verifies symbolic computations
5. Adaptive routing selects optimal model combination
6. Final answer output with uncertainty estimate

### Design Tradeoffs
The framework trades increased model complexity and computational overhead for improved accuracy and multilingual capability. Using three distinct models requires more resources than single-model approaches but provides complementary reasoning strengths. The confidence-calibrated voting mechanism adds computational cost but significantly improves accuracy. Tool augmentation increases latency but catches critical mathematical errors. Progressive curriculum fine-tuning requires longer training times but prevents catastrophic forgetting and builds more robust reasoning capabilities.

### Failure Signatures
- **Low confidence consensus**: When all models disagree and have low confidence scores, indicating the problem may be outside the training distribution
- **Tool verification failure**: When symbolic verification tools detect errors in model-generated solutions
- **Language bias**: When performance significantly differs between Chinese and English problems, suggesting incomplete bilingual reasoning
- **Catastrophic forgetting**: When performance on previously mastered problem types degrades during progressive fine-tuning

### First 3 Experiments
1. **Ablation study replication**: Disable each component (tool augmentation, progressive fine-tuning, ensemble voting) individually to verify reported contribution percentages
2. **Language balance testing**: Evaluate system performance on separately constructed Chinese and English test sets to assess true bilingual capabilities
3. **Generalization assessment**: Test on mathematical problems requiring specialized domain knowledge (calculus, linear algebra) beyond standard test sets

## Open Questions the Paper Calls Out
None

## Limitations
- Test set composition lacks transparency regarding Chinese-English problem balance
- Computational efficiency claims lack clarity on measurement conditions and hardware specifications
- Reliance on proprietary models (GPT-4o) raises reproducibility and deployment cost concerns
- Potential interaction effects between components not fully explored in ablation studies
- Catastrophic forgetting during progressive curriculum fine-tuning not explicitly addressed

## Confidence
- **High Confidence**: Ensemble reasoning with confidence-calibrated voting methodology is well-established and the general approach is sound
- **Medium Confidence**: Specific accuracy numbers and ablation results are reasonable but require independent verification
- **Low Confidence**: Multilingual capability claims lack sufficient evidence about test set construction and evaluation methodology

## Next Checks
1. Replicate performance on independently constructed bilingual mathematical problem set with balanced language distribution to verify 83.7% accuracy claim
2. Conduct factorial experiments to determine whether reported individual contributions (11.2%, 7.3%, 6.6%) hold when components are combined
3. Evaluate HERALD on mathematical problems requiring specialized domain knowledge to assess whether tool augmentation provides genuine reasoning enhancement