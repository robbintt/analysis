---
ver: rpa2
title: Is Depth All You Need? An Exploration of Iterative Reasoning in LLMs
arxiv_id: '2502.10858'
source_url: https://arxiv.org/abs/2502.10858
tags:
- reasoning
- number
- step
- answer
- paths
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether deep iterative reasoning in large language
  models can be replaced by breadth reasoning, which generates diverse initial reasoning
  paths to activate relevant knowledge. The authors propose a method that enhances
  reasoning diversity by modifying input expressions and integrating self-consistency
  sampling.
---

# Is Depth All You Need? An Exploration of Iterative Reasoning in LLMs

## Quick Facts
- arXiv ID: 2502.10858
- Source URL: https://arxiv.org/abs/2502.10858
- Reference count: 40
- Primary result: Breadth reasoning achieves 85.1% accuracy vs 83.3% for deep iterative reasoning

## Executive Summary
This paper challenges the conventional wisdom that deeper iterative reasoning is necessary for complex problem-solving in large language models. Instead, the authors propose that generating diverse initial reasoning paths can be more effective than multiple rounds of refinement. By modifying input expressions and using self-consistency sampling, their breadth reasoning approach activates relevant knowledge through diversity rather than depth. The method was tested across ten reasoning datasets, demonstrating consistent improvements over traditional iterative approaches.

## Method Summary
The authors propose a breadth reasoning method that enhances reasoning diversity by generating multiple varied initial reasoning paths through modified input expressions. This approach contrasts with traditional deep iterative reasoning that refines a single path through multiple steps. The method integrates self-consistency sampling to select the most reliable reasoning paths from the diverse initial attempts. By activating relevant knowledge through breadth rather than depth, the approach aims to capture multiple perspectives on complex problems in the first pass, potentially avoiding compounding errors that can occur in iterative refinement.

## Key Results
- Breadth reasoning achieves 85.1% average accuracy across ten reasoning datasets
- Outperforms deep iterative reasoning baseline (83.3% accuracy)
- Demonstrates consistent improvements across diverse reasoning tasks
- Shows effectiveness of diverse initial reasoning paths for complex problem-solving

## Why This Works (Mechanism)
The approach leverages diversity in initial reasoning paths to activate multiple knowledge pathways simultaneously. By generating varied input expressions, the model can explore different conceptual angles of a problem from the start, rather than iteratively refining a potentially suboptimal initial approach. Self-consistency sampling then filters these diverse paths to identify the most reliable reasoning chains, effectively combining breadth exploration with quality selection.

## Foundational Learning

**Self-consistency sampling** - A technique for selecting the most reliable outputs from multiple model generations by identifying consistent reasoning patterns. Needed to filter quality reasoning paths from diverse initial attempts. Quick check: Verify the sampling mechanism correctly identifies coherent reasoning chains across different input variations.

**Iterative refinement** - The traditional approach of improving reasoning through multiple passes. Understanding this baseline is crucial for evaluating whether breadth can truly replace depth. Quick check: Compare error propagation patterns between iterative and breadth approaches.

**Input expression modification** - Techniques for rephrasing or restructuring prompts to generate diverse reasoning paths. Essential for activating different knowledge representations. Quick check: Test whether modifications consistently produce meaningfully different reasoning approaches.

## Architecture Onboarding

**Component map**: Input prompt -> Expression modifiers -> Multiple reasoning generations -> Self-consistency sampler -> Output selection

**Critical path**: The most important sequence is generating diverse initial reasoning paths quickly, as this breadth is the core innovation. Self-consistency sampling must be fast enough to not negate the efficiency gains from avoiding deep iteration.

**Design tradeoffs**: The method trades computational depth for breadth, generating more initial paths but potentially requiring more parallel processing. This favors scenarios where parallel computation is available but iterative refinement is costly.

**Failure signatures**: The approach may fail when task requires deep logical chaining that cannot be captured in initial diverse attempts, or when input modifications don't produce meaningfully different reasoning approaches. Poor self-consistency sampling can also undermine the method by selecting suboptimal paths.

**First experiments**: 1) Compare single-path baseline against breadth approach with varying numbers of initial paths. 2) Test sensitivity to different types of input modifications. 3) Evaluate performance on tasks requiring deep logical dependencies versus those benefiting from multiple perspectives.

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow comparison scope lacking established baselines like chain-of-thought and ensemble methods
- Modest 1.8% improvement may not justify replacing well-established iterative approaches
- Unclear mechanism for ensuring diversity targets task-relevant knowledge rather than random variations

## Confidence

**High**: The technical description of the breadth reasoning method is clear and reproducible
**Medium**: The experimental results showing improvement over deep iterative reasoning are likely valid within the tested datasets
**Low**: The generalizability of breadth reasoning as a replacement for iterative reasoning across different domains and model architectures

## Next Checks
1. Conduct ablation studies to isolate the contribution of breadth reasoning from self-consistency sampling and other confounding factors
2. Test the approach on out-of-distribution reasoning tasks not included in the original ten datasets
3. Compare against established baselines including chain-of-thought, self-consistency alone, and ensemble methods to establish relative effectiveness