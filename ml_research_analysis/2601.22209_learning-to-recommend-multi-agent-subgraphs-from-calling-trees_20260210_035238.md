---
ver: rpa2
title: Learning to Recommend Multi-Agent Subgraphs from Calling Trees
arxiv_id: '2601.22209'
source_url: https://arxiv.org/abs/2601.22209
tags:
- agent
- retrieval
- agents
- recommendation
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formulates agent recommendation in multi-agent systems
  as a constrained decision problem and introduces a two-stage framework that first
  retrieves a compact candidate set conditioned on the current subtask and context,
  then performs utility optimization within this feasible set using a learned scorer.
  Grounded in historical calling trees, the method supports both agent-level (selecting
  next agent) and system-level (selecting a small connected agent team) recommendation.
---

# Learning to Recommend Multi-Agent Subgraphs from Calling Trees

## Quick Facts
- arXiv ID: 2601.22209
- Source URL: https://arxiv.org/abs/2601.22209
- Reference count: 35
- This paper introduces a two-stage constrained framework that consistently recommends higher-quality agents and more coherent agent systems than strong baselines.

## Executive Summary
This paper formulates agent recommendation in multi-agent systems as a constrained decision problem and introduces a two-stage framework that first retrieves a compact candidate set conditioned on the current subtask and context, then performs utility optimization within this feasible set using a learned scorer. Grounded in historical calling trees, the method supports both agent-level (selecting next agent) and system-level (selecting a small connected agent team) recommendation. A unified calling-tree benchmark is constructed by normalizing logs from eight heterogeneous multi-agent corpora. Experiments show the approach consistently recommends higher-quality agents and more coherent agent systems than strong baselines, improving stability, coordination, and end-to-end execution quality. Code and datasets are publicly available.

## Method Summary
The approach uses a two-stage framework for agent recommendation in multi-agent systems. First, retrieval identifies a compact feasible candidate set conditioned on the current subtask and context. Second, a learned scorer performs utility optimization within this constrained set. The method operates at two levels: SARL (Single-Agent Recommendation Learning) for selecting individual agents at each decision node, and ASRL (Agent-System Recommendation Learning) for selecting connected agent teams/subgraphs. Features include relevance, historical reliability, cooperation compatibility, and structural graph properties. Training uses softmax cross-entropy loss with L2 regularization. The method is evaluated on a unified benchmark constructed from eight heterogeneous multi-agent corpora.

## Key Results
- Consistently outperforms strong baselines across eight heterogeneous multi-agent corpora
- Improves stability, coordination, and end-to-end execution quality
- Supports both agent-level and system-level recommendation through a unified framework

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Constrained Recommendation
Separating feasibility construction from utility optimization improves selection quality by first pruning to semantically relevant candidates, then ranking within a tractable constrained set. Stage 1 retrieval uses embedding similarity or LLM-based tool-calling to build feasible sets, while Stage 2 applies a learned scorer incorporating relevance, reliability, cooperation, and structural features only within this constrained set. Core assumption: good candidates are retrievable via semantic similarity; utility differences among similar candidates are learnable from historical outcomes. Evidence shows degradation when retrieval success rate is low (e.g., 40% SR on agent-data protocol limits final performance).

### Mechanism 2: Calling-Tree Structured Supervision
Historical calling trees capture execution dependencies and cooperation patterns that flat interaction logs miss, providing richer supervision for recommendation. Parent-child call relations, branching dependencies, and local cooperation patterns extracted from invocation logs yield structured traces used to train scoring models. Core assumption: past successful execution structures are predictive of future selection quality. Evidence shows structured supervision improves coordination and execution quality. Break condition: shallow or single-step calling trees provide limited structural signal.

### Mechanism 3: Feature Decomposition for Scoring
Decomposing the scoring function into relevance, history, cooperation, and structure components enables interpretable, sample-efficient learning. Linear scoring with explicit feature vector Φ = [ϕ_rel, ϕ_hist, ϕ_coop, ϕ_struct] captures distinct inductive biases. Core assumption: features are sufficiently informative; linear combination is expressive enough for the ranking task. Evidence shows unified linear framework across single-agent and multi-agent ranking. Break condition: correlated or insufficient features may cause underfitting.

## Foundational Learning

- **Learning to Rank (LTR)**: The core optimization uses ranking losses over candidate sets. Understanding pairwise/listwise losses, softmax cross-entropy, and regularization is essential. Quick check: Can you explain why softmax cross-entropy over scores is appropriate for top-K selection?

- **Graph Representation and Subgraph Extraction**: Calling trees are structured graphs; ASRL operates on subgraph candidates. Familiarity with node/edge encoding and traversal is required. Quick check: How would you serialize a calling tree into a text representation for embedding?

- **Dense Retrieval and Embedding Similarity**: Stage 1 retrieval uses shared embedding space for queries and candidates. Cosine similarity is the default similarity function. Quick check: Given a query embedding e_q and candidate embeddings E, how do you compute top-K candidates efficiently?

## Architecture Onboarding

- **Component map:** User Query q → Orchestrator (LangGraph) → Task Decomposition → Calling Graph T (nodes t with subtask qt) → [Stage 1: Retrieval] → Feasible Set A_feasible(t) or G_feasible(t) → [Stage 2: Rerank/Recommendation] → Selected Agent a or Subgraph g → Execution → Outcome y_{t,a} or y_{t,g} → Logged to Calling Tree Dataset

- **Critical path:** Data preparation (normalize 8 corpora into unified calling-tree format), Retrieval module (implement embedding-based and/or LLM-based retrieval), Feature computation (implement four feature functions), Scorer training (train linear model with softmax cross-entropy loss), Inference pipeline (integrate retrieval + reranking under token budget constraints).

- **Design tradeoffs:** Retrieval method (embedding-based is token-efficient but lower recall; LLM-based is higher quality but costly), SARL vs. ASRL (agent-level is simpler; subgraph-level is richer but more complex), Linear vs. neural scorer (linear is interpretable; neural is more expressive), Candidate set size K (larger improves recall but increases token cost).

- **Failure signatures:** Low retrieval SR (final Top-1 limited regardless of reranker quality), Negative dataset mishandling (inverted metric direction), Token budget violation (must use embedding-based retrieval on large pools), Shallow calling trees (structural features provide limited signal).

- **First 3 experiments:** 1) Reproduce SARL baseline on a moderate dataset (e.g., GTA or MedAgentBenchBench); 2) Ablate feature components (train separate models with different feature subsets); 3) Compare SARL vs. ASRL on the same dataset (convert agent-level traces to subgraph-level candidates).

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability bottleneck: Retrieval success rate drops sharply on large agent pools (e.g., Seal-Tools at 40% SR), limiting final performance.
- Dataset heterogeneity: Eight corpora have vastly different scales and task domains, potentially introducing domain shift.
- Shallow tree impact: Datasets like MCPToolBench++ have minimal graph structure (1.00 node/graph), reducing utility of cooperation and structural features.

## Confidence

- **High confidence:** Two-stage constrained recommendation framework is sound; empirical gains over baselines are consistent across multiple datasets and evaluation metrics.
- **Medium confidence:** Feature decomposition improves interpretability and sample efficiency, but linear models may underfit complex interactions; neural scorer extension provides capacity but at higher data cost.
- **Medium confidence:** Calling-tree structured supervision captures execution dependencies better than flat logs, though impact varies with tree depth and complexity.

## Next Checks

1. **Ablate Stage 1 retrieval:** Run SARL/ASRL with random K candidates instead of learned retrieval to quantify how much final performance depends on retrieval quality versus reranker.
2. **Test cross-domain transfer:** Train on one corpus (e.g., GUI-360°) and evaluate on another (e.g., MedAgentBench) to measure robustness to domain shift.
3. **Vary K and feature subsets:** Systematically vary candidate set size K and feature combinations to identify optimal tradeoff between recall, precision, and token cost.