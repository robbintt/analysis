---
ver: rpa2
title: 'MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical
  Reasoning'
arxiv_id: '2505.10557'
source_url: https://arxiv.org/abs/2505.10557
tags:
- images
- code
- math
- arxiv
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MathCoder-VL addresses the challenge of enhancing multimodal mathematical
  reasoning in Large Multimodal Models (LMMs) by leveraging code as supervision for
  cross-modal alignment. Traditional image-caption datasets fail to capture the intricate
  details of mathematical figures critical for problem-solving, leading to poor performance
  in LMMs.
---

# MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning

## Quick Facts
- arXiv ID: 2505.10557
- Source URL: https://arxiv.org/abs/2505.10557
- Authors: Ke Wang, Junting Pan, Linda Wei, Aojun Zhou, Weikang Shi, Zimu Lu, Han Xiao, Yunqiao Yang, Houxing Ren, Mingjie Zhan, Hongsheng Li
- Reference count: 18
- Primary result: Surpasses GPT-4o and Claude 3.5 Sonnet on MathVista geometry subset by 8.9% and 9.2% respectively

## Executive Summary
MathCoder-VL addresses the challenge of multimodal mathematical reasoning in Large Multimodal Models (LMMs) by leveraging code as supervision for cross-modal alignment. Traditional image-caption datasets fail to capture the intricate details of mathematical figures critical for problem-solving. The authors co-develop FigCodifier, an image-to-code model, and construct the largest image-code dataset to date (ImgCode-8.6M) using a model-in-the-loop approach. MathCoder-VL achieves state-of-the-art performance among open-source LMMs on mathematical reasoning benchmarks, particularly excelling in geometry problem-solving.

## Method Summary
MathCoder-VL employs a two-stage training approach. First, it trains FigCodifier on an iteratively constructed image-code dataset (ImgCode-8.6M) to establish cross-modal alignment between visual figures and their code representations. The model-in-the-loop approach scales from 119K seed pairs to 8.6M pairs through iterative training and validation. Second, the model is fine-tuned on MM-MathInstruct-3M, a high-quality multimodal math instruction dataset containing 2M problems from K12-2M plus 1M newly synthesized problems. The two-stage approach preserves language capabilities while enhancing visual perception for mathematical reasoning.

## Key Results
- Achieves 73.6% accuracy on MathVista GPS, surpassing GPT-4o by 8.9% and Claude 3.5 Sonnet by 9.2%
- Demonstrates outstanding geometry capabilities: 48.6% on angle subset, 32.2% on area subset, and 32.1% on length subset of MATH-Vision
- Excels in multi-step problem-solving with +15.0% improvement on 2-step problems compared to base model
- Maintains state-of-the-art performance among open-source LMMs while surpassing proprietary models on geometry tasks

## Why This Works (Mechanism)

### Mechanism 1
Code provides lossless, verifiable supervision for vision-language alignment in mathematical domains. Code (TikZ/Python) deterministically renders to images, allowing the model to learn to extract all geometric details (coordinates, angles, relationships) that captions omit. The rendered image from predicted code must match the input, creating a precise, automatically verifiable cross-modal alignment signal.

### Mechanism 2
Model-in-the-loop data synthesis creates a scalable, self-improving training data engine. The pipeline iteratively trains on seed data, uses the model to translate collected images to code, validates rendered outputs, adds successful pairs to training set, and retrains. This bootstraps from 119K to 8.6M pairs with quality compounding at each iteration.

### Mechanism 3
Two-stage training (cross-modal alignment → reasoning fine-tuning) with frozen-then-unfrozen LLM preserves language capabilities while enhancing visual perception. Stage 1 freezes the LLM backbone to force visual features to align with code token space without corrupting reasoning ability. Stage 2 fine-tunes on math instruction data, grounding the LLM's reasoning in accurate geometric understanding.

## Foundational Learning

- **Cross-modal alignment in Vision-Language Models**: Understanding that VLMs need a shared representation space between visual and textual modalities is prerequisite to grasping why code supervision matters. Quick check: "Why can't a VLM trained on natural image captions accurately describe the angle between two lines in a geometric diagram?"

- **Code as executable specification**: The paper exploits that TikZ/Python code is both human-readable text and machine-executable. This dual nature enables automatic validation (render → compare) that natural language captions cannot provide. Quick check: "Given a caption 'a circle inside a square,' list three ambiguities. How would code resolve them?"

- **Chain-of-Thought (CoT) reasoning for mathematics**: The MM-MathInstruct-3M dataset provides step-by-step solutions, and results show MathCoder-VL excels at multi-step problems. CoT understanding explains why instruction fine-tuning on detailed solutions matters. Quick check: "Why might a model that predicts only final answers underperform on multi-step geometry problems compared to one trained on step-by-step solutions?"

## Architecture Onboarding

- **Component map**: Raw Images → FigCodifier (InternVL2-8B) → Code (TikZ/Python) → ImgCode-8.6M ← Validation ← Render Engine → MM-MathInstruct-3M → MathCoder-VL

- **Critical path**: The data engine (FigCodifier → ImgCode-8.6M) is the bottleneck. If image-to-code quality is poor, rendered images don't match inputs, validation rejects pairs, dataset doesn't scale, and Stage 1 alignment fails. Monitor: code validation success rate, visual similarity between input and rendered images.

- **Design tradeoffs**:
  1. InternVL2-8B vs larger models: 8B chosen for cost-effectiveness at scale; larger models may improve accuracy but increase training costs 3-10x
  2. TikZ + Python vs single format: Dual formats double dataset size but add translation complexity; trade-off is diversity vs. pipeline simplicity
  3. Temperature 0.7 for synthesis: Higher temperature creates diverse images but risks unrealistic figures; no ablation on temperature sensitivity provided

- **Failure signatures**:
  1. Low code validation rate (<50%): Indicates FigCodifier undertrained or input images too diverse
  2. Stage 1 improves alignment but Stage 2 regresses: LLM catastrophic forgetting
  3. Synthesized images visually inconsistent: Temperature too high or code validation too permissive

- **First 3 experiments**:
  1. Reproduce FigCodifier training loop: Start with DaTikZ-119K seed, train InternVL2-8B on image→TikZ task, measure validation success rate. Target: >70% before scaling to full dataset
  2. Ablate Stage 1 vs Stage 2: Train (a) Stage 1 only, (b) Stage 2 only, (c) both. Compare on MathVista GPS subset. Expect: (c) > (a) > (b), with (a) showing alignment gains but poor reasoning
  3. Inspect synthesized problem quality: Sample 100 examples from New-1M, manually verify (a) image renders correctly, (b) question is answerable from image alone, (c) solution is correct. Target: >90% pass rate on all three checks

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset construction reliability: While the model-in-the-loop approach scales effectively, the paper lacks analysis of error accumulation or plateau points in iterative data synthesis
- Generalization beyond mathematics: The code-based approach may not transfer to domains requiring perception of non-codeable visual elements like textures or handwritten content
- Knowledge distillation transparency: Claims of superiority through knowledge distillation from InternVL-2302B lack ablations showing distillation's specific contribution

## Confidence
- **High Confidence**: Claims about code providing lossless supervision and two-stage training preserving language while enhancing vision
- **Medium Confidence**: Claims about model-in-the-loop data synthesis efficiency and quality
- **Low Confidence**: Claims about knowledge distillation superiority and the assertion that this represents the "largest" image-code dataset

## Next Checks
1. **Error Accumulation Analysis**: Track FigCodifier's performance degradation over iterative data synthesis rounds and verify quality of synthesized pairs from later iterations
2. **Cross-Domain Transfer Test**: Evaluate MathCoder-VL on non-mathematical visual reasoning tasks to determine if code-based alignment transfers beyond geometric domains
3. **Human Evaluation of Synthesized Data**: Conduct blind human assessment of 500 randomly sampled problems from MM-MathInstruct-3M to verify visual quality, question validity, and solution correctness