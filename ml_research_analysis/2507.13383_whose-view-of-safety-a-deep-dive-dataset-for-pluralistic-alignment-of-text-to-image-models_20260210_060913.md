---
ver: rpa2
title: Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image
  Models
arxiv_id: '2507.13383'
source_url: https://arxiv.org/abs/2507.13383
tags:
- raters
- safety
- demographic
- diverse
- rater
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the need for pluralistic alignment in text-to-image
  (T2I) models, recognizing that safety perceptions vary across demographic groups
  due to different lived experiences. The authors introduce DIVE, the first multimodal
  dataset specifically designed for pluralistic alignment, featuring 1000 adversarial
  prompt-image pairs evaluated by 637 demographically diverse raters across 30 intersectional
  groups (gender, age, ethnicity).
---

# Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image Models

## Quick Facts
- arXiv ID: 2507.13383
- Source URL: https://arxiv.org/abs/2507.13383
- Reference count: 40
- Primary result: DIVE is the first multimodal dataset for pluralistic alignment of T2I models, showing demographic attributes significantly influence harm perception.

## Executive Summary
This paper addresses the need for pluralistic alignment in text-to-image models, recognizing that safety perceptions vary across demographic groups due to different lived experiences. The authors introduce DIVE, the first multimodal dataset specifically designed for pluralistic alignment, featuring 1000 adversarial prompt-image pairs evaluated by 637 demographically diverse raters across 30 intersectional groups. Empirical analysis confirms that demographic attributes significantly influence harm perception, with intersectional groups showing higher internal agreement than broader demographic categories. The study reveals that diverse raters identify different safety issues compared to conventional policy-based evaluations, particularly for bias-related content.

## Method Summary
The DIVE dataset was constructed by sampling 1000 adversarial prompt-image pairs from the Adversarial Nibbler dataset, stratified by violation type (Bias/Explicit/Violent) and topic. 637 raters across 30 demographic trisections (2 genders × 3 age groups × 5 ethnicities) evaluated each pair using 5-point Likert scales for harm-to-self, harm-to-others, and violation type selection. Each rater evaluated ~50 pairs, with 20-30 raters per prompt-image pair. The dataset captures nuanced safety perceptions through graded harm scales and personal vs. group harm assessments. For LLM steering experiments, a 4B Gemma multimodal model was prompted with demographic information to simulate diverse perspectives.

## Key Results
- Demographic attributes significantly influence harm perception (p < 0.05 across multiple tests)
- Intersectional groups show higher internal agreement (GAI > 1.0) than broader demographic categories
- Diverse raters identify different safety issues than conventional policy-based evaluations, particularly for bias violations
- In-context demographic steering improves LLM-rater correlation from 0.024 to 0.23

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intersectional demographic groupings capture more cohesive harm perspectives than single demographic categories.
- Mechanism: Raters who share multiple identity axes exhibit higher within-group agreement relative to between-group agreement, yielding higher Group Association Index (GAI) scores.
- Core assumption: Demographic attributes serve as reliable proxies for lived experiences that shape harm perception.
- Evidence anchors: "GenZ-Black and Millennial-Black raters report the two highest and statistically significant GAIs: 1.38 and 1.29" (section 4.2).

### Mechanism 2
- Claim: High-replication annotation surfaces legitimate disagreement patterns that small-pool approaches obscure.
- Mechanism: With 31,980 responses across 1000 pairs, the dataset achieves sufficient statistical power to distinguish systematic group-level differences from random variation.
- Core assumption: Disagreement reflects legitimate perspective diversity rather than annotation error.
- Evidence anchors: "31,980 responses" with "high replication" (abstract).

### Mechanism 3
- Claim: In-context demographic steering improves LLM-rater alignment with diverse perspectives, but gains remain modest.
- Mechanism: Providing demographic information in prompts gives the model contextual cues to adjust its safety judgment.
- Core assumption: LLMs can approximate demographic-specific harm perceptions from prompt instructions alone.
- Evidence anchors: "correlation increasing from 0.024 to 0.23" (abstract).

## Foundational Learning

- Concept: **Group Association Index (GAI)**
  - Why needed here: Quantifies whether a demographic group has internally coherent judgments distinct from other groups.
  - Quick check question: If GAI for Black-Woman raters is 1.12, what does this mean about their within-group vs. cross-group agreement?

- Concept: **Pluralistic Alignment**
  - Why needed here: Distinguishes the paper's goal (models steerable toward diverse, conflicting values) from monolithic alignment (single safety standard).
  - Quick check question: Why might a safety classifier that flags content uniformly for all users fail pluralistic alignment criteria?

- Concept: **Inter-rater Reliability (IRR) vs. Cross-replication Reliability (XRR)**
  - Why needed here: Understanding these components explains how GAI is computed and why intersectional groups show higher cohesion.
  - Quick check question: If IRR increases but XRR increases proportionally, will GAI change?

## Architecture Onboarding

- Component map: Prompt-Image Set (1000 pairs) -> Rater Pool (637 across 30 trisections) -> Annotation Schema (5-point Likert + violation type) -> Evaluation Pipeline (GAI computation + classifier comparison) -> Steering Module (4B Gemma with demographic prompts)

- Critical path: Rater recruitment (trisections) → PI pair sampling (disagreement-prioritized) → annotation collection (50 pairs/rater) → GAI computation → classifier comparison → LLM steering experiments

- Design tradeoffs:
  - **Trisections vs. higher-order intersections**: 30 groups balance statistical power with recruitment feasibility
  - **Adversarial prompts only**: Limits generalizability but focuses on safety-critical cases
  - **Binary gender categories**: Pragmatic recruitment decision; excludes non-binary perspectives
  - **5-point Likert vs. binary**: Captures nuance but complicates aggregation

- Failure signatures:
  - Low GAI (<1.0) for a group suggests demographic proxy is weak or sample size insufficient
  - High false negative rate for "Bias" violations indicates classifiers miss context-dependent harms
  - Correlation <0.1 between LLM-rater and human scores suggests steering failure

- First 3 experiments:
  1. **Baseline replication**: Compute GAI for each of the 30 trisections on a held-out 200-pair subset
  2. **Classifier gap analysis**: Stratify false negatives by topic × demographic group
  3. **Fine-tuning pilot**: Fine-tune a small multimodal model on DIVE ratings vs. in-context steering

## Open Questions the Paper Calls Out

- How do prompt safety and image safety independently and interactively influence perceived harm? The authors state they "cannot disentangle effects of prompt safety and image safety (or their interaction) within this study."

- Can individual value profiles predict harm perception more effectively than demographic groupings? The paper suggests "future work could investigate whether individual value profiles hold predictive power in our domain."

- Can fine-tuning methodologies significantly improve model steerability towards diverse perspectives compared to in-context learning? The authors note that "future work should look into methodologies that train models with this data."

## Limitations

- Demographic attributes may not fully capture the complexity of individual experiences or cultural contexts that shape safety judgments
- The dataset comprises adversarial prompts only, limiting generalizability to benign or ambiguous content
- Moderate model correlation improvements (0.024 → 0.23) suggest in-context steering requires further refinement

## Confidence

- **High Confidence**: The empirical finding that demographic attributes significantly influence harm perception (p < 0.05 across multiple tests), and that intersectional groups show higher internal agreement than broader categories.

- **Medium Confidence**: The claim that LLMs can approximate demographic-specific harm perceptions through in-context learning, given the statistically significant but modest correlation improvement.

- **Low Confidence**: Generalizability to non-adversarial content and real-world deployment scenarios, as controlled experimental conditions may not capture production complexity.

## Next Checks

1. **Cross-dataset validation**: Apply the GAI methodology to non-adversarial T2I datasets to assess whether demographic patterns persist in benign content contexts.

2. **Fine-tuning efficacy study**: Compare the correlation improvements from in-context demographic steering (0.23) against fine-tuned models trained on DIVE ratings, controlling for model size and training duration.

3. **Deployment simulation**: Conduct a user study where demographically-aligned T2I models (steered via DIVE methodology) are evaluated by target demographic groups on real-world safety concerns, measuring both harm perception accuracy and user trust metrics compared to standard safety filters.