---
ver: rpa2
title: 'MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems'
arxiv_id: '2509.24323'
source_url: https://arxiv.org/abs/2509.24323
tags:
- self
- problem
- operator
- arxiv
- workflow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MAS2, a self-adaptive multi-agent system that
  recursively generates and refines bespoke MAS configurations for specific tasks.
  Unlike prior approaches, MAS2 uses a "generator-implementer-rectifier" tri-agent
  team to dynamically design, instantiate, and adapt agent systems in real time, improving
  robustness and flexibility.
---

# MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems

## Quick Facts
- arXiv ID: 2509.24323
- Source URL: https://arxiv.org/abs/2509.24323
- Authors: Kun Wang; Guibin Zhang; ManKit Ye; Xinyu Deng; Dongxia Wang; Xiaobin Hu; Jinyang Guo; Yang Liu; Yufei Guo
- Reference count: 40
- One-line primary result: MAS² achieves up to 19.6% performance gains over state-of-the-art MAS on benchmarks like deep research and code generation.

## Executive Summary
MAS² introduces a recursive self-generative multi-agent system that dynamically designs, instantiates, and adapts agent systems for specific tasks. Unlike prior approaches that generate MAS once, MAS² employs a "generator-implementer-rectifier" tri-agent team to continuously refine configurations in response to real-time demands. Trained via collaborative tree optimization with value-guided preference alignment, MAS² demonstrates superior robustness and flexibility, achieving significant improvements over existing MAS frameworks.

## Method Summary
MAS² is a meta-MAS system comprising three specialized agents: a generator that creates task-specific workflow templates, an implementer that assigns LLM backbones to roles, and a rectifier that monitors execution and adapts configurations when failures or budget overruns occur. The system is trained offline using Collaborative Tree Optimization (CTO), which builds decision trees for each query, propagates cost-normalized rewards via Monte Carlo value estimation, and optimizes agents using value-scaled preference alignment loss. During inference, the tri-agent team recursively generates and refines MAS configurations for each task, enabling dynamic adaptation to complex, changing environments.

## Key Results
- Achieves up to 19.6% performance gains over state-of-the-art MAS on benchmarks including deep research and code generation
- Demonstrates effective generalization to unseen LLM backbones, with up to 15.1% improvements
- Maintains cost-effective performance across domains, optimizing the Pareto frontier between accuracy and resource consumption

## Why This Works (Mechanism)

### Mechanism 1: Recursive Self-Generation via Tri-Agent Meta-Architecture
MAS² employs a "generator-implementer-rectifier" tri-agent team that separates architectural design, resource instantiation, and runtime adaptation into specialized roles. The generator creates task-specific workflow templates; the implementer assigns concrete LLM backbones; the rectifier monitors execution and modifies configurations in real-time when failures or budget overruns occur. This specialization enables better adaptation than monolithic generation approaches.

### Mechanism 2: Collaborative Tree Optimization with Value-Guided Preference Alignment
MAS² uses offline reinforcement learning via collaborative decision trees with value-scaled loss to specialize meta-agents. For each query, the system builds a decision tree where generator, implementer, and rectifier decisions branch into trajectories. Terminal rewards are backpropagated via Monte Carlo value estimation, and preference pairs are weighted by value margin, amplifying learning from high-confidence decisions through a modified DPO-style loss.

### Mechanism 3: Real-Time Rectification with Trigger-Based Intervention
The rectifier agent monitors execution and triggers intervention when cumulative resource consumption exceeds budget or when operational outcome indicates failure. Upon activation, it modifies the current MAS configuration using policy π_rec, ranging from tool reassignment to workflow restructuring. The system resumes execution from the current state with the updated configuration.

## Foundational Learning

- **Concept: Multi-Agent System (MAS) Architecture**
  - Why needed here: Understanding agent roles, communication protocols, and tool integration is essential to grasp what MAS² generates and how the meta-agents configure target systems.
  - Quick check question: Can you explain the difference between manually configured MAS (e.g., AutoGen) and automated MAS generation approaches?

- **Concept: Preference Alignment (DPO-style)**
  - Why needed here: The CTO training framework extends preference optimization with value-scaled loss; familiarity with DPO helps understand how meta-agents are specialized.
  - Quick check question: How does a preference pair (a_win, a_lose) differ from a value-weighted preference tuple (c, a_win, a_lose, ΔV)?

- **Concept: Cost-Sensitive Reinforcement Learning**
  - Why needed here: The reward function normalizes resource consumption; understanding cost-aware RL clarifies why MAS² achieves Pareto-optimal cost-performance.
  - Quick check question: Why would a trajectory with higher accuracy but excessive token cost receive lower reward than a slightly less accurate but cost-efficient trajectory?

## Architecture Onboarding

- **Component map:** Meta-MAS (Generator, Implementer, Rectifier) -> Target MAS -> LLM Pool
- **Critical path:** 1) Generator receives query Q → produces M_temp 2) Implementer assigns backbones ϕ → produces executable M 3) M executes; Rectifier monitors state s_t → triggers if C(s_t) > θ_C or O(s_t) = Failure → generates M_{t+1}
- **Design tradeoffs:** Tri-agent vs. single meta-agent: separation enables specialization but increases coordination complexity; Rectifier trigger thresholds: lower θ_C increases intervention frequency but higher overhead; Value margin weighting: amplifies learning from clear preferences but may discard useful borderline examples
- **Failure signatures:** KeyError in ensemble selection when sc_ensemble returns empty string; Pydantic validation error when operator returns malformed structured output; Budget overrun without rectification if trigger condition is misconfigured
- **First 3 experiments:** 1) Ablation study: Run MAS² with untrained generator/implementer and without rectifier on MBPP/HotpotQA/MATH to quantify component contributions 2) Cross-backbone generalization: Add an unseen LLM to the inference pool and measure performance on MATH/Bamboogle 3) Rectifier trigger sensitivity: Vary θ_C on BrowseComp+ tasks with simulated network failures

## Open Questions the Paper Calls Out

### Open Question 1
Can MAS² support multi-level recursive generation where a generated target agent acts as a meta-agent for a sub-task? The paper defines the paradigm as "recursive self-generation" but only demonstrates single-level abstraction (Meta-MAS → Target-MAS).

### Open Question 2
To what extent does reliance on a fixed set of predefined operators limit the Generator's ability to discover novel architectural primitives? The paper claims "architectural innovation" but the search space is constrained to composition rather than invention of new operators.

### Open Question 3
Can successful interventions by the Rectifier be immediately distilled into the Generator and Implementer to prevent repeat errors in subsequent tasks? The system currently corrects errors per instance, but it's unclear if meta-agents acquire "lifelong" immunity during inference.

## Limitations
- CTO training framework is novel and lacks direct corpus validation; effectiveness depends on accurate value propagation in sparse trajectory spaces
- Real-time rectifier trigger detection and latency are not fully specified; effectiveness depends on whether intervention occurs before task collapse
- Operator internals and robustness mechanisms are underspecified, making exact replication challenging

## Confidence

- **High**: Recursive tri-agent meta-architecture is well-specified and novel; claims of up to 19.6% performance gains are supported by experimental results
- **Medium**: Collaborative Tree Optimization with value-guided preference alignment is theoretically sound but lacks direct empirical validation against standard preference alignment
- **Low**: Rectifier mechanism's practical robustness depends on trigger sensitivity and execution speed, which are not fully quantified in the paper

## Next Checks

1. **CTO ablation study**: Compare MAS² with and without value-weighted loss on a held-out task suite to isolate the contribution of value-guided preference alignment
2. **Cross-domain generalization test**: Deploy MAS² on a novel domain (e.g., medical QA) using unseen LLM backbones to verify generalization beyond reported benchmarks
3. **Rectifier sensitivity analysis**: Systematically vary θ_C across tasks with injected failures to map the relationship between intervention frequency and task success rate