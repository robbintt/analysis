---
ver: rpa2
title: 'Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning
  Limits'
arxiv_id: '2505.14178'
source_url: https://arxiv.org/abs/2505.14178
tags:
- counter
- step
- string
- found
- counting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work shows that tokenization granularity fundamentally limits\
  \ transformer models\u2019 symbolic and arithmetic reasoning ability, even when\
  \ using Chain-of-Thought prompting. By systematically evaluating arithmetic counting\
  \ and symbolic tasks, the authors demonstrate that merged tokens in BPE obscure\
  \ atomic reasoning units, causing performance degradation up to 80%."
---

# Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits

## Quick Facts
- arXiv ID: 2505.14178
- Source URL: https://arxiv.org/abs/2505.14178
- Authors: Xiang Zhang; Juntai Cao; Jiaqi Wei; Yiwei Xu; Chenyu You
- Reference count: 40
- Key outcome: Tokenization granularity fundamentally limits transformer models' symbolic and arithmetic reasoning ability, even with Chain-of-Thought prompting

## Executive Summary
This work demonstrates that tokenization granularity fundamentally constrains transformer models' ability to perform symbolic and arithmetic reasoning, even when using Chain-of-Thought prompting. By systematically evaluating arithmetic counting and symbolic tasks, the authors show that merged tokens in BPE obscure atomic reasoning units, causing performance degradation up to 80%. Atomic-aligned tokenization dramatically improves results, enabling small models to outperform larger ones in structured reasoning. The proposed Token Awareness metric formalizes how poor token granularity disrupts logical alignment and generalization.

## Method Summary
The study evaluates three symbolic tasks (counting letters, sorting characters, reversing strings) across four tokenization formats: pure BPE, space-delimited, comma-space delimited, and list format with quotes. Using synthetic data with controlled formats and lengths, the authors test GPT-4o-mini, Claude 3.5 Sonnet, Qwen Turbo, and OpenAI o1 with base, unsupervised CoT, and supervised CoT prompts. Accuracy is measured across formats to quantify the degradation caused by tokenization. The Token Awareness metric measures whether required properties are encoded in token embeddings.

## Key Results
- Switching from BPE to atomic-aligned tokenization improves counting accuracy by 54.1% for length 30-40 strings
- Small models with atomic-aligned tokenization outperform larger models with BPE tokenization on symbolic tasks
- Token Awareness metric correlates strongly with performance degradation, explaining why merged tokens cause reasoning failures

## Why This Works (Mechanism)

### Mechanism 1: Chain-of-Thought Externalizes Hidden State to Simulate Recurrence
- Claim: CoT enables fixed-depth Transformers to perform variable-depth computation by moving state through the output token stream rather than internal layers
- Mechanism: Instead of mapping x₁:ₙ → y directly, the model generates intermediate tokens o₁:ₖ that encode partial computations (counters, carries). These tokens are re-embedded as input for subsequent steps, approximating the recurrence hₜ = g(hₜ₋₁, xₜ) found in RNNs
- Core assumption: Intermediate tokens accurately encode and recover latent state information across steps
- Evidence anchors: [abstract] "Chain-of-Thought (CoT) prompting enables transformer models to approximate recurrent computation by externalizing intermediate steps"; [Section 4.2] "Each oₜ can encode intermediate computations... reinitializing the next latent state: hₜ₊₁ = fθ(Embed(oₜ), xₜ₊₁)"

### Mechanism 2: Token Granularity Determines Access to Atomic Reasoning Units
- Claim: BPE tokenizers merge characters into opaque tokens, hiding sub-token properties required for symbolic operations
- Mechanism: The paper defines TokenAware(tᵢ, prop) := I[prop ∈ Emb(tᵢ)]. When a token like "Straw" contains multiple characters, the embedding typically lacks explicit representation of character-level properties
- Core assumption: Token embeddings trained on natural language do not systematically encode fine-grained character-level statistics
- Evidence anchors: [abstract] "subword-based tokenizers like byte-pair encoding (BPE) merge atomic reasoning units, obscuring fine-grained operations necessary for tasks such as arithmetic and pattern matching"

### Mechanism 3: Input Formatting Controls Token Boundaries Without Changing Task Semantics
- Claim: Adding delimiters (spaces, commas, quotes) forces tokenizers to split merged tokens, improving symbolic reasoning accuracy
- Mechanism: BPE merges common substrings but typically treats delimiters as boundaries. By formatting "ababa" as ["a", "b", "a", "b", "a"], each character becomes its own token, restoring atomic alignment
- Core assumption: Delimiter-insertion strategies generalize across tokenizers without introducing unintended semantic shifts
- Evidence anchors: [Section 6.2, Table 1] Switching from type (a) pure BPE to type (d) atomic-aligned improves accuracy by Δtok = 54.1% for counting task

## Foundational Learning

- **Concept: Chomsky Hierarchy & Circuit Complexity**
  - Why needed here: The paper situates Transformers at TC⁰ (constant-depth threshold circuits), explaining why they cannot natively perform O(n)-depth operations like counting or carry propagation without external mechanism
  - Quick check question: Why can't a 96-layer Transformer count arbitrarily long sequences in a single forward pass?

- **Concept: Byte-Pair Encoding (BPE) Tokenization**
  - Why needed here: BPE iteratively merges frequent character pairs for compression. Understanding this explains why "Strawberry" → [Straw, berry] rather than character-level tokens
  - Quick check question: Given BPE vocabulary {"ab": 1, "c": 2, "abc": 3}, how would "abcabc" tokenize if "abc" has higher merge priority than "ab"?

- **Concept: Recurrent vs. Feed-forward Computation**
  - Why needed here: The paper's core thesis is that CoT approximates RNN-style state evolution. Understanding hₜ = f(hₜ₋₁, xₜ) vs. parallel Transformer computation clarifies the architectural motivation
  - Quick check question: In an RNN counting sequence, what must hₜ encode that a Transformer's per-token hidden state typically doesn't?

## Architecture Onboarding

- **Component map:** Tokenizer (T) -> Embedding Layer -> Transformer Stack (M) -> CoT Output Stream -> Task Function (f)
- **Critical path:** 1. Identify atomic units required by task (characters, digits) 2. Trace how tokenizer merges these units 3. Determine if TokenAware(tᵢ, required_prop) = 1 for all tokens 4. If not, apply delimiter-based formatting to force atomic alignment 5. Verify CoT steps explicitly output intermediate state (counter values, partial results)
- **Design tradeoffs:** Atomic alignment vs. sequence length (list-format increases token count 3-5× vs. "ab"); Supervised vs. unsupervised CoT (supervised improves accuracy ~10-40% but requires task-specific prompt engineering); Digit vs. letter vs. word tasks (digits generalize better)
- **Failure signatures:** Systematic under-counting (negative error shift) → likely BPE merging; Error shifts clustering at ±1, ±2 → arithmetic slip; large variance across equivalent inputs formatted differently → tokenization sensitivity; CoT produces plausible steps but wrong answer → state externalization failing at a specific step
- **First 3 experiments:** 1. Implement data generation for counting (random a/b strings at lengths 10–40). Generate four formats: (a) ababab..., (b) a b a b..., (c) a, b, a, b..., (d) ['a', 'b', 'a', 'b']. Sample 1,000 instances per length bucket. 2. Use supervised CoT prompt from Figure 12(c): instruct model to iterate, maintain a counter, output each step, and conclude with Result: <count>. Call GPT-4o-mini API for each input across all formats. Record accuracy and error shifts. 3. Compute Δtok as max accuracy difference between format (d) and format (a) per length bucket. Replicate for sorting/reversing using prompts from Figure 13–14.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited task diversity: The three tasks (counting, sorting, reversing) share similar structure and may not generalize to more complex symbolic reasoning
- Closed-source tokenizer opacity: For GPT-4o-mini and Claude 3.5 Sonnet, exact BPE merge rules and token embeddings are inaccessible
- Supervised CoT bias: Template-based prompt engineering may not reflect natural reasoning patterns

## Confidence
- Token granularity fundamentally limits symbolic reasoning: High confidence
- CoT simulates recurrence by externalizing state: Medium confidence
- Token Awareness metric captures reasoning failures: Medium confidence

## Next Checks
1. For open-source models (Qwen Turbo), extract the BPE merge table and token embeddings. Verify that atomic units (characters/digits) present in merged tokens are actually absent from embeddings
2. Apply the atomic alignment intervention to a non-string symbolic task (e.g., digit-by-digit arithmetic). Measure if the same 54%+ improvement pattern holds
3. For counting task failures, extract all CoT steps and analyze whether intermediate counter values are correctly maintained. Calculate the proportion of steps where state is lost or corrupted