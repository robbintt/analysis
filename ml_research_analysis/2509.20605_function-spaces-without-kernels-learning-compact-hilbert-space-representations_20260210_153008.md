---
ver: rpa2
title: 'Function Spaces Without Kernels: Learning Compact Hilbert Space Representations'
arxiv_id: '2509.20605'
source_url: https://arxiv.org/abs/2509.20605
tags:
- function
- basis
- functions
- kernel
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to learn compact function representations
  through neural basis functions, establishing a principled connection between function
  encoders and kernel methods. The approach defines a kernel through the inner product
  of learned feature maps, enabling kernel-style analysis of neural models while maintaining
  computational efficiency.
---

# Function Spaces Without Kernels: Learning Compact Hilbert Space Representations

## Quick Facts
- arXiv ID: 2509.20605
- Source URL: https://arxiv.org/abs/2509.20605
- Reference count: 40
- One-line primary result: Learn compact function representations through neural basis functions with kernel-style analysis while maintaining computational efficiency

## Executive Summary
This paper introduces a method to learn compact function representations using neural basis functions that define a valid reproducing kernel through inner products. The approach enables kernel-style analysis of neural models while inference cost depends only on the number of basis functions rather than dataset size. Two training algorithms are developed: progressive training that grows bases sequentially using PCA principles, and train-then-prune that starts overparameterized and removes redundant bases after training. Both methods identify the intrinsic dimension of the learned space through explained variance analysis. Theoretical contributions include Rademacher complexity and PAC-Bayes generalization bounds that scale with basis count, data size, and regularization strength. Experiments demonstrate effectiveness on polynomial regression, Van der Pol oscillators, and two-body orbital mechanics, achieving the same accuracy as overparameterized models with significantly fewer basis functions.

## Method Summary
The method learns neural basis functions through a multi-headed MLP architecture, where each output head represents one basis function. The learned basis functions define a reproducing kernel via inner products, enabling kernel-style analysis. Training occurs in two phases: an offline phase where basis functions are learned across multiple datasets, followed by an inference phase where new functions are represented as linear combinations of the learned bases. Dimension analysis via coefficient covariance PCA reveals the intrinsic dimensionality, enabling principled basis selection. Two algorithms implement this framework: progressive training grows bases sequentially while train-then-prune starts overparameterized and prunes redundant bases based on eigenvalue analysis.

## Key Results
- Progressive and train-then-prune algorithms recover known intrinsic dimensions (d+1 for degree-d polynomials)
- Van der Pol oscillators reduced from 100 to 2 basis functions while maintaining accuracy
- Two-body orbital mechanics solved with 5-6 bases instead of overparameterized models
- Generalization bounds scale with basis count n, data size m, and regularization λ as predicted by theory

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Function encoders construct valid reproducing kernels through inner products of learned neural basis functions.
- **Mechanism:** Neural basis functions $\{\psi_j\}_{j=1}^n$ define an explicit feature map $\phi(x) = [\psi_1(x), \ldots, \psi_n(x)]^\top$. The kernel $k(x, x') = \langle \phi(x), \phi(x') \rangle$ is automatically symmetric positive semi-definite, enabling kernel-style analysis while inference cost depends only on $n$, not dataset size $m$.
- **Core assumption:** Basis functions are bounded: $\sup_{x \in \mathcal{X}} |\psi_j(x)| \leq R$.
- **Evidence anchors:**
  - [abstract] "defining a kernel through an inner product of the learned feature map"
  - [Section 3.1] Proposition 1 proof shows symmetry and positive semi-definiteness follow directly from inner product properties
  - [corpus] Weak direct corpus support; related work on RKHS sampling (arXiv:2504.13623) addresses convergence in kernel spaces but not learned kernel construction

### Mechanism 2
- **Claim:** Eigenvalue spectrum of the coefficient covariance matrix reveals intrinsic dimensionality, enabling principled basis selection.
- **Mechanism:** After training, compute coefficients $c_j$ for each dataset via regularized least squares. The covariance $\Sigma_b$ of mean-centered coefficients has eigenvalues $\lambda_1 \geq \ldots \geq \lambda_b$. Cumulative explained variance ratio identifies when additional bases capture minimal variance, analogous to PCA elbow detection.
- **Core assumption:** Target functions lie on a low-dimensional manifold within the Hilbert space, causing rapid eigenvalue decay once basis spans the intrinsic dimension.
- **Evidence anchors:**
  - [abstract] "Both approaches use principles from PCA to reveal the intrinsic dimension of the learned space"
  - [Section 4.1] Algorithm 1 lines 8-9: compute $\Sigma_b$ and update CEV from eigenvalues
  - [Section 6.1] Figure 1 shows clear elbows at $d+1$ for degree-$d$ polynomials; Van der Pol reduced from 100 to 2 bases
  - [corpus] Indirect support from kernel mean embedding topology work (arXiv:2502.13486) on low-dimensional structure in Hilbert spaces

### Mechanism 3
- **Claim:** Rademacher complexity and PAC-Bayes bounds provide finite-sample generalization guarantees scaling with $n$, $m$, and $\lambda$.
- **Mechanism:** Once basis functions are fixed, function encoders reduce to ridge regression in $\mathbb{R}^n$. Rademacher complexity $\mathcal{R}_m(\mathcal{F}_{C_\lambda}) \in O(\sqrt{n / m\lambda})$ quantifies hypothesis class richness. PAC-Bayes analysis with truncated Gaussians handles unbounded least-squares loss via domain restriction.
- **Core assumption:** Fixed basis after training (no joint optimization during inference); outputs bounded $\|y\|_2 \leq Y$.
- **Evidence anchors:**
  - [abstract] "generalization bounds using Rademacher complexity and PAC-Bayes techniques, providing inference time guarantees"
  - [Section 5.1] Theorem 1: $L(\hat{f}_{c_\lambda}) \leq \hat{L}_m(\hat{f}_{c_\lambda}) + \tilde{O}(Y^2 R^2 n / \lambda\sqrt{m})$
  - [Section C.1] Proof uses boundedness to establish Lipschitz constant for loss

## Foundational Learning

- **Concept: Reproducing Kernel Hilbert Spaces (RKHS)**
  - Why needed here: Explains why $k(x, x') = \langle \phi(x), \phi(x') \rangle$ automatically satisfies kernel properties and enables kernel-style analysis of neural models.
  - Quick check question: Given feature map $\phi: \mathcal{X} \to \mathbb{R}^n$, verify that $k(x, x') = \phi(x)^\top \phi(x')$ is positive semi-definite for any set $\{x_1, \ldots, x_m\}$.

- **Concept: Regularized Least Squares (Ridge Regression)**
  - Why needed here: Coefficient computation (Eq. 2-3) is closed-form ridge regression; understanding the bias-variance tradeoff via $\lambda$ is critical for generalization.
  - Quick check question: Derive the solution $c^* = (\Phi^\top \Phi + \lambda I)^{-1} \Phi^\top y$ and explain why $\lambda > 0$ ensures invertibility.

- **Concept: Rademacher Complexity**
  - Why needed here: Interprets Theorem 1's bound—complexity grows with basis count $n$ (expressivity) and shrinks with data $m$ and regularization $\lambda$ (stability).
  - Quick check question: For function class $\mathcal{F} = \{f_c(x) = \langle c, \phi(x) \rangle : \|c\|_2 \leq B\}$, sketch why Rademacher complexity scales as $O(B \sqrt{n/m})$.

## Architecture Onboarding

- **Component map:**
  Training data: {D_1, ..., D_N} where D_j = {(x_i, f_j(x_i))}
       ↓
  [Offline Phase] Multi-headed MLP → {ψ_1, ..., ψ_n} (basis functions)
       ↓
  [Inference Phase] New dataset D_new → Compute c via (Eq. 3) → Predictor f̂(x) = ⟨c, φ(x)⟩
       ↓
  [Dimension Analysis] Coefficient covariance Σ → PCA → Select compact basis

- **Critical path:**
  1. Initialize multi-headed MLP (shared hidden layers, $n$ output heads for $n$ bases)
  2. Train on multiple functions jointly via Eq. 4 (sum over datasets and samples)
  3. Compute coefficients per dataset via Eq. 3
  4. Form covariance $\Sigma$, eigendecompose, select $r$ via cumulative explained variance ≥ τ
  5. If train-then-prune: score bases via $s_p = \sum_{i=1}^r \lambda_i U_{pi}^2$, prune, fine-tune

- **Design tradeoffs:**
  - **Progressive vs. train-then-prune:** Progressive yields ordered, interpretable bases but is sequential/slow. Train-then-prune is parallel/fast but bases lack ordering and require fine-tuning.
  - **Overparameterization level:** Start with $B \gg r$ (train-then-prune) vs. grow incrementally (progressive). Paper uses $B=10$ or $20$ for intrinsic dimensions of 2-6.
  - **Threshold τ:** Paper uses 99% or 1% drop; remains heuristic—no non-heuristic criteria derived.

- **Failure signatures:**
  - Eigenvalue decay without clear elbow → ambiguous dimension selection (seen in two-body problem)
  - Basis collapse → numerical instability in Eq. 3; use regularization or soft orthonormality penalties
  - Generalization gap → check if $m$ is insufficient relative to $n$; increase $\lambda$ or collect more data

- **First 3 experiments:**
  1. **Polynomial validation:** Sample degree-$d$ polynomials, run both algorithms, verify recovered dimension equals $d+1$. Confirm scree plot elbow matches (reproduce Fig. 1).
  2. **Ablation on threshold τ:** Sweep τ ∈ {0.90, 0.95, 0.99, 0.999} on Van der Pol. Measure MSE vs. basis count tradeoff. Expect diminishing returns past true dimension.
  3. **Generalization bound verification:** Fix $n=4$, vary $m \in \{50, 100, 200, 500\}$, measure population risk. Check if error decreases as $\tilde{O}(1/\sqrt{m})$ predicted by Theorem 1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can non-heuristic criteria be developed for selecting the optimal number of basis functions?
- Basis in paper: [explicit] The conclusion explicitly calls for "developing non-heuristic criteria for basis selection" to improve upon current methods.
- Why unresolved: Current progressive and train-then-prune algorithms rely on user-specified variance thresholds (τ) or explained variance ratios which are described as "primarily heuristic."
- What evidence would resolve it: A theoretically derived stopping criterion that automatically determines the intrinsic dimension based on data statistics without manual threshold tuning.

### Open Question 2
- Question: Can the train-then-prune method guarantee capturing the desired variance without requiring a fine-tuning step?
- Basis in paper: [inferred] Section 4.2 notes that unlike progressive training, basis functions selected by train-then-prune are "not guaranteed to capture the desired variance."
- Why unresolved: Joint training distributes information across bases such that simple eigenvalue-based scoring and pruning may discard necessary representational capacity.
- What evidence would resolve it: A modified pruning metric or theoretical proof ensuring the selected top-r bases span the principal subspace identified in the overparameterized model without retraining.

### Open Question 3
- Question: How do function encoders scale to high-dimensional scientific modeling applications?
- Basis in paper: [explicit] The conclusion identifies "extending the framework to applications such as statistical and scientific modeling" as a key open direction.
- Why unresolved: Current experiments are restricted to low-dimensional dynamical systems (Van der Pol, two-body) and polynomial benchmarks where the intrinsic dimension is small and clear.
- What evidence would resolve it: Application of the method to complex, high-dimensional physical systems (e.g., fluid dynamics or chaotic systems) where the intrinsic dimension is unknown and the basis functions must capture highly complex interactions.

## Limitations

- Generalization bounds rely on fixed basis assumption and bounded functions, which may not hold during joint optimization
- Dimension selection heuristic based on cumulative explained variance lacks theoretical grounding
- Empirical validation limited to low-dimensional problems; scalability to high-dimensional scientific modeling remains unproven
- Train-then-prune approach lacks interpretability as bases are unordered and require fine-tuning

## Confidence

**High Confidence:** The kernel construction mechanism and PCA-based dimension analysis are mathematically sound and empirically validated.

**Medium Confidence:** The Rademacher complexity and PAC-Bayes generalization bounds are correctly derived given the assumptions, but their practical tightness remains unclear.

**Low Confidence:** The empirical validation across three domains is limited in scope, and the choice of neural architectures appears tuned without ablation studies.

## Next Checks

1. **Bound Sensitivity Analysis:** Systematically vary the regularization parameter λ and basis count n in the two-body problem experiment. Measure how the generalization bound's theoretical predictions match empirical population risk.

2. **Dimensionality Selection Robustness:** For the Van der Pol oscillator, sweep the cumulative explained variance threshold τ across {0.90, 0.95, 0.99, 0.999}. Plot MSE vs. basis count to identify if the method consistently identifies the true intrinsic dimension.

3. **Kernel Method Comparison:** Implement a direct kernel ridge regression baseline using the same learned kernel k(x,x') = φ(x)ᵀφ(x'). Compare generalization performance on the two-body problem against the function encoder approach.