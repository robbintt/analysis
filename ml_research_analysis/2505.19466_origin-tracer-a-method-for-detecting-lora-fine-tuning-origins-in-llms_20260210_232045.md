---
ver: rpa2
title: 'Origin Tracer: A Method for Detecting LoRA Fine-Tuning Origins in LLMs'
arxiv_id: '2505.19466'
source_url: https://arxiv.org/abs/2505.19466
tags:
- rank
- obfuscation
- output
- layer
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Origin-Tracer, a novel method for detecting
  LoRA fine-tuning origins in large language models. The approach addresses the challenge
  of identifying whether a model has been fine-tuned from a specified base model,
  particularly when obfuscation techniques like parameter permutation are employed.
---

# Origin Tracer: A Method for Detecting LoRA Fine-Tuning Origins in LLMs

## Quick Facts
- **arXiv ID**: 2505.19466
- **Source URL**: https://arxiv.org/abs/2505.19466
- **Reference count**: 21
- **Primary result**: Origin-Tracer accurately identifies LoRA fine-tuning origins in 31 diverse open-source models, even under parameter permutation obfuscation

## Executive Summary
This paper introduces Origin-Tracer, a novel method for detecting LoRA fine-tuning origins in large language models. The approach addresses the challenge of identifying whether a model has been fine-tuned from a specified base model, particularly when obfuscation techniques like parameter permutation are employed. By leveraging the unique properties of transformer architectures and employing iterative reconstruction techniques, Origin-Tracer can accurately identify the fine-tuning source even under obfuscation. The method was validated on 31 diverse open-source models, demonstrating robust performance in identifying fine-tuning across all tested models.

## Method Summary
Origin-Tracer works by extracting and analyzing intermediate states from the self-attention mechanism to determine the LoRA rank used during fine-tuning. The method feeds single-token inputs through both base and candidate models, reconstructs intermediate states between self-attention and MLP via iterative gradient descent, computes the difference matrix between reconstructed states, applies SVD, and detects rank via peak in consecutive singular value ratios. The approach uses random sampling of n = hidden_size/2 tokens across multiple cycles to improve accuracy, selecting the minimum rank that appears consistently.

## Key Results
- Origin-Tracer successfully identified fine-tuning sources across all 31 tested models
- The method works even when parameter permutation obfuscation is applied
- Rank detection accuracy was highest in middle layers (2-6) of transformer architectures

## Why This Works (Mechanism)

### Mechanism 1
LoRA fine-tuning creates detectable low-rank structures in the combined value-output projection matrix (W_V × W_O) that persist even under permutation obfuscation. LoRA updates weight matrices as W' = W + ΔW where ΔW has low rank (the LoRA rank). When computing the product W_V × W_O, this low-rank structure manifests as a sharp drop in singular values at position r (the rank). The paper proves in Theorem 3 that V and O projection matrices are uniquely determined by input-output pairs, meaning the low-rank signature is inherent to the fine-tuning itself, not the parameter arrangement.

### Mechanism 2
Intermediate states can be reconstructed from layer outputs by exploiting the injectivity of the MLP function. The paper proves (Lemma 1) that the MLP function is injective for non-parallel vectors with probability 1. Given an output z_c, the intermediate state y can be recovered via iterative gradient descent: y_{m+1} = y_m - α∇||f(y_m) - z_c||². This allows bypassing obfuscation by working with functional outputs rather than comparing permuted weights directly.

### Mechanism 3
Random sampling over multiple selections converges to the true LoRA rank with probability approaching 1. Since individual intermediate state reconstructions may have varying accuracy, the method selects n = hidden_size/2 one-dimensional tensor inputs and performs t cycles of random selection. The algorithm picks the result with the largest ratio of consecutive singular values as the rank estimate.

## Foundational Learning

- **Concept**: Low-Rank Adaptation (LoRA)
  - **Why needed here**: The entire method hinges on recognizing that LoRA creates structured, detectable modifications. Without understanding W' = W + BA (where B×A is rank-r), the singular value analysis won't make sense.
  - **Quick check question**: If a model is fine-tuned with LoRA rank 32 on both V and O projections, what singular value drop would you expect?

- **Concept**: Singular Value Decomposition (SVD) and rank detection
  - **Why needed here**: The method identifies LoRA rank by finding "sharp declines" in singular values. Understanding how low-rank matrices produce SVD signatures is essential for interpreting results.
  - **Quick check question**: Why does a rank-r matrix have exactly r non-zero singular values?

- **Concept**: Permutation obfuscation in neural networks
  - **Why needed here**: The threat model assumes adversaries permute parameters to hide lineage. Understanding why permutation preserves function (ΠW vs W) but breaks direct comparison clarifies why this method works.
  - **Quick check question**: If MLP_obf = Π(MLP) and attention is similarly permuted, why does the final output remain unchanged?

## Architecture Onboarding

- **Component map**: Input Token → Tokenizer → Embedding → [Decoder Layer × N] → Output → Layer Input/Output Extraction → Gradient Descent Reconstruction → SVD on Intermediate Difference Matrix → Rank Estimation via Singular Value Ratio Peaks

- **Critical path**: The reconstruction step (Section 4.2) is the bottleneck—incorrect intermediate states propagate to useless rank estimates. Focus validation efforts here first.

- **Design tradeoffs**:
  - Using one-dimensional tensors simplifies attention (softmax becomes 1) but requires more samples
  - Selecting top 10% layers by singular value ratio improves accuracy but assumes middle-layer dominance
  - Hidden-size/2 samples balance computational cost against coverage

- **Failure signatures**:
  - Extracted ranks that vary wildly across layers → reconstruction instability
  - No clear peak in singular value ratios → either wrong base model or non-LoRA fine-tuning
  - Consistent rank underestimation → insufficient optimization iterations

- **First 3 experiments**:
  1. **Sanity check**: Run Origin-Tracer on a known LoRA fine-tuned model with ground-truth rank (e.g., rank-8 from Table 1). Verify extracted rank matches.
  2. **Obfuscation robustness test**: Apply random permutations to attention weights of a fine-tuned model. Confirm rank extraction still succeeds.
  3. **Negative control**: Test on a model fine-tuned with full parameter updates (not LoRA). Verify no clear singular value peak emerges.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the methodology be extended to detect fine-tuning origins when the Multi-Layer Perceptron (MLP) modules have been modified?
  - **Basis in paper**: [explicit] Section 5.4 states the method is "currently not applicable to models with changes in the multi-layer perceptron (MLP) architecture."
  - **Why unresolved**: The current reconstruction algorithm relies on the invertibility of the unmodified base model's MLP to approximate intermediate states; changes to this module break the inversion path.
  - **What evidence would resolve it**: A modified algorithm that successfully identifies base models even when the MLP layers of the candidate model have undergone fine-tuning.

- **Open Question 2**: Can the detection framework be adapted to verify origins for modifications that do not conform to low-rank constraints, such as full parameter fine-tuning?
  - **Basis in paper**: [explicit] Section 5.4 identifies the "Constraints of Low-Rank Modifications" as a limitation and suggests investigating "alternative strategies that accommodate a broader spectrum of parameter modifications."
  - **Why unresolved**: Origin-Tracer detects lineage by identifying sharp declines in singular values, a pattern specific to LoRA; full fine-tuning would likely obscure this signal.
  - **What evidence would resolve it**: Successful extraction of origin signals or rank estimations from models known to have undergone full or high-rank fine-tuning.

- **Open Question 3**: Is it feasible to apply this tracing method when parameter modifications are restricted to the query (Q) and key (K) components rather than the value (V) and output (O) components?
  - **Basis in paper**: [explicit] Section 5.4 notes the method "mandates parameter modifications specifically within the 'V' and 'O' components" and lists this as a limiting factor.
  - **Why unresolved**: The mathematical proof (Theorem 3) relies on uniquely determining $W_V$ and $W_O$ from intermediate states; separating Q/K modifications from this specific reconstruction process remains an unsolved challenge.
  - **What evidence would resolve it**: A theoretical extension or empirical demonstration showing accurate origin detection on models fine-tuned exclusively on Q and K weights.

## Limitations
- Method currently not applicable to models with changes in the multi-layer perceptron (MLP) architecture
- Requires parameter modifications specifically within the 'V' and 'O' components of self-attention
- Reconstruction accuracy varies significantly by layer—middle layers perform better than initial/final layers

## Confidence
- **High confidence**: The theoretical foundation linking LoRA low-rank structure to detectable singular value patterns is well-established
- **Medium confidence**: The practical effectiveness of iterative reconstruction depends heavily on optimization hyperparameters and input diversity
- **Low confidence**: The method's robustness against sophisticated obfuscation techniques beyond simple parameter permutation remains untested

## Next Checks
1. **Reconstruction accuracy quantification**: Measure the mean squared error between reconstructed and actual intermediate states across all layers for both base and fine-tuned models. Plot reconstruction fidelity as a function of layer depth and optimize hyperparameters to maximize accuracy.

2. **Obfuscation robustness testing**: Implement and test against advanced obfuscation strategies including random weight perturbations, mixed fine-tuning (some layers full fine-tuning, others LoRA), and combined LoRA fine-tuning on both V and O projections. Measure how these affect rank detection accuracy.

3. **Negative control validation**: Test Origin-Tracer on models fine-tuned using methods other than LoRA (full fine-tuning, prefix tuning, adapter methods) to verify the method specifically detects LoRA signatures rather than any fine-tuning. Measure false positive rates and characterize what the method detects in non-LoRA scenarios.