---
ver: rpa2
title: 'From Chaos to Automation: Enabling the Use of Unstructured Data for Robotic
  Process Automation'
arxiv_id: '2507.11364'
source_url: https://arxiv.org/abs/2507.11364
tags:
- information
- text
- unstructured
- data
- invoice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of enabling Robotic Process Automation
  (RPA) to handle unstructured data, which constitutes approximately 80% of enterprise
  data but is traditionally difficult for RPA systems to process. The researchers
  developed UNDRESS (UNstructured Document REtrieval SyStem), a hybrid system combining
  fuzzy regular expressions, named entity recognition (NER), and large language models
  (LLMs) to extract information from unstructured documents like invoices and resumes.
---

# From Chaos to Automation: Enabling the Use of Unstructured Data for Robotic Process Automation

## Quick Facts
- arXiv ID: 2507.11364
- Source URL: https://arxiv.org/abs/2507.11364
- Authors: Kelly Kurowski; Xixi Lu; Hajo A. Reijers
- Reference count: 29
- Key outcome: UNDRESS achieves 0.99 Jaccard Similarity for resume text extraction and 83-100% accuracy in information retrieval across multiple fields

## Executive Summary
This study addresses the critical challenge of enabling Robotic Process Automation (RPA) to handle unstructured data, which comprises approximately 80% of enterprise data. The researchers developed UNDRESS (UNstructured Document REtrieval SyStem), a hybrid system that combines fuzzy regular expressions, named entity recognition (NER), and large language models (LLMs) to extract structured information from unstructured documents like invoices and resumes. The system demonstrates significant improvements in text fidelity and information retrieval accuracy, making previously inaccessible unstructured data available for RPA automation.

## Method Summary
UNDRESS employs a two-module pipeline: Text Extraction uses OpenCV preprocessing, Tesseract OCR, spell-checking, and LLM correction to convert images to clean text; Information Retrieval applies a sequential fallback approach from simple to complex methods—fuzzy regex for static patterns (emails, phone numbers), spaCy NER for entities (names, organizations), and GPT-3.5 for semantic extraction of ambiguous fields. The system was evaluated on 100 real invoices and 400 synthetic resumes, achieving high accuracy across multiple extraction tasks through this tiered processing approach.

## Key Results
- Jaccard Similarity Index of 0.99 for resume text extraction and 0.81 for invoices
- Information retrieval accuracy ranging from 83-100% across various fields
- Sequential extraction cascade (Fuzzy RegEx → NER → LLM) proved more reliable than single-method approaches
- Qualitative evaluation with RPA developers found the system intuitive and valuable for automating processes involving unstructured documents

## Why This Works (Mechanism)

### Mechanism 1: Sequential Extraction Cascade
- **Claim:** A tiered pipeline (Fuzzy RegEx → NER → LLM) yields higher reliability and efficiency than a single-method approach for mixed document types.
- **Mechanism:** The system first attempts extraction using low-cost, high-precision fuzzy regular expressions (e.g., for emails/phone numbers). If these fail to match, it proceeds to Named Entity Recognition (NER) for semantic entities (e.g., names/locations). Only when these structured methods fail does it invoke a Large Language Model (LLM) for complex semantic reasoning.
- **Core assumption:** The target information can be categorized into distinct complexity levels where rule-based methods suffice for formatted data and semantic models are strictly necessary for unstructured text.
- **Evidence anchors:** Section 4.4 shows address, email, and phone fields achieved 1.00 accuracy using Fuzzy Regex, while complex fields like Job Title (0.95) and Soft Skills (0.88) required the LLM.

### Mechanism 2: OCR Noise Interpolation via LLM Correction
- **Claim:** Post-processing Optical Character Recognition (OCR) output with an LLM significantly improves text fidelity compared to raw OCR or rule-based spell-checking alone.
- **Mechanism:** Traditional OCR engines produce character-level errors. The LLM acts as a context-aware corrector, using surrounding tokens to infer the intended word, repairing the text stream before information extraction occurs.
- **Core assumption:** The LLM possesses sufficient general knowledge to disambiguate noisy text without hallucinating new content.
- **Evidence anchors:** Section 4.3 shows the text extraction module improved from a Jaccard Similarity of 0.90 to 0.99 on resumes after LLM correction.

### Mechanism 3: Semantic Alignment via Prompt Engineering
- **Claim:** Extracting specific fields from unstructured text is more accurate when using targeted prompts that explicitly map a label to the text context.
- **Mechanism:** The system uses an LLM not just as a text processor, but as a semantic aligner. By feeding the prompt `Extract {user_input} from the following text`, the model focuses its attention mechanism specifically on the definition of `{user_input}`, filtering out irrelevant data.
- **Core assumption:** The LLM has been sufficiently pretrained on similar document structures to understand the semantic role of the requested field without fine-tuning.
- **Evidence anchors:** Section 4.4 shows the LLM component achieved 0.90 accuracy for "Invoice number" and 0.83 for "Seller," outperforming NER in complex categories.

## Foundational Learning

- **Concept:** **Fuzzy Regular Expressions (Fuzzy RegEx)**
  - **Why needed here:** Standard RegEx fails if OCR introduces a single extra space or typo. Fuzzy matching allows for slight deviations (Levenshtein distance), which is critical for surviving the noise of PDF-to-text conversion.
  - **Quick check question:** How would a standard RegEx `\d{3}-\d{4}` fail if OCR outputs `8O0- 1234` (O instead of 0)? (Answer: It requires exact character matches; Fuzzy RegEx tolerates the substitution).

- **Concept:** **Jaccard Similarity Index**
  - **Why needed here:** This is the specific metric used to validate the Text Extraction module. It measures the overlap of unique words (Intersection over Union), which punishes both missing words and added hallucinated words.
  - **Quick check question:** If Ground Truth is "Apple Banana" and Extraction is "Apple Orange", what is the Jaccard Index? (Answer: Intersection = {Apple}, Union = {Apple, Banana, Orange}. Index = 1/3 ≈ 0.33).

- **Concept:** **Named Entity Recognition (NER)**
  - **Why needed here:** This is the "middle layer" of the system. It bridges the gap between strict patterns (RegEx) and open-ended generation (LLM) by identifying predefined categories like Person, Org, or Geo.
  - **Quick check question:** Why would NER fail to extract a "Total Amount" of "$1,000.00"? (Answer: NER typically classifies entities like Money/Currency, but extracting the *exact value* and associating it with the specific label "Total Amount" vs "Tax Amount" often requires relation extraction or LLM reasoning).

## Architecture Onboarding

- **Component map:** Unstructured PDF/Image → OpenCV preprocessing → Tesseract OCR (Raw Text) → Python SpellChecker → LLM (GPT-3.5-Turbo) Correction → Fuzzy RegEx (Patterns: Email, Phone, Date) → spaCy NER (Entities: Name, Org) → LLM (Semantics: Skills, ambiguous fields) → JSON/Key-Value pairs

- **Critical path:** The OCR-to-LLM Correction handoff. If the LLM correction prompt is too aggressive ("Format this text"), it destroys original spacing or structure; if too passive, it leaves OCR typos. The paper found "Correct spelling mistakes... do NOT format" was optimal.

- **Design tradeoffs:**
  - **Accuracy vs. Cost:** The system only calls the LLM (expensive/slow) if Fuzzy RegEx (cheap/fast) fails.
  - **Precision vs. Recall:** For "Invoice Date", RegEx achieved 1.00 Precision but 0.77 Recall. Using LLM would increase Recall but likely lower Precision.

- **Failure signatures:**
  - **The "Language" Ambiguity:** Asking for "Language" triggers the LLM to identify the document's language rather than the entity, causing a drastic accuracy drop.
  - **Alignment Drift:** Invoices with multiple "Total Amount" labels confuse the simple extraction logic, leading to redundant values.

- **First 3 experiments:**
  1. **Baseline OCR vs. Hybrid:** Run Tesseract on 10 noisy invoices, then run Tesseract + SpellCheck + LLM Correction. Measure Jaccard Similarity to verify the improvement delta.
  2. **Retrieval Boundary Testing:** Feed the system a resume but query a field that *overlaps* categories (e.g., "Python" as a Hard Skill vs. a Language). Determine if the Fuzzy RegEx fires first or if it falls through to LLM.
  3. **Prompt Sensitivity Analysis:** Test the "Language" failure case. Modify the prompt from `Extract Language` to `Extract spoken languages listed in skills section`. Measure if accuracy recovers to NER levels.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can spatial analysis techniques significantly improve label-entity alignment in unstructured document processing?
- **Basis in paper:** The authors state "For future work, we propose enhancing label-entity alignment through spatial analysis."
- **Why unresolved:** The current system relies primarily on sequential text processing without leveraging spatial relationships between elements on the page.
- **What evidence would resolve it:** An empirical study comparing extraction accuracy with and without spatial analysis features across diverse document layouts.

### Open Question 2
- **Question:** How can hybrid systems dynamically optimize the selection of extraction techniques (fuzzy RegEx, NER, LLM) based on document characteristics?
- **Basis in paper:** The authors mention "refining the system's decision-making about which techniques to apply" as future work.
- **Why unresolved:** Currently, the system applies techniques in a fixed sequence rather than making context-aware decisions about which method would be most effective for each extraction task.
- **What evidence would resolve it:** Development and evaluation of an adaptive selection mechanism that outperforms the current sequential approach.

### Open Question 3
- **Question:** How does UNDRESS performance compare to existing industrial solutions across a wider range of document types?
- **Basis in paper:** The qualitative evaluation revealed developers mentioned existing solutions like Klippa but noted they are limited to specific document types.
- **Why unresolved:** The evaluation was limited to resumes and invoices, with no systematic comparison to commercial document processing tools.
- **What evidence would resolve it:** A comparative study including diverse document types benchmarked against industrial solutions.

## Limitations
- The system was evaluated on only two document types (invoices and resumes) using proprietary datasets, limiting broader applicability
- No analysis of computational costs or processing time was provided, despite reliance on expensive LLM calls as fallback
- The evaluation focused on Dutch and English languages, raising questions about performance on other languages or script systems

## Confidence
- **High Confidence:** The core mechanism of sequential extraction (RegEx → NER → LLM) and its effectiveness in the tested domains
- **Medium Confidence:** The generalizability of the approach to other document types and real-world enterprise environments
- **Low Confidence:** Claims about enterprise-scale impact without supporting evidence from actual RPA deployment scenarios

## Next Checks
1. Test the system on a diverse set of document types beyond invoices and resumes, including contracts, forms, and mixed-content documents, to evaluate generalizability
2. Conduct cost-benefit analysis comparing the sequential extraction approach against pure LLM or pure rule-based alternatives, measuring both accuracy and processing time
3. Validate the system using publicly available datasets with ground truth annotations to enable independent replication and comparison with existing IDP solutions