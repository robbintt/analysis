---
ver: rpa2
title: 'Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive
  Reinforcement Learning'
arxiv_id: '2505.12432'
source_url: https://arxiv.org/abs/2505.12432
tags:
- reasoning
- arxiv
- answer
- multimodal
- difficulty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Observe-R1, a novel reinforcement learning
  framework designed to enhance reasoning capabilities of multimodal large language
  models (MLLMs). The key innovation is a progressive learning paradigm inspired by
  human learning progression, where models learn from simple to complex tasks.
---

# Observe-R1: Unlocking Reasoning Abilities of MLLMs with Dynamic Progressive Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.12432
- Source URL: https://arxiv.org/abs/2505.12432
- Reference count: 7
- Primary result: Observe-R1-3B outperforms larger reasoning models on both reasoning and general benchmarks

## Executive Summary
This paper introduces Observe-R1, a reinforcement learning framework that enhances reasoning capabilities of multimodal large language models through progressive curriculum learning, multimodal format constraints, and dynamic weighting mechanisms. The authors construct the NeuraLadder dataset organized by difficulty and complexity, implement a bonus reward system favoring concise correct answers, and introduce a dynamic weighting function that prioritizes medium-difficulty problems. Experiments demonstrate that Observe-R1-3B achieves superior performance on reasoning benchmarks compared to larger models, while maintaining clarity and conciseness in reasoning chains.

## Method Summary
Observe-R1 modifies standard GRPO training through three key innovations: a progressive curriculum using the NeuraLadder dataset sorted by difficulty and complexity metrics, a multimodal format constraint requiring explicit image observation before reasoning via `<observe></observe>` tags, and a dynamic weighting mechanism that assigns maximum importance to medium-difficulty samples. The framework also introduces a bonus reward for the shortest correct answer, encouraging concise reasoning. Training uses 20k samples with Qwen2.5-VL-3B and 7B models, achieving state-of-the-art results on both reasoning and general multimodal benchmarks.

## Key Results
- Observe-R1-3B outperforms Qwen2.5-VL-32B-RL and Qwen2.5-VL-72B-RL on reasoning benchmarks despite being significantly smaller
- The model achieves superior clarity and conciseness in reasoning chains compared to baseline approaches
- Ablation studies validate the effectiveness of progressive learning, multimodal format constraints, and dynamic weighting strategies
- Observe-R1 shows strong performance on both reasoning-specific and general multimodal benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Progressive Curriculum Learning via Difficulty-Organized Data
Organizing RL training data by difficulty and complexity enables more stable learning progression than random sampling. The NeuraLadder dataset sorts samples by difficulty (1 - correct ratio across 8 generations) and complexity (average response length), then smooths via adjacent-level mixing. This allows the model to build reasoning foundations before encountering harder problems.

### Mechanism 2: Multimodal Format Constraint Enforces Visual Grounding
Requiring explicit image observation before reasoning improves visual grounding and reduces reasoning errors from missed visual details. `<observe></observe>` tags force the model to describe relevant image content before the `<think\>` reasoning phase.

### Mechanism 3: Dynamic Weighting Prioritizes Informative Samples
Weighting samples by uncertainty (peaking at medium difficulty) improves gradient quality and training stability. Weight function `f(d) = 4σd(1-d)` assigns maximum weight at difficulty d=0.5 and zero weight at d=0 or d=1, filtering out trivially-easy and impossibly-hard samples.

## Foundational Learning

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed here: Observe-R1 modifies GRPO's objective with dynamic weighting and bonus rewards
  - Quick check question: Can you explain how GRPO computes advantages using group-relative rewards instead of a critic model?

- **Concept: Rule-based Verifiable Rewards**
  - Why needed here: The framework extends standard accuracy + format rewards with a bonus reward for conciseness
  - Quick check question: Can you distinguish between accuracy reward (correct answer), format reward (structured output), and bonus reward (shortest correct)?

- **Concept: Difficulty Metrics for Curriculum Learning**
  - Why needed here: NeuraLadder relies on accurate difficulty estimation for proper curriculum ordering
  - Quick check question: How would you compute difficulty if you lacked access to a reference model for multi-sample generation?

## Architecture Onboarding

- **Component map:** Reference MLLM multi-sample generation -> Difficulty/Complexity Metrics -> NeuraLadder Dataset Construction -> Modified GRPO Training with Dynamic Weighting, Format Constraint, Bonus Reward

- **Critical path:** 1) Generate 8 responses per question using reference MLLM 2) Compute difficulty (1 - accuracy) and complexity (avg length) 3) Sort by difficulty, then complexity; filter and smooth 4) Train with modified GRPO objective using dynamic weights and bonus rewards

- **Design tradeoffs:** Bonus reward length constraint `ℓ`: Too low → risk of skipping reasoning; too high → bonus rarely triggered; Difficulty estimator model size: 3B may underestimate medium/hard difficulty; 72B may underestimate easy difficulty

- **Failure signatures:** Response length collapse: Bonus reward without proper `ℓ` causes model to skip reasoning and output only answers; Reward instability: Without dynamic weighting's filtering, all-correct/all-incorrect samples cause gradient noise

- **First 3 experiments:** 1) Validate NeuraLadder effect: Train GRPO on random vs. difficulty-sorted data; compare reward curves and final benchmark scores 2) Ablate multimodal format: Train with standard `<think\>` format vs. `<observe\>` + `<think\>`; measure vision-heavy task accuracy 3) Tune dynamic weighting σ: Sweep σ values and observe training stability vs. final performance tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What types of correct answers and reasoning chains should receive bonus rewards, and how can this be systematically determined?
- Basis in paper: The authors state "it is promising to determine what types of correct answers and reasoning should receive additional bonus rewards" as a future direction
- Why unresolved: The current bonus reward only considers response length; distinguishing reasoning quality beyond conciseness remains unexplored
- What evidence would resolve it: A study correlating different reasoning quality metrics (e.g., logical coherence, step validity) with downstream task performance

### Open Question 2
- Question: How would Observe-R1's strategies scale to larger MLLMs (e.g., 70B+ parameters)?
- Basis in paper: "Due to limited computational resources, we are unable to conduct detailed experiments to further explore our strategies, including the use of larger MLLMs"
- Why unresolved: Only 3B and 7B models were tested; it is unclear if progressive learning and dynamic weighting provide diminishing returns at larger scales
- What evidence would resolve it: Experiments applying Observe-R1 to 70B+ MLLMs with comparisons to baseline GRPO training

### Open Question 3
- Question: What alternative dynamic weighting functions could better prioritize medium-difficulty samples?
- Basis in paper: The authors mention inability to explore "more dynamic weighting functions" due to resource constraints
- Why unresolved: Only one symmetric function f(d) = 4σd(1-d) was tested; other functional forms may better capture uncertainty dynamics
- What evidence would resolve it: Ablation studies comparing different weighting functions (e.g., asymmetric, learned functions) on training stability and final performance

### Open Question 4
- Question: Can the multimodal format constraint with `<observe>` tags generalize effectively to non-image modalities like video or audio?
- Basis in paper: The format was designed specifically for image-text pairs; the paper does not address other modalities despite MLLMs increasingly supporting them
- Why unresolved: The approach assumes static visual content; video and audio may require different observation structuring
- What evidence would resolve it: Experiments applying the format constraint to video-understanding and audio-reasoning benchmarks

## Limitations
- NeuraLadder dataset construction depends heavily on the quality of the reference MLLM's multi-sample generations
- The dynamic weighting mechanism assumes difficulty estimates remain stable during training
- Format constraint may introduce rigid structure that could hurt performance on tasks requiring more flexible reasoning approaches

## Confidence
- **High confidence**: Progressive curriculum learning improves stability over random sampling (validated by reward curves and MathVista comparisons)
- **Medium confidence**: Multimodal format constraint improves visual grounding (supported by MathVerse VO improvements but lacks broader cross-dataset validation)
- **Medium confidence**: Dynamic weighting improves training efficiency (statistically significant improvements in ablation studies but mechanism explanation could be deeper)

## Next Checks
1. Test NeuraLadder effectiveness with different reference model sizes (3B vs 72B) to quantify sensitivity to difficulty estimation quality
2. Evaluate whether multimodal format constraint generalizes to non-mathematical reasoning tasks (e.g., visual question answering, document understanding)
3. Analyze whether dynamic weighting creates overfitting to medium-difficulty samples by testing transfer to easier/harder out-of-distribution problems