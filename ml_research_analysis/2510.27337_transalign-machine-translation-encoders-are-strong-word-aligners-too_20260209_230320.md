---
ver: rpa2
title: 'TransAlign: Machine Translation Encoders are Strong Word Aligners, Too'
arxiv_id: '2510.27337'
source_url: https://arxiv.org/abs/2510.27337
tags:
- transalign
- language
- association
- word
- accalign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TransAlign, a novel word aligner that leverages
  the encoder of a massively multilingual machine translation model (NLLB) to improve
  cross-lingual transfer for token classification tasks. The core method uses the
  encoder's contextualized embeddings to compute alignment scores between source and
  target language tokens, applying an alignment threshold to establish word pairs.
---

# TransAlign: Machine Translation Encoders are Strong Word Aligners, Too

## Quick Facts
- **arXiv ID:** 2510.27337
- **Source URL:** https://arxiv.org/abs/2510.27337
- **Authors:** Benedikt Ebing; Christian Goldschmied; Goran Glavaš
- **Reference count:** 29
- **Key outcome:** TransAlign uses NLLB encoder embeddings to substantially improve cross-lingual transfer for token classification tasks compared to existing aligners

## Executive Summary
TransAlign introduces a novel word alignment method that leverages the encoder of a massively multilingual machine translation model (NLLB) to improve cross-lingual transfer for token classification tasks. The method computes alignment scores between source and target language tokens using contextualized embeddings, applying an alignment threshold to establish word pairs. Experiments on two token classification benchmarks (MasakhaNER2.0 and xSID) covering 28 diverse languages show that TransAlign substantially outperforms popular word aligners (AwsmAlign, AccAlign) and a state-of-the-art non-word aligner method (Codec) in translation-based cross-lingual transfer. The method is computationally efficient as it uses only the encoder portion of the MT model.

## Method Summary
TransAlign extracts contextualized embeddings from the NLLB encoder for source and target sentences, then computes alignment scores by measuring the cosine similarity between token embeddings across languages. An alignment threshold (τ) determines which token pairs are considered aligned. For downstream token classification tasks, the method uses these alignments to transfer annotations from source to target language instances. The approach is translation-based, requiring parallel text, but only uses the encoder component rather than the full MT model, making it computationally efficient while still achieving strong alignment quality, particularly for content words rather than stopwords.

## Key Results
- TransAlign outperforms popular word aligners (AwsmAlign, AccAlign) and non-word aligner methods (Codec) on two token classification benchmarks covering 28 languages
- The method shows particular strength in aligning content words rather than stopwords, which directly translates to improved downstream performance
- TransAlign achieves state-of-the-art results in translation-based cross-lingual transfer for named entity recognition and slot filling tasks

## Why This Works (Mechanism)
The effectiveness of TransAlign stems from the rich multilingual representations learned by the NLLB encoder during massive-scale pretraining. These contextualized embeddings capture semantic and syntactic relationships across languages, enabling accurate token-level alignment. The cosine similarity-based scoring effectively measures semantic equivalence between tokens in different languages, while the threshold mechanism filters out low-confidence alignments. The method's strength with content words (nouns, verbs, adjectives) rather than function words is particularly valuable for downstream tasks like named entity recognition where content words carry the semantic information necessary for correct classification.

## Foundational Learning
- **Cross-lingual transfer learning**: The ability to apply knowledge from one language to another; needed to understand how word alignment supports downstream task performance across language pairs; quick check: verify that transferred annotations improve target language classification accuracy
- **Contextualized embeddings**: Word representations that depend on surrounding context; needed to understand how NLLB encoder captures nuanced meaning across languages; quick check: confirm that embeddings change meaningfully with different sentence contexts
- **Cosine similarity for semantic matching**: A metric for measuring vector similarity in high-dimensional space; needed to understand how alignment scores are computed between tokens; quick check: verify that semantically similar tokens have higher cosine similarity scores
- **Translation-based vs. zero-shot transfer**: Different approaches to cross-lingual learning; needed to understand the methodological assumptions and limitations; quick check: confirm that parallel text is required for the method to function
- **Token classification**: The task of assigning labels to individual tokens (words/subwords); needed to understand the downstream evaluation tasks; quick check: verify that entity boundaries are correctly identified in target language
- **Word alignment quality metrics**: Measures like precision, recall, and F1 for evaluating alignment accuracy; needed to understand how intrinsic alignment performance is quantified; quick check: verify that content words have higher alignment accuracy than function words

## Architecture Onboarding

**Component Map:**
NLLB Encoder -> Embedding Extraction -> Cosine Similarity Scoring -> Alignment Thresholding -> Token Pair Selection -> Downstream Annotation Transfer

**Critical Path:**
The critical path flows from source sentence encoding through target sentence encoding, cosine similarity computation between all token pairs, threshold application, and finally to the selection of aligned token pairs for downstream transfer. The NLLB encoder is the core component whose quality directly determines alignment performance.

**Design Tradeoffs:**
The method trades computational efficiency for alignment quality by using only the encoder rather than the full MT model. This makes it faster and lighter but requires parallel text availability. The threshold parameter (τ) introduces a tunable hyperparameter that balances precision and recall but may need language-specific tuning. The reliance on NLLB's multilingual coverage limits applicability to languages included in its training data.

**Failure Signatures:**
Poor alignment quality when source and target languages are very distantly related or when dealing with low-resource language pairs not well-represented in NLLB's training data. Over-alignment (too many pairs above threshold) suggests threshold needs adjustment. Under-alignment (too few pairs) may indicate semantic divergence or poor encoder quality for specific language pairs. Performance degradation on function words is expected and intentional, but excessive degradation on content words indicates encoder limitations.

**First Experiments:**
1. Measure intrinsic alignment quality (precision/recall/F1) on standard word alignment test sets across multiple language pairs
2. Evaluate downstream token classification performance with different alignment thresholds (τ) to find optimal settings
3. Compare runtime efficiency against full MT model alignment approaches across different hardware configurations

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- The method requires parallel text, limiting applicability to zero-shot cross-lingual transfer scenarios where no translation is available
- Performance depends on NLLB's multilingual coverage, potentially limiting effectiveness for low-resource languages not well-represented in the model's training data
- The alignment threshold (τ) may require language-specific tuning for optimal performance across diverse language families
- Gains in token classification may not directly translate to other downstream tasks like machine translation or cross-lingual retrieval

## Confidence

**High confidence:** The core claim that NLLB encoder embeddings can be effectively used for word alignment is well-supported by intrinsic evaluation results showing superior content word alignment rates.

**High confidence:** The downstream transfer improvements on MasakhaNER2.0 and xSID benchmarks are statistically significant and consistent across multiple language pairs.

**Medium confidence:** The claim about computational efficiency relative to full MT models is supported but could benefit from more rigorous runtime comparisons across different hardware configurations.

## Next Checks

1. Evaluate TransAlign's performance in zero-shot cross-lingual transfer scenarios without parallel text, comparing against non-translation-based alignment methods.

2. Conduct ablation studies testing alignment performance using different MT model encoders (e.g., mBART, mT5) to assess whether NLLB's architecture or training data drives the improvements.

3. Measure runtime efficiency and memory usage across different MT encoder sizes (base vs. large) to establish scalability boundaries for practical deployment.