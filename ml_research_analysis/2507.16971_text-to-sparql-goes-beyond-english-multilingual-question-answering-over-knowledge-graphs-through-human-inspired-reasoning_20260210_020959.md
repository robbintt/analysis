---
ver: rpa2
title: 'Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge
  Graphs through Human-Inspired Reasoning'
arxiv_id: '2507.16971'
source_url: https://arxiv.org/abs/2507.16971
tags:
- mkgqagent
- step
- query
- plan
- sparql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces mKGQAgent, a multilingual knowledge graph
  question answering system that employs a human-inspired LLM agent architecture to
  convert natural language questions into SPARQL queries. The system decomposes the
  complex task into modular subtasks (planning, entity linking, query refinement)
  using coordinated agent workflows and an experience pool for in-context learning.
---

# Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning

## Quick Facts
- arXiv ID: 2507.16971
- Source URL: https://arxiv.org/abs/2507.16971
- Reference count: 40
- Key outcome: mKGQAgent achieves 54.83% F1 on English QALD-9-plus, wins 2025 Text2SPARQL challenge

## Executive Summary
This paper introduces mKGQAgent, a multilingual knowledge graph question answering system that converts natural language questions into SPARQL queries through a human-inspired LLM agent architecture. The system decomposes the complex KGQA task into modular subtasks (planning, entity linking, query refinement) using coordinated agent workflows and an experience pool for in-context learning. Evaluated on the QALD-9-plus benchmark across 10 languages, mKGQAgent achieved state-of-the-art performance with an F1 score of 54.83% on English questions and took first place in the 2025 Text2SPARQL challenge.

## Method Summary
mKGQAgent employs a two-phase approach: offline phase where SAgent processes training data to build an experience pool of successful and unsuccessful query attempts, and online phase where the agent system retrieves similar examples for in-context learning. The framework uses a plan step to decompose questions into subtasks, an action step with NEL tool to generate SPARQL, and a single-shot feedback step for query refinement. The system supports both native-language processing and machine translation to English, using multilingual-e5-large embeddings for experience pool retrieval and various LLMs (GPT-4o, Llama 3.1 70B, Qwen2.5) with temperature=0 and 16384 token context.

## Key Results
- Achieved 54.83% F1 on English QALD-9-plus, winning the 2025 Text2SPARQL challenge
- Demonstrated state-of-the-art multilingual performance with F1 scores ranging from 16.19% (Hindi) to 54.83% (English)
- Showed experience pool improves GPT-4o from 34.37% to 52.68% (+53.27% relative) for action step
- Outperformed existing systems on low-resource languages through machine translation approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Task decomposition through structured planning improves SPARQL generation quality by reducing cognitive load on the LLM at each step.
- **Mechanism:** The plan step generates a step-by-step list of subtasks (entity recognition, relation linking, query refinement), allowing the action step to handle one simple subtask at a time while maintaining conversation history of previous results.
- **Core assumption:** LLMs perform better when complex reasoning is decomposed into sequential, simpler operations rather than end-to-end generation.
- **Evidence anchors:**
  - [abstract] "breaks down the task of converting natural language questions into SPARQL queries into modular, interpretable subtasks"
  - [section 3.1.2] "the plan step intends to break down the complex task of writing a SPARQL query into a combination of simpler subtasks"
  - [corpus] SPARQL-LLM (FMR=0.58) similarly decomposes query generation into schema retrieval and query construction phases
- **Break condition:** If planning overhead exceeds token budget or if plan quality is poor (irrelevant steps), decomposition may introduce errors that compound through the pipeline.

### Mechanism 2
- **Claim:** Retrieving similar examples from an experience pool provides in-context learning signals that substantially improve both planning and query generation.
- **Mechanism:** During offline phase, store successful (F1=1.0) and unsuccessful attempts with embeddings in a vector database. At inference, retrieve top-N similar plans/queries and inject into prompts for the plan and action steps.
- **Core assumption:** Semantically similar questions benefit from similar reasoning patterns and query structures.
- **Evidence anchors:**
  - [abstract] "guided by an experience pool for in-context learning"
  - [section 5.5/Table 4] Experience pool for action step: GPT-4o improves from 34.37% to 52.68% (+53.27% relative); for plan step: Llama improves +110%
  - [corpus] Weak direct corpus evidence—few comparable experience-pool-based systems exist in KGQA
- **Break condition:** If training distribution diverges significantly from test queries (domain shift), retrieved examples may mislead rather than help.

### Mechanism 3
- **Claim:** Single-shot triplestore feedback enables error correction without infinite refinement loops.
- **Mechanism:** Execute intermediate SPARQL query on the triplestore, capture the response (results or error), and inject this as structured feedback into the action step for one refinement iteration.
- **Core assumption:** Most query errors are syntactic or minor structural issues fixable with execution feedback; one correction round is sufficient.
- **Evidence anchors:**
  - [section 3.2.3] "The feedback is formulated only once per input question, i.e., there are no multiple feedback options intended to avoid infinite loops"
  - [section 5.5/Table 4] Feedback step alone: GPT-4o improves from 34.37% to 40.47% (+17.74%)
  - [corpus] AGENTICT²S uses multi-agent collaborative reasoning but without explicit single-shot feedback constraint
- **Break condition:** If errors require multi-step debugging (e.g., wrong entity linked, wrong relation chain), single feedback round may be insufficient.

## Foundational Learning

- **Concept: SPARQL query structure (SELECT, WHERE, FILTER, OPTIONAL)**
  - Why needed here: The system generates executable SPARQL; understanding triple patterns, variable bindings, and query modifiers is essential to diagnose failures.
  - Quick check question: Can you explain why `SELECT ?uri WHERE { ?uri wdt:P31 wd:Q5 }` returns all humans in Wikidata?

- **Concept: Knowledge Graph entity linking (surface form → URI)**
  - Why needed here: LLMs cannot know arbitrary KG URIs; the NEL tool bridges natural language mentions to canonical identifiers.
  - Quick check question: Why can't an LLM directly output `wd:Q567` for "Angela Merkel" without external lookup?

- **Concept: In-context learning via retrieved examples**
  - Why needed here: The experience pool relies on embedding-based retrieval; understanding how similar examples condition LLM behavior is critical.
  - Quick check question: If retrieved examples use COUNT queries but the target question requires ASK, what failure mode might occur?

## Architecture Onboarding

- **Component map:** Offline phase: SAgent (plan + action + NEL) → experience pool (vector DB). Online phase: mKGQAgent (plan + action + NEL + feedback) → retrieves from experience pool → generates SPARQL → executes on triplestore → refines once.

- **Critical path:**
  1. Input question → plan step (retrieves similar plans from experience pool) → generates step-by-step plan
  2. Action step iterates through plan subtasks → calls NEL tool for entity/relation URIs → retrieves similar queries from experience pool → generates intermediate SPARQL
  3. Feedback step → executes on triplestore → captures response → action step refines → final query

- **Design tradeoffs:**
  - Quality vs. cost: Full mKGQAgent requires 13.03 LLM calls vs. 8.87 for baseline (Table 4)
  - MT vs. native processing: Translation to English helps low-resource languages but may hurt high-resource languages (German: -17.22% with GPT-4o)
  - Single vs. multi-turn feedback: One feedback round avoids infinite loops but limits error correction depth

- **Failure signatures:**
  - Low F1 on Cyrillic-script languages (Russian, Ukrainian) vs. Latin-script → suggests embedding model or LLM multilingual capability gaps
  - Open-source models (Llama 3.1 70B) achieve only 18.42% on English vs. GPT-4o's 54.83% → indicates model capability ceiling
  - Entity linking failures → query returns empty results despite syntactically valid SPARQL

- **First 3 experiments:**
  1. **Ablate each component** (plan-only, plan+experience-pool, plan+feedback, full) on a held-out language to measure individual contributions—replicate Table 4 methodology.
  2. **Test translation vs. native** on 3 languages from different families (Germanic, Slavic, Romance) using OPUS-MT—compare F1 deltas to identify where translation helps vs. hurts.
  3. **Stress-test feedback step** by injecting intentionally malformed intermediate queries—measure how often single-shot correction succeeds vs. fails to assess refinement ceiling.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** In what specific multilingual KGQA settings does native-language processing outperform machine translation to English?
- **Basis in paper:** [explicit] "the use of different translation techniques requires further systematic study to identify settings where each of them performs best"
- **Why unresolved:** GPT-4o showed degradation with MT for German (-17.22%) but gains for Russian (+12.59%), indicating language-specific dynamics that the current study does not systematically characterize.
- **What evidence would resolve it:** A controlled study across more languages varying translation quality, linguistic distance from English, and model types to identify conditions favoring each approach.

### Open Question 2
- **Question:** Would iterative feedback (multiple triplestore execution cycles) improve SPARQL query quality beyond the single-feedback constraint?
- **Basis in paper:** [inferred] "The feedback is formulated only once per input question, i.e., there are no multiple feedback options intended to avoid infinite loops."
- **Why unresolved:** The single-feedback design is a pragmatic choice to prevent loops, not an empirically determined optimum; performance gains from additional refinement cycles remain unknown.
- **What evidence would resolve it:** Experiments comparing single-feedback vs. multi-iteration feedback with loop-termination criteria (e.g., max iterations, convergence detection) on query accuracy.

### Open Question 3
- **Question:** How does prompt quality in non-native languages (machine-translated vs. human-authored) affect multilingual KGQA performance?
- **Basis in paper:** [inferred] "The prompts in English, German, and Russian were written by native speakers, the other prompts were acquired via machine translation and further structure validation."
- **Why unresolved:** Translation artifacts in prompts may contribute to the observed performance degradation in low-resource languages; isolating prompt quality from model multilingual capability is unaddressed.
- **What evidence would resolve it:** Ablation comparing native-speaker-authored vs. machine-translated prompts across languages while holding other variables constant.

## Limitations

- Performance heavily dependent on LLM quality, with open-source models achieving only 18.42% F1 vs. GPT-4o's 54.83%
- Single-shot feedback constraint may be insufficient for complex query errors requiring multi-step debugging
- Significant performance gap between Latin and Cyrillic script languages suggests multilingual embedding or LLM capability limitations

## Confidence

- **High confidence:** Task decomposition improves SPARQL quality (supported by multiple ablation results), experience pool provides measurable in-context learning benefits, single-shot feedback provides consistent moderate improvements
- **Medium confidence:** State-of-the-art claims hold on QALD-9-plus but may not generalize to other benchmarks or real-world deployments; translation strategy effectiveness varies by language family
- **Low confidence:** Multilingual superiority claims based on single benchmark; framework scalability to different domains remains unproven

## Next Checks

1. **Cross-benchmark validation:** Test mKGQAgent on QALD-10 or GrailQA benchmarks to assess generalizability beyond QALD-9-plus and identify potential overfitting to training distribution
2. **Multi-turn feedback evaluation:** Compare single-shot vs. multi-turn feedback on a subset of difficult queries to quantify tradeoff between error correction depth and computational costs
3. **Domain adaptation study:** Evaluate performance on domain-specific knowledge graphs (e.g., biomedical, financial) with curated training data to assess framework adaptability beyond general-purpose Wikidata queries