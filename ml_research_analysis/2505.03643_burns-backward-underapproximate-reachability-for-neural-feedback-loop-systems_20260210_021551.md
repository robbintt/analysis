---
ver: rpa2
title: 'BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop Systems'
arxiv_id: '2505.03643'
source_url: https://arxiv.org/abs/2505.03643
tags:
- reachable
- backward
- sets
- ball
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an algorithm for computing underapproximate
  backward reachable sets of nonlinear discrete-time neural feedback loops, addressing
  the gap in verification tools for learning-enabled systems. The core method uses
  overapproximation of system dynamics to enable computation of underapproximate backward
  reachable sets through mixed-integer linear programming (MILP) solutions.
---

# BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop Systems

## Quick Facts
- arXiv ID: 2505.03643
- Source URL: https://arxiv.org/abs/2505.03643
- Reference count: 24
- Introduces algorithm for computing underapproximate backward reachable sets of nonlinear discrete-time neural feedback loops

## Executive Summary
This paper introduces BURNS, an algorithm for computing underapproximate backward reachable sets of nonlinear discrete-time neural feedback loops controlled by ReLU neural networks. The method addresses the gap in verification tools for learning-enabled systems by using overapproximation of system dynamics to enable computation of underapproximate backward reachable sets through mixed-integer linear programming (MILP) solutions. The algorithm iteratively computes norm balls underapproximating the backward reachable set at each time step, using rejection sampling and optimization to find valid underapproximations. Theoretical analysis proves the soundness of the approach.

## Method Summary
BURNS uses OVERT-style piecewise-linear overapproximation of nonlinear dynamics f to compute underapproximate backward reachable sets. The method performs rejection sampling from a heuristic domain D to find points in the backward reachable set, then solves MILPs to maximize norm balls centered at these points while ensuring trajectories exit the goal set. The union of these norm balls forms a non-convex underapproximation. The algorithm checks goal-reaching properties by verifying if the start set is contained in the union of backward reachable sets across all timesteps.

## Key Results
- Demonstrated on 2D robot navigation with neural network control
- Computation times of 3.1-13.2 minutes for 7 timesteps
- Underapproximation errors ranging from 59% to 94% coverage depending on sample size
- Method enables checking goal-reaching properties for nonlinear neural feedback systems
- Scalability limited by MILP complexity growth with time horizon

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Overapproximating the dynamics function enables sound computation of underapproximate backward reachable sets.
- Mechanism: The dynamics function f is abstracted to a multi-valued piecewise-linear function f̂ that contains all (x,y) pairs of f and potentially more. When computing backward reachability, this overapproximation causes the backward reachable set under f̂ to be *smaller* than under f, yielding a valid underapproximation.
- Core assumption: The dynamics overapproximation f̂ is constructed with piecewise-linear upper and lower bounds that are tight enough to remain useful.
- Evidence anchors:
  - [abstract] "Our algorithm is based on overapproximating the system dynamics function to enable computation of underapproximate backward reachable sets"
  - [section IV-D] Lemma IV.6 proves R_{f̂_cl}^(-1)(Z) ⊆ R_{f_cl}^(-1)(Z)
  - [corpus] Weak direct evidence; related work (Rober et al. 2023) addresses backward reachability for NFLs but focuses on overapproximation rather than underapproximation.
- Break condition: If the overapproximation f̂ becomes too loose, the underapproximation may shrink to the empty set, providing no verification utility.

### Mechanism 2
- Claim: Norm balls centered at sampled points and maximized to touch the backward reachable set boundary form valid underapproximations.
- Mechanism: For a sampled point x_d in the backward reachable set, the algorithm solves an MILP minimizing radius ε while constraining the trajectory to exit the goal set. The optimal ball is "boundary coincident"—touching but not crossing the true backward reachable set boundary.
- Core assumption: Samples can be drawn from within the true backward reachable set via rejection sampling.
- Evidence anchors:
  - [section IV-C] Definition IV.3 and Lemma IV.5 establish boundary-coincident subset property
  - [section IV-C] Equation (3) shows the optimization: minimize ε subject to ||x - x_d||_p ≤ ε and f_cl^{∘k}(x) ∉ int(G)
  - [corpus] No direct corpus validation; underapproximate backward reachability for nonlinear NFLs is identified as a gap.
- Break condition: If rejection sampling fails to find points in the backward reachable set (e.g., if the heuristic domain D is too narrow), no norm balls can be computed.

### Mechanism 3
- Claim: The union of multiple norm balls provides non-convex coverage of the backward reachable set with tunable approximation error.
- Mechanism: Multiple samples yield multiple boundary-coincident norm balls. Their union forms a non-convex underapproximation. More samples increase coverage at linear computational cost.
- Core assumption: The backward reachable set is sufficiently "fat" that norm balls provide efficient coverage (vs. thin, elongated sets requiring many samples).
- Evidence anchors:
  - [section V, Table I] Shows coverage of 0.64–0.96 depending on nsamp and timestep
  - [section IV-B] "The non-convex union of these norm balls forms an approximation of the backward reachable set"
  - [corpus] Indirect support from Sidrane et al. 2022 (OVERT) on overapproximate forward reachability using similar MILP encoding.
- Break condition: Coverage degrades as timestep increases due to accumulated overapproximation error in f̂.

## Foundational Learning

- Concept: **Mixed-Integer Linear Programming (MILP)**
  - Why needed here: Core computational engine; ReLU networks and piecewise-linear dynamics are encoded as MILP constraints with binary variables for disjunctions.
  - Quick check question: Can you explain why encoding ReLU(t = max(x, 0)) requires a binary variable?

- Concept: **Backward vs. Forward Reachability**
  - Why needed here: The paper specifically addresses backward reachability for goal-reaching properties; understanding the distinction is essential for property selection.
  - Quick check question: If you want to verify "all trajectories from start set X_s avoid obstacle O," should you use forward or backward reachability, and should it be over- or under-approximate?

- Concept: **Piecewise-Linear Overapproximation of Nonlinear Functions**
  - Why needed here: The OVERT-style abstraction converts nonlinear dynamics f(x,u) into bounded piecewise-linear form, enabling MILP encoding.
  - Quick check question: Why does overapproximating f yield an *underapproximation* of the backward reachable set, but an *overapproximation* of the forward reachable set?

## Architecture Onboarding

- Component map:
  - Dynamics abstraction (f → f̂) -> Neural network encoder -> Rejection sampler -> Norm ball optimizer -> Set checker

- Critical path: Sample generation → MILP construction (grows as t × q variables) → Solve → Aggregate balls → Check inclusion. Bottleneck is MILP solve time, which scales with horizon k and network size.

- Design tradeoffs:
  - **nsamp vs. coverage**: More samples improve coverage (~88% at nsamp=15, ~94% at nsamp=25) but increase solve time linearly
  - **Approximation tightness vs. MILP size**: Tighter dynamics bounds yield better underapproximations but increase constraint count
  - **Horizon k vs. tractability**: Final timestep dominates compute (>50% of total time in experiments)

- Failure signatures:
  - Empty underapproximation (f̂ too loose or G poorly chosen)
  - MILP timeout (k too large or network too deep)
  - Rejection sampler starvation (domain D doesn't contain backward reachable set)

- First 3 experiments:
  1. Replicate 2D robot navigation with nsamp=15, k=7; verify coverage matches Table I before modifying.
  2. Vary goal set size/shape to observe sensitivity of underapproximation quality; thin/elongated G should degrade coverage.
  3. Test scalability by increasing network depth (e.g., 3→5 layers) while holding k fixed; measure MILP variable growth and solve time.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a hybrid-symbolic approach be successfully integrated with BURNS to enable verification over longer time horizons?
- Basis in paper: [explicit] The authors state in the Discussion that "Future work to address this limitation is to implement a hybrid-symbolic approach... which allows for analysis over longer time horizons."
- Why unresolved: The current MILP-based method faces scalability issues, with solve times growing significantly as the horizon increases (e.g., the 7th step took >50% of total solve time).
- What evidence would resolve it: A demonstration of the algorithm verifying goal-reaching properties on horizons significantly longer than 7 steps without exponential time growth.

### Open Question 2
- Question: How does the algorithm's performance and underapproximation error scale with higher-dimensional state spaces and more complex dynamics?
- Basis in paper: [explicit] The authors note that "a more complete analysis of the algorithm over a variety of example problems would also provide more insight into its performance."
- Why unresolved: The evaluation is currently restricted to a 2D robot navigation problem, leaving the method's efficacy on complex, high-dimensional robotic systems unproven.
- What evidence would resolve it: Benchmark results on systems with state dimensions $n > 2$ (e.g., quadrotors or autonomous vehicles) showing computation times and coverage fractions.

### Open Question 3
- Question: Can the sampling domain $D$ be determined automatically rather than heuristically to ensure the backward reachable set is fully captured?
- Basis in paper: [inferred] The paper notes that the domain $D$ for rejection sampling is "defined heuristically given simulation traces," implying a lack of formal guarantees on the sampling domain's adequacy.
- Why unresolved: If the heuristic domain $D$ is too small, the algorithm may fail to sample valid points, resulting in an incomplete underapproximation; if too large, sampling efficiency may suffer.
- What evidence would resolve it: An algorithmic procedure for constructing $D$ that provides probabilistic or deterministic guarantees of enclosing the true backward reachable set.

## Limitations
- MILP complexity grows quadratically with time horizon, limiting scalability
- Performance depends on tightness of dynamics overapproximation and geometry of goal set
- Heuristic sampling domain may fail to capture backward reachable set

## Confidence
- **High Confidence**: The theoretical soundness of the underapproximation method (Lemma IV.6, Definition IV.3, Lemma IV.5)
- **Medium Confidence**: Experimental coverage results (0.59-0.96 depending on parameters)
- **Medium Confidence**: MILP encoding correctness and soundness of constraint formulation
- **Low Confidence**: Scalability beyond the demonstrated 7 timesteps and 2D domain

## Next Checks
1. Test the method on goal sets with varying geometries (thin vs. fat) to quantify the impact on coverage quality and sample efficiency.
2. Perform ablation studies on the tightness of the dynamics overapproximation (f̂) to measure the tradeoff between MILP complexity and underapproximation quality.
3. Scale the experiment to longer horizons (k > 7) and higher-dimensional domains to identify the practical limits of the MILP-based approach.