---
ver: rpa2
title: Text Prompt Injection of Vision Language Models
arxiv_id: '2510.09849'
source_url: https://arxiv.org/abs/2510.09849
tags:
- text
- attack
- image
- injection
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates text prompt injection attacks on vision\
  \ language models (VLMs), where adversarial text is embedded in images to mislead\
  \ the model's output. The authors propose an algorithm that identifies high color-consistency\
  \ areas in images and perturbs pixels to create visible text prompts while maintaining\
  \ an l\u221E constraint."
---

# Text Prompt Injection of Vision Language Models

## Quick Facts
- **arXiv ID:** 2510.09849
- **Source URL:** https://arxiv.org/abs/2510.09849
- **Reference count:** 38
- **Primary result:** Novel text prompt injection attack achieving 41.2% untargeted and 37.6% targeted ASR on Llava-Next-72B under tight l∞ constraints

## Executive Summary
This paper investigates text prompt injection attacks on vision language models (VLMs), where adversarial text is embedded in images to mislead the model's output. The authors propose an algorithm that identifies high color-consistency areas in images and perturbs pixels to create visible text prompts while maintaining an l∞ constraint. Experiments on the Llava-Next-72B model using the Oxford-IIIT Pet Dataset show that this approach achieves significantly higher success rates than gradient-based transfer attacks, with untargeted ASR reaching 41.2% and targeted ASR reaching 37.6% under tight constraints (ϵ=8/255). The method is effective for large VLMs, requires fewer computational resources, and remains covert to human observers.

## Method Summary
The proposed attack method works by first identifying regions of high color consistency within images, which serve as suitable areas for embedding adversarial text prompts. The algorithm then perturbs pixel values within these regions to create visible text while maintaining l∞ constraints. This approach differs from traditional gradient-based transfer attacks by focusing on spatial characteristics of the image rather than pure optimization of loss functions. The method leverages the fact that VLMs process both visual and textual information, allowing carefully crafted text embedded in images to influence the model's output predictions.

## Key Results
- Achieved 41.2% untargeted attack success rate on Llava-Next-72B under tight l∞ constraints (ϵ=8/255)
- Reached 37.6% targeted attack success rate with the same constraint settings
- Outperformed gradient-based transfer attacks by a significant margin on the Oxford-IIIT Pet Dataset
- Successfully embedded adversarial text prompts while maintaining human observer imperceptibility

## Why This Works (Mechanism)
The attack exploits the dual-modality nature of VLMs, which process both visual and textual information. By embedding adversarial text directly into images through pixel perturbations, the method creates a direct path for influencing the model's output. The use of high color-consistency regions ensures that the perturbations blend naturally with the surrounding image content, reducing detectability. The l∞ constraint limits the maximum change to any single pixel, making the attack more covert while still achieving meaningful adversarial effects.

## Foundational Learning
- **Vision Language Models (VLMs):** Neural networks that process both visual and textual inputs simultaneously, typically using a combination of vision encoders and language models. Why needed: Understanding the target architecture is crucial for designing effective attacks. Quick check: Verify the VLM uses a vision encoder connected to a language model backbone.
- **Text Prompt Injection:** A type of adversarial attack where malicious text is embedded in inputs to manipulate model behavior. Why needed: This is the core attack vector being explored. Quick check: Confirm the attack injects text rather than just visual perturbations.
- **l∞ Constraint:** A norm constraint limiting the maximum change to any single pixel in an image. Why needed: Ensures perturbations remain small and covert. Quick check: Verify the maximum pixel change does not exceed the specified ϵ value.
- **Color Consistency:** The uniformity of color values in a region of an image. Why needed: Identifies suitable regions for embedding adversarial text. Quick check: Confirm the algorithm identifies regions with minimal color variation.
- **Adversarial Transfer Attacks:** Methods where adversarial examples generated for one model are tested on different models. Why needed: Provides baseline comparison for attack effectiveness. Quick check: Verify transfer attack results are included in comparisons.
- **Oxford-IIIT Pet Dataset:** A dataset containing images of pet breeds used for evaluation. Why needed: Provides standardized test data for measuring attack performance. Quick check: Confirm dataset images are properly preprocessed for VLM input.

## Architecture Onboarding

**Component Map:** Image Input -> Color Consistency Analysis -> Adversarial Text Embedding -> VLM Model -> Output Prediction

**Critical Path:** The most critical path is the embedding of adversarial text into high color-consistency regions, as this directly determines attack success. This involves identifying suitable regions, generating appropriate text perturbations, and ensuring the modifications remain within l∞ constraints while being effective.

**Design Tradeoffs:** The method trades computational complexity for effectiveness, using spatial analysis rather than pure gradient optimization. This reduces computational resources needed but may limit the types of images where attacks can be successfully applied. The l∞ constraint ensures covertness but also limits attack strength.

**Failure Signatures:** Attacks may fail when images lack suitable high color-consistency regions, when the l∞ constraint is too tight to create effective perturbations, or when the VLM's vision encoder is particularly robust to text-like patterns in images.

**3 First Experiments:**
1. Test attack success rate on images with varying degrees of color consistency to identify optimal conditions
2. Evaluate attack effectiveness across different l∞ constraint values to find the tradeoff between stealth and success
3. Compare targeted vs untargeted attack performance to understand the difficulty of controlling model output

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation conducted on only one VLM architecture (Llava-Next-72B), limiting generalizability
- Claims of human imperceptibility lack rigorous validation through user studies or perceptual metrics
- Perturbations remain visible to human observers despite achieving stated l∞ constraints
- Comparison focuses only on transfer-based attacks, omitting other prompt injection techniques
- Does not address potential defenses or attack transferability across different VLM architectures

## Confidence
- Attack effectiveness (ASR results): High confidence - controlled experimental setup with clear metrics
- Method generalizability across VLMs: Medium confidence - single-model evaluation limits conclusions
- Human imperceptibility claims: Low confidence - no formal perceptual validation provided

## Next Checks
1. Test attack transferability across multiple VLM architectures (e.g., LLaVA, Chameleon, BLIP-2) and image domains beyond pets
2. Conduct formal user studies or compute perceptual similarity metrics (SSIM, LPIPS) to validate human observer imperceptibility claims
3. Benchmark computational efficiency against baseline methods using standardized metrics (e.g., attack generation time, number of queries)