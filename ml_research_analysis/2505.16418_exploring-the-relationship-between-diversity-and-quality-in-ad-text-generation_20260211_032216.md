---
ver: rpa2
title: Exploring the Relationship Between Diversity and Quality in Ad Text Generation
arxiv_id: '2505.16418'
source_url: https://arxiv.org/abs/2505.16418
tags:
- diversity
- beam
- search
- text
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the relationship between diversity and
  quality in ad text generation using various diversity-enhancing methods, including
  sampling and beam search. The research reveals a trade-off between diversity and
  ad quality across multiple metrics such as ad performance, consistency, and acceptability.
---

# Exploring the Relationship Between Diversity and Quality in Ad Text Generation

## Quick Facts
- arXiv ID: 2505.16418
- Source URL: https://arxiv.org/abs/2505.16418
- Reference count: 40
- Key outcome: This study investigates the relationship between diversity and quality in ad text generation using various diversity-enhancing methods, including sampling and beam search. The research reveals a trade-off between diversity and ad quality across multiple metrics such as ad performance, consistency, and acceptability. Specifically, diverse beam search improved diversity while maintaining ad performance and acceptability, unlike other methods that exhibited a trade-off. Additionally, generating ad texts using multiple models produced the highest diversity without significantly compromising ad quality. These findings highlight the need for careful use of diversity-enhancing methods in ad text generation and suggest that combining outputs from different models can enhance diversity while maintaining quality.

## Executive Summary
This study systematically investigates the trade-off between diversity and quality in ad text generation using various decoding methods and model ensembles. The research reveals that while sampling methods increase diversity, they come at a significant cost to quality metrics including consistency and acceptability. Diverse beam search emerges as the optimal single-method approach, improving diversity while maintaining ad performance and acceptability better than other methods. The study also demonstrates that combining outputs from multiple different models produces the highest diversity without significant quality degradation, though at increased computational cost.

## Method Summary
The study uses the Japanese CAMERA dataset (798 samples) to generate ad texts with character length constraints (15 full-width/30 half-width characters). The primary model is calm3-22b-chat, supplemented by ELYZA, Mistral, Swallow, and GPT-4o. Five decoding methods are compared: nucleus sampling, beam search, diverse beam search (with beam width 5 and groups 5), and two other diverse beam variants (DMBR, KMBR). The study evaluates diversity using 1-Pairwise-BLEU, ad performance via a proprietary CTR prediction model, consistency via BERTScore, and acceptability via length compliance. Few-shot examples (3-15) are used to provide context for generation.

## Key Results
- Diverse beam search improved diversity while maintaining ad performance and acceptability better than other methods
- Generating ad texts using multiple models produced the highest diversity (0.929) without significantly compromising ad quality (0.975 performance, 0.827 acceptability)
- Increasing few-shot examples improved diversity for beam search methods but not for sampling
- All-at-once output strategy increased diversity but decreased quality compared to one-at-a-time generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Diverse beam search improves surface diversity while maintaining ad performance and acceptability better than standard sampling methods.
- **Mechanism:** By partitioning beams into groups and applying diversity penalties between groups, diverse beam search forces exploration of distinct linguistic paths while still optimizing within each group. This avoids the quality degradation seen in high-temperature sampling, which randomly perturbs all selections.
- **Core assumption:** Ad text quality is preserved when diversity is achieved through structured search rather than stochastic perturbation.
- **Evidence anchors:**
  - [abstract] "diverse beam search improved diversity while maintaining ad performance and acceptability, unlike other methods that exhibited a trade-off"
  - [section 3.1] "Compared with beam search, diverse beam search improved diversity while maintaining advertising performance and acceptability"
  - [corpus] Related work on quality-diversity trade-offs (arXiv:2505.15229, arXiv:2503.10683) confirms trade-offs exist across generation tasks, but does not specifically validate diverse beam search for advertising.
- **Break condition:** If ad length constraints tighten significantly (e.g., below 10 characters), diverse beam search's group partitioning may fail to find valid candidates per group, reducing acceptability.

### Mechanism 2
- **Claim:** Generating ad texts using multiple models produces the highest diversity without significantly compromising ad quality.
- **Mechanism:** Different models learn different token probability distributions and stylistic patterns during pretraining and fine-tuning. Aggregating one output per model naturally increases surface diversity (different phrasings, vocabulary choices) while each model individually maintains reasonable quality.
- **Core assumption:** Model architectural and training differences translate to meaningfully diverse outputs; quality averages out across models.
- **Evidence anchors:**
  - [abstract] "generating ad texts using multiple models produced the highest diversity without significantly compromising ad quality"
  - [section 3.6, Table 5] Multi-model diversity = 0.929 (highest), performance = 0.975 (near single-model average), consistency = 0.808, acceptability = 0.827
  - [corpus] No direct corpus validation for multi-model ensemble diversity in advertising; this appears underexplored in related work.
- **Break condition:** If models are too similar (e.g., same base model with minor fine-tuning), diversity gains diminish. If quality requirements are asymmetric (one poor model dominates), overall quality may degrade.

### Mechanism 3
- **Claim:** Increasing few-shot examples improves diversity for beam search methods but not for sampling.
- **Mechanism:** Beam search deterministically selects high-probability paths; more diverse examples expand the vocabulary with high output probability, giving beam search more valid high-scoring options. Sampling already explores broadly, so additional examples don't expand its effective search space proportionally.
- **Core assumption:** Few-shot examples systematically shift the probability distribution of high-scoring candidates in ways beam search can exploit.
- **Evidence anchors:**
  - [section 3.3, Table 2] Sampling: 3-shot diversity=0.890, 15-shot=0.876 (no improvement); Beam search: 3-shot=0.682, 15-shot=0.747 (improved)
  - [section 3.3] "This result is likely because seeing diverse examples led to changes such that the vocabulary with high output probability becomes more diverse"
  - [corpus] No corpus papers directly address few-shot effects on diversity-quality trade-offs; this mechanism lacks external validation.
- **Break condition:** If few-shot examples are redundant or low-quality, they may not expand the high-probability vocabulary, failing to improve beam search diversity.

## Foundational Learning

- **Concept:** Nucleus Sampling (top-p sampling)
  - **Why needed here:** Understanding how probabilistic truncation affects the diversity-quality frontier; the paper varies p and temperature to show trade-offs.
  - **Quick check question:** If you set p=0.5 vs. p=1.0 with temperature=1.0, which would yield higher diversity and why might quality suffer?

- **Concept:** Beam Search and Diverse Beam Search
  - **Why needed here:** The core decoding algorithms compared; diverse beam search is the paper's standout method for maintaining quality.
  - **Quick check question:** How does partitioning beams into groups with inter-group diversity penalties differ from simply increasing beam width?

- **Concept:** Surface vs. Semantic Diversity
  - **Why needed here:** The paper explicitly restricts evaluation to surface diversity (n-gram-based, 1-Pairwise-BLEU) because ad content must remain fixed.
  - **Quick check question:** Why would semantic diversity be inappropriate for ad text generation even though it matters for story generation?

## Architecture Onboarding

- **Component map:** Ad content (product info, keywords) → 3-15 few-shot examples (input-output pairs) → LLM (one of: calm3, ELYZA, Mistral, Swallow, GPT-4o) → Decoding layer: Sampling (nucleus/temperature), Beam Search, Diverse Beam Search, DMBR, KMBR → 5 ad texts per input → Evaluation: Diversity (1-Pairwise-BLEU), Quality (CTR prediction, BERTScore consistency, length acceptability)

- **Critical path:**
  1. Select decoding method and hyperparameters (beam width, groups, temperature, p)
  2. Configure few-shot examples (3-15) and output strategy (one-at-once vs. all-at-once)
  3. Generate 5 outputs, compute diversity and three quality metrics
  4. If diversity insufficient and quality permits, increase few-shot count (for beam search) or switch to multi-model ensemble

- **Design tradeoffs:**
  - Sampling: Higher diversity, steeper quality trade-off (consistency drops from 0.832→0.740 as diversity increases)
  - Diverse beam search: Moderate-high diversity, best quality preservation; limited by beam width/group configuration
  - Multi-model: Highest diversity (0.929), requires inference across 5 models (cost/speed penalty)
  - All-at-once output: Higher diversity than one-at-a-time but lower quality (trade-off persists)

- **Failure signatures:**
  - Acceptability drops below 0.70: Likely temperature >1.5 or excessive beam width without length filtering
  - Consistency drops below 0.78: High-temperature sampling introducing hallucinations or semantic drift
  - Diversity plateaus despite parameter changes: Few-shot examples may be redundant; try multi-model or all-at-once

- **First 3 experiments:**
  1. Baseline comparison: Run sampling (p=1, T=1), beam search (W=5), and diverse beam search (W=5, G=5) on 100 ad inputs; plot diversity vs. each quality metric to confirm trade-off shape.
  2. Few-shot scaling: Test beam search and sampling with 3, 9, and 15 few-shot examples; verify that beam search diversity improves while sampling does not.
  3. Multi-model ensemble: Generate 1 output each from 5 models (calm3, ELYZA, Mistral, Swallow, GPT-4o) for the same inputs; compare diversity and average quality against single-model 5-output baselines.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the relationship between diversity and quality hold for languages other than Japanese, given the unique linguistic features of Japanese character types?
- **Basis in paper:** [explicit] The authors state in the Limitations section that the analysis is restricted to Japanese ad texts and that "some features such as character types are unique to the Japanese language."
- **Why unresolved:** The study utilized the Japanese CAMERA dataset, and the authors explicitly note they do not intend to apply the results to all languages without further verification.
- **What evidence would resolve it:** Replicating the experimental setup on English or Chinese ad text datasets and comparing the trade-off curves between surface diversity and ad quality.

### Open Question 2
- **Question:** How does semantic diversity ("what to say") impact ad quality compared to the surface diversity ("how to say") evaluated in this study?
- **Basis in paper:** [explicit] The paper states that "we do not consider semantic diversity and evaluate only the surface diversity among the outputs in this study" because product details must remain fixed.
- **Why unresolved:** The study deliberately constrained the scope to surface-level variations (paraphrasing) to prevent factual errors, leaving the trade-offs involved in content variation unexplored.
- **What evidence would resolve it:** A study that allows for controlled semantic variations in ad content and measures the subsequent impact on consistency and performance metrics.

### Open Question 3
- **Question:** Do the trade-offs observed between diversity and quality persist when using real-world click-through rates (CTR) instead of simulated CTR predictions?
- **Basis in paper:** [inferred] The methodology relies on a CTR prediction model ("Kiwami Yosoku TD") to simulate customer behavior rather than measuring actual user interaction, with a human evaluation agreement rate of only 61%.
- **Why unresolved:** Simulated performance and crowd-sourced human evaluations are proxies that may not fully capture real advertising fatigue or user engagement in a live environment.
- **What evidence would resolve it:** Deploying the diverse ad generation strategies in a live A/B testing environment to correlate the automated quality scores with actual user clicks and conversions.

## Limitations
- The proprietary "Kiwami Yosoku TD" CTR prediction model cannot be reproduced without access to CyberAgent's internal infrastructure, limiting validation of ad performance quality metrics
- The study is restricted to Japanese ad texts, and the unique linguistic features of Japanese character types may limit generalizability to other languages
- The specific tokenization approach for BLEU calculation (character-level vs. morphological analysis) is not detailed, which could impact diversity metric comparisons

## Confidence
**High Confidence** (Strong empirical support, multiple validation points):
- Diverse beam search improves diversity while maintaining quality better than standard beam search
- Sampling methods exhibit steeper quality-diversity trade-offs than beam search variants
- Multi-model ensemble produces highest diversity but requires higher computational cost

**Medium Confidence** (Supported by paper data but limited external validation):
- Few-shot examples improve beam search diversity but not sampling diversity
- All-at-once output strategy increases diversity but decreases quality compared to one-at-a-time
- Length acceptability constraints significantly impact method selection

**Low Confidence** (Single paper evidence, no external validation):
- CTR prediction scores are valid quality indicators without the proprietary model
- The specific trade-off curves are generalizable beyond the Japanese ad text domain
- The diversity-quality relationship holds at different character length constraints

## Next Checks
1. **Public CTR Proxy Validation:** Train a regression model on public ad click data (e.g., from Kaggle advertising datasets) to predict CTR from ad text features. Use this to independently verify the ad performance quality metric and check if diverse beam search truly maintains CTR performance.

2. **Cross-Domain Diversity-Quality Trade-off:** Apply the same decoding method comparison to English ad text generation using a public dataset like Taobao ad corpus. Compare if diverse beam search maintains the quality advantage and if the trade-off curves match the Japanese results.

3. **Tokenization Sensitivity Analysis:** Recompute all diversity metrics using both character-level and morphological tokenization for Japanese text. Measure how the relative performance of sampling vs. diverse beam search changes based on tokenization choice.