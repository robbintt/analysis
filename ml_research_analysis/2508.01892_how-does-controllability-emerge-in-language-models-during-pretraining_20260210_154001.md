---
ver: rpa2
title: How Does Controllability Emerge In Language Models During Pretraining?
arxiv_id: '2508.01892'
source_url: https://arxiv.org/abs/2508.01892
tags:
- concept
- training
- intervention
- linear
- steerability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines when and how linear steerability of language
  model concepts emerges during pretraining. The authors introduce the "Intervention
  Detector" (ID), a unified framework that analyzes how concepts become linearly separable
  in hidden states across training checkpoints.
---

# How Does Controllability Emerge In Language Models During Pretraining?

## Quick Facts
- **arXiv ID:** 2508.01892
- **Source URL:** https://arxiv.org/abs/2508.01892
- **Reference count:** 10
- **Primary result:** Shows that linear steerability of concepts in LLMs emerges progressively during pretraining, not uniformly, and correlates with increasing linear separability in hidden states.

## Executive Summary
This paper introduces the "Intervention Detector" (ID) framework to analyze when and how concepts become linearly steerable during LLM pretraining. By tracking concept representations across training checkpoints, the authors demonstrate that steerability develops at different stages for different concepts, correlating with increasing linear separability in hidden space. The method successfully identifies emergence windows and generalizes across model families, providing a cost-effective monitoring tool for applications requiring linear steering.

## Method Summary
The Intervention Detector framework extracts concept representation vectors from hidden state differences between positive/negative stimulus pairs using PCA. ID scores are computed as inner products between these vectors and test-set hidden states across all layers. The method tracks how steerability emerges by analyzing ID score heatmaps, entropy curves, and layer-difference spikes across pretraining checkpoints. Validation involves applying interventions by adding scaled representation vectors to activations and measuring downstream effects.

## Key Results
- Linear steerability does not emerge uniformly but develops at different stages for different concepts (anger emerges earlier than sadness)
- Steerability strongly correlates with increasing linear separability in hidden space as training progresses
- The ID method successfully identifies when models become steerable for specific concepts
- Results generalize across different model families (CrystalCoder and Amber)
- Entropy dynamics and inter-layer ID differences serve as heuristic signals for steerability emergence timing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear steerability emerges when concepts become linearly separable in the model's hidden space, which develops progressively during training rather than uniformly.
- Mechanism: As pretraining advances, the first principal component from PCA increasingly dominates the variance in hidden state differences between positive/negative stimulus pairs. This indicates that concept representations transition from noise-dominated to signal-dominated, enabling linear intervention vectors to reliably capture and manipulate semantic directions. Higher layers develop stronger alignment first, creating a layer-wise gradient of steerability.
- Core assumption: Linear separability in hidden space is a necessary structural precondition for effective linear steering interventions (the paper shows correlation but does not definitively prove causation).
- Evidence anchors:
  - [abstract] "ID reveals that concepts become increasingly linearly separable in the hidden space as training progresses, which strongly correlates with the emergence of linear steerability."
  - [Section 3, "Analyzing Representation Vectors"] "In the early stages of training, the representation vectors derived from linear models such as PCA are dominated by noise, leading to low SNR... As training progresses, the noise decreases and the vectors are better at capturing semantic representations."
  - [corpus] Related work on linear representations (arXiv:2504.12459) suggests pretraining data frequency influences linear structure, supporting the training-dependent emergence pattern.
- Break condition: If a concept never achieves sufficient linear separability in hidden space, linear steering methods will fail regardless of intervention strength. The paper shows "sadness, surprise, and disgust" emerged later and showed weaker control outcomes.

### Mechanism 2
- Claim: Concept representation capability precedes linear steerability—they are distinct abilities that emerge at different training stages.
- Mechanism: A model can encode and express a concept (e.g., respond angrily to prompts) before its internal representations become structured enough for external linear manipulation. Steerability requires not just concept presence but linear geometric organization enabling intervention methods to locate and modify the concept direction.
- Core assumption: The geometric structure of concept representations, not just their existence, determines intervention efficacy.
- Evidence anchors:
  - [Section 1, Introduction] "We find that a model can represent a concept (e.g., anger) well before it can be effectively steered to express it."
  - [Figure 1 description] "The model demonstrates the ability to express anger earlier than it develops linear steerability over it, indicating that expression of anger and linear steerability of anger are distinct abilities."
  - [corpus] No direct corpus evidence for this specific temporal separation claim; related work focuses on post-training intervention.
- Break condition: Early checkpoints where a concept is already expressible via prompting but not yet linearly steerable.

### Mechanism 3
- Claim: Entropy dynamics and inter-layer ID score differences serve as heuristic signals for steerability emergence timing.
- Mechanism: Early training produces high entropy (ID scores diffuse across layers). As training progresses, alignment concentrates in higher layers (lower entropy), then spreads to more layers (entropy rises slightly). Sharp spikes in inter-layer ID differences correlate with checkpoints where interventions become effective.
- Core assumption: These metrics are predictive proxies, not direct measures—they suggest emergence timing but require validation through actual intervention testing.
- Evidence anchors:
  - [Section 3, "Analyzing ID Scores Across Layers"] "During training, ID scores and related metrics—such as inter-layer ID differences, entropy, and abrupt changes in the cosine similarity—can act as early signals that suggest when linear steerability begins to emerge, and are best interpreted as heuristic cues."
  - [Section 4.2, Table 10] Compares checkpoints with highest layer-difference spikes versus where interventions become effective, showing rough alignment.
  - [corpus] Weak external validation; this is a novel heuristic introduced by this work.
- Break condition: Metrics may indicate emergence windows but not guarantee intervention success—actual steering experiments remain necessary for confirmation.

## Foundational Learning

- Concept: **Principal Component Analysis (PCA) for representation extraction**
  - Why needed here: The ID framework uses PCA to extract the first principal component from hidden state differences, representing the direction of maximum variance that aligns with the target concept.
  - Quick check question: Can you explain why the first principal component would capture a concept direction rather than noise, and how you would verify this?

- Concept: **Transformer hidden states and layer-wise representations**
  - Why needed here: The method collects hidden states at the final token position across all layers, assuming later layers encode more abstract/semantic information and the final token summarizes context.
  - Quick check question: Why would the hidden state at position -1 be more informative for concept extraction than earlier token positions?

- Concept: **Activation addition/steering interventions**
  - Why needed here: The intervention step directly adds the extracted representation vector to activations at selected layers, scaling by a coefficient to control effect magnitude.
  - Quick check question: What determines whether adding a vector to activations will reliably steer output versus cause incoherent generation?

## Architecture Onboarding

- Component map:
  Stimulus Constructor -> Hidden State Collector -> Representation Extractor -> ID Score Calculator -> Intervention Engine

- Critical path:
  1. Fine-tune checkpoints with minimal instruction-following data (cold start) to enable meaningful generation
  2. Construct 256+ stimulus pairs per concept, split into train/test
  3. Extract representation vectors using PCA on training pairs
  4. Compute ID scores on test pairs to generate heatmaps across checkpoints × layers
  5. Identify emergence window via entropy curves, layer-difference spikes, and cosine similarity drops
  6. Validate by intervening at candidate checkpoints and measuring downstream effects

- Design tradeoffs:
  - **Scaling factor**: Higher values produce stronger effects but risk incoherence (paper used 40 for Crystal, 3 for Amber—empirically tuned per model)
  - **Layer selection**: More layers = stronger effect but less precision; top 10 layers was paper default
  - **Stimulus granularity**: Sentence-level (this paper) vs. token-level (CAA approach)—both viable, sentence-level requires less alignment effort

- Failure signatures:
  - Low/flat ID scores across all layers: concept not yet linearly separable, training too early
  - High ID scores but intervention fails: representation vector may capture spurious correlation, not true concept
  - Incoherent output after intervention: scaling factor too high or wrong layer selection
  - Base model produces gibberish when intervened: fine-tuning step was skipped

- First 3 experiments:
  1. **Baseline ID score heatmap**: Run ID on a single concept (e.g., "anger") across available checkpoints without intervention to observe score evolution patterns.
  2. **Intervention timing validation**: At checkpoints showing ID score emergence (high scores in upper layers), apply intervention and evaluate output change (ChatGPT scoring for emotions; accuracy for reasoning tasks).
  3. **Cross-concept comparison**: Run ID on multiple concepts simultaneously to verify differential emergence timing (e.g., anger vs. sadness) and confirm that earlier emergence correlates with higher earlier steerability.

## Open Questions the Paper Calls Out

- **Question:** Does the trajectory of linear steerability emergence generalize to model scales significantly larger than 7B parameters?
  - **Basis in paper:** [explicit] Section 6, Limitation (1), states the study was restricted to two 7B-scale models due to computational constraints and did not test generalization to larger LLMs.
  - **Why unresolved:** It remains unknown if the "sharp increase" in steerability observed at specific intermediate stages is a universal scaling property or an artifact of the 7B parameter regime.
  - **What evidence would resolve it:** Applying the Intervention Detector (ID) framework to the pretraining checkpoints of larger models (e.g., 70B+ parameters) to compare emergence timelines.

- **Question:** What underlying mechanisms cause the required intervention coefficient to vary drastically between model families?
  - **Basis in paper:** [explicit] Section 6, Limitation (2), notes that the optimal scaling factor was ~10 for Crystal models but ~3 for Amber, and the authors did not investigate the causes (e.g., internal representation scaling).
  - **Why unresolved:** Without understanding the relationship between representation magnitude and intervention strength, the method relies on empirical tuning rather than theoretical prediction.
  - **What evidence would resolve it:** A comparative analysis of activation norms and vector magnitudes across different architectures to derive a theoretical basis for coefficient selection.

- **Question:** How does the composition of the pre-training corpus influence the specific checkpoint at which a concept becomes steerable?
  - **Basis in paper:** [explicit] Section 2 mentions "examining how different pre-training corpora influence intervention outcomes" as a future direction.
  - **Why unresolved:** The current study observes emergence at specific steps (e.g., 68% for anger), but cannot disentangle whether this is driven by data distribution shifts or model capacity saturation.
  - **What evidence would resolve it:** Training identical model architectures on distinct, controlled datasets and using ID to measure the shift in steerability emergence points.

## Limitations
- The ID framework shows correlation but not definitive causation between linear separability and steerability emergence
- Cold-start fine-tuning requirement adds computational overhead and potential domain-specific biases
- Reliance on ChatGPT-based scoring for emotion validation introduces potential subjectivity
- Analysis based on limited concept sets (7 emotions, 3 reasoning tasks) may not generalize to full semantic diversity

## Confidence
**High Confidence:** The core finding that steerability emerges progressively during training rather than uniformly, with different concepts developing at different stages. The ID framework methodology for tracking linear separability is technically sound and produces consistent results across checkpoints. The cold-start fine-tuning requirement is well-demonstrated and critical for reproducibility.

**Medium Confidence:** The claim that concept representation capability precedes linear steerability, while supported by evidence, relies heavily on the specific emotion and reasoning task selection. The entropy and inter-layer difference metrics as predictive signals are promising but require more extensive validation across diverse concepts.

**Low Confidence:** The generalizability of the specific emergence timing patterns (e.g., anger before sadness) to other concept categories and model architectures beyond CrystalCoder and Amber. The exact relationship between first principal component dominance and intervention success is empirically observed but not theoretically proven.

## Next Checks
1. **Causation vs. Correlation Test:** Design an experiment where you artificially manipulate the linear separability of concept representations (e.g., through targeted weight updates) without changing the underlying concept encoding, then measure whether steerability emerges independently of actual concept learning.

2. **Cross-Domain Generalization:** Apply the ID framework to track steerability emergence for entirely different concept categories (e.g., syntactic structures, factual knowledge, or world knowledge) to determine whether the progressive emergence pattern holds across semantic domains.

3. **Alternative Intervention Validation:** Replace the ChatGPT scoring with human evaluation for emotion concepts and implement a blinded intervention study where evaluators cannot distinguish between baseline and steered outputs to test the robustness of the steerability measurements.