---
ver: rpa2
title: Rethinking the Understanding Ability across LLMs through Mutual Information
arxiv_id: '2505.23790'
source_url: https://arxiv.org/abs/2505.23790
tags:
- information
- mutual
- arxiv
- understanding
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating the intrinsic
  linguistic understanding capabilities of large language models (LLMs) in an architecture-agnostic
  manner, moving beyond traditional task-specific evaluations that have limitations
  like knowledge conflicts, vulnerability, and misrepresentation. The authors propose
  an information-theoretic framework based on mutual information (MI) to measure how
  effectively input information is preserved in model representations.
---

# Rethinking the Understanding Ability across LLMs through Mutual Information

## Quick Facts
- **arXiv ID**: 2505.23790
- **Source URL**: https://arxiv.org/abs/2505.23790
- **Reference count**: 40
- **Primary result**: An information-theoretic framework based on mutual information (MI) to measure LLM understanding, showing encoder-only models maintain higher information fidelity than decoder-only models

## Executive Summary
This paper addresses the challenge of evaluating the intrinsic linguistic understanding capabilities of large language models (LLMs) in an architecture-agnostic manner, moving beyond traditional task-specific evaluations. The authors propose an information-theoretic framework based on mutual information (MI) to measure how effectively input information is preserved in model representations. They formalize language understanding as MI between input sentences and their latent representations, decomposing sentence-level MI into token-level components between individual tokens and sentence embeddings. Through extensive experiments comparing various LLM architectures, they find that encoder-only models consistently maintain higher information fidelity than decoder-only models, which exhibit a distinctive late-layer "forgetting" pattern. When they fine-tune models to maximize token-level recoverability, understanding ability consistently improves across multiple downstream tasks without task-specific supervision.

## Method Summary
The paper introduces a framework that operationalizes language understanding as mutual information (MI) between input sentences and their latent representations. The method involves training a linear decoder to map sentence embeddings back to token probabilities, using token-level recoverability as a computable proxy for MI. The framework decomposes sentence-level MI into token-level components and applies Fano's inequality to derive a lower bound. The enhancement procedure freezes the final layer and optimizes all preceding layers using the recoverability loss. Experiments compare various LLM architectures (encoder-only vs decoder-only) on multiple datasets, evaluating recoverability through metrics like cosine similarity, BLEU, and ROUGE, and downstream task performance across classification, semantic similarity, retrieval, and clustering tasks.

## Key Results
- Encoder-only models consistently maintain higher information fidelity than decoder-only models
- Decoder-only models exhibit a distinctive late-layer "forgetting" pattern where MI is first enhanced then discarded
- Fine-tuning to maximize token-level recoverability improves understanding ability across multiple downstream tasks without task-specific supervision
- LLaMA2-7B showed improvements in semantic textual similarity (Pearson correlation from 0.8571 to 0.8590) and clustering tasks (NMI from 0.1064 to 0.1560)
- Mistral-7B demonstrated dramatic improvements in clustering performance (ARI increased by nearly 7×)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-level recoverability serves as a computable proxy for mutual information between input sentences and their latent representations.
- Mechanism: The framework decomposes sentence-level MI into token-level components, then applies Fano's inequality to derive a lower bound. When a model can accurately reconstruct original tokens from sentence embeddings, it indicates the representation preserves substantial information about the input. A linear decoder f(E) = W·E + b is trained to map embeddings back to token probabilities, with recoverability accuracy directly bounding average token-level MI.
- Core assumption: Tokens are approximately uniformly distributed over vocabulary V (H(ti) ≈ log|V|), and |V| ≫ 1 for the inequality simplification to hold.
- Evidence anchors:
  - [abstract] "we theoretically derive a computable lower bound for token-level MI using Fano's inequality, which directly relates to token-level recoverability"
  - [section 3.2, Theorem 2] "higher token-level recover accuracy corresponds to higher average mutual information between individual tokens and the sentence embedding"
  - [corpus] "Training LLMs Beyond Next Token Prediction" (arXiv:2511.00198) independently explores MI gaps in LLM training, suggesting convergent interest in information-theoretic approaches
- Break condition: If tokens are highly non-uniformly distributed (e.g., domain-specific vocabulary with power-law frequencies), the binary entropy approximation degrades and the bound becomes loose.

### Mechanism 2
- Claim: Encoder-only architectures preserve more input information in their embeddings than decoder-only models, which exhibit a distinctive late-layer "forgetting" pattern.
- Mechanism: Encoder models (RoBERTa, DistilBERT) use bidirectional attention, allowing simultaneous left-right context integration. Decoder models (LLaMA, Mistral) show an "inverted U-shape" where MI peaks at intermediate layers then declines toward output. The paper hypothesizes this reflects decoder models prioritizing next-token prediction over representation fidelity—token-specific information is first enhanced for context modeling, then selectively discarded as layers approach the generation objective.
- Core assumption: The architectural difference (bidirectional vs. causal attention) is the primary driver of the observed MI patterns, not training data or scale differences.
- Evidence anchors:
  - [abstract] "encoder-only models consistently maintain higher information fidelity than their decoder-only counterparts, with the latter exhibiting a distinctive late-layer 'forgetting' pattern"
  - [section 5.2, Figure 2] OPT-2.7b shows inverted U-shape; RoBERTa-large shows gradual decline from first to last layer
  - [corpus] Weak direct corroboration—neighbor papers focus on token-level evaluation but don't compare encoder/decoder MI patterns. This is a gap in external validation.
- Break condition: If decoder models are probed at different extraction points (e.g., attention heads rather than layer outputs), the forgetting pattern may not appear or may invert.

### Mechanism 3
- Claim: Fine-tuning models to maximize token-level recoverability (without task-specific supervision) improves performance across diverse downstream tasks.
- Mechanism: The enhancement procedure freezes the final layer and optimizes all preceding layers using the recoverability loss (Binary Cross-Entropy). By improving how well embeddings encode token information, the model develops more informative representations that transfer to classification, retrieval, STS, and clustering. The improvement is architecture-agnostic—OPT, LLaMA2, and Mistral all show gains.
- Core assumption: Higher MI in representations causally improves downstream task performance; the relationship is not merely correlational or task-artifact driven.
- Evidence anchors:
  - [abstract] "fine-tuning to maximize token-level recoverability consistently improves understanding ability of LLMs on tasks without task-specific supervision"
  - [section 5.4, Table 3] Mistral-7B clustering ARI increased ~7×; LLaMA2-7B NMI improved from 0.1064 to 0.1560
  - [corpus] "SemPA: Improving Sentence Embeddings" (arXiv:2601.05075) independently shows semantic preference alignment improves embeddings, providing convergent evidence that representation-level optimization transfers
- Break condition: If downstream tasks require information that is intentionally compressed out by the recoverability objective (e.g., abstract semantic relations that don't help token reconstruction), performance could degrade on those specific tasks.

## Foundational Learning

- Concept: Mutual Information I(X;Y) = H(X) - H(X|Y)
  - Why needed here: The entire framework defines "understanding" as MI between input S and embedding E. Without grasping MI as "reduction in uncertainty about X given Y," the decomposition and Fano bound won't make sense.
  - Quick check question: If I(S;E) = 0, what does that imply about the relationship between the sentence and its embedding?

- Concept: Fano's inequality (H(X|Y) ≤ Hb(Pe) + Pe·log(|X|-1))
  - Why needed here: This is the theoretical bridge from recoverability error probability to mutual information bounds. The paper's Theorem 2 is a direct application.
  - Quick check question: If you can recover tokens with 90% accuracy from embeddings, does Fano guarantee high or low conditional entropy H(ti|E)?

- Concept: Encoder-only vs. Decoder-only attention patterns
  - Why needed here: The paper's central architectural comparison hinges on this. Bidirectional (encoder) vs. causal (decoder) attention shapes what information can flow where.
  - Quick check question: In a decoder-only model, can token position 5 attend to token position 10? What about in an encoder-only model?

## Architecture Onboarding

- Component map: Tokenize sentence S → (t1, t2, ..., tn) → LLM Backbone → extract embedding E from layer L-1 → Linear Decoder f(E) = W·E + b → Token probabilities → Recoverability Loss (BCE)

- Critical path:
  1. Extract embeddings at consistent layer positions (normalized depth 0.0 to 1.0) for fair comparison
  2. Train decoder on held-out corpus (dbpedia used in paper) until convergence
  3. Evaluate recoverability on out-of-domain texts (AG_News, IMDB, QNLI, ChatGPT responses) to test generalization
  4. For enhancement: use WikiText corpus, optimize θ1:L-1 while freezing θL

- Design tradeoffs:
  - **Linear vs. non-linear decoder**: Paper uses linear for efficiency; notes non-linear could work but adds complexity
  - **Which layer to probe**: Paper finds layer choice affects results dramatically (encoder: early layers better; decoder: middle layers peak)
  - **Training data for decoder**: Must be separate from evaluation data to avoid overfitting claims

- Failure signatures:
  - Decoder achieves near-perfect training accuracy but near-zero test accuracy → overfitting, need more diverse training data
  - Enhancement improves recoverability but degrades downstream task performance → MI objective may be misaligned with task requirements
  - Large performance variance across text lengths → embedding capacity insufficient for long sequences (see Section 5.3)

- First 3 experiments:
  1. **Reproduce layer-wise recoverability curves**: Run decoder training on RoBERTa-large and OPT-2.7b at each layer; plot cosine similarity, BLEU-4, ROUGE-L vs. normalized depth. Expect: gradual decline for encoder, inverted U for decoder.
  2. **Ablate decoder complexity**: Compare linear decoder vs. 2-layer MLP on token recovery. If gains are marginal, confirms paper's efficiency choice.
  3. **Enhancement transfer test**: Fine-tune LLaMA2-7B on recoverability, then evaluate on a held-out task not in the paper (e.g., QA or summarization). Check if gains transfer or are task-specific.

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions in the provided text. The discussion focuses on presenting the framework and results rather than identifying future research directions.

## Limitations

- Architectural scope: The study focuses on English-centric models and datasets, which may not translate directly to multilingual or multimodal contexts
- Generalization concerns: The recoverability framework may conflate memorization with comprehension, as high MI could result from surface-level pattern matching
- Scale dependency: The relative performance of encoder-only vs decoder-only models may vary across different model scales, which wasn't extensively investigated

## Confidence

- **High Confidence**: Encoder-only models consistently show higher token-level recoverability than decoder-only models across multiple architectures and datasets; the MI enhancement procedure reliably improves downstream task performance
- **Medium Confidence**: The causal relationship between token-level recoverability and linguistic understanding; the claim that decoder models exhibit systematic "forgetting" patterns
- **Low Confidence**: Claims about MI enhancement being architecture-agnostic; generalizability of findings to non-English languages or specialized domains

## Next Checks

1. **Cross-linguistic validation**: Apply the MI framework to multilingual models (e.g., mBERT, XLM-R) and evaluate whether the encoder/decoder performance patterns hold across languages with different morphological complexity and word order patterns.

2. **Long-sequence behavior analysis**: Systematically evaluate recoverability and downstream performance as a function of input length, particularly focusing on the observation that longer sequences may exceed embedding capacity. Test with sequences of varying lengths (50, 200, 500, 1000 tokens) to identify threshold effects.

3. **Alternative MI objectives**: Replace token-level recoverability with other MI estimation methods (e.g., MINE, contrastive learning objectives) to determine whether the observed architectural differences are specific to the Fano-inequality-based approach or represent a more general phenomenon in LLM representations.