---
ver: rpa2
title: 'FreqKV: Key-Value Compression in Frequency Domain for Context Window Extension'
arxiv_id: '2505.00570'
source_url: https://arxiv.org/abs/2505.00570
tags:
- freqkv
- context
- compression
- cache
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes FreqKV, a parameter-free method for extending
  context windows in large language models (LLMs) by compressing key-value (KV) cache
  in the frequency domain. The key insight is that KV states exhibit energy concentration
  in low-frequency components, allowing high-frequency components to be discarded
  with minimal information loss.
---

# FreqKV: Key-Value Compression in Frequency Domain for Context Window Extension

## Quick Facts
- arXiv ID: 2505.00570
- Source URL: https://arxiv.org/abs/2505.00570
- Authors: Jushi Kai; Yixuan Wang; Boyi Zeng; Haoli Bai; Bo Jiang; Ziwei He; Zhouhan Lin
- Reference count: 40
- Extends context windows up to 256K tokens while maintaining stable perplexity (~7.73 on PG-19)

## Executive Summary
FreqKV introduces a parameter-free method for extending context windows in large language models by compressing key-value (KV) cache in the frequency domain. The approach leverages the observation that KV states exhibit energy concentration in low-frequency components, allowing high-frequency components to be discarded with minimal information loss. This enables significant context extension without architectural modifications or additional training parameters.

The method iteratively compresses KV cache using Discrete Cosine Transform (DCT), retaining low-frequency components while progressively compressing earlier tokens as context grows. Experiments on LLaMA-2 and LLaMA-3 demonstrate that FreqKV can extend context windows up to 256K tokens while maintaining stable perplexity and outperforming existing KV compression methods on long-context benchmarks including LongBench, RULER, and Needle-in-a-Haystack.

## Method Summary
FreqKV compresses key-value cache states by applying Discrete Cosine Transform (DCT) to convert KV embeddings into frequency domain, then selectively retaining low-frequency components while discarding high-frequency ones. The method operates iteratively, compressing earlier tokens progressively as context grows. It uses a uniform retaining ratio (γ) across all layers and heads, with a 2D DCT applied to flatten sequence dimensions. The approach requires no architectural modifications or additional parameters, making it compatible with existing models. During inference, compressed KV states are decompressed using inverse DCT before attention computation.

## Key Results
- Extends context windows up to 256K tokens with stable perplexity around 7.73 on PG-19 dataset
- Outperforms existing KV compression methods on long-context benchmarks including LongBench, RULER, and Needle-in-a-Haystack
- Achieves compression overhead under 0.3% that does not grow with context length
- Matches or surpasses full fine-tuning performance with minimal training at 8K length

## Why This Works (Mechanism)
The core insight is that KV states exhibit strong energy concentration in low-frequency components due to the redundancy in natural language embeddings. By transforming KV states to frequency domain using DCT, the method can identify and preserve the most information-rich components (low frequencies) while discarding less critical high-frequency components. This frequency-based compression exploits the inherent structure of language representations, where most semantic information is captured in smooth, low-frequency variations rather than high-frequency details.

## Foundational Learning
- Discrete Cosine Transform (DCT): A frequency-domain transformation that converts spatial/temporal data into frequency components. Why needed: Enables identification and separation of information-rich low-frequency components from less critical high-frequency ones. Quick check: Verify that DCT can be efficiently computed and inverted for KV cache compression/decompression.
- Attention mechanism with KV cache: The process where query vectors attend to key-value pairs stored in cache to avoid recomputation. Why needed: Understanding how KV compression affects attention computation is crucial for maintaining model performance. Quick check: Confirm that compressed KV states still enable meaningful attention patterns after decompression.
- RoPE positional encoding: Rotary Position Embedding used in LLaMA models for encoding position information. Why needed: The frequency-based approach may interact with RoPE's sinusoidal patterns, affecting compression effectiveness. Quick check: Analyze how RoPE interacts with frequency-domain compression across different sequence positions.

## Architecture Onboarding

Component map:
- Input tokens -> Embedding layer -> Attention layer (with KV cache) -> Output layer
- KV cache storage -> DCT compression -> Compressed storage -> Inverse DCT -> Attention computation

Critical path:
1. Token embedding generation
2. Attention computation with KV cache
3. Frequency-domain compression of KV states
4. Inverse transformation for attention use
5. Output generation and loss computation

Design tradeoffs:
- Compression ratio vs. information retention: Higher compression reduces memory but may lose critical information
- Window size for DCT: Larger windows provide better frequency resolution but increase computational overhead
- Retaining ratio (γ) selection: Balancing between compression efficiency and maintaining model performance

Failure signatures:
- Sharp degradation in perplexity when γ is too low
- Inconsistent performance across different layers or heads
- Context-dependent performance drops for specific token patterns
- Loss of fine-grained semantic distinctions in compressed representations

Three first experiments:
1. Vary γ from 0.1 to 0.9 and measure perplexity on PG-19 to find optimal compression ratio
2. Apply different DCT window sizes (8, 16, 32) to assess impact on compression quality and computational overhead
3. Compare performance across different layers to identify if certain layers are more sensitive to frequency-domain compression

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can head-wise or layer-wise adaptive compression strategies improve performance by leveraging distinct frequency profiles?
- Basis in paper: [explicit] Appendix B notes that while power spectrums vary across heads, "it could be promising to study specific differences and associations in different heads or other modules."
- Why unresolved: FreqKV currently applies a uniform retaining ratio (γ) across all layers and heads, potentially wasting capacity on redundant heads or over-compressing informative ones.
- What evidence would resolve it: Experiments applying different γ values per head/layer and measuring the resulting perplexity and downstream task accuracy compared to the uniform baseline.

### Open Question 2
- Question: How can frequency-domain compression be adapted to preserve high-frequency details necessary for symbolic reasoning?
- Basis in paper: [explicit] Appendix E observes that performance on Proof-pile trails LongLoRA because "minor variations (e.g., a single symbol or index)... are more difficult to preserve under compression."
- Why unresolved: The method relies on discarding high-frequency components, which ostensibly carry noise for text but appear to contain critical local information for mathematical derivations.
- What evidence would resolve it: A modified FreqKV that selectively retains higher frequencies for math-specific tokens or layers, showing improved accuracy on symbolic benchmarks like Proof-pile.

### Open Question 3
- Question: Does the relationship between frequency components and information density shift in multimodal contexts or non-RoPE architectures?
- Basis in paper: [inferred] The analysis (Figure 1) relies on text embeddings in LLaMA (RoPE-based), but it is unclear if the "strong energy concentration in low-frequency components" holds for image tokens or other positional encodings.
- Why unresolved: The paper validates the approach exclusively on text-based decoder-only LLMs, leaving the spectral characteristics of multimodal KV caches unexplored.
- What evidence would resolve it: Power spectrum analysis of KV states in multimodal models (e.g., LLaVA) to determine if low-frequency retention remains sufficient for visual understanding tasks.

## Limitations
- Evaluation primarily focused on LLaMA-2 and LLaMA-3 models, raising questions about generalizability to other architectures
- Limited scope of evaluation datasets, with heavy reliance on PG-19 for perplexity measurement
- Uncertainty about performance in multilingual contexts and non-English text scenarios
- Unclear practical implications for real-world deployment with varying hardware constraints

## Confidence
- High confidence: The fundamental approach of frequency-domain compression for KV cache, experimental methodology, and baseline comparisons
- Medium confidence: Generalization across different model architectures and task types
- Medium confidence: Claims about compression overhead and computational efficiency

## Next Checks
1. Test FreqKV's effectiveness on multilingual models and non-English text to verify the universality of the frequency-domain energy concentration pattern
2. Evaluate the method's performance across diverse model families beyond LLaMA (e.g., OPT, Mistral) to assess architectural generalizability
3. Conduct ablation studies varying the DCT window size and compression ratio to determine optimal parameters for different context lengths and task types