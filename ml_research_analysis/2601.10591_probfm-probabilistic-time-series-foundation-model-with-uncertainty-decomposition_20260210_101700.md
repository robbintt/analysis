---
ver: rpa2
title: 'ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition'
arxiv_id: '2601.10591'
source_url: https://arxiv.org/abs/2601.10591
tags:
- uncertainty
- evidential
- loss
- regression
- probfm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProbFM introduces the first application of Deep Evidential Regression
  (DER) to time series foundation models, addressing the fundamental limitation of
  uncertainty quantification in financial forecasting. Unlike existing approaches
  that impose restrictive distributional assumptions or conflate uncertainty sources,
  ProbFM provides principled epistemic-aleatoric decomposition through Normal-Inverse-Gamma
  priors while maintaining single-pass computational efficiency.
---

# ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition

## Quick Facts
- arXiv ID: 2601.10591
- Source URL: https://arxiv.org/abs/2601.10591
- Reference count: 8
- First application of Deep Evidential Regression (DER) to time series foundation models with epistemic-aleatoric uncertainty decomposition

## Executive Summary
ProbFM introduces a probabilistic time series foundation model that addresses the fundamental limitation of uncertainty quantification in financial forecasting. The key innovation is applying Deep Evidential Regression with Normal-Inverse-Gamma priors to achieve principled decomposition of epistemic (model) and aleatoric (data) uncertainty in a single forward pass. Unlike existing approaches that impose restrictive distributional assumptions or conflate uncertainty sources, ProbFM provides explicit uncertainty decomposition while maintaining computational efficiency. The controlled evaluation using a consistent LSTM architecture across five probabilistic methods demonstrates that DER achieves competitive point prediction accuracy while delivering superior risk-adjusted trading performance through uncertainty-aware strategies.

## Method Summary
ProbFM uses a 1-layer LSTM with 32 hidden units and 0.1 dropout, processing 1-day log returns from cryptocurrency data. The DER head outputs four Normal-Inverse-Gamma parameters (μ, λ, α, β) with bounded predictions (μ = 3.0·tanh(output)) and positivity constraints via Softplus. The model is trained with Adam (lr=0.001, batch=128, grad clip 1.0, weight decay 0.001) using a combined loss of L_EDL = L_NLL + 0.1·L_reg, where L_reg penalizes high evidence when predictions are inaccurate. The training includes evidence annealing over 10-20% of training and early stopping after 10 epochs without improvement. Data consists of 11 cryptocurrencies from Stooq (Jan 2020–Oct 2025) with 50-step lookback windows, standardized per symbol using training statistics only.

## Key Results
- DER achieves competitive point prediction accuracy (comparable RMSE and MAE to baselines across five probabilistic methods)
- Uncertainty-aware filtering improves Sharpe ratios by selectively avoiding high-uncertainty predictions
- Maintains strong forecasting accuracy while providing explicit uncertainty decomposition for effective risk management
- Demonstrates superior risk-adjusted trading performance with uncertainty-aware strategies

## Why This Works (Mechanism)

### Mechanism 1: Epistemic-Aleatoric Uncertainty Decomposition via NIG Priors
If a model learns to output Normal-Inverse-Gamma (NIG) distribution parameters rather than point predictions, then epistemic and aleatoric uncertainty can be explicitly separated in a single forward pass. The DER head projects transformer representations to four parameters (μ, λ, α, β) that characterize a distribution over Gaussian parameters. From these, aleatoric uncertainty = β/(α−1) (irreducible noise), while epistemic uncertainty = β/((α−1)λ) (model uncertainty reducible with more data). This decomposition derives from the NIG posterior structure. Core assumption: The NIG prior form is appropriate for the underlying data-generating process; financial returns approximately follow a conditional Gaussian structure.

### Mechanism 2: Evidential Regularization Prevents Overconfidence
If regularization penalizes high evidence (low uncertainty) when predictions are inaccurate, then the model learns calibrated uncertainty estimates rather than overconfident predictions. The loss L_EDL = L_NLL + λ_evd·L_reg where L_reg = |ŷ−y|·(α+ν−2). When prediction error is large, the penalty increases with evidence (α, ν), forcing the model to output higher uncertainty. Evidence annealing (evidence_scale(t) = min(1.0, t/T_anneal)) gradually enables evidence accumulation during training. Core assumption: The annealing schedule T_anneal is set appropriately for the dataset size and model capacity; too fast leads to collapse, too slow wastes capacity.

### Mechanism 3: Coverage Loss Integration Ensures Interval Reliability
If coverage probability is directly optimized alongside evidential loss, then prediction intervals achieve target coverage (e.g., 95%) without post-hoc calibration. L_coverage = |PICP_target − PICP_actual| computed over training batches. This complements L_EDL by directly penalizing under/over-coverage. The combined objective L_ProbFM balances forecasting accuracy, uncertainty calibration, and generalization. Core assumption: The batch-level PICP estimate is representative of population coverage; sufficient samples per batch for stable estimation.

## Foundational Learning

- **Concept: Epistemic vs. Aleatoric Uncertainty**
  - Why needed here: The entire contribution hinges on correctly distinguishing reducible (epistemic) from irreducible (aleatoric) uncertainty for risk management.
  - Quick check question: If you double your training data, which uncertainty component should decrease?

- **Concept: Normal-Inverse-Gamma Distribution**
  - Why needed here: This conjugate prior enables closed-form posterior inference and explicit uncertainty formulas.
  - Quick check question: What constraints must λ, α, β satisfy for a valid NIG distribution?

- **Concept: Evidential Deep Learning**
  - Why needed here: DER extends neural networks to learn distributions over distribution parameters, not just point estimates.
  - Quick check question: How does DER differ from standard heteroscedastic regression (predicting mean and variance)?

## Architecture Onboarding

- **Component map:**
  Input: Patch-based processing with frequency-adaptive patch sizes (8-128) → Encoder: Patch embedding + sinusoidal positional encoding → Standard transformer with multi-head attention and SwiGLU FFN → DER Head: Linear projections to (μ, λ, α, β) with Softplus activations and positivity constraints → Loss: L_EDL + λ_coverage·L_coverage + L2 regularization → Output: Point prediction (μ), aleatoric uncertainty, epistemic uncertainty, confidence intervals

- **Critical path:**
  1. Correct NIG parameter constraints (λ>0, α>1, β>0) in DER head—violations cause NaN losses
  2. Evidence annealing schedule timing—starts at 0, ramps to 1 over 10-20% of training
  3. Gradient clipping (max_norm 1.0-5.0) for NIG parameter stability

- **Design tradeoffs:**
  - Sharpness vs. calibration: Lower sharpness (narrower intervals) with low PICP indicates overconfidence
  - Coverage weight λ_coverage: Higher values improve interval reliability but may reduce point forecast accuracy
  - Bounded predictions (μ = 3.0·tanh(output)) stabilize training but may clip extreme returns

- **Failure signatures:**
  - PICP << target (e.g., 0.46 vs. 0.95 target on BTC): Model is overconfident; increase λ_coverage or extend evidence annealing
  - NaN losses during training: NIG constraint violations; check Softplus epsilon values
  - Epistemic ≈ 0 always: Evidence collapse; reduce regularization or slow annealing

- **First 3 experiments:**
  1. Reproduce controlled LSTM comparison on single asset (BTC) with all five probabilistic methods to verify DER achieves comparable RMSE/MAE
  2. Ablate coverage loss (λ_coverage = 0 vs. 0.1) and measure PICP change; expect ~30-40% coverage improvement
  3. Test uncertainty-aware trading filter: execute trades only when total uncertainty < 75th percentile; compare Sharpe ratio to unfiltered baseline

## Open Questions the Paper Calls Out

- **Generalization to non-crypto domains:** Does the superior risk-adjusted trading performance of Deep Evidential Regression (DER) generalize to non-cryptocurrency financial markets (e.g., equities, FX) and non-financial domains (e.g., energy, traffic)? The paper explicitly states the need to extend evaluation beyond cryptocurrency markets to validate generalizability across diversified domains with different noise characteristics.

- **Multi-horizon and multivariate extensions:** Can the framework be effectively extended to multi-horizon and multivariate forecasting using Normal-Inverse-Wishart (NIW) priors without sacrificing the single-pass inference efficiency? The paper proposes using NIW priors for multi-horizon extensions but only implements and validates the univariate, single-step Normal-Inverse-Gamma (NIG) version.

- **Component-level ablation studies:** To what extent do the proposed heuristics—coverage loss integration, evidence annealing, and bounded predictions—drive the performance improvements versus the base DER formulation? The paper explicitly calls for "Component-Level Ablation Studies" to isolate the effects of each component.

- **Calibration vs. sharpness trade-off:** Can the low Prediction Interval Coverage Probability (PICP) observed in DER models (e.g., 0.46 vs. the 0.95 target on BTC) be remedied without degrading the "sharpness" that drives superior trading utility? The paper reveals that while DER achieves the best trading metrics, its PICP is significantly lower than the target and lower than Gaussian NLL.

## Limitations

- NIG prior distributional assumptions may not hold for financial returns that exhibit heavy tails and volatility clustering
- Evidence regularization effectiveness is sensitive to annealing schedule and regularization weight (exact values not specified)
- Coverage loss weight λ_coverage mentioned but specific value not reported, limiting reproducibility
- Uncertainty decomposition validity across different market regimes or asset classes remains untested

## Confidence

**High Confidence:**
- DER achieves competitive point prediction accuracy (RMSE/MAE comparable to baselines)
- Uncertainty-aware filtering improves Sharpe ratios in the tested trading strategy
- The single-pass computational efficiency of DER compared to ensemble methods

**Medium Confidence:**
- The epistemic-aleatoric decomposition genuinely separates reducible from irreducible uncertainty sources in financial forecasting
- The NIG prior is appropriate for cryptocurrency return distributions
- Coverage loss integration ensures target PICP without post-hoc calibration

**Low Confidence:**
- The specific annealing schedule and coverage loss weight values are optimal for the dataset
- The decomposition remains valid across different market regimes or asset classes
- The reported uncertainty estimates are fully calibrated across all tested conditions

## Next Checks

1. **Distributional Assumption Validation:** Test DER on synthetic datasets with known epistemic/aleatoric uncertainty characteristics (e.g., heteroscedastic Gaussian processes with varying noise levels). Verify that DER recovers the ground truth uncertainty decomposition across different signal-to-noise ratios.

2. **Cross-Asset Robustness:** Apply ProbFM to non-cryptocurrency financial time series (e.g., equities, commodities) and compare epistemic/aleatoric decomposition patterns. This tests whether the NIG prior assumption holds across asset classes with different distributional properties.

3. **Regime Sensitivity Analysis:** During periods of market stress (high volatility, crashes), measure how the epistemic/aleatoric decomposition changes and whether uncertainty-aware strategies maintain Sharpe ratio improvements. This validates whether uncertainty estimates remain meaningful during non-stationary conditions.