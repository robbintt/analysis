---
ver: rpa2
title: Large language models accurately predict public perceptions of support for
  climate action worldwide
arxiv_id: '2601.20141'
source_url: https://arxiv.org/abs/2601.20141
tags:
- climate
- willingness
- llms
- across
- countries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models can predict public misperceptions about\
  \ climate action with high accuracy (Claude MAE \u2248 5 p.p., r = .77), comparable\
  \ to traditional statistical models, by inferring second-order beliefs from first-order\
  \ willingness data. LLMs capture the core psychological mechanism\u2014social projection\
  \ with systematic underestimation\u2014rather than memorizing country-specific patterns."
---

# Large language models accurately predict public perceptions of support for climate action worldwide

## Quick Facts
- arXiv ID: 2601.20141
- Source URL: https://arxiv.org/abs/2601.20141
- Reference count: 0
- Large language models can predict public misperceptions about climate action with high accuracy (Claude MAE ≈ 5 p.p., r = .77)

## Executive Summary
Large language models can accurately predict public misperceptions about climate action support worldwide, achieving performance comparable to traditional statistical models. The models capture the key psychological process of social projection with systematic underestimation rather than simply retrieving memorized values. Performance is highest in digitally connected, wealthier countries and improves significantly when actual willingness data is provided. This approach offers a rapid, scalable tool for identifying perception gaps where survey resources are scarce.

## Method Summary
The study predicts second-order beliefs (perceptions of others' willingness to contribute to climate action) across 125 countries using large language models. Country-level features include demographics, GDP, climate data, and first-order willingness data from Gallup World Poll. Four LLMs (Claude 3.5 Haiku, GPT-4o mini, Gemini 2.5 Flash, Llama 4 Maverick) were queried via API with temperature=0 and staged prompts ranging from country name only to full specification. Predictions were evaluated against ground-truth second-order beliefs using MAE, RMSE, and correlation, with OLS/Lasso regressions as benchmarks.

## Key Results
- LLMs achieve MAE ≈ 5 p.p. and r ≈ .77 when provided first-order willingness data, comparable to statistical benchmarks
- Prediction accuracy improves with feature richness, with first-order willingness being the most important predictor
- Models use compositional reasoning over retrieval, confirmed by counterfactual name-swapping tests
- Performance declines in less digitally connected, lower-GDP countries (R² 0.08-0.24 with connectivity metrics)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs infer second-order beliefs by applying social projection with systematic downward bias to first-order willingness data.
- **Mechanism:** Models apply a consistent discounting heuristic that mirrors human psychology—people project their own views but systematically underestimate others' support. The model treats first-order willingness as a high-weight input and applies a compositional transformation.
- **Core assumption:** The model has internalized the social projection pattern from training corpora containing psychological and social science literature.
- **Evidence anchors:** Ablation tests show first-order willingness removal causes largest degradation (~1.5-2.2 p.p. increase); counterfactual tests confirm compositional reasoning over retrieval.

### Mechanism 2
- **Claim:** Prediction accuracy depends on the richness of a country's information environment in model training data.
- **Mechanism:** Countries with higher internet penetration and GDP produce more digital content that enters training corpora, creating differential "visibility" in the model's latent representations.
- **Core assumption:** The training data contains sufficient text from digitally connected countries to support country-specific reasoning.
- **Evidence anchors:** LLM predictions become more accurate with both higher internet penetration and GDP per capita (all p < .001; R² ranges from 0.08 to 0.24).

### Mechanism 3
- **Claim:** Models use compositional reasoning over input features rather than retrieving country-specific memorized values.
- **Mechanism:** When country names are swapped with feature vectors held constant, predictions follow the features rather than the country label.
- **Core assumption:** The system prompt successfully suppresses retrieval behavior, and temperature=0 decoding ensures deterministic processing.
- **Evidence anchors:** Swapping country names and GDP values produces negligible effects (all |ΔMAE| < 1.00 p.p.; all p > .05), indicating that models integrate structural features rather than retrieve country-specific patterns.

## Foundational Learning

- **Concept: Second-order beliefs (Theory of Mind)**
  - **Why needed here:** The task requires modeling "what people think others think," not just "what people think." This is recursive mental state attribution.
  - **Quick check question:** Can you distinguish between "I support climate action" (first-order) and "I think 60% of others support climate action" (second-order)?

- **Concept: Pluralistic ignorance**
  - **Why needed here:** The outcome being predicted is the systematic gap between actual and perceived support—understanding that this gap is both widespread and patterned is essential.
  - **Quick check question:** If 70% support a policy but people believe only 40% do, what is the perception gap, and what intervention might close it?

- **Concept: Feature ablation and counterfactual testing**
  - **Why needed here:** To distinguish inference from memorization, you must systematically remove or swap inputs and observe whether predictions change appropriately.
  - **Quick check question:** If removing a feature causes large accuracy drops but swapping country names causes none, what does that imply about the model's reasoning strategy?

## Architecture Onboarding

- **Component map:** Country features -> Prompt engineering -> LLM API (Claude/GPT-4o mini/Gemini/Llama) -> Parsed numeric output -> MAE/RMSE/correlation evaluation -> Ablation/counterfactual diagnostics
- **Critical path:**
  1. Format country data into standardized prompt template
  2. Query models with system instructions forbidding memorization
  3. Parse numeric outputs, bound to [0, 100]
  4. Compute MAE against ground-truth second-order beliefs
  5. Run ablation (remove features one at a time) and counterfactuals (swap values)
  6. Compare to OLS/Lasso baselines trained on same features
- **Design tradeoffs:**
  - **Prompt format:** Natural language vs. structured bullets—natural language yields lower MAE but may be more sensitive to phrasing
  - **Model selection:** Claude offers best accuracy but is proprietary; Llama is open but slightly less accurate; Gemini underperforms and applies content filters
  - **First-order data provision:** Providing actual willingness improves accuracy (~5 p.p. MAE) but requires survey data; country-name-only still achieves ~7-8 p.p. for top models
- **Failure signatures:**
  - **High MAE with low variance:** Model may be outputting a constant or near-constant prediction (check output distribution)
  - **Accuracy collapse on ablation:** Model over-relies on a single feature; check feature weights in OLS baseline for comparison
  - **Name-swap sensitivity:** Indicates memorization rather than inference; verify system prompt is applied
  - **Content filter blocks:** Gemini returns no output on policy items; switch models or rephrase prompts
- **First 3 experiments:**
  1. **Baseline replication:** Run stage 8 (full information) prompts on all 125 countries, compute MAE and correlation; verify Claude achieves ~5 p.p. MAE, r ≈ .77
  2. **Feature ablation sweep:** Remove each feature block individually, record MAE change; confirm first-order willingness removal causes largest degradation (~1.5-2.2 p.p. increase)
  3. **Counterfactual robustness:** Swap country names between high-GDP and low-GDP pairs while keeping features constant; verify predictions change minimally (<1 p.p.), confirming compositional reasoning

## Open Questions the Paper Calls Out

- **Question:** Can LLM-generated perception gap estimates improve the targeting, framing, or timing of misperception-correction interventions compared to expert judgment or simple heuristics?
  - **Basis in paper:** Future Directions states: "future work could test whether LLM-generated estimates improve intervention targeting, message framing, or timing relative to expert judgment or simple heuristics, particularly in resource-constrained settings"
  - **Why unresolved:** The paper demonstrates LLMs can identify perception gaps but does not test whether these estimates translate into more effective real-world interventions.
  - **What evidence would resolve it:** Randomized field experiments comparing intervention outcomes (e.g., attitude change, behavioral engagement) when targeting is guided by LLM estimates vs. expert judgment vs. basic heuristics.

- **Question:** Do LLM prediction patterns hold at subnational scales—regions, cities, or demographic groups—where intervention-relevant variation may be masked by national aggregates?
  - **Basis in paper:** Limitations section: "our country-level analysis does not clarify whether these relationships hold at smaller scales—such as regions, cities, or demographic groups. National estimates may hence mask intervention-relevant variation within larger countries."
  - **Why unresolved:** All analyses were conducted at the country level; subnational validation was outside the study's scope.
  - **What evidence would resolve it:** Replication of the LLM prediction framework using regional or city-level survey data (e.g., U.S. state-level climate beliefs) to assess whether accuracy and mechanism patterns generalize.

- **Question:** Can fine-tuning with local survey data or integrating digital trace signals (e.g., social media sentiment) improve LLM accuracy in low-visibility, digitally disconnected settings?
  - **Basis in paper:** Future Directions: "Fine-tuning with local survey data, incorporating lightweight digital signals (e.g., social media sentiment, Google Trends), or integrating LLM inference with existing survey infrastructures could enhance performance in low-visibility settings"
  - **Why unresolved:** The paper documents declining accuracy in less connected countries but does not test remediation strategies.
  - **What evidence would resolve it:** Experiments comparing baseline LLM predictions against fine-tuned models or models augmented with local digital signals in low-GDP/low-internet-penetration countries.

## Limitations

- **Geographic bias:** Performance systematically declines in less digitally connected, lower-GDP countries where misperceptions may be largest and intervention most needed.
- **Subnational resolution:** Country-level analysis may mask intervention-relevant variation within larger countries at regional, city, or demographic group scales.
- **Mechanism verification:** The inference-over-retrieval claim relies on counterfactual tests; without model weight inspection, absolute confidence in the mechanism cannot be verified.

## Confidence

**High Confidence:**
- LLMs achieve MAE ≈ 5 p.p. and r ≈ .77 when provided first-order willingness data, comparable to statistical benchmarks.
- Performance improves predictably with feature richness (stage progression from country name only to full specification).
- Models use structured reasoning over simple retrieval, confirmed by counterfactual tests.

**Medium Confidence:**
- The mechanism of "social projection with systematic underestimation" is correctly captured by LLMs. This is supported by ablation results and psychological plausibility, but the exact internal reasoning process cannot be directly observed.
- Geographic performance heterogeneity (better in digitally connected countries) is correctly identified. The attribution to training data representation is plausible but not definitively proven.

**Low Confidence:**
- Claims about model-specific differences (Claude vs. Llama vs. Gemini) are based on single API snapshots. Model updates or different prompting could alter rankings.
- The assertion that models do not exaggerate first-order willingness importance is based on observed feature weights, but the psychological realism of the inferred discounting factor is not independently validated.

## Next Checks

1. **Model weights inspection:** If access to Claude/Llama weights becomes available, verify that country-specific survey data is not directly embedded in weights or fine-tuning data. This would definitively confirm inference over memorization.

2. **Cross-temporal validation:** Replicate the study with a new survey dataset collected after the model training cutoff (post-August 2024). If LLM accuracy remains high on new data, it strongly supports the inference mechanism claim.

3. **Connectivity intervention experiment:** Select 3-5 low-connectivity countries, generate synthetic text corpora (news, reports, social media) about these countries, and fine-tune an open model. Test whether prediction accuracy improves, confirming the data-visibility hypothesis.