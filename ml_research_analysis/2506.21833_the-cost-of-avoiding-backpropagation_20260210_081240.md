---
ver: rpa2
title: The Cost of Avoiding Backpropagation
arxiv_id: '2506.21833'
source_url: https://arxiv.org/abs/2506.21833
tags:
- fmad
- wwwt
- gradient
- convergence
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides a unified theoretical and empirical comparison
  of backpropagation, forward-mode automatic differentiation (FmAD), and zero-order
  optimization (ZO) for gradient computation. It addresses the lack of comprehensive
  evaluation of these methods, particularly against memory-efficient backpropagation
  variants like activation checkpointing, and their convergence behavior in high-dimensional
  settings.
---

# The Cost of Avoiding Backpropagation

## Quick Facts
- arXiv ID: 2506.21833
- Source URL: https://arxiv.org/abs/2506.21833
- Reference count: 40
- Primary result: BP with checkpointing outperforms FMAD and ZO by up to 31.1% higher accuracy, 34.8% faster convergence, and 3.8× fewer computations at comparable memory usage

## Executive Summary
This work provides a unified theoretical and empirical comparison of backpropagation, forward-mode automatic differentiation, and zero-order optimization for gradient computation in memory-constrained training of large language and vision-language models. The study reveals that while forward-mode and zero-order methods can reduce memory usage, they incur significantly higher computational costs, slower convergence rates, and greater sensitivity to dimensionality compared to backpropagation with checkpointing. Empirical results on models up to 13B parameters demonstrate that BP with checkpointing consistently achieves superior performance across accuracy, convergence speed, and total computation while maintaining comparable memory usage.

## Method Summary
The study compares seven method variants across backpropagation, forward-mode AD (FMAD), and zero-order optimization (ZO) families on 7 datasets using QLoRA finetuning with 4-bit quantization and rank=1. Experiments run on Nvidia L40, A100, and 2080ti GPUs with 3 random seeds. Methods include BP-vanilla, BP-checkpointing, ZO-vanilla with parallel/sequential perturbations, FMAD-vanilla with parallel/sequential perturbations, and variance-reduced variants (ACCUMULATE, MULTIPLE, SVRG, ADAPTIVE, SPARSE). Metrics include test accuracy, wallclock convergence time, peak memory consumption, and computation cost (TFLOPs).

## Key Results
- BP with checkpointing achieves 31.1% higher accuracy, 34.8% faster convergence, and 3.8× fewer computations than FMAD and ZO variants at comparable memory usage
- FMAD and ZO require increasingly conservative learning rates as model dimensionality increases, fundamentally limiting convergence speed
- AdamW optimizer causes Jacobian-vector product spikes in FMAD/ZO training, leading to instability; SGD remains stable
- Variance reduction techniques improve accuracy but increase computation or slow convergence

## Why This Works (Mechanism)

### Mechanism 1: Backpropagation Checkpointing Memory-Compute Tradeoff
BP with activation checkpointing achieves comparable memory to FMAD/ZO while maintaining exact gradient computation and optimal convergence. Standard BP stores all intermediate activations (O(cD) memory) for the backward pass. Checkpointing strategically discards activations and recomputes them during backpropagation, reducing memory to O(c√D) with only a log D recomputation overhead. This preserves exact gradients—critical for convergence—while trading compute for memory savings. FMAD and ZO avoid storing activations entirely but pay a steeper price: noisy gradient estimates that require smaller learning rates.

### Mechanism 2: FMAD/ZO Learning Rate Constraint from Dimensionality
FMAD and ZO require increasingly conservative learning rates as model dimensionality d increases or perturbation count n decreases, fundamentally limiting convergence speed. Both methods estimate gradients via random perturbations v~N(0,I_d). The variance of the gradient estimator scales as (d+1)/n, forcing a maximum learning rate η < 2/[L(1+(d+1)/n)] for convergence stability. In high dimensions with limited perturbations, this constraint becomes severe—learning rates must shrink, slowing convergence proportionally. BP has no such constraint (η ≤ 1/L suffices).

### Mechanism 3: Noise Propagation in Perturbation-Based Gradient Estimation
FMAD and ZO inherently inject noise through random perturbations that propagates forward through the network, contaminating gradient signals and destabilizing optimization—especially with adaptive optimizers. BP propagates loss gradients backward (loss-to-weights), providing semantically meaningful update directions. FMAD/ZO propagate weight perturbations forward (weights-to-loss), then multiply by perturbation direction to estimate gradients. The initial randomness δw ~ N(0,I) carries through all layers, with variance accumulating. This noise is inherent to the algorithm and causes Jacobian-vector product spikes with AdamW.

## Foundational Learning
- Concept: Reverse-mode vs Forward-mode Automatic Differentiation - Understanding why BP computes exact gradients efficiently while FMAD requires perturbation-based approximations is central to the paper's comparison. Quick check: Can you explain why BP's vector-Jacobian product (backward pass) is more efficient than FMAD's Jacobian-vector product for scalar losses?
- Concept: Zero-Order Optimization and Finite Differences - ZO methods estimate gradients using only function evaluations; understanding their noise characteristics explains their convergence limitations. Quick check: How does the central finite-difference gradient estimator work, and why does its variance scale with dimensionality d?
- Concept: Activation Checkpointing and Rematerialization - The paper's main claim is that checkpointed BP is the optimal memory-constrained strategy; understanding this technique is essential. Quick check: What is the memory-compute tradeoff in checkpointing, and why does it preserve BP's convergence properties?

## Architecture Onboarding

### Component map
- BP-Vanilla: Forward pass stores all activations → backward pass computes exact gradients. Memory: O(cD), Compute: O(d)
- BP-Checkpointing: Forward pass stores subset → backward pass recomputes discarded activations. Memory: O(c√D), Compute: O(d log D)
- FMAD-Vanilla: Forward pass computes jvp alongside activations using random perturbation v. Memory: O(ch), Compute: O(nd) for n perturbations
- ZO-Vanilla: Two forward passes per perturbation (f(w+εv), f(w-εv)), gradient estimated via finite differences. Memory: O(ch), Compute: O(nd)

### Critical path
1. For memory-constrained training, implement BP with checkpointing first (HuggingFace `gradient_checkpointing_enable()` or PyTorch `torch.utils.checkpoint`)
2. If BP-checkpointing still exceeds memory, consider FMAD/ZO only for small trainable parameter counts (e.g., LoRA rank=1)
3. If using FMAD/ZO, prefer non-adaptive optimizers (SGD with momentum) over AdamW to avoid jvp/gradient spikes
4. Variance reduction via -MULTIPLE or -ACCUMULATE helps accuracy but increases compute or slows convergence

### Design tradeoffs
- BP-checkpointing vs FMAD/ZO: BP-checkpointing wins on accuracy, convergence speed, and total compute at similar memory; FMAD/ZO only viable for very tight memory with small d
- Parallel vs Sequential perturbations (FMAD/ZO): Parallel evaluates n perturbations simultaneously (memory O(nch)) but faster wall-clock; sequential reduces memory to O(ch) but slower
- Optimizer choice: AdamW with FMAD/ZO risks jvp spikes and divergence; SGD is more stable but may require careful tuning

### Failure signatures
- BP-Vanilla OOM: Activation storage exceeds GPU memory → switch to checkpointing
- FMAD NaN jvp values: AdamW amplifies gradient noise → switch to SGD or reduce perturbation variance
- ZO early accuracy collapse: Gradient estimates too noisy → increase perturbation count n or use accumulation
- SVRG degradation on generation tasks: Full-gradient snapshots may have outlier magnitudes → reduce snapshot frequency or avoid SVRG for sequence tasks
- FMAD/ZO accuracy gap widening with model size: Dimensional scaling constraint activates → reduce trainable parameters (lower LoRA rank) or accept BP-checkpointing's memory cost

### First 3 experiments
1. **Baseline comparison**: Train a small model (e.g., BERT-base on AGNews) with BP-Vanilla, BP-Checkpointing, FMAD-Vanilla, ZO-Vanilla. Measure accuracy, convergence time, peak memory, and total TFLOPs. Expect BP-checkpointing to match BP-vanilla accuracy with lower memory; FMAD/ZO to lag in accuracy and convergence.
2. **Dimensionality scaling test**: Train OPT-1.3B, 6.7B, 13B with LoRA rank=1 using BP-Checkpointing, FMAD-Vanilla, ZO-Vanilla. Plot accuracy vs model size. Expect FMAD/ZO accuracy to degrade as d increases (per Theorem 3.2/3.3).
3. **Optimizer stability check**: Train with FMAD-Vanilla using AdamW vs SGD (momentum=0.9). Log jvp values and effective gradient magnitudes per step. Expect AdamW to show spikes; SGD to remain bounded (per Figure 8).

## Open Questions the Paper Calls Out

### Open Question 1
Do the performance gaps between backpropagation (BP) and alternative methods persist during full-parameter finetuning, as opposed to the parameter-efficient (LoRA) training evaluated in this study? The authors state in Appendix D that extending the comparison to "full model finetuning" is a necessary future direction to allow for a "complete characterization" of the trade-offs. Increasing d to the full parameter count could theoretically exacerbate the scalability bottlenecks of FMAD and ZO regarding convergence speed and memory.

### Open Question 2
How does network architecture shape (wide/shallow vs. deep) impact the relative efficiency of BP, FMAD, and ZO? The authors note in Appendix D that they "did not systematically evaluate" BP with checkpointing on "wider but shallower models," noting that checkpointing may offer less benefit in such architectures. It is unclear if FMAD or ZO could offer a competitive advantage in shallower networks where the recomputation overhead of checkpointing is higher relative to the forward pass cost.

### Open Question 3
Can a specialized optimizer stabilize FMAD and ZO training to match the efficiency of AdamW used with BP? Appendix F.5 identifies a failure mode where AdamW causes Jacobian-vector products (JVP) to spike in magnitude during FMAD/ZO training, leading to divergence, whereas SGD remains stable. Standard adaptive optimizers assume gradient properties inherent to BP, suggesting a need for optimization algorithms tailored to perturbation-based estimates.

## Limitations
- Theoretical analysis assumes L-smooth loss landscapes and isotropic Gaussian perturbations, which may not hold for all deep learning tasks
- Empirical results based on finetuning experiments rather than full pretraining, limiting generalizability to other training regimes
- Comparison focuses on memory-constrained scenarios with QLoRA-style parameter-efficient tuning; results may differ for full-model training
- Some results depend on framework-specific checkpointing implementations that weren't directly controlled

## Confidence
- High confidence in BP with checkpointing's superiority over FMAD/ZO for accuracy and convergence (supported by extensive empirical results across 7 datasets and multiple model scales)
- Medium confidence in the theoretical learning rate constraints for FMAD/ZO (the proofs are sound but rely on idealized assumptions about perturbation distributions and smoothness)
- Low confidence in the practical viability of ZO/FMAD for high-dimensional models beyond the studied parameter-efficient tuning setup

## Next Checks
1. Test BP-checkpointing vs FMAD/ZO on full-model training tasks to verify if the memory-compute tradeoffs remain consistent
2. Evaluate the impact of structured perturbations (non-isotropic) on FMAD/ZO convergence to test the assumptions underlying the dimensionality constraints
3. Reproduce the optimizer stability findings with other adaptive optimizers (e.g., Adagrad, RMSprop) to determine if AdamW's jvp spikes are specific or representative