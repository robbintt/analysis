---
ver: rpa2
title: 'Bi-MCQ: Reformulating Vision-Language Alignment for Negation Understanding'
arxiv_id: '2601.22696'
source_url: https://arxiv.org/abs/2601.22696
tags:
- image
- alignment
- negation
- text
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of negation understanding in
  medical vision-language models, where contrastive learning objectives struggle to
  distinguish disease presence from absence. The authors propose Bi-MCQ, a fine-tuning
  method that reformulates image-text alignment as a conditional semantic comparison
  problem through bi-directional multiple-choice learning.
---

# Bi-MCQ: Reformulating Vision-Language Alignment for Negation Understanding

## Quick Facts
- **arXiv ID**: 2601.22696
- **Source URL**: https://arxiv.org/abs/2601.22696
- **Reference count**: 26
- **Primary result**: Bi-MCQ improves negation understanding in medical vision-language models by reformulating contrastive learning as conditional semantic comparison through bi-directional MCQ tasks

## Executive Summary
This paper addresses the fundamental challenge of negation understanding in medical vision-language models, where standard contrastive learning objectives fail to distinguish between disease presence and absence. The authors propose Bi-MCQ, a fine-tuning method that reformulates image-text alignment as a conditional semantic comparison problem using bi-directional multiple-choice learning. By introducing affirmative, negative, and mixed prompts, and employing direction-specific Cross-Attention fusion modules, Bi-MCQ enables explicit discrimination between disease presence and absence. The method achieves substantial improvements in negation understanding, reducing the gap between affirmative and negative classification performance while maintaining strong positive-negative discrimination.

## Method Summary
Bi-MCQ reformulates the vision-language alignment problem from contrastive learning to conditional semantic comparison through bi-directional multiple-choice question answering. The method jointly trains two MCQ tasks: Image-to-Text (I2T) and Text-to-Image (T2I). For each direction, three types of prompts are used: affirmative (containing disease presence), negative (containing disease absence), and mixed (combining both). The model learns to compare candidate options conditioned on the query modality using direction-specific Cross-Attention fusion modules. During training, the model selects the correct answer from multiple candidates, with loss functions designed to optimize conditional semantic matching rather than simple alignment. This reformulation explicitly trains the model to discriminate between disease presence and absence, addressing the core limitation of contrastive learning approaches.

## Key Results
- Achieves up to 0.47 AUC improvement over CARZero zero-shot performance on negation understanding tasks
- Demonstrates up to 0.08 absolute gain on combined positive-negative evaluation metrics
- Reduces the affirmative-negative AUC gap by an average of 0.12 compared to InfoNCE-based fine-tuning approaches
- Shows consistent improvements across four chest X-ray datasets with statistical significance

## Why This Works (Mechanism)
The core insight is that contrastive learning objectives (like InfoNCE) optimize for overall alignment between images and text but fail to capture the fine-grained semantic distinctions required for negation understanding. By reformulating the task as conditional semantic comparison through MCQ learning, Bi-MCQ forces the model to explicitly discriminate between semantically similar but logically distinct statements (e.g., "patient has pneumonia" vs "patient does not have pneumonia"). The bi-directional nature ensures that both visual and textual modalities are equally trained in negation understanding, while the Cross-Attention fusion modules enable effective cross-modal reasoning for conditional comparisons.

## Foundational Learning
**Conditional Semantic Comparison** - Understanding that the model must learn to compare options conditioned on the query rather than just aligning pairs. This is needed because negation understanding requires distinguishing between semantically similar but logically opposite statements. Quick check: Verify that the model can correctly rank "has disease" higher than "does not have disease" when given a positive example.

**Bi-directional Training** - Training both Image-to-Text and Text-to-Image directions ensures balanced learning across modalities. This is needed because negation understanding may manifest differently in visual versus textual representations. Quick check: Compare performance degradation when training in only one direction.

**Multiple Choice Framework** - Reformulating alignment as MCQ enables explicit discrimination between options. This is needed because standard contrastive loss treats all negative samples equally without requiring fine-grained discrimination. Quick check: Test whether adding more distractors improves discrimination ability.

## Architecture Onboarding

**Component Map**: Input Images/Texts -> Vision-Language Encoder -> Cross-Attention Fusion (I2T) -> MCQ Classifier (I2T) AND Input Texts/Images -> Vision-Language Encoder -> Cross-Attention Fusion (T2I) -> MCQ Classifier (T2I)

**Critical Path**: The critical path involves the Cross-Attention fusion modules that enable conditional semantic comparison. These modules take the encoded query (image or text) and candidate options, compute attention scores conditioned on the query, and produce comparison-aware representations for the MCQ classifier.

**Design Tradeoffs**: The bi-directional approach doubles computational requirements but ensures balanced negation understanding across modalities. The MCQ reformulation increases training complexity but enables explicit discrimination. The Cross-Attention fusion adds parameters but provides crucial conditional reasoning capability.

**Failure Signatures**: If Cross-Attention modules fail, the model reverts to treating all negatives equally, showing minimal improvement in negation understanding. If bi-directional training is ineffective, one direction (typically I2T) may show strong negation understanding while the other remains poor. If MCQ reformulation fails, the model may still show contrastive learning behavior with similar affirmative-negative gaps.

**First Experiments**: 1) Test zero-shot CARZero performance on negation tasks to establish baseline gaps. 2) Implement single-direction MCQ training to isolate bi-directional benefits. 3) Compare with InfoNCE fine-tuning on the same datasets to quantify reformulation benefits.

## Open Questions the Paper Calls Out
None

## Limitations
- The method is evaluated exclusively on chest X-ray datasets, limiting claims about generalization to other medical imaging modalities or non-medical domains
- Baseline comparisons focus on CARZero and InfoNCE-based approaches, without including more recent negation-aware VLM methods that may provide stronger competition
- The contribution of Cross-Attention fusion modules to overall performance improvements lacks comprehensive ablation study validation

## Confidence
**High Confidence**: The core methodology of reformulating contrastive learning as conditional semantic comparison through bi-directional MCQ tasks is well-defined and reproducible. The experimental results showing improved AUC scores and reduced affirmative-negative gaps are statistically significant and well-documented.

**Medium Confidence**: The generalizability claims beyond chest X-ray datasets and the superiority claims over negation-aware baselines require additional validation. The specific contribution of Cross-Attention fusion modules to overall performance improvements is moderately supported but could benefit from more extensive ablation studies.

**Low Confidence**: Claims about Bi-MCQ being the optimal approach for negation understanding in VLMs are premature without broader domain testing and comparison with emerging negation-aware methodologies published after the primary baseline selection.

## Next Checks
1. Apply Bi-MCQ to non-chest X-ray medical imaging datasets (CT scans, pathology slides) and non-medical visual datasets to evaluate cross-domain negation understanding capabilities and quantify performance degradation or improvement.

2. Implement and compare against recent negation-aware VLM approaches (NegVQA, TNG-CLIP, What "Not" to Detect) on the same evaluation benchmarks to establish relative performance positioning in the current research landscape.

3. Conduct controlled experiments removing the direction-specific Cross-Attention fusion modules to quantify their individual contribution to the 0.12 average reduction in affirmative-negative AUC gap, and test alternative attention mechanisms for comparative analysis.