---
ver: rpa2
title: 'The Patient is not a Moving Document: A World Model Training Paradigm for
  Longitudinal EHR'
arxiv_id: '2601.22128'
source_url: https://arxiv.org/abs/2601.22128
tags:
- clinical
- patient
- jepa
- training
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SMB-Structure, a training paradigm for clinical
  foundation models that explicitly models patient trajectories as dynamical systems
  rather than documents to be summarized. The key innovation is combining supervised
  fine-tuning (SFT) with a Joint-Embedding Predictive Architecture (JEPA), where JEPA
  forces the model to predict future patient states in latent space before observing
  them, compelling the encoder to capture disease dynamics.
---

# The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR

## Quick Facts
- arXiv ID: 2601.22128
- Source URL: https://arxiv.org/abs/2601.22128
- Reference count: 13
- Key outcome: SMB-Structure achieves competitive trajectory-level reasoning on longitudinal EHR by combining SFT with JEPA, excelling at long-horizon predictions where autoregressive models struggle

## Executive Summary
This paper introduces SMB-Structure, a training paradigm for clinical foundation models that explicitly models patient trajectories as dynamical systems rather than documents to be summarized. The key innovation is combining supervised fine-tuning (SFT) with a Joint-Embedding Predictive Architecture (JEPA), where JEPA forces the model to predict future patient states in latent space before observing them, compelling the encoder to capture disease dynamics. The method is validated on two large-scale cohorts: 23,319 oncology patients from MSK (323,000+ patient-years) and 19,402 pulmonary embolism patients from INSPECT. Using a linear probe evaluated at multiple decision nodes along disease trajectories, SMB-Structure demonstrates competitive performance on tasks requiring trajectory-level reasoning, particularly excelling at long-horizon predictions where traditional autoregressive models struggle.

## Method Summary
SMB-Structure combines SFT (next-token prediction) with JEPA (latent-space trajectory prediction) to learn patient embeddings for time-to-event prediction on longitudinal EHR. The method uses LLaMA-3.1 8B or Qwen3 8B backbone with LoRA adapters, serializing structured EHR with domain-specific delimiter tokens. Training follows a curriculum strategy: first SFT-only to establish semantic grounding, then JEPA to encode trajectory dynamics. The JEPA objective masks 50% of future tokens and requires the encoder to predict their embeddings via a predictor network, forcing trajectory dynamics into the representation. Evaluation uses frozen embeddings with linear probes (Cox PH for survival) on 68 tasks across two large cohorts.

## Key Results
- SMB-Structure achieves 0.719 AUC-ROC on MSK disease progression vs. 0.727 for SFT baseline, with superior long-horizon predictions
- Curriculum training (SFT→JEPA) outperforms joint optimization, confirming objective interference requires sequential training
- Cross-disease transfer effect: adding INSPECT (PE trajectories) improves MSK (cancer) performance for JEPA models, suggesting trajectory regularization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Predicting future states in latent space (before observing tokens) forces trajectory dynamics into the encoder representation.
- **Mechanism:** The JEPA objective masks future tokens and requires the encoder to predict their embeddings via a predictor network. Unlike autoregressive decoding, which can defer trajectory reasoning until generation time, the encoder must encode "where the patient is going" into the initial representation to minimize prediction loss.
- **Core assumption:** Disease dynamics are compressible into a latent representation that can be linearly probed for downstream tasks.
- **Evidence anchors:** [abstract] "JEPA forces the encoder to predict future patient states in latent space before observing them, encoding trajectory dynamics into the representation"
- **Break condition:** If masking ratio is too high (≥0.75) or predictor capacity is insufficient, trajectory becomes structurally unpredictable; if too low (<0.25), leakage from local correlations makes the task trivial.

### Mechanism 2
- **Claim:** SFT provides semantic grounding that prevents JEPA from collapsing to degenerate, semantically meaningless representations.
- **Mechanism:** The SFT objective (next-token prediction) ensures the encoder learns clinically grounded token semantics. Without this, JEPA could learn representations that are predictable in latent space but decoupled from clinical vocabulary.
- **Core assumption:** Clinical semantics and trajectory dynamics can be jointly encoded without catastrophic interference.
- **Evidence anchors:** [Section 3.3.1] "This ensures the encoder learns clinically meaningful representations and prevents the JEPA objective from driving toward degenerate solutions"
- **Break condition:** Joint optimization from scratch causes gradient conflict; curriculum training (SFT first, then JEPA) resolves this.

### Mechanism 3
- **Claim:** Momentum encoder with slow EMA updates provides stable targets that prevent representational collapse.
- **Mechanism:** An EMA copy of the encoder (τ=0.996) processes unmasked sequences to produce target embeddings. The asymmetry between online encoder (masked input) and momentum encoder (full input) prevents trivial solutions where all representations become identical.
- **Core assumption:** Slowly-moving targets provide a meaningful learning signal without requiring contrastive negatives.
- **Evidence anchors:** [Section 3.2] "This asymmetry is critical: without it, the encoder could collapse to trivial solutions where all representations are identical. The high momentum (τ=0.996) ensures targets change slowly"
- **Break condition:** If τ is too low, targets shift too rapidly; if too high, learning signal becomes stale.

## Foundational Learning

- **Concept: Joint-Embedding Predictive Architecture (JEPA)**
  - **Why needed here:** Core architectural innovation; differs from masked autoencoders by predicting in representation space rather than token space.
  - **Quick check question:** Can you explain why predicting embeddings (rather than tokens) abstracts away surface variation while preserving dynamics?

- **Concept: Autoregressive Language Models for EHR**
  - **Why needed here:** The baseline paradigm being critiqued; understanding its limitation ("deferring reasoning to decode time") motivates JEPA.
  - **Quick check question:** Why does next-token prediction fail to incentivize encoding "where the patient is going"?

- **Concept: Linear Probing / Frozen Representations**
  - **Why needed here:** Evaluation protocol ensures performance gains reflect representation quality, not task-specific fine-tuning.
  - **Quick check question:** Why does the paper evaluate with frozen embeddings + linear probes rather than end-to-end fine-tuning?

## Architecture Onboarding

- **Component map:** Patient Timeline → Clinical Tokenizer (delimiter tokens) → LLM Backbone (frozen + LoRA) → Online Encoder (masked) → Predictor → Predicted Embeddings → Momentum Encoder (full) → Target Embeddings → L_JEPA, plus LLM Head ← L_SFT

- **Critical path:**
  1. Tokenize patient history with delimiter tokens (`<conditions>`, `<drugs>`, etc.)
  2. Split into context (x) and future (y); mask 50% of future tokens
  3. Pass 1 (SFT): Compute next-token loss on unmasked sequence
  4. Pass 2 (JEPA): Online encoder processes masked sequence; momentum encoder processes full sequence; compute MSE at masked positions
  5. Accumulate gradients; update online encoder; EMA update momentum encoder

- **Design tradeoffs:**
  - **Hybrid vs. Curriculum:** Joint optimization risks objective interference; curriculum (SFT→JEPA) is more stable but requires two training phases
  - **Masking ratio:** 0.50 is optimal; lower leaks local correlations, higher removes context
  - **Predictor capacity:** 2-layer MLP with width matching hidden dimension is sufficient; deeper yields diminishing returns

- **Failure signatures:**
  - Hybrid training underperforms SFT baseline → objective interference; switch to curriculum
  - Long-horizon predictions degrade → dynamics not encoded; check masking ratio and JEPA loss weight
  - Representations collapse (all similar) → momentum encoder not updating correctly; verify τ and EMA implementation

- **First 3 experiments:**
  1. **Reproduce SFT baseline:** Train LLaMA-3.1 8B with LoRA on MSK token sequences; evaluate linear probe AUC on mortality/progression tasks to establish baseline.
  2. **Ablate masking ratio:** Train JEPA-only with rm ∈ {0.25, 0.50, 0.75} on subset; verify 0.50 yields best trajectory encoding.
  3. **Compare Hybrid vs. Curriculum:** Train both on MSK+INSPECT; confirm curriculum outperforms hybrid on disease progression tasks.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can intervention-conditioned world models enable reliable counterfactual reasoning for treatment optimization in clinical EHR? The paper plans to extend to intervention-conditioned models for counterfactual reasoning and treatment optimization.

- **Open Question 2:** What mechanisms underlie the objective interference between SFT and JEPA when optimized jointly, and can this conflict be resolved architecturally? The paper observes interference but uses curriculum training as a workaround rather than resolving the fundamental conflict.

- **Open Question 3:** How does the Trajectory Regularization effect generalize across disease domains beyond oncology and pulmonary embolism? The cross-disease transfer effect needs validation across diverse trajectory types with different temporal dynamics.

## Limitations

- **Data dependency and generalization:** Requires large, temporally dense, longitudinally curated EHR datasets; performance on sparsely recorded or smaller cohorts untested.
- **Objective interference under different pretraining conditions:** Interference effect not tested when model is pretrained on diverse EHR; may be mitigated by broader pretraining.
- **Momentum encoder hyperparameter sensitivity:** Uses τ=0.996 without exploring sensitivity; no ablation of how τ affects stability and learning.

## Confidence

**High confidence:** Core claim that combining SFT with JEPA improves long-horizon prediction over pure autoregressive models, well-supported by ablation and linear probe evaluations.

**Medium confidence:** Curriculum strategy resolves objective interference is supported by MSK results, but mechanism and generalizability to pretrained models uncertain.

**Low confidence:** Claim that JEPA's latent-space prediction abstracts "surface variation while preserving dynamics" lacks direct empirical validation.

## Next Checks

1. **Probe for clinical feature disentanglement in latent space:** Train linear classifiers to predict known clinical attributes from JEPA-encoded embeddings to validate clinically meaningful structure.

2. **Test interference under diverse pretraining:** Pretrain baseline LLM on heterogeneous multi-disease EHR, then compare Hybrid vs. Curriculum fine-tuning to determine if interference is fundamental or due to lack of broad pretraining.

3. **Ablate masking ratio with mechanistic probes:** Train JEPA-only models with different masking ratios and measure prediction loss and downstream performance to determine whether 0.5 is optimal due to information preservation vs. trajectory abstraction.