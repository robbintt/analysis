---
ver: rpa2
title: Deep Ensembles Secretly Perform Empirical Bayes
arxiv_id: '2501.17917'
source_url: https://arxiv.org/abs/2501.17917
tags:
- ensembles
- deep
- learning
- posterior
- prior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that deep ensembles perform exact Bayesian averaging
  with a posterior obtained via an implicitly learned data-dependent prior. The authors
  establish that deep ensembles can be interpreted as an empirical Bayes procedure,
  where the prior is learned from the data through maximum marginal likelihood.
---

# Deep Ensembles Secretly Perform Empirical Bayes

## Quick Facts
- arXiv ID: 2501.17917
- Source URL: https://arxiv.org/abs/2501.17917
- Reference count: 7
- Deep ensembles perform exact Bayesian averaging with an implicitly learned data-dependent prior

## Executive Summary
This paper reveals that deep ensembles are performing empirical Bayes by learning a data-dependent prior through maximum marginal likelihood. The authors demonstrate that when deep ensembles are trained, they implicitly learn a prior distribution that is then used for exact Bayesian averaging. This interpretation provides a principled justification for why deep ensembles work so well in practice, as they are not just heuristic methods but are performing proper Bayesian inference with an implicitly learned prior.

The key insight is that the prior learned by deep ensembles takes the form of a mixture of point masses, which helps explain various empirical observations about ensemble behavior. This work bridges the gap between deep ensembles and Bayesian neural networks, showing they are more closely related than previously thought. Deep ensembles are essentially the only Bayesian neural networks that perform exact posterior averaging, making them both principled and practical.

## Method Summary
The paper analyzes deep ensembles through the lens of empirical Bayes, showing that the ensemble members are trained to maximize the marginal likelihood with respect to an implicitly learned prior. The authors prove that under certain conditions, the ensemble posterior is exact, meaning it performs proper Bayesian averaging. They derive the form of the implicitly learned prior and show it is a mixture of point masses at the ensemble member parameters. The analysis focuses on single-layer neural networks with Gaussian likelihoods and priors, though the insights are argued to extend more broadly.

## Key Results
- Deep ensembles perform exact Bayesian averaging with a posterior obtained via an implicitly learned data-dependent prior
- The implicitly learned prior is given by a mixture of point masses at the ensemble member parameters
- This interpretation justifies deep ensembles as a principled approach and explains their strong empirical performance
- Deep ensembles are the only Bayesian neural networks that perform exact posterior averaging

## Why This Works (Mechanism)
Deep ensembles work because they implicitly perform empirical Bayes - learning a prior from the data through maximum marginal likelihood, then performing exact Bayesian averaging with that learned prior. The mechanism involves training multiple networks independently, which collectively define a data-dependent prior through their parameter configurations. When predictions are combined, this performs proper Bayesian model averaging. The mixture-of-point-masses structure of the learned prior makes the computation tractable while capturing the essential uncertainty structure of the problem.

## Foundational Learning

1. **Empirical Bayes**: A Bayesian approach where the prior is learned from the data rather than specified a priori
   - Why needed: Understanding the connection between deep ensembles and Bayesian inference
   - Quick check: Verify that marginal likelihood maximization can be used to learn prior parameters

2. **Bayesian Model Averaging**: Combining predictions from multiple models weighted by their posterior probabilities
   - Why needed: Explains why ensemble predictions are better than individual members
   - Quick check: Confirm that ensemble predictions are weighted averages of member predictions

3. **Maximum Marginal Likelihood**: Optimizing the evidence (marginal likelihood) to learn prior parameters
   - Why needed: The mechanism by which deep ensembles implicitly learn their prior
   - Quick check: Verify that training ensemble members independently maximizes the marginal likelihood

## Architecture Onboarding

**Component Map**: Data -> Ensemble Training (multiple networks) -> Implicit Prior Learning -> Bayesian Averaging -> Predictions

**Critical Path**: The critical path is the ensemble training phase where each member is trained independently to maximize marginal likelihood. This phase determines the implicitly learned prior, which then governs the Bayesian averaging during prediction.

**Design Tradeoffs**: The main tradeoff is between computational cost (training multiple networks) and the benefits of exact Bayesian inference. Unlike variational methods that approximate the posterior, deep ensembles achieve exact averaging but require more computation.

**Failure Signatures**: If ensemble members converge to very similar parameters, the learned prior becomes degenerate (a single point mass), reducing to MAP estimation rather than full Bayesian inference. This can happen with poor initialization or insufficient diversity in training.

**First Experiments**:
1. Train a deep ensemble on a simple regression task and visualize the learned prior as the distribution of ensemble member parameters
2. Compare ensemble predictions against a single MAP estimate to verify Bayesian averaging behavior
3. Measure the diversity of ensemble members and correlate with predictive performance

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis is limited to single-layer neural networks, which may not capture the complexity of deep ensembles in practice
- The assumption of a fixed prior structure (mixture of point masses) might not hold for all architectures and training procedures
- The paper does not address computational efficiency considerations or practical implementation details for deeper networks
- Empirical validation is relatively limited, focusing primarily on the conceptual connection rather than extensive experimental verification

## Confidence
- Theoretical claims connecting deep ensembles to empirical Bayes: High
- Interpretation