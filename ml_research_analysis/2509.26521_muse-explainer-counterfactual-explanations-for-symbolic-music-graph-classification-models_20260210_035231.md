---
ver: rpa2
title: 'MUSE-Explainer: Counterfactual Explanations for Symbolic Music Graph Classification
  Models'
arxiv_id: '2509.26521'
source_url: https://arxiv.org/abs/2509.26521
tags:
- graph
- music
- explanations
- counterfactual
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MUSE-Explainer, a counterfactual explanation
  method for graph neural networks in symbolic music analysis. The core idea is to
  generate musically coherent counterfactual explanations by applying small, interpretable
  edits (e.g., updating pitch, onset, duration, or adding/removing notes) to musical
  score graphs represented as heterogeneous directed graphs.
---

# MUSE-Explainer: Counterfactual Explanations for Symbolic Music Graph Classification Models

## Quick Facts
- arXiv ID: 2509.26521
- Source URL: https://arxiv.org/abs/2509.26521
- Authors: Baptiste Hilaire; Emmanouil Karystinaios; Gerhard Widmer
- Reference count: 22
- Primary result: Generates musically coherent counterfactual explanations for GNNs on symbolic music, flipping cadence predictions with 92-100% accuracy using 1.6-3.4 operations

## Executive Summary
This paper introduces MUSE-Explainer, a counterfactual explanation method for graph neural networks in symbolic music analysis. The method generates musically coherent counterfactuals by applying small, interpretable edits to musical score graphs represented as heterogeneous directed graphs. By constraining perturbations to five musically meaningful operations, the approach avoids out-of-distribution issues while ensuring all modifications preserve musical validity.

Experiments demonstrate successful label flipping (e.g., from PAC to NC) with minimal operations and interpretable distances in the loss function. The method is validated using the SMUG-Explain framework and visualized via Verovio, with modular configuration allowing flexible operation selection.

## Method Summary
MUSE-Explainer generates counterfactual explanations for GNNs performing node-level classification on symbolic music score graphs. The method represents musical scores as heterogeneous directed graphs with 'note' nodes and four edge types ('onset', 'consecutive', 'during', 'rest'), optionally including 'beat'/'measure' nodes. Five musically meaningful edit operations (pitch/onset/duration updates, note add/remove) ensure all graph modifications correspond to valid musical transformations.

The approach uses a diffusion-inspired iterative editing process where an inner model learns one operation per training phase, building counterfactuals sequentially: G → G₁ → G₂ → ... → Gₙ. Training minimizes a loss combining cross-entropy (label flip) and distance penalties. Experiments use Mozart piano sonatas dataset with cadence detection labels (PAC and NC), showing successful label flipping with interpretable edit distances.

## Key Results
- PAC→NC label flipping achieves 92-100% accuracy with 1.4-2.3 operations needed
- NC→PAC flipping shows 8-32% accuracy with 1.6-5.0 operations, reflecting class imbalance
- Average explanation generation time: 66-135 seconds for 10 counterfactuals per piece
- All modifications maintain musical coherence and validity through constrained operation set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining perturbations to musically meaningful operations prevents out-of-distribution issues in counterfactual generation.
- Mechanism: Five predefined operations (pitch/onset/duration updates, note add/remove) ensure all graph modifications correspond to valid musical transformations. Onset updates copy values from existing notes; durations use standard musical values. Edge consistency is preserved by construction.
- Core assumption: The five operations span the musically plausible space of score edits relevant to classification decisions.
- Evidence anchors:
  - [Section 3.4]: "We propose five musically meaningful edit operations... By restricting changes to these carefully crafted operations, we ensure that the resulting graphs remain within the valid distribution of music graphs."
  - [Abstract]: "avoids unrealistic or confusing outputs"
- Break condition: If a prediction depends on features outside these five operations (e.g., tempo changes, key signature shifts), the explainer cannot generate valid counterfactuals and may fail to flip labels.

### Mechanism 2
- Claim: Diffusion-inspired iterative editing produces valid counterfactuals without requiring denoising or post-processing.
- Mechanism: Starting from input graph G, the inner model learns one operation per training phase. Each counterfactual builds on the previous: G → G₁ → G₂ → ... → Gₙ, where each step adds one learned change. Training minimizes a loss combining cross-entropy (label flip) and distance penalties.
- Core assumption: Sequential single-operation edits can reach label-flipping states without getting trapped in local minima where intermediate graphs have high loss but no clear gradient toward valid counterfactuals.
- Evidence anchors:
  - [Section 3.2]: "Similar to noise diffusion, we start with an input graph and progressively create noisier versions. However, instead of introducing random noise, we apply musically coherent modifications."
  - [Algorithm 1]: Shows iterative structure with L.append(InnerModel(G)) building sequentially.
- Break condition: If label flip requires coordinated multi-note changes (e.g., a harmonic progression), single-operation steps may produce intermediate states that are musically valid but cannot reach the target class efficiently.

### Mechanism 3
- Claim: Asymmetric label flip difficulty reflects underlying model bias from unbalanced training data.
- Mechanism: The loss function balances classification confidence against edit distance. When target labels are rare in training (PAC vs NC), the model's prior resists flipping, requiring more operations.
- Core assumption: The distance metrics adequately capture "explanation simplicity" for human interpretability.
- Evidence anchors:
  - [Section 4, Table 1]: PAC→NC achieves 92–100% accuracy with ~1.4–2.3 operations; NC→PAC achieves only 8–32% accuracy with 1.6–5.0 operations.
  - [Section 4]: "the ground-truth label distribution is highly unbalanced: most notes are not associated with cadences. As a result, the model tends to predict NC more frequently."
- Break condition: If the model's decision boundary is highly non-linear or relies on features not captured by node/edge features, the distance-minimizing counterfactual may not be the most interpretable one for domain experts.

## Foundational Learning

- **Concept: Heterogeneous Graphs for Music**
  - Why needed here: Input representation uses oriented heterogeneous graphs with node types (note, beat, measure) and edge types (onset, consecutive, during, rest). Understanding this structure is prerequisite to understanding how edits propagate.
  - Quick check question: Given two notes u and v where on(u) = on(v), which edge type connects them?

- **Concept: Counterfactual Explanations**
  - Why needed here: The entire method is framed as generating counterfactuals—minimal input changes that alter predictions. Distinguishes from feature-importance methods that don't produce modified inputs.
  - Quick check question: What makes a counterfactual explanation different from a feature importance score?

- **Concept: Graph Edit Distance**
  - Why needed here: Loss function includes D_gp as a regularization term. Understanding what contributes to edit distance (node additions, removals, edge changes) is necessary to interpret the distance column in results.
  - Quick check question: If you add one note with onset/duration/pitch, how many edit operations does this contribute to graph edit distance?

## Architecture Onboarding

- **Component map:**
  Outer Component (Explainer) -> Parameters: epochs, λ terms, learning rate -> Input: Graph G, target label y -> Inner Model -> Operation selector (which of 5 ops) -> Parameter learner (which node, which value) -> Output: Modified graph G'

- **Critical path:** Input graph → Inner model forward pass → Operation + parameters selected → Apply operation → Compute loss (classification + distance) → Backprop to inner model → Repeat for N epochs → Return final counterfactual

- **Design tradeoffs:**
  - Higher λ_ent → more aggressive label flipping but potentially larger, less interpretable edits
  - More epochs → longer runtime (Table 1: 50 epochs ~68s, 100 epochs ~135s for 10 explanations) but minimal accuracy gain observed
  - Constraining operation sequence (Section 5.3) improves interpretability but reduces search flexibility

- **Failure signatures:**
  - Low accuracy on rare-class targets (NC→PAC: 8–32%) despite many operations → model prior strongly resists flip
  - Isolated note in visualization (Figure 2) with no connected edges → RemoveNote operation succeeded but may indicate graph integrity issue
  - Distance term very low but label not flipped → inner model converged to local minimum

- **First 3 experiments:**
  1. Reproduce Table 1 with PAC→NC on one Mozart sonata; verify accuracy and operation count match reported ranges.
  2. Ablate one operation (e.g., disable RemoveNote) and measure impact on accuracy and average number of changes.
  3. Test on a different GNN task (e.g., chord classification) to assess whether the five operations transfer or require domain-specific adaptation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can MUSE-Explainer effectively generate counterfactual explanations for edge-level and graph-level classification tasks, where modifying larger portions of the input graph may be necessary?
- Basis in paper: [explicit] The authors state: "For future work, we plan to extend our experiments to a broader range of graph neural network architectures, including edge-level and graph-level classification tasks. These scenarios may pose additional challenges, as modifying larger portions of the input graph may be necessary to alter the model's prediction."
- Why unresolved: The current implementation and experiments focus exclusively on node-level classification (cadence detection). Edge-level and graph-level tasks require different explanation granularities and may need more extensive graph modifications that could compromise musical coherence.
- What evidence would resolve it: Successful application of MUSE-Explainer to tasks such as composer classification (graph-level) or harmonic relationship prediction (edge-level), with reported accuracy, interpretability metrics, and musical validity assessments comparable to the node-level results.

### Open Question 2
- Question: Can the explainer's efficiency be improved to generate counterfactual explanations in parallel for multiple input graphs within the same model?
- Basis in paper: [explicit] The authors state: "Furthermore, we aim to enhance the efficiency of our explainer, enabling it to generate counterfactual explanations in parallel for multiple input graphs within the same model. This improvement would facilitate faster analysis and broader applicability in real-world symbolic music processing tasks."
- Why unresolved: The current implementation generates counterfactuals sequentially, with experiments showing times of 66-135 seconds for 10 explanations on single pieces. Parallel processing across multiple graphs introduces optimization challenges and potential trade-offs with explanation quality.
- What evidence would resolve it: Demonstrated parallel processing capability with benchmarked speedup ratios across multiple input graphs, while maintaining or improving upon current accuracy levels (92-100% for PAC→NC flip) and explanation quality metrics.

### Open Question 3
- Question: How can the musical interpretability of explanations be systematically quantified, particularly for operations like adding notes versus more intuitive modifications like pitch or duration changes?
- Basis in paper: [explicit] The authors note: "This last aspect is the most interesting and the most challenging, since some modifications will provide clear insights, such as feature changes, and it will be much harder for others to get any precise understanding of the prediction process. In fact, the answer of what knowledge could be retrieved from the fact that adding a note at a given onset will change a model's prediction depends on many factors."
- Why unresolved: Current evaluation focuses on label-flipping accuracy and distance metrics, but lacks systematic measures for the semantic musical interpretability of different operation types. Operations vary in their explanatory clarity: removing a suspension note provides clear harmonic insights, while adding notes may have multiple competing interpretations.
- What evidence would resolve it: Development and validation of an interpretability scoring rubric or metric, correlated with human expert evaluations of explanation usefulness across diverse musical contexts and operation types.

## Limitations

- The five-operation framework cannot handle features outside its scope (e.g., tempo changes, key signatures, voice leading rules), limiting generalizability to other music analysis tasks
- Sequential single-operation editing may be inefficient for label flips requiring coordinated multi-note changes, particularly for complex harmonic progressions
- Asymmetric performance between PAC→NC and NC→PAC flipping appears driven by class imbalance rather than architectural limitations, but the specific mechanisms remain under-specified

## Confidence

- **High confidence:** The five-operation framework prevents out-of-distribution issues by construction; the iterative diffusion-inspired editing process is clearly specified and reproducible.
- **Medium confidence:** The claim that asymmetric label flip difficulty reflects model bias from unbalanced training data is plausible but requires more ablation studies to rule out architectural artifacts.
- **Low confidence:** The generalizability of the five operations to other music analysis tasks beyond cadence detection; no experiments demonstrate transfer to chord recognition, key detection, or other GNN applications.

## Next Checks

1. Test MUSE-Explainer on a chord classification GNN model using the same Mozart dataset; verify whether the five operations remain sufficient or require task-specific extensions.
2. Perform ablation studies by removing each operation individually to quantify their marginal contribution to flipping accuracy and interpretability.
3. Evaluate counterfactuals with domain experts (music theorists) to assess whether generated explanations align with musically meaningful reasoning about cadence detection.