---
ver: rpa2
title: On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning
  in Post-training
arxiv_id: '2601.07389'
source_url: https://arxiv.org/abs/2601.07389
tags:
- uni00000013
- arxiv
- reward
- loss
- post-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proves that supervised fine-tuning (SFT) and reinforcement
  learning (RL) are inherently coupled in post-training pipelines for large language
  models, making them impossible to decouple without loss of performance. The theoretical
  analysis examines two canonical pipelines: SFT-then-RL and RL-then-SFT.'
---

# On the Non-decoupling of Supervised Fine-tuning and Reinforcement Learning in Post-training

## Quick Facts
- **arXiv ID**: 2601.07389
- **Source URL**: https://arxiv.org/abs/2601.07389
- **Reference count**: 6
- **Key outcome**: Supervised fine-tuning (SFT) and reinforcement learning (RL) are inherently coupled in post-training pipelines for large language models, making them impossible to decouple without loss of performance.

## Executive Summary
This paper proves that SFT and RL stages in LLM post-training pipelines are fundamentally coupled, with each stage degrading the performance of the other when applied sequentially. The authors show that RL following SFT inevitably increases cross-entropy loss on SFT data, while SFT following RL reduces the reward achieved by the RL policy. This non-decoupling effect arises from the mismatch between the optimization objectives: SFT minimizes cross-entropy to expert demonstrations while RL maximizes expected reward, and these objectives pull the model distribution in incompatible directions. The theoretical analysis is validated empirically on the Qwen3-0.6B model using the CoLA dataset, demonstrating catastrophic forgetting and reward collapse when transitioning between stages.

## Method Summary
The paper analyzes two canonical post-training pipelines: SFT-then-RL and RL-then-SFT. For empirical validation, the authors use the Qwen3-0.6B model and the CoLA dataset, implementing GRPO for the RL stage and standard cross-entropy loss for SFT. The experiments track performance metrics across stage transitions: cross-entropy loss on held-out SFT data during SFT-then-RL, and reward (mean@1 with +1/-1 scoring) during RL-then-SFT. The theoretical analysis derives bounds on performance degradation based on KL divergence between stage distributions and reward optimality conditions.

## Key Results
- RL following SFT inevitably increases cross-entropy loss on SFT data, degrading the model's fit to expert demonstrations
- SFT following RL reduces the reward achieved by the RL policy, with the final SFT checkpoint potentially achieving lower accuracy than the base model
- The coupling arises from mismatched optimization objectives that cannot be reconciled through sequential training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: When RL follows SFT, the cross-entropy loss on SFT data inevitably increases, degrading the model's fit to expert demonstrations.
- Mechanism: The RL objective re-weights the policy away from the SFT-learned distribution toward higher-reward responses. The optimal RL policy is $p_{\theta_{RL}^{(2)}}(y|x) = \frac{1}{Z_\beta(x)} p_{\theta_{SFT}^{(1)}}(y|x) \exp(r(x,y)/\beta)$, which adds a reward-driven exponential tilt. This tilt distorts the SFT-aligned probabilities, introducing a strictly positive penalty $C_1(\beta) \geq 0$ to the SFT loss.
- Core assumption: The first SFT stage achieves near-optimal fit to $p_{\text{data}}(y|x)$, and rewards are bounded.
- Evidence anchors: [abstract] "RL increases SFT loss under SFT optimality"; [section 3, Theorem 3.1] Derivation shows $L_{\text{SFT}}(p_{\theta_{RL}^{(2)}}) = L_{\text{SFT}}(p_{\theta_{SFT}^{(1)}}) + C_1(\beta)$ with $C_1(\beta) \geq 0$ by Jensen's inequality.

### Mechanism 2
- Claim: When SFT follows RL, the reward achieved by the RL policy drops because SFT shifts the policy distribution away from reward-maximizing regions.
- Mechanism: SFT optimizes cross-entropy to expert responses, which typically differ from the reward-optimal response distribution. This shift is bounded by KL divergence: the expected reward gap is at most $R_{\max}\sqrt{2B}$ where $B$ bounds the KL from the RL policy. Under stronger curvature assumptions, the drop is a strict constant $C_2 > 0$.
- Core assumption: Assumption 3—the KL divergence between post-SFT and post-RL policies is bounded away from zero and from above ($0 < a \leq \mathbb{E}[D_{\text{KL}}] \leq A$).
- Evidence anchors: [abstract] "SFT lowers the reward achieved by RL"; [section 4, Theorem 4.1] Shows $J(p_{\theta_{SFT}^{(2)}}) \leq J(p_{\theta_{RL}^{(1)}}) - C_2$ under KL-growth conditions.

### Mechanism 3
- Claim: The fundamental coupling arises from mismatched optimization objectives—SFT minimizes cross-entropy to a fixed expert distribution, while RL maximizes expected reward—so optimizing one inevitably perturbs the other.
- Mechanism: The two objectives define different target distributions. The SFT target is $p_{\text{data}}(y|x)$; the RL target is $\frac{1}{Z}\pi_{\text{ref}}(y|x)\exp(r(x,y)/\beta)$. Unless these distributions coincide (a degenerate case), gradient steps for one objective move parameters away from the optimum of the other.
- Core assumption: Reward structure and expert data distribution are not perfectly aligned in practice.
- Evidence anchors: [section 6, Conclusion] "this loss is a consequence of optimizing mismatched objectives"; [corpus] Mitigating Forgetting Between SFT and RL explicitly frames the problem as "forgetting" and proposes replay mechanisms, implicitly affirming the objective mismatch.

## Foundational Learning

- **Concept**: Cross-Entropy Loss / Negative Log-Likelihood
  - Why needed here: The paper uses cross-entropy as the primary SFT performance metric; Theorem 3.1 quantifies RL's impact on this loss.
  - Quick check question: Given a model probability $p_\theta(y|x)$ and an expert label $y^*$, write the cross-entropy loss for a single sample.

- **Concept**: KL Divergence and Total Variation Distance
  - Why needed here: The theoretical bounds in Theorem 4.1 and Proposition 1 rely on KL and TV distances to quantify distribution shift and its reward impact.
  - Quick check question: State the Pinsker inequality relating KL divergence to TV distance.

- **Concept**: Policy Gradient and KL-Regularized RL Objective
  - Why needed here: The paper's RL stage uses a KL-regularized objective; the closed-form solution (Eq. 8) is central to the SFT-then-RL analysis.
  - Quick check question: Write the standard RLHF objective with KL penalty and identify the role of $\beta$.

## Architecture Onboarding

- **Component map**: Pretrained base model $\rightarrow$ SFT stage (minimizes cross-entropy on expert data) $\rightarrow$ RL stage (maximizes reward with KL regularization) $\rightarrow$ final checkpoint; Two canonical pipelines: (a) SFT-then-RL, (b) RL-then-SFT; Shared components: prompt distribution $q(x)$, SFT dataset $D_{\text{SFT}}$, RL dataset $D_{\text{RL}}$, reward function $r(x,y)$

- **Critical path**:
  1. Define unified prompt distribution and ensure $D_{\text{SFT}}$ and $D_{\text{RL}}$ prompts are sampled from the same $q(x)$ (Assumption 1).
  2. Implement SFT with cross-entropy loss; track loss on held-out SFT test set.
  3. Implement RL (e.g., GRPO) with bounded reward and KL regularization; track reward on RL test set.
  4. When switching stages, log the immediate change in the prior stage's metric (loss jump or reward drop).

- **Design tradeoffs**:
  - Larger KL penalty $\beta$ in RL reduces drift from SFT distribution but may limit reward gains.
  - Early stopping or limited SFT steps after RL (small $B$) bounds reward degradation but may under-utilize expert data.
  - Joint optimization (interleaved or multi-objective) may reduce coupling but increases hyperparameter complexity.

- **Failure signatures**:
  - SFT-then-RL: Cross-entropy loss spikes immediately upon RL start and eventually exceeds base model baseline (Figure 3a).
  - RL-then-SFT: Reward collapses sharply when SFT begins; final accuracy can fall below base model level (Figure 3b).
  - Monitoring: Log both loss and reward continuously across stage transitions; watch for non-monotonic or catastrophic degradation.

- **First 3 experiments**:
  1. Replicate SFT-then-RL on CoLA with Qwen3-0.6B: train SFT for 2 epochs, then switch to GRPO; plot cross-entropy on SFT test set across the transition. Expect immediate loss increase.
  2. Replicate RL-then-SFT: train RL (GRPO) on base model until convergence, then run SFT for 2 epochs; plot reward (mean@1) on RL test set. Expect sharp reward drop at SFT start.
  3. Ablate KL penalty $\beta$: run SFT-then-RL with varying $\beta$ (e.g., 0.01, 0.1, 1.0); measure the magnitude of loss increase $C_1(\beta)$. Test whether higher $\beta$ reduces coupling strength.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a unified objective function be designed to jointly optimize SFT and RL, preventing the performance degradation inherent in sequential pipelines?
- Basis in paper: [explicit] The conclusion states practitioners should "treat SFT and RL as a single joint optimization problem" and hopes the insights inform "new training strategies."
- Why unresolved: The paper proves the impossibility of decoupling sequential stages but does not propose a specific algorithm or loss function for the suggested joint optimization.
- What evidence would resolve it: A new training paradigm that mathematically guarantees SFT loss minimization while simultaneously maximizing RL reward without the trade-offs identified in Theorems 3.1 and 4.1.

### Open Question 2
- Question: Do the theoretical bounds on performance degradation ($C_1, C_2$) remain constant, improve, or worsen with increased model scale?
- Basis in paper: [inferred] Empirical validation is restricted to the small Qwen3-0.6B model; the theoretical analysis does not address how model capacity affects the constants.
- Why unresolved: It is unclear if larger models have sufficient capacity to mitigate the "catastrophic forgetting" and reward collapse observed, or if the degradation scales with parameter count.
- What evidence would resolve it: Empirical replication of the SFT-then-RL and RL-then-SFT experiments on larger models (e.g., 7B, 70B parameters) to measure the magnitude of degradation constants.

### Open Question 3
- Question: How does distribution shift between SFT and RL data (violating Assumption 1) affect the theoretical coupling?
- Basis in paper: [inferred] Assumption 1 posits that SFT and RL prompts are sampled from the same distribution $q(x)$, which is often untrue in practice (e.g., broad SFT followed by domain-specific RL).
- Why unresolved: The proofs rely on identical prompt distributions to derive the degradation bounds; real-world distribution shifts could either mask or exacerbate the non-decoupling effects.
- What evidence would resolve it: Theoretical analysis or ablation studies analyzing the degradation bounds when $D_{SFT}$ and $D_{RL}$ are drawn from distinct distributions.

## Limitations

- Theoretical analysis relies on idealized assumptions about reward optimality and KL-divergence bounds that may not hold in practical implementations
- Empirical validation limited to a single small model (Qwen3-0.6B) and one dataset (CoLA), which may not generalize to larger models or different task domains
- Paper does not explore whether certain hyperparameter regimes or architectural modifications could mitigate the coupling effects

## Confidence

- **High confidence**: The theoretical proofs establishing the fundamental impossibility of perfect decoupling under the stated assumptions (Theorems 3.1 and 4.1)
- **Medium confidence**: The empirical demonstration on Qwen3-0.6B, given the limited model and dataset scope
- **Low confidence**: The universality claim that this coupling applies to all SFT-RL pipelines without exception

## Next Checks

1. **Cross-model validation**: Replicate the experiments on larger models (1B+ parameters) and diverse datasets (math, code, dialogue) to test generalizability of the coupling phenomenon
2. **Hyperparameter sensitivity analysis**: Systematically vary KL penalties, learning rates, and training durations to quantify how much coupling strength can be reduced through careful tuning
3. **Joint optimization experiments**: Implement and evaluate alternating optimization or multi-objective approaches to measure whether they can achieve better balance than sequential pipelines while quantifying the remaining coupling effect