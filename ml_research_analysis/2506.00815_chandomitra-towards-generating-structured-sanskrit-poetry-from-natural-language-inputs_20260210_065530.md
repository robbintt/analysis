---
ver: rpa2
title: 'Chandomitra: Towards Generating Structured Sanskrit Poetry from Natural Language
  Inputs'
arxiv_id: '2506.00815'
source_url: https://arxiv.org/abs/2506.00815
tags:
- sanskrit
- poetry
- decoding
- constrained
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Chandomitra, a dataset for generating structured
  Sanskrit poetry from English inputs in the Anushtubh meter. It benchmarks various
  models using constrained decoding and instruction fine-tuning to ensure metrical
  correctness.
---

# Chandomitra: Towards Generating Structured Sanskrit Poetry from Natural Language Inputs

## Quick Facts
- arXiv ID: 2506.00815
- Source URL: https://arxiv.org/abs/2506.00815
- Reference count: 40
- Primary result: Introduces Chandomitra dataset for Sanskrit poetry generation; constrained decoding achieves 99.86% syntactic accuracy while instruction fine-tuning balances meter with semantic coherence

## Executive Summary
Chandomitra introduces a novel dataset and methodology for generating Sanskrit poetry in the Anushtubh meter from English prose inputs. The work benchmarks two approaches: constrained decoding with regex-based syllable filtering (achieving 99.86% syntactic accuracy) and instruction fine-tuning (achieving 52.5% full compliance with better semantic coherence at 68.05). The paper demonstrates that while constraint-based methods excel at meter enforcement, instruction-tuned models produce more poetically expressive outputs with better semantic alignment to the source text.

## Method Summary
The Chandomitra dataset pairs 8,306 English-Sanskrit Anushtubh verses from Valmiki Ramayana. Two generation approaches are evaluated: (1) Constrained decoding fine-tunes NLLB-dist-1.3B and enforces meter via regex pattern matching at each decoding step, achieving 99.86% syntactic accuracy; (2) Instruction fine-tuning applies LoRA to decoder-only models (Mistral-Nemo-2407-12B, Phi-4, Llama) using prompts that describe Anushtubh rules, trading some syntactic precision for improved semantic coherence and poeticness. Semantic evaluation uses fine-tuned multilingual embeddings (BAAI/bge-m3) with linearized cosine similarity.

## Key Results
- Constrained decoding achieves 99.86% syntactic accuracy on Anushtubh meter compliance
- Best instruction-tuned model (Mistral-Nemo-2407-12B) reaches 52.5% full meter compliance with 68.05 semantic similarity score
- Human evaluation confirms instruction-tuned models score higher on semantic coherence (3.775 vs 2.125) and poeticness (3.85 vs 2.225)
- Methods generalize to other Sanskrit meters (Trishtubh: 96.67% compliance) and languages (Awadhi, Bengali) via regex pattern adaptation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constrained decoding with regex-based syllable filtering enforces metrical correctness at inference time without model modification
- Mechanism: At each generation step, top-k candidate tokens are evaluated by temporarily appending each to output, computing resulting syllable weight sequences, and masking logits for candidates violating pre-compiled regex patterns encoding Anushtubh rules (8 syllables per pāda, 5th laghu, 6th guru, 7th position constraints). Valid tokens proceed to sampling
- Core assumption: Base translation model has sufficient semantic knowledge that restricting token space to metrically valid options won't catastrophically degrade meaning
- Evidence anchors: [abstract] "Our constrained decoding methodology achieves 99.86% syntactic accuracy"; [section 4.1] Algorithm 1 details top-k filtering and regex matching procedure; [corpus] Neighboring work on metrical poetry confirms interest in formal constraints

### Mechanism 2
- Claim: Instruction fine-tuning on meter-aligned English-Sanskrit pairs enables decoder-only models to internalize metrical constraints as in-context patterns
- Mechanism: Curated prompts explicitly describe Anushtubh rules in natural language. Fine-tuning with LoRA on Chandomitra corpus teaches models to generate verses that approximately satisfy constraints while preserving input semantics. Model learns soft alignment between meaning and form
- Core assumption: Instruction-tuned models can transfer pattern recognition from training examples to novel inputs; 8K examples provide sufficient coverage
- Evidence anchors: [abstract] "Our best-performing instruction-tuned model... performs better in semantic coherence... at the expense of slightly lower syntactic accuracy"; [section 5.3] Table 5 shows Mistral-Nemo-2407-12B achieving 52.5% compliance with 68.05 semantic similarity vs. NLLB-CD's 99.86% / 64.91; [section 5.4] Human evaluation confirms higher semantic coherence and poeticness ratings

### Mechanism 3
- Claim: Fine-tuned multilingual sentence embeddings (BAAI/bge-m3) with linearized cosine similarity provide viable automated proxy for cross-lingual semantic evaluation
- Mechanism: Sentence transformers fine-tuned on mitrasam.graha Sanskrit-English parallel data using multiple negatives ranking loss. For evaluation, embeddings of English input and generated Sanskrit verse are compared via linearized cosine similarity
- Core assumption: Semantic similarity between prose and poetry can be captured by embedding proximity despite genre shift; fine-tuning corpus is representative of target domain
- Evidence anchors: [section 5.1.2] Table 4 shows linearized cosine similarity with fine-tuned BAAI/bge-m3 outperforming kNN ratio-margin, BERTScore after translation, and Sanskrit-only cosine similarity; [section 5.1.2] Human annotation on ~1K pairs established ground truth

## Foundational Learning

- Concept: **Sanskrit Chandas (metrical theory)**
  - Why needed here: Anushtubh meter has precise syllable-weight constraints (laghu/guru patterns across four 8-syllable pādas). Without understanding these rules, one cannot interpret syntactic compliance metrics or debug constraint violations
  - Quick check question: Given a Sanskrit verse, can you identify whether the 5th syllable of each pāda is laghu and the 6th is guru?

- Concept: **Constrained decoding (logit masking)**
  - Why needed here: Paper's highest-accuracy approach relies on masking invalid tokens at each step. Understanding this technique is prerequisite to implementing or extending the method
  - Quick check question: If a model's vocabulary contains 50,000 tokens and only 3 satisfy a metrical constraint at step t, how do you ensure only those 3 receive non-zero probability?

- Concept: **Cross-lingual sentence embeddings**
  - Why needed here: Semantic evaluation across English input and Sanskrit output requires shared representation space. Choice of embedding model and fine-tuning procedure directly affects metric validity
  - Quick check question: Why would BLEU or chrF be inappropriate for evaluating semantic preservation when output word order differs substantially from any reference?

## Architecture Onboarding

- Component map: NLLB-dist-1.3B translation model → LogitsProcessor interface → Syllable scanner (skrutable library) → Regex constraint checker → Modified logits → Sampling strategy (greedy optimal) OR Instruction-tuned decoder-only model (Mistral-Nemo, Phi-4, Llama) → LoRA adapters (r=16) → Fine-tune on Chandomitra prompts → Standard autoregressive generation → Evaluation: Syntactic validator (skrutable/Chandojñānam) → Semantic encoder (fine-tuned BAAI/bge-m3) → Linearized cosine similarity → Human annotation calibration

- Critical path:
  1. Dataset preparation: Filter Valmiki Ramayana for Anushtubh verses via Chandojñānam, pair with English translations
  2. Constrained decoding: Implement regex patterns for each pāda position, integrate with HuggingFace LogitsProcessor, add LRU cache for syllabification (Table 9 shows 21% latency reduction)
  3. Instruction fine-tuning: Format prompts with explicit meter rules (Appendix B.1), train with LoRA for 3 epochs, evaluate on held-out test (1,421 samples) and OOD set (520 samples)
  4. Semantic metric calibration: Fine-tune sentence transformer on mitrasam.graha, correlate against 1,000 human-annotated pairs

- Design tradeoffs:
  - **Syntactic accuracy vs. semantic coherence**: Constrained decoding achieves near-perfect meter (99.86%) with lower semantic scores (64.91); IFT sacrifices meter (52.5%) for better semantics (68.05) and poeticness
  - **Latency vs. constraint coverage**: Constrained decoding adds ~6× latency overhead (0.041s → 0.219s per generation); caching mitigates but syllabification remains bottleneck (90% of time)
  - **Generalization vs. specialization**: CD approach generalizes to other meters and languages by swapping regex patterns; IFT would require retraining per meter

- Failure signatures:
  - **GPT-4o 1-shot**: 21.6% of outputs have correct length but violate meter (syllable-weight dissociation); 27.45% are off by 1-2 syllables (near-miss clustering at 31/33)
  - **CD on instruction models**: Model emits English preamble; constraint filter blocks valid Sanskrit tokens due to BPE tokenization splitting Devanagari across multiple token IDs
  - **Indic-specific LLMs (Airavata, Navarasa)**: Near-zero syntactic compliance despite Indic pretraining—suggests metrical reasoning is not acquired from general corpora

- First 3 experiments:
  1. Reproduce constrained decoding baseline on NLLB-dist-1.3B with k=25 greedy sampling; validate 99%+ syntactic accuracy on 100 held-out samples
  2. Fine-tune Mistral-Nemo-2407-12B with provided LoRA config; compare semantic similarity and human poeticness ratings against CD baseline
  3. Extend CD to second Sanskrit meter (e.g., Trishtubh) using 30 Bhagavadgita samples; verify regex pattern generalization maintains 90%+ compliance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can constrained decoding be effectively combined with instruction fine-tuning to achieve both high syntactic accuracy and strong semantic coherence?
- Basis in paper: [explicit] "One limitation of CD is it's application alongside IFT models... This is problematic because the IFT models might generate some generic English/part of the instruction before actually generating the verse."
- Why unresolved: The paper tried both approaches separately but notes that CD skews output distribution and increases perplexity when applied to instruction-tuned models due to their tendency to generate preamble text
- What evidence would resolve it: Successful hybrid approach achieving >90% syntactic accuracy AND >65 semantic similarity score on same model, or architectural modifications that prevent preamble generation in IFT models

### Open Question 2
- Question: How do these methods scale to larger language models (7B+ parameters for translation models, 70B+ for instruction models)?
- Basis in paper: [explicit] "We could not evaluate larger LLMs due to limited availability of compute resources (single GPU); hence, we selected a few representative smaller models for our experiments."
- Why unresolved: Only smaller models were tested (NLLB-1.3B, Phi-4-14B, Mistral-Nemo-12B, GPT-4o via API); larger models may exhibit different trade-offs or emergent capabilities
- What evidence would resolve it: Evaluation of at least three models >30B parameters on Chandomitra benchmark, comparing constrained decoding and instruction fine-tuning approaches

### Open Question 3
- Question: Do the constrained decoding and instruction fine-tuning approaches generalize robustly to other Sanskrit meters and input source languages beyond English?
- Basis in paper: [explicit] "we have not tested them with other meters... we have not explored generating Sanskrit poetry from inputs in other languages than English" and "preliminary experiments... should be interpreted as indicative rather than comprehensive. Future work will expand these experiments with larger datasets."
- Why unresolved: Cross-meter tests used only 30 samples from Bhagavadgita (Trishtubh), and cross-lingual tests used 30 samples each from Awadhi and Bengali—insufficient for robust conclusions
- What evidence would resolve it: Evaluation on at least 500 samples each across 3+ distinct Sanskrit meters, and translation from 3+ source languages with statistically significant performance metrics

## Limitations

- **Dataset provenance and scope**: Chandomitra corpus built exclusively from Valmiki Ramayana, limiting stylistic and topical diversity. No evidence provided that Ramayana-based patterns transfer to arbitrary prose
- **Metric robustness and domain shift**: Semantic similarity relies on fine-tuned embeddings trained on general-purpose parallel corpus. Poetic semantics (rasa, metaphor, alliteration) may not be captured by sentence-level embeddings
- **Constraint handling fragility**: Constrained decoding tightly coupled to Anushtubh regex patterns and NLLB model's syllabification output. Method is not robust to tokenization artifacts and fails on instruction-tuned models

## Confidence

- **High**: Syntactic accuracy of constrained decoding (99.86%) and existence of reproducible dataset (Chandomitra) with verifiable metrics. Algorithmic steps (regex-based logit masking, syllabification via skrutable) are clearly specified
- **Medium**: Claims of semantic superiority for IFT models (e.g., Mistral-Nemo-2407-12B at 68.05 semantic similarity). These rely on fine-tuned embeddings and human evaluation, both methodologically sound but not exhaustively validated
- **Low**: Generalization claims to other meters and languages based solely on constrained decoding extensions. No evidence provided for instruction-tuned generalization

## Next Checks

1. **Cross-meter instruction tuning**: Apply same IFT pipeline (LoRA, r=16, 3 epochs) to Trishtubh dataset (30 Bhagavadgita samples). Compare syntactic compliance and semantic similarity against CD baseline for that meter. Document qualitative differences in poetic quality or constraint adherence

2. **Metric calibration on poetic semantics**: Collect human annotations (coherence + poeticness) for 200 generated verses from both CD and IFT models, covering at least three meters (Anushtubh, Trishtubh, Jagati). Correlate human scores against linearized cosine similarity and other automated metrics. Identify which metrics best predict human poeticness judgments

3. **Out-of-domain generalization**: Use fine-tuned CD and IFT models to generate verses from English prose inputs not in Ramayana style (philosophical shlokas, modern prose, technical text). Measure syntactic compliance and semantic similarity; compare against in-domain performance. Document systematic failures or quality drops