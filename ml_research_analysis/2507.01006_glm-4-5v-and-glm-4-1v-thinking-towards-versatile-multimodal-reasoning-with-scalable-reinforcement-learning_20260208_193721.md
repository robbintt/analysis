---
ver: rpa2
title: 'GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with
  Scalable Reinforcement Learning'
arxiv_id: '2507.01006'
source_url: https://arxiv.org/abs/2507.01006
tags:
- glm-4
- wang
- training
- data
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present GLM-4.1V-Thinking, GLM-4.5V, and GLM-4.6V, a family
  of vision-language models (VLMs) designed to advance general-purpose multimodal
  understanding and reasoning. In this report, we share our key findings in the development
  of the reasoning-centric training framework.
---

# GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning

## Quick Facts
- arXiv ID: 2507.01006
- Source URL: https://arxiv.org/abs/2507.01006
- Reference count: 40
- Key outcome: GLM-4.5V achieves state-of-the-art performance among open-source models of similar size on 42 public benchmarks, with competitive results against closed-source models like Gemini-2.5-Flash on coding and GUI agent tasks.

## Executive Summary
This report presents GLM-4.1V-Thinking, GLM-4.5V, and GLM-4.6V, a family of vision-language models designed for general-purpose multimodal understanding and reasoning. The authors develop a reasoning-centric training framework centered on Reinforcement Learning with Curriculum Sampling (RLCS), which adaptively selects samples at the edge of model competence to maximize learning efficiency. Through large-scale pre-training followed by multi-domain reinforcement learning across STEM, video understanding, OCR, grounding, GUI agents, and more, these models achieve state-of-the-art performance among open-source models while demonstrating competitive results against leading closed-source alternatives.

## Method Summary
The authors employ a three-stage pipeline: multimodal pre-training on 120K steps (8K context, batch 1536) using diverse data including image-text pairs, interleaved documents, OCR, grounding, and video; long-context continual training (32K sequence, 10K steps) to extend context window; and supervised fine-tuning (SFT) with long-chain-of-thought data in standardized format. The core innovation is RLCS, which implements adaptive curriculum sampling based on difficulty evaluation (offline pass@k + human labels, online rollout outcomes) to maintain informative gradients. Multi-domain RL is trained jointly across STEM, OCR & Chart, Grounding, and GUI Agent tasks using GRPO with domain-specific verifiers, KL/entropy losses removed, forced answering for truncated responses, and top-p=1 for rollouts.

## Key Results
- GLM-4.5V achieves state-of-the-art performance among open-source models of similar size on nearly all tasks across 42 public benchmarks
- GLM-4.1V-9B-Thinking outperforms the much larger Qwen2.5-VL-72B on 29 benchmarks while being significantly smaller
- GLM-4.6V introduces native tool use and 128K context window capabilities
- Cross-domain mutual facilitation shows training on one domain improves performance in others, with joint training yielding even greater improvements

## Why This Works (Mechanism)

### Mechanism 1: Reinforcement Learning with Curriculum Sampling (RLCS)
RLCS improves training efficiency by selecting samples at the edge of model competence through adaptive difficulty grading. It evaluates sample difficulty both offline (via pass@k with prior models + human labels) and online (tracking rollout outcomes), down-sampling trivial and impossibly hard examples while boosting mid-difficulty exposure. This maintains informative gradients since all-correct/all-incorrect batches in GRPO yield zero useful updates. The core assumption is that difficulty labels correlate with learning value and the model's current capability can be approximated by recent rollout statistics.

### Mechanism 2: Cross-Domain Mutual Facilitation in Multi-Domain RL
Joint RL training across diverse multimodal domains yields positive transfer where gains in one domain improve performance in others. Shared underlying capabilities (visual perception, text recognition, reasoning) are co-activated across domains. For example, GUI-agent tasks require grounding, OCR, and reasoning—training on them transfers to pure grounding or VQA tasks. The "mix-all" setting outperforms single-domain RL on 3/5 areas in controlled experiments. The core assumption is that domains share latent sub-skills and gradient updates from one domain's reward signal generalize to others without catastrophic interference.

### Mechanism 3: Reward System Robustness as Training Stabilizer
In multi-domain RL, a single weak verifier can destabilize entire training through reward hacking or noise injection. The reward system must accurately extract and verify answers across all domains. Flaws (e.g., LLM-based extraction errors on looping outputs) allow models to game rewards without improving task performance. The paper enforces boxed-answer formatting and domain-specific verifiers to prevent this. The core assumption is that verifiable rewards (RLVR) can be designed with near-oracle accuracy for each subdomain and format enforcement doesn't constrain useful reasoning paths.

## Foundational Learning

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed here: RLCS builds on GRPO, which computes advantages from group-wise rollout comparisons rather than a separate value model
  - Quick check question: Can you explain why GRPO removes the need for a critic network and how it handles variable group sizes?

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: The entire RL framework relies on binary/continuous rewards from deterministic verifiers rather than learned reward models
  - Quick check question: What are the failure modes when verifiers have 5-10% error rates, and how does curriculum sampling interact with noisy rewards?

- **Concept: Vision-Language Model Architecture (ViT + LLM with Adapter)**
  - Why needed here: Understanding how AIMv2-Huge, MLP projector, and GLM LLM decoder interact is prerequisite to modifying the pipeline
  - Quick check question: How does 2D-RoPE in ViT and 3D-RoPE in LLM enable arbitrary aspect ratios and temporal understanding?

## Architecture Onboarding

- **Component map:**
  - Vision Encoder (AIMv2-Huge with 2D-RoPE + bicubic interpolation) -> Adapter (2-layer MLP) -> LLM Decoder (GLM-4-9B or GLM-4.5-Air MoE with 3D-RoPE) -> Training Pipeline (Pre-training -> Long-context -> SFT -> RLCS) -> Modular Reward System (domain-specific verifiers)

- **Critical path:**
  1. Pre-training data quality (caption recaptioning, interleaved corpus cleaning) sets foundation ceiling—pass@k on MathVista non-MC correlates with final RL performance
  2. SFT cold-start quality determines RL stability—noisy reasoning traces cause collapse
  3. RLCS implementation—offline difficulty grading + online EMA expansion ratio + dynamic batch balancing

- **Design tradeoffs:**
  - KL loss removal: VLMs have faster KL divergence growth than text-only models; suppressing it via KL loss constrains capabilities, so authors remove it
  - Forced truncation: Long thinking paths are truncated with forced answer tokens to recover valid gradients from overlong rollouts
  - Top-p=1 for rollouts: Lower top-p increases garbled output risk over time; top-p=1 maintains vocabulary coverage

- **Failure signatures:**
  - Reward hacking: Model outputs vague ranges ("a correct number between 0 and 10") that fool LLM-based verifiers
  - Training collapse: Sudden reward spikes in one domain with benchmark drops across all domains indicate verifier weakness
  - Gradient starvation: >50% prompts reach >90% accuracy early, leaving few informative samples—RLCS must dynamically upsample harder data

- **First 3 experiments:**
  1. Validate pre-training ceiling: Train a small base model (e.g., 1B subset) on the described pre-training mixture and measure pass@k on MathVista non-MC. Compare to ablations without interleaved data or recaptioning.
  2. Cold-start stability test: Run SFT with clean vs. noisy CoT data (introduce formatting errors, mixed-language reasoning), then start RL. Monitor early reward curves for divergence.
  3. Single-domain vs. mix-all RL: Replicate the 4-domain experiment (STEM, OCR & Chart, Grounding, GUI Agent) from Sec 6.3 on a smaller model. Verify cross-domain transfer matrix matches Figure 6 patterns.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can reinforcement learning reward models be designed to verify intermediate reasoning steps rather than just final answer accuracy?
  - Basis: The authors note RL often produces correct answers using flawed reasoning because rewards only assess final outcomes
  - Evidence needed: A study implementing process-based rewards (PRMs) for multimodal chains-of-thought that penalizes logical inconsistencies even when the final answer is correct

- **Open Question 2:** Why does joint multi-domain training boost STEM and OCR performance but fail to improve grounding and GUI agent tasks?
  - Basis: Section 6.3 shows "Mix-all" training helps some domains but doesn't yield gains for grounding or GUI agents compared to single-domain training
  - Evidence needed: An ablation study identifying shared vs. conflicting latent features between STEM and grounding tasks during RL gradient updates

- **Open Question 3:** How can large-scale RL frameworks be stabilized against "single-point-of-failure" reward signals?
  - Basis: Section 5.2 reports a weak verifier in one sub-domain (multi-image QA) caused the entire model to collapse despite a strong verifier in another (STEM)
  - Evidence needed: A training run demonstrating a robustness mechanism (e.g., gradient isolation or dynamic reward scaling) that prevents a noisy sub-domain reward from degrading STEM performance

## Limitations
- Exact composition and quality control procedures of the pre-training corpus remain unspecified, making reproducibility uncertain
- RLCS effectiveness depends heavily on accurate difficulty labeling, but reliability of offline pass@k metrics and human annotation quality is unclear
- Reward system robustness assumes verifiers can achieve near-oracle accuracy, but real-world applications often face unavoidable noise that may break stability guarantees

## Confidence
- **High Confidence**: Pre-training establishes capability ceiling (supported by MathVista non-MC pass@k correlation), RLCS improves training efficiency through adaptive sampling (mechanism is well-specified with clear failure modes), and the three-stage pipeline architecture is technically sound
- **Medium Confidence**: Cross-domain mutual facilitation (limited to 4 domains, transfer matrices show mixed patterns), reward system robustness (single-domain verifier weakness case demonstrated but general noise handling unclear), and competitive performance claims against closed-source models (benchmark selection may favor the proposed approach)
- **Low Confidence**: Claims about GLM-4.6V's native tool use and 128K context window performance, as these extensions are mentioned but not extensively evaluated in the report

## Next Checks
1. **Pre-training ceiling validation**: Train a 1B-parameter subset model on the described pre-training mixture with and without interleaved data/recaptioning, then measure pass@k on MathVista non-MC to verify the correlation between pre-training quality and RL performance ceiling

2. **Single-domain vs. mix-all RL transfer**: Replicate the controlled experiment from Section 6.3 with a smaller model, training on individual domains (STEM, OCR & Chart, Grounding, GUI Agent) versus mix-all, and measure the transfer matrix across all evaluation tasks to confirm the reported mutual facilitation patterns

3. **Reward system stress test**: Design a controlled experiment where one domain's verifier is deliberately weakened (e.g., 10% error rate) while others remain strong, then monitor for training collapse symptoms (reward increases with benchmark decreases) to validate the claimed robustness mechanism