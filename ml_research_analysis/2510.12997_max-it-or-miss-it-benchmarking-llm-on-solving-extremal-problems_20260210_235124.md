---
ver: rpa2
title: 'Max It or Miss It: Benchmarking LLM On Solving Extremal Problems'
arxiv_id: '2510.12997'
source_url: https://arxiv.org/abs/2510.12997
tags:
- reasoning
- mathematical
- problems
- arxiv
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ExtremBench is a dataset of 93 extremal problems derived from\
  \ Chinese Mathematical Olympiad inequality exercises, converted into optimization\
  \ tasks for evaluating LLM reasoning capabilities. The problems are transformed\
  \ from proof-style inequalities into extrema-finding problems (e.g., \"prove A \u2264\
  \ B\" becomes \"find max/min of A-B\"), enabling automated evaluation."
---

# Max It or Miss It: Benchmarking LLM On Solving Extremal Problems

## Quick Facts
- **arXiv ID**: 2510.12997
- **Source URL**: https://arxiv.org/abs/2510.12997
- **Reference count**: 29
- **Primary result**: LLM performance on extremal problems does not correlate with general math benchmark performance

## Executive Summary
ExtremBench is a dataset of 93 extremal problems derived from Chinese Mathematical Olympiad inequality exercises, converted into optimization tasks for evaluating LLM reasoning capabilities. The problems are transformed from proof-style inequalities into extrema-finding problems (e.g., "prove A ≤ B" becomes "find max/min of A-B"), enabling automated evaluation. The benchmark was evaluated on Qwen3, GPT-OSS, and DeepSeek model families using SGLang on NVIDIA B200 GPUs. Results showed that LLM performance on extremal problems does not correlate with their performance on general mathematical benchmarks like AIME25 and MATH-500. Some models strong at general math struggled with optimization tasks, and vice versa. Qwen3-Thinking variants achieved 75-80% accuracy on ExtremBench, while GPT-OSS models scored around 70% despite >90% on AIME25. This discrepancy highlights that extremal problem-solving represents a distinct mathematical competency not captured by existing benchmarks.

## Method Summary
The benchmark converts mathematical inequality proofs into extrema-finding optimization problems, where constraints are preserved and objectives become numerical extremum calculations. Problems are presented in LaTeX format with ground-truth numerical answers. Evaluation uses SGLang framework on NVIDIA B200 GPUs, testing multiple model families (Qwen3 1.7B-235B, GPT-OSS 20B-120B, DeepSeek-R1 1.5B-14B). Answers are verified using Math-Verify tool against ground truth. Each model undergoes three repeated trials with mean accuracy reported. The benchmark includes 62 minimization and 31 maximization problems, with response lengths ranging from 1,061 to 24,351 tokens for thinking models.

## Key Results
- LLM performance on extremal problems shows no correlation with AIME25/MATH-500 scores
- Qwen3-Thinking variants achieve 75-80% accuracy on ExtremBench
- GPT-OSS models score ~70% on ExtremBench despite >90% on AIME25
- Model scale does not consistently predict extremal-solving performance (Qwen3-14B matches Qwen3-235B)

## Why This Works (Mechanism)
Extremal problems require distinct reasoning patterns compared to general mathematical problems. While general math benchmarks test pattern recognition and procedural knowledge, extremal problems demand constraint manipulation, optimization intuition, and numerical extremum identification. The transformation from proofs to optimization preserves mathematical rigor while enabling automated evaluation, revealing that models may develop specialized capabilities for different mathematical domains rather than general reasoning abilities.

## Foundational Learning
- **Mathematical optimization**: Finding maxima/minima under constraints is fundamental to extremal problem-solving
  - Why needed: The benchmark evaluates models' ability to identify optimal values rather than just prove inequalities
  - Quick check: Can the model correctly identify that maximizing A-B is equivalent to proving A ≤ B

- **Constraint satisfaction**: Understanding and manipulating problem constraints is critical for finding valid extrema
  - Why needed: Extremal problems require respecting all given constraints while searching for optimal solutions
  - Quick check: Does the model maintain constraint validity throughout its solution process

- **Numerical computation**: Extracting and verifying numerical answers requires precise arithmetic capabilities
  - Why needed: The benchmark requires exact numerical answers rather than symbolic expressions
  - Quick check: Can the model provide accurate numerical values within verification tolerance

## Architecture Onboarding

**Component Map**: ExtremBench dataset -> SGLang inference framework -> Model evaluation -> Math-Verify verification -> Accuracy calculation

**Critical Path**: Problem presentation -> Model reasoning generation -> Numerical answer extraction -> Verification against ground truth -> Performance aggregation

**Design Tradeoffs**: The transformation from proofs to optimization enables automated evaluation but may lose some proof-specific reasoning patterns; larger models don't consistently outperform smaller ones, suggesting training data composition matters more than scale for this task

**Failure Signatures**: Models outputting symbolic expressions instead of numerical values; constraint violations in proposed solutions; answer verification failures due to formatting mismatches

**First Experiments**:
1. Test smallest Qwen3 variant (1.7B) on 10 sample problems to validate basic functionality
2. Run single trial comparison between Qwen3-14B and Qwen3-235B on identical problem sets
3. Verify Math-Verify tool correctly handles floating-point tolerance for extremal answers

## Open Questions the Paper Calls Out
- **Open Question 1**: What underlying mechanisms explain the discrepancy between general mathematical reasoning and extremal-solving abilities in LLMs?
  - The paper identifies the gap but doesn't investigate causes; GPT-OSS achieves >90% on AIME25 but only ~70% on ExtremBench without causal analysis

- **Open Question 2**: Does extremal-solving capability serve as a better predictor for downstream tasks in scientific computing, operations research, and automated theorem proving?
  - The paper establishes extremal reasoning as distinct but doesn't validate its predictive power for practical applications

- **Open Question 3**: Why doesn't model scale correlate with extremal-solving performance, as seen with Qwen3-14B matching Qwen3-235B despite 17× fewer parameters?
  - The observation is documented but no controlled experiments isolate training data, architecture, or scale as causal factors

- **Open Question 4**: Can the proof-to-extremum transformation methodology generalize to combinatorics, geometry, and analysis problems?
  - The methodology is demonstrated only on inequality proofs; generalization to other proof types requires different transformation rules

## Limitations
- Inference hyperparameters (temperature, top-p, max tokens) are unspecified, critical for thinking models with extensive chain-of-thought outputs
- Exact prompt templates and formatting used for problem presentation are not documented
- Dataset size (93 problems) may limit statistical significance of performance differences

## Confidence
- **High**: Benchmark construction methodology is clearly described and reproducible
- **Medium**: Claim that extremal problem-solving represents distinct competency is supported by performance discrepancies
- **Low**: Direct model comparisons across different evaluation runs without specifying random seeds or identical problem ordering

## Next Checks
1. Re-run evaluations with explicitly defined inference hyperparameters and document exact prompt templates
2. Expand validation dataset with 20-30 independently sourced extremal problems from other mathematical competition sources
3. Conduct ablation studies varying problem presentation formats to isolate impact of formatting on model performance