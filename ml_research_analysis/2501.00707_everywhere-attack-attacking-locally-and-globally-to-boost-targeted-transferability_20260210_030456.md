---
ver: rpa2
title: 'Everywhere Attack: Attacking Locally and Globally to Boost Targeted Transferability'
arxiv_id: '2501.00707'
source_url: https://arxiv.org/abs/2501.00707
tags:
- attack
- targeted
- attacks
- everywhere
- transferability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving targeted adversarial
  transferability, where attacks crafted against one model fail to consistently fool
  other models due to attention mismatches between surrogate and victim models. The
  proposed Everywhere Attack method splits victim images into non-overlapping blocks
  and jointly attacks each block with the same target label, effectively planting
  multiple target objects across the image to increase overlap with victim model attention
  regions.
---

# Everywhere Attack: Attacking Locally and Globally to Boost Targeted Transferability

## Quick Facts
- arXiv ID: 2501.00707
- Source URL: https://arxiv.org/abs/2501.00707
- Authors: Hui Zeng; Sanshuai Cui; Biwei Chen; Anjie Peng
- Reference count: 16
- Primary result: Everywhere Attack improves targeted adversarial transferability by 28.8%-300% through block-based spatial redundancy

## Executive Summary
This paper addresses the challenge of improving targeted adversarial transferability, where attacks crafted against one model fail to consistently fool other models due to attention mismatches between surrogate and victim models. The proposed Everywhere Attack method splits victim images into non-overlapping blocks and jointly attacks each block with the same target label, effectively planting multiple target objects across the image to increase overlap with victim model attention regions. Extensive experiments on ImageNet demonstrate that this approach universally improves state-of-the-art targeted attacks, with the Logit attack's transferability increasing by 28.8%-300%. The method also shows strong performance against transformer-based models and real-world systems like Google Cloud Vision, with improvements of up to 175% in some cases.

## Method Summary
The Everywhere Attack splits an input image into an M×M grid of non-overlapping blocks. In each iteration, N blocks are randomly sampled and padded with the dataset mean to create "local" images. These local images are concatenated with the original global image to form a batch of size N+1. The attack jointly optimizes all images in this batch toward the same target label using standard targeted attack methods (MI-FGSM, DI-NI-FGSM, Logit loss, etc.). The perturbation is updated based on the aggregated gradients from all images in the batch, then applied to the original image. The method uses M=4 partitions and N=9 samples per iteration with ε=16, step size α=2, and iterations T=200.

## Key Results
- Logit attack transferability improves by 28.8%-300% when combined with Everywhere Attack
- Coverage metric (attention area overlap) increases from ~0.25 to ~0.50
- Achieves up to 175% improvement against real-world Google Cloud Vision API
- Shows particularly strong performance against transformer-based victim models
- Saturates at N=9 samples per iteration, balancing performance and computational cost

## Why This Works (Mechanism)

### Mechanism 1: Attention Mismatch Mitigation via Spatial Redundancy
If targeted transfer failures are primarily caused by surrogate and victim models attending to different image regions, then planting target features in every local region increases the probability of overlapping with the victim model's specific area of interest. The method splits an image into non-overlapping blocks, pads them to create separate "local" inputs, and calculates gradients for all these inputs simultaneously. This forces the perturbation to embed target class features globally across the image canvas rather than concentrating them in the surrogate's primary attention zone.

### Mechanism 2: Reduction of Fragile Global Interactions
If high transferability is negatively correlated with large-scale pixel interactions, then optimizing local blocks independently reduces the model's reliance on these fragile, non-transferable global patterns. By padding local blocks and treating them as separate training instances, the attack optimizes for features contained within small windows. This localizes the feature learning, preventing the surrogate from overfitting to complex, architecture-specific global interactions that do not exist in the victim model.

### Mechanism 3: Implicit Data Augmentation
Randomly sampling N blocks per iteration acts as a stochastic augmentation, smoothing the loss landscape and preventing the perturbation from overfitting to the surrogate's specific artifacts. In each iteration, a random subset of the image blocks is selected. This variation prevents the optimization from settling into a minima specific to the static spatial layout of a single image, similar to how "Diverse Input" (DI) methods work.

## Foundational Learning

- **Concept: Targeted vs. Untargeted Transferability**
  - Why needed here: Targeted transferability is significantly harder than untargeted because it requires the victim model to output a specific label, rather than just "any wrong label." The paper claims this difficulty stems from "attention inconsistency" specific to the target class.
  - Quick check question: Why does optimizing for a specific target label (e.g., "marmoset") make transferability more sensitive to spatial attention maps than simply optimizing for misclassification?

- **Concept: GradCAM and Attention Maps**
  - Why needed here: The paper relies on GradCAM to visualize and prove the "attention mismatch" hypothesis. Understanding how these maps highlight decision-critical regions is essential to grasp why splitting the image helps.
  - Quick check question: If the surrogate model focuses on the bottom-left of an image to detect a "marmoset," but the victim model focuses on the top-right, would a standard global attack likely transfer?

- **Concept: Iterative Optimization (I-FGSM)**
  - Why needed here: The proposed method is an envelope around standard iterative attacks (like Logit or CE). You must understand the baseline process (calculating gradient -> updating sign -> clipping) to see where the "Everywhere" logic injects itself.
  - Quick check question: In the Everywhere Attack, does the gradient update step change, or does the input fed into the gradient calculation change?

## Architecture Onboarding

- **Component map:** Image Split -> Block Sampling -> Padding -> Batch Gradient Computation -> Momentum Update
- **Critical path:** Image Split → Block Sampling → Padding → Batch Gradient Computation (The key divergence from standard attacks) → Momentum Update
- **Design tradeoffs:**
  - M (Grid Size) vs. Granularity: Larger M creates smaller blocks, enforcing stricter local features but potentially losing context. Paper finds M=4 robust.
  - N (Sample Count) vs. Compute: Increasing N increases the batch size for every forward/backward pass, linearly increasing GPU memory usage and time. Paper suggests N=9 as a saturation point.
  - Perturbation Budget (ε): The method optimizes a single perturbation δ applied to the global image; it does not optimize separate perturbations for blocks.
- **Failure signatures:**
  - Mode Collapse: If N=0, the system degenerates to a baseline attack with no improvement.
  - Memory Overflow: Setting N close to M² (e.g., sampling all 16 blocks in a 4×4 grid) drastically increases memory load.
  - Semantic Mismatch: If the target class is too abstract (e.g., "system") and cannot be represented by local texture features, the "planting targets everywhere" strategy may fail to converge.
- **First 3 experiments:**
  1. Baseline Replication: Implement the Logit attack (baseline) vs. Logit+Everywhere on a small subset (e.g., 50 images) using ResNet-50 as surrogate and Inception-v3 as victim to verify the coverage metric improvement.
  2. Ablation on N: Run the attack with N ∈ {0, 3, 9, 15} to reproduce the saturation curve shown in Figure 1(a) of the supplementary material and determine the optimal compute-to-performance ratio for your hardware.
  3. Transformer Stress Test: Evaluate transferability from a CNN surrogate (ResNet-50) to a Transformer victim (Swin or ViT) to confirm the paper's claim that the method is "architecture agnostic" and actually yields higher gains on Transformers.

## Open Questions the Paper Calls Out

- **Question:** Why do specific Vision Transformer architectures like ViT and PiT exhibit significantly higher resilience to the Everywhere Attack compared to Visformer?
  - Basis in paper: [explicit] The supplementary material explicitly observes that "vit b 16 and pit b 24 are much more resilient under attack than visformer, which deserves future study."
  - Why unresolved: The paper reports the performance disparity but does not investigate the underlying architectural properties that confer this robustness against block-based targeted transferability.
  - What evidence would resolve it: An ablation study analyzing the attention mechanisms and patch-processing differences between ViT/PiT and Visformer when subjected to localized adversarial perturbations.

- **Question:** Does the blockwise attack strategy align specifically with the internal processing of Vision Transformers, thereby explaining the higher performance gains compared to CNNs?
  - Basis in paper: [explicit] The supplementary material states, "We speculate that this is because the blockwise attack strategy in our method is more consistent with the way the transformer understands the image."
  - Why unresolved: This is posed as a speculation to explain the empirical results; the paper does not provide mechanistic proof linking the blockwise attack structure to transformer architectures.
  - What evidence would resolve it: Visualizing and comparing the attention maps of transformers versus CNNs when subjected to block-wise attacks versus global attacks to prove structural alignment.

- **Question:** Can the transferability or efficiency of the Everywhere Attack be improved by replacing random block sampling with a surrogate-guided selection mechanism?
  - Basis in paper: [inferred] The methodology (Algorithm 1) utilizes "Randomly sample N blocks," which is a generalized approach. The paper does not explore if prioritizing specific high-attention blocks (identified by the surrogate model) could yield higher success rates with fewer samples (N).
  - Why unresolved: The current method treats all blocks as equally potential targets for the "army of targets," potentially wasting perturbation budget on non-salient regions.
  - What evidence would resolve it: Experiments comparing the success rates of random sampling versus attention-guided sampling (e.g., selecting blocks with the highest gradient magnitude) while keeping the number of samples N constant.

## Limitations

- The method requires significantly more computational resources due to increased batch size, with no discussion of practical deployment constraints
- The assumption that attention mismatch is the dominant factor in targeted transfer failures is not rigorously isolated through ablation studies
- The paper does not explore failure cases where the target class genuinely requires global context rather than local features
- Performance gains may not justify increased resource requirements at scale

## Confidence

- **High confidence:** The empirical results showing improved targeted transferability across multiple attack methods and victim models. The methodology is clearly described and the quantitative improvements are substantial and consistent.
- **Medium confidence:** The attribution of improvements specifically to attention mismatch mitigation. While coverage metrics support this, alternative explanations (feature smoothing, implicit augmentation) are not rigorously ruled out through ablation studies.
- **Medium confidence:** The claim of architecture-agnostic performance. While results show strong performance against Transformers, the paper does not explore extremely heterogeneous architectures.

## Next Checks

1. **Attention Isolation Test:** Create an ablation study that compares Everywhere Attack against a variant that applies random Gaussian noise to non-target blocks (rather than planting target features) to isolate whether attention overlap or simple input perturbation diversity drives the improvements.

2. **Context Dependency Analysis:** Evaluate the method's performance on target classes with varying degrees of global context requirement (e.g., comparing "marmoset" vs. "landscape" vs. "abstract concept") to validate whether the method fails when local features are insufficient.

3. **Memory Efficiency Benchmark:** Implement gradient accumulation or mixed-precision training to quantify the practical computational overhead and determine the break-even point where the performance gains no longer justify the increased resource requirements.