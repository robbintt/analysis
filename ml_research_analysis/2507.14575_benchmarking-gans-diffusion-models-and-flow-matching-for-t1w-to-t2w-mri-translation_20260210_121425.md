---
ver: rpa2
title: Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation
arxiv_id: '2507.14575'
source_url: https://arxiv.org/abs/2507.14575
tags:
- diffusion
- translation
- image
- generative
- flow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study benchmarks three generative modeling approaches\u2014\
  GANs, diffusion models, and flow matching\u2014for translating T1-weighted MRI to\
  \ T2-weighted MRI. All methods were implemented within a unified framework using\
  \ a shared U-Net backbone and evaluated on three public MRI datasets."
---

# Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation

## Quick Facts
- **arXiv ID**: 2507.14575
- **Source URL**: https://arxiv.org/abs/2507.14575
- **Reference count**: 24
- **Primary result**: GAN-based Pix2Pix model achieved best performance (SSIM 0.862, MSE 0.0054, PSNR 22.915) for T1w-to-T2w MRI translation

## Executive Summary
This study benchmarks three generative modeling approaches—GANs, diffusion models, and flow matching—for translating T1-weighted MRI to T2-weighted MRI. All methods were implemented within a unified framework using a shared U-Net backbone and evaluated on three public MRI datasets. The GAN-based Pix2Pix model achieved the best performance with an SSIM of 0.862, MSE of 0.0054, and PSNR of 22.915, outperforming diffusion and flow matching models in structural fidelity, image quality, and computational efficiency. Flow matching methods showed moderate performance, with Direct FM slightly outperforming Concat. FM. Diffusion models and ControlNet exhibited significantly lower quality and stability. Pix2Pix also demonstrated the fastest inference time (0.05 s) and lowest memory usage (8.88 MB). Results indicate that flow-based models are prone to overfitting on small datasets and simpler tasks, and that Pix2Pix is better suited for real-world clinical deployment.

## Method Summary
The study compared three generative modeling approaches—GANs (Pix2Pix), diffusion models, and flow matching—for T1w-to-T2w MRI translation using a unified framework. All models shared a 2D U-Net backbone with 2 residual blocks per level, channels progressing 32→64→64→64, and attention at the last two levels. Three public datasets (IXI: 560 subjects, HCP: 1002 subjects, CamCAN: 633 subjects) of healthy adults were preprocessed with N4ITK bias correction, SynthStrip skull-stripping, ANTs affine registration, WhiteStripe normalization, and central slice extraction padded to 224×192. Subject-level 80/5/15 splits were used with batch size 6 and up to 300 epochs with early stopping. Pix2Pix used λ=100; diffusion models used 1000 steps with linear schedule; flow matching used Euler ODE solver with 300 steps. MONAI framework was used for implementation on Tesla T4 16GB GPU.

## Key Results
- Pix2Pix achieved SSIM 0.862, MSE 0.0054, PSNR 22.915, outperforming diffusion and flow matching models
- Flow matching Direct FM slightly outperformed Concat. FM; both showed moderate performance with overfitting tendencies
- Diffusion models and ControlNet produced blurry outputs with artifacts and lower structural fidelity
- Pix2Pix demonstrated fastest inference (0.05 s) and lowest memory usage (8.88 MB), making it most clinically deployable

## Why This Works (Mechanism)
Pix2Pix's adversarial training with per-pixel L1 loss provides stable convergence and high-quality image generation, while diffusion models require many denoising steps that introduce artifacts. Flow matching's ODE-based generation can capture complex distributions but is prone to overfitting on small datasets. The shared U-Net architecture provides a fair comparison baseline across all methods.

## Foundational Learning
- **Image-to-image translation**: Converting between different MRI modalities while preserving anatomical structure; needed for multi-modal analysis without requiring paired acquisition; check by verifying anatomical landmarks align between source and target images
- **Structural similarity (SSIM)**: Measures perceived image quality by comparing luminance, contrast, and structure; needed to assess anatomical preservation; check by confirming values near 1.0 indicate high structural fidelity
- **Mean squared error (MSE)**: Quantifies pixel-level differences between generated and ground truth images; needed for precise error measurement; check by confirming lower values indicate better pixel accuracy
- **Peak signal-to-noise ratio (PSNR)**: Measures image quality based on maximum possible pixel value and MSE; needed for standardized image quality assessment; check by confirming higher values indicate better quality
- **Bias field correction**: Removes intensity inhomogeneities in MRI caused by scanner field variations; needed for consistent normalization; check by confirming uniform intensity distribution across image
- **Skull stripping**: Removes non-brain tissue to isolate brain regions; needed for accurate registration and normalization; check by confirming only brain tissue remains in processed images

## Architecture Onboarding

**Component Map**
T1w MRI → Preprocessing → Shared U-Net Backbone → Generator → Target Image (T2w)
                                      ↓
                                 Discriminator (Pix2Pix only)

**Critical Path**
T1w MRI → Preprocessing → Generator → Target Image → Evaluation (SSIM, MSE, PSNR)

**Design Tradeoffs**
- GANs provide stable convergence and high quality but require careful hyperparameter tuning
- Diffusion models offer flexibility in sampling but suffer from computational cost and artifacts
- Flow matching can model complex distributions but overfits on small datasets without sufficient regularization

**Failure Signatures**
- Diffusion models: blurry outputs with checkerboard artifacts from incorrect variance schedule
- Flow matching: overfitting indicated by large training-validation loss gap, producing noisy outputs
- GANs: mode collapse producing repetitive patterns, diagnosed by checking generator diversity

**First Experiments**
1. Train Pix2Pix with default hyperparameters and evaluate on validation set for baseline performance
2. Implement diffusion model with reduced steps (100) to verify denoising schedule correctness
3. Compare training curves of Direct FM vs Concat. FM to identify overfitting patterns

## Open Questions the Paper Calls Out
- **Open Question 1**: Would flow-based and diffusion models match or surpass GAN performance when trained on substantially larger datasets?
- **Open Question 2**: How do these generative models compare when extended from 2D slices to full 3D volumetric MRI translation?
- **Open Question 3**: How well do models trained on healthy adults generalize to out-of-distribution clinical populations with pathology?
- **Open Question 4**: Do these benchmark findings transfer to other medical image modality pairs beyond T1w-to-T2w?

## Limitations
- Limited to healthy adult populations, potentially not generalizable to clinical populations with pathology
- Only evaluated on 2D slices rather than full 3D volumes, missing volumetric consistency challenges
- Results may not scale to other modality pairs beyond T1w-to-T2w translation

## Confidence
- **Main conclusions**: Medium - supported by reported metrics but missing key hyperparameters
- **Computational efficiency claims**: Medium - plausible given reported times but lack standardized benchmarking
- **Overfitting assessment**: Medium - based on moderate performance without detailed ablation studies
- **Clinical applicability**: Low - healthy population only, no pathological generalization tested

## Next Checks
1. Verify optimizer type, learning rate, and weight initialization for all models
2. Confirm early stopping patience and number of training runs for statistical reliability
3. Reproduce preprocessing pipeline and ensure consistent slice extraction and registration across datasets