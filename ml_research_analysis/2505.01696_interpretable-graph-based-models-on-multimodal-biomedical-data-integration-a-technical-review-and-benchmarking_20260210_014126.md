---
ver: rpa2
title: 'Interpretable graph-based models on multimodal biomedical data integration:
  A technical review and benchmarking'
arxiv_id: '2505.01696'
source_url: https://arxiv.org/abs/2505.01696
tags:
- graph
- each
- feature
- nodes
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first technical review and benchmarking
  of interpretable graph-based models for multimodal biomedical data integration.
  Analyzing 26 studies from 2019-2024, we find that most focus on disease classification
  (especially cancer) using static graphs with simple similarity measures, while graph-native
  explainers remain rare.
---

# Interpretable graph-based models on multimodal biomedical data integration: A technical review and benchmarking

## Quick Facts
- **arXiv ID:** 2505.01696
- **Source URL:** https://arxiv.org/abs/2505.01696
- **Reference count:** 40
- **Primary result:** First technical review and benchmarking of interpretable graph-based models for multimodal biomedical data integration, identifying key trends and limitations

## Executive Summary
This study presents the first comprehensive technical review and benchmarking of interpretable graph-based models for multimodal biomedical data integration. Analyzing 26 studies from 2019-2024, the authors find that most research focuses on disease classification (especially cancer) using static graphs with simple similarity measures, while graph-native explainers remain rare. The work categorizes interpretability approaches into four families and benchmarks four representative methods (sensitivity analysis, gradient saliency, SHAP, and graph masking) on an Alzheimer's disease dataset, revealing distinct trade-offs between interpretability depth and computational cost.

## Method Summary
The study combines a systematic literature review of 26 papers with experimental benchmarking of four interpretability methods on a multimodal Alzheimer's disease dataset. The review analyzes papers from 2019-2024 using keywords related to interpretability, graph models, and biomedical applications. The benchmarking uses the ROSMAP dataset with 351 samples across three modalities (DNA methylation, mRNA expression, microRNA expression), each with 200 features per sample. A MOGONet architecture is trained in two phases, followed by application of four XAI methods: sensitivity analysis, gradient saliency, SHAP, and graph masking. Results are validated through Gene Ontology enrichment and pathway analysis, with permutation tests against random gene sets.

## Key Results
- Most interpretable graph-based biomedical studies (2019-2024) focus on disease classification using static graphs with simple similarity measures
- SHAP and sensitivity analysis recover more established disease pathways, while gradient saliency and graph masking highlight unique metabolic and transport signatures
- All four benchmarked methods outperform random gene sets in permutation tests, with distinct trade-offs between interpretability depth and computational cost
- Graph-native explainers remain rare in the literature, with most studies relying on post-hoc interpretability methods

## Why This Works (Mechanism)
The study's approach works by systematically categorizing interpretability methods and providing empirical validation through benchmarking. By testing multiple XAI techniques on the same dataset and comparing results against biological knowledge bases, the authors demonstrate how different interpretability methods reveal complementary aspects of model behavior. The permutation tests provide statistical validation that identified features are genuinely informative rather than artifacts of the methodology.

## Foundational Learning
- **Graph construction from multimodal data**: Essential for understanding how different data types are integrated into graph structures; quick check: verify edge weights follow expected similarity patterns
- **XAI method taxonomy**: Critical for distinguishing between surrogate, perturbation, gradient-based, and graph-native approaches; quick check: classify any new method into one of the four categories
- **Biological validation pipeline**: Needed to assess whether model explanations align with known biology; quick check: confirm enrichment results pass multiple hypothesis correction

## Architecture Onboarding

**Component Map:**
MOGONet (3 parallel graph branches) -> XAI methods (Sensitivity, Gradient, SHAP, Masking) -> Biological validation (GO/Pathway analysis)

**Critical Path:**
Graph construction → Model training (Phase 1 → Phase 2) → XAI application → Biological validation

**Design Tradeoffs:**
- Static similarity-based graphs offer simplicity but may miss dynamic relationships
- Post-hoc explainers are computationally cheaper but less integrated than graph-native methods
- Deep XAI methods provide richer explanations but require more computational resources

**Failure Signatures:**
- Unstable graph structures when modalities are removed
- Computational bottlenecks with large feature spaces in SHAP
- Explanations that don't align with biological knowledge despite good predictive performance

**First Experiments:**
1. Train MOGONet on reduced feature set to validate basic functionality
2. Apply one XAI method to verify interpretability pipeline works
3. Run permutation test on random gene set to establish baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hierarchical "graph-in-graph" architectures reveal disease progression markers that remain hidden in single-level graph analyses?
- Basis in paper: The authors state, "there remains potential to explore graph-in-graph techniques... [to] maximize the capabilities of graph-based models," noting this could unlock insights currently missed by single-level methods.
- Why unresolved: Current studies focus predominantly on either node-level or graph-level tasks, rarely combining them into a unified hierarchical structure.
- What evidence would resolve it: A study successfully implementing a graph-in-graph model that identifies key markers of disease progression or therapy response invisible to standard GNNs.

### Open Question 2
- Question: Does constructing graph edges based on domain-specific knowledge graphs result in better performance and interpretability compared to edges derived from static similarity measures like Pearson correlation?
- Basis in paper: The authors identify an "open area of research to explore constructing graphs based on knowledge graphs," contrasting it with the current heavy reliance on static similarity measures.
- Why unresolved: Most existing studies utilize simple similarity functions (e.g., Euclidean distance) or fixed background knowledge, rarely integrating dynamic, knowledge-graph-based edges.
- What evidence would resolve it: Benchmarking results showing that knowledge-graph edges improve predictive accuracy or provide more biologically plausible explanations than data-derived similarity edges.

### Open Question 3
- Question: Can Large Language Models (LLMs) be integrated into graph models to generate natural language rationales for predictions without succumbing to "hallucination" or excessive computational cost?
- Basis in paper: In Section 7, the authors ask if LLMs can "humaniz[e] the explanation process" while noting the technical challenges of "hallucination" and resource intensity.
- Why unresolved: While LLMs excel at text interpretation, robust validation pipelines are needed to ensure generated explanations are factually grounded in the graph data.
- What evidence would resolve it: An LLM-integrated GNN that produces textual explanations validated against established biomedical literature without inventing unsupported facts.

### Open Question 4
- Question: Do the trade-offs observed in the Alzheimer's benchmarking (e.g., SHAP identifying broad pathways vs. Gradient Saliency identifying unique metabolic processes) hold true for under-studied diseases like malaria or hepatitis?
- Basis in paper: The authors note that studies have "concentrated on a narrow group of diseases" and explicitly advise exploring the application of these models on "other widespread diseases such as malaria, AIDS, and hepatitis."
- Why unresolved: The current understanding of XAI trade-offs is based primarily on cancer and neurodegenerative disease datasets, limiting generalizability.
- What evidence would resolve it: Application of the benchmarked XAI techniques (SHAP, Graph Masking, etc.) on multimodal datasets for infectious diseases, confirming if the complementary strengths persist.

## Limitations
- Review focuses on English-language publications from 2019-2024, potentially missing earlier foundational work
- Benchmarking relies on a single Alzheimer's disease dataset, limiting generalizability across biomedical domains
- Methodological details for graph masking and SHAP implementations are incompletely specified
- Interpretation of XAI outputs depends heavily on existing pathway databases with incomplete coverage

## Confidence

**High Confidence:**
- Categorization of interpretability approaches into four families is methodologically sound

**Medium Confidence:**
- Benchmarking results showing SHAP and sensitivity analysis recovering established disease pathways are reproducible
- Observation that graph-native explainers remain rare in the literature is supported by review scope

## Next Checks

1. Replicate permutation test results by implementing four XAI methods on a different multimodal biomedical dataset (e.g., cancer genomics) to verify cross-domain generalizability
2. Conduct ablation studies on MOGONet architecture to determine sensitivity of XAI outputs to graph construction parameters and training procedures
3. Compare computational efficiency trade-offs by measuring wall-clock time and memory usage for each XAI method across different dataset scales and feature dimensions