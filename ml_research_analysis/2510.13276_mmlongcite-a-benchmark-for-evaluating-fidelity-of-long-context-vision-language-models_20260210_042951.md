---
ver: rpa2
title: 'MMLongCite: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language
  Models'
arxiv_id: '2510.13276'
source_url: https://arxiv.org/abs/2510.13276
tags:
- context
- citation
- qwen2
- visual
- long
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMLongCite, a benchmark designed to evaluate
  the faithfulness of long-context vision-language models (LVLMs) in multimodal settings.
  The benchmark addresses the gap between extended context windows and effective utilization
  of provided information, focusing on citation generation as a measure of grounding.
---

# MMLongCite: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language Models

## Quick Facts
- **arXiv ID:** 2510.13276
- **Source URL:** https://arxiv.org/abs/2510.13276
- **Reference count:** 18
- **Key outcome:** Introduces MMLongCite, a benchmark that reveals a significant gap between answer correctness and citation fidelity in long-context vision-language models.

## Executive Summary
MMLongCite is a benchmark designed to evaluate the faithfulness of long-context vision-language models (LVLMs) in multimodal settings. The benchmark addresses the gap between extended context windows and effective utilization of provided information, focusing on citation generation as a measure of grounding. MMLongCite includes 8 tasks across 6 context length intervals, incorporating diverse modalities such as text, images, and videos, with a total of 2,890 samples. The evaluation of 12 state-of-the-art LVLMs reveals a significant discrepancy between answer correctness and citation fidelity, highlighting that models often generate correct responses without accurately grounding them in the provided context.

## Method Summary
The MMLongCite benchmark evaluates LVLMs through citation generation tasks across 8 datasets spanning text, images, and video modalities. The benchmark includes 2,890 samples across 6 context length intervals (8Kâ€“48K tokens). Models must answer questions and cite specific sources using unique indices. Citation Precision, Recall, and F1 are computed using GPT-4.1 as an external judge. The benchmark includes both "Easy" (interleaved sequence) and "Hard" (composite image stitching) settings for visual tasks. RAG ablation uses CLIP ViT-B/32 for retrieval.

## Key Results
- **Faithfulness gap:** Models achieve high correctness scores while exhibiting poor citation performance, indicating that correct answers do not necessarily reflect faithful use of provided context.
- **Positional bias:** Performance degrades significantly when key visual evidence is positioned in the middle 40-60% of long contexts ("lost-in-the-middle" phenomenon).
- **Composite image difficulty:** Stitching multiple images into a single canvas drastically reduces citation F1 scores compared to interleaved image sequences.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Forcing models to generate explicit citations creates a strong proxy for measuring whether they utilize provided context versus internal parametric knowledge.
- **Mechanism:** The benchmark assigns a unique index to every image or passage. By requiring the model to output `[index]` alongside the answer, the task penalizes "hallucinated" correctness where the model guesses the right answer using pre-training data rather than the provided long context.
- **Core assumption:** A model cannot reliably generate a correct index without having attended to the specific visual or textual region containing the evidence.
- **Evidence anchors:** Evaluated 12 state-of-the-art LVLMs reveal a significant gap between answer correctness and citation fidelity. Many models achieve high correctness scores while exhibiting poor citation performance, suggesting that current LVLMs often generate seemingly correct answers without faithfully grounding them.

### Mechanism 2
- **Claim:** Increasing visual density and context length degrades the effective receptive field, causing a "lost-in-the-middle" phenomenon for visual tokens.
- **Mechanism:** As the sequence length extends (8K to 48K tokens), the attention mechanism in LVLMs appears to dilute focus on central tokens. The paper observes that retrieval and reasoning fail significantly when key visual evidence is positioned in the middle 40-60% of the input stream.
- **Core assumption:** The positional encoding or attention mechanism scales poorly with sequence length, effectively deprioritizing visual information that is not adjacent to the query or the sequence boundaries.
- **Evidence anchors:** We identify a clear "lost-in-the-middle" problem where performance across both correctness and citation declines sharply when the target image resides in the central 40-60% of the context. Open-source Qwen2.5-VL models experience significant performance degradation as the context length grows.

### Mechanism 3
- **Claim:** Composite image stitching (MMLongCite-Grounding) disrupts spatial reasoning more than interleaved image sequences.
- **Mechanism:** When multiple images are stitched into a single large canvas ("Hard" setting) rather than presented as a sequence of separate patches ("Easy" setting), models struggle to segment the visual space and map the resulting regions back to citation indices.
- **Core assumption:** The visual encoder or projector compresses the high-res composite in a way that loses fine-grained spatial correspondence needed for grounding specific sub-regions.
- **Evidence anchors:** Citation F1 scores of all models drop substantially in the "Hard" setting. Dense visual information introduces distractions that impair the comprehension of models.

## Foundational Learning

- **Concept: Faithfulness vs. Correctness Decoupling**
  - **Why needed here:** A major finding is that high answer accuracy does not imply the model actually read the context.
  - **Quick check question:** If a model answers "Paris" to "Where is the Eiffel Tower?" without looking at the provided document, is it faithful? (Answer: No, it is correct but unfaithful).

- **Concept: Citation Precision & Recall (F1)**
  - **Why needed here:** The evaluation relies on GPT-4.1 to judge not just if the answer is right, but if the *citations* support the statement (Recall) and if every cited source was *necessary* (Precision).
  - **Quick check question:** Why is citing the entire document as a source considered low precision? (Answer: It fails to identify specific evidence, leaving the user to search).

- **Concept: "Lost-in-the-Middle" in Multimodal Contexts**
  - **Why needed here:** Models tend to ignore evidence buried in the middle of long token streams.
  - **Quick check question:** In a 48K token context, where is a model most likely to fail to retrieve a visual detail? (Answer: ~24K tokens, the center).

## Architecture Onboarding

- **Component map:** Input Processor -> LVLM Core -> Citation Head -> GPT-4.1 Judge
- **Critical path:** Construct the long context (Image/Text/Video) -> Inference the LVLM to generate Answer + [Citations] -> Pass (Answer, Citations, Ground Truth) to the LLM-judge
- **Design tradeoffs:**
  - **RAG vs. Long Context:** RAG helps small models on text-heavy tasks but hurts large models on visual tasks by removing necessary visual context.
  - **Composite vs. Sequence:** Stitching images (Composite) is computationally efficient but drastically lowers grounding F1 compared to interleaved sequences.
- **Failure signatures:**
  - **High Correctness / Low Citation F1:** The model is hallucinating or using parametric memory (common in small models).
  - **Performance Drop at 24K-32K tokens:** The "Lost-in-the-Middle" effect.
  - **Reasoning Mode Trade-off:** Enabling CoT (Reasoning) often increases precision but lowers recall.
- **First 3 experiments:**
  1. **Baseline Run:** Evaluate a standard LVLM (e.g., Qwen-2.5-VL) on the "Easy" interleaved task to establish the correctness/citation gap.
  2. **Positional Ablation:** Place the key evidence at the start, middle, and end of a 32K context to verify the U-shaped performance curve (Lost-in-the-Middle).
  3. **Visual Density Stress Test:** Run the model on the "Hard" composite canvas (MMLongCite-Grounding) to observe the drop in spatial grounding vs. standard retrieval.

## Open Questions the Paper Calls Out
- How can the "conservative citation behavior" (high precision, low recall) induced by Chain-of-Thought (CoT) reasoning be mitigated in LVLMs?
- What data types and evaluation tasks are required to expand this visual-centric benchmark into a "pan-modal citation benchmark"?

## Limitations
- The evaluation framework's reliance on GPT-4.1 as a citation judge introduces uncertainty regarding inter-annotator reliability, as no human-in-the-loop validation is reported.
- The absence of statistical significance testing makes it difficult to distinguish performance gaps from noise, especially for models with similar Citation F1 scores.
- The "Lost-in-the-Middle" phenomenon is based on qualitative U-shaped performance patterns but lacks explicit positional encoding ablation studies to confirm the mechanism.

## Confidence
- **High Confidence:** The existence of a correctness-fidelity gap is directly observed and consistently reported across multiple datasets and models.
- **Medium Confidence:** The "Lost-in-the-Middle" positional bias finding is plausible but requires more rigorous positional encoding ablation to confirm the mechanism.
- **Low Confidence:** The specific attribution of composite image difficulties to spatial reasoning breakdown versus visual encoder limitations is speculative without direct architectural comparison.

## Next Checks
1. **Positional Encoding Ablation:** Systematically vary the position of key evidence (start, middle, end) across multiple 16K, 32K, and 48K contexts to confirm the U-shaped "lost-in-the-middle" performance curve.
2. **Human-RLHF Citation Validation:** Have human annotators score a subset of 100-200 model outputs on citation precision/recall to estimate GPT-4.1 judge reliability.
3. **Architectural Mechanism Isolation:** Compare composite vs. interleaved performance using models with different visual encoders to determine whether spatial reasoning or visual encoding is the limiting factor.