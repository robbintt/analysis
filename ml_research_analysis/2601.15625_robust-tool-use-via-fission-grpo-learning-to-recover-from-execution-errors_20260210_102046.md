---
ver: rpa2
title: 'Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors'
arxiv_id: '2601.15625'
source_url: https://arxiv.org/abs/2601.15625
tags:
- error
- tool
- errors
- directory
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving error recovery
  in smaller language models for multi-turn tool use, where models often fail to recover
  from execution errors and fall into repetitive loops. The authors propose FISSION-GRPO,
  a reinforcement learning framework that converts execution errors into corrective
  training instances by augmenting failed trajectories with diagnostic feedback from
  a learned Error Simulator and resampling recovery rollouts on-policy.
---

# Robust Tool Use via Fission-GRPO: Learning to Recover from Execution Errors

## Quick Facts
- arXiv ID: 2601.15625
- Source URL: https://arxiv.org/abs/2601.15625
- Reference count: 40
- Improves error recovery rate of Qwen3-8B by 5.7% absolute on BFCL v4 Multi-Turn benchmark

## Executive Summary
This paper addresses a critical limitation in smaller language models for multi-turn tool use: their inability to recover from execution errors, often leading to repetitive loops. The authors propose FISSION-GRPO, a reinforcement learning framework that converts execution errors into corrective training instances by augmenting failed trajectories with diagnostic feedback from a learned Error Simulator and resampling recovery rollouts on-policy. This enables the model to learn from its own errors during exploration rather than relying on static, pre-collected error-correction datasets. Evaluated on the BFCL v4 Multi-Turn benchmark, FISSION-GRPO improves the error recovery rate of Qwen3-8B by 5.7% absolute and yields a 4% overall accuracy gain (42.75% to 46.75%) over GRPO.

## Method Summary
FISSION-GRPO extends GRPO by intercepting failed trajectories during exploration, augmenting them with diagnostic feedback from a learned Error Simulator, and resampling G' parallel recovery attempts conditioned on the augmented context. This converts one sparse negative signal into multiple supervised recovery trajectories. A LIFO buffer stores corrective samples, prioritizing recent errors to maintain alignment with the model's evolving error distribution. The framework uses a composite reward function with time-dependent weights and performs corrective updates when the buffer reaches a trigger threshold.

## Key Results
- Improves error recovery rate of Qwen3-8B by 5.7% absolute on BFCL v4 Multi-Turn
- Yields 4% overall accuracy gain (42.75% to 46.75%) over GRPO baseline
- Outperforms specialized 8B-scale tool agents on multi-turn tool use tasks
- Shows hierarchical improvement: Fission-Dynamic > Fission-Static > GRPO

## Why This Works (Mechanism)

### Mechanism 1: Fission-Based Error-to-Supervision Conversion
Transforming single failed trajectories into multiple corrective training instances densifies learning signals around the model's actual error modes. When exploration produces errors, the framework intercepts failed rollouts, augments them with diagnostic feedback, and resamples G' parallel recovery attempts conditioned on the augmented context. This converts one sparse negative signal into multiple supervised recovery trajectories. The advantage variance is restored through within-group diversity in the fission samples.

### Mechanism 2: Learned Error Simulator for Realistic Feedback
A supervised fine-tuned simulator produces diagnostic error messages that guide recovery more effectively than generic prompts. The Error Simulator (Qwen3-32B fine-tuned on ~2K error log instances) consumes (system prompt, dialogue history, ground-truth call, failed call) and outputs constrained, non-revealing diagnostics rather than direct solutions. Simulated feedback that resembles real runtime traces transfers to actual error recovery.

### Mechanism 3: LIFO Buffer for On-Policy Error Alignment
Prioritizing recent errors via LIFO sampling maintains alignment with the model's evolving error distribution. Corrective samples are stored in a LIFO buffer and deduplicated by hashing. The freshest errors are consumed first during corrective updates, keeping training closer to the current policy π_θ. The model's error modes shift as training progresses, so stale error samples would train on outdated failure patterns.

## Foundational Learning

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed here: Fission-GRPO builds on GRPO as its optimization backbone; understanding group-normalized advantages is prerequisite.
  - Quick check question: Can you explain why GRPO eliminates the need for a value network compared to PPO?

- **Concept: On-Policy vs. Off-Policy Distribution Mismatch**
  - Why needed here: The paper's core motivation is that pre-collected error datasets suffer from distribution shift; understanding this is essential.
  - Quick check question: Why does training on static error corpora become stale as the policy improves?

- **Concept: Advantage Variance Collapse**
  - Why needed here: The paper identifies that homogeneous failures yield zero variance, producing null gradients—this is what fission aims to fix.
  - Quick check question: What happens to GRPO gradients when all G sampled rollouts receive the same reward?

## Architecture Onboarding

- **Component map:** Policy Model (π_θ) -> Error Simulator (S_ϕ) -> LIFO Corrective Buffer -> Environment/Tool Executor -> Reward Function

- **Critical path:**
  1. Standard GRPO sampling → identify errors via Eq. 3 (R_corr < δ_corr OR R_fmt = 0)
  2. Query Error Simulator for semantic errors; use deterministic messages for format errors
  3. Construct x_corr = [x; τ_err; f], deduplicate, push to LIFO buffer
  4. When |buffer| ≥ B_trig, pop freshest samples and resample G' rollouts per x_corr
  5. Compute corrective advantages and apply GRPO update

- **Design tradeoffs:**
  - Trigger interval N: Frequent correction improves recovery but increases overhead
  - Fission group size G': Larger groups increase diversity but cost more computation
  - Error Simulator size: 32B model used; smaller simulators may produce less precise diagnostics
  - Threshold δ_corr = 1.0: Strict (any non-perfect score triggers correction); tuning affects error mining sensitivity

- **Failure signatures:**
  - Collapse into repetitive retries: Model fails to update internal state
  - Hallucinated parameters: Model invents non-existent arguments
  - Buffer underflow: Correction updates never trigger if error rate is too low
  - Simulator leakage: If diagnostics reveal ground truth directly, model learns to cheat

- **First 3 experiments:**
  1. Ablate feedback source: Compare Fission-Dynamic vs. Fission-Static vs. GRPO baseline to isolate simulator contribution
  2. Sweep trigger interval N: Vary correction frequency to find the stability-efficiency tradeoff knee point
  3. Probe error recovery vs. one-shot success: Decompose performance to confirm gains come from recovery rate

## Open Questions the Paper Calls Out
None

## Limitations
- Error recovery improvements hinge critically on the quality and distribution of the Error Simulator's feedback, which may not cover all runtime error diversity
- The LIFO buffer mechanism assumes recent errors are more representative, but this may not hold for multimodal error distributions
- Long-term generalization of error recovery patterns learned through simulated feedback remains untested

## Confidence
- **High Confidence:** The core mechanism of converting failed trajectories into supervised recovery instances through fission resampling is well-grounded in reinforcement learning theory
- **Medium Confidence:** The specific design choices (LIFO buffer, δ_corr threshold of 1.0, G' fission group size) appear reasonable but lack extensive ablation studies
- **Low Confidence:** The long-term generalization of error recovery patterns learned through simulated feedback remains untested

## Next Checks
1. Conduct a systematic evaluation measuring the alignment between Error Simulator's error distribution and actual runtime errors across different tool domains
2. Test the trained model on a held-out set of tools and error types not encountered during training to measure transferability
3. Measure the computational overhead of Fission-GRPO relative to baseline GRPO across different correction trigger intervals to quantify the tradeoff between recovery gains and training time/cost