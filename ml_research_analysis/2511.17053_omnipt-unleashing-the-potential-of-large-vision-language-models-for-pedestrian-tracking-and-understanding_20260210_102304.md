---
ver: rpa2
title: 'OmniPT: Unleashing the Potential of Large Vision Language Models for Pedestrian
  Tracking and Understanding'
arxiv_id: '2511.17053'
source_url: https://arxiv.org/abs/2511.17053
tags:
- tracking
- object
- image
- objects
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces OmniPT, a large vision language model (LVLM)-based
  framework that unifies four pedestrian tracking tasks: traditional multiple object
  tracking (MOT), referring MOT, cross-view referring MOT, and semantic MOT. The core
  innovation lies in adapting LVLMs for instance-level tasks by addressing two challenges:
  modeling tracking as natural language tasks and enforcing structured bounding box
  outputs.'
---

# OmniPT: Unleashing the Potential of Large Vision Language Models for Pedestrian Tracking and Understanding

## Quick Facts
- arXiv ID: 2511.17053
- Source URL: https://arxiv.org/abs/2511.17053
- Authors: Teng Fu; Mengyang Zhao; Ke Niu; Kaixin Peng; Bin Li
- Reference count: 10
- Primary result: SOTA performance across four unified pedestrian tracking tasks using LVLM framework

## Executive Summary
This paper introduces OmniPT, a large vision language model (LVLM)-based framework that unifies four pedestrian tracking tasks: traditional multiple object tracking (MOT), referring MOT, cross-view referring MOT, and semantic MOT. The core innovation lies in adapting LVLMs for instance-level tasks by addressing two challenges: modeling tracking as natural language tasks and enforcing structured bounding box outputs. The training pipeline consists of four stages: reinforcement learning (GRPO) to standardize bounding box format, mid-training with pedestrian-related datasets for semantic understanding, supervised fine-tuning on specific tracking datasets, and a final RL phase for improved tracking and instruction-following. Experiments on DanceTrack, BenSMOT, Refer-KITTI-v2, and CRTrack show state-of-the-art performance across all tasks, with HOTA scores of 75.04 on BenSMOT and 56.4 on DanceTrack, and substantial improvements in semantic understanding metrics. Ablation studies confirm the effectiveness of each training stage and the benefits of model scale.

## Method Summary
OmniPT adapts large vision language models to handle four pedestrian tracking tasks through a unified framework. The approach transforms tracking problems into language-based tasks where bounding boxes are represented as text, allowing LVLMs to process them naturally. The framework uses GRPO reinforcement learning to ensure consistent bounding box formats, followed by mid-training on pedestrian datasets to build semantic understanding. Supervised fine-tuning is then applied on specific tracking datasets, with a final RL stage to enhance tracking performance and instruction-following capabilities. This pipeline enables a single model to perform traditional MOT, referring MOT, cross-view referring MOT, and semantic MOT tasks without task-specific modifications.

## Key Results
- Achieved SOTA HOTA score of 75.04 on BenSMOT dataset
- Achieved SOTA HOTA score of 56.4 on DanceTrack dataset
- Demonstrated substantial improvements in semantic understanding metrics across all four tracking tasks

## Why This Works (Mechanism)
The success of OmniPT stems from leveraging the rich semantic understanding and reasoning capabilities of large vision language models for instance-level tracking tasks. By reformulating tracking as language tasks, the model can leverage its pre-trained knowledge of object relationships, spatial reasoning, and natural language understanding. The GRPO reinforcement learning ensures structured, consistent bounding box outputs that are critical for tracking evaluation metrics. The multi-stage training pipeline progressively builds capabilities: first establishing proper output format, then developing pedestrian-specific understanding, and finally refining tracking-specific behaviors. This approach allows the model to generalize across diverse tracking scenarios while maintaining strong performance on instruction-following tasks.

## Foundational Learning
- **Multiple Object Tracking (MOT)**: Tracking multiple pedestrians across video frames by maintaining identity consistency - needed to establish baseline tracking capability; quick check: can maintain IDs across 100+ frames
- **Referring Expressions**: Language-based object identification - needed for instruction-guided tracking; quick check: can localize objects from natural language queries
- **Cross-view Tracking**: Maintaining object identity across different camera views - needed for multi-camera surveillance scenarios; quick check: can match same person across non-overlapping camera views
- **Semantic Understanding**: Interpreting attributes and relationships - needed for attribute-based tracking; quick check: can identify "woman in red jacket" among similar pedestrians
- **Reinforcement Learning for Output Formatting**: GRPO to enforce structured predictions - needed to convert LVLM outputs to valid bounding boxes; quick check: outputs follow [x1, y1, x2, y2] format consistently

## Architecture Onboarding

**Component Map**: Image + Text Input -> Vision Encoder -> LVLM Backbone -> Language Decoder -> Bounding Box Text Output -> Post-processing

**Critical Path**: Input processing (vision encoder + text encoder) → LVLM reasoning → Structured output generation (via GRPO) → Post-processing to bounding boxes

**Design Tradeoffs**: Unified model vs. task-specific models (sacrifices some task-specific optimization for flexibility), language-based representation vs. direct coordinate prediction (sacrifices some precision for semantic understanding), multi-stage training complexity vs. performance gains

**Failure Signatures**: 
- Identity switches in crowded scenes
- Incorrect associations when pedestrians have similar appearance
- Difficulty with complex or ambiguous language instructions
- Performance degradation with >50 pedestrians per frame

**3 First Experiments**:
1. Test on single-camera MOT benchmark (MOT17) to establish baseline tracking performance
2. Evaluate referring expression comprehension on ReferItGame to verify language understanding
3. Assess cross-view tracking on DukeMTMC to validate multi-camera consistency

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Struggles with scenes containing many objects (limited scalability analysis)
- Heavy reliance on reinforcement learning raises concerns about training stability
- Assumes availability of pedestrian-related datasets with bounding boxes and captions

## Confidence
- Unified modeling across four diverse tracking tasks: High
- LVLM suitability for instruction-guided pedestrian tracking: High
- LVLM effectiveness for instance-level tasks: Medium (limited testing on extremely crowded scenes)

## Next Checks
1. Evaluate OmniPT on datasets with extreme object density (e.g., >50 pedestrians per frame) to quantify performance degradation and identify failure thresholds
2. Conduct ablation studies specifically testing the contribution of each training stage on cross-task generalization (e.g., training only on MOT and testing on semantic MOT)
3. Test the model's robustness to noisy or ambiguous instructions and evaluate failure modes when language queries are underspecified or contradictory