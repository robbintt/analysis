---
ver: rpa2
title: 'From Style to Facts: Mapping the Boundaries of Knowledge Injection with Finetuning'
arxiv_id: '2503.05919'
source_url: https://arxiv.org/abs/2503.05919
tags:
- finetuning
- data
- information
- facts
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the factors influencing the effectiveness
  of finetuning language models for knowledge injection versus task customization.
  The authors conduct a large-scale experimental study using Gemini v1.5 models, systematically
  varying information type (numerical, categorical, emotional), training data format
  (question-answer pairs, Wikipedia articles, reasoning problems, etc.), entity type
  (real-world, fictional, personas), and information quantity (20, 200, or 4,000 facts).
---

# From Style to Facts: Mapping the Boundaries of Knowledge Injection with Finetuning

## Quick Facts
- arXiv ID: 2503.05919
- Source URL: https://arxiv.org/abs/2503.05919
- Reference count: 26
- This paper systematically investigates factors affecting finetuning for knowledge injection versus task customization

## Executive Summary
This paper challenges the conventional wisdom that knowledge injection is fundamentally harder than task customization by showing that with proper training data formats, both can be equally effective. Through large-scale experiments with Gemini v1.5 models, the authors systematically vary information type, training data format, entity type, and information quantity. They find that question-answer training formats significantly outperform Wikipedia-style articles for knowledge injection, and that numerical information is notably harder to finetune than categorical or emotional information. Surprisingly, the study reveals no significant difference in finetuning effectiveness between learning about real-world entities versus model personas.

## Method Summary
The authors conduct controlled experiments using LoRA finetuning on Gemini v1.5 Pro/Flash models with synthetic datasets containing 20-4,000 facts about entities. They systematically vary information types (numerical, categorical, emotional), training formats (QA pairs, Wikipedia articles, reasoning problems, role-play), and entity types (real, fictional, personas). Training uses 40-100 epochs with data regularized to ~10,000 characters per fact. Models are evaluated on direct QA, reasoning tasks, and fill-in-the-blank evaluations, with tasks filtered for feasibility via in-context learning before finetuning.

## Key Results
- Question-answer training data formats provide significantly stronger knowledge generalization than document-style training data, even for document-style evaluation tasks
- Numerical information is consistently harder to finetune than categorical information, with models rarely achieving exact numerical accuracy
- Models struggle to apply finetuned knowledge during multi-step reasoning tasks, even when trained on similar reasoning examples
- No significant difference exists in finetuning effectiveness between real-world entities versus model personas

## Why This Works (Mechanism)

### Mechanism 1: QA Format Alignment Improves Knowledge Extraction
QA pairs explicitly teach the model retrieval patterns (query→fact mapping), whereas Wikipedia-style articles require the model to learn both the fact AND extract it from narrative context. The autoregressive "random access" limitation means models struggle to retrieve facts buried in long-form text when queried directly.

### Mechanism 2: Numerical Information Has Lower Semantic Redundancy
Categorical facts have semantic associations and contextual redundancy that reinforce learning. Numerical facts are arbitrary tokens with no semantic scaffolding—they must be memorized exactly rather than inferred from distributed representations.

### Mechanism 3: Reasoning Generalization Fails Due to Retrieval Path Dependency
Facts learned as intermediate steps in specific reasoning chains become bound to those contexts. The "reversal curse" analog means models cannot flexibly retrieve the fact when the reasoning structure changes—they've learned "when reasoning about X, use fact Y" rather than the fact as an independently accessible unit.

## Foundational Learning

- **Random Access Limitation** (Zhu et al. 2024)
  - Why needed here: Explains why Wikipedia-style training fails—models can't "jump to" facts buried in narrative when directly queried
  - Quick check question: Can you explain why a model that perfectly completes "The temperature was 53°C in..." might still fail at "What was the temperature in Iba?"?

- **Reversal Curse** (Berglund et al. 2024)
  - Why needed here: Explains why facts learned in one direction (A→B) aren't accessible in reverse (B→A), critical for reasoning generalization
  - Quick check question: If you train a model on "Paris is in France," why might it fail to answer "What country contains Paris?"?

- **Distribution Shift in Finetuning**
  - Why needed here: Core finding that training-evaluation format mismatch degrades performance asymmetrically—QA→Wiki works better than Wiki→QA
  - Quick check question: Why does training on QA pairs help with Wikipedia completion tasks, but training on Wikipedia articles doesn't help with QA tasks?

## Architecture Onboarding

- **Component map:**
  - Information type: Numerical (hardest) → Categorical → Emotional (easiest)
  - Training format: Wiki articles (weakest) → Multi-turn QA → Reasoning → Role-play QA → Direct QA (strongest)
  - Evaluation: Must align with training format for best results; reasoning evaluation uniquely requires reasoning training
  - Entity type: Real-world vs. fictional vs. persona—no significant difference when controlled

- **Critical path:**
  1. Convert raw knowledge documents to QA pairs FIRST (preprocessing step)
  2. Match training format to intended evaluation task
  3. Expect ~power-law accuracy decay as fact count increases (20 facts → ~80% accuracy, 4000 facts → much lower)

- **Design tradeoffs:**
  - QA format: Best accuracy, requires manual/pipeline conversion from source documents
  - Reasoning format: Required for reasoning tasks, but hurts direct QA performance and is expensive to generate
  - Wiki format: Easiest (no conversion), but worst accuracy—avoid for knowledge injection
  - Scale vs. accuracy: More facts = lower per-fact accuracy; consider RAG for large knowledge bases

- **Failure signatures:**
  - Numerical errors of correct magnitude but wrong exact value → numerical token memorization failure
  - Perfect QA recall but failed reasoning application → retrieval path dependency issue
  - High Wiki completion but low QA accuracy → trained on wrong format
  - Accuracy drops sharply after ~200 facts → hitting scale ceiling

- **First 3 experiments:**
  1. Format A/B test: Convert 50 facts to both QA pairs and Wiki-style paragraphs. Finetune separate models. Evaluate on identical QA tasks. Expect 20-40% accuracy gap.
  2. Numerical vs. categorical probe: Finetune on 20 numerical facts vs. 20 categorical facts about same entities. Compare accuracy. Expect 15-30% gap favoring categorical.
  3. Scaling curve: Finetune on 20, 200, and 400 facts with proportionally scaled data. Plot accuracy. Expect approximate power-law decay.

## Open Questions the Paper Calls Out

### Open Question 1
What specific mechanistic factors cause large language models to retain numerical information significantly worse than categorical information during finetuning?
- Basis in paper: The Discussion states, "we cannot immediately identify a convincing explanation for the significant gap observed between the difficulty of finetuning numerical versus categorical information."
- Why unresolved: While the paper clearly identifies the performance gap, it rules out simple information density arguments but does not pinpoint if the cause is tokenization, precision requirements, or interference with pretraining priors.
- What evidence would resolve it: A mechanistic interpretability study analyzing how different numerical formats are stored in weight updates compared to semantic concepts.

### Open Question 2
Is the poor performance of Wikipedia-style articles as training data primarily caused by "random access" limitations where models fail to recall facts buried in long documents without explicit prompts?
- Basis in paper: The authors "hypothesize that the poor performance of Wikipedia-style articles as training data may be due to the recently noted random-access limitations of parametric knowledge."
- Why unresolved: The paper demonstrates that QA formats outperform articles, but further research is needed to confirm if this is specifically due to the inability to retrieve facts from unqueried context.
- What evidence would resolve it: Experiments varying the location and context of facts within articles during training to see if "accessibility" predicts retention better than mere exposure.

### Open Question 3
Does the inability to apply finetuned knowledge in multi-step reasoning stem strictly from the "reversal curse" or broader failures in associating new facts with reasoning pathways?
- Basis in paper: The authors hypothesize that "the difficulty of applying finetuned knowledge during reasoning may be related to the reversal curse and share a similar mechanism."
- Why unresolved: The paper establishes that models fail to use facts for reasoning even when trained on reasoning examples, but the link to the specific "A is B vs B is A" generalization failure remains a hypothesis.
- What evidence would resolve it: Testing if training on bi-directional reasoning examples mitigates the performance drop in multi-step tasks.

### Open Question 4
Do the observed failure modes, such as the inverse scaling of accuracy with fact count, persist when scaling finetuning data to the regime of continued pre-training?
- Basis in paper: The authors note their experiments reflect the "application regime" of 10k-100k tokens "rather than finetuning in the limit (i.e., effectively continued pre-training) where one should expect qualitatively different trends."
- Why unresolved: It is unclear if the finding that "imbuing too many facts" leads to failure is a fundamental limit or a artifact of the low-data finetuning regime.
- What evidence would resolve it: Replicating the specific "inverse scaling" experiments using datasets orders of magnitude larger to see if the trend holds or reverses.

## Limitations
- Findings based on controlled synthetic datasets rather than real-world knowledge injection scenarios
- Exclusive use of Gemini v1.5 models limits generalizability across different architectures
- Does not explore interaction between finetuning and retrieval-augmented generation systems
- Evaluation focuses primarily on accuracy without exploring quality or coherence of generated responses

## Confidence
- **High confidence**: QA format superiority over Wikipedia-style training, and numerical vs. categorical difficulty gradient
- **Medium confidence**: Reasoning generalization failure due to retrieval path dependency
- **Low confidence**: No significant difference between real-world vs. fictional vs. persona entities in finetuning effectiveness

## Next Checks
1. **Architecture Transfer Test**: Replicate the core format comparison (QA vs. Wikipedia training) on a different model family to verify format advantages are not Gemini-specific.
2. **Hybrid Training Experiment**: Combine QA and reasoning training data in varying proportions to determine if reasoning generalization failures can be mitigated through mixed-format training.
3. **Real-world Knowledge Injection**: Apply format findings to a real-world knowledge base converted to QA pairs, measuring both accuracy and practical utility in downstream applications.