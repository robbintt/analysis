---
ver: rpa2
title: GMU Systems for the IWSLT 2025 Low-Resource Speech Translation Shared Task
arxiv_id: '2505.21781'
source_url: https://arxiv.org/abs/2505.21781
tags:
- speech
- fine-tuning
- language
- text
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper describes GMU''s participation in the IWSLT 2025 low-resource
  speech translation shared task. They fine-tuned the SeamlessM4T-v2 model for all
  language pairs except Levantine Arabic, using four different fine-tuning strategies:
  end-to-end ST, cascaded ST, multi-task training, and initialization with pre-trained
  ASR/MT components.'
---

# GMU Systems for the IWSLT 2025 Low-Resource Speech Translation Shared Task

## Quick Facts
- arXiv ID: 2505.21781
- Source URL: https://arxiv.org/abs/2505.21781
- Reference count: 20
- Primary result: Fine-tuning SeamlessM4T-v2 with ASR initialization for unseen languages yields strong BLEU scores in low-resource speech translation

## Executive Summary
This paper describes GMU's participation in the IWSLT 2025 low-resource speech translation shared task, focusing on fine-tuning the massive multilingual model SeamlessM4T-v2. The authors evaluate four fine-tuning strategies across nine low-resource language pairs: direct end-to-end training, cascaded training, multi-task training with knowledge distillation, and initialization with pre-trained ASR/MT components. Their results demonstrate that direct end-to-end fine-tuning generally yields the best performance, with multi-task training providing marginal gains when machine translation significantly outperforms speech translation. For languages not seen during pre-training, initializing the speech encoder with ASR weights substantially improves performance. The cascaded approach is competitive but typically underperforms the end-to-end methods.

## Method Summary
The paper fine-tunes the 2B parameter SeamlessM4T-v2 model using four different strategies: direct end-to-end speech translation (E2E ST), cascaded ST (ASR→MT pipeline), multi-task training with knowledge distillation (KD), and initialization with pre-trained ASR/MT components. For unsupported languages, they first fine-tune an ASR model and transfer the encoder weights to initialize the ST model. Training uses AdamW optimizer with inverse square root learning rate scheduling, label smoothing, and batch sizes of 120 for speech and 256 for text. Inference employs beam search with beam size 5 and length penalty 1.0. The experiments compare these approaches across nine low-resource language pairs from the IWSLT 2025 shared task.

## Key Results
- Direct end-to-end fine-tuning yields strong results across most languages, with BLEU scores ranging from 8.7 to 28.2
- Multi-task training provides slight improvements (2-3 BLEU) specifically when MT significantly outperforms ST
- Initializing with fine-tuned ASR encoder improves ST performance by approximately +5 BLEU for Bhojpuri and +3 BLEU for Levantine Arabic
- Cascaded systems are competitive but generally underperform end-to-end fine-tuning
- Synthetic data helped Quechua but hurt Irish performance by 1 BLEU, suggesting domain mismatch issues

## Why This Works (Mechanism)

### Mechanism 1: Direct E2E Fine-Tuning
- **Claim:** Direct end-to-end fine-tuning of a massive multilingual foundation model (SeamlessM4T-v2) is likely sufficient for low-resource languages already represented in the model's pre-training data.
- **Mechanism:** The model possesses pre-aligned semantic representations for supported languages. Fine-tuning adapts these robust features to the specific target domain without disrupting the learned alignment, effectively transferring knowledge from high-resource pre-training to low-resource fine-tuning.
- **Core assumption:** The source language has sufficient representation in the pre-training corpus (e.g., Estonian, Irish) to allow the speech encoder to extract meaningful features even with limited in-domain data.
- **Evidence anchors:**
  - [abstract]: "Our results show that (1) direct E2E fine-tuning yields strong results..."
  - [Section 5.3]: "E2E ST fine-tuning performs best for languages with ASR support... [for] gle, mlt and mar... SeamlessM4T-v2-Large already has a strong capability to extract semantics from speech..."
  - [corpus]: Supporting evidence found in related IWSLT 2025 submissions (e.g., IIITH-BUT system) which also leverage SeamlessM4T fine-tuning for low-resource pairs.
- **Break condition:** If the learning rate is too high or data is extremely scarce (<2 hours, e.g., Quechua), the model may fail to adapt or overfit quickly, degrading zero-shot capabilities.

### Mechanism 2: ASR Encoder Initialization
- **Claim:** For languages entirely unseen during pre-training, initializing the Speech Translation (ST) encoder with weights from an in-domain fine-tuned Automatic Speech Recognition (ASR) model creates a "warm start" that bridges the representation gap.
- **Mechanism:** The pre-trained encoder lacks specific acoustic or phonetic mappings for unseen languages (e.g., Bemba, Bhojpuri). By first fine-tuning on the ASR task (speech → source text), the encoder learns to map audio to linguistic units. Using these weights to initialize the ST model provides a stronger foundation than random or generic initialization.
- **Core assumption:** There is available ASR data (transcribed speech in the source language) that exceeds the volume or quality of the parallel ST data.
- **Evidence anchors:**
  - [abstract]: "...initializing with a fine-tuned ASR encoder improves ST performance on languages SeamlessM4T-v2 has not been trained on..."
  - [Section 5.3]: "For languages that SeamlessM4T-v2-Large does not support ASR for, fine-tuning on in-domain ASR datasets improve the ST performance. Specifically, for bho and aeb, ASR training improves performance by about +5 and +3 BLEU..."
  - [corpus]: Evidence is consistent with HITSZ's submission using Whisper (ASR) integration for Indic languages.
- **Break condition:** If the ASR dataset is noisy or domain-mismatched, the encoder learns poor representations, potentially leading to negative transfer where ST performance degrades compared to a robust zero-shot attempt.

### Mechanism 3: Multi-Task Training with Knowledge Distillation
- **Claim:** Multi-task training with Knowledge Distillation (KD) offers marginal gains specifically when the text-based Machine Translation (MT) task is significantly easier (higher performance) than the ST task.
- **Mechanism:** The ST model (student) is regularized by the MT model (teacher) via KL-divergence loss. Since text-to-text translation is less noisy than speech-to-text, the teacher provides a cleaner target distribution, forcing the student to improve its fluency and vocabulary selection to match the teacher's output probabilities.
- **Core assumption:** The MT model is high-quality and the "gap" between MT and ST performance is large enough to justify the complexity of joint training.
- **Evidence anchors:**
  - [Section 5.3]: "Multi-task training is beneficial when MT performance is strong... Multi-task improves aeb performance by 2 BLEU... However, there is no improvement for mlt."
  - [Section 3.4]: "The goal of applying the KD objective is to use the MT components to enhance the ST components... we hope to mitigate this performance gap."
  - [corpus]: Weak or missing direct evidence in the provided corpus neighbors regarding KD efficacy in this specific low-resource context; most neighbors focus on data augmentation or cascaded systems.
- **Break condition:** If the MT model is weak or erroneous, KD will transfer those errors to the ST model (negative transfer), or the training may become unstable due to conflicting gradients.

## Foundational Learning

- **Concept: Transfer Learning & Fine-Tuning**
  - **Why needed here:** The entire methodology relies on taking a general-purpose 2B parameter model (SeamlessM4T-v2) and adapting it to specific, low-resource constraints. Understanding the difference between "zero-shot" and "fine-tuning" is critical.
  - **Quick check question:** Why would fine-tuning on just 10 hours of data improve performance over a model trained on 4.5M hours, but only if the learning rate is carefully managed?

- **Concept: Encoder-Decoder Architecture**
  - **Why needed here:** The paper manipulates specific components (Speech Encoder, Text Decoder) independently (e.g., initializing the ST encoder with ASR weights).
  - **Quick check question:** In the ASR initialization strategy, which component's weights are transferred to the ST model, and which component remains unchanged (assuming the target language is high-resource)?

- **Concept: Knowledge Distillation**
  - **Why needed here:** Used in the multi-task training paradigm to force the ST model to mimic the behavior of a stronger MT model.
  - **Quick check question:** Why must the gradient be "stopped" (stop-gradient) when calculating the loss from the teacher model?

## Architecture Onboarding

- **Component map:**
  - **Input:** Speech (Audio) → **Speech Encoder** (w2v-BERT style) → **Text Decoder** → **Output:** Translation
  - **Auxiliary (for initialization):** **Text Encoder** + **Text Decoder** (used for ASR/MT pre-training phases)

- **Critical path:**
  1. Verify language support in SeamlessM4T-v2
  2. If unsupported: Fine-tune ASR model → Extract Encoder weights → Initialize ST model
  3. Train ST model (Direct E2E or Multi-task)
  4. Decode with Beam Search (beam=5)

- **Design tradeoffs:**
  - **Official vs. HuggingFace Codebase:** The Official codebase yields better performance (+1-2 BLEU) but may be less flexible. HuggingFace requires specific patches (dropout, embedding untying) to match performance
  - **Cascaded vs. E2E:** Cascaded systems are competitive but generally underperform E2E fine-tuning in this setup because SeamlessM4T-v2 is "strong enough" to make error propagation from ASR detrimental

- **Failure signatures:**
  - **Catastrophic Forgetting:** Training on a new low-resource language causes the model to lose capability in high-resource languages (mitigate with lower learning rates)
  - **Synthetic Data Mismatch:** Adding synthetic ST data (e.g., TTS for Irish) reduced performance by 1 BLEU, suggesting domain mismatch or acoustic artifacts harmed the model
  - **Codebase Discrepancy:** If your HuggingFace model underperforms by ~1-2 BLEU, check `lm_head` tying and dropout rates (0.1 vs 0.0)

- **First 3 experiments:**
  1. **Establish Baseline:** Run zero-shot evaluation on the dev set for all target languages to identify which languages need ASR initialization
  2. **Codebase Alignment:** Train a small "dummy" run on a high-resource subset (e.g., Estonian) using both Official and HuggingFace codebases to validate implementation parity before full runs
  3. **Ablation on ASR Init:** For one unsupported language (e.g., Bhojpuri), compare (a) Direct E2E vs. (b) E2E with ASR Encoder Initialization to quantify the specific gain of the proposed mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What specific pre-training objectives can effectively mitigate the representational gap between the speech encoder and text decoder in SeamlessM4T-v2 for low-resource languages?
- **Basis in paper:** [explicit] The authors state in the conclusion, "For future work, we could explore better pre-training methods to mitigate the gap between the speech encoder and the text decoder."
- **Why unresolved:** The current work focused on initialization strategies (using ASR/MT components) and fine-tuning, but did not experiment with altering the pre-training paradigm to better align modalities.
- **What evidence would resolve it:** A study implementing and evaluating pre-training methods (e.g., CTC-based or optimal transport) on the low-resource languages showing improved BLEU scores over the initialization methods described.

### Open Question 2
- **Question:** Can speech-based Large Language Models (LLMs) outperform the current state-of-the-art SeamlessM4T-v2 systems in low-resource speech translation tasks?
- **Basis in paper:** [explicit] The authors propose as future work to "explore the use of speech large language models, as large language models have recently achieved success in MT tasks."
- **Why unresolved:** The paper exclusively utilizes SeamlessM4T-v2 and NLLB architectures; it does not include experiments utilizing LLM-based speech architectures.
- **What evidence would resolve it:** Comparative benchmarks showing the performance of a speech LLM fine-tuned on the IWSLT 2025 datasets against the SeamlessM4T-v2 baselines established in this paper.

### Open Question 3
- **Question:** Why does the inclusion of large-scale synthetic speech data improve performance for Quechua but cause a degradation in performance for Irish?
- **Basis in paper:** [inferred] In Appendix B, the authors note that adding 202 hours of synthetic data for Irish (gle) resulted in a 1 BLEU drop, whereas adding synthetic data for Quechua (que) yielded significant improvements.
- **Why unresolved:** The paper reports the discrepancy in ablation results but does not analyze the underlying cause (e.g., domain mismatch, acoustic quality, or text complexity) of the negative transfer for Irish.
- **What evidence would resolve it:** An analysis of the synthetic Irish data's characteristics (speaker variance, noise, textual domain) or experiments demonstrating that filtering/quality control on the synthetic data restores positive transfer.

## Limitations

- **Data Quality and Distribution:** The paper reports performance across 9 low-resource language pairs, but the actual size and quality of training data for each language is not fully characterized, making it difficult to assess the impact of data variability on results.
- **Baseline Comparisons:** The paper primarily compares its approach to zero-shot performance of the base SeamlessM4T-v2 model, lacking comparisons to other strong baselines for low-resource speech translation.
- **Reproducibility of Codebase Differences:** The paper identifies discrepancies between the official Facebook codebase and the HuggingFace implementation but notes that one required change (dropout in the decoder) was not fully implemented due to significant effort required.

## Confidence

**High Confidence:**
- Direct end-to-end fine-tuning yields strong results for languages already supported by the model's ASR capabilities (gle, mlt, mar)
- Cascaded systems are generally competitive but underperform end-to-end fine-tuning in this setup

**Medium Confidence:**
- Multi-task training with knowledge distillation provides marginal improvements when MT significantly outperforms ST
- ASR initialization improves performance for languages not supported by SeamlessM4T-v2's ASR

**Low Confidence:**
- Synthetic data hurt performance for Irish by 1 BLEU based on a single data point

## Next Checks

1. **Codebase Parity Validation:** Create a minimal reproducible experiment comparing the official Facebook codebase with the HuggingFace implementation on a single language pair (e.g., est-eng). Systematically test the impact of each identified discrepancy (lm_head tying, dropout rates, embedding untying) to isolate which changes are necessary to achieve parity.

2. **Data Quality Assessment:** For the two lowest-resource languages (bem-eng and que-spa, both with <2 hours of data), conduct an ablation study that varies the learning rate and fine-tuning duration. This will help determine whether the observed performance gains are due to the fine-tuning strategy itself or simply the amount of training data available.

3. **Alternative Baseline Comparison:** Implement and evaluate a baseline system using a standard speech recognition model (e.g., Whisper) followed by a standard machine translation model (e.g., mBART). Compare this cascaded baseline to the proposed end-to-end fine-tuned model across all 9 language pairs.