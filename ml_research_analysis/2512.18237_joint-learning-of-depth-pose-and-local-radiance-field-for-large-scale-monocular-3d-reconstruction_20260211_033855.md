---
ver: rpa2
title: Joint Learning of Depth, Pose, and Local Radiance Field for Large Scale Monocular
  3D Reconstruction
arxiv_id: '2512.18237'
source_url: https://arxiv.org/abs/2512.18237
tags:
- depth
- pose
- radiance
- fields
- nerf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses large-scale monocular 3D reconstruction by
  jointly learning depth, camera pose, and radiance field representation. The key
  innovation is a unified framework that couples metric-scale depth estimation (via
  a Vision Transformer with object-size priors), drift-free pose refinement (through
  multi-scale feature bundle adjustment), and scalable scene representation (via an
  incremental local radiance field hierarchy).
---

# Joint Learning of Depth, Pose, and Local Radiance Field for Large Scale Monocular 3D Reconstruction

## Quick Facts
- arXiv ID: 2512.18237
- Source URL: https://arxiv.org/abs/2512.18237
- Reference count: 30
- Achieves up to 18× lower ATE than BARF and 2× lower than NoPe-NeRF on Tanks & Temples

## Executive Summary
This paper presents a unified framework for large-scale monocular 3D reconstruction that jointly learns depth, camera pose, and radiance field representation. The system addresses three critical failure modes in monocular reconstruction: scale ambiguity, long-horizon pose drift, and memory explosion in global NeRFs. By coupling metric-scale depth estimation with object-size priors, drift-free pose refinement through feature-space bundle adjustment, and scalable scene representation via incremental local radiance fields, the method achieves metre-accurate reconstructions over 100m trajectories from a single uncalibrated RGB camera.

## Method Summary
The framework consists of three jointly optimized components: a ViT-based depth network trained with metric-scale supervision using standing-person height priors, a multi-scale feature bundle adjustment layer that refines camera poses in learned feature space rather than pixel space, and an incremental local radiance field hierarchy that spawns and freezes hash-grid NeRFs on-the-fly when view overlap drops below 80%. The system operates in sliding 32-frame windows, alternating between depth warm-up, feature-based pose refinement, and radiance fine-tuning. All components share gradients through a unified loss that combines photometric, depth, and flow consistency terms.

## Key Results
- Achieves up to 18× lower ATE than BARF and 2× lower than NoPe-NeRF on Tanks & Temples benchmark
- Maintains sub-pixel Relative Pose Error while processing 100m+ trajectories on a single A100 GPU
- Produces competitive novel-view synthesis quality (PSNR 20.19 dB, SSIM 0.704) with memory usage under 7GB

## Why This Works (Mechanism)

### Mechanism 1: Metric-Scale Depth via Object-Size Priors
Anchoring depth predictions to known physical sizes eliminates scale ambiguity in monocular reconstruction. A ViT encoder processes 16×16 image patches through ResNet-50 token embeddings, with a depth loss that includes a metric term penalizing deviation between median depth over "standing-person" pixels and a fixed anchor of 1.7m. This grounds predictions in absolute units across scenes.

### Mechanism 2: Feature-Space Bundle Adjustment Suppresses Pose Drift
Optimizing poses in learned feature space rather than pixel space reduces sensitivity to texture-poor regions and outlier matches. A shared U-Net extracts multi-scale feature maps with learned confidence weights, and weighted feature residuals are minimized via coarse-to-fine Levenberg-Marquardt, backpropagating through SE(3) Jacobians.

### Mechanism 3: Incremental Local Radiance Fields Enable Scalable Reconstruction
Dynamically spawning and freezing local hash-grid NeRFs bounds memory while preserving rendering quality over long trajectories. Each local field is a hash-grid MLP that gets frozen when >80% of new rays fall outside its contracted unit cube, with frozen fields providing L2 color priors for seamless transitions.

## Foundational Learning

- **Vision Transformer (ViT) tokenization and patch embedding**: The depth backbone splits images into 16×16 patches processed by a ViT; understanding attention over patches is essential for debugging depth predictions. *Quick check: Can you explain how a ViT differs from a CNN in terms of receptive field and global context aggregation?*

- **SE(3) transformations and Lie algebra**: Pose refinement operates in SE(3); gradients must flow through exponential/logarithmic maps during bundle adjustment. *Quick check: Given a 6-DOF twist ξ ∈ se(3), how do you compute the corresponding SE(3) transformation matrix?*

- **Hash-grid neural representations (Instant-NGP style)**: Local radiance fields use hash-grid MLPs for efficient scene encoding; understanding collision handling and resolution tradeoffs is critical. *Quick check: How does a multiresolution hash grid resolve collisions, and what is the memory-accuracy tradeoff compared to dense voxel grids?*

## Architecture Onboarding

- **Component map**: Input RGB stream → ViT Depth Encoder → U-Net Feature Extractor → Feature BA Module → Incremental Hash-Grid NeRFs → Progressive Scheduler
- **Critical path**: 1) Bootstrap first 5 frames → optimize (θ, T, K, φ₁) jointly 2) Slide N=32-frame window: depth → FBA → radiance in sequence 3) When >80% rays outside current field → freeze R_k, spawn R_{k+1} 4) Final output: metric depth maps, drift-free trajectory, photorealistic novel views
- **Design tradeoffs**: ViT vs. ResNet depth backbone (ViT gives 3× lower ATE but higher compute); Feature BA vs. pixel BA (+40% RPE-R if pixel space used); Local vs. global NeRF (incremental fields keep VRAM <7GB but may introduce subtle boundary artifacts)
- **Failure signatures**: Thin structures vanish (hash resolution insufficient); Dynamic objects leave ghost artifacts (no transient object modeling); Textureless corridors cause FBA drift; Real-time mobile inference infeasible (desktop GPU required)
- **First 3 experiments**: 1) Replace ViT with ResNet, measure ATE and PSNR change on Tanks & Temples 2) Run BA in pixel space only, quantify RPE-R and RPE-T degradation 3) Vary the 80% ray-outside threshold to 60%/90%, observe memory usage and PSNR tradeoffs

## Open Questions the Paper Calls Out

- **Transient object handling**: Can transient-object masking or motion-field layers be integrated into incremental local radiance fields to eliminate ghost artifacts from dynamic objects? The current framework assumes static scenes for photometric consistency.

- **Mobile deployment feasibility**: Can the pipeline be quantized and pruned sufficiently to run on mobile SoCs while maintaining joint optimization? The method currently requires desktop-class GPU with 6-7GB VRAM.

- **Scale estimation without person priors**: How does metric depth estimation performance degrade in scenes completely devoid of standing-person pixels? The method relies on human height as a scale anchor, which may not be available in all environments.

## Limitations
- Scale anchoring reliability depends on accurate person detection and consistent availability of human subjects
- Local field boundaries may introduce subtle artifacts when revisiting frozen regions
- Thin structures like power lines and wires are poorly reconstructed due to hash-grid resolution limits
- Real-time mobile inference remains infeasible due to computational requirements

## Confidence
- **Metric-scale depth accuracy**: High - Supported by explicit depth loss formulation and quantitative ATE improvements
- **Pose drift suppression**: Medium - Feature BA shows 40% RPE-R improvement, but real-world robustness needs validation
- **Scalable radiance field quality**: Medium - Incremental fields enable city-scale reconstruction, but boundary artifacts acknowledged

## Next Checks
1. Evaluate depth predictions on sequences with minimal human subjects to quantify scale drift without person anchor
2. Perform closed-loop trajectory revisiting frozen regions to measure photometric consistency and detect visible seams
3. Design synthetic sequences with known thin structures to quantify reconstruction fidelity using LPIPS and edge preservation metrics