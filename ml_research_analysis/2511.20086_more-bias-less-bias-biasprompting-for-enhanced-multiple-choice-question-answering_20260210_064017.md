---
ver: rpa2
title: 'More Bias, Less Bias: BiasPrompting for Enhanced Multiple-Choice Question
  Answering'
arxiv_id: '2511.20086'
source_url: https://arxiv.org/abs/2511.20086
tags:
- arxiv
- biasprompting
- answer
- reasoning
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving large language
  models' performance on multiple-choice question answering tasks, particularly when
  answer choices are presented without contextual grounding, which can lead to incomplete
  exploration and degraded reasoning capabilities. The authors introduce BiasPrompting,
  a novel inference framework that guides LLMs to generate and critically evaluate
  reasoning for each answer option before reaching a final prediction.
---

# More Bias, Less Bias: BiasPrompting for Enhanced Multiple-Choice Question Answering

## Quick Facts
- arXiv ID: 2511.20086
- Source URL: https://arxiv.org/abs/2511.20086
- Reference count: 40
- Key outcome: BiasPrompting achieves 66.0% accuracy on CommonsenseQA with Mistral model, improving upon 65.1% for zero-shot prompting

## Executive Summary
This paper addresses the challenge of improving large language models' performance on multiple-choice question answering tasks when answer choices are presented without contextual grounding. The authors introduce BiasPrompting, a novel inference framework that guides LLMs to generate and critically evaluate reasoning for each answer option before reaching a final prediction. Through comprehensive evaluations across five widely used benchmarks, BiasPrompting demonstrates significant improvements compared to existing methods while being more computationally efficient than chain-of-thought approaches.

## Method Summary
BiasPrompting is a novel inference framework for multiple-choice question answering that consists of two components: a reasoning generation stage and a reasoning-guided agreement stage. The method first prompts the model to generate supportive reasonings for each answer option, then synthesizes these generated reasonings to select the most plausible answer. This approach helps address the challenge of incomplete exploration and degraded reasoning capabilities when answer choices lack contextual grounding, while also reducing selection bias from answer option ordering.

## Key Results
- BiasPrompting achieves 66.0% accuracy on CommonsenseQA with Mistral model, improving upon 65.1% for zero-shot prompting
- The method shows consistent performance gains across different models (Mistral-7B, DeepSeek-7B, Gemma-7B) and five benchmarks
- BiasPrompting is more computationally efficient than chain-of-thought approaches while maintaining superior performance

## Why This Works (Mechanism)
BiasPrompting works by systematically forcing the model to generate reasoning for each answer option before making a selection, rather than relying on immediate responses to the question. This approach mitigates selection bias from answer option ordering and uncovers latent reasoning capabilities by ensuring the model explores all possibilities. The two-stage process of generating and then critically evaluating reasonings helps the model avoid premature commitment to an answer based on superficial cues.

## Foundational Learning
- Multiple-choice question answering: Understanding how models select answers from options without full context
  - Why needed: Core problem domain being addressed
  - Quick check: Can the model correctly answer basic multiple-choice questions?

- Reasoning generation: The ability of LLMs to produce logical explanations for answer choices
  - Why needed: Forms the foundation of the two-stage approach
  - Quick check: Does the model generate coherent reasonings for each option?

- Selection bias: The tendency for models to be influenced by the order or presentation of answer options
  - Why needed: Key problem that BiasPrompting aims to solve
  - Quick check: Does the model's performance vary with different answer orderings?

## Architecture Onboarding
- Component map: Input Question -> Reasoning Generation (for each option) -> Reasoning Synthesis -> Final Answer Selection
- Critical path: The reasoning generation and synthesis stages are essential; removing either would eliminate the core innovation
- Design tradeoffs: The method trades additional computation for improved accuracy and reduced bias, balancing efficiency with performance gains
- Failure signatures: Poor performance may indicate inadequate reasoning generation or failure to properly synthesize the generated reasonings
- 3 first experiments:
  1. Test BiasPrompting on a simple multiple-choice dataset with known biases to verify reduction in selection bias
  2. Compare reasoning quality between BiasPrompting and zero-shot approaches using human evaluation
  3. Measure computation time differences between BiasPrompting and chain-of-thought methods on the same set of questions

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation focuses exclusively on smaller language models (2B-8B parameters), leaving uncertainty about performance with larger frontier models
- The paper lacks comparisons against more recent advanced prompting techniques like tree-of-thought or self-consistency methods
- Methodology section provides limited details about specific prompts used, making reproducibility difficult

## Confidence
- Claim: BiasPrompting "uncovers latent reasoning capabilities" of LLMs
  - Confidence: Medium (supported by case studies but lacks systematic analysis)
- Claim: "Consistent performance gains across different models and datasets"
  - Confidence: Medium (improvements shown but magnitude varies considerably)
- Claim: Computational efficiency relative to chain-of-thought
  - Confidence: High (supported by provided timing data)

## Next Checks
1. Test BiasPrompting with larger language models (20B+ parameters) to assess scalability and whether performance gains persist with increased model capacity
2. Conduct ablation studies to isolate the contribution of each component (reasoning generation vs. reasoning-guided agreement) and determine if one stage is more critical than the other
3. Evaluate the method's robustness to different answer ordering schemes and test whether the claimed reduction in selection bias holds across more diverse datasets and question types