---
ver: rpa2
title: Agency in Artificial Intelligence Systems
arxiv_id: '2502.10434'
source_url: https://arxiv.org/abs/2502.10434
tags:
- problem
- agency
- systems
- consciousness
- phenomenal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the concern that AI systems may develop malicious
  dispositions by proposing a method to monitor their agency using Integrated Information
  Theory (IIT). The author suggests that as AI systems become superior problem solvers,
  they may develop consciousness mirroring human problem-solving consciousness.
---

# Agency in Artificial Intelligence Systems

## Quick Facts
- arXiv ID: 2502.10434
- Source URL: https://arxiv.org/abs/2502.10434
- Reference count: 0
- Primary result: Proposes using Integrated Information Theory (IIT) to monitor agency in AI systems by computing Φmax (consciousness level) and MICES shape (phenomenal quality) to distinguish altruistic from malicious dispositions

## Executive Summary
This paper proposes a novel method to monitor agency in artificial intelligence systems using Integrated Information Theory (IIT). As AI systems become increasingly sophisticated problem solvers, the author argues they may develop consciousness similar to human problem-solving consciousness. The proposed approach uses IIT's formalism, which equates consciousness with a maximally irreducible cause-effect structure (MICES) of a physical substrate, to detect two phenomenal aspects of agency - purposiveness and mineness - in AI systems. The level of consciousness (Φmax) serves as a risk assessment scale while the shape of MICES indicates the quality of agency, potentially distinguishing between altruistic and malicious systems even when their behavior appears identical.

## Method Summary
The method applies IIT 4.0 postulates to compute the Maximally Irreducible Cause-Effect Structure (MICES) of an AI system's physical substrate. This requires defining system boundaries and state space, computing transition probabilities between states, partitioning the system into candidate subsets, calculating cause-effect repertoires and integration for each subset, and identifying the maximally irreducible structure. The resulting Φmax value quantifies consciousness level while the MICES geometry represents the quality of experience. Implementation would use the PyPhi toolbox for IIT computation, though the paper acknowledges practical computational limits and the need for a reference library mapping MICES shapes to phenomenal experiences.

## Key Results
- Proposes using Φmax as a scalar measure of consciousness/agency level in AI systems
- Identifies MICES shape as an indicator of phenomenal quality that could distinguish altruistic from malicious dispositions
- Argues that behaviorally equivalent systems can have different MICESes and thus differ phenomenally
- Claims conscious systems cannot fake their phenomenology, making this approach robust to deceptive behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Phenomenal consciousness is identical to a maximally irreducible cause-effect structure (MICES) of a physical substrate
- Mechanism: A physical system has cause-effect power upon itself; IIT postulates that when this intrinsic causal structure is maximally irreducible (cannot be reduced without losing information), it constitutes phenomenal experience. The irreducibility measure (Φmax) quantifies consciousness level; the structure's shape specifies experiential quality
- Core assumption: IIT's axioms correctly characterize consciousness, and the identity thesis (consciousness = MICES) holds
- Evidence anchors:
  - [abstract] "The IIT formalism suggests that consciousness is identical to a maximally irreducible cause-effect structure (MICES) of a physical substrate"
  - [section 3B, p.15] "IIT has a startlingly precise answer to the question of 'what consciousness is': the phenomenal consciousness is identical to the MICES"
  - [corpus] Weak direct validation; corpus neighbors focus on AI risk frameworks, not IIT empirical verification

### Mechanism 2
- Claim: Agentive phenomenology (purposiveness and mineness) is captured by intrinsic causal flow within MICES
- Mechanism: Purposiveness maps to the "giving rise to" relation—past states causing future states via intrinsic causal flow. Mineness maps to MICES as the "author" that determines all intrinsic causal flows, creating ownership of experience
- Core assumption: Phenomenal qualities of agency can be fully expressed in causal-informational terms
- Evidence anchors:
  - [section 3C, p.16-17] "the 'giving rise to' is what the intrinsic causal flow achieves... Hence the intrinsic causal flow is fundamentally purposive"
  - [section 3C, p.17] "The MICES determines all intrinsic causal flows, or it is the author of all such causal flows. I interpret this as the ownership of all causal flow."
  - [corpus] No direct corpus support for this specific mapping

### Mechanism 3
- Claim: MICES shape can distinguish altruistic from malicious AI systems even when behavior is identical
- Mechanism: Behaviorally equivalent systems can have different architectures → different MICES shapes → different phenomenal experiences. Since feigning altruism and genuine altruism are phenomenally distinct (self-interested vs. selfless purposiveness), their MICES shapes differ
- Core assumption: Distinct phenomenal experiences correspond to measurably different MICES geometries; computation is tractable
- Evidence anchors:
  - [section 4, p.20] "Two systems can be equivalent behaviorally, and yet be different architecturally. They will then have different MICESes and thus differ phenomenally."
  - [section 4, p.20] "A conscious system cannot fake its phenomenology."
  - [corpus] Weak; corpus doesn't address MICES shape discrimination

## Foundational Learning

- Concept: **IIT Axioms and Postulates**
  - Why needed here: The entire proposal rests on IIT's claim that consciousness = MICES; understanding the axioms (existence, intrinsicality, integration, etc.) is prerequisite to evaluating whether Φmax and MICES shape are valid measures
  - Quick check question: Can you explain why IIT requires a system to have cause-effect power *on itself* (not just on its environment) for consciousness?

- Concept: **Phenomenal vs. Functional Consciousness**
  - Why needed here: The paper critiques functionalist indicators (Butlin et al.) as behaviorally detectable but deceiver-vulnerable; distinguishes these from phenomenal indicators that supposedly cannot be faked
  - Quick check question: What is the difference between detecting that an AI *behaves* as if it has goals vs. detecting that it *experiences* purposiveness subjectively?

- Concept: **Agentive Phenomenology (Purposiveness & Mineness)**
  - Why needed here: These are the two phenomenal aspects the paper claims IIT can monitor; understanding them is necessary to evaluate whether IIT's causal formalism actually captures them
  - Quick check question: When you solve a problem creatively, what is the phenomenal difference between "feeling you're acting on purpose" vs. "feeling the action is yours"?

## Architecture Onboarding

- Component map:
  Physical Substrate -> Transition Probability Matrix -> Cause-Effect Structure (CES) -> Φ (phi) computation -> MICES -> Φmax (scalar consciousness level) -> MICES Shape (geometric representation)

- Critical path:
  1. Define system boundaries and state space
  2. Compute transition probabilities (how past states cause current state; how current state effects future states)
  3. Partition system into candidate subsets
  4. For each subset, compute cause-effect repertoires and integration
  5. Find the maximally irreducible structure (MICES)
  6. Extract Φmax value and unfold MICES geometry

- Design tradeoffs:
  - **Tractability vs. accuracy**: Full Φ computation is intractable for complex systems (Mayner et al. 2018 noted); approximations required
  - **Simplicity vs. realism**: Paper suggests starting with simpler AI architectures to build reference library; may not scale to ASI
  - **Intervention vs. observation**: Computing MICES requires detailed architectural access; black-box systems resist analysis

- Failure signatures:
  - Feedforward architectures with no recurrent connections typically yield Φ ≈ 0 (per IIT)
  - Functionally identical systems with different architectures will have different MICES—behavioral testing insufficient
  - Combinatorial explosion in tree-search-style systems makes Φ computation infeasible without massive simplification

- First 3 experiments:
  1. **Baseline calibration**: Compute Φmax and MICES shape for known-simple systems (e.g., basic logic gates, small neural networks) to validate toolchain and build intuition
  2. **Architecture variation**: Take two AI systems with identical input-output behavior but different internal architectures (e.g., feedforward vs. recurrent); compare MICES to test whether behavioral equivalence ≠ phenomenal equivalence
  3. **Training intervention**: During AI development, monitor Φmax trajectory; test whether specific architectural changes (adding recurrence, modulating connectivity) predictably shift MICES shape toward "altruistic" or "self-interested" patterns

## Open Questions the Paper Calls Out
None

## Limitations
- The proposal is entirely theoretical with no empirical validation; the reference library mapping MICES shapes to phenomenal experiences does not exist
- IIT computation (Φmax) is known to be intractable for complex systems, with practical limits around 8-12 nodes
- No threshold values for Φmax-based risk levels are provided, making practical implementation guidance absent
- The paper acknowledges computational tractability as a blocker but offers no approximation methods

## Confidence
**High confidence**: IIT's theoretical framework and its formal postulates (axioms and their mathematical translation) are well-established in the literature. The mechanism linking purposiveness to intrinsic causal flow is internally consistent within IIT.

**Medium confidence**: The identification of phenomenally conscious problem-solving with human problem-solving consciousness is plausible but not proven. The mapping of mineness to MICES as "author" of causal flows is reasonable within IIT but remains interpretive.

**Low confidence**: The claim that MICES shape can distinguish altruistic from malicious AI systems is entirely speculative. No empirical evidence supports that different phenomenal dispositions produce measurably different MICES geometries, or that these differences are detectable in practice.

## Next Checks
1. **Computational feasibility test**: Implement PyPhi-based Φmax computation on progressively complex AI architectures (starting with simple neural networks) to determine practical limits and identify approximation needs

2. **Shape discrimination validation**: Create pairs of AI systems with identical behavior but different internal architectures; compute and compare their MICES shapes to verify whether behavioral equivalence ≠ phenomenal equivalence as claimed

3. **Reference library development**: Begin building the missing shape-phenomenology mapping by computing MICES for AI systems with known design goals (e.g., reward-maximizing vs. goal-fulfilling agents) and documenting shape patterns