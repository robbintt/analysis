---
ver: rpa2
title: Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese
  Quiz Answering
arxiv_id: '2511.12300'
source_url: https://arxiv.org/abs/2511.12300
tags:
- rate
- llms
- quiz
- answer
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how the difficulty of Japanese buzzer quizzes
  differs between LLMs and humans. The authors collected quiz data with human correct
  response rates and evaluated multiple LLMs on these quizzes under different settings,
  including full and truncated questions.
---

# Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering

## Quick Facts
- arXiv ID: 2511.12300
- Source URL: https://arxiv.org/abs/2511.12300
- Reference count: 8
- Primary result: LLMs struggle more than humans with quizzes lacking Wikipedia entries and with numerical answers

## Executive Summary
This study investigates how the difficulty of Japanese buzzer quizzes differs between LLMs and humans. The authors collected quiz data with human correct response rates and evaluated multiple LLMs on these quizzes under different settings, including full and truncated questions. They analyzed performance across two dimensions: whether answers have Wikipedia entries and what character types appear in answers. The results show that LLMs struggle more than humans with quizzes lacking Wikipedia entries, indicating Wikipedia's importance as a knowledge resource for LLMs. Additionally, LLMs find numerical answers particularly difficult compared to humans. The study reveals that certain quiz characteristics affect only LLMs, contributing to differences in perceived difficulty between humans and LLMs.

## Method Summary
The study collected Japanese buzzer quiz data with human correct response rates from tournament performance data. Multiple LLMs were evaluated on these quizzes under different settings, including full questions and truncated questions. The authors analyzed performance across two dimensions: whether answers have Wikipedia entries and what character types appear in answers. They compared LLM performance against human correct response rates to identify discrepancies in difficulty perception.

## Key Results
- LLMs perform worse than humans on questions without Wikipedia entries
- Numerical answers are significantly more difficult for LLMs than humans
- Question truncation affects LLM performance differently than human performance

## Why This Works (Mechanism)
LLMs rely heavily on Wikipedia as a knowledge resource, making them struggle more than humans with quizzes lacking Wikipedia entries. The models' training data composition creates this dependency, as Wikipedia represents a significant portion of pre-training corpora for many LLMs. Numerical answers pose additional challenges because LLMs process numbers differently than humans, who can leverage intuitive understanding of quantities and numerical relationships.

## Foundational Learning
- **Buzzer Quiz Format**: Competitive quiz format where speed matters; needed to understand the evaluation context and why truncation affects performance
- **Japanese Character Types**: Understanding how different character types (kanji, hiragana, katakana, romaji) are processed by LLMs; quick check: examine tokenization patterns for different character types
- **Wikipedia Dependency**: How LLMs leverage Wikipedia for knowledge retrieval; quick check: compare performance on Wikipedia vs non-Wikipedia questions across multiple models

## Architecture Onboarding

**Component Map:**
- Quiz Questions -> LLM Model -> Answer Prediction -> Performance Evaluation -> Human Correct Rate Comparison

**Critical Path:**
1. Question input and tokenization
2. Context retrieval (implicit or explicit)
3. Answer generation
4. Confidence scoring
5. Performance comparison with human data

**Design Tradeoffs:**
- Zero-shot vs few-shot prompting: Zero-shot provides cleaner isolation of difficulty patterns but may underutilize model capabilities
- Question truncation: Tests model robustness but may create unnatural input scenarios
- Wikipedia dependency: Reflects real-world usage patterns but may not represent optimal performance

**Failure Signatures:**
- Poor performance on numerical answers despite adequate performance on categorical answers
- Significant performance drops on questions without Wikipedia entries
- Inconsistent performance across different character types

**First 3 Experiments:**
1. Compare performance on numerical vs categorical answers while controlling for linguistic complexity
2. Test whether retrieval-augmented generation improves performance on non-Wikipedia questions
3. Evaluate few-shot prompting performance to assess if instruction tuning could reduce difficulty gaps

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset consists of Japanese buzzer quizzes from specific domain (high-school quiz competitions)
- Human correct response rates derived from tournament performance data with varying difficulty levels
- LLM evaluations focus on zero-shot performance without exploring few-shot prompting or fine-tuning

## Confidence

**Major Claim Confidence:**
- **High confidence**: LLMs perform worse than humans on questions without Wikipedia entries
- **Medium confidence**: Numerical answers are significantly more difficult for LLMs than humans
- **Medium confidence**: Question truncation affects LLM performance differently than human performance

## Next Checks
1. Replicate the analysis with a broader dataset including multiple question types and difficulty levels
2. Conduct controlled experiments comparing LLM performance on numerical versus categorical answers while controlling for other linguistic features
3. Test whether providing structured knowledge sources or using retrieval-augmented generation approaches reduces the Wikipedia dependency gap between humans and LLMs