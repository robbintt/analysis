---
ver: rpa2
title: Contextualizing biological perturbation experiments through language
arxiv_id: '2502.21290'
source_url: https://arxiv.org/abs/2502.21290
tags:
- gene
- genes
- perturbation
- expression
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new benchmark for structured reasoning over
  perturbation experiments, using large language models to predict discrete outcomes
  like differential expression and gene set enrichment. The authors introduce SUMMER,
  an inference-time framework that combines gene summarization, retrieval of experimental
  outcomes, and guided LLM prompting to reason over biological knowledge graphs.
---

# Contextualizing biological perturbation experiments through language

## Quick Facts
- arXiv ID: 2502.21290
- Source URL: https://arxiv.org/abs/2502.21290
- Reference count: 40
- Primary result: Introduces SUMMER, a language-based framework for predicting perturbation experiment outcomes that matches or exceeds state-of-the-art methods on the PerturbQA benchmark

## Executive Summary
This paper proposes SUMMER, a language-based framework for predicting outcomes of biological perturbation experiments. The method uses large language models to reason over knowledge graphs by converting them to textual summaries, retrieving relevant experimental outcomes based on graph proximity, and applying guided prompting to predict differential expression and gene set enrichment. On the new PerturbQA benchmark, SUMMER demonstrates that language models can effectively capture complex biological relationships, matching or exceeding traditional approaches. However, the work identifies causal directionality as a key limitation, suggesting LLMs still struggle with certain types of biological reasoning.

## Method Summary
SUMMER is an inference-time framework that combines gene summarization, retrieval of experimental outcomes, and guided LLM prompting to reason over biological knowledge graphs. The method operates in three stages: first, it summarizes knowledge graph entries for each gene using a 70B LLM; second, it retrieves perturbation-gene pairs from existing experimental data based on knowledge graph proximity; third, it applies a five-stage chain-of-thought prompt to the 8B LLM using retrieved examples as in-context learning signals. The framework predicts discrete outcomes including differential expression, direction of change, and gene set enrichment, using the new PerturbQA benchmark for evaluation.

## Key Results
- SUMMER matches or exceeds state-of-the-art methods on the PerturbQA benchmark for perturbation prediction tasks
- Ablation studies show both retrieval and chain-of-thought prompting are essential, with models without these components performing no better than random
- The framework demonstrates the potential of LLMs for biological discovery while identifying causal directionality as a primary challenge
- SUMMER achieves macro AUROC performance competitive with traditional graph-based approaches while providing interpretable reasoning traces

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Graph Serialization as Language
Converting graph-structured biological knowledge into textual summaries preserves semantic richness that adjacency matrices lose. For each gene, LLM generates two summaries: as perturbation source (downstream effects) and as downstream target (upstream influences). This bidirectional representation captures causal directionality that graph embeddings typically flatten.

### Mechanism 2: Graph-Proximity Retrieval as In-Context Learning Signal
Retrieved experimental outcomes from graph-neighboring gene pairs provide grounding that prevents LLM hallucination. For query pair (p, g), retrieve up to 15 training examples (p', g') where p' and g' share knowledge graph neighbors with query. These become in-context examples showing actual DE/non-DE outcomes.

### Mechanism 3: Guided Reasoning Over Retrieved Context
Chain-of-thought prompting that explicitly asks LLM to compare perturbation similarity, identify shared pathways, and assess overlap reduces reasoning errors. Five-stage prompt structure: (1) identify similar perturbed genes, (2) identify downstream effects, (3) identify similar downstream genes, (4) identify upstream perturbations, (5) synthesize answer.

## Foundational Learning

- **Concept: Knowledge Graphs in Biology (GO, STRING, Reactome)**
  - Why needed here: SUMMER retrieves based on graph proximity; understanding what edges mean (physical interaction vs. pathway co-membership vs. co-expression) determines retrieval quality.
  - Quick check question: If Gene A physically interacts with Gene B in STRING, does this indicate causality for perturbation response? (Answer: No—physical interaction doesn't establish direction of regulatory effect.)

- **Concept: Perturbation Biology (Perturb-seq, CRISPRi, Differential Expression)**
  - Why needed here: Tasks predict whether knockdown causes differential expression and direction of change; understanding null hypotheses and statistical testing determines label quality.
  - Quick check question: In Perturb-seq, why might a gene show differential expression under perturbation X but not perturbation Y even if both are in the same pathway? (Answer: Redundancy, compensatory mechanisms, or non-linear pathway structure.)

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: SUMMER's retrieval step is RAG applied to experimental outcomes rather than documents; understanding embedding-based retrieval helps debug retrieval quality.
  - Quick check question: If retrieved examples all show "no effect" but the query gene pair is actually DE, what retrieval strategy change might help? (Answer: Expand neighborhood definition or retrieve based on pathway membership rather than direct graph proximity.)

## Architecture Onboarding

- **Component map:**
  Knowledge Graphs (UniProt, GO, STRING, etc.) -> Gene Summaries (per-gene .json files) -> Retrieved Examples (p', g', outcome) + Query Summaries -> Final Prediction (DE/no-DE, direction)

- **Critical path:**
  1. Verify knowledge graph parsing produces complete gene coverage (paper notes 97/11,234 genes missing GENEPT embeddings—check for similar gaps)
  2. Validate retrieval actually retrieves graph neighbors (logging neighbor overlap statistics)
  3. Confirm 8B model can follow 5-stage prompt structure (paper notes 8B has limited instruction following; monitor parse failures)

- **Design tradeoffs:**
  - 70B for summarization vs. 8B for inference: Computation cost vs. reasoning quality. Paper uses 70B only for summaries due to compute limits
  - Retrieval via graph proximity vs. embedding similarity: Graph proximity preserves biological structure; embedding similarity might retrieve functionally similar but topologically distant genes
  - Discrete outcomes vs. regression: Classification task aligns with downstream analysis but loses magnitude information

- **Failure signatures:**
  - High abstention rate: LLM outputs "insufficient evidence" frequently → retrieval not finding relevant examples or prompt too conservative
  - Directional errors: LLM infers A affects B when B is actually upstream of A → insufficient causal direction knowledge in summaries
  - Generic reasoning: LLM cites overly broad pathways ("involved in cellular processes") → summaries not specific enough or retrieval not finding close neighbors

- **First 3 experiments:**
  1. Ablation retrieval strategy: Compare graph-proximity retrieval vs. random retrieval vs. embedding-based retrieval on DE prediction AUROC. Expected: graph > embedding > random, but test
  2. Model capacity test: Run full SUMMER pipeline with 70B for both summarization and reasoning (if compute allows). Compare 8B vs. 70B on directional accuracy where paper notes 8B struggles
  3. Retrieval depth sweep: Vary k (number of neighbors) and number of retrieved examples (currently top-10 neighbors, 15 examples). Plot AUROC vs. retrieval budget to find inflection point

## Open Questions the Paper Calls Out

- Can language-based frameworks effectively predict the outcomes of combinatorial (multi-gene) perturbations? The authors focused on single gene perturbations due to limited evaluation data and leave combined effects as an opportunity for better datasets.
- How can LLMs be enhanced to accurately infer causal directionality between genes rather than just association? The paper identifies incorrect causal directionality as a common error where the model fails to distinguish if gene C is upstream or downstream of gene B.
- Does integrating foundation models trained on raw biological data (e.g., single-cell or sequence models) with language models improve perturbation prediction? While the paper used natural language for knowledge representation, multimodal integration of foundation models could be a promising future direction.

## Limitations
- SUMMER's performance improvements over traditional methods are modest (2-5% AUROC gains), suggesting LLMs are competitive but not transformative
- The framework struggles with causal directionality, a key limitation the authors identify as a common failure mode
- Current benchmarks lack sufficient data for evaluating combinatorial perturbation predictions, limiting the scope of current validation

## Confidence

- **High Confidence:** LLMs can effectively capture complex biological relationships for perturbation modeling, well-supported by quantitative results matching state-of-the-art methods
- **Medium Confidence:** Textual representation preserves semantic richness compared to adjacency matrices, plausible but not definitively proven against graph neural networks
- **Low Confidence:** SUMMER demonstrates "potential for biological discovery," aspirational as benchmark focuses on classification rather than novel hypothesis generation

## Next Checks

1. **Causal Directionality Benchmark:** Create targeted evaluation set where gene A is known to causally affect gene B (verified through time-series or genetic interaction data) and measure SUMMER's accuracy on directional prediction.

2. **Knowledge Graph Fidelity Analysis:** Measure coverage and accuracy of gene summaries by comparing LLM-generated summaries against manually curated biological pathway descriptions for sample genes, calculating precision/recall of key biological relationships.

3. **Retrieval Quality Audit:** For 50 random query pairs, manually inspect top-5 retrieved examples to determine: (a) what percentage are true graph neighbors, (b) what percentage contain relevant experimental outcomes, and (c) whether retrieval improves when expanding beyond immediate graph neighborhood.