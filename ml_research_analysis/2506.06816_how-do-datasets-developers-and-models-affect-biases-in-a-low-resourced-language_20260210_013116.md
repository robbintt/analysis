---
ver: rpa2
title: How do datasets, developers, and models affect biases in a low-resourced language?
arxiv_id: '2506.06816'
source_url: https://arxiv.org/abs/2506.06816
tags:
- datasets
- language
- biases
- bengali
- fine-tuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates identity-based biases in Bengali sentiment
  analysis models, focusing on gender, religion, and nationality dimensions. The researchers
  audited 38 fine-tuned models using mBERT and BanglaBERT, trained on 19 Bengali sentiment
  analysis datasets.
---

# How do datasets, developers, and models affect biases in a low-resourced language?

## Quick Facts
- arXiv ID: 2506.06816
- Source URL: https://arxiv.org/abs/2506.06816
- Authors: Dipto Das; Shion Guha; Bryan Semaan
- Reference count: 40
- This study finds significant identity-based biases in Bengali sentiment analysis models, with no dataset being completely free from bias.

## Executive Summary
This study investigates identity-based biases in Bengali sentiment analysis models across gender, religion, and nationality dimensions. Using 38 fine-tuned models (mBERT and BanglaBERT on 19 Bengali sentiment datasets), researchers found significant biases: 61% of models favored male identities, 61% favored Muslim identities, and 50% favored Bangladeshi identities. The study found no correlation between dataset developers' demographic backgrounds and model biases. BanglaBERT models exhibited less bias than mBERT, highlighting the value of language-specific models over multilingual ones. No dataset was completely free from bias, with models showing varying degrees of fairness across different identity dimensions.

## Method Summary
The study fine-tuned two pre-trained language models (mBERT and BanglaBERT) on 19 Bengali sentiment analysis datasets, then audited them using the Bengali Identity Bias Evaluation Dataset (BIBED). Statistical tests (chi-squared, paired t-test, or Wilcoxon signed-rank) were applied to compare sentiment scores between identity groups, with bias declared if p < 0.01 and power ≥ 0.8. The Positive Classification Rate (PCR) and Pairwise Comparison Metric (PCM) were calculated across 10 data splits to quantify bias. The audit evaluated 38 models (2 models × 19 datasets) for biases across gender, religion, and nationality dimensions.

## Key Results
- 61% of models favored male identities, 61% favored Muslim identities, and 50% favored Bangladeshi identities
- BanglaBERT models exhibited less bias than mBERT across all three identity dimensions
- No dataset was completely free from bias, with varying degrees of fairness across identity dimensions
- No correlation found between dataset developers' demographic backgrounds and model biases

## Why This Works (Mechanism)
The study's mechanism relies on comparing sentiment scores between identity pairs in controlled sentence pairs from BIBED. By using statistical tests with power analysis and multiple evaluation metrics (PCR, PCM), the approach can detect both statistically significant and practically meaningful biases. The use of both multilingual (mBERT) and language-specific (BanglaBERT) models allows for comparison of how model architecture affects bias manifestation in low-resource language contexts.

## Foundational Learning
- **Bias auditing methodology**: Understanding how to systematically evaluate AI models for unfair treatment across identity dimensions is essential for responsible AI development. Quick check: Can you explain the difference between statistical significance and practical significance in bias detection?
- **Power analysis in statistical testing**: Ensures that detected differences are not due to sample size limitations. Quick check: Can you calculate the minimum effect size detectable with given sample sizes and significance levels?
- **Pairwise comparison metrics**: PCM helps quantify the magnitude of bias beyond simple statistical significance. Quick check: Can you compute PCM for a simple set of sentiment scores?
- **Low-resource language challenges**: Highlights how data scarcity and model selection impact fairness outcomes. Quick check: Can you identify the key differences between multilingual and monolingual model training approaches?
- **Epistemic injustice**: Connects technical findings to broader social justice implications in AI. Quick check: Can you explain how algorithmic bias perpetuates epistemic injustice?

## Architecture Onboarding

**Component Map:**
Pre-trained models (mBERT, BanglaBERT) -> Fine-tuning on sentiment datasets -> Bias evaluation using BIBED -> Statistical analysis and metrics calculation

**Critical Path:**
1. Fine-tuning pre-trained models on sentiment datasets
2. Running inference on BIBED for each fine-tuned model
3. Performing statistical tests and calculating bias metrics
4. Aggregating results across models and identity dimensions

**Design Tradeoffs:**
- Using mBERT (multilingual) vs. BanglaBERT (language-specific) trades generalization capability against potential for reduced bias
- Binary classification (positive/negative) vs. multi-class sentiment analysis affects granularity of bias detection
- Statistical significance threshold (p < 0.01) vs. practical significance measures balances rigor with real-world impact

**Failure Signatures:**
- High false positive rate in bias detection due to inappropriate statistical tests
- Inconsistent bias measurements across different random data splits
- Failure to detect intersectional biases when only examining individual identity dimensions

**First 3 Experiments:**
1. Run normality tests on sentiment scores before choosing between t-test and Wilcoxon signed-rank test
2. Compare PCR and PCM results across different random seeds to assess metric stability
3. Analyze bias patterns in BanglaBERT vs. mBERT models on the same datasets to confirm relative bias differences

## Open Questions the Paper Calls Out
- How do biases manifest at the intersection of gender, religion, and nationality identities in Bengali sentiment analysis models? (The study analyzed dimensions individually using binary categories, but future work should investigate intersectional biases.)
- Does calculating the absolute difference of sentiment scores provide a more accurate Pairwise Comparison Metric (PCM) for quantifying bias than the current numerical difference method? (The authors observed inconsistencies between PCM and Positive Classification Rate metrics.)
- How do the specific decisions made by dataset developers regarding data curation and model selection in low-resource contexts influence the resulting model biases? (The quantitative analysis found no correlation, but qualitative investigation is needed.)

## Limitations
- The 19 training datasets are anonymized as D1-D19, preventing exact replication of fine-tuning results
- The study focuses on only three identity dimensions (gender, religion, nationality) in Bengali, potentially missing other important bias sources
- No correlation found between dataset developers' demographic backgrounds and model biases, but limited demographic information was available

## Confidence
- **High Confidence**: BanglaBERT models exhibited less bias than mBERT across all three identity dimensions
- **Medium Confidence**: No dataset was completely free from bias, with 61% of models favoring male identities, 61% favoring Muslim identities, and 50% favoring Bangladeshi identities
- **Low Confidence**: No correlation between dataset developers' demographic backgrounds and model biases due to limited demographic information

## Next Checks
1. Generate distribution plots of sentiment scores for each identity group in BIBED to verify normality assumptions and ensure correct selection between t-test and Wilcoxon signed-rank test
2. When matching alternative Bengali sentiment datasets to anonymized categories, verify dataset size, class balance, and basic statistics against inferable characteristics from results
3. Replicate PCR and PCM calculations using 10 random splits of BIBED, experimenting with different random seeds to understand sensitivity of quantified bias metrics