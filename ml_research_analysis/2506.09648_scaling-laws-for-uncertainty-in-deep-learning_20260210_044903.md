---
ver: rpa2
title: Scaling Laws for Uncertainty in Deep Learning
arxiv_id: '2506.09648'
source_url: https://arxiv.org/abs/2506.09648
tags:
- uncertainty
- data
- scaling
- learning
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether scaling laws\u2014predictable\
  \ relationships between model performance and dataset/model size\u2014apply to predictive\
  \ uncertainties in deep learning. The authors empirically show that epistemic uncertainty\
  \ (EU), aleatoric uncertainty (AU), and total uncertainty (TU) in Bayesian and ensemble\
  \ methods follow power-law trends with respect to dataset and model size across\
  \ vision and language tasks."
---

# Scaling Laws for Uncertainty in Deep Learning

## Quick Facts
- arXiv ID: 2506.09648
- Source URL: https://arxiv.org/abs/2506.09648
- Authors: Mattia Rosso; Simone Rossi; Giulio Franzese; Markus Heinonen; Maurizio Filippone
- Reference count: 40
- Key outcome: Epistemic, aleatoric, and total predictive uncertainty in deep learning follow power-law scaling with dataset and model size across vision and language tasks, with theoretical links to generalization error.

## Executive Summary
This paper investigates whether scaling laws apply to predictive uncertainties in deep learning, extending the concept from performance to uncertainty quantification. The authors empirically demonstrate that epistemic uncertainty (EU), aleatoric uncertainty (AU), and total uncertainty (TU) follow predictable power-law relationships with respect to dataset size N and model size P across diverse architectures including ResNet, ViT, GPT-2, and Bayesian neural networks. Theoretical analysis connects uncertainty scaling to generalization error through Singular Learning Theory, showing that EU decays as O(1/N) while AU plateaus. These findings challenge skepticism about Bayesian approaches by showing non-negligible epistemic uncertainty persists even with large datasets.

## Method Summary
The study examines uncertainty scaling across multiple architectures (ResNet, ViT, GPT-2, BNNs) and tasks (CIFAR-10/100, ImageNet-32, language modeling, synthetic datasets). Three uncertainty metrics are computed: Total Uncertainty (TU = predictive entropy), Aleatoric Uncertainty (AU = mean entropy), and Epistemic Uncertainty (EU = TU - AU). Experiments use MC Dropout (p∈{0.2, 0.5}), Deep Ensembles (M∈{5,10}), and MCMC/SGHMC methods. Models are trained with SGD (momentum 0.9, weight decay 5×10⁻⁴) for 400 epochs, with data subsampled at 25%, 50%, 75%, 100%. Power-law exponents γ are fitted via log-log linear regression: log(U) = γ·log(N) + const. Results are averaged over 3-10 seeds.

## Key Results
- Epistemic uncertainty (EU) decays as O(1/N) with dataset size across all architectures
- Aleatoric uncertainty (AU) plateaus as dataset size increases, indicating irreducible noise
- Uncertainty scaling exponents are consistent across vision and language tasks (γ_EU ≈ -0.3 to -0.6 for CIFAR-10)
- Model size scaling shows negative γ_EU for ViT but near-zero for ResNet due to parameter permutation symmetries

## Why This Works (Mechanism)
The power-law scaling emerges from the interplay between model capacity, data distribution coverage, and the geometry of the loss landscape. As dataset size increases, the posterior distribution concentrates, reducing epistemic uncertainty. The decay rate follows from the local learning coefficients in Singular Learning Theory, which characterize the complexity of the model around the true parameter. Aleatoric uncertainty plateaus because irreducible noise in the data remains constant regardless of sample size. The different scaling behaviors between EU and AU reflect their distinct sources: EU stems from lack of knowledge about true parameters, while AU represents inherent data variability.

## Foundational Learning

**Bayesian Neural Networks**: Probabilistic models that place distributions over network weights to capture uncertainty in predictions. Needed to compute epistemic uncertainty; quick check: verify posterior variance decreases with more data.

**Singular Learning Theory**: Framework analyzing statistical models with singular Fisher information matrices, providing theoretical basis for power-law generalization bounds. Needed to connect uncertainty scaling to generalization; quick check: confirm effective dimensionality decreases with more data.

**MC Dropout**: Training technique using dropout at both train and test time to approximate Bayesian inference. Needed for scalable uncertainty estimation; quick check: ensure dropout remains active during inference.

**Power-Law Scaling**: Mathematical relationship where a quantity scales as a power of another quantity. Needed to characterize uncertainty decay/growth rates; quick check: verify log-log linearity with R² > 0.8.

## Architecture Onboarding

**Component Map**: Data subsampling -> Model training (with uncertainty method) -> Uncertainty computation (TU, AU, EU) -> Log-log regression -> Exponent extraction

**Critical Path**: Dataset preparation → Model architecture selection → Uncertainty method implementation → Training with proper convergence → Test-time MC sampling → Power-law fitting

**Design Tradeoffs**: MC Dropout offers scalability but may underestimate true posteriors vs. MCMC; ensembles provide better uncertainty but at higher computational cost; wider models improve uncertainty calibration but increase training time.

**Failure Signatures**: AU decreases instead of plateauing (indicates training issues or insufficient MC samples); EU shows flat scaling with model size (expected for ResNet due to symmetries); poor log-log fit (suggests non-convergence or inadequate sampling).

**First Experiments**: 1) ResNet-18 with MC Dropout on CIFAR-10 at 4 data sizes; 2) ViT scaling with model size on CIFAR-10; 3) GPT-2 uncertainty scaling on QQP dataset

## Open Questions the Paper Calls Out

**Open Question 1**: Can empirical uncertainty scaling exponents (γ) be predicted theoretically from Singular Learning Theory invariants or effective dimensionality? The authors note this is a key limitation, as current theoretical connections are limited to linear models and extending to deep networks is mathematically complex.

**Open Question 2**: How do optimization strategies and learning rate schedules mechanistically alter uncertainty scaling laws? The paper notes optimization effects are only partially characterized and doesn't explain when simple settings yield clearer scaling than complex ones.

**Open Question 3**: How does predictive uncertainty scale with computational budget, and does this relationship converge to wide-network Gaussian Process limits? The authors propose studying this relationship and relating it to theoretical limits like wide-network GP approximations.

## Limitations
- Relies on MC Dropout and ensemble methods which may underestimate true Bayesian posteriors
- Sensitivity to training hyperparameters and optimization choices
- Lack of formal uncertainty quantification for the fitted scaling exponents themselves
- MCMC experiments are sparse and less detailed

## Confidence
- High: Robust empirical evidence for power-law scaling across multiple architectures and tasks
- Medium: Theoretical link to generalization error via singular learning theory is plausible but mechanisms need more work
- Low: MCMC results are less detailed and warrant cautious interpretation

## Next Checks
1. Verify uncertainty scaling holds with alternative uncertainty estimation methods (e.g., SWAG, ensembling without MC sampling)
2. Test whether uncertainty scaling is preserved under distribution shift and adversarial perturbations
3. Confirm that scaling behavior is consistent across additional architectures (e.g., ConvNext, Llama) and diverse datasets (e.g., JFT, PubMed)