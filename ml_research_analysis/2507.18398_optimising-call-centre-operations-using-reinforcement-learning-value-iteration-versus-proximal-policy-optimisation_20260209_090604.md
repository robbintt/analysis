---
ver: rpa2
title: 'Optimising Call Centre Operations using Reinforcement Learning: Value Iteration
  versus Proximal Policy Optimisation'
arxiv_id: '2507.18398'
source_url: https://arxiv.org/abs/2507.18398
tags:
- staff
- client
- time
- routing
- simulation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper applies Reinforcement Learning (RL) to optimise call
  routing in call centres, aiming to minimise client waiting time and staff idle time.
  Two approaches are compared: Value Iteration (VI), a model-based method requiring
  full knowledge of system dynamics, and Proximal Policy Optimisation (PPO), a model-free
  method learning directly from experience.'
---

# Optimising Call Centre Operations using Reinforcement Learning: Value Iteration versus Proximal Policy Optimisation

## Quick Facts
- arXiv ID: 2507.18398
- Source URL: https://arxiv.org/abs/2507.18398
- Reference count: 0
- PPO outperforms VI and random routing in call centre simulation, achieving 98s average waiting time vs 114s (VI) and 127s (random).

## Executive Summary
This paper compares two reinforcement learning approaches for optimizing call routing in call centres: Value Iteration (VI), a model-based method requiring full system dynamics, and Proximal Policy Optimization (PPO), a model-free method learning from experience. Both methods frame the problem as a Markov Decision Process within a skills-based routing framework. PPO achieves superior performance across most metrics, including lowest client waiting time (98s) and highest total reward (-76,892), despite longer training time, demonstrating its adaptability in stochastic environments where model-based approaches struggle.

## Method Summary
The call centre environment is modelled as an MDP with Poisson arrivals, exponentially distributed service and abandonment times, and a skills-based routing framework. Two approaches are compared: VI using theoretical transition probabilities and PPO learning directly from simulation. A Discrete Event Simulation engine is integrated into OpenAI Gym to create a realistic, stochastic environment. Three routing policies (random, VI, PPO) are evaluated over 1,000 episodes, with state defined by queue lengths and inquiry type, and actions assigning calls to staff. PPO uses clipped policy updates for stable learning without requiring explicit transition models.

## Key Results
- PPO achieves highest total reward (-76,892) and lowest client waiting time (98s) among three policies
- PPO serves more clients (366) with fewer abandonments (163) compared to VI (344 served, 185 abandoned)
- VI requires only 0.12s training time versus PPO's 2,475s, but underperforms in simulation metrics
- Both methods outperform random routing (-98,900 reward, 127s waiting time)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model-free PPO adapts to stochastic dynamics without requiring explicit transition probability models.
- Mechanism: PPO samples trajectories through environment interaction and updates policy gradients directly, bypassing the need to estimate P(s'|s,a). The clipped objective prevents overly large policy updates, enabling stable learning under high variance from Poisson arrivals and exponential service times.
- Core assumption: The environment's stochastic dynamics can be sufficiently characterized through sampled experience; stationarity holds during training.
- Evidence anchors:
  - [abstract] "PPO...model-free method learning directly from experience...achieves the highest total reward (-76,892) and lowest client waiting time (98s)"
  - [section p.2] "Model-free methods learn directly from interaction, using sampled trajectories to estimate value functions or optimise policies without relying on known dynamics"
  - [corpus] Neighbor paper on value estimation notes PPO's enhanced value estimation accuracy contributes to performance, though mechanism differs from trust region enforcement
- Break condition: If environment non-stationarity increases sharply (e.g., time-varying arrival rates), PPO may require continuous retraining; performance gains may diminish if sample efficiency is critical.

### Mechanism 2
- Claim: MDP state representation capturing queue lengths and inquiry types enables Markovian decision-making.
- Mechanism: State space S=(n₀, n₁, τ) encodes both staff queue occupancies and incoming inquiry type. This satisfies the Markov property—the future depends only on current state and action, not history—allowing both VI and PPO to learn consistent routing policies.
- Core assumption: Inquiry classification is accurate at arrival; no hidden state (e.g., client patience level) affects outcomes beyond what's captured in queue length.
- Evidence anchors:
  - [abstract] "Both models frame the problem as a Markov Decision Process (MDP) within a Skills-Based Routing (SBR) framework"
  - [section p.3] "The environment's state is defined by three variables: the number of clients in Staff 0's queue (n₀), the number in Staff 1's queue (n₁), and the type of inquiry being routed (τ)"
  - [corpus] No direct corpus support for this specific MDP formulation; queueing network RL papers (Liu et al., Dai & Gluzman) cited but not in neighbor corpus
- Break condition: If queue state alone insufficient (e.g., priority clients, multi-skill agents), state space must expand; scalability becomes a concern.

### Mechanism 3
- Claim: Reward shaping with per-second penalties and abandonment costs guides policy toward operational objectives.
- Mechanism: The simulation reward R(s,a) applies -1 per second of waiting/idle time and -125 for abandonment or full-queue assignment. This dense feedback correlates routing decisions directly with KPIs, enabling PPO to learn balanced workload distribution.
- Core assumption: The -125 penalty constant (derived from ~126s average random-waiting) appropriately weights abandonment avoidance against waiting time reduction.
- Evidence anchors:
  - [section p.7] "A penalty of −125 is applied when assigning a client to a full staff queue or when a client abandons...the average client waiting time was approximately 126 seconds from 1,000 runs of random policy"
  - [section p.8] "PPO demonstrates the best performance across most metrics...best balance of idle time across both staff"
  - [corpus] Corpus papers on PPO do not address reward shaping for queueing systems; mechanism-specific evidence limited to this paper
- Break condition: If penalty weights misalign with true business costs (e.g., abandonment cost >> waiting cost), learned policy may optimize wrong objective.

## Foundational Learning

- Concept: **Markov Decision Processes (MDPs)**
  - Why needed here: Both VI and PPO require formalizing the routing problem as an MDP with defined states, actions, transitions, and rewards.
  - Quick check question: Can you explain why the state (n₀, n₁, τ) satisfies the Markov property for call routing?

- Concept: **Model-Based vs Model-Free RL**
  - Why needed here: The paper's core comparison hinges on understanding when knowing transition probabilities (VI) is feasible vs when learning from interaction (PPO) is preferable.
  - Quick check question: What information does VI require that PPO does not?

- Concept: **Discrete Event Simulation (DES)**
  - Why needed here: The simulation model uses an event queue (arrival, departure, abandonment) to drive Gym environment steps, differing from time-stepped RL.
  - Quick check question: In DES, how does the simulation clock advance compared to fixed-timestep environments?

## Architecture Onboarding

- Component map:
  - DES Engine -> OpenAI Gym Wrapper -> PPO Agent -> Staff Pool
  - DES Engine manages event queue (priority queue), schedules arrival/departure/abandonment events from Poisson/exponential distributions
  - OpenAI Gym Wrapper exposes standard reset()/step() interface; consolidates background event rewards into arrival-step returns
  - PPO Agent neural policy taking state (n₀, n₁, τ) → action (staff assignment)
  - Staff Pool maintains per-staff FIFO queues, handles service completion logic

- Critical path:
  1. Arrival event triggers Gym step()
  2. Agent observes state, selects action (route to Staff 0 or 1)
  3. Environment updates staff queue, schedules departure/abandonment
  4. Background events processed passively; rewards consolidated at next arrival
  5. Return (next_state, reward, done) to agent

- Design tradeoffs:
  - Event-driven rewards (at arrivals only) vs continuous time-stepped rewards—former computationally efficient but introduces approximation error in cumulative waiting/idle time measurement
  - Fixed max queue length (14) vs dynamic—prevents truncation but may not reflect true capacity constraints
  - Two-staff, two-inquiry simplification vs scalability—aids interpretability but limits real-world fidelity

- Failure signatures:
  - PPO training unstable or reward diverging: Check learning rate, clip ratio; ensure reward scaling reasonable
  - VI policy underperforms in simulation: Likely model mismatch—theoretical transition assumptions don't hold in stochastic DES
  - High abandonment despite training: Reward penalty may be insufficient; increase abandonment cost

- First 3 experiments:
  1. Reproduce random/VI/PPO comparison on 100 episodes to validate implementation; verify PPO reward curve convergence around 2M timesteps.
  2. Ablate reward function: Test PPO with only waiting-time penalty (no abandonment cost) to isolate reward-shaping impact.
  3. Sensitivity analysis: Vary arrival rate (inter-arrival time ±20%) to test policy robustness under load changes.

## Open Questions the Paper Calls Out
- How does the performance of the PPO agent scale when expanding the environment to include a larger number of staff agents and inquiry types?
- Can a continuous-time reward model reduce the approximation errors inherent in the current event-driven reward structure?
- How robust is the PPO policy when exposed to non-stationary conditions, such as time-of-day effects or peak periods?

## Limitations
- Reward approximation error: Event-driven simulation computes waiting/idle time only at arrivals, potentially underestimating cumulative penalties by ~5-10% per episode
- Scalability concerns: Two-staff, two-inquiry model simplifies real call centres; state-action space grows exponentially with agents/inquiry types
- Hyperparameter opacity: PPO performance depends critically on learning rate, batch size, and network architecture, none of which are specified

## Confidence
- **High**: PPO outperforms VI and random routing in simulation metrics (reward, waiting time, clients served). This is directly measured and reproducible.
- **Medium**: PPO's superior adaptability stems from model-free learning in stochastic environments. While supported by mechanism, real-world non-stationarity could invalidate this advantage.
- **Low**: Theoretical VI reward calculation accurately reflects simulation dynamics. VI assumes deterministic transitions that don't hold in Poisson/exponential DES, explaining performance gap.

## Next Checks
1. **Hyperparameter sensitivity**: Systematically vary PPO learning rate (1e-4 to 1e-3) and batch size (32-128) to identify optimal settings and test robustness of reported performance.
2. **Reward approximation accuracy**: Implement continuous time-stepped reward calculation alongside event-driven method; quantify approximation error and its impact on learned policies.
3. **Generalization test**: Evaluate trained PPO policies on arrival rates ±20% from training distribution; measure performance degradation to assess real-world applicability.