---
ver: rpa2
title: 'The Rise of Small Language Models in Healthcare: A Comprehensive Survey'
arxiv_id: '2504.17119'
source_url: https://arxiv.org/abs/2504.17119
tags:
- language
- arxiv
- healthcare
- medical
- slms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive survey of small language
  models (SLMs) in healthcare, identifying 20 SLMs and establishing a taxonomy for
  their development, adaptation, and compression. The study defines SLMs as models
  up to 7B parameters, optimized for healthcare tasks under resource constraints.
---

# The Rise of Small Language Models in Healthcare: A Comprehensive Survey

## Quick Facts
- arXiv ID: 2504.17119
- Source URL: https://arxiv.org/abs/2504.17119
- Reference count: 40
- This paper presents the first comprehensive survey of small language models (SLMs) in healthcare, identifying 20 SLMs and establishing a taxonomy for their development, adaptation, and compression.

## Executive Summary
This paper provides the first comprehensive survey of small language models (SLMs) in healthcare, identifying 20 SLMs and establishing a taxonomy for their development, adaptation, and compression. The study defines SLMs as models up to 7B parameters, optimized for healthcare tasks under resource constraints. Experimental results demonstrate competitive performance across medical question answering, natural language understanding, and mental health analysis, with models like BioMedLM, BioMistral-DARE, and MentalQLM showing promising results. The survey highlights SLMs' potential to reduce carbon emissions by up to 80% compared to large models and emphasizes their role in enabling privacy-preserving, on-device clinical decision support.

## Method Summary
The survey employed a systematic approach to identify and analyze small language models in healthcare. The methodology involved comprehensive literature review to identify 20 SLMs, followed by development of a taxonomy categorizing these models by architecture, data curation, pretraining components, and attention mechanisms. The study analyzed performance metrics across multiple healthcare tasks including medical question answering (up to 81.06% accuracy), natural language understanding (up to 63.4% F1), and mental health analysis (up to 84.20% weighted F-measure). The research focused on efficient deployment strategies and comparative analysis of resource utilization, including carbon emission estimates.

## Key Results
- Models like BioMedLM, BioMistral-DARE, and MentalQLM achieve competitive performance across medical question answering (up to 81.06% accuracy), natural language understanding (up to 63.4% F1), and mental health analysis (up to 84.20% weighted F-measure)
- SLMs demonstrate potential to reduce carbon emissions by up to 80% compared to large models
- SLMs enable privacy-preserving, on-device clinical decision support capabilities

## Why This Works (Mechanism)
Small Language Models work effectively in healthcare by leveraging efficient architectures and optimization techniques that maintain performance while reducing computational requirements. Their success stems from targeted pretraining on domain-specific healthcare data, specialized attention mechanisms, and compression techniques that preserve essential knowledge while minimizing parameter count. The smaller model size enables faster inference times, lower energy consumption, and deployment on resource-constrained devices without compromising task-specific accuracy in medical applications.

## Foundational Learning
1. **Healthcare Domain Adaptation**: SLMs require specialized training on medical datasets to understand clinical terminology and contexts. Why needed: General language models lack the domain-specific knowledge required for accurate medical diagnosis and decision support. Quick check: Verify model performance on domain-specific benchmarks versus general healthcare tasks.

2. **Parameter Efficiency**: Understanding how models achieve high performance with limited parameters through architectural optimizations. Why needed: Resource constraints in healthcare settings demand models that balance accuracy with computational efficiency. Quick check: Compare parameter-to-performance ratios across different model architectures.

3. **Privacy-Preserving Computation**: SLMs enable on-device processing that maintains patient data confidentiality. Why needed: Healthcare regulations require strict data protection, making cloud-based inference problematic for sensitive medical information. Quick check: Assess data flow and processing locations during model deployment.

4. **Energy Efficiency Metrics**: Understanding carbon footprint measurements and energy consumption patterns. Why needed: Healthcare organizations increasingly prioritize sustainable AI solutions with measurable environmental impact. Quick check: Validate carbon emission estimates through actual deployment measurements.

5. **Attention Mechanism Optimization**: Specialized attention patterns for medical text processing. Why needed: Healthcare documents have unique structural and semantic characteristics requiring adapted attention mechanisms. Quick check: Analyze attention weight distributions on medical vs. general text.

## Architecture Onboarding

**Component Map**: Data Curation -> Model Architecture -> Pretraining -> Fine-tuning -> Deployment
- Data Curation -> Model Architecture: Specialized datasets inform architectural choices
- Model Architecture -> Pretraining: Foundation for learning medical language patterns
- Pretraining -> Fine-tuning: Domain-specific adaptation for healthcare tasks
- Fine-tuning -> Deployment: Optimization for clinical environments

**Critical Path**: Data Curation → Model Architecture → Pretraining → Fine-tuning → Deployment

**Design Tradeoffs**: Model size vs. performance (smaller models sacrifice some accuracy but gain deployment flexibility), computational efficiency vs. comprehensiveness (optimized models may miss rare medical cases), privacy vs. model sophistication (on-device processing limits model complexity), energy consumption vs. inference speed (efficient models may require more inference steps)

**Failure Signatures**: Degradation in rare disease recognition, inability to handle complex multi-step reasoning, poor performance on unstructured clinical notes, increased hallucination rates with aggressive compression

**First Experiments**:
1. Benchmark SLM performance on standard medical QA datasets (MedQA, PubMedQA) to establish baseline capabilities
2. Compare inference latency and energy consumption across different hardware platforms (CPU, GPU, edge devices)
3. Evaluate model robustness through adversarial testing with medical misinformation and ambiguous clinical scenarios

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- The survey exclusively focuses on published papers, potentially missing important SLM developments in preprints, technical reports, or commercial implementations
- The classification system may not fully capture hybrid architectures or emerging approaches that combine multiple compression techniques
- Carbon emission reduction estimates (up to 80%) are based on theoretical comparisons rather than measured real-world deployments

## Confidence

- **High Confidence**: The taxonomic framework for categorizing SLMs by architecture, data curation, and attention mechanisms is methodologically sound and well-supported by the identified 20 models.
- **Medium Confidence**: Performance metrics for specific models (BioMedLM, BioMistral-DARE, MentalQLM) are based on reported results but may vary across different clinical contexts and datasets not covered in the survey.
- **Medium Confidence**: The claim about SLMs enabling privacy-preserving, on-device clinical decision support is theoretically supported but lacks extensive empirical validation in actual healthcare environments.

## Next Checks

1. Conduct a systematic review of preprints and technical reports from major AI conferences (NeurIPS, ICML, ICLR) to identify additional SLMs not captured in peer-reviewed publications.

2. Perform real-world deployment studies measuring actual carbon emissions and energy consumption of SLMs versus large models in clinical settings across different hardware configurations.

3. Develop and validate a standardized benchmark suite specifically for healthcare SLMs that includes diverse clinical scenarios, language variations, and bias detection metrics beyond current question-answering and NLU tasks.