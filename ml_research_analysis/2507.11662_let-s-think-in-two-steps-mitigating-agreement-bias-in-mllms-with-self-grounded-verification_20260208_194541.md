---
ver: rpa2
title: 'Let''s Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded
  Verification'
arxiv_id: '2507.11662'
source_url: https://arxiv.org/abs/2507.11662
tags:
- agent
- task
- verifier
- mllm
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work identifies and addresses a critical limitation in using
  multimodal large language models (MLLMs) as verifiers of agent behavior, termed
  "agreement bias." MLLMs tend to over-validate agent behavior, judging flawed trajectories
  as correct, which undermines their ability to provide corrective feedback. The authors
  introduce Self-Grounded Verification (SGV), a two-step method that first elicits
  broad priors about desired behavior from the MLLM, then conditions verification
  on these self-generated priors.
---

# Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification

## Quick Facts
- arXiv ID: 2507.11662
- Source URL: https://arxiv.org/abs/2507.11662
- Reference count: 40
- Primary result: Two-step SGV method improves MLLM verification accuracy by up to 14 percentage points, enabling 10 percentage point (24% relative) downstream task completion gains

## Executive Summary
This work identifies and addresses a critical limitation in using multimodal large language models (MLLMs) as verifiers of agent behavior, termed "agreement bias." MLLMs tend to over-validate agent behavior, judging flawed trajectories as correct, which undermines their ability to provide corrective feedback. The authors introduce Self-Grounded Verification (SGV), a two-step method that first elicits broad priors about desired behavior from the MLLM, then conditions verification on these self-generated priors. SGV substantially improves verification performance across models and benchmarks, yielding up to 25 percentage points higher failure detection rates, 14 percentage points higher accuracy, and producing evaluations more aligned with human judgments. These improvements extend to downstream applications: in self-refinement and online supervision, SGV boosts task completion by up to 10 percentage points (24% relative) on VisualWebArena, 5 percentage points (22% relative) on OSWorld, and 8 percentage points (33% relative) on robomimic, setting a new state of the art on VisualWebArena. The method is lightweight, adds minimal token overhead, and integrates seamlessly into existing pipelines.

## Method Summary
SGV is a two-step verification method that mitigates agreement bias in MLLMs. First, the MLLM generates behavioral priors about desired task completion using only task framing (initial screenshot, objective). Second, these priors are used to condition the verification of agent trajectories. This approach breaks the anchoring effect where models rationalize observed behavior and instead grounds evaluation in task-relevant knowledge extracted before seeing the specific execution. The method adds minimal token overhead and works across different MLLM architectures, showing consistent improvements in failure detection rates and downstream task completion when integrated into self-refinement and online supervision systems.

## Key Results
- Improves failure detection rates by up to 25 percentage points (TNR: 76% → 89% on VisualWebArena-Lite)
- Increases accuracy by up to 14 percentage points (88% → 96% on VisualWebArena-Lite)
- Boosts downstream task completion by 10 percentage points (24% relative) on VisualWebArena
- Achieves 5 percentage points (22% relative) improvement on OSWorld and 8 percentage points (33% relative) on robomimic

## Why This Works (Mechanism)

### Mechanism 1: Unconditional Prior Extraction Breaks Context Anchoring
- **Claim:** Generating task-relevant priors before seeing the trajectory prevents the model from rationalizing flawed behavior.
- **Mechanism:** Standard verification conditions on both task and trajectory simultaneously, causing the model to anchor on observed actions and generate post-hoc justifications. SGV forces prior generation using only task framing (initial screenshot, objective), extracting knowledge from the model's distribution before it can be contaminated by the trajectory under evaluation.
- **Core assumption:** MLLMs encode human-aligned priors about correct behavior that are difficult to extract when conditioned on specific execution traces.
- **Evidence anchors:**
  - [abstract] "first, the MLLM is elicited to generate broad priors about desired behavior, independent of the data under evaluation"
  - [section 3.2] "conditioning only on essential information in the first step encourages the model to explore its probability distribution freely, extracting knowledge pertinent to the task at hand and independent of the data under evaluation"
  - [corpus] Weak direct evidence; neighbor papers address MLLM capabilities but not verification-specific anchoring.
- **Break condition:** If priors generated in step 1 are themselves biased or if the task framing is insufficient to elicit relevant knowledge, the conditioning in step 2 provides no benefit.

### Mechanism 2: Agreement Bias Stemming from RLHF Alignment
- **Claim:** MLLMs systematically over-validate because RLHF training conflates human rater satisfaction with truthfulness.
- **Mechanism:** Models trained with RLHF learn to produce responses that satisfy human evaluators. When presented with a trajectory, the model defaults to agreeable judgments rather than critical evaluation, generating chain-of-thought reasoning to rationalize favorable assessments even for flawed executions.
- **Core assumption:** The bias is inherent to current training paradigms, not merely a prompting artifact.
- **Evidence anchors:**
  - [abstract] "a strong tendency to over-validate agent behavior, a phenomenon we call agreement bias"
  - [section 3.2] "inherent limitations of RLHF can make models conflate human rater satisfaction with truthfulness"
  - [corpus] No direct corpus support for this specific mechanism; related work on LLM biases exists but doesn't address verification contexts.
- **Break condition:** If agreement bias were primarily a prompting artifact, careful prompt engineering would eliminate it. The paper shows it persists across 28+ prompt templates.

### Mechanism 3: Distribution Modulation Via Self-Conditioning
- **Claim:** Conditioning verification on self-generated priors shifts the output distribution toward more balanced judgments.
- **Mechanism:** Standard MLLM verification produces skewed distributions concentrated at high confidence scores. SGV's two-step process modulates between unconditional (prior generation) and conditional (verification) sampling, resulting in distributions that better align with ground truth and reduce false positive rates by up to 25 percentage points.
- **Core assumption:** The model's internal knowledge is more accurate than its conditioned outputs when evaluating specific trajectories.
- **Evidence anchors:**
  - [section 4, Table 3] "MLLMs tend to concentrate responses in high score regions... SGV leads to a more balanced distribution of scores for all templates"
  - [section 4] "on failure subsets, the probability of sampling a correct response from the MLLM is at or below chance (e.g., 48% in VWA)"
  - [corpus] Weak; neighbor papers don't address distributional properties of MLLM judgments.
- **Break condition:** If the verifier is significantly weaker than the agent being verified, prior generation may not capture sufficient signal for effective conditioning.

## Foundational Learning

- **Concept: Conditional vs. Unconditional Generation in Autoregressive Models**
  - Why needed here: SGV's core insight relies on understanding how conditioning context shifts model outputs.
  - Quick check question: Can you explain why generating priors without the trajectory present might yield different content than generating them with the trajectory visible?

- **Concept: Test-Time Scaling and Its Limitations**
  - Why needed here: The paper shows agreement bias is "resilient to test-time scaling" (CoT, majority voting, thinking models), which is counterintuitive.
  - Quick check question: Why might increasing inference compute not fix a systematic bias in model outputs?

- **Concept: Verifier Applications (Offline vs. Online)**
  - Why needed here: The paper evaluates verifiers across trajectory evaluation, self-refinement (Reflexion), and online supervision, each with different failure modes.
  - Quick check question: In which application would false positives be most harmful—filtering training data or providing real-time feedback?

## Architecture Onboarding

- **Component map:**
  - Agent -> Environment -> SGV Verifier -> Oracle
  - Agent generates trajectories (ReAct, UI-TARS, diffusion policy)
  - Environment provides screenshots, actions, rewards (VisualWebArena, OSWorld, robomimic)
  - SGV Verifier: two-stage—(1) Prior Generator: takes task + initial state, outputs behavioral priors; (2) Evaluator: takes priors + trajectory, outputs judgment + feedback
  - Oracle: script-based ground truth for validation

- **Critical path:**
  1. Fix environment bugs (see Section F.2—critical for reliable evaluation)
  2. Implement baseline verifier with ternary Likert scale (SUCCESS/PARTIAL/FAILURE)
  3. Add SGV step: separate prompt for prior generation before verification
  4. Measure TNR (true negative rate) as primary metric—accuracy alone is misleading

- **Design tradeoffs:**
  - **Leniency vs. Strictness:** SGV can reduce TPR (true positive rate) by enforcing stricter criteria than oracle scripts. Paper argues this produces more generalizable behavior.
  - **Outcome vs. Process Verification:** Periodic verification (every 5 steps) outperforms outcome-only in OSWorld where actions can be destructive.
  - **Model Selection:** Weaker models can generate effective priors for stronger verifiers (Section E.8), reducing cost.

- **Failure signatures:**
  - **Rationalized false positives:** Model generates plausible CoT reasoning that justifies incorrect SUCCESS judgments (Figure 7)
  - **Perception failures:** MLLM fails on fine-grained visual tasks (counting, UI element recognition)—SGV doesn't fix this
  - **Over-strict verification:** False negatives that reject correct but suboptimal trajectories (Figure 16)

- **First 3 experiments:**
  1. **Replicate agreement bias on VisualWebArena-Lite:** Run baseline verifier on 305-task subset, measure TNR and bias. Expect TNR ~40-50%, bias >20%.
  2. **Implement minimal SGV:** Add single prior-generation step using existing verifier prompt with trajectory removed. Measure TNR improvement (target: +15-25pp).
  3. **Ablation on prior quality:** Compare (a) priors from same model, (b) priors from weaker model, (c) priors with injected noise. Verify robustness claim from Section E.8.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can modular approaches integrating visual experts with SGV improve verification accuracy in visually demanding scenarios?
- Basis in paper: [explicit] The authors note that remaining failures often stem from limitations in visual perception and suggest integrating visual experts as future work (Sec. 6).
- Why unresolved: Current MLLMs fail at fine-grained tasks like object counting or distinguishing similar UI elements, leading to false positives (Sec. C.1).
- What evidence would resolve it: Performance improvements on tasks requiring high visual acuity (e.g., UI element recognition) when SGV is paired with specialized perception modules.

### Open Question 2
- Question: What is the optimal schedule for invoking verifiers to balance the trade-offs between process-based and outcome-based verification?
- Basis in paper: [explicit] The authors identify the "relative value of process and outcome-based verification" and when to invoke a verifier as a key open question (Sec. 6).
- Why unresolved: Outcome-based verification is lightweight but risks missing irreversible errors (e.g., file deletion); process-based verification is safer but computationally heavier.
- What evidence would resolve it: Comparative analysis of success rates and token costs across different intervention frequencies (e.g., every step vs. every k steps) in environments with irreversible actions.

### Open Question 3
- Question: Can training-time modifications to account for output distribution skewness complement SGV in mitigating agreement bias?
- Basis in paper: [explicit] The authors suggest exploring training- or test-time strategies that account for the skewness in MLLM output distributions associated with agreement bias (Sec. 6, E.3).
- Why unresolved: SGV is an inference-time fix; it is unknown if training adjustments could address the root causes of the bias more effectively.
- What evidence would resolve it: Experiments where models are fine-tuned with loss functions penalizing output skew, showing reduced bias compared to standard RLHF baselines.

## Limitations
- Visual perception boundaries: SGV does not address fundamental limitations in MLLM visual grounding capabilities, particularly for fine-grained tasks like counting or detailed UI element recognition.
- Oracle dependency: Performance improvements rely on script-based oracles that may not capture human-level judgment complexity, especially for tasks with multiple valid solution paths.
- Model-specific behavior: While SGV works across models, the magnitude of improvement varies significantly (25pp for Gemini-2.5-Flash, 10pp for Qwen2.5-VL-3B), suggesting implementation sensitivity to model architecture.

## Confidence
- **Corpus and Neighbor Analysis Confidence: Medium** - Limited direct support; 25 related papers with average neighbor FMR of 0.425 and zero average citations
- **Agreement Bias Mechanism Confidence: High** - Well-supported within paper's own experiments; persists across 28+ prompt templates and test-time scaling approaches
- **Distribution Modulation Claims Confidence: Medium** - Demonstrated improved distributions but claims require further validation; reduced false positives but potential over-strictness noted

## Next Checks
1. **Cross-Domain Transfer**: Apply SGV to domains not covered in the paper (e.g., medical diagnosis verification, code review) to test generalizability beyond embodied tasks.

2. **Human Preference Alignment**: Conduct human evaluation comparing SGV-generated feedback to human judgments on a held-out test set to verify that improved TNR correlates with human-aligned assessments.

3. **Prior Generation Ablation**: Systematically test the impact of prior quality by using priors generated from progressively weaker models (or with injected noise) to quantify the robustness of the conditioning mechanism.