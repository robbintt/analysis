---
ver: rpa2
title: Exploring the Possibility of TypiClust for Low-Budget Federated Active Learning
arxiv_id: '2505.19404'
source_url: https://arxiv.org/abs/2505.19404
tags:
- learning
- data
- typiclust
- settings
- low-budget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates TypiClust, a successful low-budget active
  learning strategy, in federated active learning (FAL) settings where data cannot
  be shared and annotation is expensive. The study evaluates TypiClust against conventional
  and FAL-specific baselines across heterogeneous datasets (CINIC-10 with varying
  degrees of data heterogeneity, and ISIC2019) using tiny and small annotation budgets.
---

# Exploring the Possibility of TypiClust for Low-Budget Federated Active Learning

## Quick Facts
- **arXiv ID**: 2505.19404
- **Source URL**: https://arxiv.org/abs/2505.19404
- **Reference count**: 28
- **Primary result**: TypiClust significantly outperforms conventional and FAL-specific baselines in low-budget federated active learning, especially with heterogeneous data and simpler models.

## Executive Summary
This paper investigates TypiClust, a low-budget active learning strategy, in federated active learning settings where data cannot be shared and annotation is expensive. The study evaluates TypiClust against conventional and FAL-specific baselines across heterogeneous datasets (CINIC-10 with varying degrees of data heterogeneity, and ISIC2019) using tiny and small annotation budgets. Experimental results show TypiClust significantly outperforms other methods in low-budget FAL regimes, especially when using simpler models and with more heterogeneous data. The analysis reveals that FAL settings cause distribution shifts in typicality scores, yet TypiClust remains robust to these shifts. Additionally, TypiClust's performance is shown to be insensitive to the feature extraction method, suggesting pre-trained models can substitute for self-supervised features in data-limited scenarios, enabling broader FAL applicability.

## Method Summary
TypiClust operates within federated learning rounds by having each client perform self-supervised learning (SimCLR) on local unlabeled data to extract features, then computing typicality scores via inverse K-NN distances. The method clusters these features and selects the most typical instance from each cluster for annotation. These labeled samples are used for local training (10 epochs) before FedAvg aggregation. The approach can alternatively use pre-trained model features when unlabeled data is too limited for effective SSL. The method is evaluated across 4 random seeds using t-tests (threshold = 2.776) with tiny budgets (1× num_classes/client) and small budgets (3× num_classes/client).

## Key Results
- TypiClust significantly outperforms baselines (Random, Entropy, Margin, Coreset, BADGE, KAFAL, LoGo) in low-budget FAL settings across heterogeneous data conditions
- Performance gap widens with more heterogeneous data (α=0.1) compared to IID settings (α=∞)
- Pre-trained model features (ViT, ResNet50) perform comparably to self-supervised features, enabling TypiClust application when unlabeled data is limited
- FAL settings cause typicality distribution shifts, but TypiClust remains robust despite decentralized feature extraction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** TypiClust's typicality-based selection mitigates the cold-start problem in low-budget FAL by selecting informative samples independently of the classifier under training.
- **Mechanism:** Self-supervised learning extracts features → typicality computed via inverse K-NN distance → clustering partitions feature space → most typical instance selected per cluster. This creates diversity coverage without relying on classifier confidence/uncertainty, which is unreliable when labeled data is scarce.
- **Core assumption:** Self-supervised features capture meaningful structure even when labeled data is minimal; cluster-representative samples transfer to supervised learning.
- **Evidence anchors:** [abstract] "TypiClust... works well even in low-budget FAL settings contrasted with relatively low performances of other methods"; [Section III] "TypiClust can select informative data points... independently with the classifier under training"; [corpus] Related work confirms low-budget regimes degrade uncertainty-based methods.

### Mechanism 2
- **Claim:** TypiClust remains robust to typicality distribution shifts caused by decentralized self-supervised training across heterogeneous clients.
- **Mechanism:** In FAL, each client performs SSL separately on its partition, causing embedding space fragmentation and lower typicality values overall. Despite this shift, cluster-based selection still identifies representative local samples.
- **Core assumption:** Local typicality rankings preserve enough relative ordering for effective selection even when absolute values shift; inter-cluster diversity matters more than precise typicality scores.
- **Evidence anchors:** [abstract] "FAL settings cause distribution shifts... but TypiClust is not very vulnerable to the shifts"; [Section IV-C, Figure 7] "Decentralized feature extraction shifts the distribution... yet TypiClust works well in FAL settings."

### Mechanism 3
- **Claim:** Pre-trained model features can substitute for self-supervised features when unlabeled data is too limited for effective SSL training.
- **Mechanism:** Instead of training SSL from scratch, extract features using off-the-shelf pre-trained models (ViT, ResNet50). TypiClust then computes typicality and clusters in this transferred feature space.
- **Core assumption:** Pre-trained features (e.g., ImageNet-trained) transfer sufficiently to downstream domain; typicality in pre-trained space still correlates with sample informativeness.
- **Evidence anchors:** [abstract] "TypiClust can effectively use features from pre-trained models... enabling its application in scenarios with limited unlabeled data"; [Section IV-D, Figure 8] "TypiClust with features extracted by pre-trained models performs as well as that with self-supervised features."

## Foundational Learning

- **Concept: Federated Learning (FedAvg aggregation)**
  - **Why needed here:** TypiClust operates within FL rounds; understanding how local gradients aggregate to form global models clarifies why client-level selection matters.
  - **Quick check question:** Can you explain why FedAvg with non-IID data causes local optima divergence across clients?

- **Concept: Self-Supervised Learning (Contrastive methods like SimCLR)**
  - **Why needed here:** TypiClust's first stage requires SSL to extract features before typicality computation; knowing how contrastive learning builds representations is essential.
  - **Quick check question:** How does SimCLR's contrastive loss encourage similar images to cluster in embedding space?

- **Concept: Active Learning Acquisition Functions**
  - **Why needed here:** TypiClust is compared against uncertainty-based (entropy, margin) and diversity-based (coreset, BADGE) strategies; understanding these baselines clarifies TypiClust's relative advantages.
  - **Quick check question:** Why do uncertainty-based methods fail in batch acquisition settings (selecting similar high-uncertainty samples)?

## Architecture Onboarding

- **Component map:** Feature Extractor -> Typicality Calculator -> Clusterer -> Selector -> FAL Orchestrator
- **Critical path:** Initialize global model → Each client: SSL OR load pre-trained extractor → Compute typicality → Cluster unlabeled samples → Select top-typicality per cluster → Annotate selected samples → Train local model (10 epochs) → Send local weights to server → Aggregate via FedAvg → Repeat rounds until budget exhausted
- **Design tradeoffs:** SSL features vs. pre-trained features (adaptation vs. zero-shot); Cluster count vs. budget size (diversity vs. typicality); Local-only vs. global model for selection (TypiClust avoids this by using SSL features independent of classifier)
- **Failure signatures:** Cold-start collapse (random outperforms method → features uninformative); Degenerate clustering (all samples in one cluster → selection reduces to single sample); Typicality inflation/deflation (distribution shift causes all local typicalities near zero → may need normalization)
- **First 3 experiments:** 1) Baseline replication: TypiClust vs. Random, Entropy, Coreset on CINIC-10 with α=∞ (IID), cnn-4, tiny budget (10 samples/round, 10 classes). Confirm win rate >50% against each baseline. 2) Heterogeneity stress test: Repeat with α=0.1 (high non-IID). Measure win rate change; expect improvement. 3) Feature ablation: Swap SSL features for pre-trained ResNet50 features on ISIC2019 (small unlabeled pool). Compare final recall; expect <5% degradation.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can methods be developed to align self-supervised features across different clients during or after decentralized self-supervised learning in FAL?
  - **Basis in paper:** The authors state: "Methods to align self-supervised features from different clients during or after self-supervised learning are needed" because "features extracted by different clients are not necessarily aligned, meaning even the same data instance can have different embeddings in different clients."
  - **Why unresolved:** Feature misalignment prevents exploiting self-supervised features for model training, limiting potential performance gains in FAL.
  - **What evidence would resolve it:** Development and empirical validation of feature alignment techniques that enable consistent embeddings across clients without sharing raw data.

- **Open Question 2:** How sensitive is TypiClust's performance in FAL to hyperparameters such as aggregation algorithms, budget size, and local training epochs?
  - **Basis in paper:** The authors state: "Although it is essential to reveal the sensitivity for efficiently utilizing FAL in practice, we leave it as future work."
  - **Why unresolved:** FAL involves more hyperparameters than standard AL, and understanding sensitivity is critical for practical deployment, but systematic analysis was beyond this paper's scope.
  - **What evidence would resolve it:** Ablation studies varying aggregation algorithms (beyond FedAvg), budget sizes, and local epochs across multiple FAL strategies.

## Limitations
- **Scalability uncertainty:** Performance with larger models (beyond ResNet18) and complex vision tasks remains untested
- **Data requirement assumption:** Reliance on sufficient unlabeled data per client may not hold in highly constrained edge devices
- **Clustering parameter variability:** Unspecified clustering algorithm parameters (number of clusters, K-NN parameter K) introduce variability in typicality calculation

## Confidence

- **High confidence**: TypiClust outperforms baselines in low-budget FAL settings (t-test significance established, consistent across multiple dataset conditions)
- **Medium confidence**: TypiClust's robustness to typicality distribution shifts is real but the mechanism needs further validation
- **Medium confidence**: Pre-trained model features serve as viable SSL substitutes, though domain adaptation may be required

## Next Checks

1. **Scalability test**: Implement TypiClust with Vision Transformer backbones and evaluate on CIFAR-100 or ImageNet subsets to verify performance holds with increased model complexity and class count
2. **Client size sensitivity**: Systematically vary client dataset sizes (from 100 to 10000 samples) to determine minimum unlabeled data requirements for effective self-supervised learning and typicality calculation
3. **Domain adaptation verification**: Test pre-trained feature transferability by training TypiClust on medical datasets from different modalities (X-ray → CT scan) to quantify performance degradation and identify domain gap thresholds