---
ver: rpa2
title: 'AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents
  with Medical Dataset Grounding'
arxiv_id: '2512.10195'
source_url: https://arxiv.org/abs/2512.10195
tags:
- clinical
- medical
- patient
- conversational
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoMedic is a fully automated multi-agent framework for evaluating
  large language models (LLMs) as clinical conversational agents. It converts static
  medical QA datasets into interactive virtual patient profiles and simulates multi-turn
  clinical dialogues between LLM agents.
---

# AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding

## Quick Facts
- arXiv ID: 2512.10195
- Source URL: https://arxiv.org/abs/2512.10195
- Authors: Gyutaek Oh; Sangjoon Park; Byung-Hoon Kim
- Reference count: 15
- Key outcome: AutoMedic is a fully automated multi-agent framework for evaluating LLMs as clinical conversational agents. It converts static medical QA datasets into interactive virtual patient profiles and simulates multi-turn clinical dialogues. The CARE metric assesses performance across accuracy, conversational efficiency, empathy, and robustness. Human expert evaluations validated the framework's reliability and demonstrated strong alignment with clinical judgment. The study found that LLMs perform worse in conversational settings compared to static QA, highlighting the limitations of existing benchmarks.

## Executive Summary
AutoMedic introduces a fully automated multi-agent framework that transforms static medical question-answering datasets into interactive virtual patient profiles for evaluating large language models as clinical conversational agents. The framework simulates multi-turn dialogues between a doctor agent and supporting patient/clinical staff agents, measuring performance across four dimensions: accuracy, conversational efficiency, empathy, and robustness. Human expert evaluations validated the framework's reliability, showing strong alignment between the CARE metric and clinical judgment. The framework reveals that LLMs perform significantly worse in conversational settings compared to static QA benchmarks, highlighting the limitations of existing evaluation methods. Proprietary models like Claude Sonnet 4 and GPT-4o demonstrated balanced, high-level performance, while most medical-tuned models underperformed.

## Method Summary
AutoMedic converts medical QA datasets into interactive patient profiles through a three-stage process. First, a profile generator filters QA items for suitability and creates structured patient profiles with demographics, clinical history, and optional information. Second, a multi-turn dialogue simulation occurs between a doctor agent and supporting patient/clinical staff agents, with communication constrained by XML-style tags (<patient>, <clinical>, </end>) to enforce conversation protocols. Third, the doctor agent provides a final answer, and the CARE metric is calculated across four dimensions: conversational accuracy (S_ACC), conversational efficiency/strategy (S_CES), empathy (S_EMP), and robustness (S_ROB). The framework uses GPT-4o for all supporting agents and limits conversations to 20 turns maximum.

## Key Results
- LLMs perform worse in conversational settings compared to static QA, with a correlation of r=0.7547 between static and conversational accuracy
- Proprietary models (Claude Sonnet 4, GPT-4o) showed balanced, high-level performance across all CARE dimensions
- Medical-tuned models generally underperformed compared to proprietary models
- Human expert evaluations validated the framework's reliability with high agreement (Gwet's AC2 0.8571)
- Every evaluated model demonstrated lower performance in conversational settings compared to its static QA counterpart

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting static QA datasets into conversational simulations reveals a performance gap that static benchmarks fail to capture.
- Mechanism: The profile generator extracts patient demographics, clinical history, and optional test results from static QA items, then distributes this information across agents with controlled knowledge boundaries. The doctor agent must actively query rather than passively receive context, exposing information-gathering deficits.
- Core assumption: Static QA accuracy serves as an approximate upper bound for conversational performance; degradation indicates conversational capability limits rather than knowledge gaps.
- Evidence anchors:
  - [abstract] "LLMs perform worse in conversational settings compared to static QA, highlighting the limitations of existing benchmarks"
  - [section 5.4] "every model demonstrates lower performance in the conversational setting compared to its static QA counterpart" with correlation r=0.7547
  - [corpus] Limited direct corroboration; CP-Env similarly finds static benchmarks "inadequately evaluate" dynamic scenarios, but this is methodological convergence, not independent validation

### Mechanism 2
- Claim: Multi-agent role separation with structured tagging creates enforceable conversation protocols that surface agent robustness failures.
- Mechanism: The doctor agent uses XML-style tags (<patient>, <clinical>, </end>) to direct queries. Patient and clinical staff agents respond only to appropriately tagged input. This constraint surface failures like role-breaking (doctor self-simulating other agents) and abrupt termination (misplaced tags).
- Core assumption: Tag-based protocol adherence is a valid proxy for conversational robustness in clinical settings.
- Evidence anchors:
  - [section 3.3] "Questions for the patient agent are wrapped in <patient></patient> tags, while requests for the clinical staff agent are framed within <clinical></clinical> tags"
  - [section 3.4] Robustness score explicitly counts "failure cases" including role-breaking and improper tag usage
  - [corpus] No direct corpus validation of this specific tagging mechanism

### Mechanism 3
- Claim: Using the patient agent as an empathy evaluator creates an intra-simulation assessment that correlates with human expert judgment.
- Mechanism: After conversation termination, the patient agent (already instantiated with persona and history) rates the doctor agent's empathy on a 5-point scale. This leverages the agent's contextual understanding of the interaction rather than external annotation.
- Core assumption: The patient agent's empathy ratings reflect clinically relevant empathetic communication, not just linguistic politeness markers.
- Evidence anchors:
  - [section 3.4] "we use the patient agent as a proxy evaluator... prompted to rate the doctor agent's empathy on a 5-point scale"
  - [section 5.3] Human expert empathy ratings showed "high agreement" (Gwet's AC2 0.8571) and correlated with SEMP scores (Claude Sonnet 4: 0.8520 vs. Llama 3-70B: 0.7260)
  - [corpus] No corpus validation of intra-simulation evaluation methodology

## Foundational Learning

- Concept: **Multi-turn clinical dialogue structure**
  - Why needed here: The framework evaluates information-gathering across sequential turns, not single-shot answers. Understanding that clinical diagnosis requires iterative hypothesis refinement helps interpret CES (efficiency/strategy) scores.
  - Quick check question: Can you explain why asking multiple questions in a single turn might achieve high information density but receive a low strategy score?

- Concept: **Knowledge boundary enforcement in agent systems**
  - Why needed here: The clinical staff agent only returns information that exists in the patient profile, preventing hallucination. Understanding this constraint is essential for diagnosing why some queries return "unavailable."
  - Quick check question: What happens when the doctor agent requests a test that wasn't included in the original QA item's optional information?

- Concept: **Static vs. conversational benchmark divergence**
  - Why needed here: The S_ACC metric incorporates both raw conversational accuracy and the degradation penalty (AccConv/AccQA). Interpreting scores requires understanding that a model with lower static accuracy but smaller degradation might score similarly to a higher-knowledge model with larger degradation.
  - Quick check question: If Model A has AccQA=0.90 and AccConv=0.70, while Model B has AccQA=0.75 and AccConv=0.65, which has the higher S_ACC?

## Architecture Onboarding

- Component map: Profile Generator (GPT-4o) -> Distribute info to agents -> Multi-turn dialogue -> CARE metric calculation
- Critical path: QA dataset → Profile Generator (filter + transform) → Distribute info to agents → Multi-turn dialogue (max 20 turns) → Present final question → Doctor agent answers → CARE metric calculation
- Design tradeoffs:
  - Using GPT-4o for all supporting agents ensures stability but may mask how patient/staff agent quality affects doctor agent performance (acknowledged limitation in Section 6)
  - 20-turn maximum balances evaluation depth vs. cost; insufficient for complex differential diagnosis scenarios
  - Text-only evaluation excludes multimodal clinical data (imaging, waveforms) — noted limitation
- Failure signatures:
  - Role-breaking: Doctor agent outputs both question and patient response in single turn (observed with Med42-v2-70B)
  - Checklist behavior: Doctor agent dumps 10+ questions per turn, achieving information but scoring poorly on strategy (observed with gpt-oss-120B)
  - Premature termination: Conversation ends ≤3 turns due to missing/misplaced tags (observed with DeepSeek-R1-70B)
  - Invalid answer: Normal conversation but final answer not in provided options
- First 3 experiments:
  1. **Dataset suitability audit**: Run the profile generator filtering on your target QA dataset before full evaluation. Expect 10-15% appropriateness for MedMCQA/HEAD-QA style datasets vs. 90%+ for MedBullets/MedQA style (Section 5.2).
  2. **Baseline static vs. conversational gap**: Evaluate your doctor agent in both static QA mode (full context provided) and conversational mode. Quantify the accuracy drop; >20% degradation suggests conversational information-gathering is the bottleneck, not medical knowledge.
  3. **CARE dimension profiling**: Run evaluation on 100 samples from MedQA or MedBullets. Plot the four CARE dimensions to identify specific weaknesses (e.g., high S_ACC but low S_EMP suggests knowledge-communication gap). Compare against reported Claude Sonnet 4 profile (balanced high) vs. DeepSeek-R1-70B profile (robustness failure).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the AutoMedic framework be effectively extended to evaluate multimodal models in clinical scenarios that require processing medical images alongside text?
- **Basis in paper:** [explicit] The Limitations section states that the current evaluation is limited to text-based interactions and does not directly incorporate other modalities, identifying the extension to vision-language models as a critical area for future work.
- **Why unresolved:** The current framework architecture and CARE metric are designed solely for text-based dialogue agents and lack the protocols to handle or evaluate visual data integration.
- **What evidence would resolve it:** A modified version of AutoMedic benchmarking Vision-Language Models (VLMs) on tasks requiring image interpretation, validated against human expert assessments of multimodal interaction.

### Open Question 2
- **Question:** How can a single, aggregated performance score be developed from the CARE metric that accurately reflects the relative importance of its four dimensions for specific clinical applications?
- **Basis in paper:** [explicit] The authors note in the Limitations section that they did not define a single aggregated score because the four dimensions have different scales and varying relative importance depending on the clinical context.
- **Why unresolved:** A simple sum or average is insufficient, and a methodology for weighting accuracy, efficiency, empathy, and robustness relative to clinical priorities has not yet been established.
- **What evidence would resolve it:** The derivation of a weighted composite score that demonstrates high correlation with human expert preferences in head-to-head model comparisons across diverse clinical scenarios.

### Open Question 3
- **Question:** To what degree does the performance and behavior of the supporting patient and clinical staff agents influence the evaluation outcomes of the doctor agent?
- **Basis in paper:** [explicit] The Limitations section acknowledges that while GPT-4o was used for supporting agents to ensure stability, analyzing the impact of using different LLMs for these patient and staff roles remains a valuable direction for future investigation.
- **Why unresolved:** The reliance on a single proprietary model for the patient and staff roles leaves open the possibility that the doctor agent's performance is partly dependent on the specific characteristics of these simulators.
- **What evidence would resolve it:** An ablation study measuring the variance in doctor agent performance scores when the underlying models for the patient and clinical staff agents are systematically swapped.

## Limitations

- Framework depends on GPT-4o for all supporting agents, which may mask how agent quality affects doctor agent performance
- Evaluation is limited to text-based medical knowledge, excluding multimodal clinical data like imaging or physiological signals
- Framework inherits potential biases from underlying medical QA datasets, which may not represent diverse patient populations

## Confidence

- **High confidence**: LLMs perform worse in conversational settings compared to static QA (r=0.7547 correlation between static and conversational accuracy, every model showing degradation)
- **Medium confidence**: CARE metric's alignment with human expert evaluation (Gwet's AC2 0.8571 for overall agreement, specific correlations for empathy and accuracy)
- **Medium confidence**: Proprietary models (Claude Sonnet 4, GPT-4o) showing balanced, high-level performance while medical-tuned models underperform

## Next Checks

1. **Cross-agent generalization test**: Replace GPT-4o with smaller, task-specific models for patient/clinical staff agents and re-evaluate doctor agent performance. This would determine whether current results reflect doctor agent capability or are artifacts of high-quality supporting agents.

2. **Multimodal extension validation**: Adapt the framework to handle image-based clinical data (X-rays, pathology slides) by incorporating vision models and evaluate whether the CARE metric maintains its validity and reliability in multimodal contexts.

3. **Real-world clinical conversation alignment**: Compare AutoMedic evaluations against actual doctor-patient conversation transcripts (de-identified) to validate whether the simulated conversations capture clinically relevant conversational dynamics and information-gathering patterns.