---
ver: rpa2
title: Symmetry-Aware Transformer Training for Automated Planning
arxiv_id: '2508.07743'
source_url: https://arxiv.org/abs/2508.07743
tags:
- planning
- plan
- training
- state
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel contrastive learning objective for
  transformers to address symmetries in automated planning problems, where arbitrary
  object names and atom order create many equivalent representations. The proposed
  method encourages equivariance to object name assignments and atom order invariance,
  combined with architectural changes like removing positional encodings and using
  atom-level embeddings.
---

# Symmetry-Aware Transformer Training for Automated Planning

## Quick Facts
- arXiv ID: 2508.07743
- Source URL: https://arxiv.org/abs/2508.07743
- Authors: Markus Fritzsche; Elliot Gestrin; Jendrik Seipp
- Reference count: 40
- Primary result: Novel contrastive learning objective improves transformer generalization to harder planning problems by enforcing symmetry awareness

## Executive Summary
This paper addresses a fundamental challenge in applying transformers to automated planning: arbitrary object names and atom order create many equivalent problem representations that standard models fail to recognize as identical. The authors propose a symmetry-aware training approach that enforces equivariance to object name assignments and atom order invariance through contrastive learning. By processing pairs of planning problems with randomized object names and aligning their internal representations, the model learns to focus on relational structure rather than specific identifiers. Combined with architectural changes like removing positional encodings and using atom-level embeddings, this approach significantly improves extrapolation to harder problems compared to the PlanGPT baseline.

## Method Summary
The method introduces a novel contrastive learning objective that trains transformers on pairs of symmetric planning problems - the same logical problem with different random object name assignments. The approach enforces alignment between attention scores and hidden state projections of these equivalent representations, effectively training the model to be equivariant to object permutations. Key architectural changes include removing positional encodings from the encoder (making processing permutation-equivariant) and replacing sub-tokenization with compositional atom-level embeddings that treat each predicate-object tuple as a single unit. The training objective combines standard prediction loss with two contrastive components: $L_{att}$ for aligning attention scores and $L_{hid}$ for aligning hidden state representations. Experiments on Blocksworld, Gripper, Visitall, and Logistics domains demonstrate improved extrapolation to harder problems, though challenges remain for very large instances.

## Key Results
- Coverage scores up to 1.00 in some domains (Blocksworld, Gripper) when extrapolating to harder problems
- Significant improvement over PlanGPT baseline in coverage and training stability
- Fewer training divergences with symmetry-aware objective (58% divergence rate without contrastive loss)
- Challenges remain for very large instances, especially in Logistics domain where extrapolation remains difficult

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Enforcing alignment between internal representations of equivalent planning problems reduces sample complexity and improves extrapolation.
- **Mechanism:** The contrastive loss ($L_{att} + L_{hid}$) forces the transformer to produce identical attention patterns and hidden state projections for the same logical problem described with different arbitrary object names. This effectively trains the model to be equivariant to object permutations, preventing it from overfitting to specific name patterns found in the training distribution.
- **Core assumption:** The solution logic depends only on the relational structure of objects, not their specific identifiers.
- **Evidence anchors:**
  - [abstract] "We propose a novel contrastive learning objective... encourage equivariance to object name assignments."
  - [Section 4] "The core idea is to train the model using pairs of symmetric planning problems... The first objective is to align the attention scores... The second component... encourage[s] similarity directly within the learned token representations."
  - [corpus] Weak direct evidence; corpus focuses on general transformer efficiency or unrelated domains.

### Mechanism 2
- **Claim:** Removing explicit positional encodings (PE) in the encoder enables zero-shot generalization to larger input sizes and varying atom counts.
- **Mechanism:** Planning states are sets of atoms where order is irrelevant (permutation invariant). Standard learned PEs tie specific semantic meanings to token indices (e.g., "the token at index 5 usually describes the truck"). By using NoPE (No Positional Encoding) in the encoder, the model relies solely on the content and relational structure of the tokens, allowing it to process inputs with lengths and atom counts unseen during training.
- **Core assumption:** The transformer's self-attention mechanism is sufficient to model the relationships between atoms without needing an artificial ordering signal.
- **Evidence anchors:**
  - [abstract] "...combined with architectural changes like removing positional encodings..."
  - [Section 4] "...processing is inherently permutation-equivariant... resulting in a $|I|! \cdot |G|!$ reduction of the input space."
  - [Section 6] "This outcome reinforces that achieving robust, out-of-distribution extrapolation... remains a substantial challenge..." (Contextualizes that while helpful, it is not a silver bullet).

### Mechanism 3
- **Claim:** Compositional atom-level embeddings prevent combinatorial explosion compared to sub-tokenization.
- **Mechanism:** Instead of tokenizing `at(truck1, loc1)` into 4 separate tokens (which creates artificial order dependencies and increases sequence length), the model embeds the entire atom into a single vector via a learned linear projection of concatenated predicate and object embeddings. This preserves the "atomic" unit of truth in planning logic and works naturally with the permutation-equivariant encoder.
- **Core assumption:** A linear combination of embeddings can sufficiently capture the interaction between a predicate and its arguments before transformer processing.
- **Evidence anchors:**
  - [Section 4] "...we instead encode each atom as a single embedding... T_{p(o1,...,on)} = W(E[p]|E[o1]|...|E[on])..."
  - [Section 4] "...incompatible with an embedder without positional encoding, we instead encode each atom as a single embedding..."

## Foundational Learning

- **Concept:** **Permutation Invariance vs. Equivariance**
  - **Why needed here:** The core failure mode of standard Transformers in planning is treating `at(A, B)` and `at(B, A)` as distinct inputs due to token order. You must understand that planning states are sets, not sequences.
  - **Quick check question:** If I shuffle the order of the state atoms in the input, should the heuristic prediction change? (Answer: No).

- **Concept:** **Contrastive Learning**
  - **Why needed here:** The paper uses a specific contrastive setup (positive pairs are the same problem with renamed objects) rather than standard next-token prediction.
  - **Quick check question:** In this paper's contrastive loss, what constitutes a "positive pair"? (Answer: The same planning problem instance with different random object name assignments).

- **Concept:** **PDDL (Planning Domain Definition Language)**
  - **Why needed here:** The inputs are structured PDDL facts (predicates, objects), not natural language. Understanding the difference between a predicate (template) and a ground atom (instance) is required to implement the compositional embedder.
  - **Quick check question:** Why can't we just use a standard BPE tokenizer on PDDL text? (Answer: It splits atoms into tokens that require ordering, conflicting with the symmetry-aware architecture).

## Architecture Onboarding

- **Component map:** Input -> Compositional Atom Embedder -> Transformer Encoder (NoPE) -> Transformer Decoder (or Heuristic Head) -> Output

- **Critical path:** The **Rename-Both / Rename-One** pipeline is the most fragile component. You must implement a data loader that takes a single planning instance, performs a random permutation of object names, and batches the original and permuted instance together as a pair for the contrastive loss.

- **Design tradeoffs:**
  - **Rename-One vs. Rename-Both:** The paper notes Rename-One is better for heuristics (stable reference), Rename-Both for plan generation.
  - **Shared Weights:** Reduces parameter count (good for generalization) but may limit the complexity of the logic expressible compared to deep unshared stacks.
  - **Vocabulary Size:** You must pre-allocate a maximum object vocabulary (e.g., 123 objects). New objects > this limit cannot be encoded.

- **Failure signatures:**
  - **Training Instability:** Without the contrastive loss, the paper reports frequent "divergences" (loss spikes/NaNs) in the Encoder-only setup (Section 6).
  - **Memorization:** If validation loss drops but extrapolation coverage is 0%, the model likely failed to learn symmetry (it overfit to specific names/positions).

- **First 3 experiments:**
  1. **Overfit One Batch:** Train on a single small problem (e.g., 4-block Blocksworld) with and without the contrastive loss. Verify the model can at least memorize the plan.
  2. **Symmetry Test:** Train on easy problems. During eval, feed the *exact same* problem with randomly renamed objects. Coverage should be identical to the baseline names; if it varies, the symmetry training failed.
  3. **Extrapolation Sweep:** Train on Gripper (2-4 balls). Evaluate on 10, 20, 40 balls. Plot the coverage curve against the PlanGPT baseline to confirm the "harder problems" advantage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or representational bottlenecks prevent the proposed symmetry-aware transformers from extrapolating to larger instances in the Logistics domain?
- Basis in paper: [explicit] The authors state in Section 8 that future work should "identify the architectural or representational bottlenecks that limit generalization" and explicitly note the failure to extrapolate in Logistics in Section 6.
- Why unresolved: While the paper demonstrates improved extrapolation in other domains (e.g., Gripper, Visitall), the model fails completely on Logistics, and the exact cause (e.g., model depth, attention mechanism limits, or data complexity) remains unidentified.
- What evidence would resolve it: An ablation study or architectural modification (such as increasing network depth or employing hierarchical attention) that results in non-zero coverage for extrapolation tasks in the Logistics domain.

### Open Question 2
- Question: How can representation learning techniques be adapted to remove the dependency on a fixed-size object vocabulary?
- Basis in paper: [explicit] Section 8 highlights the need to address the "practical limitation of a fixed vocabulary size by exploring representation learning techniques independent of token vocabularies."
- Why unresolved: The current method requires pre-defining a maximum vocabulary size, which imposes a hard limit on the number of objects the model can handle.
- What evidence would resolve it: The successful implementation of a character-level or sub-token architecture that generalizes to planning problems with object counts significantly exceeding the training vocabulary without performance degradation.

### Open Question 3
- Question: How does the choice of positional encoding specifically impact length extrapolation in symmetry-aware transformers for planning?
- Basis in paper: [explicit] Section 6 notes: "Succeeding to extrapolate can depend on many factors, such as the choice of positional encoding, which is why further research is required to understand the impact of positional encodings on length extrapolation."
- Why unresolved: The paper implements NoPE (No Positional Encoding) but acknowledges that the relationship between different encoding schemes and the proposed contrastive loss is not fully explored.
- What evidence would resolve it: A comparative analysis of extrapolation performance using various positional encodings (e.g., ALiBi, Rotary, absolute) within the symmetry-aware training framework.

### Open Question 4
- Question: Why does the "Rename-Both" contrastive mode hinder convergence for heuristic prediction while benefiting plan generation?
- Basis in paper: [inferred] Section 4 mentions that "Rename-One mode is preferable for predicting heuristics, while Rename-Both is preferable for plan generation" based on preliminary experiments, but offers no theoretical explanation for this discrepancy.
- Why unresolved: The difference in optimal renaming strategies suggests a divergence in how the contrastive loss interacts with different output heads (heuristic regression vs. token generation), which is not analyzed in depth.
- What evidence would resolve it: A loss landscape analysis or gradient flow study comparing the two renaming modes across both the heuristic and plan generation objectives.

## Limitations
- Vocabulary size constraint limits scalability to domains with many objects (fixed at 123 objects)
- Modest extrapolation improvements for very large instances, particularly in Logistics domain
- Limited comparison to other symmetry-aware approaches or classical planning heuristics
- Computational overhead of processing symmetric pairs during training not quantified

## Confidence

- **High confidence:** The mechanism of enforcing symmetry through contrastive learning is technically sound and the architectural changes (NoPE encoder, atom-level embeddings) are correctly implemented and described.
- **Medium confidence:** The empirical improvements in extrapolation are real but incremental; the claim that this "significantly improves" extrapolation over PlanGPT is supported but the absolute performance remains limited for very large instances.
- **Low confidence:** The scalability claims are limited by the vocabulary constraint, and the paper doesn't adequately address how to handle domains with object counts exceeding the fixed vocabulary.

## Next Checks

1. **Vocabulary scalability test:** Systematically evaluate performance degradation as problem size increases beyond the 123-object vocabulary limit, measuring at what point the model fails completely.
2. **Contrastive loss ablation study:** Train identical models with varying weights for $L_{att}$ and $L_{hid}$ (including zero) to quantify the marginal benefit of each component and determine optimal weighting.
3. **Cross-domain generalization:** Test the trained models on completely unseen PDDL domains (not just larger instances of trained domains) to assess true generalization versus memorization of domain-specific patterns.