---
ver: rpa2
title: Block-wise Adaptive Caching for Accelerating Diffusion Policy
arxiv_id: '2506.13456'
source_url: https://arxiv.org/abs/2506.13456
tags:
- layers
- layer
- timestep
- block
- caching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Block-wise Adaptive Caching (BAC) accelerates transformer-based
  Diffusion Policy by caching intermediate action features at the block level. The
  method uses an Adaptive Caching Scheduler to identify optimal update timesteps for
  each block, maximizing feature similarity between cached and skipped features via
  dynamic programming.
---

# Block-wise Adaptive Caching for Accelerating Diffusion Policy

## Quick Facts
- arXiv ID: 2506.13456
- Source URL: https://arxiv.org/abs/2506.13456
- Reference count: 40
- Key outcome: Achieves up to 3× inference speedup without performance degradation

## Executive Summary
Block-wise Adaptive Caching (BAC) is a training-free plugin method that accelerates transformer-based Diffusion Policy through intelligent block-level feature caching. The approach uses an Adaptive Caching Scheduler to identify optimal update timesteps for each block, maximizing feature similarity between cached and skipped features via dynamic programming. BAC particularly addresses error propagation between blocks through its Bubbling Union Algorithm, which forces upstream blocks with large errors to update before downstream Feed-Forward Network layers. The method demonstrates stable acceleration rates above 3.4× across multiple robotic benchmarks while maintaining or improving success rates on challenging tasks where uniform caching fails.

## Method Summary
BAC accelerates Diffusion Policy by caching intermediate action features at the block level, where each block consists of a Multi-Head Attention (MHA) layer and a Feed-Forward Network (FFN) layer. The method employs an Adaptive Caching Scheduler that uses dynamic programming to determine optimal update timesteps for each block by maximizing feature similarity between cached and skipped features. To prevent error propagation between blocks—particularly problematic in FFN layers—BAC implements a Bubbling Union Algorithm that identifies and updates upstream blocks with large errors before downstream FFNs. As a training-free plugin, BAC seamlessly integrates with existing transformer-based Diffusion Policy and vision-language-action models, achieving up to 3× inference speedup without performance degradation across multiple robotic benchmarks.

## Key Results
- Achieves up to 3× inference speedup without performance degradation
- Maintains stable acceleration rates above 3.4× across tasks
- Improves success rates on challenging tasks where uniform caching fails

## Why This Works (Mechanism)
BAC works by exploiting the inherent redundancy in transformer-based Diffusion Policy during inference. The Adaptive Caching Scheduler identifies that many intermediate features across timesteps are highly similar, allowing safe skipping of computations without significant quality loss. The Bubbling Union Algorithm specifically addresses the challenge of error propagation between blocks, recognizing that errors in FFN layers can compound if upstream blocks with large errors are not updated first. By forcing these critical updates, BAC maintains policy performance while still achieving substantial computational savings through strategic caching.

## Foundational Learning
- Transformer Architecture: Why needed - Understanding the MHA and FFN block structure is crucial for implementing block-level caching. Quick check - Can identify where caching can be safely applied between blocks.
- Diffusion Policy: Why needed - Knowing how diffusion models generate actions over timesteps is essential for understanding the caching opportunity. Quick check - Can explain the relationship between timesteps and feature computation.
- Dynamic Programming: Why needed - The Adaptive Caching Scheduler uses DP to find optimal update schedules. Quick check - Can trace through the DP table construction for caching decisions.
- Error Propagation in Sequential Models: Why needed - Understanding how errors compound in sequential architectures is key to the Bubbling Union Algorithm. Quick check - Can identify scenarios where upstream errors affect downstream quality.
- Vision-Language-Action Models: Why needed - BAC's applicability to VLA models requires understanding multi-modal feature integration. Quick check - Can explain how BAC handles cross-modal feature caching.

## Architecture Onboarding

Component Map: Input -> Feature Extraction -> Block 1 (MHA+FFN) -> Block 2 (MHA+FFN) -> ... -> Action Output

Critical Path: The critical path involves the Adaptive Caching Scheduler selecting which blocks to update at each timestep, followed by the Bubbling Union Algorithm ensuring error propagation is controlled, and finally the caching mechanism storing and retrieving intermediate features.

Design Tradeoffs: BAC trades minimal memory overhead for significant computational speedup. The caching mechanism requires storing intermediate features, but this is offset by skipping redundant computations. The Bubbling Union Algorithm adds computational overhead for error checking but is essential for maintaining performance.

Failure Signatures: If the Bubbling Union Algorithm fails, errors will compound between blocks, particularly in FFN layers, leading to degraded policy performance despite computational savings. If the Adaptive Caching Scheduler is too aggressive, it may skip too many updates, causing feature divergence and poor action quality.

First Experiments:
1. Verify that cached features maintain similarity to their non-cached counterparts across multiple timesteps
2. Test the Bubbling Union Algorithm's effectiveness in preventing error propagation in simple sequential models
3. Measure the actual computational savings vs. memory overhead in a small-scale Diffusion Policy implementation

## Open Questions the Paper Calls Out
None

## Limitations
- The Bubbling Union Algorithm's effectiveness in preventing error propagation between blocks, particularly in complex multi-layer transformer architectures, remains to be validated across diverse model sizes and task complexities
- The dynamic programming approach for optimal timestep selection may face scalability

## Confidence

High:
- BAC achieves the claimed 3× speedup in tested scenarios
- The method maintains policy performance while accelerating inference
- The training-free plugin nature allows seamless integration with existing models

Medium:
- Generalization to extremely large transformer models with many layers
- Performance consistency across diverse robotic tasks beyond the tested benchmarks

Low:
- Long-term stability of cached features in continuous learning scenarios
- Impact of BAC on energy efficiency and power consumption in edge deployment scenarios

## Next Checks

1. Implement BAC on a medium-sized transformer model and verify the 3× speedup claim with performance preservation
2. Test the Bubbling Union Algorithm on a model with at least 12 layers to evaluate scalability and error propagation control
3. Measure memory overhead and computational savings across different caching ratios to find the optimal balance point