---
ver: rpa2
title: An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved
  LLM Agents
arxiv_id: '2505.15117'
source_url: https://arxiv.org/abs/2505.15117
tags:
- search
- training
- reward
- arxiv
- engine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper empirically studies reinforcement learning for training
  large language models to reason and interact with search engines. The authors examine
  three key design factors: reward formulation, the underlying LLM backbone, and the
  choice of search engine.'
---

# An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents

## Quick Facts
- arXiv ID: 2505.15117
- Source URL: https://arxiv.org/abs/2505.15117
- Authors: Bowen Jin; Jinsung Yoon; Priyanka Kargupta; Sercan O. Arik; Jiawei Han
- Reference count: 40
- This paper empirically studies reinforcement learning for training large language models to reason and interact with search engines. The authors examine three key design factors: reward formulation, the underlying LLM backbone, and the choice of search engine.

## Executive Summary
This paper provides the first comprehensive empirical study on training reasoning-search interleaved LLM agents using reinforcement learning. The authors investigate three critical design choices: reward formulation (outcome, format, and intermediate retrieval rewards), the underlying LLM backbone (general-purpose vs. reasoning-specialized), and the choice of search engine during training. Through extensive experiments, they find that format rewards significantly improve performance when training from base LLMs, general-purpose LLMs outperform reasoning-specialized ones in RL settings, and the quality of the training search engine strongly affects RL dynamics and learned search behavior. The study also reveals that agents trained with one search engine are generally robust to others at inference time, with stronger retrieval systems yielding better performance.

## Method Summary
The authors train LLM agents to perform reasoning-search interleaved QA tasks using reinforcement learning. They use two algorithms (PPO and GRPO) with a KL penalty to maintain stability. The agents operate in an environment where they can issue search queries to retrieve relevant passages, then reason about the answer. Three reward components are tested: outcome reward (exact match), format reward (verifying correct token usage like `<search></search>`), and intermediate retrieval reward (substring match between retrieved passages and gold answers). The study evaluates different LLMs (Qwen2.5-7B-Base, DeepSeek-R1-Distill-Qwen-7B, and various scales up to 32B parameters) and search engines (BM25, E5, and Random Noise) across the HotpotQA dataset.

## Key Results
- Format rewards improve performance, particularly when training from base LLMs (+8-28% absolute improvement)
- General-purpose LLMs outperform reasoning-specialized ones in RL settings due to better instruction-following early in training
- Larger models show consistent but diminishing performance gains
- The quality of the search engine used during training strongly affects RL dynamics and learned search behavior
- Agents trained with one search engine are generally robust to others at inference time, with stronger retrieval systems yielding better performance

## Why This Works (Mechanism)

### Mechanism 1: Format Rewards Scaffold Tool-Use Learning for Base Models
Base LLMs lack strong instruction-following capabilities for structured API calls. Format rewards provide dense feedback that guides the model toward valid action formats, enabling successful search engine invocation. Without this signal, early rollouts fail to trigger retrieval, starving the agent of positive outcome rewards. The outcome reward alone is too sparse for base models to discover valid formatting through exploration. Adding a format reward consistently improves final model performance, particularly for base LLMs. Base models with format reward show ~8-28% absolute improvement over outcome-only across datasets. If format reward weight λf is too large (>0.6), the model overfits to format compliance without improving answer quality.

### Mechanism 2: Search Engine Quality Determines Exploration Efficiency and Behavioral Patterns
High-quality retrieval provides relevant information that directly contributes to correct answers, creating a clear reward signal. Low-quality retrieval provides noisy or irrelevant results, forcing the agent to either issue many compensatory queries or abandon retrieval entirely and rely on parametric knowledge. The search engine is treated as a fixed environment; the agent cannot improve retrieval quality through better queries alone. With Random Noise, the agent quickly learns to avoid using the search engine. With BM25, the agent gradually increases search calls to compensate. With E5, the agent issues search calls more strategically. E5 (Exact) achieves 0.430 avg EM vs. 0.352 for BM25 and 0.241 for Random. If the training search engine is too weak, the agent may learn maladaptive behaviors (excessive calls or complete avoidance) that persist even when switched to a stronger inference-time engine.

### Mechanism 3: General-Purpose Initialization Outperforms Reasoning-Specialized for Agentic RL
Reasoning-specialized models have weaker instruction-following at early training stages, preventing them from learning correct search API formatting. Without valid search calls, they cannot receive positive outcome rewards from successful retrieval-augmented reasoning, leading to insufficient exploration or training collapse. The search agent task requires both reasoning and instruction-following; the bottleneck at initialization is instruction-following, not reasoning. Qwen2.5-7B-Base (general) achieves 0.434 avg EM vs. 0.344 for DeepSeek-R1-Distill-Qwen-7B (reasoning) under PPO; GRPO collapses entirely for reasoning model (0.100 avg). Reasoning LLM struggles to initiate search calls during early training; general LLM learns calls faster. If instruction-following is explicitly trained or scaffolded first (e.g., via SFT on format), reasoning-specialized models may overcome this limitation.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO) vs. Group Relative Policy Optimization (GRPO)**
  - Why needed here: The paper uses both algorithms; understanding their stability differences explains why PPO works better for reasoning-specialized models while GRPO collapses.
  - Quick check question: Which algorithm removes the dependency on a learned value function, and what tradeoff does this introduce for high-variance exploration tasks?

- **Concept: Sparse vs. Dense Reward Signals in RL**
  - Why needed here: The core finding is that outcome rewards are sparse but sufficient; format rewards add density; intermediate retrieval rewards add density but harm performance. Understanding reward sparsity explains why format rewards help.
  - Quick check question: Why might adding a dense intermediate reward (retrieval correctness) hurt final performance even though it provides more learning signal?

- **Concept: Retrieval-Augmented Generation (RAG) vs. Agentic Search**
  - Why needed here: The paper trains agents that decide *when* and *what* to search, unlike standard RAG where retrieval is fixed. This distinction is critical for understanding the interleaved reasoning-search loop.
  - Quick check question: In the agentic paradigm, what component decides the search query, and how does this differ from a retrieve-then-generate RAG pipeline?

## Architecture Onboarding

- **Component map:** Policy LLM (πθ) -> Search Engine (R) -> Retrieved Passages -> Reward Function (rϕ) -> RL Algorithm -> Updated Policy

- **Critical path:**
  1. Start with Qwen2.5-7B-Base (general-purpose, not reasoning-specialized).
  2. Add format reward with λf=0.2–0.4; skip intermediate retrieval reward (λr=0).
  3. Train with E5 (Exact) as search engine for stable learning dynamics.
  4. Use PPO for stability; GRPO may work but requires careful monitoring.

- **Design tradeoffs:**
  - Base vs. Instruct LLM: Base + format reward outperforms instruct; instruct models have smaller format reward gains but may converge faster.
  - Search Engine Strength: Stronger training engine → better performance and efficient behavior; weaker engine → robust to inference switch but lower ceiling.
  - Model Scale: Larger models (14B, 32B) improve performance with diminishing returns; smaller models (3B) benefit most from format rewards.

- **Failure signatures:**
  - Training collapse with GRPO: Seen with reasoning-specialized models; switch to PPO or verify instruction-following capability first.
  - Zero search calls during training: Agent learned retrieval is useless (random engine) or failed to learn format; check format reward and retriever quality.
  - Excessive search calls (>5 per query): Agent compensating for weak retrieval; upgrade training retriever.

- **First 3 experiments:**
  1. **Baseline validation:** Train Qwen2.5-7B-Base with PPO, outcome-only reward, E5 (Exact). Verify ~0.43 avg EM on benchmark.
  2. **Format reward ablation:** Add format reward (λf=0.4), compare convergence speed and final performance vs. baseline.
  3. **Retriever quality test:** Train identical agents with BM25 vs. E5; measure search call frequency and final performance gap.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RL-trained search agents transfer their learned capabilities to more complex agentic domains such as software engineering, data science assistance, or multi-tool task planning?
- Basis in paper: [explicit] "Our analysis is primarily confined to search-based agent scenarios... RL for more complex and open-ended agent behaviors—such as those exhibited by data science assistants, software engineering agents, or multi-tool task planners—remains underexplored."
- Why unresolved: This study only evaluated search-augmented QA tasks. Complex agentic domains require longer-horizon planning, multi-step tool compositions, and credit assignment across extended trajectories that differ fundamentally from search-reasoning loops.
- What evidence would resolve it: Training and evaluating RL-based agents on benchmarks like SWE-bench (software engineering), multi-tool reasoning tasks, or data analysis workflows, comparing performance against search-only trained baselines.

### Open Question 2
- Question: Do learned reward models or preference-based feedback improve upon rule-based outcome and format rewards for training search agents?
- Basis in paper: [explicit] "Interesting future directions include exploring more advanced reward modeling techniques, such as learned reward functions and preference-based feedback."
- Why unresolved: The paper demonstrates that simple rule-based rewards (exact match + format) are effective, but learned reward models could potentially capture more nuanced aspects of search behavior, query quality, and reasoning coherence that rule-based metrics miss.
- What evidence would resolve it: Comparative experiments training search agents with learned reward models or RLHF-style preference optimization versus the rule-based approach, evaluated on both in-distribution and out-of-distribution benchmarks.

### Open Question 3
- Question: Would alternative intermediate retrieval reward formulations (e.g., semantic similarity, relevance scoring) overcome the limitations of substring-exact-match retrieval rewards?
- Basis in paper: [inferred] The paper found intermediate retrieval rewards ineffective, but only tested substring exact match (fret). This metric may be too rigid and fail to reward semantically relevant retrievals that don't contain the exact answer string.
- Why unresolved: The negative result may be specific to the substring EM implementation. Semantic relevance measures, dense embedding similarity, or answer-aware relevance scoring could provide better gradient signals without constraining the retrieval trajectory.
- What evidence would resolve it: Systematic comparison of multiple intermediate reward formulations (BM25 relevance scores, embedding cosine similarity, NLI-based relevance) with the same base RL setup, measuring both final performance and learning dynamics.

## Limitations
- The study is primarily based on experiments with HotpotQA dataset and limited LLM scales (up to 32B parameters)
- The analysis assumes a fixed interleaved reasoning-search format without exploring alternative agent architectures
- The inference-time robustness evaluation lacks comprehensive stress-testing across diverse query types

## Confidence
- **High Confidence:** The positive impact of format rewards on base LLM training and the detrimental effect of weak search engines during training are well-supported by ablation studies
- **Medium Confidence:** The advantage of general-purpose LLMs over reasoning-specialized ones is demonstrated but may depend on initialization quality
- **Low Confidence:** The inference-time robustness to search engine switching is shown anecdotally but lacks comprehensive validation

## Next Checks
1. **Cross-Dataset Generalization:** Validate the three key design insights on at least two additional reasoning datasets (e.g., WikiHop, StrategyQA) to test robustness beyond HotpotQA.
2. **Scaling Analysis Beyond 32B:** Test the diminishing returns hypothesis by training 70B-scale models with the identified best practices, measuring whether the performance gap relative to 32B models narrows or widens.
3. **Inference-Time Robustness Under Stress:** Systematically evaluate trained agents across a spectrum of inference-time search engines with controlled degradation (e.g., synthetic noise injection, domain shift) to quantify the limits of search engine switching robustness.