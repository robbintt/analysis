---
ver: rpa2
title: 'MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languages'
arxiv_id: '2504.10356'
source_url: https://arxiv.org/abs/2504.10356
tags:
- languages
- language
- data
- english
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultiLoKo, a new benchmark designed to evaluate
  the multilingual capabilities of large language models (LLMs) across 31 languages.
  Unlike many existing benchmarks that rely on translated English data, MultiLoKo
  features locally sourced questions for each language, ensuring cultural and linguistic
  relevance.
---

# MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languages

## Quick Facts
- **arXiv ID:** 2504.10356
- **Source URL:** https://arxiv.org/abs/2504.10356
- **Reference count:** 40
- **Primary result:** None of 11 tested LLMs achieved over 35% average exact match on 31 languages; models perform better when queried in the language relevant to the content.

## Executive Summary
MultiLoKo is a new benchmark designed to evaluate the multilingual capabilities of large language models (LLMs) across 31 languages using locally sourced questions. Unlike many existing benchmarks that rely on translated English data, MultiLoKo features questions written by native speakers, ensuring cultural and linguistic relevance. The benchmark includes 500 questions per language, supplemented by human and machine translations, allowing comprehensive analysis of knowledge transfer and localization effects. Experiments reveal significant performance gaps between languages and highlight the importance of culturally relevant data for accurate multilingual evaluation.

## Method Summary
MultiLoKo evaluates LLMs on closed-form question answering tasks using 31 locally sourced datasets with 500 questions each. Questions are drawn from the most visited Wikipedia pages (2016-2021) and filtered to ensure locality (score â‰¤ 3 on a 1-5 scale). The benchmark is split into development and out-of-distribution test sets based on Wikipedia page view frequency. Base models use 5-shot prompting while chat models use 0-shot. Exact Match (EM) accuracy is the primary metric, with auxiliary measures including performance gaps between languages, Mother Tongue Effect, and Locality Effect.

## Key Results
- No tested LLM achieved over 35% average exact match across all 31 languages
- Performance was consistently higher when questions were asked in the language relevant to the content
- Using local versus translated data can result in differences of more than 20 percentage points in estimated language difficulty
- Machine translations yielded lower overall performance but preserved model rankings (R=0.97)

## Why This Works (Mechanism)

### Mechanism 1: The Mother Tongue Effect (MTE)
Models retrieve factual knowledge more effectively when queried in the language natively associated with that knowledge. This occurs because knowledge is not fully unified in a language-agnostic representation within current LLMs. Specific facts appear to be encoded and indexed primarily by the linguistic context in which they appeared in training data. Translating a query into English forces cross-lingual mapping that degrades retrieval accuracy.

### Mechanism 2: Locality Effect (LE) & Sourcing Bias
Using translated data to evaluate multilingual capability systematically distorts perceived language difficulty. Translated benchmarks suffer from "translationese" artifacts and cultural misalignment. Native speakers generate questions using different cognitive pathways and topical focuses than translators. This exposes the gap between surface-level multilinguality (translating an English thought) and deep multilinguality (thinking in the local language).

### Mechanism 3: Out-of-Distribution (OOD) Splitting by Frequency
A benchmark split based on Wikipedia page view frequency functions as an effective proxy for OOD testing. LLMs are prone to memorizing high-frequency sequences. By placing most frequently visited pages in the development set and least frequently visited in the test set, the benchmark forces generalization to tail-knowledge rather than memorization of popular web text.

## Foundational Learning

- **Concept:** Exact Match (EM) vs. Semantic Equivalence
  - *Why needed here:* MultiLoKo relies on strict string matching to evaluate answers. Understanding that the model must output the exact string is critical for interpreting low scores.
  - *Quick check question:* Does the benchmark accept a correct definition of an answer, or only the verbatim string?

- **Concept:** Cross-Lingual Consistency
  - *Why needed here:* This metric measures if a model answers the same question correctly in two languages, separating knowledge gaps from transfer gaps.
  - *Quick check question:* If a model scores 0% on a language, is it consistent? (Answer: No, consistency requires overlap in correct answers).

- **Concept:** Locality Scoring (1-5 Scale)
  - *Why needed here:* Understanding the data pipeline requires knowing how "local" was defined. Topics scoring >3 (globally known) were rejected.
  - *Quick check question:* Would a question about "YouTube" be accepted into the benchmark? (Answer: No, it scores 5 on locality).

## Architecture Onboarding

- **Component map:** 31 Locally Sourced Datasets (500 each) + Human/Machine Translations -> 5-shot (Base) vs 0-shot (Chat) Prompts -> LLM Generation -> Post-processing (Lowercase, Strip Punctuation, Remove "Answer is" preambles) -> EM Score, Gap, MTE, LE

- **Critical path:**
  1. Load language-specific prompts (use provided templates, don't default to English)
  2. Generate answers (Temperature 0)
  3. Apply post-processing (Handle Japanese "desu", English "answer is")
  4. Calculate EM

- **Design tradeoffs:**
  - Human vs. Machine Translation: Machine translation preserves model rankings (R=0.97) but systematically lowers scores. Use Machine for cheap relative comparisons; use Human for absolute capability estimation.
  - EM vs. LLM-Judge: EM is brittle (fails on valid synonyms) but objective. LLM-Judges handle synonyms but introduce bias. MultiLoKo prioritizes objectivity via EM.

- **Failure signatures:**
  - "Chatty" Refusals: Chat models often generate full sentences or reasoning traces despite instructions, causing EM to drop to 0.
  - English Bias: A high negative MTE indicates the model only knows the fact in English.

- **First 3 experiments:**
  1. **Baseline & Gap:** Run the Dev set on your target model. Calculate the "Gap" (Best vs Worst language) to identify parity failures.
  2. **Transfer Probe (MTE):** Take the top 3 worst-performing languages. Compare EM on "Local Source" vs "Translated to English" to see if the failure is linguistic or knowledge-based.
  3. **Format Robustness:** Run a sample through the chat model with/without the specific post-processing regex to quantify how much "low score" is due to instruction following vs. lack of knowledge.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does model performance differ when MultiLoKo is used as a reading comprehension task (providing source text) versus the current closed-book knowledge setting?
- **Basis in paper:** [explicit] The conclusion states that because each question is coupled with a source paragraph, MultiLoKo could be transformed into a reading-comprehension benchmark, and the authors consider studying this difference "an interesting direction for future work."
- **Why unresolved:** The current benchmark evaluates only the retrieval of internal knowledge; it is unknown how providing context affects the performance gap between high- and low-resource languages.
- **What evidence would resolve it:** A comparative evaluation of model Exact Match (EM) scores on the current prompt versus a prompt including the source paragraph for all 31 languages.

### Open Question 2
- **Question:** Does utilizing the dataset's long-answer rationales for Chain-of-Thought (CoT) prompting improve cross-lingual consistency and the Mother Tongue Effect (MTE)?
- **Basis in paper:** [explicit] The authors note that the dataset contains elaborate long answers and foresee "interesting directions including studies into CoT prompting or studying answer rationales."
- **Why unresolved:** It is unclear if explicitly reasoning through the answer in a long-form format helps models bridge the knowledge transfer gap observed between languages.
- **What evidence would resolve it:** Ablation studies comparing standard prompting against CoT prompting using the provided long answers, specifically tracking changes in the MTE and Consistency metrics.

### Open Question 3
- **Question:** To what extent can MultiLoKo scores be inflated by oversampling multilingual Wikipedia during training without improving general multilingual capabilities?
- **Basis in paper:** [inferred] In the Limitations section, the authors acknowledge that because Wikipedia is the primary source, it is possible the benchmark can be "hacked" by strongly oversampling this data.
- **Why unresolved:** This raises a validity concern regarding whether the benchmark measures generalization or mere memorization of the specific corpus used for sourcing.
- **What evidence would resolve it:** Training models with weighted multilingual Wikipedia sampling and evaluating the delta between MultiLoKo scores and performance on independent, non-Wikipedia multilingual benchmarks.

## Limitations

- The benchmark's strict Exact Match scoring may artificially deflate scores for models that provide correct but differently formatted answers
- Chat models often refuse to answer benign questions due to safety filters, affecting overall performance metrics
- The withheld test set limits full reproducibility and external validation of reported results

## Confidence

- **High Confidence:** The Locality Effect (LE) and its demonstration that translated data distorts language difficulty estimates
- **Medium Confidence:** The Mother Tongue Effect (MTE) mechanism, as it relies on controlled prompts and assumes no hidden confounding factors in training data
- **Low Confidence:** The Out-of-Distribution (OOD) splitting claim, as it depends on an unverified assumption that Wikipedia page views correlate with pre-training frequency

## Next Checks

1. **Test the OOD Assumption:** Correlate the popularity-based split with an independent estimate of pre-training data frequency to confirm that the OOD mechanism is valid.

2. **Evaluate Post-Processing Sensitivity:** Run a small sample of answers through multiple post-processing pipelines to quantify how much Exact Match scores are impacted by format adherence versus genuine knowledge gaps.

3. **Cross-Verify MTE with Manual Probes:** Select a few high-MTE languages and manually verify if the "locally relevant" answers exist in the model's training corpus to ensure the effect is not due to prompt artifacts.