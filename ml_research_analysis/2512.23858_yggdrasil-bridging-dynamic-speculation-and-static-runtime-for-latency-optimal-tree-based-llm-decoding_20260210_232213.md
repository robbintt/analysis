---
ver: rpa2
title: 'Yggdrasil: Bridging Dynamic Speculation and Static Runtime for Latency-Optimal
  Tree-Based LLM Decoding'
arxiv_id: '2512.23858'
source_url: https://arxiv.org/abs/2512.23858
tags:
- decoding
- tree
- speculative
- draft
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Yggdrasil, a system designed to optimize
  speculative decoding for large language models (LLMs) by addressing the mismatch
  between dynamic drafting algorithms and static runtime assumptions. The key innovation
  is the Equal-Growth Tree (EGT) structure, which enables context-aware tree drafting
  while maintaining compatibility with static graph compilation.
---

# Yggdrasil: Bridging Dynamic Speculation and Static Runtime for Latency-Optimal Tree-Based LLM Decoding

## Quick Facts
- arXiv ID: 2512.23858
- Source URL: https://arxiv.org/abs/2512.23858
- Reference count: 40
- Primary result: Up to 3.98× speedup over state-of-the-art baselines across multiple hardware setups

## Executive Summary
Yggdrasil addresses the fundamental tension between dynamic speculative decoding algorithms and static runtime compilation in large language models. The system introduces the Equal-Growth Tree (EGT) structure that enables context-adaptive tree drafting while maintaining the fixed operator shapes required for compiler optimization. By incorporating latency-aware optimization that considers both acceptance length and verification overhead, and implementing a stage-based scheduling framework for ahead-of-time execution, Yggdrasil achieves substantial performance improvements while supporting unmodified LLMs across diverse hardware configurations.

## Method Summary
Yggdrasil implements a speculative decoding system that bridges dynamic drafting algorithms with static runtime compilation. The core innovation is the Equal-Growth Tree (EGT) structure, which uses a lightweight depth predictor to determine tree depth, then instantiates all draft graphs concurrently with uniform width per depth level. This eliminates per-token CPU branches while allowing context adaptation through post-hoc pruning. The system incorporates a latency-aware optimization objective that models drafting iterations, verification latency as a function of token count, and tree shape parameters. A stage-based scheduling framework enables ahead-of-time execution of dependent stages with token supersets to minimize GPU idle time from CPU-GPU coordination.

## Key Results
- Achieves up to 3.98× speedup over state-of-the-art baselines including SpecInfer, Sequoia, and vLLM-Spec
- Demonstrates consistent performance gains across multiple hardware setups (A100 and A40 GPUs)
- Shows 8% additional improvement from latency-aware optimization vs. AAL-only objectives
- Achieves 1.21× improvement from ahead-of-time stage execution scheduling

## Why This Works (Mechanism)

### Mechanism 1: Equal-Growth Tree (EGT) for Static Graph Compatibility
- Claim: EGT enables context-adaptive speculative tree drafting while maintaining fixed operator shapes required for compiler optimization.
- Mechanism: The algorithm greedily determines tree depth via a lightweight predictor, then instantiates all draft graphs concurrently with uniform width per depth level. This eliminates per-token CPU branches while allowing the tree structure to adapt contextually through where tokens attach and subsequent pruning.
- Core assumption: Tree depth can be accurately predicted from target model embeddings, and the optimal subtree can be recovered via post-hoc pruning rather than needing fully dynamic growth.
- Evidence anchors:
  - [abstract] "Yggdrasil introduces an equal-growth tree structure for static graph compatibility"
  - [§4.2] "We instantiate all Ddraft draft graphs concurrently, eradicating CPU branch mispredictions"
  - [Figure 7] Visualizes the 4-step EGT process (Predict → Select → Prune → Transform)
  - [corpus] TALON and STree explore adaptive token trees but don't address static compilation; EGT's specific contribution is compilation compatibility
- Break condition: If depth prediction error exceeds ~2 levels, either graphs are wasted (over-prediction) or acceptance ceiling is capped (under-prediction). Assumption: predictor trained on in-domain calibration data generalizes to test distribution.

### Mechanism 2: Latency-Aware Optimization Objective Over AAL
- Claim: Optimizing for wall-clock speedup rather than Average Accepted Length (AAL) better reflects real performance by accounting for non-uniform verification costs.
- Mechanism: The objective (Eq. 3) explicitly models drafting iterations, verification latency as a function of token count, and tree shape parameters ⟨Wdraft, Ddraft, Wverify⟩. This captures the reality that verification latency scales non-linearly with token count while AAL assumes constant cost.
- Core assumption: Latency profiles Tverifier(W) and Tdrafter(W) can be measured offline and remain stable across executions on the same hardware.
- Evidence anchors:
  - [§4.1] "Optimizing AAL in isolation produces diminishing or even negative returns"
  - [Figure 5b] Shows speedup curves flattening then reversing as verification count increases, despite AAL improving
  - [Figure 14] Reports 8% additional improvement from speedup objective vs. AAL objective
  - [corpus] TapOut uses bandit-based dynamic speculation length but optimizes acceptance; Yggdrasil explicitly models latency
- Break condition: If verification latency characteristics shift significantly (e.g., different batch scheduling, thermal throttling), the offline profiles become stale. Assumption: hardware state is relatively stable during inference.

### Mechanism 3: Ahead-of-Time Stage Execution
- Claim: Speculatively executing dependent stages with token supersets reduces GPU idle time from CPU-GPU coordination.
- Mechanism: Tail draft and head draft stages are launched ahead-of-time with all possible tokens rather than waiting for acceptance results. When acceptance is known, results from the correct branch are reused; wasted computation is tolerated because these stages are lightweight and run during otherwise idle periods.
- Core assumption: The overhead of computing unused draft tokens is less than the GPU bubble time that would otherwise occur; acceptance rate is high enough that speculation pays off.
- Evidence anchors:
  - [§5.1] "We speculatively draft the entire candidate sequence. Once any leaf is accepted, we simply reuse the corresponding results"
  - [Figure 9] Contrasts vanilla scheduling (with bubbles) vs. Yggdrasil's overlapped execution
  - [Figure 12, O4] Reports 1.21× improvement from graph-based scheduling
  - [corpus] Weak corpus signal for this specific technique; appears novel to Yggdrasil
- Break condition: If acceptance rates are very low, ahead-of-time execution becomes pure overhead. If draft model is large relative to target, extra draft iterations may dominate savings.

## Foundational Learning

- Concept: **Speculative Decoding Basics**
  - Why needed here: Yggdrasil builds on the drafting-verification paradigm; understanding why AAL matters and how parallel verification works is prerequisite.
  - Quick check question: Can you explain why speculative decoding helps when GPU compute is underutilized during memory-bound autoregressive generation?

- Concept: **Graph Compilation and CUDA Graphs**
  - Why needed here: The core tension Yggdrasil resolves is between dynamic tree structures and static graph compilation; you need to understand what compilation buys (kernel fusion, memory planning) and what it requires (fixed shapes).
  - Quick check question: Why does a dynamic tree structure with variable-width branches break CUDA Graph capture?

- Concept: **Tree-Based Drafting vs. Sequence Drafting**
  - Why needed here: EGT is a tree structure variant; understanding prior approaches (SpecInfer's K-ary tree, Sequoia's static dataset-adaptive tree) clarifies what's novel.
  - Quick check question: What is the tradeoff between a wider tree (more candidates) and verification cost?

## Architecture Onboarding

- Component map: Compile-time (Draft/verifier model lowering, profile-driven cost models, depth predictor training, execution plan search) -> Runtime (TokenTree class, KV cache manager, attention mask generator, stage scheduler) -> External (Draft model, target model, calibration dataset)

- Critical path: 1. Offline: Profile latency characteristics → Train depth predictor → Search execution plans → Compile graphs; 2. Online per-step: Predict depth → Select width → Grow EGT → Prune to verification budget → Verify → Accept tokens → Repeat

- Design tradeoffs:
  - **Depth prediction accuracy vs. compilation flexibility**: Deeper trees enable more speculation but require more accurate prediction
  - **Ahead-of-time overhead vs. bubble reduction**: More aggressive speculation reduces idle time but wastes computation on rejected branches
  - **Verification width vs. latency**: More tokens verified increases AAL but eventually increases per-token latency (Fig. 5)

- Failure signatures:
  - Low speedup despite high AAL → Check if verification width is too large (latency saturation, §3, Fig. 5)
  - Compilation errors → EGT not generating uniform shapes; check if width varies per depth level
  - High CPU overhead → Stage scheduling not applied; verify ahead-of-time execution is enabled
  - Poor depth prediction → Calibration dataset mismatched to inference distribution

- First 3 experiments:
  1. **Baseline comparison**: Run Yggdrasil vs. SpecInfer, Sequoia, vLLM-Spec on C4/Wikitext with Llama-2-7B + Llama-68M; measure per-token latency and AAL separately to isolate algorithmic vs. runtime gains.
  2. **Ablation on EGT parameters**: Sweep ⟨Wdraft, Ddraft, Wverify⟩ combinations (Fig. 13) to find optimal settings for your hardware; verify that the default predictor selects near-optimal values.
  3. **Ahead-of-time stage profiling**: Use profiler to measure GPU utilization and bubble time with and without ahead-of-time execution; confirm that the 1.21× scheduling improvement (O4 in Fig. 12) reproduces on your setup.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Yggdrasil be extended to optimize the latency-throughput trade-off in batched serving scenarios?
- Basis in paper: [explicit] The Limitations section states that extending the framework to the "latency-throughput joint optimization space," where speculative decoding and batch scheduling are solved together, "remains an open line of research."
- Why unresolved: The current design assumes a single request monopolizing GPU memory, allowing for static graph compilation; batching introduces dynamic memory pressure and preemption, which conflict with the static assumptions of the Equal-Growth Tree (EGT).
- What evidence would resolve it: A unified scheduler that dynamically adjusts batch size and speculation parameters, demonstrating stable performance under high concurrency.

### Open Question 2
- Question: How robust is the depth predictor when facing out-of-distribution (OOD) inputs?
- Basis in paper: [inferred] Section 4.2 mentions the depth predictor is trained offline "for each dataset and drafter/verifier pair with training data collected once via profiling on an in-domain validation corpus."
- Why unresolved: If the input context diverges significantly from the validation corpus (e.g., different languages or specialized domains), the predictor may fail to estimate the optimal tree depth, potentially degrading speedup.
- What evidence would resolve it: Evaluating the system's latency and prediction accuracy on datasets structurally distinct from the calibration data (e.g., code vs. natural language).

### Open Question 3
- Question: Does the Ahead-of-Time (AoT) stage execution strategy retain its benefits on compute-bound hardware?
- Basis in paper: [inferred] Section 5.1 argues for AoT execution based on the observation that operations are "lightweight" and can run concurrently on memory-bound hardware like the A100.
- Why unresolved: AoT executes superset tokens speculatively; on hardware with less parallel capacity or for larger draft models, this redundant computation might increase critical path latency rather than hiding CPU overhead.
- What evidence would resolve it: Benchmarks on diverse hardware architectures (e.g., consumer-grade GPUs or older generations) analyzing the trade-off between overhead reduction and extra computation.

## Limitations
- Compilation compatibility trade-off: EGT sacrifices some algorithmic optimality for compilation compatibility through approximation rather than optimal solutions
- Hardware dependency: Performance optimizations rely heavily on stable hardware characteristics and may not transfer well to different configurations
- Dataset generalization: Depth predictor requires per-dataset training, creating scaling challenges for new applications

## Confidence

**High Confidence**: Compilation compatibility of EGT (demonstrated through CUDA Graph integration), superiority of latency-aware optimization over AAL-only objectives (supported by Figure 5b), and overall performance improvements (3.98× speedup across multiple baselines and hardware setups).

**Medium Confidence**: Ahead-of-time stage execution benefits (1.21× improvement cited) and specific EGT parameter choices (depth prediction architecture, width selection algorithm) - supported by ablation studies but sensitive to implementation details and workload characteristics.

**Low Confidence**: Cross-dataset generalization of the depth predictor and absolute magnitude of improvements on untested hardware configurations. Methodology for optimal execution plan search is mentioned but not fully specified.

## Next Checks

**Validation Check 1**: Reproduce the latency saturation phenomenon shown in Figure 5 on different hardware configurations. Specifically, measure per-token latency as a function of verification width W_verify for your target hardware to verify that increasing W_verify beyond a threshold actually degrades speedup despite improving AAL.

**Validation Check 2**: Implement an ablation study comparing EGT with fully dynamic tree structures on a small scale (e.g., reducing model size to enable CPU-side dynamic computation). Measure the compilation benefit (static graphs) against the algorithmic optimality loss to quantify the trade-off Yggdrasil makes.

**Validation Check 3**: Test the depth predictor's generalization by training it on one dataset (e.g., C4) and evaluating on another (e.g., Wikitext). Measure prediction accuracy degradation and resulting performance impact to assess whether per-dataset training is truly necessary or if a more general predictor is feasible.