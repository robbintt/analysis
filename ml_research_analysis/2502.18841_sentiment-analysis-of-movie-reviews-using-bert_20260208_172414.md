---
ver: rpa2
title: Sentiment Analysis of Movie Reviews Using BERT
arxiv_id: '2502.18841'
source_url: https://arxiv.org/abs/2502.18841
tags:
- sentiment
- bert
- reviews
- polarity
- movie
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper fine-tunes BERT with BiLSTM for binary sentiment classification
  on movie review datasets (IMDb, MR, SST-2, Amazon). The BERT-BiLSTM model achieved
  state-of-the-art accuracy of 97.67-98.76% across all datasets, outperforming models
  like coRNN, SMART-RoBERTa, and DualCL.
---

# Sentiment Analysis of Movie Reviews Using BERT

## Quick Facts
- arXiv ID: 2502.18841
- Source URL: https://arxiv.org/abs/2502.18841
- Reference count: 34
- This paper fine-tunes BERT with BiLSTM for binary sentiment classification on movie review datasets (IMDb, MR, SST-2, Amazon), achieving state-of-the-art accuracy of 97.67-98.76%.

## Executive Summary
This paper presents a BERT-BiLSTM model for binary sentiment classification on movie review datasets. The approach combines BERT's contextualized embeddings with BiLSTM's sequential processing, achieving state-of-the-art accuracy of 97.67-98.76% across IMDb, MR, SST-2, and Amazon video reviews. The work also introduces a heuristic algorithm to compute overall sentiment polarity from model predictions, which matched ground truth across all datasets. Ablation studies confirm both BERT and BiLSTM components are necessary for optimal performance.

## Method Summary
The method fine-tunes BERT-base-uncased with BiLSTM for binary sentiment classification. The [CLS] token from BERT is fed into a BiLSTM layer followed by a dense layer for binary classification. Early BERT layers are frozen while later layers are trainable. Training uses Adam optimizer (learning rate 3e-5, epsilon 1e-08), sparse categorical crossentropy loss, sequence length 256, and 10 epochs. The approach also implements a threshold-based heuristic to convert per-review predictions into overall polarity labels.

## Key Results
- BERT-BiLSTM achieved state-of-the-art accuracy of 97.67-98.76% across all four datasets
- Ablation studies show BERT alone achieves 93.81-94.78% and BiLSTM alone achieves 90.42-92.18%
- Overall polarity computed from predictions matched ground truth for all datasets using the heuristic algorithm

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Coupling BERT with BiLSTM improves binary sentiment classification accuracy beyond either component alone.
- **Mechanism:** BERT produces contextualized token embeddings via bidirectional self-attention; BiLSTM further processes these embeddings sequentially in both directions before a dense classification layer.
- **Core assumption:** BERT's frozen early layers capture generalizable linguistic features; only later layers need task-specific adaptation.
- **Evidence anchors:** [abstract] "achieved state-of-the-art accuracy of 97.67-98.76%"; [section III.C] "Weights from first layers of BERT are frozen"
- **Break condition:** If early layer freezing prevents adaptation to domain-specific language patterns, performance gains may diminish on out-of-distribution reviews.

### Mechanism 2
- **Claim:** The [CLS] token aggregation enables single-vector representation for sequence-level classification.
- **Mechanism:** BERT prepends a [CLS] token whose final hidden state is trained to aggregate information from all input tokens.
- **Core assumption:** A single token can sufficiently compress variable-length review semantics.
- **Evidence anchors:** [section III.B] "The first token of every sequence is denoted as [CLS]... is used for classification tasks."
- **Break condition:** Long reviews with multiple sentiment shifts may not compress cleanly into a single [CLS] representation.

### Mechanism 3
- **Claim:** A threshold-based heuristic converts per-review predictions into an overall polarity label.
- **Mechanism:** Count positive and negative predictions; assign "positive" if positives > 1.2× negatives, "negative" if negatives > 1.2× positives, else "neutral."
- **Core assumption:** Binary classification outputs are reliable enough that simple counting reflects true sentiment distribution.
- **Evidence anchors:** [section III.E] "a positive overall sentiment is assigned if there is at least 1.5 times as many negative reviews as positive reviews"; [section IV.E, Table IV] Computed overall polarity matched original overall polarity
- **Break condition:** If model predictions are systematically biased, overall polarity will be skewed regardless of threshold.

## Foundational Learning

- **Concept: Transfer learning via fine-tuning**
  - **Why needed here:** The entire approach relies on adapting a pretrained BERT model rather than training from scratch.
  - **Quick check question:** Can you explain why freezing early layers but training later layers might preserve general features while adapting to task-specific patterns?

- **Concept: Bidirectional sequence modeling**
  - **Why needed here:** Both BERT (via self-attention) and BiLSTM process text in both directions to capture full context.
  - **Quick check question:** Why would processing "this movie was not bad" left-to-right only create problems for sentiment detection?

- **Concept: Tokenization with special tokens ([CLS], [SEP])**
  - **Why needed here:** Input must be formatted correctly for BERT to aggregate sequence information and handle sentence boundaries.
  - **Quick check question:** What would happen if you omitted the [CLS] token when preparing inputs for classification?

## Architecture Onboarding

- **Component map:** Input preprocessing → BERT tokenizer → [input_ids, attention_mask] → BERT-base (12 layers, 768 hidden) → [CLS] hidden state → BiLSTM (1 hidden layer) → Dense layer (output dim: 1) → Binary prediction → Heuristic aggregator → Overall polarity

- **Critical path:**
  1. Lowercase text → tokenize → word-piece splitting → map to vocab indices → add [CLS]/[SEP]
  2. Feed through frozen BERT backbone (early layers) + trainable later layers
  3. BiLSTM processes BERT outputs bidirectionally
  4. Dense layer produces logits → binary classification
  5. Aggregate predictions via threshold heuristic

- **Design tradeoffs:**
  - Freezing early BERT layers: Faster training, less overfitting risk, but may under-adapt to domain-specific language
  - 1.2× threshold in heuristic: Rejects near-ties as "neutral," but threshold is empirically set without theoretical justification
  - Binary classification only: Simpler but discards neutral/ambiguous reviews; extension to multi-class proposed as future work

- **Failure signatures:**
  - Accuracy drops below standalone BERT (93–95%): Check that BiLSTM is receiving correct input dimensions from BERT output
  - All predictions same class: Verify label encoding and loss function (sparse categorical crossentropy expects integer labels)
  - Overfitting after epoch 10: Early layers may need stronger regularization or lower learning rate
  - Overall polarity always "neutral": Threshold may be too aggressive for dataset balance; check class distribution

- **First 3 experiments:**
  1. Reproduce ablation: Run BERT-only, BiLSTM-only, and BERT+BiLSTM on a single dataset (e.g., MR) to verify reported accuracy gaps
  2. Threshold sensitivity: Test overall polarity heuristic with thresholds from 1.0 (simple majority) to 1.5; measure how often "neutral" is assigned and whether it aligns with ground-truth dataset balance
  3. Sequence length analysis: Test model with max sequence lengths of 128, 256, 512 on IMDb (longer reviews) to identify whether truncation affects performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the BERT-BiLSTM architecture maintain its performance advantage when extended to multi-class (e.g., 3 or 4 class) or fine-grained sentiment classification?
- **Basis in paper:** [explicit] The authors state the model "can be extended to three-class, four-class, or any fine-grained classification... This is intended to be exploited in future work."
- **Why unresolved:** The current study only validates the model on binary classification tasks (positive/negative) across four datasets.
- **What evidence would resolve it:** Experimental results on datasets with fine-grained labels (e.g., SST-5) showing accuracy comparisons against SOTA multi-class models.

### Open Question 2
- **Question:** How can accuracy improvement techniques be effectively applied to transformed BERT features without suffering from the loss of semantic information?
- **Basis in paper:** [explicit] The conclusion explicitly lists this as a future direction: "Future work will additionally dwell on how to effectively apply accuracy improvement techniques to transformed BERT features despite loss of semantic information in them."
- **Why unresolved:** The paper establishes a baseline performance but does not investigate the trade-offs involved in feature transformation optimization.
- **What evidence would resolve it:** A comparative analysis of various feature transformation techniques showing metrics for both semantic preservation and classification accuracy.

### Open Question 3
- **Question:** To what extent do different components of a sentence (e.g., specific phrases or grammatical structures) contribute to the final sentiment prediction in this architecture?
- **Basis in paper:** [explicit] The authors identify the need to explore "how different components of a sentence contribute to its sentiment prediction since this is information that is not generally explored by current works."
- **Why unresolved:** The paper focuses on aggregate accuracy metrics and does not include an interpretability analysis or ablation of sentence parts.
- **What evidence would resolve it:** Attention visualization or constituent ablation studies that correlate specific sentence segments with the model's classification confidence.

## Limitations

- Several critical implementation details remain unspecified, including which specific BERT layers are frozen and BiLSTM hidden dimensions
- The overall polarity heuristic uses a 1.2× threshold without theoretical justification or sensitivity analysis
- The claim of achieving "state-of-the-art" status requires verification against contemporaneous works
- The Amazon dataset preprocessing is incompletely specified, particularly regarding class balance handling

## Confidence

**High Confidence (90-95%)**: The core mechanism of BERT+BiLSTM architecture for binary sentiment classification is well-established and reproducible. The accuracy ranges (97.67-98.76%) are plausible given the strong performance of BERT on similar tasks.

**Medium Confidence (70-85%)**: The overall polarity heuristic's effectiveness and the specific 1.2× threshold validity are supported by the paper's internal verification but lack external validation.

**Low Confidence (40-60%)**: Exact reproduction details including frozen layer specifications, BiLSTM architecture parameters, and batch size remain unknown. These omissions prevent faithful replication of the reported results.

## Next Checks

1. **Ablation Study Replication**: Reproduce the standalone BERT vs BiLSTM vs BERT+BiLSTM comparisons on the MR dataset to verify the claimed accuracy gaps (BERT: ~94%, BiLSTM: ~91%, Combined: ~98%).

2. **Threshold Sensitivity Analysis**: Systematically test the overall polarity heuristic with thresholds ranging from 1.0× (simple majority) to 1.5× across all four datasets. Measure how often "neutral" is assigned and whether this aligns with the actual sentiment distribution in each dataset.

3. **Class Balance Impact on Amazon**: Investigate whether the 239K positive vs 37K negative training samples in Amazon require class weighting or sampling adjustments. Run experiments with and without class weights to determine if the reported accuracy accounts for this severe imbalance.