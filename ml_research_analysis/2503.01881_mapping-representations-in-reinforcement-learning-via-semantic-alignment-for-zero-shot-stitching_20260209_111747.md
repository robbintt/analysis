---
ver: rpa2
title: Mapping representations in Reinforcement Learning via Semantic Alignment for
  Zero-Shot Stitching
arxiv_id: '2503.01881'
source_url: https://arxiv.org/abs/2503.01881
tags:
- learning
- different
- stitching
- latent
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of zero-shot policy reuse in reinforcement
  learning when faced with visual and task variations across environments. The proposed
  method, SAPS (Semantic Alignment for Policy Stitching), leverages semantic alignment
  to map latent spaces between independently trained RL agents, enabling seamless
  transfer of encoders and controllers without retraining.
---

# Mapping representations in Reinforcement Learning via Semantic Alignment for Zero-Shot Stitching

## Quick Facts
- arXiv ID: 2503.01881
- Source URL: https://arxiv.org/abs/2503.01881
- Authors: Antonio Pio Ricciardi; Valentino Maiorca; Luca Moschella; Riccardo Marin; Emanuele Rodolà
- Reference count: 8
- Key outcome: SAPS achieves near end-to-end performance across visual and task domain shifts in CarRacing and LunarLander without retraining

## Executive Summary
This paper addresses zero-shot policy reuse in reinforcement learning when faced with visual and task variations across environments. The proposed method, SAPS (Semantic Alignment for Policy Stitching), enables seamless transfer of encoders and controllers between independently trained RL agents by leveraging semantic alignment. By collecting a small set of semantically aligned observation pairs and estimating an affine transformation between latent embeddings, SAPS allows a controller trained in one domain to operate effectively in another with different visual styles or task parameters.

## Method Summary
SAPS tackles zero-shot stitching of encoder-controller pairs from independently trained RL agents across visual and task domain shifts, without retraining. The method involves training policies end-to-end per environment variation, splitting them at the first flatten layer into encoder ϕ and controller ψ, collecting semantically aligned anchor pairs, embedding anchors to obtain corresponding latent matrices, estimating an affine transform τ via SVD, and evaluating the stitched policy ψ[τ(ϕ(o))] zero-shot on new environments.

## Key Results
- SAPS achieves 822-859 scores on visual variations in CarRacing (comparable to end-to-end performance)
- SAPS maintains 764-824 scores on task variations like "slow" in CarRacing
- Outperforms baselines like naive stitching and relative representation methods (R3L) in most cases

## Why This Works (Mechanism)

### Mechanism 1: Semantic Anchor Correspondence Enables Latent Space Mapping
A small set of semantically aligned observation pairs suffices to estimate a functional transformation between independently trained encoder latent spaces. Anchor pairs collected by replaying identical action sequences across environments or applying pixel-space transformations provide correspondence points for SVD-based affine estimation. The core assumption is that independently trained encoders produce semantically isomorphic latent structures when operating on corresponding observations.

### Mechanism 2: Affine Transformation Preserves Controller Compatibility
An affine transformation estimated via SVD maps source encoder outputs sufficiently close to target encoder latent space for downstream controller reuse. Given anchor embedding matrices, SVD solves for rotation and bias minimizing reconstruction error, enabling the target controller to process source-domain observations as if they came from its native encoder. The core assumption is that latent space geometry differences are approximately linear.

### Mechanism 3: Policy Modularization Decouples Visual from Task Representations
Factorizing policies into encoders and controllers enables combinatorial reuse across visual-task variation combinations unseen during training. Policy π = ψ ∘ ϕ disentangles observation-specific features from task-specific decision rules. The core assumption is that sufficient separation exists for stitching to preserve functionality.

## Foundational Learning

- Concept: **Latent space embeddings and isomorphism**
  - Why needed here: Understanding that different encoders can represent equivalent information in different coordinate systems is prerequisite to grasping why a linear map might suffice.
  - Quick check question: If two encoders output 64-dimensional vectors for the same image, why might these vectors be related by a rotation rather than being completely arbitrary?

- Concept: **Singular Value Decomposition (SVD) for Procrustes-style alignment**
  - Why needed here: SAPS uses SVD to solve the orthogonal Procrustes problem—finding the best rotation/affine map between point sets.
  - Quick check question: Given two sets of paired points A and B in R^n, what does SVD minimize when finding transformation matrix R?

- Concept: **Policy decomposition (encoder-controller factorization)**
  - Why needed here: SAPS relies on treating policies as modular; understanding where to "cut" the network determines what gets transformed versus preserved.
  - Quick check question: In a CNN-based RL agent processing 84×84×12 image stacks through conv layers then MLPs, where would you place the encoder-controller boundary?

## Architecture Onboarding

- Component map: Source encoder ϕ_source -> Transformation τ -> Target controller ψ_target
- Critical path:
  1. Train policies π_source and π_target end-to-end on respective environment variations
  2. Collect anchor pairs by replaying actions or applying pixel transforms
  3. Extract embeddings Xu = ϕ_source(A_source), Xv = ϕ_target(A_target)
  4. Estimate τ via SVD: minimize ||τ(Xu) - Xv||
  5. Deploy stitched policy; no gradient updates required

- Design tradeoffs:
  - Affine vs. orthogonal: Affine includes bias term, handles mean-shifted latents; orthogonal preserves distances exactly. Paper uses affine.
  - Anchor quantity vs. quality: More anchors improve transformation estimation but require more correspondence collection.
  - Encoder-controller boundary placement: Earlier cut = larger latent space = harder alignment; later cut = more task-specific features in encoder.

- Failure signatures:
  - LunarLander underperformance: SAPS scores 19-52 vs. end-to-end 192-221
  - High variance in "slow" task: Standard deviations of 192-328 suggest unstable stitching
  - R3L training failures: Relative representation method couldn't train on LunarLander

- First 3 experiments:
  1. Single visual variation stitching: Train encoder on green background, controller on red background. Collect 100-500 anchor pairs via pixel-space color transformation. Verify stitched policy achieves >90% of end-to-end performance.
  2. Ablate anchor count: Repeat experiment 1 with 10, 50, 100, 500 anchors. Plot stitched policy performance vs. anchor count.
  3. Cross-seed stitching: Train multiple policies with different seeds on same visual variation. Stitch encoders and controllers across seeds to verify transformation captures visual—not just seed-specific—differences.

## Open Questions the Paper Calls Out

### Open Question 1
Can non-linear or learned transformations improve SAPS performance in domains with drastic gaps in observation type or reward structure where affine maps fail? The authors note the method's "reliance on affine transformations can falter in domains exhibiting larger gaps (for example, tasks that differ drastically in reward structure or observation type) where a simple linear map may be insufficient."

### Open Question 2
How can semantic correspondences be reliably established for anchor collection in highly stochastic environments where action replay yields divergent states? The paper states that "in highly stochastic domains, obtaining robust correspondences can be nontrivial" as the current method relies on action replay or deterministic pixel transformations.

### Open Question 3
Does SAPS scale effectively to real-world robotics tasks with high state space complexity and real sensor data? The authors list as a limitation that "it is unclear how well it scales to real-world robotics or continuous domains with high state space complexity."

## Limitations
- Affine transformations may be insufficient for domains with drastically different reward structures or observation types
- Performance degrades significantly on complex domains like LunarLander (scores of 19-52 vs 192-221 end-to-end)
- Unclear scalability to real-world robotics with high state space complexity and real sensor data

## Confidence
- **High confidence**: The modular policy decomposition framework and SVD-based affine estimation methodology are technically sound and well-supported by established literature
- **Medium confidence**: Semantic alignment via anchor pairs is effective for moderate visual variations, but performance on task variations suggests limitations
- **Low confidence**: Claims about universal applicability across arbitrary visual and task variations without retraining, particularly given the LunarLander failure case

## Next Checks
1. **Ablate anchor quantity**: Systematically vary anchor pair counts (10, 50, 100, 500) on CarRacing visual variations and plot stitched performance to identify minimum viable correspondence requirements
2. **Nonlinear transformation comparison**: Implement and test nonlinear alignment methods on LunarLander to determine if affine limitations explain the performance gap
3. **Cross-seed generalization**: Train multiple policies with different seeds on identical visual variations, then attempt stitching across seeds to verify transformations capture visual—not just training-specific—differences