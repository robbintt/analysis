---
ver: rpa2
title: A Vietnamese Dataset for Text Segmentation and Multiple Choices Reading Comprehension
arxiv_id: '2506.15978'
source_url: https://arxiv.org/abs/2506.15978
tags:
- text
- vietnamese
- segmentation
- question
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VSMRC, a Vietnamese dataset for text segmentation
  and multiple-choice reading comprehension. The dataset contains 15,942 documents
  for text segmentation and 16,347 multiple-choice question-answer pairs, all sourced
  from Vietnamese Wikipedia and validated by human experts.
---

# A Vietnamese Dataset for Text Segmentation and Multiple Choices Reading Comprehension

## Quick Facts
- **arXiv ID**: 2506.15978
- **Source URL**: https://arxiv.org/abs/2506.15978
- **Reference count**: 0
- **Primary result**: Multilingual models (mBERT, XLM-R) significantly outperform monolingual models on Vietnamese text segmentation and MRC tasks.

## Executive Summary
This paper introduces VSMRC, a Vietnamese dataset for text segmentation and multiple-choice reading comprehension. The dataset contains 15,942 documents for text segmentation and 16,347 multiple-choice question-answer pairs, all sourced from Vietnamese Wikipedia and validated by human experts. Experiments show that multilingual models (mBERT, XLM-R) significantly outperform monolingual models on both tasks, with mBERT achieving 88.01% accuracy on MRC and 63.15% F1 score on text segmentation. The findings suggest multilingual models excel at Vietnamese NLP tasks, highlighting the dataset's potential for cross-lingual research.

## Method Summary
The VSMRC dataset was created using Vietnamese Wikipedia dumps (20250301) as source material. For text segmentation, documents were filtered to 750-3,000 tokens and divided into segments. For MRC, segments were further filtered to 450-1,200 characters and used to generate synthetic multiple-choice questions. Question generation involved LLM-based synthetic question-answer pair creation with passage-derived distractors, followed by LLM verification and human expert sampling (20 experts, 37% sampling rate). Models were fine-tuned using AdamW optimizer (learning rate 2e-05, batch size 4 for segmentation / 8 for MRC, 3 epochs) on 80/10/10 and 70/20/10 data splits respectively.

## Key Results
- Multilingual models significantly outperform monolingual Vietnamese models on both tasks
- mBERT achieves 88.01% accuracy on MRC and 63.15% F1 score on text segmentation
- Adding English training data to multilingual models improves Vietnamese segmentation performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multilingual pre-training appears to capture structural priors for text segmentation that monolingual Vietnamese models lack.
- **Mechanism**: Models like mBERT and XLM-R, exposed to diverse typologies during pre-training, likely generalize discourse boundaries better than models trained solely on Vietnamese or English data.
- **Core assumption**: Segmentation cues (e.g., topic shifts) share cross-lingual features that are transferable, whereas monolingual models may overfit to local lexical patterns without sufficient structural signal.
- **Evidence anchors**: [Section 6.1]: Multilingual models significantly outperformed Vietnamese-specific models on segmentation (mBERT 63.15% F1 vs. PhoBERT 36.51% F1). [Section 6.1.1]: Performance improved when English data (Wiki727k) was added to Vietnamese training, suggesting shared structural patterns. [Corpus]: Related work confirms strong performance of multilingual models in low-resource settings.

### Mechanism 2
- **Claim**: Synthetic Multiple-Choice Question (MCQ) generation achieves high validity through a hierarchical filtering pipeline combining LLMs and human experts.
- **Mechanism**: An initial LLM filters low-quality segments; a second LLM generates questions with passage-derived distractors; a subsequent "judge" LLM verifies answerability; finally, human experts spot-check for ambiguity.
- **Core assumption**: LLMs are sufficiently capable of generating plausible distractors (incorrect options) that are lexically similar to the correct answer but semantically wrong, reducing the "obviousness" of the solution.
- **Evidence anchors**: [Section 3.2.5]: Expert evaluation found only 4.58% of distractors were low quality and only 7.6% of answers were unclear across 6,000 sampled pairs. [Table 2]: Shows low error rates across validation criteria. [Corpus]: *Generating Reading Comprehension Exercises with LLMs* supports the efficacy of LLM-driven educational content creation.

### Mechanism 3
- **Claim**: Model performance degrades predictably with increased reasoning complexity and context length.
- **Mechanism**: As questions require multi-sentence synthesis (Reasoning/List types) or longer context windows, the attention mechanism dilutes the signal from relevant tokens, leading to lower accuracy.
- **Core assumption**: The models rely heavily on surface-level matching (Word Matching) for simpler tasks but struggle with long-range dependencies required for complex reasoning.
- **Evidence anchors**: [Section 6.2.1]: Accuracy drops for "reasoning" and "list" questions compared to "fact-check" across all models (e.g., BERT4News drops to 43.15% on reasoning). [Section 6.2.2]: Performance declines for questions >20 words and passages >200 words. [Corpus]: *MRCEval* benchmarks similarly note performance sensitivity to question complexity/type.

## Foundational Learning

- **Concept: Topic Segmentation (TextTiling)**
  - **Why needed here**: This is the primary task for the first half of the dataset. Unlike sentence segmentation, this requires identifying *semantic* shifts between paragraphs.
  - **Quick check question**: How does the $P_k$ metric differ from standard Precision/Recall in evaluating segmentation boundaries?

- **Concept: Cross-lingual Transfer**
  - **Why needed here**: The paper's central finding is that English data improves Vietnamese segmentation. Understanding how shared sub-word tokens (via SentencePiece/WordPiece) enable this is crucial.
  - **Quick check question**: Why might adding English training data to a multilingual model improve performance on a Vietnamese task, as seen in Table 8?

- **Concept: Distractor Generation**
  - **Why needed here**: A multiple-choice dataset is only as hard as its wrong answers. The paper uses LLMs to generate "plausible" distractors rather than random sampling.
  - **Quick check question**: According to the validation criteria (Section 3.2.5), what makes a distractor "low quality"?

## Architecture Onboarding

- **Component map**: Vietnamese Wikipedia Dump -> Length filtering (750-3,000 tokens) -> Segment quality validation (Gemini/OpenAI) -> MCQ generation (Gemini 2.0 Flash Lite) -> Distractor synthesis -> LLM Verification (DeepSeek/GPT-4o-mini) -> Human Expert Sampling (20 experts) -> Train/Eval: BERT-based encoders (PhoBERT, mBERT) fine-tuned for classification (MRC) and token classification (Segmentation)

- **Critical path**: The prompt engineering for the **Distractor Generation** (Appendix A.2). If the prompts do not strictly enforce "passage-derived" distractors, the dataset becomes solvable via simple elimination, rendering the high accuracy metrics meaningless.

- **Design tradeoffs**: Synthetic vs. Human Data: The authors traded the authenticity of human-authored questions for the scalability of synthetic generation, mitigating risk via a 37% human sampling rate. Domain Specificity: Sourced entirely from Wikipedia. The models may struggle with informal text (e.g., social media) or specialized domains (e.g., legal/medical) not present in the training set.

- **Failure signatures**: High "Ambiguous/Insufficient" (AoI) rate: If human evaluation finds >15% AoI (currently 6-7%), the segmentation or question framing is flawed. Monolingual Parity: If Vietnamese monolingual models (PhoBERT) failed to compete on MRC (they did compete), it would suggest the vocabulary/dictionary preprocessing was misaligned.

- **First 3 experiments**:
  1. **Reproduce Segmentation Baselines**: Train XLM-R-base and PhoBERT-large on the text segmentation split to verify the ~26 point gap reported in Table 7.
  2. **Cross-lingual Ablation**: Train a model on *only* English Wiki727k data and evaluate zero-shot on the VSMRC test set to quantify the pure transfer capability without Vietnamese fine-tuning.
  3. **Distractor Quality Audit**: Perform a manual review of 100 random "Reasoning" questions to confirm that the correct answer cannot be identified simply by matching keywords (testing the "passage-dependence" constraint).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the VSMRC framework and the observed benefits of multilingual data scaling be effectively generalized to other under-resourced languages outside the Vietnamese context?
- **Basis in paper**: [explicit] The conclusion states, "Future work will... extend VSMRC to other underrepresented languages in Asia, Africa or Europe."
- **Why unresolved**: The current study validates the approach only for Vietnamese; it remains unconfirmed if the synthetic data generation pipeline and multilingual training advantages apply universally to languages with different morphological structures or resource levels.
- **What evidence would resolve it**: Applying the same dataset creation pipeline to a different low-resource language and observing if multilingual models maintain their performance edge.

### Open Question 2
- **Question**: What are the specific mechanisms of knowledge transfer that allow English training data to improve Vietnamese text segmentation performance?
- **Basis in paper**: [explicit] The authors note, "Future work will explore knowledge transfer across languages" after observing that adding English data to Vietnamese training improved F1 scores.
- **Why unresolved**: While the paper demonstrates the empirical result, the underlying reasons (e.g., shared structure patterns vs. regularization effects) are hypothesized but not proven.
- **What evidence would resolve it**: Ablation studies analyzing attention heads or feature alignment between English and Vietnamese during the segmentation training process.

### Open Question 3
- **Question**: How can model architectures be optimized to handle the performance drop observed in lengthy passages and complex reasoning questions?
- **Basis in paper**: [inferred] The analysis section notes a performance decline with longer passages and complex types like "list" and "reasoning," concluding that "directions for model improvement" are needed for lengthy contexts and reasoning-focused architectures.
- **Why unresolved**: Current transformer baselines (mBERT, XLM-R) show degraded accuracy as input length increases, suggesting standard architectures are insufficient for the dataset's harder subsets.
- **What evidence would resolve it**: Testing long-context models (e.g., Longformer) or reasoning-enhanced architectures on the specific "reasoning" and "list" subsets of VSMRC to measure accuracy recovery.

## Limitations

- The dataset's synthetic nature, while validated, may introduce subtle biases in question difficulty distribution that persist despite human validation.
- The Wikipedia domain constraint limits generalizability to other Vietnamese text genres like social media, news, or technical documents.
- The text segmentation task formulation remains underspecified in the paper, affecting reproducibility.

## Confidence

**High Confidence** - The core finding that multilingual models (mBERT, XLM-R) significantly outperform monolingual Vietnamese models on both tasks is well-supported by experimental results (Table 7 shows mBERT achieving 63.15% F1 vs. PhoBERT's 36.51% on segmentation).

**Medium Confidence** - The mechanism explaining why English data improves Vietnamese segmentation is plausible but not definitively proven. While Table 8 shows improvement when English Wiki727k data is added, the paper doesn't establish whether this is due to shared structural patterns or other factors.

**Low Confidence** - The claim about predictable performance degradation with reasoning complexity and context length is partially supported but may be task-specific. The evidence comes from VSMRC's particular question distribution, and similar patterns may not hold across different MRC datasets or model architectures.

## Next Checks

1. **Cross-domain Transfer Evaluation**: Test both tasks on Vietnamese text from non-Wikipedia sources (news articles, social media posts, technical documents) to assess domain generalization and identify vocabulary/structural mismatches.

2. **Human vs. Synthetic Difficulty Alignment**: Conduct a controlled study where human experts rate the difficulty of 100 randomly sampled questions from each category (fact-check, reasoning, list) and compare their assessments with model performance gaps to validate whether synthetic complexity matches human perception.

3. **Zero-shot Multilingual Transfer**: Train models exclusively on English MRC and text segmentation data, then evaluate zero-shot performance on VSMRC to quantify the pure cross-lingual transfer capability without Vietnamese fine-tuning, isolating the contribution of shared structural patterns from domain-specific adaptation.