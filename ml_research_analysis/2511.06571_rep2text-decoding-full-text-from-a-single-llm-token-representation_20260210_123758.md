---
ver: rpa2
title: 'Rep2Text: Decoding Full Text from a Single LLM Token Representation'
arxiv_id: '2511.06571'
source_url: https://arxiv.org/abs/2511.06571
tags:
- token
- representation
- score
- decoding
- inversion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the question of how much of the original input
  text can be recovered from a single last-token representation in large language
  models (LLMs). The authors propose Rep2Text, a framework that uses a trainable adapter
  to project a target model's last-token representation into the embedding space of
  a decoding language model, which then autoregressively reconstructs the input text.
---

# Rep2Text: Decoding Full Text from a Single LLM Token Representation

## Quick Facts
- arXiv ID: 2511.06571
- Source URL: https://arxiv.org/abs/2511.06571
- Reference count: 40
- Single last-token representation can recover over half of 16-token sequence information on average

## Executive Summary
This paper demonstrates that a single last-token representation from large language models contains substantial recoverable information about the original input text. The authors propose Rep2Text, a framework using a trainable adapter to project representations into a decoding model's embedding space, which then autoregressively reconstructs the input. Experiments show that on average over half of the information in 16-token sequences can be recovered while maintaining strong semantic integrity. The study reveals an information bottleneck effect where longer sequences exhibit decreased token-level recovery but preserve semantic content.

## Method Summary
Rep2Text extracts last-token representations from a target LLM, then uses a two-layer MLP adapter with gated skip connection to project these into a decoding model's embedding space. The adapter projects the representation into k token embeddings (where k equals sequence length), prepends system and user prompts, and feeds into the frozen decoder for autoregressive reconstruction. Training uses teacher forcing with smoothed cross-entropy loss, label smoothing of 0.075, and can be done with adapter-only fine-tuning or joint fine-tuning with LoRA. The framework works across models (cross-model inversion) and shows optimal extraction layers vary by information type (L10 for structure, L15 for entities).

## Key Results
- Average ROUGE-1 score of ~0.48 for 16-token sequences, indicating over half of tokens can be recovered
- Cross-model inversion achieves comparable performance to same-model inversion, supporting the platonic representation hypothesis
- Information bottleneck effect: token-level metrics degrade aggressively (ROUGE-1 drops from ~0.6 to ~0.3) while semantic metrics remain stable when scaling from 8 to 64 tokens
- Strong generalization to out-of-distribution medical data with minimal performance degradation

## Why This Works (Mechanism)

### Mechanism 1: Representation Compression Preserves Semantics Through an Information Bottleneck
The last-token representation acts as a compression bottleneck where semantic information survives better than exact lexical details. As sequence length increases, the fixed-dimensional representation prioritizes high-level semantics (topic, entity relationships) over precise token sequences. Evidence shows topic scores remain within 20% when scaling from 8 to 64 tokens while ROUGE-1 drops from ~0.6 to ~0.3.

### Mechanism 2: Cross-Model Representation Alignment via Trainable Projection
A lightweight adapter learns to map representations from one LLM's latent space to another's embedding space, enabling cross-model decoding without shared architecture. The two-layer MLP with gated skip connection projects representations via normalization, expansion, and learned projection while preserving skip-path information. The adapter learns alignment while the decoding model remains frozen.

### Mechanism 3: Layer-Specific Information Distribution With Optimal Recovery Window
Different layers encode different information types—structural/syntactic information peaks in early-to-middle layers (L10), while semantic/entity information peaks slightly later (L15), with high-level topic stable across middle layers. Information propagates through transformer layers with progressive abstraction, where early layers retain surface form, middle layers consolidate lexical details, and later layers abstract to task-relevant semantics.

## Foundational Learning

- **Autoregressive Decoding with Cross-Entropy Loss**: The adapter training uses teacher forcing to maximize log-likelihood of ground-truth tokens, with label smoothing (ε=0.075) to prevent overconfidence. If your adapter overfits to training sequences, would you increase or decrease label smoothing?
- **Residual Stream Representations in Transformers**: Rep2Text extracts hℓ from the residual stream at a specific layer. The residual stream accumulates information through skip connections, meaning layer choice determines what "version" of processing you capture. Why might extracting from layer 30 yield worse token-level recovery than layer 10, despite being "more processed"?
- **Gated Skip Connections in MLPs**: The adapter uses gated combination (Xe = LN(Ws · hℓ + gk · h2)) to balance preserving original representation vs. learned transformation. This prevents the adapter from "washing out" important information. If the gate value gk approaches 0, what happens to the skip-connection contribution?

## Architecture Onboarding

- **Component map**: Target Model M → Last-token representation hℓ → Adapter (2-layer MLP + gated skip) → Projected token embeddings → [System/User prompts] → Decoding Model M′ → Reconstructed text
- **Critical path**: 1. Layer selection (L10 for structure, L15 for entities), 2. Projected token count k (set equal to ground-truth sequence length), 3. Adapter-only vs. joint training
- **Design tradeoffs**: Hidden expansion factor f: Paper uses 0.5; ablation shows minimal impact on semantics, slight token-level improvement when scaled up. Same-model vs. cross-model: Cross-model works, but Mistral-7B shows highest recoverability across decoders—target model choice matters more than decoder. Frozen vs. LoRA-tuned decoder: Frozen adapter-only training demonstrates alignment comes from adapter, not model overfitting.
- **Failure signatures**: ROUGE-1 < 0.2 with high topic score: Normal for long sequences (>32 tokens)—bottleneck compression. Medical-domain artifacts (e.g., "://www.medscape.com" prefixes): Domain misalignment. Degraded structure but preserved entities: May indicate extraction from suboptimal layer (too deep).
- **First 3 experiments**: 1. Layer sweep on your target model: Extract representations from L5, L10, L15, L20, L25; plot ROUGE-1 and structure score to identify optimal layer. 2. Sequence length scaling test: Train separate adapters for 8, 16, 32, 64 tokens; verify bottleneck effect manifests. 3. Cross-model probe: If using non-Llama target, test inversion with both same-architecture and Llama-3.1-8B decoder; compare to assess representation alignment quality.

## Open Questions the Paper Calls Out
- Does the token-level recovery rate plateau or degrade to zero for input sequences significantly longer than the tested 64 tokens (e.g., 512+ tokens)?
- Can specific training interventions or defense strategies effectively obfuscate input information from the last-token representation without compromising the model's next-token prediction capabilities?
- Is the observed information bottleneck a fundamental limit of the single-vector representation or a failure of the MLP adapter's capacity to decompress the data?

## Limitations
- Domain generalization tested on GPT-generated synthetic medical notes rather than real clinical records, limiting ecological validity
- Layer selection (L10 for structure, L15 for entities) may not generalize across domains, model architectures, or fine-tuned variants
- Heavy reliance on LLM-as-a-judge metrics introduces potential circularity and model-specific biases

## Confidence

**High confidence**: The core technical approach (adapter-based cross-model representation projection) is well-specified and reproducible. The information bottleneck effect across sequence lengths is clearly demonstrated with consistent metric patterns.

**Medium confidence**: The semantic preservation claims are supported but rely on LLM-as-a-judge metrics that may introduce biases. The cross-model generalization to medical data shows promise but uses synthetic rather than real clinical text.

**Low confidence**: The interpretation that semantic information is "prioritized" over lexical details during compression is plausible but not definitively proven. The optimal layer selection may not generalize beyond the studied domain and model.

## Next Checks

1. **Real-world domain generalization test**: Evaluate Rep2Text on actual clinical notes from electronic health record systems (e.g., MIMIC-III) rather than GPT-generated synthetic data. Measure performance degradation and characterize the specific types of medical terminology and context that are lost or preserved.

2. **Layer transferability experiment**: Train adapters using representations from layers L5, L10, L15, L20 on Wikipedia data, then test all adapters on a completely different domain (e.g., legal documents or scientific papers). Plot performance across both layer and domain to quantify how much optimal layer choice depends on target domain versus general architecture.

3. **Information type ablation study**: For sequence lengths 16 and 64 tokens, systematically vary token types in the input (e.g., common words vs. rare technical terms, simple vs. complex syntax, factual statements vs. opinions) and measure which information types show the most/least recovery across the information bottleneck. This would validate whether semantic content truly survives better than lexical details.