---
ver: rpa2
title: 'Towards Nepali-language LLMs: Efficient GPT training with a Nepali BPE tokenizer'
arxiv_id: '2512.14585'
source_url: https://arxiv.org/abs/2512.14585
tags:
- nepali
- language
- training
- text
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addressed the lack of effective generative NLP models
  for Nepali, a low-resource language with complex grammar and limited high-quality
  corpora. A GPT-2-based language model was developed using a custom 16k Byte-Pair
  Encoding (BPE) tokenizer trained exclusively on Nepali text to improve segmentation
  and input representation.
---

# Towards Nepali-language LLMs: Efficient GPT training with a Nepali BPE tokenizer

## Quick Facts
- arXiv ID: 2512.14585
- Source URL: https://arxiv.org/abs/2512.14585
- Reference count: 21
- Primary result: GPT-2-based model trained on 10.75GB Nepali corpus achieved perplexity of 21.80

## Executive Summary
This study addresses the lack of effective generative NLP models for Nepali, a low-resource language with complex grammar and limited high-quality corpora. The researchers developed a GPT-2-based language model using a custom 16k Byte-Pair Encoding (BPE) tokenizer trained exclusively on Nepali text to improve segmentation and input representation. The model was pretrained on a 10.75GB cleaned corpus combining NepBERTa and additional web-scraped Nepali news articles, with FlashAttention integrated to enhance memory efficiency and training stability. After two epochs, the model achieved a training loss of 3.168177, a validation loss of 3.081982, and a final perplexity of 21.80, demonstrating its ability to generate coherent Nepali news-style text.

## Method Summary
The researchers developed a custom 16k BPE tokenizer trained exclusively on Nepali text to improve segmentation and input representation. The model was pretrained on a 10.75GB cleaned corpus combining NepBERTa and additional web-scraped Nepali news articles. FlashAttention was integrated to enhance memory efficiency and training stability during the two-epoch training process.

## Key Results
- Achieved training loss of 3.168177 and validation loss of 3.081982
- Final perplexity of 21.80 after two epochs of training
- Successfully generated coherent Nepali news-style text

## Why This Works (Mechanism)
The custom BPE tokenizer trained on Nepali text improves segmentation and input representation by capturing language-specific patterns and reducing out-of-vocabulary issues common in low-resource languages.

## Foundational Learning

Byte-Pair Encoding (BPE): A tokenization method that iteratively merges frequent character pairs to create subword units
- Why needed: Handles out-of-vocabulary words and reduces vocabulary size in low-resource languages
- Quick check: Count unique tokens before and after BPE tokenization

FlashAttention: A memory-efficient implementation of the attention mechanism that reduces memory footprint and speeds up training
- Why needed: Enables training larger models on limited hardware resources
- Quick check: Compare memory usage with and without FlashAttention

Perplexity: A measurement of how well a probability model predicts a sample
- Why needed: Standard metric for evaluating language model performance
- Quick check: Calculate perplexity on held-out validation set

## Architecture Onboarding

Component Map: Dataset (Nepali news corpus) -> BPE Tokenizer (16k vocab) -> GPT-2 Architecture -> FlashAttention -> Training (2 epochs)

Critical Path: The model's ability to generate coherent text depends on the quality of the Nepali-specific BPE tokenization, the size and cleanliness of the training corpus, and the stability provided by FlashAttention during training.

Design Tradeoffs: Using a custom BPE tokenizer versus standard tokenizers trades development complexity for potentially better language-specific representation. The 16k vocabulary size balances coverage with computational efficiency.

Failure Signatures: High perplexity scores would indicate poor language modeling. Training instability or divergence would suggest issues with corpus quality or FlashAttention implementation.

First Experiments:
1. Test tokenizer on Nepali text samples to verify proper subword segmentation
2. Evaluate perplexity on a held-out validation set after each training epoch
3. Generate sample text outputs and assess coherence qualitatively

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation methodology lacking comprehensive qualitative and quantitative assessment of generated text quality
- Potential biases in web-scraped news articles not addressed
- Absence of downstream task performance evaluation

## Confidence

Confidence in core findings: Medium - methodology appears sound but comprehensive evaluation is lacking
Confidence in broader applicability: Low - limited testing scope restricts generalizability claims

## Next Checks
1. Conduct human evaluation studies with native Nepali speakers to assess text coherence, fluency, and news-style appropriateness across multiple generated samples
2. Benchmark the model against existing Nepali language models like NepBERTa on downstream NLP tasks such as text classification, sentiment analysis, and named entity recognition
3. Perform ablation studies to quantify the impact of the custom BPE tokenizer versus standard tokenizers and to evaluate different corpus sizes on model performance