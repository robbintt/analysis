---
ver: rpa2
title: 'Entropy-Lens: Uncovering Decision Strategies in LLMs'
arxiv_id: '2502.16570'
source_url: https://arxiv.org/abs/2502.16570
tags:
- entropy
- profiles
- token
- across
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Entropy-Lens addresses the challenge of analyzing token-space dynamics
  in large language models (LLMs), where high dimensionality and the lack of intrinsic
  order in token distributions hinder standard statistical descriptors. The method
  introduces entropy of logit-lens predictions as a per-layer, permutation-invariant
  scalar metric to overcome these limitations.
---

# Entropy-Lens: Uncovering Decision Strategies in LLMs

## Quick Facts
- arXiv ID: 2502.16570
- Source URL: https://arxiv.org/abs/2502.16570
- Authors: Riccardo Ali; Francesco Caso; Christopher Irwin; Pietro Liò
- Reference count: 40
- Primary result: Entropy-Lens achieves >90% kNN AUC for classifying model families, tasks, and output formats by analyzing token-space dynamics via per-layer entropy profiles.

## Executive Summary
Entropy-Lens addresses the challenge of analyzing token-space dynamics in large language models (LLMs), where high dimensionality and the lack of intrinsic order in token distributions hinder standard statistical descriptors. The method introduces entropy of logit-lens predictions as a per-layer, permutation-invariant scalar metric to overcome these limitations. By computing the entropy of intermediate next-token predictions, Entropy-Lens captures the model's expansion and pruning strategies, providing a low-dimensional signal termed the "entropy profile."

Key results demonstrate that entropy profiles are family-specific and invariant under depth rescaling, indicating consistent token prediction dynamics across different model sizes within the same family. These profiles also exhibit task- and output-format-dependent characteristics, enabling reliable discrimination between different types of tasks and output formats. Furthermore, intervention experiments reveal that expansion and pruning strategies have unequal impact on downstream performance, with expansion generally being more critical. The method achieves high classification accuracy (e.g., kNN AUC > 90%) in identifying model families, tasks, and output formats, validating its effectiveness as a tool for analyzing LLM behavior.

## Method Summary
Entropy-Lens computes the entropy of logit-lens predictions at each layer to uncover expansion and pruning strategies in LLMs. It uses decoder-only models (GPT-2, Gemma-2, Llama-3.2, Qwen3) ranging from 100M to 9B parameters. The method extracts residual-stream activations via logit-lens, applies softmax to obtain per-layer next-token distributions, and computes Shannon or Rényi entropy. Profiles are aggregated across tokens and used for kNN classification or intervention experiments (layer-skipping). Classification uses kNN (k=3 or 11, Euclidean) with 10-fold cross-validation, while interventions skip top-k layers by zeroing their residual-stream contribution.

## Key Results
- Entropy profiles are family-specific and invariant under depth rescaling, indicating consistent token prediction dynamics across model sizes.
- Profiles exhibit task- and output-format-dependent characteristics, enabling reliable discrimination between different tasks and output formats.
- Intervention experiments show expansion and pruning strategies have unequal impact on downstream performance, with expansion generally being more critical.

## Why This Works (Mechanism)
Entropy-Lens works by providing a low-dimensional, permutation-invariant scalar metric (entropy) that captures the token-space dynamics of LLMs at each layer. By computing the entropy of next-token predictions from intermediate activations, it reveals the model's expansion and pruning strategies, which are otherwise obscured by the high dimensionality and lack of intrinsic order in token distributions. The entropy profile serves as a compressed representation of these dynamics, enabling reliable classification of model families, tasks, and output formats.

## Foundational Learning
- **Logit-lens**: A method to extract intermediate next-token predictions by applying the model's final LayerNorm and lm_head to hidden states at each layer. Why needed: Enables per-layer analysis of token predictions without modifying the model. Quick check: Verify that extracted logits match the model's final output for the last token.
- **Shannon Entropy**: A measure of uncertainty in a probability distribution, computed as -Σ p_i log(p_i). Why needed: Quantifies the spread of the next-token distribution, capturing expansion/pruning dynamics. Quick check: Entropy should be 0 for deterministic distributions and log(V) for uniform distributions (V = vocab size).
- **k-Nearest Neighbors (kNN)**: A non-parametric classification method that assigns labels based on the majority class among k nearest neighbors in feature space. Why needed: Provides a simple, interpretable way to classify entropy profiles. Quick check: kNN should achieve high AUC (>90%) on labeled profile datasets.
- **Rényi Entropy**: A generalization of Shannon entropy parameterized by α, computed as (1/(1-α)) log(Σ p_i^α). Why needed: Offers flexibility in emphasizing different aspects of the distribution (e.g., tails vs. peaks). Quick check: Rényi entropy should reduce to Shannon entropy as α → 1.
- **Layer-skipping Intervention**: A method to assess the importance of specific layers by zeroing their residual-stream contribution during inference. Why needed: Reveals the differential impact of expansion and pruning strategies on downstream performance. Quick check: Skipping layers with high entropy change should degrade performance more than random skipping.

## Architecture Onboarding
- **Component Map**: Model (Decoder-only LLM) -> Logit-lens (Extract per-layer predictions) -> Softmax (Convert to probabilities) -> Entropy (Compute per-layer entropy) -> Profile (Aggregate across tokens) -> kNN (Classify) or Intervention (Assess layer importance).
- **Critical Path**: Logit-lens extraction → Softmax → Entropy computation → Profile aggregation → kNN classification/intervention.
- **Design Tradeoffs**: Uses entropy as a scalar metric to compress high-dimensional token dynamics, trading off detailed distributional information for interpretability and low dimensionality. The choice of k in kNN balances bias and variance in classification.
- **Failure Signatures**: Noisy or inconsistent entropy profiles may indicate incorrect logit-lens extraction, improper softmax application, or insufficient token aggregation. Low kNN AUC could result from label leakage, imbalanced classes, or inadequate preprocessing.
- **First Experiments**: 1) Extract and visualize entropy profiles for a blank prompt across GPT-2 small, medium, large, and xl to confirm family-specific patterns. 2) Compute kNN AUC for task classification on the TinyStories dataset. 3) Perform layer-skipping intervention on MMLU to compare accuracy degradation for top-k vs. random layers.

## Open Questions the Paper Calls Out
None

## Limitations
- Entropy profiles may be sensitive to random seeds and tokenization choices, potentially affecting reproducibility.
- The observed invariance under depth rescaling may be an artifact of interpolation/alignment procedures rather than a true property of the underlying dynamics.
- The choice of α in Rényi entropy and the preprocessing steps for kNN classification (e.g., normalization, interpolation) are not fully specified, raising concerns about reproducibility.

## Confidence
- High: Entropy metric is well-defined and qualitative patterns (family-specific profiles, task/format dependence) are plausible.
- Medium: Quantitative claims (e.g., >90% AUC) require exact replication of preprocessing steps; intervention results are suggestive but preliminary.
- Low: Unknown impact of entropy profile robustness across different random seeds and tokenization choices.

## Next Checks
1. Reproduce the entropy profile extraction and kNN classification pipeline on a held-out set of models within the same family to confirm invariance and classification accuracy.
2. Perform ablation studies on the choice of entropy measure (Shannon vs Rényi with varying α) and aggregation method (concatenation vs averaging) to assess robustness.
3. Extend intervention experiments beyond layer-skipping (e.g., targeted layer modifications or attention head ablation) to validate the differential importance of expansion vs pruning strategies.