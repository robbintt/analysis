---
ver: rpa2
title: 'Expediting data extraction using a large language model (LLM) and scoping
  review protocol: a methodological study within a complex scoping review'
arxiv_id: '2507.06623'
source_url: https://arxiv.org/abs/2507.06623
tags:
- data
- extraction
- review
- source
- protocol
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study tested whether a scoping review protocol could expedite\
  \ data extraction using a large language model (LLM). Two LLM approaches\u2014one\
  \ with an extended protocol and one with a simple protocol\u2014were compared against\
  \ human baseline extraction from 10 evidence sources."
---

# Expediting data extraction using a large language model (LLM) and scoping review protocol: a methodological study within a complex scoping review

## Quick Facts
- **arXiv ID**: 2507.06623
- **Source URL**: https://arxiv.org/abs/2507.06623
- **Reference count**: 40
- **Primary result**: Protocol-based LLM extraction works well for simple citation details but poorly for complex subjective data items

## Executive Summary
This methodological study tested whether a scoping review protocol could improve large language model (LLM) performance for data extraction tasks. The researchers compared two LLM approaches (one using an extended protocol and one using a simple protocol) against human baseline extraction across 10 evidence sources. While LLM accuracy was high for straightforward citation details (83.3%-100%), performance dropped significantly for complex, subjective data items (9.6%-15.8%). Precision exceeded 90% overall, but recall remained below 25%, indicating frequent omissions and misattributions. The study suggests LLM-assisted extraction may help with simple tasks but is unreliable for complex data items and review verification.

## Method Summary
The study employed a comparative design testing LLM data extraction against human baseline extraction. Two LLM approaches were evaluated: one with an extended scoping review protocol and one with a simple protocol. Both methods were applied to the same 10 evidence sources used for human extraction. Citation detail extraction accuracy was measured alongside more complex data items. The researchers also tested LLM review of human-extracted data for error detection, deliberately introducing errors to assess detection rates.

## Key Results
- Citation detail extraction accuracy was high (83.3%-100%) for both LLM methods
- Complex data item extraction accuracy was very low (9.6%-15.8%)
- Precision exceeded 90% but recall remained below 25% for LLM-extracted data
- LLM review of human-extracted data detected only 5% of deliberately introduced errors

## Why This Works (Mechanism)
The study demonstrates that LLMs can process structured protocols effectively for straightforward tasks but struggle with subjective, complex data requiring nuanced judgment. The high precision indicates LLMs can accurately identify what they do extract, while low recall reveals systematic difficulties with completeness and attribution. The protocol structure provides helpful scaffolding for simple tasks but cannot compensate for the LLM's limitations in handling subjective complexity.

## Foundational Learning
- **Scoping review methodology**: Systematic approach to map evidence across broad research areas - needed to understand the review context and protocol structure
- **Data extraction protocols**: Standardized procedures for extracting specific information from sources - needed to evaluate how protocol structure affects LLM performance
- **Citation detail extraction**: Basic bibliographic information retrieval - needed as a baseline for assessing LLM accuracy on simple tasks
- **Complex data item extraction**: Subjective or interpretive information extraction - needed to identify LLM limitations
- **Precision vs recall metrics**: Complementary measures of extraction performance - needed to understand the nature of LLM failures
- **LLM prompt engineering**: Techniques for optimizing model responses - needed to assess protocol effectiveness

## Architecture Onboarding
- **Component map**: Human baseline extraction -> LLM simple protocol extraction -> LLM extended protocol extraction -> LLM review of human extraction
- **Critical path**: Protocol design -> LLM prompting -> Data extraction -> Accuracy measurement -> Error detection assessment
- **Design tradeoffs**: Protocol complexity vs extraction accuracy - more detailed protocols didn't improve complex data extraction
- **Failure signatures**: High precision but low recall indicates systematic omissions and misattributions
- **First experiments**: (1) Test protocol effectiveness on additional simple data types, (2) Evaluate different prompt engineering approaches for complex items, (3) Compare multiple LLM models on the same extraction tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of 10 evidence sources limits generalizability
- Simple citation details may not represent typical scoping review challenges
- Systematic issues with completeness and attribution not fully explored
- Artificial error introduction may not reflect real-world review conditions

## Confidence
- High confidence in accuracy values for tested sources
- Medium confidence in core finding about LLM utility for simple vs complex tasks
- Low confidence in specific error detection rates due to artificial error introduction

## Next Checks
1. Replicate with larger, more diverse set of scoping review sources across multiple disciplines
2. Test additional LLM models and prompt engineering approaches to improve recall
3. Conduct formal validation study comparing LLM-assisted vs human-only extraction across multiple review teams