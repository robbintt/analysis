---
ver: rpa2
title: Regression Language Models for Code
arxiv_id: '2509.26476'
source_url: https://arxiv.org/abs/2509.26476
tags:
- features
- regression
- code
- language
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Regression Language Model (RLM) for predicting
  numeric outcomes from code and computational graphs. The core method treats regression
  as a text-to-text next-token prediction problem using a small T5Gemma-based encoder-decoder.
---

# Regression Language Models for Code

## Quick Facts
- arXiv ID: 2509.26476
- Source URL: https://arxiv.org/abs/2509.26476
- Reference count: 40
- Primary result: A unified text-to-text regression framework using T5Gemma achieves >0.9 Spearman correlation on APPS, >0.5 on CodeNet, and >0.46 Kendall-Tau on NAS benchmarks without feature engineering.

## Executive Summary
This paper introduces a Regression Language Model (RLM) that treats numerical code metric prediction as a text-to-text next-token prediction problem. Using a small T5Gemma-based encoder-decoder architecture, the RLM can simultaneously predict multiple performance metrics (memory, accuracy, latency) from code and computational graphs across diverse domains. The approach unifies regression tasks under a single next-token prediction framework, eliminating the need for specialized architectures or feature engineering. The model demonstrates strong empirical results across competitive programming submissions, multiple programming languages, and neural architecture search benchmarks.

## Method Summary
The RLM reframes regression as a text-to-text problem by encoding code or computational graphs as text and decoding numeric predictions as next tokens. The core architecture uses a T5Gemma encoder-decoder, where input code is tokenized and passed through the encoder, then the decoder generates predictions in a sequence format. The model is trained on synthetic data generated from code transformations and computational graph operations, allowing it to learn the mapping between code structure and numeric outcomes. Multi-objective predictions are handled naturally by concatenating multiple target values in the output sequence. The approach avoids explicit feature engineering by relying on the model's ability to extract relevant patterns from raw text representations.

## Key Results
- Achieves >0.9 Spearman rank correlation on competitive programming submissions from APPS dataset
- Demonstrates >0.5 performance on 17 different programming languages from CodeNet
- Shows >0.46 Kendall-Tau correlation on neural architecture search benchmarks while handling multi-objective predictions simultaneously

## Why This Works (Mechanism)
The RLM's effectiveness stems from leveraging the transformer architecture's pattern recognition capabilities on text representations of code and computational graphs. By treating regression as next-token prediction, the model can utilize established pretraining techniques and scale with model size. The text-to-text formulation allows the same architecture to handle diverse regression tasks without task-specific modifications. The decoder's autoregressive nature enables natural handling of multi-objective predictions by generating sequences of numeric values. Synthetic data generation provides sufficient training examples while avoiding the need for extensive labeled datasets.

## Foundational Learning
- **Text-to-text regression framing**: Converting numerical prediction into a sequence generation problem allows leveraging powerful language model architectures. Needed to unify diverse regression tasks under a single framework. Quick check: Verify that the decoder can generate valid numeric sequences across different scales and formats.
- **Synthetic data generation**: Creating training examples through code transformations and graph operations eliminates dependency on labeled datasets. Needed to scale training while maintaining diversity. Quick check: Ensure synthetic examples cover the full range of expected input patterns and output values.
- **Multi-objective prediction handling**: Concatenating multiple target values in output sequence enables simultaneous prediction of related metrics. Needed to reduce inference overhead and capture metric correlations. Quick check: Validate that predicted values maintain logical relationships (e.g., higher accuracy shouldn't predict lower latency without justification).

## Architecture Onboarding

**Component Map**
T5Gemma encoder -> attention layers -> decoder -> next-token prediction output

**Critical Path**
Code text input → tokenizer → encoder → cross-attention → decoder → numeric predictions

**Design Tradeoffs**
- Encoder-decoder vs. decoder-only: Encoder-decoder provides better input understanding for complex code structures
- Synthetic vs. real data: Synthetic data enables scale but may miss edge cases from real-world scenarios
- Single model vs. specialized models: Single model simplifies deployment but may underperform on specific tasks

**Failure Signatures**
- Poor performance on out-of-distribution code patterns
- Inconsistent predictions across semantically similar inputs
- Degraded accuracy when predicting extreme values or rare code patterns

**3 First Experiments**
1. Measure correlation decay as input code length increases beyond training distribution
2. Compare performance between synthetic and real-world labeled data on same tasks
3. Test multi-objective prediction stability when objectives have conflicting optimal solutions

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several areas warrant further investigation based on the results and methodology presented.

## Limitations
- Performance may degrade on code patterns significantly different from training distribution due to reliance on synthetic data generation
- Limited comparison to other modern LLM architectures beyond T5 variants, potentially missing optimization opportunities
- Real-world utility and impact on actual code optimization decisions not thoroughly validated through deployment case studies

## Confidence

**High confidence**: The core methodology of treating regression as next-token prediction is sound and well-implemented. The multi-objective prediction capability is validated across diverse domains.

**Medium confidence**: The scaling results and pretraining benefits, while promising, are based on a limited comparison set. The synthetic data generation approach, though effective, may introduce domain-specific biases.

**Low confidence**: The practical utility of the predictions in real-world optimization scenarios and the model's robustness to out-of-distribution code are not thoroughly explored.

## Next Checks

1. Conduct ablation studies comparing different pretraining strategies and synthetic data generation approaches to quantify their individual contributions to performance.

2. Evaluate the model's performance on out-of-distribution code samples and analyze failure modes when predictions deviate significantly from ground truth.

3. Implement a real-world case study where the model's predictions are used to guide actual code optimization decisions, measuring the impact on downstream metrics like execution time or resource usage.