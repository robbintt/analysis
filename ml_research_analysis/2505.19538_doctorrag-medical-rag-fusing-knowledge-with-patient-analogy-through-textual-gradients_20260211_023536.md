---
ver: rpa2
title: 'DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual
  Gradients'
arxiv_id: '2505.19538'
source_url: https://arxiv.org/abs/2505.19538
tags:
- answer
- patient
- medical
- prompt
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DoctorRAG integrates explicit medical knowledge with patient case
  experience through dual retrieval and Med-TextGrad iterative refinement, outperforming
  strong RAG baselines with up to 98.27% accuracy on disease diagnosis and 31.98 Rouge-L
  on text generation.
---

# DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients

## Quick Facts
- **arXiv ID:** 2505.19538
- **Source URL:** https://arxiv.org/abs/2505.19538
- **Reference count:** 40
- **Primary result:** 98.27% accuracy on disease diagnosis and 31.98 Rouge-L on text generation

## Executive Summary
DoctorRAG is a medical RAG system that integrates explicit medical knowledge with patient case experience through dual retrieval and iterative refinement. It combines concept-constrained retrieval from a Knowledge Base with patient analogy retrieval from a Patient Base, then applies a Med-TextGrad module using multi-agent textual gradients to refine answers. The system achieves state-of-the-art performance on medical benchmarks, outperforming strong RAG baselines with up to 98.27% accuracy on disease diagnosis and 31.98 Rouge-L on text generation.

## Method Summary
DoctorRAG uses a dual-retrieval system that queries both a Knowledge Base (KB) for explicit medical guidelines and a Patient Base (PB) for experiential case data. The KB is pre-processed into atomic declarative statements annotated with medical concepts (e.g., ICD-10 codes), and retrieval uses concept-constrained filtering. The system then employs a multi-agent iterative refinement process where textual gradients from critique agents update the generation prompt via Textual Gradient Descent, ensuring answers adhere to both retrieved knowledge and patient queries.

## Key Results
- Achieves 98.27% accuracy on disease diagnosis benchmarks
- Reaches 31.98 Rouge-L score on medical text generation tasks
- Outperforms strong RAG baselines through integration of case-based patient analogies with explicit medical knowledge

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating case-based patient analogies with explicit medical knowledge improves diagnostic accuracy and text generation relevance.
- **Mechanism:** DoctorRAG employs a dual-retrieval system that queries both a Knowledge Base (KB) for explicit guidelines and a Patient Base (PB) for experiential case data. It uses a concept-constrained similarity score: for KB retrieval, it first filters for entries sharing medical concept tags (e.g., ICD-10 codes) with the query, then ranks by cosine similarity. The final context is a concatenation of top-k results from both sources, grounding generation in both expertise and experience.
- **Core assumption:** The embedding space meaningfully captures medical semantics, and similar patient presentations correlate with similar clinical outcomes (case-based reasoning).
- **Evidence anchors:**
  - [abstract] "DoctorRAG enhances retrieval precision by first allocating conceptual tags... together with a hybrid retrieval mechanism from both relevant knowledge and patient."
  - [section 2.1] Equations (1) and (3) define the concept-constrained retrieval score and context aggregation.
  - [corpus] "Experience Retrieval-Augmentation with Electronic Health Records Enables Accurate Discharge QA" (arXiv:2503.17933) supports this premise, stating that "clinical case-based knowledge is also critical for effective medic...".
- **Break condition:** The mechanism underperforms if the Patient Base is sparse, noisy, or if the embedding model fails to distinguish fine-grained clinical nuances, yielding irrelevant analogies.

### Mechanism 2
- **Claim:** Iterative refinement using multi-agent textual gradients enhances answer faithfulness to context and relevance to the patient query.
- **Mechanism:** The generated answer is modeled as a variable in a computation graph. Two separate LLM agents—a Context Criterion and a Patient Criterion—produce textual critiques (a proxy for loss). Other agents then compute "textual gradients," which are natural language instructions for improvement. These gradients update the generation prompt via a Textual Gradient Descent (TGD) step, iteratively refining the output.
- **Core assumption:** LLMs can reliably critique their own outputs and generate actionable, non-contradictory feedback that leads to iterative quality improvement.
- **Evidence anchors:**
  - [abstract] "...a Med-TextGrad module using multi-agent textual gradients is integrated to ensure that the final output adheres to the retrieved knowledge and patient query."
  - [section 2.2] Equation (8) defines the textual loss L(A) as the sum of critiques. Equation (14) defines the prompt update TGD step.
  - [corpus] External evidence for this specific mechanism is weak. While agentic workflows exist (e.g., MedAgent-Pro), the direct application of a "textual gradient descent" framework is a primary contribution of this paper.
- **Break condition:** The mechanism fails if textual gradients are circular or unhelpful, or if the process "overfits," causing answers to become verbose or drift from the core intent. The paper notes performance can plateau after 2 iterations.

### Mechanism 3
- **Claim:** Structuring knowledge as declarative statements annotated with medical concepts improves retrieval precision over raw text chunks.
- **Mechanism:** Raw medical text is pre-processed by an LLM into atomic, self-contained declarative statements. Each statement is then annotated with a controlled vocabulary of medical concepts (e.g., first-level ICD-10 codes). Retrieval is filtered to only consider knowledge entries whose concept tags intersect with the query's tags, ensuring domain alignment before semantic ranking.
- **Core assumption:** Atomic declarative statements and broad concept tags effectively disambiguate medical knowledge, overcoming the limitations of pure vector similarity.
- **Evidence anchors:**
  - [section 2.1] "Each sentence di is then annotated with a set of medical concept identifiers ci = Tagϕ(di)... sK(q, di) = ... if cq ∩ ci ≠ ∅, -∞ otherwise" (paraphrased from Eq. 1).
  - [corpus] This pre-processing combination is a key differentiator. Related medical RAGs focus on knowledge graphs or direct chunking but do not explicitly detail this declarative + concept-tagging approach.
- **Break condition:** The mechanism is brittle if the tagging agent misclassifies concepts, if the controlled vocabulary is too coarse, or if relevant knowledge is lost during the declarative transformation.

## Foundational Learning

- **Concept:** **Textual Gradient Descent (TextGrad)**.
  - **Why needed here:** This is the core logic of the Med-TextGrad module, which differentiates the system from standard single-pass RAG.
  - **Quick check question:** In this framework, what does the "loss function" for an answer represent? (Answer: The aggregated textual critiques from the Context and Patient Criterion agents).

- **Concept:** **Dual Retrieval**.
  - **Why needed here:** This architecture is essential for fusing "expertise" (from the KB) with "experience" (from the PB), mimicking clinical reasoning.
  - **Quick check question:** What are the two primary data sources queried during the retrieval stage? (Answer: The Knowledge Base (K) and the Patient Base (P)).

- **Concept:** **Declarative Statement Transformation**.
  - **Why needed here:** This pre-processing step is critical for structuring the Knowledge Base to enable more precise concept tagging and retrieval.
  - **Quick check question:** Why are medical text chunks converted into declarative sentences? (Answer: To standardize knowledge into a direct factual format for better processing and tagging).

## Architecture Onboarding

- **Component map:** Pre-processing Pipeline -> Retrieval Module -> Med-TextGrad Loop (Generator -> Criterion Agents -> Gradient Agents -> Optimizer)
- **Critical path:** The retrieval module's concept filtering and the Med-TextGrad loop's prompt update step are the most critical for system performance. A failure in concept tagging will starve the generator of relevant context, while poor textual gradients will cause the refinement loop to diverge or plateau.
- **Design tradeoffs:**
  - **Precision vs. Recall in Retrieval:** Using strict concept filtering (intersection must be non-empty) increases precision but may miss relevant information if tagging is imperfect.
  - **Quality vs. Latency/Cost:** The Med-TextGrad loop involves multiple LLM calls per iteration (generator, criteria, gradients, optimizer), significantly increasing inference cost and latency. The paper shows gains can diminish after 2 iterations.
  - **Generality vs. Specificity:** Using only first-level ICD-10 codes for tagging simplifies matching but may lose fine-grained diagnostic specificity.
- **Failure signatures:**
  - **Retrieval Failure:** System answers are generic or factually incorrect because the concept tagger failed, and no KB entries passed the filter.
  - **Refinement Divergence:** Iterative answers become verbose, repetitive, or drift away from the user's question due to conflicting or low-quality textual gradients.
  - **Context Overload:** Aggregating too many entries from K and P creates a context window that exceeds the LLM's effective attention, degrading output quality.
- **First 3 experiments:**
  1. **Ablation on Retrieval:** Run DoctorRAG with and without the Patient Base retrieval to quantify the contribution of experiential "case-based" knowledge (replicating the paper's ablation study).
  2. **Ablation on Refinement:** Run the Med-TextGrad loop for a fixed number of iterations (e.g., 0, 1, 2, 3) and measure performance changes to find the optimal trade-off between quality gain and computational cost.
  3. **Tagging Sensitivity Analysis:** Evaluate the impact of the concept tagging granularity by comparing performance using first-level ICD-10 codes versus a more granular subset to test the precision-recall trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal stopping criterion or number of iterations for Med-TextGrad to prevent performance degradation or "over-optimization"?
- Basis in paper: [explicit] The authors note that "excessive iterations might not always yield better results," observing that gains from iteration T2 to T3 were "less distinct" and sometimes declined (Section 4.3).
- Why unresolved: The paper relies on a predetermined iteration count (T=3) and identifies the plateau/overfitting risk but does not propose or validate an automated mechanism to detect convergence.
- What evidence would resolve it: The development of an automated "early stopping" heuristic based on the stability of textual gradients or critique scores that correlates with human preference.

### Open Question 2
- Question: Can robust automated metrics be developed to replace resource-intensive pairwise LLM voting for evaluating the iterative refinement process?
- Basis in paper: [explicit] The Discussion states that "Future work will focus on developing and incorporating a more diverse set of automated metrics... [as] this method can be resource-intensive" (Appendix C.2).
- Why unresolved: The current evaluation relies heavily on LLM-based voting with human verification, which presents a bottleneck for large-scale experimentation and validation.
- What evidence would resolve it: The validation of a new automated metric (e.g., using Natural Language Inference) that shows strong correlation with human expert judgments on the dimensions of comprehensiveness, relevance, and safety.

### Open Question 3
- Question: How can the retrieval mechanism be optimized to minimize token consumption while maintaining the accuracy gains observed with larger context windows ($k$)?
- Basis in paper: [explicit] The token analysis highlights a "distinct trade-off" where increasing the context variable $k$ enhances performance but "leads to a linear increase of tokens processed" (Section 4.5, Figure 5).
- Why unresolved: The paper identifies the performance saturation point ($k>4$) but does not explore methods to compress the prompt or dynamically filter retrieved chunks to reduce computational cost.
- What evidence would resolve it: Experiments demonstrating that context compression or dynamic $k$-selection techniques can maintain high diagnostic accuracy (e.g., >98% on DDXPlus) with significantly lower token usage.

## Limitations
- The iterative refinement mechanism lacks strong external validation and may degrade after 2-3 iterations
- Heavy dependency on proprietary models (DeepSeek-V3, GPT-4.1-mini) raises reproducibility concerns
- Concept tagging system could fail silently if ICD-10 assignment is inaccurate, leading to knowledge retrieval failures

## Confidence

- **High Confidence:** The dual retrieval architecture (Knowledge Base + Patient Base) and its implementation using concept-constrained filtering is well-specified and grounded in established RAG principles. The reported benchmark improvements (up to 98.27% accuracy) are internally consistent within the experimental design.
- **Medium Confidence:** The declarative statement transformation with medical concept tagging is a reasonable approach, but its effectiveness depends heavily on the quality of the tagging LLM and the granularity of the ICD-10 codes used. The paper provides limited evidence of tagging accuracy or error rates.
- **Low Confidence:** The Med-TextGrad iterative refinement mechanism, while innovative, lacks external validation and relies on LLMs' ability to generate meaningful textual gradients—a capability that remains poorly understood and may be inconsistent across different model versions or prompts.

## Next Checks

1. **Ablation on Tagging Granularity:** Systematically evaluate performance using different levels of ICD-10 code specificity (first-level vs. more granular codes) to quantify the precision-recall trade-off and identify optimal tagging granularity for the target medical tasks.

2. **Cross-Model Validation of Textual Gradients:** Reproduce the Med-TextGrad results using alternative LLM pairs (e.g., GPT-4o, Claude 3.5) for the criterion and optimizer agents to test the robustness of the textual gradient mechanism across different model architectures.

3. **Error Analysis on Concept Filtering Failures:** Conduct a detailed error analysis of retrieval failures by logging instances where concept filtering yields zero results, examining whether failures stem from tagging errors, vocabulary mismatches, or genuinely absent knowledge.