---
ver: rpa2
title: 'The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies'
arxiv_id: '2507.02152'
source_url: https://arxiv.org/abs/2507.02152
tags:
- data
- bias
- fairness
- audit
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how data from audit studies can be used
  to improve both the training and evaluation of automated hiring algorithms. The
  authors identify that traditional fairness interventions, such as equalizing base
  rates across protected classes, can create an illusion of fairness when evaluated
  on biased labels.
---

# The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies

## Quick Facts
- arXiv ID: 2507.02152
- Source URL: https://arxiv.org/abs/2507.02152
- Authors: Disa Sariola; Patrick Button; Aron Culotta; Nicholas Mattei
- Reference count: 15
- Primary result: Traditional fairness interventions can create illusion of fairness when evaluated on biased labels

## Executive Summary
This paper investigates how data from audit studies can be used to improve both the training and evaluation of automated hiring algorithms. The authors identify that traditional fairness interventions, such as equalizing base rates across protected classes, can create an illusion of fairness when evaluated on biased labels. Using a large-scale audit study dataset on age discrimination in hiring, they introduce an Individual Treatment Effect (ITE)-based approach to repair label bias in the data. Their method iteratively flips labels most likely influenced by discrimination until callback rates are equal across age groups. Results show that traditional approaches appear fair when evaluated using standard methods but exhibit roughly 10% disparity when measured appropriately. The ITE-based intervention further reduces algorithmic discrimination, achieving up to 60% reduction in disparity compared to traditional pre-processing approaches.

## Method Summary
The authors employ a two-pronged approach to address label bias in audit study data. First, they use audit study datasets that contain both true treatment effects and biased labels to train fairness interventions. Second, they implement an Individual Treatment Effect (ITE)-based approach that iteratively flips labels most likely influenced by discrimination. This iterative process continues until callback rates are equalized across age groups. The method is evaluated using a large-scale audit study dataset on age discrimination in hiring, comparing traditional fairness interventions with the ITE-based approach. The evaluation framework distinguishes between apparent fairness (when evaluated on biased labels) and actual fairness (when measured appropriately using audit study data).

## Key Results
- Traditional fairness interventions appear fair when evaluated on biased labels but exhibit approximately 10% disparity when measured appropriately
- ITE-based intervention reduces algorithmic discrimination by up to 60% compared to traditional pre-processing approaches
- Iterative label flipping procedure successfully equalizes callback rates across age groups

## Why This Works (Mechanism)
The mechanism works by addressing the fundamental problem that traditional fairness interventions assume unbiased labels, but in audit studies, the labels themselves are biased by discrimination. The ITE-based approach identifies which labels are most likely influenced by discriminatory factors and systematically corrects them through iterative flipping. This creates a more accurate training dataset where the true treatment effects are better represented, allowing fairness interventions to actually achieve their intended goals rather than simply masking existing biases. By using audit study data both to train and evaluate the interventions, the method provides a more honest assessment of algorithmic fairness that accounts for real-world discrimination patterns.

## Foundational Learning
- **Audit Studies**: Why needed - provide ground truth on discrimination by controlling for non-protected characteristics; Quick check - can identify which callbacks are likely influenced by protected characteristics
- **Individual Treatment Effect (ITE)**: Why needed - quantifies the causal effect of being in a protected group on outcomes; Quick check - can estimate discrimination probability for each observation
- **Label Bias**: Why needed - standard datasets often contain biased labels that reflect existing discrimination; Quick check - labels may not represent true merit or qualification
- **Iterative Label Flipping**: Why needed - systematically corrects biased labels while maintaining data integrity; Quick check - continues until desired fairness metrics are achieved
- **Fairness Metrics**: Why needed - different metrics can reveal different aspects of discrimination; Quick check - should be validated against audit study ground truth

## Architecture Onboarding

Component Map:
Audit Study Data -> Label Bias Detection -> Iterative Label Flipping -> Fairness Intervention -> Evaluation

Critical Path:
Audit Study Data → ITE Estimation → Label Flipping Decision → Model Training → Fairness Evaluation

Design Tradeoffs:
- Accuracy vs. interpretability: ITE-based approach provides causal interpretation but requires complex estimation
- Computational cost vs. fairness gain: Iterative flipping is computationally intensive but achieves superior fairness
- Data requirements vs. generalizability: Audit studies provide ground truth but may not generalize to all contexts

Failure Signatures:
- Overcorrection: Excessive label flipping leading to reverse discrimination
- Undercorrection: Insufficient flipping leaving persistent bias
- Data sparsity: Audit study datasets may be too small for reliable ITE estimation
- Confounding: Unmeasured variables may influence both treatment assignment and outcomes

First Experiments:
1. Baseline fairness intervention using standard pre-processing on biased labels
2. ITE-based intervention with iterative label flipping on audit study data
3. Comparative evaluation of both approaches using audit study ground truth

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability of ITE-based approach across different protected classes and domains remains uncertain
- Iterative label flipping procedure raises concerns about potential overfitting to specific audit study dataset
- Focus on callback rates may not capture all relevant aspects of hiring discrimination
- Audit study datasets, while valuable, may not fully represent real-world hiring complexity

## Confidence

**High Confidence:**
- Traditional fairness interventions can appear fair when evaluated on biased labels but reveal disparities when measured appropriately (approximately 10% disparity)

**Medium Confidence:**
- ITE-based intervention effectiveness in reducing algorithmic discrimination (up to 60% reduction), though requires further validation across diverse contexts
- Methodological contribution of using audit studies to improve both training and evaluation of automated hiring algorithms

## Next Checks
1. Validate the ITE-based approach across different protected classes (e.g., gender, race) and domains beyond hiring to assess generalizability
2. Conduct sensitivity analyses to determine optimal number of label flips in iterative procedure and evaluate potential overfitting risks
3. Test the approach with alternative fairness metrics beyond callback rates to ensure comprehensive evaluation of algorithmic fairness