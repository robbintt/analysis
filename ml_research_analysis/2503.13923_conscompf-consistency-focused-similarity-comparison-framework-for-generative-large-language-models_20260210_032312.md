---
ver: rpa2
title: 'ConSCompF: Consistency-focused Similarity Comparison Framework for Generative
  Large Language Models'
arxiv_id: '2503.13923'
source_url: https://arxiv.org/abs/2503.13923
tags:
- similarity
- comparison
- conscompf
- scores
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ConSCompF addresses the challenge of comparing generative large
  language models (LLMs) when developers do not disclose training data or model weights.
  The method generates multiple responses per instruction, encodes them with SBERT,
  calculates consistency scores to account for response variability, and uses weighted
  cosine similarity to compare models.
---

# ConSCompF: Consistency-focused Similarity Comparison Framework for Generative Large Language Models

## Quick Facts
- arXiv ID: 2503.13923
- Source URL: https://arxiv.org/abs/2503.13923
- Reference count: 5
- Method generates multiple responses per instruction, encodes with SBERT, calculates consistency scores, and uses weighted cosine similarity to compare generative LLMs without requiring model weights or training data disclosure.

## Executive Summary
ConSCompF addresses the challenge of comparing generative large language models when developers do not disclose training data or model weights. The method generates multiple responses per instruction, encodes them with SBERT, calculates consistency scores to account for response variability, and uses weighted cosine similarity to compare models. Two experiments demonstrate ConSCompF can detect LLM performance degradation due to quantization and identify similarities between models trained on identical data. In few-shot scenarios with as few as 50 samples, ConSCompF produces similarity scores highly correlated with traditional benchmarks like ROUGE-L. PCA visualization of similarity matrices effectively clusters models by training data. The framework enables effective LLM comparison without requiring labeled data, making it suitable for detecting similarities and potential investment fraud in commercial LLMs.

## Method Summary
ConSCompF compares two generative LLMs by generating k=5 responses per instruction from both models, encoding all responses with MiniLM-L12-v2 SBERT, computing consistency scores based on pairwise cosine similarity among responses, aggregating to general answer vectors via mean pooling, calculating cosine similarity between corresponding general vectors, and producing a final weighted similarity score that accounts for instruction consistency. The method operates on Alpaca dataset instructions (520 for model comparison, 5,200 for quantization experiment) and generates responses with temperature=0.7, top-k=50, top-p=0.95, max_tokens=128. The framework validates its effectiveness by correlating similarity scores with ROUGE-L benchmark differences and visualizing clustering via PCA.

## Key Results
- Quantization experiment: ConSCompF detects performance degradation from base model to 8-bit, 4-bit, and 2-bit quantization with Pearson correlation r=0.9957
- Few-shot capability: With only 50 low-consistency instructions, ConSCompF achieves Pearson correlation r=0.8644 with ROUGE-L score differences
- Model clustering: PCA visualization of similarity matrices clusters models by training data, with Mistral and OpenHermes2.5 models clustering together, and Llama2-7b and Llama2-13b clustering together

## Why This Works (Mechanism)

### Mechanism 1: Response Aggregation via General Answer Vector
- Claim: Averaging multiple response embeddings captures stable behavioral patterns that distinguish models trained on similar data.
- Mechanism: Generate k responses per instruction → SBERT encoding → mean pooling → general answer vector vi = (1/k)Σ(E1...Ek)
- Core assumption: The mean of multiple embeddings converges toward a representation of the model's characteristic response distribution.
- Evidence anchors:
  - [abstract] "generates multiple responses per instruction, encodes them with SBERT"
  - [section 3.1] "compute the average between them...reflects the overall content of all responses"
  - [corpus] Weak direct validation; "Harmonizing Diverse Models" notes LLM inconsistency but doesn't test this averaging approach.
- Break condition: If k is too small or responses have extreme variance (creative prompts), the mean fails to stabilize.

### Mechanism 2: Consistency-Weighted Similarity Adjustment
- Claim: Weighting similarity scores by instruction consistency reduces noise from prompts that elicit highly variable responses.
- Mechanism: Compute pairwise cosine similarity between k responses → consistency score ci → adjusted weighted average: y = (1/n)Σyi[c̄i + (1 - c̄i)]
- Core assumption: High-consistency instructions (factual prompts) provide more reliable comparison signal than low-consistency ones (creative prompts).
- Evidence anchors:
  - [abstract] "calculates consistency scores to account for response variability"
  - [section 3.3] Equation 4 shows the weighting formula; Table 6 shows weighted similarity has lower correlation with consistency than unweighted
  - [corpus] Related work on LLM consistency exists but doesn't validate this specific weighting scheme.
- Break condition: If all instructions have similar consistency scores, weighting provides no discriminative benefit.

### Mechanism 3: Semantic Embedding Capture of Training Data Signatures
- Claim: SBERT embeddings encode enough stylistic/semantic information to cluster models by training data lineage.
- Mechanism: MiniLM encoder produces sentence-level embeddings → cosine similarity between general vectors → PCA reveals clustering
- Core assumption: Models trained on identical or overlapping data produce semantically convergent responses even with different surface wording.
- Evidence anchors:
  - [section 4.3] "MiniLM was specifically trained to capture semantic information...rich sentence-level embedding vectors"
  - [section 5.2] Figure 4 shows Mistral/OpenHermes2.5 cluster together; Llama2-7b/13b cluster together
  - [corpus] Limited validation; related papers discuss LLM comparison but don't test SBERT specifically for this purpose.
- Break condition: If the encoder lacks sensitivity to writing style or training data artifacts, all models appear similar.

## Foundational Learning

- **Cosine Similarity for High-Dimensional Vectors**
  - Why needed here: Core metric for comparing embedding vectors; interprets vector alignment as semantic similarity.
  - Quick check question: Given embeddings [0.8, 0.6] and [0.6, 0.8], can you compute their cosine similarity?

- **Response Variance Under Stochastic Decoding**
  - Why needed here: Understanding why k=5 responses vary (temperature=0.7) and why consistency scores matter.
  - Quick check question: If temperature=0 produces deterministic outputs, why would that break this framework?

- **PCA for Similarity Matrix Compression**
  - Why needed here: ConSCompF outputs n×n similarity matrices; PCA reduces to 2D for visual clustering interpretation.
  - Quick check question: Why does PCA clustering by training data indicate the framework works?

## Architecture Onboarding

- **Component map:**
  - Response Generator: LLM with temperature=0.7, top-k=50, top-p=0.95, max_tokens=128
  - Encoder: MiniLM-L12-v2 (sentence-transformers)
  - Consistency Calculator: Pairwise cosine across k-response combinations
  - Aggregator: Mean pooling to general answer vector
  - Similarity Module: Cosine between general vectors
  - Weighting Module: Consistency-adjusted final score

- **Critical path:**
  1. Generate k responses for n instructions using both LLMs (compute bottleneck)
  2. Encode all k×n responses → embedding tensors
  3. Compute consistency scores per instruction (pairwise combinations)
  4. Aggregate to general vectors (one per instruction per model)
  5. Cosine similarity between corresponding general vectors
  6. Weighted average across instructions → final score

- **Design tradeoffs:**
  - Higher k: More stable general vectors vs. 5× compute cost per instruction
  - Instruction selection: Low-consistency prompts (creative tasks) correlate better with ROUGE-L differences but require careful curation
  - Encoder choice: MiniLM-L12-v2 selected for highest standard deviation in similarity scores (Table 2); alternatives (BERT, RoBERTa) showed insufficient sensitivity

- **Failure signatures:**
  - All similarity scores between 0.98-1.0 → encoder not discriminating; try different encoder
  - High correlation (>0.7) between unweighted similarity and consistency → weighting not effective
  - PCA shows no clustering → instruction set doesn't elicit discriminative responses

- **First 3 experiments:**
  1. Self-comparison: Generate two response sets from same model with different seeds; expect weighted similarity >0.95
  2. Quantization ladder: Compare base model vs. 8-bit/4-bit/2-bit variants; expect monotonically decreasing similarity
  3. Few-shot probe: Run with 50 low-consistency instructions (avg ci <0.6); compute correlation with ROUGE-L differences; target r >0.8

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning the encoder model using the entropy of ConSCompF similarity scores as a training target improve the accuracy of LLM comparisons?
- Basis in paper: [explicit] Section 7 states, "We assume that it is possible to fine-tune the encoder using the entropy of ConSCompF similarity scores as a training target."
- Why unresolved: The authors utilized pre-trained encoders like MiniLM but did not implement or test the proposed fine-tuning strategy, noting that developing such a methodology was out of scope.
- What evidence would resolve it: An experiment comparing the performance of a baseline encoder against an encoder fine-tuned via the proposed entropy-based objective, measuring the resulting correlation with benchmark metrics like ROUGE-L.

### Open Question 2
- Question: Which specific classification algorithms can most effectively utilize ConSCompF similarity matrices to automate the categorization of LLMs?
- Basis in paper: [explicit] Section 7 notes, "Additionally, we did not apply any specific classification algorithms for LLM categorization based on their similarity scores, which is another promising area for future research."
- Why unresolved: The paper demonstrated that similarity matrices could be visualized using PCA to reveal clusters, but it did not formalize this clustering into an automated classification system.
- What evidence would resolve it: A study applying classification algorithms (e.g., hierarchical clustering or k-nearest neighbors) to the similarity matrices and evaluating their accuracy in grouping models by training data or architecture.

### Open Question 3
- Question: Can an automated instruction selection strategy based on low consistency scores create an optimal benchmark that maximizes correlation with traditional metrics?
- Basis in paper: [inferred] Section 6 suggests that "Finding a combination of instructions that results in the highest correlation... would allow an almost zero-cost comparison," and Section 5.4 indicates low-consistency instructions correlate better with ROUGE-L.
- Why unresolved: While the authors recommend using instructions requiring creativity, they rely on manual or random sampling; a deterministic method for selecting these instructions is not defined.
- What evidence would resolve it: A comparative analysis showing that a curated set of low-consistency instructions consistently outperforms random samples in correlating with established benchmark scores.

## Limitations

- Encoder sensitivity validation is limited to standard deviation metrics rather than direct validation of training data detection capability
- Consistency weighting mechanism shows mixed empirical results, with unweighted similarity having higher correlation with consistency than weighted similarity
- Framework generalizability beyond Alpaca instructions is untested for domain-specific prompts or open-ended creative tasks

## Confidence

**High Confidence**
- ConSCompF detects performance degradation from quantization (Experiment 1)
- ConSCompF similarity scores correlate with ROUGE-L differences (Experiment 2)
- PCA clustering by training data in controlled experiment (Experiment 3)

**Medium Confidence**
- SBERT embeddings capture training data signatures (mechanism 3)
- Consistency weighting improves comparison reliability (mechanism 2)
- Few-shot detection works with 50 instructions (Experiment 2 results)

**Low Confidence**
- Averaging k responses produces stable general answer vectors (mechanism 1)
- Framework generalizes to domain-specific prompts beyond Alpaca
- Performance on open-ended creative tasks without golden answers

## Next Checks

1. **Cross-Encoder Validation**: Repeat quantization experiment using alternative encoders (BERT, RoBERTa, E5) to confirm MiniLM-L12-v2 provides unique advantages beyond just sensitivity to random variation.

2. **Domain Transfer Test**: Apply ConSCompF to compare models on domain-specific instruction sets (medical, legal, code) to verify performance signatures transfer beyond Alpaca-style questions.

3. **Creative Prompt Evaluation**: Test framework on open-ended creative prompts without golden answers to validate detection capability in realistic fraud detection scenarios where ground truth is unavailable.