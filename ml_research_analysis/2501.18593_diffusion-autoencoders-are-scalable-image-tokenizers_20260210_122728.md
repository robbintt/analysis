---
ver: rpa2
title: Diffusion Autoencoders are Scalable Image Tokenizers
arxiv_id: '2501.18593'
source_url: https://arxiv.org/abs/2501.18593
tags:
- diffusion
- image
- dito
- glpto
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiTo, a simple diffusion autoencoder for
  learning scalable image tokenizers. The key insight is that a single diffusion L2
  loss, specifically Flow Matching (an ELBO maximization objective), can train competitive
  image tokenizers without complex loss combinations or pretrained models.
---

# Diffusion Autoencoders are Scalable Image Tokenizers

## Quick Facts
- **arXiv ID:** 2501.18593
- **Source URL:** https://arxiv.org/abs/2501.18593
- **Reference count:** 28
- **Primary result:** DiTo learns scalable image tokenizers using a single diffusion L2 loss (Flow Matching), achieving better reconstruction quality than GAN-LPIPS tokenizers and superior downstream generation performance when scaled.

## Executive Summary
This paper introduces DiTo, a simple diffusion autoencoder that learns competitive image tokenizers using a single diffusion L2 loss objective (Flow Matching) instead of complex loss combinations. The key insight is that Flow Matching maximizes the ELBO, forcing the latent representation to retain sufficient information for reconstruction across all noise levels. DiTo achieves state-of-the-art reconstruction quality, particularly for structured visual content like text and symbols, and demonstrates strong scalability where larger models outperform baselines in both reconstruction fidelity and downstream image generation tasks.

## Method Summary
DiTo jointly trains an encoder and diffusion decoder using Flow Matching (v-prediction with specific noise scheduling), which theoretically maximizes the data likelihood as an ELBO. The architecture consists of a standard convolutional encoder (downsampling 8Ã— to 4 channels) and a UNet decoder conditioned on both time and latent representations. Unlike previous tokenizers that use complex combinations of GAN, LPIPS, and L1 losses, DiTo uses a single L2 loss. Training includes noise synchronization to align the noising process in latent space with pixel space, improving downstream Latent Diffusion Model performance. The model is trained on ImageNet with LayerNorm applied to latents instead of KL regularization.

## Key Results
- DiTo achieves competitive or better image reconstruction quality than state-of-the-art GAN-LPIPS tokenizers, especially for structured content like text and symbols
- When scaled up (DiTo-XL), the model outperforms baselines in both reconstruction fidelity (rFID) and downstream image generation (gFID)
- Joint training of encoder and decoder, rather than retrofitting to frozen latents, is crucial for preserving structured content during reconstruction
- Noise synchronization during training improves downstream generation quality by aligning latent and pixel space noising processes

## Why This Works (Mechanism)

### Mechanism 1: ELBO Maximization via Flow Matching
Replacing complex multi-term loss functions with a single diffusion L2 loss (Flow Matching) creates a scalable tokenizer when the objective functions as an Evidence Lower Bound (ELBO). Flow Matching forces the latent $z$ to retain sufficient information to reconstruct the image across all noise levels, rather than optimizing for heuristic feature matching that may discard high-frequency details.

### Mechanism 2: Joint Optimization of Latent and Decoder
Superior reconstruction of structured visual content stems from jointly training the encoder and diffusion decoder. When trained together, the encoder adapts specifically to the probabilistic needs of the diffusion decoder, allowing it to allocate capacity for structured information that would otherwise be lost.

### Mechanism 3: Noise Synchronization for Downstream Generation
Regularizing the latent space to synchronize its noising process with pixel space improves downstream Latent Diffusion Model performance. This encourages a smooth diffusion path in latent space, creating an easier optimization landscape for the LDM to learn generative modeling.

## Foundational Learning

- **Concept: Evidence Lower Bound (ELBO) in Diffusion**
  - Why needed here: The paper hinges on the idea that standard diffusion losses are not necessarily ELBOs, whereas Flow Matching is. Understanding this distinction is required to select the correct objective.
  - Quick check question: Why would a standard $\epsilon$-prediction loss cause color shifts in this specific architecture while Flow Matching does not?

- **Concept: Latent Diffusion Models (LDMs)**
  - Why needed here: DiTo is a tokenizer designed explicitly as the first stage for an LDM. You must understand the separation between "perceptual compression" (Tokenizer) and "generative learning" (LDM).
  - Quick check question: What specific metric (rFID vs gFID) determines if the tokenizer is good for the *generation* task vs. the *reconstruction* task?

- **Concept: LPIPS (Learned Perceptual Image Patch Similarity)**
  - Why needed here: The baseline (GLPTo) relies heavily on LPIPS. DiTo removes this dependency.
  - Quick check question: Why does the paper argue that LPIPS, while good for textures, might harm the reconstruction of "structured visual content"?

## Architecture Onboarding

- **Component map:** Image $x$ -> Encoder $E$ -> Latent $z$ -> Noise Synchronization -> Decoder $D$ -> Reconstruction $\hat{x}$

- **Critical path:** 
  1. Forward pass: Image $x \to E \to z$
  2. Noising: Create $x_t$ (noised image) and optionally $z_\tau$ (noised latent for sync)
  3. Decoding: $D(x_t, t, z)$ predicts $v$-target
  4. Loss: Simple L2 between prediction and target

- **Design tradeoffs:**
  - Loss Function: Flow Matching (ELBO) vs. Standard $\epsilon$-pred. Choice: Flow Matching required for stable latent learning.
  - Regularization: LayerNorm vs. KL Loss. Choice: LayerNorm simplifies training and works well for ELBO.
  - Speed: DiTo is slower at inference (iterative denoising) compared to GLPTo (single pass), but scales better in quality.

- **Failure signatures:**
  - Color Shift: Indicates use of a non-ELBO objective (e.g., standard $\epsilon$-prediction) for the decoder
  - High Frequency Loss (Text): Indicates the encoder is frozen or not joint-trained with the diffusion objective
  - Training Instability: Likely due to missing LayerNorm or incorrect noise schedule implementation

- **First 3 experiments:**
  1. Implement DiTo with standard $\epsilon$-prediction vs. Flow Matching on ImageNet subset to verify the "color shift" artifact
  2. Train DiTo-B, DiTo-L, and DiTo-XL and plot rFID vs. model size to confirm scalability
  3. Train a downstream DiT-XL/2 on latents from DiTo-XL with and without noise synchronization and compare gFID

## Open Questions the Paper Calls Out
- Can DiTo learn unified representations that perform effectively for both recognition and generation tasks?
- Can content-aware tokenizers encoding spatially variable information density improve compression rates beyond DiTo's fixed latent grid?
- Can one-step distillation techniques be applied to DiTo to match GAN decoder speeds without sacrificing reconstruction fidelity?

## Limitations
- Performance comparisons are limited to a single baseline (GLPTo) across two datasets (ImageNet-1K, CIFAR-10)
- Architectural specifics contain gaps that could impact reproducibility, particularly encoder configuration and decoder implementation details
- The computational expense of iterative ODE solvers makes inference significantly slower than single-step GAN decoders

## Confidence
- **High Confidence:** Ablation showing Flow Matching prevents color shifts while standard diffusion objectives cause them
- **Medium Confidence:** Joint optimization requirement for structured content reconstruction based on frozen-latent comparisons
- **Low Confidence:** Superiority of single-loss simplicity without comparison to other single-objective approaches or theoretical justification

## Next Checks
1. Train DiTo on LSUN Bedroom and COCO datasets to assess cross-dataset generalization beyond ImageNet/CIFAR-10
2. Implement DiTo with a frozen, pre-trained encoder while using Flow Matching to isolate benefits of joint optimization
3. Train Latent Diffusion Models of varying sizes (LDM-1B, LDM-2B, LDM-4B) on DiTo latents with and without noise synchronization to determine scaling properties