---
ver: rpa2
title: Multi-layer Stack Ensembles for Time Series Forecasting
arxiv_id: '2511.15350'
source_url: https://arxiv.org/abs/2511.15350
tags:
- forecasting
- time
- linear
- series
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic evaluation of 33 ensemble methods
  for time series forecasting across 50 real-world datasets. The authors demonstrate
  that stacking consistently improves forecast accuracy over individual models and
  simple averaging, with multi-layer stacking providing superior performance by combining
  multiple stacker models.
---

# Multi-layer Stack Ensembles for Time Series Forecasting

## Quick Facts
- arXiv ID: 2511.15350
- Source URL: https://arxiv.org/abs/2511.15350
- Reference count: 40
- Multi-layer stacking improves forecast accuracy by up to 5% error reduction and 200 Elo points over baselines

## Executive Summary
This paper systematically evaluates 33 ensemble methods for time series forecasting across 50 real-world datasets. The authors demonstrate that stacking consistently improves forecast accuracy over individual models and simple averaging, with multi-layer stacking providing superior performance by combining multiple stacker models. The proposed framework uses two levels of stacking (L2 models combining base forecasts, and L3 model aggregating L2 outputs) and shows significant improvements over traditional ensemble methods, challenging the "forecast combination puzzle."

## Method Summary
The proposed framework introduces a two-level stacking architecture for time series forecasting. Base models (L1) generate forecasts which are then combined by multiple L2 models. These L2 predictions are subsequently aggregated by an L3 model that learns the optimal combination weights. The framework evaluates 33 different ensemble methods across 50 diverse time series datasets, comparing performance against traditional approaches like simple averaging and single model forecasts. The authors use Elo rating methodology to quantify relative performance differences between methods.

## Key Results
- Stacking consistently outperforms individual models and simple averaging across all evaluated datasets
- Multi-layer stacking (L2 + L3) provides superior performance, reducing error by up to 5% and improving Elo ratings by 200 points
- Performance gains are robust to L1 model choice and particularly pronounced when strong individual forecasting models are unavailable
- Sufficient validation data is critical for training the L3 aggregator effectively

## Why This Works (Mechanism)
Multi-layer stacking works by learning optimal combination weights through hierarchical aggregation. The L2 models learn to combine base forecasts in ways that capture complementary strengths and weaknesses of individual models. The L3 model then learns to optimally weight these L2 combinations, creating a meta-learning process that adapts to dataset-specific characteristics. This hierarchical approach addresses the "forecast combination puzzle" by moving beyond simple averaging to learn data-adaptive combination strategies.

## Foundational Learning
- **Time series forecasting fundamentals**: Understanding stationarity, seasonality, and trend components is essential for evaluating ensemble methods. Quick check: Can you identify these components in a sample time series plot?
- **Ensemble learning principles**: Basic knowledge of bagging, boosting, and stacking is needed to understand why hierarchical combinations work. Quick check: Explain the difference between averaging and stacking approaches.
- **Elo rating methodology**: The paper uses pairwise comparison Elo ratings to quantify performance differences. Quick check: Calculate Elo rating changes after a model outperforms another in a head-to-head comparison.

## Architecture Onboarding

**Component Map**: Base Models (L1) -> L2 Stackers -> L3 Aggregator

**Critical Path**: The most important sequence is: generate base forecasts → train L2 models on validation data → train L3 aggregator on L2 outputs → produce final ensemble forecast. Each stage depends on the previous one, with the L3 model being the final arbiter of ensemble performance.

**Design Tradeoffs**: The framework balances complexity against performance gains. More L2 stackers increase diversity but also computational cost. The L3 model adds another training layer but enables optimal combination learning. The tradeoff is between achieving marginal accuracy improvements versus increased model complexity and training time.

**Failure Signatures**: Poor performance typically occurs when: validation data is insufficient for L3 training, base models are too similar (reducing L2 diversity), or the L3 model overfits to validation data patterns. Warning signs include minimal Elo improvement over simple averaging or degraded performance on certain dataset types.

**First Experiments**:
1. Compare simple averaging versus single L2 stacking on a small dataset to observe baseline improvements
2. Test different numbers of L2 stackers (2, 4, 8) to find optimal diversity/complexity balance
3. Evaluate performance on datasets with varying characteristics (seasonal vs. non-seasonal, short vs. long series)

## Open Questions the Paper Calls Out
None

## Limitations
- Excludes transformer-based forecasting models, limiting applicability to modern deep learning contexts
- Elo rating methodology may oversimplify complex multi-model interactions through pairwise comparisons
- Computational cost of multi-layer stacking not fully characterized, raising scalability concerns
- Limited analysis of model interpretability and potential overfitting when validation data is scarce

## Confidence
High confidence: Stacking improves accuracy over individual models and simple averaging
Medium confidence: Multi-layer stacking superiority is consistent but not universal
Medium confidence: Framework robustness to L1 model choice based on fixed strong individual models

## Next Checks
1. Evaluate multi-layer stacking on transformer-based forecasting models to assess performance in modern deep learning contexts
2. Conduct ablation studies on L2 stacker diversity and count to quantify complexity-performance tradeoffs
3. Analyze computational efficiency by measuring training and inference times across varying dataset sizes