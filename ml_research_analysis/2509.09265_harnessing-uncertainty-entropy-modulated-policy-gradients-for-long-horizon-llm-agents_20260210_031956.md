---
ver: rpa2
title: 'Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon
  LLM Agents'
arxiv_id: '2509.09265'
source_url: https://arxiv.org/abs/2509.09265
tags:
- policy
- learning
- empg
- arxiv
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Entropy-Modulated Policy Gradients (EMPG),
  a method designed to improve credit assignment in long-horizon LLM agents. The key
  insight is that standard policy gradients inherently couple gradient magnitude with
  policy entropy, leading to inefficient learning for confident correct actions and
  instability from uncertain ones.
---

# Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents

## Quick Facts
- arXiv ID: 2509.09265
- Source URL: https://arxiv.org/abs/2509.09265
- Reference count: 40
- Primary result: Introduces EMPG, a method that significantly improves credit assignment in long-horizon LLM agents by modulating policy gradients based on step-wise uncertainty, achieving up to +8.1% success rate gains over GRPO and DAPO baselines.

## Executive Summary
This paper addresses the challenge of credit assignment in long-horizon LLM agents trained with sparse outcome-based rewards. The core insight is that standard policy gradients couple gradient magnitude with policy entropy, creating inefficient learning where confident correct actions receive small updates while uncertain steps produce destabilizing large gradients. Entropy-Modulated Policy Gradients (EMPG) solves this by dynamically scaling gradients based on step-level uncertainty: amplifying updates for confident correct actions, penalizing confident errors, and attenuating updates from uncertain steps. The method also introduces a future clarity bonus to encourage predictable solution paths. Experiments across WebShop, ALFWorld, and Deep Search benchmarks demonstrate significant performance improvements and training stability gains.

## Method Summary
EMPG augments standard policy gradients with two key components: self-calibrating gradient scaling and future clarity bonus. The gradient scaling function g(H_t) = exp(-k·H_norm,t) / mean(exp(-k·H_norm)) amplifies confident steps (low entropy) and attenuates uncertain ones (high entropy), with batch normalization ensuring stable scaling. The future clarity bonus f(H_{t+1}) = exp(-k'·H_norm,t+1) rewards transitions to lower-entropy future states, encouraging predictable solution paths. These components modulate the advantage function: A_mod = A·g(H_t) + ζ·f(H_{t+1}), with final zero-mean normalization. The method uses step-level entropy computed as average token entropy within each assistant response, batch-min-max normalized, and applies the modulation to the ReAct paradigm agents.

## Key Results
- WebShop: +5.1% success rate improvement over GRPO baseline
- ALFWorld: +8.1% success rate improvement over GRPO baseline  
- Deep Search: +2.6% ID performance improvement, +0.5% OOD generalization gain
- KL loss stability: Baseline DAPO shows erratic KL loss after ~240 steps while EMPG remains stable
- Ablation results: g(H) component provides +2.6 ID gain, f(H) provides +2.5 ID gain but hurts OOD performance

## Why This Works (Mechanism)

### Mechanism 1: Entropy-Gradient Decoupling via Self-Calibrating Scaling
- Claim: Re-weighting gradients by inverse entropy amplifies reliable updates and attenuates noisy ones.
- Mechanism: The scaling function g(H) = exp(-k·H_norm) / mean(exp(-k·H_norm)) assigns g>1 to below-average entropy steps (amplifying) and g<1 to above-average entropy steps (attenuating), with batch-mean normalization preventing signal inflation.
- Core assumption: Step-level policy entropy is a valid proxy for action reliability and gradient trustworthiness.
- Evidence anchors:
  - [abstract] "the magnitude of policy gradients is inherently coupled with the entropy...EMPG amplifies updates for confident correct actions, attenuates updates from uncertain steps"
  - [section 4.2] Equation 10 and normalization constraint (Eq. 9)
  - [corpus] Related work SEED-GRPO uses semantic entropy similarly but for single-turn tasks; no direct validation of step-level entropy as reliability proxy in multi-turn agents
- Break condition: If entropy correlates poorly with actual step quality (e.g., confident hallucinations dominate), amplification may reinforce errors.

### Mechanism 2: Future Clarity as Intrinsic Exploration Guidance
- Claim: Rewarding transitions to lower-entropy future states guides agents toward predictable solution paths.
- Mechanism: The bonus f(H_{t+1}) = exp(-k'·H_{norm,t+1}) adds positive advantage when the next state has lower policy entropy, encouraging actions that reduce ambiguity about subsequent decisions.
- Core assumption: Lower future entropy correlates with being on a viable solution trajectory (not merely narrowing to wrong paths).
- Evidence anchors:
  - [abstract] "introducing a future clarity bonus to encourage predictable solution paths"
  - [section 4.2] Eq. 11 and ablation showing +2.6 ID gain from Future Clarity (Table 2)
  - [corpus] Weak direct evidence; Empowerment/information gain frameworks provide theoretical grounding but empirical validation for LLM agents is limited
- Break condition: If low-entropy states include premature commitment to suboptimal actions, the bonus creates local optima.

### Mechanism 3: Confident Error Penalization
- Claim: Strong penalties on confident wrong actions counteract "hallucinated confidence."
- Mechanism: When A(i) < 0 (failed trajectory) and g(H_t) > 1 (confident step), the product A(i)·g(H_t) creates a large negative advantage, strongly discouraging that behavior.
- Core assumption: Failed trajectories with confident steps indicate harmful overconfidence worth strongly suppressing.
- Evidence anchors:
  - [section 4.2] "penalizes confident errors...combating 'hallucinated confidence'"
  - [Figure 1] "Amplify Penalty: Strongly correct 'hallucinated confidence'"
  - [corpus] No direct corpus evidence on confident error penalization in multi-turn agents
- Break condition: If confident steps in failed trajectories were actually correct but later steps caused failure, strong penalization creates false negatives.

## Foundational Learning

- Concept: **Policy Gradient Theorem and Advantage Estimation**
  - Why needed here: EMPG modulates the advantage A(s_t,a_t); understanding that ∇θJ = E[A·∇logπ] is prerequisite to grasping why scaling A affects learning.
  - Quick check question: Can you explain why normalizing advantages to zero mean reduces gradient variance?

- Concept: **Shannon vs. Rényi Entropy**
  - Why needed here: Proposition 1 derives gradient norm coupling with Rényi-2 entropy; implementation uses Shannon entropy as a practical proxy.
  - Quick check question: What is the difference between H₂(π) = -log(Σπ²ⱼ) and H(π) = -Σπⱼlog(πⱼ)?

- Concept: **Credit Assignment in Sparse-Reward RL**
  - Why needed here: EMPG's core motivation is solving credit assignment when rewards only appear at trajectory end.
  - Quick check question: Why does GRPO use group-relative advantages rather than raw returns?

## Architecture Onboarding

- Component map: Entropy extraction → batch normalization → g(H) and f(H) computation → advantage modulation → final normalization → policy gradient update

- Critical path: Entropy extraction → batch normalization → g(H) and f(H) computation → advantage modulation → final normalization → policy gradient update

- Design tradeoffs:
  - Step-level vs. token-level entropy: Paper argues step-level captures decision uncertainty better (Figure 3), but token-level is cheaper
  - Batch normalization vs. running statistics: Batch-level adapts to evolving policy but may be noisy with small batches
  - ζ (clarity bonus weight): Higher ζ encourages exploitation of known paths; lower ζ relies more on external reward

- Failure signatures:
  - KL divergence spikes (Figure 2 shows baseline diverges at ~240 steps; EMPG should remain stable)
  - Performance plateau early in training (Figure D.1)
  - OOD generalization worse than ID (may indicate Future Clarity overfitting)

- First 3 experiments:
  1. Reproduce GRPO baseline on ALFWorld with Qwen2.5-1.5B; verify ~65.6% success rate matches Table 1
  2. Add EMPG modulation with k=1.0, ζ=0.05; confirm +5-8% improvement and stable KL loss
  3. Ablate each component: (a) g(H) only, (b) f(H) only, to isolate ID vs. OOD effects per Table 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative uncertainty estimators (e.g., Monte Carlo dropout, ensemble variance) outperform policy entropy as the uncertainty signal in EMPG?
- Basis in paper: [explicit] "While we use policy entropy for its computational efficiency, future work could explore alternative uncertainty estimators, such as those derived from Monte Carlo dropout or the variance in logits from an ensemble of model heads."
- Why unresolved: The paper only evaluates entropy due to its theoretical grounding and efficiency; no comparison to other uncertainty measures is provided.
- What evidence would resolve it: Ablation experiments comparing token-level entropy against MC dropout and ensemble-based uncertainty on the same benchmarks (ALFWorld, WebShop, Deep Search).

### Open Question 2
- Question: How does EMPG transfer to other long-horizon domains such as embodied AI and multi-agent collaboration?
- Basis in paper: [explicit] "For future work, we plan to explore the application of EMPG to other long-horizon tasks, such as embodied AI and multi-agent collaboration."
- Why unresolved: Experiments are limited to web navigation, text-based environments, and information retrieval; embodied and multi-agent settings involve different action spaces and reward structures.
- What evidence would resolve it: Empirical evaluation of EMPG on embodied AI benchmarks (e.g., Habitat, BEHAVIOR) and multi-agent coordination tasks, measuring success rate and training stability.

### Open Question 3
- Question: Does EMPG provide benefits under dense reward settings, or is its advantage specific to sparse outcome-based rewards?
- Basis in paper: [inferred] The paper frames EMPG as a solution to sparse reward challenges; all experiments use binary outcome rewards (Eq. 3: r_T ∈ {0,1}). The interaction with dense shaping rewards or PRM-based feedback remains unexplored.
- Why unresolved: Dense rewards could provide strong step-level signals that might reduce or eliminate the need for entropy-based modulation.
- What evidence would resolve it: Experiments combining EMPG with dense reward signals (e.g., PRM scores, shaped rewards) on the same benchmarks, comparing convergence speed and final performance.

## Limitations
- Entropy proxy validity: The core mechanism assumes step-level policy entropy reliably indicates action reliability, but this correlation is theoretically grounded but empirically unverified in multi-turn LLM agents.
- Implementation dependencies: EMPG's effectiveness relies heavily on Verl-Agent framework and GiGPO codebase, which are not publicly verified as released, requiring significant engineering assumptions.
- Hyperparameter sensitivity: The method uses fixed hyperparameters without systematic sensitivity analysis, and the paper doesn't characterize the performance-ID/OOD tradeoff curve across different settings.

## Confidence
- High confidence: The entropy-gradient decoupling mechanism is well-theoretically grounded through Proposition 1 (Rényi-2 entropy gradient norm). The empirical results showing significant improvements over GRPO/DAPO baselines (+5-8% success rate) are well-documented across multiple benchmarks. The KL stability benefit is clearly demonstrated in Figure 2.
- Medium confidence: The future clarity bonus mechanism and its contribution to performance (ID gain of +2.6 in Table 2) is supported by ablation studies but lacks theoretical justification specific to LLM agents.
- Low confidence: The implementation details around batch normalization of g(H) and the interaction between entropy scaling and the future clarity bonus are complex and not fully explored.

## Next Checks
1. **Entropy-reliability correlation study**: Conduct controlled experiments where ground truth action reliability is available (synthetic or oracle-assisted tasks). Measure the actual correlation between step-level entropy and action quality, and test whether EMPG's amplification actually prioritizes reliable actions or if it can be gamed by confident errors.

2. **Hyperparameter sensitivity sweep**: Systematically vary k, k', and ζ across multiple orders of magnitude on WebShop/ALFWorld. Document the performance-ID/OOD tradeoff curve to identify optimal settings and characterize robustness to hyperparameter choices. Include ablation of g(H) vs. f(H) components.

3. **Comparison to semantic entropy baselines**: Implement SEED-GRPO or similar semantic entropy methods on the same long-horizon LLM agent tasks. Compare not just final performance but training stability, computational overhead, and OOD generalization to determine if policy entropy is truly the optimal uncertainty proxy or if semantic approaches would be more effective.