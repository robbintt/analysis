---
ver: rpa2
title: 'Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on
  Chinese Multi-Subject Physics Problems'
arxiv_id: '2509.15839'
source_url: https://arxiv.org/abs/2509.15839
tags:
- uni00000048
- physics
- reasoning
- evaluation
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multi-Physics, a comprehensive Chinese multimodal
  benchmark for physics reasoning that addresses gaps in existing benchmarks regarding
  fine-grained subject coverage, step-by-step reasoning evaluation, and visual information
  assessment. The benchmark comprises 1,412 image-associated multiple-choice questions
  across 11 high-school physics subjects and 5 difficulty levels.
---

# Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems

## Quick Facts
- arXiv ID: 2509.15839
- Source URL: https://arxiv.org/abs/2509.15839
- Reference count: 0
- This paper introduces Multi-Physics, a comprehensive Chinese multimodal benchmark for physics reasoning that addresses gaps in existing benchmarks regarding fine-grained subject coverage, step-by-step reasoning evaluation, and visual information assessment.

## Executive Summary
This paper introduces Multi-Physics, a comprehensive Chinese multimodal benchmark designed to evaluate multimodal large language models (LLMs) on physics reasoning tasks. The benchmark addresses limitations in existing evaluations by providing fine-grained subject coverage across 11 high-school physics subjects, implementing step-by-step reasoning assessment through a dual evaluation framework, and incorporating visual information analysis. The benchmark consists of 1,412 image-associated multiple-choice questions spanning 5 difficulty levels, with problems generated from standard educational materials and verified using GPT-4o. The dual evaluation framework assesses both final answer accuracy and chain-of-thought (CoT) integrity using model-based evaluation, providing a more comprehensive understanding of model reasoning capabilities.

## Method Summary
The Multi-Physics benchmark was constructed through a systematic process involving curriculum analysis, problem collection, generation, and verification. The authors analyzed the Chinese high-school physics curriculum to identify 11 subjects and 5 difficulty levels, then collected 6,263 problems from standard educational materials. Using GPT-4o, they generated 1,412 image-associated multiple-choice questions and verified solutions through GPT-4o-based solution checking. The dual evaluation framework combines final answer accuracy assessment with step-by-step CoT integrity evaluation, using model-based evaluation to score the logical consistency of reasoning chains. The benchmark was evaluated across 20 different multimodal LLMs, comparing performance with and without visual information to assess the impact of multimodal inputs on physics reasoning capabilities.

## Key Results
- Visual information significantly improves performance across all evaluated models, with some models like o4-mini and Gemini-2.5-Pro demonstrating particularly notable improvements when images are provided.
- The benchmark reveals varying reasoning capabilities across physics subjects, with models showing different strengths and weaknesses in specific areas like mechanics versus electromagnetism.
- While CoT accuracy correlates with final answer accuracy, models sometimes arrive at correct answers through flawed reasoning, highlighting the importance of step-by-step evaluation.

## Why This Works (Mechanism)
The effectiveness of the Multi-Physics benchmark stems from its comprehensive approach to evaluating multimodal reasoning in physics. By incorporating visual information alongside textual problems, the benchmark captures the real-world complexity of physics problem-solving where diagrams, graphs, and experimental setups are integral to understanding and solving problems. The dual evaluation framework addresses the limitation of traditional accuracy-only assessments by examining the reasoning process itself, revealing whether correct answers result from sound reasoning or fortuitous guesses. The fine-grained subject coverage ensures that models are tested across the full spectrum of physics knowledge rather than being evaluated on a narrow subset of topics.

## Foundational Learning
- **Multimodal reasoning**: Understanding how models integrate visual and textual information is crucial for physics problem-solving, where diagrams and equations work together to convey complete information. Quick check: Can the model correctly interpret a circuit diagram while simultaneously processing the textual description of the problem?
- **Chain-of-thought evaluation**: Assessing reasoning quality requires examining intermediate steps, not just final answers, to distinguish between correct reasoning and lucky guesses. Quick check: Does the model's reasoning follow logical steps from given information to solution?
- **Physics domain knowledge**: The benchmark covers 11 subjects requiring different types of reasoning, from kinematic equations to electromagnetic field calculations. Quick check: Can the model apply appropriate formulas and principles for each subject area?
- **Visual information processing**: Physics problems often rely heavily on visual elements like graphs, diagrams, and experimental setups that contain critical information. Quick check: Does the model extract relevant information from images and integrate it with textual data?

## Architecture Onboarding

**Component Map**: Problem Generation -> Solution Verification -> Dual Evaluation Framework -> Performance Analysis

**Critical Path**: Curriculum Analysis → Problem Collection → GPT-4o Generation → Model-Based Evaluation → Result Analysis

**Design Tradeoffs**: The benchmark prioritizes comprehensive coverage and rigorous evaluation over scale, choosing quality and depth over quantity of problems. The use of model-based evaluation for solution verification trades potential accuracy for scalability and consistency.

**Failure Signatures**: Models may fail by misinterpreting visual information, applying incorrect physics principles, or making calculation errors in intermediate steps. Some models may achieve correct final answers through flawed reasoning chains.

**First 3 Experiments**:
1. Evaluate a single model on a subset of 50 problems to establish baseline performance metrics.
2. Compare performance with and without visual information on identical problems to quantify multimodal benefits.
3. Analyze reasoning chains for 20 randomly selected problems to identify common failure patterns and reasoning strengths.

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark's exclusive focus on Chinese-language physics problems limits generalizability to other educational contexts and languages.
- The reliance on GPT-4o for problem generation and solution verification introduces potential biases in problem selection and solution accuracy.
- The benchmark is limited to high-school level physics, potentially missing more complex university-level concepts that could better stress-test multimodal reasoning capabilities.

## Confidence
**High Confidence**: The finding that visual information significantly improves performance across all evaluated models is well-supported by empirical results showing consistent accuracy improvements when images are provided.

**Medium Confidence**: The comparative performance rankings of different models should be interpreted cautiously, as performance may vary with different problem sets or evaluation conditions.

**Low Confidence**: The claim about models arriving at correct answers through flawed reasoning requires more systematic analysis to distinguish between genuine reasoning flaws and alternative valid solution paths.

## Next Checks
1. **Cross-linguistic validation**: Replicate the benchmark evaluation using translated physics problems in multiple languages to assess whether observed performance patterns hold across linguistic contexts.

2. **Expert verification**: Have physics education experts independently verify a random sample of 100+ problems and solutions to assess the accuracy and quality of the GPT-4o-generated content.

3. **University-level extension**: Develop and evaluate an expanded benchmark including university-level physics problems to determine whether observed multimodal reasoning patterns persist at higher complexity levels.