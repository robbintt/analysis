---
ver: rpa2
title: 'APRIL: Annotations for Policy evaluation with Reliable Inference from LLMs'
arxiv_id: '2511.17818'
source_url: https://arxiv.org/abs/2511.17818
tags:
- annotations
- counterfactual
- dataset
- behavior
- potassium
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using LLMs to generate counterfactual annotations
  for off-policy evaluation (OPE) in healthcare, addressing the scalability challenge
  of expert-labeled annotations. The method guides LLMs to predict clinical features
  under alternate treatments using domain knowledge, then transforms these predictions
  into counterfactual annotations via known reward functions.
---

# APRIL: Annotations for Policy evaluation with Reliable Inference from LLMs
## Quick Facts
- arXiv ID: 2511.17818
- Source URL: https://arxiv.org/abs/2511.17818
- Reference count: 30
- LLM-generated counterfactual annotations improve OPE estimates by up to 83% in high-distribution-shift settings

## Executive Summary
This paper introduces APRIL, a method that leverages large language models (LLMs) to generate counterfactual annotations for off-policy evaluation (OPE) in healthcare. The approach addresses the scalability challenge of obtaining expert-labeled annotations by using LLMs to predict clinical features under alternate treatments, then transforming these predictions into counterfactual annotations via known reward functions. Evaluated on MIMIC-IV data for IV potassium and sodium repletion, APRIL significantly improves OPE accuracy, particularly in scenarios with high distribution shift. The method demonstrates that LLM-generated counterfactual annotations offer a scalable solution for reliable policy evaluation in clinical settings.

## Method Summary
APRIL uses LLMs to generate counterfactual annotations by predicting clinical features under alternate treatment scenarios. The process involves guiding LLMs with domain knowledge to estimate how clinical features would have evolved under different interventions, then converting these predictions into counterfactual annotations using predefined reward functions. The method was evaluated using a logistic bandit model on MIMIC-IV data, comparing OPE estimates with and without LLM-generated counterfactual annotations across multiple LLM models (o1, o3-mini, gpt-4o-mini, Gemini 1.5, Claude 3.7). The approach specifically targets high-distribution-shift scenarios where traditional OPE methods struggle, and quantifies diminishing returns with additional annotations through marginal entropy over the action space.

## Key Results
- Multiple LLM models (o1, o3-mini, gpt-4o-mini, Gemini 1.5, Claude 3.7) achieved comparable performance in predicting downstream lab values
- LLM-generated counterfactual annotations reduced RMSE by up to 83% in high-distribution-shift settings
- Diminishing returns with additional annotations were observed and quantifiable via marginal entropy over the action space

## Why This Works (Mechanism)
The method works by leveraging LLMs' ability to reason about counterfactual scenarios using domain knowledge. When LLMs predict clinical features under alternate treatments, they effectively simulate what would have happened if different decisions were made, providing the counterfactual information needed for accurate OPE. This is particularly valuable in healthcare where obtaining expert-labeled counterfactual annotations is prohibitively expensive and time-consuming. The known reward functions then translate these LLM predictions into actionable counterfactual annotations that can be incorporated into OPE estimation, bridging the gap between LLM reasoning capabilities and concrete evaluation metrics.

## Foundational Learning
- Counterfactual reasoning in healthcare: Understanding how outcomes would change under different treatments is essential for safe policy evaluation, but requires expert annotations that are costly to obtain
- Off-policy evaluation (OPE) challenges: Traditional OPE methods struggle with high distribution shift, where the evaluation policy differs significantly from the behavior policy
- LLM reasoning capabilities: Modern LLMs can leverage domain knowledge to make informed predictions about clinical scenarios, enabling scalable counterfactual generation
- Reward function design: Known reward functions are needed to convert LLM predictions into actionable counterfactual annotations for OPE
- Marginal entropy quantification: Measuring entropy over the action space helps identify when additional annotations provide diminishing returns

## Architecture Onboarding
**Component Map:** Clinical data -> Feature extraction -> LLM counterfactual prediction -> Reward function transformation -> OPE estimation
**Critical Path:** The core workflow processes each admission through feature extraction, generates counterfactual predictions using LLMs, applies reward functions to create annotations, then feeds these into OPE estimation
**Design Tradeoffs:** The method trades potential LLM prediction errors for scalability gains, accepting that perfect expert annotations are infeasible while LLM-based approximations can still significantly improve OPE
**Failure Signatures:** Performance degrades when distribution shift is extreme beyond LLM capabilities, when reward functions poorly capture true clinical value, or when insufficient domain knowledge is provided to guide LLM reasoning
**First Experiments:** 1) Test LLM counterfactual prediction accuracy on held-out validation data 2) Evaluate OPE performance with varying numbers of annotations per admission 3) Compare different LLM models' ability to generate useful counterfactual annotations

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single clinical dataset (MIMIC-IV) and two specific interventions (potassium and sodium repletion)
- Performance gains most pronounced in high-distribution-shift settings; effectiveness in low-shift scenarios unclear
- Small sample size (5,752 admissions) and limited annotation quantities (4, 8, or 12) leave questions about optimal annotation quantities

## Confidence
- Core methodology and baseline OPE improvements: High
- Scalability claims: Medium
- Diminishing returns with additional annotations: Medium

## Next Checks
1. Test the approach across multiple clinical datasets and intervention types to assess generalizability beyond electrolyte management
2. Evaluate computational costs and latency for LLM-based annotation generation at scale in production environments
3. Conduct a sensitivity analysis with varying numbers of counterfactual annotations (beyond 4, 8, 12) to determine optimal quantities for different clinical scenarios