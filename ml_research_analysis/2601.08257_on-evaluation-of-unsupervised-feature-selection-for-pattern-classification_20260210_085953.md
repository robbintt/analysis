---
ver: rpa2
title: On Evaluation of Unsupervised Feature Selection for Pattern Classification
arxiv_id: '2601.08257'
source_url: https://arxiv.org/abs/2601.08257
tags:
- multi-label
- feature
- methods
- label
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the limitations of single-label evaluation
  in unsupervised feature selection (UFS), where arbitrary label selection from multi-label
  datasets can lead to biased assessments of feature subset quality. The authors propose
  a multi-label evaluation framework that better reflects the structural and discriminative
  properties of selected features in real-world multi-label data scenarios.
---

# On Evaluation of Unsupervised Feature Selection for Pattern Classification

## Quick Facts
- arXiv ID: 2601.08257
- Source URL: https://arxiv.org/abs/2601.08257
- Reference count: 35
- Primary result: Multi-label evaluation framework reveals evaluation bias in single-label feature selection assessments

## Executive Summary
This study challenges the conventional single-label evaluation paradigm for unsupervised feature selection (UFS) by proposing a multi-label evaluation framework. The authors demonstrate that arbitrary label selection from multi-label datasets can lead to biased assessments of feature subset quality. Through systematic experiments on 21 multi-label datasets comparing six representative UFS methods, they show that method rankings differ significantly between single-label and multi-label evaluation settings. EMUFS generally achieves the best or near-best performance across datasets, ranking first in Multi-Label Accuracy, while classical methods like MCFS perform competitively or superiorly in multi-label settings.

## Method Summary
The paper evaluates six UFS methods (EMUFS, CNAFS, EGCFS, FSDK, MCFS, RUSLP) using 21 multi-label datasets. For each dataset and method, top-k features are selected in a fully unsupervised manner. The selected features are then used with an ML-kNN classifier (k=10 neighbors) for multi-label prediction. Evaluation employs four multi-label measures: Hamming Loss, Ranking Loss, One-Error, and Multi-Label Accuracy. The experimental setup uses 80/20 train-test splits repeated 10 times. The key intervention is the shift from single-label to multi-label evaluation, while all other components remain standard.

## Key Results
- Method rankings differ significantly between single-label and multi-label evaluation settings
- EMUFS generally achieves the best or near-best performance across datasets
- MCFS demonstrates competitive or superior performance under multi-label evaluation
- Performance rankings from single-label evaluation do not consistently translate to multi-label settings

## Why This Works (Mechanism)

### Mechanism 1
Single-label evaluation introduces arbitrary variance that misrepresents true feature subset quality. When a multi-label dataset is reduced to single-label by selecting one label, the discarded labels are unknown to the observer. The reported performance becomes contingent on which label was arbitrarily chosen, not on the intrinsic structural representation captured by the features. Real-world data inherently exhibits multi-label associations; single-label datasets are instantiations of an implicit selection process.

### Mechanism 2
Multi-label measures collectively capture inter-label dependencies that single-label accuracy cannot reflect. The four measures—Hamming Loss, Ranking Loss, One-Error, and Multi-Label Accuracy—assess different aspects of label prediction quality. Together they reveal structural generalization that feature subsets preserving intrinsic data structure will generalize across multiple label prediction tasks simultaneously.

### Mechanism 3
Graph-based and entropy-maximizing methods retain competitive performance under multi-label evaluation because they optimize for global structure rather than label-specific discriminability. Methods like MCFS preserve local manifold structures through affinity graphs; EMUFS maximizes joint entropy for intrinsic discriminability. These approaches capture structural properties that remain informative across multiple labels, whereas methods optimized for single-label discrimination may overfit to the selected label.

## Foundational Learning

- Concept: Multi-label classification evaluation metrics (Hamming Loss, Ranking Loss, One-Error, Multi-Label Accuracy/Jaccard)
  - Why needed here: Understanding these four metrics is essential to interpret Table 1 and Appendix results. Each metric measures a different failure mode.
  - Quick check question: Can you explain why a method could have low Hamming Loss but high One-Error?

- Concept: Unsupervised feature selection paradigms (graph-based, information-theoretic, evolutionary)
  - Why needed here: The six compared methods span these categories. Knowing which paradigm each represents helps interpret why performance differs across evaluation settings.
  - Quick check question: Which paradigm does MCFS belong to, and what structural property does it preserve?

- Concept: Label cardinality and label dependency in multi-label data
  - Why needed here: The 21 datasets vary in label cardinality (average labels per instance) and feature dimensionality. Understanding these properties explains why some methods perform better on certain datasets.
  - Quick check question: Why would high label cardinality make single-label evaluation particularly unreliable?

## Architecture Onboarding

- Component map: UFS method → top-k feature selection → ML-kNN classifier → multi-label metrics
- Critical path: The evaluation paradigm change (single-label → multi-label) is the key intervention; all other components remain standard.
- Design tradeoffs:
  - Using ML-kNN as downstream classifier: Simple and interpretable, but results may differ with other multi-label classifiers
  - Fixed k for feature selection: Paper does not specify how k was chosen; varying k could reveal sensitivity
  - 21 datasets provide breadth but limited depth per domain; domain-specific patterns may be obscured
- Failure signatures:
  - Method performs well on single-label but poorly on multi-label → likely overfitting to selected label's discriminative features
  - High variance across 10 repetitions → feature selection is unstable on that dataset
  - Large discrepancy between Hamming Loss and One-Error → method captures per-label patterns but fails at ranking relevant labels highest
- First 3 experiments:
  1. Reproduce Table 1 on 3 representative datasets (high, medium, low label cardinality) to validate evaluation pipeline before scaling
  2. Vary the number of selected features k (e.g., 10%, 20%, 30% of original features) to assess robustness of method rankings to feature subset size
  3. Replace ML-kNN with a different multi-label classifier (e.g., Binary Relevance with SVM) to test whether findings generalize across downstream learners

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several emerge from the analysis:

- Does the relative superiority of specific UFS methods persist when evaluated using diverse multi-label classifiers beyond ML-kNN?
- To what extent does label cardinality influence the discrepancy between single-label and multi-label evaluation rankings?
- Why do classical graph-based methods exhibit improved robustness in multi-label settings compared to modern sparse-representation methods?
- How does the optimal number of selected features differ between single-label and multi-label evaluation paradigms?

## Limitations
- Hyperparameter choices (k selection, method-specific parameters) and preprocessing details are not fully specified
- Results may be biased by the specific inductive biases of the ML-kNN algorithm
- Limited analysis of how label cardinality affects the discrepancy between evaluation paradigms
- No theoretical explanation for why classical methods perform better under multi-label evaluation

## Confidence
- Multi-label evaluation reveals evaluation bias: High
- EMUFS performs best under multi-label evaluation: Medium (limited to ML-kNN classifier)
- Classical methods competitive under multi-label evaluation: Medium (depends on k and evaluation metric weighting)

## Next Checks
1. Replicate Table 1 results on three representative datasets (high/medium/low label cardinality) to verify the evaluation pipeline before scaling to all 21 datasets
2. Conduct sensitivity analysis by varying k (10%, 20%, 30% of original features) to determine if method rankings remain stable across different feature subset sizes
3. Replace ML-kNN with an alternative multi-label classifier (e.g., Binary Relevance with SVM) to test whether EMUFS's superior performance generalizes beyond the specific downstream learner