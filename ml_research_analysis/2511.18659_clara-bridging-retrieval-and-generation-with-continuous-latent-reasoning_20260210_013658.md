---
ver: rpa2
title: 'CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning'
arxiv_id: '2511.18659'
source_url: https://arxiv.org/abs/2511.18659
tags:
- document
- retrieval
- generation
- compression
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLaRa (Continuous Latent Reasoning) introduces a unified framework
  for retrieval-augmented generation by encoding documents into shared continuous
  latent representations. It uses SCP (Salient Compressor Pretraining) to synthesize
  QA and paraphrase supervision for compressing documents into semantically rich memory
  tokens.
---

# CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning

## Quick Facts
- arXiv ID: 2511.18659
- Source URL: https://arxiv.org/abs/2511.18659
- Authors: Jie He; Richard He Bai; Sinead Williamson; Jeff Z. Pan; Navdeep Jaitly; Yizhe Zhang
- Reference count: 40
- Primary result: State-of-the-art compression and reranking performance on NQ, HotpotQA, MuSiQue, and 2Wiki benchmarks, often surpassing text-based baselines even at 16Ã— compression

## Executive Summary
CLaRa (Continuous Latent Reasoning) introduces a unified framework for retrieval-augmented generation that encodes documents into shared continuous latent representations. The system uses SCP (Salient Compressor Pretraining) to synthesize QA and paraphrase supervision for compressing documents into semantically rich memory tokens. CLaRa then jointly optimizes a retriever and generator through contrastive learning and supervised fine-tuning.

## Method Summary
CLaRa uses SCP (Salient Compressor Pretraining) to compress documents into continuous latent memory tokens through QA and paraphrase synthesis supervision. The framework jointly optimizes a retriever and generator using contrastive learning and supervised fine-tuning. The memory tokens are continuous embeddings that enable dense retrieval, while a retriever-generator interaction module facilitates token-level memory generation during