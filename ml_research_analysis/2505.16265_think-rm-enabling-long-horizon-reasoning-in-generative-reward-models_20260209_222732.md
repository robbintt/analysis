---
ver: rpa2
title: 'Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models'
arxiv_id: '2505.16265'
source_url: https://arxiv.org/abs/2505.16265
tags:
- reasoning
- reward
- arxiv
- rlhf
- think-rm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Think-RM, a training framework that enables
  long-horizon reasoning in generative reward models (GenRMs) by modeling an internal
  thinking process. The approach consists of two key stages: first, supervised fine-tuning
  (SFT) on long chain-of-thought (CoT) trajectories generated by a pretrained reasoning
  model, followed by rule-based reinforcement learning (RL) to refine the reasoning
  process.'
---

# Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models

## Quick Facts
- **arXiv ID**: 2505.16265
- **Source URL**: https://arxiv.org/abs/2505.16265
- **Reference count**: 40
- **Primary result**: Think-RM achieves 8% improvement over baselines on RM-Bench by enabling long-horizon reasoning in generative reward models.

## Executive Summary
Think-RM introduces a training framework that enables long-horizon reasoning in generative reward models (GenRMs) by modeling an internal thinking process. The approach consists of two key stages: first, supervised fine-tuning (SFT) on long chain-of-thought (CoT) trajectories generated by a pretrained reasoning model, followed by rule-based reinforcement learning (RL) to refine the reasoning process. Think-RM generates flexible, self-guided reasoning traces supporting advanced capabilities like self-reflection, hypothetical reasoning, and divergent reasoning, extending reasoning from hundreds to thousands of tokens. The framework also proposes a novel pairwise RLHF pipeline that directly optimizes policies using pairwise preference rewards, eliminating the need for pointwise reward conversion.

## Method Summary
The method involves four main stages: (1) generate M=10 CoT trajectories per preference instance using QwQ-32B, filtering to keep the longest correct trajectory; (2) SFT warmup on Llama-3.1-8B-Instruct using filtered long-CoT data; (3) rule-based RL refinement using GRPO with accuracy-based rewards; and (4) pairwise RLHF pipeline using trained Think-RM to construct preference strength matrices. The framework trains binary and multiclass variants, with binary achieving higher accuracy with less data (6K vs. 4K examples).

## Key Results
- Think-RM achieves 8% improvement over Bradley-Terry reward models and vertically scaled GenRMs on RM-Bench
- Trained with both SFT and RL, Think-RMs significantly outperform SFT-only counterparts while reducing average response length
- Pairwise RLHF with Think-RM achieves higher win rates than pointwise RLHF with BT RM (47.20 vs 33.14 WR)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Long-horizon internal reasoning traces enable more accurate preference judgments than shallow, vertically-scaled reasoning paths.
- **Mechanism**: Think-RM extends a single reasoning trajectory to thousands of tokens rather than generating multiple independent short CoT paths and aggregating. This allows the model to track long-term dependencies, perform self-reflection, and catch subtle issues that shallow paths miss.
- **Core assumption**: Longer reasoning traces naturally incorporate more self-correction and analytical depth when the model has been trained to produce them.
- **Evidence anchors**: [abstract] "Think-RM generates flexible, self-guided reasoning traces supporting advanced capabilities like self-reflection, hypothetical reasoning, and divergent reasoning, extending reasoning from hundreds to thousands of tokens." [section 1] "Each path generated by existing GenRM is typically shallow (limited to a few hundred tokens) making it difficult for any single path to fully capture complex or subtle implicit context."

### Mechanism 2
- **Claim**: Two-stage training (SFT on longest correct CoT, then rule-based RL) yields more efficient and accurate reasoning than SFT alone.
- **Mechanism**: SFT on long CoT trajectories (filtered to keep the longest correct one per instance) injects reasoning patterns from a pretrained reasoning model. Rule-based RL then refines this by eliminating redundant reasoning steps while preserving correctness.
- **Core assumption**: The pretrained reasoning model's outputs, though verbose, contain valid reasoning patterns that can be compressed through RL.
- **Evidence anchors**: [section 3.2] "We select the trajectory with the longest R̂ᵢⱼ among those satisfying ŝᵢⱼ = sᵢ for each instance." [section 4.2.1] "Think-RMs, trained with both SFT and RL, significantly outperform their SFT-only counterparts while also reducing average response length."

### Mechanism 3
- **Claim**: Direct pairwise RLHF using a preference strength matrix eliminates information loss from pointwise reward conversion.
- **Mechanism**: Rather than converting pairwise preferences to scalar rewards, Think-RM constructs a skew-symmetric matrix D where dᵢⱼ encodes preference strength. Advantages are computed directly from this matrix, preserving relative preference information.
- **Core assumption**: Pairwise preference signals contain richer information than pointwise scalars, and reasoning length inversely correlates with confidence in binary judgments.
- **Evidence anchors**: [section 3.4] "This formulation ensures that longer reasoning chains, which often indicate more ambiguous comparisons, result in smaller reward differences." [table 4] Pairwise RLHF with Think-RM achieves higher win rates than pointwise RLHF with BT RM (47.20 vs 33.14 WR).

## Foundational Learning

- **Concept: Bradley-Terry vs. Generative Reward Models**
  - Why needed here: Think-RM is positioned as an alternative to BT RMs; understanding BT's limitations motivates the generative approach.
  - Quick check question: Can you explain why a scalar reward model might fail on out-of-distribution prompts where a CoT-generating model succeeds?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: Think-RM's rule-based RL stage uses GRPO; understanding how group-based advantage estimation works is essential.
  - Quick check question: How does GRPO compute advantages differently from standard PPO with GAE?

- **Concept: Skew-symmetric matrices in preference modeling**
  - Why needed here: The pairwise RLHF pipeline constructs matrix D with properties dᵢⱼ = -dⱼᵢ.
  - Quick check question: If you have G responses per prompt, what is the dimensionality of D, and which entries correspond to responses from different prompts?

## Architecture Onboarding

- **Component map**: Preference dataset → QwQ-32B generates M CoT trajectories → Filter to longest correct trajectory → Llama-3.1-8B-Instruct SFT → GRPO RL refinement → Pairwise RLHF policy update
- **Critical path**: The quality of Stage 1's CoT filtering directly determines Stage 2's effectiveness; noisy or incorrect "longest" trajectories will propagate errors.
- **Design tradeoffs**: Longer CoT selection improves accuracy but increases inference cost (~10% length increase for ~3% accuracy gain). Binary vs. multiclass output: Binary achieves higher accuracy with less data (6K vs. 4K examples). Group size G: Larger G (8 for RL, 4 for RLHF) improves advantage estimation but increases compute.
- **Failure signatures**: Systematic length explosion after SFT without RL refinement. RM accuracy degrading on OOD tasks despite strong ID performance. Pairwise RLHF producing shorter but lower-quality responses.
- **First 3 experiments**: 1) Reproduce Stage 1 filtering: Generate M=10 CoT trajectories for 100 preference pairs, implement longest-correct selection, verify trajectory quality manually. 2) Ablate SFT vs. SFT+RL: Train Think-RM with SFT only vs. full pipeline on same data, compare on RewardBench "Chat Hard" subset. 3) Pairwise vs. pointwise RLHF: Using same backbone, compare policy trained with Think-RM pairwise pipeline vs. BT RM pointwise pipeline on AlpacaEval2.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the relative performance gain of Think-RM over vertically scaled GenRMs persist when using backbone models significantly larger than 8B parameters? The authors acknowledge using Llama-3.1-8B-Instruct because "integrating a larger one into a full pairwise RLHF pipeline is prohibitively expensive."
- **Open Question 2**: Can the high inference cost of Think-RM be reduced without sacrificing the accuracy gains derived from long-horizon reasoning? Tables 1-3 show Think-RM generates significantly longer outputs (avg. 1000-1700 tokens) compared to baselines (avg. 100-400 tokens) to achieve higher accuracy.
- **Open Question 3**: Is the "longest correct trajectory" selection strategy robust against reinforcing subtle reasoning errors or hallucinations present in the teacher model? The method relies on QwQ-32B to generate trajectories, assuming that the longest path provides the best training signal.

## Limitations

- Think-RM's effectiveness heavily depends on the quality of the pretrained reasoning model (QwQ-32B) used for CoT generation; poor reasoning quality will amplify errors through SFT and RL stages.
- The pairwise RLHF pipeline's skew-symmetric matrix approach assumes preference strength can be reliably encoded through reasoning length, which may not generalize across all domains.
- The framework's performance gains come at significant computational cost - generating 10 CoT trajectories per instance and running pairwise comparisons creates substantial inference overhead.

## Confidence

- **High confidence**: The two-stage training approach (SFT + rule-based RL) improving accuracy while reducing length is well-supported by ablation studies (Section 4.2.1).
- **Medium confidence**: The 8% improvement over baselines on RM-Bench depends on proper implementation of the QwQ-32B CoT generation and filtering pipeline.
- **Medium confidence**: The end-policy performance gains on AlpacaEval2 are promising but may not generalize to all RLHF applications.

## Next Checks

1. **Ablation on CoT generation quality**: Train Think-RM with varying numbers of CoT trajectories per instance (M=1, 5, 10) and different filtering criteria (longest vs. highest accuracy) to quantify sensitivity to Stage 1 quality.

2. **Cross-domain generalization**: Evaluate Think-RM on preference tasks outside the HelpSteer domain (e.g., academic paper preferences, code quality judgments) to test robustness of the long-horizon reasoning approach.

3. **Computational overhead analysis**: Measure wall-clock time and token generation costs for Think-RM vs. BT RM baselines across different batch sizes and inference configurations to quantify practical deployment tradeoffs.