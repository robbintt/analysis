---
ver: rpa2
title: OCR-Enhanced Multimodal ASR Can Read While Listening
arxiv_id: '2601.18393'
source_url: https://arxiv.org/abs/2601.18393
tags:
- visual
- speech
- whisper
- audio
- donut-whisper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Donut-Whisper, an audio-visual ASR model
  that combines Whisper's speech encoder with Donut's OCR encoder via a dual-encoder
  architecture with cross-attention fusion. The authors propose a lightweight knowledge
  distillation scheme where the multimodal model teaches an audio-only model to achieve
  better performance.
---

# OCR-Enhanced Multimodal ASR Can Read While Listening

## Quick Facts
- arXiv ID: 2601.18393
- Source URL: https://arxiv.org/abs/2601.18393
- Reference count: 0
- Introduces Donut-Whisper, achieving 5.75% WER reduction (English) and 16.5% CER reduction (Chinese) over Whisper-large-V3 baseline.

## Executive Summary
This paper presents Donut-Whisper, an audio-visual ASR model that fuses speech and subtitle text through a dual-encoder architecture with cross-attention. By combining Whisper's speech encoder with Donut's OCR encoder, the model leverages visual text as a complementary prior to correct ASR errors. The authors also propose a lightweight knowledge distillation scheme where the multimodal teacher improves an audio-only student without requiring in-domain fine-tuning. A new multilingual dataset (33h English, 57h Chinese) based on movie clips with embedded subtitles enables evaluation.

## Method Summary
Donut-Whisper combines Whisper-large-V3 and Donut-base encoders via a sliding-window Q-Former and cross-attention fusion module. Audio features are locally aggregated before attending to projected visual features. The decoder uses LoRA for efficient fine-tuning. Knowledge distillation transfers multimodal priors to an audio-only Whisper-large-V3 student using temperature-scaled logits and combined CE/KL losses. The custom dataset extracts middle frames from subtitle intervals paired with aligned audio clips.

## Key Results
- Donut-Whisper achieves 5.75% absolute WER reduction on English and 16.5% absolute CER reduction on Chinese compared to Whisper-large-V3 baseline.
- Cross-attention fusion with visual text provides within-line alignment and character-shape priors that help revise both ASR and OCR predictions.
- Knowledge distillation from multimodal teacher to audio-only student significantly reduces error rates without in-domain fine-tuning.

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Attention Fusion for OCR-Enhanced Correction
Visual text provides complementary priors that correct ASR errors through attention-based feature fusion. Audio features serve as queries attending to linearly-projected visual features, allowing the decoder to incorporate textual shape and spelling priors from OCR into speech recognition.

### Mechanism 2: Knowledge Distillation from Multimodal Teacher to Audio-Only Student
A multimodal model transfers visual-grounded knowledge to an audio-only model through soft label distillation. The multimodal teacher produces temperature-scaled logits, and the student learns via combined cross-entropy and KL divergence loss, transferring "dark knowledge" about secondary candidates.

### Mechanism 3: Sliding-Window Q-Former for Local Temporal Aggregation
Local temporal aggregation before cross-modal fusion improves audio representation quality by reducing attention diffusion. Audio feature sequences are divided into overlapping windows, each processed by a Q-Former to aggregate local patterns before cross-attention.

## Foundational Learning

- **Concept: Cross-Attention Mechanism**
  - Why needed here: Implements the core fusion strategy where audio queries "read" visual text features.
  - Quick check question: Given audio features Ha and projected visual features Hv, what are Q, K, and V in the cross-attention formula (Eq. 3)?

- **Concept: Q-Former (Query Former)**
  - Why needed here: Aggregates variable-length sequences into fixed-length query representations for cross-modal alignment.
  - Quick check question: How does a Q-Former with learnable queries differ from a standard transformer encoder when compressing a sequence?

- **Concept: Knowledge Distillation with Temperature Scaling**
  - Why needed here: Enables soft label transfer that preserves uncertainty information beyond argmax predictions.
  - Quick check question: Why would temperature τ > 1 in softmax help transfer "dark knowledge" about secondary candidates?

## Architecture Onboarding

- **Component map**: Donut encoder (visual) -> Linear projection -> Sliding-window Q-Former -> Cross-attention -> Whisper decoder (LoRA)
- **Critical path**: Extract middle frame from subtitle interval [ts, te] as visual input → Extract aligned audio segment as auditory input → Donut encoder → Hv; Whisper encoder → Ha → Linear project Hv; Sliding-window Q-Former on Ha → queries → Cross-attention: queries attend to projected Hv → Fused features → Whisper decoder → transcript
- **Design tradeoffs**: Window size 64 optimal for movie dialogue (domain-dependent); Frozen encoders vs full fine-tuning (efficiency vs capacity); Teacher size 0.15B vs student 1.5B (smaller multimodal teacher can improve larger unimodal student)
- **Failure signatures**: No WER/CER improvement → Check temporal alignment between audio and subtitle frames; Distillation not helping → Verify identical vocabulary/tokenizer between teacher and student; Sliding-window ineffective → Window size mismatch with speech cadence
- **First 3 experiments**: 1) Run Donut-base and Whisper-large-V3 unimodal baselines on your test set to establish reference WER/CER. 2) Ablate fusion types (Linear, Q-Former, Sliding-Window Q-Former + Cross-Attention) to isolate each component's contribution. 3) Sweep sliding-window sizes (32, 64, 128, 256) on validation data to find optimal temporal granularity for your domain.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can iterative self-training with confidence-based filtering on unlabeled corpora further improve performance in low-resource settings?
- **Open Question 2**: Do adaptive sliding-window sizes and learnable query vectors improve support for fine-grained timestamp alignment?
- **Open Question 3**: How robust is the Donut-Whisper fusion mechanism to visual noise or the absence of subtitles in video inputs?

## Limitations
- Assumes perfect temporal alignment between audio segments and subtitle frames; no error handling for misalignment or corrupted visual text
- Sliding-window Q-Former optimal size (64) appears domain-specific to movie dialogue without systematic cross-domain validation
- Knowledge distillation assumes identical tokenization between teacher and student models, limiting cross-architecture applicability

## Confidence
- Cross-modal attention fusion effectiveness: High
- Knowledge distillation benefits: Medium
- Sliding-window Q-Former optimality: Low

## Next Checks
1. Systematically introduce subtitle-audio misalignment (0-2 seconds lag/lead) and measure degradation in WER/CER to quantify the model's sensitivity to alignment errors.
2. Evaluate Donut-Whisper on at least three distinct speech domains (conversational speech, lectures, podcasts) with varying speaking rates and background noise to test the sliding-window Q-Former's claimed optimality.
3. Test distillation from multimodal teachers of varying sizes (0.15B, 0.3B, 0.6B) to audio-only students of different scales (0.3B, 0.6B, 1.5B) to establish the relationship between teacher/student size ratios and distillation effectiveness.