---
ver: rpa2
title: Multiple Object Stitching for Unsupervised Representation Learning
arxiv_id: '2506.07364'
source_url: https://arxiv.org/abs/2506.07364
tags:
- uni00000013
- object
- multi-object
- learning
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Multiple Object Stitching (MOS), a method
  to improve unsupervised representation learning for multi-object images. The core
  idea is to synthesize multi-object images by stitching together single-object images,
  thereby creating object-level correspondences without human annotations.
---

# Multiple Object Stitching for Unsupervised Representation Learning

## Quick Facts
- arXiv ID: 2506.07364
- Source URL: https://arxiv.org/abs/2506.07364
- Reference count: 40
- Key outcome: MOS improves unsupervised representation learning for multi-object images by stitching single-object images, achieving state-of-the-art performance on ImageNet, CIFAR, and COCO

## Executive Summary
Multiple Object Stitching (MOS) addresses the challenge of unsupervised representation learning for multi-object images by synthesizing multi-object scenes from single-object images. The method creates object-level correspondences without human annotations by stitching single-object images into multi-object scenes, then training with three dedicated contrastive losses: multiple-to-single, multiple-to-multiple, and single-to-single. MOS significantly outperforms existing methods on both single-object and multi-object datasets while maintaining competitive performance on downstream tasks like object detection and instance segmentation.

## Method Summary
MOS improves unsupervised representation learning by synthesizing multi-object images through stitching single-object images together, creating predetermined object correspondences. The method employs a three-part contrastive objective: Lm2s (multiple-to-single) forces multi-object representations to align with constituent single-object representations, Lm2m (multiple-to-multiple) uses weighted similarity scores based on object overlap to teach partial similarity between multi-object images, and Ls2s (single-to-single) maintains natural-image alignment. The approach uses Vision Transformer architecture, which is less sensitive to artificial stitching boundaries than CNNs. Experiments demonstrate state-of-the-art performance on ImageNet, CIFAR, and COCO datasets.

## Key Results
- Achieves 77.6% linear probe accuracy on ImageNet-1K, outperforming previous unsupervised methods
- Improves COCO object detection APbb by 3.3 points compared to baselines
- Maintains competitive performance on single-object datasets (ImageNet, CIFAR) while significantly improving multi-object representation quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Stitching single-object images creates predetermined object correspondences that eliminate the semantics inconsistency problem in standard image-level contrastive learning on multi-object images.
- **Mechanism:** By constructing multi-object images from known single-object sources, the method provides ground-truth object-level labels without human annotation. This allows the model to learn that multiple distinct objects can coexist in one image.
- **Core assumption:** The model must be able to transfer learned multi-object representations from synthesized images to natural multi-object images despite the artificial stitching boundaries.
- **Evidence anchors:**
  - [abstract] "construct the multi-object images by stitching the single object centric ones, where the objects in the synthesized multi-object images are predetermined"
  - [section 1] "we stitch several single object images into an artificial multi-object image, to construct object-level correspondences between image views without human annotations"
  - [corpus] Related work on object-centric representations supports the broader hypothesis that explicit object decomposition aids downstream tasks

### Mechanism 2
- **Claim:** The three-part contrastive objective (Lm2s, Lm2m, Ls2s) jointly teaches object discrimination, multi-object global structure, and natural-image alignment.
- **Mechanism:** Lm2s forces multi-object representation to align with each constituent single-object representation; Lm2m uses weighted similarity scores based on object overlap; Ls2s prevents domain gap degradation.
- **Core assumption:** The weighted overlap scores in Lm2m correctly capture semantic similarity between partially overlapping multi-object images.
- **Evidence anchors:**
  - [section 3.2] Equations 11-14 define the three losses and their combination
  - [section 4.3.1, Table 4] Ablation shows combining all three losses achieves best results
  - [corpus] Weak direct corpus evidence for this specific triple-loss design

### Mechanism 3
- **Claim:** Vision Transformer architecture is essential because it is less sensitive to artificial stitching boundaries than CNNs.
- **Mechanism:** ViT's global self-attention mechanism treats patches uniformly, allowing it to integrate information across stitched boundaries more naturally than CNNs with local receptive fields.
- **Core assumption:** ViT's insensitivity to stitching artifacts generalizes across datasets and scales.
- **Evidence anchors:**
  - [section 1] "Vision Transformer architecture (ViT) is less sensitive to artificiality produced by the boundary of image stitching"
  - [section 4.1] All experiments use ViT backbones
  - [corpus] No direct comparison to CNN backbones for MOS is provided in corpus

## Foundational Learning

- **Concept: Instance Discrimination (Contrastive Learning)**
  - Why needed here: MOS builds on standard contrastive learning where augmented views of the same image are pulled together. Understanding this baseline is essential to see why it fails on multi-object images.
  - Quick check question: Can you explain why random cropping creates "false positive pairs" in multi-object images but not in single-object images?

- **Concept: Vision Transformer (ViT) Patch Embedding and Self-Attention**
  - Why needed here: MOS relies on ViT