---
ver: rpa2
title: An Optimal Policy for Learning Controllable Dynamics by Exploration
arxiv_id: '2512.20053'
source_url: https://arxiv.org/abs/2512.20053
tags:
- policy
- state
- time
- states
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an optimal policy for learning controllable
  dynamics in unknown environments by exploration. The key idea is to maximize an
  information measure (e.g., predicted information gain) over a constraint set of
  controls that changes over time, preventing premature entry into restrictive states
  like absorbing states or non-backtracking states.
---

# An Optimal Policy for Learning Controllable Dynamics by Exploration

## Quick Facts
- arXiv ID: 2512.20053
- Source URL: https://arxiv.org/abs/2512.20053
- Authors: Peter N. Loxley
- Reference count: 8
- One-line primary result: Proposes a parametric policy that maximizes predicted information gain while avoiding restrictive states to achieve near-optimal exploration in unknown controllable Markov chain environments

## Executive Summary
This paper addresses the problem of learning transition probabilities in controllable Markov chains (CMCs) through exploration. The key insight is that restrictive states (absorbing, transient, non-backtracking) require a non-stationary policy to achieve optimal exploration over finite horizons. The proposed approach parameterizes a control set that temporarily excludes controls leading to restrictive states, then uses cross-entropy method (CEM) optimization to find optimal parameters. The method demonstrates near-optimal exploration performance compared to greedy and random policies across six examples including mazes and gridworlds.

## Method Summary
The method learns CMC transition probabilities by maximizing predicted information gain (PIG) over a dynamically constrained control set. A parametric policy defines control availability based on current state, time, and parameters r that encode which states are restrictive and when controls leading to them become available. The cross-entropy method optimizes these parameters by sampling candidates, evaluating their performance via Monte Carlo simulation, and iteratively updating a binomial distribution. Rollout with one-step lookahead can optionally improve the resulting policy. The approach assumes restrictive states can be identified and that the time horizon is finite.

## Key Results
- Parametric policy achieves optimal or near-optimal exploration compared to greedy and random policies in six test environments
- The time-constrained control parameterization successfully prevents premature entry into restrictive states while preserving exploration capability
- Online application of the policy enables dynamic updating of CMC estimates as exploration progresses
- The method handles environments with limited time horizons where stationary policies (like Q-learning) fail to capture necessary temporal structure

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Parameterizing the control set to avoid restrictive states before a time threshold enables near-optimal exploration over finite horizons.
- **Mechanism**: The policy defines U_k(i,r) = U_k(i) − U'(i,r) when current state i can lead to a restrictive state AND time k < t_i. This set-difference removes controls that would prematurely trap the agent, preserving exploration options.
- **Core assumption**: Restrictive states (absorbing, transient, non-backtracking) can be identified or learned from environment structure.
- **Evidence anchors**:
  - [abstract]: "The key idea is to maximize an information measure...over a constraint set of controls that changes over time, preventing premature entry into restrictive states"
  - [section 3.1]: "The set-difference in (8) prevents any controls being applied that would lead the current state directly to a restrictive state before some time period t_i has passed."
  - [corpus]: Weak direct evidence—related papers discuss exploration but not this specific time-constrained control mechanism.
- **Break condition**: If restrictive states cannot be identified a priori or learned online, the parameterization fails to prevent premature trapping.

### Mechanism 2
- **Claim**: Predicted Information Gain (PIG) provides an effective signal for selecting controls that maximize learning progress about transition probabilities.
- **Mechanism**: PIG computes KL divergence between current CMC estimate p̂_i·(u,F) and temporary updates p̂_i·(u,F^{i→j*}) that would result from exploring transition (i→j*). Larger divergence predicts higher information gain from that control.
- **Core assumption**: Information gain measured by KL divergence correlates with actual improvement in CMC knowledge for downstream tasks.
- **Evidence anchors**:
  - [abstract]: "maximize an information measure (e.g., predicted information gain)"
  - [section 3.9]: "The larger the difference between these estimates, the greater the predicted information gain."
  - [corpus]: Limited validation—corpus papers mention exploration strategies but don't specifically test PIG in CMC settings.
- **Break condition**: If PIG poorly correlates with learning progress in specific domains (e.g., sparse reward settings), greedy selection becomes suboptimal.

### Mechanism 3
- **Claim**: Non-stationary policies are essential for optimal finite-horizon exploration, as stationary policies (e.g., from Q-learning) cannot capture temporal structure needed to avoid restrictive states.
- **Mechanism**: The policy makes control availability time-dependent: certain controls only become available after threshold t_i. This enables planning to exhaust transient states before entering irreversible regions.
- **Core assumption**: The time horizon N is finite and known; restrictive states exist that limit future control options.
- **Evidence anchors**:
  - [section 2]: "We show why the occurrence of these states makes a non-stationary policy essential for achieving optimal exploration."
  - [section 4]: "Q-learning is an infinite horizon technique leading to stationary policies, and so is not able to capture the necessary temporal structure and non-stationary policies required for optimal exploration over a finite time horizon."
  - [corpus]: Related work (Storck et al., 1995) applied Q-learning to exploration but produced stationary policies unsuitable for finite horizons.
- **Break condition**: If the horizon is effectively infinite or all states permit backtracking, non-stationary policies may not provide advantage over simpler approaches.

## Foundational Learning

- **Concept: Controllable Markov Chains (CMCs)**
  - Why needed here: The entire framework models environments as CMCs where transition probabilities p_ij(u) depend on both current state i and selected control u. Understanding this control-dependence is essential for the parameterization strategy.
  - Quick check question: How does a CMC differ from a standard Markov chain in terms of how transition probabilities are defined?

- **Concept: Dynamic Programming and Bellman's Curse of Dimensionality**
  - Why needed here: The paper derives optimal policies from DP foundations (Eq. 3-5) but highlights two blockers to exact backward iteration: unknown p_ij(u) and exponential state-space growth from the (i,F) pair.
  - Quick check question: Why does the state-space grow exponentially when including the count tensor F in the state representation?

- **Concept: Cross-Entropy Method (CEM) for Policy Optimization**
  - Why needed here: CEM is the core optimizer for finding parameter r* when exhaustive search becomes intractable. It uses binomial distributions to generate candidates, selects top performers, and updates distribution parameters iteratively.
  - Quick check question: In CEM, what triggers convergence and how does the binomial parameter p_i get updated after each "select" step?

## Architecture Onboarding

- **Component map**:
  - Count tensor F: |S|×|S|×|U| tensor storing F^u_ij transition counts
  - Information measure h(i,u,F): PIG module computing expected KL divergence (O(|S|) complexity)
  - Parameter vector r: encodes (restrictive_states, leading_controls, time_thresholds)
  - Constrained control set U_k(i,r): applies Eq. 8 filter based on current state and time
  - Parametric policy μ_k(i,F,r): greedy argmax over U_k(i,r)
  - CEM optimizer: generates r candidates via binomial sampling, evaluates via simulation
  - Rollout improver: one-step lookahead using base policy for J̃_k+1 approximation

- **Critical path**:
  1. Initialize F=0, set prior α=0.05, define parameter ranges for r
  2. For each CEM iteration: sample r candidates, simulate N-step trajectories, accumulate J_r via Monte Carlo
  3. Select top ε% candidates, update binomial parameters p_i = x̄_i/n_i
  4. Converged r* → optionally apply rollout for sequential improvement
  5. Deploy μ_k(i,F,r*) for exploration or online CMC updating

- **Design tradeoffs**:
  - Parameterization granularity: More parameters capture more restrictive states but increase CEM search space
  - Monte Carlo samples: Higher counts reduce variance in J estimates but increase computation per CEM iteration
  - PIG vs simpler measures: PIG is O(|S|); alternatives exist but may sacrifice accuracy
  - Assumption: Paper assumes restrictive states are few enough to parameterize tractably

- **Failure signatures**:
  - Premature absorption: t_i values learned too low → agent enters restrictive states before exhausting exploration
  - CEM non-convergence: parameter space too large or binomial initialization poor
  - Under-exploration: missing restrictive states from parameterization → greedy behavior dominates
  - High variance: insufficient Monte Carlo samples → unstable J_r estimates mislead CEM

- **First 3 experiments**:
  1. Replicate Example 1 (2-state, 2-control CMC with one absorbing state) using exhaustive search; verify r*=(1,1,7) matches paper and count sample divisions (6,7,6) across transition probabilities
  2. Implement CEM optimization for Example 2 (4-state CMC) and compare missing information reduction against greedy-unrestricted and random baselines
  3. Test online adaptation: learn policy for Example 4 maze, then modify maze exit location and verify CMC estimates update correctly when applying previously-learned parametric policy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is the optimality of the proposed parametric policy to the specific choice of the information measure $h(i,u,F)$?
- Basis in paper: [explicit] The authors state they "intentionally kept the information measure $h(i,u,F)$ abstract," and cite empirical evidence suggesting "different forms for $h$ could be substituted... and lead to qualitatively similar results."
- Why unresolved: This study only verifies results using Predicted Information Gain (PIG); alternative measures remain untested in this specific framework.
- What evidence would resolve it: Comparative analysis of exploration efficiency using alternative measures (e.g., entropy) on the provided CMC examples.

### Open Question 2
- Question: What approximation methods can effectively reduce the computational complexity of evaluating information measures in very large environments?
- Basis in paper: [explicit] The paper notes that for "very large environments with many states this evaluation [Eq. 12] could become prohibitive," implying a need for "some form of approximation."
- Why unresolved: The evaluation focuses on small-scale examples (gridworlds, mazes) where the $O(s)$ complexity is tractable.
- What evidence would resolve it: A formulated approximation for the information measure and its successful application to environments with high-dimensional state spaces.

### Open Question 3
- Question: To what extent does the performance of the parametric policy degrade under imprecise parameter optimization or under-parameterization?
- Basis in paper: [explicit] The authors observe that "Precise optimization might not even be necessary in all cases," noting that under-optimized or under-parameterized models often yielded "good" suboptimal policies.
- Why unresolved: The paper demonstrates optimality using specific resources but does not quantify the trade-off between optimization resource expenditure and exploration quality.
- What evidence would resolve it: A sensitivity analysis comparing fully optimized parameters against those derived from limited optimization resources.

## Limitations
- The framework assumes restrictive states can be identified or learned a priori, which may not hold in complex real-world environments
- CEM optimization requires careful hyperparameter tuning (elite fraction, number of candidates) that was not fully specified
- The information measure relies on KL divergence predictions that may poorly correlate with actual learning progress in sparse reward domains
- The non-stationary policy parameterization becomes increasingly complex as the number of restrictive states grows, potentially making the approach intractable for large state spaces

## Confidence
- **High**: The theoretical framework connecting CMC exploration to optimal control and the necessity of non-stationary policies for finite horizons
- **Medium**: The effectiveness of the parametric control set mechanism and PIG as an exploration signal, based on limited experimental validation
- **Low**: The scalability of the approach to high-dimensional state spaces and the robustness of CEM to initialization

## Next Checks
1. Test the approach on environments with unknown restrictive states to validate online detection and adaptation capabilities
2. Evaluate sensitivity to CEM hyperparameters by systematically varying candidate counts, elite fractions, and initialization strategies
3. Compare PIG against alternative exploration strategies (e.g., UCB, Thompson sampling) in domains with varying reward sparsity