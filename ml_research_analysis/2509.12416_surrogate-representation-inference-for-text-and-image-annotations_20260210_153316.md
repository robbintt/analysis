---
ver: rpa2
title: Surrogate Representation Inference for Text and Image Annotations
arxiv_id: '2509.12416'
source_url: https://arxiv.org/abs/2509.12416
tags:
- human
- annotations
- surrogate
- assumption
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Surrogate Representation Inference (SRI) improves statistical inference
  with machine-annotated unstructured data by exploiting the assumption that human
  coders rely only on the unstructured data (e.g., texts, images) and not on other
  structured variables. This allows SRI to use a low-dimensional "surrogate representation"
  of the unstructured data that fully mediates the relationship between human annotations
  and the predictors of interest.
---

# Surrogate Representation Inference for Text and Image Annotations

## Quick Facts
- arXiv ID: 2509.12416
- Source URL: https://arxiv.org/abs/2509.12416
- Authors: Kentaro Nakamura
- Reference count: 40
- Primary result: Surrogate Representation Inference (SRI) reduces standard errors by over 50% compared to existing bias-correction methods when machine learning accuracy is moderate (70-85%), while maintaining valid statistical inference.

## Executive Summary
Surrogate Representation Inference (SRI) is a statistical framework that improves inference with machine-annotated unstructured data by exploiting the experimental design where human coders rely only on the unstructured data itself, not on other structured variables. The method learns a low-dimensional "surrogate representation" that fully mediates the relationship between human annotations and predictors of interest, enabling more efficient estimation than existing bias-correction approaches. SRI is validated through simulation studies and an empirical application analyzing partisan framing of immigrants in congressional speeches, showing significant efficiency gains while maintaining valid inference even when human annotations contain measurement errors.

## Method Summary
SRI learns a neural network that maps unstructured data (text/images) and covariates to a low-dimensional representation, with two prediction heads: one for the outcome and one for the predictor. The method uses a joint loss function that balances outcome prediction accuracy with enforcing the surrogacy constraint (predictor independence given the representation). Training employs K-fold cross-fitting to prevent overfitting bias, and the final estimator solves an efficient influence function-based estimating equation using the trained nuisance functions.

## Key Results
- Reduces standard errors by over 50% compared to Prediction-Powered Inference and Double-Sampling Learning when machine learning accuracy is moderate (70-85%)
- Maintains valid inference while achieving efficiency gains by restricting the semiparametric tangent space
- Provides unbiased estimates even with non-differential measurement errors in human annotations by leveraging multiple coders
- Validated through simulation studies and an empirical application to congressional speech analysis

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SRI enables valid statistical inference by enforcing that a low-dimensional representation of unstructured data fully mediates the relationship between the predictor of interest and the human annotation.
- **Mechanism:** The architecture learns a representation $W = f(Y, Z)$ such that $T \perp \tilde{L} | W$. This exploits the experimental design where human coders are "blind" to the structured variable $T$ and rely solely on the text $Y$. By preserving this conditional independence in the low-dimensional representation, the target parameter $\Psi_t$ becomes nonparametrically identified via Proposition 1, allowing the model to adjust for the text without directly modeling the high-dimensional raw input.
- **Core assumption:** Assumption 1 (True Coding Rule) and Assumption 6 (Noisy Human Annotations) must hold; specifically, coders must derive $\tilde{L}$ solely from $Y$ without observing $T$.
- **Evidence anchors:**
  - [abstract]: "assumption that human coders rely only on the unstructured data... guaranteed by design"
  - [section]: Section 3.1, Definition 1 (Surrogate Representation).
  - [corpus]: *GenAI-Powered Inference* (GPI) is a related framework leveraging GenAI for unstructured data, but SRI specifically introduces the surrogacy constraint for efficiency.
- **Break condition:** If human coders have side information about the predictor $T$ (e.g., knowing the speaker's party while coding tone), the mediation assumption fails, potentially introducing bias.

### Mechanism 2
- **Claim:** SRI achieves statistically significant efficiency gains (lower standard errors) compared to existing bias-correction methods like Prediction-Powered Inference (PPI) by restricting the semiparametric tangent space.
- **Mechanism:** Standard methods treat the ML prediction as an unconstrained proxy. SRI imposes the surrogacy constraint, which restricts the statistical model $M_{SRI}$ to a subset of the model space $M_{PPI}$ used by existing methods. Because the tangent space $\mathcal{T}_{SRI}$ is a linear subspace of $\mathcal{T}_{PPI}$, the semiparametric efficiency bound is lower, reducing the theoretical minimum variance of the estimator.
- **Core assumption:** The surrogate representation accurately captures the conditional independence; otherwise, the efficiency gain comes at the cost of bias.
- **Evidence anchors:**
  - [abstract]: "reduces standard errors by over 50% when machine learning accuracy is moderate"
  - [section]: Appendix S1 (Theoretical Explanation of Efficiency Gains).
  - [corpus]: *FNBench* discusses robustness to noisy labels in federated learning, but SRI specifically addresses efficiency bounds in inference.
- **Break condition:** If the machine learning predictions are already near-perfect (e.g., >90% accuracy), the efficiency gains diminish as the "residual term" $\Delta$ in the variance decomposition approaches zero.

### Mechanism 3
- **Claim:** SRI corrects for non-differential measurement errors in human annotations using multiple coders, removing the need for gold-standard ground truth.
- **Mechanism:** Using multiple noisy annotations $\tilde{L}^{(1)}$ and $\tilde{L}^{(2)}$, SRI constructs a "surrogate outcome" $M_c$. Under the assumption of independent coding errors, $M_c$ serves as an unbiased proxy for the true label indicator $1\{L=c\}$. This allows the model to identify and correct for human error by solving a system of equations derived from the observed joint distributions.
- **Core assumption:** Assumption 7 (Conditional Independence of Human Annotations), specifically that errors are non-differential and independent across coders.
- **Evidence anchors:**
  - [abstract]: "provides valid inference even when human annotations contain non-differential measurement errors"
  - [section]: Section 4.1 (Proposition 2).
  - [corpus]: *A Bayesian Approach to Segmentation with Noisy Labels* addresses correlated noise; SRI focuses on *non-differential* noise.
- **Break condition:** If coder errors are correlated (e.g., both coders rely on the same ambiguous dictionary definition) or systematic (differential), the identification fails.

## Foundational Learning

- **Concept:** **Semiparametric Efficiency Bounds**
  - **Why needed here:** To understand *why* imposing the surrogacy constraint mathematically guarantees lower variance (Mechanism 2). It explains the trade-off between model flexibility and estimation precision.
  - **Quick check question:** How does restricting the tangent space (model space) affect the CramÃ©r-Rao lower bound for an estimator?

- **Concept:** **Mediation Analysis (Surrogacy)**
  - **Why needed here:** To grasp the core identification strategy. You must understand what it means for text $Y$ to "fully mediate" the effect of $T$ on $\tilde{L}$ to implement the neural network correctly.
  - **Quick check question:** If text $Y$ fully mediates the relationship, does the predictor $T$ provide any additional information about the annotation $\tilde{L}$ once $Y$ is known?

- **Concept:** **Double Machine Learning (DML) & Cross-fitting**
  - **Why needed here:** The paper uses a K-fold cross-fitting procedure to prevent overfitting bias when estimating the nuisance functions (representation, propensity, outcome).
  - **Quick check question:** Why can't we simply train the neural network on the full dataset and then use the same predictions to estimate the target parameter? (Answer: Overfitting leads to bias in the influence function).

## Architecture Onboarding

- **Component map:** Input $(Y, Z)$ -> Shared Encoder $f$ -> Dense(100, ReLU) -> Dense(100, ReLU) -> Dense(50, ReLU) -> Output $W$ -> Two Heads: Outcome Head ($\mu$) and Surrogacy Head ($\rho$)
- **Critical path:**
  1. **Split Data:** Partition data into $K$ folds for cross-fitting.
  2. **Train Network:** For each fold, train the encoder and heads on the training set using the joint loss function (Equation 4).
  3. **Construct Influence Function:** Use the trained models to compute the efficient influence function $\psi_t$ on the validation fold.
  4. **Solve Estimating Equation:** Aggregate across folds to solve for $\hat{\Psi}_t$.
- **Design tradeoffs:**
  - **Hyperparameter $\alpha$:** Balances Outcome Loss vs. Surrogacy Score Loss. High $\alpha$ prioritizes the constraint (efficiency) but might harm representation quality if the constraint is misspecified.
  - **Dimensionality of $W$:** Must be low enough to reduce variance but high enough to satisfy the surrogacy condition (Lemma 1).
- **Failure signatures:**
  - **Exploding Standard Errors:** Check if the "blind coding" assumption holds; violations cause the mediation to fail.
  - **High Bias with Noisy Labels:** Check Assumption 7; if coders use similar external knowledge, errors become correlated.
- **First 3 experiments:**
  1. **Sanity Check (Simulation):** Replicate Figure S2. Vary ML prediction accuracy from 70% to 90% and confirm SRI standard errors are lower than PPI/DSL.
  2. **Ablation Study:** Set $\alpha = 0$ (remove surrogacy loss). Compare standard errors against the full SRI model to quantify the efficiency gain directly from the constraint.
  3. **Robustness to Noise:** Vary human coder accuracy (80%-90%) with multiple coders. Verify if the error correction mechanism (Mechanism 3) maintains unbiased estimates as per Table S2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the SRI framework be adapted to utilize Large Language Model (LLM) annotations that likely violate the independent coding assumption?
- Basis: [explicit] The conclusion explicitly calls for future work to "relax these assumptions or devise prompting strategies that ensure LLM annotations meet the necessary conditions."
- Why unresolved: LLMs share common training data, potentially inducing correlation in errors across different annotation instances, which violates the independent coding requirement (Assumption 7) used to correct measurement error.
- What evidence would resolve it: A modified estimation strategy or specific prompting protocol that provides valid inference despite the dependency structure inherent in LLM-generated labels.

### Open Question 2
- Question: Can SRI be extended to identify text features influenced by a predictor of interest without prior specification by the researcher?
- Basis: [explicit] The conclusion identifies "discovery of text features influenced by predictor of interest" as a key direction for future work.
- Why unresolved: The current framework assumes a deterministic "True Coding Rule" (Assumption 1) exists, requiring researchers to define the outcome concept beforehand.
- What evidence would resolve it: A unified framework that performs data-driven discovery of outcome concepts while maintaining the statistical guarantees of the SRI estimator.

### Open Question 3
- Question: Does the SRI estimator maintain its efficiency and validity when applied to settings where unstructured data serves as a control variable or predictor rather than an outcome?
- Basis: [explicit] The conclusion states the procedure "can be applied to diverse settings and modalities, such as those where unstructured data serves as control or predictors."
- Why unresolved: All formal identification proofs, asymptotic analyses, and empirical applications in the paper focus exclusively on text-as-outcome settings.
- What evidence would resolve it: Formal theoretical derivations and simulation studies confirming SRI's performance in text-as-covariate or text-as-treatment scenarios.

## Limitations
- **Blind coding assumption:** The method requires human coders to rely solely on unstructured data without observing the predictor variable, which may not hold in practice
- **Multiple coder requirement:** Error correction mechanism needs multiple human coders with independent, non-differential errors, which may be costly or impractical
- **Underspecified integration step:** The final estimator integration method for computing $\hat{m}_t(Z_i)$ is not fully detailed in the main algorithm description

## Confidence

- **High confidence:** The theoretical framework for efficiency gains under correct mediation assumptions (Mechanism 2), supported by Appendix S1 and the semiparametric efficiency theory.
- **Medium confidence:** The error-correction mechanism using multiple coders (Mechanism 3), as it relies on strong independence assumptions that may not hold in practice.
- **Medium confidence:** The practical implementation details, as some aspects like the final integration step for computing $\hat{m}_t(Z_i)$ are underspecified in the main algorithm description.

## Next Checks

1. **Assumption verification:** Before applying SRI, explicitly test whether $T \perp \tilde{L} | Y$ holds in your dataset using conditional independence tests or domain knowledge about the coding process.
2. **Cross-fitting implementation:** Verify that your cross-fitting procedure correctly separates training and validation folds to prevent overfitting bias in the nuisance function estimates.
3. **Alpha sensitivity:** Conduct a sensitivity analysis varying $\alpha$ (the trade-off parameter) to ensure the surrogacy constraint improves efficiency without sacrificing representation quality.