---
ver: rpa2
title: Joint Optimization of Cooperation Efficiency and Communication Covertness for
  Target Detection with AUVs
arxiv_id: '2510.18225'
source_url: https://arxiv.org/abs/2510.18225
tags:
- task
- auvs
- underwater
- energy
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficient underwater target
  detection using autonomous underwater vehicles (AUVs) while maintaining communication
  covertness. A hierarchical multi-agent reinforcement learning framework, HMAPPO,
  is proposed to decompose the problem into macro-level task allocation and micro-level
  trajectory and power control.
---

# Joint Optimization of Cooperation Efficiency and Communication Covertness for Target Detection with AUVs

## Quick Facts
- arXiv ID: 2510.18225
- Source URL: https://arxiv.org/abs/2510.18225
- Reference count: 36
- Primary result: Hierarchical Multi-Agent PPO (HMAPPO) achieves 100% target coverage with up to 70% execution time reduction while maintaining communication covertness through KL divergence constraints.

## Executive Summary
This paper addresses the challenge of efficient underwater target detection using autonomous underwater vehicles (AUVs) while maintaining communication covertness. A hierarchical multi-agent reinforcement learning framework, HMAPPO, is proposed to decompose the problem into macro-level task allocation and micro-level trajectory and power control. The framework optimizes collaboration efficiency under covert communication constraints by modeling the macro-level decision-making as an MDP and the micro-level control as a POMDP, solved via proximal policy optimization algorithms. Results show the framework achieves high task completion rates (up to 100%), significantly reduces execution time (by up to 70%), and maintains low KL divergence for covertness, outperforming baseline methods such as MADDPG and random strategies.

## Method Summary
The HMAPPO framework uses hierarchical control with macro-level task allocation and micro-level trajectory/power control under KL divergence covertness constraints. The central AUV uses PPO to select agent subsets for each task, while Worker AUVs use MAPPO to control velocity and power based on local observations. Training employs Centralized Training, Decentralized Execution (CTDE) with a global critic during training but local execution during deployment. The environment includes Thorp acoustic path loss, Lamb-Oseen vortex currents, and a fixed eavesdropper location. Rewards combine covertness penalties, task completion bonuses, distance reduction to targets, and energy penalties.

## Key Results
- Achieves 100% target coverage rate across experiments
- Reduces execution time by up to 70% compared to baseline methods
- Maintains KL divergence below 2ε² threshold while outperforming MADDPG and random baselines

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Decomposition of Time-Scales
- **Claim:** Decomposing the joint optimization problem into macro-level task allocation and micro-level trajectory/power control allows the system to handle long-horizon planning constraints that single-timescale methods cannot.
- **Mechanism:** The architecture decouples decision frequencies, with the Master AUV operating on macro time slots for agent selection (MDP) while Worker AUVs operate on micro time slots for velocity and power adjustment (POMDP).
- **Core assumption:** Task allocation decisions remain valid for the duration of a macro time slot, and micro-level dynamics don't destabilize macro-level preconditions.
- **Evidence anchors:** [abstract] states hierarchical formulation with macro MDP and micro POMDP; [section III.A] defines distinct macro and micro time slots; [corpus] shows dual-timescale hierarchical approaches work for similar cooperative detection tasks.
- **Break condition:** If the environment changes faster than the macro time slot duration, invalidating agent selection before task completion.

### Mechanism 2: Covertness via Information Density Constraint
- **Claim:** Constraining the KL divergence between the eavesdropper's received signal distribution and noise distribution strictly bounds detection probability without requiring complex likelihood-ratio tests during execution.
- **Mechanism:** The method transforms the covertness requirement into a differentiable penalty term using KL divergence constraint D(H₀||H₁) ≤ 2ε², allowing AUVs to learn power control policies that hide in the noise floor.
- **Core assumption:** The eavesdropper uses optimal detectors (LRT) or energy detectors with channel noise following defined statistical models.
- **Evidence anchors:** [section III.D] derives the relationship between covertness constraint and KL divergence; [section V.E] demonstrates the trade-off with Fig. 8 showing performance drop as ε tightens.
- **Break condition:** If an adversary uses non-linear or ML-based detectors not relying on second-order statistics, the KL constraint may not guarantee low detectability.

### Mechanism 3: Centralized Training for Non-Stationarity Mitigation
- **Claim:** Using a global critic during training allows individual agents to learn cooperative policies in a non-stationary environment that would otherwise fail to converge.
- **Mechanism:** The framework uses CTDE paradigm where actors execute based on local observations while critics access global state, providing stable value estimates that account for teammate actions.
- **Core assumption:** The simulation environment accurately reflects real underwater dynamics and noise profiles.
- **Evidence anchors:** [section IV.C] states CTDE mitigates inherent non-stationarity in multi-AUV systems; [abstract] mentions centralized training and decentralized execution; [corpus] shows CTDE is standard for stable convergence in cooperative underwater teams.
- **Break condition:** Significant sim-to-real gap (e.g., unmodeled currents or sensor drift) may cause decentralized policies to fail in real-world partial observability.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) vs. Partially Observable MDP (POMDP)**
  - **Why needed here:** The paper explicitly assigns MDPs to the Master AUV (which sees everything) and POMDPs to the Workers (which see only local data). Understanding this distinction is critical for implementing observation functions correctly.
  - **Quick check question:** Can you explain why the Master AUV is modeled with an MDP while the workers use a POMDP, and what information the worker is missing?

- **Concept: Proximal Policy Optimization (PPO) & Clipping**
  - **Why needed here:** The algorithm relies on PPO's clipped surrogate objective to prevent the policy from changing too drastically during updates, crucial for the long training times required.
  - **Quick check question:** What happens to the policy update if the probability ratio πθ/πθ_old falls outside the range [1-ε, 1+ε]?

- **Concept: Kullback-Leibler (KL) Divergence**
  - **Why needed here:** This is the mathematical core of the "covertness" mechanism. One must understand that minimizing KL divergence minimizes the "distance" between the signal distribution and the noise distribution.
  - **Quick check question:** Does a lower KL divergence D(H₀||H₁) mean the signal is more or less distinguishable from noise?

## Architecture Onboarding

- **Component map:** Environment Wrapper -> Master AUV (MDP PPO) -> Selected Workers (POMDP MAPPO) -> Environment Physics
- **Critical path:** Environment sets task → Master AUV observes global state Sₜ → selects agents Gₜ → Selected agents observe local oₘ → Micro-step loop begins → Agents execute joint action (P,V) → Environment computes next state, covertness penalty, movement cost → Data stored in buffer → Update Actors/Critics via PPO
- **Design tradeoffs:** Efficiency vs. Covertness - the paper identifies a "performance cliff" where if the covertness constraint ε is set too strictly (e.g., < 0.02), task efficiency η drops by >54% because communication becomes too unreliable to coordinate.
- **Failure signatures:** High KL Divergence indicates agents are transmitting too loudly or moving too predictably; Divergent Rewards (macro rises but micro crashes) indicates the Master is selecting agents that are over-exhausting their energy to meet deadlines.
- **First 3 experiments:** 1) Sanity Check (Random Baseline) - run with random action selection to establish floor for task efficiency and KL divergence; 2) Covertness Threshold Scan - train with varying ε values (0.1, 0.05, 0.01) to replicate the "Performance Cliff" curve and determine optimal operating point; 3) Scalability Stress Test - increase AUVs from M=2 to 8 to verify if "Intelligent Selection" behavior emerges.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the framework perform when the eavesdropper's location is unknown or estimated with error, rather than fixed and known?
- **Open Question 2:** Can the "performance cliff" in task efficiency be mitigated when covertness constraints become extremely stringent?
- **Open Question 3:** Is the HMAPPO framework transferable to physical AUVs in real ocean environments?

## Limitations
- The hierarchical decomposition assumes task allocations remain valid for extended periods, but no sensitivity analysis quantifies how rapidly changing environments affect performance.
- The KL divergence covertness constraint assumes specific eavesdropper detection models (LRT/energy detector) and may not generalize to non-linear ML-based detectors.
- The centralized training assumption requires perfect simulation-to-real transfer, which is particularly challenging in underwater environments with complex, unmodeled physics.

## Confidence

**High Confidence** - Claims about hierarchical decomposition improving coordination efficiency are well-supported by explicit mathematical formulation and clear performance gains in execution time (up to 70% reduction). The mechanism of using KL divergence as a covertness constraint is explicitly derived and demonstrated through controlled experiments showing the performance-covertness trade-off.

**Medium Confidence** - Claims about HMAPPO outperforming MADDPG and random baselines are supported by specific quantitative results, but the robustness across different environment configurations and longer time horizons is not fully established. The CTDE paradigm's effectiveness in mitigating non-stationarity is theoretically sound but would benefit from ablation studies.

**Low Confidence** - Claims about the framework's scalability beyond 6 AUVs and generalization to different underwater environments lack empirical validation. The exact reward weight coefficients and sub-target allocation algorithm details are not fully specified, making faithful reproduction challenging.

## Next Checks

1. **Robustness Testing Under Dynamic Environments** - Systematically vary environmental parameters (current speed, acoustic noise profiles) to test whether the macro-level task allocations remain effective when conditions change faster than the macro time slot duration.

2. **Adversarial Detector Generalization** - Test the covertness mechanism against non-linear ML-based eavesdroppers (e.g., neural network detectors) to verify whether the KL divergence constraint provides meaningful protection against advanced detection methods.

3. **Hyperparameter Sensitivity Analysis** - Conduct systematic ablation studies on the reward weight coefficients (ξ₁-ξ₃, φ₁-φ₄) and KL divergence threshold ε to quantify how sensitive performance metrics are to these critical parameters.