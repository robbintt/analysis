---
ver: rpa2
title: 'Daunce: Data Attribution through Uncertainty Estimation'
arxiv_id: '2505.23223'
source_url: https://arxiv.org/abs/2505.23223
tags:
- training
- data
- daunce
- influence
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DAUNCE, a scalable training data attribution
  method that estimates data influence through uncertainty estimation. The method
  generates multiple perturbed models and computes covariance of per-example losses
  across these models to identify influential training examples.
---

# Daunce: Data Attribution through Uncertainty Estimation

## Quick Facts
- arXiv ID: 2505.23223
- Source URL: https://arxiv.org/abs/2505.23223
- Authors: Xingyuan Pan; Chenlu Ye; Joseph Melkonian; Jiaqi W. Ma; Tong Zhang
- Reference count: 40
- Primary result: DAUNCE achieves more accurate training data attribution than existing TDA methods including TRAK, LoGra, and EKFAC Influence Function on both linear datamodeling score tasks and LLM-scale influential subset removal tasks.

## Executive Summary
This paper introduces DAUNCE, a scalable training data attribution method that estimates data influence through uncertainty estimation. The method generates multiple perturbed models and computes covariance of per-example losses across these models to identify influential training examples. DAUNCE achieves more accurate attribution than existing TDA methods including TRAK, LoGra, and EKFAC Influence Function on linear datamodeling score tasks. For LLM-scale attribution, DAUNCE outperforms LoGra on MATH and IFEval benchmarks in most influential subset removal tasks. The method is also demonstrated to work under black-box access constraints, including the first empirical demonstration of data attribution on proprietary LLMs like OpenAI's GPT models, identifying backdoor-injected training examples through qualitative analysis.

## Method Summary
DAUNCE estimates training data influence by generating K perturbed models through subsampling and perturbing the training objective, then computing covariance of per-example losses across these models. The method approximates influence-function attribution without explicit Hessian inversion by using correlation (not covariance) of losses across perturbed models. For black-box access, DAUNCE uses a simplified objective without the first-order perturbation term, justified by small gradient norms near convergence. The method scales to LLMs using LoRA for parameter-efficient fine-tuning, requiring training K=100 perturbed models for 30 steps each.

## Key Results
- DAUNCE achieves LDS scores of 0.130 on CIFAR-10 vs 0.124 for covariance baseline, demonstrating correlation normalization improves attribution
- On LLM-scale MATH tasks, DAUNCE outperforms LoGra across all removal sizes (500-3000 examples) in subset removal accuracy preservation
- DAUNCE successfully performs black-box data attribution on proprietary GPT models, identifying backdoor-injected training examples through qualitative analysis
- Attribution quality scales exponentially with number of perturbed models K, following y = -0.16·e^(-0.085x) + 0.16

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Covariance of per-example losses across perturbed models approximates influence-function attribution without explicit Hessian inversion
- Mechanism: Generate K models by perturbing the first-order term of a Taylor-expanded objective (Equation 5). For each training-test pair, compute covariance of losses across K models. Theoretical analysis shows this covariance is an approximately unbiased estimator of ∇Li(θ0)⊤H(θ0)−1∇Lj(θ0)/n
- Core assumption: The parameter shift ∆θ = θ̂ − θ0 is small enough that first and second-order Taylor approximations hold
- Evidence anchors:
  - [abstract] "computing the covariance of per-example losses across these models as the attribution score"
  - [section 3.1] Equation 5 and Theorem 1 derivation showing EI(xi, xj) ≈ (1/n)∇Li(θ0)⊤H(θ0)−1∇Lj(θ0)
  - [corpus] "Better Training Data Attribution via Better Inverse Hessian-Vector Products" confirms iHVP is the computational bottleneck DAUNCE avoids
- Break condition: Perturbation magnitude too large → Taylor approximation fails → covariance no longer tracks influence structure

### Mechanism 2
- Claim: Correlation-based normalization mitigates outlier gradients, improving attribution quality over raw covariance
- Mechanism: Replace covariance with Pearson correlation across K models. This is equivalent to unit-normalizing gradients in influence space: H−1/2∇Li / ||H−1/2∇Li||
- Core assumption: High-gradient-magnitude outliers exist and corrupt attribution ranking; unit normalization preserves relative influence structure
- Evidence anchors:
  - [section 4.1] Table 3 shows correlation (0.130 LDS) outperforms covariance (0.124 LDS) for DAUNCE-Hessian
  - [section 6] "corresponds to unit-normalized gradients, which helps mitigate the influence of outlier training examples"
  - [corpus] Corpus papers do not directly validate this normalization choice for DAUNCE specifically
- Break condition: If gradient magnitude is itself informative for attribution, normalization removes signal

### Mechanism 3
- Claim: Black-box access suffices because the first-order perturbation term has small gradient contribution relative to empirical risk
- Mechanism: Under black-box access, use simplified objectives (Table 2) without the ∇Li(θ0)⊤(θ − θ0) term. The removed term's gradient is empirically small near convergence
- Core assumption: Model θ0 is near a local optimum where ∇Li(θ0) is small; loss-only perturbations preserve sufficient covariance structure
- Evidence anchors:
  - [section 5] Figure 3 shows first-order term gradient norm is small relative to full objective
  - [section 5] "removing the first-order term has minimal impact on our method—especially when θ0 is already close to the local optimum"
  - [corpus] "Exploring Training Data Attribution under Limited Access Constraints" addresses similar access scenarios but differs in approach
- Break condition: Early-training checkpoints where gradients are large → simplified objective diverges from full perturbation

## Foundational Learning

- Concept: Influence Functions for TDA
  - Why needed here: DAUNCE is motivated as an efficient approximation to influence-function structure; understanding the target (1/n)∇L⊤H−1∇L clarifies what DAUNCE estimates
  - Quick check question: Can you explain why Hessian inversion is the computational bottleneck for influence functions at LLM scale?

- Concept: Bootstrap Variance Estimation
  - Why needed here: The paper's theoretical motivation draws from bootstrap uncertainty estimation in linear models, where variance across resampled estimators approximates x⊤Σ−1x
  - Quick check question: How does averaging over K independent perturbations reduce variance in the covariance estimator?

- Concept: LoRA for Parameter-Efficient Fine-Tuning
  - Why needed here: LLM-scale experiments use LoRA to make training K perturbed models tractable; understanding low-rank adaptation is necessary for implementation
  - Quick check question: Why does LoRA reduce storage overhead when training multiple perturbed models?

## Architecture Onboarding

- Component map: Subsampling sampler -> K perturbed model trainers -> Loss recorder -> Covariance computer -> Attribution scores
- Critical path: Subsampling ratio r → perturbation scale → number of models K → covariance computation. LDS experiments use K=200, r=0.3; LLM experiments use K=100
- Design tradeoffs:
  - Higher K improves attribution (Figure 6 shows exponential scaling y = −0.16·e−0.085x + 0.16) but linearly increases compute
  - Correlation vs covariance: correlation helps outliers but may discard magnitude signal
  - Subsampling reduces per-model cost but may increase variance
- Failure signatures:
  - LDS near zero: Check that models are actually diverging (loss variance > 0); verify perturbation is applied correctly
  - Black-box attribution fails: Confirm loss API returns correct NLL; verify θ0 is fine-tuned, not base model
  - Scaling plateaus: Figure 6 suggests diminishing returns; K>200 may not justify cost
- First 3 experiments:
  1. Reproduce CIFAR-10 LDS baseline (K=200, r=0.3, ResNet-9) with both correlation and covariance to validate implementation
  2. Ablate K ∈ {50, 100, 200, 400} on a small LLM (e.g., Qwen2.5-7B subset) to confirm exponential scaling holds
  3. Test black-box setting on a local model with held-out gradients before attempting proprietary API access

## Open Questions the Paper Calls Out

- Question: Does the attribution quality of DAUNCE have a theoretical or empirical upper bound as the number of perturbed models K increases?
  - Basis in paper: [explicit] "However, the marginal gains at larger K raise an interesting question of whether the attribution quality of DAUNCE has an upper bound. We leave a deeper investigation of this question to future work."
  - Why unresolved: The paper observes an exponential scaling relationship y = -0.16·e^(-0.085x) + 0.16, but only experiments up to K≈1000. The asymptotic behavior and whether gains plateau completely remain unexplored.
  - What evidence would resolve it: Theoretical analysis of convergence properties plus experiments at much larger K values (e.g., 2000-5000) to identify if/where performance saturates.

- Question: Can the computational overhead of training K perturbed models be further reduced while maintaining attribution accuracy?
  - Basis in paper: [inferred] The method requires training K=100 perturbed models for LLM experiments using LoRA, with each trained for 30 steps. While more scalable than explicit Hessian computation, this still represents significant compute compared to single-pass methods like TRAK.
  - Why unresolved: The paper demonstrates feasibility but does not explore alternatives like training fewer steps per model, using smaller K with variance reduction techniques, or sharing computation across perturbed models.
  - What evidence would resolve it: Ablation studies varying training steps per perturbed model, exploring warm-start initialization from shared checkpoints, or developing theoretical bounds on minimum K required for given accuracy thresholds.

- Question: What causes the task-dependent performance gap where DAUNCE underperforms LoGra on IFEval at small removal sizes (5,000 examples) but excels at larger removals?
  - Basis in paper: [inferred] Figure 2 shows DAUNCE slightly lags LoGra at the 5,000-example removal point on IFEval but surpasses it significantly at larger removals, while consistently outperforming on MATH across all removal sizes.
  - Why unresolved: The paper notes this discrepancy but does not investigate whether it stems from differences in task structure (instruction-following vs. math reasoning), data characteristics, or how influence manifests at different granularities.
  - What evidence would resolve it: Analysis of the distribution of attribution scores for each task, examination of what types of examples are highly-ranked by each method, and experiments with additional task types to identify patterns.

## Limitations

- The method requires training K perturbed models, which still represents significant compute overhead compared to single-pass attribution methods, even with LoRA acceleration
- The black-box access extension relies on assumptions about gradient magnitudes near convergence that may not hold for all architectures or training stages
- The exact score thresholding procedure for baseline comparison is mentioned but not detailed, creating potential reproducibility concerns

## Confidence

**High Confidence**: The core mechanism of using covariance across perturbed models to approximate influence structure is theoretically grounded and empirically validated on controlled vision tasks. The exponential scaling relationship between model count K and attribution quality (Figure 6) provides strong empirical support.

**Medium Confidence**: The black-box access extension and LLM-scale experiments demonstrate practical utility, but rely on assumptions about gradient magnitudes and perturbation effects that require broader validation. The correlation-based normalization shows consistent improvement over covariance, but the theoretical justification for when magnitude information should be preserved remains incomplete.

**Low Confidence**: The exact impact of subsampling ratio r on attribution quality variance, and the method's behavior under non-IID training distributions, are not thoroughly explored. The paper also does not address computational trade-offs in distributed settings or the sensitivity to random seed choices.

## Next Checks

1. **Perturbation Sensitivity Analysis**: Systematically vary perturbation magnitude ξ ∈ [0.1, 1.0] on CIFAR-10 to identify the regime where Taylor approximations break down and attribution quality degrades.

2. **Early-Training Validation**: Apply DAUNCE to models at different training stages (0%, 50%, 100% convergence) to verify the assumption that first-order gradients are small near local optima holds across architectures.

3. **Distribution Shift Robustness**: Test DAUNCE on non-IID training splits (e.g., class-imbalanced CIFAR-10) to evaluate whether the method maintains attribution quality when standard assumptions about data distribution are violated.