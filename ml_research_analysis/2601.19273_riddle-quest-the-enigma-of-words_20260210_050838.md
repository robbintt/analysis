---
ver: rpa2
title: 'Riddle Quest : The Enigma of Words'
arxiv_id: '2601.19273'
source_url: https://arxiv.org/abs/2601.19273
tags:
- riddle
- riddles
- generation
- semantic
- validator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a general-purpose pipeline for analogy-based
  riddle generation, extending a previous educational-focused framework to support
  open-domain, genre-flexible riddle creation. The method integrates structured concept
  triples, semantic attribute mapping, stylized generation, and a validator for extracting
  all plausible answers.
---

# Riddle Quest : The Enigma of Words

## Quick Facts
- arXiv ID: 2601.19273
- Source URL: https://arxiv.org/abs/2601.19273
- Reference count: 0
- A general-purpose pipeline for analogy-based riddle generation across 5 genres, producing riddles with median 3.1 answers per riddle

## Executive Summary
This work presents a modular pipeline for generating analogy-based riddles across five genres (descriptive, metaphorical, poetic, humorous, situational) from 120 open-domain concepts. The system uses structured triples to capture concept knowledge, categorizes semantic attributes to support analogical clue formation, and employs a lookup-based validator to enumerate all plausible answers. A case study reveals that large language models retrieve only ~56% of validator-identified answers on average, with metaphorical and poetic riddles showing even lower coverage. The work reframes riddle ambiguity as a feature rather than an error, highlighting riddles as a micro-benchmark for evaluating generative reasoning and analogy-making in AI systems.

## Method Summary
The method uses a 4-module pipeline: (1) Triples Creator builds structured ⟨concept, relation, property⟩ facts from open-domain sources like ConceptNet; (2) Semantic Mapper categorizes properties into functional, perceptual, relational, and behavioral attributes; (3) Stylized Generator produces genre-specific riddles from semantic profiles; (4) Validator uses a lookup dictionary to exhaustively enumerate all plausible answers. The system generates 120 concepts across 5 genres, with riddles showing median 3.1 answers per riddle. A case study evaluates LLM performance on retrieving all possible answers, finding significant gaps particularly for metaphorical and poetic riddles.

## Key Results
- Pipeline produces riddles across 5 genres with median 3.1 answers per riddle (higher for metaphorical/poetic, lower for descriptive)
- LLM retrieval of validator-identified answers averages 56%, dropping to 30-45% for metaphorical and poetic riddles
- Riddles are more constrained when using functional/perceptual attributes (descriptive genre) versus relational/behav (metaphorical/poetic)
- LLMs tend to overcommit to single answers and compress multiple concepts into high-level abstractions

## Why This Works (Mechanism)

### Mechanism 1: Semantic Attribute Categorization Enables Analogical Mapping
- Claim: Organizing concept properties into semantic categories (functional, perceptual, relational, behavioral) supports systematic analogical clue formation across genres.
- Mechanism: The Semantic Attribute Mapper converts structured triples into categorized semantic profiles; these categories then determine which attributes are suitable for different analogical strategies. Metaphorical and poetic riddles draw more heavily from relational and behavioral attributes, while descriptive riddles foreground functional and perceptual ones.
- Core assumption: Property categories meaningfully correspond to distinct analogical strategies and genre conventions.
- Evidence anchors:
  - [Section 4.1]: "Descriptive riddles tended to foreground functional and perceptual attributes, whereas metaphorical and poetic riddles relied more heavily on relational or behavioral semantic mappings."
  - [Section 3.2]: "It groups properties into broad categories—functional, perceptual, relational, and behavioural—to support analogical clue formation."
  - [Corpus]: Related work on riddle evaluation (e.g., "The Riddle of Reflection") focuses on LLM reasoning ability but does not validate the specific semantic categorization mechanism proposed here.
- Break condition: If different genres show no meaningful difference in which attribute categories they use, the categorization is not driving analogical variation.

### Mechanism 2: Modular Decoupling of Semantic Content from Stylistic Expression
- Claim: Separating the semantic mapping stage from the stylized generation stage enables controlled genre variation without sacrificing logical grounding.
- Mechanism: The pipeline first produces a style-agnostic semantic profile, then passes it to a genre-aware generator. This allows the same underlying concept representation to support multiple narrative forms.
- Core assumption: The semantic profile captures sufficient information to support diverse stylistic interpretations without additional domain knowledge.
- Evidence anchors:
  - [Abstract]: "The system includes a triples creator that builds structured facts about a concept, a semantic mapper that selects attributes useful for analogy, a stylized generator that turns them into riddle clues."
  - [Section 4.1]: "Qualitative inspection confirmed that genre cues were consistently expressed, demonstrating that the modular separation between semantic attribute mapping and stylistic construction supports controlled variation."
  - [Corpus]: Weak direct validation—corpus papers evaluate riddle-solving in LLMs but do not test modular pipeline architectures.
- Break condition: If riddles across genres are indistinguishable in style or if semantic coherence degrades significantly for certain genres, modularity is not providing the claimed benefit.

### Mechanism 3: Lookup-Based Exhaustive Enumeration Exposes LLM Retrieval Gaps
- Claim: A dictionary-based validator systematically enumerates plausible answers in ways LLMs do not, revealing a gap between generative interpretation and exhaustive reasoning.
- Mechanism: The validator uses an expanded lookup dictionary to identify all concepts that could plausibly satisfy a riddle's clues. LLMs, by contrast, tend to overcommit to single answers or compress multiple concepts into high-level abstractions.
- Core assumption: The lookup dictionary has sufficient coverage to represent the true answer space for generated riddles.
- Evidence anchors:
  - [Section 5.2]: "Across the 20 riddles, the LLM retrieved an average of 56% of the validator's answer set. Performance varied by genre... Metaphorical and poetic riddles: LLM performance dropped notably, retrieving only 30–45%."
  - [Section 5.3]: "LLMs often treated riddles as having one 'correct' answer, even when explicitly asked for all possible answers."
  - [Corpus]: "RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models" directly addresses generator-validator discrepancies, supporting the broader claim that LLMs exhibit systematic gaps in exhaustive reasoning tasks.
- Break condition: If the validator's dictionary is incomplete or biased, the claimed gap may reflect dictionary limitations rather than true LLM reasoning deficiencies.

## Foundational Learning

- **Structured Knowledge Representation (Triples/Concept Networks)**
  - Why needed here: The Triples Creator requires understanding how to represent concepts as ⟨concept, relation, property⟩ structures, typically drawing from resources like ConceptNet or WordNet.
  - Quick check question: Given the concept "bridge," can you write three distinct triples capturing different relation types (e.g., functional, structural, relational)?

- **Semantic Attribute Typology**
  - Why needed here: The Semantic Mapper's categorization scheme (functional, perceptual, relational, behavioral) is the foundation for analogical clue selection; misunderstanding these categories will propagate errors downstream.
  - Quick check question: For the concept "clock," classify these attributes: "shows time" (functional), "ticking sound" (perceptual), "measures duration" (relational), "runs continuously" (behavioral)—can you explain why each fits its category?

- **Exhaustive vs. Probabilistic Reasoning**
  - Why needed here: Understanding why LLMs struggle with exhaustive enumeration (56% retrieval rate) requires recognizing the difference between generating a plausible answer and systematically covering all valid possibilities.
  - Quick check question: For the riddle "I have hands but cannot clap," list ALL valid answers you can think of (e.g., clock, statue, doll)—did you stop at the first obvious one?

## Architecture Onboarding

- **Component map:**
  Input Concept → Triples Creator → Structured ⟨concept, relation, property⟩ triples → Semantic Mapper → Categorized attribute profile (functional/perceptual/relational/behavioral) → Stylized Generator → Genre-specific riddle text (descriptive/metaphorical/poetic/humorous/situational) → Validator → Exhaustive candidate answer set via lookup dictionary

- **Critical path:** Triples Creator → Semantic Mapper → Stylized Generator. The validator is evaluative, not generative; it does not gate riddle production.

- **Design tradeoffs:**
  - Open-domain triples vs. fixed ontology: Greater coverage and genre flexibility, but reduced consistency guarantees.
  - Generator-only validation vs. external validator: Trade-off between creative freedom and systematic answer-space characterization.
  - Median 3.1 answers per riddle: Higher for metaphorical/poetic (interpretive flexibility) vs. lower for descriptive (constraint).

- **Failure signatures:**
  - Empty validator output: Riddle too specific; clues may overconstrain.
  - Validator returns >10 candidates: Riddle too ambiguous; clues underconstrain.
  - LLM retrieval >90% or <20%: Check for genre-specific generation artifacts or dictionary coverage issues.

- **First 3 experiments:**
  1. **Pipeline smoke test:** Run 10 diverse concepts (mix concrete/abstract) through all 5 genres; manually inspect output coherence and validator answer counts.
  2. **LLM retrieval replication:** Select 5 riddles per genre, prompt an LLM for "all possible answers," compare against validator output—quantify the gap.
  3. **Genre ablation:** Generate riddles for the same concept using only one attribute category at a time (e.g., functional-only vs. relational-only) to verify category-to-genre mapping claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanisms could reduce LLMs' tendency to overcommit to single answers and perform semantic compression when tasked with exhaustive enumeration?
- Basis in paper: [explicit] Section 5.3 identifies "overcommitment to a single guess" and "high-level paraphrasing" (e.g., responding "a measuring device" instead of listing individual items) as two notable failure patterns in LLMs when asked for all possible answers.
- Why unresolved: The paper documents these behaviors but does not propose or test interventions to address them.
- What evidence would resolve it: A study comparing prompt engineering, fine-tuning, or architectural modifications that specifically target multi-answer enumeration tasks.

### Open Question 2
- Question: How can systematic enumeration (validator) and generative interpretation (LLM) be combined to achieve both exhaustive coverage and creative inference?
- Basis in paper: [explicit] The paper notes that "LLMs frequently generated plausible answers that were not in the validator's lookup dictionary, suggesting complementary strengths."
- Why unresolved: The paper observes complementarity but does not explore hybrid approaches.
- What evidence would resolve it: Development and evaluation of a hybrid system that integrates lookup-based enumeration with LLM generative capabilities.

### Open Question 3
- Question: What formal metrics could evaluate riddle quality and solvability when ambiguity (median 3.1 candidates per riddle) is treated as a feature rather than an error?
- Basis in paper: [inferred] The validator "does not judge correctness, ambiguity, or stylistic fidelity," and the paper explicitly reframes ambiguity as "a characteristic of open-domain, creativity-oriented riddle generation."
- Why unresolved: No quantitative framework is proposed for assessing whether a riddle's ambiguity level is appropriate or productive.
- What evidence would resolve it: Correlation studies between answer-set size, human solver satisfaction, and downstream task utility.

### Open Question 4
- Question: Would the LLM performance gap (56% retrieval) generalize across larger samples and additional riddle genres or languages?
- Basis in paper: [inferred] The case study examined only 20 riddles across 5 genres, with no cross-linguistic evaluation.
- Why unresolved: Small sample size and monolingual focus limit claims about generalizability.
- What evidence would resolve it: Large-scale evaluation across hundreds of riddles in multiple languages and genres.

## Limitations

- Knowledge source ambiguity: Exact triples extraction mechanism from open-domain sources is underspecified
- Validator coverage assumptions: Lookup dictionary scope and construction method are not detailed
- Genre-to-attribute mapping validation: Claims lack quantitative validation of category-to-genre relationships

## Confidence

- **High confidence**: Modular pipeline architecture and 56% LLM retrieval gap are directly measurable
- **Medium confidence**: Semantic attribute categorization scheme lacks quantitative validation
- **Low confidence**: External validity of validator depends entirely on dictionary completeness

## Next Checks

1. **Semantic category-to-genre mapping validation**: For 50 randomly selected riddles, manually code which attribute categories appear in each and test whether descriptive riddles statistically favor functional/perceptual attributes versus relational/behav for metaphorical/poetic.

2. **Dictionary coverage audit**: For 20 diverse riddles, enumerate all validator answers and manually verify each against a comprehensive external knowledge base to quantify false negatives and assess whether the 56% gap is artifactual.

3. **Cross-model retrieval consistency**: Repeat the LLM retrieval experiment with 3 different models (GPT-4, Claude, Llama) on the same 20 riddles to determine if the 56% gap is model-specific or a general phenomenon across architectures.