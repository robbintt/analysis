---
ver: rpa2
title: 'HAMMER: Hamiltonian Curiosity Augmented Large Language Model Reinforcement'
arxiv_id: '2509.25240'
source_url: https://arxiv.org/abs/2509.25240
tags:
- zhang
- wang
- learning
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HAMMER, a curriculum learning method that
  improves large language model reinforcement learning by reordering training samples
  to maximize semantic diversity early in training. Instead of using difficulty-based
  ordering, HAMMER uses sentence embeddings from the backbone LLM to construct a minimum-semantic
  Hamiltonian path, exposing the model to more diverse samples at the start to encourage
  exploration and prevent early overfitting to easy problems.
---

# HAMMER: Hamiltonian Curiosity Augmented Large Language Model Reinforcement

## Quick Facts
- arXiv ID: 2509.25240
- Source URL: https://arxiv.org/abs/2509.25240
- Authors: Ming Yang; Xiaofan Li; Zhiyuan Ma; Dengliang Shi; Jintao Du; Yu Cheng; Weiguo Zheng
- Reference count: 29
- Primary result: HAMMER achieves 3%–4% accuracy gains on math benchmarks by reordering training samples for maximum semantic diversity early in RLVR training

## Executive Summary
HAMMER introduces a curriculum learning method that improves large language model reinforcement learning by reordering training samples to maximize semantic diversity early in training. Instead of difficulty-based ordering, HAMMER uses sentence embeddings from the backbone LLM to construct a minimum-semantic Hamiltonian path, exposing the model to more diverse samples at the start to encourage exploration and prevent early overfitting to easy problems. Theoretically, it is shown that diverse subsets preserve the optimal policy while tightening generalization bounds, and that minimizing semantic similarity aligns with maximizing dataset diversity. Empirically, HAMMER consistently achieves 3%–4% accuracy gains over baselines across multiple math benchmarks (AIME 2024/2025, AMC 2023, Olympiad) when integrated with RLVR algorithms like DAPO and GRPO.

## Method Summary
HAMMER is a curriculum learning method for RLVR that reorders training samples based on semantic diversity. It extracts sentence embeddings from the backbone LLM via mean-pooling of hidden states, computes pairwise cosine similarities to form a complete graph, then finds a minimum-semantic Hamiltonian path using an η-greedy heuristic. This path is used to reorder the training dataset, exposing the model to diverse samples early in training to encourage exploration. The method is integrated with RLVR algorithms like DAPO and GRPO, with specific hyperparameters including learning rate 1e-6, batch size 16, and KL penalty β=0.001 for GRPO.

## Key Results
- HAMMER achieves 3%–4% accuracy gains over baselines on AIME 2024/2025, AMC 2023, and Olympiad benchmarks
- Consistent improvements in both pass@k and answer consistency metrics across different batch sizes and model scales
- Gains maintained when integrated with both DAPO and GRPO RLVR algorithms
- Theoretical analysis shows diverse subsets preserve optimal policy and tighten generalization bounds

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Exposing the model to semantically diverse samples early in RL training prevents premature convergence to local optima caused by overfitting to easy samples.
- **Mechanism:** HAMMER constructs a "Hamiltonian Curiosity Order"—a path through training samples that minimizes cumulative semantic similarity between consecutive samples. This greedy maximization of early diversity forces the policy to explore a broader region of the input space before settling into exploitation patterns.
- **Core assumption:** Early training dynamics disproportionately shape final policy quality; high-variance RLVR benefits more from diverse exposure than from gradual difficulty ramping.
- **Evidence anchors:**
  - [abstract] "training samples are ordered via a minimum-semantic Hamiltonian path making the initial training retrain more exploration"
  - [section 1.1] "continual training on simple samples in the early steps can cause the policy to lose its exploration"
  - [corpus] Related work "Curiosity-Driven Reinforcement Learning from Human Feedback" confirms diversity-alignment tradeoffs in RLHF, though HAMMER specifically targets RLVR with verifiable rewards
- **Break condition:** If tasks are highly interdependent (later samples require mastering earlier ones), diversity-first ordering may fragment necessary skill accumulation.

### Mechanism 2
- **Claim:** Diverse training subsets tighten the generalization bound while preserving the optimal policy, enabling faster convergence.
- **Mechanism:** Theorem 1 proves that for sufficiently diverse subsets S ⊂ X, the optimal policy π* on X remains in the induced policy subset Π_S. Theorem 2 shows generalization error Δ_π ≤ O(√(d·log(n/d) + log(1/δ))/n). By training on diverse subsets first, the empirical risk better approximates true risk earlier.
- **Core assumption:** The VC-dimension-based bound applies meaningfully to LLM policy classes; the policy space has bounded complexity relative to sample diversity.
- **Evidence anchors:**
  - [section 4.2] Theorem 1: "selecting a subset S from X that satisfies the γ-condition ensures that the optimal policy π* is preserved"
  - [section 4.2] Theorem 2 provides explicit bound with ρ ∝ O(√log n/n)
  - [corpus] Weak direct evidence—corpus papers don't address subset-based generalization bounds for LLM RL
- **Break condition:** If the policy class VC-dimension vastly exceeds practical sample sizes, the bound becomes vacuously loose.

### Mechanism 3
- **Claim:** Using backbone LLM embeddings (vs. external models) yields semantic representations aligned with training dynamics.
- **Mechanism:** Mean-pooled hidden states from forward passes capture the model's internal semantic space. Pairwise cosine similarities define the edge weights for the Hamiltonian path, ensuring "diversity" reflects what the model actually perceives as distinct.
- **Core assumption:** Internal representations correlate with gradient update diversity; the backbone's semantic space remains stable during early RL training.
- **Evidence anchors:**
  - [section 3.1] "embeddings derived from external models... may be misaligned with the backbone model's internal representations"
  - [figure 1] Shows embedding space construction and path weight computation
  - [corpus] "LLM2vec" paper (cited) supports using LLMs as text encoders aligned with their training
- **Break condition:** If RL training rapidly shifts internal representations, precomputed embeddings become stale.

## Foundational Learning

- **Concept: Hamiltonian Path/Cycle (TSP family)**
  - **Why needed here:** HAMMER frames sample ordering as finding a minimum-weight path through a complete graph where edge weights = semantic similarity.
  - **Quick check question:** Can you explain why finding an exact minimum Hamiltonian cycle is NP-hard and how η-greedy heuristic search approximates it?

- **Concept: VC-Dimension and Generalization Bounds**
  - **Why needed here:** Theoretical justification relies on uniform convergence bounds from statistical learning theory.
  - **Quick check question:** What does the VC inequality state about the relationship between sample size, hypothesis class complexity, and generalization gap?

- **Concept: RLVR (Reinforcement Learning with Verifiable Rewards)**
  - **Why needed here:** HAMMER operates on RLVR algorithms (GRPO, DAPO) where rewards come from outcome verification rather than learned reward models.
  - **Quick check question:** How does GRPO differ from PPO in advantage estimation, and why does this matter for curriculum design?

## Architecture Onboarding

- **Component map:** Embedding extraction -> Similarity matrix computation -> Path computation -> Curriculum training
- **Critical path:** Embedding extraction must complete before training begins. Path computation is O(n²) with n = dataset size (~40K for DeepScaleR). Precompute and cache.
- **Design tradeoffs:**
  - η (expand factor): Higher η explores more candidates but increases path computation time. Paper uses η=3 with minimal impact on final performance.
  - Embedding freshness: Precompute once vs. periodically refresh. Paper uses static precomputation.
  - Path vs. cycle: Paper uses Hamiltonian path (open); cycle (closed) is slightly different optimization target.
- **Failure signatures:**
  - **Convergence slower than baseline**: Check if similarity matrix is dominated by near-identical samples (low diversity dataset). Path ordering provides no benefit.
  - **Early training instability spikes**: Diverse samples may produce high-variance gradients. Consider warmup or smaller initial learning rates.
  - **No improvement over random shuffle**: Embeddings may not capture task-relevant diversity; verify by inspecting nearest neighbors in embedding space.
- **First 3 experiments:**
  1. **Sanity check**: On a small subset (1K samples), visualize the Hamiltonian path order vs. random order. Confirm consecutive samples are semantically distinct via manual inspection.
  2. **Ablation on η**: Run path computation with η ∈ {1, 3, 5, 10} on full dataset. Measure path quality (cumulative similarity) vs. compute time. Verify η=3 is sufficient.
  3. **Baseline comparison**: Train GRPO on shuffled data vs. HAMMER-ordered data for fixed step budget. Plot pass@k curves to validate the 3-4% gain claim on a held-out benchmark split.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the performance improvement from HAMMER generalize to non-mathematical domains such as code generation or general reasoning?
- **Basis in paper:** [inferred] The experimental evaluation (Section 5) is restricted to mathematical benchmarks (AIME, AMC, Olympiad) and the DeepScaleR dataset, despite the method being proposed for general LLM reinforcement learning.
- **Why unresolved:** The theoretical benefits are general, but the semantic embedding structures and diversity requirements in code or natural language tasks may differ significantly from mathematics.
- **What evidence would resolve it:** Application of HAMMER to RLVR training on code synthesis (e.g., HumanEval, MBPP) or logical reasoning benchmarks.

### Open Question 2
- **Question:** How does the performance of the η-greedy heuristic compare to an exact solution for the minimum-semantic Hamiltonian path?
- **Basis in paper:** [explicit] The paper states in Section 3.2 that solving for the exact path is NP-hard and proposes an efficient heuristic, but it does not quantify the optimality gap or performance loss caused by this approximation.
- **Why unresolved:** It is unclear if the computational savings of the heuristic compromise the theoretical diversity guarantees necessary for optimal convergence.
- **What evidence would resolve it:** A comparison on smaller datasets where the exact path is computable, measuring the difference in cumulative semantic similarity and downstream accuracy.

### Open Question 3
- **Question:** Does the effectiveness of semantic diversity ordering persist in models significantly larger than 4B parameters?
- **Basis in paper:** [inferred] Experiments are limited to Qwen3-1.7B and Qwen3-4B; while the paper claims gains are maintained across these scales, it is unknown if the "curiosity" mechanism scales to models with inherently stronger exploration capabilities.
- **Why unresolved:** Larger models may exhibit different exploration-exploitation dynamics where the marginal benefit of forced semantic diversity is reduced or altered.
- **What evidence would resolve it:** Empirical results integrating HAMMER into the training of 7B, 70B, or frontier-scale models.

## Limitations
- Theoretical bounds rely on idealized assumptions about VC-dimension that may not capture LLM complexity
- Empirical results limited to specific model scales (1.7B, 4B) and benchmark domains (math problems)
- Static embedding assumption may not hold if RL training rapidly shifts internal representations

## Confidence
- Mechanism claims (early diversity → better exploration): **High** - Directly supported by experimental results across multiple benchmarks
- Theoretical bounds (generalization improvement): **Medium** - Proofs are valid but assumptions may not hold in practice
- Embedding-based path computation: **Medium** - Algorithm is clearly specified, but sensitivity to embedding quality and freshness is not fully characterized

## Next Checks
1. Test HAMMER on non-mathematical reasoning tasks (e.g., code generation, common sense QA) to verify domain generality
2. Conduct ablation studies on different backbone model scales (e.g., 7B, 13B) to assess scaling behavior
3. Measure embedding space drift during training to validate the static embedding assumption and determine optimal refresh intervals