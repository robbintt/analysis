---
ver: rpa2
title: 'KANO: Kolmogorov-Arnold Neural Operator'
arxiv_id: '2509.16825'
source_url: https://arxiv.org/abs/2509.16825
tags:
- kano
- operator
- spectral
- neural
- fourier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KANO addresses the generalization limitations of Fourier Neural
  Operators (FNO) on position-dependent dynamics (variable coefficient PDEs) by jointly
  parameterizing in both spectral and spatial bases. Unlike FNO, which relies solely
  on spectral kernels and exhibits input-dependent spectral off-diagonals leading
  to poor out-of-distribution generalization, KANO uses Kolmogorov-Arnold Networks
  (KANs) for both activation functions and pseudo-differential symbols, enabling sparse
  representations in both domains.
---

# KANO: Kolmogorov-Arnold Neural Operator

## Quick Facts
- **arXiv ID:** 2509.16825
- **Source URL:** https://arxiv.org/abs/2509.16825
- **Reference count:** 40
- **Key outcome:** KANO achieves robust out-of-distribution generalization on position-dependent dynamics by jointly parameterizing in spectral and spatial bases, enabling polynomial scaling versus FNO's super-exponential scaling while maintaining symbolic interpretability.

## Executive Summary
KANO (Kolmogorov-Arnold Neural Operator) addresses the fundamental generalization limitations of Fourier Neural Operators (FNO) when learning position-dependent differential operators (variable coefficient PDEs). While FNO relies solely on spectral kernels, leading to input-dependent spectral mixing and poor out-of-distribution performance, KANO uses Kolmogorov-Arnold Networks (KANs) to parameterize pseudo-differential symbols in both spatial and spectral domains simultaneously. This dual-domain approach enables sparse representations where the operator is naturally sparse, achieving polynomial parameter scaling versus FNO's super-exponential scaling. KANO demonstrates robust generalization on synthetic position-dependent operators with significantly lower relative loss while recovering ground-truth Hamiltonians to the fourth decimal place in symbolic coefficients.

## Method Summary
KANO implements a Kohn-Nirenberg quantization layer that parameterizes pseudo-differential operators using KAN-based symbols $p(x,\xi)$ and activations. The architecture jointly learns in spatial and spectral bases through a double summation that mixes position and frequency information, replacing FNO's purely spectral approach. For synthetic benchmarks, KANO uses single-layer KANs with 10 cubic B-splines, no lift-up/projection networks, and trains with Adam optimizer on relative $\ell_2$ loss using 2000 samples. The Kohn-Nirenberg quantization enables polynomial scaling in parameter complexity while maintaining the ability to extract closed-form symbolic formulas from learned edge functions.

## Key Results
- KANO achieves one-order lower relative loss than FNO on out-of-distribution position-dependent operators using only 0.03% of FNO's parameters
- On quantum Hamiltonian learning benchmarks, KANO attains four orders lower state infidelity compared to FNO
- KANO recovers ground-truth Hamiltonians to the fourth decimal place in symbolic coefficients through edge visualization

## Why This Works (Mechanism)

### Mechanism 1: Dual-Domain Sparse Representation
KANO resolves FNO's super-exponential parameter scaling by representing operators in the basis where they are sparse. FNO's hard-coded spectral kernels force spatial multipliers to be represented as dense Toeplitl matrices in Fourier domain, while KANO uses Kohn-Nirenberg quantization to parameterize symbols $p(x,\xi)$ in both domains simultaneously. This allows differential terms to remain spectral and localized potentials to remain spatial, preventing spectral density explosion. The approach assumes the target operator is a finite composition of spatial and spectral multipliers with smooth symbols.

### Mechanism 2: Input-Independent Spectral Mixing
KANO generalizes better on out-of-distribution inputs because its spectral mixing is structurally decoupled from input distribution. In FNO, spectral off-diagonals are generated by input-dependent activation gate $\sigma'(z(u))$, tying the learned operator to training subspace. KANO's symbol $p(x,\xi)$ is a learnable parameter independent of input $u$, allowing it to learn a fixed dense Toeplitz kernel that applies consistently across functional space. The assumption is that FNO's generalization failure is primarily due to input-dependent mode mixing rather than network capacity.

### Mechanism 3: Symbolic Regression via KAN Edges
KANO enables direct extraction of closed-form symbolic equations from trained network weights by replacing fixed activation functions and linear weights with learnable univariate functions (splines) on edges. Because the symbol $p(x,\xi)$ and activation $\Phi$ are KAN-subnetworks, learned edge functions can be visualized and regressed into symbolic formulas, yielding interpretable physical laws. This assumes the target operator is smooth and low-dimensional enough for B-splines to capture the functional form accurately.

## Foundational Learning

- **Concept: Pseudo-differential Operators & Symbols**
  - **Why needed here:** KANO is built on Kohn-Nirenberg quantization, which generalizes Fourier multipliers to position-dependent kernels. Without this, the shift from $R(\xi)$ (FNO) to $p(x,\xi)$ (KANO) is unintelligible.
  - **Quick check question:** Can you explain why a spatial multiplier $x \cdot u(x)$ acts as a derivative in the Fourier domain ($\partial_\xi \hat{u}$)?

- **Concept: Spectral Bias & Spectral Off-diagonals**
  - **Why needed here:** The core critique of FNO is its inability to robustly learn spectral off-diagonals for variable coefficients. Understanding that FNO is "diagonal-dominant" helps explain why it fails on position-dependent dynamics.
  - **Quick check question:** In an FNO layer, where does the mixing of different frequency modes occur? (Answer: Only via the non-linearity $\sigma$ or the dense linear $W$).

- **Concept: Kolmogorov-Arnold Representation Theorem**
  - **Why needed here:** This justifies the KAN architecture which serves as the building block for KANO's symbols and activations.
  - **Quick check question:** How does a KAN layer differ from an MLP layer in terms of where the "learning" happens (nodes vs. edges)?

## Architecture Onboarding

- **Component map:** Input $a(x)$ -> FFT -> KAN-based Symbol $p_\theta(x,\xi)$ -> Kohn-Nirenstein Operator $Op_m(p)$ -> KAN-based Activation $\Phi_\theta$ -> Output $u(x)$

- **Critical path:**
  1. FFT: Project input $a(x)$ to spectral domain $\hat{a}(\xi)$
  2. Symbol Eval: Compute symbol $p(x,\xi)$ for all grid points and modes
  3. Double Sum (Quantization): Compute $\sum_{\xi} \sum_y e^{i(x-y)\cdot\xi} p(x,\xi) a(y)$
  4. Activation: Apply the activation KAN $\Phi$ to the result

- **Design tradeoffs:**
  - Efficiency vs. Generality: KANO has higher per-layer FLOPs ($O(m^3)$) than FNO ($O(m \log m)$) due to the double sum. Use KANO only if FNO fails to generalize or if symbolic extraction is required.
  - Interpretability vs. Stability: The paper excludes wide lift/project networks to ensure symbolic recovery stability, which may limit performance on very high-dimensional data compared to standard FNOs with large embedding layers.

- **Failure signatures:**
  - OOM: The $O(m^2)$ or $O(m^3)$ complexity of the symbol grid or double sum crashes memory on fine grids
  - Symbolic Noise: If regularization isn't strong enough, the KAN edges fit noise instead of smooth symbolic functions
  - Slow Convergence: On purely constant-coefficient PDEs, KANO is overkill and likely trains slower than FNO

- **First 3 experiments:**
  1. Sanity Check (1D): Train on $G_1 = x^2 f - \partial_{xx} f$ using Group A data. Verify that Group B test loss remains low while FNO's loss spikes
  2. Symbolic Recovery: After training, freeze the KAN symbol $p(x,\xi)$. Visualize the edges using `splines`. Check if they align with $x^2$ and $\xi^2$ shapes
  3. Compute Bench: Profile the latency of a single KANO layer vs. an FNO layer on a 128x128 2D grid to quantify the computational penalty of the dual-domain summation

## Open Questions the Paper Calls Out

- **Open Question 1:** Can KANO be extended to conventional high-dimensional use cases without sacrificing its symbolic interpretability?
  - **Basis in paper:** Remark 3 states that "extending KANO to conventional high-dimensional use cases is a natural and promising direction for future work."
  - **Why unresolved:** KANO removes the wide lift-up and projection networks to prioritize symbolic recovery, yet these components are known to be essential for performance on high-dimensional benchmarks.
  - **What evidence would resolve it:** Empirical results showing KANO maintaining high accuracy and interpretability on standard high-dimensional benchmarks (e.g., 3D fluid dynamics).

- **Open Question 2:** What is the precise theoretical impact of lift-up and projection networks on the generalization bounds of Fourier Neural Operators (FNO)?
  - **Basis in paper:** Remark 1 notes that "the theoretical analysis on the role of the lift-up and projection networks is yet an open research question."
  - **Why unresolved:** While upper bounds exist, current analyses often neglect the specific contribution of these networks to the projection error $\epsilon_{proj}$, leaving a gap in understanding FNO's generalization capacity.
  - **What evidence would resolve it:** A theoretical formulation (e.g., lower bounds) quantifying how lift-up and projection networks influence the approximation error relative to the latent network.

- **Open Question 3:** Can the computational bottleneck of the Kohn-Nirenstein quantization ($O(m^{3d})$) be reduced to make KANO practical for finer spatial resolutions?
  - **Basis in paper:** Appendix E highlights the "double sum" computational cost of the KANO layer, contrasting it with FNO's lower per-layer FLOPs ($O(m^d)$).
  - **Why unresolved:** While parameter efficiency compensates asymptotically, the high constant factors and cubic scaling with resolution pose a practical barrier for large-scale simulations.
  - **What evidence would resolve it:** Algorithmic optimizations or approximations (e.g., sparse quantization) that lower the FLOP count without degrading the dual-domain expressivity.

## Limitations
- KANO's per-layer computational complexity is significantly higher than FNO due to the double summation ($O(m^3)$ vs $O(m \log m)$), limiting its practicality for fine spatial resolutions
- The exclusion of lift-up and projection networks, while beneficial for symbolic interpretability, may constrain performance on standard high-dimensional PDE benchmarks
- The dual-domain efficiency gain may vanish if the underlying PDE operator has no sparse representation in either spatial or spectral bases

## Confidence
- **High:** The core mechanism of dual-domain sparse representation via Kohn-Nirenberg quantization
- **Medium:** The input-independent spectral mixing claim
- **Medium:** The symbolic regression capability

## Next Checks
1. **Memory Complexity Benchmark:** Profile KANO's per-layer memory usage on 2D grids (e.g., 128x128) and compare against FNO to quantify the O(m²) symbol grid and O(m³) double sum overhead. Document the exact tensor operations used to mitigate this.

2. **OOD Generalization Robustness:** Train KANO and FNO on Group A data, then test on a new OOD group (e.g., multi-frequency chirps or discontinuous functions) not mentioned in the paper. Report both training and test losses to confirm the generalization gap.

3. **Symbolic Recovery Stress Test:** After training KANO on G₁, G₂, and G₃, extract the learned symbolic coefficients. Then, intentionally corrupt the training data with noise or reduce the training set size to test the robustness of symbolic recovery. Compare the recovered coefficients' accuracy against the ground truth under these stress conditions.