---
ver: rpa2
title: 'Generative World Modelling for Humanoids: 1X World Model Challenge Technical
  Report'
arxiv_id: '2510.07092'
source_url: https://arxiv.org/abs/2510.07092
tags:
- world
- video
- arxiv
- wang
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents solutions for the 1X World Model Challenge,
  which benchmarks generative world models for humanoid robots. The sampling track
  adapts Wan-2.2 TI2V-5B, a large-scale video generation foundation model, to predict
  future frames conditioned on robot states using AdaLN-Zero for state conditioning
  and LoRA for efficient fine-tuning.
---

# Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report

## Quick Facts
- **arXiv ID:** 2510.07092
- **Source URL:** https://arxiv.org/abs/2510.07092
- **Reference count:** 40
- **Primary result:** First place in both tracks of the 1X World Model Challenge with PSNR 23.0 dB (sampling) and Top-500 cross-entropy 6.6386 (compression)

## Executive Summary
This work presents solutions for the 1X World Model Challenge, benchmarking generative world models for humanoid robots. The authors adapt the Wan-2.2 TI2V-5B foundation model for video generation to predict future frames conditioned on robot states using AdaLN-Zero for state conditioning and LoRA for efficient fine-tuning in the sampling track. For the compression track, they employ a spatio-temporal Transformer architecture trained from scratch on discrete latent token sequences. Their methods achieve first place in both tracks, demonstrating the effectiveness of leveraging foundation models and specialized architectures for high-fidelity video prediction in robotics applications.

## Method Summary
The authors tackle two tracks of the 1X World Model Challenge. In the sampling track, they fine-tune Wan-2.2 TI2V-5B, a large-scale video generation foundation model, to predict future frames 2 seconds ahead given 17 past frames and robot states. They implement state conditioning through AdaLN-Zero, which modulates timestep embeddings with compressed state information, and use LoRA for efficient adaptation. In the compression track, they design a spatio-temporal Transformer architecture that operates directly on discrete latent token sequences, learning to predict future token grids conditioned on past observations and robot states. Both approaches are trained on a dataset of 77-frame clips with synchronized robot state information.

## Key Results
- Achieved first place in sampling track with PSNR of 23.0 dB on frame 77 prediction
- Achieved first place in compression track with Top-500 cross-entropy of 6.6386
- Demonstrated successful adaptation of internet-scale video generation foundation model to robotics domain
- Showed specialized spatio-temporal architecture effective for latent token prediction in robotics setting

## Why This Works (Mechanism)
The sampling track leverages the strong inductive biases of a foundation model pre-trained on internet-scale video data, enabling rapid adaptation to the robotics domain through parameter-efficient fine-tuning. The compression track's spatio-temporal Transformer architecture is well-suited to capture the hierarchical dependencies in both spatial and temporal dimensions of video sequences, while operating directly in the discrete latent space provides computational efficiency and avoids pixel-level noise.

## Foundational Learning
- **AdaLN-Zero conditioning:** A parameter-efficient method to inject state information into diffusion models by modulating timestep embeddings; needed to incorporate robot states into video prediction; quick check: verify state MLP outputs match expected modulation dimensions.
- **LoRA fine-tuning:** Low-Rank Adaptation technique that freezes original weights while training small adapter matrices; needed for efficient adaptation of large foundation models; quick check: confirm rank-32 LoRA matrices are properly initialized and updated.
- **Spatio-temporal Transformers:** Architecture that alternates spatial and temporal attention layers to capture video structure; needed to model dependencies across both dimensions; quick check: verify alternating attention pattern and QKNorm implementation.
- **Discrete latent spaces:** Tokenization of video frames into discrete representations for efficient modeling; needed to reduce computational complexity and focus on semantic content; quick check: confirm tokenization produces expected grid dimensions (32×32).
- **Teacher-forced vs autoregressive generation:** Training with ground truth inputs vs. generated inputs during inference; needed to understand exposure bias in sequence prediction; quick check: monitor both metrics during training to detect divergence.
- **Ensemble averaging for PSNR:** Using multiple stochastic samples and averaging predictions to optimize PSNR; needed to reduce uncertainty in generative predictions; quick check: verify ensemble size and averaging implementation.

## Architecture Onboarding
**Component map:** Robot states (s) → MLP compression → AdaLN-Zero modulation → Wan-2.2 TI2V-5B (sampling) OR Cosmos tokenizer → ST-Transformer → Token prediction (compression)

**Critical path:** For sampling: input frames + states → AdaLN-Zero conditioning → LoRA-adapted Wan-2.2 → predicted frame. For compression: input tokens + states → ST-Transformer layers → predicted tokens.

**Design tradeoffs:** Sampling track prioritizes leveraging pre-trained knowledge for rapid adaptation vs. computational cost of large model inference; compression track prioritizes computational efficiency through latent space modeling vs. potential information loss from discretization.

**Failure signatures:** Sampling: blurry predictions, state conditioning not affecting outputs, LoRA not learning. Compression: mode collapse in autoregressive generation, large gap between teacher-forced and autoregressive performance, instability in alternating attention layers.

**First experiments:** 1) Verify state conditioning by checking if predictions change with different robot states. 2) Test autoregressive generation quality on a small validation set. 3) Profile inference time and memory usage for both models.

## Open Questions the Paper Calls Out
- **Inference strategy for downstream decision-making:** While ensemble averaging optimized PSNR, the most suitable inference strategy for actual robot planning remains unexplored. The trade-off between metric optimization and preserving actionable visual details is unexamined.
- **Reducing teacher-forced vs autoregressive gap:** Scheduled sampling failed to meaningfully reduce the performance gap between training and inference modes, indicating exposure bias persists despite training interventions.
- **Pre-trained foundation models for latent token prediction:** The success of internet-scale pre-training in pixel-space prediction raises the question of whether similar transfer learning benefits apply to discrete latent token prediction, which remains unexplored.

## Limitations
- Results may not generalize to longer prediction horizons, out-of-distribution robot states, or alternative camera viewpoints beyond the benchmark conditions.
- The evaluation metrics (PSNR, Top-500 cross-entropy) are narrow and may not capture practical utility in real-world humanoid control tasks.
- Large-scale pre-trained model dependencies introduce reliance on external model weights and inference infrastructure not fully specified in public releases.

## Confidence
- **Sampling track claims (High):** The AdaLN-Zero state conditioning and LoRA adaptation of Wan-2.2 TI2V-5B is technically sound and PSNR results are reproducible given access to base model and correct preprocessing.
- **Compression track claims (High):** The ST-Transformer architecture with specified hyperparameters is clearly defined and should yield reported cross-entropy if implemented as described.
- **Major uncertainties (Low):** Precise availability of Wan-2.2 TI2V-5B checkpoint, Cosmos 8×8×8 tokenizer implementation, and exact state MLP dimensions prevent immediate replication without additional engineering effort.

## Next Checks
1. Verify model checkpoint availability and run forward pass on sample frame to confirm preprocessing (downsampling + VAE encoding) is correct.
2. Train compression model on small data subset and compare teacher-forced vs autoregressive loss to ensure expected behavior.
3. Test sampling model's PSNR with ensemble inference (n=20) on validation set to confirm reproducibility of 23.0 dB metric.