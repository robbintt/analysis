---
ver: rpa2
title: 'In-Context Examples Matter: Improving Emotion Recognition in Conversation
  with Instruction Tuning'
arxiv_id: '2508.11889'
source_url: https://arxiv.org/abs/2508.11889
tags:
- in-context
- examples
- tuning
- emotion
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes InitERC, a one-stage in-context instruction
  tuning framework for emotion recognition in conversation (ERC). Unlike existing
  multi-stage approaches, InitERC jointly captures speaker characteristics, conversational
  context, and emotional states within a unified framework.
---

# In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning

## Quick Facts
- arXiv ID: 2508.11889
- Source URL: https://arxiv.org/abs/2508.11889
- Reference count: 22
- One-stage in-context instruction tuning achieves SOTA on ERC, outperforming best baseline by 20.25%, 9.44%, and 14.42% on three benchmarks.

## Executive Summary
This paper introduces InitERC, a one-stage in-context instruction tuning framework for emotion recognition in conversation (ERC). Unlike multi-stage approaches that learn speaker characteristics and emotional states sequentially, InitERC jointly captures these elements within a unified framework. The method constructs a demonstration pool, retrieves relevant in-context examples, designs prompt templates, and performs instruction tuning to align speaker-context-emotion relationships. Comprehensive experiments on IEMOCAP, MELD, and EmoryNLP demonstrate state-of-the-art performance, with the ablation study confirming the critical synergy between retrieval and fine-tuning.

## Method Summary
InitERC is a one-stage in-context instruction tuning framework that fine-tunes a large language model to jointly learn speaker characteristics, conversational context, and emotional states. The method constructs a demonstration pool from training data, uses Contriever-MS MARCO to retrieve top-5 relevant examples for each target utterance (excluding same-dialogue examples), and formats prompts combining task description, retrieved examples, and target input. The model (LLaMA3.1-8B-Instruct) is fine-tuned using LoRA adapters with a negative log-likelihood loss function. This unified approach avoids the "weak alignment" of sequential multi-stage methods and enables the model to learn generalized retrieval-to-emotion mappings during training.

## Key Results
- InitERC achieves state-of-the-art performance on three benchmark datasets, outperforming the best baseline by 20.25% (IEMOCAP), 9.44% (MELD), and 14.42% (EmoryNLP) in weighted F1 score.
- Retrieval strategy significantly impacts performance, with Contriever-MS MARCO outperforming Random and BM25 methods across all datasets.
- The number of retrieved examples shows diminishing returns, with performance saturating around k=5-6 examples.
- Ablation studies confirm the synergistic effect between retrieval and fine-tuning, as models without either component perform substantially worse.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Unified one-stage training improves alignment between speaker identity and context compared to sequential multi-stage methods.
- **Mechanism:** InitERC jointly optimizes speaker traits, conversational history, and emotional states in a single pass, avoiding the weak alignment that arises when learning these elements in isolation.
- **Core assumption:** Joint optimization allows the model to capture dynamic interactions between speaker characteristics and context interpretation that sequential training decouples.
- **Evidence anchors:** Introduction argues sequential methods lead to sub-optimal alignment; abstract notes multi-stage methods constrain capacity to jointly capture dynamic interactions.
- **Break condition:** Insufficient model capacity to resolve increased complexity without catastrophic forgetting of pre-trained knowledge.

### Mechanism 2
- **Claim:** Performance gains depend on synergy between retrieved examples and gradient updates, not retrieval alone.
- **Mechanism:** The framework uses demonstration pools to provide context- and speaker-aware examples during fine-tuning, updating model weights to recognize patterns in retrieved demonstrations.
- **Core assumption:** The model learns a generalizable retrieval-to-emotion mapping function during training rather than overfitting to specific examples.
- **Evidence anchors:** Ablation Study Table 3 shows "w/o fine-tuning" performs worse than zero-shot, while "w/o in-context examples" drops performance; In-Context Instruction Tuning section details loss function.
- **Break condition:** If retriever selects spurious examples, gradient updates may degrade pre-trained reasoning capabilities.

### Mechanism 3
- **Claim:** Similarity-based retrieval effectively transfers emotional context from distinct conversations to target utterance.
- **Mechanism:** Contriever-MS MARCO finds semantic matches, enabling the model to infer emotional labels via analogical reasoning across different speakers and contexts.
- **Core assumption:** Semantic similarity in embedding space correlates with emotional response patterns across different speakers and contexts.
- **Evidence anchors:** Impact of Retrieval Strategy Table 5 shows Contriever outperforming Random and BM25; In-Context Example Selection excludes same-dialogue examples to prevent leakage.
- **Break condition:** If target utterance contains domain-specific jargon or emotional sarcasm not represented in demonstration pool, semantic similarity may fail to retrieve functionally analogous contexts.

## Foundational Learning

- **Concept: In-Context Instruction Tuning**
  - **Why needed here:** Core contribution distinguishing method from standard fine-tuning and standard ICL; requires understanding batch inputs with retrieved demonstration examples.
  - **Quick check question:** How does the loss function change when input sequence includes variable-length demonstration examples preceding target utterance?

- **Concept: Dense Retrieval (Contriever)**
  - **Why needed here:** Paper identifies retrieval strategy as critical factor; understanding Contriever encoding is necessary to debug example selection.
  - **Quick check question:** Does retriever use emotional label during search, and how does paper prevent data leakage from similar dialogues?

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here:** Method fine-tunes LLaMA-3.1-8B; practical implementation relies on LoRA for limited hardware.
  - **Quick check question:** If you increase rank r of LoRA adapters, how might this affect model's ability to align speaker-context-emotion vs. overfitting?

## Architecture Onboarding

- **Component map:** Demonstration Pool -> Retriever (Contriever-MS MARCO) -> Prompt Constructor -> LLM Backbone (LLaMA3.1-8B-Instruct with LoRA)

- **Critical path:** In-Context Example Selection is highest leverage point. Ablation shows without fine-tuning, in-context examples hurt performance. System breaks if examples are irrelevant and model hasn't learned to ignore them, or if examples are relevant but model hasn't learned to use them.

- **Design tradeoffs:**
  - **Number of Examples (k):** Figure 4 shows performance saturates around k=5 or 6; increasing k increases context window usage and compute cost for negligible gain.
  - **Retrieval vs. Random:** Table 5 shows Random competitive on IEMOCAP but fails on MELD; suggests some datasets benefit from any structure while others require precise semantic matching.

- **Failure signatures:**
  - **"W/o fine-tuning" Collapse:** If validation loss diverges but few-shot accuracy seems okay initially, check if model is actually learning or just relying on priors. "W/o fine-tuning" baseline (11.12% on IEMOCAP) shows adding examples to untrained model can cause catastrophic failure.
  - **Data Leakage:** If validation accuracy suspiciously high (>95% early on), verify retriever strictly filters out examples from same conversation ID as target.

- **First 3 experiments:**
  1. **Baseline Sanity Check:** Reproduce "w/o in-context examples" vs. InitERC on MELD to verify implementation of retrieval mechanism.
  2. **Retriever Swap:** Replace Contriever with Random retrieval to quantify upper/lower bounds of performance on target dataset.
  3. **Scaling Law:** Vary k âˆˆ {1, 3, 5, 8} to find saturation point for specific LLM backbone, as this may differ from paper's LLaMA-3.1 results.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can jointly training the retriever and emotion recognition model improve performance compared to using fixed, off-the-shelf retrievers?
- **Basis in paper:** [explicit] Authors state in "Impact of Retrieval Strategy" section that "future work could explore the potential of jointly training the retriever and the emotion recognition model to further harmonize example selection with emotion inference."
- **Why unresolved:** Current framework uses Contriever-MS MARCO as frozen retriever; while effective, it relies on general semantic similarity which may not perfectly align with specific nuances of emotional context required for ERC.
- **Evidence:** Experiments comparing jointly trained retriever-LLM architecture against current fixed-retriever setup on IEMOCAP, MELD, and EmoryNLP benchmarks.

### Open Question 2
- **Question:** How does specific design of retrieval strategy impact model's ability to capture dynamic interaction between speaker characteristics and context?
- **Basis in paper:** [explicit] Authors conclude that "careful design retrieval methods warrants further attention and in-depth investigation," noting performance varied significantly across Random, BM25, and Contriever strategies.
- **Why unresolved:** Paper demonstrates retrieval strategy matters but does not fully explore theoretical alignment between retrieved examples' content and specific psychological demands of target conversation.
- **Evidence:** Analysis of correlation between retrieval metrics (semantic similarity vs. speaker-profile matching) and resulting emotion classification accuracy on difficult, context-dependent utterances.

### Open Question 3
- **Question:** Can framework be extended to leverage multimodal cues (audio/visual) present in datasets, rather than relying solely on text?
- **Basis in paper:** [inferred] Methodology relies exclusively on text-based LLMs and transcripts, despite utilizing datasets (IEMOCAP, MELD) that contain rich audio and video information critical for emotion recognition.
- **Why unresolved:** Emotion recognition often relies on non-verbal cues; by limiting input to text, model may fail to capture emotional states conveyed through tone or facial expression, potentially capping performance.
- **Evidence:** Implementation of multimodal InitERC variant encoding audio/video features into demonstration pool and prompt, benchmarked against text-only baseline.

## Limitations

- **Training duration unspecified:** Paper details optimizer settings but omits epoch or step count, creating risk that reported performance improvements depend on training longer than minimal reproduction would run.
- **Limited evidence for analogical reasoning:** While retrieval strategy experiments show Contriever outperforms Random/BM25, there's limited direct evidence that semantic similarity enables cross-speaker emotional inference through analogical reasoning rather than surface-level pattern matching.
- **Resource-intensive implementation:** Method requires substantial computational resources (RTX 4090, 24GB VRAM) and careful implementation of retriever-pipeline integration, making it difficult to scale or deploy in resource-constrained settings.

## Confidence

**High confidence:** Empirical claim that InitERC outperforms multi-stage baselines on three benchmark datasets is well-supported by reported F1 scores and ablation studies. Synergistic effect between retrieval and fine-tuning is validated by "w/o fine-tuning" collapse and "w/o in-context examples" drop in Table 3.

**Medium confidence:** Claim that one-stage training provides superior speaker-context-emotion alignment relies primarily on theoretical argumentation about "weak alignment" rather than direct comparative ablation of training stages. Retrieval-based emotion transfer shows empirical superiority of Contriever over Random/BM25 but lacks mechanistic explanation of why semantic similarity enables cross-speaker emotional inference.

**Low confidence:** Assertion that model learns generalizable "retrieval-to-emotion" mapping function (rather than overfitting to demonstration examples) is not directly tested - no evaluation on held-out retrieval pools or qualitative analysis of which patterns model actually learns.

## Next Checks

1. **Training Duration Sensitivity:** Run experiments varying epoch count (1, 3, 5, 10) on single dataset to identify when performance plateaus and whether claimed 20.25% improvement over baselines is robust to different training lengths.

2. **Retrieval Pool Generalization:** Evaluate InitERC on test set where retrieved examples come from entirely disjoint domains (e.g., using pool from Reddit conversations to classify IEMOCAP utterances) to test whether model has learned transferable emotional reasoning or memorized specific patterns.

3. **Cross-Speaker Transfer Analysis:** For utterances where retriever successfully identifies semantically similar contexts from different speakers, conduct qualitative analysis of whether model's predictions actually align with emotional patterns suggested by those examples, or if it's relying on other cues.