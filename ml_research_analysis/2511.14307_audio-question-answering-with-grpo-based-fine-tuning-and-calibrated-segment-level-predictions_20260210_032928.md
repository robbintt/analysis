---
ver: rpa2
title: Audio Question Answering with GRPO-Based Fine-Tuning and Calibrated Segment-Level
  Predictions
arxiv_id: '2511.14307'
source_url: https://arxiv.org/abs/2511.14307
tags:
- audio
- calibration
- question
- events
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents an Audio Question Answering (AQA) system for
  the DCASE 2025 Challenge that uses a two-stage approach: first extracting and calibrating
  acoustic event predictions using a pretrained BEATs sound event detection model,
  then using these predictions as structured prompts for a fine-tuned Qwen2.5-7B-Instruct
  language model via Group Relative Policy Optimization (GRPO). The system achieves
  an accuracy of 62.6% on the development set, outperforming baseline models such
  as Gemini-2.0-Flash (52.5%) by over 10%.'
---

# Audio Question Answering with GRPO-Based Fine-Tuning and Calibrated Segment-Level Predictions

## Quick Facts
- arXiv ID: 2511.14307
- Source URL: https://arxiv.org/abs/2511.14307
- Reference count: 21
- Primary result: Achieved 62.6% accuracy on DCASE 2025 AQA development set, outperforming baseline models by over 10%

## Executive Summary
This paper presents a cascaded Audio Question Answering system for the DCASE 2025 Challenge that first extracts and calibrates acoustic event predictions using a pretrained BEATs sound event detection model, then uses these predictions as structured prompts for a fine-tuned Qwen2.5-7B-Instruct language model via Group Relative Policy Optimization (GRPO). The system achieves strong performance by leveraging calibrated segment-level predictions and reinforcement learning fine-tuning, demonstrating that cascaded symbolic reasoning can outperform end-to-end audio-language models on certain AQA tasks. The approach shows particular strength in parts of the task where sound event detection ontology aligns with question domains.

## Method Summary
The system uses a two-stage cascaded approach: first, BEATs extracts frame-level audio features and classifies them into segment-level posterior probabilities for 527 AudioSet classes. These predictions undergo per-class logistic regression calibration on likelihood ratios, followed by prior adjustment and thresholding (0.1) to produce discrete event predictions with timestamps. The events are formatted as text strings and combined with questions and options into a structured prompt. A Qwen2.5-7B-Instruct model, fine-tuned via GRPO with LoRA (rank=16, alpha=32), receives this prompt and selects answers by computing token likelihood for each option. The GRPO fine-tuning uses group-relative advantages with a binary reward function and KL regularization to improve format adherence and reasoning capabilities.

## Key Results
- Achieved 62.6% overall accuracy on development set, outperforming Gemini-2.0-Flash (52.5%) by over 10%
- Calibration significantly improved reliability, reducing CLLR from 0.653 to 0.266 for Male Speech class
- GRPO fine-tuning with events during training improved Part 2 from 50.9% to 54.0% and Part 3 from 65.5% to 67.5%
- Cascaded approach outperformed end-to-end models even without audio input, suggesting much needed information resides in questions and answer choices

## Why This Works (Mechanism)

### Mechanism 1: Calibrated Likelihood Ratio Calibration
The system trains separate logistic regression models per class to transform raw likelihood ratios into calibrated scores, then reapplies class priors from training data to recover well-calibrated posteriors. This approach improves reliability by decoupling calibration from prior distributions, allowing consistent thresholding across classes. The evidence shows systematic CLLR reduction across 447 AudioSet classes, though gains are limited for out-of-ontology sounds like marine mammals.

### Mechanism 2: GRPO Fine-Tuning with Group-Relative Advantages
GRPO removes the need for a separate critic model by using group-relative advantages—the model generates multiple candidate outputs per prompt, and rewards are normalized against the group mean. A simple binary reward (1 for correct answer, 0 otherwise) signals success. The policy is optimized to increase probability of high-advantage responses while KL divergence to a reference policy regularizes updates. This approach improves format adherence and reasoning capabilities compared to supervised fine-tuning.

### Mechanism 3: Cascaded Symbolic Reasoning Architecture
The audio signal is processed by BEATs to produce frame-level features, classified into segment-level event probabilities, calibrated, thresholded into discrete event predictions, and formatted as text strings. The LLM receives only this structured text plus the question and options, performing linguistic reasoning without direct audio access. This approach trades direct audio reasoning for interpretability and potentially better sample efficiency, though it fails when SED ontology misses relevant sounds.

## Foundational Learning

- **Likelihood ratio calibration and proper scoring rules**: Understanding why LRs are prior-independent and how CLLR aggregates calibration error is essential for reproducing or extending the calibration module. Quick check: If a model outputs P(y=1|x)=0.7 but the true class frequency at that score is 0.9, is the model overconfident or underconfident? What does CLLR penalize?

- **Group Relative Policy Optimization (GRPO) vs PPO**: Understanding how the advantage is computed and why removing the critic can be beneficial is necessary for debugging training dynamics. Quick check: If all 8 samples in a group receive reward 0, what is the advantage for each sample? What happens to the policy gradient in that case?

- **Sound event detection ontologies and AudioSet**: Knowing that AudioSet has 527 classes and that DCASE Part 1 involves out-of-ontology marine mammals explains why calibration helped Parts 2-3 but not Part 1. Quick check: If your target domain includes sounds not in AudioSet (e.g., specific industrial machinery), what modifications would the pipeline require?

## Architecture Onboarding

- **Component map**: Audio → BEATs encoder → classification head → calibration module → median filtering + threshold 0.1 → text formatter → LLM prompt → token likelihood scoring → answer selection

- **Critical path**: Audio → BEATs → classification head → calibration → thresholding → text formatting → LLM prompt → token likelihood scoring → answer selection. Errors propagate through the pipeline: miscalibrated scores → wrong threshold decisions → missing/spurious events → misleading LLM context → wrong answer.

- **Design tradeoffs**:
  - **Cascaded vs end-to-end**: Cascaded approach trades off direct audio reasoning for interpretability and potentially better sample efficiency; fails when SED ontology misses relevant sounds
  - **Calibration on LRs vs posteriors**: LRs are prior-independent but require prior estimation step; posteriors are directly usable but calibration depends on class balance in calibration data
  - **GRPO vs SFT**: GRPO may improve format adherence and reasoning with limited data; SFT is simpler but may not enforce output constraints as effectively
  - **Threshold selection (0.1)**: Lower threshold increases recall (more events to LLM) but risks false positives; higher threshold increases precision but may miss relevant events

- **Failure signatures**:
  - **Part 1 underperformance (marine mammals)**: SED model has no corresponding AudioSet classes → events are irrelevant or misleading → calibration may hurt
  - **Format extraction failures**: LLM outputs correct reasoning but wrong format (e.g., "The answer is B" instead of "B"). GRPO with format-aware reward may help
  - **Miscalibration after domain shift**: If evaluation prior differs from training prior without adjustment, reliability curves diverge
  - **High variance in GRPO advantages**: Small group size (|G|=8) may yield unstable baselines

- **First 3 experiments**:
  1. **Ablate calibration**: Run the full pipeline with and without LR calibration; measure CLLR on a held-out subset and accuracy on each AQA part. Expect Part 2/3 gains, minimal Part 1 change or slight drop.
  2. **Vary GRPO group size**: Train with |G|∈{4,8,16}; monitor training stability (advantage variance) and final accuracy. Hypothesis: larger groups stabilize advantages but increase compute.
  3. **Probe information sources**: Following Table 4, train/inference with and without events; quantify how much performance comes from question/options alone vs acoustic context. This establishes a baseline for future improvements to SED or prompt design.

## Open Questions the Paper Calls Out
None

## Limitations
- Ontology mismatch: The system's reliance on AudioSet-based sound event detection creates fundamental limitations for AQA tasks involving out-of-ontology classes like marine mammals
- Calibration data assumptions: The calibration module assumes training and evaluation data share similar class priors, with limited validation of robustness to distributional shifts
- Simple reward function: The binary reward (1 for correct, 0 for incorrect) may be insufficient for complex reasoning tasks and may not capture nuanced aspects of answer quality

## Confidence

- **High Confidence**: Calibration mechanism effectiveness (CLLR reduction from 0.653 to 0.266 for Male Speech) is well-supported by quantitative evidence
- **Medium Confidence**: GRPO fine-tuning's contribution to reasoning improvements is supported but less definitively isolated
- **Medium Confidence**: Cascaded architecture's superiority over end-to-end models is demonstrated empirically, but analysis could be more thorough

## Next Checks

1. **Ontology Coverage Analysis**: Systematically analyze the intersection between AQA question domains and AudioSet classes to quantify the proportion of questions whose answers depend on out-of-ontology sounds versus in-ontology sounds.

2. **GRPO Reward Function Ablation**: Compare binary rewards against more nuanced reward functions (e.g., partial credit for correct reasoning, rewards for format adherence separately from correctness) to test whether the simple binary signal is truly sufficient.

3. **Cross-Domain Calibration Robustness**: Evaluate calibration performance on datasets with deliberately shifted class distributions to measure how CLLR and downstream accuracy degrade as the gap between calibration and evaluation priors increases.