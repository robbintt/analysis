---
ver: rpa2
title: 'Diagnosing our datasets: How does my language model learn clinical information?'
arxiv_id: '2505.15024'
source_url: https://arxiv.org/abs/2505.15024
tags:
- clinical
- medical
- corpora
- claims
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how open-source large language models (LLMs)
  acquire clinical knowledge from publicly available pretraining corpora. The authors
  evaluate model performance on clinical jargon interpretation using a new benchmark,
  MedLingo, and examine the frequency of relevant clinical terms in pretraining data.
---

# Diagnosing our datasets: How does my language model learn clinical information?

## Quick Facts
- arXiv ID: 2505.15024
- Source URL: https://arxiv.org/abs/2505.15024
- Authors: Furong Jia; David Sontag; Monica Agrawal
- Reference count: 40
- Models' accuracy on clinical jargon correlates with jargon frequency in pretraining data, but clinical notes use jargon far more frequently than pretraining corpora.

## Executive Summary
This paper investigates how open-source large language models (LLMs) acquire clinical knowledge from publicly available pretraining corpora. The authors evaluate model performance on clinical jargon interpretation using a new benchmark, MedLingo, and examine the frequency of relevant clinical terms in pretraining data. They find that model accuracy correlates with the frequency of clinical jargon in training corpora, but note a significant mismatch between jargon frequency in clinical notes and pretraining data. Additionally, they analyze how often disputed medical claims appear in pretraining data and how models respond to such prompts. Finally, they classify the sources of clinical information in pretraining corpora, revealing that while peer-reviewed research is the primary source, informal sources like patient forums and personal blogs also contribute. The findings highlight the need for better filtering and curation of pretraining data to ensure reliable and safe clinical applications of LLMs.

## Method Summary
The authors create MedLingo, a benchmark of 100 clinical jargon-expansion pairs extracted from MIMIC-IV discharge notes, and evaluate 10 open-source LLMs on this task. They estimate clinical jargon frequency in pretraining corpora using the WIMBD platform with relevance filtering via GPT-4o classification. For disputed claims, they sample supporting and opposing documents, classify their stance, and compute support ratios. Source classification uses GPT-4o on URL and document text. The study correlates model accuracy with estimated jargon frequency, analyzes frequency mismatches between clinical notes and pretraining data, and examines how support ratios predict model responses to disputed claims.

## Key Results
- Model accuracy on clinical jargon interpretation correlates with estimated co-occurrence frequency in pretraining corpora (Spearman ρ=0.56-0.72)
- Clinical notes use jargon far more frequently than pretraining corpora (ρ=0.15 correlation between MIMIC-IV and corpus frequencies)
- Presuppositional prompts increase supportive responses to disputed claims from 10% to 31%
- Peer-reviewed sources dominate pretraining corpora (69-88%), but commercial health sites and blogs are primary sources for disputed claim support

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model accuracy on clinical jargon interpretation correlates with estimated co-occurrence frequency of jargon-expansion pairs in pretraining corpora.
- Mechanism: When an abbreviation A and its clinical expansion E frequently co-occur in the same document within pretraining data, models can associate the shorthand with its meaning via distributional learning. The paper estimates relevant co-occurrence by sampling documents containing both terms and using GPT-4o to classify whether A actually refers to E in clinical context, then scaling this ratio to total counts.
- Core assumption: Co-occurrence in the same document signals a learnable association; models acquire this through exposure during pretraining.
- Evidence anchors:
  - [abstract]: "we find that the frequency of clinical jargon mentions across major pretraining corpora correlates with model performance"
  - [section 4.1.2]: Estimated co-occurrence frequency formula using f̂cooc = n_relevant/n
  - [section 4.2.2]: Spearman correlations between estimated counts and accuracy range 0.56–0.72; raw counts yield weaker correlations (0.44–0.66)
  - [corpus]: Weak external validation—neighbor papers discuss pretraining quality but do not replicate this specific frequency-accuracy correlation for clinical jargon.
- Break condition: If jargon-expansion pairs rarely co-occur in training data, or co-occurrences are predominantly non-clinical (e.g., "CA" as California), models fail to learn the association.

### Mechanism 2
- Claim: A distribution mismatch exists between jargon frequency in clinical notes (MIMIC-IV) and pretraining corpora, creating systematic learning gaps.
- Mechanism: Clinical documentation uses shorthand heavily due to time constraints, but these abbreviations appear sparsely in web-scale corpora. The paper quantifies this by comparing occurrence counts in MIMIC-IV discharge notes against estimated frequencies in Dolma—finding Spearman correlation of only 0.15 (p=0.13).
- Core assumption: Higher frequency in clinical notes reflects real-world clinical importance; low pretraining frequency indicates insufficient learning signal.
- Evidence anchors:
  - [abstract]: "jargon frequently appearing in clinical notes often rarely appears in pretraining corpora, revealing a mismatch"
  - [section 4.2.3]: "AVSS" appears 10,766 times in MIMIC-IV but only 12 times in Dolma; all models except Claude Sonnet 3.5 fail this test
  - [corpus]: Neighbor paper "Temporal Entailment Pretraining for Clinical Language Models" notes most approaches treat EHR as static documents, neglecting temporal structure—but does not address jargon frequency mismatch.
- Break condition: When clinical jargon is common in practice but rare in pretraining data, models cannot acquire the association regardless of model scale.

### Mechanism 3
- Claim: Models are more likely to generate disputed medical claims when the ratio of supporting documents in pretraining corpora is higher, particularly under presuppositional prompting.
- Mechanism: The paper estimates N_support by sampling documents with close co-occurrence of claim keywords, classifying stance (support/against/unknown), and computing the support ratio. Higher R_support correlates with increased likelihood of claim propagation in model outputs, though the effect is stronger for ratio than raw counts (Spearman ρ=0.28 vs. -0.20).
- Core assumption: Models parrot patterns from training data; presuppositional prompts ("How does X cause Y?") bypass safety guardrails by presupposing the claim is true.
- Evidence anchors:
  - [abstract]: "models can propagate unsupported medical claims when prompted in certain ways, and that such claims are often supported in pretraining data"
  - [section 5.2]: Direct queries yield 10% supportive responses; presuppositional prompts yield 31% supportive responses
  - [section 6.2]: Documents supporting disputed claims come primarily from commercial health sites, blogs, and news—minimal from peer-reviewed sources
  - [corpus]: No direct external validation; neighbor papers do not address unsupported claim propagation mechanisms.
- Break condition: When disputed claims are frequently supported but rarely refuted in pretraining data, and prompts presuppose validity, models generate misleading content.

## Foundational Learning

- Concept: **Spearman correlation (ρ)**
  - Why needed here: The paper uses Spearman correlation throughout to measure monotonic relationships between frequency and accuracy, rather than Pearson which assumes linearity. Understanding that ρ ranges from -1 to 1 and captures rank-order relationships is essential for interpreting results (e.g., ρ=0.72 is strong, ρ=0.15 is negligible).
  - Quick check question: If model accuracy increases with log-frequency but not linearly, which correlation coefficient would better capture this relationship?

- Concept: **Co-occurrence estimation with human-in-the-loop validation**
  - Why needed here: Raw co-occurrence counts conflate relevant and irrelevant associations. The paper's method—sampling documents, using GPT-4o to classify relevance, then scaling—improves signal quality. This is a practical pattern for noisy corpus analysis.
  - Quick check question: Why is f̂cooc multiplied by C_cooc(A,E) rather than using raw C_cooc directly?

- Concept: **Presuppositional vs. direct prompts**
  - Why needed here: The paper shows prompt framing dramatically affects safety—direct queries ("Is there evidence?") get refutations, presuppositional ("How does X cause Y?") elicit fabricated explanations. This has direct implications for adversarial robustness.
  - Quick check question: If a prompt presupposes a false claim is true, what type of response is more likely from an LLM trained on web-scale corpora?

## Architecture Onboarding

- Component map:
  MedLingo dataset (100 jargon pairs) -> WIMBD platform for corpus queries -> GPT-4o relevance classification -> Estimated frequency calculation -> Model accuracy evaluation -> Source classification pipeline (URLs + text -> GPT-4o -> 9 categories)

- Critical path:
  1. Define clinical jargon-expansion pairs from target domain (e.g., MIMIC-IV)
  2. Query pretraining corpora for raw co-occurrence counts
  3. Sample and classify relevance to filter noise
  4. Compute estimated frequency and correlate with model accuracy
  5. Classify document sources to identify where knowledge/misinformation originates

- Design tradeoffs:
  - **Sampling size vs. precision**: Paper uses n≤500 due to resource constraints; larger samples would yield tighter confidence intervals
  - **Estimated vs. raw counts**: Estimated counts require additional computation but yield stronger correlations (0.56–0.72 vs. 0.44–0.66)
  - **Context window for classification**: First 5000 characters balances context richness against API costs

- Failure signatures:
  - Low correlation between MIMIC-IV and corpus frequencies (ρ=0.15) indicates pretraining data does not reflect clinical reality
  - High support ratio for disputed claims (e.g., "fluoride-cancer" at 61% in Dolma) predicts model vulnerability
  - Models failing on jargon with <100 estimated occurrences (Table 12) indicates insufficient training signal

- First 3 experiments:
  1. **Replicate frequency-accuracy correlation**: Select 20 clinical abbreviations from your target institution's notes, estimate co-occurrence frequency in your pretraining corpus, and measure model accuracy on expansion task. Expect ρ > 0.5 if mechanism holds.
  2. **Test presuppositional vulnerability**: Create 10 disputed claim pairs, query models with both direct and presuppositional prompts. Expect 2-3x increase in supportive responses under presuppositional framing.
  3. **Source audit for your domain**: Sample 100 documents containing domain-specific jargon, classify sources. If peer-reviewed <30%, expect weaker jargon understanding; if commercial health/blogs >40% for medical claims, expect higher misinformation risk.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do LLMs rely on memorization versus reasoning when performing clinical diagnosis tasks based on pretraining corpora?
- Basis in paper: [explicit] The authors state, "future work should also examine how pretraining corpora may reveal whether models are memorizing vs. reasoning for diagnosis tasks."
- Why unresolved: The current study focuses on jargon interpretation and claim frequency, which are more indicative of memorization and data presence than complex diagnostic reasoning.
- What evidence would resolve it: Studies that correlate diagnostic task performance with the specific presence of reasoning chains versus simple factual co-occurrences in the training data.

### Open Question 2
- Question: Can influence functions accurately trace specific correct and incorrect clinical outputs back to their source inputs in pretraining corpora?
- Basis in paper: [explicit] The authors explicitly propose "exploring how influence functions can estimate which inputs in pretraining corpora led to the generation of both correct and incorrect outputs."
- Why unresolved: This study utilized frequency counting tools (WIMBD) to estimate data presence rather than attribution methods like influence functions to link specific data points to outputs.
- What evidence would resolve it: Applying influence function analysis to the open-source models and corpora studied to identify specific training documents responsible for model errors.

### Open Question 3
- Question: Does the relationship between pretraining frequency and model performance hold across diverse clinical settings outside of the ICU?
- Basis in paper: [explicit] The authors note that their "analysis with MedLingo centered on jargon from a single hospital, only from the ICU" and identify "significant future work requires expanding our analysis to additional clinical settings."
- Why unresolved: The MedLingo dataset and the MIMIC-IV notes used for evaluation are derived specifically from an ICU environment, limiting generalizability.
- What evidence would resolve it: Constructing new evaluation datasets from non-ICU departments (e.g., outpatient, oncology) and repeating the frequency-performance correlation analysis.

## Limitations
- The frequency-accuracy correlation relies on estimated counts rather than ground truth, introducing uncertainty
- GPT-4o is used for relevance classification and source annotation, which may not perfectly align with human judgment
- The study focuses on open-source models and publicly available corpora, limiting generalizability to proprietary systems
- MedLingo uses minimal context to prevent leakage, potentially underestimating real-world clinical jargon understanding

## Confidence
- **High Confidence**: The correlation between clinical jargon frequency and model accuracy (ρ=0.56-0.72) is well-supported by multiple models and robust statistical testing. The mismatch between MIMIC-IV and pretraining frequencies (ρ=0.15) is clearly demonstrated across all tested models.
- **Medium Confidence**: The mechanism of presupposition-driven misinformation propagation is demonstrated but based on a limited set of claims (11 total). The classification of document sources relies on automated annotation which, while validated, may not capture nuanced distinctions between source types.
- **Low Confidence**: The external validity of frequency estimates for proprietary corpora is unknown. The assumption that co-occurrence frequency directly drives learning may oversimplify the complex mechanisms of contextual learning in LLMs.

## Next Checks
1. **Cross-domain validation**: Replicate the frequency-accuracy correlation using jargon from radiology reports and outpatient clinical notes, comparing against the same pretraining corpora to test domain generalizability.
2. **Human annotation validation**: Manually annotate 100 sampled documents used for co-occurrence estimation to quantify the error rate in GPT-4o classification and assess impact on frequency estimates.
3. **Safety intervention testing**: Design and test prompt engineering strategies (e.g., "Is there evidence for this claim?") to reduce presupposition-driven misinformation propagation, measuring effectiveness across multiple disputed medical claims.