---
ver: rpa2
title: DNA 1.0 Technical Report
arxiv_id: '2501.10648'
source_url: https://arxiv.org/abs/2501.10648
tags:
- language
- korean
- tasks
- data
- instruct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DNA 1.0 8B Instruct is a bilingual language model optimized for
  Korean and English tasks. The model is built on Llama 3.1 8B architecture and enhanced
  through continual pre-training with high-quality Korean datasets, supervised fine-tuning,
  SLERP merging with Llama 3.1 8B Instruct, and direct preference optimization.
---

# DNA 1.0 Technical Report

## Quick Facts
- arXiv ID: 2501.10648
- Source URL: https://arxiv.org/abs/2501.10648
- Authors: Jungyup Lee; Jemin Kim; Sang Park; SeungJae Lee
- Reference count: 5
- Primary result: State-of-the-art bilingual Korean-English performance (KMMLU 53.26%, KoBEST 83.40%, MMLU 66.64%)

## Executive Summary
DNA 1.0 8B Instruct is a bilingual language model optimized for Korean and English tasks. Built on Llama 3.1 8B architecture, it achieves state-of-the-art performance on Korean-specific benchmarks while maintaining strong English capabilities. The model employs continual pre-training with high-quality Korean datasets, supervised fine-tuning, SLERP merging with Llama 3.1 8B Instruct, and direct preference optimization. DNA 1.0 8B Instruct demonstrates efficient parameter utilization and robust long-context performance up to 32K tokens.

## Method Summary
DNA 1.0 8B Instruct is developed through a multi-stage optimization pipeline. The process begins with continual pre-training (CPT) on Llama 3.1 8B using curated Korean datasets to expand language coverage while preserving English competence. Supervised fine-tuning (SFT) follows with instruction datasets, heavily augmented with synthetic data. The model is then merged with Llama 3.1 8B Instruct using spherical linear interpolation (SLERP) to combine specialized capabilities. Direct preference optimization (DPO) refines alignment through both offline (pre-compiled preference pairs) and online (reward model) training. Finally, knowledge distillation transfers reasoning capabilities from larger teacher models via synthetic data generation and SKLD loss optimization.

## Key Results
- KMMLU 53.26% and KoBEST 83.40% lead Korean-language benchmarks
- MMLU 66.64% and GSM8K 80.52% demonstrate preserved English capabilities
- LongBench v2 30.8% and NIAH success up to 32K tokens validate long-context performance
- Efficient 8B parameter utilization achieves bilingual optimization without model scale expansion

## Why This Works (Mechanism)

### Mechanism 1: Continual Pre-training for Language Injection
- Claim: CPT on high-quality Korean data expands language coverage while preserving English competence
- Mechanism: Stage-wise CPT incrementally strengthens Korean token representations and cross-lingual alignment while monitoring both Korean and English validation sets to avoid catastrophic forgetting
- Core assumption: Base Llama 3.1 8B representations are sufficiently plastic to accommodate new language patterns
- Evidence: CPT with Korean datasets creates enhanced Korean capabilities while maintaining English performance

### Mechanism 2: SLERP Merging for Bilingual Capability Fusion
- Claim: SLERP combines Korean-optimized and English-optimized models more effectively than simple weight averaging
- Mechanism: SLERP interpolates along geodesic arc in high-dimensional weight space, preserving directional structure and avoiding capability collisions through dynamic interpolation rates
- Core assumption: Language-specific capabilities are encoded in separable weight subspaces that can be selectively merged
- Evidence: SLERP chosen over linear interpolation for sophisticated handling of weight interpolation in high-dimensional spaces

### Mechanism 3: Two-Stage DPO with Knowledge Distillation
- Claim: Combining offline and online DPO plus knowledge distillation yields compact but capable models
- Mechanism: Offline DPO uses pre-compiled preference data for hard-to-evaluate tasks; online DPO uses reward model for real-time alignment; KD transfers reasoning traces from larger teachers via SKLD loss
- Core assumption: Preference signals and teacher outputs provide sufficient supervision for student models to approximate expert reasoning
- Evidence: Both offline and online DPO training implemented, with KD from Llama 3.1 405B and Qwen2.5 72B

## Foundational Learning

- **Continual Pre-training (CPT)**: Understanding how new language capabilities are added without full retraining is essential to grasp why DNA 1.0 can be efficient while achieving bilingual performance. Quick check: Can you explain why CPT differs from training from scratch, and what risks it introduces regarding previously learned knowledge?

- **Spherical Linear Interpolation (SLERP)**: Model merging is a key innovation in this work; understanding why spherical interpolation outperforms linear averaging explains the performance gains. Quick check: Why might averaging model weights directly cause capability loss in high-dimensional parameter spaces?

- **Direct Preference Optimization (DPO)**: DPO replaces more complex RLHF pipelines; understanding its objective helps explain how alignment is achieved efficiently. Quick check: How does DPO differ from PPO-based RLHF, and what tradeoffs does it introduce?

## Architecture Onboarding

- **Component map**: Llama 3.1 8B -> CPT (Korean data) -> SFT (instruction following) -> SLERP merge (with Llama 3.1 8B Instruct) -> Offline DPO (reasoning, safety) -> Online DPO (quality alignment) -> Knowledge Distillation (teacher transfer)

- **Critical path**: CPT → SFT → SLERP merge → Offline DPO → Online DPO → Knowledge Distillation

- **Design tradeoffs**: 8B scale provides efficient inference but limited capacity vs. larger teachers; SLERP over linear merge is more complex but preserves bilingual capabilities better; synthetic data in SFT scales coverage but requires rigorous quality filtering; RoPE base=500,000 enables long context but requires validation for stability

- **Failure signatures**: English capability drop indicates CPT over-training or imbalanced merge weights; Korean benchmark underperformance suggests insufficient Korean data quality; long-context retrieval failure indicates RoPE scaling issues; alignment failures suggest preference data lacks safety coverage

- **First 3 experiments**: 1) Ablation: CPT data volume vs. Korean benchmark performance, varying Korean token count while tracking MMLU degradation; 2) Merge weight sensitivity analysis, testing interpolation parameter t values across layers (0.3-0.7) on both Korean and English benchmarks; 3) Long-context stress test beyond 32K, running NIAH at 64K and 128K contexts to validate 128K support

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the performance gap between arithmetic reasoning (GSM8K: 80.5%) and advanced mathematical problem-solving (MATH: 34.9%) be reduced for specialized bilingual models? The paper identifies the discrepancy but does not propose or evaluate interventions for high-level mathematical reasoning.

- **Open Question 2**: Does DNA 1.0 8B Instruct maintain robust long-context performance beyond 32K tokens up to its architectural limit of 128K? NIAH evaluation only tests up to 32K tokens, leaving full context window capability unverified.

- **Open Question 3**: Can general knowledge capabilities (MMLU, BBH) be improved without compromising Korean-language specialization? The paper acknowledges the gap but does not investigate whether training interventions for general knowledge trade off with Korean benchmarks.

- **Open Question 4**: Are the optimal SLERP interpolation ratios specific to Korean-English model pairs, or generalizable to other language pairs? Without reported ratios and cross-lingual transfer experiments, generalizability remains unknown.

## Limitations

- Critical experimental details omitted, including CPT hyperparameters, SLERP interpolation ratios, Korean preference dataset composition, and KD implementation specifics
- Performance gap remains between arithmetic (GSM8K: 80.5%) and advanced mathematical problem-solving (MATH: 34.9%)
- Long-context capability claimed (128K theoretical) but only empirically validated up to 32K tokens

## Confidence

- High confidence: Korean benchmark performance (KMMLU 53.26%, KoBEST 83.40%, BELEBELE 57.99%) - standard, verifiable metrics with published baselines
- Medium confidence: English capability preservation (MMLU 66.64%, GSM8K 80.52%) - reasonable given reported methodology but lacks direct ablation studies
- Medium confidence: Long-context capability (NIAH 32K success) - demonstrated but lacks stress testing beyond reported range
- Low confidence: SLERP mechanism effectiveness - theoretically sound but lacks empirical validation against simpler alternatives
- Low confidence: DPO and KD optimization - described but lacks detailed hyperparameter reporting and ablation analysis

## Next Checks

1. **CPT Stability Boundary Test**: Systematically vary Korean pre-training token count (1B, 2B, 4B, 6B tokens) while monitoring English capability (MMLU) degradation to identify the exact point where Korean gains plateau while English performance remains stable.

2. **SLERP Merge Ablation**: Compare DNA 1.0 performance against models created using uniform linear interpolation (t=0.5) and no merging control, testing multiple layer-wise t configurations to determine if reported dynamic rates provide statistically significant improvements.

3. **Long-Context Robustness Analysis**: Evaluate NIAH performance at 64K and 128K token contexts to validate the claimed 128K RoPE support, tracking retrieval accuracy, attention stability, and any context-length-dependent degradation patterns.