---
ver: rpa2
title: 'Failures Are the Stepping Stones to Success: Enhancing Few-Shot In-Context
  Learning by Leveraging Negative Samples'
arxiv_id: '2507.23211'
source_url: https://arxiv.org/abs/2507.23211
tags:
- negative
- positive
- examples
- samples
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for improving few-shot in-context
  learning (ICL) by leveraging negative samples. The approach involves constructing
  positive and negative sample corpora based on zero-shot CoT inference, then using
  semantic similarity to retrieve relevant examples for test queries.
---

# Failures Are the Stepping Stones to Success: Enhancing Few-Shot In-Context Learning by Leveraging Negative Samples

## Quick Facts
- arXiv ID: 2507.23211
- Source URL: https://arxiv.org/abs/2507.23211
- Reference count: 29
- Primary result: Incorporates negative samples into few-shot ICL via negative-anchored positive retrieval, achieving 1.6-7.6% accuracy improvements across seven reasoning datasets

## Executive Summary
This paper introduces a novel method for improving few-shot in-context learning by leveraging negative samples. The approach constructs positive and negative sample corpora based on zero-shot CoT inference, then uses semantic similarity to retrieve relevant examples for test queries. For each query, the method retrieves both positive and negative examples, then further retrieves new positive examples based on the negative samples. These positive examples are concatenated to form the final demonstration. Experiments on seven datasets across three reasoning tasks show that the proposed method significantly outperforms baseline approaches that rely solely on the most similar positive examples.

## Method Summary
The method operates in two stages: corpus construction and demonstration construction. In corpus construction, the training data is clustered using k-means, then split into train/test subsets per cluster. Zero-Shot-CoT inference is run on the training subset to classify samples as positive (correct predictions) or negative (incorrect predictions). In demonstration construction, for each test query, the method retrieves k/2 positive and k/2 negative examples via semantic similarity, then retrieves new positive examples most similar to each negative example. These positive examples are concatenated to serve as ICL demonstrations for the LLM.

## Key Results
- The proposed method significantly outperforms baseline approaches that rely solely on the most similar positive examples
- Task-specific optimal negative sample ratios are identified: arithmetic/symbolic tasks benefit from 2 negatives, while commonsense tasks achieve highest performance with exclusively negative-anchored positives
- The approach demonstrates consistent improvements across seven datasets spanning arithmetic, commonsense, and symbolic reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Negative-Anchored Positive Retrieval
- Claim: Retrieving positive examples that are semantically similar to negative samples surfaces "implicit error correction" patterns without requiring explicit error analysis from stronger models.
- Mechanism: When the LLM fails on a query type, those failed examples (negative samples) are structurally similar to other cases where the model would also struggle. By finding *correct* examples (positive samples) that resemble these failures, the demonstration provides corrective reasoning patterns precisely where the model is weak.
- Core assumption: Negative samples cluster around specific reasoning error modes; positives similar to negatives contain correct reasoning that addresses those error modes.
- Evidence anchors: [abstract] "we further retrieve Positive examples from the Positive sample corpus based on semantic similarity to the Negative examples"; [Section 1] "we believe that the new positive examples retrieved based on negative samples can be viewed as implicit error correction data, eliminating the need for explicit utilization of stronger LLM like GPT-4 for error correction"
- Break condition: If negative samples are distributed randomly (not clustered around error modes), retrieved positives will not provide targeted correction and may introduce noise.

### Mechanism 2: Dual-Coverage Demonstration Composition
- Claim: Combining query-similar positives with negative-anchored positives provides broader coverage of reasoning patterns than either strategy alone.
- Mechanism: Query-similar positives address "known" reasoning patterns; negative-anchored positives address "edge case" patterns where the model fails. The concatenation ensures the demonstration spans both proficient and failure-prone regions of the problem space.
- Core assumption: The optimal demonstration contains examples from multiple regions of the semantic space, not just the nearest neighbors to the query.
- Evidence anchors: [Section 2.2] "concatenating them with the previously selected Positive examples to serve as ICL demonstrations"; [Section 3.2, Table 2] Mixed m/n ratios (e.g., m=4, n=2 for AddSub at 88.6%) outperform pure positive retrieval (m=6, n=0 at 84.0%)
- Break condition: If the query is already in a high-accuracy region, adding negative-anchored positives may dilute relevance without benefit.

### Mechanism 3: Task-Specific Optimal Negative Sample Ratio
- Claim: Different reasoning tasks benefit from different proportions of negative-anchored examples due to varying error distributions and knowledge requirements.
- Mechanism: Arithmetic/symbolic tasks require limited error correction (2 negatives optimal); commonsense tasks benefit from denser error-driven signals (all 6 examples from negative-anchored retrieval optimal in CommonsenseQA).
- Core assumption: Error modes differ across task types—procedural errors (arithmetic) are localized; associative errors (commonsense) are diffuse.
- Evidence anchors: [Section 3.2] "for arithmetic and symbolic reasoning tasks, the optimal number of negative samples to use is two, whereas for commonsense reasoning problems, employing exclusively newly retrieved positive examples through negative sample retrieval results in the highest performance"; [Section 3.2, Table 2] CommonsenseQA: m=0, n=6 achieves 80.0% vs. m=6, n=0 at 78.3%
- Break condition: Applying a fixed ratio across tasks will underperform; semantic drift from excessive negatives degrades accuracy (observed in Last Letter at n=4, n=6).

## Foundational Learning

- **In-Context Learning (ICL)**
  - Why needed here: The entire method operates within the ICL paradigm—understanding that demonstrations influence predictions without weight updates is essential.
  - Quick check question: Can you explain why ICL demonstration ordering affects model output?

- **Chain-of-Thought (CoT) Prompting**
  - Why needed here: Zero-Shot-CoT is used to generate the initial corpus predictions; understanding CoT reasoning chains is necessary for interpreting positive/negative sample generation.
  - Quick check question: What trigger phrase initiates Zero-Shot-CoT, and how does it differ from standard few-shot CoT?

- **Semantic Similarity Retrieval (Embedding-based)**
  - Why needed here: The method relies on Sentence-BERT embeddings to compute similarity between queries, positives, and negatives; misunderstanding retrieval mechanics will break implementation.
  - Quick check question: How does cosine similarity between query and corpus embeddings determine which examples are retrieved?

## Architecture Onboarding

- **Component map:**
  - Corpus Construction: K-means clustering → random split → Zero-Shot-CoT inference → positive/negative corpus creation
  - Demonstration Construction: Query → retrieve k/2 positives + k/2 negatives → for each negative, retrieve most-similar positive → concatenate all positives → prompt LLM

- **Critical path:**
  1. Ensure clustering produces balanced train/test splits (half per cluster)
  2. Zero-Shot-CoT inference must be run on full training set to populate both corpora
  3. At inference time, the negative-to-positive retrieval step is the differentiator—do not skip it

- **Design tradeoffs:**
  - Corpus size vs. retrieval latency: More clusters increase granularity but require more storage
  - k value (demonstration length): Paper uses k=2 due to API cost; larger k may improve accuracy but increase token costs
  - Negative ratio (m/n): Task-specific tuning required; no universal optimal setting

- **Failure signatures:**
  - Accuracy drops below random baseline → likely missing negative-to-positive retrieval step
  - CommonsenseQA underperforms → try n=k (all negative-anchored positives)
  - Last Letter Concatenation degrades → reduce n (semantic drift from excessive negatives)

- **First 3 experiments:**
  1. Reproduce Table 1 baseline comparison on a single dataset (e.g., AddSub) with k=2 to validate negative-anchored retrieval adds value over similarity-only retrieval.
  2. Ablate the negative-to-positive retrieval: compare full method vs. (k positives only) vs. (k negative-anchored positives only) to quantify each component's contribution.
  3. Sweep m/n ratios on your target task type to identify optimal balance before production deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the optimal ratio of negative-to-positive samples transfer effectively to open-source or larger proprietary LLM architectures?
- Basis in paper: [inferred] The "Implementation Details" section restricts evaluation to GPT-3.5-Turbo, leaving the generalizability of the specific $m/n$ sample ratios to other model families unverified.
- Why unresolved: Different model architectures may exhibit varying sensitivities to "error-driven" signals, potentially altering the ideal balance between direct positive examples and negatively-retrieved positives.
- What evidence would resolve it: Replicating the ablation study on sample quantity (Table 2) using diverse model families (e.g., Llama 3, GPT-4).

### Open Question 2
- Question: Can the "semantic drift" observed when using higher numbers of negative samples be mitigated by employing more sophisticated retrieval metrics?
- Basis in paper: [inferred] The authors identify semantic drift as a key failure mode in the results but rely solely on basic Sentence-BERT cosine similarity for retrieval.
- Why unresolved: It is unclear if the performance decline is due to the inherent noise of negative samples or the inability of simple cosine similarity to capture nuanced "error-relatedness."
- What evidence would resolve it: Comparing the current retrieval method against cross-encoders or re-ranking strategies for the second-stage retrieval step.

### Open Question 3
- Question: Why does commonsense reasoning benefit from exclusively negatively-retrieved positives ($m=0$) while arithmetic tasks require a mix?
- Basis in paper: [explicit] The paper states, "We hypothesize that this discrepancy stems from the nature of the required knowledge," suggesting commonsense reasoning relies on "denser error-driven corrective signals."
- Why unresolved: This remains a hypothesis to explain empirical results; the specific mechanism (e.g., "decision boundary tightening") has not been isolated or visualized.
- What evidence would resolve it: A fine-grained error analysis comparing the procedural steps of arithmetic against the associative nature of commonsense reasoning in the context of negative samples.

## Limitations

- The method's effectiveness depends entirely on the quality of Zero-Shot-CoT predictions used for corpus construction, as the answer extraction/cleaning procedure is underspecified
- The optimal negative-to-positive ratio appears highly task-dependent, requiring task-specific tuning rather than being a universal solution
- The claim of "implicit error correction" without needing stronger models like GPT-4 is questionable given the method's reliance on zero-shot predictions for corpus quality

## Confidence

- High confidence: The core mechanism of retrieving positive examples based on negative samples is well-specified and experimentally validated through direct comparisons showing 1.6-7.6% accuracy improvements over baselines
- Medium confidence: Task-specific optimal ratios are supported by experimental data but lack theoretical grounding for why arithmetic requires fewer negative samples than commonsense tasks
- Low confidence: The claim that this approach provides "implicit error correction" without needing stronger models like GPT-4 for explicit error analysis, as the method's effectiveness depends entirely on the quality of Zero-Shot-CoT predictions used for corpus construction

## Next Checks

1. Reproduce the baseline comparison (Zero-Shot-CoT, Similarity-Based Few-Shot, Random Few-Shot) on AddSub with k=2 to verify the 1.6% improvement claim and ensure the negative-to-positive retrieval mechanism is functioning
2. Implement the negative-to-positive retrieval ablation: compare full method vs. similarity-only retrieval vs. negative-anchored positives only to isolate the contribution of each component
3. Sweep m/n ratios on a commonsense task (CommonsenseQA) to confirm that n=6 outperforms balanced ratios, and test whether excessive negatives degrade performance on symbolic tasks like Last Letter Concatenation