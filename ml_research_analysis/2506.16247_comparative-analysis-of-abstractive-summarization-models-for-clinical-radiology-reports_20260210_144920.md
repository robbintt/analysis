---
ver: rpa2
title: Comparative Analysis of Abstractive Summarization Models for Clinical Radiology
  Reports
arxiv_id: '2506.16247'
source_url: https://arxiv.org/abs/2506.16247
tags:
- summarization
- radiology
- language
- evaluation
- rehman
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates multiple abstractive summarization models
  for generating clinical impressions from radiology findings in the MIMIC-CXR dataset.
  Six models were compared: BART-base, T5-base, PEGASUS-x-base, ChatGPT-4, LLaMA-3-8B,
  and a Pointer Generator Network with coverage mechanism.'
---

# Comparative Analysis of Abstractive Summarization Models for Clinical Radiology Reports

## Quick Facts
- **arXiv ID:** 2506.16247
- **Source URL:** https://arxiv.org/abs/2506.16247
- **Reference count:** 34
- **Primary result:** BART-base fine-tuned model achieved ROUGE-1 0.37, ROUGE-2 0.22, ROUGE-L 0.33, METEOR 0.34, BERTScore 0.88 on MIMIC-CXR dataset

## Executive Summary
This study evaluates six abstractive summarization models for generating clinical impressions from radiology findings in the MIMIC-CXR dataset. The research demonstrates that fine-tuned transformer-based models significantly outperform zero-shot large language models and traditional pointer-generator networks. BART-base emerged as the top performer across all automated metrics, with human evaluation corroborating these results. The findings provide actionable insights for healthcare automation by identifying optimal model architectures for medical text summarization.

## Method Summary
The study fine-tuned BART-base, T5-base, PEGASUS-x-base, LLaMA-3-8B, and a Pointer Generator Network with coverage mechanism on a subset of the MIMIC-CXR dataset (2,400 training samples, 500 test samples). Models were evaluated using ROUGE-1, ROUGE-2, ROUGE-L, METEOR, and BERTScore metrics, with additional human evaluation by four NLP Master's students. Training used specific hyperparameters including learning rates (3e-5 for BART/T5/PEGASUS, 2e-4 for LLaMA-3-8B), batch sizes, and maximum sequence lengths.

## Key Results
- BART-base achieved the best overall performance with ROUGE-1 of 0.37, ROUGE-2 of 0.22, and ROUGE-L of 0.33
- Human evaluation favored T5-base and BART-base for clinical summary quality over other models
- Fine-tuned transformer models significantly outperformed zero-shot LLMs (ChatGPT-4) and traditional methods
- LLaMA-3-8B showed promising results but required LoRA fine-tuning due to resource constraints
- Pointer Generator Network with coverage mechanism consistently underperformed across all metrics

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning encoder-decoder transformers on domain-specific radiology data improves summarization coherence and factual alignment compared to zero-shot general-purpose LLMs. Models like BART and T5 use denoising autoencoder or span-corruption pre-training objectives. Fine-tuning adapts the bidirectional encoder to understand specific medical relationships in the "Findings" and conditions the autoregressive decoder to generate the concise "Impression" style.

### Mechanism 2
Large Language Models operating in zero-shot modes rely on prompt engineering and parametric knowledge, yielding high fluency but variable factual precision compared to fine-tuned smaller models. LLMs like ChatGPT-4 map input context to vast internal weights without gradient updates, generating fluent text but potentially including extraneous details or missing subtle domain-specific nuances.

### Mechanism 3
Standard Pointer Generator Networks with coverage mechanisms struggle to compete with modern transformer architectures in complex abstractive tasks due to instability in decoding long sequences. PGNs attempt to combine extraction and generation, but without deep bidirectional context of transformers, they can suffer from decoding drift, leading to repetitive loops or failure to synthesize novel abstractive text.

## Foundational Learning

- **Concept: Encoder-Decoder Architecture (e.g., BART/T5)**
  - **Why needed here:** This is the structural distinction between top performers (BART/T5) and others. Understand how the encoder processes full "Findings" context simultaneously while the decoder generates "Impression" token-by-token.
  - **Quick check question:** How does the bidirectional nature of BART's encoder differ from unidirectional attention of standard GPT decoder, and why might that aid in understanding dense medical text?

- **Concept: Fine-Tuning vs. Zero-Shot Inference**
  - **Why needed here:** The study hinges on performance delta between updating model weights (Fine-Tuning) vs. prompting frozen model (Zero-Shot). Understanding this trade-off is critical for system design (cost vs. accuracy).
  - **Quick check question:** Why would smaller model (BART-base, ~140M params) outperform massive model (ChatGPT-4) when former is fine-tuned and latter is not?

- **Concept: Evaluation Metrics (ROUGE vs. BERTScore)**
  - **Why needed here:** The study relies on these metrics to claim superiority. ROUGE measures lexical overlap, while BERTScore measures semantic similarity. Distinguish why high BERTScore with lower ROUGE might still indicate "good" summary.
  - **Quick check question:** If model paraphrases ground truth perfectly but uses none of original words, which metric (ROUGE-L or BERTScore) would better capture quality of summary?

## Architecture Onboarding

- **Component map:** Input (MIMIC-CXR "Findings") -> Tokenization -> Model Layer (6 parallel paths) -> Generated "Impression" -> Evaluation (ROUGE/METEOR/BERTScore + Human Review)

- **Critical path:**
  1. Data Loading: Ingest MIMIC-CXR (2,400 train, 500 test)
  2. Tokenization: Ensure sequence length of 512 (input) and 150 (output)
  3. Fine-Tuning (Winning Path): Load `facebook/bart-base`, set lr=3e-5, epochs=5. Train to minimize Cross-Entropy Loss
  4. Inference: Generate summaries for 500 test samples
  5. Validation: Run ROUGE calculations and check against Table 2 baselines

- **Design tradeoffs:**
  - **BART-base (Recommended):** Highest accuracy and lowest hardware cost (runs on Google Colab T4). Best for controlled, on-premise deployment
  - **ChatGPT-4:** Highest fluency/semantic score (BERTScore 0.87) but requires API dependency and may hallucinate non-essential details. Best for drafts requiring human review
  - **LLaMA-3-8B:** High resource cost (LoRA required) and lower performance than BART in this specific few-shot setting

- **Failure signatures:**
  - **Repetitive Loops:** (PGN) "cardiopulmonary vascular vascular vascular..." -> Switch to Transformer architecture
  - **Hallucination:** (LLMs) Adding facts not in source -> Tighten prompt constraints or switch to fine-tuned BART
  - **Omission:** (BART/T5) Missing "bibasilar opacities" -> Increase beam width or adjust length penalty

- **First 3 experiments:**
  1. Reproduce BART Baseline: Fine-tune `BART-base` on provided 2,400 sample split to verify ROUGE-1 ~0.37 benchmark
  2. Ablation on PGN: Run PGN model to reproduce failure mode (repetitive output) to understand decoding instability
  3. Prompt Sensitivity Test: Run ChatGPT-4 with provided prompt vs. simplified prompt on 10 samples to observe shift in "non-essential content" inclusion

## Open Questions the Paper Calls Out

- Does the performance ranking of BART-base and T5-base over ChatGPT-4 persist when evaluated by board-certified radiologists rather than NLP students?
- Does the superiority of fine-tuned BART-base generalize when trained on the full MIMIC-CXR dataset rather than a 3,000-record subset?
- Would fine-tuning ChatGPT-4 close the performance gap with smaller, fine-tuned encoder-decoder models?

## Limitations

- Small sample size (2,400 training samples) raises concerns about generalization to rare pathologies
- Human evaluation methodology not fully detailed (number of evaluators, inter-rater reliability)
- Limited comparison to other domain-specific summarization models beyond the six evaluated

## Confidence

- **High Confidence:** BART-base outperforming zero-shot LLMs (supported by quantitative metrics and multiple case studies)
- **Medium Confidence:** Clinical superiority claims (based on single annotator human evaluation without reliability metrics)
- **Low Confidence:** Generalizability claims beyond the MIMIC-CXR dataset (not validated on external data)

## Next Checks

1. External Validation: Test BART-base on a separate radiology dataset (e.g., OpenI) to verify generalizability
2. Ablation Study: Systematically vary training sample sizes (500, 1000, 2000) to quantify data efficiency
3. Clinical Expert Review: Deploy BART and T5 outputs to a panel of radiologists for blinded assessment of clinical accuracy and safety