---
ver: rpa2
title: 'Powering Job Search at Scale: LLM-Enhanced Query Understanding in Job Matching
  Systems'
arxiv_id: '2509.09690'
source_url: https://arxiv.org/abs/2509.09690
tags:
- query
- search
- linkedin
- understanding
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified query understanding framework powered
  by a Large Language Model (LLM) for job search applications. Traditional approaches
  using multiple Named Entity Recognition models were fragmented and difficult to
  maintain.
---

# Powering Job Search at Scale: LLM-Enhanced Query Understanding in Job Matching Systems

## Quick Facts
- arXiv ID: 2509.09690
- Source URL: https://arxiv.org/abs/2509.09690
- Reference count: 38
- Unified LLM framework reduces system complexity by over 75% while improving search relevance

## Executive Summary
This paper presents a unified query understanding framework powered by a Large Language Model (LLM) for job search applications. Traditional approaches using multiple Named Entity Recognition models were fragmented and difficult to maintain. The proposed solution uses a single LLM to jointly model user queries and contextual signals like profile attributes, enabling structured interpretations that improve personalization and relevance. A multi-task instruction tuning approach with homogeneous batching was used to train the model, which was deployed using vLLM with streaming mode for low-latency serving. Offline evaluation showed improvements in geographic location and company tagging compared to legacy NER models. Online A/B testing demonstrated a 33% increase in NDCG and a 59% reduction in poor matches among top 10 results, with a median latency of 400ms and P95 latency of 600ms.

## Method Summary
The framework uses a single fine-tuned LLM (Qwen2.5-1.5B) to replace multiple specialized NER models for query understanding tasks including tagging, rewriting, and planning. Multi-task instruction tuning with homogeneous batching (each mini-batch from a single task) was used to train the model. vLLM serving with streaming mode enables low-latency inference by allowing incremental parsing of JSON outputs while parallel downstream components execute concurrently. The system jointly models user queries with profile attributes to resolve implicit and self-referential queries, producing structured interpretations for candidate selection and ranking.

## Key Results
- 33% increase in NDCG and 59% reduction in poor matches among top 10 results in online A/B testing
- Median latency of 400ms and P95 latency of 600ms achieved with vLLM streaming mode
- System complexity and maintenance overhead reduced by over 75% compared to legacy NER stack

## Why This Works (Mechanism)

### Mechanism 1: Unified Multi-Task LLM with Homogeneous Batching
Consolidating multiple task-specific NER models into a single fine-tuned LLM reduces maintenance overhead while improving extraction accuracy, conditional on proper batching strategy and model size selection. Homogeneous batching provides more stable gradient signals per optimization step than heterogeneous batching, reducing interference between disparate task gradients during multi-task instruction tuning. The unified model shares representational capacity across tasks. If task objectives become highly conflicting (e.g., precision-critical tagging vs. recall-focused rewriting), homogeneous batching may insufficiently prevent interference, requiring architectural separation or task-specific adapters.

### Mechanism 2: Context-Aware Query Enrichment via Profile Integration
Jointly modeling user queries with profile attributes enables disambiguation of implicit and self-referential queries, conditional on the LLM's capacity to reason across modalities and profile data quality. The Query Rewriter component receives both query text and member profile as input, enabling the LLM to resolve underspecified queries (e.g., "jobs near me" → "jobs near Bay Area, CA") by retrieving contextual signals from profile fields like location, skills, and experience. If profile data is stale, sparse, or contradicts real-time behavioral signals, query enrichment may produce contextually inappropriate rewrites that reduce relevance.

### Mechanism 3: Streaming Inference with Incremental Parsing for Latency Reduction
vLLM serving with streaming mode and incremental JSON parsing enables sub-600ms P95 latency at scale, conditional on structured output formats and parallel downstream execution. Streaming mode allows the custom JSON parser to incrementally process output tokens, recognizing and dispatching tool calls as they complete rather than waiting for full generation. This enables parallel execution of downstream components (candidate selection, filtering) while generation continues. If tool calls have sequential dependencies or JSON schema validation fails on partial outputs, parallel execution benefits diminish; streaming overhead may increase latency.

## Foundational Learning

- **Concept: Named Entity Recognition (NER) vs. Generative Extraction**
  - Why needed here: The paper contrasts legacy NER approaches with LLM-based generative extraction. Understanding this distinction clarifies why NER struggles with evolving taxonomies and why generative approaches can handle unseen entity types.
  - Quick check question: Given a query "Senior ML engineer at Series B fintech," would an NER model extract "Series B" as a company stage entity if that label didn't exist in training data? How would a generative LLM approach differ?

- **Concept: Multi-Task Instruction Tuning and Catastrophic Forgetting**
  - Why needed here: The framework relies on fine-tuning a single model for multiple tasks (planning, tagging, rewriting). Understanding gradient interference and mitigation strategies (homogeneous batching, upsampling) is essential for reproducing results.
  - Quick check question: If you trained on tagging tasks for 1000 steps, then rewriting tasks for 1000 steps (sequential), what would likely happen to tagging performance? How does homogeneous batching during training differ?

- **Concept: Transformer Inference Optimization (KV Caching, Batching, Streaming)**
  - Why needed here: The 600ms P95 latency target at 20 QPS per GPU requires understanding vLLM's optimizations: PagedAttention for memory efficiency, continuous batching for throughput, and streaming for perceived latency.
  - Quick check question: Why does sharing long prompt prefixes (common system prompts across queries) particularly benefit vLLM's efficiency? What happens to memory if you process each request independently?

## Architecture Onboarding

- **Component map**: Query → Query Planner (classify intent) → [branch: Tagger for criteria search | Rewriter for self-reference] → Structured Output → Candidate Selection → Ranking
- **Critical path**: Query → Query Planner (classify intent) → [branch: Tagger for criteria search | Rewriter for self-reference] → Structured Output → Candidate Selection → Ranking. Latency budget: 400ms median, 600ms P95.
- **Design tradeoffs**:
  - Model size vs. precision: <2B models met latency but had hallucinations; >7B too slow. Chose 1.5B with SFT for structured output control.
  - SFT vs. DPO: DPO reduced hallucinations slightly but compromised format fidelity for JSON schema. SFT chosen for production reliability.
  - Streaming vs. batch completion: Streaming adds parsing complexity but enables parallel downstream execution, critical for latency targets.
- **Failure signatures**:
  - Hallucinated entities in tagging: Small pretrained LLMs hallucinate from few-shot examples. Fine-tuning mitigates but requires labeled data.
  - Incomplete JSON in streaming: If generation cuts off mid-JSON, incremental parser must handle gracefully. Implement partial validation.
  - Profile-query mismatch: If profile location is outdated or ambiguous, rewrites may produce irrelevant geographic constraints.
  - P95 latency spikes: Monitor batch queue depth; vLLM continuous batching can introduce tail latency under load spikes.
- **First 3 experiments**:
  1. Baseline comparison: Deploy unified LLM alongside legacy NER stack in shadow mode. Compare precision/recall on held-out evaluation set (3-5K samples per task) for location and company tagging. Target: match or exceed legacy precision (0.934/0.688) and recall (0.894/0.710).
  2. A/B test relevance metrics: Route 50% traffic to LLM framework. Measure NDCG change and "poor match" rate in top-10 results. Target: statistically significant improvement (paper reports +33% NDCG, -59% poor matches at p<0.05).
  3. Latency stress test: Load test at 2x expected QPS. Monitor P50/P95/P99 latency and GPU memory utilization with vLLM streaming. Target: P95 <600ms, identify throughput ceiling per A100 GPU (paper reports ~20 QPS).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can preference optimization algorithms be modified to reliably enforce strict structural constraints (e.g., valid JSON schemas) required for agentic tool calling?
- Basis in paper: Section 4.1 notes that while Direct Preference Optimization (DPO) reduced hallucinations, the authors rejected it because "DPO’s preference-based objective occasionally compromised format fidelity," forcing a return to Supervised Fine-Tuning (SFT).
- Why unresolved: There is currently a lack of training objectives that simultaneously optimize for semantic preference (handling ambiguity) and syntactic correctness (machine-readable output) without trade-offs.
- What evidence would resolve it: A modified DPO loss function or constrained decoding mechanism that achieves low hallucination rates while maintaining 100% schema compliance in tool-calling tasks.

### Open Question 2
- Question: What are the theoretical dynamics of gradient interference that cause homogeneous batching to outperform heterogeneous batching in multi-task instruction tuning?
- Basis in paper: Section 4.1 reports that homogeneous batching improved performance, but the explanation provided ("disjoint batches provide a more stable gradient signal") is explicitly labeled a hypothesis rather than a proven theory.
- Why unresolved: The paper demonstrates the efficacy of the approach empirically, but the underlying mathematical reasons why task-isolated batches stabilize optimization in LLMs remain unproven.
- What evidence would resolve it: A comparative analysis of gradient conflict metrics (e.g., cosine similarity of gradients) between homogeneous and heterogeneous batches during the training process.

### Open Question 3
- Question: How can facet suggestion systems be expanded to handle implicit user intent without increasing the risk of irrelevant recommendations?
- Basis in paper: Section 3.4 states that the system currently activates facet suggestions "only when an industry is explicitly mentioned," avoiding the complexity of inferring facets from ambiguous contexts.
- Why unresolved: Inferring relevant filters from implicit signals (e.g., suggesting "Remote" based on browsing history rather than query text) introduces a higher risk of "overly broad suggestions" that the current safeguards do not address.
- What evidence would resolve it: Successful online A/B testing results where an LLM-driven implicit suggestion module maintains or improves user engagement metrics (click-through rate) without increasing query abandonment rates.

### Open Question 4
- Question: Can the latency-fidelity trade-off be resolved to allow larger models (>7B parameters) to serve high-throughput query understanding tasks?
- Basis in paper: Section 4.1 explains that while larger models offered higher fidelity, they were "computationally impractical" compared to the 1.5B parameter model selected to meet strict 600ms latency constraints.
- Why unresolved: The paper relies on a small model to meet production serving requirements, leaving the potential accuracy gains of larger models inaccessible for real-time, user-facing search applications.
- What evidence would resolve it: Deployment of techniques like speculative decoding or advanced quantization that allow a 7B+ model to operate within the 400ms median latency benchmark reported in the paper.

## Limitations

- Online A/B testing metrics are reported only for a single time period without longer-term performance validation
- Offline evaluation relies on a proprietary evaluation set of only 2,000 samples without public benchmarks for comparison
- Streaming inference claims lack detailed GPU specifications, batch sizes, and memory utilization patterns

## Confidence

- **High Confidence**: The unified LLM framework reduces system complexity and maintenance overhead (>75% reduction). The latency targets (400ms median, 600ms P95) are achieved with vLLM streaming and incremental parsing.
- **Medium Confidence**: The multi-task instruction tuning with homogeneous batching improves extraction accuracy over legacy NER models for geographic location and company tagging. Online A/B testing results are statistically significant but lack long-term validation.
- **Low Confidence**: The generalizability of homogeneous batching benefits across diverse task combinations and the robustness of context-aware query enrichment when profile data quality degrades.

## Next Checks

1. **Extended A/B Testing Validation**: Deploy the LLM framework for minimum 8-week continuous A/B testing across multiple user cohorts and query distributions. Measure not only NDCG@10 and poor match rates but also conversion metrics, user engagement patterns, and performance stability across weekdays vs. weekends. Target: Confirm statistical significance persists across extended timeframes and diverse user segments.

2. **Profile Data Quality Impact Study**: Systematically evaluate query enrichment performance across varying profile data quality levels (complete vs. sparse, current vs. stale, consistent vs. contradictory). Measure the degradation in query rewrite relevance and tagging accuracy when profile attributes are missing or outdated. Target: Quantify the performance cliff and identify minimum viable profile data requirements for reliable enrichment.

3. **Streaming Inference Robustness Testing**: Conduct fault injection testing on the incremental JSON parser and parallel downstream execution pipeline. Simulate partial outputs, malformed tokens, and tool call failures during streaming generation. Measure downstream system behavior when processing incomplete structured outputs and identify failure modes in concurrent execution. Target: Establish error handling protocols and determine when to fall back to batch completion mode.