---
ver: rpa2
title: 'A Study of Large Language Models for Patient Information Extraction: Model
  Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning'
arxiv_id: '2509.04753'
source_url: https://arxiv.org/abs/2509.04753
tags:
- llms
- clinical
- fine-tuning
- extraction
- instruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates large language models (LLMs)
  for patient information extraction from clinical narratives, focusing on model architecture,
  fine-tuning strategies, and multi-task instruction tuning. The researchers benchmarked
  both encoder-based models (BERT, GatorTron) and decoder-based generative models
  (GatorTronGPT, Llama 3.1, GatorTronLlama) across five diverse clinical datasets
  for clinical concept and relation extraction tasks.
---

# A Study of Large Language Models for Patient Information Extraction: Model Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning

## Quick Facts
- arXiv ID: 2509.04753
- Source URL: https://arxiv.org/abs/2509.04753
- Reference count: 0
- Primary result: Generative LLMs with PEFT and multi-task instruction tuning achieved F1 scores of 0.8981 (CE) and 0.8978 (RE) while reducing computational costs by 6×

## Executive Summary
This study systematically evaluates large language models for patient information extraction from clinical narratives, comparing encoder-based models (BERT, GatorTron) against decoder-based generative models (GatorTronGPT, Llama 3.1, GatorTronLlama) across five clinical datasets. The research benchmarks traditional full-size fine-tuning against parameter-efficient fine-tuning (PEFT) strategies and examines multi-task instruction tuning for improved generalizability. Decoder-based LLMs with prompt-based PEFT achieved the best performance, with GatorTronLlama reaching F1 scores of 0.8981 for concept extraction and 0.8978 for relation extraction. Multi-task instruction tuning significantly improved zero-shot and few-shot learning, with models using only 20% of training data achieving performance comparable to full fine-tuning.

## Method Summary
The study evaluates five clinical datasets (2010 i2b2, 2018 n2c2, 2022 n2c2, RadGraph, UFHealth) for clinical concept extraction (CCE) and clinical relation extraction (CRE) using both encoder-based models (BERT, GatorTron) and decoder-based generative models (GatorTronGPT, Llama 3.1, GatorTronLlama). Models are fine-tuned using NVIDIA NeMo with LoRA-based PEFT (rank=256, dropout=0.2, lr=1e-4) and compared against full fine-tuning. Multi-task instruction tuning employs a leave-one-dataset-out strategy to test cross-domain generalization. Evaluation uses micro-averaged F1 with strict matching (exact entity boundaries and types).

## Key Results
- Decoder-based LLMs with prompt-based PEFT achieved F1 scores of 0.8981 (CE) and 0.8978 (RE), outperforming encoder-based models by 1.8-6.6% absolute F1
- Multi-task instruction tuning enabled models to achieve near-full performance with only 20% of training data, with F1 gaps less than 0.005
- PEFT with LoRA reduced computational costs by 6× while maintaining or improving performance compared to full fine-tuning
- Zero-shot F1 improved from 0.0155 to 0.3596 for CE (34.4% absolute improvement) with multi-task tuning on GatorTronLlama

## Why This Works (Mechanism)

### Mechanism 1
Parameter-efficient fine-tuning (PEFT) with LoRA achieves comparable or superior performance to full fine-tuning while reducing computational cost by 6×. LoRA injects small, trainable low-rank matrices into Transformer attention layers. During fine-tuning, only these matrices (representing <1% of total parameters) are updated, while the base model weights remain frozen. The low-rank decomposition approximates full weight updates at dramatically reduced dimensionality.

### Mechanism 2
Multi-task instruction tuning enables cross-domain generalization, allowing models to achieve near-full performance with only 20% of target-domain training data. By training on a mixture of datasets spanning different clinical domains (medications, radiology, social determinants), the model learns transferable extraction patterns. Instruction templates teach the model to map task descriptions to output schemas, creating a shared representation space across tasks.

### Mechanism 3
Decoder-based generative models with prompt-based extraction outperform encoder-based classification approaches, particularly for relation extraction tasks. Generative models reformulate extraction as text-to-text generation, producing structured outputs directly. This avoids the token-level classification constraints of encoder models, which cannot handle overlapping or discontinuous entities. The prompt format also provides explicit task context that guides reasoning.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**: The core fine-tuning technique enabling efficient adaptation of multi-billion parameter models. Understanding rank, adapter layers, and weight merging is essential for configuring PEFT pipelines.
  - Quick check: If LoRA rank=256 and you observe overfitting on a small dataset, should you increase or decrease the rank?

- **Prompt-based Machine Reading Comprehension (MRC)**: The paper reformulates extraction tasks as question-answering rather than classification. Understanding span prediction and question templates is essential for designing instruction formats.
  - Quick check: How would you convert a relation extraction task ("drug X causes Y") into an MRC-style prompt?

- **Zero-shot vs Few-shot Learning**: The multi-task instruction tuning evaluation hinges on these transfer learning paradigms. Distinguishing them is critical for interpreting the leave-one-dataset-out results.
  - Quick check: In the leave-one-dataset-out protocol, which setting tests true zero-shot capability, and which tests data efficiency?

## Architecture Onboarding

- **Component map**: Clinical Text Input → Instruction Template + Prompt Engineering → Base LLM (Frozen) → LoRA Adapter Layers (Trainable, ~0.5-0.8% params) → Text-to-Text Generation Output → Post-processing: Parse structured entities/relations from generated text

- **Critical path**: Select base model (Clinical domain-adapted > General domain), configure LoRA (rank=256, dropout=0.2, adapter on attention layers only), design instruction templates (3-5 variations per task), multi-task data mixing (combine datasets with task-specific instruction prefixes)

- **Design tradeoffs**:
  | Decision | Option A | Option B | Paper evidence |
  |----------|----------|----------|----------------|
  | Architecture | Encoder-only | Decoder-only | Decoder +1.8-6.6% F1 for RE |
  | Fine-tuning | Full | PEFT/LoRA | PEFT: 6× faster, comparable F1 |
  | Training strategy | Single-task | Multi-task | Multi-task: 20% data ≈ full performance |
  | Model size | Small (<1B) | Large (>5B) | Large models benefit more from PEFT |

- **Failure signatures**: Boundary errors on nested entities, duplicate span generation in decoder models, schema mismatch causing zero-shot transfer failures, prompt sensitivity with significant performance variance

- **First 3 experiments**: 1) Baseline PEFT setup: Fine-tune Llama 3.1-8B with LoRA (rank=256) on 2018 n2c2 for medication extraction, target F1 >0.96 within ±0.01. 2) Multi-task ablation: Train on 3 datasets, evaluate zero-shot on held-out 4th, compare with single-dataset fine-tuning to quantify instruction tuning benefit (~20-30% F1 improvement expected). 3) Data efficiency curve: Fine-tune with 5%, 10%, 20%, 50%, 100% of training data, verify 20% threshold where performance plateaus near full-dataset levels.

## Open Questions the Paper Calls Out

1. How do advanced prompt engineering techniques (e.g., chain-of-thought, self-consistency prompting) compare to the enumeration-based prompt selection used in this study for clinical information extraction? The study relied on manual enumeration to select prompts, which may not have identified optimal prompt formulations.

2. How do recently proposed PEFT algorithms beyond LoRA (e.g., Prefix Tuning, AdaLoRA, DoRA) perform for clinical information extraction compared to the LoRA-based approach evaluated here? Only LoRA was tested as the PEFT method.

3. Can multi-task instruction tuning generalize effectively to entirely new annotation schemas and clinical domains not represented in the training mixture? The study demonstrates transfer across four datasets with overlapping task definitions, but real-world deployment may require adaptation to completely novel extraction schemas.

## Limitations

- Prompt engineering constraints: Instruction prompt design was "limited by enumeration capacity," suggesting potential performance ceilings and unexplored optimization opportunities.
- Dataset heterogeneity and evaluation gaps: Limited external validation across diverse clinical domains and annotation schemas; three datasets from the same n2c2 series with similar schemas.
- Hardware and cost asymmetry: Computational efficiency claims rely on comparing different model architectures rather than identical models under different fine-tuning regimes.

## Confidence

**High confidence**: Decoder-based LLMs with PEFT outperform encoder-based approaches for clinical extraction; multi-task instruction tuning enables 20% data efficiency with minimal performance loss; LoRA-based PEFT reduces computational costs while maintaining or improving F1 scores.

**Medium confidence**: Zero-shot and few-shot learning benefits generalize across all five datasets; prompt-based extraction eliminates boundary errors for overlapping/discontinuous entities; clinical domain adaptation provides consistent advantages.

**Low confidence**: Performance thresholds for industrial deployment; scalability of instruction tuning to 10+ clinical extraction tasks; long-term stability of LoRA adapters after model updates.

## Next Checks

1. **Prompt ablation study**: Systematically vary instruction templates across 3-5 formats per task to quantify prompt sensitivity. Compare performance variance against reported "enumeration capacity" limitations.

2. **Cross-schema transfer evaluation**: Design an experiment using datasets with incompatible annotation schemas. Test whether multi-task instruction tuning provides any benefit when schemas fundamentally mismatch.

3. **Cost-normalized comparison**: Replicate the PEFT vs full fine-tuning comparison using identical model architectures and hyperparameter sweeps. Measure F1 per GPU-hour to validate the claimed 6× efficiency improvement under fair comparison conditions.