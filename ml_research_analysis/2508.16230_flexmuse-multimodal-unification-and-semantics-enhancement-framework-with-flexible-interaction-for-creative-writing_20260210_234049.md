---
ver: rpa2
title: 'FlexMUSE: Multimodal Unification and Semantics Enhancement Framework with
  Flexible interaction for Creative Writing'
arxiv_id: '2508.16230'
source_url: https://arxiv.org/abs/2508.16230
tags:
- semantic
- arxiv
- flexmuse
- https
- mmcw
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FlexMUSE is a framework for multimodal creative writing that aims
  to generate illustrated articles from flexible inputs like topic or image. The core
  method idea is to enable optional visual inputs via text-to-image module, use a
  modality semantic alignment gate to filter redundant textual information based on
  semantic similarity, apply cross-modality fusion to capture correlations and enhance
  features, and implement a modality semantic creative direct preference optimization
  to improve creativity.
---

# FlexMUSE: Multimodal Unification and Semantics Enhancement Framework with Flexible interaction for Creative Writing

## Quick Facts
- arXiv ID: 2508.16230
- Source URL: https://arxiv.org/abs/2508.16230
- Reference count: 40
- FlexMUSE outperforms baselines in consistency, creativity, and coherence on ArtMUSE dataset, with significant improvements in BertScore, creativity, and incoherence metrics.

## Executive Summary
FlexMUSE is a framework for multimodal creative writing that generates illustrated articles from flexible inputs like topic or image. The core innovation is a probabilistic redundancy filtering gate (msaGate) that masks textual features when text-image similarity is high, combined with cross-modality fusion via attention and a novel mscDPO training strategy that uses semantically related rejected samples from the same reference document. Experiments on the ArtMUSE dataset show FlexMUSE outperforms baselines in consistency, creativity, and coherence, with significant improvements in BertScore, creativity, and incoherence metrics.

## Method Summary
FlexMUSE enables optional visual inputs via a text-to-image module, uses CLIP to extract text and image embeddings, and applies a modality semantic alignment gate to filter redundant textual information based on semantic similarity. Cross-modality fusion captures correlations via attention and enhances features, while modality semantic creative direct preference optimization improves creativity by constructing semantically related but diverse rejected samples from within the same reference document. The framework is trained on the ArtMUSE dataset using supervised fine-tuning followed by mscDPO, achieving state-of-the-art results in multimodal creative writing.

## Key Results
- FlexMUSE outperforms baselines (mm-cot, mPLUG-Owl, LaDic, DOC) in Style Consistency, Context Consistency, Creativity, Richness, and Coherence on ArtMUSE dataset
- Significant improvements in BertScore (+4.5%) and creativity metrics compared to SFT-only baseline
- Achieves highest scores in reference-free and reference-aware evaluations for multimodal creative writing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The msaGate mechanism reduces cross-modal semantic inconsistency by probabilistically filtering redundant textual information when text-image similarity is high.
- Mechanism: For each input pair (T, V), sample a threshold u ~ U(0,1). Compute cosine similarity σ between CLIP embeddings Z^T and Z^V. When σ ≥ u, mask the textual feature (null vector); otherwise, pass it through (all-ones vector). Higher similarity triggers more aggressive masking, encouraging the model to rely on visual anchors rather than redundant text.
- Core assumption: When modalities share high semantic overlap, the textual input contains redundant information that increases entropy without adding novel signal for creative generation.
- Evidence anchors:
  - [abstract] "use a modality semantic alignment gate to filter redundant textual information based on semantic similarity"
  - [section 3.2] "msaGate blocks the redundant information to downstream modules by masking textual inputs based on the semantic similarity between modalities"
  - [corpus] Weak direct corpus evidence for this specific gating pattern; related work (FSRF, MoLAN) addresses missing/noisy modalities but not probabilistic redundancy filtering via cosine similarity.
- Break condition: If text and images are semantically unrelated (low σ), msaGate becomes ineffective; masking occurs rarely, and redundant text may still propagate. Also, if the sampling distribution is poorly calibrated, the probabilistic threshold may mask informative text unpredictably.

### Mechanism 2
- Claim: Cross-modality fusion captures text-image correlations via attention and amplifies modality-specific semantics while preserving shared information through adaptive weighting.
- Mechanism: Compute visual query Q = W^Q ⊙ Z^V and textual key-value pairs from gated text. Aggregate via attention: Z^A = softmax(QK^T/√d_K)V. Then compute adaptive correlation λ = sigmoid(W^λ · (Z^A)^T · Z^V). Final fused representation: Z^F = (1-λ) ⊙ Z^A + λ ⊙ Z^V.
- Core assumption: Attention between visual queries and textual key-values surfaces cross-modal correlations, and a learned λ can balance aggregated features against raw visual features to preserve modality-specific signals.
- Evidence anchors:
  - [abstract] "apply cross-modality fusion to capture correlations and enhance features"
  - [section 3.3] "capture the correlation between vision and text by attention mechanism, then leverage this correlation to conduct semantic augmentation"
  - [corpus] Sync-TVA uses graph-attention for cross-modal fusion in emotion recognition; Structurs Meet Semantics uses graph contrastive learning for multimodal fusion—conceptually aligned but different architectures.
- Break condition: If the attention mechanism fails to find meaningful correlations (e.g., due to misaligned embeddings or poorly initialized weights), Z^A becomes uninformative, and λ cannot recover useful signal. Also, if visual and textual features are orthogonal, fusion may introduce noise.

### Mechanism 3
- Claim: mscDPO enhances creative generation by constructing semantically related but diverse rejected samples from within the same reference document, providing fine-grained negative supervision.
- Mechanism: Split each reference into chunks where each image V corresponds to a text segment. For each generated result, the chosen answer is the matching reference chunk; rejected samples are all other chunks from the same reference. This extends DPO by providing multiple semantically proximate negatives rather than random or unrelated rejections.
- Core assumption: Within-document chunks share topic coherence but differ in creative expression; using them as rejected samples teaches the model to prefer appropriate semantic alignment while encouraging creative variation.
- Evidence anchors:
  - [abstract] "implement a modality semantic creative direct preference optimization to improve creativity"
  - [section 3.4] "mscDPO adjusts the choosing strategy in conventional DPO... the remaining chunks in the same references are all set as rejected samples, which are naturally diversified but related in semantic"
  - [corpus] No direct corpus evidence for this specific DPO extension; standard DPO and RLHF are well-established, but semantic chunking for creative MMCW appears novel.
- Break condition: If reference chunks are too similar (low semantic diversity), rejected samples provide weak contrastive signal. Conversely, if chunks are too dissimilar (high semantic gap), the model may learn to reject anything not matching the chosen chunk, reducing creative flexibility.

## Foundational Learning

- Concept: **CLIP-style multimodal embeddings**
  - Why needed here: FlexMUSE uses CLIP to extract Z^T and Z^V for both msaGate similarity computation and cross-modality fusion. Understanding contrastive vision-language pretraining is essential to debug alignment failures.
  - Quick check question: Given a CLIP text embedding and image embedding, what range of cosine similarity indicates semantic alignment vs. misalignment?

- Concept: **Direct Preference Optimization (DPO)**
  - Why needed here: mscDPO extends standard DPO. You must understand how DPO replaces reward modeling with direct policy optimization using chosen/rejected pairs before modifying the sampling strategy.
  - Quick check question: In standard DPO, what is the role of the reference model, and how does the β hyperparameter control divergence from it?

- Concept: **Attention-based feature fusion**
  - Why needed here: The cross-modality fusion module uses multi-head attention patterns (query-key-value) to aggregate visual-textual features. Debugging attention weights is critical for diagnosing fusion failures.
  - Quick check question: If attention weights are uniformly distributed across all positions, what does this indicate about the learned relationship between Q and K?

## Architecture Onboarding

- Component map:
  Input layer (Text, optional Image) -> CLIP Encoder (produces Z^T, Z^V) -> msaGate (masks Z^T) -> Cross-modality Fusion (Attention + λ-weighted) -> Z^F -> T5 Decoder (SFT then mscDPO) -> Output (illustrated article)

- Critical path:
  1. Verify CLIP embeddings load correctly and cosine similarities fall in expected range [-1, 1].
  2. Confirm msaGate masking behavior by logging σ values and mask decisions per sample.
  3. Inspect attention weights in cross-modality fusion—check for collapse to uniform or single-position focus.
  4. During mscDPO, validate that chosen/rejected pairs are correctly constructed from semantic chunks.

- Design tradeoffs:
  - T2I module enables flexible input but adds inference latency and potential visual semantic drift.
  - Probabilistic msaGate introduces randomness that aids generalization but reduces reproducibility.
  - mscDPO provides creative supervision but requires high-quality chunked references; noisy chunking degrades performance.

- Failure signatures:
  - Low BertScore with high ROUGE: Model generates generic text; check if msaGate is over-masking (high σ values always triggering null masks).
  - High incoherence scores: Cross-modality fusion may be introducing conflicting signals—inspect λ distribution.
  - mscDPO shows no improvement over SFT: Verify rejected samples are semantically distinct; if all chunks are near-identical, contrastive signal is weak.

- First 3 experiments:
  1. **Ablate msaGate**: Set u=1 (always pass text) vs. u=0 (always mask) vs. standard probabilistic. Measure BertScore, Creativity, Coherence on ArtMUSE validation set.
  2. **Probe cross-modality attention**: Visualize attention heatmaps for Q (visual) × K (textual). Confirm attention focuses on semantically relevant tokens; if uniform, investigate embedding initialization or scaling.
  3. **Validate mscDPO chunking**: Manually inspect 20 reference documents to verify chunks are topically coherent but creatively diverse. Compare mscDPO vs. standard DPO on held-out test set using LLM-based creativity metrics.

## Open Questions the Paper Calls Out
None

## Limitations
- Framework relies heavily on the availability and quality of the ArtMUSE dataset, which is not publicly accessible and contains only ~3,000 manually calibrated samples.
- Probabilistic masking mechanism in msaGate introduces stochasticity that may reduce reproducibility and could lead to over-masking if semantic similarity scores are miscalibrated.
- mscDPO extension lacks direct corpus evidence for its specific chunking strategy, and its effectiveness depends critically on the semantic diversity of reference chunks.

## Confidence
- High confidence in overall architectural design and alignment with established multimodal learning principles
- Medium confidence in specific implementations of msaGate and mscDPO due to novel probabilistic and chunking strategies
- Low confidence in reproducibility without access to ArtMUSE dataset and exact hyperparameter configurations

## Next Checks
1. **Ablate msaGate**: Compare model performance with u=1 (always pass text), u=0 (always mask), and standard probabilistic masking to determine optimal redundancy filtering.
2. **Probe cross-modality attention**: Visualize attention heatmaps for Q (visual) × K (textual) to ensure attention focuses on semantically relevant tokens rather than collapsing to uniform distributions.
3. **Validate mscDPO chunking**: Manually inspect 20 reference documents to verify chunks are topically coherent but creatively diverse, then compare mscDPO vs. standard DPO on held-out test set using LLM-based creativity metrics.