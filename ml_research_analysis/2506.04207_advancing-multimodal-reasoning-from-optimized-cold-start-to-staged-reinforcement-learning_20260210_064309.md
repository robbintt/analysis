---
ver: rpa2
title: 'Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement
  Learning'
arxiv_id: '2506.04207'
source_url: https://arxiv.org/abs/2506.04207
tags:
- reasoning
- multimodal
- arxiv
- zhang
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of developing sophisticated
  multimodal reasoning capabilities in Multimodal Large Language Models (MLLMs). The
  core method introduces a three-stage training curriculum: (1) a text-centric cold-start
  phase using high-difficulty textual data to establish foundational reasoning, (2)
  a multimodal reinforcement learning (RL) stage enhanced by a novel Prioritized Advantage
  Distillation (PAD) algorithm to mitigate gradient stagnation, and (3) a text-only
  RL refinement phase to consolidate reasoning and linguistic fluency.'
---

# Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.04207
- Source URL: https://arxiv.org/abs/2506.04207
- Authors: Shuang Chen; Yue Guo; Zhaochen Su; Yafu Li; Yulun Wu; Jiacheng Chen; Jiayu Chen; Weijie Wang; Xiaoye Qu; Yu Cheng
- Reference count: 40
- One-line primary result: Three-stage curriculum (text cold start → multimodal RL → text-only RL) achieves SOTA among open-source 7B MLLMs, scoring 53.1% average across multimodal and textual reasoning benchmarks.

## Executive Summary
This paper addresses the challenge of developing sophisticated multimodal reasoning capabilities in Multimodal Large Language Models (MLLMs). The authors introduce a three-stage training curriculum that begins with high-difficulty text-only reasoning data, progresses through multimodal reinforcement learning enhanced by a novel Prioritized Advantage Distillation (PAD) algorithm, and concludes with text-only RL refinement. This structured approach, culminating in the ReVisual-R1 model, achieves state-of-the-art performance among open-source 7B MLLMs on challenging reasoning benchmarks.

## Method Summary
The method employs a three-stage training curriculum: (1) text-centric cold start using 283K high-difficulty textual reasoning samples to establish foundational reasoning patterns, (2) multimodal reinforcement learning using GRPO with Prioritized Advantage Distillation (PAD) to mitigate gradient stagnation from sparse rewards, and (3) text-only RL refinement to consolidate reasoning and linguistic fluency. The model is built on Qwen2.5-VL-7B-Instruct and trained using 8× A100-80G GPUs. Key innovations include the PAD algorithm for filtering near-zero advantage samples and the strategic ordering of training stages to maximize performance across both visual and textual reasoning tasks.

## Key Results
- ReVisual-R1 achieves 53.1% average across challenging multimodal and textual reasoning benchmarks, outperforming existing open-source 7B MLLMs.
- Text-only cold start improves performance by +14.3% average compared to multimodal cold start (+7.2% improvement for multimodal cold start).
- PAD algorithm improves multimodal RL performance from 45.1% to 47.7% average, with faster convergence shown in Figure 5.
- Optimal stage ordering (CS → MRL → TRL) achieves 49.6% average vs 45.5% for reverse ordering and 47.6% for mixed-RL.

## Why This Works (Mechanism)

### Mechanism 1: Text-Centric Cold Start Establishes Foundational Reasoning
Training MLLMs on high-difficulty text-only reasoning data before multimodal RL produces stronger multimodal reasoning than starting with multimodal data. Text-only datasets contain longer reasoning chains and higher difficulty, enabling the model to develop extended Chain-of-Thought and self-reflective reasoning patterns that transfer to multimodal tasks.

### Mechanism 2: Prioritized Advantage Distillation (PAD) Mitigates Gradient Stagnation
Standard GRPO in multimodal RL with sparse binary rewards produces near-zero advantages when groups have uniform rewards, halting learning. PAD filters samples whose absolute advantage falls below threshold T_low, then re-samples from remaining "effective set" using temperature-controlled softmax prioritization over absolute advantages, enriching batches with informative gradients.

### Mechanism 3: Staged Multimodal-Then-Text RL Ordering
Performing multimodal RL first to ground visual perception, followed by text-only RL to refine linguistic fluency and abstract reasoning, outperforms simultaneous mixed-modality RL or reverse ordering. MRL establishes cross-modal grounding while TRL restores and sharpens linguistic expression degraded by intensive MRL, consolidating both capabilities synergistically.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: The paper's RL stages use GRPO as the base algorithm; understanding advantage estimation via group normalization is prerequisite.
  - Quick check question: Can you explain why GRPO normalizes advantages within groups rather than globally?

- Concept: **Sparse Reward Learning**
  - Why needed here: The paper uses binary rule-based rewards (correct/incorrect), and PAD specifically addresses gradient stagnation from sparse signals.
  - Quick check question: What happens to policy gradients when all samples in a group receive the same reward?

- Concept: **Transfer Learning / Cold Start Initialization**
  - Why needed here: The first stage is explicitly a cold start; understanding how pretrained weights transfer across modalities is essential.
  - Quick check question: Why might text-only pretraining benefit multimodal reasoning despite no visual exposure?

## Architecture Onboarding

- Component map: Qwen2.5-VL-7B-Instruct (vision encoder + LLM backbone) -> Cold Start SFT (283K text-only samples) -> Multimodal RL (GRPO + PAD, 26K multimodal samples) -> Text-only RL (frozen vision tower, GRPO + PAD, 30K text samples)
- Critical path: Cold Start → MRL with PAD → TRL. Skipping or reordering stages degrades performance (Table 3).
- Design tradeoffs: Omitting KL constraint in MRL encourages exploration but risks instability; adding small KL in TRL stabilizes refinement. Higher T_low in PAD filters more aggressively but may lose diversity. Freezing vision tower in TRL preserves grounding but limits visual adaptation.
- Failure signatures: If reward curves plateau early with near-zero variance, suspect gradient stagnation → check PAD thresholds. If multimodal benchmarks regress after TRL, TRL phase may be too long → reduce text RL steps. If cold start shows no improvement, verify data difficulty (pass rate should be <80%).
- First 3 experiments: 1) Replicate cold start ablation: Compare text-only vs multimodal cold start on 40K samples each; expect text-only to win by ~7% average margin. 2) PAD ablation: Train MRL with GRPO baseline vs PAD; expect +2-3% avg improvement with PAD. 3) Stage ordering ablation: Compare MRL→TRL vs TRL→MRL vs mixed-RL; expect MRL→TRL to achieve highest avg (~49.6%).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What theoretical mechanisms explain why text-centric cold-start initialization enhances multimodal reasoning capabilities, and can this be formalized mathematically?
- Basis in paper: [explicit] Section 6.3 states: "we currently lack a rigorous theoretical foundation. Specifically, a deeper theoretical account is needed to explain why these text-centric optimization strategies, such as the cold start, effectively enhance model reasoning capabilities."
- Why unresolved: The paper provides empirical evidence (Figure 3) but no formal explanation for the transfer from textual to multimodal reasoning domains.
- What evidence would resolve it: A theoretical framework (e.g., relating to representation alignment, transfer learning theory) validated through controlled experiments varying linguistic complexity and measuring cross-modal activation patterns.

### Open Question 2
- Question: Does the ReVisual-R1 training curriculum generalize to Mixture-of-Experts (MoE) architectures and model scales beyond 7B parameters?
- Basis in paper: [explicit] Section 6.3: "we have not yet validated these advantages across larger-scale architectures, such as MoE models, or generalized them to other foundational model families and larger size regimes."
- Why unresolved: Experiments were limited to 3B and 7B dense transformer architectures based on Qwen2.5-VL.
- What evidence would resolve it: Evaluation of ReVisual-R1's three-stage curriculum on MoE models (e.g., DeepSeek-V3) and larger dense models (e.g., 32B, 70B) across the same benchmark suite.

### Open Question 3
- Question: What is the optimal ratio and quality balance between multimodal and textual data during the cold-start and RL phases for different skill clusters (STEM reasoning, perceptual grounding, general domains)?
- Basis in paper: [explicit] Section 6.3: "We will extensively explore the impact of the ratio and quality of multimodal versus textual data utilized during both the cold start and RL phases. The ultimate goal... is to determine an optimal curriculum recipe that jointly maximizes performance across diverse skill clusters."
- Why unresolved: Current work uses fixed ratios (283K text for cold-start, 26K multimodal + 30K text for RL) without systematic variation analysis.
- What evidence would resolve it: Ablation studies varying data ratios systematically across phases and measuring performance on skill-specific benchmarks to identify optimal trade-offs.

### Open Question 4
- Question: How does the PAD mechanism's effectiveness vary with different reward sparsity levels and advantage distributions beyond binary rule-based rewards?
- Basis in paper: [inferred] PAD was designed for sparse binary rewards (r ∈ {0,1}) common in multimodal settings. The paper does not test whether gradient stagnation and PAD's benefits generalize to denser reward signals or continuous reward functions.
- Why unresolved: The GRPO formulation and PAD design assume group-based advantage estimation with potentially sparse rewards; applicability to other reward paradigms remains unexplored.
- What evidence would resolve it: Comparative experiments applying PAD to tasks with dense/continuous rewards (e.g., preference-based RL, process reward models) measuring training stability and final performance.

## Limitations

- **PAD hyper-parameters**: Critical thresholds T_low/T_high and sub-sampling ratio ρ for Prioritized Advantage Distillation are not fully specified in the paper. The claim that PAD resolves gradient stagnation depends on these settings being tuned correctly for the specific reward sparsity and dataset distribution.
- **Cold-start dataset representativeness**: The paper claims text-only cold start works better than multimodal, but the specific datasets and filtering criteria for GRAMMAR are only partially described. It's unclear if this generalizes to other domains or if the improvement is specific to the selected math and logic corpora.
- **Generalizability beyond math/logic**: All reported results are on reasoning benchmarks; performance on real-world multimodal tasks (e.g., document Q&A, image understanding) is not evaluated, limiting claims about broad applicability.

## Confidence

- **High confidence**: The staged training approach (cold start → MRL → TRL) is well-supported by ablation studies showing clear performance gains from the ordering. The mechanism of text-first cold start establishing reasoning patterns is plausible given the longer, more complex chains in text-only datasets.
- **Medium confidence**: The PAD algorithm's effectiveness is demonstrated empirically but relies on sensitive hyper-parameters not fully disclosed. The core idea of filtering near-zero advantage samples is sound, but the exact improvement magnitude may vary with dataset and reward structure.
- **Low confidence**: Claims about PAD being the sole or primary solution to gradient stagnation are not independently verified. Alternative approaches (e.g., reward shaping, curriculum learning) might achieve similar results. The superiority of text cold start over multimodal is shown but the sample sizes and exact methodology are unclear.

## Next Checks

1. **PAD ablation with ablation**: Run MRL with and without PAD on a held-out multimodal reasoning dataset (e.g., MathVision), logging advantage distributions to confirm gradient stagnation is mitigated by PAD.

2. **Cold-start generalization**: Replace GRAMMAR text data with a different reasoning corpus (e.g., GSM8K or PACT) and retrain cold start; check if multimodal reasoning still improves.

3. **Stage reordering stress test**: Train models with all 6 possible stage orderings (including simultaneous mixed-RL) on the same hardware budget; compare not just average scores but also convergence curves and stability.