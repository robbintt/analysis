---
ver: rpa2
title: 'Revisiting LRP: Positional Attribution as the Missing Ingredient for Transformer
  Explainability'
arxiv_id: '2506.02138'
source_url: https://arxiv.org/abs/2506.02138
tags:
- positional
- relevance
- rules
- attention
- relevancy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses a critical limitation in Transformer explainability:
  existing Layer-wise Relevance Propagation (LRP) methods fail to attribute relevance
  to positional encodings, violating conservation properties and missing important
  positional information. The authors propose Positional-Aware LRP (PA-LRP), which
  reformulates the input space as position-token pairs and introduces specialized
  LRP rules for propagating relevance through various positional encoding methods
  (Rotary, Learnable, and Absolute PE).'
---

# Revisiting LRP: Positional Attribution as the Missing Ingredient for Transformer Explainability

## Quick Facts
- **arXiv ID**: 2506.02138
- **Source URL**: https://arxiv.org/abs/2506.02138
- **Reference count**: 40
- **Primary result**: PA-LRP improves Transformer explainability by attributing relevance to positional encodings, outperforming state-of-the-art methods across NLP and vision tasks.

## Executive Summary
This paper addresses a critical limitation in Transformer explainability: existing Layer-wise Relevance Propagation (LRP) methods fail to attribute relevance to positional encodings, violating conservation properties and missing important positional information. The authors propose Positional-Aware LRP (PA-LRP), which reformulates the input space as position-token pairs and introduces specialized LRP rules for propagating relevance through various positional encoding methods (Rotary, Learnable, and Absolute PE). Their method significantly outperforms state-of-the-art baselines across both NLP and vision tasks. In NLP, PA-LRP improves AU-MSE scores by up to 51% on Tiny-LLaMA and shows 7% improvement in zero-shot LLaMA-3 evaluations. For vision, it achieves 3.97 average points improvement in negative perturbation tests on DeiT models and improves segmentation metrics by up to 2.07 points. The method demonstrates that positional relevance captures unique spatial and structural relationships complementary to semantic relevance, producing more faithful and comprehensive explanations.

## Method Summary
PA-LRP reformulates the Transformer input space from token embeddings E to position-token pairs (E, P), explicitly including positional encodings as relevance sinks. The method implements specialized LRP rules: standard epsilon-LRP for input-level PE (addition), matrix multiplication rules for attention-level PE (RoPE), and positive-only aggregation across layers. This approach ensures conservation of relevance by preventing the "leak" that occurs when positional encodings are treated as constants rather than variables during backpropagation.

## Key Results
- PA-LRP improves AU-MSE scores by up to 51% on Tiny-LLaMA compared to state-of-the-art baselines
- Achieves 7% improvement in zero-shot LLaMA-3 evaluations on NLP tasks
- Demonstrates 3.97 average points improvement in negative perturbation tests on DeiT vision models
- Improves segmentation metrics by up to 2.07 points compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1: Conservation Restoration via Input Space Reformulation
Standard Transformer LRP violates relevance conservation because it treats positional encodings as constants rather than relevance sinks. PA-LRP reformulates the input space from token embeddings E to position-token pairs (E, P), explicitly propagating relevance to positional embeddings P and sealing the conservation property "leak."

### Mechanism 2: Attribution of Structural Biases (RoPE/ALiBi)
For attention-level positional encodings like RoPE, PA-LRP applies LRP matrix multiplication rules to rotation matrices, splitting relevance equally between the rotated Query/Key and the rotation matrix itself. This allows relevance to flow into the position component of the input space.

### Mechanism 3: Multi-Sink Positive Aggregation
PA-LRP aggregates relevance by summing only positive contributions from both semantic and positional sinks, preventing cancellation of positional signals by negative noise in deeper layers. This ensures structural signals captured in early layers aren't obscured by negative relevance scores later.

## Foundational Learning

- **Concept: Layer-wise Relevance Propagation (LRP) Conservation**
  - **Why needed here:** The paper's central thesis is that ignoring Positional Encodings breaks the "conservation property" (Input Relevance = Output Relevance). Understanding this is required to grasp why the "violation" is a problem.
  - **Quick check question:** If a model outputs a relevance score of 1.0, but the sum of input attributions is 0.8, which property is violated?

- **Concept: Positional Encoding Mechanisms (RoPE vs. Learnable)**
  - **Why needed here:** The method implements different rules for Input-Level (Addition) vs. Attention-Level (Rotation) PE. You must distinguish adding a vector (Learnable) from rotating a vector (RoPE) to implement the correct rule.
  - **Quick check question:** Does RoPE add a bias vector to the input, or does it multiply the query/key by a rotation matrix?

- **Concept: Perturbation-based Faithfulness Metrics**
  - **Why needed here:** The paper validates its claims using "Positive" and "Negative" perturbation tests (AUAC, AU-MSE). Understanding these metrics is necessary to interpret the results tables (e.g., why a "lower" AU-MSE is better).
  - **Quick check question:** In a "positive perturbation" test, do you remove the most relevant tokens first or the least relevant?

## Architecture Onboarding

- **Component map:** Input Space (E, P) -> Forward Pass (Standard Transformer with rotation matrix logging) -> Backward Pass (Specialized LRP rules) -> Aggregator (Positive-only summation)

- **Critical path:** The exact implementation of Eq. 10 (RoPE propagation). The paper claims simply treating the rotation as a standard linear layer is insufficient; the relevance must be explicitly split between the content (Q/K) and the position (R).

- **Design tradeoffs:** Faithfulness vs. Complexity - introducing per-layer sinks increases memory overhead but captures structural features. PE-Only vs. Composite - PE-Only maps are less fragmented but may miss semantic nuance; the composite method balances both.

- **Failure signatures:** Zero Relevance for Position (if rotation matrices are treated as constants), Conservation Violation (if total relevance diverges from output), Noisy Heatmaps (if negative values aren't properly filtered).

- **First 3 experiments:**
  1. Conservation Check (Sanity): Run PA-LRP on DeiT and plot positional-to-total relevance ratio, confirming it's non-zero (~10-20%).
  2. Ablation on Tiny-LLaMA: Reproduce Table 1, comparing AttnLRP vs. Ours on AU-MSE metric for generation task (~50% improvement).
  3. Visual Structural Check: Run PA-LRP on DeiT snake image, comparing PE Only vs. Ours heatmaps to confirm PE Only highlights whole elongated structure.

## Open Questions the Paper Calls Out

### Open Question 1
Can systematically redesigning existing non-positional LRP rules, rather than merely adding PE-aware rules to current frameworks, further improve attribution faithfulness? The current work focuses on "patching" the missing positional component rather than re-deriving relevance propagation rules for other components.

### Open Question 2
Can specific concepts, such as objects in context-dependent environments, be formally characterized as being primarily attributed by positional relevance rather than semantic relevance? The paper suggests objects in specific contexts (e.g., boats on water) might display this pattern.

### Open Question 3
Does the competitive performance of the "PE Only" ablation imply that standard semantic attributions in Transformers are currently overestimating distinct semantic content that is actually positionally encoded? The "PE Only" success raises questions about whether semantic embeddings rely heavily on positional heuristics.

## Limitations
- Computational overhead increases with per-layer positional sinks, potentially limiting scalability to larger models
- Evaluation focuses on relatively small-scale models (Tiny-LLaMA, DeiT variants) rather than production-scale systems
- Conservation property violation demonstration relies heavily on synthetic and narrow task evaluations

## Confidence
**High Confidence**: Conservation property violation is real and measurable; positional attribution mechanism works for Learnable PE and Sinusoidal PE.
**Medium Confidence**: RoPE attribution rules produce meaningful positional explanations, though equal splitting assumption may not generalize to all attention patterns.
**Low Confidence**: Assertions that PA-LRP explanations are more "faithful" than baselines require further validation beyond perturbation metrics.

## Next Checks
1. **Conservation Validation**: Run PA-LRP on LLaMA-2-70B and measure positional-to-total relevance ratio across all layers, verifying violation scales with model size.
2. **Cross-Architecture Generalizability**: Test PA-LRP on GPT-3-style absolute positional embeddings with learned position embeddings to validate method flexibility.
3. **Downstream Task Integration**: Evaluate whether PA-LRP explanations improve human-in-the-loop debugging tasks for identifying positional bias errors in real-world applications.