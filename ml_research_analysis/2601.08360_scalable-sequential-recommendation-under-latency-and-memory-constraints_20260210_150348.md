---
ver: rpa2
title: Scalable Sequential Recommendation under Latency and Memory Constraints
arxiv_id: '2601.08360'
source_url: https://arxiv.org/abs/2601.08360
tags:
- state
- item
- sequential
- holographic
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HoloMambaRec combines holographic reduced representations with
  a selective state space encoder to enable scalable, metadata-aware sequential recommendation
  under strict memory and latency constraints. Item and attribute embeddings are bound
  via circular convolution, preserving dimensionality while encoding structured relationships,
  and processed using a shallow (2-3 layer) selective state space backbone inspired
  by Mamba-style models.
---

# Scalable Sequential Recommendation under Latency and Memory Constraints

## Quick Facts
- arXiv ID: 2601.08360
- Source URL: https://arxiv.org/abs/2601.08360
- Reference count: 25
- Primary result: HoloMambaRec outperforms SASRec and achieves state-of-the-art ranking on MovieLens-1M under strict 10-epoch memory/latency constraints

## Executive Summary
HoloMambaRec addresses sequential recommendation under severe memory and latency constraints by combining holographic reduced representations (HRR) with a selective state space encoder. The model uses FFT-based circular convolution to bind item and attribute embeddings into constant-dimensional vectors, enabling structured metadata encoding without expanding model size. A shallow 2-3 layer selective state space backbone processes these bound representations, offering linear-time complexity versus quadratic attention. Experiments on Amazon Beauty and MovieLens-1M show consistent improvements over SASRec, with particular state-of-the-art performance on MovieLens-1M, while substantially reducing memory requirements compared to attention-based methods.

## Method Summary
The method processes user interaction sequences by first embedding items and attributes (synthetic or single-valued), then binding them via holographic circular convolution with a learnable scalar α. This produces constant-dimensional bound vectors that preserve semantic relationships while avoiding embedding dimension blow-up. These bound vectors feed into a selective state space encoder inspired by Mamba-style models, with 2 layers for Amazon Beauty and 3 for MovieLens-1M, using pre-LayerNorm, residual connections, and masked cross-entropy loss. The model supports forward-compatible temporal bundling and compression, though initial compression attempts led to accuracy collapse. Evaluation uses HR@10 and NDCG@10 with last-item holdout under a fixed 10-epoch training budget.

## Key Results
- Consistently outperforms SASRec on both Amazon Beauty and MovieLens-1M benchmarks
- Achieves state-of-the-art ranking performance on MovieLens-1M with limited memory budget
- Substantial memory savings versus attention-based methods due to shallow selective SSM architecture
- Single-attribute binding shows limited benefit; multi-attribute extension identified as important future direction

## Why This Works (Mechanism)
HoloMambaRec exploits holographic reduced representations to encode structured item-attribute relationships in constant space via circular convolution, avoiding the quadratic complexity of attention mechanisms. The selective state space encoder processes these representations with linear-time complexity, maintaining performance while drastically reducing memory footprint. This combination enables scalable sequential recommendation where traditional attention-based methods fail due to resource constraints.

## Foundational Learning
- Holographic reduced representations: Why needed - enables constant-dimensional encoding of structured relationships; Quick check - verify FFT-based circular convolution preserves dimensionality while binding embeddings
- Selective state space models: Why needed - provides linear-time sequential processing alternative to attention; Quick check - confirm diagonal A matrix and input-conditioned Δ/B/C parameters
- Masked cross-entropy training: Why needed - handles variable-length sequences with padding; Quick check - ensure padding indices are properly masked in loss computation

## Architecture Onboarding
**Component Map**: Item/Attribute Embeddings → Holographic Binding (FFT Conv) → LayerNorm → Selective SSM Encoder → Classification Head
**Critical Path**: Input sequence → Embedding lookup → Circular convolution binding → SSM layer processing → Item logit projection
**Design Tradeoffs**: Constant-dimensional HRR binding trades off exact attribute reconstruction for memory efficiency; shallow SSM layers trade off representational capacity for speed
**Failure Signatures**: HR@10/NDCG@10 collapse to ~0.01 when compression/bundling enabled; no benefit from binding when attributes are synthetic/single-valued
**First Experiments**: 1) Train with `use_compression=False` only to establish baseline performance; 2) Ablate binding scalar α initialization values; 3) Profile per-layer memory usage versus linear projection baseline

## Open Questions the Paper Calls Out
- Can temporal bundling achieve memory/latency reductions without accuracy collapse? (Initial bundling attempts caused HR@10 to drop near zero despite training loss decrease)
- Does holographic binding provide measurable benefits with multiple rich attributes versus single-attribute/item-only baselines? (Single-attribute ablation showed limited benefit)
- Can selective state space models reliably recover individual item-attribute pairs from superimposed holographic representations? (Unbinding reliability remains uncharacterized)
- How does performance scale to production-level item catalogs and longer interaction histories? (Current evaluation limited to small benchmarks with L=50 truncation)

## Limitations
- Initialization strategy for selective state space parameters and binding scalar α is unspecified, potentially impacting convergence and performance
- Memory comparison with SASRec based on reported values rather than direct profiling; per-layer memory usage characterization incomplete
- Compression/bundling functionality leads to accuracy collapse in initial experiments, requiring architectural safeguards
- Only single coarse attribute per item considered; multi-attribute generalization untested

## Confidence
- High: Core architecture (embedding binding + selective SSM encoder) is implementable and trainable under stated constraints
- Medium: Consistent SASRec outperformance and MovieLens-1M state-of-the-art achievement given single-attribute limitation
- Medium: Substantial memory savings versus attention-based methods pending direct profiling validation

## Next Checks
1. **Initialization Ablation**: Train with multiple initialization schemes for A, Δ, B, C, D, and α; report performance variance and convergence curves
2. **Memory Profiling**: Instrument model to log per-layer memory usage during training; compare selective SSM memory to linear projection baseline
3. **Compression Robustness Test**: Systematically enable bundling (k=1→2→3), monitor metrics per epoch, implement gradient masking to prevent collapse