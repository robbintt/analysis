---
ver: rpa2
title: 'FreEformer: Frequency Enhanced Transformer for Multivariate Time Series Forecasting'
arxiv_id: '2501.13989'
source_url: https://arxiv.org/abs/2501.13989
tags:
- uni00000013
- attention
- matrix
- rank
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FreEformer introduces a frequency-enhanced Transformer for multivariate
  time series forecasting, leveraging frequency spectra to capture cross-variate dependencies.
  It addresses the low-rank attention issue in vanilla Transformers by adding a learnable
  matrix and applying row-wise L1 normalization, improving representation diversity
  and gradient flow.
---

# FreEformer: Frequency Enhanced Transformer for Multivariate Time Series Forecasting

## Quick Facts
- arXiv ID: 2501.13989
- Source URL: https://arxiv.org/abs/2501.13989
- Reference count: 40
- **Primary result**: Outperforms state-of-the-art multivariate forecasting models on 18 benchmarks using frequency-enhanced attention and L1 normalization.

## Executive Summary
FreEformer introduces a novel Transformer architecture that leverages frequency spectra to capture cross-variate dependencies in multivariate time series forecasting. The key innovation is an enhanced attention mechanism that addresses the low-rank issue in vanilla Transformers by incorporating a learnable matrix and applying row-wise L1 normalization instead of Softmax. This approach improves representation diversity and gradient flow while preserving the global frequency perspective. Evaluated across 18 real-world datasets spanning electricity, traffic, and finance domains, FreEformer consistently outperforms existing state-of-the-art methods including both Transformer-based and frequency-domain forecasters.

## Method Summary
The model processes multivariate time series by first applying reversible instance normalization and expanding input dimensions via learnable vectors. It then transforms the temporal dimension using Discrete Fourier Transform (DFT), retaining the first ⌈(T+1)/2⌉ frequency coefficients. The real and imaginary parts are processed independently through an enhanced Transformer architecture featuring an attention mechanism that adds a learnable matrix B and uses row-wise L1 normalization instead of Softmax. After processing, the model applies inverse DFT, adds residual connections, denormalizes, and projects to the output space. Training employs Adam optimizer with weighted L1 loss that emphasizes recent predictions.

## Key Results
- Consistently outperforms state-of-the-art models including iTransformer and DLinear across 18 real-world benchmarks
- Enhanced attention mechanism improves performance when applied to existing Transformer-based forecasters
- Achieves superior results across multiple domains: electricity consumption, traffic flow, and financial markets
- The frequency-enhanced approach provides better cross-variate dependency capture compared to purely temporal methods

## Why This Works (Mechanism)
The core mechanism addresses a fundamental limitation of vanilla Transformers in frequency domain applications. Standard attention mechanisms suffer from low-rank issues when processing frequency spectra due to the inherent sparsity of frequency domain representations. FreEformer's enhanced attention adds a learnable matrix B that provides additional degrees of freedom for the model to capture complex relationships, while the row-wise L1 normalization preserves gradient flow and prevents the collapse of attention weights that can occur with Softmax in sparse distributions. By processing real and imaginary parts independently and then combining them, the model maintains the global frequency perspective crucial for capturing long-range dependencies across multiple variates.

## Foundational Learning
- **Discrete Fourier Transform (DFT)**: Converts temporal signals to frequency domain by decomposing into sinusoidal components. Why needed: Enables frequency-based processing of time series to capture global patterns. Quick check: Verify DFT(x) correctly transforms simple periodic signals.
- **Row-wise L1 Normalization**: Normalizes attention weights across rows instead of applying Softmax. Why needed: Prevents gradient vanishing in sparse frequency distributions while maintaining diversity. Quick check: Ensure attention weights sum to 1 across each row.
- **Reversible Instance Normalization (RevIN)**: Normalizes inputs reversibly for stable training. Why needed: Stabilizes training across diverse datasets with different scales. Quick check: Confirm denormalization recovers original scale.
- **Learnable Attention Matrix B**: Adds adaptive parameters to attention computation. Why needed: Provides additional capacity to capture complex cross-variate relationships. Quick check: Monitor B's learning trajectory during training.
- **Frequency-Real/Imag Separation**: Processes real and imaginary components independently. Why needed: Preserves phase information crucial for time series reconstruction. Quick check: Validate IDFT(DFT(x)) ≈ x for unit test signals.

## Architecture Onboarding

**Component Map**: Input -> RevIN -> DFT -> Enhanced Transformer (Real/Imag) -> IDFT -> Residual -> Denorm -> Output

**Critical Path**: The enhanced attention mechanism is the critical innovation. The attention computation follows: Attention = Norm(Softmax(QK^T/√D) + Softplus(B))V, where B is a learnable matrix and Norm is row-wise L1 normalization.

**Design Tradeoffs**: The use of L1 normalization instead of Softmax trades the probabilistic interpretation of attention weights for improved gradient flow and representation diversity in sparse frequency distributions. This choice is specifically motivated by the challenges of frequency domain processing rather than general sequence modeling.

**Failure Signatures**: Gradient instability is the primary failure mode, particularly if the learnable matrix B is initialized poorly or if the L1 normalization is implemented incorrectly. The model may also fail if the DFT/IDFT pipeline is misaligned, leading to reconstruction errors.

**First Experiments**:
1. Implement the Enhanced Attention module with B initialized to zeros and verify it reduces to vanilla attention initially.
2. Create a unit test verifying that IDFT(DFT(x)) ≈ x for simple periodic signals before integrating into the full model.
3. Train on a small dataset (e.g., subset of ECL) with varying numbers of layers and attention heads to understand hyperparameter sensitivity.

## Open Questions the Paper Calls Out
- **Adaptive Basis Functions**: The paper suggests exploring learnable basis functions beyond fixed DFT to potentially capture domain-specific frequency characteristics more effectively. Evidence needed: Experiments comparing fixed DFT with learnable orthogonal dictionary learning layers.
- **Frequency-Domain Prediction Heads**: The current architecture uses temporal prediction heads that outperform frequency-domain heads, but the paper notes this leaves unresolved the challenge of directly forecasting complex-valued frequency spectra. Evidence needed: Modified prediction architectures capable of generating future frequency spectra with lower error than temporal-head baselines.
- **Enhanced Attention Generalization**: While the mechanism solves low-rank attention for frequency data, it's unclear if it provides advantages in dense time-domain data where low-rank attention is also a concern. Evidence needed: Application of Enhanced Attention to state-of-the-art time-domain Transformers and evaluation on standard long-sequence benchmarks.

## Limitations
- The paper does not specify critical architectural hyperparameters including the number of layers and attention heads used in reported results
- Initialization strategy for the learnable attention matrix B is not provided, which is crucial for training stability
- Exact dataset splits, history lengths, and prediction horizons are not specified for the 18 benchmark datasets
- The weighted L1 loss formulation for multivariate outputs is not fully detailed

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Core frequency-enhanced attention concept | High |
| Experimental results across 18 benchmarks | Medium |
| Implementation details for Enhanced Attention | Low |
| Data preprocessing and training specifics | Low |

## Next Checks
1. **Gradient Stability Test**: Implement Enhanced Attention with B initialized to zeros, monitor gradient norms for first 100 steps to ensure no explosion/vanishing.
2. **Frequency Domain Verification**: Create unit test applying DFT → FreEformer block → IDFT and verify output ≈ input for simple periodic signals.
3. **Hyperparameter Sensitivity Analysis**: Reproduce model with different layer counts and attention heads, compare validation performance to understand architectural sensitivity.