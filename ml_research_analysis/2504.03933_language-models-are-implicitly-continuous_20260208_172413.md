---
ver: rpa2
title: Language Models Are Implicitly Continuous
arxiv_id: '2504.03933'
source_url: https://arxiv.org/abs/2504.03933
tags:
- other
- tokens
- probability
- factor
- interpolation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that transformer-based language models learn a
  continuous representation of language, even when trained on discrete tokens. By
  generalizing the causal attention mechanism to accept continuous inputs (Continuous
  Causal Transformer, CCT), the authors demonstrate that duration of tokens and interpolation
  between embeddings have meaningful effects on model outputs.
---

# Language Models Are Implicitly Continuous

## Quick Facts
- **arXiv ID:** 2504.03933
- **Source URL:** https://arxiv.org/abs/2504.03933
- **Reference count:** 40
- **Primary result:** Transformer-based LLMs learn continuous representations of language even when trained on discrete tokens, enabling meaningful manipulation of token duration and embedding interpolation.

## Executive Summary
This paper demonstrates that transformer-based language models implicitly learn continuous representations of language, even when trained on discrete tokens. By introducing the Continuous Causal Transformer (CCT) architecture that generalizes causal attention to continuous inputs, the authors show that varying token duration and interpolating between embeddings produces coherent, semantically meaningful changes in model outputs. Experiments across multiple LLMs (Llama3, Gemma, Mistral) reveal that reducing token duration causes continuous shifts in predicted counts and answers, while linear interpolations between embeddings yield intermediate semantic properties. These findings challenge the assumption that LLM language understanding mirrors human intuition and have implications for both linguistic theory and engineering applications in model training and inference.

## Method Summary
The paper modifies the standard transformer's causal attention mechanism to accept continuous inputs by replacing discrete sums with integrals over time. The CCT architecture scales attention weights by token duration and accepts arbitrary embeddings rather than just vocabulary indices. The implementation involves modifying HuggingFace models to support floating-point positional indices and custom attention coefficients that weight tokens by their duration. The approach is tested through duration-based word counting tasks and semantic interpolation experiments, where token embeddings are linearly blended and their effects on model outputs are measured.

## Key Results
- Varying token duration (0.1 to 1.0) causes continuous, predictable changes in word count predictions across multiple LLM architectures
- Linear interpolation between embeddings produces coherent intermediate semantics (e.g., interpolating "apples" and "bananas" yields predictions for "green")
- The model shows translation invariance but not scale invariance in positional encoding, with shifting outputs remaining stable while scaling causes fluctuations
- These effects are observed across Llama3, Gemma, and Mistral models without requiring retraining

## Why This Works (Mechanism)

### Mechanism 1: Duration-Weighted Attention
If token duration is varied, the model scales its semantic contribution proportionally rather than treating it as a discrete binary presence. The CCT generalizes the discrete attention sum to an integral, where token influence is multiplied by its duration $d_k$. Reducing duration lowers the token's influence on output probabilities. Evidence: Shrinking "apple" duration causes predicted counts to drop continuously in Section 4.1.

### Mechanism 2: Semantic Linear Interpolation
Linearly interpolated token embeddings are processed as coherent semantic blends rather than noise. The learned embedding manifold is smooth and convex with respect to semantic attributes, so intermediate vectors map to valid output regions. Evidence: Model predicts "green" for [apple $\leftrightarrow$ banana] interpolation in Section 4.3.

### Mechanism 3: Shifting Invariance vs. Scale Dependence
The model is translation invariant (shifting time) but not scale invariant (stretching time). Positional encodings preserve relative positions under translation, but scaling changes information density per unit time. Evidence: Appendix D.3 shows shifting outputs remain stable while scaling outputs fluctuate.

## Foundational Learning

- **Riemann Sum vs. Integral Approximation**: Needed to understand transforming discrete Transformer sums into CCT integrals via time discretization. Quick check: How does changing rectangle width ($\Delta x$) affect total area under a curve?

- **Embedding Space Geometry**: Needed to grasp how linear interpolation in vector space maps to semantic transitions. Quick check: Does averaging two word vectors usually correspond to a word "between" them semantically?

- **Causal Masking in Attention**: Needed to understand modifying causal masks for continuous time without future information leakage. Quick check: Why are upper triangle values typically set to $-\infty$ in causal attention matrices?

## Architecture Onboarding

- **Component map**: Input embeddings $x(t)$ -> CCT Block (modified attention) -> Positional Encoding (floating-point compatible) -> Output logits
- **Critical path**: Modifying HuggingFace attention mask implementation to accept floating-point duration coefficients (Eq. 22/23 in Appendix B.1)
- **Design tradeoffs**: Precision vs computation (high-precision durations require custom kernels) and semantic drift (interpolated embeddings may drift into out-of-distribution areas)
- **Failure signatures**: Breakdown of linearity for specific domains like math, and positional extrapolation degradation for extreme duration scaling
- **First 3 experiments**: 1) Duration Counting: Vary "apple" duration and verify count predictions shift from 4 toward 1-2. 2) Color Interpolation: Interpolate "Blue" and "Yellow" embeddings and check for "Green" predictions. 3) Translation Test: Shift positional indices by +0.5 and confirm output probability stability.

## Open Questions the Paper Calls Out

### Open Question 1
How does CCT behave with continuous input functions other than stepwise constants, such as piece-wise polynomials? The paper theoretically defines continuous generalization but restricts experimental validation to stepwise constants, noting other forms remain untested.

### Open Question 2
How can output tokens be disentangled and reassigned to original sentences when training with superposed tokens? While superposition could improve training efficiency, the mechanism to map composite outputs back to distinct training targets remains undefined.

### Open Question 3
Does implicit time continuity emerge as a consequence of smooth function approximation or specific architectural details? The paper demonstrates continuity empirically but offers only hypotheses about underlying causes, leaving specific drivers unidentified.

### Open Question 4
To what extent does interpolation between overlapping concepts hinder concept erasure techniques? Current methods assume distinct representation regions, but continuous interpolations may cause unintended side effects on neighboring concepts.

## Limitations
- The CCT implementation requires precise floating-point positional encoding handling that may not be compatible with all pretrained models
- The distinction between multiplicative and additive attention coefficient implementations could affect results across different architectures
- Continuity claims may not hold uniformly across all domains, particularly numerical reasoning where discontinuity exists
- Broader claims about fundamental language understanding remain philosophical rather than definitively proven

## Confidence
- **High Confidence**: Duration-weighted attention producing continuous word count changes is well-supported by empirical evidence across multiple models
- **Medium Confidence**: Translation invariance vs scale dependence claims require careful interpretation and depend on specific architectural details
- **Low Confidence**: Claims about fundamental language understanding through continuous representations are more philosophical than technical

## Next Checks
1. **Numerical Domain Validation**: Test continuity specifically on arithmetic tasks and number embeddings to verify continuous behavior for numerical reasoning
2. **Cross-Architecture Generalizability**: Implement CCT modifications for models with different positional encoding schemes to test architecture dependence
3. **Extreme Duration Scaling**: Systematically test duration factors well beyond [0.1, 1.0] to establish practical limits of continuous representation hypothesis