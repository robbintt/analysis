---
ver: rpa2
title: 'MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative
  Correction'
arxiv_id: '2509.23368'
source_url: https://arxiv.org/abs/2509.23368
tags:
- reasoning
- medical
- test
- student
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedCritical addresses the challenge of enhancing medical reasoning
  in small language models (SLMs), which often underperform compared to large language
  models (LLMs) in complex medical tasks like diagnosis and treatment planning. The
  proposed two-stage framework first uses supervised fine-tuning with long-chain thought
  templates extracted from a teacher LLM to guide the SLM in generating more sophisticated
  reasoning.
---

# MedCritical: Enhancing Medical Reasoning in Small Language Models via Self-Collaborative Correction

## Quick Facts
- **arXiv ID**: 2509.23368
- **Source URL**: https://arxiv.org/abs/2509.23368
- **Reference count**: 21
- **Primary result**: Achieved 70.62% accuracy on CMExam benchmark, outperforming Taiyi and Huatuo-o1-7B by 3.04% and 10.12% respectively

## Executive Summary
MedCritical addresses the challenge of enhancing medical reasoning in small language models (SLMs), which often underperform compared to large language models (LLMs) in complex medical tasks like diagnosis and treatment planning. The proposed two-stage framework first uses supervised fine-tuning with long-chain thought templates extracted from a teacher LLM to guide the SLM in generating more sophisticated reasoning. In the second stage, a model self-collaborative direct preference optimization (DPO) mechanism enables the SLM to learn from its own error-driven insights through self-iteration and adversarial correction. This approach achieves comparable results to traditional knowledge distillation methods at a lower cost.

## Method Summary
MedCritical employs a two-stage framework to enhance medical reasoning in small language models. The first stage involves supervised fine-tuning using long-chain thought templates extracted from a teacher LLM, which provides structured reasoning guidance. The second stage implements a self-collaborative direct preference optimization (DPO) mechanism that enables the SLM to learn from its own error-driven insights through self-iteration and adversarial correction. This approach combines knowledge distillation with self-improvement, allowing the SLM to develop more sophisticated medical reasoning capabilities while maintaining computational efficiency compared to traditional methods.

## Key Results
- Achieved 70.62% accuracy on CMExam benchmark, establishing new state-of-the-art performance among 7B-class models
- Outperformed Taiyi and Huatuo-o1-7B by 3.04% and 10.12% respectively
- Demonstrated comparable results to traditional knowledge distillation methods at lower computational cost

## Why This Works (Mechanism)
MedCritical leverages the complementary strengths of teacher-guided learning and self-improvement. The first stage provides structured reasoning templates from a teacher LLM, establishing a foundation for sophisticated medical reasoning. The second stage's self-collaborative DPO mechanism allows the SLM to identify and correct its own reasoning errors through iterative refinement, creating a feedback loop that progressively improves reasoning quality. This combination addresses the inherent limitations of small models in handling complex medical reasoning tasks by providing both external guidance and internal self-correction capabilities.

## Foundational Learning
- **Direct Preference Optimization (DPO)**: A training method that optimizes models based on preference data between different responses; needed to enable the SLM to learn from its own reasoning errors
- **Knowledge Distillation**: The process of transferring knowledge from a larger teacher model to a smaller student model; required to provide sophisticated reasoning templates
- **Adversarial Correction**: A technique where models identify and correct their own errors through competitive processes; essential for the self-improvement component
- **Long-chain Thought Templates**: Structured reasoning patterns that guide complex problem-solving; necessary to provide explicit reasoning frameworks for medical tasks
- **Supervised Fine-tuning**: Training on labeled data with specific supervision; required to establish initial reasoning capabilities before self-improvement
- **Self-iteration**: The process of repeatedly applying a model to its own outputs for refinement; critical for progressive improvement

## Architecture Onboarding

**Component Map**: Teacher LLM -> Template Extraction -> SLM Supervised Fine-tuning -> Self-collaborative DPO -> Enhanced SLM

**Critical Path**: The supervised fine-tuning stage must successfully transfer reasoning templates from the teacher LLM before the self-collaborative DPO stage can effectively refine the SLM's capabilities. The quality of template extraction directly impacts the downstream performance of both stages.

**Design Tradeoffs**: The framework balances computational efficiency against reasoning quality by using a two-stage approach rather than continuous fine-tuning. While this reduces training costs compared to traditional knowledge distillation, it requires careful coordination between stages to ensure effective knowledge transfer and self-improvement.

**Failure Signatures**: 
- Poor template extraction from teacher LLM leads to inadequate reasoning guidance in stage one
- Insufficient diversity in self-iteration data causes the DPO stage to converge to suboptimal reasoning patterns
- Mismatched scales between teacher and student models result in knowledge transfer failures
- Overfitting during supervised fine-tuning prevents effective self-improvement in stage two

**First Experiments**:
1. Validate template extraction quality by comparing reasoning complexity between teacher-generated and student-generated outputs
2. Test the impact of self-iteration count on reasoning quality to identify optimal training cycles
3. Compare performance across different medical reasoning task types to assess domain generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope remains narrow, focusing primarily on a single medical benchmark (CMExam)
- Reliance on teacher LLM for template generation raises scalability and bias concerns
- Limited analysis of how adversarial correction dynamics affect reasoning quality over multiple iterations
- Cost-effectiveness claims lack detailed comparative metrics against traditional knowledge distillation

## Confidence
- **Technical Implementation**: High
- **Benchmark Performance Claims**: Medium
- **Generalizability Claims**: Medium
- **Long-term Effectiveness**: Medium
- **Cost-effectiveness Analysis**: Medium

## Next Checks
1. Evaluate MedCritical on multiple diverse medical reasoning benchmarks (e.g., MedQA, PubMedQA) to assess generalizability beyond CMExam
2. Conduct ablation studies comparing the two-stage framework against single-stage fine-tuning to isolate the contribution of self-collaborative correction
3. Analyze the evolution of reasoning quality across self-iteration cycles to identify potential degradation or convergence patterns in the adversarial correction process