---
ver: rpa2
title: 'ToxicTone: A Mandarin Audio Dataset Annotated for Toxicity and Toxic Utterance
  Tonality'
arxiv_id: '2505.15773'
source_url: https://arxiv.org/abs/2505.15773
tags:
- speech
- toxicity
- toxic
- dataset
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ToxicTone, the largest publicly available
  dataset for Mandarin Chinese toxic speech detection. The dataset includes detailed
  annotations for both the form and source of toxicity, such as profanity, bullying,
  anger, and sarcasm, capturing prosodic and emotional cues in spoken language.
---

# ToxicTone: A Mandarin Audio Dataset Annotated for Toxicity and Toxic Utterance Tonality

## Quick Facts
- **arXiv ID**: 2505.15773
- **Source URL**: https://arxiv.org/abs/2505.15773
- **Reference count**: 0
- **Primary result**: Multimodal detection framework achieves 64.16% F1, 77.90% accuracy on Mandarin toxic speech

## Executive Summary
ToxicTone introduces the largest publicly available dataset for Mandarin Chinese toxic speech detection, featuring detailed annotations for both the form and source of toxicity including profanity, bullying, anger, and sarcasm. The dataset captures prosodic and emotional cues in spoken language through 52,062 audio clips annotated by native Mandarin speakers. The authors develop a multimodal detection framework integrating acoustic, linguistic, and emotional features using state-of-the-art speech and emotion encoders, demonstrating that their approach significantly outperforms text-only models in identifying toxic expressions.

## Method Summary
The authors construct ToxicTone by collecting 52,062 Mandarin audio clips (2-10 seconds each) and annotating them for binary toxicity (toxic vs non-toxic) along with multi-label source classification (Specific Words, Angry/Violent, Dismissive/Impatient, Sarcastic/Satirical, Threatening). They develop a multimodal detection framework using three parallel encoders: XLS-R 1B for acoustic features, SONAR text encoder on ASR transcripts for linguistic features, and Emotion2Vec+ Large for emotional features. These embeddings are concatenated and fed into a three-layer linear classifier with binary cross-entropy loss, using one-vs-all classification for the multi-label source detection task.

## Key Results
- Multimodal approach (X + ST + E) achieves 64.16% F1 and 77.90% accuracy on toxicity detection
- Emotion features boost recall from 57.98% to 70.09% in ST + E configuration
- Ensemble configuration excels at detecting Angry/Violent and Sarcastic/Satirical toxicity with highest F1 scores
- Text-only models underperform compared to multimodal approaches, validating the importance of acoustic and emotional cues

## Why This Works (Mechanism)

### Mechanism 1
Fusing acoustic, linguistic, and emotional embeddings improves toxicity detection over single-modality approaches. Each encoder captures complementary toxicity signals—XLS-R encodes prosody and acoustic patterns; SONAR text encoder captures semantic meaning from ASR transcripts; Emotion2Vec+ encodes emotional states that may indicate toxic intent. Concatenating these representations allows the downstream classifier to learn cross-modal patterns.

### Mechanism 2
Emotional tone encoders enable detection of implicit toxicity that lacks explicit toxic words. Emotion2Vec+ provides emotional embeddings that capture states like anger, sarcasm, or dismissiveness. When combined with semantic encoders, the model can identify utterances where words appear neutral but delivery signals hostile intent.

### Mechanism 3
Dual-axis annotation (form + source) enables finer-grained classification of toxic expressions. Separating what toxicity manifests as from where it originates allows models to learn distinct patterns for each source category, particularly useful for implicit toxicity detection.

## Foundational Learning

- **Self-supervised speech representations (XLS-R, SONAR)**: These encoders provide the acoustic and linguistic embeddings that form the input to toxicity classifiers. Understanding their pre-training objectives helps interpret what features they capture.
  - Quick check: Can you explain why XLS-R uses layerwise weighted sums rather than just the final layer?

- **Multi-label vs hierarchical classification**: Toxicity can have multiple simultaneous forms and sources; the paper uses one-vs-all classifiers per source category rather than a single multi-class head.
  - Quick check: How does the one-vs-all approach handle an utterance that is both sarcastic and threatening?

- **Ensemble feature concatenation**: The best-performing model concatenates three encoder outputs before classification. Understanding concatenation vs late-fusion is critical for architectural decisions.
  - Quick check: What are the tradeoffs between early fusion (concatenation) and late fusion (ensembling predictions)?

## Architecture Onboarding

- **Component map**: Audio → Speaker diarization → ASR (K2D) → text-based toxicity pre-filter (threshold 0.75) → human annotation → three encoder branches (XLS-R 1B, SONAR speech/text, Emotion2Vec+) → feature concatenation → three-layer linear classifier → toxicity/source predictions
- **Critical path**: Audio → three encoder branches → concatenation → classifier → toxicity/source predictions. The text branch requires ASR preprocessing; emotion and acoustic branches operate on raw audio.
- **Design tradeoffs**: Pre-filtering at 0.75 toxicity score reduces annotation burden but may exclude edge cases; threshold choice affects dataset composition. Concatenation fusion increases dimensionality and computational cost but outperforms single encoders. One-vs-all classification handles multi-label scenarios but trains independent classifiers without modeling label correlations.
- **Failure signatures**: Low recall on implicit toxicity may indicate emotion encoder underweighting; poor threat detection due to class imbalance (only ~560 samples); ASR errors propagating to text encoder degrade SONAR text branch quality.
- **First 3 experiments**: 1) Replicate single-encoder baselines (E, X, SA, ST) on ToxicTone test set to verify reported F1 scores before ensemble experiments. 2) Ablate each encoder from the X + ST + E ensemble to quantify marginal contribution of each modality. 3) Test on domain-shifted audio (e.g., gaming vs podcast content) to assess whether category-specific toxicity patterns generalize.

## Open Questions the Paper Calls Out

### Open Question 1
To what extent does the text-based pre-filtering threshold exclude toxic speech that relies solely on prosodic cues? The dataset composition is biased towards textually explicit toxicity, making it difficult to assess how well models detect toxicity that is purely prosodic. An analysis of false negatives from the text-filtering step would resolve this.

### Open Question 2
How do individual annotator backgrounds and cultural contexts influence the perception of "borderline" toxicity in Mandarin? The paper aggregates annotations, potentially obscuring subjective disagreements on implicit forms like sarcasm or dismissiveness. A detailed analysis of the released annotator-level labels would help understand this variation.

### Open Question 3
Can the proposed multimodal framework generalize to low-resource Chinese dialects without extensive re-annotation? The current dataset is restricted to Mandarin, and it is unclear if the acoustic features learned for Mandarin toxicity transfer to dialects with different prosodic patterns. Zero-shot or few-shot evaluation on dialectal audio data would address this question.

## Limitations
- The optimal fusion strategy remains unclear - the paper demonstrates concatenation works better than individual encoders but doesn't explore alternative fusion methods like attention mechanisms or late fusion ensembles.
- Source classification performance is not thoroughly analyzed across all categories, with "Threatening" achieving low F1 despite high accuracy, suggesting the one-vs-all approach may struggle with minority classes and class imbalance.
- The threshold of 0.75 for ASR-based pre-filtering likely introduces selection bias, systematically excluding potentially toxic utterances that rely solely on prosodic cues.

## Confidence
- **High confidence**: The multimodal fusion approach (X + ST + E) demonstrably outperforms text-only baselines, with clear statistical improvements in F1 score (64.16% vs lower single-modality scores).
- **Medium confidence**: Emotional cues significantly improve recall for implicit toxicity detection, though absolute performance (62.47% F1) remains moderate.
- **Low confidence**: Dual-axis annotation (form + source) provides clear advantages over single-axis approaches, as the paper doesn't include comparative experiments against single-axis datasets.

## Next Checks
1. Conduct ablation studies on the threshold parameter for ASR pre-filtering (e.g., test thresholds of 0.6, 0.75, 0.9) to quantify how dataset composition affects downstream model performance and identify potential bias introduction.
2. Evaluate model performance on cross-domain Mandarin audio (e.g., social media clips vs. podcast content) to assess whether toxicity patterns generalize beyond the dataset's specific domain distribution.
3. Implement alternative fusion strategies (attention-based fusion, late fusion ensemble voting) and compare against the concatenation baseline to determine if the current approach is truly optimal or simply the first viable solution.