---
ver: rpa2
title: Impact of Pretraining Word Co-occurrence on Compositional Generalization in
  Multimodal Models
arxiv_id: '2507.08000'
source_url: https://arxiv.org/abs/2507.08000
tags:
- clip
- concept
- accuracy
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how word co-occurrence statistics in pretraining
  data affect compositional generalization in multimodal models like CLIP and LMMs.
  The authors use pointwise mutual information (PMI) to measure co-occurrence of concept
  pairs in captions, disentangling pair frequencies from single-word frequencies.
---

# Impact of Pretraining Word Co-occurrence on Compositional Generalization in Multimodal Models

## Quick Facts
- **arXiv ID**: 2507.08000
- **Source URL**: https://arxiv.org/abs/2507.08000
- **Reference count**: 40
- **Primary result**: CLIP zero-shot accuracy correlates highly with PMI in LAION-400M (r=0.97), showing a 14% accuracy gap between top and bottom 5% PMI values.

## Executive Summary
This paper investigates how word co-occurrence statistics in pretraining data affect compositional generalization in multimodal models like CLIP and LMMs. The authors use pointwise mutual information (PMI) to measure co-occurrence of concept pairs in captions, disentangling pair frequencies from single-word frequencies. They find a strong correlation between PMI and model accuracy: CLIP zero-shot classification accuracy correlates highly with PMI in LAION-400M (r=0.97), showing a 14% accuracy gap between top and bottom 5% PMI values. This correlation extends to LMMs built on CLIP, with r=0.70 for TextVQA and r=0.62 for VQAv2. The findings suggest that model performance depends heavily on co-occurrence statistics rather than understanding of individual concepts, highlighting the need for new algorithms to improve compositional generalization without requiring combinatorially large training datasets.

## Method Summary
The authors extract concepts from LAION-400M captions, compute PMI for concept pairs with Laplace smoothing, and evaluate CLIP zero-shot accuracy on synthetic GenPairs images and edited ImageNet-Paste images. They then fine-tune LLaVA-1.5 with LAION-trained CLIP backbones and evaluate VQA accuracy on TextVQA and VQAv2, computing PMI from lemmatized question+answer concepts. The correlation between PMI and accuracy is measured using Pearson's r, with accuracy gaps calculated between top/bottom 5% PMI bins.

## Key Results
- CLIP zero-shot accuracy correlates highly with PMI in LAION-400M (r=0.97), showing a 14% accuracy gap between top and bottom 5% PMI values.
- Correlation extends to LMMs built on CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2).
- Fine-tuning CLIP with image-editing augmentation removes correlation on edited natural images but fails to transfer to synthetic GenPairs dataset.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP zero-shot accuracy correlates with Pointwise Mutual Information (PMI) of concept pairs in pretraining data.
- Mechanism: PMI normalizes joint probability by independent probabilities, disentangling co-occurrence from single-word frequency. CLIP's contrastive training aligns image-text pairs, creating sensitivity to co-occurrence statistics captured in the embedding space.
- Core assumption: Word co-occurrence in captions is a valid proxy for visual concept co-occurrence.
- Evidence anchors:
  - [abstract] "CLIP zero-shot classification accuracy correlates highly with PMI in LAION-400M (r=0.97), showing a 14% accuracy gap."
  - [section 4] "We observe an r = 0.97 correlation and an accuracy gap of 14% between images in the top and bottom 5% of PMI values."
  - [corpus] "Pretraining Frequency Predicts Compositional Generalization of CLIP on Real-World Tasks" supports frequency-driven compositional failures.

### Mechanism 2
- Claim: Low PMI concept pairs degrade CLIP accuracy even when individual concepts are common.
- Mechanism: CLIP embeddings bind concepts associatively; uncommon pairings are under-represented, reducing alignment scores during zero-shot inference.
- Core assumption: Accuracy on synthetic images generalizes to natural images.
- Evidence anchors:
  - [section 4] "14% accuracy gap between images in the bottom vs. top 5% of concept pair PMI."
  - [section 5] "Editing natural images by pasting a concept with low PMI relative to the target class can significantly degrade performance (r=0.75, 10% gap)."
  - [corpus] "CLIP Behaves like a Bag-of-Words Model Cross-modally but not Uni-modally" suggests CLIP struggles with compositional binding.

### Mechanism 3
- Claim: PMI-accuracy correlation transfers to CLIP-based LMMs.
- Mechanism: LMMs use frozen CLIP embeddings as visual features; biases in CLIP propagate through the vision-language connector to the LLM.
- Core assumption: Visual instruction tuning does not fully override CLIP embedding biases.
- Evidence anchors:
  - [abstract] "Correlation extends to LMMs built on CLIP (r=0.70 for TextVQA, r=0.62 for VQAv2)."
  - [section 6] "LLaVA-1.5-LAION shows 15% accuracy gap on TextVQA and open-ended VQAv2."
  - [corpus] Corpus lacks direct replication of LMM transfer; evidence is primarily from this paper.

## Foundational Learning

- **Concept: Pointwise Mutual Information (PMI)**
  - Why needed here: Measures co-occurrence normalized for individual frequencies, enabling analysis of compositional generalization.
  - Quick check question: Does PMI increase when two words appear together more often than expected by chance?

- **Concept: Zero-shot classification in CLIP**
  - Why needed here: Primary evaluation metric for CLIP's compositional generalization in this work.
  - Quick check question: How does CLIP classify an image without explicit class labels during training?

- **Concept: Visual instruction tuning for LMMs**
  - Why needed here: Explains how LMMs are trained on top of CLIP and why CLIP biases persist.
  - Quick check question: What is the role of the vision-language connector in an LMM?

## Architecture Onboarding

- **Component map**: LAION-400M captions -> Concept extraction -> PMI calculation -> GenPairs/Imagenet-Paste -> CLIP/LMM evaluation
- **Critical path**:
  1. Extract concepts from LAION-400M captions -> calculate PMI for pairs
  2. Generate GenPairs images -> evaluate CLIP zero-shot accuracy vs PMI
  3. Edit ImageNet images -> evaluate CLIP zero-shot accuracy vs PMI
  4. Fine-tune CLIP with augmentation -> test if correlation reduces on ImageNet-Paste
  5. Train LLaVA with LAION-400M CLIP -> evaluate VQA accuracy vs PMI
- **Design tradeoffs**:
  - Synthetic vs natural images: Synthetic allows PMI control but may introduce diffusion artifacts; natural edits are more realistic but less controlled
  - Model scale vs accuracy gap: Larger CLIP models slightly reduce gap (14.8% -> 13.4%) but do not eliminate correlation
- **Failure signatures**:
  - High PMI but low accuracy -> concept extraction noise or caption unrepresentative of visual content
  - Low PMI but high accuracy -> concept pair may be semantically simple or diffusion model compensates
- **First 3 experiments**:
  1. Replicate PMI calculation on a different pretraining dataset (e.g., DataComp) to test generalizability
  2. Fine-tune CLIP with PMI-weighted contrastive loss to penalize low-PMI pair misalignment
  3. Evaluate non-CLIP visual encoders (e.g., SigLIP) on GenPairs to test architecture-specificity

## Open Questions the Paper Calls Out

- **Open Question 1**: Why does fine-tuning CLIP with image-editing augmentation remove the correlation between PMI and accuracy on edited natural images (ImageNet-Paste) but fail to transfer to the synthetic GenPairs dataset?
- **Open Question 2**: Can new algorithms or architectural modifications enable multimodal models to learn individual concepts independently, thereby breaking the dependence of accuracy on concept co-occurrence statistics?
- **Open Question 3**: To what extent does word co-occurrence in text captions serve as a noisy proxy for actual visual co-occurrence, and does this noise introduce systematic errors in the model's visual representations?
- **Open Question 4**: Does the strong negative correlation between PMI and accuracy degrade non-linearly as the number of concepts in an image increases beyond pairs (e.g., triplets or quadruplets)?

## Limitations

- Correlation results based primarily on CLIP and one LMM architecture (LLaVA) may not generalize to other models.
- PMI proxy assumes captions accurately represent visual content, but this is not directly validated.
- Synthetic GenPairs images may not fully capture the complexity of natural co-occurrences.

## Confidence

- **High confidence**: PMI-accuracy correlation in CLIP zero-shot classification (r=0.97 on GenPairs, 14% accuracy gap on ImageNet-Paste).
- **Medium confidence**: Transfer of PMI-accuracy correlation to LMMs (r=0.70 and r=0.62).
- **Medium confidence**: PMI as a proxy for visual concept co-occurrence.

## Next Checks

1. **Generalize PMI-accuracy correlation to non-CLIP visual encoders**: Evaluate a non-contrastive model (e.g., SigLIP) on GenPairs and ImageNet-Paste to test whether the correlation is CLIP-specific or a broader phenomenon.

2. **Validate PMI-proxy with human annotations**: Collect human judgments on whether concept pairs co-occur visually in the original images corresponding to LAION captions. Compare human co-occurrence scores with PMI to assess the validity of the proxy.

3. **Test PMI-augmented training**: Fine-tune CLIP with a PMI-weighted contrastive loss that penalizes misalignment of low-PMI pairs. Measure whether this reduces the accuracy gap on ImageNet-Paste and improves compositional generalization on downstream VQA tasks.