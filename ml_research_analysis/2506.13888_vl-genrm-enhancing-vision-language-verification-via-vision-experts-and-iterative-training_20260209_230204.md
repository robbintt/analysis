---
ver: rpa2
title: 'VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative
  Training'
arxiv_id: '2506.13888'
source_url: https://arxiv.org/abs/2506.13888
tags:
- arxiv
- response
- reward
- training
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training Vision-Language
  Reward Models (VL-RMs) in the presence of bootstrapping dilemmas and modality bias,
  where self-generated data reinforces existing model limitations and hallucinated
  visual attributes lead to negative example amplification. To mitigate these issues,
  the authors propose an iterative training framework that incorporates vision experts
  for automated preference dataset construction, Chain-of-Thought (CoT) rationales
  for structured reasoning, and Margin-based Rejection Sampling for iterative refinement.
---

# VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training

## Quick Facts
- arXiv ID: 2506.13888
- Source URL: https://arxiv.org/abs/2506.13888
- Authors: Jipeng Zhang; Kehao Miao; Renjie Pi; Zhaowei Wang; Runtao Liu; Rui Pan; Tong Zhang
- Reference count: 40
- Primary result: VL-GenRM achieves 72.3% overall accuracy on VLRewardBench, outperforming baselines in hallucination detection (82.4%) and multimodal reasoning.

## Executive Summary
This paper addresses the challenge of training Vision-Language Reward Models (VL-RMs) in the presence of bootstrapping dilemmas and modality bias, where self-generated data reinforces existing model limitations and hallucinated visual attributes lead to negative example amplification. To mitigate these issues, the authors propose an iterative training framework that incorporates vision experts for automated preference dataset construction, Chain-of-Thought (CoT) rationales for structured reasoning, and Margin-based Rejection Sampling for iterative refinement. The approach refines preference datasets, enhances structured critiques, and iteratively improves multimodal reasoning. Experimental results demonstrate superior performance across VL-RM benchmarks, achieving a 72.3% overall accuracy and showing significant improvements in hallucination detection (82.4%) and multimodal reasoning tasks, advancing VL model alignment with reinforcement learning.

## Method Summary
VL-GenRM addresses the bootstrapping dilemma and modality bias in VL-RM training through a two-stage iterative framework. First, it constructs a preference dataset using vision experts (GroundingDINO and Detectron2) to filter hallucinated responses and generate Chain-of-Thought critiques for structured reasoning. Second, it applies iterative optimization with Margin-based Rejection Sampling to refine the dataset and improve multimodal reasoning. The method employs LoRA-based fine-tuning and Bradley-Terry loss for preference modeling, achieving superior performance across VL-RM benchmarks.

## Key Results
- VL-GenRM achieves 72.3% overall accuracy on VLRewardBench, outperforming baselines in hallucination detection (82.4%) and multimodal reasoning tasks.
- The iterative training framework significantly reduces modality bias, with performance improvements maintained across multiple iterations.
- The method demonstrates superior performance on LLaVA-Wild Best-of-N (N=16) evaluation, indicating enhanced practical applicability.

## Why This Works (Mechanism)
The method works by breaking the bootstrapping loop through automated, vision-expert-driven dataset curation that filters out hallucinated examples before they can reinforce model biases. The Chain-of-Thought critiques provide structured reasoning that guides the model toward multimodal understanding rather than text-only patterns. The margin-based rejection sampling ensures that only high-quality, informative samples contribute to training, preventing the model from overfitting to ambiguous or misleading examples.

## Foundational Learning
- **Bootstrapping Dilemma**: When models learn from their own outputs, errors compound over iterations. Needed to understand why VL-RM training fails without intervention. Quick check: Verify performance degradation when training without vision expert filtering.
- **Modality Bias**: Models favor textual patterns over visual understanding in multimodal tasks. Needed to identify the core problem VL-GenRM addresses. Quick check: Compare accuracy on text-only vs. multimodal test sets.
- **Vision Expert Filtering**: Using object detection to verify response accuracy against image content. Needed to understand the hallucination detection mechanism. Quick check: Measure hallucination detection accuracy with different vision models.
- **Margin-based Rejection Sampling**: Filtering training samples based on preference score margins to improve data quality. Needed to understand the iterative refinement process. Quick check: Plot performance vs. margin threshold to find optimal value.

## Architecture Onboarding
- **Component Map**: Vision Experts (GroundingDINO, Detectron2) -> Data Curation -> CoT Rationale Generation -> Iterative Training (LoRA + Margin-based Rejection) -> VL-GenRM
- **Critical Path**: Data curation and vision expert filtering are critical - errors here propagate through all subsequent training stages and directly impact final performance.
- **Design Tradeoffs**: Object detection-based vision experts excel at hallucination detection but limit abstract reasoning capability, creating a fundamental tension between accuracy and reasoning depth.
- **Failure Signatures**: Descriptive CoT instead of critique CoT leads to modality bias amplification; margin threshold too narrow results in insufficient training data; vision experts missing hallucinated objects reduces hallucination detection accuracy.
- **First Experiments**: 1) Verify vision expert filtering removes hallucinated responses with >80% accuracy. 2) Test CoT rationale generation produces critique-style outputs vs. descriptive text. 3) Measure performance improvement from first vs. second iteration to identify saturation point.

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** How can the trade-off between hallucination suppression and abstract reasoning capability be mitigated when using object-detection-based vision experts?
- **Basis in paper:** [explicit] The authors state in Section 7.1 that VL-GenRM "lags in reasoning due to its object detection-based vision module, which improves general understanding and hallucination resistance but is less suited for abstract reasoning requiring fine-grained scene analysis."
- **Why unresolved:** While the current method successfully reduces hallucinations by verifying concrete objects, it inherently limits the model's capacity for high-level multimodal inference (e.g., mathematical or logical reasoning) which does not rely solely on object presence.
- **What evidence would resolve it:** A modified framework integrating semantic or relational experts (bounding boxes) that demonstrates improved performance on MMMU-Pro/MathVerse benchmarks without sacrificing the high hallucination detection rates reported on POVID/RLAIF-V.

### Open Question 2
- **Question:** Does the iterative training framework continue to yield significant improvements beyond the second iteration when applied to larger models or datasets?
- **Basis in paper:** [explicit] In Section 7.4 (Training Method Ablation), the authors note that "Iteration 2 shows marginal gains, indicating saturation in our current setup," while suggesting that "further iterations to remain beneficial with larger models and datasets."
- **Why unresolved:** The provided experiments only demonstrate saturation at iteration 2 for a 7B parameter model. It remains unclear if the margin-based rejection sampling hits a theoretical ceiling or if the ceiling is merely a constraint of the current model capacity.
- **What evidence would resolve it:** Experiments extending the iterative training pipeline to models with 70B+ parameters or significantly larger data pools, tracking performance stability across 4-5 iterations.

### Open Question 3
- **Question:** To what extent does the accuracy of the external vision expert model act as a bottleneck for the upper bound of VL-GenRM performance?
- **Basis in paper:** [inferred] The methodology relies heavily on the vision expert (Section 4.1.2) to verify object presence ($O^*(I)$) and filter hallucinated negative responses. If the expert detector misses objects or produces false positives, the resulting preference dataset $D_{train}$ will contain incorrect "ground truth" signals.
- **Why unresolved:** The paper assumes the vision expert provides reliable supervision. However, detection models have their own error rates and class limitations. The robustness of the VL-GenRM against expert failures (e.g., failing to detect small or rare objects) is not quantified.
- **What evidence would resolve it:** An error analysis quantifying the percentage of training samples where the vision expert contradicts ground truth, and an ablation study measuring VL-GenRM accuracy when using vision experts of varying precision/recall capabilities.

## Limitations
- The method lags in abstract reasoning tasks due to its reliance on object detection-based vision experts, which are less suited for fine-grained scene analysis.
- Performance improvements show diminishing returns after the second iteration, suggesting potential saturation with current model sizes and datasets.
- The accuracy of the VL-GenRM is inherently limited by the precision and recall of the external vision expert models used for hallucination detection.

## Confidence
- **High Confidence:** The core iterative framework (vision experts + CoT rationales + margin-based rejection sampling) is well-described and the reported benchmark results are verifiable through VLRewardBench evaluation. The method's superiority over baseline VL-RMs is supported by multiple metrics.
- **Medium Confidence:** The vision expert methodology (GroundingDINO + Detectron2) is specific and reproducible, but the hallucination detection accuracy (82.4%) may be sensitive to the vision model versions and object detection thresholds used.
- **Low Confidence:** The exact implementation details of the "score" function in margin-based rejection sampling and the specific configuration of the "weak VLM" could significantly affect the method's performance and generalization.

## Next Checks
1. **Hyperparameter Sensitivity Analysis:** Systematically vary Î» in the L_IFT loss (0.1, 0.5, 0.9) and margin thresholds ([0.1, 0.3, 0.5]) to assess the robustness of reported improvements and identify optimal configurations for different VL models.
2. **Vision Expert Generalization Test:** Replace GroundingDINO with alternative object detection models (e.g., YOLO, CLIP) to verify that hallucination detection performance remains above 80% and that modality bias reduction is consistent across different vision backbones.
3. **Data Source Impact Evaluation:** Compare performance when using different sources for the additional 5K samples per iteration (same initial dataset vs. external data vs. synthetic generation) to determine the method's dependency on data diversity and availability.