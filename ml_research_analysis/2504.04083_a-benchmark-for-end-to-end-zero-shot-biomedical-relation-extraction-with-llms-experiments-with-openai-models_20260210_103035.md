---
ver: rpa2
title: 'A Benchmark for End-to-End Zero-Shot Biomedical Relation Extraction with LLMs:
  Experiments with OpenAI Models'
arxiv_id: '2504.04083'
source_url: https://arxiv.org/abs/2504.04083
tags:
- relations
- relation
- datasets
- entity
- biomedical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a benchmark for evaluating zero-shot biomedical
  relation extraction using large language models. The authors test GPT-4, OpenAI
  o1, and GPT-OSS-120B on seven diverse biomedical datasets, finding that while LLMs
  perform well on simpler datasets with fewer relation types, they struggle with complex
  inputs containing multiple relations and predicates.
---

# A Benchmark for End-to-End Zero-Shot Biomedical Relation Extraction with LLMs: Experiments with OpenAI Models

## Quick Facts
- **arXiv ID**: 2504.04083
- **Source URL**: https://arxiv.org/abs/2504.04083
- **Reference count**: 16
- **Primary result**: Zero-shot LLMs struggle with complex biomedical relation extraction, particularly in information-dense instances with multiple relations per text.

## Executive Summary
This paper establishes a benchmark for evaluating zero-shot biomedical relation extraction using large language models (LLMs). The authors test GPT-4, OpenAI o1, and GPT-OSS-120B on seven diverse biomedical datasets, finding that while LLMs perform well on simpler datasets with fewer relation types, they struggle with complex inputs containing multiple relations and predicates. The study reveals that models tend to under-predict relations in information-dense instances and often produce partial entity matches. GPT-OSS-120B achieves the best overall performance, outperforming both GPT-4 and o1 across most datasets. The research highlights the current limitations of zero-shot approaches for complex biomedical relation extraction tasks.

## Method Summary
The authors evaluate three OpenAI models (GPT-4, o1, and GPT-OSS-120B) on seven biomedical relation extraction datasets using zero-shot prompting. Models receive JSON output templates via the OpenAI API and must simultaneously identify entities and their relations without training data. The evaluation uses strict matching criteria where a predicted relation must match gold relations exactly on entity mentions, entity types, and relation type. Two JSON modes are tested on GPT-4: explicit schema (user-provided) and inferred schema (model-guessed). The datasets vary in text length (sentence vs. abstract), relation types (1-8), and annotation level (mention-level vs. entity-level).

## Key Results
- GPT-OSS-120B achieves the best overall performance across most datasets, outperforming GPT-4 and o1
- Models exhibit under-prediction of relations in information-dense instances, with average relations predicted per test instance lagging behind as gold relations increase
- Partial entity matching represents a major source of error, with models often extracting correct relations but with imprecise entity spans
- Reasoning-enhanced models (o1, GPT-OSS-120B) better accommodate tasks with greater relational and entity complexity through more robust reasoning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Zero-shot relation extraction performance degrades in information-dense contexts due to generative limitations with long sequences.
- **Mechanism**: As the number of valid relations in a single text instance increases, LLMs tend to under-predict rather than hallucinate. This suggests a "generation fatigue" or context window management issue where the model fails to exhaustively enumerate all valid tuples, prioritizing the start of the output sequence over the end.
- **Core assumption**: The model's probability distribution for generating subsequent relations drops sharply after the first few extractions, independent of the actual text content.
- **Evidence anchors**: "models tend to under-predict relations in information-dense instances"; "Average number of relations predicted per test instance lags further behind... as the number of gold relations increases."

### Mechanism 2
- **Claim**: Reasoning-enhanced models mitigate predicate confusion in complex relation schemas better than standard instruction-tuned models.
- **Mechanism**: Datasets with many semantically similar predicates require multi-step inference rather than pattern matching. Reasoning models utilize test-time compute to decompose the biological context before committing to a label, reducing the semantic collision seen in GPT-4.
- **Core assumption**: The performance gap is driven by the ability to resolve semantic ambiguity rather than just the scale of pre-training data.
- **Evidence anchors**: "o1 and GPT-OSS-120B better accommodate tasks with greater relational and entity complexity through more robust reasoning."

### Mechanism 3
- **Claim**: Schema-constrained decoding improves parseability but exposes limitations in entity boundary detection.
- **Mechanism**: Forcing JSON output successfully formats the data, but because the model generates text token-by-token rather than pointing to spans, it often produces partial or "fuzzy" entity matches that strict evaluation metrics mark as errors.
- **Core assumption**: The model retrieves the concept from internal knowledge rather than extracting strictly from the provided context window.
- **Evidence anchors**: "often produce partial entity matches"; "Predicted entity mentions were usually assigned the correct entity type... [but] partial matching entity mentions... was a major source of error."

## Foundational Learning

- **Concept: End-to-End (E2E) Relation Extraction**
  - **Why needed here**: The paper explicitly distinguishes E2E from relation classification. In E2E, the model must identify entities *and* their relations simultaneously. This is critical because the reported performance drops are partly due to the added burden of entity detection.
  - **Quick check question**: Does the task require identifying *that* two entities are related (classification), or finding *which* entities are related and how (extraction)?

- **Concept: Mention-Level (ML) vs. Entity-Level (EL) Annotation**
  - **Why needed here**: The paper evaluates both. ML requires matching the exact text span (string), while EL requires mapping to a normalized concept ID. Understanding this distinction is necessary to interpret the "partial match" errors.
  - **Quick check question**: If the text says "Aspirin" and the database ID is "B01AC06", is the task successful if the model outputs "Aspirin" (ML) or if it outputs the ID (EL)?

- **Concept: Schema Modes (Explicit vs. Inferred)**
  - **Why needed here**: The paper compares GPT-4's "explicit" (user-provided schema) vs. "inferred" (model-guessed schema) modes. This is an architectural detail that impacts reliability.
  - **Quick check question**: When calling the LLM API, are we providing a rigid JSON structure for the output, or letting the model structure its own response?

## Architecture Onboarding

- **Component map**: Input Layer -> Prompting Layer -> Model Layer -> Parsing Layer -> Evaluation Layer
- **Critical path**: The **Prompting Layer** is the highest leverage component. Defining the "nature of the problem" and the "output template" in natural language is the primary driver of success in a zero-shot setting.
- **Design tradeoffs**:
  - Strict vs. Relaxed Evaluation: Strict matching penalizes the LLM for synonyms (low recall); relaxed matching inflates scores (low precision).
  - Inferred vs. Explicit Schema: Explicit schemas ensure valid JSON structure but may confuse the model if complex; Inferred schemas are more flexible but prone to syntax errors.
- **Failure signatures**:
  - Low Recall / Under-prediction: Model outputs 2 relations in a document with 20.
  - Partial Match: Model outputs "lung cancer" when text says "non-small cell lung cancer".
  - Predicate Hallucination: Model invents a relation type not in the instruction set.
- **First 3 experiments**:
  1. Predicate Complexity Stress Test: Run GPT-4 on ChemProt (5 classes) vs. BioRED (8 classes) to confirm hypothesis that "number of relation types" correlates negatively with F1.
  2. Entity Boundary Analysis: Measure "partial match" rate and calculate F1 if partial matches were counted as true positives.
  3. Output Length Ablation: Truncate inputs with >50 relations to see if under-prediction is caused by input context size or output token limits.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does performance improve when datasets are sliced by entity and relation types, validating the hypothesis that lower scores are driven by task complexity rather than other factors? The authors observed a correlation between datasets with many relation types and low F1 scores but did not perform granular analysis to determine if specific relation types are responsible.

- **Open Question 2**: How do other frontier models (e.g., Anthropic, Google, xAI) perform on this benchmark compared to the OpenAI models tested? The study's resource constraints limited evaluation to GPT-4, o1, and GPT-OSS-120B, leaving generalizability across the broader landscape of frontier LLMs unknown.

- **Open Question 3**: Can specific prompt engineering or decoding strategies reduce the high frequency of partial entity matching errors? The models frequently extracted correct relations but with imprecise entity spans, causing failures in strict evaluation metrics.

## Limitations
- Strict evaluation methodology may underestimate true capability by penalizing semantically correct but partially-matched entities
- Reproducibility concerns due to unclear model versions for GPT-4 and o1, with only GPT-OSS-120B precisely specified
- Limited exploration of alternative prompting strategies (chain-of-thought, few-shot examples) that could improve performance on complex datasets

## Confidence

- **High Confidence**: Core finding that zero-shot LLMs underperform on complex biomedical relation extraction tasks with multiple relations per instance, and that reasoning-enhanced models show systematic improvements over GPT-4
- **Medium Confidence**: Mechanism explanations for under-prediction and partial entity matching, though alternative explanations could contribute to these behaviors
- **Low Confidence**: Relative performance rankings between o1 and GPT-OSS-120B across all datasets due to limited model configurations and prompt variations tested

## Next Checks

1. **Entity Boundary Precision Analysis**: Implement a relaxed evaluation metric that counts partial entity matches as true positives to isolate whether performance limitations stem from relation extraction capability versus entity boundary detection accuracy.

2. **Reasoning Step Ablation**: Test whether the performance advantage of o1 and GPT-OSS-120B persists when reasoning steps are explicitly disabled through prompt engineering, to confirm whether their advantage derives from test-time reasoning versus pre-training.

3. **Output Length Constraint Study**: Systematically vary input truncation and output token limits to determine whether under-prediction is caused by context window constraints, output generation fatigue, or fundamental limitations in the model's ability to enumerate multiple relations from dense information.