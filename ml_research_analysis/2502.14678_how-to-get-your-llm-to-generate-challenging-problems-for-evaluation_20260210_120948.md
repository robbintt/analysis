---
ver: rpa2
title: How to Get Your LLM to Generate Challenging Problems for Evaluation
arxiv_id: '2502.14678'
source_url: https://arxiv.org/abs/2502.14678
tags:
- answer
- question
- page
- code
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CHASE, a framework for synthetically generating
  challenging evaluation problems using large language models (LLMs) without human
  involvement. The key innovation is a bottom-up approach that iteratively hides components
  of simpler problems to create complex ones, combined with decomposition into independently
  verifiable sub-tasks for quality control.
---

# How to Get Your LLM to Generate Challenging Problems for Evaluation

## Quick Facts
- **arXiv ID**: 2502.14678
- **Source URL**: https://arxiv.org/abs/2502.14678
- **Reference count**: 40
- **Primary result**: CHASE generates challenging evaluation problems that state-of-the-art models solve at only 40-60% accuracy.

## Executive Summary
CHASE is a framework for synthetically generating challenging evaluation problems using large language models without human involvement. The key innovation is a bottom-up approach that iteratively hides components of simpler problems to create complex ones, combined with decomposition into independently verifiable sub-tasks for quality control. The framework was implemented across three domains—document-based QA with long contexts, repository-level code completion, and grade-school math word problems—producing benchmarks that effectively differentiate between models that perform similarly on standard benchmarks.

## Method Summary
CHASE uses a two-phase framework: bottom-up problem construction starting from simple problem-solution pairs, then iteratively hiding reasoning components to increase difficulty; and decomposition of generation into independently verifiable sub-tasks using separate Generator (G) and Verifier (V) LLMs. For each domain, the pipeline defines sequential stages with per-stage verification, discarding and regenerating when verification fails. The framework uses rejection sampling against reference models to ensure final benchmarks retain only challenging problems that SOTA models cannot easily solve.

## Key Results
- Generated benchmarks proved highly challenging, with SOTA models achieving only 40-60% accuracy
- Performance degrades uniformly with increasing context size, with drops up to 70% when scaling beyond 50k tokens
- The framework effectively differentiates between models that perform similarly on standard benchmarks
- Generated benchmarks contain 3-6% error rates despite LLM-based verification

## Why This Works (Mechanism)

### Mechanism 1: Bottom-Up Problem Construction
Starting from simple problems and iteratively adding complexity creates problems that challenge even the generating model. Instead of generating a hard problem then solving it (which guarantees solvability), the framework starts with verifiable simple problem-solution pairs, then progressively hides intermediate reasoning steps and adds distracting context. This breaks the inherent solvability guarantee of forward-generation approaches.

### Mechanism 2: Decomposition into Independently Verifiable Sub-Tasks
Breaking generation into atomic steps with per-step verification reduces error accumulation and enables quality control without human oversight. Each pipeline stage is treated as an independent task with a separate verifier model checking each stage's output before proceeding. This prevents compounding errors and allows targeted regeneration when verification fails.

### Mechanism 3: Strategic Information Hiding to Increase Reasoning Depth
Concealing intermediate reasoning steps within expanded context forces multi-hop inference, creating difficulty through retrieval and synthesis demands rather than problem formulation complexity. For math problems, the answer to step i becomes an implicit assumption for step i+1 without being stated. For QA, answer points are scattered across documents embedded within irrelevant but topically related content.

## Foundational Learning

- **Chain-of-Thought (CoT) Reasoning**: Understanding how models decompose multi-step problems is essential for designing the math pipeline's iterative extension mechanism and for interpreting why hiding intermediate steps creates difficulty. Quick check: Given a 3-step arithmetic problem, can you identify which intermediate results would need to be inferred if step 2's answer were omitted from the context?

- **Rejection Sampling for Difficulty Filtering**: All three CHASE benchmarks use rejection sampling—discarding problems that weaker models solve correctly—to ensure the final benchmark retains challenging examples. Understanding this helps interpret why baseline approaches (without this filtering) produce easier datasets. Quick check: If you generate 1000 problems and a model solves 800 correctly, what proportion should you discard to maximize difficulty while maintaining dataset size, and what tradeoff does this introduce?

- **Long-Context Retrieval vs. Reasoning Trade-offs**: The paper shows performance degrades with context length, but distinguishing whether this is a retrieval failure (can't find relevant info) or reasoning failure (can't synthesize found info) is critical for interpreting results and designing interventions. Quick check: In a 50k-token document with 5 relevant sentences scattered throughout, what behavioral differences would you expect between a retrieval-limited model and a reasoning-limited model?

## Architecture Onboarding

- **Component map**: 
  - Generator G (produces content at each pipeline stage)
  - Verifier V (checks each stage's output before proceeding)
  - Pipeline orchestration (sequential stages with failure handling)
  - Rejection sampler (post-hoc filter removing problems solved by reference model)

- **Critical path**: 
  1. Domain-specific pipeline definition (design prompts for each sub-task)
  2. Seed data acquisition (existing datasets for math; bootstrapping for QA/code)
  3. Iterative generation with per-stage verification
  4. Rejection sampling against reference model
  5. Final quality check (manual review of sample, LLM-judge calibration)

- **Design tradeoffs**: 
  - Generator strength vs. problem difficulty (stronger generators produce more coherent problems but may also solve them)
  - Verification strictness vs. yield (aggressive verification improves quality but dramatically reduces yield)
  - Context length vs. evaluation cost (longer contexts better stress-test models but increase inference costs)
  - Rejection sampling rate vs. discriminative power (aggressive filtering creates harder benchmarks but may eliminate entire problem categories)

- **Failure signatures**: 
  - Contamination leakage (generated problems replicate patterns from training data)
  - Verification blind spots (systematic errors where verifier and generator make identical mistakes)
  - Unnatural language (LLM-generated text uses awkward phrasing that introduces spurious difficulty)
  - Yield collapse (pipeline produces very few valid examples after all verification stages)

- **First 3 experiments**: 
  1. Implement direct prompting baseline (Evol-Instruct style) for one domain and compare against CHASE-pipeline output on difficulty and quality
  2. Take the 100-example QA subset and progressively extend context with irrelevant documents, measuring accuracy degradation across model families
  3. Run the math pipeline with single-model verification vs. ensemble verification vs. no verification, measuring error rate in final output through manual review

## Open Questions the Paper Calls Out

- How can we modify this framework to easily adapt to different tasks beyond the specific domains of QA, code, and math?
- How can we verify LLM generations more effectively to reduce error rates in ground-truth annotations?
- Can the pipeline efficiency be improved to reduce the high discard rate of intermediate generations and the associated computational costs?

## Limitations

- The framework's reliance on LLMs for both generation and verification introduces risks of systematic biases in verifier models
- Quality of generated problems depends heavily on prompt engineering quality for both G and V, which may not generalize well across different domains
- The approach hasn't been validated beyond the three demonstrated domains and may not scale to other reasoning-intensive tasks

## Confidence

- **High confidence**: Bottom-up problem construction with iterative hiding of reasoning components is well-supported by experimental results showing consistent performance degradation across all three domains
- **Medium confidence**: Decomposition into independently verifiable sub-tasks shows promise but has weaker direct validation of verifier reliability
- **Low confidence**: Scalability and generalizability claims beyond the three demonstrated domains are speculative without additional evidence

## Next Checks

1. Run the math pipeline with single-model verification vs. ensemble verification vs. no verification, measuring error rate in final output through manual review of 50 examples per condition
2. Implement CHASE for a fourth domain (e.g., scientific reasoning) using the same framework components, measuring whether the 40-60% difficulty target can be achieved
3. Systematically test whether problems in each benchmark are solvable by models trained on similar data distributions using model capability tracking and training corpus analysis