---
ver: rpa2
title: 'Chain-of-Experts: Unlocking the Communication Power of Mixture-of-Experts
  Models'
arxiv_id: '2506.18945'
source_url: https://arxiv.org/abs/2506.18945
tags:
- expert
- experts
- routing
- arxiv
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Chain-of-Experts introduces iterative expert communication within
  Mixture-of-Experts layers, allowing tokens to be routed sequentially through different
  experts across multiple iterations rather than being assigned to a static subset.
  This design uses independent routers at each iteration, enabling tokens to re-evaluate
  and select new experts based on intermediate representations, thus increasing expert
  diversity and compositional depth.
---

# Chain-of-Experts: Unlocking the Communication Power of Mixture-of-Experts Models

## Quick Facts
- arXiv ID: 2506.18945
- Source URL: https://arxiv.org/abs/2506.18945
- Reference count: 40
- Primary result: CoE reduces validation loss from 1.20 to 1.12 on math reasoning vs. standard MoE under fixed compute

## Executive Summary
Chain-of-Experts (CoE) introduces iterative expert communication within Mixture-of-Experts layers, allowing tokens to be routed sequentially through different experts across multiple iterations rather than being assigned to a static subset. This design uses independent routers at each iteration, enabling tokens to re-evaluate and select new experts based on intermediate representations, thus increasing expert diversity and compositional depth. Experiments show that under fixed compute, Chain-of-Experts reduces validation loss from 1.20 to 1.12 on math reasoning tasks compared to standard MoE.

## Method Summary
CoE processes tokens iteratively across a chain of experts inside a single layer, rather than selecting a static subset as in standard MoE. Each iteration uses an independent router that gates based on the current hidden state, allowing tokens to "reevaluate" expert affinity after intermediate transformations. The model uses inner residual connections at each iteration to stabilize updates and preserve information across the iterative chain. With fixed compute, CoE achieves lower validation loss by increasing the diversity of expert combinations and enriching the model's representational capacity.

## Key Results
- Validation loss reduction: 1.20 → 1.12 on MetaMathQA under equal compute (MoE K=8, C=1 vs. CoE K=4, C=2)
- Memory efficiency: 2× iterations match 3× expert selections in width while reducing memory by 17.6-42%
- Combinatorial advantage: 823× more expert pairings than one-shot routing with n=64, k=4

## Why This Works (Mechanism)

### Mechanism 1: Sequential Expert Composition
Processing tokens through a chain of experts within a single layer increases representational capacity beyond parallel expert activation with equivalent compute. Rather than selecting K experts simultaneously, CoE processes tokens over C iterations, activating K/C experts per iteration. Each iteration receives the refined representation from the previous step, enabling compositional depth.

### Mechanism 2: Iteration-Conditional Routing
Independent routers per iteration enable dynamic expert re-selection based on evolving token representations, which is critical for compositional reasoning. Each iteration uses a dedicated gating function that routes based on the current hidden state, not the original input.

### Mechanism 3: Inner Residual Connections for Gradient Stability
Residual connections applied at each iteration stabilize multi-step expert processing and enable effective credit assignment along the iterative path. The update rule preserves information across iterations, allowing gradients to flow through the chain without vanishing.

## Foundational Learning

### Concept 1: Mixture-of-Experts (MoE) Sparse Gating
Why needed: CoE modifies the standard MoE layer; understanding baseline top-k routing, expert selection, and sparsity is prerequisite to grasping why iterative chaining changes representational capacity.
Quick check: Given input x and N=64 experts, can you compute the output of a standard MoE layer that selects top-k=8 experts? What happens to representational capacity if k is reduced to 4?

### Concept 2: Residual Connections in Deep Networks
Why needed: CoE's inner residual mechanism differs from standard transformer residuals; understanding why residuals stabilize training and enable gradient flow is essential for diagnosing convergence issues.
Quick check: In a 4-iteration CoE chain, explain why adding x(t-1) at each step (inner residual) might outperform adding only x(0) at the final step (outer residual). What gradient flow pattern does each enable?

### Concept 3: Scaling Laws and Compute-Efficiency Trade-offs
Why needed: CoE introduces "depth through iteration" as a new scaling axis; understanding traditional width/depth scaling helps contextualize why memory-efficient scaling matters for large models.
Quick check: If doubling model depth increases memory by 2× but doubling iteration count C increases memory by only 17.6-42% (per Section 5.2), under what conditions would you prefer iteration scaling over layer scaling?

## Architecture Onboarding

### Component Map
Transformer Layer -> Attention Sublayer -> CoE MoE Sublayer -> Input Hidden State x(0) -> Iteration 1: Router_1, Expert Processing, Residual Addition -> x(1) -> Iteration 2: Router_2, Expert Processing, Residual Addition -> x(2) -> ... -> Output Hidden State x(C)

### Critical Path
1. Input processing: Token embedding enters CoE layer as x(0)
2. Iteration loop (C times):
   - Router_t computes affinity scores for all experts
   - Top-K/C selection produces sparse expert subset
   - Selected experts process x(t-1) in parallel
   - Outputs are weighted by gate scores and summed
   - Inner residual: x(t) = expert_sum + x(t-1)
3. Output: Final hidden state x(C) passed to next layer

### Design Tradeoffs
| Parameter | Increase Effect | Risk |
|-----------|-----------------|------|
| Iteration count C | More compositional depth, higher effective capacity | Diminishing returns beyond C=2; training instability |
| Experts per iteration (K/C) | More parallel expert capacity per step | Reduces sparsity benefit; dense settings limit gains |
| Total experts N | More routing flexibility | Memory overhead; CoE with N=48 matches MoE N=64 |
| Shared experts | Stable backbone for generalization | Limited benefit on some benchmarks |

### Failure Signatures
- Shared gating (no iteration-specific routing): Validation loss plateaus early (~1.5), worse than baseline
- Outer residuals only: Slower convergence, higher final loss
- High iteration count (C≥3): Unstable optimization, diminishing returns
- Dense routing (K≈N): Minimal improvement over standard MoE

### First 3 Experiments
1. Baseline comparison: Train MoE (K=8, C=1) and CoE (K=4, C=2) on MetaMathQA with identical compute budget; plot validation loss curves to replicate 1.20→1.12 reduction
2. Ablation on routing: Remove iteration-specific gating (share router across all C iterations); expect performance collapse per Figure 7 to validate routing mechanism
3. Scaling comparison: Compare CoE (C=2, L=4 layers) vs. MoE (C=1, L=8 layers) with matched parameters; measure memory usage and final loss to confirm 42% memory reduction claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the performance and efficiency advantages of Chain-of-Experts (CoE) persist under standard scaling laws with larger model sizes and batch sizes?
- Basis in paper: [explicit] The authors state in the Limitations section that future work must examine whether CoE's advantages persist under "standard scaling law conditions," as current experiments were restricted to smaller models (544M parameters).
- Why unresolved: The paper validates CoE primarily on a specific small-scale architecture (DeepSeek-V2-Lite) to ensure reproducibility, leaving the behavior of CoE at billion-parameter scales unknown.
- What evidence would resolve it: Training curves and benchmark results for CoE models scaled to 7B parameters or higher compared against standard MoE baselines of equivalent size.

### Open Question 2
- Question: How can the number of expert processing iterations (C) be increased efficiently without causing optimization instability?
- Basis in paper: [explicit] Section C.2 reports that increasing iterations C beyond 2 leads to "diminishing or unstable gains," and Section 8 notes that the effect of deeper iterative depth remains unexplored.
- Why unresolved: The paper suggests that naive scaling of the communication horizon leads to inefficient communication dynamics, limiting main experiments to C=2.
- What evidence would resolve it: The development of normalization techniques or learning rate schedules that allow stable training at C=4 or higher, resulting in further validation loss reduction.

### Open Question 3
- Question: Can CoE be successfully integrated with architectures that utilize cross-layer parameter sharing (e.g., MoEUT)?
- Basis in paper: [explicit] Section 8 explicitly lists combining CoE with "architectural variants that share experts across layers such as MoEUT" as an open problem.
- Why unresolved: CoE introduces iteration within a layer, while methods like MoEUT rely on reusing experts across layers; the interaction between these two forms of parameter reuse is currently undefined.
- What evidence would resolve it: A hybrid model that successfully combines intra-layer iteration with inter-layer sharing, demonstrating parameter efficiency gains greater than either method alone.

## Limitations
- Domain specificity: CoE shows strong performance on mathematical reasoning but mixed results on translation and coding tasks, suggesting task-dependent benefits
- Scaling constraints: Benefits diminish beyond C=2 iterations due to optimization instability and require more compute to converge
- Limited generalization: Current experiments focus on reasoning tasks; broader domain evaluation is necessary to validate universal applicability

## Confidence

- **High confidence**: The core architectural mechanism (iterative expert processing with independent per-iteration routing) is well-validated through ablation studies. The performance improvement from 1.20 to 1.12 validation loss on MetaMathQA under equal compute is directly measurable and reproducible.

- **Medium confidence**: The theoretical advantage of increased expert combinatorial diversity (823× more pairings) is mathematically sound, but the practical impact on downstream task performance varies by domain.

- **Low confidence**: The generality of CoE's benefits across all sparse model applications remains unproven. The paper demonstrates strong performance on reasoning tasks but shows limited gains on translation and coding, suggesting the mechanism may be specialized rather than universal.

## Next Checks

1. **Cross-domain generalization test**: Train CoE (K=4, C=2) and standard MoE (K=8, C=1) on a diverse benchmark suite including reasoning, translation, coding, and general knowledge tasks. Measure not just final accuracy but convergence speed and training stability across all domains.

2. **Scaling law validation**: Systematically vary model scale (N=64, 128, 256 experts) and task complexity to test the claimed "new scaling dimension." Specifically, measure how CoE's performance advantage evolves as model size increases.

3. **Routing mechanism analysis**: Conduct a detailed study of expert co-activation patterns across iterations using attribution methods or influence functions. Track how specific tokens' routing decisions evolve through the iteration chain and correlate these patterns with task performance.