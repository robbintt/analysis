---
ver: rpa2
title: 'Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds'
arxiv_id: '2512.22473'
source_url: https://arxiv.org/abs/2512.22473
tags:
- attention
- gradient
- bayesian
- routing
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how cross-entropy training reshapes attention
  scores and value vectors in a transformer attention head. The authors derive complete
  first-order gradients for all parameters and show that attention scores follow an
  advantage-based routing law while values evolve via responsibility-weighted updates.
---

# Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds

## Quick Facts
- arXiv ID: 2512.22473
- Source URL: https://arxiv.org/abs/2512.22473
- Reference count: 8
- Key outcome: Derives first-order gradients for attention parameters showing EM-like two-timescale dynamics where attention routes to values that help reduce loss, and those values specialize into prototypes via responsibility-weighted updates.

## Executive Summary
This paper analyzes how cross-entropy training reshapes attention scores and value vectors in a transformer attention head. The authors derive complete first-order gradients for all parameters and show that attention scores follow an advantage-based routing law while values evolve via responsibility-weighted updates. This creates a feedback loop where queries route to values that are above-average for their error signal, and those values move toward their users, enabling specialization. The dynamics resemble a two-timescale EM procedure: attention implements an E-step (soft responsibilities) while values implement an M-step (responsibility-weighted prototype updates). Through controlled simulations on a sticky Markov-chain task, the authors demonstrate that EM-like training schedules reach low loss and sharp attention faster than standard SGD. The same gradient dynamics that minimize cross-entropy also sculpt the low-dimensional Bayesian manifolds required for in-context probabilistic reasoning, providing a unified picture where optimization (gradient flow) gives rise to geometry (Bayesian manifolds), which in turn supports function (Bayesian inference).

## Method Summary
The paper studies single-head attention with cross-entropy loss on synthetic tasks. For the toy simulation, it uses T=5 positions, 3 classes, and d_k=d_v=2. For the sticky Markov-chain task, it uses T=2000, 8-symbol vocabulary with self-transition probability 0.3, and causal masking. The training compares standard SGD with uniform learning rates against an EM-like schedule with 10× larger learning rate for values. The analysis derives gradients ∂L/∂s_ij for scores and ∂L/∂v_j for values, showing that scores implement advantage-based routing (α_ij(b_ij − E_αi[b])) while values aggregate responsibility-weighted gradients (Σ_i α_ij u_i). The paper validates the two-timescale dynamics through these controlled experiments.

## Key Results
- Attention scores follow an advantage-based routing law that reallocates mass toward values that are above-average at reducing loss for each query
- Value vectors become specialized prototypes through responsibility-weighted updates, creating a positive feedback loop with attention
- EM-like training schedules (10× larger LR for values) reach SGD's final loss in 430 ± 143 steps vs. 1000, demonstrating 2.3× speedup

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Attention reallocates mass toward positions whose values are better-than-average at reducing loss for each query.
- **Mechanism:** The score gradient ∂L/∂s_ij = α_ij(b_ij − E_αi[b]) implements an advantage rule. Since gradient descent subtracts the gradient, scores *increase* when b_ij < E_αi[b] (below-average compatibility with the error direction, i.e., helpful) and *decrease* when b_ij > E_αi[b] (harmful). This creates content-dependent routing that tracks which values help which queries.
- **Core assumption:** Small learning rates such that first-order approximations hold; upstream gradient u_i meaningfully indicates the error direction.
- **Evidence anchors:**
  - [abstract] "queries route more strongly to values that are above-average for their error signal"
  - [Section 3.4] Derives advantage quantity A_ij := −(b_ij − E_αi[b]); positive advantage → increasing attention reduces loss
  - [corpus] "Gradient Descent as Implicit EM" (FMR=0.58) documents analogous advantage-style dynamics in distance-based models
- **Break condition:** If attention entropy collapses prematurely before values specialize (e.g., very large learning rates), routing freezes on suboptimal positions and the feedback loop stalls.

### Mechanism 2
- **Claim:** Value vectors become specialized prototypes for subsets of queries that attend to them.
- **Mechanism:** The value update Δv_j = −η Σ_i α_ij u_i aggregates upstream gradients weighted by attention. If a small set of queries heavily uses position j, v_j moves to serve that subset. This is reinforced by the score gradient: as v_j becomes more helpful, b_ij decreases, advantage increases, α_ij grows, giving u_i more weight in future v_j updates—a positive feedback loop.
- **Core assumption:** Different queries have non-identical error directions u_i, creating pressure for value differentiation rather than all values converging to the same prototype.
- **Evidence anchors:**
  - [abstract] "values evolve via responsibility-weighted updates... those values move toward their users, enabling specialization"
  - [Section 4.4] Explicit feedback loop description: helpful values → increased attention → larger gradient weight → further specialization
  - [corpus] Limited direct evidence on responsibility-weighted prototypes in transformers; corpus focuses on EM/entropy rather than attention specialization specifically
- **Break condition:** If all queries have aligned error directions (e.g., trivial task), values may collapse toward a single prototype rather than specialize.

### Mechanism 3
- **Claim:** Attention stabilizes early (defining a hypothesis frame) while values continue to refine (improving precision), producing EM-like two-timescale dynamics.
- **Mechanism:** Score gradients approach zero when b_ij ≈ E_αi[b] for all i,j—compatibility equalizes and attention "freezes." But value gradients remain non-zero as long as loss is non-zero, continuing to sculpt the value manifold. This matches EM: E-step (responsibilities/attention) converges, then M-step (prototypes/values) refines under fixed responsibilities.
- **Core assumption:** The task structure admits approximately separable routing and content optimization; value gradients are smaller magnitude than routing gradients early in training.
- **Evidence anchors:**
  - [abstract] "dynamics resemble a two-timescale EM procedure: attention implements an E-step... while values implement an M-step"
  - [Section 6.1] "Once score gradients have largely equalized... attention freezes. But unless loss is exactly zero, u_i remains non-zero and value updates continue"
  - [Section 7.2] EM-like schedule (10× larger LR for values) reaches SGD's final loss in 430 ± 143 steps vs. 1000—2.3× speedup
  - [corpus] "Gradient Descent as Implicit EM" provides convergent evidence for EM interpretations of gradient descent
- **Break condition:** If learning rates are too uniform or task requires tightly coupled routing-content co-adaptation, the timescale separation may not emerge; attention and values may oscillate rather than stabilize sequentially.

## Foundational Learning

- **Concept: Cross-entropy gradient through softmax**
  - **Why needed here:** The entire mechanism derives from how CE loss propagates through softmax attention. Without understanding ∂L/∂ℓ_i = p_i − e_yi and the softmax Jacobian ∂α_ij/∂s_ik = α_ij(δ_jk − α_ik), the advantage-based routing law is opaque.
  - **Quick check question:** For a 3-class problem with logits [2.0, 1.0, 0.5] and target class 1, what is the gradient ∂L/∂ℓ_2? (Answer: softmax ≈ [0.659, 0.242, 0.099]; gradient = [0.659, 0.242−1, 0.099] = [0.659, −0.758, 0.099])

- **Concept: Expectation-Maximization (EM) intuition**
  - **Why needed here:** The paper frames attention-value dynamics as implicit EM. Understanding E-step (soft assignments/responsibilities) and M-step (parameter updates under fixed responsibilities) clarifies why the two-timescale interpretation is more than analogy.
  - **Quick check question:** In a Gaussian mixture model with responsibilities γ_ij for data point i belonging to cluster j, what does the M-step update for cluster mean μ_j look like? (Answer: μ_j ← Σ_i γ_ij x_i / Σ_i γ_ij—responsibility-weighted average, analogous to Δv_j = −η Σ_i α_ij u_i)

- **Concept: Manifold in representation space**
  - **Why needed here:** The paper claims gradient flow sculpts "low-dimensional Bayesian manifolds"—values lie on curves/surfaces parameterized by posterior entropy. This is about geometry in the v_j embedding space, not data manifolds.
  - **Quick check question:** If you have 100 value vectors in R^50 but their first principal component explains 90% of variance, what does this imply about the effective dimensionality of the "value manifold"? (Answer: Approximately 1-dimensional—values lie near a curve in the 50-d space)

## Architecture Onboarding

- **Component map:** Cross-entropy loss → upstream gradient u_i = W_O^T(p_i − e_yi) → compatibility b_ij = u_i^T v_j → advantage A_ij = −(b_ij − E_αi[b]) → score gradient → attention reallocation → responsibility-weighted value updates ū_j = Σ_i α_ij u_i

- **Critical path:** The feedback loop starts with the error signal u_i, which determines compatibility b_ij. This drives attention reallocation via advantage-based routing. The resulting responsibilities weight value updates, which improve compatibility, creating the specialization cycle. The manifold emerges as values move to serve distinct query subsets.

- **Design tradeoffs:**
  - **Uniform vs. parameter-specific LR:** Uniform LR may slow value specialization; EM-like schedule (larger value LR) accelerates convergence but requires tuning
  - **Attention dropout:** Disrupts the feedback loop, preventing over-specialization but potentially slowing manifold formation
  - **Single-head vs. multi-head:** Single head creates competition among queries for value specialization; multi-head allows parallel specialized manifolds

- **Failure signatures:**
  - **Attention collapse:** All attention mass on single position; check if learning rate is too large or task is trivial
  - **Value norm explosion:** ∥v_j∥ growing unbounded; monitor value norms, consider LayerNorm on values
  - **Underused values:** Low column usage Σ_i α_ij indicates dead positions; may need attention entropy regularization
  - **Diffuse routing:** Attention remains high-entropy after many steps; advantage signals may be too uniform (task doesn't differentiate positions)

- **First 3 experiments:**
  1. **Monitor compatibility matrix B = (b_ij):** Track whether values differentiate—do different query subsets have distinct "most helpful" positions? Visualize as heatmap over training.
  2. **Test EM-like schedule:** Compare uniform LR vs. 5–10× larger value LR on a structured sequence task (e.g., Markov chain). Measure steps to target loss and final attention entropy.
  3. **PCA trajectory of value vectors:** Project v_j over training steps. Verify that values move coherently toward low-dimensional structure rather than randomly scattering. Check correlation between PC1 and task variable (e.g., posterior entropy if known).

## Open Questions the Paper Calls Out

- **Question:** Do advantage-based routing dynamics generalize to selective state-space models (SSMs) like Mamba?
  - **Basis in paper:** [explicit] The paper proposes Conjectures 9.2 and 9.3 for "Content-Based Value Routing" but states in Section 9.5 that "formal derivation for future work" is needed for SSMs.
  - **Why unresolved:** SSMs use input-dependent gating and recurrences, complicating the gradient analysis compared to the parallelizable softmax attention mechanism.
  - **What evidence would resolve it:** A closed-form derivation of SSM gradients revealing an advantage-based term, or empirical confirmation of EM-like specialization in Mamba training dynamics.

- **Question:** How do inter-head coordination and hierarchical specialization manifest in multi-layer transformers?
  - **Basis in paper:** [explicit] Section 11 lists "Multi-head, multi-layer dynamics—including inter-head coordination and hierarchical specialization" as an area that "remain[s] open."
  - **Why unresolved:** The analysis deliberately isolates a single head to maintain mathematical transparency, ignoring interactions between heads or residual streams.
  - **What evidence would resolve it:** Extending the gradient analysis to multi-head settings or tracking the formation of value manifolds across different layers in a full-scale model.

- **Question:** Do modern adaptive optimizers (e.g., Adam) preserve the two-timescale EM-like convergence identified in the first-order analysis?
  - **Basis in paper:** [explicit] Section 11 notes the analysis assumes "small learning rates" and ignores "momentum, Adam," listing the extension to realistic optimizers as an "important next step."
  - **Why unresolved:** Adaptive learning rates might disrupt the specific gradient magnitude ratios required for the observed "frame-precision dissociation."
  - **What evidence would resolve it:** Comparing value vector trajectory convergence and manifold formation between vanilla SGD and Adam using the diagnostics proposed in Section 8.

## Limitations

- The mechanistic claims are derived analytically from gradient equations, but real transformer attention involves residual connections, LayerNorm, and multi-head architectures that can significantly alter dynamics.
- The advantage-based routing mechanism assumes that error signals u_i meaningfully distinguish query subsets—if the task or value initialization produces uniform u_i across queries, the feedback loop for specialization may not emerge.
- The connection between cross-entropy gradient flow and the emergence of low-dimensional Bayesian manifolds is conceptually appealing but lacks rigorous geometric analysis.

## Confidence

- **High confidence:** The mathematical derivations of score and value gradients are correct and clearly presented. The advantage-based routing law ∂L/∂s_ij = α_ij(b_ij − E_αi[b]) follows directly from chain rule application and softmax gradient properties. The two-timescale EM interpretation is well-supported by gradient equations and the controlled simulation results.
- **Medium confidence:** The claim that values specialize into prototypes for query subsets is supported by the gradient dynamics and toy simulation, but the corpus lacks direct transformer-specific evidence. The sticky Markov-chain results show EM-like schedules converge faster, but it's unclear if this speedup generalizes beyond the specific task structure.
- **Low confidence:** The connection between cross-entropy gradient flow and the emergence of low-dimensional Bayesian manifolds is conceptually appealing but lacks rigorous geometric analysis. The paper asserts that value vectors "sculpt" these manifolds but doesn't quantify manifold dimensionality or demonstrate the Bayesian inference capabilities that depend on this geometry.

## Next Checks

1. **Test specialization robustness:** Run the sticky Markov-chain experiment with 3 variants: (a) add LayerNorm after values, (b) add residual connections, (c) use 4 heads instead of 1. Measure whether attention still sharpens and values still specialize under these realistic architectural modifications.

2. **Vary query diversity:** Generate synthetic tasks where error signals u_i have controlled similarity (e.g., uniform random, clustered, or identical across queries). For each, track attention entropy and value differentiation over training to identify the minimum query diversity needed for the specialization feedback loop.

3. **Quantify manifold geometry:** After training on the Markov-chain task, compute PCA on value vectors v_j and measure explained variance ratios. Compare the effective dimensionality (number of components needed for 90% variance) against random initialization to quantify how much gradient flow reduces the value space.