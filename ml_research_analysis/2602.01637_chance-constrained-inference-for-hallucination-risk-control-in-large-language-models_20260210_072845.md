---
ver: rpa2
title: Chance-Constrained Inference for Hallucination Risk Control in Large Language
  Models
arxiv_id: '2602.01637'
source_url: https://arxiv.org/abs/2602.01637
tags:
- risk
- inference
- violation
- feasibility
- inputs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces chance-constrained inference (CCI) for controlling
  hallucination frequency in large language models under stochastic decoding. The
  key insight is to treat hallucination as a stochastic event and enforce probabilistic
  bounds on violation frequency among accepted generations, rather than relying on
  heuristic confidence thresholds.
---

# Chance-Constrained Inference for Hallucination Risk Control in Large Language Models

## Quick Facts
- arXiv ID: 2602.01637
- Source URL: https://arxiv.org/abs/2602.01637
- Authors: Sreenivasan Mohandas
- Reference count: 8
- Primary result: CCI controls hallucination frequency with probabilistic guarantees under stochastic decoding, outperforming confidence-based baselines.

## Executive Summary
This paper introduces chance-constrained inference (CCI) for controlling hallucination frequency in large language models under stochastic decoding. The key insight is to treat hallucination as a stochastic event and enforce probabilistic bounds on violation frequency among accepted generations, rather than relying on heuristic confidence thresholds. The method uses sequential, anytime-valid inference with finite-sample guarantees to adaptively certify feasibility or infeasibility of the chance constraint for each input. Experiments demonstrate that CCI reliably controls hallucination risk across inputs of varying difficulty while confidence-based baselines fail to provide consistent guarantees. A major advantage is early detection of intrinsically infeasible inputs and safe composition under repeated use, with outputs guaranteed to satisfy the prescribed risk budget. The framework is complementary to existing hallucination mitigation strategies and provides a principled foundation for deployment-time reliability control in stochastic generative models.

## Method Summary
The method treats hallucination as a stochastic event under random decoding and enforces probabilistic bounds on violation frequency among accepted generations. It uses sequential, anytime-valid inference with finite-sample guarantees to adaptively certify feasibility or infeasibility of chance constraints for each input. The algorithm draws i.i.d. stochastic generations and evaluates a binary violation indicator, maintaining an empirical violation rate with a time-uniform confidence radius derived from stitched Hoeffding bounds. Sampling terminates adaptively when either the certified feasibility or infeasibility condition is met, or when a maximum budget is exhausted. This procedure guarantees that, with probability at least 1-δ, Feasible-certified inputs satisfy the true violation probability bound and Infeasible-certified inputs have provably violated constraints, regardless of the data-dependent stopping time.

## Key Results
- CCI reliably controls hallucination risk across inputs of varying difficulty while confidence-based baselines fail to provide consistent guarantees
- The method provides early detection of intrinsically infeasible inputs and safe composition under repeated use
- Outputs are guaranteed to satisfy the prescribed risk budget, with zero empirical risk on accepted outputs by construction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential, anytime-valid inference certifies feasibility or infeasibility of chance constraints under stochastic decoding with finite-sample guarantees.
- Mechanism: For each input, the algorithm draws i.i.d. stochastic generations $y_i \sim p_\theta(\cdot|x)$ and evaluates a binary violation indicator $H(x,y_i) \in \{0,1\}$. It maintains an empirical violation rate $\hat{R}_n(x) = \frac{1}{n}\sum_{i=1}^n H(x,y_i)$ and a time-uniform confidence radius $r_n = \sqrt{\log(2\log_2(2n)/\delta)/(2n)}$ derived from a stitched Hoeffding bound. Sampling terminates adaptively when either $\hat{R}_n(x) + r_n \le \epsilon(x)$ (certified Feasible) or $\hat{R}_n(x) - r_n > \epsilon(x)$ (certified Infeasible), or when a maximum budget $N_{\max}$ is exhausted (Undecided). This procedure guarantees that, with probability at least $1-\delta$, Feasible-certified inputs satisfy the true violation probability $R(x) \le \epsilon(x)$ and Infeasible-certified inputs have $R(x) > \epsilon(x)$, regardless of the data-dependent stopping time.
- Core assumption: Conditional independence of stochastic generations given the input; violations are Bernoulli-distributed events.
- Evidence anchors:
  - [abstract]: "We propose a sequential, anytime-valid inference procedure that adaptively certifies feasibility or infeasibility using finite samples, avoiding conservative fixed-sample bounds."
  - [section 7, Theorem 1]: "With probability at least $1-\delta$: 1. If the algorithm returns Feasible, then $P_{y \sim p_\theta(\cdot|x)}[H(x,y) = 1] \le \epsilon(x)$."
  - [corpus]: Related work (e.g., Token-Guard) addresses token-level control but lacks anytime-valid guarantees; direct corpus evidence for this mechanism is weak and primarily from the paper itself.
- Break condition: If generations are coupled (e.g., deterministic decoding, shared KV caches) or the Bernoulli assumption fails, the finite-sample guarantees may be invalidated.

### Mechanism 2
- Claim: A three-way input partition (Feasible/Infeasible/Undecided) enables principled abstention that reflects intrinsic model-input mismatch rather than conservative failure.
- Mechanism: The algorithm outputs one of three decisions. Feasible inputs return an output to the user; Infeasible or Undecided inputs lead to abstention. The feasibility gap $\Delta(x) = R(x) - \epsilon(x)$ governs behavior: strongly feasible ($\Delta \ll 0$) or strongly infeasible ($\Delta \gg 0$) inputs terminate quickly, while boundary cases ($\Delta \approx 0$) require more samples or may remain undecided. Abstention on infeasible inputs is informative (indicating intrinsic unsuitability), not merely a fallback.
- Core assumption: The risk budget $\epsilon(x)$ appropriately encodes tolerable hallucination frequency; the violation indicator $H(x,y)$ is a meaningful proxy for hallucinations.
- Evidence anchors:
  - [section 5]: "Inference induces a three-way partition of inputs: feasible inputs, for which the constraint is certified to hold; infeasible inputs, for which the constraint is provably violated; and undecided inputs, for which neither conclusion can be reached within the sampling budget. Abstention on infeasible or undecided inputs is therefore a correct and informative outcome."
  - [section 10.1, Table 1]: Shows sharp partition behavior—easy inputs all Feasible, hard all Infeasible, medium split—with adaptive sample counts (6.1–14.7 on average).
  - [corpus]: Corpus papers discuss hallucination detection but lack evidence for three-way partitions with formal guarantees; evidence is primarily from this paper.
- Break condition: If the violation indicator is incomplete or misaligned with true hallucinations, the partition may misclassify inputs.

### Mechanism 3
- Claim: Confidence-based selective prediction fails to enforce conditional chance constraints, while CCI provides explicit probabilistic bounds on violation frequency among accepted outputs.
- Mechanism: Baselines like Conf-SP or SC-SP accept outputs based on heuristic confidence or self-consistency, without guaranteeing $P(H=1|A=1) \le \epsilon$. In contrast, CCI enforces this conditional constraint by construction: an output is returned only after Feasible certification. Empirically, baselines accept nearly all inputs but incur violation rates of 0.28–0.92 on medium/hard inputs, whereas CCI's accepted outputs have zero empirical risk by design.
- Core assumption: The automatic verifier reliably approximates hallucinations; experimental conditions (GROQ LLaMA-3.3-70B, specific datasets) are representative.
- Evidence anchors:
  - [abstract]: "Confidence-based selective prediction does not, in general, imply probabilistic risk guarantees."
  - [section 10.2, Table 2]: "While heuristic baselines accept nearly all inputs regardless of intrinsic difficulty, they incur substantial violation rates on hard inputs. In contrast, CCI abstains whenever feasibility cannot be certified, yielding zero empirical risk on accepted outputs by construction."
  - [corpus]: Corpus lacks direct comparative evidence; Token-Guard addresses decoding-based mitigation but without formal risk guarantees.
- Break condition: If the verifier is unreliable or model/decoding setups differ substantially, the comparative advantage may diminish.

## Foundational Learning

- Concept: Chance Constraints
  - Why needed here: Core mathematical formulation—bounding $P(H(x,y)=1|A(x,y)=1) \le \epsilon(x)$—defines the problem and solution criteria.
  - Quick check question: If $R(x)=0.2$ and $\epsilon(x)=0.1$, is input $x$ feasible? (No; $0.2 > 0.1$, so infeasible.)

- Concept: Anytime-Valid Confidence Sequences
  - Why needed here: Sequential algorithm relies on time-uniform bounds valid under adaptive stopping; standard fixed-sample intervals would be invalid if stopped early.
  - Quick check question: Why does repeated sampling with early stopping based on a standard 95% CI inflate error rates? (Optional stopping violates fixed-sample assumptions; anytime-valid sequences preserve coverage under any stopping rule.)

- Concept: Stochastic Decoding in LLMs
  - Why needed here: Generation is treated as random draw $y \sim p_\theta(\cdot|x)$; hallucination is a stochastic event over this distribution, not a deterministic property of a single output.
  - Quick check question: If 10 samples for a fixed input are all correct, can you conclude the model never hallucinates on this input? (No—finite samples cannot prove $R(x)=0$; feasibility certification requires sufficient samples with confidence bounds.)

## Architecture Onboarding

- Component map: Input x -> Stochastic Generator (LLM with sampling) -> Violation Evaluator (computes H(x,y)) -> Sequential Feasibility Certifier (maintains $\hat{R}_n$, $r_n$, checks stopping conditions) -> Output Policy (return output if Feasible, else abstain)

- Critical path: Sequential Feasibility Certifier—must correctly implement anytime-valid bounds, accumulate violations, enforce three-way decision. Violation Evaluator accuracy is deployment-critical.

- Design tradeoffs:
  - Sample efficiency vs. certainty: Lower δ or small |Δ(x)| requires more samples; $N_{\max}$ trades latency vs. Undecided rate
  - Verifier fidelity: Guarantees hold on proxy hallucinations; true risk control requires reliable H(x,y)
  - Independence assumptions: May need temperature sampling, cache resets, or randomized prompts to ensure i.i.d. generations

- Failure signatures:
  - High Undecided rate: Inputs near feasibility boundary or overly strict ε(x)
  - Infeasible on expected-easy inputs: Verifier too strict or model systematically unreliable
  - Baseline-like acceptance: $N_{\max}$ too low or δ too high, compromising guarantees

- First 3 experiments:
  1. Replicate Table 1 on a controlled subset of NaturalQuestions with an off-the-shelf verifier. Vary ε(x) and $N_{\max}$ to observe changes in partition rates and sample efficiency.
  2. Ablate anytime-valid bounds: Replace with fixed-sample Hoeffding intervals and demonstrate that early stopping (without proper adjustment) breaks guarantees.
  3. Stress-test with noisy verifier: Flip H(x,y) with probability $p_{\text{noise}}$ and measure degradation in conditional risk control; verifies robustness to imperfect detection.

## Open Questions the Paper Calls Out
None

## Limitations

- The primary uncertainty lies in the fidelity of the automatic verifier used to compute the violation indicator H(x,y), which could compromise the validity of feasibility certificates if imperfect
- The assumption of i.i.d. stochastic generations may be violated in practice due to KV cache reuse or deterministic decoding seeds, potentially invalidating anytime-valid confidence sequences
- The compositionality claim for repeated use is stated but not rigorously validated under iterative generation scenarios
- Generalization to other domains, models, or decoding strategies beyond the specific experimental setup remains untested

## Confidence

**High Confidence:**
- The sequential, anytime-valid inference framework correctly implements finite-sample guarantees under stated assumptions
- The conditional chance constraint $P(H=1|A=1) \le \epsilon(x)$ is provably enforced by construction for Feasible-certified outputs
- The three-way input partition (Feasible/Infeasible/Undecided) is a valid consequence of the sequential testing procedure

**Medium Confidence:**
- The comparative advantage over confidence-based baselines holds under the experimental conditions reported
- The adaptive sample efficiency (6.1–14.7 samples on average) is representative of typical input difficulty distributions
- The abstention behavior on infeasible inputs is genuinely informative rather than a conservative fallback

**Low Confidence:**
- The method's robustness to imperfect verifiers and its impact on conditional risk guarantees
- The validity of the i.i.d. assumption under practical decoding configurations without explicit cache clearing
- The compositionality of guarantees under repeated or multi-turn generation scenarios

## Next Checks

1. **Verifier Robustness Test:** Introduce controlled noise into the violation indicator H(x,y) (e.g., flip bits with probability $p_{\text{noise}} = 0.05, 0.10$) and measure degradation in the conditional risk guarantee $P(H=1|A=1)$ for Feasible-certified outputs. This validates whether the anytime-valid bounds are robust to imperfect detection.

2. **Independence Assumption Stress Test:** Run the same input x under two conditions: (a) standard decoding with KV cache reuse, and (b) explicit cache clearing and re-initialization between samples. Compare the variance in $\hat{R}_n(x)$ and the rate of early stopping to detect coupling effects that could invalidate the i.i.d. assumption.

3. **Cross-Domain Generalization:** Apply CCI to a non-factoid QA dataset (e.g., MedQA for medical reasoning) and a long-form generation task (e.g., summarization of multi-document inputs). Evaluate whether the three-way partition behavior and sample efficiency generalize beyond NaturalQuestions, and whether the abstention rate remains informative.