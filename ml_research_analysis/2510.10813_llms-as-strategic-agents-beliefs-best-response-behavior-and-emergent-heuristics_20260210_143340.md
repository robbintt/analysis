---
ver: rpa2
title: 'LLMs as Strategic Agents: Beliefs, Best Response Behavior, and Emergent Heuristics'
arxiv_id: '2510.10813'
source_url: https://arxiv.org/abs/2510.10813
tags:
- reasoning
- llms
- strategic
- behavior
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# LLMs as Strategic Agents: Beliefs, Best Response Behavior, and Emergent Heuristics

## Quick Facts
- arXiv ID: 2510.10813
- Source URL: https://arxiv.org/abs/2510.10813
- Reference count: 40
- Primary result: Frontier LLMs can exhibit belief-coherent best-response behavior at targeted reasoning depths, with emergent meta-reasoning about opponent types.

## Executive Summary
This paper systematically evaluates strategic reasoning in frontier language models across three canonical games: Beauty Contest Game, 11-20 Money Request Game, and Unlabeled Matrix Game. The authors demonstrate that when provided with explicit conjectures about opponents' reasoning depth, LLMs can derive logical implications and select payoff-maximizing actions. Without explicit priors, models autonomously form differentiated conjectures about human versus synthetic opponents and self-limit their reasoning depth to L3-L4. Under increasing complexity, models transition from explicit recursive reasoning to equilibrium-seeking behavior or stable, model-specific heuristics distinct from known human biases.

## Method Summary
The authors evaluate 10 language models (including OpenAI o3, Claude 3.7 Sonnet, and DeepSeek R1) across three complete-information static games using structured prompts with temperature t=0.25. They conduct experiments varying exogenous opponent depth priors (k=0-10), opponent types (Human, LLM, Expert, Yourself), and game complexity. Choice accuracy is measured via best-response regret (BRR) and best-response accuracy metrics. The study extracts reasoning traces to analyze belief formation, depth estimation, and heuristic emergence, employing keyword analysis and distributional distance metrics to quantify strategic behavior patterns.

## Key Results
- Frontier models achieve belief-coherent best responses when given explicit opponent depth priors, with accuracy varying by game complexity and model size
- Unconstrained models self-limit reasoning depth (typically L3-L4) and differentiate conjectures about human versus synthetic opponents
- Under complexity (MRG), models shift from explicit recursion to equilibrium-seeking behavior or stable heuristics, with distinct patterns from human strategic biases

## Why This Works (Mechanism)

### Mechanism 1: Belief-Coherent Best Response at Targeted Depths
When provided with explicit conjectures about opponents' reasoning depth, frontier LLMs can derive logical implications and select payoff-maximizing actions. The mechanism relies on level-k recursion where models simulate belief hierarchies and compute expected payoffs conditional on assigned opponent depth. Evidence shows performance separation between reasoning and non-reasoning models across BCG, MRG, and UMG, though arithmetic errors in numeric games can impair performance.

### Mechanism 2: Self-Constrained Meta-Reasoning with Opponent-Type Differentiation
Without explicit priors, LLMs autonomously form conjectures about opponents, self-limit reasoning depth to L3-L4, and condition depth on opponent identity. Models access latent priors about human and synthetic agent behavior from training data, selecting depths that best-respond to inferred opponent types. Evidence shows modal depth varies systematically by opponent type, with models explicitly reasoning about expected opponent behavior based on training data patterns.

### Mechanism 3: Complexity-Induced Shift to Equilibrium Logic and Emergent Heuristics
As strategic complexity increases, models transition from explicit recursive reasoning to equilibrium-seeking behavior or stable, model-specific heuristic rules. Under complexity, explicit recursion becomes computationally expensive; models substitute equilibrium concepts or focal-point heuristics. Evidence shows keyword correlations with equilibrium-support choices and documented heuristic rationales like boundary-value selection, with patterns distinct from known human biases.

## Foundational Learning

- **Concept: Level-k and Cognitive Hierarchy Theory** - Core framework for measuring strategic depth; all experiments operationalize reasoning via level-k targets. Quick check: In a game where L0 randomizes uniformly, what action does L1 select if the payoff-maximizing response to uniform play is action A?

- **Concept: Best Response vs. Nash Equilibrium** - The paper distinguishes coherent strategic thinking (best-responding to held beliefs) from equilibrium imitation; unilaterally playing Nash is not always a best response. Quick check: If your opponent plays a non-equilibrium strategy, is playing your Nash-equilibrium action guaranteed to maximize your expected payoff?

- **Concept: Mixed-Strategy Equilibrium and Strategic Randomization** - MRG results show models fail to implement probabilistic mixing even when equilibrium requires it. Quick check: In a game with mixed-equilibrium support {15, 16, ..., 20}, how should a rational player distribute their choice probability across these actions?

## Architecture Onboarding

- **Component map:** Prompt construction (game rules + opponent specification + structured output format) -> Model inference (reasoning trace generation + final choice emission) -> Trace parsing (extract declared reasoning steps, depth, keywords) -> Choice extraction (parse `<response>` tags or structured JSON) -> Belief inference (map choices to implied level-k depth or τ parameter) -> Best-response verification (compute BRR; check equilibrium-support coverage)

- **Critical path:** Game specification clarity → model correctly understands payoff structure → Opponent-type framing → model forms appropriate conjectures → Trace faithfulness → reasoning steps reflect actual computation → Choice–trace alignment → final action is consistent with declared reasoning

- **Design tradeoffs:** Temperature: Lower (0.25) increases choice consistency; higher (0.75) does not fix probabilistic-strategy implementation; Structured vs. open-ended prompts: Structured formats aid parsing but may constrain reasoning expression; With-trace vs. no-trace: Traces provide belief-formation evidence but introduce faithfulness concerns

- **Failure signatures:** Calculation errors: Small models fail BCG numeric computations; Deterministic collapse of mixed strategies: Models concentrate on 1–2 focal actions instead of distributing across equilibrium support; Overthinking: Some models take more steps than their final choice implies; Trace–choice inconsistency: Declared reasoning depth does not match choice-implied depth

- **First 3 experiments:** 1. Validate targeted best-response capacity: Run BCG, MRG, UMG with exogenous level-k priors (k=1,2,3,5,10); measure BRR accuracy and identify arithmetic-failure modes. 2. Characterize self-constrained depth and opponent differentiation: Run open-ended BCG with opponent types (Human, LLM, Expert, Yourself); infer modal depth and τ from choice distributions; analyze traces for meta-reasoning language. 3. Test heuristic emergence under complexity: Compare MRG (non-convergent) vs. BCG (convergent) choice distributions and trace keywords; quantify equilibrium-support coverage and focal-point concentration.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do emergent strategic heuristics generalize to unstructured environments lacking formal payoff matrices? The authors note that future research is needed to analyze this ability when situations are unstructured and lack formal instructions on gameplay.

- **Open Question 2:** Do LLMs exhibit context-dependent variation in reasoning depth similar to humans? The authors state that whether LLMs display similar context-dependent variation in their inferred reasoning depth represents an important question for future research.

- **Open Question 3:** Is the observed "overthinking" (excess reasoning steps) a beneficial feature for error correction or an unnecessary inefficiency? The authors note that whether this type of overthinking is a feature more than a bug remains unexplored.

## Limitations

- Arithmetic precision issues in BCG suggest fundamental computational constraints that may extend to more complex strategic reasoning
- Focus on complete information games limits generalizability to real-world scenarios where information asymmetry is common
- Trace-faithfulness concerns remain partially unresolved despite controlled experiments

## Confidence

- **High confidence**: Models can achieve accurate best responses when given explicit depth priors (Mechanism 1)
- **Medium confidence**: Models autonomously form differentiated conjectures about opponent types and self-limit reasoning depth (Mechanism 2)
- **Medium confidence**: Under complexity, models shift to equilibrium-seeking behavior and develop stable heuristics distinct from human biases (Mechanism 3)

## Next Checks

1. **Trace faithfulness validation**: Run a control experiment where models are given false payoff information in the prompt but must report the true payoffs in their trace. Compare whether final choices align with the prompt (cheating) or the trace (faithful reasoning).

2. **Cross-game generalization**: Test the same strategic reasoning models on incomplete information games (e.g., signaling games or auctions) to evaluate whether the observed depth-of-reasoning patterns transfer beyond complete information settings.

3. **Adversarial opponent design**: Create synthetic opponent types with highly unusual strategic profiles (e.g., cyclic best responses, non-myopic optimization) to test whether models' opponent differentiation mechanism breaks down when training data priors become unreliable.