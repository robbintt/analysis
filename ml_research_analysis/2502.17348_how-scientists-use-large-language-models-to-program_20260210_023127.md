---
ver: rpa2
title: How Scientists Use Large Language Models to Program
arxiv_id: '2502.17348'
source_url: https://arxiv.org/abs/2502.17348
tags:
- code
- programming
- https
- llms
- scientific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Scientists often lack training in software development practices,
  yet programming is critical for data analysis, simulations, and experiments across
  disciplines. While large language models (LLMs) that generate code have become widely
  available, little is known about how scientists use them or verify their outputs.
---

# How Scientists Use Large Language Models to Program
## Quick Facts
- arXiv ID: 2502.17348
- Source URL: https://arxiv.org/abs/2502.17348
- Reference count: 40
- Scientists use LLMs primarily through browser interfaces for syntax lookup and basic code generation, with limited verification capabilities

## Executive Summary
This study investigates how scientists at a research university adopt and use code-generating large language models (LLMs). Through a survey of 199 researchers and interviews with 14 users, the research reveals that scientists predominantly access LLMs through browser-based chat interfaces rather than integrated development environment (IDE) tools. Many scientists are multilingual programmers working in languages where they lack fluency, leading them to use LLMs primarily for looking up functions and syntax rather than generating complex code. The study finds that verification strategies rely heavily on visual inspection and line-by-line reading of code, with some researchers using LLM-generated explanations as reference. User logs revealed instances where errors were introduced and not detected, suggesting scientists may need better tools for understanding generated code and clearer interfaces that distinguish information retrieval from code generation.

## Method Summary
The study employed a mixed-methods approach combining surveys and interviews with scientists at a research university. Researchers distributed a survey to 199 participants to gather quantitative data on usage patterns, tool preferences, and verification strategies. Additionally, they conducted in-depth interviews with 14 LLM users to gain qualitative insights into their experiences, challenges, and approaches to code verification. The researchers also analyzed user logs to identify instances of error introduction and detection. The study focused on understanding how scientists with varying levels of programming expertise interact with code-generating LLMs and the strategies they employ to verify generated outputs.

## Key Results
- Scientists primarily access LLMs through browser-based chat interfaces rather than IDE-integrated tools
- Verification strategies rely heavily on visual inspection and line-by-line code reading
- User logs revealed instances where errors were introduced and not detected by researchers

## Why This Works (Mechanism)
Scientists use LLMs as a supplementary tool for programming tasks, particularly when working in languages where they lack fluency or need quick reference for syntax and functions. The browser-based interface approach allows for easy access without requiring integration into existing development workflows. The reliance on visual inspection and line-by-line reading reflects the limited technical capacity of many scientific users to perform more sophisticated code analysis. The introduction of undetected errors suggests a gap between the perceived and actual capabilities of LLMs for scientific programming tasks.

## Foundational Learning
- Browser-based LLM interfaces - why needed: Allow scientists to access LLMs without technical integration, quick check: Survey data on tool preferences
- Code verification strategies - why needed: Essential for ensuring accuracy of generated code, quick check: Interview responses about error detection
- Multilingual programming challenges - why needed: Many scientists work across different programming languages, quick check: Survey questions about language fluency

## Architecture Onboarding
Component map: Browser interface -> LLM model -> Code output -> Visual inspection -> Error detection
Critical path: User query → Browser chat → LLM response → User verification → Code implementation
Design tradeoffs: Browser convenience vs. IDE integration capabilities; simple verification vs. sophisticated analysis tools
Failure signatures: Undetected errors in user logs, reliance on visual inspection rather than automated testing
First experiments:
1. Compare error detection rates between visual inspection and automated testing approaches
2. Evaluate the effectiveness of LLM-generated explanations as verification aids
3. Test browser interface modifications to better distinguish information retrieval from code generation

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size of 14 interviewees may not represent the broader scientific community
- Potential selection bias in survey respondents who may be more familiar with LLMs
- Reliance on self-reported usage patterns and retrospective accounts may not accurately capture actual coding behaviors

## Confidence
- Usage patterns and tool preferences: Medium confidence - based on survey data but may not represent the broader scientific community
- Verification strategies: Low to Medium confidence - primarily derived from interviews, which may not reflect actual practice
- Error introduction and detection: Low confidence - based on limited user log analysis and self-reported incidents

## Next Checks
1. Conduct a larger-scale study with diverse scientific disciplines and experience levels to validate the reported usage patterns and verification strategies
2. Implement a controlled experiment where scientists attempt to complete coding tasks with and without LLM assistance, measuring both success rates and error introduction
3. Develop and test prototype tools that provide better visualization and explanation of generated code, then evaluate their impact on error detection and understanding among scientific users