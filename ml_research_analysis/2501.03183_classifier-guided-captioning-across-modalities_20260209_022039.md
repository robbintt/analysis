---
ver: rpa2
title: Classifier-Guided Captioning Across Modalities
arxiv_id: '2501.03183'
source_url: https://arxiv.org/abs/2501.03183
tags:
- captioning
- audio
- classifier
- captions
- audibility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a classifier-guided captioning framework that
  adapts language models to modality-specific semantics, such as distinguishing audible
  vs. non-audible elements in audio captioning.
---

# Classifier-Guided Captioning Across Modalities

## Quick Facts
- arXiv ID: 2501.03183
- Source URL: https://arxiv.org/abs/2501.03183
- Reference count: 36
- Primary result: Introduces inference-time context cache optimization for modality-specific caption generation, achieving state-of-the-art zero-shot audio captioning results

## Executive Summary
This paper presents a classifier-guided captioning framework that adapts language models to modality-specific semantics during inference without retraining. The method uses a frozen language model and a binary classifier trained on GPT-4-generated data to guide caption generation via context cache optimization. Evaluated on audio and image captioning tasks, the framework improves performance across standard metrics and sets state-of-the-art results in zero-shot audio captioning, notably boosting audibility accuracy from 59.8% to 78.2% on AudioCaps.

## Method Summary
The framework trains a binary classifier (DistilBERT) on 10k GPT-4-generated captions to distinguish modality-specific elements (e.g., audible vs. non-audible). During inference, it optimizes the transformer context cache (keys and values) using gradient descent on a combined loss: classifier guidance (λ1=0.6) plus cross-entropy regularization (λ0=0.2) to maintain fluency. This inference-time optimization shifts token selection toward modality-appropriate semantics without modifying model weights. The approach is modality-agnostic and operates solely during inference, eliminating the need for further training of the underlying captioning model.

## Key Results
- Audibility Accuracy improved from 59.8% to 78.2% on AudioCaps
- BERT-S scores increased from 0.80 to 0.88 in zero-shot audio captioning
- CLIP-S scores improved from 0.87 to 0.89 in image captioning
- Outperforms baselines across BLEU4, METEOR, CIDEr, SPICE metrics

## Why This Works (Mechanism)

### Mechanism 1
Optimizing the context cache during inference shifts token selection toward modality-specific semantics without modifying model weights. The framework stores transformer keys (K) and values (V) in a context cache Ci. During each token generation step, gradient descent updates Ci using Lclassifier = −log(ha(LM(xi; Ci)[1])), pushing the LM toward classifier-preferred tokens. A regularization term LCE = CE(LM(xi; Ci), LM(xi; C°i)) constrains deviation from the original distribution. The combined loss L = λ0·LCE + λ1·Lclassifier (λ0=0.2, λ1=0.6) balances guidance with fluency. Core assumption: The context cache sufficiently encodes semantic biasing information; classifier gradients transfer meaningfully to token-level decisions.

### Mechanism 2
A GPT-4-generated binary dataset enables the classifier to learn modality-specific distinctions (e.g., audible vs. non-audible) that guide caption generation. The authors prompt GPT-4 to generate 10k captions (5k audible, 5k non-audible) with explicit instruction to consider "coherence, grammatical correctness, context, and likelihood of representing a meaningful auditory scenario." A DistilBERT classifier is trained on this dataset to output pseudo-probabilities for the audibility label, which then provides gradients during inference-time optimization. Core assumption: GPT-4's textual understanding of "audibility" aligns with human perceptual judgments about what should appear in audio captions; classifier generalizes to real captioning outputs.

### Mechanism 3
Cross-entropy regularization preserves language model fluency while allowing semantic guidance to modify token selection. LCE computes cross-entropy between the guided LM distribution LM(xi; Ci) and the original LM distribution LM(xi; C°i), where C°i contains unmodified context cache values. This anchors generation to the pretrained LM's behavior while Lclassifier provides directional bias. The λ0=0.2 weight on LCE is lower than λ1=0.6 on Lclassifier, prioritizing semantic guidance. Core assumption: The original LM distribution encodes fluency and coherence that should be preserved; small deviations maintain grammaticality.

## Foundational Learning

- **Inference-Time Optimization**: Why needed here: The entire framework operates during inference without weight updates; understanding how gradients flow through frozen models via context caches is essential. Quick check question: Can you explain why backpropagating through a context cache differs from standard fine-tuning?

- **Classifier-Guided Decoding**: Why needed here: The core innovation uses a trained classifier to provide gradient signals that steer generation; this differs from reinforcement learning or prompt-based guidance. Quick check question: How does computing Lclassifier = −log(ha(LM(xi; Ci)[1])) differ from using the classifier as a reward model in RL?

- **Cross-Modal Transfer in Captioning**: Why needed here: The paper claims modality-agnostic applicability; understanding why audio and image captioning share learnable structures clarifies generalization potential. Quick check question: What shared properties between audio and image captioning enable a single framework to improve both?

## Architecture Onboarding

- Component map: Input (Audio/Image) -> Frozen GPT-2 LM -> Context Cache (K,V) -> DistilBERT Classifier -> Gradient Optimizer -> Output Token
- Critical path:
  1. Encode input (audio/image) → LM generates initial token predictions with context cache C°i
  2. For each token position i: compute Lclassifier using classifier output on partial sentence; compute LCE against original distribution
  3. Optimize Ci via gradient descent for n steps (paper doesn't specify n; experiments suggest 512 token candidates evaluated per position)
  4. Select next token from optimized distribution; repeat until sequence length (30 tokens) or EOS
- Design tradeoffs:
  - **Guidance strength (λ1) vs. fluency (λ0)**: Paper uses λ0=0.2, λ1=0.6; higher λ1 improves audibility but risks incoherence
  - **Compute budget per token**: 512 candidates in ~2 seconds; increasing candidates improves selection but linearly increases latency
  - **Classifier dataset size**: 10k synthetic examples; larger datasets may improve classifier accuracy but require more GPT-4 API calls
- Failure signatures:
  - **Repetitive outputs**: May indicate λ1 too high or classifier overfitting to specific phrases
  - **Grammatical errors**: Suggests LCE weight too low or optimization steps insufficient
  - **No improvement over baseline**: Check classifier accuracy independently; if <60%, synthetic data may be misaligned with target domain
  - **Excessive latency (>5 sec/token)**: Reduce candidate tokens or optimization steps
- First 3 experiments:
  1. **Classifier validation**: Train classifier on GPT-4 data; evaluate held-out accuracy. Target: >85% on binary classification before integrating with LM.
  2. **Hyperparameter sweep**: Test λ0 ∈ {0.1, 0.2, 0.3} and λ1 ∈ {0.4, 0.6, 0.8} on small validation set (100 samples). Monitor BLEU4 and Audibility Accuracy.
  3. **Modality transfer test**: Apply audio-trained classifier to image captioning (or vice versa) without retraining classifier. Expect: degraded but non-zero improvement, validating modality-agnostic claim requires classifier retraining per modality.

## Open Questions the Paper Calls Out
- **Real-time applicability**: The framework requires 2 seconds per token with 512 candidates, making it unsuitable for real-time applications. Future work will explore methods to reduce this overhead.
- **Video captioning extension**: The framework currently validates only on static images and audio clips. Future work will explore applying this framework to additional modalities, such as video.
- **Bias analysis**: The paper notes that relying on pre-trained models like GPT-2 may introduce biases that affect performance and generalization, but does not conduct a bias audit of the generated captions.

## Limitations
- Computational overhead during inference: 2 seconds per token with 512 candidates, representing a 100-1000x slowdown compared to standard beam search.
- Classifier generalization from synthetic data: The audibility classifier is trained entirely on GPT-4-generated captions, which may not capture the full distribution of human-generated captions.
- Modality-agnostic claim verification: The classifier is trained separately for each modality rather than demonstrating true cross-modal transfer.

## Confidence
- **High confidence**: The core mechanism of context cache optimization during inference is technically sound and well-supported by mathematical formulation and consistent metric improvements.
- **Medium confidence**: The effectiveness of GPT-4-generated synthetic data for training modality-specific classifiers, though results show improvement, lacks classifier accuracy reporting on real data.
- **Low confidence**: The scalability and practical applicability of the method due to computational requirements and reliance on GPT-4 API calls for dataset generation.

## Next Checks
1. **Classifier validation on real data**: Train the audibility classifier on GPT-4 data, then evaluate its accuracy on a held-out test set from AudioCaps and Clotho. Target: >85% accuracy on synthetic test data, >70% on real caption data. If classifier accuracy on real data falls below 70%, augment the training dataset with human-annotated examples.

2. **Minimal optimization step determination**: Systematically vary the number of optimization steps per token (n ∈ {1, 5, 10, 20}) and measure the tradeoff between performance improvement and computational cost. Identify the minimal n that achieves 90% of the full 512-candidate performance to reduce inference time significantly.

3. **Cross-modal classifier transfer test**: Train an audibility classifier and attempt to apply it to image captioning without modification, then train an image-specific classifier and apply it to audio captioning. Measure the performance degradation compared to modality-specific classifiers. Target: <50% performance retention when transferring classifiers across modalities.