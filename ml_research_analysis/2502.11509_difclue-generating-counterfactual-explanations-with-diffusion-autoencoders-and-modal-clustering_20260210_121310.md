---
ver: rpa2
title: 'DifCluE: Generating Counterfactual Explanations with Diffusion Autoencoders
  and modal clustering'
arxiv_id: '2502.11509'
source_url: https://arxiv.org/abs/2502.11509
tags:
- counterfactual
- explanations
- class
- difclue
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DifCluE, a novel approach for generating
  multiple counterfactual explanations for different modes within a single class.
  The method combines a Diffusion Autoencoder with modal clustering in latent space
  to uncover and explain distinct sub-patterns within classes.
---

# DifCluE: Generating Counterfactual Explanations with Diffusion Autoencoders and modal clustering

## Quick Facts
- arXiv ID: 2502.11509
- Source URL: https://arxiv.org/abs/2502.11509
- Reference count: 34
- Generates multiple counterfactual explanations for classes with multiple modes, achieving 83% mode alignment accuracy vs 53% for DISSECT

## Executive Summary
This paper introduces DifCluE, a novel approach for generating multiple distinct counterfactual explanations for classes with multiple modes (subclasses sharing a single label). The method combines a Diffusion Autoencoder with modal clustering in latent space to uncover and explain distinct sub-patterns within classes. DifCluE demonstrates superior performance compared to the state-of-the-art DISSECT model, achieving 83% accuracy in aligning generated explanations with actual mixed classes versus 53% for DISSECT, while also showing better realism (FID 9.26 vs 11.0), distinctness, substitutability, and importance metrics.

## Method Summary
DifCluE works by first training a Diffusion Autoencoder on FFHQ to extract semantic encodings from images. K-means clustering is then applied to these semantic encodings to identify distinct modes within classes. A linear classifier is trained to predict cluster labels from the semantic latent space, and perturbation directions are extracted from the classifier weights. These directions are used to generate counterfactual explanations by perturbing the latent space and decoding back to image space. The approach is evaluated on CelebA with intentionally mixed classes (e.g., "blonde hair" and "bangs hairstyle") to test mode disentanglement.

## Key Results
- 83% accuracy in aligning generated explanations with actual mixed classes vs 53% for DISSECT
- Better realism (FID score of 9.26 vs 11.0 for DISSECT)
- Superior performance across multiple evaluation metrics: distinctness, substitutability, and importance
- Computational advantages by not requiring retraining when adjusting the number of counterfactuals

## Why This Works (Mechanism)
DifCluE leverages the Diffusion Autoencoder's ability to capture semantic information in a disentangled latent space, where K-means clustering can identify distinct modes within classes. The linear classifier provides interpretable perturbation directions that correspond to these modes, enabling the generation of multiple distinct counterfactuals for the same class. This approach effectively separates the mixed classes into their constituent modes, allowing for more precise and interpretable explanations.

## Foundational Learning

**Diffusion Autoencoder**: A generative model that learns to reconstruct images through a diffusion process. Needed for capturing rich semantic information in the latent space. Quick check: Verify the model can generate realistic images from random latent vectors.

**Modal Clustering**: Clustering technique that identifies distinct modes or sub-patterns within data distributions. Needed for discovering the multiple modes within a single class label. Quick check: Evaluate cluster purity against known attribute labels.

**Latent Space Perturbation**: The process of modifying latent representations to generate counterfactuals. Needed for creating explanations that differ from the original in semantically meaningful ways. Quick check: Plot class probability changes as a function of perturbation magnitude.

## Architecture Onboarding

**Component Map**: Diffusion Autoencoder -> K-means Clustering -> Linear Classifier -> Latent Perturbation -> Decoder

**Critical Path**: Image → Diffusion Autoencoder → Semantic Latent Space → K-means Clustering → Linear Classifier → Perturbation → Decoder → Counterfactual Image

**Design Tradeoffs**: Linear classifier provides interpretability and computational efficiency but may limit performance on non-linearly separable modes. The method trades off some potential accuracy for explainability and simplicity.

**Failure Signatures**: Poor mode separation (clusters don't align with actual mixed classes), unrealistic counterfactuals (incorrect perturbation scaling), and mode collapse (all counterfactuals look similar).

**First Experiments**:
1. Train Diffusion Autoencoder on FFHQ and verify it can generate realistic images
2. Apply K-means clustering to semantic encodings and evaluate cluster purity against known labels
3. Generate counterfactuals with varying perturbation magnitudes and plot class probability changes

## Open Questions the Paper Calls Out

**Open Question 1**: How effectively does DifCluE generalize to non-image domains, specifically time-series data? The conclusion states plans to extend to other domains, but current evaluation is exclusively on image data.

**Open Question 2**: Can the methodology automatically determine the optimal number of modes (clusters) K for a class without prior knowledge? The current approach relies on known ground truth for the number of modes during evaluation.

**Open Question 3**: Does the use of a linear classifier to determine perturbation directions impose limitations on datasets with non-linearly separable modes? While linear classifiers simplify extraction of perturbation directions, they assume linear separability which may not hold for complex class structures.

## Limitations

- Diffusion Autoencoder architecture and training hyperparameters are only referenced externally without specific details
- Perturbation scaling factor α lacks concrete values or ranges
- Handling of diffusion timesteps and relationship between z_sem and x_T during decoding remains underspecified
- Reliance on known ground truth for number of modes during evaluation

## Confidence

**High confidence** in the core methodology and overall experimental design. The approach is technically sound and evaluation metrics are appropriate.

**Medium confidence** in the quantitative results, which are well-documented and show clear improvements over DISSECT. Some metric implementations may vary with different attribute classifiers.

**Low confidence** in exact reproducibility due to missing implementation details, particularly around Diffusion Autoencoder architecture, perturbation scaling, and cluster initialization strategies.

## Next Checks

1. Verify mode separation quality by running K-means clustering on z_sem and checking cluster purity against known attribute labels, using silhouette scores to quantify separation quality.

2. Perform sensitivity analysis on perturbation scaling factor α by generating counterfactuals across multiple α values and plotting class probability changes to ensure the Importance metric shows consistent positive correlation.

3. Validate counterfactual realism by conducting human evaluation studies comparing DifCluE-generated images against DISSECT outputs for the same attribute pairs, measuring both visual quality and attribute correctness.