---
ver: rpa2
title: Multi-Agent Multimodal Models for Multicultural Text to Image Generation
arxiv_id: '2502.15972'
source_url: https://arxiv.org/abs/2502.15972
tags:
- image
- across
- agent
- country
- person
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MosAIG, a multi-agent framework for multicultural
  text-to-image generation that leverages LLM interactions with distinct cultural
  personas to improve representation of diverse demographics. The framework addresses
  the Western-centric bias in existing datasets by generating culturally nuanced image
  captions and creating a novel dataset of 9,000 multicultural images spanning five
  countries, three age groups, two genders, 25 landmarks, and five languages.
---

# Multi-Agent Multimodal Models for Multicultural Text to Image Generation

## Quick Facts
- **arXiv ID**: 2502.15972
- **Source URL**: https://arxiv.org/abs/2502.15972
- **Reference count**: 40
- **Primary result**: Multi-agent models significantly outperform simple models across Alignment (0.31 vs 0.20), Aesthetics (0.59 vs 0.58), Quality (0.77 vs 0.48), and Knowledge (0.43 vs 0.42) metrics

## Executive Summary
This paper introduces MosAIG, a multi-agent framework for multicultural text-to-image generation that addresses Western-centric bias in existing datasets by generating culturally nuanced image captions. The framework leverages LLM interactions with distinct cultural personas (Country, Landmark, Age-Gender agents) to produce culturally grounded visual descriptions, creating a novel dataset of 9,000 multicultural images spanning five countries, three age groups, two genders, 25 landmarks, and five languages. Multi-agent models significantly outperform simple models across multiple metrics, though fairness scores decrease due to richer descriptive captions that amplify CLIPScore differences between demographic groups.

## Method Summary
The MosAIG framework employs a five-agent architecture (Moderator + 3 Social Agents + Summarizer) built on CrewAI, using LLaMA-3.1-8B instances to generate culturally nuanced captions through two rounds of persona-based conversation. Demographic inputs are processed by the Moderator, which delegates tasks to specialized agents that generate domain-specific descriptions (Country Agent: attire/architecture; Landmark Agent: setting details; Age-Gender Agent: demographic-appropriate features). These descriptions are cross-validated and refined over two conversation rounds, then consolidated by the Summarizer into ≤77-token captions for image generation with either AltDiffusion (multilingual) or FLUX (English-only). The resulting images are evaluated using CLIPScore-based metrics (Alignment, Fairness, Knowledge), Inception Score (Quality), and SigLIP predictor (Aesthetics).

## Key Results
- Multi-agent models achieve significantly higher Alignment (0.31 vs 0.20) and Quality (0.77 vs 0.48) than simple models
- Fairness scores decline for multi-agent models due to increased caption detail amplifying demographic differences
- Non-English captions show substantially lower Alignment (Hindi: 0.14, Vietnamese: 0.17) compared to English (0.31)
- FLUX-M produces sharper images with fewer body distortions (5/75) than AltDiffusion (15/75)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent persona-based caption refinement improves text-to-image alignment and quality.
- Mechanism: Three specialized social agents (Country, Landmark, Age-Gender) assume distinct cultural personas and engage in two rounds of question-answering conversation. Each agent generates domain-specific descriptions that are cross-validated by peers—for example, the Age-Gender Agent evaluates whether the Country Agent's suggested attire is appropriate for the target demographic. This iterative refinement produces captions with culturally grounded visual details (clothing materials, architectural features) that simple templated prompts lack.
- Core assumption: LLaMA-3.1-8B can reliably simulate culturally specific knowledge when prompted with persona instructions, and this knowledge transfers effectively to image generation models.
- Evidence anchors:
  - [abstract] "multi-agent models significantly outperform simple models across Alignment (0.31 vs 0.20)...Quality (0.77 vs 0.48)"
  - [section 3.1] "After two rounds of conversation, the agents enhance and refine the descriptions with culturally sensitive and contextually rich details"
  - [corpus] Limited direct corpus support for this specific mechanism; related work "Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning" suggests multi-agent coordination benefits but uses different methodology (reinforcement learning vs. persona-based conversation)
- Break condition: If base LLM lacks sufficient cultural knowledge for underrepresented demographics, persona prompting may produce stereotyped or incorrect details (e.g., misattributed clothing patterns noted in error analysis).

### Mechanism 2
- Claim: Caption richness creates a trade-off between quality metrics and fairness scores.
- Mechanism: Multi-agent captions include specific descriptors (e.g., "Áo Dài," "bindi," "Dirndl") that improve visual fidelity but amplify CLIPScore differences when demographic terms are swapped. Simple captions ("A person at Golden Gate Bridge") contain fewer demographic markers, so substituting "German" for "Indian" produces smaller score variations—artificially better fairness scores despite poorer representation.
- Core assumption: The fairness metric (∆CLIPScore between demographic variants) captures meaningful bias rather than reflecting the model's appropriate sensitivity to distinct cultural presentations.
- Evidence anchors:
  - [section 4.2] "Fairness scores decline for multi-agent models. We attribute this to the increased level of detail in their generated captions—such as references to clothing, facial features, and hairstyles—which amplifies the absolute difference in CLIPScore"
  - [section 4.4] Error analysis shows multi-agent Flux-M reduces cultural element errors (2/75) vs. Flux-S while maintaining body distortion rates
  - [corpus] No corpus papers directly address this fairness-quality trade-off in multicultural image generation
- Break condition: If evaluation metrics reward demographic blindness over accurate cultural representation, optimizing for fairness scores may reduce cultural authenticity.

### Mechanism 3
- Claim: Multilingual caption performance correlates with training data availability.
- Mechanism: Alignment scores for non-English captions (Hindi: 0.14, Vietnamese: 0.17) substantially lag English (0.31). The paper reports moderate correlation (Pearson 0.5) between performance and estimated CommonCrawl dataset size per language. AltDiffusion's multilingual text encoder (XLM-R) may have weaker cross-modal alignment for lower-resource languages.
- Core assumption: The observed performance gap stems from training data imbalance rather than fundamental architectural limitations in multilingual diffusion models.
- Evidence anchors:
  - [section 4.3e] "English achieving the highest Alignment (0.31) and Knowledge (0.46), while Hindi and Vietnamese score the lowest (0.14 and 0.43, respectively)"
  - [section 4.3e] "model performance moderately correlates with dataset size (Pearson coefficient: 0.5), estimated from CommonCrawl"
  - [corpus] "Cultural Bias Matters" paper confirms cultural skew in training data leads to performance degradation but focuses on metaphors rather than image generation
- Break condition: If the correlation is spurious or other factors dominate (tokenization quality, cultural concept encoding), simply increasing training data may not close the gap.

## Foundational Learning

- Concept: Multi-agent orchestration patterns
  - Why needed here: Understanding CrewAI-style sequential task delegation, role assignment, and conversation round limits (2 rounds, 77-token summary cap) is essential for debugging caption quality.
  - Quick check question: Can you trace how the Moderator's task assignment flows through each Social Agent and terminates at the Summarizer?

- Concept: CLIP-based evaluation metrics
  - Why needed here: Alignment, Fairness, and Knowledge metrics all derive from CLIPScore cosine similarity. Understanding how text-image embeddings work explains why richer captions affect fairness scores.
  - Quick check question: Why would swapping "German boy" to "Indian boy" in a caption change the CLIPScore for the same image?

- Concept: Diffusion model text encoders
  - Why needed here: AltDiffusion (XLM-R encoder, 77-token limit, 18 languages) and FLUX (English-only, 8B transformer) have different multilingual capabilities that explain performance gaps.
  - Quick check question: What happens when you feed Hindi text to an English-trained text encoder in a diffusion pipeline?

## Architecture Onboarding

- Component map:
  Moderator Agent (function) -> Country/Landmark/Age-Gender Agents (LLaMA-3.1-8B) -> Summarizer Agent (LLaMA-3.1-8B) -> Image Generation (AltDiffusion/FLUX) -> CLIPScore Evaluation

- Critical path: Demographic input → Moderator task assignment → Round 1 (agents generate initial descriptions) → Round 2 (agents cross-validate and refine) → Summarizer produces final caption → AltDiffusion/FLUX generates image → Automated metrics + optional human evaluation

- Design tradeoffs:
  - Caption length vs. detail: 77-token limit ensures compatibility with AltDiffusion but may truncate cultural nuances
  - Agent count vs. latency: 5 agents with 2 conversation rounds processed 750 prompts in 45 minutes; adding agents or rounds scales linearly
  - English vs. multilingual: FLUX produces sharper images but requires English captions; AltDiffusion supports 18 languages but with lower alignment scores for non-English

- Failure signatures:
  - Stereotyped cultural representations (e.g., incorrect bindi placement, generic "traditional attire" without specificity)
  - Body distortions more common in AltDiffusion (15/75 images) than FLUX (5/75)
  - Landmark errors higher in simple models (15/75) vs. multi-agent (2/75)
  - Background inconsistencies: incorrect setting details despite correct subject rendering

- First 3 experiments:
  1. **Ablate conversation rounds**: Compare 1-round vs. 2-round agent interaction to quantify the marginal benefit of iterative refinement on alignment scores.
  2. **Cross-encoder evaluation**: Test whether a multilingual CLIP variant (e.g., AltCLIP) reduces the English/non-English alignment gap for the same generated images.
  3. **Fairness metric refinement**: Implement element-weighted CLIPScore that penalizes landmark errors more heavily than background details, addressing the paper's finding that current metrics reward correct gardens even with wrong landmarks.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can multi-agent frameworks be refined to maintain high alignment and quality while mitigating the decline in fairness scores caused by richer descriptive captions?
- **Basis in paper:** [explicit] The authors report that multi-agent models outperform simple models in alignment and quality but suffer lower fairness scores due to detailed descriptors amplifying CLIPScore differences between demographic groups.
- **Why unresolved:** The paper identifies this trade-off as a limitation of richer text generation but does not propose a specific mechanism to balance expressiveness with demographic consistency.
- **What evidence would resolve it:** A modified framework that normalizes caption complexity or uses fairness constraints during generation, resulting in high alignment without statistically significant differences in CLIPScore across demographics.

### Open Question 2
- **Question:** What specific architectural or training modifications are required to achieve parity in text-to-image generation performance between English and non-English prompts?
- **Basis in paper:** [explicit] The results show a distinct performance gap, with English captions achieving significantly higher alignment than non-English variants (e.g., Hindi, Vietnamese) in multilingual models.
- **Why unresolved:** The authors attribute this to training data availability but leave the development of stronger multilingual capabilities as a future direction.
- **What evidence would resolve it:** A model that demonstrates statistically similar alignment and knowledge scores for low-resource languages compared to English when using the MosAIG framework.

### Open Question 3
- **Question:** Can evaluation metrics that assign specific weights to key elements (such as landmarks) align more closely with human judgment than current global metrics?
- **Basis in paper:** [explicit] The authors note that current metrics like CLIPScore can be inflated by correct background elements (e.g., gardens) even when the main subject (e.g., the specific landmark) is missing or incorrect.
- **Why unresolved:** The study recommends refining alignment metrics to prioritize key elements but relies on standard, unweighted metrics for its final analysis.
- **What evidence would resolve it:** A new weighted evaluation protocol that correlates strongly with human error analysis specifically regarding landmark accuracy.

## Limitations

- Cultural representation scope remains narrow (only five countries across three continents, omitting major populations from Africa, Latin America, and Middle East)
- Evaluation framework relies heavily on automated metrics that may not capture culturally nuanced visual correctness
- Translation pipeline (Google Translation API) introduces potential fidelity loss for non-English captions without validation
- 77-token caption limit may truncate culturally specific descriptors, creating systematic bias toward simpler images
- Fairness metric conflates cultural specificity with bias, potentially rewarding demographic blindness over authentic representation

## Confidence

- **High Confidence** in core multi-agent framework effectiveness: 0.31 vs 0.20 Alignment improvement and 0.77 vs 0.48 Quality improvement demonstrate statistically significant performance gains
- **Medium Confidence** in fairness-quality trade-off claim: While paper correctly identifies richer captions amplify CLIPScore differences, interpretation remains debatable
- **Low Confidence** in multilingual performance correlation: Moderate Pearson correlation (0.5) provides suggestive but insufficient evidence for causal relationship

## Next Checks

1. **Ablate and compare conversation rounds**: Systematically test 1-round vs. 2-round agent interactions on a subset of prompts to quantify marginal benefit of iterative refinement and identify optimal conversation depth for different cultural contexts.

2. **Cross-encoder fairness evaluation**: Re-evaluate the same image set using a multilingual CLIP variant (e.g., AltCLIP) to determine whether English/non-English alignment gap stems from text encoder limitations or fundamental cross-modal alignment issues.

3. **Element-weighted CLIPScore**: Implement fairness metric that assigns differential weights to landmark accuracy versus background details, addressing finding that current metrics reward correct gardens even with wrong landmarks.