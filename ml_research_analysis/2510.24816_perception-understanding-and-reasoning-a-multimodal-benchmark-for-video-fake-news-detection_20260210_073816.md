---
ver: rpa2
title: Perception, Understanding and Reasoning, A Multimodal Benchmark for Video Fake
  News Detection
arxiv_id: '2510.24816'
source_url: https://arxiv.org/abs/2510.24816
tags:
- news
- video
- fake
- text
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces POVFNDB, a process-oriented benchmark for
  video fake news detection (VFND) that evaluates MLLMs across 10 tasks spanning perception,
  understanding, and reasoning capabilities. The benchmark includes 36,240 human-annotated
  question-answer pairs across 15 evaluation dimensions, enabling fine-grained assessment
  of the entire detection process rather than just final accuracy.
---

# Perception, Understanding and Reasoning, A Multimodal Benchmark for Video Fake News Detection

## Quick Facts
- arXiv ID: 2510.24816
- Source URL: https://arxiv.org/abs/2510.24816
- Authors: Cui Yakun; Peng Qi; Fushuo Huo; Hang Du; Weijie Shi; Juntao Dai; Zhenghao Zhu; Sirui Han; Yike Guo
- Reference count: 40
- Key outcome: POVFNDB benchmark evaluates MLLMs across 10 tasks with 15 metrics, achieving 81.14% accuracy with POVFND-CoT fine-tuning on Qwen2.5VL-7B-Instruct

## Executive Summary
This paper introduces POVFNDB, a process-oriented benchmark for video fake news detection that evaluates MLLMs across 10 tasks spanning perception, understanding, and reasoning capabilities. The benchmark includes 36,240 human-annotated question-answer pairs across 15 evaluation dimensions, enabling fine-grained assessment of the entire detection process rather than just final accuracy. Using POVFNDB, comprehensive evaluations reveal that even leading MLLMs struggle with creator-added text perception and multi-element temporal grounding. To address these limitations, the authors propose POVFND-CoT, a chain-of-thought framework that generates process-aware reasoning data. Fine-tuning Qwen2.5VL-7B-Instruct on this data achieves state-of-the-art performance with 81.14% accuracy on VFND, demonstrating that domain-specific reasoning strategies significantly outperform general capabilities.

## Method Summary
The method constructs POVFNDB by decomposing VFND into 10 sequential tasks: creator-added text position/color perception, key elements perception, creator-added content recognition, key elements temporal grounding, source attribution classification, human/animal identification recognition, knowledge verification, named entity understanding, and fake news detection reasoning. The benchmark uses 3,624 videos from FakeSV dataset with 36,240 QA pairs. POVFND-CoT generates reasoning annotations using Gemini2.5-Flash prompts, which are then used to fine-tune Qwen2.5VL-7B-Instruct with LoRA (12 epochs, batch size 2, learning rate 5e-5). The fine-tuning achieves 81.14% accuracy, outperforming zero-shot approaches by 12.35 points.

## Key Results
- POVFND-CoT fine-tuning on Qwen2.5VL-7B-Instruct achieves 81.14% accuracy, outperforming zero-shot baseline of 68.79%
- MLLMs exhibit poor performance on creator-added text tasks, with best models achieving below 60% accuracy on both CCP and CPP
- OSF features provide larger improvements than CAC features, with OSF-POVFND-CoT achieving 73.87% accuracy vs CAC-POVFND-CoT's 71.58%
- Temporal grounding performance correlates positively with VFND accuracy, with IoU drops when videos contain more than 3 key elements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing VFND into perception, understanding, and reasoning stages enables diagnostic identification of model failures.
- Mechanism: The benchmark separates 10 tasks with 15 distinct metrics, forcing models to demonstrate competence at each stage before reaching final veracity judgment. This reveals that even leading models achieve below 60% accuracy on creator-added text tasks.
- Core assumption: VFND requires sequential capability accumulation—perception failures cascade to reasoning failures.
- Evidence anchors: [abstract]: "comprising 10 tasks designed to systematically evaluate MLLMs' perception, understanding, and reasoning capabilities in VFND"; [section 4.2]: "MLLMs exhibit poor performance on creator-added text tasks... with the best model achieving below 60% accuracy on both tasks"

### Mechanism 2
- Claim: Original Shooting Footage (OSF) features contribute more to detection accuracy than Creator-Added Content (CAC) features.
- Mechanism: OSF captures dynamic visual elements with richer contextual information from authentic footage, while CAC represents author-added overlays with constrained feature space. Table 5 shows OSF-POVFND-CoT achieves 73.87% accuracy vs CAC-POVFND-CoT's 71.58%.
- Core assumption: OSF originates from less-manipulated sources and provides more reliable evidence signals.
- Evidence anchors: [section 4.2]: "OSF features provide larger improvements than CAC... OSF originates from authentic footage and is less susceptible to manipulation"

### Mechanism 3
- Claim: Domain-specific chain-of-thought fine-tuning significantly outperforms zero-shot general capabilities.
- Mechanism: POVFND-CoT generates reasoning paths tailored to news type (knowledge-oriented vs content-oriented vs hybrid). Fine-tuning Qwen2.5VL-7B-Instruct on this data achieves 81.14% accuracy, compared to 68.79% zero-shot—a 12.35 point improvement.
- Core assumption: VFND requires specialized reasoning strategies that general MLLM training doesn't develop.
- Evidence anchors: [abstract]: "Fine-tuning Qwen2.5VL-7B-Instruct on this data achieves state-of-the-art performance with 81.14% accuracy"; [Table 5]: Zero-Shot ACC 68.79 → Instruct-Tuning ACC 81.14

## Foundational Learning

- Concept: **Temporal Grounding (IoU-based localization)**
  - Why needed here: Key Elements Grounding (KEG) task requires identifying time ranges when critical evidence appears. Multi-element grounding correlates positively with VFND accuracy.
  - Quick check question: Given a 30-second video with 3 key events, can your model output `<t1>5.2s-12.1s</t1><t2>18.3s-24.7s</t2><t3>27.0s-29.5s</t3>` with IoU > 0.3?

- Concept: **Source Attribution (CAC vs OSF distinction)**
  - Why needed here: Models must distinguish creator-added overlays from original footage to weight evidence appropriately. Figure 4 shows MLLMs "uncritically accept text added by creators."
  - Quick check question: Given a news frame with on-screen text, can your model identify which text originates from the scene (e.g., street sign) vs post-production overlay?

- Concept: **Process Metrics (ENHR, KHR, RHR)**
  - Why needed here: Final accuracy masks where reasoning fails. Entity Hit Rate, Knowledge Hit Rate, and Rationale Hit Rate provide diagnostic signals.
  - Quick check question: If your model achieves 75% accuracy but ENHR=25%, where is the failure likely occurring?

## Architecture Onboarding

- Component map: Video Input → Frame Sampler → Perception Module → Understanding Module → Reasoning Module → Veracity Output + Process Metrics

- Critical path: Perception failures propagate. If CCP (text color perception) fails at 28-47% accuracy across models, downstream reasoning receives corrupted signals. Prioritize creator-added text perception improvements first.

- Design tradeoffs:
  - Dense frame sampling vs model capacity: Figure 20 shows optimal frame count scales with video duration, but excessive frames degrade performance due to input pressure.
  - CAC+OSF integration vs single-source focus: ALL-POVFND-CoT underperforms OSF-only, suggesting current models can't weight features appropriately.
  - Zero-shot generalization vs domain fine-tuning: 12+ point accuracy gain from fine-tuning, but requires annotated reasoning data.

- Failure signatures:
  - High accuracy, low ENHR: Model memorizing patterns without evidential reasoning
  - CCP/CPP < 60%: MLLM prioritizing semantics over spatial/visual attributes
  - IoU drops with >3 key elements: Temporal grounding capacity exceeded

- First 3 experiments:
  1. **Baseline diagnostic**: Run your MLLM on all 10 POVFNDB tasks. If CCP/CPP < 50%, add text perception pretraining (color, position annotations).
  2. **Frame sampling sweep**: Test 8/16/24/32 frames on videos grouped by duration (<30s, 30-60s, >60s). Identify optimal sampling per duration bucket.
  3. **CoT variant comparison**: Implement CAC-POVFND-CoT vs OSF-POVFND-CoT vs ALL-POVFND-CoT. If ALL underperforms OSF, add learned feature weighting module rather than concatenation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the authenticity of external knowledge retrieved by MLLMs be reliably verified during the video fake news reasoning process?
- Basis in paper: [explicit] The authors state in the Limitations section that "The application of external knowledge requires careful verification of its authenticity, which typically necessitates cross-validation from multiple sources. Further exploration of these approaches remains necessary."
- Why unresolved: While the paper introduces a Knowledge Hit Rate (KHR) metric, it does not address the verification of the knowledge itself, leaving models susceptible to hallucinations or unverified sources when checking facts.
- What evidence would resolve it: A verification module or pipeline that cross-references retrieved knowledge with trusted fact-checking databases, improving the KHR metric's reliability and reducing hallucination rates in the FDR task.

### Open Question 2
- Question: How can the deceptive characteristics shared by specific fake news types be formally modeled to improve detection generalization?
- Basis in paper: [explicit] The Limitations section notes that "Fake news instances of the same type may exhibit similar deceptive characteristics, as the nature of the facts they aim to obscure is comparable. This aspect, however, remains underexplored in our current work."
- Why unresolved: The current benchmark evaluates instance-level detection without explicitly leveraging or analyzing the shared deceptive patterns (e.g., specific editing styles or narrative structures) that might exist within a category like "knowledge-based" fake news.
- What evidence would resolve it: A study showing that clustering training data by "deceptive characteristics" (rather than just news topic) leads to improved few-shot learning or better performance on unseen instances within the same deceptive category.

### Open Question 3
- Question: How can MLLMs be optimized to dynamically integrate Original Shooting Footage (OSF) and Creator-Added Content (CAC) features without performance degradation?
- Basis in paper: [inferred] Section 4.3 (Results Analysis) reveals that the "ALL-POVFND-CoT" model underperforms the "OSF-POVFND-CoT" model, which the authors attribute to MLLMs' "inability to properly weight each feature's contribution," suggesting a lack of effective integration strategies.
- Why unresolved: The paper demonstrates that simply concatenating all available multimodal features (CAC + OSF) creates noise that degrades accuracy, identifying a specific failure mode in current architectures.
- What evidence would resolve it: An architecture or attention mechanism that allows the model to selectively up-weight OSF features (which provide larger improvements) while filtering noise from CAC, resulting in the ALL-variant outperforming single-source variants.

## Limitations
- The benchmark's focus on Chinese-language news videos may limit generalizability to other linguistic and cultural contexts
- The staged approach may oversimplify complex interactions between modalities in real-world detection scenarios
- Reliance on human-annotated data for POVFND-CoT generation introduces potential annotation bias

## Confidence

- **High Confidence**: POVFNDB benchmark construction and task decomposition methodology (well-documented with clear evaluation metrics)
- **Medium Confidence**: Domain-specific CoT fine-tuning effectiveness (12.35% improvement demonstrated, but attribution to reasoning structure vs. dataset exposure unclear)
- **Medium Confidence**: OSF vs CAC feature superiority claim (statistically supported but based on limited cross-model comparisons)

## Next Checks

1. Conduct ablation studies comparing POVFND-CoT performance with simple classification fine-tuning to isolate reasoning structure benefits
2. Test benchmark on non-Chinese news videos to assess cross-cultural generalization and language dependency
3. Evaluate models on mixed-content videos containing both deepfake manipulations and creator-added overlays to stress-test the CAC/OSF distinction assumptions