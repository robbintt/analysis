---
ver: rpa2
title: 'BRIDGES: Bridging Graph Modality and Large Language Models within EDA Tasks'
arxiv_id: '2504.05180'
source_url: https://arxiv.org/abs/2504.05180
tags:
- graph
- code
- bridges
- text
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BRIDGES addresses the challenge of integrating graph modality into
  large language models (LLMs) for electronic design automation (EDA) tasks. It introduces
  an automated data generation workflow that converts RTL and netlist data into dataflow
  and netlist graphs, creating a large-scale dataset with over 500,000 graph instances
  and 1.5 billion tokens.
---

# BRIDGES: Bridging Graph Modality and Large Language Models within EDA Tasks

## Quick Facts
- arXiv ID: 2504.05180
- Source URL: https://arxiv.org/abs/2504.05180
- Reference count: 40
- Primary result: BRIDGES achieves 2x-10x improvements across EDA tasks by integrating graph modality into LLMs with negligible overhead

## Executive Summary
BRIDGES addresses the challenge of integrating graph modality into large language models (LLMs) for electronic design automation (EDA) tasks. It introduces an automated data generation workflow that converts RTL and netlist data into dataflow and netlist graphs, creating a large-scale dataset with over 500,000 graph instances and 1.5 billion tokens. BRIDGES employs a lightweight cross-modal projector that encodes graph representations into text-compatible prompts, enabling LLMs to effectively utilize graph data without architectural modifications. Experimental results demonstrate significant improvements across multiple tasks compared to text-only baselines, including accuracy in design retrieval (up to 93% vs. 46% for text), type prediction (73% vs. 63%), and function description generation (perplexity of 2.08 vs. 4.65), with negligible computational overhead.

## Method Summary
BRIDGES uses a two-stage training approach to integrate graph modality into LLMs. Stage 1 trains a graph encoder (NetlistGNN) and Q-Former cross-modal projector on graph-text pairs using contrastive learning, graph-text matching, and graph-grounded text generation objectives. The Q-Former learns to extract text-relevant graph features through query-based attention with 8 learnable tokens. Stage 2 aligns the Q-Former outputs with LLM embedding space through a linear projector and optionally fine-tunes the LLM with LoRA. The framework processes RTL code into dataflow and netlist graphs, then uses the Q-Former to generate soft graph prompts that are prepended to text tokens for LLM processing. This approach enables effective graph-text integration without modifying LLM architecture.

## Key Results
- Design-function retrieval accuracy: 93% (full set) vs 46% (text-only)
- Circuit type prediction accuracy: 73% vs 63% (text-only)
- Function description generation perplexity: 2.08 vs 4.65 (text-only)
- Computational overhead: <1% model weights increase, <30% additional runtime overhead
- Outperforms text-only and graph-only approaches even without LLM fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- The Q-Former cross-modal projector enables graph-to-text translation by learning to extract text-relevant graph features through query-based attention.
- Learnable query tokens attend to graph embeddings via cross-attention while simultaneously attending to each other via self-attention. Three pre-training objectives force queries to extract representations that maximize mutual information with associated text descriptions.
- Core assumption: A fixed number of query tokens (8) can compress arbitrarily large graph structures into a text-compatible representation without catastrophic forgetting.
- Evidence: "lightweight cross-modal projector that encodes graph representations into text-compatible prompts" [abstract]; "The goal is for the queries in Q-Former to be able to extract graph representations that are most informative for the text" [section V]

### Mechanism 2
- Graph modality provides structural information that sequential text cannot efficiently encode, enabling LLMs to reason about circuit topology.
- NetlistGNN performs message passing on directed heterogeneous graphs, capturing gate connectivity and signal flow. Pooling operations aggregate node embeddings into fixed-size graph representations that preserve topological properties lost in linearized text.
- Core assumption: The graph encoder's inductive bias captures task-relevant structure more efficiently than LLM's learned text patterns.
- Evidence: "LLMs struggle to fully interpret and learn from essential graph properties, like structure and functionality, when they are encoded as linear sequences" [section I]; Netlist graph retrieval achieves 93% accuracy vs 46% for netlist text [section VI-B]

### Mechanism 3
- Two-stage training decouples representation learning from task alignment, enabling efficient transfer without modifying LLM architecture.
- Stage 1 trains graph encoder + Q-Former on graph-text pairs. Stage 2 freezes encoder, trains linear projector to align Q-Former outputs with LLM embedding space, and optionally fine-tunes LLM with LoRA for task-specific behavior.
- Core assumption: Stage 1 pre-training learns general graph-text correlations that transfer across EDA tasks; task-specific fine-tuning only needs lightweight adaptation.
- Evidence: "stage 1 is the pre-training representation learning stage to extract text-relevant graph representation, while stage 2 is the alignment learning stage" [section V]; Removing Stage 1 drops type prediction accuracy from 73% to 55% [section VI-H]

## Foundational Learning

- **Message-passing Graph Neural Networks**
  - Why needed here: NetlistGNN encodes circuit topology by iteratively aggregating neighbor information. Understanding how node embeddings propagate through gates is essential for debugging representation quality.
  - Quick check question: Can you explain why heterogeneous graphs (different node/edge types) are necessary for representing netlists versus homogeneous graphs?

- **Cross-attention mechanisms**
  - Why needed here: The Q-Former uses cross-attention to attend from query tokens to graph embeddings. Understanding attention patterns reveals which graph regions the model focuses on.
  - Quick check question: How does cross-attention differ from self-attention, and why must query tokens attend to graph embeddings but not vice-versa in GTG objective?

- **Contrastive learning objectives**
  - Why needed here: GTC objective trains the model to distinguish positive graph-text pairs from negatives. This is the primary driver of representation quality in Stage 1.
  - Quick check question: Why does the model use hard negatives (highest-similarity negative pairs) for graph-text matching rather than random negatives?

## Architecture Onboarding

- **Component map**: Parse netlist → NetworkX directed heterogeneous graph → NetlistGNN → node embeddings → 4 pooling operations → graph embedding → Q-Former → 8 query embeddings → Linear projection → prepend to text token embeddings → LLM

- **Critical path**: 1) Parse netlist → NetworkX directed heterogeneous graph 2) NetlistGNN forward pass → node embeddings → 4 pooling operations → graph embedding 3) Q-Former: queries attend to graph embedding via cross-attention → 8 query embeddings 4) Linear projection → prepend to text embeddings → LLM forward pass 5) Stage 1: Train encoder + Q-Former with 3 objectives (no LLM) 6) Stage 2: Train projector + optional LoRA on LLM

- **Design tradeoffs**: Query token count vs compression (8 tokens minimize overhead but may lose information on large graphs); Graph encoder capacity (d1=512 scales better but adds 5.3M parameters); LLM fine-tuning (LoRA adds ~1% parameters but improves performance); Graph type selection (paper uses netlist graphs; dataflow graphs mentioned but not evaluated)

- **Failure signatures**: Catastrophic forgetting with Graph2Text (text-only LLMs perform worse with longer sequences); Graph encoder saturation (smaller encoders show accuracy degradation with more data); Stage 1 omission (18% accuracy drop in type prediction); Retrieval failure modes (full-set retrieval accuracy much lower than in-batch)

- **First 3 experiments**: 1) Validate graph encoder quality: Train Stage 1 on subset, visualize attention patterns of query tokens on graph nodes for simple circuits, confirm queries attend to functionally relevant subgraphs 2) Ablate query token count: Compare 4, 8, 16, 32 query tokens on design retrieval task, measure accuracy vs runtime/memory overhead 3) Test cross-dataset transfer: Train Stage 1 on RTLCoder designs, evaluate zero-shot on MG-Verilog to probe whether learned representations generalize or overfit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What graph encoder architectures beyond NetlistGNN could better capture structural and functional properties of large-scale EDA graphs (10,000–800,000 nodes)?
- Basis in paper: "Investigating more powerful graph encoders remains an avenue for future research"; "the study of graph encoder deserves more exploration"
- Why unresolved: Current experiments use only NetlistGNN with 5 layers and 512-dim embeddings; scalability study shows smaller encoders overfit while larger encoders remain data-limited
- What evidence would resolve it: Systematic comparison of transformer-based graph encoders, hierarchical GNNs, or subgraph pooling methods on the same BRIDGES dataset, measuring retrieval accuracy and perplexity improvements

### Open Question 2
- Question: How much additional performance gain can BRIDGES achieve with dataset scaling beyond the current 500K graphs and 1.5B tokens?
- Basis in paper: "the current dataset size limits model performance, indicating the potential for better performance with larger datasets"; scalability study shows d1=512 configuration improves "without signs of saturation"
- Why unresolved: The paper demonstrates consistent accuracy gains with increasing data proportions but does not test beyond the collected dataset
- What evidence would resolve it: Training on 2x, 5x, 10x larger datasets and reporting accuracy curves with saturation analysis

### Open Question 3
- Question: Can alternative cross-modal projector architectures (e.g., attention-based adapters, multi-layer perceptrons with skip connections) match or exceed Q-Former's performance while reducing computational overhead?
- Basis in paper: ablation study shows replacing Q-Former with a simple linear projector drops type prediction accuracy from 73.0% to 66.2%
- Why unresolved: The Q-Former adds 96.9M parameters and ~20-30% runtime overhead; the trade-off between projector expressivity and efficiency is unexplored
- What evidence would resolve it: Benchmarking alternative projectors on identical tasks, reporting both accuracy metrics and inference latency

## Limitations

- Dataset composition uncertainty: The paper doesn't fully characterize diversity or potential biases in the synthesized netlists across 27 synthesis configurations
- Graph encoder architectural details: NetlistGNN implementation details (node/edge features, aggregation functions) are referenced but not specified, creating reproducibility gaps
- Query token compression: The 8-query-token design is a significant compression factor for large graphs, and the paper doesn't ablate query token count or examine catastrophic forgetting as graph complexity increases

## Confidence

- **High confidence**: Retrieval accuracy improvements (93% vs 46% text), type prediction gains (73% vs 63%), and computational overhead claims (<1% parameter increase, <30% runtime overhead)
- **Medium confidence**: The 2-stage training methodology and its decoupling benefits are well-grounded in BLIP-2 literature and ablation results, but specific hyperparameter choices lack systematic exploration
- **Low confidence**: Generalization claims across EDA tasks are supported by four task evaluations but limited to specific circuit types in the dataset; transfer to novel circuit topologies remains untested

## Next Checks

1. **Query token ablation study**: Systematically evaluate 4, 8, 16, and 32 query tokens on design retrieval accuracy and computational overhead to identify optimal compression vs. performance tradeoffs

2. **Cross-dataset generalization test**: Train Stage 1 on RTLCoder designs and evaluate zero-shot performance on MG-Verilog or completely different circuit types to verify representation generalization beyond the 8 training categories

3. **Large graph stress test**: Construct evaluation sets with graphs exceeding 100K nodes (near the 800K upper bound) and measure accuracy degradation, memory usage, and any evidence of pooling-induced information loss in critical circuit paths