---
ver: rpa2
title: 'Towards Fine-Grained Interpretability: Counterfactual Explanations for Misclassification
  with Saliency Partition'
arxiv_id: '2511.07974'
source_url: https://arxiv.org/abs/2511.07974
tags:
- fine-grained
- feature
- explanations
- class
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a fine-grained counterfactual explanation framework
  that addresses the limitation of traditional attribution-based methods in providing
  granular insights for misclassification analysis. The core idea involves generating
  object-level and part-level interpretability through a saliency partition module
  based on Shapley value contributions, enabling the isolation of region-specific
  relevant features.
---

# Towards Fine-Grained Interpretability: Counterfactual Explanations for Misclassification with Saliency Partition

## Quick Facts
- arXiv ID: 2511.07974
- Source URL: https://arxiv.org/abs/2511.07974
- Authors: Lintong Zhang; Kang Yin; Seong-Whan Lee
- Reference count: 40
- Primary result: Achieves compact activation score of 9.53 on CUB-200-2011 using non-generative counterfactual explanations

## Executive Summary
This paper addresses the limitation of traditional attribution-based interpretability methods by proposing a fine-grained counterfactual explanation framework that provides object-level and part-level insights for misclassification analysis. The approach combines a saliency partition module based on Shapley value contributions with an iterative non-generative counterfactual generation method. By isolating region-specific relevant features and creating explainable counterfactuals through similarity-weighted component contributions, the framework demonstrates superior performance on fine-grained datasets like CUB-200-2011 and Stanford Dogs, achieving metrics such as 43.02% insertion and 10.27% deletion rates.

## Method Summary
The framework operates in three stages: (1) Feature extraction from the final convolutional layer of a CNN backbone (ResNet-50/VGG-16), (2) Saliency Partition using spatially localized Gaussian kernels to compute Shapley value approximations for each feature position, and (3) Iterative counterfactual generation by selecting features with highest Shapley values and replacing them with semantically similar candidates from a reference set until prediction changes. The method generates dual-region explanations by subtracting pre- and post-counterfactual feature maps, isolating invariant (misclassification-causing) and dominant (correct-class) regions. Unlike generative approaches, this non-generative method relies on feature retrieval and replacement, avoiding latent space manipulation while preserving semantic consistency.

## Key Results
- Compact Activation Score of 9.53 on CUB-200-2011 and 10.45 on Stanford Dogs
- Insertion metric of 43.02% on CUB-200-2011
- Deletion metric of 10.27% on CUB-200-2011
- Superior localization of granular, intuitively meaningful regions compared to existing approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spatially localized Gaussian kernels disentangle feature contributions better than holistic saliency maps.
- Mechanism: The Saliency Partition (SP) module constructs n² Gaussian kernels centered at each feature map position, then creates "reversed" kernels (1-G) that suppress local regions. By passing partitioned feature maps through the classifier, the method approximates each feature point's marginal contribution via s = p_h·1 - p̃H, where p̃H captures predictions when each region is suppressed.
- Core assumption: Feature contributions are approximately additive and can be isolated by local suppression without catastrophic interference from neighboring features.
- Evidence anchors: [abstract] "we introduce a saliency partition module grounded in Shapley value contributions, isolating features with region-specific relevance"; [Section 3.1, Eq. 1-4] Defines the Shapley approximation and spatially localized kernels G and G̃; [corpus] "Rethinking Saliency Maps" paper notes fundamental ambiguity in saliency map purposes, supporting the need for more structured approaches like SP
- Break condition: If σ (kernel scale) is too small, suppression is too localized to capture meaningful feature interactions; if too large, partitioning effect blurs. Paper sets σ=0.8 without systematic justification.

### Mechanism 2
- Claim: Iterative feature replacement guided by Shapley values and semantic similarity produces semantically consistent counterfactuals.
- Mechanism: At each iteration t, select feature h_s*^(t) with highest Shapley value from misclassified sample. Match it to candidate feature h̃_s[k,i,j] from correctly classified set U that maximizes L_tot = L_sim + L_cls, where L_sim uses cosine similarity and L_cls uses classification loss. Replace and repeat until prediction changes.
- Core assumption: Cosine similarity in feature space correlates with perceptual/semantic similarity, and high-Shapley features from correct samples transfer class-discriminative information.
- Evidence anchors: [abstract] "A non-generative iterative method generates counterfactual explanations by modifying misclassified samples until the prediction aligns with the correct class, while preserving semantic consistency"; [Section 3.2, Eq. 5-9] Formalizes the joint optimization objective and replacement procedure; [corpus] "LeapFactual" and "GCFX" papers use generative counterfactual approaches; this paper's non-generative approach contrasts by avoiding latent space manipulation
- Break condition: If Top-m selection is too aggressive (small m), candidate pool lacks semantically similar features; if too large, computational cost increases. Paper uses m determined empirically but doesn't specify value.

### Mechanism 3
- Claim: Subtracting pre- and post-counterfactual feature maps isolates invariant (misclassification-causing) and dominant (correct-class) regions.
- Mechanism: ∆M_Inv = s^(0)·σ_N(h^(0) - h*) highlights features persistently contributing to incorrect class; ∆M_Dom = s*·σ_N(h* - h^(0)) highlights features that drove the correction. ReLU (σ_N) ensures only positive contributions remain.
- Core assumption: Feature map differences directly correspond to class-discriminative regions, and Shapley weighting refines localization.
- Evidence anchors: [Section 3.3, Eq. 10-11] Defines the dual-region contrastive explanation formulation; [Figure 4] Visualizes invariant/dominant regions for bird and dog misclassifications; [corpus] Weak direct coverage—neighbor papers focus on counterfactual generation rather than dual-region decomposition
- Break condition: If iterative replacement modifies too many features, the subtraction becomes noisy; early stopping or iteration limit (t_max=100) constrains this.

## Foundational Learning

- Concept: **Shapley Values for Feature Attribution**
  - Why needed here: Core to SP module; approximates each feature's marginal contribution to prediction by comparing full vs. ablated feature maps.
  - Quick check question: Can you explain why exact Shapley computation is intractable for neural network features, motivating the paper's approximation?

- Concept: **Counterfactual Explanations in Vision**
  - Why needed here: Framework's goal is generating "what-if" visual modifications that flip predictions, contrasting with purely attribution-based methods.
  - Quick check question: What's the difference between generative (GAN-based) and non-generative (retrieval-based) counterfactual approaches, and why does this paper choose the latter?

- Concept: **Fine-Grained Classification Challenges**
  - Why needed here: Paper targets datasets (CUB-200-2011, Stanford Dogs) where inter-class differences are subtle and localized (e.g., bird head shape vs. wing pattern).
  - Quick check question: Why do traditional GradCAM-style attributions fail to distinguish between similar classes in fine-grained tasks?

## Architecture Onboarding

- Component map:
  - Feature Extraction (CNN backbone) -> Saliency Partition (Gaussian kernels) -> Shapley value computation -> Iterative Counterfactual Generator -> Explanation Output (∆M_Inv, ∆M_Dom)

- Critical path:
  1. Input misclassified image I + correctly classified reference set U
  2. Extract features h and h_U from final conv layer
  3. Compute Shapley contributions s and S̃_U via SP module
  4. Iterate: select top Shapley feature -> find best replacement candidate via L_tot -> replace
  5. Generate dual-region explanation from initial and final feature maps

- Design tradeoffs:
  - **Non-generative vs. Generative**: Avoids GAN artifacts and training overhead, but limited to features present in reference set U
  - **Top-m filtering**: Reduces search space (efficiency) but may exclude optimal candidates (recall)
  - **σ=0.8 fixed**: No adaptive scale; may under/over-partition for different feature map resolutions

- Failure signatures:
  - **Iteration timeout**: If t reaches 100 without prediction change, counterfactual generation fails (paper doesn't report failure rate)
  - **Semantically incoherent replacement**: If cosine similarity doesn't correlate with perceptual similarity, replaced features may look unnatural
  - **Diffuse attributions**: If SP module's σ is misconfigured, ∆M maps may be too scattered

- First 3 experiments:
  1. **Sanity check**: Run SP module on a single image; verify that suppressing high-Shapley regions causes largest prediction drop (deletion test sanity)
  2. **Ablation on |U|**: Vary reference set size (5, 10, 20, 50) and measure compact activation score ξ; confirm paper's claim that |U|=20 balances performance/cost
  3. **Visualization validation**: Generate ∆M_Inv and ∆M_Dom for 10 misclassified samples; manually check if highlighted regions align with known class-discriminative parts (e.g., bird beak vs. wings)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Saliency Partition module be effectively adapted for Vision Transformer (ViT) architectures, given its current reliance on spatially localized Gaussian kernels?
- Basis in paper: [Explicit] The conclusion explicitly states, "In future work, we focus on the adaptability of this explanation method within transformer architectures."
- Why unresolved: The proposed SP module is defined on continuous coordinate grids ($X, Y$) designed for CNN feature maps, which may not directly translate to the discrete patch-based structure and global attention mechanisms of Transformers.
- Evidence: Applying FG-VCE to ViT backbones and comparing the Compact Activation Score and localization fidelity against CNN baselines.

### Open Question 2
- Question: How sensitive is the iterative counterfactual generation to the composition and cardinality of the correctly classified reference set $U$?
- Basis in paper: [Inferred] The method relies on a set $U$ of size 20 for candidate features, fixed to balance performance and computational cost, but the impact of this specific selection or set size on explanation stability is not analyzed.
- Why unresolved: The framework selects replacement features based on similarity to $U$; if $U$ lacks diversity or representative features, the explanation may converge slowly or produce less semantically consistent results.
- Evidence: An ablation study varying the size of $U$ (e.g., 5, 10, 50) and measuring the variance in the Compact Activation Score and the number of iterations required for convergence.

### Open Question 3
- Question: What is the failure rate of the iterative replacement process when the maximum iteration limit ($t=100$) is reached, and how does it correlate with inter-class similarity?
- Basis in paper: [Inferred] The implementation details set a maximum of 100 iterations, where exceeding the limit indicates "failure in counterfactual generation," but the paper does not report the frequency or conditions of these failures.
- Why unresolved: It is unclear if the method struggles with specific types of misclassifications where distinct local features are difficult to isolate or replace from the reference set.
- Evidence: Reporting the percentage of failed counterfactual generations on the test datasets and analyzing the feature variance between the misclassified class and the correct class in these cases.

## Limitations
- The Saliency Partition module's fixed Gaussian kernel scale (σ=0.8) lacks systematic justification and may not generalize across different feature map resolutions
- Non-generative approach limits counterfactuals to features present in reference set U, potentially missing optimal modifications
- The paper doesn't report counterfactual generation failure rates or conduct cross-dataset validation beyond the two fine-grained benchmarks

## Confidence
- **High Confidence**: The core framework architecture (SP module + iterative replacement) is clearly defined and the experimental methodology is reproducible. Performance metrics (Insertion 43.02%, Deletion 10.27%, ξ=9.53 on CUB-200-2011) are directly measurable from the paper.
- **Medium Confidence**: The Shapley value approximation method is sound but the choice of kernel parameters and reference set size (|U|=20) appears somewhat arbitrary without ablation studies showing sensitivity.
- **Low Confidence**: The claim that ∆M_Inv/∆M_Dom provide "object-level and part-level interpretability" lacks quantitative validation - there's no human study confirming whether these visualizations align with domain expert expectations for bird/dog classification.

## Next Checks
1. **Parameter Sensitivity**: Systematically vary σ in the Gaussian kernel (0.4, 0.6, 0.8, 1.0) and reference set size (|U|=10, 20, 30) to identify optimal configurations and quantify robustness.
2. **Cross-Architecture Generalization**: Apply FG-VCE to non-fine-grained datasets (ImageNet, CIFAR-10) to test whether the approach maintains performance when class differences are more global rather than localized.
3. **Human Interpretability Study**: Conduct a user study with domain experts (ornithologists/breed specialists) to validate whether ∆M_Inv and ∆M_Dom visualizations correctly identify class-discriminative regions compared to ground truth annotations.