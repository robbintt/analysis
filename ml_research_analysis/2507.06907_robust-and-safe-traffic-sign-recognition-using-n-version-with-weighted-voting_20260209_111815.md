---
ver: rpa2
title: Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting
arxiv_id: '2507.06907'
source_url: https://arxiv.org/abs/2507.06907
tags:
- voting
- weighted
- safety
- soft
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a safety-aware weighted soft voting mechanism
  for N-version machine learning systems to improve the robustness and safety of traffic
  sign recognition in autonomous driving. The method uses Failure Mode and Effects
  Analysis (FMEA) to assess risks of misclassification and assign dynamic weights
  to model predictions.
---

# Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting

## Quick Facts
- arXiv ID: 2507.06907
- Source URL: https://arxiv.org/abs/2507.06907
- Authors: Linyun Gao; Qiang Wen; Fumio Machida
- Reference count: 40
- Primary result: N-version ML with FMEA-based weighted soft voting outperforms baselines in accuracy, safety, and adversarial robustness for traffic sign recognition.

## Executive Summary
This paper introduces a safety-aware weighted soft voting mechanism for N-version machine learning systems to enhance traffic sign recognition in autonomous driving. The approach leverages Failure Mode and Effects Analysis (FMEA) to quantify misclassification risks and assign dynamic weights to ensemble outputs. Experimental results demonstrate that the proposed method significantly outperforms majority voting, simple soft voting, and LLM-based weighting in terms of accuracy, safety score, and reduction of severe misclassifications, particularly under adversarial attacks.

## Method Summary
The method employs three independently trained CNNs (AlexNet, VGG16, EfficientNetB0) on the GTSRB dataset. FMEA-derived risk scores are computed using a severity matrix (expert-defined) and probability matrix (from confusion matrix on a weight assignment set). Weights are assigned inversely proportional to risk (w_i = Σrs_j / rs_i). During inference, probability outputs from all models are aggregated via weighted soft voting: G_w(x) = argmax_j Σ_i w_i · g_ij(x). The system is evaluated on accuracy, safety score, and SL2/SL1 misclassification counts under both clean and adversarial conditions (FGSM/PGD).

## Key Results
- Weighted soft voting achieves 98.40% accuracy and 97.92% safety score on GTSRB, reducing SL2 misclassifications to 11 (vs. 17 for majority voting).
- Under PGD attack (ε=0.05), NVML with weighted soft voting maintains 62.17% accuracy and 77.88% safety score, significantly outperforming single-model baselines.
- The safety-aware weighting method outperforms LLM-based weighting, which produced more evenly distributed weights and suboptimal safety outcomes.

## Why This Works (Mechanism)

### Mechanism 1: Safety-Aware Weight Assignment via FMEA-Derived Risk Scores
Assigning lower weights to models with higher misclassification risk reduces severe safety failures. FMEA quantifies risk per model as R = P × S, where P is misclassification probability and S is severity score. Weights are computed inversely to risk: w_i = Σrs_j / rs_i. Core assumption: Historical misclassification patterns generalize to operational conditions.

### Mechanism 2: N-Version Redundancy with Weighted Soft Voting Aggregation
Aggregating probability outputs from diverse models using safety-aware weights reduces both accuracy loss and severe misclassifications. Each model outputs probability distribution; final prediction uses weighted sum of probabilities. Core assumption: Models fail independently or with uncorrelated failure modes.

### Mechanism 3: Adversarial Robustness via Diversity and Soft Voting
NVML systems maintain higher accuracy and safety under adversarial perturbations than single models. Adversarial perturbations optimized for one model may not transfer effectively to others due to architectural differences. Core assumption: Attack transferability is incomplete across ensemble members.

## Foundational Learning

- **Concept: N-Version Programming (NVP)**
  - Why needed: Extends classical software fault tolerance to ML models, relying on independent training to produce diverse failure modes.
  - Quick check: Can you explain why simply using the same architecture with different random seeds might produce correlated failures?

- **Concept: FMEA (Failure Mode and Effects Analysis)**
  - Why needed: Provides structured methodology to enumerate failure modes, assign severity levels, and quantify risk—essential for the safety metric that drives weight assignment.
  - Quick check: For traffic sign recognition, why is "Speed Limit 30 → Speed Limit 80" a higher-severity misclassification than "Stop → Yield"?

- **Concept: Soft Voting vs. Hard Voting**
  - Why needed: Soft voting leverages full probability distributions, capturing model confidence; hard voting discards this information.
  - Quick check: If three models output probabilities [0.9, 0.1, 0.0], [0.4, 0.5, 0.1], and [0.3, 0.4, 0.3] for classes A, B, C respectively, what would simple soft voting predict versus majority hard voting?

## Architecture Onboarding

- **Component map**: Model Pool -> FMEA Module -> Weight Calculator -> Aggregation Layer -> Output
- **Critical path**: 1) Train T diverse models independently. 2) Define severity matrix S. 3) Evaluate on weight assignment set to generate confusion matrices. 4) Compute probability matrix P and risk matrix R = S ⊙ P. 5) Derive safety scores and weights. 6) At inference: collect probability outputs, aggregate via weighted soft voting.
- **Design tradeoffs**: More models → higher robustness but increased latency; Weight assignment set size → larger sets give more stable weights but reduce evaluation data; Severity score scaling (σ(0)=0, σ(1)=1, σ(2)=10) → higher penalties for SL2 errors.
- **Failure signatures**: All models disagree (no consensus); Weight assignment set not representative; Correlated adversarial failures; Uncalibrated probabilities.
- **First 3 experiments**: 1) Replicate weight assignment process: split test set 50/50, compute confusion matrices on first half, derive weights, evaluate on second half with simple vs. weighted soft voting. 2) Diversity ablation: replace EfficientNetB0 with second AlexNet, measure robustness degradation under FGSM/PGD. 3) Adaptive attack test: generate PGD attacks using gradient information from full ensemble, compare robustness of weighted soft voting vs. single-model.

## Open Questions the Paper Calls Out

- **Can specialized LLMs or refined prompt engineering strategies generate safety-aware weights that outperform the proposed safety-score method?**
  - Basis: LLM-based approach (GPT-4o) produced "suboptimal solutions" with the authors calling for "more specialized LLMs and improved prompt engineering."
  - Why unresolved: Experiments limited to general-purpose model and basic prompts.
  - Evidence needed: Experiments with fine-tuned LLMs or advanced chain-of-thought prompting showing higher Safety Score and lower SL2 misclassification count than safety-score baseline.

- **Does the proposed safety-aware weighted soft voting mechanism generalize to other safety-critical perception tasks beyond traffic sign recognition?**
  - Basis: Conclusion suggests extending framework to other ML tasks to deepen understanding of reliability and safety.
  - Why unresolved: Current study confined to GTSRB and ArTS traffic sign datasets.
  - Evidence needed: Successful application of FMEA-weighting methodology on distinct tasks (e.g., pedestrian detection) with demonstrated reduction in critical failure modes.

- **Can dynamic, instance-level weight assignment improve robustness compared to the static weights derived from historical performance?**
  - Basis: Paper assigns fixed weights based on "weight assignment set" and acknowledges probability estimates may not always be well-calibrated.
  - Why unresolved: Static weights assume historical risk is uniform, whereas adversarial attacks may target specific model vulnerabilities dynamically.
  - Evidence needed: Comparative study showing adaptive weighting algorithm (e.g., based on real-time prediction entropy) yields higher RSafety scores under PGD attacks than static method.

## Limitations
- Effectiveness of FMEA-derived weights depends on weight assignment set being representative of operational conditions; distribution shifts could invalidate historical risk scores.
- Paper does not explore how performance scales with larger ensembles (e.g., 5, 10, or more models), which could impact both robustness and computational overhead.
- Robustness against standard FGSM and PGD attacks is demonstrated, but adaptive attacks specifically targeting the ensemble are not evaluated.

## Confidence
- **Safety-aware weighted voting improves robustness and safety** (High): Strong empirical evidence from multiple tables demonstrates consistent improvements.
- **FMEA-based risk scoring effectively guides weight assignment** (Medium): Methodology is sound and shows quantitative improvement, but critical assumption of representative weight assignment sets is not experimentally validated under distribution shift.
- **N-version redundancy provides adversarial robustness** (Medium): Mechanism is theoretically sound, empirical results under standard attacks are positive, but absence of adaptive attack testing limits confidence in worst-case guarantees.

## Next Checks
1. **Distribution shift validation**: Retrain the ensemble on a different traffic sign dataset (e.g., BelgiumTS) and evaluate whether the GTSRB-derived weights maintain their safety-aware performance.
2. **Adaptive attack evaluation**: Generate PGD attacks using ensemble gradients and compare the robustness of weighted soft voting versus single models and majority voting.
3. **Model pool size ablation**: Evaluate the NVML system with 5 or 7 independently trained models to quantify how safety-aware weighted voting scales with ensemble size.