---
ver: rpa2
title: 'Hybrid-Vector Retrieval for Visually Rich Documents: Combining Single-Vector
  Efficiency and Multi-Vector Accuracy'
arxiv_id: '2510.22215'
source_url: https://arxiv.org/abs/2510.22215
tags:
- retrieval
- query
- pages
- document
- heaven
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HEAVEN addresses the efficiency-accuracy trade-off in visual document
  retrieval by introducing a two-stage hybrid-vector framework. In Stage 1, it efficiently
  retrieves candidate pages using single-vector retrieval over visually-summarized
  pages (VS-pages), which compress multiple pages into compact visual layouts.
---

# Hybrid-Vector Retrieval for Visually Rich Documents: Combining Single-Vector Efficiency and Multi-Vector Accuracy

## Quick Facts
- arXiv ID: 2510.22215
- Source URL: https://arxiv.org/abs/2510.22215
- Reference count: 17
- Primary result: 99.87% of multi-vector Recall@1 with 99.82% computation reduction

## Executive Summary
HEAVEN introduces a two-stage hybrid-vector framework that addresses the efficiency-accuracy trade-off in visual document retrieval. The method combines single-vector retrieval over visually-summarized pages (VS-pages) for efficient candidate retrieval with multi-vector reranking on linguistically important tokens for accuracy. By aggregating representative visual layouts from multiple pages and filtering query tokens by linguistic importance, HEAVEN achieves near-multi-vector performance while dramatically reducing computation. The authors also introduce VIMDOC, the first benchmark for visually rich, multi-document, and long-document retrieval.

## Method Summary
HEAVEN is a two-stage hybrid retrieval framework. Stage 1 uses single-vector retrieval over VS-pages, which aggregate representative visual layouts from multiple pages to reduce index size. Stage 2 re-ranks candidates using multi-vector retrieval while filtering query tokens to only linguistically important ones (nouns, named entities). The method uses DocLayout-YOLO for VS-page construction, DSE for single-vector retrieval, and ColQwen2.5 for multi-vector reranking. Hyperparameters include α=0.1, β=0.3 for score fusion, with reduction factor r=min(15,|D_k|).

## Key Results
- Achieves 99.87% of multi-vector Recall@1 performance while reducing computation by 99.82%
- Outperforms state-of-the-art methods across four benchmarks including the newly introduced VIMDOC
- Query token filtering reduces computation by retaining only ~30% key tokens while preserving accuracy

## Why This Works (Mechanism)

### Mechanism 1: Visually-Summarized Pages (VS-Pages) for Index Compression
VS-pages aggregate representative visual layouts from multiple pages, reducing redundant similarity computations while preserving retrieval signals. Document Layout Analysis extracts title layouts, assembled into VS-pages at reduction factor r. Similarity computed against |VS| ≈ |P|/r pages instead of |P| raw pages. Core assumption: title layouts contain sufficient semantic anchors for coarse-grained retrieval.

### Mechanism 2: Query Token Filtering via Linguistic Importance
Filtering query tokens to ~30% "key tokens" (nouns, named entities) preserves multi-vector accuracy while reducing MaxSim computation cost. POS tagging identifies tokens with tags NN, NNS, NNP, NNPS. Multi-vector score computes MaxSim only over key tokens, reducing computation by factor |q_key|/n_q. Core assumption: stopwords and non-content tokens contribute minimally to relevance scoring.

### Mechanism 3: Two-Stage Score Fusion
Combining single-vector coarse scores with multi-vector fine-grained scores stabilizes ranking across candidate quality tiers. Stage 1 fusion: S*_SV = α·S_SV + (1−α)·S_SV. Stage 2 fusion: S*_MV = β·S*_SV + (1−β)·S_MV. Default α=0.1, β=0.3. Core assumption: single-vector scores provide useful priors even when multi-vector scores are computed.

## Foundational Learning

- **Concept: Multi-Vector Retrieval (MaxSim)**
  - Why needed: HEAVEN's Stage 2 relies on ColQwen2.5's late interaction; understanding S_MV = Σ_i max_j ⟨E_q^(i), E_P^(j)⟩ is essential for grasping why query filtering works.
  - Quick check: Given a query with 22 tokens and a page with 256 patches, how many dot products does standard MaxSim require? (Answer: 22 × 256 = 5,632)

- **Concept: Document Layout Analysis (DLA)**
  - Why needed: VS-page construction depends on DLA extracting title layouts; understanding failure modes informs robustness expectations.
  - Quick check: What happens to a VS-page if DLA fails to detect any title layouts in a 10-page document segment?

- **Concept: Recall@K as Coarse-to-Fine Metric**
  - Why needed: The paper's key insight is that single-vector suffices for Recall@200 but not Recall@1; understanding this gradient justifies the two-stage design.
  - Quick check: If single-vector achieves 99.37% of multi-vector Recall@200 but only 77.5% at Recall@1, where should you focus efficiency optimizations?

## Architecture Onboarding

- **Component map**: Corpus → [DLA → VS-Page Construction] → VS-Page Index → Query → [Single-Vector Retrieval on VS-Pages] → Candidate VS-Pages → [Page Expansion Γ] → Candidate Pages → [Refinement Score S*_SV] → Top-K Pages → [POS Tagging → Key Token Filter] → [Multi-Vector Reranking on Key Tokens] → [Final Reranking on All Tokens + Score Fusion] → Ranked Results

- **Critical path**: Query → VS-page scoring → Page refinement → Key token filtering → Multi-vector reranking → Final fusion. Latency bottleneck is Stage 2 multi-vector computation on K×p2 pages.

- **Design tradeoffs**:
  - r (reduction factor): Higher r = more compression, faster Stage 1, but risk of missing relevant pages in coarse retrieval
  - p1, p2 (filtering ratios): Aggressive filtering improves speed but reduces recall ceiling
  - α, β (fusion weights): Domain-dependent; M3DocVQA uses α=0.4, β=0.4 vs. defaults 0.1, 0.3

- **Failure signatures**:
  - Recall@1 drops sharply but Recall@200 stable → VS-page construction failing, check DLA output
  - Latency unchanged after enabling query filtering → POS tagging not applied correctly, verify key token ratio is ~30%
  - Stage 1 outperforms Stage 2 → Multi-vector model not loaded correctly or embedding dimension mismatch

- **First 3 experiments**:
  1. Baseline reproduction: Run HEAVEN vs. ColQwen2.5 vs. DSE on VIMDOC; verify 99.87% Recall@1 retention and 99.82% FLOPs reduction
  2. Ablation sweep: Disable VS-pages, query filtering, and refinement individually; confirm each contributes independently
  3. Domain stress test: Apply HEAVEN to documents with minimal text (pure charts/scans) and documents with dense text; observe where DLA-based VS-page construction degrades

## Open Questions the Paper Calls Out

- **Question:** How does HEAVEN perform within an end-to-end Retrieval-Augmented Generation (RAG) pipeline for visually rich documents?
  - Basis: The Limitations section states that HEAVEN "focuses on retrieval efficiency and does not yet integrate retrieval augmentation generation, which we leave as future work."
  - Why unresolved: Current evaluation relies solely on retrieval metrics and does not measure impact on downstream generation tasks.
  - Evidence needed: Benchmarks measuring end-to-end answer quality on Visual QA tasks when using HEAVEN as the retriever.

- **Question:** How does the efficiency-accuracy trade-off shift when scaling HEAVEN to larger vision-language model backbones?
  - Basis: The authors note that the method "relies on pretrained vision-language encoders, and its performance may vary with model scale."
  - Why unresolved: Experiments were restricted to specific model sizes, leaving behavior on larger architectures untested.
  - Evidence needed: Analysis of latency and Recall@1 trends when implementing HEAVEN using larger backbone models.

- **Question:** How robust is the VS-page construction mechanism when applied to documents with highly irregular or noisy visual layouts?
  - Basis: The Limitations section highlights that VS-pages "depend on document layout analysis, which can be sensitive to noisy or irregular layouts."
  - Why unresolved: Failure cases where DocLayout-YOLO misidentifies regions on degraded or non-standard document structures are not quantified.
  - Evidence needed: Performance evaluation on a curated dataset of noisy documents comparing VS-page failure rates against standard single-vector retrieval.

## Limitations
- VS-page construction relies on Document Layout Analysis, which may fail on documents without clear title layouts
- Performance claims based on limited samples across four benchmarks may not capture full variability of real-world document collections
- Computational savings don't account for preprocessing overhead or memory constraints at scale

## Confidence

- **High Confidence**: Core two-stage hybrid retrieval mechanism is well-supported by ablation studies and achieves state-of-the-art results across all four benchmarks
- **Medium Confidence**: VS-page construction reliability depends on Document Layout Analysis quality, which may vary with irregular document layouts
- **Low Confidence**: Generalizability to documents with minimal text content (pure figures, charts) is not evaluated

## Next Checks

1. **Cross-Domain Robustness Test**: Apply HEAVEN to image-heavy documents and measure performance degradation compared to text-heavy documents. Track DLA failure rates and evaluate alternative VS-page construction methods.

2. **Memory-Constrained Scaling**: Implement HEAVEN on a corpus exceeding available GPU memory and measure VS-page construction time, query latency degradation, and whether disk-based indexing is required.

3. **Fusion Hyperparameter Sensitivity**: Systematically vary α and β across [0,1] range on each benchmark, measuring Recall@1, FLOPs, and latency. Generate Pareto frontiers to identify optimal values.