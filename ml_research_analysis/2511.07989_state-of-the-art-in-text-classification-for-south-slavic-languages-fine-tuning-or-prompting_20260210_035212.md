---
ver: rpa2
title: 'State of the Art in Text Classification for South Slavic Languages: Fine-Tuning
  or Prompting?'
arxiv_id: '2511.07989'
source_url: https://arxiv.org/abs/2511.07989
tags:
- classification
- llms
- topic
- languages
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) were compared with fine-tuned BERT
  models on South Slavic text classification tasks. Zero-shot prompting was used to
  evaluate sentiment, topic, and genre classification across parliamentary speeches,
  news, and web text domains.
---

# State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?

## Quick Facts
- arXiv ID: 2511.07989
- Source URL: https://arxiv.org/abs/2511.07989
- Reference count: 0
- Large language models (LLMs) achieved comparable or higher performance than fine-tuned BERT models on South Slavic text classification tasks using zero-shot prompting.

## Executive Summary
This study compares large language models (LLMs) with fine-tuned BERT models for text classification tasks in South Slavic languages. The evaluation covers sentiment, topic, and genre classification across parliamentary speeches, news, and web text domains using zero-shot prompting. Results show that LLMs, particularly closed-source models like GPT-4o, GPT-5, and Gemini 2.5, achieved performance comparable to or exceeding fine-tuned BERT models, with only minor performance differences between English and South Slavic languages (2–10 F1 points). While LLMs demonstrated strong generalization, they had slower inference times and occasionally produced invalid labels, making fine-tuned BERT models more practical for large-scale annotation tasks.

## Method Summary
The study evaluated text classification performance using zero-shot prompting with large language models (LLMs) across South Slavic languages. Models tested included both open-source and closed-source LLMs (GPT-4o, GPT-5, Gemini 2.5) compared against fine-tuned BERT models. Classification tasks covered sentiment analysis, topic classification, and genre classification using datasets from parliamentary speeches, news, and web text domains. Performance was measured using F1 scores, and comparisons were made between English and South Slavic language variants.

## Key Results
- LLMs achieved comparable or higher performance than fine-tuned BERT models in South Slavic text classification tasks.
- Closed-source LLMs (GPT-4o, GPT-5, Gemini 2.5) generally outperformed open-source models.
- Performance differences between English and South Slavic languages were minimal (2–10 F1 points).

## Why This Works (Mechanism)
The strong performance of LLMs in South Slavic text classification is attributed to their ability to generalize across languages with minimal performance degradation. Zero-shot prompting enables LLMs to perform classification tasks without task-specific training, leveraging their pre-trained multilingual capabilities. The closed-source models benefit from larger parameter counts and more extensive training data, leading to superior performance compared to open-source alternatives.

## Foundational Learning
- **Multilingual Text Classification**: Understanding how language models perform across different languages is crucial for evaluating cross-linguistic generalization.
  - *Why needed*: To assess the model's ability to handle language-specific nuances and domain variations.
  - *Quick check*: Compare F1 scores between English and South Slavic languages to quantify performance differences.

- **Zero-Shot Prompting**: The ability to perform tasks without task-specific training data is a key advantage of LLMs.
  - *Why needed*: To evaluate the practicality and effectiveness of using pre-trained models for new tasks.
  - *Quick check*: Measure performance using zero-shot prompting versus fine-tuning approaches.

- **Model Architecture Comparison**: Understanding the differences between fine-tuned BERT models and LLMs is essential for interpreting results.
  - *Why needed*: To identify which model type is more suitable for specific use cases based on performance and practical considerations.
  - *Quick check*: Analyze inference times and error rates to determine trade-offs between model types.

## Architecture Onboarding

**Component Map**: Input Text -> LLM/BERT Model -> Classification Output

**Critical Path**: Text preprocessing -> Model inference -> Label generation -> Performance evaluation

**Design Tradeoffs**: 
- LLMs offer strong generalization and no training data requirement but have slower inference and occasional invalid label generation.
- Fine-tuned BERT models provide faster inference and reliable labels but require task-specific training data.

**Failure Signatures**: 
- LLMs: Invalid label generation, slower inference times.
- BERT: Potential overfitting to training data, limited generalization to new domains.

**First Experiments**:
1. Evaluate F1 scores for sentiment classification in parliamentary speeches using zero-shot prompting with GPT-4o.
2. Compare inference times for topic classification in news text between GPT-5 and fine-tuned BERT models.
3. Assess the frequency of invalid labels generated by Gemini 2.5 in genre classification tasks.

## Open Questions the Paper Calls Out
None

## Limitations
- Zero-shot prompting was used exclusively, without exploring few-shot or chain-of-thought strategies that could yield different performance patterns.
- The study did not quantify the frequency or impact of invalid labels produced by LLMs.
- Inference times of LLMs versus fine-tuned BERT models were not benchmarked for large-scale annotation tasks.

## Confidence
- **High**: Closed-source LLMs (GPT-4o, GPT-5, Gemini 2.5) consistently outperformed fine-tuned BERT models in South Slavic text classification tasks.
- **Medium**: Open-source LLMs showed competitive performance but could benefit from improved prompting strategies.
- **High**: The observation that language differences between English and South Slavic languages result in only a 2–10 F1 point gap is well-supported by systematic evaluation.

## Next Checks
1. Quantify the frequency and impact of invalid labels produced by LLMs in text classification tasks.
2. Benchmark inference times of LLMs versus fine-tuned BERT models for large-scale annotation tasks.
3. Test few-shot prompting and chain-of-thought strategies to determine if performance can be further improved.