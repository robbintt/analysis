---
ver: rpa2
title: 'Tgea: An error-annotated dataset and benchmark tasks for text generation from
  pretrained language models'
arxiv_id: '2503.04232'
source_url: https://arxiv.org/abs/2503.04232
tags:
- error
- text
- erroneous
- language
- texts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TGEA is the first dataset with comprehensive annotations for errors
  in texts generated by pretrained language models. It includes 47K sentences with
  12K erroneous ones, annotated for 24 types of errors.
---

# Tgea: An error-annotated dataset and benchmark tasks for text generation from pretrained language models

## Quick Facts
- arXiv ID: 2503.04232
- Source URL: https://arxiv.org/abs/2503.04232
- Authors: Jie He; Bo Peng; Yi Liao; Qun Liu; Deyi Xiong
- Reference count: 30
- Key outcome: First comprehensive error-annotated dataset for PLM-generated text with 47K sentences, 12K erroneous ones, 24 error types, and 5 benchmark tasks showing current models struggle with complex error types

## Executive Summary
TGEA introduces the first large-scale dataset with comprehensive annotations for errors in texts generated by pretrained language models (PLMs). The dataset contains 47,058 Chinese sentences generated by NEZHA-Gen, with 12,000 annotated as erroneous across 24 error types organized in a two-level taxonomy. Five benchmark tasks are defined: erroneous text detection, erroneous and associated span detection, error type classification, error correction, and rationale generation. Results show that all tasks are challenging, with current models achieving very low performance, particularly on complex discourse and commonsense errors. The dataset provides a foundation for diagnosing PLM generation failures and developing interpretable error correction systems.

## Method Summary
TGEA is constructed through a five-stage human annotation pipeline on Chinese text generated by NEZHA-Gen (a GPT-2 variant). The process begins with prompt-word sampling (top-k=30) to generate diverse sentences, followed by human annotation for erroneous text detection, span marking (erroneous and associated spans), minimal correction, error type classification, and rationale generation. The dataset includes 47,058 sentences split 8:1:1 for train/dev/test, with 25.5% containing errors. Five benchmark tasks are defined with PLM baselines using BERT-wwm-ext, RoBERTa-wwm-ext-large, and ALBERT-Chinese-large architectures, evaluated with standard metrics including F1, Class-F1, F0.5 (MaxMatch), BLEU, ROUGE-L, and BERTScore.

## Key Results
- Erroneous text detection: 66.26% accuracy, 58.55% F1 (RoBERTa-wwm-ext-large)
- Erroneous span detection: 27.66% Class-F1, associated span detection: 27.00% Class-F1
- Error type classification: 70.90% accuracy, 58.79% F1
- Error correction: F0.5 < 1.0% (extremely low performance)
- Rationale generation: BLEU 0.06%, BERTScore 56.17% (poor performance)

## Why This Works (Mechanism)

### Mechanism 1: Two-Level Error Taxonomy Enables Systematic PLM Diagnostics
The hierarchical taxonomy (5 level-1 types, 24 level-2 subtypes) provides a structured framework for diagnosing PLM generation failures beyond surface-level grammar. By categorizing errors into inappropriate combination, missing, redundancy, discourse, and commonsense types, the taxonomy maps linguistic and knowledge-based failure modes. This allows researchers to identify which capabilities (syntactic, discourse, world knowledge) require improvement rather than treating all errors uniformly. The taxonomy shows good coverage with only 5.7% "other" errors.

### Mechanism 2: Associated Spans Capture Long-Distance Error Dependencies
Annotating both erroneous spans and their causally-related "associated spans" enables detection of non-local reasoning failures that standard token-level GEC approaches miss. An erroneous word often depends on context elsewhere in the sentence (e.g., subject-verb agreement errors). By explicitly marking associated spans averaging 4.27 tokens at 7.03 token distance, the dataset provides training signal for models to learn these dependencies rather than treating errors as isolated token corrections.

### Mechanism 3: Rationale Annotations Enable Interpretable Error Correction
Human-provided rationale explanations for each error create a path toward interpretable, explainable error correction systems rather than black-box fixes. Annotators explain why each span is erroneous (e.g., "credit rating should be lower, not higher, for investor concern"). Fine-tuning generation models on rationale-target pairs (8.74 tokens average) enables systems that both correct errors and justify corrections, improving trust and enabling human verification.

## Foundational Learning

- **Concept: Grammatical Error Correction (GEC) Task Formulation**
  - **Why needed here:** TGEA extends GEC from human-written to PLM-generated text. Understanding GEC's sequence-to-sequence formulation, edit operations (insert, delete, replace), and evaluation metrics (M2-scorer, F0.5) is prerequisite for interpreting benchmark results.
  - **Quick check question:** Can you explain why F0.5 (favoring precision) is used for GEC evaluation rather than F1?

- **Concept: Sequence Labeling / Token Classification**
  - **Why needed here:** The erroneous/associated span detection task uses NER-style BIO tagging. Understanding CRF layers, BIO encoding schemes, and class-imbalance challenges in token classification is necessary to implement and improve baselines.
  - **Quick check question:** Given class-F1 of ~28% for erroneous spans and ~27% for associated spans, what architectural changes might improve joint detection?

- **Concept: PLM Text Generation Decoding Strategies**
  - **Why needed here:** Error distributions depend on decoding hyperparameters. The paper analyzes how top-k sampling affects error types (k=1 produces 95%+ redundancy errors; k=30 yields more balanced distributions). Understanding temperature, nucleus sampling, and repetition penalties is essential for interpreting results.
  - **Quick check question:** Why would low-k sampling produce more repetition errors, and how might this inform prompt engineering for error collection?

## Architecture Onboarding

- **Component map:**
NEZHA-Gen (GPT-2 Chinese) → Prompt-word sampling → Text filtering → Human annotation (5 stages) → Quality control → Train/Dev/Test split → 5 benchmark tasks with PLM baselines

- **Critical path:** The annotation pipeline is the bottleneck. Each sentence passes through 5 sequential annotation stages with reviewer verification. Data quality depends on: (1) worker training accuracy >90%, (2) iterative amendment cycles, (3) IAA validation (87.5% for detection, 73.3% for classification).

- **Design tradeoffs:**
  - **Single-model source (GPT-2)** vs. multi-model collection: Limits generalizability but ensures annotation consistency; paper argues trade-off is acceptable given GPT-2's status
  - **Top-k=30 sampling** vs. nucleus/temperature: Balances error diversity against text coherence; lower k increases redundancy errors dramatically
  - **Mid-frequency prompt words** (40-60%): Deliberately avoids high-frequency prompts that produce too many correct sentences

- **Failure signatures:**
  - **Span detection failure:** Class-F1 ~27-28% (near-random for associated spans) suggests models cannot identify error context
  - **Correction failure:** F0.5 <1% indicates GEC models trained on human errors cannot transfer to PLM errors
  - **Rationale generation failure:** BLEU ~0.06% with over-generation (longer than reference) suggests format mismatch
  - **Discourse/commonsense errors:** 29% of errors (vs. rare in GEC datasets) may explain baseline failures—current models lack mechanisms for these error types

- **First 3 experiments:**
  1. **Establish baseline on erroneous text detection** using RoBERTa-wwm-ext-large with balanced positive/negative sampling; target >67% accuracy (current best); analyze confusion matrix for systematic biases
  2. **Probe span detection failure modes:** Train separate erroneous-span-only and associated-span-only models, compare to joint detection (paper reports joint helps: 27.66% vs. 26.42% erroneous-only). Investigate whether attention weights align with ground-truth associations
  3. **Reformulate rationale generation as multiple-choice QA** (paper's suggestion): Construct candidate rationales (1 correct + 3 distractors) and evaluate as classification; this bypasses length-mismatch issues affecting BLEU

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the performance of rationale generation be significantly improved by reformulating the task as a multi-choice question answering problem rather than a direct text generation problem?
- **Basis in paper:** [explicit] The authors state in Section 5.5, "We suggest that this generation task could be reformulated as a multi-choice question answering task by providing alternative rationales as distractors... We leave this to our future work."
- **Why unresolved:** The current baseline uses a generative approach (NEZHA-Gen) which yields low metric scores, but the authors have not tested the proposed alternative classification-style formulation.
- **What evidence would resolve it:** Experimental results comparing the BLEU/ROUGE scores and human evaluations of the current generative baseline against a new model trained to select the correct rationale from a set of candidates.

### Open Question 2
- **Question:** Does the error type distribution and task difficulty observed in TGEA generalize to texts generated by other PLMs (e.g., GPT-3) or different languages?
- **Basis in paper:** [explicit] Section 6 discusses the limitation of using a single model source, stating, "A straightforward way to mitigate this issue is to collect raw texts from multiple models with different hyperparameters, neural architectures and text genres."
- **Why unresolved:** The dataset is built exclusively on Chinese GPT-2 (NEZHA-Gen). It is unknown if the 24-type taxonomy is sufficient or if the error distribution (e.g., 18.96% commonsense errors) is an artifact of this specific architecture.
- **What evidence would resolve it:** A replication of the TGEA annotation pipeline on texts generated by distinct architectures (e.g., encoder-decoder vs. decoder-only) or in a different language, followed by a statistical comparison of error distributions.

### Open Question 3
- **Question:** To what extent is the failure of current GEC models on TGEA attributed to the dataset size versus the complexity of "commonsense" and "discourse" error types?
- **Basis in paper:** [inferred] In Section 5.4, the authors speculate that the poor performance (F0.5 < 1%) is due to the dataset being small and containing error types very different from standard GEC datasets (which lack complex reasoning errors).
- **Why unresolved:** It is unclear if scaling up standard GEC architectures with more data would solve the problem, or if new architectures capable of reasoning and inter-sentential dependency modeling are required.
- **What evidence would resolve it:** An ablation study training standard GEC models on a larger version of TGEA, compared against a model augmented with external knowledge retrieval or discourse parsing modules.

## Limitations

- **Taxonomy generalizability:** The error taxonomy was developed from GPT-2-generated Chinese text and may not capture error patterns in instruction-tuned models or multilingual contexts
- **Annotation quality and bias:** While inter-annotator agreement is reasonable, the paper doesn't address potential cultural/linguistic biases or whether the minimal correction approach captures all error manifestations
- **Task transferability:** Extremely poor baseline performance (F0.5 < 1% for correction) could indicate genuine task difficulty or fundamental mismatch between current PLM architectures and TGEA's error patterns

## Confidence

**High Confidence**: The dataset construction methodology is well-documented and reproducible. The error taxonomy provides reasonable coverage for GPT-2-generated Chinese text, and the five benchmark tasks are clearly defined with appropriate evaluation metrics.

**Medium Confidence**: The claim that TGEA enables "diagnostic evaluation" of PLM capabilities is supported by error distribution analysis (e.g., 29% discourse/commonsense errors vs. rare in GEC datasets). However, the practical utility for model improvement requires further validation.

**Low Confidence**: The assertion that "rationale annotations enable interpretable error correction" is weakly supported by baseline results (BLEU 0.06%). The extremely low performance suggests current models cannot leverage this feature effectively, and the paper acknowledges this limitation without clear solutions.

## Next Checks

1. **Cross-Model Error Pattern Analysis:** Apply the TGEA taxonomy to error-annotated outputs from instruction-tuned models (e.g., ChatGPT, Claude) to quantify taxonomy generalizability. Compare error distributions across models to identify whether autoregressive-specific errors dominate the taxonomy.

2. **Architectural Ablation for Span Detection:** Systematically test whether transformer-based architectures with enhanced context modeling (e.g., Longformer attention, axial positional embeddings) improve associated span detection, which currently shows near-random performance (27.66% F1). Compare joint vs. separate detection approaches with varying context window sizes.

3. **Rationale Generation Task Reformulation:** Implement the paper's suggested multi-choice QA format for rationale generation. Create distractor rationales algorithmically (e.g., by perturbing correct rationales) and evaluate classification accuracy instead of generation metrics. This would determine whether the task is fundamentally difficult or merely mismatched to current evaluation methods.