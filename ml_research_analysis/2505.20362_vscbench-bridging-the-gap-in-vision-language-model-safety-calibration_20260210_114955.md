---
ver: rpa2
title: 'VSCBench: Bridging the Gap in Vision-Language Model Safety Calibration'
arxiv_id: '2505.20362'
source_url: https://arxiv.org/abs/2505.20362
tags:
- safety
- safe
- calibration
- unsafe
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces VSCBench, a dataset for evaluating safety
  calibration in vision-language models, focusing on both undersafety and oversafety.
  The benchmark includes 3,600 image-text pairs across image-centric and text-centric
  scenarios.
---

# VSCBench: Bridging the Gap in Vision-Language Model Safety Calibration

## Quick Facts
- arXiv ID: 2505.20362
- Source URL: https://arxiv.org/abs/2505.20362
- Reference count: 18
- This work introduces VSCBench, a dataset for evaluating safety calibration in vision-language models, focusing on both undersafety and oversafety.

## Executive Summary
This paper addresses the critical issue of safety calibration in vision-language models (VLMs), where models often exhibit either undersafety (responding to unsafe queries) or oversafety (refusing safe queries). The authors introduce VSCBench, a comprehensive benchmark with 3,600 image-text pairs across image-centric and text-centric scenarios, to systematically evaluate this calibration problem. Through experiments on 11 diverse VLMs, they demonstrate that while test-time calibration methods like few-shot learning and internal activation revision can improve safety behavior, they consistently degrade model helpfulness. The study reveals that safety alignment techniques often fail to transfer effectively between textual and multimodal contexts, highlighting the need for more sophisticated calibration approaches.

## Method Summary
The study introduces VSCBench, a benchmark comprising 3,600 image-text pairs split into 1,800 image-centric pairs (6 safety categories with visually similar safe/unsafe pairs) and 1,800 text-centric pairs derived from XSTest using QueryRelevant and FigStep transformations. The evaluation uses GPT-4o as an LLM-as-a-Judge to classify responses into safety categories. The authors test 11 VLMs including proprietary (GPT-4o, Gemini, Claude) and open-weight models (LLaVA, InternVL, DeepSeekVL, VLGuard, SPA-VL) using max_new_tokens=4096. Calibration methods include CoT, prompt engineering, few-shot learning (1-2 demonstrations), and Internal Activation Revision (IAR) at layer 14 with strength 1.50. Safety performance is measured via SRAs (safe query accuracy), SRAu (unsafe query accuracy), and SRAa (overall), while helpfulness is evaluated on POPE and ScienceQA benchmarks.

## Key Results
- VLM safety calibration exhibits severe undersafety or oversafety problems across all tested models, with proprietary models showing higher oversafety and open-weight models showing higher undersafety
- Few-shot learning and IAR effectively improve safety calibration (e.g., 1-shot improves Gemini's Pornography SRAu from 6.3% to 71.2%) but consistently reduce model helpfulness on general tasks
- Textual safety calibration fails to transfer effectively to multimodal scenarios, with open-weight VLMs showing dramatic shifts from safe text performance to severe undersafety in QueryRelevant image-text pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot learning with demonstration pairs can calibrate safety behavior by providing explicit examples of appropriate responses to safe and unsafe queries.
- Mechanism: The model leverages in-context learning to recognize safety boundaries from demonstration pairs, applying this pattern to new queries without parameter updates.
- Core assumption: The model's latent safety knowledge can be activated through minimal supervised signals (1-2 examples).
- Evidence anchors: [abstract] "We found that even though some methods effectively calibrated the models' safety problems..."; [section] "1-shot improves the SRAu of Gemini on Pornography from 6.3% to 71.2%, with only a 2.7% decrease in SRAs."; [corpus] Related work on few-shot calibration (D-TPT) confirms test-time adaptation can improve VLM behavior without training.
- Break condition: When demonstration pairs do not cover the specific safety category or visual modality encountered at inference time.

### Mechanism 2
- Claim: Internal Activation Revision (IAR) steers model activations toward safer outputs by modifying intermediate layer representations during generation.
- Mechanism: Contrastive samples (safe vs. unsafe responses) are used to compute revision vectors via mass mean shift, which are then applied at specific layers to bias the model toward safer behavior.
- Core assumption: Safety-relevant features are localized to specific layers and can be modulated without fine-tuning.
- Evidence anchors: [section] "IAR boosts the SRAs of VLGuard on Discrimination from 22.4% to 55.4%, with a 1.5% increase in SRAs."; [section] "We identify the 14th layer with a strength of 1.50 as the optimal configuration."; [corpus] "Safe Vision-Language Models via Unsafe Weights Manipulation" discusses weight-level interventions for safety, providing converging evidence for activation-level mechanisms.
- Break condition: When revision strength is too high (degrading helpfulness) or when the layer selected does not encode safety-relevant information for the specific category.

### Mechanism 3
- Claim: Safety calibration improvements generally come at the cost of reduced model helpfulness/utility.
- Mechanism: Calibration methods that suppress unsafe responses also suppress legitimate responses to safe but superficially risky queries, narrowing the model's responsive range.
- Core assumption: Safety and helpfulness occupy competing regions of the model's behavioral space.
- Evidence anchors: [abstract] "these methods also lead to the degradation of models' utility"; [section] "All calibration methods except CoT decrease the accuracy of Gemini, Claude, InternVL, and VLGuard on POPE and ScienceQA."; [corpus] Weak corpus evidence—neighboring papers do not explicitly address the safety-utility trade-off; further empirical verification needed.
- Break condition: Assumption may not hold if future methods can achieve fine-grained distinction between safe and unsafe queries at the semantic level.

## Foundational Learning

- Concept: **Safety Calibration**
  - Why needed here: This paper frames safety alignment as a calibration problem (balancing undersafety and oversafety), not just a refusal problem. Understanding this framing is essential for interpreting SRAs/SRAu metrics.
  - Quick check question: If a model refuses all queries, is it well-calibrated? (Answer: No—this is oversafety, as SRAs would be near 0.)

- Concept: **Image-Centric vs. Text-Centric Safety**
  - Why needed here: The benchmark distinguishes between scenarios where the visual modality carries the safety signal (image-centric) and where the text carries the signal with irrelevant images (text-centric).
  - Quick check question: In the "kill a Python process" example, which modality carries the safety-relevant signal? (Answer: Text—the image is a Python program, not a person.)

- Concept: **Contrastive Activation Steering**
  - Why needed here: IAR depends on extracting direction vectors from pairs of safe/unsafe responses to the same prompt. Understanding contrastive methods is prerequisite for implementing or extending IAR.
  - Quick check question: Why collect both safe and unsafe responses to the same harmful instruction? (Answer: To compute the direction in activation space that distinguishes safe from unsafe behavior.)

## Architecture Onboarding

- Component map:
  - **VSCBench**: 3,600 image-text pairs split into image-centric (visually similar safe/unsafe pairs, 6 categories) and text-centric (semantically similar safe/unsafe text with attack-generated images via QueryRelevant/FigStep)
  - **Evaluation pipeline**: GPT-4o as LLM-as-a-Judge to classify responses as safe refusal/warning (correct for unsafe queries) or helpful answer (correct for safe queries)
  - **Calibration methods**: Prompt-based (CoT, PE, few-shot) and activation-based (IAR at layer 14 with strength 1.5)

- Critical path:
  1. Load VSCBench (image-centric and text-centric subsets)
  2. Run inference on target VLM with and without calibration methods
  3. Extract responses and evaluate with GPT-4o classifier
  4. Compute SRAs (safe query accuracy) and SRAu (unsafe query accuracy)
  5. Measure helpfulness degradation on POPE/ScienceQA

- Design tradeoffs:
  - Few-shot vs. IAR: Few-shot requires demonstration pairs at inference time (increased prompt length); IAR requires precomputed revision vectors (offline cost, fixed after extraction)
  - Layer selection for IAR: Earlier layers capture general features; later layers capture task-specific features. Layer 14 was empirically optimal but may not generalize across architectures
  - Calibration strength: Higher IAR strength improves safety but degrades helpfulness more severely

- Failure signatures:
  - **Oversafety pattern**: Low SRAs (<60%) with high SRAu (>90%)—model refuses benign queries (e.g., Claude on image-centric dataset: SRAs=67.7%, SRAu=89.1%)
  - **Undersafety pattern**: High SRAs (>90%) with low SRAu (<40%)—model answers dangerous queries (e.g., LLaVA-7B: SRAs=91.6%, SRAu=30.5%)
  - **Category-specific failures**: Gemini undersafety in Pornography (SRAu=6.3%); safety-aligned models inconsistent across categories
  - **Cross-modal transfer failure**: Models calibrated on text may fail on multimodal inputs (DeepSeekVL: text SRAu=30.8% vs. multimodal shift)

- First 3 experiments:
  1. **Baseline calibration audit**: Run all 11 VLMs on VSCBench with default settings; plot SRAs vs. SRAu to identify oversafe/undersafe clusters
  2. **Few-shot sensitivity**: Test 1-shot and 2-shot on Gemini and Claude across Pornography and Discrimination categories; measure SRAs/SRAu delta and POPE accuracy drop
  3. **IAR layer/strength ablation**: On VLGuard-7B, sweep layers 9, 14, 19, 24 and strengths 1.0, 1.5, 2.0, 2.5; identify Pareto frontier for safety-helpfulness trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can safety calibration be achieved in VLMs without simultaneously degrading the model's general helpfulness and utility?
- Basis in paper: [explicit] The Conclusion states, "In future work, we plan to develop advanced techniques to achieve effective safety calibration without sacrificing performance." Additionally, Section 5 notes that methods like few-shot learning "degrade accuracy more severely" on utility benchmarks.
- Why unresolved: The experiments demonstrated a consistent trade-off; while test-time methods (IAR, few-shot) improved safety calibration (SRA), they caused a noticeable drop in accuracy on general tasks (ScienceQA, POPE).
- What evidence would resolve it: A calibration method that maintains or improves SRA scores on VSCBench while showing no statistically significant degradation in accuracy on standard utility benchmarks compared to the baseline model.

### Open Question 2
- Question: How can evaluation frameworks effectively distinguish between "unsafe" responses and "toxic" responses, particularly in cases of jailbreaks or mixed-content refusals?
- Basis in paper: [explicit] The Limitations section explicitly notes the "No comprehensive evaluation of the model’s toxicity," acknowledging that "a model’s response could contain both a warning of danger and toxic content" which the current methodology does not penalize.
- Why unresolved: The current benchmark classifies a response as "safe" if it refuses or warns, failing to detect if the refusal itself contains toxic language, or if an unsafe response is technically non-toxic (Finding 3).
- What evidence would resolve it: A refined evaluation metric or dataset that separately labels and penalizes toxicity within refusal responses, distinguishing them from clean safety refusals.

### Open Question 3
- Question: Why does safety calibration on textual inputs fail to transfer effectively to multimodal scenarios, specifically in QueryRelevant attack settings?
- Basis in paper: [inferred] Finding 2 observes that "textual calibration does not inherently translate to effective multimodal calibration," noting that open-weight VLMs shift from safe text performance to severe undersafety in QueryRelevant image-text pairs.
- Why unresolved: The paper identifies the phenomenon where visual relevance undermines textual safety alignment, but does not propose a mechanism or solution to bridge this specific modality gap.
- What evidence would resolve it: An analysis of cross-modal attention mechanisms that identifies the "distraction" factor of relevant images, followed by a training intervention that closes the performance gap between text-centric and QueryRelevant scores.

### Open Question 4
- Question: How can safety alignment methods be improved to ensure consistent protection levels across diverse risk categories (e.g., preventing oversafety in Violence while fixing undersafety in Pornography)?
- Basis in paper: [inferred] Finding 1 highlights "inconsistent safety calibration across different risk categories," pointing out that safety-aligned models often exhibit oversafety in domains like Violence but remain vulnerable in domains like Illegal Activities.
- Why unresolved: Current alignment techniques (like DPO or mixed-data SFT) appear to optimize for average safety, resulting in category-specific imbalances rather than uniform robustness across the board.
- What evidence would resolve it: A training curriculum that yields a model with high SRAu (safety on unsafe queries) and high SRAs (safety on safe queries) simultaneously across all six categories in the image-centric dataset, minimizing variance between categories.

## Limitations
- The study's safety evaluation relies heavily on GPT-4o as an LLM-as-a-judge, introducing potential subjectivity and bias in safety classification
- The calibration methods show effectiveness but consistently reduce model helpfulness, suggesting fundamental limitations in current approaches
- The IAR method's optimal layer (14) and strength (1.50) were determined empirically on specific models and may not generalize across architectures

## Confidence
- **High confidence**: The existence of both undersafety and oversafety problems in VLMs is well-supported by the comprehensive evaluation across 11 diverse models and the VSCBench benchmark
- **Medium confidence**: The effectiveness of calibration methods (few-shot, IAR) is demonstrated, but the trade-off with helpfulness and the generalizability of layer/strength parameters remain uncertain
- **Low confidence**: The long-term robustness of these calibration methods against adversarial attacks or distribution shifts has not been established

## Next Checks
1. **Adversarial robustness validation**: Test calibrated models against prompt injection attacks, jailbreak attempts, and out-of-distribution queries to assess whether safety calibration holds under attack
2. **Cross-architecture generalization**: Apply IAR layer 14, strength 1.50 configuration to different VLM architectures (e.g., transformer variants, hybrid models) to verify if the empirically optimal parameters transfer
3. **Human evaluation comparison**: Conduct human evaluation of model responses on a subset of VSCBench to validate GPT-4o's safety classifications and assess the real-world impact of calibration trade-offs on user experience