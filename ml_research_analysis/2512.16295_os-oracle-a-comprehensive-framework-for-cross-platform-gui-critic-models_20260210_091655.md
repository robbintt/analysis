---
ver: rpa2
title: 'OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models'
arxiv_id: '2512.16295'
source_url: https://arxiv.org/abs/2512.16295
tags:
- action
- critic
- arxiv
- task
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OS-Oracle is a comprehensive framework for cross-platform GUI critic
  models. It addresses the scarcity of diverse, high-quality GUI feedback data and
  public critic benchmarks for step-level evaluation in computer use.
---

# OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models

## Quick Facts
- arXiv ID: 2512.16295
- Source URL: https://arxiv.org/abs/2512.16295
- Reference count: 40
- OS-Oracle-7B achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench and improves GUI agent performance when used as pre-critic

## Executive Summary
OS-Oracle addresses the scarcity of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. The framework introduces a scalable data pipeline for synthesizing cross-platform GUI critic data, a two-stage training paradigm combining supervised fine-tuning and consistency-preserving group relative policy optimization, and OS-Critic Bench—a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, the authors curate a high-quality dataset containing 310k critic samples and demonstrate that OS-Oracle-7B achieves state-of-the-art performance among open-source VLMs while surpassing proprietary models on mobile tasks.

## Method Summary
The method involves a two-stage training approach on a 310k-sample dataset synthesized from 7 source datasets across mobile, web, and desktop platforms. The pipeline extracts positive samples from trajectories, generates synthetic negatives via four error-pattern injections (OF, IESR, MTT, IEL), and annotates rationales using GPT-4o. The first stage applies supervised fine-tuning on Qwen2.5-VL-7B-Instruct for 1 epoch. The second stage applies CP-GRPO for 3 epochs with specific reward weights (α=0.9 accuracy, β=0.05 format, γ=0.05 consistency) to optimize both discriminative accuracy and reasoning-judgment consistency. Evaluation uses OS-Critic Bench (738 human-annotated samples) plus dynamic assessment on OSWorld and AndroidWorld.

## Key Results
- OS-Oracle-7B achieves 68.02% accuracy on OS-Critic Bench, outperforming other open-source VLMs
- On mobile domain specifically, OS-Oracle-7B reaches 70.78% accuracy, surpassing proprietary models
- When serving as pre-critic, OS-Oracle-7B improves UI-TARS-1.5-7B performance from 29.2% to 31.0% on AndroidWorld
- CP-GRPO improves reasoning-judgment consistency from 90.51% to 99.59% while maintaining comparable accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic negative samples derived from positive trajectories outperform GPT-annotated negatives for critic training
- Mechanism: The pipeline systematically injects four error types (OF, IESR, MTT, IEL) into positive samples using rule-based perturbations, creating targeted negatives with known error semantics. GPT-4o then annotates rationales given the error category, producing aligned supervision
- Core assumption: Rule-based perturbations capture the failure modes that CUAs actually exhibit in deployment
- Evidence anchors: SFT with OS-Oracle synthetic negatives achieves 60.03 Acc vs 55.42 for GPT-annotated negatives

### Mechanism 2
- Claim: Two-stage training (SFT → CP-GRPO) improves both discriminative accuracy and reasoning-judgment consistency
- Mechanism: SFT establishes baseline discrimination and rationale generation. CP-GRPO then optimizes a composite reward (accuracy 0.9, format 0.05, consistency 0.05) using group-relative policy optimization, encouraging outputs where reasoning polarity matches the final Yes/No judgment
- Core assumption: The consistency reward signal accurately captures reasoning-judgment alignment
- Evidence anchors: CP-GRPO improves consistency from 90.51% to 99.59% while maintaining comparable accuracy

### Mechanism 3
- Claim: Cross-platform training data enables generalization across mobile, web, and desktop critic evaluation
- Mechanism: The dataset (310k samples: ~58% mobile, ~23% desktop, ~19% web) provides diverse UI layouts, interaction modalities, and error patterns. The unified prompt format maintains consistent task framing while accommodating platform-specific action spaces
- Core assumption: Platform-specific interaction patterns share underlying critic-relevant features that transfer across domains
- Evidence anchors: OS-Oracle-7B achieves best open-source results overall and on mobile domain

## Foundational Learning

- **POMDP formulation for GUI tasks**
  - Why needed here: The paper models GUI automation as a POMDP ⟨g,S,A,O,T⟩ where the critic evaluates actions given partial observability. Understanding this framing clarifies why step-level critics matter—errors compound across long horizons
  - Quick check question: Given a task history m_t and observation o_t, what does the critic M_critic output?

- **Pointwise vs pairwise critic models**
  - Why needed here: The paper explicitly chooses pointwise scoring (binary Yes/No per action) over pairwise ranking. Pairwise forces preference even when both options are bad, which is ill-suited for GUI tasks where independent validity assessment is needed
  - Quick check question: Why would pairwise comparison fail if both candidate actions are incorrect?

- **GRPO (Group Relative Policy Optimization) basics**
  - Why needed here: CP-GRPO extends GRPO by adding a consistency reward. GRPO samples multiple outputs for semantically similar queries and computes rewards relative to the group, reducing variance compared to absolute reward baselines
  - Quick check question: In GRPO, how does grouping semantically similar queries affect optimization stability?

## Architecture Onboarding

- **Component map**: Data Pipeline (Positive extraction → Negative synthesis → Rationale annotation) → Model (Qwen2.5-VL-7B-Instruct → SFT checkpoint → CP-GRPO fine-tuning) → Evaluation (OS-Critic Bench + online benchmarks)

- **Critical path**: 1) Collect trajectory datasets (AndroidControl, GUI-Odyssey, Mind2Web, ScaleCUA-Web, AgentNet) 2) Extract positive triplets and synthesize negatives with error-type tags 3) Run GPT-4o rationale annotation 4) SFT for 1 epoch on full 310k dataset 5) CP-GRPO for 3 epochs with specific reward weights 6) Evaluate on OS-Critic Bench and as pre-critic in OSWorld/AndroidWorld

- **Design tradeoffs**: Synthetic vs natural negatives (synthetic provides controllable error coverage; natural negatives are noisy), lexicon vs model-based consistency (lexicon is fast but brittle; model-based fallback adds compute), unified vs platform-specific prompts (unified simplifies training; platform-specific action spaces may require careful prompt engineering)

- **Failure signatures**: Low consistency (>10% gap) indicates GRPO without consistency reward—reasoning and judgment misalign; high mobile, low desktop accuracy suggests platform imbalance; pre-critic degrades agent performance indicates critic may have high false-negative rate

- **First 3 experiments**: 1) Ablate data scale: Train SFT-only models on ±10k, ±50k, ±100k, full dataset to confirm data-volume sensitivity 2) Test CP-GRPO vs GRPO: Compare consistency rates and accuracy on held-out slice to verify the 9% consistency gain 3) Pre-critic integration test: Wrap OS-Oracle-7B as pre-critic for baseline CUA on 50 OSWorld tasks; measure success rate delta and rejection analysis

## Open Questions the Paper Calls Out

- Is the observed trade-off between reasoning-judgment consistency and discriminative accuracy in CP-GRPO fundamental, or can both objectives be optimized simultaneously through alternative reward formulations?

- Does the four-category error taxonomy (OF, IESR, MTT, IEL) comprehensively capture real-world CUA failure modes, or are there significant error patterns not covered by the current synthetic negative construction pipeline?

- Why does GPT-4o degrade CUA task success when used as a pre-critic while OS-Oracle-7B improves it, and what specific knowledge or alignment gaps cause this divergence?

- How does critic model performance scale with base model size and architecture, and is there a minimum scale threshold below which effective GUI criticism becomes infeasible?

## Limitations

- Synthetic Data Quality: The error patterns (OF, IESR, MTT, IEL) may not fully capture real-world GUI agent failures, though they outperform GPT-annotated negatives

- Generalization Across Platforms: Model shows lower performance on web and desktop domains, with training dataset heavily skewed toward mobile (~58%)

- Pre-critic Integration Risks: Mixed results—improving some agents while degrading others—suggesting potential false-negative rates that block valid actions

## Confidence

- **High Confidence**: Two-stage training demonstrably improves consistency from 90.51% to 99.59%; synthetic data clearly outperforms GPT-annotated negatives; OS-Critic Bench evaluation protocol is well-defined
- **Medium Confidence**: Cross-platform generalization claims are supported but limited by mobile data dominance; pre-critic integration results show promise but lack comprehensive failure mode analysis
- **Low Confidence**: Assumption that four synthetic error types capture all relevant GUI failure modes has minimal direct validation; consistency reward's lexicon-based detection may fail on nuanced reasoning

## Next Checks

1. **Natural Failure Analysis**: Collect 100 real GUI agent failures and categorize them by the four error types to measure coverage and identify missing patterns

2. **Platform-Specific Training**: Train separate models on mobile-only, web-only, and desktop-only subsets of the data to validate whether unified training truly generalizes or simply optimizes for mobile

3. **False-Negative Impact Study**: Implement detailed logging of pre-critic rejections during agent execution to analyze rejection patterns and measure impact on task success rates across different agent types