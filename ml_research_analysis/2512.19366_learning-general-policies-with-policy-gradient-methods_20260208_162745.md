---
ver: rpa2
title: Learning General Policies with Policy Gradient Methods
arxiv_id: '2512.19366'
source_url: https://arxiv.org/abs/2512.19366
tags:
- policies
- policy
- learning
- planning
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether policy gradient methods can learn
  general policies that generalize across planning domains. The authors propose using
  graph neural networks (GNNs) to represent policies as state transition classifiers,
  addressing the challenge that ground actions change across instances while predicates
  remain fixed.
---

# Learning General Policies with Policy Gradient Methods

## Quick Facts
- **arXiv ID:** 2512.19366
- **Source URL:** https://arxiv.org/abs/2512.19366
- **Reference count:** 15
- **Primary result:** Nearly perfect generalization in 6 out of 10 classical planning domains using policy gradient methods with GNNs

## Executive Summary
This paper investigates whether policy gradient methods can learn general policies that generalize across planning domains. The authors propose using graph neural networks (GNNs) to represent policies as state transition classifiers, addressing the challenge that ground actions change across instances while predicates remain fixed. Experimental results on 10 classical planning domains show nearly perfect generalization in 6 domains out of 10, with 81% coverage overall. The work demonstrates that deep reinforcement learning can learn general policies in classical planning domains, with limitations arising from representation issues rather than the RL algorithms themselves.

## Method Summary
The method adapts actor-critic algorithms for generalized planning by representing policies as classifiers over state transitions rather than actions. A shared GNN architecture computes object embeddings from relational state representations, which are then used by separate value and policy readout networks. The policy π(s'|s) outputs a distribution over successor states rather than actions, and training samples individual state transitions rather than full trajectories. The approach uses AC-1 (model-free) or AC-M (model-based) variants, with the critic approximating the value function and the actor following a policy gradient update. Training is performed on small instances with reachable state spaces up to 200K transitions.

## Key Results
- Achieved 81% coverage across 10 classical planning domains
- Nearly perfect generalization (100% coverage) in 6 out of 10 domains
- Plan quality ratios close to 1.0 (optimal) in many successful cases
- Single-step trajectory sampling (T=1) outperformed longer trajectories for generalization

## Why This Works (Mechanism)

### Mechanism 1: State Transition Classification via GNNs
Using Graph Neural Networks to classify state transitions (good vs. bad) allows policies to generalize across planning instances with varying object sets. The GNN operates on relational structures rather than fixed-size state vectors, computing object embeddings through iterative message passing where information flows from objects to atoms and back. These embeddings are aggregated via readout functions to produce value estimates V(s) and policy logits π(s'|s). This architecture is permutation invariant and scales to arbitrary numbers of objects.

### Mechanism 2: Actor-Critic with Sampled State Transitions
Standard actor-critic algorithms can learn general policies by sampling individual state transitions rather than full trajectories, using successor states rather than actions as the policy output. The policy π outputs a distribution over successor states s' given current state s. The actor update follows θ := θ - α[C + V(s') - V(s)]∇ln π(s'|s), where the critic V approximates the value function. The temporal difference error δ = 1 + γV(s') - V(s) serves as a baseline-corrected advantage estimate.

### Mechanism 3: Derived Predicates and Alternative Cost Structures
Representation limitations (GNN expressiveness) and the optimality-generalization tradeoff can be addressed by extending states with derived predicates and modifying the cost structure to optimize for goal-reaching probability rather than expected cost. When base predicates cannot express necessary features (e.g., "package in correct city"), derived predicates are added as additional atoms in the state representation. When optimal policies don't generalize, the cost structure is modified to optimize for goal-reaching probability.

## Foundational Learning

- **Concept: Classical Planning State Representation**
  - Why needed here: Understanding how planning states are represented as sets of ground atoms over domain predicates is essential for comprehending the GNN input structure and why policies must be transition classifiers rather than action selectors.
  - Quick check question: Given a Blocksworld state with blocks {A, B, C} and atoms {on(A,B), on(B,Table), clear(A), clear(C)}, what would the GNN receive as input?

- **Concept: Bellman Equations and Temporal Difference Learning**
  - Why needed here: The actor-critic algorithms are derived as approximations of dynamic programming methods, specifically using TD errors for both policy improvement and value function updates.
  - Quick check question: In the update ω := ω + βδ∇V(S) where δ = 1 + γV(s') - V(s), why is this called a "semi-gradient" method and what assumption does it make?

- **Concept: GNN Expressiveness and the C2 Fragment**
  - Why needed here: Understanding why GNNs fail on certain domains (Logistics, Grid) requires knowing that message-passing GNNs can only express features in the two-variable fragment of first-order logic with counting.
  - Quick check question: If a domain requires determining "is there a path of length 3 between any two objects," could a 2-layer GNN with sufficient width express this feature?

## Architecture Onboarding

- **Component map:**
  ```
  Training Instance → State Sampling → GNN Encoder → Object Embeddings
                                                              ↓
                          ┌──────────────────────────────────┴──────────────────────────────────┐
                          ↓                                                                    ↓
                   Value Readout (MLP)                                               Policy Readout (MLP)
                    outputs V(s)                                               outputs logits(s', s) for all s' ∈ N(s)
                          ↓                                                                    ↓
                   Critic Loss (Bellman)                                         Actor Loss (Policy Gradient)
                          ↓                                                                    ↓
                          └──────────────────────────────────┬──────────────────────────────────┘
                                                             ↓
                                                    Gradient Updates (Adam)
  ```

- **Critical path:**
  1. Instance generation: Create training/validation instances with reachable state spaces (max 200K transitions)
  2. State sampling: Sample MDP index uniformly, then sample non-goal state S from that MDP
  3. For AC-1: Sample successor S' ~ π(·|S). For AC-M: iterate over all successors
  4. Forward pass through shared GNN to get object embeddings f_L(o)
  5. Compute V(S) via value readout MLP on aggregated embeddings
  6. Compute π(s'|S) via policy readout MLP followed by softmax over successors
  7. Compute TD error δ and update both networks
  8. Validate periodically; select model with best policy evaluation on validation set

- **Design tradeoffs:**
  - **AC-1 vs AC-M**: AC-1 is model-free (samples single successor) but noisier; AC-M uses full model (iterates all successors) and converges faster but requires knowing the transition model
  - **Stochastic vs Deterministic evaluation**: Stochastic policies can escape cycles; deterministic policies require cycle detection via visited set
  - **GNN depth L**: Deeper networks compute longer distances but increase memory and training time; L=30 was used, limiting distance computation to 30 hops
  - **Trajectory length T**: Longer trajectories were found detrimental (Figure 1); T=1 performs best for generalization across instances without privileged initial states

- **Failure signatures:**
  - **GNN expressiveness failure**: Policy fails on large instances in domains like Logistics/Grid; V^π vastly exceeds V^π* on validation; solution is derived predicates
  - **Optimality-generalization tradeoff**: Policy achieves reasonable coverage but with poor plan quality (PQ >> 1.0) or fails on instances requiring non-greedy optimal choices; solution is alternative cost structure
  - **Dead-end undersampling**: Policy fails when test instances have more objects than training (Spanner); solution is tabular policy evaluation to identify dead-ends or increased dead-end sampling

- **First 3 experiments:**
  1. **Baseline replication**: Implement AC-1 on Blocks domain with k=64, L=30, training instances with 4-7 blocks, validation with 7 blocks. Target: 100% coverage on test instances with 8-17 blocks, plan quality ratio ~1.0. This validates the core mechanism works.
  2. **Expressiveness stress test**: Train on Logistics without derived predicates. Observe near-zero coverage. Add derived predicate "package-in-correct-city" and verify coverage increases to ~91%. This demonstrates the GNN expressiveness limitation and the derived predicate fix.
  3. **Trajectory length ablation**: On Gripper domain (training up to 9 balls, validation 10 balls), compare T∈{1,4,8,12,14}. Replicate Figure 1 showing expected policy cost increases with trajectory length. This validates the design choice of single-transition sampling.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the combination of derived predicates and alternative cost structures extend the success in Logistics to the Grid domain?
- **Basis in paper:** [explicit] The authors state they "expect to obtain similar results in Grid with these two extensions, but we have not achieved them yet because the instance generator produces too many unsolvable or trivial instances."
- **Why unresolved:** The specific instance generation for Grid creates data issues (unsolvable/trivial instances) that prevent the experimental validation of the theoretical fix.
- **What evidence would resolve it:** Successful training and 100% test coverage on a refined Grid dataset using the proposed derived predicates and modified cost structure.

### Open Question 2
- **Question:** How can actor-critic methods be adapted to reliably handle domains with rare dead-end states without relying on tabular methods?
- **Basis in paper:** [inferred] In the Spanner domain, the neural actor-critic failed because it did not sample dead-end states enough, whereas a tabular version succeeded (but cannot scale).
- **Why unresolved:** Standard stochastic sampling in deep RL may under-represent critical but rare states (dead-ends) necessary for learning robust avoidance behaviors in generalized planning.
- **What evidence would resolve it:** A deep RL sampling strategy (e.g., prioritized experience replay) that achieves 100% coverage in Spanner on large instances.

### Open Question 3
- **Question:** Why does sampling single state transitions (T=1) outperform sampling longer trajectories for learning general policies?
- **Basis in paper:** [inferred] The authors note that sampling longer trajectories was "detrimental to performance," yet they do not provide a theoretical justification for why breaking the trajectory correlation works better in this setting.
- **Why unresolved:** This contradicts standard RL practices where temporal correlations are often useful; the result suggests a unique property of the generalized planning state space distribution.
- **What evidence would resolve it:** An ablation study or theoretical analysis showing that T=1 reduces overfitting to specific instance structures or improves gradient variance.

## Limitations

- GNN expressiveness constraints (C2 logic limitation) require manual intervention with derived predicates for domains needing longer-range reasoning
- The optimality-generalization tradeoff remains unresolved - optimal policies often don't generalize while approximate policies may achieve better coverage
- The approach relies on manual domain engineering (derived predicates, cost structure modifications) rather than automatic discovery of these fixes

## Confidence

- **High Confidence**: The core mechanism of using GNNs for state transition classification works as described, with clear empirical validation showing 81% coverage across 10 domains
- **Medium Confidence**: The identification of GNN expressiveness limitations and proposed solutions (derived predicates) is well-founded, though the manual engineering aspect limits generalizability
- **Medium Confidence**: The actor-critic adaptation for single-transition sampling is theoretically sound, but the optimality-generalization tradeoff discussion lacks comprehensive empirical validation

## Next Checks

1. **Expressiveness Stress Test**: Systematically evaluate GNN performance across domains with varying C2/C3 expressiveness requirements to quantify the exact boundary where derived predicates become necessary
2. **Automatic Predicate Discovery**: Implement and test methods for automatically identifying which derived predicates are needed rather than manual specification, to address the engineering limitation
3. **Optimality-Generalization Benchmark**: Design experiments that explicitly measure the tradeoff between plan quality and generalization coverage, testing whether the proposed alternative cost structure consistently resolves this tension across multiple domains