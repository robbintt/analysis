---
ver: rpa2
title: 'ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning,
  and Decoding the Curse of Multilinguality'
arxiv_id: '2510.22037'
source_url: https://arxiv.org/abs/2510.22037
tags:
- language
- scaling
- languages
- e-03
- e-04
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the gap in scaling law research for multilingual\
  \ AI systems, focusing on the largest multilingual scaling study to date. The authors\
  \ introduce the Adaptive Transfer Scaling Law (ATLAS), which models both monolingual\
  \ and multilingual pretraining more effectively than prior methods, with improved\
  \ out-of-sample generalization (R\xB2 gains often over 0.3)."
---

# ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality

## Quick Facts
- **arXiv ID:** 2510.22037
- **Source URL:** https://arxiv.org/abs/2510.22037
- **Reference count:** 40
- **Key outcome:** Introduces ATLAS scaling law for multilingual pretraining with improved out-of-sample generalization (R² gains often over 0.3) and quantifies the curse of multilinguality.

## Executive Summary
This work addresses the gap in scaling law research for multilingual AI systems by introducing the Adaptive Transfer Scaling Law (ATLAS), which models both monolingual and multilingual pretraining more effectively than prior methods. The authors present the largest multilingual scaling study to date, spanning 774 training runs across 10M–8B parameters and 400+ training languages. Key contributions include a cross-lingual transfer matrix empirically measuring mutual benefit scores between 1,444 language pairs, a language-agnostic scaling law that guides optimal scaling of model size and data when adding languages, and a formula identifying when to pretrain from scratch versus finetune from multilingual checkpoints.

## Method Summary
The study introduces ATLAS, an adaptive transfer scaling law that models per-language effective data exposure and cross-lingual transfer in multilingual pretraining. Using MADLAD-400 dataset (400+ languages, 48 evaluation languages) and Flores-101 as parallel test set, the authors train models from 10M–8B parameters with vocabulary-insensitive loss. The methodology involves 774 experiments: 280 monolingual, 240 bilingual, 120 multilingual mixtures, and 130 finetunes. The ATLAS law (L = E + A/N^α + B/D_eff^β with saturation function) is fitted alongside a bilingual transfer score (BTS) matrix for 1,444 language pairs and a curse of multilinguality law. Training uses WSD learning rate scheduling, AdamW optimizer, and UNIMAX sampling for multilingual training.

## Key Results
- ATLAS scaling law achieves improved out-of-sample generalization with R² gains often over 0.3 compared to prior methods
- Empirical 1,444 language-pair transfer matrix quantifies cross-lingual mutual benefit scores, revealing how language similarity and script sharing influence transfer
- Identifies compute-optimal pretrain vs finetune thresholds, with crossover points ranging from 144B–283B tokens depending on target language
- Quantifies the curse of multilinguality, showing how adding more languages impacts model capacity requirements

## Why This Works (Mechanism)
The paper does not explicitly detail the mechanism behind why ATLAS works more effectively than prior scaling laws, though it demonstrates improved performance through empirical validation.

## Foundational Learning
- **Multilingual Scaling Laws:** Understanding how model performance scales with parameters and data across multiple languages is essential for efficient model development. *Quick check:* Verify that the scaling exponents (α, β) differ significantly from monolingual baselines.
- **Cross-Lingual Transfer:** Measuring mutual benefit between language pairs through the BTS matrix enables informed decisions about which languages to include together. *Quick check:* Confirm that related languages (e.g., Spanish-Portuguese) show higher BTS scores than distant pairs.
- **Curse of Multilinguality:** The phenomenon where adding more languages requires disproportionate increases in model capacity. *Quick check:* Validate that the K^φ term in the scaling law increases with language count.
- **Vocabulary-Insensitive Loss:** Using Tao et al.'s formulation prevents tokenization bias when comparing across different language sets. *Quick check:* Ensure loss values are comparable across experiments with different vocabulary sizes.
- **Compute-Optimal Pretraining:** Determining when to pretrain from scratch versus finetune from existing checkpoints. *Quick check:* Verify crossover points align with the 144B–283B token range for different target languages.

## Architecture Onboarding

**Component Map:**
MADLAD-400 dataset -> 64k SentencePiece vocab -> 774 training runs (monolingual/bilingual/multilingual/finetune) -> ATLAS scaling law fitting -> BTS matrix calculation -> Curse of multilinguality quantification

**Critical Path:**
Dataset preparation → Model pretraining (varying N, D, K) → Loss computation on test sets → Scaling law fitting → Transfer matrix derivation → Cross-validation of predictions

**Design Tradeoffs:**
- **Uniform vs Temperature Sampling:** The paper uses uniform sampling for simplicity, but real-world applications may benefit from temperature-based resampling strategies
- **Fixed vs Adaptive Vocabularies:** Uses fixed 64k SentencePiece vocab to ensure comparability, though adaptive vocabularies might improve low-resource language performance
- **Monolingual vs Multilingual Pretraining:** Tradeoff between specialized monolingual models and generalist multilingual models, with ATLAS providing guidance on optimal approach

**Failure Signatures:**
- Poor R² extrapolation indicates insufficient model size or token diversity coverage
- Negative BTS values suggest incorrect bilingual sampling or reference horizon selection
- Curse of multilinguality law failing to fit (R²<0.8) indicates inadequate coverage of language mixture configurations

**3 First Experiments:**
1. Train monolingual models for 7 core languages (EN, FR, RU, ZH, HI, SW, YO) across 20 scale configurations (9M–8B params)
2. Train bilingual (50/50) models for 10 language pairs to compute BTS scores
3. Train UNIMAX multilingual models for 1T tokens across varying language mixture sizes (4–50 languages)

## Open Questions the Paper Calls Out

**Open Question 1:**
What underlying mechanisms cause the language-specific convergence rate differences observed between pretraining from scratch and finetuning from a multilingual checkpoint? While the paper identifies crossover points (144B–283B tokens) where pretraining surpasses finetuning, it does not explain why these points vary significantly by language. Resolving this would require ablation studies analyzing gradient dynamics or representation alignment during the warm-start phase of finetuning compared to random initialization of pretraining.

**Open Question 2:**
Do the derived exponents for the curse of multilinguality (φ=0.11 and ψ=-0.04) remain stable for model sizes significantly larger than 8B parameters? The experimental setup limits model scale to 10M–8B parameters, but scaling laws often exhibit "breaks" or shifts in exponents at extreme scales. Extending the "Capacity" experiments to include model sizes in the 30B–100B range would confirm if the iso-loss frontiers retain the same curvature.

**Open Question 3:**
How does the assumption of uniform language sampling impact the accuracy of the "curse of multilinguality" scaling law? The paper states uniform sampling is a reasonable assumption, but real-world multilingual models rarely use uniform sampling (often using temperature-based resampling). Fitting scaling law variants on experiments that use temperature-scaled sampling distributions would compare the resulting coefficients against the uniform baseline.

## Limitations
- Limited extrapolation beyond 8B parameters raises uncertainty about scaling law predictions for future model sizes
- 1,444 language-pair transfer matrix derived from core languages may not capture all linguistic phenomena, particularly for low-resource languages
- Vocabulary-agnostic loss formulation may not fully capture task-specific performance variations across different downstream applications

## Confidence
- **High confidence:** Empirical scaling law fitting methodology, curse of multilinguality quantification, and BTS calculation are well-specified and reproducible
- **Medium confidence:** Cross-lingual transfer matrix and its implications for transfer learning, as these depend on the specific language pairs studied
- **Low confidence:** Predictions for model scales beyond 8B parameters and recommendations for extreme low-resource scenarios due to limited experimental coverage

## Next Checks
1. **Transfer Matrix Robustness:** Replicate BTS calculation for new language pairs not in original study, particularly distant language families, to validate generalizability of the 38×38 transfer matrix
2. **Scaling Law Extrapolation:** Test ATLAS predictions on held-out model sizes (e.g., 16B and 32B parameters) and token counts beyond training regime to assess out-of-sample generalization
3. **Low-Resource Scenario Validation:** Apply scaling law and transfer matrix to a truly low-resource language (<100k sentences in MADLAD-400) to evaluate practical utility for extreme data scarcity scenarios