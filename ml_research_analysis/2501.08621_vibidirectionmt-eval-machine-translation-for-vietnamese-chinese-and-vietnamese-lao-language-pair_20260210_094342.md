---
ver: rpa2
title: 'ViBidirectionMT-Eval: Machine Translation for Vietnamese-Chinese and Vietnamese-Lao
  language pair'
arxiv_id: '2501.08621'
source_url: https://arxiv.org/abs/2501.08621
tags:
- translation
- machine
- evaluation
- vietnamese
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The VLSP 2022-2023 shared tasks focused on Vietnamese-Chinese and
  Vietnamese-Lao machine translation. Five teams participated in 2022 (Chinese-Vietnamese)
  and seven in 2023 (Lao-Vietnamese), with each team submitting systems evaluated
  on 1,000 test sentence pairs.
---

# ViBidirectionMT-Eval: Machine Translation for Vietnamese-Chinese and Vietnamese-Lao language pair

## Quick Facts
- arXiv ID: 2501.08621
- Source URL: https://arxiv.org/abs/2501.08621
- Authors: Hong-Viet Tran; Minh-Quy Nguyen; Van-Vinh Nguyen
- Reference count: 28
- Five teams participated in 2022 (Chinese-Vietnamese) and seven in 2023 (Lao-Vietnamese), with human evaluation decisive for ranking

## Executive Summary
The VLSP 2022-2023 shared tasks evaluated bidirectional machine translation for Vietnamese-Chinese and Vietnamese-Lao language pairs. Teams employed fine-tuning of multilingual models (mBART, M2M-100, mT5), back-translation with domain-filtered monolingual data, and ensemble methods. Systems were assessed using BLEU, SacreBLEU, and human evaluation by language experts. The tasks demonstrated significant progress in low-resource Vietnamese multilingual translation and highlighted the importance of human evaluation for reliable benchmarking.

## Method Summary
Teams fine-tuned multilingual pretrained models (mBART-50, M2M-100, mT5) on VLSP parallel data (300K Vi-Zh or 100K Vi-Lo). Data augmentation included TF-IDF selection of monolingual sentences followed by back-translation to create synthetic parallel data. Training used AdamW optimizer with batch size 16, 4-5 epochs, and beam search (size 4). Checkpoint averaging (last 5 checkpoints) was applied for ensemble. Preprocessing used SentencePiece with BPE (4K Vietnamese, 16K Chinese vocabulary) and Moses tokenization. Evaluation combined automatic metrics (BLEU, SacreBLEU) with human post-editing by 5 language experts.

## Key Results
- SDS team won Chinese-Vietnamese task (2022) with 71.27% final score
- Bluesky team won Lao-Vietnamese task (2023) with 52.83% final score
- Human evaluation was decisive for ranking, with MTA AI achieving highest human score (61.31) despite lower SacreBLEU than Bluesky
- TF-IDF domain filtering improved BLEU by 3.19 points in Chinese-Vietnamese translation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Back-translation with domain-filtered monolingual data improves low-resource MT quality
- Mechanism: TF-IDF scoring selects sentences from large monolingual corpora (25M Vietnamese, 19M Chinese) that match training data domain; baseline model translates these to create synthetic parallel data; synthetic data is merged with authentic bilingual data for final training
- Core assumption: Monolingual data selected by TF-IDF similarity to parallel domain produces higher-quality synthetic pairs than random selection
- Evidence anchors:
  - [section] SDS team applied TF-IDF to filter 200k sentences from 25M Vietnamese and 19M Chinese monolingual corpora, achieving 3.19 BLEU improvement on Chinese-to-Vietnamese
  - [section] MTA AI generated 1.5M synthetic pairs via Google Translate back-translation for Vietnamese-Lao
  - [corpus] Related work confirms back-translation efficacy for low-resource pairs including Assamese, Mizo, and Kazakh-Russian code-switching

### Mechanism 2
- Claim: Fine-tuning multilingual pretrained models (mBART, M2M-100, mT5) outperforms training from scratch for low-resource pairs
- Mechanism: Multilingual denoising pre-training provides cross-lingual representations; fine-tuning on limited bilingual data adapts these representations to specific language pairs without requiring large parallel corpora
- Core assumption: Pre-trained multilingual representations transfer effectively to Vietnamese-Chinese and Vietnamese-Lao despite these languages being underrepresented in pre-training data
- Evidence anchors:
  - [section] SDS used mBART-50 fine-tuning as baseline, VBD-MT used mBART-25 with vocabulary reduction from 250K to 67K tokens
  - [section] BlueSky adapted mBART for Lao (not natively supported) by pre-training on 1.8GB Vietnamese and 1GB Lao monolingual data before fine-tuning
  - [corpus] WMT 2024 submissions confirm mBART and M2M-100 effectiveness for low-resource Indic languages

### Mechanism 3
- Claim: Human evaluation via post-editing provides more reliable system ranking than automatic metrics alone
- Mechanism: Five language experts perform post-editing on system outputs; final ranking uses human evaluation scores rather than BLEU/SacreBLEU; post-editing produces both error annotations and additional reference translations
- Core assumption: Expert post-editing correlates better with actual translation utility than n-gram overlap metrics
- Evidence anchors:
  - [abstract] "Human evaluation played a decisive role in ranking"
  - [section] MTA AI achieved highest human score (61.31 Vi-Lo) despite lower SacreBLEU (41.88) than Bluesky (43.08)
  - [corpus] Limited corpus evidence on human vs. automatic metric correlation for these specific language pairs

## Foundational Learning

- Concept: **Back-translation for data augmentation**
  - Why needed here: Both Vi-Zh (300K pairs) and Vi-Lo (100K pairs) are low-resource; back-translation creates synthetic parallel data from abundant monolingual text
  - Quick check question: Can you explain why translating target-language monolingual text to source language creates useful training data?

- Concept: **Subword tokenization (BPE, SentencePiece)**
  - Why needed here: Chinese lacks word boundaries; Vietnamese has complex morphology; vocabulary management is critical for low-resource scenarios
  - Quick check question: Why does BPE with 4,000 operations for Vietnamese vs. 16,000 for Chinese make sense given their orthographic differences?

- Concept: **Ensemble methods via checkpoint averaging**
  - Why needed here: VBD-MT averaged last 5 checkpoints; reduces variance without additional inference cost
  - Quick check question: How does weight averaging differ from beam ensembling, and what computational tradeoffs exist?

## Architecture Onboarding

- Component map:
  Input Processing: SentencePiece/BPE tokenization → Vocabulary reduction
  Backbone Models: mBART-50 / M2M-100 / mT5 / Transformer-WMT / Custom T5
  Data Augmentation: TF-IDF selection → Back-translation → Synthetic data merge
  Training: Fine-tuning with AdamW (lr 1e-5 to 5e-5, batch 16, 4-5 epochs)
  Inference: Beam search (size 4) → Checkpoint averaging → Post-processing rules
  Evaluation: BLEU/SacreBLEU → Human post-editing by 5 experts → Final ranking

- Critical path:
  1. Tokenizer setup (SentencePiece with vocab 20K-67K depending on model)
  2. Fine-tune multilingual model on VLSP parallel data
  3. Generate synthetic data via back-translation
  4. Merge and retrain on expanded corpus
  5. Apply checkpoint averaging for final model

- Design tradeoffs:
  - **mBART vs. from-scratch T5**: mBART leverages cross-lingual transfer but requires GPU memory for large vocabulary; custom T5 (BGSV AI approach) allows vocabulary control but needs more training data
  - **Back-translation quality vs. quantity**: Top-k sampling (k=5, VBD-MT) produces diverse outputs; greedy decoding may be more consistent
  - **Automatic vs. human evaluation**: Human evaluation is costly but decisive for ranking; BLEU is fast but poorly correlates with human judgments in these tasks

- Failure signatures:
  - SacreBLEU high but human score low → likely fluent but semantically incorrect translations
  - Vi→Zh underperforming Zh→Vi consistently → possible tokenization issues or asymmetric data quality
  - Numeric/date errors → indicates missing post-processing rules (addressed by VBD-MT)
  - Vocabulary explosion → SentencePiece vocabulary not properly fitted to combined corpus

- First 3 experiments:
  1. Establish baseline by fine-tuning mBART-50 on VLSP training data (300K Vi-Zh or 100K Vi-Lo) with default hyperparameters; evaluate on dev set using SacreBLEU
  2. Implement TF-IDF domain filtering on available monolingual data, generate 100K-200K synthetic pairs via back-translation, retrain and compare BLEU delta
  3. Apply checkpoint averaging over last 5 epochs and measure variance reduction across multiple inference runs; correlate with human evaluation on 100-sample subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why did participating systems fail to improve upon the baseline in the Vietnamese-to-Chinese translation direction during the VLSP 2022 shared task?
- Basis in paper: [explicit] The authors state that "on Vietnamese-to-Chinese direction, none was able to improve the baseline translation: despite a deep analysis, we were unable to find a plausible explanation for this surprising outcome."
- Why unresolved: The paper reports the anomaly where systems outperformed baselines in Chinese-to-Vietnamese but not the reverse, yet provides no conclusive analysis or linguistic explanation for this directional disparity.
- What evidence would resolve it: A detailed error analysis comparing the baseline outputs against the submitted systems for the Vietnamese-to-Chinese direction, specifically examining segmentation issues or Sino-Vietnamese lexical mapping failures.

### Open Question 2
- Question: How can the significant discrepancy between SacreBLEU scores and human evaluation scores in Vietnamese-Lao translation be explained and mitigated?
- Basis in paper: [inferred] In the VLSP 2023 task, the MTA AI team achieved a lower SacreBLEU score (41.88) than the Bluesky team (43.08), yet received a substantially higher human evaluation score (61.31 vs 51.37), suggesting standard metrics do not correlate well with human judgment for this pair.
- Why unresolved: The paper presents the final rankings but does not investigate why the automatic metric (SacreBLEU) favored one system while human experts clearly favored another, leaving the reliability of automated benchmarking for Lao-Vietnamese uncertain.
- What evidence would resolve it: A correlation study (e.g., Kendall's tau) between automatic metric scores and human post-editing scores across all submissions to determine if specific linguistic features are being penalized or ignored by the algorithm.

### Open Question 3
- Question: To what extent can Vietnamese Large Language Models (LLMs) enhance translation quality compared to current encoder-decoder models like mBART and M2M-100 for these low-resource pairs?
- Basis in paper: [explicit] The conclusion notes: "Looking ahead, we aim to expand the translation task by incorporating pre-trained models and Vietnamese large language models to support additional languages, such as Chinese, Lao, and Khmer."
- Why unresolved: The current study focuses on fine-tuning standard multilingual translation models (mBART, M2M-100, mT5); the potential specific advantage or methodology for integrating generative LLMs remains unexplored in the presented results.
- What evidence would resolve it: A comparative study evaluating the performance of prompt-engineered or fine-tuned Vietnamese-centric LLMs against the current shared task winners using the same human evaluation protocol.

## Limitations

- Data Quality and Domain Coverage: Effectiveness of TF-IDF domain filtering depends critically on domain match between monolingual corpora and test sets, with no specification of exact corpus sources
- Human Evaluation Reliability: Limited details on evaluation rubric, inter-annotator agreement, or post-editing guidelines raise questions about ranking consistency
- Reproducibility Constraints: Missing critical details including source of monolingual corpora, specific TF-IDF parameters, and Lao-specific preprocessing steps

## Confidence

**High Confidence Claims**:
- VLSP 2022-2023 shared tasks were conducted with specified participant counts and evaluation metrics
- SDS team won Chinese-Vietnamese task (2022) and Bluesky won Lao-Vietnamese task (2023) based on final human evaluation scores
- Back-translation and fine-tuning multilingual models were successfully applied by multiple teams

**Medium Confidence Claims**:
- TF-IDF domain filtering improved BLEU scores by 3.19 points (limited to SDS team results)
- mBART-50 fine-tuning provides effective baseline for low-resource Vietnamese multilingual translation
- Human evaluation is more reliable than automatic metrics for ranking system quality

**Low Confidence Claims**:
- Generalizability of TF-IDF back-translation approach to other low-resource language pairs
- Optimal monolingual corpus size and selection criteria for maximum improvement
- Long-term stability of human evaluation rankings across different annotator pools

## Next Checks

1. **Domain Transfer Validation**: Conduct ablation studies comparing TF-IDF filtered back-translation against random selection across multiple domain shifts (news, social media, technical text) to quantify the domain dependence of the 3.19 BLEU improvement.

2. **Human Evaluation Reproducibility**: Implement the same post-editing protocol with new language experts on the VLSP test sets, measuring inter-annotator agreement (Krippendorff's alpha) and comparing rankings with the original evaluation to assess consistency.

3. **Model Architecture Scaling**: Test whether the same fine-tuning and back-translation approach on smaller multilingual models (m2m-100-418M) or larger models (mBART-50 vs. BLOOMZ) yields proportional improvements, establishing the approach's sensitivity to model capacity.