---
ver: rpa2
title: 'Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging'
arxiv_id: '2505.22934'
source_url: https://arxiv.org/abs/2505.22934
tags:
- merging
- performance
- task
- arxiv
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of performance degradation when
  merging multiple LoRA-based fine-tuned language models into a single multi-task
  model. The core issue stems from parameter-data interference where learned LoRA
  subspaces for different tasks adversely affect each other during merging.
---

# Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging

## Quick Facts
- arXiv ID: 2505.22934
- Source URL: https://arxiv.org/abs/2505.22934
- Reference count: 17
- This paper addresses performance degradation when merging multiple LoRA-based fine-tuned language models by constraining the LoRA subspace to minimize cross-task interference.

## Executive Summary
This paper tackles the problem of performance degradation when merging multiple LoRA-based fine-tuned language models into a single multi-task model. The core issue stems from parameter-data interference where learned LoRA subspaces for different tasks adversely affect each other during merging. The proposed method, OSRM (Orthogonal Subspaces for Robust model Merging), constrains the LoRA subspace prior to fine-tuning by finding an analytical solution that minimizes interference between tasks. Extensive experiments on eight GLUE datasets using three widely used language models demonstrate that OSRM consistently outperforms existing merging baselines, achieving average improvements of 3.8% to 7.9% across different merging techniques while preserving single-task accuracy.

## Method Summary
OSRM constrains the LoRA A matrix to operate in low-variance subspaces of out-of-task data to reduce cross-task interference during merging. The method computes latent features from out-of-task samples, performs eigendecomposition on their covariance matrix, and initializes A with eigenvectors corresponding to the smallest eigenvalues. This initialization is then used as a starting point for fine-tuning rather than being frozen, preserving both interference reduction and task-specific adaptation capacity. The approach is a plug-and-play pre-processing step that integrates with existing merging methods like TA, TIES, and Fisher.

## Key Results
- OSRM achieves average improvements of 3.8% to 7.9% across different merging techniques compared to standard LoRA initialization
- The method consistently outperforms existing baselines on RoBERTa-large, T5-large, and Llama3.2-1B models across 8 GLUE datasets
- OSRM shows greater robustness to hyperparameters such as scaling coefficients and sample sizes (k=10-100 optimal)
- Single-task accuracy is preserved while improving merged model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining the LoRA A matrix to operate in low-variance subspaces of out-of-task data reduces cross-task interference during merging.
- Mechanism: When merging two LoRA models, applying the merged model to task T_1 data produces an unwanted perturbation term from the other task's A matrix. By initializing A_2 in directions where h_1 has minimal variance, this perturbation is minimized.
- Core assumption: Latent features from different tasks are available or can be approximated; the row-orthonormal constraint on A preserves sufficient capacity for task learning.
- Evidence anchors:
  - [abstract]: "restricts the A matrix in LoRA to have orthonormal rows corresponding to directions with minimal variance in the latent features"
  - [Section 4.2-4.3]: Formal derivation showing that minimizing ||AH^T||_F subject to AA^T = I yields eigenvectors corresponding to smallest eigenvalues
  - [corpus]: Related work (Decouple and Orthogonalize, arxiv 2505.15875) similarly finds orthogonality-based disentanglement improves LoRA merging, though uses data-free approach
- Break condition: If H_1 is not full-rank or if tasks share near-identical feature distributions, the low-variance directions may not provide sufficient separation for orthogonalization.

### Mechanism 2
- Claim: Initializing A with the analytically-derived orthogonal subspace and allowing it to update during fine-tuning preserves single-task accuracy while maintaining merging benefits.
- Mechanism: Freezing A completely restricts adaptation capacity and degrades downstream performance. By using the constrained solution only as initialization, the model retains the interference-reducing property while recovering task-specific accuracy.
- Core assumption: The orthogonal Procrustes distance between initialized and fine-tuned A remains small (empirically ~14% change observed).
- Evidence anchors:
  - [Section 4.4]: "empirical results show this can significantly degrade single-task accuracy... we propose that Ã_2 should only be used as the initialization"
  - [Figure 3]: Shows A changes by up to ~14% after fine-tuning, indicating the initialization constraint is approximately preserved
  - [corpus]: Corpus evidence is limited; no direct comparison of frozen vs. updatable A in related work
- Break condition: If fine-tuning substantially rotates A away from its initialized subspace, the interference-reduction property may degrade.

### Mechanism 3
- Claim: Averaging sample-wise latent features (rather than storing all features) provides a practical approximation for privacy-sensitive or memory-constrained settings.
- Mechanism: Instead of concatenating all out-of-task features, compute the mean feature vector per task and use these for subspace computation. This reduces memory while preserving the directionality needed for orthogonalization.
- Core assumption: Task-level feature means capture sufficient information about the data distribution for subspace identification.
- Evidence anchors:
  - [Section 4.4, Eq. 4]: Defines the averaging scheme Ĥ_i = (1/k) Σ_j H_{i,j,:}
  - [Section 5.3, Table 6]: Shows k=100 samples achieves strong performance; k=1000-5000 does not consistently improve further, suggesting overlap effects
  - [corpus]: Corpus does not directly validate the mean-approximation approach
- Break condition: If task distributions are highly multi-modal, mean features may not adequately represent the subspace structure.

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA) decomposition
  - Why needed here: OSRM operates on the A matrix within LoRA's BA decomposition; understanding that ΔW = BA with r << min(m,n) is essential to grasp why subspace constraints matter.
  - Quick check question: Given a weight matrix W ∈ R^{4096×4096} and LoRA rank r=8, what are the dimensions of B and A?

- Concept: Eigendecomposition of covariance matrices
  - Why needed here: The core solution extracts eigenvectors corresponding to smallest eigenvalues of S = (1/(k-1)) H^T H to find low-variance directions.
  - Quick check question: If a covariance matrix has eigenvalues [5.0, 2.0, 0.5, 0.1] and r=2, which eigenvectors form the A matrix?

- Concept: Task arithmetic and model merging
  - Why needed here: OSRM is a plug-and-play pre-processing step that integrates with existing merging methods (TA, TIES, Fisher, RegMean, EMR).
  - Quick check question: In task arithmetic, if θ_0 is the pre-trained model and θ_1, θ_2 are fine-tuned models, how is the merged model θ_m computed?

## Architecture Onboarding

- Component map:
  Pre-training phase (frozen LLM) -> Subspace computation (extract H, eigendecomposition) -> Fine-tuning phase (initialize B=0, A=eigenvectors) -> Merging phase (apply any standard merging algorithm)

- Critical path:
  1. Forward pass through frozen model on k samples per task to collect latent features H
  2. Compute H^TH, perform eigendecomposition, extract V_{:,n-r:n}
  3. Initialize each task's A matrix with its orthogonal subspace solution
  4. Fine-tune all tasks independently
  5. Merge using chosen algorithm (TA, TIES, etc.)

- Design tradeoffs:
  - **Sample size k**: Paper finds k=10-100 optimal; larger k increases knowledge overlap risk (Table 6)
  - **Frozen vs. updatable A**: Frozen preserves orthogonality but degrades accuracy; updatable maintains accuracy with ~14% drift
  - **LoRA blocks**: Q+V (paper default) vs. Q+K+V (slightly better) vs. Q-only (worse)

- Failure signatures:
  - Individual task accuracy drops >1%: A initialization may be too restrictive; verify updatable-A mode
  - Merging performance no better than baseline: Check that H_{¬t} excludes current task's features; verify eigendecomposition uses smallest eigenvalues
  - Performance degrades with more tasks (>5): Normal baseline behavior; OSRM advantage increases with task count

- First 3 experiments:
  1. Replicate RoBERTa-large on 2 tasks (e.g., MNLI + SST-2) with k=100, comparing standard LoRA init vs. OSRM init on merged accuracy
  2. Ablation: Freeze A completely vs. allow updates; measure single-task vs. merged performance gap
  3. Scaling test: Vary k ∈ {10, 100, 1000} on Fisher merging to identify optimal sample count for your compute budget

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can individual task performance be preserved when applying OSRM's orthogonal subspace decomposition to already-fine-tuned LoRA modules (post-hoc setting)?
- Basis in paper: [explicit] Section 5.4 states: "Preserving individual performance in the post-fine-tuning setting poses an interesting yet non-trivial challenge, which we identify as a promising direction for future work."
- Why unresolved: The post-hoc decomposition of learned weights causes severe individual performance drops (from 88.05% to 35.07% on RoBERTa-large), even though merged performance is maintained.
- What evidence would resolve it: A modified decomposition or recovery algorithm that maintains ||B̃_t Ã_t - ΔW_t||_F < ε while preserving orthogonality properties, with experimental validation showing individual performance within 5% of original.

### Open Question 2
- Question: What are the underlying mechanisms causing the counter-intuitive relationship between sample size k and merging performance, where moderate values (10-100) outperform larger values (1000-5000)?
- Basis in paper: [explicit] Section 5.3 states: "Due to the complex interplay between fine-tuning procedures, model parameters, and data characteristics, it is challenging to analytically determine the cause of the observed counter-intuitive results. We believe the exploration of this situation is interesting."
- Why unresolved: The paper hypothesizes knowledge overlap between in-task and out-of-task samples as k increases, but cannot analytically confirm this mechanism.
- What evidence would resolve it: Controlled experiments varying task similarity and sample diversity, combined with analysis of the eigenvalue spectrum of covariance matrices at different k values, showing correlation between knowledge overlap measures and performance degradation.

### Open Question 3
- Question: Can the OSRM approach be generalized to fully fine-tuned models without the LoRA low-rank constraint?
- Basis in paper: [explicit] Section 7 (Limitations): "Since we only focus on LoRA models, our method cannot be applied to merging fully fine-tuned models."
- Why unresolved: The method fundamentally relies on the orthonormal row constraint on matrix A in the LoRA factorization ΔW = BA, which has no direct analogue in full fine-tuning where ΔW is dense.
- What evidence would resolve it: A theoretical framework extending the orthogonal subspace concept to full weight matrices, perhaps through SVD-based truncation or projection methods, validated on full fine-tuning experiments with performance comparable to OSRM on LoRA models.

## Limitations
- The method cannot be applied to merging fully fine-tuned models, only LoRA-based adaptations
- Requires extracting latent features from held-out samples per task, which may be impractical for privacy-sensitive applications or tasks with limited data
- The orthogonal constraint may break down when tasks share similar semantic distributions or when fine-tuning substantially rotates the initialized A matrices

## Confidence
- **High confidence**: The experimental results showing consistent performance improvements across multiple model architectures (RoBERTa, T5, Llama) and merging algorithms (TA, TIES, Fisher, RegMean, EMR). The average improvements of 3.8% to 7.9% are well-documented across different experimental conditions.
- **Medium confidence**: The theoretical derivation of the orthogonal subspace solution, which assumes that minimizing interference through low-variance directions will translate to practical performance gains. While the math is sound, the real-world effectiveness depends on how well the idealized assumptions match actual task distributions.
- **Medium confidence**: The claim that OSRM preserves single-task accuracy while improving merged performance. The paper shows this empirically, but the mechanism's robustness to extreme task diversity or very large numbers of tasks (>8) is not fully explored.

## Next Checks
1. **A-matrix stability test**: Track the Procrustes distance between initialized and fine-tuned A matrices across all tasks and layers to quantify how much the orthogonal constraint degrades during fine-tuning, and correlate this with performance drops.
2. **Privacy-sensitive variant**: Implement and evaluate the k=1 sample averaging scheme across all GLUE tasks to verify the claimed memory-efficiency and privacy benefits while maintaining performance.
3. **Task similarity sensitivity**: Systematically vary task similarity (e.g., pair semantically related tasks like MNLI+QNLI vs. unrelated pairs like CoLA+SST-2) to identify the boundary conditions where OSRM's interference-reduction mechanism breaks down.