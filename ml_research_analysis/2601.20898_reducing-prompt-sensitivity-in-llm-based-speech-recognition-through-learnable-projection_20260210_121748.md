---
ver: rpa2
title: Reducing Prompt Sensitivity in LLM-based Speech Recognition Through Learnable
  Projection
arxiv_id: '2601.20898'
source_url: https://arxiv.org/abs/2601.20898
tags:
- prompt
- speech
- prompts
- projector
- llm-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the problem of high sensitivity to prompt choice\
  \ in LLM-based ASR systems, where different manually designed prompts lead to significant\
  \ variability in transcription accuracy across datasets. The authors propose a simple,\
  \ model-agnostic extension called the prompt projector\u2014a learnable module that\
  \ projects prompt embeddings into a more effective region of the LLM input space,\
  \ inspired by the speech-to-LLM projector."
---

# Reducing Prompt Sensitivity in LLM-based Speech Recognition Through Learnable Projection

## Quick Facts
- arXiv ID: 2601.20898
- Source URL: https://arxiv.org/abs/2601.20898
- Reference count: 0
- This paper introduces a learnable prompt projector module that improves robustness and reduces variability in LLM-based ASR systems across different prompts and datasets.

## Executive Summary
This paper addresses a fundamental challenge in LLM-based speech recognition: the high sensitivity of ASR performance to the choice of prompt. Different manually designed prompts can lead to significant variability in transcription accuracy across datasets. The authors propose a simple, model-agnostic extension called the prompt projectorâ€”a learnable module that projects prompt embeddings into a more effective region of the LLM input space, inspired by the speech-to-LLM projector. Evaluated across four diverse ASR datasets (ContactCenter, CallHome, AMI, LibriSpeech) with 10 different prompts, the prompt projector consistently improves robustness and reduces variability in word error rate (WER). Compared to the best manually selected prompts, adding the prompt projector achieves lower or comparable WER while stabilizing performance across prompts, and even outperforms the best prompt alone in several cases. The method requires no prompt engineering and is easy to integrate into existing LLM-based ASR systems.

## Method Summary
The proposed method introduces a learnable prompt projector module that transforms prompt embeddings before they are input to the LLM. The projector consists of a linear layer followed by layer normalization, which projects the prompt embedding into a space that better aligns with the LLM's processing capabilities for speech recognition tasks. The module is trained using the same loss function as the main ASR system, with the key innovation being that the projector learns to map diverse prompts into a more effective representation space. This approach is model-agnostic and can be integrated into existing LLM-based ASR pipelines without requiring changes to the underlying speech encoder or LLM architecture. The training process leverages multiple reference transcripts for each utterance to create diverse prompts, and the projector learns to optimize the prompt representation for improved ASR performance.

## Key Results
- The prompt projector consistently improves robustness and reduces WER variability across 10 different prompts and four diverse ASR datasets
- Compared to the best manually selected prompts, adding the prompt projector achieves lower or comparable WER while stabilizing performance
- The method shows particular effectiveness in the ContactCenter dataset, improving worst-case performance by 6.9% relative while only modestly affecting best-case results

## Why This Works (Mechanism)
The prompt projector works by learning a transformation that maps diverse prompt embeddings into a more effective region of the LLM's input space. Since different prompts can lead to vastly different ASR performance due to how the LLM processes them, the projector learns to normalize these differences by projecting all prompts into a space where the LLM can more consistently and accurately process the speech information. This is analogous to how speech-to-LLM projectors transform speech features, but here the focus is on optimizing the prompt representation rather than the speech features themselves.

## Foundational Learning
- **LLM-based ASR systems**: Why needed - Understanding how large language models are adapted for speech recognition tasks. Quick check - Can explain the role of speech encoders and how LLMs process speech input.
- **Prompt sensitivity in LLMs**: Why needed - Recognizing why different prompts lead to different model behaviors and outputs. Quick check - Can describe how prompt wording affects LLM responses in any task.
- **Learnable projection modules**: Why needed - Understanding how linear transformations with learnable parameters can optimize input representations. Quick check - Can explain how a linear layer followed by normalization can transform data distributions.
- **Cosine similarity in embedding spaces**: Why needed - Understanding how to measure relationships between prompt embeddings before and after projection. Quick check - Can compute and interpret cosine similarity between vectors.
- **Model-agnostic extensions**: Why needed - Recognizing how modules can be added to existing systems without modifying core architectures. Quick check - Can identify points where new modules can be inserted into existing pipelines.

## Architecture Onboarding

**Component Map**: Speech Encoder -> Wav2Vec Features -> LLM -> Output
                             \-> Prompt Projector -> LLM Input

**Critical Path**: The prompt projector is inserted between the prompt generation stage and the LLM input, transforming raw prompt embeddings before they reach the LLM. This is the critical path because it directly affects how the LLM processes the combined speech+prompt information.

**Design Tradeoffs**: The projector uses a simple linear layer + layer normalization design for minimal computational overhead, but this simplicity means it may not capture complex relationships between prompts. The tradeoff is between ease of integration and potential performance gains from more complex architectures.

**Failure Signatures**: Poor projector performance manifests as increased WER variability across prompts or degradation compared to baseline systems. If the projector overfits to training prompts, it may actually harm generalization to new prompts.

**First Experiments**: 
1. Test the projector with a single prompt on a small dataset to verify basic functionality
2. Compare WER variance across multiple prompts with and without the projector
3. Visualize cosine similarity matrices of prompt embeddings before and after projection to verify the projector is learning meaningful transformations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the prompt projector compare to soft-prompt learning methods like prefix-tuning?
- Basis in paper: The authors state, "A natural next step is to compare the prompt projector with soft-prompt learning methods... however, a fair comparison would require extensive experimentation."
- Why unresolved: The current study focused on architectural modifications rather than a comparative analysis against parameter-efficient tuning strategies that add learnable tokens.
- What evidence would resolve it: A controlled study comparing WER and parameter efficiency between the prompt projector and methods like P-Tuning across multiple datasets.

### Open Question 2
- Question: Can the prompt projector generalize effectively across different LLM architectures and speech encoders?
- Basis in paper: The conclusion notes, "Future work should examine the applicability of the approach to different LLMs and speech encoders."
- Why unresolved: Experiments were restricted to the WavLM-large encoder and Vicuna-7B LLM based on prior benchmarks.
- What evidence would resolve it: Evaluation of the prompt projector module when applied to other configurations, such as Whisper encoders or Llama-based LLMs.

### Open Question 3
- Question: Can a single trained prompt projector generalize across different languages or distinct tasks beyond transcription?
- Basis in paper: The authors ask, "Finally, an open question is whether a single projector can generalize across prompts, tasks, or languages."
- Why unresolved: The projector was trained and tested specifically on English ASR tasks; cross-task or cross-lingual transfer capabilities were not assessed.
- What evidence would resolve it: Testing a projector trained on English ASR directly on speech translation tasks or multilingual datasets without retraining.

## Limitations
- The analysis of how the projector transforms prompt embeddings is largely qualitative, relying on visualization without deeper investigation into the learned projection space structure
- The method's reliance on having multiple reference transcripts for training the projector could limit its applicability in low-resource scenarios
- The paper does not fully explain why certain prompts benefit more than others from projection, nor does it explore whether the learned projections might encode undesirable biases

## Confidence
- **High**: The core claim that the prompt projector reduces WER variability across prompts is directly supported by consistent numerical improvements across all four datasets
- **High**: The claim that the projector is easy to integrate and requires no prompt engineering is supported by the straightforward architecture and ablation studies
- **Medium**: The broader claim about generalization to unseen prompts is supported by testing with multiple prompts, but true generalization to completely novel prompts was not assessed

## Next Checks
1. Evaluate the projector's performance when trained on prompts from one domain and tested on prompts from a completely different domain to assess true generalization
2. Analyze the learned projection space to determine whether it captures meaningful semantic or functional distinctions between prompts that correlate with ASR performance
3. Test the method in low-resource settings with limited reference transcripts to understand the minimum data requirements for effective projector training