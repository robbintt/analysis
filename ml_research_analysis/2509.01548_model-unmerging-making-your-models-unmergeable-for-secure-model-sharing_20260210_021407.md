---
ver: rpa2
title: 'Model Unmerging: Making Your Models Unmergeable for Secure Model Sharing'
arxiv_id: '2509.01548'
source_url: https://arxiv.org/abs/2509.01548
tags:
- merging
- mergelock
- alig
- normal
- merged
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MergeLock, a method to prevent unauthorized
  model merging by making models unmergeable. The core idea leverages the symmetry
  properties of self-attention layers in Transformers by inserting invertible transformation
  matrices into the query-key and value-output branches.
---

# Model Unmerging: Making Your Models Unmergeable for Secure Model Sharing

## Quick Facts
- arXiv ID: 2509.01548
- Source URL: https://arxiv.org/abs/2509.01548
- Authors: Zihao Wang; Enneng Yang; Lu Yin; Shiwei Liu; Li Shen
- Reference count: 40
- Primary result: MergeLock degrades merged model performance by over 95% when a protected model is involved

## Executive Summary
This paper introduces MergeLock, a method to prevent unauthorized model merging by making models unmergeable while preserving their original task performance. The core idea leverages the symmetry properties of self-attention layers in Transformers by inserting invertible transformation matrices into the query-key and value-output branches. These transformations push models to different loss basins, breaking the parameter space alignment required for effective merging. Experiments across vision and language tasks show that MergeLock provides strong protection against unauthorized merging while maintaining the protected model's original capabilities.

## Method Summary
MergeLock works by inserting invertible transformation matrices into the self-attention layers of Transformer models. Specifically, it adds matrices A and A⁻¹ to the query-key (QK) branch and B and B⁻¹ to the value-output (VO) branch. The transformation A is constructed as R·P·D (random × permutation × diagonal scaling), with each attention head and layer receiving different transformations. This preserves the model's output while pushing it to a different loss basin, making parameter alignment for merging difficult. The method is evaluated on CLIP-ViT models fine-tuned on 8 vision datasets and Flan-T5 on GLUE tasks, demonstrating over 95% degradation in merged model performance when protected models are involved.

## Key Results
- Protected models maintain original accuracy (>95% on SUN397, >97% on Cars)
- Merged models with protected models show >95% performance degradation compared to baseline merging
- Alignment-based recovery attacks yield minimal improvement in performance
- MergeLock introduces approximately 270 Frobenius distance in QK branch vs 20 for baseline methods
- Transformation preserves output while making parameter alignment for merging extremely difficult

## Why This Works (Mechanism)
MergeLock exploits the symmetry properties inherent in self-attention layers of Transformers. By inserting carefully designed invertible transformations into the query-key and value-output branches, the method maintains the model's functional output while altering its parameter space representation. This pushes the protected model to a different loss basin, breaking the parameter alignment necessary for successful merging. The use of random matrices, permutation matrices, and diagonal scaling ensures that the transformations are both invertible and sufficiently disruptive to merging attempts while being reversible for the original task.

## Foundational Learning
- **Self-attention symmetry**: Why needed - Understanding how query-key and value-output branches interact in attention layers; Quick check - Verify that QK^T·K = K^T·QK holds for symmetric transformations
- **Loss basin dynamics**: Why needed - Concept of how models settle into different local minima in parameter space; Quick check - Confirm that protected model reaches same accuracy from different initialization path
- **Invertible matrix transformations**: Why needed - Ensuring transformations can be reversed without losing model functionality; Quick check - Verify A·A⁻¹ = I and B·B⁻¹ = I for all transformation matrices
- **Parameter space alignment**: Why needed - Core challenge in model merging where parameters need to be aligned before averaging; Quick check - Measure Frobenius distance between protected and original parameters
- **Task arithmetic in model merging**: Why needed - Standard approach for combining models that MergeLock specifically disrupts; Quick check - Verify baseline merging performance before applying MergeLock

## Architecture Onboarding

**Component Map:**
Input → Transformer Encoder → Self-Attention Layers → Output
                          ↑
                          └─→ MergeLock Transformations (A, A⁻¹, B, B⁻¹)

**Critical Path:**
Fine-tuning → Apply MergeLock transformations → Merge with unprotected model → Evaluate degradation

**Design Tradeoffs:**
- Transformation strength vs. maintaining original accuracy
- Computational overhead of matrix operations vs. protection level
- Randomness in transformations vs. reproducibility

**Failure Signatures:**
- Protected model accuracy drops → Transformation too aggressive or numerically unstable
- Merged model still performs well → Transformation insufficient or incorrect application
- Alignment attack succeeds → Missing components in transformation (e.g., only diagonal scaling used)

**First Experiments:**
1. Apply MergeLock to a fine-tuned ViT model and verify original accuracy is preserved
2. Merge protected model with unprotected model using Task Arithmetic and measure performance degradation
3. Attempt alignment recovery on merged model and verify minimal improvement

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can the symmetry-based unmergeable transformation be adapted for non-Transformer architectures, such as Convolutional Neural Networks (CNNs) or MLP-Mixers?
- Basis in paper: The conclusion states, "this paper primarily focuses on protecting models based on the Transformer architecture; future work could extend to other network architectures."
- Why unresolved: The proposed MergeLock specifically leverages the symmetry properties of self-attention layers in Transformers, which do not directly translate to the convolutional or fully-connected layers dominant in other architectures.
- What evidence would resolve it: A demonstration of a protection mechanism derived from the symmetries of convolutional layers (e.g., in ResNet) that prevents merging while maintaining accuracy.

### Open Question 2
- Question: Can the unmergeable transformation be integrated into the fine-tuning process itself rather than applied as a post-hoc modification?
- Basis in paper: The authors note, "the current approach provides a post-hoc protection strategy... whereas future efforts could explore protecting models during the fine-tuning process itself."
- Why unresolved: Applying transformations after training might leave subtle statistical signatures that could be exploited, whereas training with the constraint might conceal the model's capabilities more effectively.
- What evidence would resolve it: A comparison of robustness against recovery attacks between models protected post-hoc versus models trained with the protection as a regularization term or structural constraint.

### Open Question 3
- Question: Does MergeLock remain effective and computationally efficient when applied to multi-billion parameter Large Language Models (LLMs) or multimodal models?
- Basis in paper: The paper suggests, "the proposed method could be applied to safeguard larger-scale models, such as large language models and multimodal large models, in model merging scenarios."
- Why unresolved: The experiments were limited to ViT and Flan-T5 (base/large sizes); it is unverified if the matrix transformations scale efficiently or if the "loss basin" dynamics hold for massive, sparse-activated, or multimodal architectures.
- What evidence would resolve it: Experimental results showing MergeLock's success rate and computational overhead when applied to models like LLaMA-70B or multimodal LLMs.

### Open Question 4
- Question: Can high-cost, data-dependent optimization attacks successfully reverse MergeLock's transformations where low-cost alignment failed?
- Basis in paper: The abstract and conclusion explicitly highlight robustness against "low-cost restoration methods," leaving the vulnerability to sophisticated, computationally intensive alignment attacks using available data as an open gap.
- Why unresolved: While the paper proves robustness against closed-form alignment solutions (e.g., Kabsch algorithm), it does not test against iterative optimization methods that might use small amounts of public data to align the protected model back to the original loss basin.
- What evidence would resolve it: An evaluation of MergeLock's resilience against gradient-based alignment attacks that utilize a small proxy dataset to minimize the parameter distance between the protected and original models.

## Limitations
- Limited evaluation to Transformer-based architectures only
- Missing implementation details for transformation parameters and fine-tuning hyperparameters
- No testing against sophisticated, data-dependent optimization attacks
- Computational overhead of matrix operations for large-scale models unverified

## Confidence
- **High confidence** in the core technical contribution and experimental methodology for measuring mergeability degradation (>95% performance drop)
- **Medium confidence** in the specific transformation construction and application details due to missing implementation specifics
- **Low confidence** in the exact numerical values for transformation parameters and fine-tuning procedures

## Next Checks
1. Verify numerical stability of matrix transformations by testing matrix invertibility across multiple random seeds and checking Frobenius norm preservation
2. Reproduce the Task Arithmetic merging baseline with λ=0.3-0.6 to establish baseline merge performance before applying MergeLock
3. Test alignment recovery attempts using the same methodology described in related work (PaRaMS) to verify the claimed robustness against parameter alignment attacks