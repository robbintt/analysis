---
ver: rpa2
title: An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence
arxiv_id: '2602.02400'
source_url: https://arxiv.org/abs/2602.02400
tags:
- noise
- noisy
- data
- divergence
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates whether noisy data causes
  large language model pretraining loss divergence and how it does so. The authors
  inject controlled synthetic uniform random noise into clean datasets and analyze
  training dynamics across model sizes from 480M to 5.2B parameters.
---

# An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence

## Quick Facts
- arXiv ID: 2602.02400
- Source URL: https://arxiv.org/abs/2602.02400
- Reference count: 24
- Primary result: Noisy data induces LLM pretraining loss divergence, with divergence probability depending on noise type, amount, and model scale

## Executive Summary
This paper systematically investigates whether noisy data causes large language model pretraining loss divergence and how it does so. The authors inject controlled synthetic uniform random noise into clean datasets and analyze training dynamics across model sizes from 480M to 5.2B parameters. The study finds that noisy data does induce training loss divergence, with divergence patterns differing from those caused by high learning rates, enabling diagnostic separation. Dense and MoE models show similar sensitivity to noisy data, providing empirical evidence that uniform random noise can destabilize LLM pretraining and offering practical guidance for identifying and mitigating noise-induced failures.

## Method Summary
The authors conduct controlled experiments by injecting synthetic uniform random noise into clean datasets and monitoring pretraining dynamics. They systematically vary noise levels and model scales from 480M to 5.2B parameters to establish divergence thresholds. The study compares dense and MoE architectures to assess architectural sensitivity to noise. Training runs are monitored using perplexity-based metrics to detect divergence, and the results are compared against learning rate-induced divergence to establish diagnostic signatures. The synthetic noise injection allows precise control over noise characteristics and amounts.

## Key Results
- Noisy data induces training loss divergence, with probability strongly dependent on noise type, amount, and model scale
- Divergence patterns differ from learning rate-induced divergence, enabling diagnostic separation
- Dense and MoE models show similar sensitivity to noisy data across tested configurations

## Why This Works (Mechanism)
The mechanism behind noise-induced divergence involves the model's inability to learn meaningful patterns when a significant portion of training data is random. Uniform random noise lacks the statistical structure that language models are designed to capture, causing the optimization process to fail when noise exceeds a critical threshold. The study demonstrates that as noise increases, the model's training loss fails to converge and instead diverges, with the threshold varying based on model capacity and noise characteristics.

## Foundational Learning

**Language Model Pretraining**
- Why needed: Understanding how models learn from sequential data is fundamental to grasping pretraining dynamics
- Quick check: Verify knowledge of autoregressive training objectives and token prediction

**Training Divergence Detection**
- Why needed: Recognizing when and how training fails is critical for model development
- Quick check: Understand perplexity metrics and loss curve interpretation

**Synthetic Data Injection**
- Why needed: Controlled experiments require precise control over data characteristics
- Quick check: Verify understanding of data augmentation and controlled corruption methods

**Model Scale Effects**
- Why needed: Scaling laws govern how model capacity affects learning dynamics
- Quick check: Understand the relationship between parameter count and learning capacity

## Architecture Onboarding

**Component Map**
Data Pipeline -> Noise Injection Module -> Model Architecture (Dense/MoE) -> Training Loop -> Loss Monitoring -> Divergence Detection

**Critical Path**
Clean data → Noise injection → Token embedding → Transformer layers → Output prediction → Loss computation → Parameter updates → Monitoring for divergence

**Design Tradeoffs**
- Noise type selection: Uniform vs. structured noise affects model sensitivity differently
- Model scale: Larger models may tolerate more noise but also suffer more severe divergence
- Architecture choice: Dense vs. MoE affects noise sensitivity but shows similar patterns in this study

**Failure Signatures**
- Divergence from expected loss curve
- Inability to improve perplexity on validation data
- Training instability manifesting as oscillating or exploding loss values

**Three First Experiments**
1. Inject varying levels of uniform noise into a small clean dataset and monitor training stability
2. Compare divergence patterns between noise-induced and high learning rate-induced divergence
3. Test different model sizes with fixed noise levels to establish scaling relationships

## Open Questions the Paper Calls Out
None

## Limitations
- Uses synthetic uniform random noise rather than real-world naturally occurring noisy data
- Analysis focuses primarily on perplexity-based divergence metrics without exploring other failure modes
- Limited to specific dense and MoE architectures without investigating generalization to other architectures

## Confidence

**High confidence**: The empirical observation that synthetic noise induces training divergence is directly measured and repeatable across experiments.

**Medium confidence**: The claim that divergence patterns differ from learning rate-induced divergence requires careful distinction and may depend on specific hyperparameter settings.

**Medium confidence**: The finding that dense and MoE models show similar sensitivity is based on specific configurations and may not generalize across all sparse architectures.

## Next Checks

1. **Real-world data validation**: Test the divergence hypothesis using naturally occurring noisy data (e.g., web-scraped data with known quality issues) to verify whether synthetic uniform noise is representative of real failure modes.

2. **Architecture generalization study**: Extend experiments to additional model architectures including RNNs, CNNs, and alternative attention mechanisms to determine if observed sensitivity patterns are architecture-specific or universal.

3. **Long-term training dynamics**: Conduct extended training runs beyond initial divergence to study whether models can recover, how divergence propagates through training, and what checkpointing strategies might mitigate catastrophic failures.