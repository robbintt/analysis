---
ver: rpa2
title: Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers for
  Long Contexts
arxiv_id: '2506.05229'
source_url: https://arxiv.org/abs/2506.05229
tags:
- batching
- diagonal
- memory
- segment
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Diagonal Batching enables efficient parallel execution of recurrent
  memory transformers by reorganizing layer-segment computations into independent
  diagonals, achieving 3.3x speedup over standard transformers and 1.8x over sequential
  RMT on 131,072-token sequences. This technique eliminates the sequential bottleneck
  in PRMTs without retraining, allowing models to scale inference linearly with constant
  memory.
---

# Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers for Long Contexts

## Quick Facts
- **arXiv ID:** 2506.05229
- **Source URL:** https://arxiv.org/abs/2506.05229
- **Reference count:** 40
- **Primary result:** 3.3x speedup over standard transformers and 1.8x over sequential RMT on 131,072-token sequences

## Executive Summary
Diagonal Batching is a runtime scheduling technique that enables efficient parallel execution of recurrent memory transformers by reorganizing layer-segment computations into independent diagonals. This approach achieves significant speedups (3.3x over standard transformers, 1.8x over sequential RMT) on long-context sequences up to 131,072 tokens without requiring model retraining. The method maintains high output accuracy (less than 2% error) while reducing memory usage by 167.1x compared to standard Llama KV cache.

## Method Summary
The technique reorganizes the computation grid of Parallel Recurrent Memory Transformers (PRMTs) into independent diagonals where segment+layer=k, enabling concurrent execution of up to N_layers operations per step. It requires converting standard Llama weights to the ARMT architecture using a repository-provided script, then applying grouped GEMM (CUTLASS) and FlashAttention for efficient parallel processing. The scheduler processes N_s + N_l - 1 diagonal groups instead of the sequential N_s × N_l steps, maintaining constant memory usage while processing theoretically infinite context.

## Key Results
- 3.3x speedup over standard Llama-1B on 131,072-token sequences
- 1.8x speedup over sequential ARMT implementation
- Less than 2% error accumulation compared to sequential execution
- 167.1x memory savings compared to standard Llama KV cache
- Near-ideal batch scaling matching large batch-size performance without additional memory overhead

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reorganizing the computation grid into independent diagonals breaks the sequential bottleneck in Parallel Recurrent Memory Transformers (PRMTs) without altering the recurrence logic.
- **Mechanism:** PRMTs define dependencies such that computing state at (Layer l, Segment s) requires only (Layer l-1, Segment s) and (Layer l, Segment s-1). By grouping operations where l + s = k (a diagonal), the scheduler unlocks parallel execution of up to N_layers operations per step. This transforms a sequential chain of N_s × N_l steps into N_s + N_l - 1 parallel groups.
- **Core assumption:** The model strictly adheres to layer-local recurrence (PRMT property) where memory flows horizontally (segment to segment within a layer) and vertically (layer to layer within a segment), but not diagonally across time and depth simultaneously.
- **Evidence anchors:**
  - [abstract] "scheduling scheme that unlocks parallelism across segments... preserves exact recurrence."
  - [section 3.1] "Given this dependency, all pairs where segment + layer = i can be computed in parallel... visualized as a diagonal."
- **Break condition:** If the architecture introduces inter-layer recurrence (e.g., standard RMT where the final layer of segment s-1 feeds the first layer of segment s), the diagonal independence assumption fails.

### Mechanism 2
- **Claim:** Treating diagonal groups as batch dimensions improves hardware utilization (FLOPS) by maximizing parallel work per kernel launch.
- **Mechanism:** Standard sequential inference processes small matrices (segment size × hidden dim), leading to memory-bound operations. Diagonal Batching stacks the weights of all layers into a GroupedMatmul operation. It feeds inputs from different segments/layers belonging to the same diagonal as a "batch." This increases arithmetic intensity, moving the workload closer to the compute-bound roofline limit.
- **Core assumption:** The overhead of stacking weights and managing the diagonal scheduler is lower than the latency of memory-bound sequential operations.
- **Evidence anchors:**
  - [abstract] "enabling efficient GPU inference even for single long-context inputs without complex batching."
  - [section 4.1] "grouped GEMM FLOPS scales similar through group size to GEMM with corresponding batch size."
- **Break condition:** If segment sizes are already very large (saturating GPU memory/compute), the additional "grouped" batching may cause out-of-memory errors or yield diminishing returns.

### Mechanism 3
- **Claim:** The method decouples memory footprint from context length, maintaining constant memory usage while processing theoretically infinite context.
- **Mechanism:** Unlike standard Transformers which store a KV cache scaling O(L), or standard batching which scales O(B × L), Diagonal Batching processes segments using the RMT's fixed memory state. It only needs to hold activations for the current diagonal in VRAM. Once a diagonal step completes, intermediate activations for finished segments/layers can be discarded or offloaded.
- **Core assumption:** The recurrent memory state size is sufficient to retain necessary information (no catastrophic forgetting), and the implementation correctly frees memory of prior diagonals.
- **Evidence anchors:**
  - [abstract] "allowing models to scale inference linearly with constant memory."
  - [section 1] Figure 1 shows "167.1x memory savings" compared to standard Llama KV cache.
- **Break condition:** If the implementation materializes the full compute graph (e.g., for training without gradient checkpointing), memory usage ceases to be constant.

## Foundational Learning

- **Concept: Topological Sorting in DAGs**
  - **Why needed here:** Diagonal Batching is essentially a topological sort of the PRMT compute graph. You must understand which operations depend on others to see why (l, s) can run parallel to (l-1, s+1).
  - **Quick check question:** If Node A depends on Node B, and Node B depends on Node C, can A and C be executed in the same diagonal?

- **Concept: Compute-Bound vs. Memory-Bound Operations (Roofline Model)**
  - **Why needed here:** The paper argues that batching diagonal groups improves "FLOPS." This only works if the original operation was memory-bound (low arithmetic intensity) and the batching increases intensity sufficiently to hit the compute ceiling.
  - **Quick check question:** Does increasing the batch size of a matrix multiplication primarily help memory bandwidth utilization or compute core utilization if the matrix is small?

- **Concept: Recurrence vs. State-Space Models**
  - **Why needed here:** You must distinguish between standard RMTs (sequential layers pass memory to the next segment's first layer) and PRMTs (layers pass memory horizontally). The paper explicitly excludes standard RMTs from this optimization.
  - **Quick check question:** In a PRMT, does Layer 1 of Segment 2 receive inputs from Layer 12 of Segment 1 or Layer 1 of Segment 1?

## Architecture Onboarding

- **Component map:** Input Splitter -> Grouped Layer -> Diagonal Scheduler -> Associative Memory
- **Critical path:** The transition from sequential layer-by-layer calls to the GroupedMatmul kernel. The scheduler must correctly align inputs from segment i (layer l) and segment i-1 (layer l+1) into the same batch tensor for the kernel launch.
- **Design tradeoffs:**
  - Optimized for single-request latency via parallel execution, unlike traditional batching which optimizes throughput
  - Requires rewriting model layers into grouped versions (CUTLASS)
  - Assumption: Heterogeneous layer dimensions likely break the simple stacking logic
- **Failure signatures:**
  - Deadlock/Dependency Error: Output logits for segment i are generated before segment i-1 has finished updating the memory state required for segment i
  - Numerical Drift: Logits diverge >2% from baseline (paper reports <2% is acceptable)
  - Memory Spike: Forgetting to clear intermediate tensors from previous diagonals
- **First 3 experiments:**
  1. Unit Test - Diagonal Validity: Run a 2-layer, 4-segment mock model. Print the execution order. Verify that (L0, S1) and (L1, S0) execute simultaneously, and no layer executes before its horizontal dependency.
  2. Micro-benchmark - Kernel Efficiency: Profile GroupedMatmul vs. sequential Matmul on A100. Vary group size (1 to 32) and hidden dim to find the crossover point where Grouped becomes faster (expected: group > 4).
  3. E2E Accuracy Validation: Run the "BABILong" benchmark or similar retrieval task at 4k context. Compare exact logits between Diagonal Batching and naive sequential RMT to confirm error accumulation is within tolerance (<2%).

## Open Questions the Paper Calls Out

- **Question 1:** How can Diagonal Batching be generalized to support model architectures with heterogeneous layers or varied hidden sizes without requiring intricate manual engineering?
  - **Basis in paper:** [explicit] The authors state in the Limitations section that the current implementation assumes uniform layer configuration, and applying the technique to models with varied hidden sizes currently requires "intricate grouping logic and manual engineering."
  - **Why unresolved:** The method relies on stacking weights and layer normalization parameters into uniform tensors (Section 3.3) to utilize GroupedGEMM, which imposes structural constraints on the model.
  - **What evidence would resolve it:** A modified scheduler or kernel implementation that dynamically handles variable tensor shapes within the diagonal groups without sacrificing the parallelism gains.

- **Question 2:** Can the Diagonal Batching scheduling scheme be adapted to support standard Recurrent Memory Transformers (RMTs) that exhibit inter-layer recurrence, or is it strictly limited to Parallel RMTs?
  - **Basis in paper:** [explicit] The Limitations section notes the method is "not directly compatible" with standard RMTs due to their intra-layer recurrence dependencies.
  - **Why unresolved:** Standard RMTs require the final layer's output from segment s-1 to begin segment s, which creates a dependency chain that the current diagonal scheduling does not support.
  - **What evidence would resolve it:** A theoretical extension of the diagonal algorithm or a graph restructuring method that accommodates the cross-layer memory dependencies found in standard RMTs.

- **Question 3:** To what extent does the <2% error accumulation observed in Diagonal Batching impact performance on sensitive downstream tasks or sequences significantly longer than the tested 131k tokens?
  - **Basis in paper:** [inferred] While Section 4.5 and Appendix A report "negligible" effects on BABILong and <2% error, the authors note that "logit-level floating-point drift" exists and aligning training/inference code helps, implying potential stability issues in untested scenarios.
  - **Why unresolved:** The evaluation is limited to specific benchmarks (BABILong) and lengths up to 131k; numerical stability might degrade differently for tasks requiring higher precision or vastly longer contexts.
  - **What evidence would resolve it:** Comprehensive evaluation on a wider range of downstream tasks (e.g., complex reasoning or coding) and stress-testing on sequence lengths exceeding 1 million tokens.

## Limitations
- Limited to Parallel RMTs; incompatible with standard RMTs due to inter-layer recurrence dependencies
- Requires uniform layer configuration; heterogeneous architectures need intricate manual engineering
- Error accumulation (up to 2%) may impact sensitive downstream tasks, though current benchmarks show negligible effects

## Confidence
- **High Confidence:** The mechanism of reorganizing computations into independent diagonals is well-supported by the topological sorting argument and the explicit dependency structure of PRMTs. The empirical speedups (3.3x vs. standard, 1.8x vs. sequential) are directly measured and reported.
- **Medium Confidence:** The FLOPS utilization improvement is plausible given the roofline model arguments, but the actual hardware utilization gains depend heavily on specific GPU characteristics not fully characterized in the paper.
- **Medium Confidence:** The constant memory claim is theoretically sound for the architectural design, but practical implementation details about memory management are not fully specified.

## Next Checks
1. **Dependency Structure Validation:** Create a test case where the strict PRMT assumption is violated (e.g., introduce inter-layer recurrence) and measure the exact performance degradation and error accumulation to establish boundary conditions.
2. **Numerical Stability Across Scales:** Run the same inference task across the full Llama model range (160M to 8B) and measure error accumulation relative to sequential baselines to determine if the 2% tolerance holds uniformly.
3. **Memory Management Verification:** Profile memory usage during diagonal batching execution to verify that intermediate tensors are actually being freed between diagonal steps, validating the constant memory claim on different hardware configurations.