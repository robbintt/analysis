---
ver: rpa2
title: 'Meow: End-to-End Outline Writing for Automatic Academic Survey'
arxiv_id: '2509.19370'
source_url: https://arxiv.org/abs/2509.19370
tags:
- survey
- outline
- generation
- learning
- outlines
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Meow, a metadata-driven framework for end-to-end
  survey outline generation that formulates outline writing as a hierarchical structured
  generation task from paper metadata. The authors construct a high-quality dataset
  of 2.82 million papers from arXiv, bioRxiv, and medRxiv, and develop systematic
  evaluation metrics including LLM-as-a-Judge for structure, content, and pragmatics.
---

# Meow: End-to-End Outline Writing for Automatic Academic Survey

## Quick Facts
- arXiv ID: 2509.19370
- Source URL: https://arxiv.org/abs/2509.19370
- Authors: Zhaoyu Ma; Yuan Shan; Jiahao Zhao; Nan Xu; Lei Wang
- Reference count: 0
- Primary result: End-to-end metadata-to-outline generation with 8B Qwen3 model achieves 36.79 LLM-Judge score on test set

## Executive Summary
This paper introduces Meow, a metadata-driven framework for end-to-end survey outline generation that formulates outline writing as a hierarchical structured generation task from paper metadata. The authors construct a high-quality dataset of 2.82 million papers from arXiv, bioRxiv, and medRxiv, and develop systematic evaluation metrics including LLM-as-a-Judge for structure, content, and pragmatics. Their two-stage training approach combines supervised fine-tuning with reinforcement learning using Group Relative Policy Optimization and custom reward functions for structural similarity and format compliance. The 8B Qwen3 model achieves strong performance, with LLM-Judge scores of 36.79 on their test set and 35.62 on the SurveyX benchmark, demonstrating superior structural fidelity and stylistic coherence compared to existing methods.

## Method Summary
Meow processes all candidate paper metadata (titles + abstracts) in a single inference pass to generate hierarchical survey outlines, leveraging LLM long-context capabilities rather than decomposing into sequential reasoning steps. The training pipeline consists of three stages: (1) CoT distillation using DeepSeek-R1 to generate reasoning chains from reference clustering, (2) SFT cold-start on the CoT-augmented dataset, and (3) GRPO fine-tuning with combined structural similarity (tree edit distance) and format compliance rewards. The framework is evaluated using LLM-as-a-Judge across five criteria in three dimensions (structure, content, pragmatics) plus structural distance metrics.

## Key Results
- 8B Qwen3-SFT-GRPO achieves 36.79 LLM-Judge score on held-out test set
- Model attains lowest Structural Distance of 0.39, indicating enhanced structural fidelity
- Outperforms SurveyX benchmark with 35.62 LLM-Judge score
- Demonstrates superior structural fidelity and stylistic coherence compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: End-to-end metadata-to-outline generation captures global structural coherence better than multi-step agent pipelines.
- Mechanism: By processing all candidate paper metadata (titles + abstracts) in a single inference pass, the model learns direct mappings from collective content patterns to hierarchical outline structures, leveraging LLM long-context capabilities rather than decomposing into sequential reasoning steps.
- Core assumption: The paper assumes that structural coherence emerges from jointly modeling all input metadata rather than aggregating locally-optimal decisions.
- Evidence anchors:
  - [abstract] "formulates outline writing as an end-to-end task that generates hierarchical structured outlines from paper metadata"
  - [section 1] "agent-based approaches are inefficient due to multi-step reasoning and generate rigid outlines with shallow coherence"
  - [corpus] SurveyForge, SurveyGen-I, and SurveyBench all adopt multi-agent or multi-stage approaches; Meow's direct comparison advantage suggests the end-to-end formulation is a genuine differentiator, though head-to-head benchmarks across systems are limited.
- Break condition: If input paper count exceeds model context window, the end-to-end assumption fails and requires chunking or hierarchical aggregation.

### Mechanism 2
- Claim: CoT distillation from DeepSeek-R1 provides explicit reasoning scaffolds that improve structural organization during cold-start SFT.
- Mechanism: DeepSeek-R1 is prompted to derive taxonomies from references through clustering, generating intermediate reasoning chains that the student model learns to replicate before producing final outlines—this creates a structured reasoning prior rather than direct input-output mapping.
- Core assumption: The distilled reasoning chains represent transferable structural reasoning patterns, not domain-specific artifacts from the teacher model.
- Evidence anchors:
  - [section 3.2] "DeepSeek-R1 was prompted to derive a taxonomy from the references through clustering, thereby constructing the logical deduction chain that bridges the input and final answer"
  - [section 3.3] "We initialize the model through supervised fine-tuning on a curated CoT dataset"
  - [corpus] No direct corpus comparison for CoT distillation efficacy in survey generation; this appears novel to Meow but lacks independent validation.
- Break condition: If reasoning chains become too domain-specific or teacher-model-biased, they may not generalize to new topics not represented in the distillation data.

### Mechanism 3
- Claim: GRPO with structural rewards aligns model outputs toward human-like outline organization beyond what SFT alone achieves.
- Mechanism: Group Relative Policy Optimization samples multiple candidate outlines per input, computes advantages relative to group mean, and optimizes using combined structural similarity (tree edit distance) and format compliance rewards—this pushes models toward structurally faithful outputs while maintaining valid schema.
- Core assumption: Tree edit distance to human-written outlines is a meaningful proxy for outline quality, and format compliance is necessary but not sufficient.
- Evidence anchors:
  - [section 4.4] "Meow-8B-SFT-GRPO improves to 36.79 and attains the lowest Structural Distance of 0.39, indicating that reinforcement learning enhances structural fidelity"
  - [section 3.5] "The final reward combines both components with weighting: R_total = λR_struct + (1-λ)R_format"
  - [corpus] SurveyX and SurveyEval also propose multi-dimensional evaluation, but Meow's explicit structural reward integration during training is distinct; corpus lacks comparative RL approaches for this task.
- Break condition: If λ weighting is poorly calibrated, the model may over-optimize for structural similarity at the expense of content appropriateness, or vice versa.

## Foundational Learning

- Concept: **Tree Edit Distance (TED)**
  - Why needed here: Core metric for structural similarity reward; measures minimum operations (insert, delete, rename) to transform one tree into another.
  - Quick check question: Can you explain why TED is more appropriate than sequence-level metrics (BLEU, ROUGE) for hierarchical outline comparison?

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: RL algorithm used in second training stage; normalizes rewards across sampled candidates rather than requiring a separate value function.
  - Quick check question: How does GRPO differ from PPO in terms of advantage estimation and computational requirements?

- Concept: **LLM-as-a-Judge Evaluation**
  - Why needed here: Primary evaluation paradigm; requires understanding how to design rubrics, prompt judges, and interpret scored dimensions (structure, content, pragmatics).
  - Quick check question: What are the failure modes of LLM-as-a-Judge, and how might you detect them in your evaluation pipeline?

## Architecture Onboarding

- Component map:
  Data Curation Pipeline: Metadata collection → survey filtering → reference enrichment → CoT distillation (DeepSeek-R1)
  Training Pipeline: SFT cold-start (CoT-augmented) → GRPO fine-tuning (structural + format rewards)
  Evaluation Pipeline: LLM-Judge (5 criteria × 3 dimensions) + Structural Distance (TED)

- Critical path:
  1. Curate survey papers with complete bibliographic metadata
  2. Enrich references with abstracts via vector similarity (all-MiniLM-L6-v2)
  3. Distill CoT reasoning chains from DeepSeek-R1
  4. Run SFT on (metadata, CoT, outline) triples
  5. Run GRPO with sampled candidates and reward computation
  6. Evaluate on held-out test set (post-2025 papers to minimize pretrain contamination)

- Design tradeoffs:
  - **8B model size vs. capability**: Paper shows 8B Qwen3-SFT-GRPO outperforms larger API models on structural fidelity, but may lag on complex domain reasoning.
  - **End-to-end vs. modular**: Single-pass inference trades off interpretability for coherence; cannot easily debug intermediate decisions.
  - **Reward design**: λ parameter trades off structural faithfulness to human patterns vs. format correctness; paper does not report sensitivity analysis.

- Failure signatures:
  - **Context overflow**: Input exceeds 8B model context → truncation or chunking breaks end-to-end assumption
  - **Reward hacking**: Model generates syntactically valid but semantically empty outlines to maximize format compliance
  - **Domain shift**: Outlines on topics poorly represented in arXiv/bioRxiv/medRxiv may degrade (e.g., humanities, social sciences)
  - **CoT collapse**: Student model learns to generate shallow reasoning chains that don't improve final output quality

- First 3 experiments:
  1. **Ablate CoT distillation**: Train SFT without CoT data, compare to full pipeline on structural distance and LLM-Judge scores.
  2. **Vary λ in reward weighting**: Test λ ∈ {0.2, 0.5, 0.8} to understand sensitivity of structural vs. format optimization.
  3. **Cross-domain generalization**: Evaluate on held-out domains not in training corpus (e.g., legal surveys, education research) to assess out-of-distribution robustness.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited ablation studies: Paper lacks direct comparative analysis between end-to-end vs. multi-stage approaches on identical inputs
- Domain bias: Training corpus limited to arXiv/bioRxiv/medRxiv, potentially limiting generalization to humanities and social sciences
- Evaluation methodology: Relies entirely on LLM-as-a-Judge without human-grounded validation for claimed structural and content superiority

## Confidence
- **High confidence**: The technical feasibility of end-to-end metadata-to-outline generation and the reported performance improvements over existing benchmarks (SurveyX, SurveyEval)
- **Medium confidence**: The claimed advantages of end-to-end over multi-stage approaches and the effectiveness of CoT distillation for structural reasoning
- **Low confidence**: The robustness of LLM-Judge evaluations across diverse domains and the generalizability beyond arXiv/bioRxiv/medRxiv corpora

## Next Checks
1. Conduct human evaluation on a subset of generated outlines to validate LLM-Judge scores, particularly for structure and content dimensions
2. Perform systematic ablation studies comparing: (a) end-to-end vs. modular multi-stage approaches on identical inputs, (b) SFT with vs. without CoT distillation
3. Test cross-domain generalization on surveys from humanities, social sciences, and legal domains to assess distributional robustness beyond the STEM-focused training corpus