---
ver: rpa2
title: A Multi-Stage Large Language Model Framework for Extracting Suicide-Related
  Social Determinants of Health
arxiv_id: '2508.05003'
source_url: https://arxiv.org/abs/2508.05003
tags:
- problem
- sdoh
- factors
- factor
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a multi-stage large language model (LLM) framework
  for extracting suicide-related social determinants of health (SDoH) factors from
  unstructured death investigation notes. The framework uses a three-step process:
  context retrieval to find relevant sentences, relevance verification to filter them,
  and final SDoH factor extraction.'
---

# A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health

## Quick Facts
- **arXiv ID:** 2508.05003
- **Source URL:** https://arxiv.org/abs/2508.05003
- **Reference count:** 40
- **Primary result:** Multi-stage LLM framework achieves 17.7% F1-score improvement over fine-tuned BioBERT for infrequent SDoH factors

## Executive Summary
This paper presents a multi-stage large language model framework for extracting suicide-related social determinants of health (SDoH) factors from unstructured death investigation notes. The framework employs a three-step process: context retrieval to find relevant sentences, relevance verification to filter them, and final SDoH factor extraction. Evaluated on the National Violent Death Reporting System (NVDRS) dataset, the framework demonstrates significant improvements over traditional approaches, particularly for infrequent SDoH factors. A pilot user study shows that experts using the framework's intermediate explanations annotated SDoH factors 62.39 seconds faster per incident while maintaining 83.33% accuracy, demonstrating improved efficiency and transparency.

## Method Summary
The framework implements a three-stage pipeline: (1) Context retrieval using GPT-3.5-turbo to identify relevant sentences from death investigation notes, (2) Relevance verification using either fine-tuned FLAN-T5-base or GPT-3.5-turbo as a binary classifier to filter retrieved sentences, and (3) SDoH extraction using GPT-3.5-turbo to make final True/False decisions based only on verified sentences. The approach operates in a zero-shot setting for the final extraction stage. The framework was evaluated on the NVDRS 2020 dataset using 18 SDoH factors (10 infrequent, 8 frequent) and compared against fine-tuned BioBERT, GPT-3.5-turbo End2End, and GPT-3.5-turbo Chain-of-Thought baselines.

## Key Results
- Achieved 17.7% average F1-score improvement over fine-tuned BioBERT for infrequent SDoH factors
- Improved context retrieval accuracy from 59.3% to 73.1% through relevance verification stage
- Experts annotated SDoH factors 62.39 seconds faster per incident while maintaining 83.33% accuracy using framework's explanations
- Outperformed BioBERT by 4.0% F1-score on frequent factors in zero-shot setting

## Why This Works (Mechanism)

### Mechanism 1: Noise Reduction via Two-Stage Filtering
- **Claim:** Decomposing the extraction task into sentence retrieval and subsequent verification significantly reduces irrelevant context, improving the signal-to-noise ratio for the final decision stage.
- **Core assumption:** The final LLM performs better when reasoning over a concise set of verified sentences rather than the full, noisy text, likely due to reduced "distraction" or context window saturation.
- **Evidence anchors:**
  - Relevance verification stage improved overall context retrieval accuracy from 59.3% to 73.1%
  - "This step ensures that only the most contextually pertinent sentences are passed on, significantly reducing noise..."
  - Fine-tuned BioBERT showed limited generalizability to low-frequency SDoH factors
- **Break condition:** If the verification model is overly aggressive (high precision, low recall) or misinterprets the factor definition, it may truncate the context necessary for the final stage, leading to false negatives.

### Mechanism 2: Zero-Shot Generalization on Long-Tailed Classes
- **Claim:** A multi-stage zero-shot LLM framework can outperform fine-tuned discriminative models (like BioBERT) on infrequent factors by leveraging pre-trained semantic understanding rather than relying on class frequency.
- **Core assumption:** The semantic gap between general pre-training and specific clinical extraction is bridged more effectively by explicit reasoning steps than by fine-tuning on sparse labels.
- **Evidence anchors:**
  - 17.7% average F1-score improvement over fine-tuned BioBERT for infrequent SDoH factors
  - Fine-tuned BioBERT showed limited generalizability to low-frequency SDoH factors
  - LLMs are being leveraged for SDoH encoding, but do not explicitly validate the specific "long-tail" performance gain mechanism
- **Break condition:** If the definitions provided in the prompt are ambiguous or the context is highly idiomatic, the zero-shot approach may hallucinate or miss the factor entirely without labeled data to correct it.

### Mechanism 3: Human-in-the-Loop Acceleration via Explainability
- **Claim:** Exposing the model's intermediate "reasoning" (retrieved context) reduces the cognitive load for human annotators, speeding up the task without compromising accuracy.
- **Core assumption:** The accuracy of the "Context Retrieval" stage is high enough that experts can trust the highlighted text as a sufficient proxy for reading the full document.
- **Evidence anchors:**
  - Experts using the framework's intermediate explanations annotated SDoH factors 62.39 seconds faster per incident while maintaining 83.33% accuracy
  - "Participants noted that the AI assistance resulted in quicker, more accurate, and more confident annotations..."
- **Break condition:** If the retrieval mechanism "hides" critical contradictory evidence (e.g., a sentence that negates the factor) by not highlighting it, human annotators may unknowingly approve false positives.

## Foundational Learning

- **Concept:** Long-Tailed Distribution
  - **Why needed here:** The paper specifically targets this statistical challenge in SDoH data, where "infrequent" factors are critical but rare. Understanding this explains why standard classifiers (BioBERT) fail and why the authors use LLMs.
  - **Quick check question:** Why would a model trained to maximize overall accuracy struggle with a rare class like "Exposure to Disaster" even if it excels at "Alcohol Problem"?

- **Concept:** Zero-Shot Learning
  - **Why needed here:** The proposed framework operates in a zero-shot setting for the final extraction, meaning it wasn't trained on the specific NVDRS labels for that stage. This is key to its flexibility compared to fine-tuned baselines.
  - **Quick check question:** How does the model "know" what constitutes a "Civil Legal Problem" if it was not explicitly trained on NVDRS examples for that factor?

- **Concept:** Pipeline vs. End-to-End Architecture
  - **Why needed here:** The paper's core contribution is breaking the monolithic extraction task into a pipeline (Retrieval -> Verification -> Extraction). Distinguishing this from End2End prompting is necessary to understand the performance trade-offs.
  - **Quick check question:** What is the trade-off in terms of latency or complexity when replacing a single LLM call with a three-stage pipeline?

## Architecture Onboarding

- **Component map:** Death investigation notes -> NLTK sentence splitter -> Stage 1 (Context Retrieval via GPT-3.5) -> Stage 2 (Relevance Verification via FLAN-T5 or GPT-3.5) -> Stage 3 (SDoH Extraction via GPT-3.5)
- **Critical path:** The Stage 2 Verification is the critical differentiator. If this component is removed or fails, the noise reduction collapses, likely degrading performance back to baseline levels.
- **Design tradeoffs:**
  - Latency vs. Accuracy: The multi-stage approach requires multiple LLM calls (higher latency/cost) compared to End2End, but provides higher F1-scores on infrequent factors
  - Explainability vs. Black Box: Stage 2 provides transparent intermediate artifacts (sentences), unlike BioBERT
  - Model Size: The authors suggest Stage 2 can use a smaller fine-tuned model (FLAN-T5) to reduce cost, while Stage 3 requires a larger LLM for reasoning
- **Failure signatures:**
  - Hallucinated Context: Stage 1 retrieves sentences that look relevant but don't exist in the source text
  - Over-Filtering: Stage 2 is too strict, resulting in an empty context list for Stage 3, forcing a "False" negative
  - JSON Parsing Errors: The pipeline relies on valid JSON output from the LLMs; malformed JSON breaks the chain
- **First 3 experiments:**
  1. Ablation Study on Stage 2: Run the pipeline skipping Stage 2 to quantify the exact F1-score drop
  2. Prompt Robustness Test: Vary the factor definitions in the prompt to see how sensitive the zero-shot Stage 3 is to prompt engineering
  3. Latency Profiling: Measure the end-to-end runtime difference between the "GPT-3.5 End2End" baseline and the full 3-stage pipeline

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the framework's extraction performance vary across different demographic subpopulations (e.g., race, sex) within the NVDRS dataset?
  - Basis: The authors state that "differences may introduce variability in model performance across demographic groups" and explicitly call for "future research to systematically evaluate model fairness across subpopulations."

- **Open Question 2:** Can the efficiency gains observed in the pilot user study (62.39 seconds faster) be replicated in a between-subjects experimental design?
  - Basis: The authors acknowledge that the "within-subject design may introduce potential confounding effects related to task familiarity" and explicitly suggest that "future studies should adopt a between-subjects design."

- **Open Question 3:** Does the multi-stage framework maintain high accuracy when expanded to the full range of over 600 SDoH factors defined in the NVDRS coding manual?
  - Basis: The paper notes the current work evaluated only 18 factors and states, "Expanding this framework in future research to incorporate a broader spectrum of SDoH factors would further enhance the comprehensiveness of information extraction."

## Limitations
- Evaluation relies on a curated gold-standard test set of 655 sentences from 160 cases, which may not fully represent the full NVDRS dataset distribution
- Zero-shot performance claims are contingent on prompt engineering quality, which is not extensively explored for robustness
- Human study involved only 6 experts annotating 12 cases, limiting statistical power and generalizability

## Confidence

- **High Confidence:** Framework architecture design (multi-stage pipeline), comparative performance metrics against BioBERT and End2End baselines, and the general noise-reduction mechanism of the verification stage
- **Medium Confidence:** Zero-shot generalization claims for infrequent factors, as these depend heavily on prompt quality and may not generalize across different SDoH definitions or datasets
- **Low Confidence:** Scalability and latency claims, as these are not explicitly measured or compared against baselines in the paper

## Next Checks
1. **Prompt Robustness Testing:** Systematically vary the factor definitions and context retrieval prompts to measure sensitivity to prompt engineering and identify breaking points
2. **Temporal Extraction Verification:** Implement explicit temporal extraction (e.g., date parsing) to validate that the two-week constraint is reliably enforced, not just prompt-dependent
3. **Larger Human Study:** Expand the user study to include more experts and cases to establish statistical significance for the efficiency gains and accuracy maintenance claims