---
ver: rpa2
title: 'CS3-Bench: Evaluating and Enhancing Speech-to-Speech LLMs for Mandarin-English
  Code-Switching'
arxiv_id: '2510.07881'
source_url: https://arxiv.org/abs/2510.07881
tags:
- speech
- language
- arxiv
- performance
- code-switching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CS3-Bench, the first code-switching speech-to-speech
  benchmark for Mandarin-English interactions. The authors find that existing models
  show up to 66% performance drop in knowledge-intensive QA when handling code-switched
  input, along with significant understanding failures in open-ended conversations.
---

# CS3-Bench: Evaluating and Enhancing Speech-to-Speech LLMs for Mandarin-English Code-Switching

## Quick Facts
- arXiv ID: 2510.07881
- Source URL: https://arxiv.org/abs/2510.07881
- Reference count: 0
- Primary result: Up to 66% performance drop in knowledge QA when models handle code-switched input

## Executive Summary
This paper introduces CS3-Bench, the first code-switching speech-to-speech benchmark for Mandarin-English interactions. Existing models show significant performance degradation on code-switched input, with up to 66% drop in knowledge-intensive QA accuracy. To address this, the authors propose a multi-faceted approach combining Chain of Recognition (CoR) to improve English content recognition in queries and Keyword Highlighting (KH) to guide generation of mixed-language responses. Their method improves knowledge accuracy from 25.14% to 46.13% and increases open-ended understanding from 64.5% to 86.5%, while reducing pronunciation errors in secondary language speech.

## Method Summary
The authors construct CS3-Bench using a speech synthesis and translation pipeline, creating 19.4k code-switching instances validated through ASR-in-the-loop. They propose two main techniques: Chain of Recognition (CoR) which prepends recognition tokens to force attention to foreign-language named entities during training, and Keyword Highlighting (KH) which uses special tokens to mark language transitions. These are implemented on VocalNet-ML, a speech-to-speech LLM based on Qwen2.5-7B-Instruct with LoRA fine-tuning, frozen speech encoder, and multi-token prediction decoder. The model is trained on synthetic code-switching data plus speech translation samples.

## Key Results
- Knowledge accuracy improves from 25.14% to 46.13% with proposed method
- Open-ended understanding increases from 64.5% to 86.5%
- Pronunciation success rate and WER both improve with CoR/KH
- Up to 66% performance drop observed on code-switched input without intervention

## Why This Works (Mechanism)

### Mechanism 1: Chain of Recognition (CoR)
- **Claim:** Explicitly recognizing English terms before response generation improves comprehension in code-switched queries
- **Mechanism:** A pre-task token sequence is prepended to model output during training, forcing attention to foreign-language named entities. The recognition tokens are discarded at inference, leaving only improved internal representations
- **Core assumption:** Recognition errors on English content (especially rare named entities) are the primary failure mode in code-switched understanding
- **Evidence anchors:**
  - [abstract] "introducing Chain of Recognition (CoR) to improve English content recognition in queries"
  - [section 3.2] "we strictly limit the recognition length to a maximum of four individual words to avoid impairing the instruction-following capability"
  - [corpus] Limited direct evidence; related work CS-Dialogue notes code-switching ASR challenges but doesn't validate CoR specifically
- **Break condition:** If recognition tokens exceed ~4 words or include non-English content, instruction-following degrades per authors' observation

### Mechanism 2: Keyword Highlighting (KH)
- **Claim:** Indicative tokens marking language transitions guide both text and speech decoders to produce cleaner mixed-language output
- **Mechanism:** Special tokens (e.g., `<en>...</en>`) wrap secondary-language segments during training. The backbone LLM (via LoRA) learns to emit these markers, and the speech decoder conditions on them for pronunciation
- **Core assumption:** Explicit boundary signals reduce ambiguity in how speech tokens should be generated for foreign-language segments
- **Evidence anchors:**
  - [abstract] "Keyword Highlighting (KH) to guide generation of mixed-language responses"
  - [section 3.3] "keyword highlighting guides the speech decoder to produce speech tokens related to both languages"
  - [corpus] No corpus papers validate KH; assumption relies on paper's internal ablation
- **Break condition:** Applying CoR and KH simultaneously causes degradation due to "inconsistency" between recognized vs. generated English items (Table 2 discussion)

### Mechanism 3: Language Alignment Data Construction
- **Claim:** Synthetic code-switching speech data, verified through ASR-in-the-loop validation, provides sufficient training signal despite 80% synthesis rejection rate
- **Mechanism:** Rewriter LLM converts Chinese→mixed text; Checker LLM validates naturalness; CosyVoice2 synthesizes speech; Whisper-large-v3 verifies English content correctness. Only ~20% pass quality filters
- **Core assumption:** The remaining 20% of synthetic instances are representative of natural code-switching patterns and don't introduce systematic artifacts
- **Evidence anchors:**
  - [section 3.1] "20% of the text examples are successfully synthesized and preserved, resulting in 19.4k instances"
  - [table 2] "+ ST, CS data" shows accuracy jump from 26.24% → 41.99%
  - [corpus] PROST-LLM uses similar progressive synthetic data strategies for speech translation
- **Break condition:** If ASR verification (Whisper) has systematic biases on code-switched content, synthetic data may inherit those errors

## Foundational Learning

- **Concept: Code-Switching Types (Acoustic, Semantic, Integrated)**
  - **Why needed here:** The benchmark partitions failure modes; "acoustic" involves phonetically similar proper names, "semantic" involves conceptual mappings, "integrated" mixes both. Strategies like KH show stronger gains on semantic/integrated (Table 2)
  - **Quick check question:** Can you explain why a query like "CRISPR-Cas9 的原理是什么？" would be "integrated" rather than purely "semantic"?

- **Concept: Speech-to-Speech LLM Architecture (Encoder-LLM-Decoder)**
  - **Why needed here:** VocalNet-ML uses frozen speech encoder, LoRA-adapted LLM backbone, and multi-token prediction (MTP) decoder. CoR/KH modify token sequences at the LLM level but require decoder awareness
  - **Quick check question:** Where in the architecture do KH tokens exert influence—text decoder, speech decoder, or both?

- **Concept: Multi-Token Prediction (MTP)**
  - **Why needed here:** Accelerates decoding by predicting multiple speech tokens per step. Authors don't ablate whether MTP interacts with KH markers, but it's the default generation mode
  - **Quick check question:** If MTP predicts 4 tokens ahead, how might this interact with KH boundary markers at segment edges?

## Architecture Onboarding

- **Component map:**
  Speech Encoder (frozen) -> LLM Backbone (Qwen2.5-7B-Instruct + LoRA) -> Text Decoder -> Speech Decoder (MTP) -> Speech Vocoder

- **Critical path:**
  1. Code-switched audio → Speech Encoder
  2. Hidden states → LLM (with CoR pre-task during training)
  3. LLM output includes KH-marked text tokens
  4. Speech Decoder attends to KH markers for language-transition guidance
  5. Vocoder produces final audio

- **Design tradeoffs:**
  - CoR alone vs. KH alone: CoR improves understanding (86.5% open-ended) and WER (15.18%); KH improves knowledge accuracy (46.13%) but slightly lower understanding (84.0%)
  - CoR + KH together: Causes degradation—authors hypothesize inconsistency between recognized vs. generated English items
  - Data volume: 19.4k CS instances vs. millions in competing models; authors attribute performance gap largely to this disparity

- **Failure signatures:**
  - Recognition errors on rare named entities → incorrect knowledge responses
  - Missing or misplaced KH markers → pronunciation errors in secondary language
  - Simultaneous CoR + KH → training instability, degraded performance
  - Whisper verification failures → low synthetic data yield (80% rejected)

- **First 3 experiments:**
  1. **Baseline reproduction:** Evaluate VocalNet-ML on CS3-Bench knowledge set; expect ~26% accuracy. Verify PSR and WER metrics match Table 2
  2. **CoR ablation:** Add CoR tokens (max 4 words) to training; measure understanding rate change on open-ended set. Expect ~86.5% if implemented correctly
  3. **Data scaling test:** Double CS training data (if compute allows) and measure knowledge accuracy delta. If improvement plateaus, investigate whether ASR verification is filtering out valid natural patterns

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the training process be modified to resolve the inconsistency between recognized and generated English items, allowing the effective simultaneous application of Chain of Recognition (CoR) and Keyword Highlighting (KH)?
- **Basis in paper:** [explicit] The authors note that applying both methods simultaneously causes degradation, "probably due to an inconsistency - the recognized and generated English items are consistent in some examples but differ in others, which disrupts the training process"
- **Why unresolved:** The paper identifies the conflict but does not propose a mechanism to align recognition tokens with generation keywords when they differ
- **What evidence would resolve it:** A training strategy that harmonizes CoR and KH outputs (e.g., dynamic alignment or distinct token handling) resulting in performance metrics exceeding those of either method used individually

### Open Question 2
- **Question:** To what extent does the reliance on synthetic speech generation and LLM-based rewriting in the data construction pipeline limit the model's ability to handle natural, spontaneously code-switched human speech?
- **Basis in paper:** [inferred] The training data and benchmark queries are synthesized using CosyVoice2 and rewritten by LLMs (Section 2.1, 3.1), which may lack the prosodic irregularities or lexical spontaneity of natural code-switching
- **Why unresolved:** The paper evaluates on synthetic benchmarks, leaving the "sim-to-real" gap for naturalistic, noisy, or colloquial code-switching unexplored
- **What evidence would resolve it:** Evaluation results on a dataset of human-spoken, naturally occurring code-switched conversations (rather than synthesized or rephrased text)

### Open Question 3
- **Question:** Do the Chain of Recognition and Keyword Highlighting strategies transfer effectively to code-switching scenarios involving language pairs other than Mandarin-English?
- **Basis in paper:** [inferred] The benchmark and methods are specifically tailored for Mandarin-English interactions (Title, Abstract), relying on specific syntactic assumptions about how English terms are embedded in Mandarin
- **Why unresolved:** Code-switching patterns (e.g., intra-sentential vs. borrowing) vary significantly across language pairs, and the current architecture may be overfitted to the specific characteristics of Mandarin-English alignment
- **What evidence would resolve it:** Results from applying the same CoR and KH techniques to a different code-switching pair (e.g., Spanish-English or Hindi-English) using the proposed pipeline

## Limitations

- ASR verification (Whisper-large-v3) filters out 80% of synthetic instances, potentially introducing systematic biases that the model optimizes for rather than natural code-switching patterns
- Simultaneous application of CoR and KH causes degradation with no proposed solution beyond qualitative "inconsistency" explanation
- Limited training data (19.4k CS instances) compared to millions in competing models may explain much of the performance gap

## Confidence

**High Confidence:**
- The 66% performance drop on code-switched input is directly measured on CS3-Bench
- Individual method improvements (CoR or KH alone) are validated through ablation studies
- Pronunciation errors and WER metrics are objectively measurable

**Medium Confidence:**
- The CoR mechanism's assumption that recognition errors are the primary failure mode
- KH's assumption that explicit boundary signals reduce pronunciation ambiguity
- The claim that 20% of synthetic data is representative of natural patterns

**Low Confidence:**
- The explanation for CoR + KH degradation (inconsistency hypothesis untested)
- Generalization of results beyond Mandarin-English to other language pairs
- Long-term stability of improvements without continual code-switching exposure

## Next Checks

1. **ASR Bias Investigation**: Run Whisper-large-v3 on both accepted and rejected synthetic samples to identify systematic patterns in rejections. Compare linguistic features (named entity frequency, switching points, phrase lengths) between accepted vs. rejected data to determine if filtering introduces bias

2. **CoR + KH Reconciliation Experiment**: Systematically vary the timing and conditioning between CoR recognition and KH generation. Test whether delaying KH marker emission until after CoR completion, or using CoR output as guidance for KH placement, resolves the inconsistency issue

3. **Data Scaling Validation**: Train parallel models with (a) 2x the current CS data volume, (b) 0.5x the volume, and (c) the current volume with CoR/KH disabled. Measure performance deltas to quantify whether improvements stem from methodology or data quantity, and identify saturation points