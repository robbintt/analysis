---
ver: rpa2
title: Weight Decay may matter more than muP for Learning Rate Transfer in Practice
arxiv_id: '2510.19093'
source_url: https://arxiv.org/abs/2510.19093
tags:
- weight
- learning
- rate
- decay
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates learning rate transfer across model widths
  in neural network training, focusing on the role of weight decay and the Maximal
  Update Parameterization (muP). The authors find that muP's core assumptions about
  alignment between weights and updates break down during practical training, especially
  when batch size is large relative to model width.
---

# Weight Decay may matter more than muP for Learning Rate Transfer in Practice

## Quick Facts
- arXiv ID: 2510.19093
- Source URL: https://arxiv.org/abs/2510.19093
- Authors: Atli Kosson; Jeremy Welborn; Yang Liu; Martin Jaggi; Xi Chen
- Reference count: 40
- Primary result: Weight decay, not muP, enables learning rate transfer across model widths

## Executive Summary
This work investigates learning rate transfer across model widths in neural network training, focusing on the role of weight decay and the Maximal Update Parameterization (muP). The authors find that muP's core assumptions about alignment between weights and updates break down during practical training, especially when batch size is large relative to model width. Instead of muP's scaling rules, it is independent weight decay that stabilizes feature learning across widths and enables effective learning rate transfer. This reveals muP's practical role as an implicit learning rate warmup, which can be largely replaced by explicit warmup schedules. The findings challenge prevailing beliefs about muP's necessity and provide practical guidance for achieving robust learning rate transfer in large-scale training.

## Method Summary
The authors conduct extensive experiments across multiple architectures (MLPs, transformers) and tasks (synthetic regression, CIFAR-10, WikiText-2) to investigate learning rate transfer. They systematically vary model width, depth, batch size, learning rate, and weight decay to study their interactions. The experiments compare muP scaling with independent weight decay scaling, examining both feature learning stability and optimal learning rate transferability. The study uses controlled synthetic tasks to isolate the effects of different hyperparameter configurations on training dynamics.

## Key Results
- Independent weight decay stabilizes feature learning across widths, enabling effective learning rate transfer
- muP's core assumptions about weight-update alignment break down during practical training, particularly with large batch sizes
- muP's practical role is primarily as implicit learning rate warmup, which can be replaced by explicit warmup schedules
- The importance of weight decay for learning rate transfer may be overstated in controlled experiments

## Why This Works (Mechanism)

## Foundational Learning
- **Weight decay scaling**: Critical for stabilizing feature learning across different model widths. Without proper weight decay scaling, feature learning becomes unstable as width changes.
- **muP parameterization**: Designed to keep pre-activations and activations finite as width increases, but relies on assumptions that break down in practice.
- **Batch size effects**: Large batch sizes relative to model width exacerbate the breakdown of muP's alignment assumptions.
- **Feature learning dynamics**: The rate at which features are learned during training affects generalization and transfer capabilities across model sizes.
- **Learning rate transfer**: The ability to reuse learning rates across different model widths without significant performance degradation.

## Architecture Onboarding

**Component Map**: Data -> Model (MLP/Transformer) -> Loss -> Optimizer (with weight decay) -> Updates -> Weights

**Critical Path**: Hyperparameters (width, depth, batch size, learning rate, weight decay) -> Training dynamics -> Feature learning stability -> Generalization performance

**Design Tradeoffs**: 
- muP provides implicit warmup but breaks down with large batch sizes
- Independent weight decay provides explicit stabilization but requires careful tuning
- Large batch sizes improve throughput but destabilize feature learning
- Model width affects feature learning dynamics and optimal learning rate selection

**Failure Signatures**: 
- Poor learning rate transfer across widths
- Unstable feature learning during training
- Performance degradation when scaling model width
- Inconsistent optimal learning rates across different widths

**3 First Experiments**:
1. Train models of different widths with fixed weight decay to observe feature learning stability
2. Compare muP scaling versus independent weight decay scaling for learning rate transfer
3. Vary batch size relative to width to quantify muP alignment breakdown

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical findings may not generalize to larger-scale training scenarios beyond the tested models and tasks
- The importance of weight decay for learning rate transfer might be overstated in controlled experiments
- Study focuses primarily on feedforward networks and transformers, leaving questions about other architectures
- The characterization of muP as implicit warmup is observational and may oversimplify its benefits

## Confidence
**High confidence**: The empirical observation that independent weight decay improves learning rate transfer across widths is well-supported by experiments across multiple tasks and architectures.

**Medium confidence**: The characterization of muP as primarily providing implicit warmup is plausible but may oversimplify muP's benefits in certain training scenarios, particularly for very deep or wide models.

**Medium confidence**: The claim that muP's core assumptions about weight-update alignment break down during practical training is supported by experiments but may depend on specific hyperparameter regimes not fully explored.

## Next Checks
1. Test the weight decay transfer hypothesis on production-scale language models (e.g., 1B+ parameters) trained on diverse datasets to verify if independent weight decay maintains its stabilizing effect.

2. Conduct ablation studies systematically varying batch size, width, and depth to quantify the precise conditions under which muP's alignment assumptions fail.

3. Compare learning rate transfer performance using weight decay-based stabilization versus explicit warmup schedules across multiple downstream tasks to determine which approach generalizes better to unseen model sizes.