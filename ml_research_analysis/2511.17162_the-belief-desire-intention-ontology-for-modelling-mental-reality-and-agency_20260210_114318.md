---
ver: rpa2
title: The Belief-Desire-Intention Ontology for modelling mental reality and agency
arxiv_id: '2511.17162'
source_url: https://arxiv.org/abs/2511.17162
tags:
- ontology
- agent
- mental
- which
- intention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a formal BDI Ontology as a modular design
  pattern capturing the cognitive architecture of agents through beliefs, desires,
  and intentions, ensuring semantic precision and reusability. Two key experiments
  demonstrate its applicability: (1) coupling the ontology with LLMs via Logic Augmented
  Generation (LAG) shows that ontological grounding enhances the model''s ability
  to detect logical inconsistencies and generate coherent mental-state representations;
  (2) integrating the ontology within the Semas reasoning platform using the Triples-to-Beliefs-to-Triples
  (T2B2T) paradigm enables a bidirectional flow between RDF triples and agent mental
  states.'
---

# The Belief-Desire-Intention Ontology for modelling mental reality and agency

## Quick Facts
- arXiv ID: 2511.17162
- Source URL: https://arxiv.org/abs/2511.17162
- Reference count: 37
- Introduces a formal BDI Ontology as a modular design pattern for capturing cognitive architecture of agents through beliefs, desires, and intentions, ensuring semantic precision and reusability.

## Executive Summary
This paper introduces a formal BDI Ontology designed to capture the cognitive architecture of artificial agents through beliefs, desires, and intentions, providing a semantically precise and reusable design pattern. The ontology is validated through two key experiments: coupling with LLMs via Logic Augmented Generation (LAG) to detect logical inconsistencies, and integration within the Semas reasoning platform using a Triples-to-Beliefs-to-Triples (T2B2T) paradigm for bidirectional interoperability between RDF triples and agent mental states. Together, these contributions demonstrate how the BDI Ontology serves as both a conceptual and operational bridge between declarative and procedural intelligence, enabling explainable, semantically interoperable, and cognitively grounded multi-agent systems.

## Method Summary
The authors designed a modular OWL ontology grounded in the BDI theory of Michael Bratman, defining core classes like Agent, MentalState, MentalProcess, and Plan. They validated the ontology through two experimental setups: (1) using LAG to couple the ontology schema with LLMs, prompting them to map natural language tasks to structured RDF triples while detecting logical inconsistencies; and (2) integrating the ontology within the Semas reasoning platform, implementing a T2B2T layer to enable bidirectional translation between RDF triples and agent mental states. The ontology leverages DOLCE+DnS UltraLite (DUL) as an upper-level ontology and includes extensions for justifications, temporal validity, and goal descriptions.

## Key Results
- Ontology-augmented LLM prompting (LAG) successfully uncovered logical inconsistencies that plain LLM models overlooked, such as task preconditions conflicting with agent beliefs.
- Integration with the Semas reasoning platform enabled a bidirectional flow between RDF triples and agent mental states through the T2B2T paradigm.
- The ontology provides a semantic bridge between declarative knowledge (RDF) and procedural reasoning, supporting explainable AI through explicit justification modeling.

## Why This Works (Mechanism)

### Mechanism 1: Logic Augmented Generation (LAG) for Consistency
The ontology defines rigid semantic types (e.g., `Belief`, `Desire`) and relations (e.g., `motivates`, `fulfils`). When an LLM is prompted to map natural language tasks to this schema, it must adhere to the ontology's axioms, forcing the model to validate preconditions against task requirements and surface contradictions identified by formal logic.

### Mechanism 2: T2B2T (Triples-to-Beliefs-to-Triples) Interoperability
The "Triples-to-Beliefs-to-Triples" paradigm functions as a synchronization bridge. External RDF data are ingested and reified as internal `Beliefs`. The agent reasons over these using a logic engine, generating `Intentions` and `Plans` that are then serialized back into RDF triples, updating the shared environment.

### Mechanism 3: Cognitive Traceability via Explicit Justification
The ontology introduces a `Justification` class linked to `MentalEntity`. When a mental state is formed, the system generates a `Justification` instance that explicitly references the conflicting `Belief` or `WorldState`, creating a queryable graph explaining why an agent adopted a specific state.

## Foundational Learning

- **Concept: BDI Model (Bratman)**
  - Why needed here: This paper is a formalization of Bratman's theory. You must distinguish *Informational* (Belief), *Motivational* (Desire), and *Deliberative* (Intention) stances to understand the ontology's class hierarchy.
  - Quick check question: Can you explain why an agent might have a "Desire" to fly but no "Intention" to do so? (Answer: Desire is abstract preference; Intention requires a commitment to a feasible plan).

- **Concept: Ontology Design Patterns (ODP) & DOLCE**
  - Why needed here: The authors build on DOLCE+DnS UltraLite (DUL). You need to grasp the distinction between `Endurant` (objects/states existing in time, like Beliefs) and `Perdurant` (events/processes happening in time, like Mental Processes) to read the axioms.
  - Quick check question: In this ontology, is a "Belief" modeled as an Event or an Object? (Answer: It's a `MentalState`, treated as an endurant/situation).

- **Concept: Logic Augmented Generation (LAG)**
  - Why needed here: One of the paper's key contributions is coupling LLMs with symbolic logic. Understanding LAG requires knowing how prompt engineering can force an LLM to output structured data (RDF/Turtle) rather than free text.
  - Quick check question: How does providing an ontology in the prompt change the LLM's output format compared to standard prompting? (Answer: It constrains the output to valid knowledge graph triples conforming to the schema).

## Architecture Onboarding

- **Component map:**
  - BDI Ontology (OWL) -> LAG Interface (LLM) -> Semas (Reasoning Platform) -> T2B2T Layer

- **Critical path:**
  1. **Perception:** An external event (e.g., "Push notification") is converted to RDF.
  2. **Ingestion:** The T2B2T layer maps this RDF to a `WorldState` and triggers a `BeliefProcess`.
  3. **Deliberation:** The `Belief` triggers a `DesireProcess` -> `Desire` -> `IntentionProcess` -> `Intention`.
  4. **Planning:** The `Intention` specifies a `Plan` composed of `Tasks`.
  5. **Action:** The `PlanExecution` generates a new `WorldState` and outputs it as RDF.

- **Design tradeoffs:**
  - **Decoupling Goals from Desires:** The paper models `Goals` as descriptions/planning objects, not mental states. This allows goals to be shared across agents but adds a layer of abstraction between "wanting" (Desire) and "targeting" (Goal).
  - **Expressivity vs. Decidability:** The ontology uses $\mathcal{SROIQ}(D)$ (OWL 2 DL). While expressive, reasoning complexity is high (though the paper uses a Prolog-engine for execution rather than a generic OWL reasoner for real-time control).

- **Failure signatures:**
  - **Stale Beliefs:** If the `hasValidity` temporal property is ignored, agents act on outdated `WorldStates`.
  - **Hallucinated Triples:** The LLM generates triples that are syntactically valid but semantically nonsense (e.g., defining a "Location" as a "Time").
  - **Cyclic Deliberation:** A `Belief` triggers a `Desire` which loops back to trigger the same `Belief` without progress.

- **First 3 experiments:**
  1. **Inconsistency Detection:** Run the provided MS-LaTTE test cases (Section 4.1) to verify that the LAG setup correctly identifies the "Location Contradiction" (e.g., checking into a hotel while at home).
  2. **Trace Generation:** Trigger a specific event (e.g., "Payment Request") and query the knowledge graph to visualize the full chain: `WorldState` -> `Belief` -> `Desire` -> `Intention` -> `Justification`.
  3. **T2B2T Loop:** Set up the Semas environment to ingest a simple RDF triple (e.g., "Light is On"), formulate a plan to turn it off, and verify the resulting "Light is Off" triple is generated back to the RDF store.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the BDI ontology be extended to explicitly model conflicts and trade-offs among mental states, particularly intention-intention conflicts?
- Basis in paper: The conclusion states that future work includes extending the ontology to model conflicts and trade-offs, which are crucial for realistic decision-making but currently omitted to keep the vocabulary flexible.
- Why unresolved: The current design deliberately delegates conflict detection to the reasoner (or LAG layer) based on context rather than encoding it directly into the ontology axioms.
- What evidence would resolve it: A formal extension of the ontology defining conflict relations (e.g., mutually exclusive intentions) and a case study demonstrating automated conflict resolution.

### Open Question 2
- Question: How can the integration between the BDI ontology and Logic-Augmented Generation (LAG) be refined to strengthen the synergy between symbolic and sub-symbolic reasoning?
- Basis in paper: Section 5 lists refining the integration with LAG approaches as a key objective for future work to enhance hybrid AI systems.
- Why unresolved: While initial experiments show improved inconsistency detection, the optimal mechanisms for bidirectional feedback between the formal ontology and probabilistic LLMs remain under-explored.
- What evidence would resolve it: A comparative study showing that a refined LAG framework significantly reduces hallucinations or logical errors compared to standard Retrieval-Augmented Generation (RAG) or non-ontological prompts.

### Open Question 3
- Question: What specific reasoning services and tool supports are necessary to ensure the BDI ontology scales effectively for real-world multi-agent systems?
- Basis in paper: Section 5 notes that future work must address scalability and automated reasoning to make the ontology deployable in operational environments.
- Why unresolved: The paper focuses on the ontological design and initial validation (using SEMAS and LAG) but does not provide the dedicated tooling or infrastructure required for high-volume, industrial-scale agent coordination.
- What evidence would resolve it: The development and benchmarking of software libraries or APIs that utilize the ontology to manage mental states for large agent populations (e.g., thousands of agents) in real-time.

## Limitations
- The primary uncertainty lies in the scalability and robustness of the Logic Augmented Generation (LAG) mechanism, particularly how well the LLM consistently maps complex, ambiguous natural language to the precise ontological schema without hallucinating invalid triples.
- The integration with the Semas reasoning platform relies on a specific Prolog-based execution model, and it's unclear how well this T2B2T paradigm generalizes to other reasoning engines or handles more complex belief revision and plan generation cycles.
- The ontology's completeness is another consideration, as it focuses on core BDI elements but does not extensively cover all aspects of agent interaction, such as learning from experience or handling multi-agent coordination beyond shared goal descriptions.

## Confidence
- **Medium**: The primary uncertainty lies in the scalability and robustness of the Logic Augmented Generation (LAG) mechanism, with limited evidence on how well the LLM consistently maps complex, ambiguous natural language to the precise ontological schema without hallucinating invalid triples.
- **Medium**: The integration with the Semas reasoning platform relies on a specific Prolog-based execution model, and it's unclear how well this "T2B2T" paradigm generalizes to other reasoning engines or handles more complex belief revision and plan generation cycles.
- **Medium**: The ontology's completeness is another consideration, as the paper focuses on core BDI elements but does not extensively cover all aspects of agent interaction, such as learning from experience or handling multi-agent coordination beyond shared goal descriptions.

## Next Checks
1. Test the LAG mechanism with a broader and more diverse set of natural language inputs to assess its robustness and error rate in mapping to the ontology.
2. Implement the T2B2T paradigm with a different reasoning engine (e.g., a standard OWL reasoner or a different rule-based system) to evaluate the generality of the interoperability layer.
3. Design and execute a test scenario involving dynamic belief revision and complex plan execution to identify potential limitations in the ontology's handling of evolving agent states and interactions.