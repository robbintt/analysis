---
ver: rpa2
title: 'From Measurement to Mitigation: Exploring the Transferability of Debiasing
  Approaches to Gender Bias in Maltese Language Models'
arxiv_id: '2507.03142'
source_url: https://arxiv.org/abs/2507.03142
tags:
- bias
- maltese
- language
- gender
- bertu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates gender bias in Maltese language models
  (BERTu and mBERTu) and evaluates the transferability of English debiasing techniques
  to this low-resource, morphologically rich language. Bias was measured using CrowS-Pairs,
  SEAT, and sentence template analysis, revealing that both models exhibit gender
  bias, with monolingual BERTu showing stronger biases.
---

# From Measurement to Mitigation: Exploring the Transferability of Debiasing Approaches to Gender Bias in Maltese Language Models

## Quick Facts
- arXiv ID: 2507.03142
- Source URL: https://arxiv.org/abs/2507.03142
- Authors: Melanie Galea; Claudia Borg
- Reference count: 11
- Primary result: CDA most effective for Maltese models, reducing CrowS scores by up to 13.1% and SEAT scores by up to 14.1%

## Executive Summary
This study investigates gender bias in Maltese language models (BERTu and mBERTu) and evaluates the transferability of English debiasing techniques to this low-resource, morphologically rich language. Bias was measured using CrowS-Pairs, SEAT, and sentence template analysis, revealing that both models exhibit gender bias, with monolingual BERTu showing stronger biases. Among the debiasing methods tested—Counterfactual Data Augmentation (CDA), Dropout Regularization, Auto-Debias, and GuiDebias—CDA was most effective, reducing CrowS scores by up to 13.1% and SEAT scores by up to 14.1%. Dropout Regularization showed moderate success, particularly for multilingual models, while GuiDebias increased bias in Maltese models. t-SNE visualizations confirmed persistent gender associations in BERTu, though multilingual mBERTu exhibited more balanced representations. The findings highlight the need for tailored debiasing approaches and multiple evaluation metrics for low-resource languages.

## Method Summary
The study measured gender bias in Maltese language models using CrowS-Pairs (stereotype likelihood) and SEAT (embedding association test), supplemented with template-based probing. Two models were evaluated: BERTu (monolingual Maltese) and mBERTu (mBERT further pre-trained on Maltese). Four debiasing techniques were applied: Counterfactual Data Augmentation (CDA) with morphologically-aware adaptations, Dropout Regularization on attention weights, Auto-Debias (prompt-based), and GuiDebias (fine-tuning). The researchers created Maltese-specific resources including a gender wordlist (193 pairs translated) and debiasing datasets. Effectiveness was measured by comparing bias scores before and after debiasing, with t-SNE visualizations analyzing embedding space changes.

## Key Results
- Both Maltese models exhibit gender bias, with monolingual BERTu showing stronger bias than multilingual mBERTu
- CDA proved most effective, reducing CrowS scores by up to 13.1% and SEAT scores by up to 14.1%
- Dropout Regularization showed moderate success, particularly for multilingual models
- GuiDebias increased bias in Maltese models rather than reducing it
- t-SNE visualizations showed persistent gender associations in BERTu, with more balanced representations in mBERTu

## Why This Works (Mechanism)

### Mechanism 1: Counterfactual Data Augmentation (CDA)
CDA reduces measured gender bias by balancing gender representation in training data. By creating sentence pairs with gender-swapped terms while preserving context, the model encounters equal associations between gendered terms and occupations/attributes, weakening stereotypical correlations learned from imbalanced pretraining data. The core assumption is that bias manifests through skewed co-occurrence patterns that can be corrected through exposure rebalancing. Evidence anchors show CDA significantly reduced bias across metrics, particularly for monolingual models. Break condition: Naive word-swapping fails in morphologically complex languages—in Maltese, 25.5% of generated sentences contained grammatical errors because verb agreement and morphological inflections weren't updated alongside noun gender.

### Mechanism 2: Dropout Regularization on Attention Weights
Increasing dropout rates on hidden activations and attention weights may reduce gender bias by disrupting biased association pathways. Higher dropout forces the model to distribute representational capacity more broadly, preventing over-reliance on specific gender-attribute correlations that encode stereotypes. The core assumption is that stereotypical associations are encoded through stable, low-entropy neural pathways that can be disrupted by stochastic regularization. Evidence anchors show dropout reduced both CrowS and SEAT scores for English and multilingual models, though results for Maltese models were mixed. Break condition: Effectiveness varies significantly by model architecture (multilingual vs. monolingual) and evaluation metric—dropout reduced implicit bias (SEAT) in multilingual models but increased it in monolingual BERTu.

### Mechanism 3: Multilingual Training as Implicit Regularization
Multilingual models may exhibit lower gender bias in specific languages due to cross-lingual representation sharing. Exposure to diverse grammatical gender systems across languages prevents the model from overfitting to language-specific gender stereotypes, as the same embedding space must accommodate conflicting gender associations from different languages. The core assumption is that cross-linguistic variation in gender associations provides natural regularization against language-specific bias. Evidence anchors show multilingual models (mBERT and mBERTu) exhibit less bias in CrowS scores compared to their monolingual counterparts, with multilingual training providing more generalized representations. Break condition: The paper establishes correlation, not causation—whether multilingual training directly causes bias reduction or whether other factors explain the difference remains an open question.

## Foundational Learning

- Concept: Grammatical gender vs. semantic gender
  - Why needed here: Maltese has grammatical gender marked on nouns, verbs, and adjectives. English-centric methods assume gender is only semantically meaningful, but in morphologically rich languages, gender marking is obligatory and grammatical, not just stereotypical.
  - Quick check question: When you swap "tabib" (male doctor) to "tabiba" (female doctor) in Maltese, what other words in the sentence might need to change to maintain grammaticality?

- Concept: Embedding space geometry and bias
  - Why needed here: The paper uses t-SNE visualizations showing that "tabiba" (female doctor) clusters closer to "inkompetenti" (incompetent) while "tabib" (male doctor) clusters near "kompetenti" (competent). Understanding that bias is encoded in geometric relationships between word vectors is fundamental.
  - Quick check question: If debiasing were fully successful, what would you expect to see in a t-SNE plot of gendered profession pairs and their associated adjectives?

- Concept: Multiple bias metrics measure different phenomena
  - Why needed here: CrowS-Pairs measures explicit stereotyping (which sentence is more likely?), while SEAT measures implicit associations in embedding space. The paper found these metrics sometimes disagreed—GuiDebias increased CrowS scores but decreased SEAT for some models.
  - Quick check question: Why might a debiasing technique reduce measured bias on one metric while increasing it on another?

## Architecture Onboarding

- Component map: BERTu (monolingual Maltese) -> mBERTu (mBERT further pre-trained on Maltese) -> CrowS-Pairs/SEAT/Template evaluation -> CDA/Dropout/Auto-Debias/GuiDebias interventions -> Re-evaluation

- Critical path:
  1. Establish baseline bias measurements on both models using multiple metrics
  2. Prepare Maltese-specific resources: gender wordlist (193 pairs translated), debiasing dataset
  3. Apply debiasing technique with language-appropriate adaptations
  4. Re-evaluate with same metrics; visualize embedding changes
  5. Compare monolingual vs. multilingual model responses to same intervention

- Design tradeoffs:
  - CDA effectiveness vs. grammatical integrity: 25.5% error rate from naive swapping
  - Metric selection: Multiple metrics provide fuller picture but may give conflicting signals
  - Monolingual performance vs. bias: Monolingual models show stronger bias but may have better language-specific capabilities
  - Translation approach: Machine translation vs. manual translation vs. synthetic generation (paper tested all three)

- Failure signatures:
  - Debiasing reduces one metric while increasing another (e.g., Dropout reduced CrowS but increased SEAT for BERTu)
  - Persistent gender clustering in t-SNE post-debiasing (gender terms remain distant, adjectives unevenly distributed)
  - Grammatical errors in counterfactual data introducing noise
  - Methods designed for English showing negative or unpredictable transfer

- First 3 experiments:
  1. Reproduce baseline CrowS-Pairs and SEAT measurements on BERTu/mBERTu to validate your evaluation pipeline against reported scores (CrowS: 55.40/51.20, SEAT: 0.530/0.540)
  2. Implement morphologically-aware CDA that handles verb agreement and adjective inflection alongside noun gender-swapping to reduce the 25.5% grammatical error rate
  3. Test whether debiasing effectiveness correlates with model depth by applying interventions at different transformer layers and measuring per-layer bias changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does multilingual pre-training inherently reduce gender bias compared to monolingual training in low-resource languages?
- Basis in paper: The authors state in the Limitations section that while results suggest multilingual models exhibit reduced bias, "the extent to which multilingual training influences bias remains an open question, requiring further investigation."
- Why unresolved: The study compared BERTu (monolingual) and mBERTu (multilingual) but could not isolate the specific causal mechanisms of multilingual training versus data composition.
- What evidence would resolve it: A controlled ablation study varying the language composition in pre-training data while holding model architecture and total data volume constant.

### Open Question 2
- Question: How does the application of debiasing techniques affect the performance of Maltese language models on downstream NLP tasks?
- Basis in paper: The authors note that "debiasing techniques may unintentionally alter meaningful linguistic relationships" and explicitly call for future work to analyze "how debiasing affects LLM performance on NLP downstream tasks such as Named-Entity Recognition and Sentiment Analysis."
- Why unresolved: The current study focused on bias metrics (CrowS-Pairs, SEAT) and intrinsic embedding analysis, but did not measure the trade-off between bias reduction and task accuracy.
- What evidence would resolve it: Benchmarking debiased BERTu and mBERTu models on standard Maltese downstream tasks (e.g., sentiment analysis, NER) to compare performance drops against bias reduction gains.

### Open Question 3
- Question: Can Counterfactual Data Augmentation (CDA) be adapted to handle the morphological agreement of gendered languages without introducing the 25.5% grammatical error rate observed in this study?
- Basis in paper: The paper highlights that CDA was the most effective method but "constructs poorly crafted sentences for gendered languages" because it swaps words without adjusting related verbs/adjectives, limiting its effectiveness.
- Why unresolved: The simple word-swapping algorithm used failed to account for Maltese's grammatical gender concord, and manual correction was deemed infeasible.
- What evidence would resolve it: Developing a syntax-aware CDA pipeline for Maltese and measuring whether grammatical error rates decrease while maintaining or improving the bias reduction scores.

## Limitations
- Effectiveness of debiasing techniques may vary significantly with corpus size and quality for low-resource languages, with the study using relatively small datasets (~15K sentences for Maltese)
- Grammatical error rates (25.5% in CDA-generated Maltese data) introduce noise that may affect both baseline measurements and debiasing effectiveness
- The study demonstrates correlation between multilingual training and reduced bias but cannot establish causation—other factors such as training data composition may explain the differences

## Confidence
- **High confidence**: CDA effectiveness (consistent positive results across metrics and models), multilingual models showing less bias than monolingual counterparts
- **Medium confidence**: Dropout regularization effectiveness (mixed results across models and metrics), Auto-Debias showing different effects on monolingual vs. multilingual models
- **Low confidence**: Generalization of English debiasing methods to morphologically rich languages, interpretation of metric disagreements (CrowS vs SEAT) as meaningful indicators of debiasing success

## Next Checks
1. Test debiasing effectiveness with varying dataset sizes (from 1K to 50K sentences) to determine the minimum corpus size required for effective gender bias mitigation in low-resource languages.
2. Conduct ablation studies on dropout regularization parameters to identify optimal rates for different Maltese language model architectures, controlling for model depth and attention mechanisms.
3. Perform cross-linguistic transfer experiments by applying Maltese-debiased models to English tasks and vice versa to quantify how morphological complexity affects debiasing transferability.