---
ver: rpa2
title: 'Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects
  in Large Language Models'
arxiv_id: '2502.11028'
source_url: https://arxiv.org/abs/2502.11028
tags:
- calibration
- answer
- accuracy
- confidence
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the calibration of large language models
  (LLMs) in Question-Answering (QA) tasks, focusing on the problem of overconfidence
  where predicted confidence is misaligned with true correctness. The study introduces
  a novel evaluation paradigm using structured distractor-augmented prompts, where
  models select answers from one correct and multiple plausible incorrect options.
---

# Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models

## Quick Facts
- arXiv ID: 2502.11028
- Source URL: https://arxiv.org/abs/2502.11028
- Authors: Prateek Chhikara
- Reference count: 40
- Primary result: Structured distractors reduce LLM calibration error by up to 90% and increase accuracy by up to 460% in QA tasks.

## Executive Summary
This paper investigates LLM calibration in Question-Answering tasks, revealing significant overconfidence where predicted confidence misaligns with actual correctness. The study introduces a novel evaluation paradigm using structured distractor-augmented prompts, where models select answers from one correct and multiple plausible incorrect options. Across nine state-of-the-art LLMs, incorporating distractors substantially improves calibration, reducing Expected Calibration Error by up to 90% and increasing accuracy by up to 460% relative to standard free-generation settings. The findings demonstrate that achieving reliable, trustworthy LLM deployments requires a multifaceted calibration strategy integrating robust fine-tuning, structured prompting, and post-hoc adjustments.

## Method Summary
The study evaluates 9 state-of-the-art LLMs on three QA datasets (SimpleQA, FaVIQ, TriviaQA) using two prompting regimes: free-generation (N) and distractor-augmented (D). Distractors are generated using GPT-4o-mini with type-matched, plausible-but-incorrect prompts. Models self-report confidence (0-100) and responses are classified as CORRECT/INCORRECT/NOT_ATTEMPTED by a unified GPT-4o-mini judge. ECE is computed via empirical binning (10 bins) to quantify calibration gaps. The D-setting forces comparative evaluation, reducing overconfidence by shifting models from free-form generation to discriminative selection.

## Key Results
- Structured distractors reduce ECE by up to 90% and increase accuracy by up to 460% compared to free-generation
- Smaller models benefit disproportionately from distractors but remain significantly miscalibrated
- Larger RLHF-tuned models show better baseline calibration but can paradoxically suffer increased miscalibration on easier queries when distractors inflate confidence
- Person-based queries exhibit persistent calibration failures due to name ambiguities and entity ambiguity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Presenting explicit distractor options alongside correct answers reduces LLM overconfidence by forcing comparative evaluation.
- **Mechanism:** Inspired by cognitive psychology's "consider-the-opposite" strategy, structured multiple-choice framing shifts the model from free-form generation (where confidence inflates) to discriminative selection, enabling better self-assessment.
- **Core assumption:** Models possess latent calibration ability that emerges when forced to compare alternatives rather than generate freely.
- **Evidence anchors:** Explicit incorporation of distractors can substantially mitigate miscalibration, achieving relative accuracy improvements up to 460% and ECE reductions up to 90%. On SimpleQA, distractors often halve ECE and more than double accuracy for smaller variants.

### Mechanism 2
- **Claim:** Larger RLHF-tuned models achieve better baseline calibration, but can paradoxically degrade on easier queries when distractors inflate confidence.
- **Mechanism:** Scale improves certainty quantification ability; RLHF reinforces self-assessment during alignment. However, on already-easy examples, multiple-choice framing encourages over-weighting the correct option, amplifying residual misalignment.
- **Core assumption:** Calibration gains from scale/RLHF are task-difficulty dependent, not uniform.
- **Evidence anchors:** GPT-4o and GPT-4-turbo exhibit slight ECE increases under D despite accuracy gains. Increasing model scale primarily improves ability to quantify certainty rather than uncover new knowledge.

### Mechanism 3
- **Claim:** Calibration failure is query-type dependent, with person-based queries most severely miscalibrated due to entity ambiguity.
- **Mechanism:** Name ambiguities, overlapping roles, and contextual dependencies require deeper reasoning beyond surface pattern matching. Structured choices help by providing explicit disambiguation anchors.
- **Core assumption:** Miscalibration stems partly from knowledge representation gaps, not just model scale.
- **Evidence anchors:** Person-based queries are most challenging, seeing highest relative ECE drop (69%) yet remaining problematic due to name ambiguities and inherent variability.

## Foundational Learning

- **Expected Calibration Error (ECE):**
  - Why needed here: Core metric quantifying gap between predicted confidence and actual accuracy; paper's primary evaluation criterion.
  - Quick check question: If a model reports 80% confidence across 100 predictions, how many should be correct for perfect calibration?

- **Verbalized (Elicited) Confidence vs. Token Probability:**
  - Why needed here: Paper uses self-reported 0-100 confidence scores rather than log-probabilities; understanding why matters for replication.
  - Quick check question: Why might RLHF-tuned models have degraded log-probability signals for calibration compared to verbalized confidence?

- **RLHF Calibration Effects:**
  - Why needed here: Paper shows RLHF improves calibration for larger models but isn't universally effective; understanding this interaction is critical for model selection.
  - Quick check question: What factors beyond RLHF influence calibration outcomes?

## Architecture Onboarding

- **Component map:** Distractor Generator (GPT-4o-mini) -> Prompt Templates (N vs D settings) -> LLM Inference -> LLM Judge (GPT-4o-mini) -> ECE Calculator

- **Critical path:** Question input -> distractor generation -> prompt construction (N or D) -> model inference -> confidence extraction -> judge evaluation -> ECE computation

- **Design tradeoffs:**
  - Unified judge ensures comparability but introduces generator-judge coupling bias
  - Elicited confidence enables black-box evaluation but may not reflect true internal uncertainty
  - Distractor quality trades off with generation cost; implausible distractors break Mechanism 1

- **Failure signatures:**
  - High NOT_ATTEMPTED rate in N-setting indicates model defers rather than miscalibrates
  - D-harmed cases where distractors introduce errors signal distractor quality or model susceptibility
  - ECE rising under D for already-well-calibrated large models on easy queries (confidence inflation)

- **First 3 experiments:**
  1. Replicate N vs D comparison on SimpleQA subset (100 questions) for a single model; verify ECE reduction magnitude
  2. Ablate distractor count (1 vs 3 vs 5) to find calibration-accuracy tradeoff curve
  3. Test cross-model judge sensitivity: compare GPT-4o-mini judge vs human annotation on 50 samples to quantify judge bias

## Open Questions the Paper Calls Out
- **Cross-model distractor sources and independent judges:** Exploring cross-model distractor sources and independent judges (including human-only adjudication) is important future work; the study prioritizes comparability under a fixed setup and does not claim generator/judge invariance.
- **Paradoxical calibration effects on easy queries:** Large RLHF-tuned models display inherent calibration strengths but can paradoxically suffer increased miscalibration on easier queries when distractors are added, requiring further investigation into the mechanism.
- **Targeted interventions for person-based queries:** The paper identifies persistent calibration failures in person-based queries but does not propose or test specific interventions beyond general distractor augmentation, which remains insufficient for person-based calibration.

## Limitations
- Results rely entirely on GPT-4o-mini as judge, introducing potential bias that cannot be independently verified without human annotation
- Systematic quality and plausibility of generated distractors across all 4,326 SimpleQA questions is not independently validated
- Findings are based on QA tasks only; whether structured distractor effects generalize to other NLP tasks remains untested

## Confidence
- Overconfidence and ECE reduction claims: High confidence
- Distractor mechanism effectiveness: Medium confidence
- Scale and RLHF calibration effects: Medium confidence
- Query-type dependent miscalibration: Low confidence

## Next Checks
1. Compare GPT-4o-mini judge classifications against human annotations on 100 randomly selected samples to quantify systematic bias
2. Manually inspect 50 randomly selected distractor sets across different question types to verify plausibility and type-matching
3. Replicate core N vs D comparison on an additional QA dataset (e.g., Natural Questions) to test generalizability beyond SimpleQA