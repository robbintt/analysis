---
ver: rpa2
title: Generating Storytelling Images with Rich Chains-of-Reasoning
arxiv_id: '2512.07198'
source_url: https://arxiv.org/abs/2512.07198
tags:
- image
- images
- story
- b-instruct
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Storytelling Image Generation, a novel task
  to generate single images that tell rich, logically coherent stories. The authors
  propose a two-stage pipeline, StorytellingPainter, combining LLM reasoning and T2I
  synthesis.
---

# Generating Storytelling Images with Rich Chains-of-Reasoning

## Quick Facts
- arXiv ID: 2512.07198
- Source URL: https://arxiv.org/abs/2512.07198
- Reference count: 21
- Key outcome: StorytellingPainter pipeline combines LLM reasoning with T2I synthesis; CoR-Guided prompting significantly improves story quality; Mini-Storytellers achieve 15.2-17.3 point gains over baselines

## Executive Summary
This paper introduces Storytelling Image Generation, a novel task requiring single images to convey rich, logically coherent narratives through Chains-of-Reasoning (CoR). The authors propose StorytellingPainter, a two-stage pipeline combining LLM-based story generation with T2I synthesis, and develop an evaluation framework with three specialized evaluators. Experiments show CoR-Guided prompting significantly enhances semantic richness, with GPT-4 models achieving the highest performance. The work advances AI-driven storytelling image generation with potential applications in cognitive assessment, illustration, and multimodal modeling.

## Method Summary
The approach employs a two-stage pipeline: Storyteller LLMs generate single-moment stories using CoR-Guided prompting across seven dimensions (Time, Location, Character Role/Relationship, Event, Event Causal Relationship, and Mental State), then Painter T2I models render these stories into images. The framework includes three evaluators (Semantic Complexity, Diversity, and Alignment) and introduces Mini-Storyteller models via knowledge distillation (SFT and DPO) to compress GPT-4.1 storytelling capabilities into smaller models. Training uses LoRA fine-tuning on 2,000 GPT-4.1-generated samples, with DPO-Mix incorporating smaller model outputs as negative samples.

## Key Results
- CoR-Guided prompting significantly improves semantic complexity of generated stories and images
- GPT-image-1 outperforms DALL-E 3 in alignment, highlighting task demands on T2I capabilities
- Mini-Storytellers show substantial improvements (15.2-17.3 points) over baseline open-source models
- SFT improves semantic scores but reduces diversity; DPO-Mix balances both tradeoffs

## Why This Works (Mechanism)

### Mechanism 1
Structured prompting with seven CoR dimensions improves semantic richness by forcing systematic attention to narrative elements that might otherwise be omitted. The CoR-Guided mode explicitly constrains generation across Time, Location, Character Role/Relationship, Event, Event Causal Relationship, and Mental State dimensions.

### Mechanism 2
Separating story generation from image synthesis enables modular optimization and human-interpretable intermediate output. The two-stage pipeline mimics human expert workflows (story design → image rendering) and allows independent evaluation of each stage.

### Mechanism 3
Knowledge distillation via SFT and DPO transfers proprietary LLM storytelling capabilities to small open-source models. GPT-4.1 (teacher) generates high-quality examples; student models learn through supervised fine-tuning or preference optimization.

## Foundational Learning

- **Concept: Chain-of-Reasoning (CoR)**
  - Why needed: Core formalization for what makes an image "storytelling" - visual clues jointly imply a conclusion K
  - Quick check: Given an image with a clock showing 2:25, a "FOOTBALL 2 PM" sign, and a frustrated woman, what is the inferred conclusion K?

- **Concept: Text-to-Image Alignment**
  - Why needed: The Painter model must accurately render all story elements; misalignment breaks the CoR chain
  - Quick check: If a story specifies "a boy looking fearful" but the generated image shows a neutral expression, which evaluator component would detect this failure?

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed: Enables training on preference pairs without explicit reward model; critical for Mini-Storyteller training
  - Quick check: In DPO, why is it important that chosen and rejected samples come from the same prompt?

## Architecture Onboarding

- **Component map:** Storyteller (LLM) → Story validation → Painter (T2I) → Multi-evaluator assessment
- **Critical path:** Prompt design → Storyteller generation → Story validation → Painter synthesis → Multi-evaluator assessment
- **Design tradeoffs:** SFT vs. DPO (semantic scores vs. diversity); Painter quality vs. cost (GPT-image-1 vs. DALL-E 3); prompt complexity (CoR-Guided benefits vs. complexity)
- **Failure signatures:** Long temporal sequences in stories (small models failing instruction-following); missing causal relationships in images (T2I alignment failure); repetitive themes across diverse prompts (mode collapse)
- **First 3 experiments:** 1) Compare Naive vs. CoR-Guided prompting on same Storyteller with fixed Painter; 2) Generate identical stories with DALL·E 3 vs. GPT-image-1 and compute Alignment Scores; 3) Train Mini-Storyteller-LLaMA-3.1-8B with SFT vs. DPO-Mix on 2,000 samples

## Open Questions the Paper Calls Out

- **Open Question 1:** How can CoRs in generated storytelling images be enhanced to match semantic density of top human-created illustrations? (Current gap: 62.9 vs 71.8 human semantic score)
- **Open Question 2:** Can end-to-end multimodal training improve story-image coherence and reduce semantic information loss compared to two-stage pipeline?
- **Open Question 3:** How can output diversity be preserved in Mini-Storytellers while maintaining semantic complexity? (SFT diversity drops from 0.432 to 0.344)
- **Open Question 4:** To what extent does GPT-based evaluator reliance introduce systematic bias, and can human-aligned open-source alternatives be developed?

## Limitations

- Task formulation requires complex causal relationships and mental states in single static images, pushing current T2I capabilities
- Evaluation framework relies heavily on LLM-based metrics that may not fully capture human perception
- SFT variant reduces diversity through mode collapse, though DPO-Mix partially addresses this

## Confidence

- **High Confidence:** Two-stage pipeline architecture and modularity; CoR-Guided prompting effectiveness; GPT-image-1 outperforming DALL-E 3
- **Medium Confidence:** Knowledge distillation effectiveness; evaluation framework capturing nuanced quality; Mini-Storyteller scalability
- **Low Confidence:** Task feasibility with current T2I for complex narratives; long-term generalization of distilled models; cross-cultural robustness of CoR dimensions

## Next Checks

1. **Painter Capability Validation:** Systematically test GPT-image-1's ability to render all seven CoR dimensions by isolating each dimension in test stories and measuring alignment failures

2. **Cross-Cultural Generalization:** Evaluate Mini-Storytellers on culturally diverse story prompts from non-English or non-Western contexts to test universal storytelling principle capture

3. **Long-Term Stability:** Track Mini-Storyteller performance over extended use to determine if SFT-induced diversity degradation manifests as mode collapse over time or with extended sampling