---
ver: rpa2
title: Online Kernel Dynamic Mode Decomposition for Streaming Time Series Forecasting
  with Adaptive Windowing
arxiv_id: '2510.15404'
source_url: https://arxiv.org/abs/2510.15404
tags:
- uni00000013
- uni00000003
- uni00000055
- uni00000048
- uni00000052
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WORK-DMD introduces an efficient online forecasting method that
  combines Random Fourier Features with dynamic mode decomposition to handle streaming
  time series with non-stationary dynamics. The approach explicitly lifts data into
  a kernel feature space while maintaining fixed computational cost through Sherman-Morrison
  updates, enabling continuous adaptation without catastrophic forgetting.
---

# Online Kernel Dynamic Mode Decomposition for Streaming Time Series Forecasting with Adaptive Windowing

## Quick Facts
- **arXiv ID:** 2510.15404
- **Source URL:** https://arxiv.org/abs/2510.15404
- **Reference count:** 5
- **Primary result:** Achieves 2× MSE reduction vs deep learning baselines while requiring 37× fewer sample exposures

## Executive Summary
WORK-DMD introduces an efficient online forecasting method that combines Random Fourier Features with dynamic mode decomposition to handle streaming time series with non-stationary dynamics. The approach explicitly lifts data into a kernel feature space while maintaining fixed computational cost through Sherman-Morrison updates, enabling continuous adaptation without catastrophic forgetting. By processing only the current windowed snapshot, WORK-DMD eliminates the need for extensive historical data storage or multiple training passes.

Experimental results demonstrate WORK-DMD achieves competitive or superior accuracy compared to state-of-the-art online forecasting methods across multiple benchmark datasets. The method excels particularly in short-term forecasting scenarios, with error metrics showing MSE reductions of up to 2× compared to deep learning baselines while requiring 37× fewer sample exposures. Cumulative error analysis reveals effective adaptation to regime shifts in real-world time series, recovering from dramatic non-stationary changes without manual intervention.

## Method Summary
WORK-DMD processes streaming multivariate time series through a four-stage pipeline: block-Hankel embedding to capture temporal correlations, Random Fourier Features to map data into a kernel feature space, online Dynamic Mode Decomposition with Sherman-Morrison updates for fixed-cost learning, and rank-r projection followed by eigendecomposition for forecasting. The method maintains only a rolling window of recent data, using online updates to adapt to non-stationary dynamics while preserving computational efficiency. A least-squares decoder maps feature space predictions back to the original space, with periodic recomputation to maintain accuracy.

## Key Results
- Achieves 2× MSE reduction compared to deep learning baselines in short-term forecasting
- Requires 37× fewer sample exposures while maintaining superior accuracy
- Effectively adapts to regime shifts without manual intervention through cumulative error recovery
- Demonstrates competitive performance across multiple benchmark datasets including ETTh2, ETTm1, Traffic, and WTH

## Why This Works (Mechanism)

### Mechanism 1: Explicit Kernel Feature Mapping via Random Fourier Features
Random Fourier Features map input data explicitly to an s-dimensional space approximating a Gaussian kernel. The mapping ψ(x) = √(2/s) Σ cos(θᵢ + zᵢᵀx) uses random frequencies zᵢ ~ N(0, γ⁻¹I) and phases θᵢ ~ Uniform[0, 2π). This avoids the O(n²) kernel matrix of implicit methods while preserving nonlinear representational capacity.

### Mechanism 2: Sherman-Morrison Incremental Updates for Fixed-Cost Online Learning
When old snapshots exit and new ones enter the window, auxiliary matrix Γ = (C⁻¹ + UᵀPₜU)⁻¹ captures alignment between current precision matrix Pₜ and change directions. Updates follow Pₜ₊₁ = Pₜ - PₜUΓUᵀPₜ and Aₜ₊₁ = Aₜ + (V - AₜU)ΓUᵀPₜ. The term (V - AₜU) represents prediction error scaled by Γ.

### Mechanism 3: Block-Hankel Embedding for Temporal Correlation Capture
Each feature's time series is reshaped into a Hankel matrix where row j contains [xₜ₋w₊₁, ..., xₜ₋d₊₁] shifted by one timestep per row. Stacking p such blocks creates a pd×(w-d+1) matrix capturing autoregressive depth d across all features. This transformation enables DMD to identify sparse eigenvalue structure.

## Foundational Learning

- **Dynamic Mode Decomposition (DMD)**
  - Why needed here: DMD provides the core operator framework. You must understand that DMD finds a best-fit linear operator A where Y ≈ AX, then extracts eigenvalues/modes for forecasting.
  - Quick check question: Given snapshot matrices X and Y, can you explain why the DMD operator A = YX† enables multi-step forecasting via eigenvalue powers?

- **Koopman Operator Theory**
  - Why needed here: WORK-DMD approximates the infinite-dimensional Koopman operator in a finite lifted space. Understanding this clarifies why nonlinear dynamics become linear in feature space.
  - Quick check question: Why does lifting data to a higher-dimensional space allow linear methods to capture nonlinear dynamics?

- **Sherman-Morrison Formula**
  - Why needed here: The efficiency claim rests on rank-2 updates to matrix inverses. You should recognize (A + uvᵀ)⁻¹ = A⁻¹ - (A⁻¹u)(1 + vᵀA⁻¹u)⁻¹vᵀA⁻¹.
  - Quick check question: How does Sherman-Morrison avoid full matrix inversion when adding/removing columns from a covariance matrix?

## Architecture Onboarding

- **Component map:** Hankel Constructor → RFF Lifter → Online DMD Core → POD Compressor → Eigen-Forecaster → Decoder
- **Critical path:** Input window → Hankel embedding → RFF lifting → Online DMD update (Pₜ, Aₜ) → POD compression → Eigendecomposition → Forecast propagation → Decode to output
- **Design tradeoffs:**
  - Window size w: Larger = more temporal context but slower adaptation; smaller = fast response but may miss slow modes
  - RFF dimension s: Higher = better kernel approximation but O(s²) memory/compute
  - Rank r: Lower = compression/noise filtering but may lose dynamics; higher = overfitting risk in streaming
  - Decoder update frequency: Every step = max accuracy; periodic = lower cost but drift risk
- **Failure signatures:**
  - Γ matrix entries → ∞: Ill-conditioned Pₜ, likely from redundant/constant features
  - Eigenvalues outside unit circle growing: Numerical instability or genuine unstable dynamics
  - Cumulative error sudden spike without recovery: Window too small for new regime
  - Decoder reconstruction error diverging: Feature space drift, need D re-computation
- **First 3 experiments:**
  1. **Sanity check**: Apply WORK-DMD to a known Lorenz or Van der Pol trajectory. Verify eigenvalues capture expected frequencies and that 1-step predictions match ground truth within tolerance. Confirm Sherman-Morrison updates match batch DMD computed periodically.
  2. **Regime shift test**: Generate synthetic data with abrupt parameter changes. Plot cumulative MSE comparing window sizes w ∈ {30, 60, 120}. Quantify recovery time (timesteps to return to pre-shift error rate) for each configuration.
  3. **Hyperparameter sweep**: On the ETTh2 validation split, grid search γ ∈ {10⁻⁶, 10⁻⁵, 10⁻⁴}, s ∈ {256, 512, 1024}, r ∈ {20, 40, 80}. Report MSE for H=1 and H=48 to confirm paper's finding that γ is most sensitive.

## Open Questions the Paper Calls Out

### Open Question 1
Can adaptive hyperparameter tuning mechanisms be developed for WORK-DMD that automatically respond to changing data characteristics without requiring domain expertise? The Discussion section states that "hyperparameter optimization remains a practical challenge" and suggests "adaptive hyperparameter tuning mechanisms... could enhance robustness across diverse applications."

### Open Question 2
How can WORK-DMD be extended to explicitly model cross-variable interactions for multivariate forecasting in complex physical systems? The authors identify the "Extension to explicit multivariate forecasting with cross-variable modeling" as necessary to expand applicability to domains like chemical process control and smart manufacturing.

### Open Question 3
Can the eigenvalue evolution in WORK-DMD be utilized for robust online change point detection to trigger automatic model resets? The paper notes that "developing robust online change point detection methods from eigenvalue patterns remains an active area of research" despite suggesting DMD's suitability for system health assessment.

## Limitations
- The Sherman-Morrison update formulation requires clarification on update stride (every timestep vs batch updates)
- The rank-r truncation strategy for POD compression is underspecified with conflicting indications
- Limited corpus validation exists for the Hankel embedding mechanism central to temporal correlation capture

## Confidence
- **High Confidence:** The RFF-based kernel approximation mechanism and its computational benefits are well-established theoretically
- **Medium Confidence:** The online Sherman-Morrison updates are mathematically sound, but empirical validation of stability across diverse non-stationary regimes is limited
- **Low Confidence:** The Hankel embedding dimension choices (w=120, d=30) appear arbitrary without sensitivity analysis demonstrating optimality

## Next Checks
1. **Numerical Stability Audit:** Monitor P_t conditioning and Γ matrix entries during streaming. If condition numbers exceed 1e6 or Γ entries diverge, adjust regularization ε or kernel bandwidth γ.
2. **Regime Recovery Timing:** On synthetic data with abrupt parameter shifts, measure recovery time (timesteps to return to pre-shift error) for window sizes w ∈ {30, 60, 120}. This validates the adaptation speed claims.
3. **RFF Dimension Sensitivity:** Grid search s ∈ {256, 512, 1024} while holding other hyperparameters constant. Verify that MSE improvements plateau, confirming the approximation is not the bottleneck.