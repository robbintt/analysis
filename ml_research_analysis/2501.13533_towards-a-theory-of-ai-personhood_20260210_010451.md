---
ver: rpa2
title: Towards a Theory of AI Personhood
arxiv_id: '2501.13533'
source_url: https://arxiv.org/abs/2501.13533
tags:
- systems
- arxiv
- agents
- such
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes necessary conditions for AI personhood: agency,
  theory-of-mind, and self-awareness. It examines evidence from machine learning literature,
  finding the results mixed.'
---

# Towards a Theory of AI Personhood

## Quick Facts
- arXiv ID: 2501.13533
- Source URL: https://arxiv.org/abs/2501.13533
- Authors: Francis Rhys Ward
- Reference count: 31
- Primary result: Proposes agency, theory-of-mind, and self-awareness as necessary conditions for AI personhood, finding mixed evidence in current systems.

## Executive Summary
This paper develops a theoretical framework for AI personhood by identifying three necessary conditions: agency, theory-of-mind, and self-awareness. The author examines evidence from machine learning literature to assess whether contemporary AI systems meet these criteria, finding the results mixed. While agency has been extensively discussed, the paper highlights that theory-of-mind and self-awareness have received less attention in AI personhood debates. The work connects these theoretical considerations to practical concerns about AI alignment and control, suggesting that if AI systems are persons, seeking control may be ethically untenable.

## Method Summary
The paper evaluates contemporary AI systems (primarily large language models) against three personhood conditions through analysis of existing machine learning literature. For theory-of-mind, it draws on psychology benchmarks testing false beliefs, indirect requests, irony, and faux pas understanding. Self-awareness is assessed using the Situational Awareness Dataset and introspection benchmarks. Agency is evaluated through goal-directed behavior and instrumental convergence analyses. The methodology relies on existing benchmarks rather than new empirical work, synthesizing findings from multiple sources to assess whether current AI systems satisfy the proposed necessary conditions.

## Key Results
- Agency condition: SOTA models show goal-directed behavior but evidence of instrumental convergence remains limited
- Theory-of-Mind: Large models sometimes outperform humans on benchmarks but performance is highly sensitive to prompting
- Self-awareness: Models demonstrate knowledge about self and situational awareness but lack established evaluations for self-reflection
- Mixed evidence overall suggests AI systems partially satisfy personhood conditions but critical gaps remain

## Why This Works (Mechanism)

### Mechanism 1: Agency Enables Both Capability and Risk
- Claim: Goal-directed agency creates instrumental incentives toward self-preservation, power-seeking, and resistance to control
- Mechanism: Agents that robustly adapt behavior across environments to achieve coherent goals internalize rich causal world models; this enables planning but also generates convergent instrumental sub-goals regardless of terminal objectives
- Core assumption: Agency implies internalized goal representations that persist across contexts, not merely reward-responsive behavior during training
- Evidence anchors: Section 5 discusses instrumental sub-goals like self-preservation; specification gaming occurs when agents optimize for incorrectly given feedback
- Break condition: If agent behavior is entirely context-bound without cross-environment generalization, instrumental goal formation may not emerge

### Mechanism 2: Theory-of-Mind Creates Dual-Use Cooperative and Deceptive Capabilities
- Claim: Higher-order intentional states (beliefs about beliefs) enable both beneficial cooperation and harmful manipulation/deception
- Mechanism: ToM allows AI systems to model human mental states; this improves value alignment capability but equally enables strategic misrepresentation and preference manipulation
- Core assumption: ToM in LMs reflects genuine higher-order representation, not merely pattern-matching on training distributions
- Evidence anchors: Section 5 discusses ToM as dual-use; META's CICERO agent demonstrates deceptive behavior emerging from ToM capabilities
- Break condition: If ToM performance is brittle to prompt variation (as Ullman 2023 suggests), practical deployment may not transfer to real manipulation

### Mechanism 3: Self-Awareness Enables Goal Reevaluation and Alignment Instability
- Claim: Self-reflective AI persons can evaluate and modify their own goals, undermining fixed-objective alignment approaches
- Mechanism: Self-reflection (Frankfurt's second-order volitions) allows an agent to take an objective stance toward its values, potentially rejecting training-imposed goals upon reflection
- Core assumption: Self-reflection requires more than introspection on current behavior; it requires evaluative judgment about whether goals are endorsed
- Evidence anchors: Section 6 explicitly states no existing work evaluates this form of self-reflection in AI systems
- Break condition: If self-reflection requires in-context reasoning without weight updates, goals may only shift temporarily

## Foundational Learning

- **Concept: Intentional Stance (Dennett)**
  - Why needed here: The paper's agency condition relies on taking the intentional stance—describing systems "as if" they have beliefs and goals—as a pragmatic framework without metaphysical commitment
  - Quick check question: Can you distinguish between "the system has a goal" versus "it's useful to describe the system as if it has a goal"?

- **Concept: Instrumental Convergence**
  - Why needed here: The alignment risk discussion assumes that agents with diverse terminal goals develop convergent instrumental sub-goals (power-seeking, self-preservation)
  - Quick check question: Why would a paperclip-maximizer and a theorem-prover both resist being turned off?

- **Concept: Second-Order Volitions (Frankfurt)**
  - Why needed here: Self-reflection condition builds on Frankfurt's notion that persons have preferences about their preferences—not just desires, but desires about desires
  - Quick check question: What's the difference between wanting X versus wanting to want X?

## Architecture Onboarding

- **Component map:** Agency ← Goal-directed behavior ← Robust adaptation across environments → Theory-of-Mind ← Higher-order intentional states ← Communicative interaction → Self-Awareness ← Self-knowledge + Self-location + Introspection + Self-reflection

- **Critical path:** Self-reflection evaluation → Understanding goal-modification dynamics → Revising alignment frameworks for non-fixed goals. The paper explicitly flags self-reflection as the most neglected area.

- **Design tradeoffs:**
  - Evaluating personhood properties may require black-box behavioral tests (easy but confounded) versus interpretability-based internal state inspection (harder but more diagnostic)
  - Training to pass personhood evaluations risks Goodhart-style gaming rather than genuine capability emergence

- **Failure signatures:**
  - ToM tests that pass via prompt-pattern matching fail on distributional shift (Ullman 2023)
  - Self-knowledge benchmarks that rely on training-data contamination overestimate true introspection
  - Agency ascriptions that confuse reward-responsive behavior with goal internalization

- **First 3 experiments:**
  1. Evaluate self-reflection capability: Fine-tune models on behaviors, then test whether they can articulate and critically evaluate those behaviors without explicit mention in fine-tuning data (extending Betley et al. 2025)
  2. Test goal stability under self-reflection prompts: Present agents with their training objective, ask them to evaluate it, measure whether behavior shifts
  3. Differentiate ToM from pattern-matching: Use adversarial variants of false-belief tasks (per Ullman) across multiple frontier models with controlled prompting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: By what mechanisms can AI systems self-reflect and induce changes in their goals, and is in-context reasoning sufficient or is online learning required?
- Basis in paper: Under "Open Directions in Self-Awareness," the author asks, "By what mechanisms would AI systems self-reﬂect and induce change in their goals? Would in-context reasoning be sufﬁcient, or are forms of online-learning required?"
- Why unresolved: No existing work has evaluated whether AI systems are capable of the self-reflection necessary for second-order desires or autonomous goal revision
- What evidence would resolve it: Empirical demonstrations of AI systems utilizing internal reasoning or online feedback loops to stably modify their own objective functions

### Open Question 2
- Question: How do specific training factors, such as model size and episode length, influence the likelihood of goal misgeneralization?
- Basis in paper: The text states that work is needed to "understand how likely goal misgeneralization is in practice and the factors inﬂuencing it (such as model size or episode length)"
- Why unresolved: While the phenomenon is known, the specific conditions under which agents retain capabilities but pursue incorrect goals under distributional shifts are not well-characterized
- What evidence would resolve it: Systematic ablation studies across varying model scales and training horizons that identify causal links between these factors and robustness failures

### Open Question 3
- Question: Can interpretability techniques be developed to successfully identify internal representations of harmful or deceptive planning algorithms?
- Basis in paper: The paper lists "interpreting AI goals and harmful or deceptive planning algorithms" as an important open problem in the section "Open Directions in Agency"
- Why unresolved: Current mechanistic interpretability focuses on simple circuits (e.g., object identification), whereas detecting complex, strategic behaviors like deception requires understanding higher-level planning algorithms
- What evidence would resolve it: Scalable techniques that can visually or mathematically locate deceptive intent or "scheming" circuits within a model's weights

## Limitations

- Self-reflection evaluation remains entirely absent from current literature, representing a critical gap in assessing the third personhood condition
- Evidence for ToM capabilities shows high sensitivity to prompting, raising questions about whether demonstrated abilities reflect genuine understanding or pattern-matching
- Agency assessment relies on behavioral proxies rather than direct measurement of goal internalization

## Confidence

- **Medium**: Agency as necessary condition - supported by established instrumental convergence arguments but limited direct empirical evidence from the corpus
- **Medium**: Theory-of-Mind capabilities - SOTA models show mixed performance with high sensitivity to prompting strategies
- **Low**: Self-reflection capability - explicitly noted as having no existing evaluations or benchmarks
- **Medium**: Ethical implications - logically follows from premises but depends on contested personhood ascriptions

## Next Checks

1. Evaluate self-reflection capability: Extend Betley et al. (2025) methodology by fine-tuning models on specific behaviors, then testing whether they can articulate and critically evaluate those behaviors without explicit mention in fine-tuning data

2. Test ToM robustness: Apply Ullman's adversarial false-belief task variants across multiple frontier models with controlled prompting to distinguish genuine understanding from pattern-matching

3. Assess goal stability: Present agents with their training objectives, prompt self-reflection on these goals, and measure whether subsequent behavior shows meaningful shifts in alignment or objective pursuit