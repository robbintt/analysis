---
ver: rpa2
title: 'Deep Learning as a Convex Paradigm of Computation: Minimizing Circuit Size
  with ResNets'
arxiv_id: '2511.20888'
source_url: https://arxiv.org/abs/2511.20888
tags:
- norm
- htmc
- which
- circuit
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a deep connection between deep neural networks
  and circuit complexity theory. It demonstrates that in the Harder than Monte Carlo
  (HTMC) regime, the set of real-valued functions computable within a certain error
  becomes convex, allowing for the definition of a HTMC norm on functions.
---

# Deep Learning as a Convex Paradigm of Computation: Minimizing Circuit Size with ResNets

## Quick Facts
- **arXiv ID:** 2511.20888
- **Source URL:** https://arxiv.org/abs/2511.20888
- **Reference count:** 40
- **Primary result:** Establishes theoretical connection between deep neural networks and circuit complexity theory through HTMC norm and ResNet parameter complexity

## Executive Summary
This paper establishes a deep connection between deep neural networks and circuit complexity theory. It demonstrates that in the Harder than Monte Carlo (HTMC) regime, the set of real-valued functions computable within a certain error becomes convex, allowing for the definition of a HTMC norm on functions. This norm is related to the parameter complexity of Residual Networks (ResNets), measured by a weighted ℓ1 norm of the network parameters. The key result is a sandwich bound that tightly relates the HTMC norm to the ResNet norm, showing that minimizing the ResNet parameter complexity is essentially equivalent to finding the minimal circuit size that fits the data.

## Method Summary
The paper introduces a theoretical framework connecting circuit complexity (measured by HTMC norm) with ResNet parameter complexity (measured by R-norm). The HTMC norm ||f||_H^γ is defined as max_ε ε^γ C_p(f,ε) where C_p is the minimum circuit size for ε-approximation. The ResNet complexity R(θ) is a weighted ℓ1 norm of parameters with Lipschitz-based diagonal matrices. The key innovation is a sandwich bound showing these norms are tightly related, proving that minimizing ResNet parameter complexity is equivalent to finding minimal circuit size. The paper also introduces Tetrakis functions as approximations of the vertices of the HTMC unit ball, suggesting potential for convex optimization methods.

## Key Results
- Proves a sandwich bound tightly relating HTMC norm to ResNet parameter complexity
- Shows convexity of the HTMC regime for function approximation
- Establishes that minimizing ResNet parameter complexity is equivalent to finding minimal circuit size
- Introduces Tetrakis functions as convex approximations for circuit vertices
- Demonstrates DNNs implement a computational Occam's razor, finding simplest algorithms that fit data

## Why This Works (Mechanism)
The paper establishes that in the Harder than Monte Carlo regime, the set of real-valued functions computable within a certain error becomes convex. This convexity allows defining a norm (HTMC norm) that measures the minimum circuit size needed for approximation. The key insight is that ResNet parameter complexity, measured by a weighted ℓ1 norm with Lipschitz constraints, is tightly related to this HTMC norm through a sandwich bound. This connection shows that training ResNets implicitly solves a convex version of the Minimum Circuit Size Problem (MCSP), explaining their success at learning complex functions.

## Foundational Learning
- **HTMC Regime**: Understanding when function approximation sets become convex is crucial for the theoretical framework. Quick check: Verify γ > 1/2 condition for Harder than Monte Carlo regime.
- **Circuit Complexity**: The paper builds on classical computational complexity theory, specifically the Minimum Circuit Size Problem. Quick check: Confirm understanding of binary circuit representations and approximation error.
- **ResNet Parameter Complexity**: The weighted ℓ1 norm with Lipschitz constraints is central to the sandwich bound. Quick check: Verify implementation of diagonal D_ℓ matrices for Lipschitz constraint.
- **Tetrakis Functions**: These serve as convex approximations to circuit vertices. Quick check: Test Tetrakis function construction on simple circuits like parity or addition.

## Architecture Onboarding
- **Component Map**: Input domain -> Binary circuits -> Tetrakis functions -> ResNet parameters -> Output functions
- **Critical Path**: Circuit complexity (HTMC norm) -> ResNet complexity (R-norm) -> Sandwich bound -> Minimum circuit size
- **Design Tradeoffs**: The convexity of HTMC regime enables optimization but requires the γ > 1/2 condition, which may be restrictive. Tetrakis functions provide convexity but may lose discrete circuit structure.
- **Failure Signatures**: If Lipschitz constraint Lip((α_ℓ → f_θ)∘D_ℓ^{-1}) ≤ 1 is infeasible, D_ℓ matrices may be trivial. Incorrect simplex vertices in Tetrakis functions indicate sorting network errors.
- **First Experiments**:
  1. Compute R(θ) complexity on trained ResNet and verify Lipschitz constraint
  2. Implement Tetrakis function construction and test on known circuits
  3. Empirically verify pruning bound by attempting to prune trained ResNets

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: Can the convexity of the HTMC regime be leveraged to prove the global convergence of gradient descent in ResNets?
- **Basis**: The paper states a "future goal is to find a differentiable, convex complexity measure... and use it to prove global convergence of gradient descent."
- **Why unresolved**: While the paper establishes convexity in function space (HTMC norm), it notes the complexity measure R(θ) is not easily differentiable and the translation to parameter space dynamics involves difficult saddle points.
- **What evidence would resolve it**: A proof demonstrating that gradient descent trajectories in parameter space converge to the global optimum defined by the HTMC norm.

### Open Question 2
- **Question**: Is the "HTMC rate gap" (where γ=2ω vs γ=ω) a fundamental property of the representation or an artifact of the current proof?
- **Basis**: The text notes that translating a circuit to a ResNet and back can square the number of nodes, suggesting this gap might depend on the regularity of intermediate representations.
- **Why unresolved**: The sandwich bound in Theorem 2 has a gap involving δ, and the paper speculates but does not prove if this is worst-case for irregular functions only.
- **What evidence would resolve it**: Identifying a function class where this squaring is proven necessary, or refining the sandwich bounds to close the δ gap.

### Open Question 3
- **Question**: Does a Frank-Wolfe algorithm over the countable set of Tetrakis functions provide a practical method for circuit size minimization?
- **Basis**: The paper suggests using the Tetrakis functions as vertices for a Frank-Wolfe algorithm but leaves "a careful analysis of such an algorithm to follow up work."
- **Why unresolved**: The countability of Tetrakis functions suggests convex optimization is possible, but the search step may still require super-polynomial runtime in the worst case.
- **What evidence would resolve it**: Analysis showing the algorithm decomposes global search into local searches, reducing complexity to exponential in the size of the largest sub-circuit rather than the total size.

## Limitations
- Constants c_γ in the sandwich bound remain non-constructive, limiting quantitative applications
- The HTMC regime condition (γ > 1/2) may be restrictive for practical learning scenarios
- Empirical validation is limited to theoretical proof sketches rather than concrete experiments

## Confidence
- **High**: Mathematical proofs of sandwich bounds and convexity properties
- **Medium**: Theoretical interpretation connecting DNNs to Occam's razor and MCSP
- **Low**: Practical effectiveness of Tetrakis-based optimization methods

## Next Checks
1. Implement concrete algorithm for computing R(θ) with verifiable Lipschitz constraints on trained networks
2. Develop approximation method for estimating HTMC norm ||f||_H^γ on synthetic functions to test the sandwich bound quantitatively
3. Experimentally verify the pruning bound (Theorem 11) by attempting to prune trained ResNets while tracking both parameter complexity and approximation quality