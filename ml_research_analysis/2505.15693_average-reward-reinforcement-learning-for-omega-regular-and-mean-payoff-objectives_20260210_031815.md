---
ver: rpa2
title: Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives
arxiv_id: '2505.15693'
source_url: https://arxiv.org/abs/2505.15693
tags:
- reward
- learning
- average
- policy
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of synthesizing policies for\
  \ continuing reinforcement learning tasks where objectives are expressed as \u03C9\
  -regular specifications. The authors introduce a novel model-free approach that\
  \ translates absolute liveness specifications into average-reward objectives, enabling\
  \ the use of standard average-reward RL algorithms without requiring episodic resets."
---

# Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff Objectives

## Quick Facts
- **arXiv ID**: 2505.15693
- **Source URL**: https://arxiv.org/abs/2505.15693
- **Reference count**: 13
- **Primary result**: Novel model-free RL approach for continuing tasks with ω-regular specifications that achieves near-optimal average rewards while satisfying liveness objectives

## Executive Summary
This paper addresses the challenge of synthesizing policies for continuing reinforcement learning tasks where objectives are expressed as ω-regular specifications. The authors introduce a novel model-free approach that translates absolute liveness specifications into average-reward objectives, enabling the use of standard average-reward RL algorithms without requiring episodic resets. By constructing reward machines that preserve the communicating property of the underlying MDP, they guarantee convergence to optimal policies that maximize both satisfaction probability and mean-payoff objectives. Experimental results show their method outperforms existing discounted RL approaches in continuing settings while remaining competitive in episodic settings. The framework also supports lexicographic multi-objective optimization, achieving near-optimal average rewards while satisfying absolute liveness specifications.

## Method Summary
The authors propose a model-free reinforcement learning approach for continuing tasks with ω-regular specifications by transforming absolute liveness objectives into average-reward formulations. They construct reward machines from the ω-regular specifications and prove that if the underlying MDP is communicating, the resulting reward machine-based MDP (RM-MDP) is also communicating. This preservation of the communicating property ensures that standard average-reward RL algorithms like R-learning converge to optimal policies. The method operates without requiring episodic resets, instead using the reward machine structure to maintain appropriate rewards throughout the continuing task. For lexicographic optimization, they introduce a meta-learner that alternates between satisfying the ω-regular specification and maximizing the mean-payoff objective, using a threshold-based approach to balance these competing goals.

## Key Results
- The proposed method achieves near-optimal average rewards while satisfying absolute liveness specifications in continuing settings
- Outperforms discounted RL approaches on continuing tasks while remaining competitive in episodic settings
- Provides theoretical guarantees for convergence to optimal policies when the underlying MDP is communicating
- Successfully handles lexicographic multi-objective optimization with a meta-learner approach

## Why This Works (Mechanism)
The approach works by leveraging the structure of reward machines to transform ω-regular specifications into reward-based objectives that can be optimized using average-reward RL algorithms. By preserving the communicating property of the underlying MDP in the RM-MDP, the method ensures that standard RL algorithms converge to optimal policies. The key insight is that absolute liveness specifications can be expressed as average-reward objectives, eliminating the need for episodic resets. The reward machine structure provides a systematic way to maintain appropriate rewards throughout the continuing task, while the meta-learner approach enables balancing multiple objectives through threshold-based switching.

## Foundational Learning

**ω-regular specifications**: Why needed - to express complex temporal logic properties for continuing tasks; Quick check - verify that specifications can be converted to deterministic Rabin automata

**Reward machines**: Why needed - to translate ω-regular specifications into reward-based objectives; Quick check - confirm reward machine construction preserves communicating property

**Communicating MDPs**: Why needed - to ensure convergence guarantees for average-reward RL algorithms; Quick check - verify underlying MDP satisfies communicating property

**Average-reward RL algorithms**: Why needed - to optimize continuing tasks without episodic resets; Quick check - confirm R-learning or similar algorithms can be applied to RM-MDP

**Lexicographic optimization**: Why needed - to balance multiple competing objectives; Quick check - verify threshold-based meta-learner achieves desired trade-off

## Architecture Onboarding

**Component map**: MDP -> Reward Machine -> RM-MDP -> Average-reward RL algorithm -> Policy

**Critical path**: The critical execution path involves the agent interacting with the environment, updating the reward machine state based on observations, receiving rewards from the RM-MDP, and updating the policy using the average-reward RL algorithm. The reward machine serves as the bridge between the ω-regular specification and the RL algorithm.

**Design tradeoffs**: The main tradeoff is between the expressiveness of ω-regular specifications and the computational complexity of maintaining and updating reward machines. More complex specifications require larger reward machines, potentially impacting real-time performance. The method trades off the simplicity of discounted RL for the theoretical guarantees and better performance in continuing settings offered by average-reward RL.

**Failure signatures**: Common failure modes include non-convergence when the MDP is not communicating, poor performance with sparse rewards due to the continuing nature of the task, and computational overhead from maintaining complex reward machines. The method may also struggle with specifications that require very long-term memory, as the reward machine state space can grow exponentially with specification complexity.

**First experiments**:
1. Verify convergence on a simple communicating MDP with a basic liveness specification
2. Compare performance against discounted RL on a continuing task with a safety specification
3. Test lexicographic optimization on a multi-objective benchmark problem

## Open Questions the Paper Calls Out
The paper identifies several open questions, including the scalability of the approach to large-scale MDPs with complex ω-regular specifications, the handling of non-communicating MDPs, and the extension to scenarios with sparse rewards. The authors also note the need for further investigation into the computational overhead of maintaining reward machines in real-time applications and the potential for combining their approach with function approximation methods to handle continuous state spaces.

## Limitations
- Scalability concerns for large MDPs with complex ω-regular specifications due to reward machine state space growth
- Limited experimental evaluation covering only a small set of benchmark problems
- Potential computational overhead from maintaining and updating reward machines in real-time
- Theoretical guarantees only apply to communicating MDPs, with unclear performance on non-communicating MDPs

## Confidence

**Theoretical framework and convergence guarantees**: High
**Experimental results on benchmark problems**: Medium
**Scalability to complex specifications**: Low
**Comparison with alternative approaches**: Medium

## Next Checks

1. Evaluate performance on larger MDPs with more complex ω-regular specifications to assess scalability
2. Conduct extensive comparisons against leading episodic RL methods to quantify trade-offs
3. Measure computational overhead and memory requirements for maintaining reward machines in real-time applications