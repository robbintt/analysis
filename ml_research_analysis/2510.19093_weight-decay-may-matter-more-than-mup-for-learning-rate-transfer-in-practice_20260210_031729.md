---
ver: rpa2
title: Weight Decay may matter more than muP for Learning Rate Transfer in Practice
arxiv_id: '2510.19093'
source_url: https://arxiv.org/abs/2510.19093
tags:
- weight
- learning
- rate
- decay
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates learning rate transfer across model widths
  in neural network training, focusing on the role of weight decay and the Maximal
  Update Parameterization (muP). The authors find that muP's core assumptions about
  alignment between weights and updates break down during practical training, especially
  when batch size is large relative to model width.
---

# Weight Decay may matter more than muP for Learning Rate Transfer in Practice

## Quick Facts
- arXiv ID: 2510.19093
- Source URL: https://arxiv.org/abs/2510.19093
- Reference count: 40
- Key outcome: Weight decay scaling rules are more critical than maximal update parameterization (muP) for learning rate transfer across model widths, with muP acting as implicit warmup.

## Executive Summary
This work investigates learning rate transfer across model widths in neural network training, focusing on the role of weight decay and the Maximal Update Parameterization (muP). The authors find that muP's core assumptions about alignment between weights and updates break down during practical training, especially when batch size is large relative to model width. Instead of muP's scaling rules, it is independent weight decay that stabilizes feature learning across widths and enables effective learning rate transfer. This reveals muP's practical role as an implicit learning rate warmup, which can be largely replaced by explicit warmup schedules. The findings challenge prevailing beliefs about muP's necessity and provide practical guidance for achieving robust learning rate transfer in large-scale training.

## Method Summary
The authors conduct controlled experiments on LLaMA-style transformer architectures trained on next-token prediction tasks using the DCLM dataset. They systematically vary model width (128-2048 hidden dimensions) and test different combinations of muP learning rate scaling (η→η/m) with standard versus independent weight decay scaling ((η,λ)→(η/m,λ) vs (η,λ)→(η/m,mλ)). The training uses AdamW optimizer with 10% linear warmup plus linear decay, and measurements include validation loss, relative representation changes, update alignment ratios, and relative weight updates. The core analysis involves measuring how alignment between inputs and weight updates evolves during training, particularly when batch size exceeds width, and how different weight decay scaling strategies affect learning rate transfer quality.

## Key Results
- Standard weight decay scaling (η→η/m, λ→λ) breaks learning rate transfer across widths, while independent scaling (η→η/m, λ→mλ) preserves it
- muP's alignment assumptions (αΔW = Θ(1), αW = Θ(1/√C)) break down early when batch size exceeds width, causing the alignment ratio to approach ~1
- muP with independent weight decay creates an implicit learning rate warmup effect that can be replicated with explicit warmup schedules
- Independent weight decay maintains relative update magnitudes (‖ΔW‖/‖W‖) across widths by controlling the equilibrium weight norm

## Why This Works (Mechanism)

### Mechanism 1
Weight decay establishes an equilibrium weight norm that controls relative update magnitudes independently of muP's learning rate scaling. In AdamW, weight decay multiplies weights by (1−ηλ) per step. At equilibrium, the shrinking from decay balances growth from gradient updates, yielding ‖W‖ ≈ √(KC·η/λ) and relative updates ‖ΔW‖/‖W‖ ∝ √(ηλ). Crucially, only the product ηλ matters for equilibrium behavior—so independent WD scaling (η→η/m, λ→mλ) preserves relative updates across widths. This contradicts muP by eventually overriding its update scaling.

### Mechanism 2
muP's alignment assumptions break down early in training when batch size B exceeds width C, making the alignment ratio width-independent rather than √C-dependent. Update alignment αΔW measures cosine similarity between inputs and weight updates. When B≫C, interference terms from B−1 other samples dominate the self-contribution term, introducing C-dependence. As gradient correlation decreases over training, alignment shifts from ~1 toward ~1/√C. Combined with weight alignment also approaching ~1/√C, the alignment ratio αΔW/αW ≈ 1 for most training—not the √C muP assumes.

### Mechanism 3
muP with independent weight decay creates an implicit learning rate warmup effect that can be replicated with explicit warmup schedules. Independent WD scaling uses higher λ (mλ) with lower η (η/m). Early in training, weight norms are near initialization, so the high decay rate dominates, shrinking relative updates by ~1/m. As training progresses and weights approach equilibrium, the effect fades. This matches muP's brief correct-scaling window when alignment assumptions hold, then transitions to the equilibrium behavior that independent WD provides anyway.

## Foundational Learning

- **Concept:** Update alignment (αΔW = ‖ΔW·X‖/(‖ΔW‖·‖X‖))
  - **Why needed here:** Core to understanding why muP fails—the paper shows this becomes width-dependent during training, violating muP's assumptions.
  - **Quick check question:** For your model's largest layer, is batch_size × sequence_length ≫ fan_in? If yes, expect alignment breakdown early.

- **Concept:** Relative vs absolute weight updates (‖ΔW‖/‖W‖ vs ‖ΔW‖)
  - **Why needed here:** The paper reframes muP in terms of relative updates, which better captures update impact and connects to weight decay's equilibrium effect.
  - **Quick check question:** If you double your learning rate, does your effective update magnitude (relative to weights) double? Only if not at WD equilibrium.

- **Concept:** Standard vs independent weight decay scaling
  - **Why needed here:** The practical choice that determines whether learning rates transfer. Standard: (η,λ)→(η/m,λ). Independent: (η,λ)→(η/m,mλ).
  - **Quick check question:** When scaling model width by m, do you scale weight decay proportionally? If not, you're using standard scaling.

## Architecture Onboarding

- **Component map:** Hidden layers (attention/MLP weight matrices) → muP scales learning rate by 1/m; independent WD scales WD by m → These are the layers where alignment dynamics matter
- **Critical path:** Measure alignment ratio early vs mid-training → If it drops to ~1, independent WD is required → If using muP with standard WD, switch to independent or add explicit warmup
- **Design tradeoffs:**
  - muP + independent WD: Best transfer, implicit warmup, but requires tuning both η and λ at proxy scale
  - muP + standard WD: Fails to transfer (relative updates diverge across widths)
  - No muP + exponential warmup: Can match muP's benefits, but requires tuning warmup schedule and is less stable at high learning rates
  - No weight decay: Relative updates become learning-rate-independent over time; weaker transfer, worse final performance
- **Failure signatures:**
  - Standard WD with muP: Wide models train slower/lower quality at same base LR—relative updates are too small post-alignment-breakdown
  - No WD: Continuously decaying relative updates, potential loss of plasticity, poor control over effective learning rate
  - Insufficient warmup: Instability at high learning rates, especially with large batch-to-width ratios
- **First 3 experiments:**
  1. **Measure alignment ratio trajectory:** Log αΔW/αW for a representative layer at proxy and target widths. If it converges to ~1 within warmup period, alignment breakdown is confirmed.
  2. **Compare WD scaling variants:** Sweep base learning rate at proxy scale with both standard and independent WD, then transfer to target scale. Expect independent to transfer well, standard to shift optimal LR.
  3. **Test explicit warmup replacement:** Replace muP + independent WD with no muP + exponential warmup (Equation 12: ŝt = m^min(0, t/TW − 1)). Tune TW to match muP's stability; expect similar but potentially less stable results.

## Open Questions the Paper Calls Out

- **Question:** Can matrix-level optimizers like Muon eliminate the need for muP-style learning rate scaling and warmup?
  - **Basis in paper:** Discussion section states "Muon...can result in a constant low update alignment that does not vary over time, allowing it to bypass much of the complexities we have discussed in this work. This could potentially explain its reduced need for learning rate warmup and makes it a promising alternative to µP."
  - **Why unresolved:** The paper speculates about Muon but conducts no experiments with matrix-level optimizers.
  - **What evidence would resolve it:** Learning rate transfer experiments across widths comparing Muon against muP with independent weight decay.

- **Question:** Would scaling learning rate by 1/√m and weight decay by √m better preserve training dynamics than standard muP's 1/m scaling?
  - **Basis in paper:** Section 6 suggests "it would make sense to scale (η, λ) → (η/√m, √m·λ)" to preserve both the decay multiplier and equilibrium weight norm ratio, noting current scaling may cause signal propagation issues.
  - **Why unresolved:** The alternative √m scaling was suggested but never empirically tested.
  - **What evidence would resolve it:** Comparative learning rate transfer experiments using √m scaling versus standard and independent muP scaling across multiple architectures.

- **Question:** Why does the optimal learning rate for validation loss shift oppositely to training loss when scaling ResNet width with independent weight decay?
  - **Basis in paper:** Appendix E notes this "strong indication of a confounding regularization effect" with optimal LRs shifting down for training but up for validation as width increases.
  - **Why unresolved:** The paper does not isolate whether this comes from weight norms, gradient-epsilon interactions, or architecture-specific factors.
  - **What evidence would resolve it:** Ablation studies controlling weight norms and Adam epsilon values independently across ResNet widths.

## Limitations

- **Empirical scope limits:** Analysis conducted on LLaMA-style transformers and next-token prediction tasks; results may vary on different architectures or tasks.
- **Alignment measurement assumptions:** Assumes cosine similarity adequately captures feature learning dynamics; alternative metrics might yield different quantitative conclusions.
- **Theoretical idealizations:** Weight decay equilibrium analysis assumes AdamW behaves like plain SGD with WD in the long run; momentum and adaptive scaling introduce deviations.

## Confidence

- **High confidence:** The claim that independent weight decay enables learning rate transfer while standard WD breaks it.
- **Medium confidence:** The claim that muP's update scaling is dominated by independent WD's equilibrium effect during practical training.
- **Medium confidence:** The claim that muP acts as an implicit warmup that can be replaced by explicit schedules.

## Next Checks

1. **Measure alignment trajectory on a held-out architecture:** Apply the alignment ratio measurement to a different model family (e.g., GPT-style or hybrid MoE) to verify the B≫C breakdown pattern holds universally.

2. **Test extreme batch-size regimes:** Systematically vary batch size relative to width (B/C ratios from 0.1 to 100) to map the exact threshold where muP assumptions break, and whether independent WD scaling remains effective.

3. **Compare explicit warmup variants:** Replace muP+independent WD with no-muP plus different warmup shapes (linear, cosine, step) to determine if exponential is uniquely effective or if other schedules work equally well.