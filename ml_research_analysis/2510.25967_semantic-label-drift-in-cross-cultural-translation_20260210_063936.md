---
ver: rpa2
title: Semantic Label Drift in Cross-Cultural Translation
arxiv_id: '2510.25967'
source_url: https://arxiv.org/abs/2510.25967
tags:
- cultural
- translation
- label
- greek
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates semantic label drift in cross-cultural
  machine translation, focusing on mental health and irony domains. The core method
  involves translating English datasets into Bengali and Greek using both traditional
  MT tools and modern LLMs, with labels annotated via a Human-LLM collaboration scheme.
---

# Semantic Label Drift in Cross-Cultural Translation

## Quick Facts
- **arXiv ID**: 2510.25967
- **Source URL**: https://arxiv.org/abs/2510.25967
- **Reference count**: 13
- **One-line primary result**: Translation induces label drift, particularly in culturally sensitive domains like mental health and irony, with nuanced classes showing the most degradation.

## Executive Summary
This study investigates semantic label drift when translating English datasets into Bengali and Greek using both traditional MT tools and modern LLMs. The core finding is that label preservation varies dramatically across domains and cultural contexts. While extreme classes (e.g., severe depression, non-ironic) are well-preserved, nuanced categories (e.g., mild depression, situational irony) frequently drift. Literal prompting consistently outperforms cultural prompting in label preservation, and cultural similarity between source and target languages mitigates drift in specific domains, particularly irony. Translation refusal and cultural misalignment further complicate label preservation, especially in underrepresented classes.

## Method Summary
The study translates three English datasets (DEPTWEET for depression severity, SemEval-2018 Task 3 for irony, and Amazon Product Reviews for sentiment) into Bengali and Greek using six translation systems. Two prompting strategies are employed: Literal (direct translation instruction) and Cultural (anthropological prompting with demographic persona). Annotations are performed by a 3-LLM ensemble (GPT-4.1-Mini, Claude Haiku 3.5, DeepSeek V3) using original annotation guidelines, with human validation for ambiguous cases. Label preservation is evaluated using Label Preservation Rate, KL Divergence, and Matthews Correlation Coefficient.

## Key Results
- Literal prompting shows statistically significant MCC advantage over cultural prompting (ΔMCC = +0.0085, p < 0.01)
- Greek translations show significantly higher MCC for irony (Δ = +0.039, p < 0.05) but not mental health (Δ = +0.001, ns)
- Mild/moderate depression classes drop to 46-62% preservation while severe depression maintains 82-88%
- Situational irony preservation ranges from 28-62% while non-ironic maintains 92-97%

## Why This Works (Mechanism)

### Mechanism 1: Cultural Sensitivity Amplifies Label Drift via Contextual Reinterpretation
Culturally sensitive content requires affective and stylistic interpretation dependent on cultural context. When source and target cultures diverge, the same text is reinterpreted through target-culture norms, changing classifications. Neutral content lacks this dependency.

### Mechanism 2: LLM Cultural Knowledge Activation Can Increase Semantic Divergence
Explicitly activating LLM cultural knowledge through anthropological prompting may worsen label preservation compared to literal translation. LLMs encode cultural norms and when prompted to translate "from a cultural perspective," models adapt content to target-culture appropriateness—intensifying, softening, or censoring expressions.

### Mechanism 3: Cultural Proximity Mitigates Drift Domain-Selectively
Domains with shared conceptual structures across cultures benefit from proximity while domains with culture-specific expressions may not. The Inglehart-Welzel cultural map dimensions serve as proxies for cultural similarity, but these may not capture all relevant aspects for label preservation.

## Foundational Learning

- **KL Divergence vs. Instance-Level Preservation**: Mental health shows low KL divergence (distribution preserved) but low instance-level MCC (individual labels shift). Quick check: If KL divergence is low but MCC is weak, what does this tell you about the nature of label changes?

- **Non-Propositional Content in Translation**: LLMs struggle with "non-propositional aspects" like politeness, emotion, and tone. These are precisely what label-sensitive domains require. Quick check: Name three non-propositional features relevant to mental health text classification.

- **Translation Refusal as Signal Loss**: Models refuse to translate content involving self-harm, aggression, or explicit themes. These refusals cluster in minority but information-rich classes. Quick check: If refused samples are predominantly from a specific class, what downstream bias does this introduce?

## Architecture Onboarding

- **Component map**: Sample selection -> Translation layer -> Prompting layer -> Annotation layer -> Evaluation layer
- **Critical path**: 1) Sample selection (stratified by class) 2) Translation with specified prompting strategy 3) Multi-LLM annotation using original guidelines 4) Native speaker validation on ambiguous cases 5) Metric computation comparing original vs. translated labels
- **Design tradeoffs**: Literal vs. Cultural prompting (literal better for preservation, cultural better for appropriateness); Human vs. LLM annotation (LLM cheaper, human needed for ambiguous cases); Language selection (balance cultural distance vs. feasibility)
- **Failure signatures**: High KL divergence with low MCC (distributional collapse); Low preservation on mild/moderate classes with high preservation on extremes (severity inflation); MCC near or below zero (random classification)
- **First 3 experiments**: 1) Establish baseline drift with neutral domain 2) Isolate cultural knowledge effect with parallel translations 3) Validate cultural proximity hypothesis with intermediate language

## Open Questions the Paper Calls Out
- Can translation strategies be developed that simultaneously achieve cultural appropriateness and high semantic label fidelity?
- Does semantic label drift occur to the same extent in other culturally sensitive domains like sarcasm and humor?
- How does cultural dissimilarity and linguistic typology affect label drift in languages outside the Indo-European family?
- How can cross-cultural translation pipelines effectively mitigate selection bias from LLM refusal to translate sensitive content?

## Limitations
- Cultural prompting effects inferred from label drift patterns rather than direct model behavior analysis
- Annotation reliability not fully characterized despite 3-LLM ensemble approach
- Cultural distance quantification using Inglehart-Welzel map may not capture all relevant dimensions
- Results based on three specific datasets may not generalize to other domains

## Confidence

**High Confidence**:
- Label drift more pronounced in culturally sensitive domains than neutral ones
- Literal prompting outperforms cultural prompting for label preservation
- Cultural proximity reduces drift in irony domain but not mental health

**Medium Confidence**:
- Translation refusal systematically biases class distributions
- Domain-specific conceptual differences explain differential proximity effects
- KL divergence captures distributional shifts while MCC captures instance-level accuracy

**Low Confidence**:
- Exact mechanism by which cultural prompting induces label drift
- Generalizability of cultural distance effects across different dimensions
- Relative contribution of translation errors vs. genuine cultural reinterpretation

## Next Checks
1. Apply the same translation and annotation pipeline to a fourth culturally sensitive domain (e.g., politeness, emotion recognition) to test whether drift patterns generalize
2. Add a third target language at intermediate cultural distance from English to test whether drift scales continuously with cultural distance
3. Analyze actual outputs of cultural vs. literal prompting to directly characterize intensification, softening, and censorship patterns