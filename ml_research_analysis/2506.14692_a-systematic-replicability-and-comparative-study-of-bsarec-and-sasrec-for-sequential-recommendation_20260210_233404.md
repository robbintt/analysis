---
ver: rpa2
title: A Systematic Replicability and Comparative Study of BSARec and SASRec for Sequential
  Recommendation
arxiv_id: '2506.14692'
source_url: https://arxiv.org/abs/2506.14692
tags:
- bsarec
- sequential
- sasrec
- recommendation
- frequency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically compares SASRec and BSARec for sequential
  recommendation to evaluate whether BSARec's frequency enhancement provides meaningful
  performance gains. Both models were re-implemented using a common base structure
  from EasyRec to ensure fair comparison.
---

# A Systematic Replicability and Comparative Study of BSARec and SASRec for Sequential Recommendation

## Quick Facts
- arXiv ID: 2506.14692
- Source URL: https://arxiv.org/abs/2506.14692
- Reference count: 7
- BSARec outperforms SASRec by 3.7%-5.7% on MovieLens 1M and 10.3%-14.3% on Foursquare-NYC

## Executive Summary
This study systematically compares SASRec and BSARec for sequential recommendation to evaluate whether BSARec's frequency enhancement provides meaningful performance gains. Both models were re-implemented using a common base structure from EasyRec to ensure fair comparison. BSARec extends SASRec by incorporating a frequency-based inductive bias that addresses the low-pass filtering limitation of standard self-attention, using Fourier transforms to capture high-frequency signals. Experiments on MovieLens 1M and Foursquare-NYC datasets show that BSARec outperforms SASRec across most metrics, with larger gains on the geographically-rich Foursquare-NYC data (10.3%-14.3% improvement) compared to MovieLens 1M (3.7%-5.7%). The results confirm BSARec's effectiveness in mitigating oversmoothing while highlighting the importance of consistent implementation for reliable model comparison.

## Method Summary
Both SASRec and BSARec were re-implemented using EasyRec library with common PyTorch-native base components (TransformerEncoder, LayerNorm, Embedding). BSARec adds a BSARecLayer that applies FFT to input sequences, separates low-frequency and high-frequency components using a cutoff parameter c, re-weights them via learnable β, and reconstructs via inverse FFT. This frequency-filtered output is blended with standard self-attention output using α weighting before the feed-forward network. Hyperparameter search was conducted for α ∈ [0.1, 0.5, 0.7, 0.9], c ∈ [1, 3, 5, 7, 9] for BSARec, and dropout ∈ [0.0005, 0.2] for SASRec. Best configurations: BSARec α=0.7, c=1 (ml-1m); α=0.3, c=1 (fs-nyc); SASRec dropout=0.0005 both datasets.

## Key Results
- BSARec outperforms SASRec by 3.7%-5.7% on MovieLens 1M dataset
- BSARec shows larger gains of 10.3%-14.3% on geographically-rich Foursquare-NYC dataset
- Optimal α=0.7 for MovieLens (favors self-attention) vs α=0.3 for Foursquare (favors frequency module)
- c=1 (minimal cutoff preserving most high frequencies) was optimal for both datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BSARec's frequency enhancement mitigates oversmoothing by recovering high-frequency signals that standard self-attention suppresses.
- Mechanism: The model applies Fast Fourier Transform (FFT) to input sequences, separates low-frequency (long-term) and high-frequency (short-term) components using a cutoff parameter c, re-weights them via learnable β, and reconstructs via inverse FFT—then blends this with standard self-attention output using α weighting.
- Core assumption: User behavior sequences contain meaningful short-term patterns that are systematically suppressed by self-attention's averaging operation (low-pass filtering).
- Evidence anchors:
  - [abstract] "BSARec extends SASRec by incorporating a frequency-based inductive bias that addresses the low-pass filtering limitation of standard self-attention, using Fourier transforms to capture high-frequency signals."
  - [section 3] "BSARec...addresses the lack of inductive bias in transformer-based RecSys models, which often focus on long-range dependencies while neglecting fine-grained sequential patterns. This limitation is attributed to the low-pass filtering nature of self-attention mechanism, which tends to oversmooth item embeddings."
  - [corpus] Related work confirms: "the self-attention mechanism of Transformer-based models acts as a low-pass filter, limiting their ability to capture high-frequency signals that reflect short-term user interests."

### Mechanism 2
- Claim: Parallel blending of self-attention and frequency-filtered paths provides complementary representations that improve next-item prediction.
- Mechanism: BSARec runs two parallel branches—standard causal self-attention and attentive inductive bias (frequency filtering)—each independently normalized, then combined via α weighting before the feed-forward network.
- Core assumption: Long-range dependencies captured by self-attention and short-term patterns captured by frequency filtering encode non-redundant information; optimal performance requires balancing both.
- Evidence anchors:
  - [abstract] "BSARec outperforms SASRec across most metrics, with larger gains on the geographically-rich Foursquare-NYC data."
  - [section 3] "The BSALayer...consists of the standard self-attention mechanism, alongside an attentive inductive bias module that incorporates frequency re-scaling...These two - the self-attention and the attentive and inductive bias blocks - are added considering a parameter α."
  - [corpus] Weak/missing—no corpus papers explicitly validate the parallel blending architecture; evidence is internal to this study.

### Mechanism 3
- Claim: BSARec's effectiveness is modulated by dataset characteristics—specifically, datasets with richer temporal/sequential structure show larger improvements.
- Mechanism: Geographic check-in data (Foursquare) exhibits stronger periodicity and short-term behavioral variation than sparse rating data (MovieLens), giving the frequency module more recoverable signal.
- Core assumption: High-frequency components in user sequences are domain-dependent; not all sequential recommendation tasks benefit equally from frequency enhancement.
- Evidence anchors:
  - [abstract] "larger gains on the geographically-rich Foursquare-NYC data (10.3%-14.3% improvement) compared to MovieLens 1M (3.7%-5.7%)."
  - [section 4] "This might be due to the geographic nature of fs-nyc nature of data, as the temporal aspect is more impactful - hence supporting the frequency identification and filtering performed by BSARec."
  - [corpus] Weak—no corpus papers systematically validate this domain sensitivity hypothesis.

## Foundational Learning

- Concept: Self-attention as low-pass filter
  - Why needed here: Understanding why transformer attention systematically suppresses short-term signals is essential for diagnosing when BSARec is applicable.
  - Quick check question: Can you explain how attention's weighted averaging operation acts as a smoothing function on sequence representations?

- Concept: Discrete Fourier Transform for sequence analysis
  - Why needed here: BSARec's core innovation relies on FFT to decompose sequences into frequency components; understanding this enables proper tuning of c (cutoff) and β (re-weighting).
  - Quick check question: What does the cutoff parameter c control in the frequency domain, and how would setting c=1 differ from c=9?

- Concept: Inductive bias in neural architectures
  - Why needed here: BSARec explicitly adds frequency-domain inductive bias to compensate for transformer's implicit bias toward low frequencies.
  - Quick check question: What specific assumption about user behavior does BSARec encode that standard SASRec does not?

## Architecture Onboarding

- Component map:
  Input -> Item Embedding + Positional Embedding -> N× [BSARecEncoderLayer] -> Final Hidden State -> Item Prediction
  BSARecEncoderLayer = Self-Attention (normalized) || BSARecLayer (filter) -> α-weighted sum -> FFN -> Residual + LayerNorm
  BSARecLayer = Dropout -> FFT -> Frequency split (cutoff c) -> β-weighted recombination -> IFFT -> LayerNorm

- Critical path:
  1. Embedding layer produces sequence representations
  2. Each encoder layer runs parallel: self-attention branch + frequency filter branch
  3. α controls blend ratio (optimal: 0.7 for ml-1m, 0.3 for fs-nyc per this study)
  4. FFN processes blended representation
  5. Final hidden state predicts next item via dot product with candidate embeddings

- Design tradeoffs:
  - α near 1.0 favors self-attention (long-term); near 0.0 favors frequency module (short-term)
  - Lower α optimal on Foursquare suggests short-term patterns more valuable for location prediction
  - c=1 (both datasets) indicates minimal cutoff—preserving most high frequencies—was optimal; higher c may introduce noise
  - EasyRec base ensures fair comparison but yields slightly lower scores than original BSARec paper (implementation details matter)

- Failure signatures:
  - Oversmoothing diagnosis: Hidden representations across sequence positions converge to high cosine similarity; NDCG plateaus early
  - Wrong α: Performance drops below SASRec baseline (validate on held-out set before deployment)
  - Wrong c: High c includes noise (unstable training); very low c may still oversmooth
  - Reproducibility gaps: Different normalization or dropout placement yields inconsistent results vs. reported benchmarks

- First 3 experiments:
  1. Establish SASRec baseline using EasyRec with identical preprocessing; log NDCG@5/10/20, Precision@10, Recall@10 to confirm parity.
  2. Grid search α ∈ {0.1, 0.3, 0.5, 0.7, 0.9} and c ∈ {1, 3, 5, 7, 9} on validation split; select best by NDCG@10.
  3. Compare convergence curves (NDCG@10 vs. epoch) for both models; verify BSARec achieves stable improvement without extreme oscillation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do specific dataset characteristics, such as temporal density or geographic locality, determine the performance gains of frequency-based inductive biases over standard self-attention?
- Basis in paper: [inferred] The authors observe that BSARec provides larger gains on Foursquare-NYC (10.3%-14.3%) compared to MovieLens 1M (3.7%-5.7%), hypothesizing that the "geographic nature" of the data makes frequency identification more impactful.
- Why unresolved: The study is limited to only two datasets, making it impossible to isolate which specific data features (e.g., sequence sparsity, temporal regularity) cause the variance in improvement.
- What evidence would resolve it: A systematic evaluation across a wider variety of datasets with controlled variables for temporal granularity and sequence length.

### Open Question 2
- Question: Which specific implementation details in the original BSARec paper account for the performance discrepancy observed in the standardized EasyRec re-implementation?
- Basis in paper: [inferred] The authors note that their results confirm BSARec's superiority but display "slightly lower" performance than the original paper, suggesting that implementation-specific choices impact final scores.
- Why unresolved: The paper identifies a reproducibility gap but does not perform an ablation study to pinpoint which non-standard components in the original codebase (e.g., custom initialization or normalization) contributed to the higher original metrics.
- What evidence would resolve it: A line-by-line ablation study swapping specific PyTorch-native modules for the original custom implementations to identify the source of the score differential.

### Open Question 3
- Question: Can frequency enhancement mechanisms be effectively combined with other strategies for mitigating oversmoothing, such as contrastive learning?
- Basis in paper: [explicit] The conclusion states that the study "paves the way for future research on the topic of enhanced attention modules for sequential recommendation and inductive bias augmentation."
- Why unresolved: The current work focuses strictly on comparing frequency enhancement against the baseline, without exploring its interaction with other methods mentioned in the literature review (e.g., DuoRec's contrastive learning).
- What evidence would resolve it: Experiments integrating BSARec's frequency filters into contrastive learning frameworks to see if the approaches are complementary or redundant.

## Limitations
- Implementation differences: EasyRec re-implementation yields slightly lower performance than original BSARec paper
- Missing hyperparameters: Training configuration details (batch size, learning rate, optimizer, epochs) not specified
- Limited dataset scope: Only two datasets evaluated, preventing systematic analysis of domain sensitivity

## Confidence
- Frequency enhancement mechanism (Mechanism 1): Medium confidence - theoretically grounded but domain-dependent
- Parallel blending architecture (Mechanism 2): Medium confidence - limited external validation
- Domain sensitivity claim (Mechanism 3): High confidence - clear empirical support with performance differentials
- Overall performance claims: Medium confidence - valid but implementation-dependent

## Next Checks
1. Replicate the α and c hyperparameter search on held-out validation data to confirm optimal values (α=0.7/c=1 for MovieLens; α=0.3/c=1 for Foursquare) match reported findings
2. Conduct ablation studies removing the frequency module to quantify its marginal contribution versus baseline self-attention performance
3. Test BSARec on additional sequential datasets (e.g., Amazon, YooChoose) to validate whether geographic datasets consistently show larger improvements than sparse rating datasets