---
ver: rpa2
title: New Evidence of the Two-Phase Learning Dynamics of Neural Networks
arxiv_id: '2505.13900'
source_url: https://arxiv.org/abs/2505.13900
tags:
- training
- neural
- learning
- phase
- dynamics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies two key phenomena that characterize the phase
  transition in neural network training. The authors introduce an interval-wise analysis
  framework comparing model states across time windows rather than at individual points.
---

# New Evidence of the Two-Phase Learning Dynamics of Neural Networks

## Quick Facts
- arXiv ID: 2505.13900
- Source URL: https://arxiv.org/abs/2505.13900
- Reference count: 39
- Primary result: Two-phase transition in neural network training characterized by Chaos Effect and Cone Effect

## Executive Summary
This paper identifies two key phenomena that characterize the phase transition in neural network training. The authors introduce an interval-wise analysis framework comparing model states across time windows rather than at individual points. They demonstrate that neural networks undergo a transition from an exploratory, chaotic phase to a more stable, refinement-oriented phase during training, validated through experiments with VGG-16 and ResNet-20 on CIFAR-10.

## Method Summary
The authors introduce an interval-wise analysis framework that compares model states across time windows rather than at individual points. This approach allows for identifying phase transitions by tracking how small perturbations propagate through the optimization trajectory. The method involves measuring divergence between perturbed and unperturbed training paths to detect the Chaos Effect, and analyzing the evolution of the empirical Neural Tangent Kernel to observe the Cone Effect. Experiments were conducted using VGG-16 and ResNet-20 architectures on the CIFAR-10 dataset.

## Key Results
- Early training exhibits chaotic behavior where small parameter perturbations lead to significant divergence (Chaos Effect)
- After a critical transition point, functional evolution becomes constrained within a narrow cone-shaped region in function space (Cone Effect)
- The empirical Neural Tangent Kernel evolves but remains confined to a tight angular region around its value at the transition point
- Both effects were consistently observed across VGG-16 and ResNet-20 architectures on CIFAR-10

## Why This Works (Mechanism)
The two-phase dynamics emerge from the interaction between the loss landscape geometry and the learning process. Early in training, the network explores a high-dimensional parameter space where small changes can lead to large functional differences due to the flat or ill-conditioned regions of the loss landscape. As training progresses, the network settles into regions where the functional output becomes more constrained, leading to the cone-shaped evolution. The empirical NTK captures this transition by measuring how the Jacobian of the network evolves over time, with the cone constraint reflecting the network's increasing specialization to the training data.

## Foundational Learning
- Neural Tangent Kernel (NTK): A theoretical framework for understanding neural network training dynamics in the infinite-width limit. Why needed: Provides a mathematical foundation for analyzing how network function evolves during training. Quick check: Can you explain the difference between the theoretical NTK and empirical NTK used in this paper?
- Function space vs Parameter space: Understanding that neural networks can be analyzed both by their parameters and by the functions they represent. Why needed: The paper's analysis focuses on functional evolution rather than parameter trajectories. Quick check: Can you describe how a small change in parameters might lead to large functional changes?
- Phase transitions in optimization: The concept that optimization dynamics can change qualitatively during training. Why needed: The paper identifies a specific transition point between two distinct training phases. Quick check: What are the characteristics of the two phases identified in this paper?

## Architecture Onboarding
Component map: Data -> Model (VGG-16/ResNet-20) -> Training (SGD/Adam) -> Analysis (interval-wise comparison)
Critical path: Data preprocessing → Model initialization → Training loop with perturbation tracking → Interval-wise analysis → Phase transition detection
Design tradeoffs: The interval-wise analysis provides better phase detection but requires more computational resources compared to point-wise analysis. The perturbation magnitude needs to be small enough to not disrupt training but large enough to be measurable.
Failure signatures: If perturbations don't show divergent behavior early in training, the Chaos Effect may not be properly detected. If the NTK doesn't show cone-shaped constraint, the Cone Effect may be absent or the transition point misidentified.
First experiments:
1. Reproduce the VGG-16 and ResNet-20 results on CIFAR-10 to validate the basic phenomena
2. Test different perturbation magnitudes to understand the sensitivity of the Chaos Effect detection
3. Vary the interval size in the interval-wise analysis to optimize phase transition detection

## Open Questions the Paper Calls Out
None

## Limitations
- The Chaos Effect characterization lacks quantitative thresholds for defining "chaotic" behavior versus normal training variability
- The transition between phases is identified visually rather than through formal statistical criteria
- The generality of the cone-shaped constraint across different architectures and tasks is not established

## Confidence
- Two-phase transition identification: High
- Chaos Effect characterization: Medium
- Cone Effect constraints: Medium
- Cross-architecture generality: Low

## Next Checks
1. Test the two-phase dynamics across diverse architectures (RNNs, Transformers) and tasks (regression, reinforcement learning) to establish broader applicability
2. Implement quantitative metrics for chaos detection and cone constraint measurement to replace visual/qualitative assessments
3. Design perturbation experiments with varying magnitudes and types to better understand the stability boundary between phases