---
ver: rpa2
title: Are LLMs Really Not Knowledgeable? Mining the Submerged Knowledge in LLMs'
  Memory
arxiv_id: '2412.20846'
source_url: https://arxiv.org/abs/2412.20846
tags:
- knowledge
- hits
- correct
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a systematic gap between knowledge storage
  and expression in large language models (LLMs), revealing that models often retain
  correct information in their internal probability distributions even when generating
  incorrect answers. To quantify this phenomenon, the authors propose Hits@k, a novel
  metric that measures knowledge retention by checking if the correct answer appears
  within the top-k tokens of the model's output distribution.
---

# Are LLMs Really Not Knowledgeable? Mining the Submerged Knowledge in LLMs' Memory

## Quick Facts
- arXiv ID: 2412.20846
- Source URL: https://arxiv.org/abs/2412.20846
- Authors: Xingjian Tao; Yiwei Wang; Yujun Cai; Zhicheng Yang; Jing Tang
- Reference count: 10
- Primary result: LLMs retain significantly more knowledge than expressed in standard QA metrics

## Executive Summary
This paper identifies a systematic gap between knowledge storage and expression in large language models (LLMs), revealing that models often retain correct information in their internal probability distributions even when generating incorrect answers. To quantify this phenomenon, the authors propose Hits@k, a novel metric that measures knowledge retention by checking if the correct answer appears within the top-k tokens of the model's output distribution. Extensive experiments across multiple datasets (DBPedia, IMDB, GoodReads) and models (LLaMA2/3, Qwen2, Mistral) demonstrate that LLMs possess significantly more factual knowledge than standard QA accuracy metrics suggest. The paper further reveals that prompting strategies allowing "unsure" responses can inadvertently suppress correct answers and proposes filtering uninformative tokens to recover masked knowledge.

## Method Summary
The paper introduces Hits@k as a metric to measure knowledge retention in LLMs by checking if the correct answer appears within the top-k tokens of the model's output distribution. The method involves generating probability distributions for each token position and analyzing whether correct answers are present but suppressed in lower-ranked tokens. The authors conduct experiments across multiple datasets (DBPedia, IMDB, GoodReads) and models (LLaMA2/3, Qwen2, Mistral) to quantify the knowledge-expression gap. They also investigate the impact of "unsure" prompts on knowledge expression and propose token filtering techniques to recover masked knowledge from the probability distributions.

## Key Results
- LLaMA3-8B achieves only 17.2% Hits@1 on DBPedia but reaches 57.9% for Hits@5, demonstrating substantial knowledge retention beyond expressed answers
- Allowing "unsure" responses significantly reduces knowledge expression, with accuracy dropping from 53.5% to 17.2% on DBPedia
- Token filtering methods can recover masked knowledge by removing uninformative tokens that suppress correct answers in probability distributions

## Why This Works (Mechanism)
The paper's approach works by recognizing that LLMs maintain rich probability distributions over all possible tokens at each generation step, even when the highest-probability token is incorrect. This creates a gap between the model's internal knowledge representation (captured by the full probability distribution) and its expressed output (determined by decoding strategy). By measuring Hits@k across different values of k, the authors can quantify how much knowledge is present but suppressed in the model's output distribution. The mechanism relies on the observation that correct answers often appear in the top-k tokens for k>1, even when the model generates incorrect answers at k=1. This suggests that the model "knows" the correct answer but fails to express it due to decoding artifacts or prompt-induced uncertainty suppression.

## Foundational Learning
- **Knowledge Expression Gap**: The difference between what LLMs know internally (probability distributions) versus what they express (top-1 token). Why needed: Understanding this gap is crucial for evaluating true model capabilities beyond surface-level performance. Quick check: Compare Hits@k scores across different k values to quantify the gap magnitude.
- **Probability Distribution Analysis**: Examining the full token probability distribution rather than just top-1 predictions. Why needed: Standard accuracy metrics miss knowledge present in lower-ranked tokens. Quick check: Visualize probability distributions for correct vs incorrect generations.
- **Decoding Strategy Impact**: How different decoding approaches (temperature, top-k, etc.) affect knowledge expression. Why needed: Decoding choices can mask or reveal latent knowledge. Quick check: Test Hits@k stability across different decoding temperatures.

## Architecture Onboarding
**Component Map**: LLM inference -> Probability distribution generation -> Token ranking -> Hits@k calculation -> Knowledge gap analysis -> Token filtering -> Knowledge recovery

**Critical Path**: Probability distribution generation is the critical component, as all downstream analysis depends on accurate token probabilities from the LLM.

**Design Tradeoffs**: The Hits@k metric trades computational efficiency for comprehensive knowledge assessment, requiring full probability distributions rather than just top predictions. Token filtering introduces complexity but enables knowledge recovery.

**Failure Signatures**: Knowledge gaps may be artifacts of temperature effects or distribution smoothing rather than true knowledge retention. Filtering methods may introduce biases toward certain knowledge types.

**Three First Experiments**:
1. Test Hits@k metric stability across different decoding temperatures and beam search configurations
2. Compare knowledge retention in models trained on diverse cultural/linguistic data versus Western-centric sources
3. Evaluate token filtering approach on multi-hop reasoning tasks to assess generalizability

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- The Hits@k metric may conflate genuine knowledge retention with artifacts of probability distribution smoothing and temperature effects
- Experimental design relies on controlled QA datasets that may not generalize to complex, multi-hop reasoning tasks
- Token filtering methods lack rigorous validation on out-of-distribution data and may introduce biases

## Confidence
- LLMs retain more knowledge than expressed (Medium confidence): Evidence from Hits@k scores is compelling but metric sensitivity to decoding strategies introduces uncertainty
- Unsure responses suppress correct answers (Medium confidence): Experimental results support this but could benefit from more rigorous controls
- Filtering uninformative tokens recovers masked knowledge (Low-Medium confidence): Shows promise in controlled experiments but requires more extensive validation

## Next Checks
1. Test Hits@k metric stability across different decoding temperatures and beam search configurations to determine whether knowledge gaps persist under varying generation conditions
2. Evaluate the token filtering approach on out-of-distribution datasets and multi-hop reasoning tasks to assess its generalizability beyond simple QA formats
3. Conduct ablation studies comparing knowledge retention in LLMs trained on diverse cultural and linguistic data versus those trained primarily on Western-centric sources to identify potential biases in knowledge storage patterns