---
ver: rpa2
title: 'ELITE: Embedding-Less retrieval with Iterative Text Exploration'
arxiv_id: '2505.11908'
source_url: https://arxiv.org/abs/2505.11908
tags:
- retrieval
- query
- time
- information
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ELITE, an embedding-free retrieval framework
  for long-context question answering that leverages LLM reasoning capabilities instead
  of vector similarity. The method iteratively refines search space using importance-based
  sufficiency judgments and extends retrieval results with logically related information
  without explicit graph construction.
---

# ELITE: Embedding-Less retrieval with Iterative Text Exploration

## Quick Facts
- arXiv ID: 2505.11908
- Source URL: https://arxiv.org/abs/2505.11908
- Reference count: 24
- Key outcome: Embedding-free retrieval achieves 71.27% accuracy on NovelQA and 68.46% on Marathon benchmarks using LLaMA-3.1:70b, ranking 2nd while reducing storage and runtime by over an order of magnitude.

## Executive Summary
ELITE introduces an embedding-free retrieval framework for long-context question answering that leverages large language model reasoning instead of vector similarity. The method iteratively refines search space using importance-based sufficiency judgments and extends retrieval results with logically related information without explicit graph construction. ELITE achieves strong performance on NovelQA and Marathon benchmarks while eliminating reliance on embedding models and dense indexing, requiring no extra storage and delivering rapid responses.

## Method Summary
ELITE operates through a 5-stage iterative process: (1) Agent-guided terms of interest generation to identify relevant search terms; (2) Lexical overlap retrieval with symmetric contextual expansion (±5 sentences); (3) Importance-based sufficiency judging using Monte Carlo noise perturbation with cosine similarity; (4) Breadth/depth-wise extension via implicit graph traversal by re-generating search terms from retrieved chunks; (5) Evidence trimming and response generation. The framework uses LLaMA models (8B, 70B, 1B, 3B) with specific hyperparameters including recall_index=25, neighbor_num=2, deep_search_index=5, deep_search_num=10, voter_num=10, and iter_max=5. The approach handles counting questions through specialized detection and tense/form enumeration.

## Key Results
- Achieves 71.27% accuracy on NovelQA benchmark
- Achieves 68.46% accuracy on Marathon benchmark
- Ranks 2nd on both leaderboards using LLaMA-3.1:70b
- Reduces storage and runtime by over an order of magnitude compared to baselines

## Why This Works (Mechanism)
ELITE replaces traditional embedding-based retrieval with LLM reasoning capabilities, eliminating the need for vector similarity calculations and dense indexing. By iteratively refining search space through importance-based sufficiency judgments, the system focuses computational resources on the most relevant information. The breadth/depth extension mechanism allows the retrieval to naturally expand to related concepts without explicit graph construction, while the evidence trimming step ensures concise, relevant responses. This approach leverages the LLM's ability to understand semantic relationships and logical connections between text chunks.

## Foundational Learning
- **Lexical overlap retrieval with contextual expansion**: Why needed - to find relevant passages without embeddings; Quick check - verify ±5 sentence expansion captures sufficient context
- **Monte Carlo noise perturbation for importance scoring**: Why needed - to assess chunk relevance without embeddings; Quick check - ensure noise perturbation produces stable importance scores
- **Iterative breadth/depth extension**: Why needed - to expand retrieval to related information; Quick check - monitor convergence behavior across iterations
- **Counting question detection and handling**: Why needed - to properly process quantitative questions; Quick check - validate detection accuracy on test samples
- **Sufficiency judgment via LLM**: Why needed - to determine when retrieval is complete; Quick check - verify sufficiency prompts produce consistent decisions
- **Evidence trimming and response generation**: Why needed - to produce concise, relevant answers; Quick check - measure response relevance and conciseness

## Architecture Onboarding

**Component Map**
LLM Agent -> Term Extraction -> Lexical Retrieval -> ±5 Sentence Expansion -> Importance Scoring -> Sufficiency Judgment -> Breadth/Depth Extension -> Evidence Trimming -> Response Generation

**Critical Path**
Question -> Term Extraction -> Lexical Retrieval -> Importance Scoring -> Sufficiency Judgment -> Response Generation

**Design Tradeoffs**
The framework trades embedding storage and indexing infrastructure for LLM reasoning overhead, accepting potential variability in LLM responses for flexibility and reduced storage requirements. The iterative approach balances thoroughness against computational cost through the iter_max parameter.

**Failure Signatures**
- Importance score computation too slow due to Monte Carlo sampling
- Iterative retrieval fails to converge or hits iter_max too often
- Counting question detection misses or misclassifies questions
- Sufficiency judgments are inconsistent across iterations

**3 First Experiments**
1. Test counting question detection and handling logic on a small subset of questions
2. Validate importance scoring computation with different noise perturbation parameters
3. Verify iterative extension behavior with sample questions and monitor convergence

## Open Questions the Paper Calls Out
None

## Limitations
- Key implementation details remain underspecified, particularly prompt templates for agent behavior
- Performance depends heavily on LLM reasoning quality and may vary with different base models
- Counting question handling logic and termination criteria are not fully detailed
- The method inherits LLM reasoning variability, potentially affecting consistency

## Confidence

**High Confidence**: Embedding-free approach works and delivers strong benchmark results; storage and runtime improvements are well-documented; the 5-stage iterative framework is clearly defined.

**Medium Confidence**: The general methodology is reproducible, but exact prompt formulations and hyperparameters for importance scoring are unclear, which may affect sufficiency judgments and retrieval quality.

**Low Confidence**: Counting question handling details and precise termination criteria beyond iter_max are not fully specified, creating ambiguity in edge-case processing.

## Next Checks

1. **Prompt Template Validation**: Reconstruct and test the Generation, Evaluation, and Exploration prompt templates with small-scale runs to verify they produce the expected agent behaviors (term extraction, sufficiency judgment, iterative extension).

2. **Importance Scoring Parameter Sweep**: Experiment with different Monte Carlo sample counts and noise levels λ in the importance score computation to identify configurations that balance accuracy and runtime, ensuring they match the reported efficiency gains.

3. **Counting Question Detection Logic**: Implement and validate the counting detection criteria and tense/form enumeration logic on a subset of NovelQA/Marathon to confirm it correctly identifies and handles counting questions without introducing errors.