---
ver: rpa2
title: 'SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based on
  Temporal Masked Transformer'
arxiv_id: '2502.11094'
source_url: https://arxiv.org/abs/2502.11094
tags:
- speech
- text
- tokens
- syncspeech
- duration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SyncSpeech, a dual-stream text-to-speech
  (TTS) model capable of receiving streaming text input while simultaneously generating
  streaming speech. The core innovation is the Temporal Masked Transformer (TMT),
  which uses a novel attention mask and a two-stage training strategy to predict both
  speech tokens and durations in a temporally-ordered manner.
---

# SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based on Temporal Masked Transformer

## Quick Facts
- **arXiv ID**: 2502.11094
- **Source URL**: https://arxiv.org/abs/2502.11094
- **Authors**: Zhengyan Sheng; Zhihao Du; Shiliang Zhang; Zhijie Yan; Yexin Yang; Zhenhua Ling
- **Reference count**: 22
- **Key outcome**: SyncSpeech achieves 3.2-3.8× faster first packet delay and 6.4-8.5× faster real-time factor than recent dual-stream TTS models while maintaining comparable speech quality.

## Executive Summary
This paper introduces SyncSpeech, a dual-stream text-to-speech model that can generate streaming speech from streaming text input. The core innovation is the Temporal Masked Transformer (TMT), which uses a novel attention mask and two-stage training to predict both speech tokens and durations in a temporally-ordered manner. SyncSpeech begins speech generation upon receiving the second text token and achieves high efficiency by decoding all corresponding speech tokens in one step. Experiments on English and Mandarin datasets demonstrate significant improvements in latency and real-time factor compared to recent dual-stream TTS models, while maintaining speech quality comparable to traditional autoregressive models.

## Method Summary
SyncSpeech is a dual-stream TTS model that processes text and speech tokens simultaneously through a shared encoder-decoder architecture. The key innovation is the Temporal Masked Transformer (TMT), which employs a novel attention mask that allows the model to predict speech tokens and durations in the correct temporal order while maintaining efficiency. The TMT uses both forward-looking and backward-looking attention masks, where the forward-looking mask predicts future tokens based on past information, and the backward-looking mask refines predictions based on future context. The model is trained using a two-stage strategy: first predicting speech tokens and durations jointly, then refining these predictions with additional training steps. This architecture enables SyncSpeech to begin generating speech immediately after receiving the second text token while decoding all corresponding speech tokens in a single step, achieving both low latency and high efficiency.

## Key Results
- SyncSpeech achieves 3.2-3.8× faster first packet delay compared to recent dual-stream TTS models
- Real-time factor improves by 6.4-8.5× over competing approaches
- Speech quality maintains comparable Mean Opinion Score (MOS) to traditional autoregressive models
- Model demonstrates robustness across both English and Mandarin datasets

## Why This Works (Mechanism)
The Temporal Masked Transformer's efficiency stems from its ability to predict multiple speech tokens simultaneously while maintaining temporal ordering through the attention mask design. By allowing the model to look ahead and behind in the sequence during different training stages, it learns to make accurate predictions without requiring autoregressive decoding. The two-stage training strategy first establishes basic duration and token predictions, then refines them with additional context, enabling both speed and accuracy. The forward-looking mask enables immediate response to new text tokens while the backward-looking mask ensures coherence in the generated speech.

## Foundational Learning

1. **Dual-Stream Architecture** - Why needed: Separates text and speech processing to enable streaming generation
   Quick check: Verify text and speech streams can operate independently without waiting for complete input

2. **Temporal Masked Attention** - Why needed: Enables efficient prediction of speech tokens while maintaining temporal order
   Quick check: Confirm mask allows correct future context access without breaking streaming constraints

3. **Two-Stage Training** - Why needed: Jointly predicts tokens and durations first, then refines predictions
   Quick check: Validate both stages improve performance over single-stage approaches

4. **Streaming Text Processing** - Why needed: Enables immediate response to incoming text tokens
   Quick check: Measure latency from text input to speech output at different text positions

5. **Duration Prediction** - Why needed: Critical for synchronizing text to speech timing
   Quick check: Compare predicted durations against ground truth across various phoneme contexts

6. **Non-Autoregressive Decoding** - Why needed: Enables parallel token generation for efficiency
   Quick check: Measure decoding speed versus autoregressive baselines

## Architecture Onboarding

**Component Map**: Text Encoder -> TMT Encoder -> Duration Predictor + Speech Decoder -> Audio Output

**Critical Path**: Streaming text input → Text Encoder → TMT Encoder → Duration Predictor → Speech Decoder → Speech Output

**Design Tradeoffs**: The two-stage training adds complexity but enables better accuracy; the attention mask design balances lookahead capability with streaming constraints; parallel decoding sacrifices some refinement capability for speed.

**Failure Signatures**: Incorrect duration predictions cause timing misalignment; attention mask errors lead to word order mistakes; two-stage training instability can cause degraded quality; streaming context limits can break longer sentence coherence.

**First Experiments**:
1. Measure latency from first text token to first speech output
2. Compare MOS scores against autoregressive baseline models
3. Benchmark real-time factor under varying CPU/GPU conditions

## Open Questions the Paper Calls Out

None specified in the provided content.

## Limitations

- Relative efficiency improvements (6.4-8.5× faster) lack absolute baseline metrics for meaningful comparison
- Two-stage training complexity may impact practical deployment and out-of-domain generalization
- Latency metrics focus on first token delay without comprehensive distributions across text lengths and complexities
- Comparisons with traditional autoregressive models may not represent current state-of-the-art streaming TTS approaches

## Confidence

**High Confidence**: The Temporal Masked Transformer implementation with forward-looking and backward-looking attention masks is well-documented and reproducible. The claim about beginning generation on the second text token is specific and verifiable.

**Medium Confidence**: Performance improvements are based on the paper's experimental setup but lack detailed baseline implementation information. MOS score comparisons support speech quality claims but require independent verification.

**Low Confidence**: Efficiency assertions relative to other dual-stream models are difficult to verify without exact implementations and computational measurements. Robustness claims need validation across broader linguistic contexts.

## Next Checks

1. **Latency Distribution Analysis**: Measure comprehensive latency across varying text lengths (short phrases to long paragraphs) and linguistic complexities to validate efficiency gains under real-world conditions.

2. **Cross-Lingual Robustness Testing**: Evaluate performance on diverse linguistic structures beyond English and Mandarin, including morphologically rich languages and different writing systems, to verify generalizability of the two-stage training approach.

3. **Resource Efficiency Benchmarking**: Measure actual computational requirements (FLOPs, memory usage, inference time) compared to baseline systems under identical hardware conditions to substantiate the claimed real-time factor improvements with concrete absolute metrics.