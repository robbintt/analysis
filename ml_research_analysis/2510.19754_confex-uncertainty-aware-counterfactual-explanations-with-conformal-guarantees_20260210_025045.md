---
ver: rpa2
title: 'CONFEX: Uncertainty-Aware Counterfactual Explanations with Conformal Guarantees'
arxiv_id: '2510.19754'
source_url: https://arxiv.org/abs/2510.19754
tags:
- coverage
- confextree
- counterfactual
- conformal
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CONFEX addresses the problem of generating reliable counterfactual
  explanations by incorporating predictive uncertainty into the explanation process.
  The core method idea uses conformal prediction with localized coverage guarantees,
  solving an optimization problem via mixed-integer linear programming to find counterfactuals
  that are both close to the original instance and have high certainty in their predictions.
---

# CONFEX: Uncertainty-Aware Counterfactual Explanations with Conformal Guarantees

## Quick Facts
- arXiv ID: 2510.19754
- Source URL: https://arxiv.org/abs/2510.19754
- Reference count: 40
- Primary result: CONFEX generates counterfactual explanations that are both plausible and uncertainty-aware, with formal coverage guarantees that standard methods lack.

## Executive Summary
CONFEX addresses the problem of generating reliable counterfactual explanations by incorporating predictive uncertainty into the explanation process. The core method uses conformal prediction with localized coverage guarantees, solving an optimization problem via mixed-integer linear programming to find counterfactuals that are both close to the original instance and have high certainty in their predictions. The evaluation shows that CONFEX-Tree consistently produces more plausible and stable explanations across diverse datasets compared to state-of-the-art methods, while also providing formal coverage guarantees that vanilla conformal prediction methods fail to achieve.

## Method Summary
CONFEX generates counterfactual explanations by constraining the search to regions where model predictions are highly certain, using conformal prediction to enforce singleton prediction regions. The method employs localized conformal prediction to address exchangeability violations when generating counterfactuals, and uses a tree-based encoding of local quantiles to make the optimization computationally tractable via mixed-integer linear programming. The framework is evaluated on four tabular datasets using both neural network and random forest models.

## Key Results
- CONFEX-Tree consistently produces more plausible explanations (LOF scores 0.42-0.72) compared to baselines across all datasets
- The method achieves better stability with sensitivity scores between 0.05-0.28, outperforming gradient-based approaches
- CONFEX provides formal coverage guarantees that standard conformal prediction methods fail to maintain for generated counterfactuals

## Why This Works (Mechanism)

### Mechanism 1: Uncertainty Constraint via Conformal Prediction
- **Claim:** Constraining the counterfactual search to points that yield singleton prediction regions improves plausibility by explicitly avoiding high-uncertainty areas near decision boundaries.
- **Mechanism:** CONFEX uses a log-likelihood ratio non-conformity score, s(x, y) = -l(x)y + maxy'≠y l(x)y'. It computes a critical quantile q1−α from a held-out calibration set. During optimization, it enforces two constraints for a candidate counterfactual x': (1) s(x', y+) ≤ q1−α, and (2) ∀y≠y+, s(x', y) > q1−α. This ensures the prediction region for x' is the singleton {y+}, indicating a high-certainty prediction.
- **Core assumption:** The non-conformity score effectively captures prediction uncertainty relevant to recourse reliability.
- **Evidence anchors:**
  - [abstract]: "The key idea is to constrain the search for CFXs to regions where the model's prediction is highly certain, avoiding misleading explanations near decision boundaries."
  - [section]: Eq. (6) defines the score; Eq. (5) defines the CONFEX-Naive constraint C1−α(x') = {y+}.
  - [corpus]: Corpus support for this specific CP mechanism is weak/indirect. Related work on "Provably Robust Bayesian Counterfactual Explanations" addresses a different uncertainty source (model staleness).
- **Break condition:** If the calibration set is not representative of the test distribution, the quantile q1−α may be misestimated, and the singleton constraint may not reliably indicate certainty or guarantee coverage.

### Mechanism 2: Exchangeability Repair via Localised Conformal Prediction (LCP)
- **Claim:** Standard CP guarantees are invalid for generated CFXs because the search process violates the exchangeability assumption. Localized CP restores approximate test-conditional guarantees by reweighting calibration points based on local similarity.
- **Mechanism:** Instead of a global quantile, CONFEX-LCP computes a local quantile qLCP1−α(x*) using a localizer kernel H(x, x'). Calibration points xi near the candidate x* (within L1-distance bandwidth h) are assigned higher weights wi. The quantile is computed from this reweighted distribution of calibration scores, making the coverage guarantee local to x* and circumventing the exchangeability violation.
- **Core assumption:** The kernel bandwidth h correctly defines "local" neighborhoods that restore approximate exchangeability.
- **Evidence anchors:**
  - [abstract]: "CONFEX addresses the challenge of exchangeability in CP by employing localized CP to enforce test-conditional guarantees."
  - [section]: Section 4.1 defines LCP, Eq. (8) the L1-box kernel, Eq. (9) the local quantile, and Eq. (10) the approximate conditional guarantee.
  - [corpus]: Related work "Who Moved My Distribution? Conformal Prediction for Interactive Multi-Agent Systems" also deals with distribution shift, supporting the general need for adaptive CP.
- **Break condition:** If the kernel bandwidth h is too large, the method reverts to marginal CP, failing to provide conditional guarantees. If h is too small, calibration neighborhoods may have insufficient points for stable quantile estimation.

### Mechanism 3: Efficient Optimization via Tree-based MILP Encoding
- **Claim:** A tree-based pre-computation of local quantiles enables efficient encoding of LCP constraints in Mixed-Integer Linear Programming (MILP), making the search computationally tractable.
- **Mechanism:** CONFEX-Tree partitions the feature space offline using a KD-tree-like construction. Each leaf node g represents a region with a pre-computed quantile qg. At solve time, the MILP traverses this tree to select the appropriate quantile for the candidate x'. This replaces the expensive online computation of the local quantile (linear in calibration set size) with a fixed, efficient tree-based lookup encoded as MILP constraints.
- **Core assumption:** The tree partition is sufficiently fine-grained such that all points in a leaf are effectively "local" and share a suitable quantile.
- **Evidence anchors:**
  - [abstract]: "The method is implemented efficiently using a tree-based encoding of local quantiles."
  - [section]: Algorithm 1 details the CONFEX-Tree construction; Eq. (12) shows its integration into the MILP formulation.
  - [corpus]: No direct corpus support for this specific tree-based MILP encoding for CP.
- **Break condition:** In high-dimensional feature spaces, the tree may become sparse (curse of dimensionality), leading to leaves with few or no calibration points, causing quantile instability or failure.

## Foundational Learning

- **Concept: Conformal Prediction (CP) & Exchangeability**
  - **Why needed here:** CP is the core uncertainty quantification tool. Understanding its marginal guarantees and the critical exchangeability assumption is essential to grasp why a naive CP application is insufficient and why LCP is introduced.
  - **Quick check question:** Explain in one sentence why a standard CP guarantee might not hold for a counterfactual candidate generated by an optimization process.

- **Concept: Mixed-Integer Linear Programming (MILP) for ML Models**
  - **Why needed here:** The entire search for an optimal CFX is formulated as a MILP problem. One must understand that models like ReLU networks can be encoded as linear constraints with binary variables, allowing for provably optimal solutions.
  - **Quick check question:** How does a MILP-based CFX search guarantee optimality compared to a gradient-based approach like Wachter's?

- **Concept: Test-Conditional vs. Marginal Coverage**
  - **Why needed here:** The central problem is that standard CP provides marginal coverage (on average), but for a specific generated CFX, we need a guarantee that holds conditionally. Understanding this distinction is key.
  - **Quick check question:** What does "marginal coverage" mean, and why is "test-conditional coverage" a stronger and more desirable property for an individual explanation?

## Architecture Onboarding

- **Component map:** Offline Phase (Calibration data + trained model → Compute non-conformity scores → Build CONFEX-Tree) -> Online Phase (Receive factual instance x0 and target class y+) -> MILP Solver (Formulate optimization problem with distance, model prediction, tree traversal, and CP score constraints) -> Output (Optimal, uncertainty-aware counterfactual xcf)

- **Critical path:**
  1. Correct computation of non-conformity scores on the calibration set.
  2. Proper construction of the CONFEX-Tree with appropriate bandwidth `h` (controls locality vs. stability tradeoff).
  3. Accurate MILP encoding of the classifier and the tree-based quantile selection logic.

- **Design tradeoffs:**
  - **Bandwidth (h):** Small `h` → stronger locality (better guarantees), but risk of unstable quantiles. Large `h` → closer to marginal CP (weaker guarantees), but more stable quantiles.
  - **Coverage level (α):** Small `α` (e.g., 0.01) → conservative prediction sets, making valid CFXs potentially infeasible. Larger `α` → more feasible solutions but potentially less certain.
  - **LCP vs. Tree:** CONFEX-LCP provides a purer form of LCP but is computationally expensive. CONFEX-Tree is an efficient approximation suitable for practical use.

- **Failure signatures:**
  - **100% Failure Rate / "nan" in results:** Alpha is too small or bandwidth too narrow, resulting in no feasible singleton regions for the target class.
  - **Low Plausibility (negative LOF score):** CFX is pushed into sparse regions of the data manifold, possibly due to a mis-specified kernel.
  - **High Coverage Gap:** Localisation is failing (bandwidth too large), and the generated CFXs violate the desired coverage level, indicating the method is not properly accounting for exchangeability violation.

- **First 3 experiments:**
  1. **Baselines vs. CONFEX-Tree (Table 1):** Run CONFEX-Tree (with tuned h) against MILP-MinDist, Wachter, and ECCCo on GermanCredit and AdultIncome. Measure distance, plausibility (LOF), and sensitivity.
  2. **Bandwidth & Alpha Ablation (Fig. 2):** Systematically vary kernel bandwidth (h) and miscoverage rate (α) on the CaliforniaHousing dataset. Plot distance, plausibility, and coverage gap to determine the optimal operating region.
  3. **Conformal Guarantee Validation (Simulated Setup):** Implement the resampling procedure described in Section 5 to compute empirical coverage for CFX-like points. Compare the coverage gap of CONFEX-Naive (expected to fail) vs. CONFEX-Tree with various bandwidths to confirm the restoration of approximate conditional guarantees.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the localisation kernel bandwidth be selected automatically or robustly without extensive domain knowledge or separate validation sets?
- **Basis in paper:** [explicit] The authors state in the Limitations section that "Picking an appropriate kernel bandwidth is an additional task which requires domain knowledge or evaluation on a validation set."
- **Why unresolved:** Currently, the bandwidth is a hyperparameter that significantly affects plausibility and coverage, requiring manual tuning.
- **What evidence would resolve it:** An adaptive algorithm or heuristic that sets the bandwidth based on local data density or intrinsic dataset properties while maintaining coverage guarantees.

### Open Question 2
- **Question:** Can the CONFEX framework be extended to scale efficiently to very large models (e.g., deep neural networks for vision) where MILP encodings are currently infeasible?
- **Basis in paper:** [explicit] The paper notes that "Since our approach uses MILP... it will struggle scaling to very large models; gradient-based methods... are less prone to this problem."
- **Why unresolved:** The combinatorial complexity of MILP limits the method's applicability to simple models like MLPs and Random Forests.
- **What evidence would resolve it:** A modification of the method (e.g., a hybrid gradient-MILP approach) that successfully generates CFXs for large-scale architectures like ResNets or Transformers.

### Open Question 3
- **Question:** How does the method perform in low-data regimes where the held-out calibration set is insufficient to construct meaningful prediction regions?
- **Basis in paper:** [explicit] The authors list as a limitation: "CP requires a held-out calibration dataset, which may be problematic when data is scarce."
- **Why unresolved:** While CP guarantees hold theoretically for any $n$, small calibration sets result in overly conservative (wide) prediction regions, potentially making the CFX search infeasible.
- **What evidence would resolve it:** An empirical analysis of CONFEX's success rate and CFX quality as the calibration set size $n$ approaches the minimum theoretical bounds.

## Limitations
- Requires a held-out calibration dataset, which may be problematic when data is scarce
- Picking an appropriate kernel bandwidth is an additional task which requires domain knowledge or evaluation on a validation set
- The MILP-based approach struggles to scale to very large models, where gradient-based methods are less prone to this problem

## Confidence

- **High**: The theoretical framework of using conformal prediction to constrain counterfactual search to high-certainty regions is sound and well-motivated.
- **Medium**: The effectiveness of CONFEX-Tree in producing more plausible and stable explanations compared to baselines, as empirical results show improvements but depend on careful hyperparameter tuning.
- **Medium**: The restoration of approximate conditional guarantees via localized CP, though this requires proper bandwidth selection and may not fully address exchangeability violations in all cases.

## Next Checks

1. **Reproduce the MILP vs. Gradient Comparison**: Implement CONFEX-Tree and compare its performance against Wachter's gradient-based method on GermanCredit and AdultIncome datasets. Measure distance, plausibility (LOF), and sensitivity to confirm the claimed improvements.

2. **Systematically Vary Bandwidth and Alpha**: Conduct an ablation study on the CaliforniaHousing dataset, varying kernel bandwidth (h) and miscoverage rate (α). Plot distance, plausibility, and coverage gap to identify the optimal operating region and understand the tradeoff between locality and stability.

3. **Validate Conformal Guarantee Restoration**: Implement the resampling procedure to compute empirical coverage for CFX-like points. Compare the coverage gap of CONFEX-Naive (expected to fail) vs. CONFEX-Tree with various bandwidths to confirm the restoration of approximate conditional guarantees and the effectiveness of the exchangeability repair mechanism.