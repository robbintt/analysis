---
ver: rpa2
title: 'MS-SSM: A Multi-Scale State Space Model for Efficient Sequence Modeling'
arxiv_id: '2512.23824'
source_url: https://arxiv.org/abs/2512.23824
tags:
- arxiv
- ms-ssm
- sequence
- state
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MS-SSM, a multi-scale state space model designed
  to capture both fine-grained and coarse-grained temporal dependencies in sequence
  modeling. Traditional SSMs have limited effective memory, and MS-SSM addresses this
  by decomposing input sequences into multiple resolution scales, each processed by
  specialized state-space dynamics.
---

# MS-SSM: A Multi-Scale State Space Model for Efficient Sequence Modeling

## Quick Facts
- **arXiv ID:** 2512.23824
- **Source URL:** https://arxiv.org/abs/2512.23824
- **Reference count:** 29
- **Primary result:** MS-SSM outperforms prior SSM-based models on sCIFAR, ImageNet-1K, ListOps, and PTB-XL benchmarks

## Executive Summary
MS-SSM introduces a multi-scale state space model designed to capture both fine-grained and coarse-grained temporal dependencies in sequence modeling. The approach decomposes input sequences into multiple resolution scales, each processed by specialized state-space dynamics. By using multi-scale convolutions and input-dependent scale-mixing, MS-SSM dynamically fuses information across resolutions while maintaining computational efficiency. Experimental results demonstrate significant improvements in capturing long-range dependencies compared to traditional SSMs.

## Method Summary
MS-SSM replaces standard SSM blocks with a multi-scale architecture consisting of three key components: (1) a multi-scale decomposition layer using nested dilated convolutions with dilations 1, 2, and 4; (2) an array of independent SSMs (S4 or S6) applied to each scale; and (3) an input-dependent scale mixer that dynamically weights and combines outputs. The model uses scale-dependent initialization for the state transition matrix, with slower decay rates for low-resolution scales to capture long-range dependencies. Training follows standard ViT-style approaches with Batch Normalization for most tasks.

## Key Results
- MS-SSM consistently outperforms prior SSM-based models on benchmarks including sCIFAR, ImageNet-1K, ListOps, and PTB-XL
- Notable gains in long-range and hierarchical tasks, particularly on ListOps (63% accuracy vs 57% for Mamba initialization)
- Maintains computational efficiency while capturing both fine-grained and coarse-grained temporal dependencies
- Ablation studies confirm the importance of scale-dependent initialization and input-dependent mixing

## Why This Works (Mechanism)

### Mechanism 1: Resolution-Specific Memory Allocation
The effective memory of an SSM is determined by the distance of state transition eigenvalues from the unit circle. MS-SSM initializes low-resolution scales with eigenvalues closer to 1 (slow decay) for long-range dependencies, while high-resolution scales use faster decay for local dynamics. This aligns memory retention with temporal resolution.

### Mechanism 2: Redundant Multi-Resolution Representation
Using stationary wavelet transform without downsampling preserves sequence length across scales, maintaining spatial alignment and translation invariance. This enables direct summation of parallel SSM outputs without complex alignment logic, at the cost of increased memory usage.

### Mechanism 3: Dynamic Cross-Scale Gating
The input-dependent scale mixer projects the original input to generate weights for combining SSM outputs from different scales. This allows the model to dynamically prioritize global context versus local detail based on specific token semantics, rather than using static averaging.

## Foundational Learning

- **State Space Models & Discretization ($\Delta$):** MS-SSM relies on the mathematical properties of discretized state matrices $\bar{A} = \exp(\Delta A)$. Understanding how step size $\Delta$ and matrix $A$ interact is crucial for implementing scale-dependent initialization correctly.
  - *Quick check:* How does increasing step size $\Delta$ affect discretized state matrix $\bar{A}$ and the model's ability to capture fine-grained vs. long-range dependencies?

- **Receptive Field vs. Effective Memory:** The paper distinguishes between receptive field (fixed by convolution kernel length/dilation) and effective memory (dynamically controlled by eigenvalue decay). This distinction is central to MS-SSM's design of using convolutions for local patterns and SSMs for global memory.
  - *Quick check:* In an SSM, if eigenvalues of $\bar{A}$ are all 0.9, is the effective memory longer or shorter than if they were 0.5?

- **Stationary Wavelet Transform (SWT):** MS-SSM modifies standard wavelet approaches by removing downsampling. Understanding why translation invariance is lost in standard wavelets explains why MS-SSM adopts the redundant, undecimated approach.
  - *Quick check:* Why does standard Discrete Wavelet Transform struggle with translation invariance, and how does skipping downsampling in SWT fix this?

## Architecture Onboarding

- **Component map:** Input -> Multi-Scale Conv Block -> SSM Array -> Scale Mixer
- **Critical path:** Input → Multi-Scale Conv Block (ensure no padding artifacts) → SSM Array (check stability of $\bar{A}$) → Scale Mixer (check for weight collapsing)
- **Design tradeoffs:** The undecimated approach requires $O(S \times L \times D)$ memory for activations but guarantees translation invariance. MS-SSM(S4) offers simpler implementation if S6 kernels are unavailable.
- **Failure signatures:** Training divergence from incorrect $\bar{A}$ initialization (eigenvalues > 1); scale collapse from uniform mixer weights; shift sensitivity from non-stationary convolution.
- **First 3 experiments:**
  1. sCIFAR Sanity Check: Train on sCIFAR to verify basic implementation, target accuracy > 90%
  2. Initialization Ablation: Run ListOps with proposed initialization vs. standard initialization to validate resolution-specific memory hypothesis
  3. Mixer Variants: Compare input-dependent vs. linear layer mixer on PTB-XL to isolate value of dynamic fusion

## Open Questions the Paper Calls Out
None

## Limitations
- Scale-dependent initialization parameter $\Delta_0$ is not specified across all experiments, affecting reproducibility
- The $O(S \times L \times D)$ memory requirement for undecimated multi-scale representation may pose challenges in memory-constrained settings
- Performance may degrade on tasks lacking clear hierarchical structure due to design assumptions

## Confidence
- **High:** Effectiveness of scale-dependent initialization and input-dependent scale mixing supported by ablation studies
- **Medium:** Consistent outperformance of prior SSM-based models, though absolute performance gaps vary across tasks
- **Medium:** Theoretical soundness of receptive field vs. effective memory distinction, but empirical validation is limited

## Next Checks
1. Verify initialization parameter: Implement and test different values of $\Delta_0$ to determine optimal setting for scale-dependent initialization
2. Memory overhead analysis: Profile memory usage of multi-scale decomposition on larger datasets to assess scalability
3. Cross-task generalization: Evaluate MS-SSM on dataset without clear hierarchical structure to test robustness beyond proposed use cases