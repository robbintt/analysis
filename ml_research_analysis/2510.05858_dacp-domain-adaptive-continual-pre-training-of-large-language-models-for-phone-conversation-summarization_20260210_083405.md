---
ver: rpa2
title: 'DACP: Domain-Adaptive Continual Pre-Training of Large Language Models for
  Phone Conversation Summarization'
arxiv_id: '2510.05858'
source_url: https://arxiv.org/abs/2510.05858
tags:
- data
- arxiv
- pre-training
- llms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores continual pre-training as a scalable, self-supervised
  approach to adapt smaller large language models for domain-specific phone conversation
  summarization tasks. The authors propose a Domain-Adaptive Continual Pre-training
  (DACP) framework that combines 25B tokens of anonymized real-world business conversation
  data with 25B replay tokens from general text corpora, using the Next Token Prediction
  objective.
---

# DACP: Domain-Adaptive Continual Pre-Training of Large Language Models for Phone Conversation Summarization

## Quick Facts
- **arXiv ID:** 2510.05858
- **Source URL:** https://arxiv.org/abs/2510.05858
- **Reference count:** 17
- **Primary result:** Domain-adaptive continual pre-training (DACP) with 50B tokens (25B domain + 25B replay) improves summarization performance on business phone conversations by 4-6% ROUGE-1 over baselines.

## Executive Summary
This work explores continual pre-training as a scalable, self-supervised approach to adapt smaller large language models for domain-specific phone conversation summarization tasks. The authors propose a Domain-Adaptive Continual Pre-training (DACP) framework that combines 25B tokens of anonymized real-world business conversation data with 25B replay tokens from general text corpora, using the Next Token Prediction objective. Experiments on two base models (LLaMA-3.1-8B and Mistral-V0.3-7B) show DACP improves performance on both internal business conversational tasks (Action Items, Support Call Summarization) and external benchmarks (QMSUM, QMSUM-I). For Mistral-V0.3-7B, DACP yields gains of 6.32% ROUGE-1 on Action Items and 4.11% ROUGE-1 on Support Call Summarization, with even larger improvements in factual consistency (AlignScore). On external benchmarks, DACP achieves average performance increases of 38.15% (ROUGE-1) and 9.75% (BERTScore) for LLaMA, and 150.04% (ROUGE-1) and 20.74% (BERTScore) for Mistral. A qualitative LLM judge evaluation found DACP-generated responses were preferred in 45% of cases versus 29% for baseline models.

## Method Summary
The DACP framework uses Next Token Prediction (NTP) on a mixture of 25B anonymized business conversation tokens and 25B general text replay tokens. The domain data is filtered using token-type entropy to select the most diverse 25M transcripts from a pool of 50M. The model is then fine-tuned on 84k instruction examples combining internal business tasks and GPT-4-generated data. The training uses LR=2e-6, context length 8000, and runs for 1 epoch on 6 nodes with 8x A100 80GB each using DeepSpeed.

## Key Results
- DACP improves Mistral-V0.3-7B by 6.32% ROUGE-1 on Action Items and 4.11% ROUGE-1 on Support Call Summarization
- Factual consistency (AlignScore) improvements are even larger than ROUGE gains
- On external QMSUM benchmarks, DACP achieves 38.15% ROUGE-1 and 9.75% BERTScore improvements for LLaMA
- LLM judge evaluation preferred DACP outputs in 45% of cases vs 29% for baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Continued Next Token Prediction (NTP) on noisy, domain-specific ASR data adapts the model's internal representations to the unique linguistic distribution of business calls (e.g., disfluencies, turn-taking).
- **Mechanism:** The base pre-training distribution (general web text) differs significantly from noisy transcripts. By minimizing NTP loss on 25B tokens of real-world transcripts, the model adjusts token probability distributions to better handle the structural noise and specific vocabulary of the domain.
- **Core assumption:** The patterns in anonymized business calls are consistent enough to be learned via self-supervision and transfer to downstream summarization tasks.
- **Evidence anchors:**
  - [abstract] "...continual pre-training as a scalable, self-supervised approach to adapt LLMs... in the context of noisy real-world conversation transcripts."
  - [section 3.1] "Since our focus is to leverage unlabeled business conversations, we also utilize self-supervised learning based on the NTP objective..."
  - [corpus] Weak direct validation; however, neighbor *ixi-GEN* supports the premise that CPT aids industrial sLLMs.
- **Break condition:** If the Signal-to-Noise ratio in transcripts is too low (excessive ASR errors), the model may hallucinate or learn garbage patterns, degrading factual consistency.

### Mechanism 2
- **Claim:** Mixing 50% general "replay" data (FineWeb-Edu) with 50% in-domain data mitigates catastrophic forgetting while allowing domain acquisition.
- **Mechanism:** Pure domain adaptation often degrades general reasoning capabilities. The 1:1 mixture ratio forces the model to optimize for both the new domain and general language modeling simultaneously, preserving the "general intelligence" required for complex summarization logic.
- **Core assumption:** A 1:1 ratio provides sufficient "plasticity" to learn the new domain without losing "stability" of prior knowledge.
- **Evidence anchors:**
  - [section 3.1.2] "...combined 25B replay tokens with 25B domain-specific tokens... A common mitigation strategy, known as experience replay..."
  - [abstract] "...maintaining strong generalization and robustness."
  - [corpus] Neighbor *DoMIX* highlights challenges in continual DAP, supporting the need for careful mixture strategies like replay.
- **Break condition:** If the downstream task requires complex reasoning not present in the replay data (e.g., specific logic from the base model's training that is washed out by the 50% dilution), performance on complex queries may drop.

### Mechanism 3
- **Claim:** Entropy-based data selection filters low-quality transcripts, ensuring the model trains on diverse, representative conversations rather than noise.
- **Mechanism:** High token-type entropy correlates with linguistic diversity and complexity. By selecting the top 25M transcripts from a pool of 50M, the authors heuristicistically remove repetitive or empty calls, maximizing the information density per training step.
- **Core assumption:** Token-type entropy is a proxy for "summarization value" in a conversation.
- **Evidence anchors:**
  - [section 3.1.1] "...select 25M transcripts... with the highest token type entropy scores, following Xie et al. (2023)."
  - [section 4.2] "...more data is generally more useful..." (Implies data quality/quantity scaling matters).
  - [corpus] No direct validation of entropy specifically in neighbors; standard scaling laws apply.
- **Break condition:** If high-entropy transcripts contain adversarial noise or non-business content (e.g., arguments, gibberish), the model may learn undesirable generation patterns.

## Foundational Learning

- **Concept: Next Token Prediction (NTP) vs. Instruction Tuning**
  - **Why needed here:** DACP relies on *unsupervised* NTP to learn the "dialect" of call transcripts before *supervised* Instruction Tuning teaches the "task" of summarizing. Confusing these leads to expecting reasoning from raw pre-training.
  - **Quick check question:** Does the model need to learn *new words/syntax* (NTP) or *new tasks/behaviors* (Instruction Tuning)?

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** The core architectural risk of DACP. Adapting a general model to a niche domain can erase its general capabilities. The 1:1 replay ratio is the primary defense.
  - **Quick check question:** If we trained on 100% medical data, would the model still know who the President is? (Yes/No -> Forgetting).

- **Concept: AlignScore (Factual Consistency)**
  - **Why needed here:** ROUGE scores measure n-gram overlap (syntactic similarity), but AlignScore measures semantic factuality. The paper shows DACP improves AlignScore significantly, meaning the mechanism improves *truthfulness*, not just word matching.
  - **Quick check question:** Does a high ROUGE score guarantee the summary is factually correct? (No).

## Architecture Onboarding

- **Component map:** Raw Audio -> ASR -> Raw Transcript -> DLP Anonymization -> Entropy Filter (50M -> 25M) -> Mixer (25B Domain + 25B Replay) -> LLaMA/Mistral Trainer (NTP) -> SFT (84k examples)

- **Critical path:** The **Data Selection** and **Mixture Ratio** are the two highest-leverage variables. If the data is dirty or the ratio skews too far toward domain data, the model will either fail to converge or lose general reasoning.

- **Design tradeoffs:**
  - **Ratio:** 50% Replay preserves generalization but slows domain acquisition (plasticity).
  - **Context Length:** Set to 8000 tokens. Truncates very long calls but fits in A100 memory efficiently.
  - **Model Size:** Uses <10B models (Mistral/LLaMA) for cost efficiency, accepting lower raw capability than GPT-4.

- **Failure signatures:**
  - **Hallucination:** Model invents actions not in the transcript (Detected by low AlignScore).
  - **Format Drift:** Model generates summaries in prose when bullet points are requested (Instruction Tuning failure).
  - **Privacy Leak:** Model reproduces PII (Anonymization pipeline failure).

- **First 3 experiments:**
  1. **Baseline Comparison:** Run `Base-Model + FT` vs. `DACP-Model + FT` on a held-out Support Call set. Confirm ROUGE-1 lift.
  2. **Forgetting Test:** Evaluate DACP model on a general NLP benchmark (e.g., MMLU or generic Hellaswag) to ensure the Replay Data preserved general capabilities.
  3. **Data Ablation:** Train a smaller version (e.g., 1B tokens) with *Random Selection* vs. *Entropy Selection* to validate the data curation strategy before scaling to 50B.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the efficacy of DACP scale with model parameters beyond the sub-10B ("smaller") range tested in this study?
- Basis in paper: [explicit] The Conclusion states: "In the future, we will explore the interplay between the model size and the data size in DACP-style training."
- Why unresolved: The current experiments are strictly limited to LLaMA-3.1-8B and Mistral-V0.3-7B to address production costs, leaving the scaling dynamics for larger models unknown.
- What evidence would resolve it: A comparison of DACP performance across a spectrum of model sizes (e.g., 13B, 70B) using the identical 50B token mixture.

### Open Question 2
- Question: Does DACP enhance performance on non-summarization conversational tasks (e.g., sentiment analysis, intent recognition)?
- Basis in paper: [explicit] The Conclusion notes the intent to develop "a new domain-specific benchmark with a broader task selection," acknowledging the current focus is narrow.
- Why unresolved: The paper evaluates only "Action Items" and "Support Call Summarization," leaving the transferability of the domain adaptation to other task types unverified.
- What evidence would resolve it: Evaluation of the DACP-adapted checkpoints on a diverse suite of conversational benchmarks beyond text generation/summarization.

### Open Question 3
- Question: Is the 1:1 ratio of in-domain data to replay data optimal for maximizing domain knowledge retention while mitigating catastrophic forgetting?
- Basis in paper: [inferred] The methodology fixes the ratio at 50% domain / 50% replay based on external literature (Gu et al., 2024) without ablating the ratio specifically for noisy ASR data.
- Why unresolved: Noisy transcripts may require a different mixing balance compared to the cleaner text domains (e.g., biomedicine) used in prior work to prevent overfitting to noise.
- What evidence would resolve it: Ablation studies comparing performance stability across different domain-to-replay ratios (e.g., 25%/75%, 75%/25%) on the same total token budget.

## Limitations

- **Internal Data Quality and Generality** - The paper's primary claims rely heavily on proprietary business conversation data, creating a fundamental reproducibility barrier.
- **Mixture Ratio Justification** - The 1:1 ratio lacks rigorous ablation study evidence and appears empirically derived rather than theoretically grounded.
- **External Benchmark Selection** - QMSUM benchmarks are meeting/summarization datasets, not phone conversation datasets, potentially inflating the 150.04% ROUGE-1 improvement.

## Confidence

**High Confidence** (Multiple independent evidences, clear mechanism):
- NTP-based continual pre-training adapts models to domain-specific linguistic patterns
- Entropy-based data selection improves training signal quality
- DACP + SFT pipeline produces coherent, task-relevant summaries

**Medium Confidence** (Sufficient evidence but with caveats):
- 1:1 replay ratio optimally balances domain acquisition and forgetting prevention
- Performance improvements generalize to external benchmarks
- Factual consistency (AlignScore) improvements reflect true semantic gains

**Low Confidence** (Single evidence point or significant uncertainty):
- Absolute magnitude of performance gains (particularly the 150% ROUGE-1 figure)
- Long-term generalization beyond the specific business conversation domain
- Privacy preservation claims without independent audit

## Next Checks

1. **Forgetting Prevention Validation** - Evaluate the DACP model on general language understanding benchmarks (MMLU, HellaSwag) to quantitatively measure whether the 50% replay data successfully preserves general capabilities. Compare against a 100% domain pre-training variant to demonstrate the catastrophic forgetting mitigation.

2. **Ratio Sensitivity Analysis** - Systematically vary the domain:replay mixture ratio (0:100, 25:75, 50:50, 75:25, 100:0) while keeping all other parameters constant. Measure both domain-specific performance (Action Items, Support Call) and general capability retention to identify the optimal tradeoff point empirically.

3. **Noise Robustness Test** - Generate synthetic ASR error patterns (substitutions, deletions, insertions at varying rates 5-30%) and evaluate DACP vs. baseline models on their ability to produce coherent summaries. This validates whether the model learns to handle noise patterns or simply memorizes clean transcript structures.