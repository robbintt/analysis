---
ver: rpa2
title: 'When Models Know More Than They Can Explain: Quantifying Knowledge Transfer
  in Human-AI Collaboration'
arxiv_id: '2506.05579'
source_url: https://arxiv.org/abs/2506.05579
tags:
- human
- arxiv
- knowledge
- problems
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study introduces KITE, a framework for evaluating knowledge
  transfer in human-AI collaboration through a two-phase protocol: collaborative ideation
  with an AI followed by independent problem-solving. Across 578 problem-solving sessions
  with 118 participants, the research found that while model benchmark performance
  correlates with collaborative outcomes, the relationship is inconsistent with significant
  outliers, indicating that knowledge transfer requires dedicated optimization.'
---

# When Models Know More Than They Can Explain: Quantifying Knowledge Transfer in Human-AI Collaboration

## Quick Facts
- arXiv ID: 2506.05579
- Source URL: https://arxiv.org/abs/2506.05579
- Reference count: 40
- Primary result: KITE framework reveals knowledge transfer in human-AI collaboration requires dedicated optimization beyond raw model capability

## Executive Summary
This study introduces KITE (Knowledge Transfer Evaluation), a framework for measuring how effectively AI models convey knowledge to humans during collaborative problem-solving. Through 578 problem-solving sessions with 118 participants across coding and mathematics domains, the research reveals that model benchmark performance poorly predicts collaborative success. Some models like Claude-3.7-Sonnet enabled collaborative outcomes exceeding solo capabilities, while higher-performing models like Gemini-2.5-Pro showed reduced collaborative efficacy. The findings suggest that as AI models grow more capable, their ability to convey reasoning may lag behind their raw knowledge, highlighting the need for better communicative alignment.

## Method Summary
The KITE framework employs a two-phase protocol: collaborative ideation where participants work with an AI model to solve problems, followed by independent problem-solving where participants must solve new problems without AI assistance. The evaluation measures both collaborative outcomes and knowledge transfer by comparing performance gains across 118 participants working with seven different model configurations. The study uses a within-subjects design where each participant experiences multiple conditions across coding and mathematics domains, enabling controlled comparison of model effectiveness in knowledge transfer.

## Key Results
- Model benchmark performance correlates inconsistently with collaborative outcomes, showing significant outliers
- Claude-3.7-Sonnet enabled collaborative outcomes exceeding solo capabilities, while Gemini-2.5-Pro showed reduced collaborative efficacy
- Human preferences for models varied by domain and skill hierarchy, with strategic guidance valued in coding but intuitive framing in mathematics

## Why This Works (Mechanism)
The study's mechanism centers on the gap between model knowledge and model explainability. When models possess capabilities that exceed their ability to communicate reasoning, collaborative outcomes suffer despite high raw performance. The KITE framework captures this by measuring not just what models can do, but what humans can learn from them. The inconsistent correlation between benchmark performance and collaborative efficacy suggests that knowledge transfer requires specific communicative abilities that are not captured by standard evaluation metrics.

## Foundational Learning
- Knowledge transfer vs. knowledge access: Understanding the difference between using an AI's capabilities versus learning from them is crucial for evaluating collaborative AI systems
- Communicative alignment: Models must be optimized not just for accuracy but for explainability and pedagogical effectiveness
- Domain-specific scaffolding: Different fields require different approaches to knowledge transfer, with coding benefiting from strategic guidance and mathematics from intuitive framing

Quick check: Evaluate whether a model can enable users to solve new problems independently after collaborative sessions

## Architecture Onboarding
Component map: KITE framework -> Two-phase protocol -> Collaborative ideation -> Independent problem-solving -> Knowledge transfer measurement

Critical path: The measurement of knowledge transfer through performance comparison between collaborative and independent problem-solving phases

Design tradeoffs: Balancing model capability with explainability, focusing on generalizable knowledge transfer rather than domain-specific performance

Failure signatures: Overreliance patterns where users cannot solve problems independently, representation misalignment between model and user understanding

First experiments:
1. Test KITE framework across additional domains beyond coding and mathematics
2. Compare knowledge transfer effectiveness between different model families
3. Measure long-term retention of knowledge gained through collaboration

## Open Questions the Paper Calls Out
None

## Limitations
- Sample size of 578 sessions may not capture full variability across domains and user populations
- Focus on coding and mathematics limits generalizability to other knowledge-intensive fields
- Subjective human preferences introduce potential confounding factors related to user expertise

## Confidence
- Knowledge transfer requires dedicated optimization: High
- Domain-specific preferences and qualitative patterns: Medium
- Generalizability to other domains and model architectures: Low

## Next Checks
1. Replicate the study across additional domains (e.g., scientific research, creative writing, medical diagnosis) to assess generalizability
2. Conduct controlled experiments varying user expertise levels systematically to isolate skill hierarchy effects
3. Implement longitudinal studies tracking individual users' collaborative performance over time to distinguish initial learning curves from sustainable knowledge transfer