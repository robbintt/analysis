---
ver: rpa2
title: 'Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness
  Analysis'
arxiv_id: '2511.01425'
source_url: https://arxiv.org/abs/2511.01425
tags:
- agent
- evidence
- policy
- score
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a verifiable reasoning agent for medical image
  diagnosis, addressing the challenge of trust in AI explanations. The core idea is
  an interactive agent that learns to strategically seek external visual evidence
  to support its diagnostic reasoning, modeled as a transparent loop between hypothesis
  updates and evidence validation.
---

# Learning to Seek Evidence: A Verifiable Reasoning Agent with Causal Faithfulness Analysis

## Quick Facts
- **arXiv ID**: 2511.01425
- **Source URL**: https://arxiv.org/abs/2511.01425
- **Reference count**: 9
- **Primary result**: Verifiable reasoning agent improves calibrated accuracy, reducing Brier score by 18% via evidence-seeking reinforcement learning

## Executive Summary
This paper introduces a verifiable reasoning agent for medical image diagnosis that strategically seeks external visual evidence to support its diagnostic reasoning. The agent operates in an interactive loop, updating hypotheses and validating evidence to produce transparent, trustable explanations. Using reinforcement learning to optimize its evidence-seeking policy, the agent demonstrates significantly improved calibrated accuracy over non-interactive baselines. A novel causal intervention method validates the faithfulness of the agent's explanations by showing that masking chosen evidence degrades performance, confirming the evidence's integral role in decision-making. This work provides a practical framework for building AI systems with verifiable and faithful reasoning capabilities.

## Method Summary
The approach introduces a reasoning agent that interacts with a large vision-language model (VLM) to diagnose medical images by strategically seeking external visual evidence. The agent operates in a transparent loop where it updates hypotheses based on gathered evidence and validates new evidence against current hypotheses. The evidence-seeking policy is optimized through reinforcement learning, enabling efficient and generalizable performance across medical imaging tasks. A key innovation is the causal faithfulness analysis, which validates that the agent's chosen evidence genuinely influences its decisions by demonstrating performance degradation when evidence is masked. This framework addresses the fundamental challenge of trust in AI explanations by making the reasoning process both verifiable and faithful to the underlying decision-making.

## Key Results
- Action-based reasoning process improves calibrated accuracy over non-interactive baseline
- 18% reduction in Brier score achieved through strategic evidence seeking
- Causal intervention confirms faithfulness: masking chosen evidence degrades performance

## Why This Works (Mechanism)
The agent's effectiveness stems from its interactive reasoning loop that mirrors human diagnostic processes. By strategically seeking external evidence rather than relying solely on initial observations, the agent can correct initial hypotheses and build more robust diagnostic conclusions. The reinforcement learning framework allows the agent to learn optimal evidence-seeking strategies that balance exploration costs against diagnostic accuracy gains. The causal faithfulness analysis provides a rigorous validation method that distinguishes genuine explanatory evidence from superficial correlations, ensuring the agent's reasoning can be trusted in high-stakes medical applications.

## Foundational Learning
- **Reinforcement Learning for Sequential Decision Making**: Needed to optimize the agent's evidence-seeking policy over multiple steps; quick check: verify policy gradient updates improve evidence selection quality
- **Causal Intervention Analysis**: Required to validate faithfulness of explanations by measuring performance degradation when evidence is masked; quick check: confirm masking strategy actually degrades accuracy
- **Vision-Language Model Integration**: Essential for processing both visual evidence and textual hypotheses in unified reasoning loop; quick check: ensure VLM can effectively integrate heterogeneous evidence types
- **Medical Image Diagnosis Task Formulation**: Provides concrete evaluation framework for verifiable reasoning; quick check: validate diagnostic accuracy improvements are statistically significant

## Architecture Onboarding
- **Component Map**: VLM <-[hypotheses]-> Agent <-[evidence]-> External Image Sources
- **Critical Path**: Initial hypothesis generation → Evidence selection policy → Evidence retrieval → Hypothesis update → Final diagnosis
- **Design Tradeoffs**: Interactive reasoning vs. computational cost; evidence quantity vs. diagnostic accuracy; exploration vs. exploitation in evidence seeking
- **Failure Signatures**: Over-reliance on initial hypotheses; selection of irrelevant evidence; failure to update hypotheses despite contradictory evidence
- **First 3 Experiments**: 1) Compare calibrated accuracy with non-interactive baseline, 2) Measure Brier score improvement across diagnostic categories, 3) Validate causal faithfulness through evidence masking interventions

## Open Questions the Paper Calls Out
None provided in source material.

## Limitations
- Performance depends on quality and availability of external visual evidence sources
- Reinforcement learning approach requires careful reward shaping for medical applications
- Causal faithfulness analysis assumes masking intervention is valid perturbation

## Confidence
- **Causal faithfulness validation method**: High
- **Reinforcement learning policy optimization**: High
- **Medical image diagnosis performance claims**: Medium (dependent on dataset and task specifics)

## Next Checks
1. Replicate Brier score improvement across different medical imaging datasets
2. Test causal faithfulness analysis with multiple masking strategies
3. Evaluate computational efficiency of interactive reasoning loop compared to baseline approaches