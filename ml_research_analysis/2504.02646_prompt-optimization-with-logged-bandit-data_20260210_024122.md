---
ver: rpa2
title: Prompt Optimization with Logged Bandit Data
arxiv_id: '2504.02646'
source_url: https://arxiv.org/abs/2504.02646
tags:
- policy
- sentence
- reward
- prompt
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of optimizing large language model
  pipelines for personalized sentence generation using prompts, leveraging naturally
  available user feedback like clicks. Traditional methods struggle with high variance
  due to large action spaces or bias from inaccurate reward predictions.
---

# Prompt Optimization with Logged Bandit Data

## Quick Facts
- arXiv ID: 2504.02646
- Source URL: https://arxiv.org/abs/2504.02646
- Reference count: 40
- Primary result: Introduces DSO algorithm achieving up to 5x improvement in policy value for prompt optimization

## Executive Summary
This paper addresses the challenge of optimizing large language model pipelines for personalized sentence generation using prompts, leveraging naturally available user feedback like clicks. Traditional off-policy learning methods struggle with high variance due to large action spaces or bias from inaccurate reward predictions. The proposed solution, Direct Sentence Off-policy Gradient (DSO), estimates the policy gradient in the sentence space rather than the prompt space, using kernel-based similarity to reduce variance and suppress bias. Empirical results on a newly established benchmark suite show that DSO significantly outperforms baselines, particularly when the number of candidate prompts is large.

## Method Summary
DSO optimizes a discrete prompt policy for personalized sentence generation (movie descriptions) using logged bandit feedback (clicks) without fine-tuning the LLM. The method uses Mistral-7B to generate descriptions and DistilBert (fine-tuned on MovieLens) as the reward simulator. Sentence embeddings are extracted using Mistral-7B and reduced to 20 dimensions via PCA. The marginal density model f_ψ (2-layer NN, Adam, lr=1e-4) estimates π₀(φ(s)|x) once before policy optimization. The policy π_θ (2-layer NN, Adagrad, lr=8e-4) is optimized using the DSO gradient estimator with weight clipping (max 200) for stability. The method uses a Gaussian kernel (τ=1.0) rather than the prompt space to reduce variance.

## Key Results
- DSO achieves up to 5x improvement in policy value over baselines
- Performance improves as the number of candidate prompts increases
- Outperforms standard importance sampling, regression, doubly robust, and POTEC methods
- Particularly effective in large action space settings (1000+ prompts)

## Why This Works (Mechanism)

### Mechanism 1: Sentence-Space Gradient Estimation
Estimating policy gradients in the marginalized sentence space rather than the discrete prompt space reduces variance while controlling bias. Instead of computing importance weights as πθ(a|x)/π0(a|x) over potentially thousands of prompts, DSO computes πθ(ϕ(s)|x)/π0(ϕ(s)|x) over sentence neighborhoods, where multiple prompts can map to similar sentences. This aggregates information across prompts that generate similar outputs.

### Mechanism 2: Kernel-Based Implicit Data Augmentation
Applying smooth kernels (e.g., Gaussian) to sentence embeddings creates soft neighborhoods that distribute reward signal across similar outputs, effectively augmenting sparse logged data. For each observed (si, ri), DSO weights neighboring sentences s' by K(s', s; x, τ) ∝ exp(-d(s', s)), sharing the reward gradient proportionally.

### Mechanism 3: Logging Policy Marginal Density Estimation
Pre-training a function approximator for π0(ϕ(s)|x) once, rather than Monte Carlo estimation, stabilizes DSO and reduces sensitivity to bandwidth hyperparameter τ. The marginal density model fψ(x, s) is trained to minimize ℓ(fψ) ≈ (1/n)Σ[Es,s'~π0(fψ(x, s) − K(s, s'; x, τ))²].

## Foundational Learning

- Concept: Off-Policy Learning (OPL) with Bandit Feedback
  - Why needed here: The core problem setting—you have logged data from a deployed policy (only observed rewards for chosen prompts) and want to learn a better policy without online exploration.
  - Quick check question: Can you explain why naive supervised learning on logged (prompt, reward) pairs fails when the new policy selects prompts the logging policy rarely chose?

- Concept: Importance Sampling (IS) Bias-Variance Tradeoff
  - Why needed here: DSO's innovation is fundamentally an IS technique modified for sentence space; understanding why standard IS fails (high variance from large action spaces, bias from support deficiency) is prerequisite.
  - Quick check question: Given a logging policy π0 and target policy πθ, what happens to IS estimator variance when |A| = 10,000 prompts and π0 has narrow support?

- Concept: Kernel Density Estimation and Bandwidth Selection
  - Why needed here: DSO requires choosing τ (kernel bandwidth) that trades off bias (small τ = less neighborhood mixing) vs. variance (large τ = more averaging). Theoretical analysis in Theorems 1-2 formalizes this.
  - Quick check question: In a sentence embedding space with dimension d=768, what happens to kernel density estimates if τ is set too small relative to inter-sentence distances?

## Architecture Onboarding

- Component map:
  Dataset Module → ContextQueryLoader + CandidateActionsLoader + FrozenLLM + RewardSimulator → logged bandit data
  Marginal Density Model → Neural network fψ(x, s) trained once before policy optimization
  Policy Network → Two-layer MLP parameterizing πθ(a|x) with softmax over prompt logits
  Sentence Encoder → Mistral-7B hidden states → PCA → 20-dim embeddings
  Kernel Function → Gaussian K(s, s'; x, τ) or Uniform with configurable τ
  Policy Learner → KernelPolicyLearner class implementing DSO gradient updates

- Critical path:
  1. Encode all candidate prompts and pre-compute sentence embeddings for representative queries
  2. Train marginal density model fψ on logged data using kernel-matching loss
  3. Initialize policy network πθ
  4. For each gradient step: sample (xi, ai, si, ri) from logged data, compute w(ϕ(si), xi) using fψ, compute ∇θ via soft rejection sampling over (a, s') ~ πθ(a|x)pLLM(s'|x,a), update θ

- Design tradeoffs:
  - Gaussian vs. Uniform kernel: Gaussian provides smoother gradients but requires careful τ tuning; Uniform is more robust but discards data outside threshold
  - Function approximation vs. Monte Carlo for π0(ϕ(s)|x): Function approximation is more stable but requires pre-training; MC is simpler but noisy for small τ
  - Sentence embedding source: Off-the-shelf (e.g., Mistral-7B) works reasonably (paper shows) but domain-specific finetuned embeddings may improve performance

- Failure signatures:
  - Gradient explosion with near-zero π0(ϕ(s)|x): Indicates density model underestimates support; switch to function approximation or increase τ
  - No improvement over uniform policy: Check if logging policy had sufficient exploration; if logging policy is near-deterministic, DSO cannot infer counterfactual rewards
  - Performance degrades as |A| increases unexpectedly: Kernel bandwidth τ may be too small; sentence embeddings may not capture prompt similarity structure
  - DSO matches IS performance rather than exceeding: Sentence space may not provide meaningful similarity (each prompt → unique sentence cluster)

- First 3 experiments:
  1. Synthetic benchmark with varying |A| (10, 50, 100, 500, 1000): Verify DSO maintains performance as action space grows while baselines degrade; plot optimality ratio vs. |A|
  2. Ablation on kernel bandwidth τ (0.5, 1.0, 2.0, 4.0): Demonstrate bias-variance tradeoff; expect small τ → lower bias but potential instability, large τ → smoother but biased
  3. Full-LLM movie description task with n=50,000 logged samples: Compare DSO vs. IS, regression, DR, POTEC on real Mistral-7B generations; measure 5x improvement claim reproduction

## Open Questions the Paper Calls Out
None

## Limitations
- The core DSO mechanism relies on sentence embeddings capturing reward-relevant similarity, but the paper provides limited validation that this assumption holds across domains
- The kernel bandwidth τ requires careful tuning, and the paper doesn't fully explore sensitivity to this hyperparameter
- The OfflinePrompts benchmark, while valuable, represents a single domain (movie descriptions), limiting generalizability claims

## Confidence
- High confidence: DSO reduces variance through sentence-space gradient estimation and improves over uniform policy baselines in the established benchmark
- Medium confidence: The 5x improvement claim is based on specific synthetic and movie description settings; real-world generalization may vary
- Low confidence: Claims about broad applicability to arbitrary personalized generation tasks without further domain-specific validation

## Next Checks
1. Cross-domain transferability: Test DSO on a non-movie domain (e.g., product descriptions or news headlines) to verify the sentence similarity assumption holds beyond the established benchmark
2. Kernel bandwidth sensitivity: Systematically vary τ across multiple orders of magnitude to map the bias-variance tradeoff curve and identify robust operating ranges
3. Embedding quality validation: Quantitatively measure whether sentence embeddings from different models (e.g., RoBERTa vs. Mistral) correlate with reward similarity to validate the core similarity assumption