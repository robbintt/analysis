---
ver: rpa2
title: 'From Metaphor to Mechanism: How LLMs Decode Traditional Chinese Medicine Symbolic
  Language for Modern Clinical Relevance'
arxiv_id: '2503.02760'
source_url: https://arxiv.org/abs/2503.02760
tags:
- medicine
- chinese
- metaphors
- traditional
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a multi-agent and chain-of-thought (CoT)\
  \ framework to interpret Traditional Chinese Medicine (TCM) metaphors and map them\
  \ to Western medical (WM) concepts. The core method uses specialized agents\u2014\
  a TCM Expert, WM Expert, and Coordinator\u2014each contributing domain-specific\
  \ reasoning, with CoT prompting to ensure transparency and reduce errors."
---

# From Metaphor to Mechanism: How LLMs Decode Traditional Chinese Medicine Symbolic Language for Modern Clinical Relevance

## Quick Facts
- arXiv ID: 2503.02760
- Source URL: https://arxiv.org/abs/2503.02760
- Reference count: 34
- Primary result: Multi-agent CoT framework achieves 82.41% accuracy and 81.68% F1 in mapping TCM metaphors to WM concepts

## Executive Summary
This paper addresses the challenge of translating Traditional Chinese Medicine's metaphorical language into Western medical terminology using large language models. The authors develop a multi-agent framework combining specialized TCM and WM experts with a coordinator agent, enhanced by chain-of-thought reasoning to improve interpretability and accuracy. Experiments demonstrate that this approach significantly outperforms baseline methods in mapping 2,801 TCM-WM sentence pairs, with CoT prompting providing substantial accuracy gains across multiple model architectures.

## Method Summary
The authors propose a multi-agent system where specialized agents (TCM Expert, WM Expert, and Coordinator) collaborate to interpret TCM metaphors and map them to Western medical concepts. Each agent contributes domain-specific knowledge through iterative reasoning, with chain-of-thought prompting enabling transparent decision-making. The framework processes TCM symptom descriptions and identifies corresponding Western medical conditions through structured agent communication and knowledge integration.

## Key Results
- Chain-of-thought prompting improves Qwen-7B-Chat accuracy from 72.36% to 82.41% and F1 from 71.20% to 81.68%
- Multi-agent architecture consistently outperforms single-agent approaches across all tested models
- The framework successfully bridges TCM's symbolic language with WM pathophysiology in 2,801 sentence pairs

## Why This Works (Mechanism)
The multi-agent architecture enables specialized domain reasoning while chain-of-thought prompting provides transparent decision pathways. TCM metaphors often describe complex pathophysiological states through symbolic language that requires both domain expertise and structured reasoning to decode. By separating expertise into dedicated agents and requiring explicit reasoning steps, the system reduces the cognitive load on any single model component and makes error sources traceable.

## Foundational Learning
- **Symbolic language interpretation**: TCM uses metaphors like "liver wind" that require cultural and medical context to decode - needed to bridge conceptual gaps between systems, quick check: can model map "liver wind" to neurological tremors
- **Multi-agent collaboration**: Specialized agents can leverage complementary knowledge bases - needed for cross-system reasoning, quick check: do TCM and WM agents agree on common concepts
- **Chain-of-thought prompting**: Explicit reasoning steps improve accuracy and interpretability - needed for clinical trust, quick check: can intermediate reasoning steps be validated by experts

## Architecture Onboarding
**Component Map**: Input -> TCM Expert -> WM Expert -> Coordinator -> Output
**Critical Path**: Input sentence → TCM Expert analysis → WM Expert cross-reference → Coordinator validation → Final mapping
**Design Tradeoffs**: Multi-agent improves accuracy but increases latency; CoT enhances interpretability but requires more computation
**Failure Signatures**: Coordinator misdirection, agent misinterpretation of partner outputs, systematic bias in mapping patterns
**First Experiments**: 1) Test single-agent baseline performance, 2) Evaluate coordinator-only control, 3) Compare different CoT prompt structures

## Open Questions the Paper Calls Out
None

## Limitations
- Limited dataset size (2,801 pairs) may not capture full TCM-WM mapping complexity
- Evaluation focuses on sentence-level classification rather than full clinical reasoning pipelines
- No physician validation of clinical relevance of generated mappings

## Confidence
- **Multi-agent CoT effectiveness**: Medium-High (consistent gains across multiple model architectures)
- **Clinical relevance**: Medium (lacks physician validation and real-world diagnostic testing)
- **Generalization**: Low-Medium (small dataset, domain-specific evaluation)

## Next Checks
1. Clinical Expert Validation: Engage TCM and WM practitioners to validate model's top-5 confidence mappings for clinical plausibility
2. Temporal Generalization Test: Evaluate performance on TCM-WM pairs from different historical periods to assess conceptual framework capture
3. Error Analysis on Borderline Cases: Systematically analyze cases where models disagree with ground truth to identify ambiguity sources