---
ver: rpa2
title: Scalable In-Context Learning on Tabular Data via Retrieval-Augmented Large
  Language Models
arxiv_id: '2502.03147'
source_url: https://arxiv.org/abs/2502.03147
tags:
- data
- tabular
- datasets
- learning
- tabicl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extending in-context learning
  (ICL) for tabular data to large-scale datasets, as current LLM-based approaches
  are limited by context length constraints. The authors propose a retrieval-augmented
  generation (RAG) framework for tabular data, combining a universal non-parametric
  retrieval module with retrieval-guided instruction-tuning of LLMs.
---

# Scalable In-Context Learning on Tabular Data via Retrieval-Augmented Large Language Models

## Quick Facts
- **arXiv ID**: 2502.03147
- **Source URL**: https://arxiv.org/abs/2502.03147
- **Authors**: Xumeng Wen; Shun Zheng; Zhen Xu; Yiming Sun; Jiang Bian
- **Reference count**: 40
- **Primary result**: Proposed retrieval-augmented framework achieves median performance following power-law scaling with training data (α ≈ 0.102 for classification, α ≈ 0.053 for regression) across 69 benchmark datasets.

## Executive Summary
This paper addresses the challenge of extending in-context learning for tabular data to large-scale datasets, where current LLM-based approaches are limited by context length constraints. The authors propose a retrieval-augmented generation (RAG) framework that combines a universal non-parametric retrieval module with retrieval-guided instruction-tuning of LLMs. Their method enables LLMs to effectively leverage larger datasets, achieving median performance that follows a power-law scaling relationship with training data size. Across 69 benchmark datasets, the approach significantly outperforms ablated variants and ranks among top-performing models, though it still lags behind well-tuned numeric models in overall performance.

## Method Summary
The paper proposes TabRAG, a retrieval-augmented framework for scalable in-context learning on tabular data. The method consists of two main components: a universal non-parametric retrieval module and retrieval-guided instruction-tuning of LLMs. The retrieval module uses weighted feature-wise similarity (absolute difference for numerical features after quantile normalization, categorical distance as 0 or 1) with predictive power score (PPS) and Pearson correlation for weighting. The LLM (Phi-3-Medium-128k) is post-trained on 319 real-world datasets. During inference, the model retrieves relevant instances and uses them as in-context examples for prediction, enabling effective learning from large datasets beyond context length limitations.

## Key Results
- The approach achieves median performance following power-law scaling with training data size (α ≈ 0.102 for classification, α ≈ 0.053 for regression)
- Across 69 benchmark datasets, TabRAG significantly outperforms ablated variants that remove retrieval or retrieval-guided training
- The method ranks among top-performing models but still lags behind well-tuned numeric models like TabPFN
- Retrieval-guided training is crucial for aligning LLMs with retrieval patterns, with significant performance drops when using random retrieval

## Why This Works (Mechanism)
The paper demonstrates that retrieval-augmented generation enables LLMs to effectively learn from large tabular datasets by retrieving relevant in-context examples during inference. The universal non-parametric retrieval module uses weighted feature similarity to identify relevant training instances, while retrieval-guided instruction-tuning aligns the LLM with the retrieval patterns. This approach overcomes the context length limitations of traditional in-context learning and allows the model to leverage information from datasets much larger than its context window.

## Foundational Learning
- **In-Context Learning (ICL)**: LLMs' ability to learn from examples provided in the prompt rather than through gradient updates. Needed because it enables few-shot learning without fine-tuning.
- **Quantile Normalization**: Transforming numerical features to follow a uniform distribution. Quick check: Verify that normalized values fall between 0 and 1 with approximately uniform distribution.
- **Predictive Power Score (PPS)**: A feature importance metric that measures how well one variable predicts another. Needed to weight features by their predictive value during retrieval.
- **Retrieval-Augmented Generation (RAG)**: Framework combining retrieval of relevant documents with generation. Quick check: Confirm retrieved instances are semantically similar to the query instance.
- **Power-Law Scaling**: Performance improvement following a power-law relationship with training data size. Quick check: Plot performance vs. training data on log-log scale to verify linear relationship.

## Architecture Onboarding

**Component Map**: TabRAG (Retrieval Module) -> Phi-3-Medium-128k (LLM) -> In-Context Prediction

**Critical Path**: Data preprocessing → Quantile normalization → Feature importance computation (PPS + Pearson) → Instance retrieval using weighted L2 distance → Prompt construction → LLM inference → Output generation

**Design Tradeoffs**: The universal non-parametric retrieval policy prioritizes generality over domain-specific optimization, trading potential performance gains from specialized retrieval strategies for broader applicability across diverse tabular datasets.

**Failure Signatures**: 
- Poor retrieval performance on datasets with discrete or clumped numerical values due to quantile normalization artifacts
- Suboptimal retrieval on datasets where feature interactions dominate rather than individual feature importance
- Tokenization issues with certain LLM architectures that split numbers into multiple tokens

**First Experiments**:
1. Implement and validate the TabRAG retrieval module against baseline metrics for a subset of 5-10 datasets
2. Compare quantile normalization performance against standard/min-max normalization on discrete-valued numerical features
3. Evaluate retrieval performance using alternative feature importance methods (tree-based, SHAP) versus current approach on interaction-heavy datasets

## Open Questions the Paper Calls Out
- **Open Question 1**: Can "retrieval engineering" involving domain knowledge systematically outperform the universal non-parametric retrieval policy? The current study uses a universal policy for fair benchmarking; implementing domain-specific retrieval requires external knowledge not present in raw datasets.
- **Open Question 2**: Can generating large-scale synthetic tabular data for post-training enhance LLM-based TabICL capabilities similar to numeric-based models like TabPFN? The authors hypothesize performance gaps stem from TabPFN's exposure to diverse synthetic priors versus their real-world dataset approach.
- **Open Question 3**: How can LLMs be encouraged to learn smoother decision boundaries while retaining their unique generalization capabilities? Current training objectives result in high-uncertainty, non-smooth boundaries that differ significantly from classic numeric models.

## Limitations
- The exact anonymized prompt template and complete list of 319 training datasets are not provided, preventing full reproduction
- The feature importance computation based on single-feature correlation metrics may miss important feature interactions
- The power-law scaling relationship is derived from a specific set of 69 datasets and may not generalize to all tabular data distributions

## Confidence
- **High confidence**: The retrieval-augmented framework concept and core components are clearly specified and reproducible; power-law scaling relationship is empirically supported
- **Medium confidence**: Overall performance ranking among top models is demonstrated, but exact numerical results cannot be fully verified without complete training dataset list and prompt template
- **Low confidence**: The claim that "language as a universal interface" for tabular data is fully validated, as the study is limited to specific benchmark datasets

## Next Checks
1. Reproduce retrieval logic independently by implementing the TabRAG retrieval module and validating against provided baseline metrics for a subset of 5-10 datasets
2. Test normalization robustness by comparing quantile normalization performance against standard/min-max normalization on discrete-valued numerical features
3. Benchmark feature importance methods by evaluating retrieval performance using alternative approaches (tree-based, SHAP) versus current method on interaction-heavy datasets