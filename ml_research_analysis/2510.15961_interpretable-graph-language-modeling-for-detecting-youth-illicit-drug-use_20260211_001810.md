---
ver: rpa2
title: Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use
arxiv_id: '2510.15961'
source_url: https://arxiv.org/abs/2510.15961
tags:
- question
- graph
- user
- drug
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LAMI, a joint graph-language framework for
  detecting youth illicit drug use from survey data. It constructs individual-level
  relational graphs from survey responses, learns latent relations among questions
  via graph structure learning, and couples GNNs with LLMs to generate interpretable
  explanations.
---

# Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use

## Quick Facts
- arXiv ID: 2510.15961
- Source URL: https://arxiv.org/abs/2510.15961
- Reference count: 27
- LAMI achieves 82.21% accuracy on YRBS and 81.47% on NSDUH for detecting youth illicit drug use

## Executive Summary
LAMI is a joint graph-language framework that detects youth illicit drug use from survey data by constructing individual-level relational graphs and coupling graph neural networks with a frozen LLM for interpretable explanations. The model learns latent question-question dependencies through a specialized graph structure learning layer and uses these learned relations to enhance prediction accuracy while providing natural language rationales. Evaluated on YRBS and NSDUH, LAMI significantly outperforms competitive baselines including zero-shot and fine-tuned LLMs, achieving state-of-the-art performance while identifying key psychosocial and behavioral risk factors that align with established drug use pathways.

## Method Summary
LAMI represents each survey respondent as a relational graph with user, question, and topic nodes connected by edges encoding answer values. The model employs a two-stage approach: first, RGCN layers with a Relational Graph Structure Learning (RGSL) component learn latent cross-question dependencies through a self-supervised edge-type prediction task; second, an attention-based aggregator selects top-k predictive questions for textualization, and a projection head maps the graph embedding to a frozen LLM's embedding space as a soft prefix token. The LLM generates both the binary prediction and an interpretable natural language explanation. The model is trained on YRBS 2023 and NSDUH 2023 data using dual generation and classification losses, with rigorous exclusion of direct drug indicators to prevent information leakage.

## Key Results
- LAMI achieves 82.21% accuracy on YRBS and 81.47% on NSDUH, outperforming competitive baselines
- Ablation studies confirm each component (RGSL, relation conditioning, LLM prefix) contributes to performance gains
- Interpretability analysis reveals LAMI identifies key risk factors including early substance use, family dynamics, and mental health
- Self-supervised edge prediction accuracy validates the learned graph structure captures meaningful dependencies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Representing survey responses as relational graphs enables discovery of latent cross-question dependencies that tabular models miss.
- **Mechanism:** Each respondent becomes a graph with a user node connected to question nodes via answer-encoded edges. The RGSL layer learns an adjacency matrix by computing pairwise similarity between projected question embeddings, selecting top-k neighbors, and injecting user-question relation context into cross-question message passing. A self-supervised edge-type prediction task forces the model to infer masked user-question edges from newly learned question-question connections.
- **Core assumption:** Survey questions from different topics share latent psychosocial dependencies that predict drug use when modeled jointly.
- **Evidence anchors:**
  - [abstract] "LAMI represents individual responses as relational graphs, learns latent connections through a specialized graph structure learning layer"
  - [Section 3.2] "For each graph, we randomly mask one edge (u, q)... the model must rely on the RGSL-induced question–question edges to route information to the masked target"
  - [corpus] Weak direct evidence; related work (GraphT5, MolTextNet) shows graph-language modeling benefits in molecular domains but not surveys.
- **Break condition:** If latent relations are noise or survey design already captures all dependencies via topic groupings, RGSL adds no signal.

### Mechanism 2
- **Claim:** Conditioning cross-question relations on user-question edge types enables context-dependent structure learning.
- **Mechanism:** The RGSL layer computes relation vectors r_ij that concatenate the target question embedding with the user-question relation matrix W_r, transforming via learned weights. This injects how the user answered question i into how question i relates to question j. Message passing aggregates only along learned edges weighted by these relation vectors.
- **Core assumption:** A user's answer to one question changes the semantic relationship between that question and others.
- **Evidence anchors:**
  - [Section 3.2] "Injecting W_r into r_ij conditions the cross-question relation on the user–question relation context"
  - [Section 4.3] "Incorporating relation matrices leads to faster convergence and higher edge-type prediction accuracy"
  - [corpus] No direct corpus evidence for this specific context-conditioning mechanism.
- **Break condition:** If user answers don't meaningfully alter inter-question semantics, relation matrices add parameters without benefit.

### Mechanism 3
- **Claim:** Projecting graph embeddings as soft prefix tokens into a frozen LLM aligns structural and semantic reasoning for interpretable prediction.
- **Mechanism:** Attention scores over question nodes identify top-k contributors. These are textualized with question text and user answers. The aggregated user embedding is projected via P_proj into LLM embedding space and prepended to text embeddings. The LLM generates a binary prediction token and natural language rationale. Training uses combined generation loss (on Yes/No token) and classification loss (from graph classifier).
- **Core assumption:** Graph embeddings capture predictive structure that text alone misses, and LLM can verbalize this when conditioned on graph tokens.
- **Evidence anchors:**
  - [Section 3.3] "The concatenation [z_u; Z] forms the full prompt, with z_u acting as a soft graph-conditioned prefix"
  - [Table 2] LAMI outperforms Qwen3-8B zero-shot (65.08% vs 82.21% on YRBS) and LoRA (82.20% vs 82.21%), suggesting graph structure adds value
  - [corpus] LLM-Empowered Class Imbalanced Graph Prompt Learning paper shows similar GNN-LLM integration for drug detection, providing indirect support.
- **Break condition:** If graph embeddings don't encode information beyond what textualized questions provide, prefix tokens add noise.

## Foundational Learning

- **Concept: Relational Graph Convolutional Networks (RGCN)**
  - **Why needed here:** Survey data has typed edges (different answer values encode different relations); RGCN learns relation-specific weight matrices for message passing.
  - **Quick check question:** Can you explain why standard GCN would fail if edge types (e.g., "answered Yes" vs "answered No") carry distinct semantic meaning?

- **Concept: Graph Structure Learning (GSL)**
  - **Why needed here:** The paper's core claim is that latent dependencies exist but are not explicit; GSL learns adjacency during training rather than relying on fixed topology.
  - **Quick check question:** What would happen to RGSL if all questions were already connected by topic edges—would learned edges still provide signal?

- **Concept: Prefix Tuning / Soft Prompts**
  - **Why needed here:** The LLM is frozen; graph information enters via learned prefix tokens rather than weight updates, requiring projection head P_proj to align embedding spaces.
  - **Quick check question:** If P_proj outputs random vectors, would the LLM still generate plausible explanations? Why or why not?

## Architecture Onboarding

- **Component map:** Graph Constructor -> RGCN Layers -> RGSL Layer -> Self-Supervised Pretext -> Attention Aggregator -> Projection Head P_proj -> Frozen LLM
- **Critical path:**
  1. Run RGCN → RGSL → RGCN to get updated node embeddings
  2. Aggregate user embedding via attention over question neighbors
  3. Project to LLM space; concatenate with text embeddings
  4. Compute L_gen (on Yes/No token) and L_cls (from graph classifier); backprop to ϕ only (LLM frozen)
- **Design tradeoffs:**
  - k_sim: Too few learned edges miss latent relations; too many add noise. Paper finds k=5 optimal.
  - k_att: Controls how many questions go to LLM. More context helps up to ~20, then plateaus.
  - LLM size: Paper uses 0.6B for compute efficiency; larger models may improve explanation quality but risk overfitting.
  - Frozen vs. fine-tuned LLM: Frozen reduces compute but limits adaptation to domain jargon.
- **Failure signatures:**
  - Edge prediction accuracy doesn't improve during pretext task → RGSL not learning useful structure
  - Attention concentrates on trivial questions (e.g., demographics) → check if question embeddings are informative
  - LLM generates plausible but incorrect explanations → faithfulness gap; graph tokens may not ground reasoning
  - Performance degrades when adding RGSL → k_sim too high or relation matrices add noise
- **First 3 experiments:**
  1. **Ablate RGSL:** Replace with fixed fully-connected or topic-based adjacency; compare edge prediction accuracy and downstream F1 to validate latent structure learning contributes uniquely.
  2. **Vary k_att:** Run with k_att ∈ {5, 10, 20, 40} on validation set; plot accuracy and inspect which questions get selected to understand what signal the model relies on.
  3. **Probe prefix token information:** Train linear probe on z_u to predict ground-truth label; compare to full LAMI accuracy to assess how much predictive signal survives projection.

## Open Questions the Paper Calls Out

- **Faithfulness gap:** Do the LLM-generated natural language explanations faithfully reflect the GNN's actual decision pathways? The authors note that "free-form explanations may not always be fully faithful to the decision process," and quantitative fidelity metrics are needed.
- **Temporal robustness:** Is LAMI robust to temporal distribution shifts and evolving survey instruments? Experiments focus only on 2023 releases, and validating across multiple years is necessary.
- **Topic induction:** Can data-driven topic induction outperform the current reliance on expert-defined survey taxonomies? The authors acknowledge that using survey-defined topic groupings "may bias which cross-item relations are discoverable."

## Limitations
- **Faithfulness uncertainty:** The model generates plausible explanations, but without quantitative faithfulness evaluation, it's unclear if they accurately reflect the decision process or constitute plausible rationalization.
- **Temporal scope:** Results are validated only on 2023 survey data, limiting conclusions about robustness to temporal shifts and evolving patterns.
- **Component interdependence:** While ablations show individual contributions, interactions between RGSL, relation conditioning, and LLM prefix components remain unexplored.

## Confidence

- **High confidence:** The overall architecture is technically sound and reported performance metrics (82.21% accuracy on YRBS, 81.47% on NSDUH) are reproducible with specified components.
- **Medium confidence:** The claim that latent structure learning improves performance over fixed structures is plausible given ablation results, but mechanism validation is incomplete.
- **Low confidence:** Interpretability claims are weakest—generating explanations aligned with known risk factors is encouraging but unproven faithful without quantitative evaluation.

## Next Checks

1. **Faithfulness evaluation:** Implement automated faithfulness metrics (input perturbation tests, attention visualization) to verify generated explanations accurately reflect the model's reasoning path.
2. **Structure learning ablation:** Replace RGSL with fixed topic-based adjacency and compare performance to determine if learned edges provide unique signal beyond topic structure.
3. **Cross-survey generalization:** Test LAMI on a third survey dataset (e.g., Monitoring the Future or international YRBS) with different question wording and cultural context to evaluate transfer and explanation consistency.