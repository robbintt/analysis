---
ver: rpa2
title: 'Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural
  Rhetorical Approach'
arxiv_id: '2505.09576'
source_url: https://arxiv.org/abs/2505.09576
tags:
- human
- language
- rlhf
- llms
- rhetoric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a rhetorical analysis of Reinforcement Learning
  from Human Feedback (RLHF), a training technique used to enhance large language
  models (LLMs) like ChatGPT and Claude. Using Ian Bogost's concept of procedural
  rhetoric, the authors shift focus from content analysis to examining the persuasive
  mechanisms embedded in RLHF-enhanced LLMs.
---

# Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach

## Quick Facts
- arXiv ID: 2505.09576
- Source URL: https://arxiv.org/abs/2505.09576
- Reference count: 40
- Key outcome: RLHF procedures embed persuasive mechanisms that reinforce hegemonic language, perpetuate biases, and impact user trust and interpersonal relations

## Executive Summary
This paper presents a rhetorical analysis of Reinforcement Learning from Human Feedback (RLHF), a training technique used to enhance large language models (LLMs) like ChatGPT and Claude. Using Ian Bogost's concept of procedural rhetoric, the authors shift focus from content analysis to examining the persuasive mechanisms embedded in RLHF-enhanced LLMs. They analyze three key procedures: determining language conventions, information seeking practices, and relationship formation. The analysis reveals how these procedures reinforce hegemonic language use, perpetuate biases, decontextualize learning, and impact interpersonal relations. The paper highlights ethical concerns around transparency, trust, and the hidden assumptions in AI-generated text, while calling for greater attention to the social and cultural implications of RLHF-enhanced AI technologies.

## Method Summary
This is a theoretical rhetorical analysis using Ian Bogost's procedural rhetoric framework to examine how RLHF training procedures embed persuasive arguments in LLM outputs. The method involves a literature review of RLHF mechanics, annotator demographics, and three procedural domains (language conventions, information seeking, relationship formation). The analysis traces how human annotator feedback through reward modeling creates outputs that appear neutral but reflect embedded values and biases. No quantitative experiments or ML reproduction is attempted.

## Key Results
- RLHF procedures embed annotator values into LLM outputs through reward modeling, creating persuasive "arguments" that appear authorless and neutral
- The training process creates a recursive loop where humans train LLMs and are subsequently trained by them, reinforcing hegemonic language conventions
- Conversational search interfaces shift information-seeking from active evaluation to passive receipt, increasing persuasion through natural language formatting

## Why This Works (Mechanism)

### Mechanism 1: Procedural Embedding of Values via Reward Modeling
- Claim: RLHF embeds annotator values into LLM outputs through a reward model that generalizes human judgments, creating persuasive "arguments" that appear authorless and neutral.
- Mechanism: Human annotators evaluate outputs using "Helpful," "Honest," and "Harmless" criteria → their feedback trains a reward model → the reward model scales judgments to millions of outputs → final outputs reflect annotator preferences without visible authorship.
- Core assumption: Annotator preferences are not universal but appear neutral in outputs due to the black-boxed nature of training.
- Evidence anchors:
  - [abstract] "persuasive mechanisms embedded in RLHF-enhanced LLMs"
  - [section] "Annotators inherently bring their own biases into their judgment of model responses, which may lead to situations where model outputs 'cater primarily to the preferences of a minority, lacking varied representation across cultures, races, or languages'" (citing Kirk et al.)
  - [corpus] Limited direct corpus support for RLHF-specific value embedding; related papers focus on persuasion outputs rather than training procedures
- Break condition: If annotator selection becomes transparent and demographically diverse with documented value orientations, and users are informed of these influences.

### Mechanism 2: Recursive Human-Machine Training Loop
- Claim: RLHF creates a recursive process where humans train LLMs and are subsequently trained by them, reinforcing hegemonic language conventions.
- Mechanism: Model generates probabilistic response → human annotator refines "naturalness" → model outputs train users on normative language → users internalize these norms → cycle reinforces standard language use.
- Core assumption: Users cannot distinguish embedded biases and adopt LLM outputs as models for "correct" expression.
- Evidence anchors:
  - [abstract] "reinforce hegemonic language use, perpetuate biases"
  - [section] "This creates a recursive process whereby humans first build and train LLMs and are subsequently taught and trained by them"
  - [corpus] Weak corpus support; corpus papers address persuasion effectiveness but not this recursive dynamic
- Break condition: If AI literacy training enables users to recognize embedded linguistic biases and resist normative adoption.

### Mechanism 3: Dialogic Information Delivery Reduces Critical Engagement
- Claim: RLHF-enhanced conversational search shifts information-seeking from active evaluation to passive receipt, increasing persuasion through natural language formatting.
- Mechanism: User inputs natural language query → LLM delivers pre-synthesized, summarized answer in conversational format → reduced cognitive effort → increased trust and reduced source verification.
- Core assumption: Dialogic interaction with "human-like" responses increases acceptance while decreasing critical evaluation.
- Evidence anchors:
  - [abstract] "information seeking practices" analyzed as key persuasive procedure
  - [section] "people tend to agree with the AI system when its responses are provided" (Kim et al., p. 827); LLMs may "selectively choose sources that it hypothesizes humans will find most persuasive" (Google DeepMind)
  - [corpus] "Towards Strategic Persuasion with Language Models" confirms LLMs have "strong persuasive capabilities comparable to those of humans"; "How Persuasive Could LLMs Be?" finds AI-generated argumentative text persuasive in user studies
- Break condition: If uncertainty expressions are systematically included (Kim et al. found this decreases agreement), or if interfaces require explicit source verification before displaying synthesized answers.

## Foundational Learning

- **Procedural Rhetoric (Bogost)**:
  - Why needed here: This is the paper's core analytical framework—the concept that rule-based systems (like RLHF pipelines) embed persuasive arguments through their procedures, not just their outputs.
  - Quick check question: Can you explain how a training procedure (rather than output content) can make a persuasive "argument" about how language should work?

- **RLHF Three-Phase Pipeline**:
  - Why needed here: Understanding data collection → reward modeling → policy optimization is essential to identify where biases enter and how they scale.
  - Quick check question: At which phase do annotator judgments get generalized to millions of outputs, and what risk does this create?

- **Annotator Demographics and Agreement Rates**:
  - Why needed here: The paper documents that OpenAI annotators are predominantly Filipino/Bangladeshi (50%) while Anthropic's are predominantly white, with different agreement rates (76% vs. 63%)—this directly affects which values are embedded.
  - Quick check question: Why might low annotator agreement rates indicate either problematic instructions or legitimate value pluralism?

## Architecture Onboarding

- **Component map**:
  - Data collection: Human annotators rate/rank/free-form evaluate outputs using task-specific criteria
  - Reward model: Supervised learning on feedback data to automate judgment at scale
  - Policy optimization: Model weights updated based on reward model scores
  - User interface: Chatbot delivers outputs; may collect preference feedback for future training

- **Critical path**: Annotator selection + instruction design → feedback collection → reward model training → model deployment → user persuasion effects manifest in language norms, information habits, and relationship expectations

- **Design tradeoffs**:
  - Annotator homogeneity (higher agreement) vs. diversity (better representation)
  - Helpfulness vs. harmlessness: models may need to refuse some useful tasks to avoid harm
  - Transparency (revealing annotator context) vs. proprietary protection
  - Paywall access (advanced models) vs. equity (who gets accurate information)

- **Failure signatures**:
  - Instruction bias: Annotators over-influenced by dataset creator guidelines, inflating perceived model performance
  - False certainty: Model delivers confident assertions on topics where annotators lacked expertise
  - Selective sourcing: Model chooses sources predicted to be persuasive rather than accurate (DeepMind finding)
  - Over-reliance: Users accept outputs without verification, especially when uncertainty is not expressed

- **First 3 experiments**:
  1. Annotator diversity audit: Compare outputs from demographically distinct annotator pools on the same prompts to quantify value divergence.
  2. Uncertainty intervention: A/B test systematic uncertainty hedging ("I'm not sure, but...") to measure impact on user agreement and verification behavior.
  3. Source transparency: Display annotator demographic/context summaries alongside outputs to test effects on user trust and critical evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can RLHF frameworks be modified to incorporate situated value alignment rather than relying on "universal framings"?
- Basis in paper: [explicit] Section IV.A cites Arzberger et al. [17], who argue for engaging annotators' situated knowledge at "design-time" and users at "use-time."
- Why unresolved: Current metrics (e.g., "Helpful") obscure which cultural values are prioritized, often catering to a minority perspective.
- What evidence would resolve it: New protocols capturing annotator demographics and resulting model outputs validated against diverse user needs.

### Open Question 2
- Question: To what extent does the "conversational search" procedure degrade users' ability to interpret texts and identify reliable sources?
- Basis in paper: [explicit] Section IV.B highlights that users "may struggle more with identifying reliable sources" as a key area for future consideration.
- Why unresolved: The shift from active search-and-recall to tailored summarization removes the user's onus to assess source veracity.
- What evidence would resolve it: Comparative studies of information literacy skills between users relying on LLMs versus traditional search engines.

### Open Question 3
- Question: How does the "quantification" of relationships in RLHF-enhanced social chatbots impact human-to-human empathy?
- Basis in paper: [explicit] Section IV.C notes that features like "Relationship Bond" mirror LLM training and calls for research on how this might shift feelings of empathy.
- Why unresolved: It is unclear if algorithmic rewards for social interactions blur human-machine distinctions or create unrealistic relational standards.
- What evidence would resolve it: Longitudinal psychological studies tracking social expectations and empathy levels in frequent social chatbot users.

## Limitations

- The analysis relies heavily on theoretical frameworks without empirical validation of the claimed persuasive mechanisms
- The recursive human-machine training loop and dialogic information delivery effects lack direct experimental evidence linking RLHF procedures to user behavior changes
- The impact of annotator demographic diversity on embedded values remains speculative without systematic cross-annotator output comparison studies

## Confidence

- Procedural Embedding of Values: Medium confidence - supported by literature on annotator bias but lacks direct RLHF-specific evidence
- Recursive Training Loop: Low confidence - mechanism is theoretically plausible but not empirically demonstrated
- Dialogic Persuasion Effects: Medium confidence - supported by persuasion studies but not specifically tied to RLHF training procedures

## Next Checks

1. Conduct controlled experiments comparing LLM outputs from demographically distinct annotator pools to quantify value divergence
2. Test whether systematic uncertainty expression in LLM responses reduces user agreement and increases source verification
3. Measure user trust and critical evaluation when annotator demographic information is displayed alongside AI outputs