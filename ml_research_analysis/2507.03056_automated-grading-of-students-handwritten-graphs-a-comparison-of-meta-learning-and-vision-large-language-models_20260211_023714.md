---
ver: rpa2
title: 'Automated Grading of Students'' Handwritten Graphs: A Comparison of Meta-Learning
  and Vision-Large Language Models'
arxiv_id: '2507.03056'
source_url: https://arxiv.org/abs/2507.03056
tags:
- grading
- learning
- meta-learning
- graphs
- vllms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automatically grading students'
  handwritten graphs in math-related courses, an area with limited prior research.
  The authors propose multimodal meta-learning models that process both the graph
  images and extracted text to predict grades based on predefined criteria.
---

# Automated Grading of Students' Handwritten Graphs: A Comparison of Meta-Learning and Vision-Large Language Models

## Quick Facts
- **arXiv ID:** 2507.03056
- **Source URL:** https://arxiv.org/abs/2507.03056
- **Reference count:** 39
- **Primary result:** Meta-learning models outperform VLLMs in 2-way graph classification (accuracy up to 56.87%), while VLLMs show slightly higher accuracy in complex 3-way tasks (up to 50.00%).

## Executive Summary
This paper addresses the challenge of automatically grading students' handwritten graphs in math-related courses, an area with limited prior research. The authors propose multimodal meta-learning models that process both graph images and extracted text to predict grades based on predefined criteria. Using a real-world dataset from a Swiss distance university, the study compares these specialized meta-learning models with Vision Large Language Models (VLLMs) in few-shot learning scenarios. Results show meta-learning models outperform VLLMs in simpler 2-way classification tasks, while VLLMs achieve slightly better performance in more complex 3-way classification. The authors conclude that while VLLMs show promise, their reliability for practical grading applications remains uncertain and requires further investigation.

## Method Summary
The study employs few-shot learning to classify handwritten student graphs into discrete grade categories. For meta-learning approaches, the system uses ResNet-18 to encode graph images and BERT to encode OCR-extracted text, with embeddings concatenated for classification via metric-based methods (Prototypical Networks, MAML variants). VLLM approaches use in-context learning with GPT-4.1/4o series models, processing both the graph image and grading criteria through prompt engineering. The dataset consists of student graphs from three assignment modules at a Swiss distance university, with grades derived from binary criterion vectors. Models are evaluated using N-way K-shot classification accuracy across 2-way and 3-way tasks.

## Key Results
- Meta-learning models achieve higher accuracy than VLLMs in 2-way classification tasks (up to 56.87% vs. VLLM performance)
- VLLMs demonstrate slightly better performance in complex 3-way classification tasks (up to 50.00% accuracy)
- Multimodal fusion (combining graph and text embeddings) improves accuracy over single-modality inputs when OCR quality is sufficient
- VLLMs exhibit instability issues when processing multiple graphs in single prompts, producing skipped or hallucinated responses

## Why This Works (Mechanism)

### Mechanism 1
Multimodal fusion combining graph and text embeddings improves grading accuracy over single-modality inputs when OCR extraction is functional. The ResNet-18 encoder processes cropped graph images while BERT processes OCR-extracted text, with concatenated embeddings providing superior context for grade prediction. Evidence shows combined multimodal approaches yield highest accuracy (e.g., 2-way 1-shot: 54.09% combined vs. 52.86% graph-only).

### Mechanism 2
Metric-based meta-learning (Prototypical Networks) excels at simpler binary distinctions in low-data regimes. The model computes class prototypes as mean embeddings from few support examples and classifies queries by Euclidean distance to these prototypes. This geometric separation works effectively when visual differences between correct and incorrect graphs are distinct, achieving up to 56.87% accuracy in 2-way tasks.

### Mechanism 3
VLLMs leverage pre-trained world knowledge to handle complex classification tasks better than simple metric distances. Using in-context learning, they interpret nuanced graph features and map them to abstract grading criteria, showing superior performance in 3-way tasks (up to 50.00% accuracy). However, they suffer from reliability issues including skipped/hallucinated responses when processing multiple graphs simultaneously.

## Foundational Learning

**Concept: Meta-Learning (N-way K-shot)**
- Why needed: The core problem is few-shot gradingâ€”learning to grade new assignment types with very few labeled examples.
- Quick check: In a 3-way 2-shot episode, how many distinct classes (grades) and support examples per class are sampled?

**Concept: Embedding Fusion**
- Why needed: Student graphs contain visual drawings and textual labels/axis titles; the architecture must merge these to understand full context.
- Quick check: Why does the paper use concatenation of ResNet and BERT embeddings instead of processing image and text separately for grading?

**Concept: OCR (Optical Character Recognition)**
- Why needed: To feed handwritten text into the text encoder (BERT), it must first be converted from pixel clusters to string tokens.
- Quick check: Based on the ablation study, does the OCR-derived text provide the primary signal or a complementary signal compared to the graph image?

## Architecture Onboarding

**Component map:** Raw Student Image -> Morphology/Contour detection (Crops Graph) + Tesseract OCR (Extracts Text) -> ResNet-18 (Image -> Vector) + BERT (Text -> Vector) -> Concatenation Layer -> Meta-learning Classifier (Proto/MAML) OR VLLM Prompt Interface

**Critical path:** The contour detection pipeline must accurately crop the "square-like" graph; failure here sends noise to the ResNet. The annotation rubric is critical for mapping criteria to integer grades used as labels.

**Design tradeoffs:** Meta-learning is cheaper at inference and more stable for 2-way tasks but harder to train. VLLMs require no training but have high API costs, latency, and instability in 3-way batch processing. Automated cropping uses contours but allows manual adjustment.

**Failure signatures:** Class imbalance prevents valid episode construction when grades are underrepresented. VLLM batching causes skipped or hallucinated outputs. Single-annotator labeling introduces subjective bias.

**First 3 experiments:**
1. Run contour detection on a hold-out set to measure the "manual adjustment rate" (how often a human must fix the crop).
2. Retrain the best Meta-learner using only the graph crop vs. only the text to establish baseline feature importance.
3. Query the VLLM with single-graph vs. batched-graph prompts to quantify the "instability" (drop in accuracy or rate of parsing errors).

## Open Questions the Paper Calls Out

**Open Question 1:** Can advanced prompt engineering strategies resolve the instability and hallucinations observed in VLLMs when processing multiple student graphs within a single prompt? The authors note VLLMs produce inconsistent results (skipped or hallucinated responses) when multiple test graphs are included in one prompt to reduce costs, requiring further investigation.

**Open Question 2:** How does the introduction of multiple expert annotators and consensus-based rubrics impact the performance and bias of the proposed meta-learning models? The authors identify reliance on a single annotator as a "major limitation" that introduces potential human error and subjective bias, explicitly listing "involving multiple annotators" as necessary for future work.

**Open Question 3:** Will the superior performance of VLLMs in complex 3-way classification tasks persist when evaluated on larger, more balanced datasets? The authors note VLLMs slightly outperformed meta-learning in 3-way tasks, but highlight the dataset is small with strong class imbalance, limiting robustness of these findings.

## Limitations

- Proprietary dataset from a single Swiss university limits generalizability across educational contexts and institutions
- Accuracy levels remain below practical deployment thresholds (56.87% for 2-way, 50.00% for 3-way)
- VLLM instability when processing multiple graphs in batch prompts represents a fundamental reliability barrier for classroom implementation

## Confidence

**High Confidence:** Meta-learning models consistently outperform VLLMs in 2-way classification tasks, with specific accuracy metrics reported across multiple experimental configurations.

**Medium Confidence:** VLLMs show superior performance in 3-way classification but suffer from reliability issues. The claimed superiority of metric-based meta-learning for binary distinctions is supported but requires further validation.

**Low Confidence:** The practical applicability of these systems for real classroom deployment remains uncertain due to cost constraints, API reliability issues, and the gap between achieved accuracy and educational standards.

## Next Checks

1. Apply the best-performing meta-learning model to handwritten graph datasets from different universities and educational systems to assess whether the observed performance gap between meta-learning and VLLMs persists across diverse student populations and grading standards.

2. Conduct multi-day evaluations of VLLM performance to quantify the frequency and severity of skipped/hallucinated responses when processing varying numbers of graphs per prompt, establishing the maximum reliable batch size for classroom deployment.

3. Implement a blind comparison where human graders independently assess the same graph samples, measuring inter-rater reliability alongside AI-human agreement rates to establish whether the AI models achieve acceptable consistency with expert grading standards.