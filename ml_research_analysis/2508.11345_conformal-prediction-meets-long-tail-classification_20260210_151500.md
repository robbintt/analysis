---
ver: rpa2
title: Conformal Prediction Meets Long-tail Classification
arxiv_id: '2508.11345'
source_url: https://arxiv.org/abs/2508.11345
tags:
- coverage
- prediction
- tacp
- classes
- tail
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of imbalanced coverage in conformal
  prediction under long-tail label distributions, where head classes are over-covered
  while tail classes are under-covered. The authors propose Tail-Aware Conformal Prediction
  (TACP) to mitigate this coverage gap by introducing a regularization term that selectively
  penalizes head classes based on their label rank.
---

# Conformal Prediction Meets Long-tail Classification

## Quick Facts
- **arXiv ID:** 2508.11345
- **Source URL:** https://arxiv.org/abs/2508.11345
- **Reference count:** 40
- **Primary result:** Tail-Aware Conformal Prediction (TACP) reduces head-tail coverage gap in long-tail classification from 2.18 to 1.11 on ImageNet-LT while maintaining valid marginal coverage.

## Executive Summary
This paper addresses the problem of imbalanced coverage in conformal prediction under long-tail label distributions, where head classes are over-covered while tail classes are under-covered. The authors propose Tail-Aware Conformal Prediction (TACP) to mitigate this coverage gap by introducing a regularization term that selectively penalizes head classes based on their label rank. Theoretical analysis shows TACP consistently achieves a smaller head-tail coverage gap compared to standard methods. To further improve coverage balance across all individual classes, they extend TACP to soft TACP (sTACP) using a smooth reweighting mechanism based on class prior probabilities.

## Method Summary
The paper introduces Tail-Aware Conformal Prediction (TACP) which modifies the non-conformity score function by adding a selective rank-based penalty for head classes: $s_{TACP}(x,y) = s(x,y) + \lambda \cdot \mathbb{I}(y \in G_h) \cdot (o_x(y) - k_r)_+$. This selectively inflates scores of head classes when their prediction rank is low, forcing the calibration quantile to adapt in a way that improves tail-conditional coverage. The method is extended to soft TACP (sTACP) which replaces the binary head/tail indicator with smooth class prior weighting. Both methods are evaluated on CIFAR100-LT and ImageNet-LT datasets using APS, LAC, TOPK, and RAPS scores, showing significant reductions in coverage gaps while maintaining valid marginal coverage guarantees.

## Key Results
- TACP reduces head-tail coverage gap (CovGap-HT) from 2.18 to 1.11 on ImageNet-LT using APS score
- sTACP further improves class-conditional coverage balance, outperforming baselines like CLASSWISE and CLUSTER
- Both TACP and sTACP maintain valid marginal coverage while improving efficiency (smaller prediction sets)
- The improvements are consistent across different non-conformity scores (APS, LAC, TOPK, RAPS)

## Why This Works (Mechanism)

### Mechanism 1: Selective Rank Regularization
- **Claim:** If a rank-based penalty is selectively applied to head classes during calibration, the coverage gap between head and tail classes decreases compared to standard conformal prediction.
- **Mechanism:** TACP modifies the non-conformity score $s(x, y)$ by adding a term $\lambda \cdot \mathbb{I}(y \in G_h) \cdot (o_x(y) - k_r)_+$. This selectively increases the scores of head classes when their prediction rank is low, inflating their scores and forcing the global quantile threshold to adapt.
- **Core assumption:** The model's label ranking $o_x(y)$ correlates with uncertainty, and head class samples are sufficiently abundant in calibration to influence the global quantile.
- **Evidence anchors:** Abstract states "selectively penalizes head classes based on their label rank." Section 4.2 explains the key insight is utilizing LT information to selectively penalize label rankings for head.

### Mechanism 2: Marginal Coverage Preservation via Score Modification
- **Claim:** Modifying the non-conformity score function preserves the statistical validity of the marginal coverage guarantee, provided the calibration and test data are exchangeable.
- **Mechanism:** Since TACP defines a deterministic transformation of the scores, the exchangeability of data points is preserved, maintaining the theoretical guarantee $P(Y_{n+1} \in C_{TACP}(X_{n+1})) \ge 1 - \alpha$.
- **Core assumption:** The transformation applied to the score is deterministic and data is i.i.d. (or exchangeable).
- **Evidence anchors:** Theorem 1 in section 4.2 proves the lower bound $1-\alpha$ holds for the constructed set.

### Mechanism 3: Smooth Reweighting via Class Priors (sTACP)
- **Claim:** Replacing the binary head/tail indicator with smooth weighting based on class prior probabilities reduces class-conditional coverage gap more effectively than binary partitioning.
- **Mechanism:** sTACP replaces the hard indicator $\mathbb{I}(y \in G_h)$ with class prior $\hat{p}(y)$, allowing penalty to scale continuously with class frequency.
- **Core assumption:** Estimated class prior $\hat{p}(y)$ accurately reflects test distribution's long-tail structure.
- **Evidence anchors:** Section 5 introduces sTACP using smooth weighting function that reflects both long-tail structure and class-specific information.

## Foundational Learning

- **Concept: Split Conformal Prediction (SCP)**
  - **Why needed here:** TACP is built directly on top of SCP. Understanding how SCP uses calibration set to find threshold $\hat{\tau}$ that guarantees marginal coverage $1-\alpha$ is prerequisite.
  - **Quick check question:** If you have 100 calibration samples and want 90% coverage, which quantile of the calibration scores do you select?

- **Concept: Non-conformity Scores (e.g., APS, LAC)**
  - **Why needed here:** TACP requires a base score $s(x,y)$ to function. Choice of base score determines baseline efficiency and coverage properties.
  - **Quick check question:** Does a higher non-conformity score indicate higher or lower model confidence in the label?

- **Concept: Long-Tail Distribution (Head vs. Tail)**
  - **Why needed here:** Understanding partitioning of label space into Head ($G_h$) and Tail ($G_t$) based on cumulative probability $\eta$ is required to configure TACP.
  - **Quick check question:** In a dataset with 1000 classes where top 50 classes contain 80% of data, would partition threshold of $\eta=0.5$ result in large or small Head group?

## Architecture Onboarding

- **Component map:** Pretrained Classifier -> Base Score Calculator -> TACP Modifier -> Quantile Estimator -> Set Predictor
- **Critical path:** The dependency lies in the Head-Tail Partitioning. You must compute class frequencies on calibration set to determine $G_h$ before calculating TACP scores.
- **Design tradeoffs:**
  - **Partition $\eta$:** Higher $\eta$ includes more classes in "Head" group, potentially protecting more classes but diluting effect on true majority classes.
  - **Hyperparameters ($\lambda, k_r$):** High $\lambda$ aggressively reduces head coverage to boost tail coverage (narrowing gap but potentially violating marginal coverage if not careful).
  - **TACP vs. sTACP:** Use TACP for reducing head-tail gap (binary fairness); use sTACP if fine-grained per-class balance is priority and reliable prior estimates exist.
- **Failure signatures:**
  - **Inflated Set Sizes:** If $k_r$ is too small, penalty applies to almost all ranks, causing quantile threshold to spike and prediction sets to include nearly all labels.
  - **No Gap Reduction:** If calibration set is too small, noisy quantile estimation means selective penalty may not have intended effect on threshold.
  - **sTACP Collapse:** If priors $\hat{p}(y)$ are uniform (balanced data), sTACP reduces to standard CP with constant offset, offering no advantage.
- **First 3 experiments:**
  1. **Baseline Comparison:** Implement STANDARD (Split CP) vs. TACP on CIFAR100-LT. Measure `CovGap-HT` to verify TACP reduces gap (e.g., from ~4.8 to ~0.8 as in Table 2).
  2. **Hyperparameter Sensitivity:** Vary $k_r$ while fixing $\lambda$ and $\eta$. Plot `CovGap-HT` vs. `AvgSize` to find operating point where gap is minimized without exploding set sizes.
  3. **Score Ablation:** Run TACP using APS vs. LAC scores. Check if improvement in `CovGap-HT` is consistent across both score types.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the hyperparameters $\lambda$ and $k_r$ be determined theoretically without consuming calibration data for tuning?
- **Basis in paper:** Appendix A.3 states that $\lambda$ and $k_r$ are "chosen using the calibration dataset" via a search procedure.
- **Why unresolved:** Using calibration data for hyperparameter tuning reduces effective sample size for threshold estimation, which may affect coverage validity or efficiency in data-scarce scenarios.
- **What evidence would resolve it:** A theoretical derivation for optimal default values or an adaptive mechanism that does not require holding out a validation subset of the calibration data.

### Open Question 2
- **Question:** Is the linear reweighting by class prior $\hat{p}(y)$ in sTACP the theoretically optimal strategy for minimizing class-conditional coverage gaps?
- **Basis in paper:** Section 5 introduces sTACP heuristically using $\hat{p}(y)$ to replace hard indicator, but does not prove this specific weighting minimizes gap compared to other functions.
- **Why unresolved:** While empirically effective, paper does not establish if non-linear or inverse-frequency weighting scheme might achieve lower CovGap with smaller prediction sets.
- **What evidence would resolve it:** A theoretical analysis comparing different weighting functions or empirical results showing $\hat{p}(y)$ outperforms inverse-frequency weighting across diverse datasets.

### Open Question 3
- **Question:** How robust is TACP when underlying pretrained model has highly noisy or inaccurate label rankings for tail classes?
- **Basis in paper:** Equation (4) relies on rank $o_x(y)$ derived from model's estimated posterior $\hat{\pi}(x)$.
- **Why unresolved:** Method assumes ranking is informative enough to selectively penalize head classes; if model is uncalibrated, rank penalty might fail to correct coverage imbalance.
- **What evidence would resolve it:** Experiments evaluating TACP performance on models with intentionally degraded calibration or randomized rankings for tail classes.

## Limitations
- The theoretical guarantees rely on head class samples in calibration being sufficiently abundant to influence global quantile threshold effectively, which may break down in extreme long-tail scenarios.
- The smooth weighting in sTACP assumes accurate estimation of class priors, which may not hold under label shift between training and test distributions.
- The paper does not provide explicit variance analysis across trials, making it difficult to assess statistical significance of reported improvements.

## Confidence
- **High Confidence**: The mechanism for reducing head-tail coverage gap through selective rank regularization (TACP) is well-supported by both theoretical analysis (Theorem 1) and experimental results showing consistent reduction in CovGap-HT across different scores and datasets.
- **Medium Confidence**: The extension to sTACP for improving class-conditional coverage balance is theoretically sound but has limited empirical validation in the paper.
- **Medium Confidence**: The claim that TACP maintains efficient prediction sets while reducing coverage gaps is supported by AvgSize metrics, though efficiency comparison with standard CP is not always explicitly quantified.

## Next Checks
1. **Calibration Set Size Sensitivity**: Systematically vary calibration set size (e.g., 10%, 20%, 50% of total data) and measure how CovGap-HT and AvgSize change for both TACP and sTACP to identify minimum calibration size needed for reliable performance.

2. **Label Shift Robustness**: Evaluate TACP and sTACP on test sets with different long-tail distributions (e.g., different Pareto exponents) from calibration set to assess performance degradation when class priors shift.

3. **Statistical Significance Analysis**: Compute confidence intervals for CovGap-HT and AvgSize across the 100 trials to determine if improvements over baselines are statistically significant rather than due to random variation.