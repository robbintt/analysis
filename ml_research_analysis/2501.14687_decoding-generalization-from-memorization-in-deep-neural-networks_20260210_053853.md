---
ver: rpa2
title: Decoding Generalization from Memorization in Deep Neural Networks
arxiv_id: '2501.14687'
source_url: https://arxiv.org/abs/2501.14687
tags:
- masc
- corruption
- accuracy
- training
- degree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work explores why deep networks trained with shuffled labels\u2014\
  i.e., models that memorize\u2014tend to generalize poorly to true labels. It introduces\
  \ the Minimum Angle Subspace Classifier (MASC), a lightweight probe that classifies\
  \ data points by measuring the angle between layer outputs and class-conditioned\
  \ subspaces built from training data."
---

# Decoding Generalization from Memorization in Deep Neural Networks

## Quick Facts
- arXiv ID: 2501.14687
- Source URL: https://arxiv.org/abs/2501.14687
- Reference count: 40
- Key outcome: MASC achieves up to 150% higher accuracy than memorized models on true labels, even with 80-100% label corruption

## Executive Summary
This work investigates why deep networks trained on shuffled labels (memorization) generalize poorly to true labels, and whether the latent generalization capacity can be extracted. It introduces the Minimum Angle Subspace Classifier (MASC), which classifies by measuring angles between layer outputs and class-conditioned subspaces. Experiments across multiple architectures and datasets show MASC often significantly outperforms the original model, even under extreme label corruption, revealing that deep networks preserve substantial ability to generalize to true labels despite memorization.

## Method Summary
The approach trains deep networks on corrupted labels (randomly shuffled with probability p) until near-perfect training accuracy. MASC is then applied to layer activations: for each class, it constructs a subspace from training activations (augmented with negatives for zero-centering) using PCA capturing 99% variance. Test points are classified by computing the minimum angle between the point and each class subspace. The method is tested on MLP, CNN, AlexNet, and ResNet-18 models trained on MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and Tiny ImageNet.

## Key Results
- MASC applied to at least one layer achieves significantly higher test accuracy than the trained model, sometimes by over 150%
- Performance persists even with 80-100% label corruption
- MASC using true-label subspaces performs even better than corrupted-label subspaces
- Relabeling with MASC predictions can modestly improve model generalization after retraining
- Logistic regression and nearest-class mean probes show comparable performance in some settings

## Why This Works (Mechanism)
The mechanism relies on the observation that deep network layers, even when trained to memorize corrupted labels, retain geometric structure in their representations that correlates with true class boundaries. By constructing class-specific subspaces in the activation space and measuring angular proximity, MASC can extract this latent structure that the softmax classifier fails to utilize when trained on corrupted labels.

## Foundational Learning
- **Label corruption and memorization**: Understanding how random label shuffling affects training dynamics and generalization
- **Subspace geometry in high-dimensional spaces**: PCA-based subspace construction and angular distance metrics
- **Layer-wise representations**: How different network layers capture and preserve class-relevant information
- **Probe methods**: Lightweight classifiers applied to intermediate representations to extract latent information

## Architecture Onboarding

**Component map**: Data → Model (MLP/CNN/AlexNet/ResNet) → Layer outputs → MASC (PCA + Angle computation) → Classification

**Critical path**: The most important steps are: 1) training model to high training accuracy on corrupted labels, 2) extracting and zero-centering layer activations with negative augmentation, 3) building PCA subspaces capturing 99% variance, 4) computing minimum angle classification

**Design tradeoffs**: The method trades computational simplicity (linear algebra operations) for interpretability and the ability to extract latent generalization. The negative augmentation ensures subspaces pass through origin, while the 99% variance threshold balances discriminative power against overfitting to noise.

**Failure signatures**: MASC accuracy matching model accuracy indicates insufficient subspace dimensionality or incorrect zero-centering. Failure to memorize training data suggests missing negative augmentation step.

**First experiments**: 1) Train MLP on MNIST with 60% label corruption until 100% training accuracy. 2) Apply MASC to first hidden layer and measure accuracy improvement. 3) Vary PCA variance threshold (90%, 95%, 99%) to assess sensitivity.

## Open Questions the Paper Calls Out
None

## Limitations
- Results depend on random label corruption without specified seeds, causing numerical variation
- Analysis focuses on synthetic corruption rather than naturally noisy real-world datasets
- Claims about "substantial ability to generalize" are relative rather than absolute performance improvements

## Confidence
- **High confidence**: MASC can achieve higher accuracy than trained models on corrupted-label tasks; subspace geometry encodes true-label information
- **Medium confidence**: MASC consistently outperforms all baselines across all settings; 99% variance threshold is optimal
- **Low confidence**: Claims about "substantial ability to generalize" based on relative improvements

## Next Checks
1. Reproduce MASC on MLP-MNIST with 60% label corruption, measuring both training and test accuracy across multiple random seeds
2. Systematically vary the PCA variance threshold (90%, 95%, 99, 99.9%) to verify insensitivity to subspace dimensionality
3. Test MASC's performance on a naturally noisy dataset (e.g., WebVision or Clothing1M) rather than synthetic label corruption