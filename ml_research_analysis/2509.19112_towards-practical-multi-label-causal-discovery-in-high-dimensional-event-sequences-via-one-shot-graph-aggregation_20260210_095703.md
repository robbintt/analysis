---
ver: rpa2
title: Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences
  via One-Shot Graph Aggregation
arxiv_id: '2509.19112'
source_url: https://arxiv.org/abs/2509.19112
tags:
- causal
- event
- labels
- markov
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CARGO tackles scalable multi-label causal discovery in high-dimensional
  event sequences, where thousands of event types lead to multiple outcomes like vehicle
  failures or diseases. Traditional methods struggle with this scale due to super-exponential
  complexity.
---

# Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation

## Quick Facts
- arXiv ID: 2509.19112
- Source URL: https://arxiv.org/abs/2509.19112
- Authors: Hugo Math; Rainer Lienhart
- Reference count: 40
- Primary result: CARGO recovers causal structures in minutes on 29,100-event automotive dataset where classical methods timeout in days, achieving 44.9% weighted F1

## Executive Summary
CARGO tackles scalable multi-label causal discovery in high-dimensional event sequences where thousands of event types lead to multiple outcomes like vehicle failures or diseases. Traditional methods struggle with this scale due to super-exponential complexity. CARGO addresses this by repurposing pretrained causal Transformers: Phase 1 infers local Markov Boundaries for each label in parallel via conditional mutual information testing, and Phase 2 fuses these local graphs using an adaptive frequency threshold that accounts for long-tail label distributions. This two-stage approach enables efficient probabilistic reasoning at scale.

## Method Summary
CARGO uses two pretrained autoregressive Transformers (Tfx for events, Tfy for labels) to perform conditional mutual information estimation via Monte Carlo sampling. Phase 1 extracts local Markov Boundaries per sequence by testing CMI between each event and label given context. Phase 2 aggregates these local graphs using frequency-based fusion with adaptive thresholds that adjust for label support size, enabling scalable causal discovery on datasets with thousands of event types and hundreds of labels.

## Key Results
- Completes causal discovery in 11.7 minutes vs. >3 days timeout for classical methods on 50K samples
- Achieves 44.9% weighted F1 and 62.8% precision on real-world automotive dataset with 29,100 event types
- Adaptive frequency thresholds outperform static thresholds by 4.88 percentage points in weighted F1
- Scales to 474 imbalanced labels while maintaining computational efficiency through parallelization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Conditional mutual information computed via autoregressive Transformers can identify Markov Boundaries per sequence
- **Mechanism:** Two pretrained Transformers estimate P(Xi|Z) and P(Y|Xi,Z). CMI is approximated via Monte Carlo sampling: ÎN(Yj, Xi|Z) = (1/N) Σ IG(Yj, Xi|Z=z(l)). If I(Yj, Xi|Z) > θj ≈ 0, Xi is considered a cause of Yj.
- **Core assumption:** Oracle Models perfectly approximate true conditional distributions; bounded lagged effects; temporal precedence
- **Evidence anchors:** Abstract states Phase 1 infers local Markov Boundaries via conditional mutual information testing; Page 4 formalizes CMI definition and threshold-based edge inclusion
- **Break condition:** Poor Transformer density estimates (e.g., on rare events) make CMI estimates unreliable

### Mechanism 2
- **Claim:** Parallelized one-shot extraction makes complexity independent of |X| and |Y| per batch
- **Mechanism:** CMI estimation at each position i ∈ [c, L] is performed independently with sampling pushed into batch dimension. Complexity transitions from O(BS × N × L) to O(1) per batch on GPUs, bounded by Transformer inference O(L²)
- **Core assumption:** GPU parallelism available; minimum context c stabilizes entropy estimates
- **Evidence anchors:** Page 5 describes independent CMI estimations with batch dimension sampling; Table 1 shows 11.7 min vs. >3 days runtime improvement
- **Break condition:** If sequence length or batch size exceeds GPU memory, parallelization benefits collapse

### Mechanism 3
- **Claim:** Adaptive frequency thresholds improve fusion under long-tail label distributions
- **Mechanism:** Edge frequency π̂i,j(m) converges to (1-β)πij + α(1-πij). For rare labels (small mj), high variance necessitates high threshold; for common labels, lower threshold retains weaker valid edges. Threshold function: τj(mj) = (τmax - τmin) · 1/(1 + e^(k(log mj - log m0))) + τmin
- **Core assumption:** Long-tail distribution exists; LLN holds for common labels but not rare ones; 1-β >> α
- **Evidence anchors:** Page 6-7 defines adaptive threshold; Figure 4 shows 44.88% weighted F1 vs. 40% for static thresholds
- **Break condition:** If label supports are uniformly distributed, adaptive thresholds reduce to static thresholds with no benefit

## Foundational Learning

- **Concept: Markov Boundary (MB)**
  - Why needed here: MB(Yj) is the minimal variable set rendering Yj conditionally independent of all others. CARGO aims to recover MB per label.
  - Quick check question: Given variables {A, B, C, D} where D causes Y, and B mediates A→Y, what is MB(Y)?

- **Concept: Conditional Mutual Information (CMI)**
  - Why needed here: CMI quantifies dependence between Xi and Yj given context Z. I(Yj, Xi|Z) = 0 iff conditionally independent. Core CI-test in Phase 1.
  - Quick check question: If I(X; Y|Z) = 0 but I(X; Y) > 0, what does this imply about Z's role?

- **Concept: Bayesian Model Averaging / Structural Fusion**
  - Why needed here: CARGO treats each sequence's local graph as a sample from a global causal model. Fusion aggregates local graphs into consensus G* via frequency voting.
  - Quick check question: If 60% of local graphs contain edge A→B and 40% contain B→A, what does frequency-based fusion output?

## Architecture Onboarding

- **Component map:** Tfx (90M params, next-event prediction) -> CMI estimation context -> Tfy (15M params, next-label prediction) -> local graph extraction -> edge frequency counting -> adaptive threshold -> global Markov Boundaries

- **Critical path:** Tfx/Tfy pretraining quality -> context c selection (15) -> sampling N (68) -> CMI threshold k -> adaptive τj(mj) parameters (τmax=0.5, τmin=0.05, m0, decay k)

- **Design tradeoffs:**
  - Higher N → lower variance but longer inference
  - Higher c → more stable entropy but fewer positions evaluated
  - Higher τmax → higher precision, lower recall for tail labels
  - Union fusion → max recall, poor precision; frequency cutoff → tradeoff depends on threshold choice

- **Failure signatures:**
  - Empty local graphs: Tfy poorly calibrated or threshold θj too high
  - Spurious edges dominate: Tfx sampling misses true context distribution or α too high
  - Tail labels have no edges: τj(mj) too conservative; check m0 and decay rate k
  - OOM errors: Reduce N or batch size; sequence length L too long for attention

- **First 3 experiments:**
  1. Train Tfx/Tfy with varying parameters and context c on held-out data; measure Tfy F1 and downstream MB recovery
  2. Vary N ∈ {16, 32, 64, 128}; plot F1, precision, recall, and elapsed time per sample
  3. Compare union, static frequency (τ = 0.05, 0.25, 0.5, 0.8), beta-fitting, and adaptive thresholding on held-out sequences with known ground-truth MB

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the CARGO framework be extended to discover event-to-event causal relationships, rather than limiting discovery to event-to-label relationships?
- **Basis in paper:** [explicit] The Conclusion explicitly states, "Future work should extend this framework to support event-to-event causality."
- **Why unresolved:** Current theoretical guarantees rely on labels being leaf nodes with no descendants, a structural property that doesn't hold for intermediate events.
- **What evidence would resolve it:** Successful modification to recover inter-event directed acyclic graphs on synthetic datasets with known event-event edges.

### Open Question 2
- **Question:** Is it possible to relax assumptions of bounded lagged effects and temporal precedence while maintaining identifiability guarantees?
- **Basis in paper:** [explicit] Authors list relaxing "assumptions such as bounded lagged effects or temporal precedence" as future work.
- **Why unresolved:** Theorem 1 proof depends on these constraints to orient edges and bound history length; removing them risks introducing cycles.
- **What evidence would resolve it:** New theoretical bounds or empirical demonstration on datasets with unbounded time delays or instantaneous effects.

### Open Question 3
- **Question:** Can general-purpose foundation models replace domain-specific Transformers without significant degradation?
- **Basis in paper:** [explicit] Paper suggests method "could scale further by leveraging general-purpose foundation models... beyond automotive diagnostics."
- **Why unresolved:** CARGO relies on high-quality density estimation for conditional mutual information calculations; generic models may not capture fine-grained dependencies.
- **What evidence would resolve it:** Experiments applying CARGO with off-the-shelf time-series transformers showing comparable F1 scores to domain-specific baselines.

## Limitations
- Performance depends critically on Transformer density estimation quality, which may degrade on datasets with different characteristics than automotive domain
- Adaptive threshold mechanism assumes long-tail label distributions exist - if this assumption fails, benefits over static thresholds disappear
- Uses proprietary data with 29,100 event types, making generalization to other datasets with different sparsity patterns unclear

## Confidence

- **High Confidence:** Parallelization mechanism and complexity analysis are well-supported by text and table evidence showing 11.7 min vs. 3+ days runtime improvement
- **Medium Confidence:** Adaptive threshold mechanism shows promising results in Figure 4 but relies on specific assumptions about label distribution not validated across multiple datasets
- **Medium Confidence:** Core CMI estimation approach has theoretical grounding but depends critically on Oracle Models assumption with only indirect evidence from related work TRACE

## Next Checks

1. **Distribution Sensitivity Test:** Apply CARGO to datasets with uniform vs. long-tail label distributions to verify adaptive thresholds provide benefits only when the long-tail assumption holds

2. **Architecture Transferability:** Evaluate CARGO on non-automotive high-dimensional event sequence datasets (healthcare, finance) to assess generalization beyond proprietary automotive data

3. **Oracle Model Sensitivity:** Systematically vary Tfx/Tfy quality (model size, training data) to quantify how density estimation errors propagate to final causal discovery performance