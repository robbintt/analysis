---
ver: rpa2
title: Comparative Analysis of Lion and AdamW Optimizers for Cross-Encoder Reranking
  with MiniLM, GTE, and ModernBERT
arxiv_id: '2506.18297'
source_url: https://arxiv.org/abs/2506.18297
tags:
- lion
- adamw
- training
- trec
- modernbert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares the recently proposed Lion optimizer with AdamW
  for fine-tuning three transformer-based cross-encoder rerankers (MiniLM, GTE, ModernBERT)
  on the MS MARCO passage ranking task. We investigate whether Lion's simpler optimization
  mechanics and reduced memory overhead translate to improved effectiveness or efficiency
  for reranking models with varying architectures and context lengths.
---

# Comparative Analysis of Lion and AdamW Optimizers for Cross-Encoder Reranking with MiniLM, GTE, and ModernBERT

## Quick Facts
- arXiv ID: 2506.18297
- Source URL: https://arxiv.org/abs/2506.18297
- Reference count: 35
- Primary result: Lion optimizer achieves higher NDCG@10 and MAP on ModernBERT while providing 2.67-10.33% GPU efficiency gains

## Executive Summary
This study compares the recently proposed Lion optimizer with AdamW for fine-tuning three transformer-based cross-encoder rerankers (MiniLM, GTE, ModernBERT) on the MS MARCO passage ranking task. The authors investigate whether Lion's simpler optimization mechanics and reduced memory overhead translate to improved effectiveness or efficiency for reranking models with varying architectures and context lengths. Experiments conducted on Modal's GPU platform show that ModernBERT with Lion achieves the highest NDCG@10 (0.7225) and MAP (0.5121) on TREC DL 2019, while MiniLM with Lion ties ModernBERT for MRR@10 (0.5988) on MS MARCO dev. Lion demonstrates superior GPU utilization efficiency across all models, with gains of 2.67% to 10.33% compared to AdamW, while maintaining competitive performance. The results indicate that optimizer effectiveness depends on model architecture and training configuration, with Lion showing particular strength for ModernBERT under low learning rates with cosine annealing.

## Method Summary
The authors fine-tune three cross-encoder rerankers (MiniLM-L12-H384, GTE-multilingual-base, ModernBERT-base) on MS MARCO passage ranking triplets using both Lion and AdamW optimizers. Models are trained for 3 epochs with batch size 64, BF16 precision, and different learning rates (2e-5 for MiniLM/GTE, 2e-6 for ModernBERT with cosine annealing). First-stage retrieval uses BM25 via Pyserini to generate top-1000 candidates per query. Reranked results are evaluated on TREC DL 2019 and MS MARCO dev using standard metrics including NDCG@10, MAP, and MRR@10.

## Key Results
- ModernBERT+Lion achieves highest NDCG@10 (0.7225) and MAP (0.5121) on TREC DL 2019
- MiniLM+Lion ties ModernBERT+Lion for MRR@10 (0.5988) on MS MARCO dev
- Lion provides GPU efficiency gains of 2.67% to 10.33% compared to AdamW
- Optimizer effectiveness is architecture-dependent, with ModernBERT+Lion outperforming other combinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lion's sign-based momentum update reduces memory and computational overhead while maintaining competitive optimization
- Mechanism: Lion computes updates as `sign(momentum) * lr` rather than tracking both first and second moment estimates like AdamW. The update rule `ct = β1*mt-1 + (1-β1)*gt` followed by `θt = θt-1 - η(sign(ct) + λθt-1)` avoids storing variance statistics, reducing per-parameter state from 2 tensors to 1
- Core assumption: Sign information preserves sufficient gradient direction information for effective optimization despite discarding magnitude
- Evidence anchors:
  - [Section III.D] "Lion... requiring less memory by avoiding second moment estimates"
  - [Section V.A] "Lion shows a modest efficiency improvement of 2.67%, reducing mean GPU usage from 33.09% to 32.21%... The most significant efficiency gain of 10.33% is observed with [GTE]"
  - [corpus] Related work on optimizer comparisons (Pre-Training LLMs paper) validates Lion as viable alternative, though corpus lacks direct cross-encoder reranking studies
- Break condition: If gradient magnitudes carry critical task-specific information (e.g., fine-grained relevance scoring), sign-only updates may underperform

### Mechanism 2
- Claim: Optimizer effectiveness is architecture-dependent, requiring model-specific hyperparameter tuning
- Mechanism: Different transformer architectures (distilled vs. long-context, different attention mechanisms) interact differently with optimizer dynamics. ModernBERT's RoPE embeddings, Flash Attention, and GeGLU activations combined with low learning rate (2e-6) and cosine annealing favor Lion's update dynamics, while GTE's architecture performs better with AdamW at higher learning rates (2e-5)
- Core assumption: The optimizer-model interaction stems from how gradient distributions align with each optimizer's update characteristics during fine-tuning
- Evidence anchors:
  - [Section V.B] "For ModernBERT... Lion substantially outperformed AdamW... For GTE, AdamW... was more effective than Lion"
  - [Section IV.B] "2e-6 for ModernBERT with Lion optimizer... 2e-5 for MiniLM and GTE"
  - [corpus] No corpus evidence specifically addresses optimizer-architecture interactions in reranking
- Break condition: If hyperparameter search was insufficient, observed differences may reflect suboptimal configurations rather than true architectural affinities

### Mechanism 3
- Claim: Training dynamics vary non-monotonically across epochs, requiring checkpoint-based selection
- Mechanism: Different optimizer-model combinations exhibit different convergence patterns—some peak early (GTE+AdamW at epoch 1-2) while others improve gradually (MiniLM+AdamW through epoch 3) or decline after early peaks (MiniLM+Lion TREC performance)
- Core assumption: Validation set characteristics (TREC DL 2019 vs. MS MARCO dev) capture different aspects of reranking quality that respond differently to training duration
- Evidence anchors:
  - [Section V.D] "GTE+AdamW peaks early (Epoch 1 or 2) and then degrades... MiniLM+AdamW shows more gradual improvement"
  - [Table I] Shows non-monotonic performance across all model-optimizer combinations
  - [corpus] No corpus evidence on epoch-wise training dynamics for reranking
- Break condition: Early stopping based on one benchmark may miss optimal checkpoints for deployment scenarios with different query distributions

## Foundational Learning

- Concept: **Cross-encoder reranking architecture**
  - Why needed here: Understanding how `[CLS] query [SEP] document [SEP]` tokenization enables deep query-document interaction explains why optimizer choice affects final relevance scoring
  - Quick check question: Can you explain why cross-encoders are more computationally expensive than bi-encoders but achieve better reranking quality?

- Concept: **Adaptive vs. sign-based optimization**
  - Why needed here: AdamW's adaptive learning rates per parameter (via second moment estimation) vs. Lion's uniform-magnitude sign updates represents fundamentally different optimization philosophies
  - Quick check question: What state variables does AdamW track per parameter that Lion does not, and how does this affect memory usage?

- Concept: **Learning rate scheduling and its interaction with optimizers**
  - Why needed here: The paper uses Cosine Annealing for ModernBERT but not MiniLM/GTE—understanding why different architectures benefit from different LR schedules is critical for reproducing results
  - Quick check question: Why might a low learning rate (2e-6) with cosine annealing work better for ModernBERT+Lion while a higher constant rate (2e-5) works for MiniLM?

## Architecture Onboarding

- Component map:
  MS MARCO Triplets → Pair Conversion → CrossEncoder(Model + Tokenizer) → [CLS] Q [SEP] D [SEP] → Transformer → [CLS] Logits → Sigmoid → Score → Optimizer (Lion/AdamW) updates via BCE Loss → Checkpoints evaluated on TREC DL 2019 + MS MARCO Dev

- Critical path:
  1. Data preparation: Convert triplets (query, pos, neg) to labeled pairs
  2. Optimizer configuration: Match LR and scheduler to model architecture (ModernBERT: 2e-6 + cosine; others: 2e-5, no scheduler)
  3. Training loop: 3 epochs with BF16 precision, batch size 64
  4. First-stage retrieval: BM25 top-1000 candidates via Pyserini
  5. Reranking: Score and re-sort candidates per query
  6. Evaluation: `trec_eval` for TREC DL metrics, custom MRR@10 for MS MARCO dev

- Design tradeoffs:
  - **Lion vs. AdamW**: Lion offers 2.67-10.33% GPU efficiency gains but requires architecture-specific tuning; AdamW is more stable across architectures but less efficient
  - **Context length vs. compute**: GTE/ModernBERT (8192 tokens) outperform MiniLM (512 tokens) on TREC DL but require more GPU memory
  - **Early vs. late checkpoint selection**: Peak performance varies by epoch (E1 for GTE+AdamW, E2 for ModernBERT+Lion, E3 for MiniLM+AdamW on TREC)

- Failure signatures:
  - **Performance degradation over epochs**: MiniLM+Lion TREC scores decline from E1→E3 (NDCG@10: 0.7031→0.6808) while MS MARCO MRR improves—indicates overfitting to training distribution
  - **Suboptimal LR/scheduler mismatch**: ModernBERT+AdamW with low LR (2e-6) + cosine performs worse (NDCG@10: 0.7105 at E1, declining) than ModernBERT+Lion with same config
  - **GTE+Lion underperformance**: Never reaches GTE+AdamW levels despite same hyperparameters—suggests architectural incompatibility

- First 3 experiments:
  1. **Reproduce ModernBERT+Lion best config**: Train with LR=2e-6, CosineAnnealingLR, batch=64, 3 epochs. Validate that E2 checkpoint achieves NDCG@10 ≈ 0.7225 on TREC DL 2019
  2. **Ablate learning rate for ModernBERT+Lion**: Test LR ∈ {1e-6, 2e-6, 5e-6, 1e-5} to verify 2e-6 is optimal or if gains exist
  3. **Test Lion on GTE with lower LR**: Since GTE+Lion underperformed at 2e-5, test whether lower LR (2e-6) with cosine annealing improves results, matching ModernBERT's successful configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a more extensive hyperparameter search, specifically involving different learning rates and schedules for Lion, yield performance gains for MiniLM and GTE models comparable to those observed in ModernBERT?
- Basis in paper: [explicit] The conclusion states future work should involve a "more thorough hyperparameter search... particularly exploring different learning rates and schedules for Lion with MiniLM and GTE."
- Why unresolved: This study used specific configurations (e.g., LR 2e-5 for MiniLM/GTE vs. 2e-6 for ModernBERT), leaving the optimal parameter space for Lion on the other models unexplored.
- What evidence would resolve it: Results from grid or random search experiments varying learning rates and schedulers for Lion on MiniLM and GTE.

### Open Question 2
- Question: To what extent does the extended 8K context length capability of models like GTE and ModernBERT improve reranking effectiveness on datasets containing significantly longer documents than those found in MS MARCO?
- Basis in paper: [explicit] The authors suggest "investigating the impact of the 8K context length more directly (e.g., on datasets with longer documents)" as a direction for future work.
- Why unresolved: The current experiments rely on MS MARCO passages and TREC DL queries, which may not utilize the full 8192-token capacity, making the benefits of long-context support unclear.
- What evidence would resolve it: Comparative evaluation on long-document IR benchmarks (e.g., TREC DL Track document ranking or LongEval) showing performance correlation with sequence length.

### Open Question 3
- Question: Do the GPU utilization efficiency gains achieved by the Lion optimizer persist across diverse hardware architectures (beyond NVIDIA L40S) and larger model scales?
- Basis in paper: [explicit] The paper notes that the "observed GPU efficiency gains with Lion optimizer warrant further investigation across different hardware configurations and model scales."
- Why unresolved: Efficiency metrics were isolated to a specific cloud setup (3x NVIDIA L40S), raising questions about generalizability to consumer hardware or larger cluster deployments.
- What evidence would resolve it: Benchmarking results showing training throughput and memory usage for Lion vs. AdamW on different GPU types (e.g., A100, H100) and larger model variants.

## Limitations

- **Optimizer-implementation fidelity**: The Lion optimizer implementation details are not fully specified beyond betas and weight decay, which could affect comparative results
- **Architecture-specific tuning ambiguity**: The hyperparameter selection process lacks systematic justification, making it unclear whether ModernBERT+Lion's success reflects optimizer choice or specific configuration
- **Limited generalizability**: Results based on a single passage retrieval dataset (MS MARCO) and two evaluation benchmarks may not extend to other IR tasks

## Confidence

- **High confidence**: Lion optimizer provides consistent GPU efficiency improvements (2.67-10.33%) across all three models
- **Medium confidence**: Optimizer effectiveness depends on model architecture, as evidenced by ModernBERT+Lion outperforming other combinations
- **Low confidence**: The claim that Lion's "reduced memory overhead translates to improved effectiveness" is only partially supported—Lion shows competitive performance but not universally superior results

## Next Checks

1. **Ablation study on ModernBERT hyperparameters**: Systematically test learning rates (1e-6, 2e-6, 5e-6, 1e-5) and scheduler configurations (constant LR, cosine annealing, step decay) for both Lion and AdamW to determine whether ModernBERT+Lion's success is due to the optimizer choice or the specific hyperparameter combination.

2. **Extended training duration analysis**: Train all configurations for 5-10 epochs to better understand convergence patterns and potential overfitting behaviors. This would clarify whether early stopping at 3 epochs captures peak performance or misses optimal checkpoints.

3. **Cross-dataset generalization test**: Evaluate the best-performing configurations (ModernBERT+Lion, MiniLM+AdamW) on a different retrieval dataset such as TREC-COVID or ArguAna to assess whether architecture-optimizer affinities hold across different domains and query types.