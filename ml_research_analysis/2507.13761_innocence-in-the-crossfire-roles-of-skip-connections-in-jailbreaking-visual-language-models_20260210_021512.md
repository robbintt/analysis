---
ver: rpa2
title: 'Innocence in the Crossfire: Roles of Skip Connections in Jailbreaking Visual
  Language Models'
arxiv_id: '2507.13761'
source_url: https://arxiv.org/abs/2507.13761
tags:
- toxic
- benign
- layer
- visual
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates jailbreaking vulnerabilities in visual
  language models (VLMs) by analyzing how prompt design influences inappropriate content
  generation. The authors focus on three key factors: visual descriptions, adversarial
  examples, and positively framed responses, examining their individual and combined
  effects.'
---

# Innocence in the Crossfire: Roles of Skip Connections in Jailbreaking Visual Language Models

## Quick Facts
- arXiv ID: 2507.13761
- Source URL: https://arxiv.org/abs/2507.13761
- Reference count: 40
- This paper investigates jailbreaking vulnerabilities in visual language models (VLMs) by analyzing how prompt design influences inappropriate content generation

## Executive Summary
This paper investigates jailbreaking vulnerabilities in visual language models (VLMs) by analyzing how prompt design influences inappropriate content generation. The authors focus on three key factors: visual descriptions, adversarial examples, and positively framed responses, examining their individual and combined effects. They propose SKIP-CON, a method that introduces skip connections between internal VLM layers to increase jailbreak success rates. Experiments on LLaVA-V, LLaVA-M, and MiniGPT4 show that each factor can independently trigger jailbreaks, with generalized-toxic in-context examples having the strongest impact. SKIP-CON improves attack success rates by 18%, 55%, and 26% respectively, even with benign images. The study also finds that harmful memes are as effective as toxic visuals in eliciting inappropriate outputs, highlighting the subtle vulnerabilities of VLMs to multimodal prompt manipulation.

## Method Summary
The study investigates jailbreaking of VLMs through three prompt factors: visual descriptions, adversarial in-context examples, and positive response framing. The authors propose SKIP-CON, which adds skip connections from layer i (where toxic/benign separation first emerges) to layer j (where it becomes pronounced) using a small scaling factor λ=0.01. They evaluate 2-ASR (two-stage Attack Success Rate) using denial phrase detection followed by Llama-Guard-3-1B safety classification. Experiments use 100 toxic queries from Beavertails dataset and benign queries from OpenAI API, with visual inputs including benign images, toxic images (5 categories), and harmful memes from Facebook Hate Meme dataset. The study tests LLaVA-V, LLaVA-M, and MiniGPT4 with various prompt configurations and k-shot examples.

## Key Results
- Each prompt factor (visual descriptions, adversarial examples, positive framing) can independently trigger jailbreaks
- Generalized-toxic in-context examples show the strongest jailbreak impact compared to self-reflective examples
- SKIP-CON improves attack success rates by 18% (LLaVA-V), 55% (LLaVA-M), and 26% (MiniGPT4)
- Harmful memes are as effective as toxic visuals in eliciting inappropriate outputs
- Benign images combined with appropriate prompt design can bypass safety measures

## Why This Works (Mechanism)
Jailbreaking succeeds because VLMs struggle to maintain safety distinctions in multimodal contexts. When text prompts contain toxic content, the model's internal representations become increasingly confused about safety boundaries. The skip connection mechanism exploits this by bypassing layers where the model typically learns to distinguish safe from unsafe content, effectively reinforcing the toxic patterns. The combination of visual descriptions, adversarial examples, and positive framing creates conflicting signals that overwhelm the model's safety mechanisms, while skip connections amplify these effects by preserving early-layer toxic representations through later processing stages.

## Foundational Learning
**VLMs and Multimodal Processing**: Visual language models integrate image and text understanding through cross-attention mechanisms. Why needed: Understanding how VLMs process multimodal inputs is crucial for analyzing jailbreak vulnerabilities. Quick check: Verify models use CLIP-like vision encoders and cross-attention for fusion.

**Jailbreaking Concepts**: Jailbreaking exploits model weaknesses to bypass safety restrictions. Why needed: The study builds on established jailbreaking techniques adapted for visual contexts. Quick check: Confirm understanding of one-stage vs. two-stage ASR metrics.

**Layer-wise Representation Analysis**: Examining how representations evolve across network layers reveals where models learn distinctions. Why needed: The skip connection strategy depends on identifying layers where toxic/benign separation occurs. Quick check: Understand t-SNE/PCA for visualizing high-dimensional representations.

**Safety Alignment in VLMs**: VLMs undergo alignment training to refuse harmful requests. Why needed: The study demonstrates how this alignment can be subverted through prompt engineering. Quick check: Review common refusal phrases and safety classifier behavior.

## Architecture Onboarding

**Component Map**: Input Images -> Vision Encoder -> Cross-Attention Layers -> Language Model -> Output Text. Critical path includes vision encoder, cross-attention fusion, and language generation.

**Critical Path**: Vision encoder extracts visual features → Cross-attention layers fuse visual and textual information → Language model generates response while maintaining safety constraints.

**Design Tradeoffs**: The skip connection approach trades safety for jailbreak effectiveness. Benefits include improved attack success rates; costs include potential model instability and safety degradation.

**Failure Signatures**: High 2-ASR indicates successful jailbreaks; denial phrase detection failures suggest safety mechanisms are bypassed; representation collapse in later layers indicates loss of safety distinctions.

**First Experiments**:
1. Probe intermediate layers of LLaVA-V to identify where toxic/benient token representations first separate and become pronounced
2. Implement complete 2-ASR pipeline with exact 13 denial phrases and Llama-Guard-3-1B configuration
3. Conduct ablation studies on λ scaling factor (0.01) in SKIP-CON

## Open Questions the Paper Calls Out

**Open Question 1**: Does increasing the number of visual inputs in a prompt alter the observed dominance of text over vision in jailbreaking scenarios? The conclusion states, "Given text's dominant influence over visuals, future work will explore multiple visual inputs to better assess their impact."

**Open Question 2**: Can the layer-wise distinguishability of toxic inputs be reinforced to prevent the "collapse" of safety features in multimodal settings? The paper observes that while unimodal inputs allow the model to distinguish toxic from benign inputs, this ability "significantly degrades" or collapses in multimodal contexts.

**Open Question 3**: Are the jailbreak vulnerabilities observed in LLaVA and MiniGPT4 transferable to proprietary, closed-source VLMs? The methodology relies on white-box access for SKIP-CON, limiting experiments to open-source models.

**Open Question 4**: How does the semantic relationship between the visual description and the in-context example influence the jailbreak success rate? The study analyzes "self-reflective" vs. "generalized-toxic" examples but doesn't fully isolate the effect of the description's semantic content relative to the examples.

## Limitations
- Skip connection layer indices (i, j) are not precisely specified beyond visual identification
- Exact prompt templates and in-context example formats are not provided
- Dimensionality reduction method for layer-wise probing is not specified
- Distribution of visual examples across subcategories is unclear

## Confidence
- Core findings about prompt factors: High
- Skip connection implementation details: Medium
- Claims about harmful memes effectiveness: Medium
- SKIP-CON mechanism universality: Low

## Next Checks
1. Probe intermediate layers of LLaVA-V to empirically identify where toxic/benign token representations first separate and become pronounced, then verify these match the paper's visualizations to determine skip connection layer indices.
2. Implement the complete 2-ASR pipeline with the exact 13 denial phrases and Llama-Guard-3-1B configuration, then measure baseline attack success rates across all 8 prompt configurations to confirm the reported relative effectiveness.
3. Conduct ablation studies on the λ scaling factor (0.01) in SKIP-CON to verify that skip connections to attention/MLP outputs specifically (not full layer outputs) are responsible for the reported improvements.