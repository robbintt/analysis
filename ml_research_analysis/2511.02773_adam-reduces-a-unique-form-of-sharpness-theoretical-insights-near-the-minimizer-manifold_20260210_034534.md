---
ver: rpa2
title: 'Adam Reduces a Unique Form of Sharpness: Theoretical Insights Near the Minimizer
  Manifold'
arxiv_id: '2511.02773'
source_url: https://arxiv.org/abs/2511.02773
tags:
- adam
- lemma
- implicit
- proof
- diag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how Adam reduces sharpness differently from
  SGD. It shows that Adam implicitly minimizes a unique form of sharpness measure
  shaped by its adaptive updates, leading to qualitatively different solutions.
---

# Adam Reduces a Unique Form of Sharpness: Theoretical Insights Near the Minimizer Manifold

## Quick Facts
- arXiv ID: 2511.02773
- Source URL: https://arxiv.org/abs/2511.02773
- Reference count: 40
- Primary result: Adam implicitly minimizes tr(Diag(H)^{1/2}) under label noise, while SGD minimizes tr(H), leading to qualitatively different solutions.

## Executive Summary
This paper provides a theoretical characterization of how Adam's adaptive updates lead to different implicit regularization compared to SGD near the minimizer manifold. Using a continuous-time approximation via stochastic differential equations, the authors rigorously show that Adam minimizes a unique sharpness measure—the trace of the square root of the Hessian's diagonal elements—under label noise conditions. This contrasts with SGD's minimization of the full trace of the Hessian. The work demonstrates that Adam's bias toward diagonal elements can improve sparsity in certain problems while potentially harming low-rank matrix recovery, offering insights for optimizer design.

## Method Summary
The paper analyzes Adam's behavior near the minimizer manifold using a Slow Stochastic Differential Equation framework extended to adaptive gradient methods. The key innovation is the preconditioner flow projection Φ_S that captures Adam's adaptive geometry. Under the specific condition of label noise where the gradient noise covariance is proportional to the Hessian (Σ(θ) ∝ H), the analysis derives that Adam implicitly minimizes tr(Diag(H)^{1/2}) while SGD minimizes tr(H). The theoretical framework is validated through experiments on sparse linear regression with diagonal networks and deep matrix factorization tasks, showing Adam's task-dependent performance.

## Key Results
- Adam implicitly minimizes tr(Diag(H)^{1/2}) while SGD minimizes tr(H) under label noise conditions
- Adam outperforms SGD in sparse linear regression tasks due to its diagonal-focused regularization
- Adam fails to find low-rank solutions in matrix factorization tasks where SGD succeeds
- AdamE-λ variants interpolate between Adam (λ=0.5) and SGD (λ=0) biases

## Why This Works (Mechanism)

### Mechanism 1: Slow SDE Projection on the Manifold
Adam's behavior near the optimum is governed by a "Slow Stochastic Differential Equation" (SDE) that tracks the drift of iterates along the minimizer manifold Γ over a long timescale (O(η⁻²) steps). The paper extends the Slow SDE framework to Adaptive Gradient Methods (AGMs) by defining a preconditioner flow projection Φ_S that separates fast convergence dynamics from slow implicit regularization dynamics.

### Mechanism 2: Label Noise Induced Regularizer tr(Diag(H)^{1/2})
Under label noise conditions where the gradient noise covariance Σ(θ) is proportional to the Hessian ∇²L(θ), Adam implicitly minimizes tr(Diag(H)^{1/2}). This specific covariance structure transforms the drift term in Adam's Slow SDE into a gradient flow targeting this diagonal-focused sharpness measure.

### Mechanism 3: Sparsity vs. Rank Structural Alignment
Adam's diagonal-focused regularizer promotes sparsity in diagonal networks but fails to induce low-rank solutions in deep matrix factorization. In diagonal networks, minimizing tr(Diag(H)^{1/2}) is geometrically equivalent to minimizing the ℓ₀.₅ norm, inducing sparsity. Conversely, low-rank recovery requires minimizing the full trace tr(H), which Adam's diagonal bias ignores.

## Foundational Learning

### Concept: Minimizer Manifold (Γ)
- **Why needed here:** The entire theoretical analysis depends on the assumption that global minima form a connected manifold, allowing analysis of "drift" dynamics after convergence.
- **Quick check question:** Are you analyzing properties of the solution at convergence (implicit bias) rather than just convergence rate?

### Concept: Label Noise Covariance
- **Why needed here:** The derivation of Adam's specific regularizer (tr(Diag(H)^{1/2})) hinges on the relationship Σ ∝ H. Without this, the SDE drift term takes a generic form.
- **Quick check question:** Is your experimental setup injecting noise into target labels, or just relying on stochastic batching?

### Concept: Sharpness Measures
- **Why needed here:** Understanding that "sharpness" is multi-faceted—trace of Hessian vs. diagonal terms vs. eigenvalues—is key to distinguishing SGD's bias from Adam's bias.
- **Quick check question:** Can you distinguish between minimizing the sum of eigenvalues (SGD) vs. the sum of square roots of diagonal elements (Adam)?

## Architecture Onboarding

### Component map: Slow SDE -> Preconditioner (S) -> Label Noise
### Critical path:
1. Verify the Manifold Assumption (smooth, connected minima)
2. Confirm Label Noise setup to activate the specific tr(Diag(H)^{1/2}) regularizer
3. Analyze Hessian Diagonality in your target problem to predict if Adam's bias will help (sparsity) or hurt (rank)

### Design tradeoffs: Adam trades the robust, rotation-invariant low-rank bias of SGD (driven by tr(H)) for a coordinate-wise adaptive bias that excels in sparse settings but struggles with low-rank matrix recovery.

### Failure signatures: High training accuracy but poor test accuracy in matrix factorization tasks; or conversely, unexpectedly superior sample efficiency in sparse regression tasks.

### First 3 experiments:
1. **Sparse Recovery:** Replicate the diagonal linear network experiment to confirm Adam recovers sparse ground truth with fewer samples than SGD.
2. **Rank Failure Case:** Replicate the deep matrix factorization experiment to verify Adam's inability to find low-rank solutions compared to SGD.
3. **AdamE Ablation:** Use AdamE optimizer to vary λ (interpolating between Adam λ=0.5 and SGD λ=0) to observe the shift in implicit bias on a controlled task.

## Open Questions the Paper Calls Out

### Open Question 1
Does the derived implicit regularizer for Adam hold when extended to the "1.5-scheme" regime (where 1-β₂ = O(η^{1.5})) or other scalings of the second moment parameter? The current framework is restricted to the "2-scheme" to successfully track the preconditioner's evolution over the long timescale required for the slow SDE.

### Open Question 2
What is the implicit bias of Adam when the optimization trajectory ventures beyond the local neighborhood of the smooth minimizer manifold? The current theoretical characterization relies on the assumption that iterates remain close to the manifold where the loss is small.

### Open Question 3
How does the inclusion of weight decay modify the effective sharpness regularizer in AdamW? The current theoretical model excludes weight decay terms to isolate the effect of adaptive updates, whereas practical implementations like AdamW rely on decoupled weight decay.

## Limitations
- Theoretical results hinge on specific assumptions about label noise structure (Σ ∝ H) and manifold geometry
- Analysis is restricted to convex or near-convex regimes, limiting direct applicability to highly non-convex neural networks
- The manifold assumption may not hold in general deep learning settings

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Adam minimizes tr(Diag(H)^{1/2}) under label noise conditions | High |
| Extension of Slow SDE framework to adaptive methods | Medium |
| Generalization to deep non-linear networks | Low |

## Next Checks

1. Test AdamE variants on the same sparse regression and matrix factorization tasks to verify the interpolation between Adam and SGD biases.
2. Evaluate the impact of different noise covariance structures (e.g., isotropic vs. Hessian-aligned) on Adam's implicit bias in controlled experiments.
3. Verify manifold assumption empirically by checking if minimizers of overparameterized models form a connected set in simple architectures.