---
ver: rpa2
title: A large-scale evaluation of commonsense knowledge in humans and large language
  models
arxiv_id: '2505.10309'
source_url: https://arxiv.org/abs/2505.10309
tags:
- statement
- human
- commonsensicality
- humans
- people
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new framework for evaluating commonsense
  knowledge in large language models (LLMs) by measuring their agreement with empirically
  observed human heterogeneity, rather than relying on ground-truth labels. The key
  idea is to treat both humans and LLMs as survey respondents who rate statements
  for agreement, then measure how well each model's ratings align with the majority
  human opinion.
---

# A large-scale evaluation of commonsense knowledge in humans and large language models

## Quick Facts
- arXiv ID: 2505.10309
- Source URL: https://arxiv.org/abs/2505.10309
- Reference count: 40
- Primary result: LLMs generally score below human median in individual commonsensicality, with smaller open-weight models outperforming larger proprietary ones

## Executive Summary
This paper introduces a novel framework for evaluating commonsense knowledge in large language models (LLMs) by measuring their agreement with human populations rather than relying on ground-truth labels. The framework treats both humans and LLMs as survey respondents who rate statements for agreement, then measures how well each model's ratings align with the majority human opinion. Two evaluation settings are used: individual-level commonsensicality (treating each LLM as an individual respondent) and statement-level commonsensicality (using each LLM to simulate hypothetical populations).

The key insight is that commonsense knowledge is inherently subjective and heterogeneous across human populations, making traditional ground-truth-based evaluations problematic. By using majority human agreement as the benchmark, the framework captures the distributed nature of commonsense knowledge and reveals significant differences between how humans and LLMs understand what constitutes "common sense." The results show that most LLMs fall below human median performance, with some surprising findings about model size and architecture effects.

## Method Summary
The evaluation framework operates by presenting both human participants and LLMs with a set of commonsense statements and collecting agreement ratings on a scale. For individual commonsensicality evaluation, each LLM is treated as a single survey respondent, and its ratings are compared to the majority opinion of human raters using correlation metrics. For population-level commonsensicality, each LLM generates a hypothetical population of "silicon samples" by running multiple inference passes with temperature sampling, creating a distribution of responses that is then compared to the observed human commonsense distribution.

The framework uses two correlation measures: Spearman correlation for individual-level evaluation (comparing ranked preferences) and Pearson correlation for population-level evaluation (comparing response distributions). The human reliability benchmark is established by measuring internal consistency across different human samples, providing a ceiling for how well any model could theoretically perform. The evaluation covers a diverse set of commonsense statements spanning social norms, physical reasoning, and everyday knowledge.

## Key Results
- Most LLMs score below the human median in individual commonsensicality, with only about one-third ranking above it
- Smaller open-weight models like Mistral-7B and Flan-T5-XXL are more competitive than larger proprietary models in individual commonsensicality
- LLM-generated populations correlate modestly (up to r = 0.43) with human commonsense distributions, well below human internal reliability (r = 0.60)
- Some models like Gemini Pro 1.0 show systematic differences in what they consider commonsensical compared to humans

## Why This Works (Mechanism)
The framework works by recognizing that commonsense knowledge is inherently distributed and heterogeneous across human populations rather than being a set of universal truths. By treating majority human agreement as the ground truth, the evaluation captures the statistical nature of commonsense knowledge while acknowledging that individual humans and models will disagree. The population simulation approach leverages the stochastic nature of LLM inference to generate response distributions that can be compared to observed human heterogeneity.

The mechanism relies on the assumption that commonsense knowledge emerges from shared experiences and social consensus rather than formal logical rules. This makes traditional ground-truth evaluations problematic, as they often conflate statistical popularity with actual commonsense validity. By measuring agreement with human populations directly, the framework sidesteps these issues and captures the distributed nature of commonsense reasoning across different cultural and individual contexts.

## Foundational Learning

**Human heterogeneity in commonsense knowledge**: Commonsense varies across individuals and cultures, making it a statistical rather than absolute property. Why needed: Without accounting for human variation, evaluations would impose artificial standards that don't reflect real-world commonsense diversity. Quick check: Compare agreement rates across different demographic groups on the same statements.

**Population simulation through stochastic sampling**: LLMs can generate response distributions by varying inference parameters like temperature. Why needed: Single-point predictions don't capture the uncertainty and variation present in human commonsense reasoning. Quick check: Verify that temperature scaling produces monotonically smoother distributions.

**Correlation vs. absolute agreement**: Measuring statistical alignment rather than exact match accounts for systematic differences in rating scales. Why needed: Different models may use rating scales differently while still capturing the same underlying commonsense patterns. Quick check: Compare correlation results with exact match metrics.

## Architecture Onboarding

**Component map**: Human raters -> Statement dataset -> Majority opinion calculation -> LLM individual evaluation -> LLM population simulation -> Correlation metrics -> Performance comparison

**Critical path**: The evaluation flow moves from human data collection to benchmark establishment, then through individual and population-level model evaluations, with correlation calculations at each stage feeding into the final performance assessment.

**Design tradeoffs**: The framework trades the simplicity of ground-truth evaluations for the realism of human-agreement benchmarks, accepting that this introduces some subjectivity but gains ecological validity. Population simulation adds computational overhead but provides richer distributional comparisons.

**Failure signatures**: Poor performance indicates either fundamental gaps in commonsense knowledge or systematic differences in what the model considers commonsensical. Individual-level failures suggest the model lacks basic understanding, while population-level failures indicate difficulty capturing human heterogeneity.

**First experiments**:
1. Run the framework on a small subset of statements with one LLM to verify correlation calculations work as expected
2. Compare human internal reliability across different statement types to identify which domains have highest consensus
3. Test the effect of temperature scaling on population simulation quality by varying temperature and measuring correlation stability

## Open Questions the Paper Calls Out
None

## Limitations
- The framework relies on majority human agreement as ground truth, which may conflate statistical consensus with actual commonsense validity
- The moderate correlation between LLM-generated populations and human commonsense distributions (r = 0.43) suggests significant gaps in model understanding
- The finding that smaller models outperform larger ones requires careful interpretation and may reflect evaluation artifacts rather than inherent capability differences

## Confidence
- **High confidence**: The methodological framework for evaluating commonsense knowledge through population agreement is sound and well-implemented
- **Medium confidence**: Comparative performance rankings between models, as these could be influenced by evaluation artifacts
- **Medium confidence**: Interpretation of individual vs. population-level commonsensicality differences, given the complex relationship between model size and performance

## Next Checks
1. Conduct ablation studies varying the number of human raters to establish stability of the majority opinion baseline and determine the minimum sample size needed for reliable consensus measurement
2. Test model performance on commonsense statements from diverse cultural contexts to assess whether the framework captures universal vs. culturally-specific commonsense knowledge
3. Implement cross-validation by training smaller diagnostic models to predict human ratings from LLM outputs, providing an independent measure of commonsense alignment beyond direct correlation metrics