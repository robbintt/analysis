---
ver: rpa2
title: An Evolutionary Framework for Automatic Optimization Benchmark Generation via
  Large Language Models
arxiv_id: '2601.12723'
source_url: https://arxiv.org/abs/2601.12723
tags:
- benchmark
- evolutionary
- problems
- benchmarks
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes LLM-EBG, a framework that uses a large language
  model (LLM) as a generative operator within an evolutionary algorithm to automatically
  create optimization benchmarks. By representing problems as symbolic expressions
  and guiding their evolution with fitness functions based on algorithm performance,
  LLM-EBG generates benchmarks that induce significant performance differences between
  a genetic algorithm (GA) and differential evolution (DE).
---

# An Evolutionary Framework for Automatic Optimization Benchmark Generation via Large Language Models

## Quick Facts
- arXiv ID: 2601.12723
- Source URL: https://arxiv.org/abs/2601.12723
- Authors: Yuhiro Ono; Tomohiro Harada; Yukiya Miura
- Reference count: 40
- Primary result: LLM-EBG generates optimization benchmarks that induce significant performance differences between GA and DE

## Executive Summary
This paper introduces LLM-EBG, a framework that uses a large language model (LLM) as a generative operator within an evolutionary algorithm to automatically create optimization benchmarks. The framework represents problems as symbolic expressions and evolves them to maximize performance differences between optimization algorithms. By leveraging the LLM's ability to generate valid mathematical expressions and combining this with fitness functions based on algorithm performance, LLM-EBG successfully creates benchmarks that favor different algorithms. The approach demonstrates that LLMs can effectively replace traditional genetic programming operators while discovering complex landscape features that exploit specific algorithmic weaknesses.

## Method Summary
LLM-EBG combines evolutionary algorithms with LLM-based generative operators to evolve optimization benchmarks. The framework starts with a seed function and uses Llama 3.3-70B to generate new expressions through crossover and mutation operations. Each candidate benchmark undergoes pre-validation with 1000 random samples to ensure numerical stability, then is evaluated by running both GA and DE for 20 trials each. The fitness function maximizes rank differences between algorithms while penalizing benchmarks that diverge to negative infinity. The evolution runs for 20 generations with a population of 10, using elitist selection to propagate the best-performing benchmarks.

## Key Results
- Generated benchmarks favoring GA in 8 out of 10 trials and favoring DE in 9 out of 10 trials
- GA-preferred benchmarks showed high sensitivity to variable scaling (ill-conditioning)
- DE-preferred benchmarks exhibited more moderate curvature and anisotropy
- Generated benchmarks clustered near BBOB functions with strong anisotropic properties in ELA feature space

## Why This Works (Mechanism)

### Mechanism 1: LLM as a Structural Genetic Operator
The framework treats LLMs as crossover and mutation operators that manipulate symbolic expressions rather than numerical vectors. Through few-shot prompting, the LLM combines structural components from parent expressions (crossover) or introduces new functional terms (mutation). The LLM leverages its pre-trained understanding of code syntax and mathematical logic to maintain validity while exploring the space of mathematical expressions.

### Mechanism 2: Feedback-Guided Landscape Shaping
The evolutionary process optimizes benchmarks to maximize performance discrepancy between two algorithms by assigning fitness based on rank differences. Selection pressure drives the benchmark toward geometric features where one algorithm succeeds while the other fails. This creates landscapes that exploit the comparative weaknesses of the "losing" algorithm.

### Mechanism 3: Implicit Feature Discovery
Evolution discovers complex, composite landscape features without explicit feature engineering by searching the space of symbolic expressions. The framework stumbles upon complex interactions (e.g., "sin(x) * sqrt(1 + x^2)") that create the necessary difficulty. Sobol' sensitivity analysis confirms these features differentiate variable importance.

## Foundational Learning

- **Differential Evolution (DE) vs. Genetic Algorithms (GA)**: Why needed - The fitness mechanism relies on understanding why these algorithms differ. GA maintains diversity via crossover/mutation rates, while DE uses difference vectors. Quick check - How does DE's mutation strategy react differently to a narrow, elongated valley compared to GA mutation?

- **Symbolic Regression / Genetic Programming (GP)**: Why needed - The paper replaces standard GP tree-structures with an LLM. Understanding GP helps recognize what the LLM is replacing: syntactically valid combination of primitives. Quick check - Why might an LLM be more flexible than standard GP subtree crossover when generating mathematical expressions?

- **Exploratory Landscape Analysis (ELA)**: Why needed - The paper uses ELA features to prove generated benchmarks are structurally distinct. Quick check - If two benchmarks look different as equations but have identical ELA features, would they represent the same difficulty for an optimizer?

## Architecture Onboarding

- **Component map**: Prompt Constructor -> LLM Engine -> Evaluator/Sandbox -> Solver Loop -> Fitness Scorer -> Selector
- **Critical path**: The Sandbox/Evaluator is the bottleneck, checking every generated expression for runtime errors and running two different algorithms 20 times each
- **Design tradeoffs**: Penalty weight Î±=10 balances avoiding "cheating" benchmarks that drift to negative infinity against rejecting valid but steep landscapes
- **Failure signatures**: Syntax errors from LLM, numerical instability causing NaN/Inf outputs, trivial solutions converging to penalty term
- **First 3 experiments**: 1) Zero-shot generation to check syntax and runtime error rates, 2) Ablation comparing LLM-Mutation vs. LLM-Crossover performance, 3) Algorithm pair swap (GA/DE for PSO/CMA-ES) to test framework generality

## Open Questions the Paper Calls Out

1. **Adaptation to constrained/multi-objective problems**: The framework currently only validates on unconstrained, single-objective problems. Can it be effectively extended to generate benchmarks for constrained or multi-objective optimization?

2. **Impact of initial population diversity**: How does the diversity of the initial population affect the structural novelty and convergence speed of generated benchmarks? The current method relies on a seed function and in-context learning which may induce structural homogeneity.

3. **LLM bias toward incremental expansion**: Does the LLM's preference for incremental term expansion over structural remapping hinder generation of benchmarks with fundamentally novel topology? It is unclear if this bias prevents discovery of benchmarks requiring radical structural changes.

## Limitations

- **Algorithm selection sensitivity**: Framework effectiveness depends on choosing algorithms with sufficiently distinct search behaviors, limiting generalizability to other algorithm pairs
- **LLM dependency and cost**: Substantial LLM calls required for each candidate benchmark create computational costs that may be prohibitive at commercial API rates
- **Inconsistent convergence patterns**: Evolutionary process shows inconsistent convergence, with some trials stagnating at suboptimal fitness values

## Confidence

- **High Confidence**: The mechanism of using LLM as generative operator within evolutionary search is technically sound and well-documented
- **Medium Confidence**: Landscape analysis connecting benchmark features to algorithm performance differences is supported by ELA analysis
- **Low Confidence**: Framework's generalizability to other algorithm pairs and problem domains remains untested

## Next Checks

1. **Cross-Algorithm Validation**: Test LLM-EBG with algorithm pairs that have less obvious performance differences (e.g., GA vs. CMA-ES) to validate whether it can discover nuanced landscape features

2. **Cost-Benefit Analysis**: Measure actual LLM API costs and inference times for generating benchmarks, comparing computational requirements against traditional GP-based approaches

3. **Feature Attribution Study**: Systematically remove individual mathematical operators from the allowed set and rerun evolution to quantify which primitive operations are essential for generating discriminating landscape features