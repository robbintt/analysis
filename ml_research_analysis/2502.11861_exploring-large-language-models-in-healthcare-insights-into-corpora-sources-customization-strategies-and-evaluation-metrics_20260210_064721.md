---
ver: rpa2
title: 'Exploring Large Language Models in Healthcare: Insights into Corpora Sources,
  Customization Strategies, and Evaluation Metrics'
arxiv_id: '2502.11861'
source_url: https://arxiv.org/abs/2502.11861
tags:
- llms
- evaluation
- studies
- metrics
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This scoping review identified significant gaps in the fairness
  and reliability of healthcare Large Language Models (LLMs). Most studies relied
  on unverified or unstructured data sources, such as web-crawled content and synthetic
  datasets, leading to potential biases from geographic, cultural, and socio-economic
  factors.
---

# Exploring Large Language Models in Healthcare: Insights into Corpora Sources, Customization Strategies, and Evaluation Metrics

## Quick Facts
- arXiv ID: 2502.11861
- Source URL: https://arxiv.org/abs/2502.11861
- Reference count: 40
- Primary result: Most healthcare LLM studies use unverified data sources, creating bias risks; future work needs tiered architectures, dynamic weighting, and comprehensive evaluation frameworks

## Executive Summary
This scoping review identifies critical gaps in healthcare LLM development, particularly around data source verification and evaluation standardization. The authors find that 90% of studies rely on unverified or unstructured data sources like web-crawled content and synthetic datasets, leading to geographic, cultural, and socio-economic biases. While combining multiple data sources improves performance, the lack of integration with evidence-based clinical guidelines remains problematic. The review proposes a future research direction centered on tiered corpus architectures with dynamic weighting and hybrid evaluation approaches combining automated metrics with expert assessment.

## Method Summary
The review systematically analyzed 40 studies on healthcare LLM development, categorizing data sources into four types: Literature (textbooks, guidelines), Open-source datasets (MedMCQA, PubMedQA), Real-world clinical resources (EHRs), and Web-crawled data. The authors identified common customization strategies including RAG (Retrieval-Augmented Generation), PEFT (parameter-efficient fine-tuning), and Prompt Engineering. For evaluation, they found current metrics focus on task-specific accuracy and F1 scores, lacking standardized frameworks for clinical applicability. The review proposes a "tiered corpus architecture" as the future standard, combining Level I (clinical guidelines) and Level II (EHRs/case reports) data with dynamic weighting.

## Key Results
- 90% of healthcare LLM studies use unverified or unstructured data sources, creating significant bias risks
- Combining multiple data sources improves model performance but requires integration with evidence-based clinical guidelines
- Current evaluation metrics are limited to task-specific accuracy and F1 scores, lacking comprehensive clinical safety and fairness assessments

## Why This Works (Mechanism)
The proposed framework works by addressing the fundamental tension between model performance and clinical reliability. By implementing tiered corpus architectures, models can prioritize high-evidence sources while maintaining broad knowledge coverage. Dynamic weighting allows the system to adjust emphasis based on clinical context and evidence quality. The hybrid evaluation approach ensures both quantitative performance and qualitative clinical safety are assessed, addressing the gap between statistical metrics and real-world clinical utility.

## Foundational Learning
- **Tiered Corpus Architecture**: Why needed - to prioritize evidence-based sources while maintaining broad coverage; Quick check - verify distinct quality levels between clinical guidelines and web-crawled data
- **Dynamic Weighting**: Why needed - to adjust source emphasis based on clinical context and evidence quality; Quick check - test weighting adjustments on different clinical scenarios
- **RAG (Retrieval-Augmented Generation)**: Why needed - to inject real-time guideline context and reduce hallucination; Quick check - measure reduction in hallucinated responses with RAG implementation
- **Hybrid Evaluation**: Why needed - to capture both quantitative performance and clinical safety; Quick check - compare automated metrics against expert clinical assessments
- **PEFT (Parameter-Efficient Fine-Tuning)**: Why needed - to adapt models efficiently without full retraining; Quick check - verify performance gains vs computational cost
- **Clinical Semantic Accuracy**: Why needed - to ensure medical facts are correctly understood and applied; Quick check - test model responses against clinical case studies

## Architecture Onboarding

**Component Map**: Level I Corpus (Guidelines) -> Dynamic Weighting Engine -> PEFT Fine-tuning -> RAG Pipeline -> Hybrid Evaluation

**Critical Path**: Data curation (tiered corpus) → Model adaptation (PEFT + RAG) → Comprehensive evaluation

**Design Tradeoffs**: High-evidence sources improve safety but may reduce general knowledge coverage; automated evaluation is scalable but misses nuanced clinical safety issues; parameter-efficient fine-tuning saves resources but may limit adaptation depth

**Failure Signatures**: High metric scores but low clinical utility; geographic/socio-economic bias in outputs; hallucination in critical medical information

**First Experiments**:
1. Build tiered corpus with Level I (clinical guidelines) and Level II (EHRs) sources, test baseline performance
2. Implement LoRA fine-tuning on curated corpus, measure parameter efficiency vs performance
3. Add RAG pipeline with clinical guidelines, evaluate hallucination reduction

## Open Questions the Paper Calls Out
None

## Limitations
- The proposed tiered corpus architecture and dynamic weighting scheme lack specific implementation details and mathematical formulations
- Most cited studies originate from US and Chinese contexts, potentially limiting generalizability to other healthcare systems
- The review is based on literature analysis rather than empirical validation, lacking standardized clinical assessment protocols

## Confidence
- **High confidence**: Data source verification as critical issue affecting fairness and reliability (well-supported by systematic review)
- **Medium confidence**: Hybrid evaluation combining automated metrics with expert assessment (reasonable but lacks specific protocols)
- **Low confidence**: Tiered corpus architecture with dynamic weighting as ideal framework (lacks implementation specifications)

## Next Checks
1. **Implementation Validation**: Develop and test a prototype tiered corpus with dynamic weighting using Level I (clinical guidelines) and Level II (EHRs) structure to assess practical feasibility
2. **Cross-Cultural Generalizability**: Evaluate the proposed framework on non-US, non-Chinese healthcare datasets to verify if identified biases and solutions apply across different healthcare systems
3. **Evaluation Protocol Development**: Create a standardized clinical evaluation protocol that operationalizes suggested expert assessment criteria (hallucination detection, safety assessment, reasoning quality) with specific rubrics and scoring mechanisms