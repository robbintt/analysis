---
ver: rpa2
title: What Drives Compositional Generalization in Visual Generative Models?
arxiv_id: '2510.03075'
source_url: https://arxiv.org/abs/2510.03075
tags:
- compositional
- generalization
- maskgit
- training
- continuous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates what drives compositional generalization
  in visual generative models. The authors conduct controlled experiments to isolate
  the effects of different design choices: tokenizer type (VAE vs VQ-VAE), training
  objective (continuous vs discrete), and conditioning information (full vs partial).'
---

# What Drives Compositional Generalization in Visual Generative Models?

## Quick Facts
- arXiv ID: 2510.03075
- Source URL: https://arxiv.org/abs/2510.03075
- Authors: Karim Farid; Rajat Sahay; Yumna Ali Alnaggar; Simon Schrodi; Volker Fischer; Cordelia Schmid; Thomas Brox
- Reference count: 40
- Primary result: Continuous training objectives and complete conditioning enable compositional generalization; JEPA augmentation recovers compositionality in discrete models.

## Executive Summary
This paper investigates the factors driving compositional generalization in visual generative models through controlled experiments. The authors systematically isolate the effects of tokenizer type, training objective (discrete vs continuous), and conditioning information completeness. They identify two critical factors: continuous distribution modeling in the training objective and complete, non-quantized conditioning information. To address discrete models' limitations, they propose augmenting MaskGIT with a JEPA-based continuous loss, which significantly improves compositional performance and induces more disentangled internal representations.

## Method Summary
The authors conduct controlled experiments on synthetic Shapes2D data with three binary factors (color, shape, size) creating 8 total compositions. They systematically vary tokenizer type (VAE vs VQ-VAE), training objective (continuous diffusion/MAR/GMM-NLL vs discrete MaskGIT), and conditioning information (full continuous vs quantized vs partial). They evaluate compositional generalization using level-1 (seen combinations) and level-2 (held-out combinations) metrics. The JEPA augmentation attaches continuous prediction losses to intermediate transformer layers using stop-gradient and EMA targets.

## Key Results
- Continuous objectives (diffusion, GMM-NLL) consistently outperform discrete objectives on held-out compositional combinations
- Complete conditioning information is necessary for compositional generalization; quantization or partial conditioning degrades performance
- JEPA-based continuous auxiliary loss significantly improves MaskGIT's compositional generalization and reduces polysemanticity in attention heads
- VAE vs VQ-VAE tokenizer choice has minimal impact on final compositional performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous distribution modeling enables compositional generalization in visual generative models.
- Mechanism: Continuous objectives preserve gradient information across the full latent space, allowing smooth interpolation between concept clusters. Discrete categorical objectives collapse nearby representations to the same token, breaking gradient flow needed for novel combinations.
- Core assumption: Compositional factors lie on continuous manifolds that are better traversed with continuous gradients.
- Evidence anchors:
  - [abstract]: "identify two key factors: (i) whether the training objective operates on a discrete or continuous distribution"
  - [section 4, Figure 3]: "Models that learn continuous distributions (DiT, MAR, and GIVT) consistently show better level-2 compositions than MaskGIT, with the decisive shift in performance occurring at the categorical-to-continuous intervention"
  - [corpus]: Weak direct support; "Does Data Scaling Lead to Visual Compositional Generalization?" examines scaling but not objective type.
- Break condition: If concepts are inherently discrete with no continuous structure, continuous objectives may not help.

### Mechanism 2
- Claim: Full, non-quantized conditioning information is necessary for reliable compositional generalization.
- Mechanism: When conditioning is quantized ("red" vs exact RGB) or partially dropped during training, the model learns entangled or incomplete factor representations. At inference, novel combinations require precise factor specifications that the model never learned to use independently.
- Core assumption: Compositional factors are independent and need to be separately specified during training.
- Evidence anchors:
  - [abstract]: "(ii) to what extent conditioning provides information about the constituent concepts during training"
  - [section 5, Figure 4]: "Combining both conditions–a setting common in practice–produces the strongest negative effect"
  - [corpus]: No direct corpus support for conditioning granularity.
- Break condition: If downstream tasks only require coarse-grained generation, quantized conditioning may suffice.

### Mechanism 3
- Claim: JEPA-based auxiliary objectives can recover compositional abilities in discrete models by structuring internal representations.
- Mechanism: The JEPA loss operates on continuous intermediate hidden states, encouraging the model to learn predictable, factor-aligned representations even when the output space is discrete. This reduces polysemanticity in attention heads and creates more factor-specific circuits.
- Core assumption: Disentangled internal representations transfer to better compositional generation even with discrete outputs.
- Evidence anchors:
  - [abstract]: "augmenting MaskGIT's training objective with a JEPA-based continuous loss, which significantly improves its compositional performance"
  - [section 6.2, Figure 6]: "fewer attention heads are polysemantic in MaskGIT with the auxiliary continuous loss"
  - [corpus]: No corpus support for JEPA specifically in generative contexts.
- Break condition: If the discrete tokenizer itself destroys factor structure (e.g., tokens span multiple concepts), auxiliary losses cannot fully recover compositionality.

## Foundational Learning

- Concept: Discrete vs Continuous Latent Spaces
  - Why needed here: The core finding hinges on why continuous latents generalize compositionally while discrete tokens often fail.
  - Quick check question: Can you explain why VQ-VAE codebook indices might prevent gradient-based interpolation between concepts?

- Concept: Compositional Generalization Evaluation
  - Why needed here: The paper uses level-1/level-2 held-out combinations as the key metric; understanding this setup is essential for reproducing results.
  - Quick check question: What distinguishes a level-2 composition from a level-1 composition in this evaluation framework?

- Concept: JEPA / Self-Supervised Representation Alignment
  - Why needed here: The proposed intervention uses JEPA-style prediction on intermediate representations; familiarity with stop-gradient and EMA targets helps understand why this works.
  - Quick check question: Why does the JEPA predictor use a stop-gradient on the target representation instead of direct supervision?

## Architecture Onboarding

- Component map: VAE/VQ-VAE tokenizer → Transformer backbone (DiT/MAR/MaskGIT) → Conditioning via adaLN. JEPA predictor attaches to intermediate transformer layers (e.g., layers 7, 9, 11) with separate MLP per layer.

- Critical path: Start with controlled Shapes2D experiments (3 binary factors, 8 compositions). Train on 4, hold out 4. Verify tokenizer ablation first (VAE vs VQ-VAE should show similar final performance), then objective ablation (continuous vs discrete is the decisive factor).

- Design tradeoffs: Continuous models (DiT) generalize better but sample slower. Discrete models (MaskGIT) are faster but need JEPA augmentation. Full conditioning is ideal but often unavailable in practice—expect degraded compositionality with partial information.

- Failure signatures:
  - MaskGIT generates correct factors but assigns them to wrong objects (color-shape binding failures).
  - Partial conditioning causes regression to nearest training example (e.g., novel "blue big triangle" renders as "red big triangle").
  - Discrete models show high polysemanticity—same attention head responds to multiple unrelated factors.

- First 3 experiments:
  1. Replicate DiT vs MaskGIT comparison on Shapes2D with 4 train / 4 held-out compositions. Measure linear probe accuracy on generated samples.
  2. Ablate conditioning: train DiT with (a) full continuous, (b) quantized, (c) 10% dropout per factor. Compare level-2 performance.
  3. Add JEPA auxiliary loss to MaskGIT: apply at layers {7, 9, 11}, λ=0.6, use EMA target with stop-gradient. Verify polysemanticity reduction via head ablation analysis.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to synthetic Shapes2D dataset with three binary factors; generalization to complex real-world data is uncertain
- Evaluation relies on linear probe accuracy which may not capture perceptual quality or diversity
- JEPA augmentation effectiveness on higher-dimensional, real-world data is untested
- Implementation specificity (layer choices, λ=0.6) may not generalize across model scales

## Confidence
- High Confidence: Continuous Objectives (clean experimental isolation, consistent advantage across ablations)
- Medium Confidence: Full Conditioning (well-designed but effect size depends on specific quantization scheme)
- Medium Confidence: JEPA Augmentation (promising but limited to one model and dataset, causal link not definitively established)

## Next Checks
1. **Cross-Dataset Generalization**: Test continuous-vs-discrete objective effect on complex compositional dataset (CLEVR or multi-object scenes) with hierarchical structure. Measure performance gap persistence and JEPA scaling effectiveness.

2. **Factor Complexity Ablation**: Systematically vary number of compositional factors (2-10) and cardinality (binary vs multi-valued) across synthetic and real-world datasets to identify when continuous objectives become essential.

3. **Representation Analysis Extension**: Beyond polysemanticity, analyze learned representations using representational similarity analysis (RSA) or factorized component analysis to quantify how well different training objectives separate compositional factors in latent space. Compare against ground-truth factor alignments.