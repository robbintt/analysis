---
ver: rpa2
title: 'SemSegBench & DetecBench: Benchmarking Reliability and Generalization Beyond
  Classification'
arxiv_id: '2505.18015'
source_url: https://arxiv.org/abs/2505.18015
tags:
- miou
- attacks
- methods
- segmentation
- corruptions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SEMSEGBENCH and DETEC BENCH, the first comprehensive
  benchmarking tools for evaluating reliability and generalization of semantic segmentation
  and object detection models beyond classification. The authors benchmark 76 segmentation
  models across four datasets and 61 object detectors across two datasets, evaluating
  their performance under diverse adversarial attacks and common corruptions.
---

# SemSegBench & DetecBench: Benchmarking Reliability and Generalization Beyond Classification

## Quick Facts
- **arXiv ID**: 2505.18015
- **Source URL**: https://arxiv.org/abs/2505.18015
- **Reference count**: 40
- **Primary result**: First comprehensive benchmarks for reliability and generalization of semantic segmentation and object detection models, revealing systematic weaknesses and better OOD robustness in transformer backbones.

## Executive Summary
This paper introduces SEMSEGBENCH and DETEC BENCH, comprehensive benchmarking frameworks for evaluating reliability (adversarial robustness) and generalization (out-of-distribution robustness) in semantic segmentation and object detection beyond standard classification metrics. The authors benchmark 76 segmentation models across four datasets and 61 object detectors across two datasets, systematically evaluating performance under diverse adversarial attacks and common corruptions. Their analysis reveals that state-of-the-art models show significant vulnerabilities, with no strong correlation between in-distribution accuracy and robustness. Notably, transformer-based backbones demonstrate superior out-of-distribution robustness compared to CNN-based backbones. The benchmarks provide 6,139 pre-computed evaluations with multiple metrics, enabling immediate analysis without re-computation. The work establishes that synthetic image corruptions can serve as reliable proxies for real-world distribution shifts, providing a foundation for improving model reliability in safety-critical applications.

## Method Summary
The benchmarks evaluate pre-trained models from mmsegmentation (for segmentation) and mmdetection (for detection) using standardized threat models. For adversarial attacks, they use PGD, SegPGD, and CosPGD for segmentation with ℓ∞-norm (ε=8/255, 20 iterations, α=0.01) and ℓ2-norm (ε=64, α=0.1), and BIM and PGD for detection with similar parameters. For out-of-distribution evaluation, they apply 15 types of 2D Common Corruptions (severity 3) to segmentation models and both 2D and 3D Common Corruptions to detection models. They introduce two key metrics: Reliability Measure (ReM), the worst-case performance under adversarial attacks, and Generalization Ability Measure (GAM), the worst-case performance across corruptions. The evaluation pipeline loads models, applies threat models to validation images, computes task-specific metrics (mIoU for segmentation, mAP for detection), and optionally aggregates them into ReM and GAM.

## Key Results
- Transformer-based backbones demonstrate significantly better OOD robustness than CNN-based backbones across both segmentation and detection tasks
- No strong correlation exists between i.i.d. performance and adversarial reliability or OOD generalization ability, especially under strong ℓ∞ attacks
- Synthetic image corruptions serve as reliable proxies for real-world distribution shifts, with Pearson correlation of 0.885 between 2D corruptions and ACDC dataset performance
- Standard ℓ∞-norm attacks with ε=8/255 cause almost complete failure for most semantic segmentation methods
- The benchmarks provide 6,139 pre-computed evaluations, enabling immediate analysis without re-computation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Transformer-based backbones empirically show better out-of-distribution (OOD) generalization than CNN-based backbones for both segmentation and detection.
- **Mechanism**: The inductive biases of vision transformers (e.g., global self-attention, hierarchical feature aggregation) may capture more robust, shape-oriented features less sensitive to texture or local corruption patterns.
- **Core assumption**: The evaluated models are representative and differences in training procedures do not confound the backbone comparison.
- **Evidence anchors**: [abstract] "Notably, transformer-based backbones demonstrate better OOD robustness than CNN-based ones." [section 4.1.2] "We observe in Fig. 2b that models with transformer-based backbones have significantly better generalization abilities... [citing] transformer-based models are inherently more OOD robust."

### Mechanism 2
- **Claim**: Standard i.i.d. performance metrics (e.g., mIoU, mAP) are weakly correlated with adversarial reliability and OOD generalization, especially under strong ℓ∞ attacks.
- **Mechanism**: Architectures optimized for i.i.d. accuracy may overfit to dataset-specific patterns that do not transfer to adversarial or corrupted distributions; robustness requires dedicated architectural or training interventions.
- **Core assumption**: The attacks (PGD, SegPGD, CosPGD) and corruptions (2D/3D Common Corruptions) adequately proxy worst-case and natural distribution shifts.
- **Evidence anchors**: [abstract] "Their analysis reveals systematic weaknesses... showing no strong correlation between i.i.d. performance and reliability or generalization ability." [section 4.1.1] "Typical ℓ∞-norm attacks with ε = 8/255... cause almost all semantic segmentation methods to completely fail."

### Mechanism 3
- **Claim**: Synthetic image corruptions (2D/3D Common Corruptions) can serve as a practical proxy for real-world distribution shifts, reducing need for expensive real-world data collection.
- **Mechanism**: Synthetic corruptions simulate noise, blur, weather, and digital artifacts that approximate the visual variations encountered in real environments (e.g., ACDC dataset).
- **Core assumption**: The correlation between synthetic and real-world corruption performance holds across diverse scenes and sensor characteristics.
- **Evidence anchors**: [abstract] "The work demonstrates that synthetic image corruptions can serve as reliable proxies for real-world distribution shifts." [appendix A.1] "We observe a very strong positive correlation in performance against ACDC and mean performance across all 2D Common Corruptions (Pearson: 0.885)."

## Foundational Learning

- **Concept: Adversarial Robustness (White-box Attacks)**
  - Why needed here: The Reliability Measure (ReM) quantifies worst-case performance under iterative attacks (PGD, SegPGD, CosPGD). Understanding attack formulation (ε-budget, norm, iterations) is essential to interpret ReM.
  - Quick check question: Can you explain the difference between ℓ∞ and ℓ2 norm-bounded attacks and why ℓ∞ attacks are more destructive for these models?

- **Concept: Out-of-Distribution (OOD) Generalization**
  - Why needed here: The Generalization Ability Measure (GAM) uses common corruptions to evaluate robustness to distribution shifts. This concept underpins the entire OOD evaluation.
  - Quick check question: What is the "Generalization Ability Measure (GAM)" and how does it aggregate performance across corruptions?

- **Concept: Task-Specific Metrics (mIoU, mAP)**
  - Why needed here: The benchmarks compute mIoU for segmentation and mAP for detection as base metrics, then aggregate them into ReM and GAM. Without understanding these, results are uninterpretable.
  - Quick check question: Why might a model have high mIoU on clean data but low ReM under adversarial attack?

## Architecture Onboarding

- **Component map**: mmsegmentation/ -> Model loading -> Threat model application -> Metric computation -> ReM/GAM aggregation
- **Critical path**: 1) Select model from supported zoo 2) Configure threat model parameters 3) Run evaluation via API 4) Retrieve or compute mIoU/mAP and optionally aggregate for ReM/GAM
- **Design tradeoffs**:
  - Computational cost vs. exhaustive evaluation: Authors limit attack iterations to 20 and corruption severity to 3 to manage cost; more iterations or higher severity may reveal further degradation
  - Generic vs. task-specific attacks: DETECBENCH uses generic attacks (BIM, PGD) for fairness across architectures, potentially missing task-specific vulnerabilities
  - Synthetic vs. real corruptions: SEMSEGBENCH includes both; DETECBENCH is synthetic-only. Real-world datasets (e.g., ACDC) are used to validate proxy quality
- **Failure signatures**:
  - Adversarial collapse: Near-zero ReM under ℓ∞ attacks indicates extreme vulnerability
  - OOD collapse: Very low GAM (e.g., <5 mAP for detection) indicates almost no generalization to common corruptions
  - Backbone discrepancy: High i.i.d. performance but low GAM for CNN backbones vs. transformer backbones highlights architectural influence
- **First 3 experiments**:
  1. Baseline characterization: Evaluate 3-5 diverse models (e.g., DeepLabV3+ with ResNet50, Mask2Former with Swin-Base, SegFormer with MIT-B2) on ADE20K under clean, PGD (ℓ∞, ε=8/255), and 2D Common Corruptions (severity=3) to observe failure patterns
  2. Backbone comparison: For a fixed architecture (e.g., Mask2Former), compare CNN (ResNet50) vs. transformer (Swin-Base) backbones on reliability and GAM to empirically verify the trend
  3. Proxy validation: For 3-5 Cityscapes-trained models, correlate performance on ACDC (real) vs. 2D Common Corruptions (synthetic) to assess proxy reliability, following Appendix A.1

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Do object detection-specific adversarial attacks reveal failure modes in DETECBENCH that are not captured by the general-purpose attacks (BIM, PGD) currently used?
- **Basis in paper**: [explicit] The authors state in the Limitations and Future Work that their choice of general-purpose attacks "may overlook task-specific vulnerabilities" and that "expanding to include dedicated object detection attacks is a key direction for future work."
- **Why unresolved**: The current benchmark prioritizes consistency across architectures using general ℓp-bounded attacks, but it has not yet evaluated how models respond to attacks specifically designed to exploit detection mechanisms (e.g., attacking bounding box regression or class confidence separately).
- **What evidence would resolve it**: A comparative analysis showing whether model rankings in DETECBENCH change significantly when evaluated against detection-specific attacks (like those cited in [39, 48]) versus the standard PGD/BIM attacks.

### Open Question 2
- **Question**: How does the reliability and generalization of semantic segmentation models change when evaluated against 3D Common Corruptions?
- **Basis in paper**: [explicit] In the Future Work section, the authors explicitly state: "We also plan to integrate 3D Common Corruptions [4] for more realistic OOD evaluations in SEMSEGBENCH."
- **Why unresolved**: While DETECBENCH already includes 3D corruptions, SEMSEGBENCH currently relies on 2D corruptions and real-world datasets like ACDC; it is unknown if the strong correlation found between 2D synthetic and real-world corruptions holds for 3D synthetic corruptions in segmentation tasks.
- **What evidence would resolve it**: Benchmarking the SEMSEGBENCH model suite against 3D Common Corruptions and analyzing the correlation (Pearson coefficients) between these new Generalization Ability Measures (GAM) and existing 2D metrics.

### Open Question 3
- **Question**: To what extent can direct integration of adversarial training frameworks into SEMSEGBENCH and DETECBENCH close the gap between i.i.d. performance and reliability?
- **Basis in paper**: [explicit] The authors note in Future Work that they "aim to support benchmarking for adversarial training methods directly within the frameworks" to complement the evaluation of pretrained models.
- **Why unresolved**: The current work evaluates existing checkpoints (mostly standard training), revealing a lack of reliability. It does not yet provide a standardized workflow to test if robust training methods (e.g., TRADES, adversarial fine-tuning) can mitigate these vulnerabilities within the benchmark's constraints.
- **What evidence would resolve it**: Implementation of adversarial training recipes in the benchmark tools followed by a comparison of the resulting models' Reliability Measures (ReM) against their standard-trained counterparts.

## Limitations

- Analysis is restricted to specific datasets (ADE20K, Cityscapes, PASCAL VOC, MS-COCO) and model architectures from mmsegmentation and mmdetection, limiting generalizability
- Choice of attack parameters (ε=8/255 for ℓ∞, 20 iterations) and corruption severity 3 may not capture the full robustness landscape
- DETEC BENCH benchmark lacks real-world corruption evaluation, potentially limiting proxy validation for detection tasks

## Confidence

- **High**: Transformer backbones show better OOD generalization (supported by direct empirical results in multiple figures and datasets)
- **Medium**: Synthetic corruptions serve as reliable proxies for real-world shifts (supported by strong correlation with ACDC, but limited to segmentation)
- **Medium**: Weak correlation between i.i.d. and robust performance (supported by benchmarking results, but dataset and model specific)

## Next Checks

1. Evaluate the same model comparisons on additional datasets (e.g., BDD100K, Mapillary) to verify backbone trends hold across domains
2. Test correlation between synthetic and real corruptions for object detection using available real-world datasets
3. Vary attack parameters (higher ε, more iterations) to assess if current parameter choices underestimate vulnerability