---
ver: rpa2
title: DeGLIF for Label Noise Robust Node Classification using GNNs
arxiv_id: '2506.00244'
source_url: https://arxiv.org/abs/2506.00244
tags:
- noise
- deglif
- noisy
- node
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeGLIF addresses label noise in node classification for graph data
  by proposing a novel denoising technique based on leave-one-out influence function.
  The method leverages a small set of clean nodes to approximate the impact of removing
  training nodes on validation loss, enabling identification of noisy nodes.
---

# DeGLIF for Label Noise Robust Node Classification using GNNs

## Quick Facts
- **arXiv ID:** 2506.00244
- **Source URL:** https://arxiv.org/abs/2506.00244
- **Reference count:** 40
- **Primary result:** Achieves up to 17.9% higher accuracy than baselines under label noise

## Executive Summary
DeGLIF addresses label noise in graph node classification by leveraging a small clean validation set to identify and relabel noisy training nodes. The method uses leave-one-out influence functions to estimate the impact of removing each training node on validation loss, identifying noisy nodes without requiring prior knowledge of noise level or model architecture. Through iterative application, DeGLIF progressively reduces noise in training data, achieving significant accuracy improvements over existing baselines on Cora, Citeseer, and Amazon Photo datasets under symmetric and pairwise label noise.

## Method Summary
DeGLIF works by first training a standard GNN (Model-1) on the noisy dataset, then using a small clean validation set to compute leave-one-out influence functions that estimate how removing each training node would affect validation loss. Nodes whose removal would decrease validation loss are flagged as noisy. These nodes are then relabeled to the class with highest predicted probability (excluding their current label) based on Model-1's predictions. The denoised dataset is used to train a final GNN (Model-2). The process can be applied iteratively to progressively clean the data.

## Key Results
- Up to 17.9% higher accuracy than baselines under symmetric and pairwise label noise
- Consistent performance across Cora, Citeseer, and Amazon Photo datasets
- Progressive noise reduction through successive applications (3-5 iterations)
- Works without prior knowledge of noise level or model architecture

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DeGLIF identifies noisy nodes by estimating if removing a specific training node decreases the loss on a small, clean validation set.
- **Mechanism:** The system calculates the leave-one-out influence $I_{up}(-z, v_i)$ for training nodes $z$ against clean validation nodes $v_i$. If the estimated change in validation loss is negative for a majority of clean nodes (DeGLIF(mv)) or the aggregate sum is positive (DeGLIF(sum)), the node $z$ is flagged as noisy.
- **Core assumption:** The small clean set $D_c$ is representative of the true distribution, and the Hessian of the loss function is invertible.
- **Evidence anchors:** [abstract] "approximate the change in validation loss, if a training node is removed"; [section 3.1] Equation 5 defines $I_{up}(-z, v_i)$; the condition for flagging noise is defined for DeGLIF(mv) and DeGLIF(sum).

### Mechanism 2
- **Claim:** Relabeling noisy nodes to the highest probability class (excluding the current label) is theoretically safer than simply discarding them.
- **Mechanism:** Once a node $z$ is identified as noisy, it is relabeled to the class $k \neq m$ that maximizes the model's predicted probability $f(z)_k$. The paper proves (Theorem 2) that this relabeling strategy leads to a lower risk on the clean set than training on the dataset with the noisy nodes removed entirely.
- **Core assumption:** The model Model-1, trained on noisy data, retains enough signal to rank the correct label higher than the noisy label, even if the top prediction is wrong.
- **Evidence anchors:** [section 3.2] "Theorem 2... relabelling a node is equivalent to removing that node and adding back the node with a new label."; [section 5.1] Table 4 shows high accuracy (79%) in relabeling correctly at low noise.

### Mechanism 3
- **Claim:** Successive applications of DeGLIF progressively reduce the noise ratio in the training data.
- **Mechanism:** By treating the output of one DeGLIF pass as the input for the next, the method iteratively cleans the dataset. The relabeling of high-confidence errors reduces the propagation of error signals through the graph structure in subsequent training rounds.
- **Core assumption:** The rate of correct relabeling exceeds the rate of error introduction.
- **Evidence anchors:** [abstract] "reduces noise in training data through successive applications."; [section 5.3] Figure 2 shows the fraction of noisy nodes dropping consistently over 3-5 counts (iterations).

## Foundational Learning

- **Concept:** Leave-One-Out Influence Function
  - **Why needed here:** This is the mathematical engine of the paper. It allows the estimation of parameter changes (and thus loss changes) without the prohibitive cost of retraining the GNN for every single node removal.
  - **Quick check question:** Can you explain why calculating the influence function for GNNs requires accounting for the change in adjacency matrix (edges) when a node is removed, unlike standard i.i.d. data?

- **Concept:** Graph Neural Networks (GNNs) & Message Passing
  - **Why needed here:** The paper exploits the structure of GNNs. Noise propagates through the graph via message passing; understanding this explains why local label noise degrades performance globally and why influence must track edge changes.
  - **Quick check question:** If you remove a "bridge" node between two clusters, how does that affect the representation of nodes in the disconnected cluster?

- **Concept:** Hessian Inversion and Regularization
  - **Why needed here:** The influence calculation requires the inverse of the Hessian matrix $H_\theta$. If this matrix is not positive definite or invertible, the mechanism fails.
  - **Quick check question:** Why does the damping parameter (often used in influence function implementations) matter for numerical stability in GNNs?

## Architecture Onboarding

- **Component map:** Input (Noisy Dataset $D$, Clean Set $D_c$) -> Model-1 (GNN trained on $D$) -> Influence Estimator (computes $I_{up}(-z, v_i)$) -> Detector (applies DeGLIF logic) -> Relabeler (assigns new labels) -> Model-2 (GNN trained on denoised $D^*$)

- **Critical path:** The computation of the Hessian Inverse for the Influence Estimator. This is the most computationally expensive step and dictates scalability.

- **Design tradeoffs:**
  - **Model-1 Complexity:** Simpler models (fewer parameters) make Hessian inversion faster and more stable but might provide weaker probability estimates for relabeling.
  - **$D_c$ Size:** Larger $D_c$ improves detection stability but increases the cost of calculating influence pairs ($O(|D| \times |D_c|)$).
  - **Method Choice:** DeGLIF(sum) generally outperforms DeGLIF(mv) as it utilizes magnitude information, not just signs.

- **Failure signatures:**
  - **Singular Matrix:** If the loss landscape is flat, Hessian inversion fails.
  - **Over-flipping:** If hyperparameter $\lambda$ is too low or $\mu$ too small, clean nodes are erroneously flipped, degrading Model-2 performance.
  - **Stagnation:** At very high noise (>50%), Model-1 may fail to learn anything useful, making the influence estimates noise.

- **First 3 experiments:**
  1. **Sanity Check (Synthetic Noise):** Inject 20% symmetric noise into Cora. Run one pass of DeGLIF. Verify that "detected noisy nodes" overlap significantly with the artificially flipped nodes.
  2. **Ablation on $D_c$:** Fix noise at 30%. Vary the size of $D_c$ (e.g., 10, 50, 100 nodes). Plot the accuracy of Model-2 vs. $D_c$ size to find the minimal viable clean set.
  3. **Iterative Convergence:** Run DeGLIF 5 times sequentially on a high-noise (40%) dataset. Plot the "Fraction of Noisy Nodes" after each pass to see if noise reduces or plateaus.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Reliance on a small clean validation set whose quality directly impacts detection accuracy
- Computationally expensive Hessian inversion step that becomes unstable for large graphs or deep architectures
- Assumes Model-1's probability estimates remain meaningful even after significant noise injection
- Does not address label noise that creates misleading graph structures affecting message passing

## Confidence
- **High Confidence:** The mathematical formulation of leave-one-out influence function and its application to identify noisy nodes. The empirical results showing performance gains over baselines are well-documented with statistical significance.
- **Medium Confidence:** The theoretical justification for relabeling and its practical effectiveness, as the proof relies on assumptions about probability estimates that may not hold in practice. The iterative cleaning mechanism shows consistent improvement but lacks theoretical guarantees about convergence.
- **Low Confidence:** The generalizability to datasets with different homophily properties or when the clean set cannot be obtained through validation data sampling.

## Next Checks
1. **Influence Function Stability:** Test the influence estimation across different Hessian regularization parameters on Cora with 30% noise to identify the stability threshold.
2. **Extreme Noise Robustness:** Evaluate DeGLIF's performance on 60-70% noise levels to determine the breaking point where Model-1's probability estimates become unreliable.
3. **Clean Set Sensitivity:** Systematically vary the size of $D_c$ (10, 25, 50, 100 nodes) on Citeseer with 25% noise to quantify the minimum viable clean set for stable detection.