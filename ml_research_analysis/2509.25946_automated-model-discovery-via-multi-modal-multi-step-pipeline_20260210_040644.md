---
ver: rpa2
title: Automated Model Discovery via Multi-modal & Multi-step Pipeline
arxiv_id: '2509.25946'
source_url: https://arxiv.org/abs/2509.25946
tags:
- data
- analyzervlm
- kernel
- discovery
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a multi-modal and multi-step automated model
  discovery pipeline that leverages vision-language models for effective model proposal
  and evaluation. The core method introduces AnalyzerVLM for iterative multi-step
  analysis of data and models, and EvaluatorVLM for visual-based model assessment
  using a proposed Visual Information Criterion (VIC).
---

# Automated Model Discovery via Multi-modal & Multi-step Pipeline

## Quick Facts
- arXiv ID: 2509.25946
- Source URL: https://arxiv.org/abs/2509.25946
- Reference count: 40
- Primary result: Automated model discovery pipeline achieves superior RMSE performance on real-world univariate datasets through multi-modal visual and text analysis

## Executive Summary
This paper presents a novel automated model discovery pipeline that leverages vision-language models (VLMs) for iterative multi-step analysis and evaluation of candidate models. The approach introduces AnalyzerVLM for autonomous multi-step analysis and EvaluatorVLM for visual-based model assessment using a proposed Visual Information Criterion (VIC). Experimental results demonstrate consistently lower RMSE values compared to five competing methods on seven real-world univariate datasets, with particular effectiveness in capturing fine-grained details and ensuring strong generalizability.

## Method Summary
The pipeline operates through a multi-round process where AnalyzerVLM autonomously plans and executes multi-step analyses on data, generating candidate models that are then evaluated by EvaluatorVLM using visual fitness and generalizability scores combined with BIC. The system uses a basis kernel space (LIN, PER, SE, C, WN) with composition operations (+, ×), optimizing parameters through marginal likelihood with random restarts. The VIC metric balances visual assessment (α=50 for GP discovery) with traditional BIC to penalize overfitting while capturing generalization behavior.

## Key Results
- Consistently lower RMSE values compared to five competing methods on all seven tested datasets
- Multi-step analysis demonstrates improved model discovery with reduced MSE over single-step approaches
- Visual fitness and generalizability scores show strong correlations with normalized log-likelihood and generalization gap respectively
- The pipeline effectively captures fine-grained details and ensures strong generalizability across training and test regions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-step iterative analysis enables better model proposals than single-step generation
- Mechanism: AnalyzerVLM sequentially alternates between natural language reasoning, code execution, and model proposal actions, building contextual understanding before committing to candidates
- Core assumption: VLMs can effectively plan analysis sequences and recognize when sufficient information has been gathered
- Evidence anchors: Additional analysis steps lead to better models (Fig. 5 shows MSE reduction with multi-step)
- Break condition: If VLM fails to recognize when analysis is sufficient, it may loop indefinitely or propose prematurely

### Mechanism 2
- Claim: Visual Information Criterion (VIC) captures generalization signals that pure likelihood-based metrics miss
- Mechanism: EvaluatorVLM scores visual fitness (prediction-data similarity, uncertainty region size) and visual generalizability (structural consistency in extrapolated regions), combined with BIC via weighted sum
- Core assumption: Human visual judgment of model plots correlates with actual generalization performance
- Evidence anchors: Visual fitness correlates with normalized log-likelihood (Spearman ρ=0.7995); visual generalizability correlates with generalization gap (ρ=0.777)
- Break condition: If visual assessment doesn't actually correlate with generalization, VIC provides no benefit over BIC alone

### Mechanism 3
- Claim: Multi-modal (visual + text) representation improves data understanding compared to text-only
- Mechanism: AnalyzerVLM and EvaluatorVLM process plot visualizations alongside numerical/text data, enabling perception of overall trends rather than local point-wise focus
- Core assumption: VLMs can extract meaningful structural information from 2D plots that informs model decisions
- Evidence anchors: Restricting to text-only representation increases MSE across datasets (Fig. 3)
- Break condition: If visualization quality is poor or VLM visual perception is unreliable, this degrades to noise

## Foundational Learning

- Concept: Gaussian Process kernel composition
  - Why needed here: The model space is defined as compositions of basis kernels (LIN, PER, SE, C, WN) via operations (+, ×)
  - Quick check question: Given a dataset with linear trend and yearly seasonality, what kernel composition would you expect?

- Concept: Vision-Language Models as agents
  - Why needed here: AnalyzerVLM operates as an agent with action space (analyze/execute/propose)
  - Quick check question: What distinguishes a VLM agent from a standard VLM inference call?

- Concept: Bayesian Information Criterion (BIC)
  - Why needed here: VIC modifies BIC by adding visual priors
  - Quick check question: Why does BIC sometimes favor models that generalize poorly?

## Architecture Onboarding

- Component map: Dataset D → AnalyzerVLM (multi-step analysis + code execution) → Model candidates Mr → Parameter Optimization → Fitted models → EvaluatorVLM (visual fitness + generalizability) + BIC computation → VIC scores → Model Pool (top-k selection) → Loop back to AnalyzerVLM (R rounds)

- Critical path:
  1. AnalyzerVLM's first analysis step (visualization quality determines downstream understanding)
  2. Parameter optimization initialization (AnalyzerVLM-proposed vs random; Fig A13 shows sensitivity)
  3. EvaluatorVLM's visual assessment (corrupts VIC if visual perception fails)
  4. Model pool selection threshold (too aggressive discards promising candidates)

- Design tradeoffs:
  - VLM choice (GPT-4o vs mini vs Qwen2.5-VL): Higher capability → better analysis, higher cost/latency
  - α parameter (BIC vs visual weight balance): Paper uses α=50 for GP, 0.05 for symbolic regression; tuning required per domain
  - Number of analysis steps: More steps improve proposals but increase latency (Fig. 4 shows LLM needs more steps than VLM)
  - Evaluation repetitions: Averaging multiple EvaluatorVLM scores reduces stochasticity but multiplies API calls

- Failure signatures:
  - AnalyzerVLM loops without proposing: Check max context length (Nmax) or add step limit
  - VIC selects overfit models: α may be too low; visual generalizability not penalizing boundary behavior
  - Discovered models ignore periodicity: AnalyzerVLM may not be generating frequency analysis code; check code execution environment
  - Test MSE increases across rounds (Fig. 8): BIC-only selection active; verify EvaluatorVLM is being called

- First 3 experiments:
  1. Reproduce ablation in Table 2 on a single dataset (Airline recommended): Test 4 configurations (±AnalyzerVLM, ±EvaluatorVLM) to validate component contributions
  2. Vary α parameter and plot train vs test MSE: Identify the sweet spot where visual generalizability penalty prevents overfitting without rejecting good fits
  3. Single-step vs multi-step comparison (Fig. 5 replication): Restrict AnalyzerVLM to 1 step, measure MSE degradation to quantify analysis depth value

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the multi-modal pipeline be adapted to effectively handle multivariate datasets where visualizing variable relationships is non-trivial?
- Basis in paper: [explicit] The authors state in the Limitations section that the current pipeline focuses on 1D datasets and suggest "future work should include extending this to multivariate data to capture relationships between variables."
- Why unresolved: The current EvaluatorVLM relies on 2D posterior visualizations which do not scale intuitively to high-dimensional spaces, and the AnalyzerVLM's code generation for visualization is tailored to univariate plotting libraries.
- What evidence would resolve it: Demonstration of the pipeline successfully identifying models for multivariate regression benchmarks with performance comparable to its univariate results.

### Open Question 2
- Question: How can the system ensure robustness against variations in the quality and style of input visualizations used for model evaluation?
- Basis in paper: [explicit] The authors acknowledge that "pipeline’s performance may depend on the quality of input visualizations" and identify "searching for the good quality of input visualizations that fits to VLM" as a necessary future direction.
- Why unresolved: VLMs are known to be sensitive to visual artifacts (e.g., axis scaling, color, noise), yet the pipeline currently uses fixed plotting code which may not be optimal for the VLM's perceptual capabilities.
- What evidence would resolve it: A systematic ablation showing that the EvaluatorVLM's scoring remains consistent across varying visualization styles or an adaptive plotting module that optimizes visual clarity for the VLM.

### Open Question 3
- Question: Is there a principled, automated method to determine the weighting hyperparameter α in the Visual Information Criterion (VIC) to replace manual tuning?
- Basis in paper: [inferred] Appendix A.3 reveals that the weighting α is manually set to 50 for Gaussian Process discovery and 0.05 for Symbolic Regression
- Why unresolved: The necessity to manually retune α for different tasks undermines the goal of a fully "automated" model discovery pipeline
- What evidence would resolve it: The development of a self-tuning mechanism for α that maintains competitive performance across both GP and SR tasks without human intervention.

## Limitations
- Limited to univariate datasets, with scalability to multivariate settings unclear
- Performance depends on VLM's visual interpretation capabilities which aren't rigorously benchmarked
- Manual tuning required for α parameter balancing visual and numerical criteria
- No systematic validation of VLM visual perception reliability across varying visualization qualities

## Confidence
- High confidence in experimental methodology and results presentation for tested datasets
- Medium confidence in VIC metric's general applicability beyond studied domains
- Low confidence in scalability claims without testing on more diverse problem types

## Next Checks
1. Conduct ablation studies testing VIC performance across diverse data distributions (non-periodic, non-Gaussian) to validate general applicability
2. Benchmark AnalyzerVLM's multi-step reasoning capability against established multi-step reasoning benchmarks to quantify effectiveness
3. Test the pipeline on multivariate datasets to assess scalability beyond univariate GP kernel discovery