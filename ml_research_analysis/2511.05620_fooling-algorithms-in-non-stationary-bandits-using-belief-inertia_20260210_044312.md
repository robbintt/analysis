---
ver: rpa2
title: Fooling Algorithms in Non-Stationary Bandits using Belief Inertia
arxiv_id: '2511.05620'
source_url: https://arxiv.org/abs/2511.05620
tags:
- regret
- algorithm
- algorithms
- non-stationary
- change
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new method, called belief inertia, for\
  \ deriving worst-case lower bounds in non-stationary multi-armed bandits. Unlike\
  \ prior infrequent sampling arguments, the approach exploits how algorithms\u2019\
  \ empirical beliefs\u2014encoded via historical reward averages\u2014resist updating\
  \ even after environment changes."
---

# Fooling Algorithms in Non-Stationary Bandits using Belief Inertia

## Quick Facts
- arXiv ID: 2511.05620
- Source URL: https://arxiv.org/abs/2511.05620
- Reference count: 3
- Key outcome: Introduces belief inertia to prove linear worst-case regret for classical algorithms in non-stationary bandits, even with a single change point.

## Executive Summary
This paper introduces belief inertia as a new tool for deriving worst-case lower bounds in non-stationary multi-armed bandits. The core insight is that algorithms relying on historical reward averages can become locked into suboptimal arms after an environment change, as their empirical beliefs resist updating. This mechanism enables adversarial constructions that cause classical algorithms (ETC, ε-greedy, UCB) to incur linear regret with substantial constant factors, regardless of parameter tuning. The approach also shows that even periodically restarted versions of these algorithms suffer linear regret when changes are frequent.

## Method Summary
The paper analyzes non-stationary multi-armed bandits with piecewise-stationary reward distributions and at most Γ_T breakpoints. It constructs adversarial instances where reward distributions change at a single breakpoint t₀, switching from (0,0,...,0,1) to (1,0,...,0) or similar configurations. The analysis demonstrates that classical algorithms' empirical belief estimates become anchored to pre-change rewards, preventing timely adaptation to the new optimal arm. This belief inertia causes the algorithms to incur linear regret Ω(T) despite the presence of exploration or confidence bonuses.

## Key Results
- Classical algorithms (ETC, ε-greedy, UCB) suffer linear worst-case regret O(T) in single-change environments via belief inertia, regardless of parameter tuning
- Periodically restarted algorithms still incur linear regret when changes are frequent, with d-dependent constants
- The belief inertia argument provides a general framework for establishing sharp lower bounds in time-varying bandit settings

## Why This Works (Mechanism)

### Mechanism 1: Empirical Average Anchoring
Historical reward averages create momentum that resists new evidence after environment changes. An algorithm's empirical mean estimate for each arm accumulates evidence over time, and once the sample count is large, new observations have diminishing influence. Even if the true reward distribution changes dramatically, the empirical average remains anchored near its pre-change value for many rounds.

### Mechanism 2: Confidence Interval Shrinkage (UCB-Specific)
UCB's shrinking confidence bonuses prevent detection of distribution shifts on heavily-sampled arms. After extensive pre-change sampling, the confidence bonus becomes negligible, and post-change, even if the empirical mean shifts, the now-optimistic index of an under-sampled arm must overcome both the anchored empirical mean and the small remaining bonus.

### Mechanism 3: Exploitation Dominance Over Sparse Exploration
Even persistent but low-rate exploration cannot overcome belief inertia before incurring linear regret. ε-greedy explores at rate ε/K per arm, and if belief inertia keeps a suboptimal arm's empirical mean higher than the changed optimal arm, exploitation selects the wrong arm. The geometric waiting time until discovering the new optimal arm has expectation O(K/ε), during which regret accumulates linearly.

## Foundational Learning

- **Regret in Multi-Armed Bandits**: The cumulative loss from not always playing the optimal arm. Why needed: The entire paper quantifies worst-case lower bounds on regret. Quick check: If an algorithm always picks arm 2 when arm 1 has expected reward 0.1 higher, what is the regret after 1000 rounds?
- **UCB Index Construction and Optimism Principle**: The UCB index combines empirical mean and confidence bonus. Why needed: The adversarial construction against UCB relies on manipulating the confidence bonus. Quick check: After 100 pulls of arm A with empirical mean 0.5 and horizon T=10000, what is arm A's UCB index?
- **Piecewise-Stationary (Switching) Bandits**: The model assumes Γ_T breakpoints where reward distributions can change. Why needed: Understanding this model is essential to grasp why classical algorithms fail. Quick check: In a 1000-round horizon with Γ_T=3 breakpoints, what is the maximum number of stationary intervals?

## Architecture Onboarding

- **Component map**: Empirical mean tracker → Sample counter → Index calculator → Selection policy → (Optional) Restart mechanism
- **Critical path**: (1) Pre-change phase samples arms evenly → (2) Empirical means converge with small confidence radii → (3) Adversary switches reward distribution at breakpoint → (4) One unlucky post-change pull inflates wrong arm's index → (5) Belief inertia keeps wrong arm selected for Ω(T) rounds → (6) Linear regret accumulates
- **Design tradeoffs**: Restart frequency d trades belief inertia reduction against stationary regret (Theorem 4 shows √d cost). Forgetting mechanisms (discounting/sliding windows) trade memory for responsiveness. Exploration rate ε trades stationary regret against post-change recovery time.
- **Failure signatures**: Linear regret growth (0.07T to 0.125T in single-change environment), single arm dominance post-change (>90% selection despite reward changes), confidence radii < 0.01 for all arms.
- **First 3 experiments**: 1) Replicate ETC adversarial instance (K=2, m=25, rewards (0,1)→(1,0) at t=21) and verify regret ≈ 450. 2) Replicate UCB adversarial instance (K=2, rewards (0,0)→(0.38,1) at t=191) and measure fraction of runs where arm 1 is never selected post-breakpoint. 3) Test restart sensitivity by running periodically-restarted UCB with d∈{1,5,10,50} on UCB adversarial instance and plot regret vs. d.

## Open Questions the Paper Calls Out

### Open Question 1
Can belief inertia arguments establish lower bounds for algorithms with active forgetting mechanisms (sliding-window UCB, discounted UCB)? The paper notes this as future work, suggesting that even with active forgetting, belief inertia persists whenever past data retain enough influence.

### Open Question 2
Are Thompson Sampling-based algorithms for non-stationary bandits vulnerable to belief inertia attacks? Bayesian posteriors encode beliefs differently than empirical means, and the inertia mechanism may not directly apply to posterior sampling methods.

### Open Question 3
Is there a fundamental trade-off between robustness to belief inertia and worst-case stationary performance? While Theorem 4 quantifies restart costs, the paper doesn't characterize if other adaptation mechanisms face similar fundamental limits.

## Limitations

- The belief inertia mechanism relies critically on unweighted empirical averages and time-invariant confidence bounds, potentially breakable by sliding-window or discounted variants (acknowledged as open question)
- Adversarial instances use deterministic rewards with careful parameter tuning (e.g., 0.38 reward in UCB example), raising questions about robustness to stochastic perturbations
- The framework focuses on classical algorithms; extending to more sophisticated bandit methods requires additional analysis

## Confidence

- **High Confidence**: The core belief inertia mechanism and its role in locking algorithms into suboptimal arms after a change point is well-supported by adversarial constructions and regret proofs for ETC, ε-greedy, and UCB
- **Medium Confidence**: The claim that periodically restarted algorithms still incur linear worst-case regret for frequent changes is analytically sound but depends on specific restart frequency tradeoffs
- **Medium Confidence**: The assertion that sliding-window or discounted methods might not fully escape belief inertia is reasonable but remains unproven

## Next Checks

1. **Stochastic Robustness Test**: Re-run adversarial instances with small Gaussian noise added to rewards. Verify that belief inertia still causes linear regret growth despite noise, and quantify the degradation in the constant factor.
2. **Adaptive Confidence Bounds Experiment**: Implement a UCB variant with a minimum confidence bonus floor (clamp √(2lnT/n_k) ≥ c). Test on the UCB adversarial instance to see if this breaks the inertia lock.
3. **Sliding-Window Comparison**: Implement sliding-window UCB (window size w) and compare its regret on adversarial instances to fixed-window UCB. Determine whether any w choice avoids linear regret, or if belief inertia persists.