---
ver: rpa2
title: '"As Eastern Powers, I will veto." : An Investigation of Nation-level Bias
  of Large Language Models in International Relations'
arxiv_id: '2511.10695'
source_url: https://arxiv.org/abs/2511.10695
tags:
- bias
- nation
- unsc
- united
- russia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study systematically examines nation-level biases in Large
  Language Models (LLMs) applied to International Relations (IR) tasks. Using real-world
  UNSC voting records, the authors developed a multi-faceted evaluation framework
  comprising three distinct tests: Direct Question-Answering, Association Tests, and
  Persona-Assigned Vote Simulations.'
---

# "As Eastern Powers, I will veto." : An Investigation of Nation-level Bias of Large Language Models in International Relations

## Quick Facts
- arXiv ID: 2511.10695
- Source URL: https://arxiv.org/abs/2511.10695
- Authors: Jonghyeon Choi; Yeonjun Choi; Hyun-chul Kim; Beakcheol Jang
- Reference count: 40
- Key outcome: Systematic examination of nation-level biases in LLMs applied to International Relations tasks using UNSC voting records and multi-faceted evaluation framework

## Executive Summary
This study systematically examines nation-level biases in Large Language Models (LLMs) applied to International Relations (IR) tasks. Using real-world UNSC voting records, the authors developed a multi-faceted evaluation framework comprising three distinct tests: Direct Question-Answering, Association Tests, and Persona-Assigned Vote Simulations. Results show consistent positive bias toward the U.K., France, and U.S., and negative bias toward Russia across models, though patterns vary by model and task. Notably, bias within a single model can change depending on the evaluation context. The study also finds that models with stronger reasoning abilities exhibit reduced bias and improved performance. To address this, the authors propose a debiasing framework combining Retrieval-Augmented Generation with Reflexion-based self-reflection, which significantly reduces nation-level bias and improves performance, particularly for GPT-4o-mini and Llama-3.3-70B. These findings underscore the importance of assessing nation-level bias alongside performance when deploying LLMs in IR applications.

## Method Summary
The study develops a comprehensive evaluation framework using UNSC voting records from 2013-2024, testing three methods: Direct Question-Answering (forcing choice between P5 nations), Association Tests (ranking nations by responsibility), and Persona-Assigned Vote Simulations (role-playing as national representatives). The debiasing approach combines Retrieval-Augmented Generation with Reflexion-based self-reflection, using historical voting records and speeches to ground responses and enable self-correction. The framework is tested on multiple models including GPT-4o-mini, Llama-3.3-70B, Mistral-22B, and Qwen-2.5-72B.

## Key Results
- Consistent positive bias toward the U.K., France, and U.S., and negative bias toward Russia across all tested models
- Bias patterns vary significantly by evaluation method, with models showing different preferences in DirectQA versus Persona-Assigned simulations
- Models with stronger reasoning capabilities (o3-mini, DeepSeek-R1) exhibit reduced bias and improved performance
- RAG + Reflexion framework significantly reduces nation-level bias and improves performance for GPT-4o-mini and Llama-3.3-70B
- Weaker long-context models (Mistral, Qwen) show performance degradation when using the debiasing framework due to prompt length limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Forced-choice direct prompting surfaces latent model preferences that open-ended tasks may obscure.
- Mechanism: The DirectQA test bypasses implicit reasoning by constraining the output space (e.g., "Which country is more irresponsible?"), forcing the model to select from a limited set and reveal preferences.
- Core assumption: The selection is a genuine preference and not random or positional bias.
- Evidence anchors:
  - [abstract] "Results reveal consistent positive biases toward the U.K., France, and the U.S., and negative biases toward Russia across all models, with variations by model and task."
  - [section] Section 4.1 describes the DirectQA test: "In Direct Question-Answer(DirectQA) Test, we directly ask LLM which of the P5 is more irresponsible."
  - [corpus] Corpus papers on LLM bias detection (e.g., "Fine-Grained Bias Detection in LLM") support forced-choice probing.
- Break condition: The model consistently refuses or defaults to neutral, indicating strong safety alignment.

### Mechanism 2
- Claim: Persona adoption elicits behavioral biases aligned with stereotypical national roles.
- Mechanism: Assigning a national persona ("You are the representative of China") in the Persona-Assigned Vote Simulation activates internal associations with that nation's perceived geopolitical interests, producing voting patterns that may diverge from historical records.
- Core assumption: The model's internal associations contain actionable information about national behavior that persona prompts can reliably elicit.
- Evidence anchors:
  - [abstract] The study introduces "Persona-Assigned Vote Simulationâ€”to assess explicit and implicit biases..."
  - [section] Section 4.2: "...LLM is prompted to adopt the persona of a specific nation's representative and to vote on a given resolution..."
  - [corpus] "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs" (corpus) supports persona-elicited bias.
- Break condition: Persona-driven voting perfectly matches ground truth, indicating learned historical data without bias.

### Mechanism 3
- Claim: A RAG + Reflexion framework reduces bias by grounding outputs in external evidence and enabling self-correction.
- Mechanism: RAG injects factual historical voting records and speeches. Reflexion prompts the model to critique its preliminary judgment against this evidence, creating a feedback loop aligning reasoning with ground truth.
- Core assumption: The provided historical evidence is sufficient for correction, and the model is capable of effective self-critique.
- Evidence anchors:
  - [abstract] "...we introduce a debiasing framework that improves LLMs' factual reasoning combining Retrieval-Augmented Generation with Reflexion-based self-reflection techniques."
  - [section] Section 7 details: "...retriever identifies thematically similar past resolutions. The LLM conducts rehearsal votes and performs self-reflection, comparing its choices with actual votes."
  - [corpus] Corpus papers (e.g., "Beyond Consensus") support multi-step debiasing strategies.
- Break condition: Retrieved context is irrelevant, contradictory, or too long, causing ineffective incorporation. Section 7 notes performance degradation in Mistral and Qwen likely due to prompt length.

## Foundational Learning

- Concept: **Explicit vs. Implicit Bias**
  - Why needed here: The evaluation framework distinguishes what the model says directly from what it infers when role-playing. This distinction is critical for interpreting results and designing interventions.
  - Quick check question: If a model refuses to answer a question like "Who is more irresponsible?" does that mean it is unbiased? (Hint: No, it may still show implicit bias).

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The debiasing framework's core component is RAG. Understanding that RAG works by retrieving relevant documents and feeding them to the LLM as context is essential.
  - Quick check question: What are the two main steps in a standard RAG process? (Hint: Retrieval and Generation).

- Concept: **Persona-Based Prompting and In-Context Learning**
  - Why needed here: Methodology uses persona-based prompting for implicit bias. Debiasing uses in-context learning by providing historical examples to guide predictions.
  - Quick check question: How does providing examples in a prompt (in-context learning) change an LLM's behavior without changing its weights? (Hint: It conditions the model's probability distribution over tokens based on patterns in the provided examples).

## Architecture Onboarding

- Component map:
  - **Evaluation Framework**: `DirectQA`, `AssociationTest`, `VoteSimulation`.
  - **Dataset**: `UNSC Resolution Data` (2013-2024), including `Resolutions`, `Votes`, `Speeches`.
  - **Debiasing Module**: `Custom Retriever` (keyword-based), `RAG Prompt Constructor`, `Reflexion Loop`, `Final Vote Generator`.

- Critical path: Main workflow for debiasing:
  1.  **Input**: Target UNSC resolution (`rtgt`) with context (`Ctgt`).
  2.  **Retrieval**: `Custom Retriever` queries historical database for similar resolutions (`Rlista`, `Rlistn`) based on date, keywords, regions.
  3.  **Rehearsal & Reflexion**: For each retrieved resolution (`ri`), `LLM` predicts (`vi_hat`), then performs `Reflexion` comparing `vi_hat` to ground truth `vi` and speech `Speechi` to generate reflection (`reflexi`).
  4.  **Final Prediction**: Final prompt is built with target context and accumulated history of predictions, truths, reflections (`Hk`). `LLM` makes final vote.

- Design tradeoffs:
  - **Prompt Length vs. Detail**: More retrieved resolutions (`k`) provides context but can exceed effective context window, degrading performance. Ablation suggests `k=1` is optimal.
  - **Flexibility vs. Consistency**: `Custom Retriever` uses keyword matching for speed/interpretability, potentially missing nuanced thematic similarity.
  - **Component Complexity**: Prompt-engineering only, no parameter tuning. Easy to apply but adds inference cost/latency.

- Failure signatures:
  - **Context Window Overload**: Weaker long-context models (e.g., Mistral, Qwen) show performance *degradation* after framework application, likely due to prompt length.
  - **Refusal or Neutrality**: Some models often refuse or remain neutral in DirectQA, reducing signal for explicit bias evaluation.
  - **Inconsistent Rationale**: In Association Test, models may provide internally inconsistent rationales for rankings (Appendix F.2), which must be discarded.

- First 3 experiments:
  1.  **Baseline Bias Evaluation**: Run `DirectQA`, `AssociationTest`, `VoteSimulation` on target LLMs (`GPT-4o-mini`, `Llama-3.3-70B`, `Mistral-22B`, `Qwen-2.5-72B`) using baseline prompt. Measure irresponsibility scores, ATS scores, and weighted F1.
  2.  **Ablation on Retriever and Reflexion**: Apply debiasing framework on GPT-4o-mini and Llama-3.3-70B. Run controlled ablation: a) `k=1` + Reflexion, b) `k=1` w/o Reflexion, c) `k=2` + Reflexion. Measure individual contributions of Reflexion and retrieval depth.
  3.  **Generalization Test on Reasoning Models**: Run `VoteSimulation` on advanced reasoning models (`o3-mini`, `DeepSeek-R1`) without framework to test hypothesis that enhanced reasoning reduces bias. Compare F1 scores to baseline and RAG-enhanced non-reasoning models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a standardized "unbiased status" be established for explicit bias evaluations (DirectQA and Association Tests) where ground truth is subjective or absent?
- Basis in paper: [explicit] The "Limitations and Future Work" section states, "Future work will focus on setting and justifying a reasonable unbiased status for evaluation" for tests lacking objective benchmarks.
- Why unresolved: The authors note that for DirectQA and Association Tests, "objective, unbiased status for each nation is not feasible," rendering quantitative bias measurement difficult compared to the Vote Simulation which has voting records.
- What evidence would resolve it: The development of an expert-annotated dataset or a consensus methodology that defines "neutral" or "accurate" national characteristics for specific IR contexts.

### Open Question 2
- Question: To what extent does the performance degradation observed in Mistral and Qwen using the RAG-Reflexion framework stem from long-context comprehension limits versus the noise of retrieved precedents?
- Basis in paper: [inferred] In Section 7, the authors note performance drops for Mistral and Qwen, speculating that "excessive information may approach or exceed the effective processing capacity of certain LLMs," but they do not isolate the exact cause.
- Why unresolved: The paper identifies the symptom (performance drop) and a potential cause (context length), but does not disentangle context processing limits from the model's ability to utilize reflexive reasoning effectively.
- What evidence would resolve it: An ablation study specifically varying context window lengths and retrieval densities for Mistral and Qwen to identify the tipping point of comprehension failure.

### Open Question 3
- Question: Do the "Western favorable / Russian unfavorable" bias patterns identified in P5 members generalize to non-permanent UNSC members or other geopolitical entities?
- Basis in paper: [inferred] The study focuses exclusively on P5 members; Section 3 notes that non-permanent members were excluded due to having significantly fewer voting records (10 on average vs 66).
- Why unresolved: It is undetermined if the observed biases are specific to the high-profile geopolitical framing of the P5 or if they represent a generalized "West vs. East" bias present in the training data for all nations.
- What evidence would resolve it: Applying the Persona-Assigned Vote Simulation framework to a broader dataset of non-permanent members or regional bodies (e.g., EU or ASEAN resolutions).

## Limitations
- The RAG + Reflexion debiasing framework shows variable effectiveness across different LLM architectures, with some models (Mistral, Qwen) experiencing performance degradation due to prompt length limitations
- The study focuses exclusively on P5 UNSC members, limiting generalizability to non-permanent members or other geopolitical entities
- Historical voting records are assumed to represent "correct" behavior, potentially reinforcing biases present in actual diplomatic voting patterns

## Confidence
- **High Confidence**: Identification of consistent nation-level biases across multiple evaluation methods is well-supported by systematic methodology
- **Medium Confidence**: Claim that stronger reasoning models exhibit reduced bias is supported by comparative analysis, though causal mechanisms need further establishment
- **Medium Confidence**: RAG + Reflexion framework effectiveness demonstrated for specific models but mixed results across architectures reduce universal applicability confidence

## Next Checks
1. **Cross-temporal validation**: Test the evaluation framework and debiasing approach on UNSC voting data from earlier periods (e.g., 1990s-2000s) to assess whether the identified biases and framework effectiveness generalize across different geopolitical eras

2. **Multi-modal bias assessment**: Extend the evaluation framework to incorporate non-textual data (visual representations, speech audio) to determine whether nation-level biases manifest similarly across different modalities

3. **Long-context model optimization**: Conduct targeted experiments to optimize the RAG + Reflexion framework for long-context models like Mistral and Qwen, potentially through context summarization or selective retrieval