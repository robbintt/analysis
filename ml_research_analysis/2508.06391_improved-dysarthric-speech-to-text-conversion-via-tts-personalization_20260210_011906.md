---
ver: rpa2
title: Improved Dysarthric Speech to Text Conversion via TTS Personalization
arxiv_id: '2508.06391'
source_url: https://arxiv.org/abs/2508.06391
tags:
- speech
- dysarthric
- data
- train-dys
- speaker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a case study on developing a personalized speech-to-text
  system for a Hungarian speaker with severe dysarthria, using synthetic speech augmentation.
  The authors fine-tuned an ASR model using synthetic dysarthric speech generated
  via a personalized TTS system trained on premorbidity recordings and speaker embedding
  interpolation.
---

# Improved Dysarthric Speech to Text Conversion via TTS Personalization

## Quick Facts
- arXiv ID: 2508.06391
- Source URL: https://arxiv.org/abs/2508.06391
- Reference count: 28
- The paper presents a case study on developing a personalized speech-to-text system for a Hungarian speaker with severe dysarthria, using synthetic speech augmentation.

## Executive Summary
This paper addresses the challenge of creating personalized ASR systems for speakers with severe dysarthria through a novel approach combining TTS personalization and synthetic data augmentation. The authors develop a system for a Hungarian speaker with post-stroke dysarthria, fine-tuning an ASR model using synthetic dysarthric speech generated via a personalized TTS system. By interpolating between fluent premorbidity recordings and limited dysarthric speech, they create controlled variations of speech impairment severity. The approach achieves a significant reduction in character error rate from 36-51% to 7.3%, with synthetic data contributing an 18% relative CER reduction. The study also demonstrates that language-specific monolingual models significantly outperform multilingual models like Whisper for dysarthric speech recognition.

## Method Summary
The authors develop a personalized ASR system for a Hungarian speaker with severe dysarthria through a multi-stage approach. First, they fine-tune an XTTS-v2 TTS model on premorbidity lecture recordings (13 hours) filtered by ASR ensemble agreement to create a speaker-adapted TTS system. They then generate synthetic dysarthric speech using speaker embedding interpolation between fluent and dysarthric speech, creating multiple variants with different severity levels. Finally, they fine-tune a FastConformer Hu ASR model on a combination of real dysarthric speech (21 minutes of transcribed data) and synthetic dysarthric speech (~10-12 hours), achieving significant CER reduction compared to zero-shot performance.

## Key Results
- Fine-tuning on both real and synthetic dysarthric speech reduced CER from 36-51% (zero-shot) to 7.3%
- Synthetic data contributed an 18% relative CER reduction
- Monolingual FastConformer Hu ASR model significantly outperformed Whisper-turbo when fine-tuned on the same data
- XTTS-v2 interpolation weights enabled controlled generation of dysarthric speech with varying severity levels

## Why This Works (Mechanism)
The approach leverages synthetic data augmentation through TTS personalization to address the data scarcity problem in dysarthric speech recognition. By creating a personalized TTS system trained on premorbidity recordings and interpolating speaker embeddings with dysarthric speech characteristics, the authors generate diverse synthetic training data that captures the continuum of speech impairment severity. This synthetic data supplements the limited real dysarthric speech recordings, allowing the ASR model to learn robust representations of the speaker's unique speech patterns. The monolingual FastConformer Hu model's superior performance highlights the importance of language-specific adaptation for handling the complex acoustic distortions present in dysarthric speech.

## Foundational Learning
- **Speaker embedding interpolation**: Why needed - to create controlled variations of speech impairment severity; Quick check - verify synthetic samples span the intended severity continuum
- **TTS personalization for dysarthric speech**: Why needed - premorbidity recordings provide clean reference for generating impaired speech patterns; Quick check - ensure synthetic dysarthric speech maintains speaker identity while incorporating impairment characteristics
- **Synthetic data augmentation in low-resource scenarios**: Why needed - real dysarthric speech data is scarce and expensive to collect; Quick check - validate that synthetic data improves generalization beyond the training set
- **Monolingual vs multilingual ASR for dysarthric speech**: Why needed - dysarthria introduces complex language-specific distortions; Quick check - compare CER reduction rates between monolingual and multilingual models

## Architecture Onboarding
**Component Map**: Premorbidity recordings -> XTTS-v2 fine-tuning -> Speaker embedding interpolation -> Synthetic dysarthric speech generation -> ASR fine-tuning (FastConformer Hu) -> CER evaluation

**Critical Path**: The core innovation relies on the pipeline from TTS personalization through synthetic data generation to ASR fine-tuning. The interpolation between fluent and dysarthric embeddings is critical for creating diverse training samples that capture the speaker's unique impairment patterns.

**Design Tradeoffs**: The approach trades computational cost of TTS fine-tuning and synthetic speech generation for improved ASR performance. Alternative approaches might use voice conversion or direct feature-space augmentation, but TTS provides better control over speech characteristics and severity levels.

**Failure Signatures**: 
- TTS generates incoherent or non-dysarthric speech when interpolation weights are extreme
- ASR overfits to synthetic patterns, showing poor generalization to real dysarthric speech
- Limited CER improvement despite extensive synthetic data generation
- Monolingual model fails to outperform multilingual baseline

**First Experiments**:
1. Validate XTTS-v2 fine-tuning produces high-quality speaker-adapted TTS using premorbidity recordings
2. Test interpolation weights to ensure synthetic dysarthric speech captures intended severity levels
3. Compare FastConformer Hu fine-tuning with and without synthetic data to measure performance impact

## Open Questions the Paper Calls Out
**Open Question 1**: Can the proposed TTS personalization approach be replicated using only a few seconds of premorbidity audio rather than the hours of lecture recordings utilized in this study? The conclusion states that given zero-shot TTS capabilities, results are "likely reproducible with much less typical speech data... which will be subject of future work." The current study relied on 13 hours of spontaneous premorbidity speech and 2 hours of curated audio, which the authors acknowledge is atypical for the general population.

**Open Question 2**: Does the synthetic data augmentation strategy generalize to speakers with different dysarthria etiologies (e.g., Parkinson's, cerebral palsy) or is it specific to stroke-related impairments? The paper is framed as a "case study" of a single stroke survivor, and the authors note that dysarthria manifests in multiple forms, making a single general system infeasible. The method relies on fine-tuning TTS on dysarthric speech; the acoustic characteristics of stroke-related dysarthria may differ significantly from progressive or developmental disorders.

**Open Question 3**: Does the "continuum of impairment" generated by embedding interpolation and training checkpoints correlate with perceptual assessments of dysarthria severity by human experts? The authors infer severity levels from ASR error rates on synthetic speech (Table III), but do not provide human evaluation confirming that "overtrained" or "extrapolated" embeddings sound like more severe dysarthria. ASR error rates measure intelligibility to a machine, which may not align with the phonetic or prosodic distortions that define clinical severity levels.

## Limitations
- The approach requires substantial premorbidity speech recordings (13 hours of lectures), which may not be available for most dysarthric speakers
- XTTS-v2 fine-tuning hyperparameters are not fully specified, making exact reproduction difficult
- The study is limited to a single speaker with stroke-related dysarthria, limiting generalizability to other etiologies
- No human perceptual validation of the synthetic speech severity levels was conducted

## Confidence
- **High confidence**: The core finding that fine-tuning FastConformer Hu on combined real and synthetic dysarthric speech reduces CER from 36-51% to 7.3% is well-supported by the experimental design and results
- **Medium confidence**: The attribution of the 18% relative CER reduction specifically to synthetic data contribution relies on controlled ablation that wasn't explicitly performed
- **Medium confidence**: The interpolation weight methodology for controlled severity generation is theoretically sound, but the paper doesn't validate whether the perceived severity levels actually correspond to the numerical interpolation values

## Next Checks
1. Replicate the XTTS-v2 fine-tuning process with specified lecture filtering criteria (edit-distance â‰¤4) and validate whether synthetic speech quality degrades at extreme interpolation weights (C and D)
2. Conduct ablation study comparing FastConformer Hu fine-tuned on real-dysarthric-only vs. real+synthetic to independently verify the 18% relative CER improvement attribution
3. Test the generalizability of the monolingual ASR advantage by fine-tuning Whisper-turbo with the same synthetic augmentation protocol used for FastConformer Hu, ensuring both models receive identical training data