---
ver: rpa2
title: Optimizing Model Selection for Compound AI Systems
arxiv_id: '2502.14815'
source_url: https://arxiv.org/abs/2502.14815
tags:
- performance
- module
- llmselector
- compound
- modules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of model selection for compound
  AI systems, where multiple LLM calls are composed to tackle complex tasks. The authors
  propose LLMSelector, a framework that iteratively selects the best LLM for each
  module by estimating per-module performance using an LLM diagnoser.
---

# Optimizing Model Selection for Compound AI Systems

## Quick Facts
- **arXiv ID:** 2502.14815
- **Source URL:** https://arxiv.org/abs/2502.14815
- **Reference count:** 21
- **Primary result:** LLMSelector achieves 5%-70% accuracy gains compared to using the same LLM for all modules in compound AI systems

## Executive Summary
This paper addresses the problem of model selection for compound AI systems, where multiple LLM calls are composed to tackle complex tasks. The authors propose LLMSelector, a framework that iteratively selects the best LLM for each module by estimating per-module performance using an LLM diagnoser. The key insight is that end-to-end performance is often monotonic in per-module performance, and per-module performance can be accurately estimated by an LLM. Experiments on compound systems like self-refine and multi-agent-debate using real-world LLM APIs (GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro) show that LLMSelector achieves 5%-70% accuracy gains compared to using the same LLM for all modules.

## Method Summary
LLMSelector is a framework for optimizing model selection in compound AI systems. It iteratively selects the best LLM for each module by estimating per-module performance using an LLM diagnoser. The framework assumes that end-to-end performance is monotonic in per-module performance, allowing independent module optimization rather than joint search over all possible allocations. The LLM diagnoser estimates per-module performance by judging which module caused errors in a given task, combining this judgment with end-to-end performance signals. The algorithm cycles through modules, estimates performance for each candidate model, and updates allocations until convergence or budget exhaustion.

## Key Results
- LLMSelector achieves 5%-70% accuracy gains compared to using the same LLM for all modules
- The framework reduces search cost by 60% compared to exhaustive search
- On TableArithmetic, LLMSelector reaches 100% accuracy while greedy search gets stuck at 0%

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimizing individual module performance improves end-to-end compound system performance, avoiding exponential search.
- **Mechanism:** The system decomposes end-to-end performance as p(f,z) = h(p₁(f,z), ..., p_L(f_L,z)) where h is monotonically increasing in each argument. This allows independent module optimization rather than joint search over |M|^|V| allocations.
- **Core assumption:** Module-wise performance is intra-monotone (model rankings persist regardless of other allocations) and inter-monotone (improving one module never hurts others).
- **Evidence anchors:**
  - [abstract] "end-to-end performance is often monotonic in how well each module performs, with all other modules held fixed"
  - [Section 4.2] Formal definitions of intra/inter-monotonicity conditions
  - [corpus] Weak direct evidence; related work (Optimas, Compound AI Systems Survey) addresses optimization but not this specific decomposition
- **Break condition:** When modules have strong dependencies (e.g., one module's output format affects another's ability to parse), monotonicity may fail.

### Mechanism 2
- **Claim:** An LLM diagnoser can accurately estimate per-module performance without ground-truth module labels.
- **Mechanism:** Given system architecture, a task (query, desired answer), and module inputs/outputs, an LLM judges which module caused the error. The estimated performance combines LLM judgment with end-to-end signal: p̂_j(f,z) = LLM_judgment + γ × p(f,z).
- **Core assumption:** LLM-as-judge correlates sufficiently with actual module-level errors; γ balances local vs. global signal.
- **Evidence anchors:**
  - [abstract] "per-module performance can be estimated accurately by an LLM"
  - [Section 5.1, Figure 4(c3)] LLM diagnoser correctly identifies Claude 3.5 performs well on locate module despite no end-to-end improvement
  - [corpus] LLM-as-judge validity studied (Zheng et al., Shankar et al.) but not specifically for module diagnosis in compound systems
- **Break condition:** When errors are subtle or distributed across modules, LLM diagnoser may misattribute blame.

### Mechanism 3
- **Claim:** Iterative module-wise allocation escapes local optima that trap end-to-end greedy search.
- **Mechanism:** Standard greedy search optimizes end-to-end accuracy directly, getting stuck when single-model swaps show no improvement. LLMSelector's module-wise estimates detect latent improvements invisible to end-to-end metrics, enabling escape.
- **Core assumption:** The LLM diagnoser provides signal even when p(f,z) is locally flat.
- **Evidence anchors:**
  - [Section 5.1] Greedy search achieves low accuracy; LLMSelector reduces search cost by 60%
  - [Section 5.1, Figure 4(c2)] GPT-4o-mini for both modules is a local optimum; end-to-end greedy cannot escape
  - [corpus] No direct corpus evidence for this specific escape mechanism
- **Break condition:** If LLM diagnoser is systematically biased toward certain models, iterations may converge to suboptimal allocations.

## Foundational Learning

- **Concept: Compound AI Systems (DAG-structured modules)**
  - Why needed here: LLMSelector operates on static compound systems defined as directed acyclic graphs; understanding data flow is essential for module nomination order.
  - Quick check question: Can you sketch the self-refine DAG (generator → critic → refiner) and identify which modules depend on others' outputs?

- **Concept: Monotonicity in Optimization**
  - Why needed here: The theoretical guarantee (Theorem 4.1) depends on monotone relationships; violations mean no convergence guarantee.
  - Quick check question: If swapping model A→B for module 1 improves p₁ but degrades p₂, does inter-monotonicity hold?

- **Concept: LLM-as-Judge / Evaluation**
  - Why needed here: The diagnoser is an LLM judge; understanding calibration and failure modes informs γ selection and prompt design.
  - Quick check question: What biases might an LLM diagnoser have when evaluating outputs from the same model family vs. different families?

## Architecture Onboarding

- **Component map:**
  - Input: Compound system G=(V,E), candidate LLM pool M (K models), training dataset D_Tr (query-answer pairs), budget B
  - Module Nominator: Cycles through modules (j = i mod L + 1)
  - LLM Diagnoser: Estimates p̂_j for each candidate model; uses prompt template (Appendix B.4)
  - Model Updater: Selects argmax_k p̂_j(f_{j→k}, z) per task, then aggregates via mode across tasks
  - Output: Optimized allocation f̂

- **Critical path:** Diagnoser prompt design → γ selection → iteration until convergence or budget exhaustion. The diagnoser's ability to attribute errors to specific modules determines search efficiency.

- **Design tradeoffs:**
  - Higher γ: More weight on end-to-end signal, slower escape from local optima but more robust to diagnoser errors
  - Lower γ: Faster convergence if diagnoser is accurate, but risk of following noisy module estimates
  - Module nomination order: Random vs. topological; paper uses cyclic modulo (line 4) but doesn't analyze sensitivity

- **Failure signatures:**
  - Convergence to suboptimal allocation despite budget: Likely diagnoser bias or monotonicity violation
  - High variance across runs: Diagnoser instability; consider ensemble judgments or increase γ
  - No improvement over single-model baseline: Candidate pool may lack complementary strengths; check per-module model rankings

- **First 3 experiments:**
  1. **Reproduce TableArithmetic case study (Section 5.1):** 5 models, 2 modules, γ=0. Verify LLMSelector reaches 100% accuracy and diagnose any deviation.
  2. **Ablate the LLM diagnoser:** Replace with random module nomination or pure end-to-end greedy. Quantify performance gap and cost reduction.
  3. **Stress-test monotonicity assumption:** Construct a synthetic compound system where module dependencies violate inter-monotonicity. Observe whether LLMSelector still improves or diverges.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the LLMSelector framework be adapted for dynamic compound AI systems where the control flow or number of modules is not fixed?
- **Basis in paper:** [explicit] The paper restricts its definition and optimization to "static compound AI systems" where the "number of modules" and "sequencing" are fixed (Section 1).
- **Why unresolved:** The current algorithm iterates over a fixed set of modules, making it inapplicable to systems with conditional branching or dynamic agent generation.
- **What evidence would resolve it:** An algorithm extension that handles variable-length execution paths and an evaluation on dynamic agentic frameworks.

### Open Question 2
- **Question:** How robust is LLMSelector in scenarios where per-module performance is not intra-monotone or inter-monotone?
- **Basis in paper:** [explicit] The authors state the conditions for optimality "are not always satisfied in practice" and that "in these cases... the derived algorithm is still applicable" but the theoretical guarantees may fail (Section 4.2).
- **Why unresolved:** The paper relies on these conditions for theoretical convergence; empirical behavior when they are violated remains under-explored.
- **What evidence would resolve it:** Stress tests on adversarial compound systems designed to exhibit non-monotonic module interactions (e.g., where improving a critic hurts the refiner).

### Open Question 3
- **Question:** What is the optimal strategy for jointly optimizing model selection and prompt engineering?
- **Basis in paper:** [explicit] The authors describe model selection as a "third axis... complementary to prompt optimization" but the current framework does not simultaneously optimize both (Section 2).
- **Why unresolved:** Changing the model often necessitates changing the prompt; optimizing them sequentially might lead to sub-optimal local maxima.
- **What evidence would resolve it:** A comparative study of joint optimization vs. sequential optimization (prompt-then-model or model-then-prompt) on benchmarks like LiveCodeBench.

## Limitations
- The core assumptions of intra- and inter-monotonicity are asserted but not rigorously validated
- The LLM diagnoser's accuracy is not systematically benchmarked against ground-truth module-level errors
- The choice of γ (balancing diagnoser signal vs. end-to-end signal) is heuristic and not optimized

## Confidence

**High Confidence:** The experimental results showing 5%-70% accuracy gains over baselines are well-documented with multiple datasets and compound systems. The mechanism of using per-module optimization to escape local optima is convincingly demonstrated in the TableArithmetic case study.

**Medium Confidence:** The theoretical framework (Theorem 4.1) is sound given the stated assumptions, but the practical applicability depends heavily on monotonicity conditions that are not thoroughly validated. The LLM diagnoser approach is plausible but lacks systematic evaluation of failure modes.

**Low Confidence:** The optimal γ value and stopping criteria (δ) are not rigorously determined. The sensitivity to module nomination order and the diagnoser's performance on more complex compound systems with longer chains remain unclear.

## Next Checks

1. **Monotonicity Stress Test:** Construct synthetic compound systems with controlled dependencies that violate inter-monotonicity. Measure whether LLMSelector still converges or produces degraded performance compared to single-model baselines.

2. **Diagnoser Benchmark:** Create a ground-truth dataset with per-module annotations (which module caused errors) for a subset of tasks. Compare LLM diagnoser accuracy against this ground truth across different model families and error types.

3. **Ablation on γ:** Systematically vary γ from 0 to 1 in increments of 0.2 across all compound systems. Plot the tradeoff curve between search efficiency (cost reduction) and final accuracy to identify optimal settings for different problem characteristics.