---
ver: rpa2
title: 'keepitsimple at SemEval-2025 Task 3: LLM-Uncertainty based Approach for Multilingual
  Hallucination Span Detection'
arxiv_id: '2505.17485'
source_url: https://arxiv.org/abs/2505.17485
tags:
- hallucination
- spans
- span
- language
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a zero-resource, multilingual approach for\
  \ detecting hallucinated text spans in LLM-generated content. The method analyzes\
  \ entropy from stochastically-sampled model responses\u2014assuming consistent outputs\
  \ for factual content and divergent outputs for hallucinations."
---

# keepitsimple at SemEval-2025 Task 3: LLM-Uncertainty based Approach for Multilingual Hallucination Span Detection

## Quick Facts
- arXiv ID: 2505.17485
- Source URL: https://arxiv.org/abs/2505.17485
- Reference count: 3
- Primary result: Zero-resource multilingual hallucination detection using LLM uncertainty signals achieved IoU scores of 0.35-0.47 across 14 languages

## Executive Summary
This paper presents a zero-resource, multilingual approach for detecting hallucinated text spans in LLM-generated content by analyzing entropy from stochastically-sampled model responses. The method assumes that LLMs generate consistent outputs for factual content but divergent outputs for hallucinations, using semantic and lexical entropy combined with frequency-based scoring to identify hallucinated spans without requiring training data. Using Llama-3.2-3B-Instruct with 20 samples per query and language-specific hyperparameters, the approach achieved competitive performance across 14 languages in the Mu-SHROOM task, with peak results in Chinese (IoU: 0.4703, rank 10) and strong scores in Basque, Italian, and Hindi.

## Method Summary
The approach uses zero-resource hallucination detection by generating 20 candidate responses per query using Llama-3.2-3B-Instruct with temperature=0.1, top-p=0.9, top-k=50. For each sliding-window span, it computes semantic entropy via cosine similarity of sentence embeddings, lexical entropy via Shannon entropy over matched span frequencies, and frequency score = 1 - (matches/total_samples). These are combined with weights (α=0.4, β=0.4, γ=0.2) to produce a hallucination score, which is then refined by aligning spans to token/phrase/entity boundaries and merging overlapping detections using language-specific thresholds.

## Key Results
- Achieved IoU scores of 0.35-0.47 across 14 languages in Mu-SHROOM task
- Peak performance in Chinese with IoU of 0.4703 (rank 10)
- Strong scores in Basque (0.3735, rank 11), Italian (0.3565, rank 12), and Hindi (0.3387, rank 12)
- Demonstrated effectiveness of zero-resource approach without training data

## Why This Works (Mechanism)

### Mechanism 1: Stochastic Sampling Consistency as Hallucination Signal
- Claim: LLMs generate consistent outputs for well-represented factual knowledge but diverge on hallucinated content when sampled stochastically.
- Mechanism: Generate n=20 candidate responses using temperature=0.1, top-p=0.9, top-k=50. Factual spans match across samples; hallucinated spans produce conflicting variants. Entropy of match distributions signals uncertainty.
- Core assumption: Hallucinations arise from model uncertainty rather than confident but wrong generations.
- Evidence anchors: SelfCheckGPT (Manakul et al., 2023) provides theoretical foundation; related Mu-SHROOM submissions use similar ensemble-based verification.

### Mechanism 2: Multi-Signal Entropy Aggregation
- Claim: Combining semantic entropy, lexical entropy, and frequency scoring captures different manifestations of model uncertainty at the span level.
- Mechanism: For each sliding-window span, compute semantic entropy via cosine similarity, lexical entropy via Shannon entropy over matched spans, and frequency score = 1 - (matches/total_samples). Weighted sum: Sh = 0.4·Hs + 0.4·Hl + 0.2·F.
- Core assumption: Heuristic weights (α=0.4, β=0.4, γ=0.2) generalize across languages and domains.
- Evidence anchors: Related work cites entropy as signal (Farquhar et al., 2024; Kossen et al., 2024); language-specific hyperparameter tuning shows optimal window sizes vary.

### Mechanism 3: Boundary Refinement via Entropy Gradients
- Claim: Aligning detected spans with token/phrase/entity boundaries improves localization accuracy.
- Mechanism: Post-process raw entropy-based detections by adjusting to word boundaries, preserving phrase integrity, avoiding entity splits, and merging overlapping spans with length-weighted score averaging.
- Core assumption: Hallucinations respect linguistic boundaries; partial-word or mid-entity hallucinations are rare or noise.
- Evidence anchors: IoU scores (0.35-0.47) suggest reasonable localization; refinement described using "entropy gradient at span boundaries."

## Foundational Learning

- **Shannon Entropy for Discrete Distributions**
  - Why needed: Core mathematical tool for quantifying response variability. You must understand H = -Σ p(x)log p(x) and why uniform distributions maximize entropy.
  - Quick check: If 5 sampled responses are all identical vs. 5 completely different responses, which produces higher lexical entropy? (Answer: Different responses → higher entropy → higher hallucination score.)

- **Cosine Similarity in Embedding Space**
  - Why needed: Semantic entropy requires measuring meaning similarity beyond lexical overlap. Understanding that embedding similarity captures paraphrase equivalence.
  - Quick check: Would "The capital is Paris" and "Paris is the capital" have high or low cosine similarity in a good sentence embedding model? (Answer: High similarity → treated as consistent → low semantic entropy.)

- **Stochastic Decoding Parameters (Temperature, Top-p, Top-k)**
  - Why needed: The method's effectiveness depends on sampling parameters that produce meaningful diversity without degeneration.
  - Quick check: Why would temperature=1.0 potentially harm this method compared to temperature=0.1? (Answer: Excessive diversity may cause false positives on factual content.)

## Architecture Onboarding

- **Component map:**
```
Input: Generated text G
  ↓
[Sliding Window Segmenter] → Spans s₁...sₙ
  ↓
[Stochastic Sampler] → 20 responses per query (Llama-3.2-3B-Instruct)
  ↓
[Lexical Matcher] → Matched spans Mᵢ per window (threshold τ)
  ↓
[Multi-Signal Scorer] → Semantic entropy + Lexical entropy + Frequency
  ↓
[Boundary Refiner] → Token/phrase/entity alignment
  ↓
[Span Merger] → Overlap consolidation
  ↓
[Threshold Filter] → λ-based final selection
  ↓
Output: Hallucination spans H with scores
```

- **Critical path:** The stochastic sampling quality directly determines signal reliability. If sampling parameters produce insufficient diversity or excessive noise, all downstream entropy calculations degrade. The paper uses 20 samples—this is the computational bottleneck but also the signal source.

- **Design tradeoffs:**
  - Window size (w) vs. granularity: Larger windows capture more context but reduce localization precision. Chinese required w=7 vs. w=4 for most languages.
  - Sample count vs. cost: 20 samples per query is expensive at inference time. The paper uses a 3B model to manage cost.
  - Zero-resource vs. supervised: Trading off adaptability (no training data needed) against performance ceiling.

- **Failure signatures:**
  - False positives on rare-but-correct facts: Low sampling diversity might still occur, but lexical entropy could spike incorrectly.
  - Confident hallucinations: If the model internally "believes" a hallucination, sampling won't help—this is the fundamental limitation.
  - Language-specific tokenization issues: Agglutinative languages may have different optimal window/stride parameters.

- **First 3 experiments:**
  1. **Baseline ablation:** Run pipeline with only semantic entropy (α=1, β=0, γ=0), only lexical entropy (α=0, β=1, γ=0), and only frequency (α=0, β=0, γ=1) on validation data. Compare IoU to understand signal contribution.
  2. **Sample count sensitivity:** Test n ∈ {5, 10, 20, 40} samples on a single language (e.g., English) to characterize the performance-compute tradeoff curve.
  3. **Failure mode analysis:** Manually inspect false positives in the error analysis sample (Table 4). Categorize whether errors come from (a) paraphrase confusion, (b) partial matching noise, or (c) threshold misalignment.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical foundation uncertainty: The core hypothesis that stochastic sampling consistency reliably signals factual vs. hallucinated content remains empirically unproven.
- Computational cost constraints: Generating 20 candidate responses per query requires significant inference resources without exploring whether fewer samples could achieve comparable performance.
- Boundary refinement mechanism opacity: The implementation details of entropy gradient-based boundary alignment are underspecified, making faithful reproduction difficult.

## Confidence
- **High confidence:** The zero-resource multilingual approach architecture is clearly specified and reproducible with well-documented mathematical framework.
- **Medium confidence:** Reported performance metrics (IoU scores of 0.35-0.47) appear credible given task complexity, but lack direct comparison to supervised baselines.
- **Low confidence:** The claim that semantic and lexical entropy capture "different manifestations of model uncertainty" lacks empirical validation; fixed weighting heuristic (0.4/0.4/0.2) is presented without justification.

## Next Checks
1. **Signal Ablation Study:** Run complete pipeline with three variants (semantic only, lexical only, frequency only) and compare IoU scores across all 14 languages to quantify each signal's independent contribution.
2. **Sample Count Sensitivity Analysis:** Test relationship between number of sampled responses (n ∈ {5, 10, 20, 40}) and detection accuracy on representative languages to determine optimal tradeoff.
3. **Confidence-Based Error Analysis:** Manually examine 50 false positive detections from validation set and categorize errors into paraphrase confusion, boundary alignment errors, or threshold misalignment to inform targeted improvements.