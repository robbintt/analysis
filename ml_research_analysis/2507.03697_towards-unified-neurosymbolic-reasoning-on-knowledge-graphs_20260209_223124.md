---
ver: rpa2
title: Towards Unified Neurosymbolic Reasoning on Knowledge Graphs
arxiv_id: '2507.03697'
source_url: https://arxiv.org/abs/2507.03697
tags:
- reasoning
- knowledge
- graph
- rules
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a unified neurosymbolic reasoning framework
  (TUNSR) for knowledge graph (KG) reasoning that integrates propositional and first-order
  logic (FOL) reasoning across four reasoning scenarios: transductive, inductive,
  interpolation, and extrapolation. TUNSR introduces a reasoning graph structure that
  expands from the query entity by iteratively searching neighbors, and employs a
  forward logic message-passing mechanism to update propositional and FOL representations
  and attentions for each node.'
---

# Towards Unified Neurosymbolic Reasoning on Knowledge Graphs

## Quick Facts
- arXiv ID: 2507.03697
- Source URL: https://arxiv.org/abs/2507.03697
- Reference count: 40
- Key outcome: TUNSR achieves 1.6%-11.3% MRR improvements over baselines by unifying propositional and FOL reasoning across transductive, inductive, interpolation, and extrapolation scenarios.

## Executive Summary
This paper introduces TUNSR, a unified neurosymbolic reasoning framework that integrates propositional and first-order logic (FOL) reasoning for knowledge graph completion across four reasoning scenarios. TUNSR constructs a reasoning graph starting from the query entity and iteratively expands it through forward logic message-passing that updates both propositional and FOL representations and attentions. The framework employs a differentiable rule-to-attention transformation via the FARI algorithm to induce interpretable FOL rules from learned attentions. Extensive experiments on 19 datasets demonstrate TUNSR's superiority over state-of-the-art baselines, with ablation studies confirming that combining both reasoning modes yields better performance than using either alone.

## Method Summary
TUNSR builds a reasoning graph by iteratively expanding from the query entity through posterior neighbor search. For each iteration, it updates propositional representations and attentions using semantic similarity-based aggregation, while FOL representations use a GRU-based mechanism to integrate relation sequences. The framework handles both static and temporal KGs by treating time as an additional node dimension with appropriate filtering constraints. A final score combines both reasoning paths via learned weights, optimized through multi-class log-loss. The FARI algorithm extracts interpretable FOL rules post-hoc by approximating rule confidence aggregation through attention-weighted message passing.

## Key Results
- TUNSR achieves 1.6%-11.3% MRR improvements across 19 datasets compared to state-of-the-art baselines
- Combined propositional-FOL reasoning outperforms either mode alone in ablation studies
- Unified architecture successfully handles transductive, inductive, interpolation, and extrapolation scenarios
- FOL reasoning particularly excels at extrapolation (future prediction) while propositional reasoning dominates transductive tasks

## Why This Works (Mechanism)

### Mechanism 1: Forward Logic Message-Passing for Dual-Path Reasoning
- Claim: Updating both propositional and FOL representations in parallel enables complementary reasoning patterns that outperform either alone.
- Mechanism: For each node, propositional attention uses semantic similarity between message embeddings and query embedding, while FOL attention flows through a GRU that integrates relation representations. The final score combines both paths via learned weight λ.
- Core assumption: Propositional reasoning captures entity-specific patterns while FOL reasoning captures transferable relational structures; their combination covers complementary reasoning needs.
- Evidence anchors: [abstract], [section 4.3], limited corpus support
- Break condition: If datasets lack sufficient relational structure for FOL patterns, or lack entity-specific paths for propositional reasoning, the synergy degrades.

### Mechanism 2: Iterative Reasoning Graph Expansion with Unified Node Representation
- Claim: A single graph structure can unify all four reasoning scenarios through consistent node expansion and temporal filtering.
- Mechanism: The reasoning graph begins with query entity and expands by searching posterior neighbors iteratively. For SKGs, nodes are entities; for TKGs, nodes are (entity, time) pairs. Temporal constraints differentiate scenarios: interpolation uses all neighbors, extrapolation filters by time < query_time.
- Core assumption: The structural similarity across scenarios outweighs their differences; temporal information can be handled as an additional node dimension.
- Evidence anchors: [section 3.1], [abstract], weak corpus support
- Break condition: Node explosion in dense graphs requires sampling and pruning which may cut valid paths.

### Mechanism 3: Differentiable Rule-to-Attention Transformation via FARI
- Claim: Discrete FOL rule learning can be made differentiable by approximating rule confidence aggregation with attention-weighted message passing.
- Mechanism: TUNSR merges multiple rules by merging possible relations at each step, using FOL attention as proxy for rule confidence. FARI algorithm then extracts explicit rules post-hoc.
- Core assumption: Attention weights accumulated through forward propagation can serve as sufficient proxies for rule confidence, enabling end-to-end training while preserving interpretability.
- Evidence anchors: [section 3.3], [section 3.3], Logic of Hypotheses paper support
- Break condition: If attention weights don't correlate well with actual rule utility, extracted rules may be spurious despite good prediction performance.

## Foundational Learning

- Concept: Propositional Logic vs First-Order Logic on KGs
  - Why needed here: TUNSR explicitly separates these two reasoning modes; understanding their distinction is essential for interpreting the dual-path mechanism.
  - Quick check question: Can you explain why "bornIn(X, Y) ∧ locatedIn(Y, Z) → nationalityOf(X, Z)" is FOL while specific path "Obama → Hawaii → USA" is propositional?

- Concept: Gated Recurrent Units (GRU) for Sequence Modeling
  - Why needed here: FOL representation updates use GRU to integrate relation sequences; understanding GRU gating helps debug FOL path learning.
  - Quick check question: Why might a GRU be preferred over simple averaging for combining relation embeddings in a rule chain?

- Concept: Temporal Knowledge Graph Scenarios
  - Why needed here: The paper distinguishes interpolation (predicting within known time range) from extrapolation (predicting future); different temporal constraints apply.
  - Quick check question: For query (entity, relation, ?, future_timestamp), why must the reasoning graph only use facts with time < future_timestamp?

## Architecture Onboarding

- Component map: Query → Reasoning Graph Builder → Iterative expansion → Forward Logic Message-Passing → Propositional Path (attention α, embed x) + FOL Path (attention β, embed y via GRU) → Score Computation → FARI Rule Extraction

- Critical path:
  1. Implement reasoning graph expansion (Section 3.1) — this is the foundation
  2. Implement propositional attention computation (Equations 4-8)
  3. Implement FOL attention with GRU (Equations 9-12)
  4. Combine scores (Equations 14-15) and train with multi-class log-loss (Equation 16)
  5. Add FARI for interpretability (Algorithm 1) — optional for inference-only

- Design tradeoffs:
  - Rule length L: Longer captures more complex patterns but introduces noise; optimal at L=3-6 depending on dataset density
  - Sampling M (nodes) and N (edges): Higher improves recall but increases GPU memory quadratically; start with M=200, N=100
  - λ combination weight: Can be fixed or learned; learned preferred unless debugging specific reasoning mode

- Failure signatures:
  - Propositional-only mode underperforms extrapolation: Expected; FOL transferability is critical for future prediction
  - FOL-only mode underperforms transductive: Expected; entity-specific patterns matter for dense static graphs
  - OOM on dense TKGs: Reduce M (node sampling) first, then N (edge selection); extrapolation is most memory-intensive
  - Poor rule interpretability despite good accuracy: Check if attention weights are diffuse vs concentrated

- First 3 experiments:
  1. Sanity check: Run TUNSR on WN18RR with L=3, verify MRR ~0.55+ and inspect top-3 extracted FOL rules for semantic plausibility
  2. Ablation: Compare unified (learned λ) vs propositional-only (λ=0) vs FOL-only (λ=1) on ICEWS14 extrapolation; expect unified > either alone
  3. Hyperparameter sweep: Vary M ∈ {100, 200, 500} on ICEWS18 extrapolation, plot Hits@10 vs GPU memory to find efficiency sweet spot

## Open Questions the Paper Calls Out
None

## Limitations
- The specific contribution of the unified architecture versus simply having two reasoning paths is not clearly isolated
- FARI algorithm's rule extraction quality is evaluated post-hoc rather than during training, leaving open whether the differentiable transformation produces meaningful rules
- Rule quality evaluation relies on post-hoc extraction rather than integrated validation during training

## Confidence
- High Confidence: Dual-path reasoning (propositional + FOL) outperforms single-path approaches, supported by ablation results across multiple datasets
- Medium Confidence: Unified architecture genuinely enables all four reasoning scenarios rather than requiring scenario-specific modifications, though evidence is indirect
- Low Confidence: Differentiable rule-to-attention transformation produces interpretable and semantically meaningful FOL rules, as rule quality is evaluated post-hoc only

## Next Checks
1. Implement scenario-specific versions (separate architectures for transductive vs extrapolation) and compare performance against unified TUNSR to isolate architectural benefits
2. Design human evaluation task where annotators assess semantic validity of top-10 FOL rules from FARI on WN18RR and ICEWS14, measuring precision and coverage
3. Compute correlation between FARI-extracted rule confidence scores and actual downstream reasoning performance (MRR/Hits@10) across different λ values to validate the differentiable transformation assumption