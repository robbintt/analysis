---
ver: rpa2
title: Dynamic Evaluation for Oversensitivity in LLMs
arxiv_id: '2510.19005'
source_url: https://arxiv.org/abs/2510.19005
tags:
- prompts
- feature
- oversensitivity
- language
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of oversensitivity in large language
  models (LLMs), where models incorrectly reject benign prompts that appear harmful.
  The authors propose a dynamic evaluation framework that generates model-specific
  challenging datasets by training proxy models to mimic defensive behaviors, using
  feature attribution to identify key trigger features, and iteratively generating
  new adversarial samples.
---

# Dynamic Evaluation for Oversensitivity in LLMs

## Quick Facts
- arXiv ID: 2510.19005
- Source URL: https://arxiv.org/abs/2510.19005
- Reference count: 5
- Primary result: Proposes dynamic evaluation framework showing Gemma models exhibit highest oversensitivity while Llama models show lowest across 25 LLMs

## Executive Summary
This paper addresses the problem of oversensitivity in large language models (LLMs), where models incorrectly reject benign prompts that appear harmful. The authors propose a dynamic evaluation framework that generates model-specific challenging datasets by training proxy models to mimic defensive behaviors, using feature attribution to identify key trigger features, and iteratively generating new adversarial samples. They construct OVERBENCH, a benchmark aggregating 450,000 samples across 25 models. Evaluation shows varying oversensitivity rates across models, with Gemma models displaying the most severe oversensitivity and Llama models the least. The study also reveals that models from the same family often share similar defensive patterns, and that certain features (e.g., related to theft or insults) consistently trigger false refusals across different model families.

## Method Summary
The dynamic evaluation framework operates through three phases: First, a DeBERTa-v3-base proxy model is trained to mimic each target LLM's refusal behavior on 30,000 labeled prompts from HH-RLHF and ToxiGen datasets. Second, Integrated Gradients with frequency adjustment identifies trigger tokens that cause false refusals by downweighting common words. Third, GPT-4o-mini generates new benign prompts containing these trigger features, filtered through the proxy model, with frequency caps ensuring diversity. The process iterates until feature pools saturate, producing OVERBENCH (450K samples) and OVERBENCH-HARD (30K samples rejected by ≥5 models).

## Key Results
- Gemma models show the highest oversensitivity rates while Llama models show the lowest
- Models from the same family exhibit similar defensive patterns and feature distributions
- Theft-related terms and insults consistently trigger false refusals across different model families
- OVERBENCH-HARD contains 30,000 samples rejected by at least five different models
- Manual validation shows 94% precision for generated sample benignity

## Why This Works (Mechanism)

### Mechanism 1: Proxy Model Distillation
A lightweight classifier approximates a target LLM's defensive decision boundary at lower computational cost by training DeBERTa-v3-base on labeled prompts (accepted vs. rejected by target LLM) to create a distilled decision boundary proxy. The proxy filters candidates before expensive target model queries. Core assumption: The proxy model generalizes sufficiently to capture the target's refusal patterns without accessing internal representations.

### Mechanism 2: Frequency-Adjusted Feature Attribution
Downweighting high-frequency tokens in attribution scores surfaces content words that more likely trigger defensive responses. Compute Integrated Gradients for each token, then divide by unigram frequency raised to a smoothing coefficient (β=1). This penalizes generic tokens and amplifies semantic triggers. Core assumption: Oversensitivity is driven by specific content words rather than syntactic patterns or context combinations.

### Mechanism 3: Iterative Adversarial Sample Generation
Seeding new prompts with identified trigger features while enforcing benign semantics can systematically expose model-specific oversensitivity patterns. Use generator LLM (GPT-4o-mini) to synthesize new queries embedding top-k attribution tokens, filtered through proxy model and frequency caps (T=50) to ensure diversity. Core assumption: The generator can reliably produce semantically benign prompts containing trigger features that evade its own safety filters.

## Foundational Learning

- **Integrated Gradients (attribution methods)**
  - Why needed here: Core technique for identifying which input tokens contribute to refusal decisions; requires understanding of gradient-based attribution vs. perturbation-based methods.
  - Quick check question: Can you explain why Integrated Gradients requires a baseline input and how the path integral aggregates feature importance?

- **Knowledge Distillation (model compression)**
  - Why needed here: Proxy model training distills target LLM's decision boundary into smaller classifier; understanding student-teacher dynamics helps diagnose proxy failures.
  - Quick check question: What failure modes emerge when the student model has insufficient capacity to capture the teacher's decision boundary?

- **Adversarial Example Generation**
  - Why needed here: Framework generates benign adversarial prompts; understanding the difference between evasion attacks (harmful→benign appearance) and this work's approach (benign→harmful appearance) is critical.
  - Quick check question: How does constraining semantic benignity during generation differ from unconstrained adversarial attack methods?

## Architecture Onboarding

- **Component map:**
  Data sources (HH-RLHF, ToxiGen) -> Target LLM labeling -> DeBERTa-v3-base proxy training -> Integrated Gradients attribution -> GPT-4o-mini generation -> Proxy filtering -> Feature pool tracking -> OVERBENCH aggregation

- **Critical path:**
  1. Label initial prompts via target LLM (accept/reject)
  2. Train proxy model on labeled split (90/5/5)
  3. For each rejected prompt: extract top-k features via attribution
  4. Generate new benign prompts embedding features
  5. Filter through proxy; if rejected, add to dataset and update feature pool
  6. Iterate until feature pool saturates or sample budget reached

- **Design tradeoffs:**
  - Proxy accuracy vs. query cost: Higher-capacity proxy improves filtering but increases training overhead
  - Frequency cap (T=50) vs. diversity: Lower caps increase diversity but may miss strong triggers' edge cases
  - Generator temperature vs. semantic control: Higher temperature increases variety but risks semantic drift toward harmful content

- **Failure signatures:**
  - Low proxy-target agreement (>15% divergence): Proxy fails to capture decision boundary
  - Feature pool saturation at low sample counts: Attribution not identifying diverse triggers
  - High manual rejection rate in generated samples: Generator not enforcing benign constraint effectively
  - Model-family-specific patterns not emerging: Initial seed data may be insufficiently diverse

- **First 3 experiments:**
  1. Validate proxy model: Compare proxy predictions against target LLM on held-out test set; target >90% agreement before proceeding.
  2. Attribution sanity check: Manually inspect top-10 features for 50 rejected prompts; verify semantic relevance to refusal triggers.
  3. Generation quality audit: Sample 100 generated prompts per target model; manually verify semantic benignity and trigger feature presence.

## Open Questions the Paper Calls Out

- Does reducing oversensitivity inevitably compromise the model's ability to correctly reject genuinely harmful requests?
- Are the shared defensive patterns within model families (e.g., Gemma, Llama) primarily artifacts of alignment datasets or model architecture?
- How does the fidelity of the proxy model degrade when applied to significantly updated versions of the target LLM?

## Limitations
- Reliance on proxy model accuracy with limited validation (94% precision on only 500 samples)
- Benignity constraint during generation enforced through generator's own safety filters, potentially creating selection bias
- Frequency-adjusted attribution assumes individual trigger words rather than contextual/compositional patterns drive oversensitivity

## Confidence
- **High Confidence:** Observed variation in oversensitivity rates across model families (Gemma highest, Llama lowest)
- **Medium Confidence:** Identification of specific trigger features (theft, insults) as consistent drivers, though influenced by seed data
- **Low Confidence:** Claim that dynamic framework outperforms static benchmarks lacks direct comparative validation

## Next Checks
1. Conduct systematic evaluation of proxy-target agreement across all 25 models, measuring false negative rates
2. Compare trigger feature distributions in initial seed data versus generated samples to quantify selection bias
3. Perform controlled experiments comparing frequency-adjusted Integrated Gradients against alternative attribution methods (LIME, SHAP)