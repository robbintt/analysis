---
ver: rpa2
title: Zero-Shot Transferable Solution Method for Parametric Optimal Control Problems
arxiv_id: '2509.18404'
source_url: https://arxiv.org/abs/2509.18404
tags:
- control
- target
- optimal
- problem
- obstacle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a transferable solution method for parametric
  optimal control problems using function encoders. The approach learns neural network
  basis functions that span the control policy space, enabling zero-shot adaptation
  to new objectives through coefficient estimation from limited data or direct mapping
  from problem specifications.
---

# Zero-Shot Transferable Solution Method for Parametric Optimal Control Problems

## Quick Facts
- arXiv ID: 2509.18404
- Source URL: https://arxiv.org/abs/2509.18404
- Reference count: 40
- One-line primary result: Transferable solution method for parametric optimal control using learned neural basis functions enables zero-shot adaptation with near-optimal performance and minimal online computation

## Executive Summary
This paper introduces a function encoder-based approach for solving parametric optimal control problems where system dynamics are fixed but objectives vary with task parameters. The method learns a reusable set of neural basis functions that span the control policy space during an offline imitation learning phase, enabling zero-shot adaptation to new objectives through coefficient estimation. The offline-online decomposition ensures that computationally expensive basis function training occurs once, while online adaptation requires only lightweight coefficient estimation via least-squares projection or direct task specification mapping.

The approach demonstrates significant computational efficiency by reducing online control computation to a p-dimensional least-squares problem (typically p=100), independent of system complexity. Extensive numerical experiments validate the method's effectiveness across diverse dynamics and cost structures, showing near-optimal performance with minimal overhead when generalizing across tasks. The framework successfully handles both seen and unseen target states in path planning problems, with results including 0.4% error in objective value for a 12D quadcopter path planning problem and accurate control policy transfer across varying obstacle configurations in bicycle control problems.

## Method Summary
The method addresses parametric optimal control problems where dynamics are fixed but objectives vary with task parameter η. During offline training, the approach generates datasets of optimal trajectories for different task parameters using standard solvers, then trains a set of neural network basis functions φ_j(x,t) to minimize reconstruction error of control actions across all tasks. For online adaptation, new task coefficients c(η) are estimated either through least-squares projection from limited trajectory data or via an operator network that maps task parameters directly to coefficients. The control policy is reconstructed as u(x,t;η) ≈ Σ c_j(η)φ_j(x,t;θ_j), enabling zero-shot transfer with minimal online computation.

## Key Results
- Achieves 0.4% error in objective value for 12D quadcopter path planning compared to ground truth solver
- Successfully transfers control policies across varying obstacle configurations in bicycle control problems
- Demonstrates computational efficiency with online adaptation reduced to O(1) operations independent of problem complexity

## Why This Works (Mechanism)

### Mechanism 1
A finite set of learned neural basis functions can span a policy space containing optimal controls for multiple task objectives. The function encoder learns basis functions {φ_j} that approximate any policy u(·,·;η) in a separable Hilbert space via linear combination: u(x,t;η) ≈ Σ c_j(η)φ_j(x,t;θ_j). Theorem 1 guarantees that for any continuous f ∈ H and ε > 0, there exist neural basis functions and coefficients such that the approximation error is bounded by ε‖f‖_H. Core assumption: The space of optimal control policies across task variations lies in a representable Hilbert space and admits smooth parameterization.

### Mechanism 2
Offline-online decomposition enables amortized computation with O(1) online cost independent of problem complexity. Basis functions are trained once via imitation learning over N task instances with associated datasets. Online adaptation reduces to solving a p-dimensional least-squares problem (p = number of basis functions, typically 100), with closed-form solution via Gram matrix inversion. Core assumption: Task distribution during training covers the space of online tasks (interpolation); extrapolation is not guaranteed.

### Mechanism 3
Coefficient estimation from limited trajectory data or direct task specification enables zero-shot transfer. Two inference modes: (1) LS projection estimates c by minimizing reconstruction loss over M measurements (Theorem 2 gives O(M^(-1/2)) convergence); (2) Operator network ψ:η→c(η) enables data-free inference when η is low-dimensional and structured. Core assumption: For LS, measurements provide sufficient coverage of state-time domain; for operator, η admits compact representation.

## Foundational Learning

- **Concept: Parametric Optimal Control**
  - Why needed here: The entire framework addresses problems where dynamics are fixed but objectives J(u;η) vary with task parameter η. Understanding this formulation is prerequisite to grasping what is being transferred.
  - Quick check question: Can you explain why the control policy u(x,t;η) depends on both state-time and the task parameter?

- **Concept: Hilbert Space Projection**
  - Why needed here: Coefficient estimation is fundamentally projection onto learned basis functions. The Gram matrix and L² inner product structure determine how coefficients are computed.
  - Quick check question: Given basis functions φ_j, derive the closed-form solution for coefficients c that minimize ‖f−Σc_jφ_j‖²_H.

- **Concept: Imitation Learning (Behavior Cloning)**
  - Why needed here: Offline training uses solver-generated trajectories as expert demonstrations. The basis functions learn to reproduce expert controls, not to optimize rewards directly.
  - Quick check question: What happens to imitation learning performance when the expert policy distribution differs from the test distribution?

## Architecture Onboarding

- **Component map:**
  ```
  [Offline Phase]
  Task params {η_k} → Expert solver → Datasets D_Sk → Basis training (Alg 1) → {φ_j}
  (Optional) → Operator training (Alg 2) → ψ:η→c

  [Online Phase]
  New task η → Coefficient estimation (LS or ψ) → c(η)
  State (x,t) → Basis eval {φ_j(x,t)} → Control u = Σc_jφ_j
  ```

- **Critical path:** Basis function quality → coefficient estimation accuracy → control performance. The most sensitive stage is offline basis training; poorly trained bases cannot be recovered online.

- **Design tradeoffs:**
  - More basis functions (higher p) → better approximation but larger Gram matrix and potential overfitting
  - LS vs. operator: LS more accurate with data; operator is data-free but requires structured η and more offline training
  - Training task diversity vs. cost: more tasks improve generalization but increase offline computation

- **Failure signatures:**
  - High reconstruction loss during offline training → basis functions not expressive enough
  - Large coefficient variance across similar tasks → basis not capturing shared structure
  - Divergent controls on out-of-distribution states → extrapolation beyond training distribution
  - Operator network fails to generalize → η parameterization too complex or training data insufficient

- **First 3 experiments:**
  1. **Sanity check:** Reproduce 2D path planning experiment (Section V-A). Start with p=20 basis functions, verify reconstruction loss decreases during training. Confirm coefficient estimation achieves <5% objective error on seen targets.
  2. **Ablation study:** Vary number of training tasks N ∈ {4, 8, 16, 32} and basis functions p ∈ {20, 50, 100}. Measure interpolation vs. extrapolation performance. Identify minimum viable configuration.
  3. **Robustness test:** Add Gaussian noise to expert demonstrations (σ ∈ {0.01, 0.05, 0.1}). Verify Theorem 2's noise robustness claim holds empirically. Check if Tikhonov regularization improves stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the function encoder framework be extended to multi-agent optimal control problems with interacting dynamics?
- Basis in paper: [explicit] The conclusion states: "Future work will explore extensions to multi-agent systems with interacting dynamics."
- Why unresolved: The current formulation assumes a single agent with fixed dynamics. Multi-agent systems introduce coupled dynamics, non-stationary environments, and game-theoretic considerations that the existing basis function representation does not address.
- What evidence would resolve it: Successful demonstration of FE policies on benchmark multi-agent problems (e.g., pursuit-evasion, cooperative navigation) with theoretical guarantees on convergence and scalability.

### Open Question 2
- Question: What theoretical guarantees exist for extrapolation to task specifications outside the convex hull of training tasks?
- Basis in paper: [inferred] The paper notes that "in the original FE work model performance is most reliable when generalizing to tasks within the convex hull of the training task distribution" yet shows promising extrapolation results, without providing theoretical justification.
- Why unresolved: Theorem 1 guarantees approximation capability but not generalization bounds for extrapolation. Empirical extrapolation success is shown but no formal analysis explains when or why it works.
- What evidence would resolve it: Generalization bounds characterizing extrapolation error as a function of distance from training distribution, or counterexamples showing systematic extrapolation failure modes.

### Open Question 3
- Question: How can the operator network approach be made effective for high-dimensional or complex task specifications?
- Basis in paper: [inferred] The paper explicitly states the operator method "can be particularly hard when η is high-dimensional or complex" without proposing solutions.
- Why unresolved: The operator network ψ: η → c(η) must learn a mapping from potentially unstructured, high-dimensional task parameters to coefficient space. Standard neural networks may struggle with such mappings without architectural innovations.
- What evidence would resolve it: Demonstration of operator network success on problems with η > 10 dimensions or structured representations (e.g., cost maps, obstacle fields), possibly using specialized architectures like attention or graph networks.

## Limitations

- Generalization capability is uncertain for highly non-smooth or discontinuous task variations, as theoretical guarantees assume continuous parameterization of the policy space
- The operator network approach for direct coefficient mapping from task parameters lacks corpus validation and may fail for high-dimensional or unstructured η
- Performance on tasks outside the convex hull of training data is empirically successful but lacks theoretical justification for when and why it works

## Confidence

- **High confidence**: Offline-online decomposition and computational efficiency claims are well-supported by the algorithm structure and basic least-squares theory
- **Medium confidence**: Universal approximation guarantees via Theorem 1 are theoretically sound but rely on idealized assumptions about the policy space structure
- **Medium confidence**: Zero-shot transfer performance claims are supported by numerical experiments but limited to specific problem instances with smooth parameter variations

## Next Checks

1. **Out-of-distribution stress test**: Evaluate basis function performance on task parameters that lie outside the convex hull of training data. Measure objective value degradation as a function of extrapolation distance to quantify the interpolation/extrapolation boundary.

2. **High-dimensional parameter validation**: Test the operator network approach on tasks with implicit or high-dimensional task parameters (e.g., image-based terrain maps) to verify the claim about direct mapping capabilities and identify the dimensionality threshold where the approach breaks down.

3. **Cross-dynamics transfer**: Assess whether basis functions learned on one dynamics model (e.g., linear systems) can be effectively transferred to structurally similar but different dynamics (e.g., slightly nonlinear systems), testing the limits of the dynamics-agnostic basis function assumption.