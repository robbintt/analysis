---
ver: rpa2
title: Domain Adaptation of LLMs for Process Data
arxiv_id: '2509.03161'
source_url: https://arxiv.org/abs/2509.03161
tags:
- process
- llms
- event
- data
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic approach for adapting large language
  models (LLMs) to predictive process monitoring (PPM) tasks using parameter-efficient
  fine-tuning (PEFT) methods. The authors directly fine-tune LLMs on process data
  without converting it to natural language narratives, replacing the embedding layers
  to handle domain-specific activity labels.
---

# Domain Adaptation of LLMs for Process Data

## Quick Facts
- arXiv ID: 2509.03161
- Source URL: https://arxiv.org/abs/2509.03161
- Authors: Rafael Seidi Oyamada; Jari Peeperkorn; Jochen De Weerdt; Johannes De Smedt
- Reference count: 19
- Primary result: LLM-based solutions outperform state-of-the-art RNNs for predictive process monitoring, especially for remaining time prediction

## Executive Summary
This paper presents a systematic approach for adapting large language models (LLMs) to predictive process monitoring (PPM) tasks using parameter-efficient fine-tuning (PEFT) methods. The authors directly fine-tune LLMs on process data without converting it to natural language narratives, replacing the embedding layers to handle domain-specific activity labels. They compare different PEFT methods (LoRA, freezing) across three LLMs (PM-GPT2, Qwen2.5, Llama3.2) for next activity and remaining time prediction tasks using real-world event logs.

Results show that LLM-based solutions outperform state-of-the-art recurrent neural networks and narrative-style approaches, particularly for RT prediction and in multi-task settings. The models converge quickly within few epochs and require minimal hyperparameter optimization. Among the PEFT methods, LoRA generally performs better, especially for RT prediction, while freezing configurations work well for NA tasks.

## Method Summary
The authors adapt LLMs for PPM by replacing their embedding layers with domain-specific activity embeddings and adding task-specific linear heads. They employ parameter-efficient fine-tuning methods (LoRA and freezing) to adapt large models (PM-GPT2, Qwen2.5, Llama3.2) to next activity (classification) and remaining time (regression) prediction tasks. The methodology uses trace encoding instead of prefix encoding, processes five real-world event logs (BPI12, BPI17, BPI20 variants), and compares LLM performance against RNN baselines and narrative-style approaches. Training involves grid search over hyperparameters with 10 epochs for LLMs and 25 for RNNs.

## Key Results
- LLMs outperform state-of-the-art RNNs and narrative-style approaches for both next activity and remaining time prediction
- LoRA generally outperforms freezing configurations, particularly for remaining time prediction tasks
- Models converge quickly within few epochs and require minimal hyperparameter optimization
- Multi-task training improves performance compared to single-task baselines, especially for RT prediction
- Properly adapted LLMs effectively learn sequential process patterns across different datasets

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Embedding Alignment
Replacing natural language tokenizers with domain-specific embedding layers allows the model to learn process syntax without linguistic noise. By mapping activity labels directly to unique embeddings rather than sub-word tokens, the model treats process events as a structured "vocabulary," forcing the backbone to learn sequential dependencies based on behavioral patterns rather than semantic meaning of labels.

### Mechanism 2: LoRA for Regression Adaptation
Low-Rank Adaptation (LoRA) effectively transforms a pre-trained classifier into a regressor for Remaining Time prediction by introducing learnable bypass weights. Since LLMs are pre-trained on cross-entropy loss (classification) but RT prediction requires regression (MSE), LoRA introduces sufficient capacity to adapt the frozen backbone's representations for numerical output tasks where standard fine-tuning or freezing fails to converge smoothly.

### Mechanism 3: Multi-Task Sequential Regularization
Joint training on Next Activity and Remaining Time stabilizes learning and improves performance by forcing the model to learn a shared process representation. Training a single model for both tasks prevents overfitting to specific task noise, and the paper observes that while RNNs struggle with multi-task complexity (underfitting RT), LLMs leverage their capacity to handle the joint objective, resulting in consistent improvements over single-task baselines.

## Foundational Learning

- **Concept: Trace Encoding vs. Prefix Encoding**
  - Why needed here: The paper utilizes "trace encoding" (full traces with teacher forcing) rather than traditional prefix extraction
  - Quick check question: How does the input-output pair differ between prefix encoding and the trace encoding used in this paper? (Hint: think about $x_t$ and $y_t$ alignment)

- **Concept: Parameter-Efficient Fine-Tuning (PEFT)**
  - Why needed here: The core contribution relies on PEFT (specifically LoRA and Freezing) to adapt massive LLMs on small event logs without prohibitive cost
  - Quick check question: In LoRA, which matrices are trained, and which remain frozen? ($A, B$ vs $W$)

- **Concept: Decoder-Only Architectures**
  - Why needed here: The study uses GPT-2, Qwen, and Llama (decoder-only). These models predict the next token based on prior context, analogous to predicting the next event in a trace
  - Quick check question: Why might a decoder-only model be preferred over an encoder-only model (like BERT) for next-activity prediction?

## Architecture Onboarding

- **Component map:** Categorical Embeddings (Activity Labels) + Linear Projection (Time features) -> Summation -> Frozen LLM Backbone with LoRA Adapters -> Dual Linear Heads (Multi-task) -> Softmax (NA) / MSE (RT)

- **Critical path:**
  1. **Tokenizer Replacement:** DO NOT use standard LLM tokenizers. Map activity labels to unique integer IDs and initialize a new `nn.Embedding` layer
  2. **Projection Alignment:** Ensure the output of the new embedding layer matches the hidden dimension of the pre-trained backbone (e.g., 1024 for Llama)
  3. **Head Attachment:** Add linear heads to the final hidden state; do not use the pre-trained language modeling head

- **Design tradeoffs:**
  - **Freezing vs. LoRA:** Use LoRA for RT prediction (better regression adaptation) and Freezing (or unfreezing last layers) for NA prediction (classification alignment)
  - **RNN vs. LLM:** RNNs are faster and smaller but require heavy hyperparameter tuning. LLMs converge faster (fewer epochs) but have higher inference latency

- **Failure signatures:**
  - **Spiky Loss:** High variance in RT loss curves indicates the LLM is struggling to adapt to regression; verify LoRA implementation or lower learning rate
  - **Language Interference:** If the model predicts tokens not in the activity set, you have failed to replace the output layer correctly or are using the wrong tokenizer
  - **S-NAP Performance Drop:** If using narrative styles, ensure future information isn't leaked in the prompt (unrealistic setting)

- **First 3 experiments:**
  1. **Sanity Check (Trace Encoding):** Train a Single-Task RNN (LSTM) using trace encoding to establish a baseline for convergence speed and accuracy
  2. **PEFT Ablation (NA Task):** Compare Full Freezing vs. Unfreezing Last Layer on Next Activity prediction to verify the "classification alignment" claim
  3. **LoRA vs. Freezing (RT Task):** Run a multi-task setup comparing standard Freezing against LoRA specifically on the RT MSE metric to validate the regression adaptation hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can quantization techniques effectively reduce the memory footprint of PEFT-adapted LLMs for process data while maintaining predictive performance?
- **Basis in paper:** [explicit] The authors state in Section 5.3: "future work could explore quantization to shrink models further" since "PEFT requires more trainable parameters than RNNs."
- **Why unresolved:** The paper did not experiment with any quantization methods, focusing only on PEFT approaches (LoRA, freezing) without model compression techniques
- **What evidence would resolve it:** Experiments comparing INT8/INT4 quantized LLMs against full-precision models on the same NA and RT tasks, measuring both predictive accuracy (MAcc, MSE) and memory usage

### Open Question 2
- **Question:** How can decoder-only LLMs be effectively adapted for suffix prediction given the structural disalignment between activity-label inputs and activity-label-plus-remaining-time outputs?
- **Basis in paper:** [explicit] The authors explicitly note: "suffix prediction is omitted due to the fact that we are using decoder-only LLMs, so it raises the disalignment between input and output data."
- **Why unresolved:** The paper focused only on NA and RT prediction; suffix prediction requires generating variable-length sequences with mixed output types, which does not map cleanly to next-token prediction
- **What evidence would resolve it:** Architectural modifications or output layer designs enabling suffix prediction with process-adapted LLMs, evaluated on standard suffix metrics (e.g., edit distance, accuracy)

### Open Question 3
- **Question:** What LoRA hyperparameter configurations (rank r, alpha scaling, target layer selection) optimize the trade-off between memory efficiency and RT prediction accuracy?
- **Basis in paper:** [explicit] Section 5.3 states: "we used LoRA with default settings, which could be tuned to reduce memory usage" and Section 5.1 used fixed r=256, alpha=512
- **Why unresolved:** The paper did not conduct ablation studies on LoRA hyperparameters, leaving unclear whether smaller ranks could achieve comparable RT performance with lower memory
- **What evidence would resolve it:** Ablation experiments varying r (e.g., 8, 32, 64, 128, 256) and alpha across multiple datasets, reporting MSE, training time, and GPU memory consumption

## Limitations
- The study relies on a relatively small set of public event logs (5 datasets), limiting generalizability across diverse process domains
- RT prediction remains challenging even with LLMs, with performance still inferior to human experts in many real-world scenarios
- The work assumes fixed-length input traces without addressing variable-length trace handling mechanisms
- No ablation study on the optimal number of LoRA adapters or their placement within the transformer layers

## Confidence
- **High confidence**: LLM superiority over RNN baselines for NA prediction (supported by multiple datasets and consistent accuracy improvements)
- **Medium confidence**: LoRA effectiveness for RT regression (results show improvement but with higher variance across datasets)
- **Low confidence**: Claims about faster convergence requiring minimal hyperparameter tuning (training was still performed with grid search over multiple parameters)

## Next Checks
1. Test the adapted LLMs on additional process mining datasets (beyond BPI challenges) to validate cross-domain robustness
2. Conduct controlled experiments comparing different LoRA configurations (adapter placement, rank values) to optimize RT prediction performance
3. Implement and evaluate a variable-length trace handling mechanism to address the fixed-length limitation and assess impact on both NA and RT tasks