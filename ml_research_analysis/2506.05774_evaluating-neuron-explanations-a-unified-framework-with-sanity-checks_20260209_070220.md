---
ver: rpa2
title: 'Evaluating Neuron Explanations: A Unified Framework with Sanity Checks'
arxiv_id: '2506.05774'
source_url: https://arxiv.org/abs/2506.05774
tags:
- test
- neuron
- concept
- labels
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper unifies diverse neuron explanation evaluation methods
  under a single mathematical framework, NeuronEval, and proposes two novel sanity
  checks to identify reliable evaluation metrics. The authors theoretically and empirically
  test 18 metrics across vision and language models, finding that most commonly used
  metrics fail at least one sanity check.
---

# Evaluating Neuron Explanations: A Unified Framework with Sanity Checks

## Quick Facts
- **arXiv ID**: 2506.05774
- **Source URL**: https://arxiv.org/abs/2506.05774
- **Reference count**: 40
- **Primary result**: Unified neuron explanation evaluation framework identifies five metrics (Correlation, Cosine, AUPRC, F1, IoU) that pass both proposed sanity checks.

## Executive Summary
This paper introduces NeuronEval, a unified mathematical framework that formalizes diverse neuron explanation evaluation methods as similarity functions between neuron and concept activation vectors. The authors propose two novel sanity checks—Missing Labels Test and Extra Labels Test—to identify metrics that reliably distinguish between correct and incorrect explanations. Testing 18 metrics across vision and language models reveals that most commonly used metrics (Recall, AUC, Precision, MAD) fail at least one sanity check, while Correlation, AUPRC, F1, IoU, and Cosine pass both. Meta-AUPRC evaluation confirms Correlation, Cosine, and AUPRC as top performers. The paper concludes that evaluation should use metrics passing both sanity checks and highlights the importance of handling concept imbalance.

## Method Summary
The NeuronEval framework unifies explanation evaluation by expressing all methods as similarity functions s_M(a_k, c_t) between neuron activation vectors a_k and concept activation vectors c_t. Two sanity checks are proposed: the Missing Labels Test (randomly removing 50% of positive concept labels) detects metrics biased toward overly specific explanations, while the Extra Labels Test (doubling concept labels with spurious entries) detects metrics biased toward overly generic explanations. A metric passes if its score decreases when spurious labels are added or correct labels are removed (Decrease Acc > 90%). Meta-AUPRC evaluation on neurons with known ground truth provides final ranking.

## Key Results
- Only five metrics (Correlation, Cosine, AUPRC, F1, IoU) pass both sanity checks
- Recall, AUC, and top-and-random sampling fail Extra Labels Test (0.00% decrease accuracy)
- Precision, Inverse Balanced Accuracy, and MAD fail Missing Labels Test (45.73-59.81% decrease accuracy)
- Correlation, Cosine, and AUPRC achieve highest meta-AUPRC scores across vision and language models
- Most failing metrics' scores approach random as concept frequency γ→0

## Why This Works (Mechanism)

### Mechanism 1
Diverse neuron explanation evaluation methods can be unified under a single mathematical formulation. The NeuronEval framework expresses all evaluations as functions s_M(a_k, c_t) where a_k is the neuron activation vector and c_t is the concept activation vector. This reveals that seemingly different evaluations (crowdsourced human ratings, IoU with labeled data, automated LM simulation) are special cases varying only in metric choice, concept source, granularity, and probing dataset. The core assumption is that explanation quality can be measured by comparing how well concept presence predicts neuron activation (simulation framing) or vice versa (classification framing).

### Mechanism 2
The Missing Labels Test detects metrics biased toward overly specific explanations. By randomly removing half of positive concept labels (c_t → c_t^-), the test measures whether a metric's score decreases. Metrics like Precision and MAD fail because they don't penalize explanations that miss activating inputs—their scores remain unchanged when labels go missing. The core assumption is that if concept t correctly describes neuron k, a random subset of t should be a worse description.

### Mechanism 3
The Extra Labels Test detects metrics biased toward overly generic explanations. By randomly doubling the concept label set (c_t → c_t^+), the test measures whether scores decrease. Metrics like Recall and AUC fail because they reward any explanation covering high-activation inputs, even if the explanation is too broad (e.g., "animal" for a "pet" neuron). The core assumption is that if concept t correctly describes neuron k, adding spurious labels should worsen the explanation.

## Foundational Learning

- **Concept**: Binary classification metrics (TP/FP/TN/FN, Precision, Recall, F1)
  - Why needed here: The paper shows these metrics have systematic biases on imbalanced data. Understanding their definitions reveals why Recall fails Extra Labels (numerator fixed, only TP matters) while Precision fails Missing Labels.
  - Quick check question: Given a neuron activating on 1% of inputs, why would Accuracy ≈ 99% even for a wrong explanation?

- **Concept**: Imbalanced classification and AUPRC
  - Why needed here: The core theoretical insight is that metric failures correlate with concept sparsity. As γ→0 (rare concepts), failing metrics' score differences approach zero while passing metrics maintain constant Δs.
  - Quick check question: Why does AUC perform well on balanced data but fail when concepts activate rarely (γ < 0.001)?

- **Concept**: Correlation vs. Cosine Similarity
  - Why needed here: These are mathematically equivalent after mean-centering, but Cosine has a critical failure mode: it's sensitive to average activation. Adding a constant to all activations drops Cosine meta-AUPRC from 89.85% to 4.43% while Correlation is unchanged.
  - Quick check question: If you shift all neuron activations by +1, which metrics' scores change?

## Architecture Onboarding

- **Component map**:
  ```
  NeuronEval Framework
  ├── Inputs: neuron activations a_k, concept vector c_t
  ├── Metric M: similarity function (18 tested)
  ├── Concept source: labeled data / crowdsourced / model pseudo-labels / generative
  ├── Granularity: whole-input / per-pixel / per-token
  └── Output: score s_M(a_k, c_t) ∈ [0,1] or [-1,1]

  Sanity Check Pipeline
  ├── Missing Labels Test: c_t → c_t^- (remove 50% labels)
  ├── Extra Labels Test: c_t → c_t^+ (double labels)
  └── Pass criterion: Decrease Acc > 90% on both tests
  ```

- **Critical path**: Choose metric → validate with sanity checks → apply to explanation evaluation. The paper's recommended path: use Correlation, AUPRC, or F1-score (all pass both tests); avoid Recall, AUC, MAD, and top-and-random sampling.

- **Design tradeoffs**:
  - Correlation: Best overall performance (avg rank 1.6), but requires full c_t evaluation
  - F1-score/IoU: Passes tests, lower labeling cost possible via combined Recall (crowdsourced) + Precision (generative)
  - AUPRC: Strong on imbalanced data, but no closed-form solution
  - Continuous metrics outperform binary (binarization loses information, hard to pick universal α threshold)

- **Failure signatures**:
  - Recall gives perfect scores to overly generic explanations (e.g., "animal" for "dog" neuron)
  - Precision gives perfect scores to overly specific explanations (e.g., "black cat" for "cat" neuron)
  - Top-and-random sampling causes Correlation to fail Extra Labels test
  - Cosine similarity collapses when neuron has high baseline activation
  - All failing metrics approach random performance as concept frequency γ→0

- **First 3 experiments**:
  1. Replicate Missing/Extra Labels tests on your target neurons using ground-truth c_t from labeled data. Compute Decrease Acc for Recall, Precision, F1, Correlation to verify framework applicability.
  2. Run meta-AUPRC evaluation on final-layer neurons (where ground truth is known) to rank metrics for your specific domain. Use 5% validation split for hyperparameter tuning.
  3. Test your explanation pipeline with a passing metric (Correlation) vs. a failing one (Recall). Generate explanations for polysemantic neurons and verify that only the passing metric penalizes incomplete explanations.

## Open Questions the Paper Calls Out

### Open Question 1
How can the NeuronEval framework be effectively extended to evaluate output-based neuron explanations? The authors state in Appendix B.2 that extending this framework or creating a similar one for output-based evaluations is an important direction for future work. This remains unresolved because the current framework strictly models input-based explanations (x → f(x)), whereas output-based explanations involve modeling how intervening on a neuron changes the network output (z → f(z)), which requires different methodological tools.

### Open Question 2
How can evaluation methods that rely on generative models for concept sources (c_t) be modified to pass the Missing Labels Test? Appendix B.1 notes that c_t derived from generative models is inherently incomplete (missing labels) and proposes that finding remedies or combining them with other evaluations is a necessary future direction. This remains unresolved because generative models typically produce positive examples (prompts) but treat all other data as negative, resulting in false negatives that cause metrics to fail the Missing Labels Test.

### Open Question 3
What sampling strategies can reduce the labeling cost of high-performance metrics (e.g., Correlation, IoU) to make them viable for large-scale human evaluation? Section B.2 discusses the high cost of evaluating full concept vectors for metrics like Correlation compared to Recall (which only requires evaluating highly activating inputs), noting that effective sampling design is a promising direction for future work. This remains unresolved because while Recall is cost-efficient, it fails sanity checks. Passing metrics require dense labeling, which is currently prohibitively expensive for large datasets without biased sampling techniques like "top-and-random."

## Limitations
- The framework assumes binary concept activation vectors, which may oversimplify real-world explanations with graded relevance or multi-label structure.
- The Missing Labels Test may unfairly penalize Precision for generative concept sources where labels are inherently incomplete rather than missing.
- The Extra Labels Test's uniform random sampling creates artificial spurious labels that may not reflect actual domain-specific label noise patterns.
- The sanity checks focus on systematic biases but don't directly validate whether metrics capture human-intelligible explanations.

## Confidence
- **High Confidence**: The mathematical unification framework and the empirical finding that five metrics (Correlation, Cosine, AUPRC, F1, IoU) pass both sanity checks are well-supported with theoretical proofs and consistent results across datasets and models.
- **Medium Confidence**: The meta-AUPRC evaluation ranking of metrics is methodologically sound but depends on the specific choice of ground truth neurons and may not generalize to all neuron types.
- **Low Confidence**: The assumption that failing the Missing Labels Test is always problematic for generative concept sources—this may be domain-dependent and requires further validation.

## Next Checks
1. **Domain Transfer Validation**: Apply the sanity checks to neuron explanations in domains without ground truth (e.g., molecular biology or autonomous driving), using human expert validation to verify whether passing metrics indeed produce more faithful explanations.
2. **Multi-Label Extension**: Generalize the framework to handle multi-label concepts where c_t contains multiple positive labels per input, and test whether the same five metrics pass sanity checks in this more realistic setting.
3. **Label Noise Robustness**: Systematically vary the amount of random label noise (not just doubling or halving) to map the full robustness landscape of each metric and identify the noise threshold where passing metrics degrade to failing performance.