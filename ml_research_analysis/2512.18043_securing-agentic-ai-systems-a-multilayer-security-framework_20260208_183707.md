---
ver: rpa2
title: Securing Agentic AI Systems -- A Multilayer Security Framework
arxiv_id: '2512.18043'
source_url: https://arxiv.org/abs/2512.18043
tags:
- agentic
- security
- systems
- layer
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research addresses the gap in AI security frameworks for agentic
  AI systems, which possess autonomous, decision-making, and adaptive behaviors. Using
  Design Science Research methodology, the study develops MAAIS, a lifecycle-aware
  multilayer security framework designed to maintain CIAA (Confidentiality, Integrity,
  Availability, and Accountability) for agentic AI systems.
---

# Securing Agentic AI Systems -- A Multilayer Security Framework

## Quick Facts
- arXiv ID: 2512.18043
- Source URL: https://arxiv.org/abs/2512.18043
- Authors: Sunil Arora; John Hastings
- Reference count: 40
- Primary result: Lifecycle-aware multilayer security framework (MAAIS) for agentic AI systems with seven interdependent layers validated against MITRE ATLAS

## Executive Summary
This research addresses the critical gap in AI security frameworks for agentic AI systems, which possess autonomous, decision-making, and adaptive behaviors distinct from traditional generative models. Using Design Science Research methodology, the study develops MAAIS, a comprehensive framework designed to maintain CIAA (Confidentiality, Integrity, Availability, and Accountability) across the AI lifecycle. The framework consists of seven interdependent security layers: Infrastructure Security, Data Security, Model Security, Agent Execution & Control, Accountability & Trustworthiness, User & Access Management, and Monitoring & Audit.

Validation against MITRE ATLAS demonstrates comprehensive coverage of adversarial tactics targeting agentic AI systems, from reconnaissance through impact. The framework provides a structured, standardized approach for secure deployment and governance of agentic AI in enterprise environments, intended for security teams and AI platform engineers. The study contributes a practical solution for organizations deploying autonomous agents while maintaining robust security and accountability.

## Method Summary
The research employs Design Science Research methodology across four phases: problem identification (recognizing the gap in lifecycle-aware frameworks for agentic AI), objective definition (maintaining CIAA across AI lifecycle), artifact design (developing the seven-layer MAAIS framework), and evaluation (validating against MITRE ATLAS). The systematic literature review used search terms "AI security," "agentic AI," "AI agents," and "agentic AI risks" with preference for studies from 2022 onwards. The framework's effectiveness was assessed by mapping each layer's controls to MITRE ATLAS adversarial tactics to demonstrate comprehensive defensive coverage.

## Key Results
- Seven-layer defense-in-depth architecture provides comprehensive coverage against adversarial tactics targeting agentic AI systems
- Framework extends CIA triad to CIAA (adding Accountability) for governance of autonomous agent decisions post-hoc
- MITRE ATLAS mapping validates defensive coverage against documented adversarial behaviors across all attack stages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A seven-layer defense-in-depth architecture provides comprehensive coverage against adversarial tactics targeting agentic AI systems.
- Mechanism: Each layer addresses a distinct attack surface (infrastructure, data, model, execution, accountability, access, monitoring) while sharing interdependencies that create redundant controls—if one layer fails, adjacent layers provide compensating detection or containment.
- Core assumption: Adversaries must chain multiple tactic sequences (e.g., initial access → execution → persistence) to cause meaningful harm; layered controls increase the probability of detection or disruption at each step.
- Evidence anchors:
  - [abstract] "The framework consists of seven interdependent security layers"
  - [section IV] "The proposed MAAIS Framework is built on a layered architecture to achieve defense-in-depth and a zero-trust approach"
  - [corpus] Neighbor paper "A Safety and Security Framework for Real-World Agentic Systems" similarly argues that "safety and security are emergent properties arising from dynamic interactions among models, orchestrators, tools, and data"

### Mechanism 2
- Claim: Extending the CIA triad to CIAA (adding Accountability) enables governance of autonomous agent decisions post-hoc.
- Mechanism: Immutable logging, audit trails, and provenance mechanisms create attribution chains linking agent actions to initiating users, system contexts, and policy configurations. This enables forensic reconstruction and responsibility assignment even when humans were not in the loop during execution.
- Core assumption: Accountability mechanisms (logging, provenance) are tamper-resistant and cannot be altered by compromised agents; human oversight structures remain functional after incidents.
- Evidence anchors:
  - [section IV.A] "Accountability enables organizations to identify who is responsible for the AI Agent's decisions and outcomes"
  - [section V, Table I] MITRE ATLAS mapping shows Accountability & Trustworthiness layer addresses "Impact" tactics
  - [corpus] TRiSM paper emphasizes "Trust, Risk, and Security Management" as integrated concerns for agentic systems

### Mechanism 3
- Claim: Mapping framework controls to MITRE ATLAS tactics validates defensive coverage against known adversarial behaviors.
- Mechanism: Systematic alignment of each MAAIS layer to ATLAS tactics (e.g., Reconnaissance → Monitoring; Initial Access → User/Access + Infrastructure; Execution → Agent Execution & Control) demonstrates that for each documented adversary tactic, at least one framework layer provides detection, prevention, or response capability.
- Core assumption: MITRE ATLAS represents a sufficiently complete taxonomy of current adversarial tactics; novel attack patterns not yet documented in ATLAS may have incomplete coverage.
- Evidence anchors:
  - [section V] "Framework validation is conducted by mapping with the established MITRE ATLAS"
  - [Table I] Shows 12 ATLAS tactics mapped to 6+ MAAIS layers
  - [corpus] "Securing AI Systems: A Guide to Known Attacks and Impacts" provides complementary attack taxonomy but corpus evidence on validation methodology is weak

## Foundational Learning

- Concept: **Agentic AI vs. Traditional AI**
  - Why needed here: The framework's necessity hinges on understanding that agentic systems differ from generative models in their ability to take autonomous, multi-step actions with tool access—not just produce outputs.
  - Quick check question: Can you distinguish why a ChatGPT query-response interaction requires different security controls than an agent that books travel, schedules meetings, and executes financial transactions?

- Concept: **Defense-in-Depth / Zero-Trust Architecture**
  - Why needed here: MAAIS assumes familiarity with layered security models where no single control is trusted exclusively; each layer assumes others may fail.
  - Quick check question: If the Model Security layer fails to detect a backdoor injection, which other MAAIS layers could provide detection or containment?

- Concept: **MITRE ATLAS Taxonomy**
  - Why needed here: The paper uses ATLAS as its validation benchmark; understanding tactic categories (Reconnaissance, Initial Access, Execution, etc.) is essential for interpreting the mapping in Table I.
  - Quick check question: Which ATLAS tactic would "prompt injection leading to unauthorized API calls" most closely align with?

## Architecture Onboarding

- Component map:
  - Infrastructure Security → Data Security → Model Security → Agent Execution & Control → Accountability & Trustworthiness → User & Access Management → Monitoring & Audit
  - (Note: While presented as sequential, layers 5-7 provide cross-cutting horizontal concerns)

- Critical path: Infrastructure → Data → Model → Agent Execution form the foundational stack; Monitoring and Accountability provide cross-cutting observability; User/Access governs entry points. Assumption: Implementation should proceed bottom-up for layers 1-4, with layers 5-7 as parallel horizontal concerns.

- Design tradeoffs:
  - Strong sandboxing (Layer 4) may limit agent flexibility and task complexity
  - Extensive logging (Layer 7) creates storage costs and potential privacy concerns
  - Human oversight (Layer 5) adds latency but reduces autonomous risk

- Failure signatures:
  - Agent escapes sandbox and accesses unauthorized tools → Layer 4 policy enforcement gap
  - Model outputs drift from expected behavior without detection → Layer 3 or Layer 7 monitoring failure
  - Unable to attribute unauthorized action to user or configuration → Layer 5 provenance/logging compromise

- First 3 experiments:
  1. **Layer mapping exercise**: Take a documented ATLAS attack scenario (e.g., model extraction via API queries) and trace which MAAIS layers would detect, prevent, or respond at each stage.
  2. **CIAA stress test**: Design a scenario where an agent makes an unauthorized financial transaction; identify which accountability mechanisms would enable attribution and which logging must remain immutable.
  3. **Layer interdependency analysis**: Simulate Layer 3 (Model Security) failure via backdoor injection; document which other layers provide compensating controls and their detection latency.

## Open Questions the Paper Calls Out

- Question: How does the MAAIS framework map to specific global regulatory standards, such as the EU AI Act or ISO/IEC 42001, beyond the currently validated MITRE ATLAS tactics?
  - Basis in paper: [explicit] The Conclusion states, "Future work will include mapping it to other regulatory, compliance, and security standards."
  - Why unresolved: The current validation is limited to adversarial tactics (MITRE ATLAS) and does not address compliance with legal or organizational standards.
  - What evidence would resolve it: A comprehensive mapping matrix linking MAAIS controls to specific clauses of target regulations.

- Question: Does the application of the MAAIS framework effectively secure agentic AI in live enterprise environments without degrading operational performance?
  - Basis in paper: [inferred] The methodology section describes validation as a "preliminary exercise" mapping to MITRE ATLAS, without empirical testing in live systems.
  - Why unresolved: The paper provides theoretical coverage of adversarial tactics but lacks empirical evidence regarding latency, resource overhead, or efficacy in runtime deployments.
  - What evidence would resolve it: Quantitative results from pilot deployments showing incident reduction rates and system performance metrics.

- Question: What specific implementation guidance is required for practitioners to effectively operationalize the seven interdependent MAAIS layers?
  - Basis in paper: [explicit] The authors note the intent to "focus on creating clear guidance for each layer... [to] make it easier to integrate the model."
  - Why unresolved: The paper defines the "what" (the layers) but acknowledges the need for detailed guidance on the "how" of integration.
  - What evidence would resolve it: A technical implementation playbook or reference architecture utilized by enterprise engineering teams.

## Limitations

- Framework validation against MITRE ATLAS represents tactic-level coverage rather than technique-level granularity, creating uncertainty about protection against novel attack vectors
- Interdependence assumptions between layers remain theoretical without empirical validation of compensating control effectiveness
- Applicability across different agentic AI architectures (LLM-based agents, multi-agent systems, autonomous robotics) is not explicitly validated

## Confidence

**High Confidence**: The layered architecture approach and the identification of seven distinct security domains for agentic AI systems are well-supported by the literature review and align with established security principles.

**Medium Confidence**: The CIAA extension of the CIA triad is theoretically sound, but practical implementation challenges for accountability mechanisms in autonomous systems remain under-specified.

**Low Confidence**: The specific effectiveness of interdependent layer controls, the framework's performance against novel attack patterns not documented in ATLAS, and the generalizability across diverse agentic AI architectures require further empirical validation.

## Next Checks

1. **Technique-Level Coverage Validation**: Conduct a detailed mapping exercise comparing each MAAIS layer's controls against the complete MITRE ATLAS technique catalog (not just tactic categories). Identify any techniques that lack corresponding controls and assess whether novel techniques outside ATLAS are adequately addressed.

2. **Interdependency Stress Testing**: Design and execute controlled failure simulations where individual layers are compromised (e.g., model backdoor injection, sandbox escape) to measure the effectiveness and latency of compensating controls in adjacent layers. Document detection time, containment effectiveness, and any control gaps.

3. **Architecture-Agnostic Applicability Assessment**: Test the framework's controls against three distinct agentic AI architectures: LLM-based conversational agents, multi-agent coordination systems, and autonomous robotic systems. Document necessary adaptations, control effectiveness variations, and architectural dependencies that affect implementation.