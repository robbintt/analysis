---
ver: rpa2
title: Optimizing Long-Form Clinical Text Generation with Claim-Based Rewards
arxiv_id: '2510.02338'
source_url: https://arxiv.org/abs/2510.02338
tags:
- grpo
- clinical
- reward
- completeness
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces an evaluation-integrated reinforcement learning
  framework that couples Group Relative Policy Optimization (GRPO) with DocLens claim-level
  rewards to optimize long-form clinical text generation. Unlike prior approaches,
  it directly maximizes factual grounding and completeness without a separate reward
  model or human-authored references.
---

# Optimizing Long-Form Clinical Text Generation with Claim-Based Rewards

## Quick Facts
- arXiv ID: 2510.02338
- Source URL: https://arxiv.org/abs/2510.02338
- Reference count: 16
- Primary result: GRPO with DocLens claim rewards improves clinical note F1 by ~5% over baseline, with gating accelerating convergence

## Executive Summary
This work introduces an evaluation-integrated reinforcement learning framework for long-form clinical text generation, coupling Group Relative Policy Optimization (GRPO) with DocLens claim-level rewards. The approach directly maximizes factual grounding and completeness without a separate reward model or human-authored references. A reward-gating strategy accelerates convergence by suppressing low-quality updates. On two clinical benchmarks, GRPO improves DocLens F1 scores by ~4.6–6.9% over a strong instruction-tuned baseline, with the gated variant converging in fewer epochs. Independent GPT-5 evaluation further confirms gains in completeness, brevity, and reduced hallucinations.

## Method Summary
The method uses Llama-3.1-8B-Instruct as the policy model, generating k=3 SOAP note candidates per dialogue. Reference claims are pre-extracted from dialogues using DocLens with GPT-4o. Rewards are computed as claim-level precision, recall, and F1 between generated and reference claims, scaled to [0,10] and gated at F1<0.6. GRPO updates the policy using group-relative baselines (mean reward of sampled candidates) without a critic network. Training runs for 2-3 epochs with learning rate 5×10⁻⁶ and gradient accumulation 2.

## Key Results
- GRPO improves DocLens F1 by ~4.6–6.9% over Llama-3.1-8B-Instruct baseline on clinical benchmarks
- Reward-gating variant achieves comparable performance in 2 epochs vs 3 epochs for standard GRPO
- GPT-5 evaluation shows reductions in hallucinations and omissions, with gains in completeness and brevity

## Why This Works (Mechanism)

### Mechanism 1: Deterministic Claim-Based Rewards
Replacing a learned reward model with deterministic DocLens claim evaluation stabilizes policy optimization by removing noisy approximation errors. The LLM-as-judge provides a stable ground truth based on source dialogue rather than fluctuating proxy preferences.

### Mechanism 2: Group Relative Baseline
Relative reward comparison within a generation group acts as a substitute for a value function, reducing memory overhead while maintaining gradient directionality. The mean reward of k samples serves as the baseline, eliminating the need for a separate critic network.

### Mechanism 3: Reward Gating
Setting rewards to zero for outputs below a threshold (F1<0.6) accelerates convergence by filtering low-quality gradient noise. This forces the model to climb strictly toward the high-reward manifold defined by the claim structure.

## Foundational Learning

### Concept: Claim-Level Entailment vs. Token Overlap
Why needed: Traditional metrics fail to capture clinical accuracy because they optimize for surface-form overlap rather than semantic truth. Understanding entailment is required to debug reward signals.

Quick check: If the generated note says "The patient has a history of smoking" and the dialogue says "The patient quit smoking 5 years ago," would claim-level precision reward penalize this? (Answer: Yes, if entailment check is strict).

### Concept: Critic-Free Policy Gradients (GRPO)
Why needed: The architecture requires understanding that the actor is updated directly using group statistics, rather than the actor-critic paradigm most engineers are familiar with.

Quick check: In PPO, you estimate Advantage A = Q - V. In GRPO, what acts as the V term? (Answer: The mean reward of the sampled group).

### Concept: SOAP Note Structure
Why needed: The reward signal is often section-specific. The system prompt enforces this structure, so engineers must ensure the tokenizer/model respects these headers as anchors.

Quick check: Which section is likely to suffer most from hallucinations if the dialogue is vague: Subjective or Plan? (Answer: Plan, as it requires inference of clinical guidelines not present in the dialogue).

## Architecture Onboarding

### Component map
Input dialogue -> GPT-4o + DocLens -> Reference claims (cached) -> Llama-3.1-8B-Instruct (policy) -> Sampler (k=3) -> Reward engine (claim extraction + entailment) -> GRPO optimizer

### Critical path
The pipeline relies on pre-cached reference claims. If initial GPT-4o extraction misses a critical medical fact, the RL loop will actively penalize the policy model for including that fact.

### Design tradeoffs
- Compute vs. Stability: Using GPT-4o as reward judge increases latency/cost vs small BERT model but likely provides more stable convergence
- Exploration vs. Thresholding: The 0.6 F1 threshold speeds up convergence but reduces exploration of novel phrasing that might initially score low

### Failure signatures
- Reward hacking via verbosity: Model generates massive lists of vague claims to maximize recall at precision's expense
- Structural collapse: Model ignores SOAP format because claim-level reward is agnostic to section headers

### First 3 experiments
1. Sanity check: Feed "gold standard" human note and "bad" note into reward engine to verify F1 delta > 20%
2. Threshold ablation: Run training with gating thresholds 0.0, 0.4, 0.6 to validate optimal convergence speed vs final performance
3. Critic baseline comparison: Implement moving-average baseline alongside group-mean to verify group relative aspect adds specific value

## Open Questions the Paper Calls Out

### Open Question 1: Real-world robustness
To what extent does the framework maintain performance gains on noisy, real-world clinical conversations compared to clean benchmarks? Current datasets are "relatively clean," calling for deployment studies on noisier conversations.

### Open Question 2: Multi-objective optimization
Can the deterministic reward signal be expanded to handle competing constraints like balancing billing compliance with brevity? The framework is claimed to be "scalable to real-world settings" but hasn't demonstrated multi-objective trade-offs.

### Open Question 3: Clinician validation
Do reductions in hallucinations and omissions correlate with higher utility and safety ratings from human clinicians? While automated metrics show improvement, alignment with clinical workflow preferences remains unverified by domain experts.

## Limitations

- Evaluation environment is unusually clean with curated datasets exhibiting minimal noise
- Deterministic rewards inherit biases of the underlying LLM judge (GPT-4o) which lacks independent clinical reasoning validation
- Claim-level metric optimizes semantic truth but may overlook stylistic coherence and institutional documentation standards
- Scaling to longer or more complex note types may challenge the fixed-group baseline assumption

## Confidence

- **High Confidence**: GRPO algorithm and reward-gating mechanism function as described, with convergence speed gains supported by ablation data
- **Medium Confidence**: Claim that deterministic rewards are more stable than learned models is plausible but extent of gains untested beyond controlled corpus
- **Low Confidence**: Robustness to diverse clinical domains, document types, or noisier input data is speculative and unmeasured

## Next Checks

1. Test DocLens pipeline on held-out dialogues with adversarial cases (negations, temporal qualifiers, ambiguous terminology) to verify clinical nuance capture
2. Perform threshold ablation study sweeping 0.0, 0.4, 0.6, 0.8 while tracking final F1, reward variance, and generation diversity
3. Evaluate trained model on out-of-domain clinical corpus to quantify robustness and detect overfitting to training dialogue style