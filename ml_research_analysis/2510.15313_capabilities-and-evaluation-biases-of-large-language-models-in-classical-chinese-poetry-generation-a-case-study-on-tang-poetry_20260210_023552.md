---
ver: rpa2
title: 'Capabilities and Evaluation Biases of Large Language Models in Classical Chinese
  Poetry Generation: A Case Study on Tang Poetry'
arxiv_id: '2510.15313'
source_url: https://arxiv.org/abs/2510.15313
tags:
- uni0000004a
- uni00000005
- uni00000054
- uni00000053
- uni00000057
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates large language models (LLMs)
  for generating and judging classical Chinese Tang poetry. A three-step framework
  combining computational feature extraction, LLM-as-a-judge cross-evaluation, and
  human expert validation was applied to six state-of-the-art models.
---

# Capabilities and Evaluation Biases of Large Language Models in Classical Chinese Poetry Generation: A Case Study on Tang Poetry

## Quick Facts
- arXiv ID: 2510.15313
- Source URL: https://arxiv.org/abs/2510.15313
- Reference count: 40
- Models produce fluent poetry but LLM judges exhibit "echo chamber" effect, diverging from human expert evaluation

## Executive Summary
This study systematically evaluates six large language models for generating and judging classical Chinese Tang poetry using a three-step framework combining computational metrics, LLM-as-a-judge cross-evaluation, and human expert validation. While models demonstrate fluency and semantic competence, they exhibit significant failures in prosodic adherence that LLM judges consistently miss. The research reveals an "echo chamber" effect where automated evaluators converge on flawed quality standards that diverge sharply from human assessments, particularly in technical dimensions like tone patterns and rhyme schemes.

## Method Summary
The evaluation framework combines computational feature extraction (TF-IDF, Shannon entropy, semantic similarity via Chinese sentence embeddings), LLM cross-evaluation (6×6 matrix of models scoring each other on five dimensions at T=0.2), and human expert validation (150 poems scored by two classical literature experts). Generation occurs at T=0.4 with structured prompts specifying form, poet, theme, emotion, and imagery. The study processes 15,000 poems total, with human evaluation focusing on top-performing models from the LLM ranking phase. Statistical analysis includes one-way ANOVA, Tukey HSD post-hoc tests, and Spearman correlation for human-AI alignment.

## Key Results
- LLM judges converge on flawed standards that diverge from human expert judgment, exhibiting an "echo chamber" effect
- Gemma generated severe prosodic violations (human score 1.72/5) unnoticed by LLM judges who scored it similarly to technically correct Qwen output
- High cross-model semantic similarity (>0.94) on cultural concepts did not translate to authentic generation or evaluation of those concepts
- Qwen performed best technically but received low automated scores, highlighting misalignment between human and LLM evaluation standards

## Why This Works (Mechanism)

### Mechanism 1: Echo Chamber Effect in LLM-as-a-Judge Evaluation
LLMs trained on overlapping corpora develop shared evaluation biases, creating a closed loop where evaluators reinforce each other's blind spots. When Model A and Model B both trained on similar web text evaluate each other's poetry, they converge on standards even if both are wrong about quality. The echo chamber is strongest when evaluators share similar architectures and training data, missing technical failures like prosodic violations.

### Mechanism 2: Divergence Between Surface Fluency and Prosodic/Formal Competence
LLMs optimize for statistical fluency and semantic relevance learned from broad text corpora, but prosodic rules (tone patterns, rhyme schemes) require precise, rule-based knowledge often underrepresented in training. An LLM evaluator may rate a semantically beautiful line highly even if it violates the "level-oblique" tonal rule fundamental to Tang poetry, because standard models lack explicit prosodic checkers.

### Mechanism 3: Cultural and Stylistic Knowledge Alignment Paradox
Models encode statistical approximations of cultural knowledge (e.g., Li Bai's style, willow-sadness associations) sufficient for classification but insufficient for faithful generation. Despite >0.94 cross-model semantic similarity on poet styles, generated poems fail to express stylistic differences. The internal representation binds abstract cultural associations poorly into coherent, formally correct outputs.

## Foundational Learning

**LLM-as-a-Judge:** Understanding that using an LLM to score generated text is not neutral; the judge inherits biases from its training. Quick check: If two models trained on similar web text evaluate each other's essays, why might their scores converge even if both are wrong?

**Prosodic Constraints in Classical Chinese Poetry:** Tang poetry has rigid, non-local rules (tone patterns, rhyme) distinct from Western meter, making them a hard test for LLMs. Quick check: In regulated verse, if a line's tonal pattern violates the "level-oblique" rule, should an LLM evaluator penalize it even if semantically beautiful?

**Information Theory in Text Analysis (Entropy):** Used to quantify lexical diversity and distinguish between creative flexibility and template-driven generation. Quick check: Two models generate poems with different entropy. Does higher entropy guarantee better quality? What else would you check?

## Architecture Onboarding

**Component map:** Generation Module → Computational Feature Extractor → LLM Cross-Evaluator → Human Expert Validator

**Critical path:** 1) Define generation dimensions and prompts 2) Run generation for all models at T=0.4 3) Compute computational metrics 4) Run 6×6 LLM cross-evaluation at T=0.2 5) Select top models for human validation 6) Compare human vs. LLM scores

**Design tradeoffs:** LLM-as-a-judge is faster but risks echo chamber bias; the three-step framework trades speed for validity. Generic metrics like BLEU fail for poetry; custom prosodic checkers are domain-specific but harder to implement. Human evaluation is expensive, so only 150 poems are sampled.

**Failure signatures:** Echo chamber: LLM judges give high scores to models human experts rate poorly on prosody. Semantic convergence without stylistic distinction: High similarity (>0.94) but no style differentiation in human evaluation. Entropy paradox: Low entropy may indicate template-driven generation or formal mastery.

**First 3 experiments:** 1) Reproduce echo chamber diagnostic: Run 3×3 LLM cross-evaluation on small set, check prosodic penalties 2) Ablate prosodic dimension: Remove from rubric, observe human-LLM correlation change 3) Test cultural motif association: Generate poems with specific imagery-emotion pairs, rate authenticity

## Open Questions the Paper Calls Out

**Open Question 1:** Does the "echo chamber" effect persist across other culturally complex creative domains like sonnets or multilingual poetry? The study's confinement to Tang poetry and six models limits generalizability.

**Open Question 2:** Can specialized metrics like rule-based prosody checkers or literary corpus embeddings better capture poetry nuances than prose-based semantic measures? Current metrics rely on prose-trained models missing metaphorical and rhythmic quality.

**Open Question 3:** To what degree does training data homogeneity cause high semantic similarity (>0.94) and collective blind spots? The study observes shared biases but doesn't isolate training data composition as causal variable.

## Limitations

- Human evaluation protocol lacks detail on expert qualifications and inter-annotator agreement reliability
- Cross-lingual contamination from non-Chinese trained models (Gemma) filtered but impact not discussed
- Data sampling strategy across five generation dimensions not specified, limiting generalizability of echo chamber findings

## Confidence

- Echo Chamber Effect in LLM-as-a-Judge Evaluation: **High** - Strong evidence from 6×6 matrix and specific Gemma case
- Divergence Between Surface Fluency and Prosodic/Formal Competence: **Medium** - Well-supported by contrast in human scores but lacks detailed prosodic error analysis
- Cultural and Stylistic Knowledge Alignment Paradox: **Low-Medium** - Suggestive evidence of knowledge gap but not systematically quantified or analyzed

## Next Checks

1. **Reproduce the Echo Chamber Diagnostic:** Run 3×3 LLM cross-evaluation on 50 poems per model, compute human-LLM correlation specifically for prosodic dimension to confirm LLM judges miss violations

2. **Ablate the Prosodic Dimension:** Remove prosodic adherence from evaluation rubric, re-run human-LLM alignment analysis to test if misalignment concentrates in formal constraints

3. **Test Cultural Motif Association:** Prompt models to generate poems with specific imagery-emotion pairs (willow-sadness, moon-loneliness), have human annotators rate authenticity and compare with LLM judges' scores on imagery/emotion dimensions