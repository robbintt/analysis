---
ver: rpa2
title: Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through
  Clinical Cognitive Chain Reasoning
arxiv_id: '2507.17539'
source_url: https://arxiv.org/abs/2507.17539
tags:
- data
- fundus
- retinal
- image
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of multimodal large language
  models (MLLMs) in specialized domains like ophthalmology, where fragmented annotation
  granularity and inconsistent clinical reasoning logic hinder precise cross-modal
  understanding. To overcome this, the authors introduce FundusExpert, an ophthalmology-specific
  MLLM with integrated positioning-diagnosis reasoning capabilities, along with FundusGen,
  a dataset constructed using the intelligent Fundus-Engine system.
---

# Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning

## Quick Facts
- **arXiv ID:** 2507.17539
- **Source URL:** https://arxiv.org/abs/2507.17539
- **Reference count:** 40
- **Primary result:** FundusExpert achieves 69.7% on Fundus-MMBench and 77.0% clinical consistency in report generation, surpassing GPT-4o by 29.4 percentage points

## Executive Summary
This work addresses the challenge of multimodal large language models (MLLMs) in specialized domains like ophthalmology, where fragmented annotation granularity and inconsistent clinical reasoning logic hinder precise cross-modal understanding. The authors introduce FundusExpert, an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning capabilities, along with FundusGen, a dataset constructed using the intelligent Fundus-Engine system. FundusExpert, fine-tuned with instruction data from FundusGen, achieves state-of-the-art performance in ophthalmic question-answering tasks, surpassing the average accuracy of the 40B MedRegA by 26.6%.

## Method Summary
The method constructs FundusExpert by fine-tuning the InternVL2.5-8B model on FundusGen, a specialized dataset created through the Fundus-Engine pipeline. The engine automates localization and leverages MLLM-based semantic expansion to integrate global disease classification, local object detection, and fine-grained feature analysis within single fundus images. The dataset construction involves three stages: automated segmentation using nnU-Net, bounding box generation through DBSCAN clustering, and cognitive chain reasoning format conversion. The model is trained with full parameter fine-tuning using DeepSpeed ZeRO Stage 2 optimization for one epoch.

## Key Results
- FundusExpert achieves 69.7% accuracy on Fundus-MMBench and 66.7% on GMAI-MMBench
- Clinical consistency in report generation reaches 77.0%, significantly outperforming GPT-4o's 47.6%
- Localization IoU scores reach 73.8% for optic disc detection
- Scaling law between data quality and model capability shows L ∝ N^0.068

## Why This Works (Mechanism)

### Mechanism 1: Multi-Granularity Semantic Alignment
Fusing global disease labels, local bounding boxes, and fine-grained features into a single "collaborative annotation" reduces the visual-language gap by forcing the model to associate specific image regions with high-level diagnostic concepts. The Fundus-Engine automates the generation of "Grounding Reports" where text descriptions are explicitly anchored to coordinates (e.g., `<ref>optic disc</ref><box>[[136, 452...]]</box>`), preventing hallucination by requiring spatial verification.

### Mechanism 2: Cognitive Chain Reasoning
Structuring training data to mimic the clinical workflow (localization → feature analysis → diagnosis) improves diagnostic accuracy and generalization compared to single-turn flat mappings. The model is fine-tuned on multi-turn dialogues that explicitly build an "evidence chain," learning a dependency graph rather than just pattern matching by verifying fine-grained features before concluding a diagnosis.

### Mechanism 3: Specialized Semantic Expansion (Data Scaling Law)
High-quality, domain-specific synthetic data generated by a fine-tuned specialist (FundusExpert) yields better downstream performance than data generated by generalist models (GPT-4o), adhering to a specific scaling law (L ∝ N^0.068). The paper proposes replacing generalist MLLMs in the data generation loop with the specialized FundusExpert, creating tighter domain alignment where the "teacher" model produces less semantic noise regarding ophthalmic specifics.

## Foundational Learning

- **Instruction Tuning (Visual):** Why needed: The core contribution relies on curating specific instruction types (e.g., "Grounding Report" vs. "General Report"). Quick check: Can you distinguish between a "Regional QA" task and a "Multi-turn Diagnostic Reasoning" task in the dataset schema?

- **Semi-Supervised Segmentation (nnU-Net):** Why needed: The system relies on generating bounding boxes from segmentation masks. If you cannot trust the segmentation, you cannot trust the grounding. Quick check: How does the "Iterative Pseudo Labels" strategy in Stage 2 improve the detection of small lesions like microaneurysms?

- **Clustering for Object Detection:** Why needed: The translation from pixel-level segmentation to bounding boxes uses DBSCAN. Quick check: Why is a density-based clustering algorithm like DBSCAN preferred over simple bounding box calculation for irregularly shaped retinal lesions?

## Architecture Onboarding

- **Component map:** nnU-Net segmentation → DBSCAN clustering → GPT-4o/FundusExpert expansion → FundusGen dataset → InternVL2.5-8B fine-tuning → FundusExpert
- **Critical path:** 1) Run 3-stage annotation pipeline on raw fundus images, 2) Format output into 5 instruction types, 3) Full parameter fine-tuning of InternVL2.5 on curated FundusGen dataset
- **Design tradeoffs:** The system uses a complex 3-stage pipeline (segmentation → clustering → expansion) rather than end-to-end learning, trading pipeline complexity for data control and interpretability. The model is highly specialized (outperforms GPT-4o in domain) but relies on a specific cognitive chain structure that may be rigid for out-of-domain general tasks.
- **Failure signatures:** Spatial Hallucination (model correctly identifies disease but places lesion in wrong quadrant), Reasoning Disconnect (model outputs correct diagnosis but intermediate feature analysis describes inconsistent symptoms)
- **First 3 experiments:** 1) Verify Localization Pipeline (evaluate IoU of automated bounding box generation), 2) Ablate Cognitive Chain (train baseline on flat data vs. Cognitive Chain data), 3) Scaling Law Validation (train smaller models on subsets of FundusGen to verify L ∝ N^0.068)

## Open Questions the Paper Calls Out
None

## Limitations
- The reproducibility of the automated localization pipeline is uncertain due to unspecified DBSCAN clustering parameters and nnU-Net segmentation accuracy for rare ophthalmic lesions
- The effectiveness of the cognitive chain reasoning mechanism may be overestimated due to potential overfitting to specific reasoning templates used during training
- The quality and accessibility of the in-house dataset, which represents approximately half of the total data, remains unknown

## Confidence

**High Confidence:** The overall performance improvements of FundusExpert over baseline models (26.6% better than MedRegA, 77.0% clinical consistency vs 47.6% for GPT-4o) are well-supported by reported metrics and follow expected patterns for domain-specific fine-tuning.

**Medium Confidence:** The scaling law (L ∝ N^0.068) and the superiority of FundusExpert-generated data over GPT-4o-generated data are supported by the presented experiments, but the exact prompt templates and data generation process details are incomplete, limiting full reproducibility.

**Low Confidence:** The generalization capability of the cognitive chain reasoning mechanism to out-of-distribution cases or different reasoning patterns not seen during training remains uncertain, as the paper focuses primarily on in-distribution performance.

## Next Checks

1. **Localization Pipeline Verification:** Evaluate the automated bounding box generation (Stage 2) on a held-out test set to verify the reported IoU scores and assess whether the grounding mechanism is functioning as claimed.

2. **Cognitive Chain Ablation Replication:** Independently reproduce the ablation study comparing models trained on flat data versus cognitive chain-structured data to verify the 3.5% performance difference.

3. **Scaling Law Validation:** Train smaller models on systematically sampled subsets (10%, 25%, 50%, 75%) of the FundusGen dataset to independently verify the reported L ∝ N^0.068 scaling relationship.