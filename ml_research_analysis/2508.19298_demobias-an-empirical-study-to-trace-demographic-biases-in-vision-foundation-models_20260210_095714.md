---
ver: rpa2
title: 'DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation
  Models'
arxiv_id: '2508.19298'
source_url: https://arxiv.org/abs/2508.19298
tags:
- demographic
- groups
- blip-2
- paligemma
- llav
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates demographic biases in Large Vision Language\
  \ Models (LVLMs) for biometric face recognition tasks. We fine-tuned and evaluated\
  \ three widely used LVLMs\u2014LLaVA, BLIP-2, and PaliGemma\u2014on a demographically\
  \ balanced dataset covering 48 groups across ethnicity, gender, and age."
---

# DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models

## Quick Facts
- arXiv ID: 2508.19298
- Source URL: https://arxiv.org/abs/2508.19298
- Reference count: 31
- Primary result: Study found demographic biases in three LVLMs for face recognition, with PaliGemma and LLaVA showing higher biases across ethnic groups.

## Executive Summary
This study investigates demographic biases in Large Vision Language Models (LVLMs) for biometric face recognition tasks. We fine-tuned and evaluated three widely used LVLMs—LLaVA, BLIP-2, and PaliGemma—on a demographically balanced dataset covering 48 groups across ethnicity, gender, and age. Using group-specific BERTScores and Fairness Discrepancy Rate (FDR), we quantified performance disparities. Results showed that PaliGemma and LLaVA exhibited higher biases, particularly for Hispanic/Latino, Caucasian, and South Asian groups, with FDR values up to 0.552. BLIP-2 demonstrated more consistent performance across demographics but still showed some bias. The findings highlight the need for improved fairness in LVLMs for equitable biometric face recognition across diverse populations.

## Method Summary
The study fine-tuned three LVLMs—LLaVA, BLIP-2, and PaliGemma—on a demographically balanced dataset covering 48 groups across ethnicity, gender, and age. The researchers used BERTScores to evaluate performance across demographic groups and calculated Fairness Discrepancy Rate (FDR) to quantify disparities. The methodology involved training these models on a curated dataset with synthetic data augmentation to ensure demographic balance, then testing their performance on biometric face recognition tasks across the different groups.

## Key Results
- PaliGemma and LLaVA showed higher demographic biases with FDR values up to 0.552
- BLIP-2 demonstrated more consistent performance across demographics
- Hispanic/Latino, Caucasian, and South Asian groups were most affected by biases

## Why This Works (Mechanism)
The study's approach works by systematically evaluating LVLMs on a demographically balanced dataset, allowing for direct comparison of performance across different population groups. By using BERTScores and Fairness Discrepancy Rate (FDR), the researchers can quantify and compare biases in a standardized way. The fine-tuning process on diverse data helps reveal how well these models generalize across different demographic characteristics in face recognition tasks.

## Foundational Learning
- **Fairness Discrepancy Rate (FDR)**: A metric to quantify performance disparities across demographic groups; needed to measure bias in a standardized way, quick check involves calculating difference between best and worst performing groups.
- **BERTScore**: A metric for evaluating text generation quality using pre-trained language models; needed to assess face recognition accuracy in LVLMs, quick check involves comparing similarity scores across demographic groups.
- **Synthetic Data Augmentation**: Technique to create balanced datasets by artificially generating diverse examples; needed to achieve demographic balance when real-world data is limited, quick check involves verifying distribution of generated samples across groups.

## Architecture Onboarding
**Component Map**: Dataset -> LVLM Fine-tuning -> Performance Evaluation -> Bias Quantification
**Critical Path**: Data Preparation -> Model Training -> Inference -> Metric Calculation
**Design Tradeoffs**: Synthetic data augmentation provides demographic balance but may not capture real-world variations; using BERTScore focuses on semantic similarity but may miss biometric-specific nuances.
**Failure Signatures**: High FDR values indicate demographic bias; inconsistent performance across groups suggests model generalization issues.
**First Experiments**: 1) Test on additional LVLMs to verify generalizability. 2) Use alternative biometric metrics alongside BERTScore. 3) Validate synthetic data results with real-world face recognition scenarios.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on synthetic data augmentation may not fully capture real-world variations in face recognition scenarios
- Focus on three specific LVLMs limits generalizability to other models in the field
- Evaluation of only four demographic groups may miss subtler biases affecting smaller subgroups or intersectional identities

## Confidence
- PaliGemma and LLaVA exhibit higher biases compared to BLIP-2: Medium
- BLIP-2 demonstrates more consistent performance across demographics: Medium

## Next Checks
1. Conduct real-world face recognition tests across diverse populations to validate synthetic data findings
2. Expand evaluation to include a broader range of LVLMs and additional demographic subgroups to enhance generalizability
3. Incorporate additional biometric performance metrics beyond BERTScore to provide a more comprehensive assessment of recognition accuracy and fairness