---
ver: rpa2
title: 'What-If Analysis of Large Language Models: Explore the Game World Using Proactive
  Thinking'
arxiv_id: '2509.04791'
source_url: https://arxiv.org/abs/2509.04791
tags:
- game
- state
- wia-llm
- action
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WiA-LLM, a framework that trains large language
  models to perform explicit what-if analysis by forecasting the consequences of actions
  in dynamic game environments. The core innovation lies in modeling the game world
  through language-based state transitions, allowing the model to generate human-readable
  predictions and justifications for how the game state evolves in response to different
  actions.
---

# What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking

## Quick Facts
- **arXiv ID**: 2509.04791
- **Source URL**: https://arxiv.org/abs/2509.04791
- **Reference count**: 40
- **Key outcome**: WiA-LLM achieves 74.2% accuracy in forecasting game-state changes, outperforming base models by 27-41.6%

## Executive Summary
This paper introduces WiA-LLM, a framework that trains large language models to perform explicit what-if analysis by forecasting consequences of actions in dynamic game environments. The core innovation lies in modeling game worlds through language-based state transitions, enabling human-readable predictions and justifications for how game states evolve. Evaluated on the Honor of Kings MOBA environment, WiA-LLM demonstrates significant improvements in forecasting accuracy and strategic decision-making compared to baseline models.

## Method Summary
WiA-LLM employs a two-stage training approach: first, supervised fine-tuning on human-like reasoning traces, followed by reinforcement learning with rule-based rewards that compare predicted versus actual future states. The framework uses language-based state transitions to model the game world, allowing the model to generate explicit predictions about how different actions will affect the game environment. This explicit world modeling enables proactive reasoning about potential outcomes before actions are taken.

## Key Results
- WiA-LLM achieves 74.2% accuracy in forecasting game-state changes
- Outperforms base model by 27% and Deepseek-R1 by 41.6%
- Demonstrates strategic decision-making closer to expert human behavior
- Effective specifically in the Honor of Kings MOBA environment

## Why This Works (Mechanism)
The framework's effectiveness stems from explicit world modeling through language-based state transitions. By generating human-readable predictions and justifications for game state evolution, the model can reason about consequences before taking actions. The two-stage training approach (supervised fine-tuning followed by reinforcement learning with rule-based rewards) allows the model to learn both human-like reasoning patterns and optimize for accurate forecasting through comparison of predicted versus actual outcomes.

## Foundational Learning

**Language-based state transitions**: Representing game states and actions through natural language sequences, enabling the model to generate human-readable predictions and justifications. Why needed: Provides interpretability and allows reasoning about complex game dynamics through familiar language patterns. Quick check: Verify the model can generate coherent state transition narratives for simple game scenarios.

**Two-stage training approach**: Combining supervised fine-tuning on human reasoning traces with reinforcement learning using rule-based rewards. Why needed: First stage captures human-like reasoning patterns, while second stage optimizes for accurate forecasting through feedback on prediction quality. Quick check: Compare performance with only one training stage to isolate their individual contributions.

**Rule-based reward systems**: Using explicit comparison between predicted and actual future states as optimization signals. Why needed: Provides clear, objective feedback for reinforcement learning while avoiding the complexity of human judgment in reward design. Quick check: Test whether alternative reward formulations (e.g., probabilistic scoring) improve performance.

## Architecture Onboarding

**Component map**: Game environment -> State observation -> Language encoder -> Prediction generator -> Rule-based evaluator -> Reward signal -> Model parameters

**Critical path**: State observation → Language encoding → Prediction generation → Reward calculation → Parameter update

**Design tradeoffs**: Explicit world modeling provides interpretability and reasoning capability but increases computational overhead compared to end-to-end approaches. The rule-based rewards ensure objective evaluation but may miss nuanced strategic considerations that humans would prioritize.

**Failure signatures**: 
- Predictions that are grammatically correct but semantically implausible
- Overfitting to specific game scenarios rather than generalizable reasoning
- Computational bottlenecks during inference due to explicit state modeling
- Reward hacking through predictions that technically match rules but lack strategic value

**3 first experiments**:
1. Test prediction accuracy on simple, controlled game scenarios with known outcomes
2. Compare WiA-LLM's reasoning traces against human expert explanations
3. Evaluate transfer performance to different game environments or rule sets

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation limited to single MOBA game environment, raising generalizability concerns
- Computational costs and inference-time latency implications not addressed
- Rule-based rewards may not capture full complexity of real-world consequences
- No comparison to alternative forecasting approaches beyond two baseline models

## Confidence

| Claim Cluster | Confidence Level |
|---------------|------------------|
| Forecasting accuracy | Medium |
| Strategic decision improvement | Medium |
| Explicit world modeling benefits | Medium |

## Next Checks
1. Evaluate WiA-LLM performance across multiple distinct game environments and real-world decision-making scenarios to assess generalizability beyond the Honor of Kings MOBA environment.

2. Compare the computational overhead and inference-time performance of WiA-LLM against standard LLMs to quantify the practical trade-offs of explicit what-if analysis.

3. Conduct ablation studies to determine which components of the two-stage training approach (supervised fine-tuning vs. reinforcement learning) contribute most significantly to performance improvements.