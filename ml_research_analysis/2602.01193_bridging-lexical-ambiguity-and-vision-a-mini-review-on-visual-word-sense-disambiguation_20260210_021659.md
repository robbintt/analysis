---
ver: rpa2
title: 'Bridging Lexical Ambiguity and Vision: A Mini Review on Visual Word Sense
  Disambiguation'
arxiv_id: '2602.01193'
source_url: https://arxiv.org/abs/2602.01193
tags:
- disambiguation
- visual
- word
- sense
- vwsd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a systematic review of Visual Word Sense Disambiguation
  (VWSD), which extends traditional Word Sense Disambiguation to multimodal contexts
  using vision-language models like CLIP. It examines methods ranging from early multimodal
  fusion and graph-based techniques to recent approaches leveraging CLIP fine-tuning,
  LLM-generated glosses, and diffusion-based text-to-image generation.
---

# Bridging Lexical Ambiguity and Vision: A Mini Review on Visual Word Sense Disambiguation

## Quick Facts
- arXiv ID: 2602.01193
- Source URL: https://arxiv.org/abs/2602.01193
- Reference count: 23
- Primary result: LLM-enhanced and CLIP-based fine-tuned models achieve 6–8% MRR gains and improved Hit@1 accuracy over zero-shot baselines.

## Executive Summary
This review systematically examines Visual Word Sense Disambiguation (VWSD), an extension of traditional WSD that leverages vision-language models to ground ambiguous words in visual context. It traces the evolution from early multimodal fusion and graph-based methods to modern approaches using CLIP fine-tuning, LLM-generated glosses, and diffusion-based text-to-image generation. The review highlights that hybrid systems combining contrastive alignment with LLM reasoning consistently outperform baselines, though challenges persist in context scarcity, dataset bias, multilingual coverage, and evaluation frameworks. The analysis identifies a convergence towards hybrid architectures integrating contrastive models, generative models, and LLM reasoning for robust, context-aware, and multilingual disambiguation.

## Method Summary
This paper provides a systematic review of VWSD methods, synthesizing approaches from zero-shot CLIP retrieval to fine-tuned vision-language models and LLM-enhanced context generation. The review analyzes performance on benchmark datasets (SemEval-2023 Task 1) using MRR and Hit@1 metrics, identifying trends in method development and persistent challenges. While no single method is proposed, the review maps the landscape of VWSD techniques and their relative strengths, particularly emphasizing the role of fine-tuning and LLM integration in overcoming common sense and context scarcity biases.

## Key Results
- Fine-tuning CLIP/BLIP on VWSD-specific data improves MRR by 6–8% over zero-shot baselines.
- LLM-generated glosses significantly boost disambiguation accuracy by resolving semantic sparsity.
- Hybrid architectures combining retrieval with LLM reasoning achieve up to 95.77 MRR and 92% Hit@1.

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning contrastive vision-language models (e.g., CLIP, BLIP) on task-specific data improves disambiguation accuracy over zero-shot baselines, provided the training data aligns with the target distribution. Standard CLIP models prioritize general text-image alignment. By optimizing contrastive loss on VWSD-specific pairs (ambiguous word + correct image), the model learns to separate fine-grained visual senses (e.g., "bat" the animal vs. "bat" the equipment) within the shared embedding space. Performance degrades if the model overfits to the specific visual style of the training set or if the "ground truth" image for a sense is visually dissimilar to test candidates.

### Mechanism 2
Augmenting limited textual context with LLM-generated glosses or definitions significantly improves visual retrieval accuracy by resolving semantic sparsity. VWSD inputs often lack context (e.g., just the word "bank"). An LLM expands this into a rich definition ("a financial institution" vs. "river edge"). This richer text embedding provides a stronger signal for the vision-language model to match against the correct visual candidate, bridging the gap between abstract text and concrete pixels. Fails if the LLM "hallucinates" a plausible but incorrect definition for the specific context, or if the definition is too abstract for the visual encoder to match.

### Mechanism 3
Hybrid architectures combining retrieval (CLIP) with reasoning (LLMs) or structural propagation (Graphs) achieve state-of-the-art results by filtering candidates through semantic consistency checks. Pure similarity matching can fail on visually similar but semantically distinct concepts. Hybrid systems use an LLM to reason over candidates (Chain-of-Thought) or propagate labels over a graph of image/text similarities. This adds a "semantic verification" layer on top of raw feature alignment. High latency or API costs make deployment infeasible; failure occurs if the reasoning module is confused by the ambiguity of the retrieved image candidates.

## Foundational Learning

- **Concept**: Contrastive Language-Image Pre-training (CLIP)
  - **Why needed here**: CLIP is the foundational baseline for VWSD. Understanding its zero-shot mechanism (cosine similarity in shared latent space) is required to diagnose why it fails on ambiguous words (bias toward common meanings) and how fine-tuning fixes it.
  - **Quick check question**: If you embed the text "crane" and two images (a bird and a machine), will a standard CLIP model necessarily distinguish them correctly without context?

- **Concept**: Lexical Ambiguity & Polysemy
  - **Why needed here**: The paper explicitly differentiates general WSD from VWSD. You must understand that "sense" is not just a definition but a visual grounding problem (e.g., "coach" implies a bus in one context and a trainer in another).
  - **Quick check question**: Does the ambiguity in the phrase "I saw the man with a telescope" stem from lexical ambiguity (word meaning) or syntactic ambiguity (sentence structure)?

- **Concept**: Prompt Engineering & Context Augmentation
  - **Why needed here**: The review highlights that VWSD inputs are minimal. Understanding how to format prompts (e.g., "A photo of a [word] used for [context]") is critical for the LLM-integration mechanisms described.
  - **Quick check question**: How does adding a synonym or hypernym to a prompt change the embedding vector in a way that might help disambiguation?

## Architecture Onboarding

- **Component map**: Input Context -> Context Expander (Optional) -> Visual Encoder -> Textual Encoder -> Alignment Module -> Reasoning Module (Advanced)
- **Critical path**: The interaction between the Context Expander and the Alignment Module. If the expansion is noisy, the alignment fails. If the alignment is zero-shot (not fine-tuned), it may not capture the nuances of the expansion.
- **Design tradeoffs**:
  - Zero-shot vs. Fine-tuning: Zero-shot is faster to deploy but suffers from "common sense" bias. Fine-tuning resolves this but requires labeled data and compute.
  - LLM Dependency: Using LLMs for glosses boosts accuracy but introduces latency and hallucination risks.
  - Multilingual Approach: Translation-based approaches are simple but noisy; language-agnostic visual features are robust but harder to train.
- **Failure signatures**:
  - Common Sense Bias: Model always selects the image of a "baseball bat" even when context implies the animal.
  - Text-in-Image Bias: CLIP focuses on text rendered inside images rather than the visual content.
  - Hallucination: LLM generates a definition for the wrong sense, leading the visual retriever to the wrong image cluster.
- **First 3 experiments**:
  1. Baseline Establishment: Run zero-shot CLIP on the SemEval-2023 dataset to measure the "common sense bias" and establish a lower bound for MRR.
  2. Context Ablation: Implement a basic LLM gloss generator. Compare retrieval accuracy using: (a) raw word only, (b) word + LLM definition.
  3. Fine-tuning Validation: Fine-tune a lightweight adapter (e.g., LoRA) on CLIP using the VWSD training pairs. Compare against the zero-shot baseline to quantify the 6-8% MRR gain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can vision-language model architectures be redesigned to prioritize fine-grained semantic differentiation over general text-image similarity?
- Basis in paper: [explicit] The authors state that current models like CLIP have a "fundamental mismatch" with VWSD demands because they prioritize common meanings and text-in-image features.
- Why unresolved: Current contrastive models are pre-trained on general web data that favors frequent senses, making them architecturally biased against the nuanced distinctions required for disambiguation.
- What evidence would resolve it: An architecture that maintains high disambiguation accuracy on rare/polysemous senses without relying on OCR or textual overlays in candidate images.

### Open Question 2
- Question: How can non-English textual features be effectively aligned with visual representations for low-resource languages without relying on error-prone translation pipelines?
- Basis in paper: [explicit] The review notes that "The problem of aligning non-English textual features with shared visual representations remains largely unresolved" due to structural differences and data scarcity.
- Why unresolved: Existing multilingual approaches often depend on translation, which introduces noise and ambiguity, while low-resource languages lack sufficient visual-textual datasets.
- What evidence would resolve it: A language-agnostic model achieving comparable performance across diverse languages (e.g., English vs. Ukrainian) without intermediate English translation.

### Open Question 3
- Question: What mechanisms can effectively leverage LLMs for context expansion while preventing the propagation of generated hallucinations?
- Basis in paper: [explicit] The paper highlights that while LLMs solve context scarcity, they introduce "ongoing hallucination problems" where plausible but incorrect definitions mislead the disambiguation process.
- Why unresolved: Pipeline-based architectures currently lack verification steps to filter out semantic errors generated by LLMs before they cascade into the visual matching phase.
- What evidence would resolve it: A hybrid framework that demonstrates improved MRR over baselines while quantifying and reducing the error rate attributed to hallucinated glosses.

## Limitations

- The review aggregates results across multiple VWSD datasets and models without controlling for dataset-specific biases, making cross-method performance comparisons potentially misleading.
- Critical implementation details (fine-tuning hyperparameters, LLM prompting strategies, ensemble weighting schemes) are not provided, limiting reproducibility of the reported 6-8% MRR improvements.
- The analysis primarily focuses on English-language VWSD, with limited discussion of cross-lingual generalization and multilingual ambiguity resolution challenges.

## Confidence

- **High confidence**: The identification of common sense bias and text-in-image bias as persistent challenges in VWSD; these are consistently observed across methods and datasets.
- **Medium confidence**: The reported performance gains from LLM-context augmentation and fine-tuning; while well-supported by individual studies, the absolute magnitude varies significantly across implementations.
- **Medium confidence**: The convergence towards hybrid architectures combining contrastive alignment and LLM reasoning; this represents a clear trend but specific architectural choices and their relative contributions remain unclear.

## Next Checks

1. **Bias Characterization**: Conduct systematic error analysis on a held-out test set to quantify the frequency and types of common sense biases (e.g., "bat" always selected as sports equipment) and text-in-image artifacts across different vision-language models.
2. **Context Expansion Validation**: Implement a controlled ablation study comparing retrieval accuracy with: (a) raw words only, (b) minimal context, (c) LLM-generated glosses using standardized prompting, to measure the true contribution of context augmentation.
3. **Fine-tuning Configuration**: Reproduce a VWSD-specific fine-tuning experiment using the SemEval-2023 dataset with documented hyperparameter sweeps (learning rates, batch sizes, adapter configurations) to establish reliable performance baselines and identify optimal training settings.