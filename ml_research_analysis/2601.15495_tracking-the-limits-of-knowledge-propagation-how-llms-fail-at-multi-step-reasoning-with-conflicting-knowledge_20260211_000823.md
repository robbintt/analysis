---
ver: rpa2
title: 'Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step
  Reasoning with Conflicting Knowledge'
arxiv_id: '2601.15495'
source_url: https://arxiv.org/abs/2601.15495
tags:
- knowledge
- answer
- question
- reasoning
- facts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TRACK introduces a benchmark for evaluating how LLMs propagate
  knowledge through multi-step reasoning under conflicting conditions. It uses a two-stage
  process of knowledge probing to identify gaps, followed by knowledge injection with
  conflicting facts.
---

# Tracking the Limits of Knowledge Propagation: How LLMs Fail at Multi-Step Reasoning with Conflicting Knowledge

## Quick Facts
- arXiv ID: 2601.15495
- Source URL: https://arxiv.org/abs/2601.15495
- Authors: Yiyang Feng; Zeming Chen; Haotian Wu; Jiawei Zhou; Antoine Bosselut
- Reference count: 40
- Key outcome: TRACK benchmark reveals LLMs struggle to integrate conflicting knowledge during multi-step reasoning, with performance degrading as more conflicting facts are provided.

## Executive Summary
TRACK introduces a benchmark for evaluating how LLMs propagate knowledge through multi-step reasoning under conflicting conditions. The benchmark uses a two-stage process of knowledge probing to identify gaps, followed by knowledge injection with conflicting facts. Experiments across three scenarios—multi-hop QA (WIKI), code generation (CODE), and mathematical reasoning (MATH)—show that providing correct conflicting facts often yields limited gains or even degrades performance. Models struggle both with faithful integration of new facts and with accurate reasoning even when facts are correctly used.

## Method Summary
TRACK evaluates LLMs' ability to propagate conflicting knowledge through multi-step reasoning via two stages: knowledge probing (identify gaps) and knowledge injection (test reasoning with new facts). The benchmark uses 1,500 examples across WIKI, CODE, and MATH scenarios. Knowledge probing identifies unknown facts through M=10 samples per atomic fact query. Knowledge injection provides context C (aggregated facts based on KAS parameter) in open-book setting. Evaluation uses domain-specific AP, FKE (via NLI verification), and HP metrics. Backbone models include Llama-3.2, Qwen-3, GPT-4.1-mini, and o4-mini with methods like Append, FT-CK, and MeLLo.

## Key Results
- Models frequently fail to integrate contextually-provided conflicting facts, instead relying on parametric knowledge
- Even when models successfully integrate all required atomic facts, they can still produce incorrect final answers due to flawed reasoning execution
- Initial failures to integrate facts trigger downstream failures, creating a compounding effect where a single mistake derails multi-step reasoning
- Performance degrades further as more conflicting facts are provided (higher KAS values)
- A primary failure mode is the large gap between achieving correct answers (AP) and using faithful reasoning (FKE)

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Integration Failure Under Conflict
- Claim: LLMs frequently fail to incorporate contextually-provided conflicting facts into their reasoning chains, instead relying on parametric knowledge.
- Core assumption: The gap between AP and FKE scores indicates faithfulness failures, not just measurement noise.
- Evidence: Large gaps between AP and FKE scores in Llama-3.2 models on WIKI scenario demonstrate integration failures.

### Mechanism 2: Reasoning Breakdown Post-Integration
- Claim: Even when models successfully integrate all required atomic facts, they can still produce incorrect final answers due to flawed reasoning execution.
- Core assumption: The FKE metric correctly identifies when all facts are entailed in reasoning.
- Evidence: Consistent gap between FKE and HP scores, where o4-mini with Append-T achieves 76.7% FKE but only 40.4% HP.

### Mechanism 3: Error Propagation from Cascading Failures
- Claim: Initial failures to integrate facts trigger downstream failures, creating a compounding effect where a single mistake derails multi-step reasoning.
- Core assumption: Error propagation can be distinguished from independent failures at each step.
- Evidence: Direct failures account for 7.9–14.6% of outcomes, triggering massive cascades of error propagation failures (7.6%–32.2%).

## Foundational Learning

- Concept: Knowledge Conflicts in LLMs
  - Why needed here: TRACK's framework is built on creating and evaluating conflicts between parametric and contextual knowledge.
  - Quick check question: If a model correctly answers "Who is the current UK PM?" as "Rishi Sunak" but is given context stating "Keir Starmer is the current UK PM," what type of conflict exists? (Answer: Context-memory conflict)

- Concept: Parametric vs. Contextual Knowledge
  - Why needed here: The benchmark distinguishes between internal parametric knowledge and externally provided contextual knowledge.
  - Quick check question: A model trained on data up to December 2023 encounters a question about September 2025 events. Is its outdated knowledge parametric or contextual? (Answer: Parametric—learned during pretraining)

- Concept: Multi-Step Reasoning Decomposition
  - Why needed here: TRACK decomposes complex questions into atomic facts (Kq) and probing questions (qi) to identify knowledge gaps.
  - Quick check question: The question "Where was the spouse of the current UK Prime Minister born?" decomposes into which atomic facts? (Answer: Current UK PM identity, spouse identity, spouse's birthplace)

## Architecture Onboarding

- Component map: Knowledge Probing Stage -> Knowledge Injection Stage -> Evaluation Framework -> Benchmark Datasets
- Critical path: 1) Run knowledge probing to identify knowledge gaps (Kg) for all 1,500 examples, 2) Select injection method and KAS setting, 3) Provide context C in open-book setting, 4) Extract reasoning chain and final answer, 5) Evaluate using AP, FKE, and HP metrics
- Design tradeoffs: KAS=1 provides targeted injection but tests only individual conflict handling; KAS=500 tests robustness to distractors but may overwhelm model attention
- Failure signatures: High AP + Low FKE indicates unfaithful reasoning; Low AP + High FKE indicates reasoning breakdown; Performance degradation with increased KAS indicates attention failure
- First 3 experiments: 1) Baseline Probing: Run knowledge probing using provided scripts to generate KConf distributions, 2) Single-Conflict Injection: Evaluate using KAS=1 and Append method, 3) Scalability Stress Test: Increase KAS to 100 or 500 to evaluate robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on knowledge propagation benchmarks when the injected context contains non-factual or malicious data rather than correct updates?
- Basis in paper: The authors state in the Ethical Considerations that "Future work should build upon our benchmark by incorporating non-factual or malicious data... creating a more comprehensive testbed."
- Why unresolved: TRACK currently only evaluates the propagation of correct, ground-truth facts to measure reliability, ignoring adversarial inputs.
- What evidence would resolve it: Evaluation results from a modified version of TRACK containing adversarial or false knowledge injections.

### Open Question 2
- Question: Do the observed failures in knowledge propagation generalize to multi-modal, multi-lingual, or agentic reasoning environments?
- Basis in paper: The Limitations section notes the study is "restricted to knowledge propagation within English, text-based reasoning scenarios" and does not evaluate "multi-modal reasoning... or agent settings."
- Why unresolved: The dynamics of integrating conflicting knowledge may differ significantly when models utilize external tools or process non-textual inputs.
- What evidence would resolve it: Application of the TRACK framework to multimodal or tool-use benchmarks.

### Open Question 3
- Question: Can models maintain faithful knowledge propagation capabilities over continual or life-long sequences of knowledge updates?
- Basis in paper: The Limitations section mentions the "current evaluation does not cover such dynamics" of temporal drift and suggests the "extensibility paves the way for future studies on continual and life-long knowledge propagation."
- Why unresolved: The benchmark utilizes a static snapshot of knowledge and does not test how propagation performance holds up under sequential updates.
- What evidence would resolve it: A longitudinal evaluation tracking HP and FKE scores across sequential, timestamped knowledge updates.

## Limitations
- Benchmark relies heavily on GPT-5-mini as an LLM judge, which is not publicly documented or available
- Knowledge conflict evaluation framework assumes all provided facts are correct and relevant, ignoring real-world uncertainty
- Benchmark focuses on knowledge integration failures and reasoning breakdowns but doesn't isolate whether performance degradation stems from attention mechanisms, memory limitations, or fundamental reasoning architecture constraints

## Confidence

- **High Confidence:** The observation that LLMs struggle with multi-step reasoning when provided conflicting facts is well-supported by consistent performance gaps across all three scenarios.
- **Medium Confidence:** The claim that even successful fact integration can lead to reasoning failures requires further validation as the distinction between integration failure and reasoning breakdown could be influenced by evaluation metric sensitivity.
- **Low Confidence:** The error propagation mechanism, while intuitively plausible, relies on indirect inference from aggregated statistics rather than direct causal tracing of reasoning chains.

## Next Checks

1. **Judge Model Dependency Test:** Replicate key experiments using alternative judge models (e.g., GPT-4o, Claude 3.5) to assess whether observed performance patterns are consistent across different evaluation standards.

2. **Attention Mechanism Isolation:** Design controlled experiments varying context length and fact presentation order to determine whether performance degradation at higher KAS values stems from attention saturation or specific fact integration patterns.

3. **Reasoning Chain Causality Analysis:** Implement automated analysis of reasoning chains to directly trace how specific fact integration failures cascade into downstream errors, moving beyond aggregated error flow statistics.