---
ver: rpa2
title: Accurate and Efficient Low-Rank Model Merging in Core Space
arxiv_id: '2509.17786'
source_url: https://arxiv.org/abs/2509.17786
tags:
- space
- merging
- core
- full
- knots
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Core Space merging, a method to efficiently
  merge low-rank adaptation (LoRA) models by projecting their updates into a shared,
  information-preserving subspace. The approach uses singular value decomposition
  to construct reference bases that align task-specific updates, enabling merging
  in a compact Core Space without losing information.
---

# Accurate and Efficient Low-Rank Model Merging in Core Space

## Quick Facts
- arXiv ID: 2509.17786
- Source URL: https://arxiv.org/abs/2509.17786
- Reference count: 40
- Primary result: State-of-the-art normalized accuracy (94.16% on Llama 3 8B NLI tasks) with 600× speedup over full-space merging

## Executive Summary
This paper introduces Core Space merging, a method to efficiently merge low-rank adaptation (LoRA) models by projecting their updates into a shared, information-preserving subspace. The approach uses singular value decomposition to construct reference bases that align task-specific updates, enabling merging in a compact Core Space without losing information. This preserves the efficiency benefits of LoRA while improving accuracy across tasks. Empirical results show that merging in Core Space achieves state-of-the-art normalized accuracy on both vision and language tasks while reducing merging time by up to 600× compared to full-space methods.

## Method Summary
Core Space merging projects LoRA matrices into a shared reference basis constructed via SVD stacking across tasks. The method extracts orthonormal bases from stacked A and B matrices, then projects each task's updates into this common space. This enables efficient merging in T·r × T·r Core Matrices rather than full parameter space, preserving information while filtering out redundant directions. The approach supports various merging strategies (TIES, TSV, DARE) and generalizes to other parameter-efficient fine-tuning methods like VeRA.

## Key Results
- Achieves 94.16% normalized accuracy on Llama 3 8B NLI tasks, surpassing full-space methods
- Reaches 76.3% accuracy on vision tasks with 100× speedup over full-space TIES merging
- Reduces merging time by up to 600× while maintaining or improving accuracy across 8 tasks
- Core Space shows superior information density—no performance loss until 100% truncation vs. 80% in full space

## Why This Works (Mechanism)

### Mechanism 1: Shared Reference Basis via SVD Stacking
Stacking and decomposing LoRA components across tasks yields orthonormal bases spanning all task subspaces without information loss. The stacked SVD extracts reference bases U_ref_B and V_ref_A that capture the union of task-specific directions in a common coordinate system.

### Mechanism 2: Lossless Projection Through Least-Squares Alignment
Projecting task-specific LoRA updates into the shared reference basis incurs zero alignment error, enabling exact reconstruction. The least-squares solution R_B(t) = U_ref_B^T U_B(t) and Q_A(t) = V_ref_A^T V_A(t) achieves zero reconstruction error when reference bases span task subspaces.

### Mechanism 3: Information Density in Core Space Enables Better Merging
Core Space is information-dense (no redundant components), causing merging algorithms to operate on semantically meaningful directions rather than noise. Task-relevant information concentrates in the aligned subspace while non-aligned directions represent noise or task-specific interference.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed: Core Space operates on LoRA's decomposed A and B matrices rather than full ΔW
  - Quick check: Given LoRA rank r=16 and 8 tasks, what is the dimension of the Core Matrix M(t)?

- **Concept: Singular Value Decomposition (SVD)**
  - Why needed: Reference bases are extracted via SVD of stacked matrices
  - Quick check: Why does SVD of stacked [B(1); ...; B(T)] yield orthonormal columns spanning all B(t) column spaces?

- **Concept: Subspace Alignment / Task Interference**
  - Why needed: The paper frames merging as an alignment problem—misaligned task vectors interfere destructively
  - Quick check: If two task subspaces have SAR=0.3, what does this imply about their overlap and expected merge quality?

## Architecture Onboarding

- **Component map:** LoRA checkpoints {(A(t), B(t))} → Stack vertically/horizontally → SVD → Reference bases (U_ref_B, V_ref_A) → Core matrices M(t) → Merge operation M(·) → M_merged → Reconstructed update ΔW = U_ref_B M_merged V_ref_A^T → Apply to base model

- **Critical path:**
  1. Stacking correctness: Ensure dimensions align—A_stack: (T·r, n), B_stack: (m, T·r)
  2. Reference basis extraction: SVD must return full orthonormal columns
  3. Projection verification: After computing M(t), confirm reconstruction error ≈ 0
  4. Merge compatibility: Merging function M(·) must accept (T·r)×(T·r) inputs

- **Design tradeoffs:**
  - T·r vs. model dimension n: Larger T·r increases Core Matrix size but remains << n
  - Linear vs. non-linear merging: Linear methods are exactly equivalent in Core and Full space
  - Precision vs. speed: Float32 recommended for SVD; float16 may introduce reconstruction errors

- **Failure signatures:**
  - High reconstruction error (>1e-3): SVD numerical issues or incorrect stacking order
  - Degraded performance vs. full-space merge: Reference bases not spanning task subspaces
  - Memory overflow on large T: T·r × T·r Core Matrix grows cubically

- **First 3 experiments:**
  1. Validation on single task: Project one task's (A,B) to Core Space and reconstruct
  2. Two-task merge comparison: Merge 2 LoRA adapters using Task Arithmetic in Full vs. Core Space
  3. Multi-task scaling test: Merge 8 tasks on ViT-B/32. Compare TIES-Core vs. TIES-Full

## Open Questions the Paper Calls Out

### Open Question 1
How does Core Space merging scale in terms of performance and efficiency when the number of tasks (T) increases significantly (e.g., T > 50)? The cubic relationship with task count could create computational bottlenecks for large-scale merging scenarios.

### Open Question 2
Can the Core Space framework be adapted for parameter-efficient methods that do not explicitly use low-rank matrix decomposition, such as adapters or prompt tuning? The method relies on SVD of specific low-rank matrices, making it unclear how to project "core matrices" for methods with different structural representations.

### Open Question 3
Does applying Core Space alignment constraints during the fine-tuning process further enhance multi-task merging performance? While post-hoc alignment is efficient, proactively enforcing a shared subspace during training might reduce interference more effectively than aligning disparate bases after training.

## Limitations
- Numerical precision issues may introduce small reconstruction errors despite theoretical guarantees
- Assumes task subspaces can be meaningfully aligned without losing task-specific features
- Heterogeneous rank handling lacks detailed empirical validation

## Confidence

- **High Confidence:** Core Space achieves substantial speedups (up to 600×) and state-of-the-art normalized accuracy (94.16% on Llama 3 8B NLI tasks). SVD-based reference basis construction is mathematically sound.
- **Medium Confidence:** The claim that Core Space filtering reduces cross-task interference is supported by SAR correlation but needs more rigorous validation across diverse task combinations.
- **Medium Confidence:** Generalization to VeRA and other LoRA variants is theoretically sound but only briefly validated without comprehensive benchmarks.

## Next Checks

1. **Numerical Precision Validation:** Implement single-task projection and reconstruction to verify reconstruction error remains below 1e-6 across different hardware and SVD implementations.

2. **Task Heterogeneity Stress Test:** Create synthetic task pairs with controlled subspace overlap (0-100%) and measure Core Space performance degradation as overlap decreases, comparing against full-space baselines.

3. **Cross-Method Generalization:** Apply Core Space to heterogeneous-rank LoRA checkpoints from different fine-tuning runs, validating the padding/truncation approach with quantitative accuracy comparisons.