---
ver: rpa2
title: 'MedIL: Implicit Latent Spaces for Generating Heterogeneous Medical Images
  at Arbitrary Resolutions'
arxiv_id: '2504.09322'
source_url: https://arxiv.org/abs/2504.09322
tags:
- medil
- image
- images
- conv
- brain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedIL introduces an implicit latent space autoencoder that directly
  processes medical images at their native resolutions, avoiding the information loss
  from resampling. It leverages implicit neural representations to treat images as
  continuous signals, enabling encoding and decoding at arbitrary resolutions without
  prior interpolation.
---

# MedIL: Implicit Latent Spaces for Generating Heterogeneous Medical Images at Arbitrary Resolutions

## Quick Facts
- arXiv ID: 2504.09322
- Source URL: https://arxiv.org/abs/2504.09322
- Reference count: 26
- Key outcome: Implicit latent space autoencoder processes medical images at native resolutions without resampling, preserving fine anatomical details and improving downstream latent diffusion model outputs.

## Executive Summary
MedIL introduces an implicit latent space autoencoder that directly processes medical images at their native resolutions, avoiding the information loss from resampling. It leverages implicit neural representations to treat images as continuous signals, enabling encoding and decoding at arbitrary resolutions without prior interpolation. Trained on large, heterogeneous datasets of T1w brain MRIs and lung CTs, MedIL preserves fine anatomical details and outperforms fixed-size convolutional autoencoders in both reconstruction quality and downstream generative modeling.

## Method Summary
MedIL maps input 3D volumes to a latent space defined by physical coordinate grids rather than voxel counts. The architecture uses a 3D convolutional backbone combined with a Local Texture Estimator (LTE) module that learns frequency-based positional encodings to map continuous coordinates to feature values. Training occurs in two stages: global low-resolution pretraining followed by local high-resolution patch training. The model is evaluated on T1w brain MRIs (5 datasets, ~3,700 subjects) and lung CTs (LIDC-IDRI, 1,000 scans), with preprocessing including N4ITK bias correction, Freesurfer skull-stripping, and AC-PC alignment.

## Key Results
- MedIL preserves fine anatomical details and outperforms fixed-size convolutional autoencoders in both reconstruction quality and downstream generative modeling.
- Quantitative metrics show competitive or superior performance in PSNR, SSIM, and perceptual scores.
- Qualitative results highlight better boundary preservation and reduced artifacts compared to baseline methods.

## Why This Works (Mechanism)

### Mechanism 1
Treating medical images as continuous spatial signals rather than fixed discrete grids allows the autoencoder to preserve high-frequency anatomical details typically lost during standard resampling. The architecture utilizes a Local Texture Estimator (LTE) module within the encoder and decoder, which learns frequency-based positional encodings to map continuous coordinates to feature values. This effectively functions as a learned, resolution-agnostic super-resolution kernel, allowing the model to query signal intensity at any physical coordinate. Fails if the "continuous signal" assumption is violated by noise or artifacts that are discontinuous and stochastic.

### Mechanism 2
Decoupling the latent space shape from the input resolution improves downstream generative modeling by preventing the distortion of anatomical geometry. MedIL maps input volumes to a latent volume based on physical coordinate grids, allowing a small, high-resolution patch and a large, low-resolution slab to map to similar latent distributions if they cover similar anatomical regions. This stability aids the Diffusion model in learning anatomy rather than resolution artifacts. Fails if coordinate alignment is inaccurate, causing the encoder to map the same anatomical features to different locations in the latent space.

### Mechanism 3
Two-stage training (global low-res pretraining followed by local high-res patching) enables the model to learn both global context and fine details without exceeding GPU memory. The model is first pretrained on downsampled volumes to learn global anatomical layout, then fine-tuned on random patches extracted at native resolutions. The LTE allows the model to "zoom in" on these patches while retaining global priors established in stage one. Fails if patch size during stage two is too small to capture the receptive field required for the LTE to determine local orientation or texture.

## Foundational Learning

**Concept: Implicit Neural Representations (INRs) & Spectral Bias**
- Why needed here: MedIL replaces standard convolution/upsampling with an MLP-based function (LTE). You must understand that standard MLPs struggle with high-frequency details (spectral bias) and why positional encoding (used in LTE) is necessary to force the network to learn fine textures.
- Quick check question: Why would a standard MLP render a blurry image when mapping coordinates to pixel values, and how does the LTE module address this?

**Concept: Latent Diffusion Models (LDMs) & Perceptual Compression**
- Why needed here: The paper frames MedIL as an autoencoder for LDMs. Understanding that the LDM can only generate what the autoencoder preserves is crucial for interpreting the results.
- Quick check question: If the autoencoder blurs tissue boundaries, will the LDM be able to generate sharp boundaries? Why or why not?

**Concept: Coordinate Systems & Physical Resolution**
- Why needed here: Unlike natural images, medical images exist in physical space (mm). MedIL relies on continuous coordinate space. Understanding the difference between voxel space (indices) and physical space (mm) is vital for data preprocessing.
- Quick check question: How does MedIL handle two images of the same brain, one acquired at 1mm and one at 0.7mm, if they share the same physical coordinate bounds?

## Architecture Onboarding

**Component map:** Input Volume X + Coordinate Grid CX → Encoder (3D Conv Backbone + LTE) → Latent Volume Z → Decoder (LTE + Conv Backbone) → Output Reconstruction X̂

**Critical path:** The LTE (Local Texture Estimator) is the critical differentiator. It acts as the bridge between the discrete convolutional features and the continuous coordinate space. Implementation correctness here dictates reconstruction quality.

**Design tradeoffs:**
- Flexibility vs. Artifacts: The LTE allows arbitrary resolution generation but may introduce "hallucinated" structures (e.g., extraneous gyri) when extrapolating beyond training resolutions.
- Memory vs. Context: Two-stage training fixes memory issues but requires careful patch sampling to ensure the model doesn't lose global anatomical context during the high-res stage.

**Failure signatures:**
- Blurring: If LTE reverts to simple interpolation or spectral bias is not mitigated.
- Geometric Hallucinations: Generation of anatomically impossible structures (e.g., extra gyri) at resolutions far from the training mean.
- Coordinate Misalignment: Structural disjointedness if coordinate transformations are incorrectly calculated.

**First 3 experiments:**
1. **Ablation Validation:** Implement the "MedIL-Interp" baseline (replace LTE with trilinear) to confirm the LTE is actually providing a statistical benefit over standard interpolation on a hold-out set.
2. **Resolution Extrapolation:** Train on 1mm data only, then attempt to encode/decode a 0.8mm volume. Check if the model genuinely recovers details or just smooths the input.
3. **Latent Consistency Check:** Encode the same physical volume resampled to two different resolutions (e.g., 1mm and 2mm). Calculate the distance between the resulting latent vectors Z to verify resolution invariance.

## Open Questions the Paper Calls Out

**Open Question 1**
How can the spectral bias inherent in implicit neural representations be reduced to better preserve high-frequency details like narrow lung bronchial tubes? The authors state that future work "should focus on lowering the spectral bias found in many INRs" and note that the model currently "struggles with small, narrow branches."

**Open Question 2**
How can MedIL be effectively adapted for conditional image generation to ensure the synthesis of anatomically correct features? The discussion concludes that future work must focus on "expanding to conditional image generation to push towards generating clinical images with fine-scale, anatomically correct features."

**Open Question 3**
What architectural or training modifications are required to prevent the generation of unrealistic gyral topology when decoding at resolutions higher than the pretraining data? The authors observe that "MedIL did struggle to decode some samples into higher resolutions (roughly 0.8mm), producing unrealistic gyral topology."

**Open Question 4**
Is the reduced diversity (lower MS-SSIM) in generated brain MRIs a direct result of the model's sharpening of tissue boundaries? The authors note that MedIL samples had lower diversity than Conv. LDMs, but hypothesize this "may be due to MedIL's sharper tissue boundaries."

## Limitations
- Two-stage training approach may lead to inconsistencies between global anatomical priors and local high-resolution details.
- Model occasionally generates anatomically impossible structures (e.g., extraneous gyri) when extrapolating beyond training resolutions.
- LTE module's performance heavily depends on proper implementation of frequency encodings and ensemble averaging, with insufficient details provided for exact replication.

## Confidence

**High Confidence:** Claims regarding superior reconstruction quality (PSNR, SSIM metrics) and the general mechanism of implicit neural representations for continuous signal processing are well-supported by quantitative results and ablation studies.

**Medium Confidence:** Claims about resolution invariance and improved downstream generation quality are supported but require careful interpretation given occasional generation of anatomically implausible structures at extrapolated resolutions.

**Low Confidence:** Claims regarding the exact mechanism by which LTE preserves high-frequency details are difficult to verify without more specific implementation details about the frequency encoding and ensemble averaging procedures.

## Next Checks

1. **Ablation Validation:** Implement the "MedIL-Interp" baseline (replace LTE with trilinear interpolation) and compare reconstruction quality on a hold-out test set to confirm LTE provides statistically significant improvement beyond standard interpolation.

2. **Resolution Extrapolation Test:** Train exclusively on 1mm brain data, then attempt to encode/decode a 0.8mm volume. Analyze whether fine details are genuinely recovered or if the model merely smooths the input, revealing limitations of the continuous signal assumption.

3. **Latent Space Consistency:** Encode the same physical brain volume resampled to two different resolutions (e.g., 1mm and 2mm). Calculate the L2 distance between resulting latent vectors Z to empirically verify resolution invariance claims and test coordinate alignment accuracy.