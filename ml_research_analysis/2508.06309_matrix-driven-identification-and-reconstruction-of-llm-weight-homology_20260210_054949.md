---
ver: rpa2
title: Matrix-Driven Identification and Reconstruction of LLM Weight Homology
arxiv_id: '2508.06309'
source_url: https://arxiv.org/abs/2508.06309
tags:
- zhang
- homology
- mdir
- matrix
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Matrix-Driven Identification and Reconstruction
  (MDIR), a novel white-box method for detecting weight homology between large language
  models. MDIR operates solely on model weights without requiring inference, using
  matrix analysis, polar decomposition, and Large Deviation Theory to compute rigorous
  p-values for statistical significance.
---

# Matrix-Driven Identification and Reconstruction of LLM Weight Homology
## Quick Facts
- **arXiv ID**: 2508.06309
- **Source URL**: https://arxiv.org/abs/2508.06309
- **Reference count**: 40
- **Primary result**: MDIR achieves perfect AUC and accuracy scores on LeaFBench, outperforming existing homology detection methods

## Executive Summary
This paper introduces Matrix-Driven Identification and Reconstruction (MDIR), a white-box method for detecting weight homology between large language models using only weight matrices without requiring inference. MDIR leverages matrix analysis, polar decomposition, and Large Deviation Theory to compute rigorous p-values for statistical significance. The method achieves perfect detection scores on LeaFBench and can identify homology even with different tokenizers or layer counts, while also reconstructing weight correspondence mappings and detecting specific transformations like permutations or scaling.

## Method Summary
MDIR detects weight homology by computing orthogonal transformations between embedding matrices using polar decomposition, then solving permutation problems with the Hungarian algorithm. For models with different tokenizers, it uses common vocabulary tokens to align embeddings. The method applies Large Deviation Theory to establish rigorous p-value bounds under the null hypothesis of no homology. Layer correspondence is recovered through maximal ordered matching of attention matrices. The approach operates solely on weight matrices without requiring inference and demonstrates robustness against noise and certain adversarial attacks.

## Key Results
- Achieves perfect AUC = 1.0 and Accuracy = 1.0 on LeaFBench benchmark
- Successfully detects homology across different tokenizers and layer counts
- Provides rigorous p-value estimation using Large Deviation Theory
- Outperforms existing methods on cross-tokenizer and cross-architecture tests

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training trajectories preserve coordinates along function-preserving transformation groups, enabling homology detection via distance from identity.
- Mechanism: For orthogonal invariant optimizers, gradients along G-orbit directions are zero, so G-component remains at initialization. Homologous models share same G-coordinates, so g₁⁻¹g₂ ≈ e. MDIR computes this via trace of recovered transformation matrix.
- Core assumption: AdamW sufficiently respects gradient directions despite not being fully orthogonal invariant.
- Evidence anchors: Section 2.1 states "component of parameters along G-orbit remains unchanged" and derives ∂Loss/∂α⁽ⁱ⁾ = 0.
- Break condition: Adversary applies non-universally invariant transformations specific to weight values.

### Mechanism 2
- Claim: Orthogonal relationships in embedding matrices can be recovered via polar decomposition even under noise and tokenizer changes.
- Mechanism: Given E' ≈ EU^T + N_E, solve max_{X∈O(n)} Tr((E'^T E)X^T). Optimal X equals orthogonal factor from polar decomposition of E'^T E. For different tokenizers, use common vocabulary tokens C.
- Core assumption: Common tokens retain approximately aligned embeddings up to global transformation U.
- Evidence anchors: Abstract states method "identifies weight correspondence mappings by analyzing orthogonal relationships in embedding matrices."
- Break condition: VocabSize < EmbDim (degenerate case not applicable to modern LLMs).

### Mechanism 3
- Claim: Large Deviation Theory provides statistically rigorous p-values by characterizing tail probabilities of random orthogonal matrix traces.
- Mechanism: Under null hypothesis, Ũ is uniform over O(n) by Haar measure. Apply Bonferroni correction for n! permutations. LDT yields p ≤ n!·exp(−c²/2) where c = Tr(PŨ^T).
- Core assumption: Haar measure assumption justified when no systematic relationship exists between weights.
- Evidence anchors: Abstract states method "provides rigorous p-value estimation of statistical significance."
- Break condition: Theoretical bound may not be tight for r > 1/2 (rate function changes behavior).

## Foundational Learning

- **Polar Decomposition via SVD**
  - Why needed here: Core operation for recovering orthogonal transformations from noisy weight products.
  - Quick check question: Given A = USV^T from SVD, what is Ortho(A)?

- **Haar Measure on Orthogonal Groups**
  - Why needed here: Justifies null hypothesis distribution for statistical testing.
  - Quick check question: Why does invariance under left multiplication by any U ∈ O(n) imply uniformity?

- **Large Deviation Principle**
  - Why needed here: Connects trace statistics to p-values beyond what classical tests provide.
  - Quick check question: Why does the quadratic term −c²/2 dominate log(n!) for homologous models?

## Architecture Onboarding

- **Component map**: EmbeddingAnalyzer → computes Ũ and p-value; LayerMatcher → solves maximal ordered matching for different layer counts; LayerAnalyzer → recovers per-layer inner transformations (Q,K,V,O weights)

- **Critical path**: Start with embedding analysis (Algorithm 1) — in most non-adversarial cases this alone provides conclusive detection. Only proceed to layer analysis if embedding p-value is insignificant and you suspect obfuscation.

- **Design tradeoffs**: Hungarian algorithm gives exact O(n³) matching but is expensive for large dimensions. Paper notes faster heuristic: find max element in each row and verify valid permutation. Polar decomposition via SVD is ill-conditioned when singular values approach zero — use robust implementations.

- **Failure signatures**: Diagonal patterns in Ũ with significant p-values suggest similar training data but different seeds. Uniformly high CKA similarity across layers without clear diagonal patterns indicates REEF-style false positives on unrelated models.

- **First 3 experiments**:
  1. Verify MDIR on known homologous pair (e.g., Llama-3.1-8B vs Llama-3.2-1B) — expect clear diagonal in Ũ and p < 10⁻⁶⁰⁰.
  2. Test noise robustness: inject Gaussian noise at 0.1×, 0.3×, 1.0× RMS and verify trace drops by <2% while model performance collapses first.
  3. Cross-tokenizer validation: retrain Qwen2.5-0.5B with GPT-NeoX tokenizer on ~4B tokens and confirm all 896 channels match original.

## Open Questions the Paper Calls Out

- **Multi-way phylogeny reconstruction**: Can MDIR be extended to resolve complex model phylogenies, such as determining which of several candidate models is the closest ancestor? Current method provides binary homology decisions but lacks metrics for multi-way evolutionary distance.

- **General invertible transformations**: How can the method recover exact weight correspondences when inner transformations (specifically for W_V and W_O) involve arbitrary general invertible matrices? Current approach relies on Hungarian algorithm for discrete permutations, which are destroyed by arbitrary invertible transformations.

- **Adversarial obfuscation robustness**: Can the detection method be made robust to sophisticated adversarial obfuscation strategies that combine transformations beyond currently supported permutations and scalings? If an adversary applies dense orthogonal transformations, current numerical methods may fail to identify homology.

## Limitations

- Method currently struggles to study model phylogeny and understand family trees of language models
- Cannot guarantee recovering exact transformation for general invertible matrices applied to attention layers
- Limited against sophisticated adversarial obfuscation combining transformations beyond permutations and scalings
- Core G-invariance mechanism requires empirical validation across different optimizer configurations

## Confidence

- **High**: Polar decomposition approach and mathematical foundations
- **Medium**: G-invariance mechanism and Large Deviation Theory application bounds
- **Medium**: Cross-tokenizer alignment assumptions and common token requirements

## Next Checks

1. **Optimization Robustness Test**: Evaluate MDIR across different optimizer configurations (SGD, Adam, AdamW with varying learning rates) on the same homologous model pair to quantify sensitivity to optimization hyperparameters.

2. **Adversarial Transformation Benchmark**: Systematically test MDIR against non-permutation orthogonal transformations (rotations, reflections) applied to embeddings to establish method's limitations against sophisticated obfuscation.

3. **Cross-Architecture Generalization**: Apply MDIR to homologous pairs across different architectures (e.g., GQA vs MQA) to validate whether G-invariance mechanism extends beyond assumed architecture class.