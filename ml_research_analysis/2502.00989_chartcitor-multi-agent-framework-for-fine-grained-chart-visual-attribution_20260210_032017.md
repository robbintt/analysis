---
ver: rpa2
title: 'ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution'
arxiv_id: '2502.00989'
source_url: https://arxiv.org/abs/2502.00989
tags:
- chart
- table
- chartcitor
- answer
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChartCitor is a multi-agent LLM framework for fine-grained chart
  visual attribution that addresses hallucination in chart QA by grounding LLM-generated
  answers with precise bounding box citations. The system orchestrates six specialized
  agents to extract structured data tables from charts, reformulate answers into logical
  steps, generate contextual entity descriptions, retrieve supporting evidence through
  pre-filtering and re-ranking, and localize cited cells in chart images.
---

# ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution

## Quick Facts
- arXiv ID: 2502.00989
- Source URL: https://arxiv.org/abs/2502.00989
- Reference count: 24
- Key outcome: 27.4% IoU for chart attribution, outperforming baselines by 9-15%

## Executive Summary
ChartCitor addresses the critical challenge of hallucination in chart-based question answering by introducing a multi-agent LLM framework that provides fine-grained visual attributions through precise bounding box citations. The system orchestrates six specialized agents to extract structured data from charts, reformulate answers into logical steps, generate contextual entity descriptions, retrieve supporting evidence, and localize cited cells within chart images. By grounding LLM-generated answers with visual citations, ChartCitor enhances trust and verification utility for professionals relying on AI-assisted chart interpretation.

## Method Summary
ChartCitor employs a multi-agent LLM framework consisting of six specialized agents that work in concert to extract structured data tables from charts, reformulate answers into logical steps, generate contextual entity descriptions, retrieve supporting evidence through pre-filtering and re-ranking, and localize cited cells in chart images. The framework addresses hallucination in chart QA by grounding LLM-generated answers with precise bounding box citations, achieving 27.4% IoU for chart attribution compared to baseline models that score between 3.89% and 13.8%.

## Key Results
- Achieves 27.4% IoU for chart attribution, outperforming Kosmos-2 (3.89%), LISA (4.34%), GPT-4V (12.5%), and Claude-3.5 (13.8%)
- User study shows 41% of citations rated completely accurate versus 28% for direct GPT-4o prompting
- Reduces verification time from hours to minutes through reliable visual citations

## Why This Works (Mechanism)
The framework's success stems from its decomposition of the complex chart QA task into specialized sub-tasks handled by different agents, allowing each component to focus on its specific expertise. The evidence retrieval and re-ranking agent ensures that only the most relevant information supports each claim, while the logical step reformulation creates a traceable chain of reasoning. The bounding box localization provides verifiable visual evidence that can be directly inspected by users, addressing the fundamental trust issue in LLM-generated answers.

## Foundational Learning
- **Multi-agent orchestration**: Six agents collaborate to handle different aspects of chart interpretation, reducing individual agent complexity and improving overall accuracy
- **Visual grounding through bounding boxes**: Converts abstract answers into concrete visual evidence that users can verify directly in the chart
- **Logical step reformulation**: Breaks down complex answers into verifiable intermediate steps, making the reasoning process transparent
- **Evidence pre-filtering and re-ranking**: Ensures only relevant information supports claims, reducing noise and improving citation quality
- **Structured data extraction**: Converts chart visual elements into structured tables that LLMs can reason about more effectively
- **Contextual entity description**: Provides semantic context for extracted data, improving the LLM's understanding of chart elements

## Architecture Onboarding

**Component Map**: Data Extraction -> Answer Reformulation -> Entity Description -> Evidence Retrieval -> Bounding Box Localization -> Citation Generation

**Critical Path**: The evidence retrieval and re-ranking agent is the critical path component, as it determines which data supports each claim and directly impacts citation accuracy. Any failure in this stage cascades to subsequent agents.

**Design Tradeoffs**: The framework trades computational efficiency for accuracy by using multiple specialized agents instead of a single end-to-end model. This increases latency but provides more reliable citations. The reliance on OCR for data extraction introduces a dependency on image quality and chart complexity.

**Failure Signatures**: Poor OCR quality leads to incorrect data tables, which propagates through all subsequent agents. Complex chart layouts with overlapping elements cause localization errors. Ambiguous chart elements without clear textual labels result in entity description failures.

**First 3 Experiments**:
1. Run the data extraction agent on a simple bar chart to verify structured table output
2. Test the evidence retrieval agent with a known question-answer pair to validate re-ranking accuracy
3. Validate bounding box localization on a single data point citation to confirm visual grounding works

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on small-scale user study (14 participants) that may not represent diverse professional contexts
- Performance heavily depends on input chart quality and OCR model compatibility
- Lacks ablation studies to isolate individual agent contributions to overall performance
- Comparison limited to IoU metrics without examining qualitative citation usefulness

## Confidence

**High Confidence**: ChartCitor achieves 27.4% IoU for chart attribution, outperforming baselines (Kosmos-2: 3.89%, LISA: 4.34%, GPT-4V: 12.5%, Claude-3.5: 13.8%) by 9-15% absolute gain.

**Medium Confidence**: ChartCitor reduces verification time from hours to minutes based on user study feedback, though lacks systematic workflow measurement. Enhanced trust and verification utility supported by user ratings but may reflect novelty effects.

**Low Confidence**: Generalization claim for reliable visual citations across diverse professional domains not substantiated by testing beyond ChartQA dataset or real-world professional validation.

## Next Checks
1. Conduct longitudinal study with 50+ professional users across finance, healthcare, and research domains over 3+ months to validate real-world time savings and accuracy improvements.

2. Perform ablation studies systematically disabling each agent to quantify individual contributions to the 27.4% IoU performance and identify bottlenecks.

3. Test framework robustness across diverse chart types and quality levels using systematically degraded images to establish performance bounds and failure modes.