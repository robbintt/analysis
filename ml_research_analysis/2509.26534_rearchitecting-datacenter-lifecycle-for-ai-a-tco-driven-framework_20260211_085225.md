---
ver: rpa2
title: 'Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework'
arxiv_id: '2509.26534'
source_url: https://arxiv.org/abs/2509.26534
tags:
- hardware
- power
- datacenter
- https
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a holistic lifecycle management framework
  for AI datacenters, rethinking the traditional lifecycle across three stages: building,
  IT provisioning, and operation. The authors address the challenge of minimizing
  Total Cost of Ownership (TCO) for AI datacenters by evaluating cross-stage strategies
  that coordinate infrastructure design, hardware refresh policies, and software optimizations.'
---

# Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework

## Quick Facts
- **arXiv ID**: 2509.26534
- **Source URL**: https://arxiv.org/abs/2509.26534
- **Reference count**: 40
- **Primary result**: Holistic lifecycle management framework achieving up to 40% TCO reduction for AI datacenters through coordinated cross-stage optimization

## Executive Summary
This paper presents a comprehensive Total Cost of Ownership (TCO) framework for AI datacenter lifecycle management, rethinking traditional approaches across three stages: building, IT provisioning, and operation. The authors address the challenge of minimizing TCO for AI workloads by evaluating cross-stage strategies that coordinate infrastructure design, hardware refresh policies, and software optimizations. Through Monte Carlo simulations over 15-year horizons, they demonstrate that combined stage optimizations achieve up to 40% TCO reduction compared to traditional approaches, with individual stages contributing 15-23% savings each. The framework identifies optimal strategies for emerging AI workloads, including flatter power distribution, hybrid cooling, hierarchical networking, flexible refresh cycles, and heterogeneity-aware scheduling, providing actionable guidelines for managing AI datacenter lifecycles under varying model and hardware trends.

## Method Summary
The authors develop a TCO modeling framework that integrates CapEx (infrastructure, power, cooling, networking, IT) and OpEx (energy, maintenance, software) with amortization schedules over 15-year horizons. They employ Monte Carlo simulation to sample uncertainty in workload growth, hardware availability, and cost projections. The framework evaluates three lifecycle stages: building (infrastructure design), IT provisioning (hardware refresh policies), and operation (software optimizations). Each stage offers specific strategies - flatter power distribution, hybrid cooling, and hierarchical networking for building; skip-generation and demand-driven refresh for IT provisioning; quantization, disaggregation, and heterogeneity-aware scheduling for operation. Cross-stage co-optimization is achieved by modeling interdependencies between decisions, such as how build choices constrain refresh flexibility and how operation software reduces compute demand.

## Key Results
- Individual stage optimizations achieve 15% (build), 23% (IT provisioning), and 19% (operation) TCO reduction
- Combined cross-stage optimization achieves up to 40% TCO reduction vs. traditional approaches
- Hybrid cooling (75% liquid, 25% air) achieves 9% TCO reduction compared to all-air or all-liquid approaches
- Heterogeneity-aware scheduling extends older GPU useful life, achieving 15-20% TCO reduction by routing small models to older hardware
- Per-DC power delivery reduces stranded capacity and achieves 4.2% TCO reduction vs. hierarchical baseline

## Why This Works (Mechanism)

### Mechanism 1: Flatter Power Distribution Reduces Stranded Capacity
Traditional hierarchical power distribution creates power fragmentation because each domain must respect capacity limits, leaving residual capacity unusable. AI servers with 10+ kW TDP exacerbate this. Flatter architectures (e.g., per-DC pooling) allow more servers per domain by sharing headroom, reducing stranded power from fragmentation that scales with server power density and domain granularity.

### Mechanism 2: Hybrid Cooling Optimizes PUE Without Full Liquid Retrofit
Mixing liquid cooling (cold plates) for high-density AI racks with air cooling for lower-density loads achieves PUE ~1.15 without full facility retrofit CapEx. Liquid cooling handles 75% high-density load with better heat transfer and lower fan power, while air cooling covers remaining 25%, yielding 9% TCO reduction compared to all-air or all-liquid approaches.

### Mechanism 3: Heterogeneity-Aware Scheduling Extends Older GPU Useful Life
LLM inference latency components (TTFT, TBT) scale differently across GPU generations. For small models (1B–8B params), older GPUs (V100, A100) remain within ~20% of H100 performance-per-watt-per-dollar. Scheduling that matches workload to hardware capability routes smaller models and memory-bound phases (e.g., decode) to older GPU generations while reserving newer GPUs for compute-intensive workloads, deferring refresh CapEx without violating SLOs.

### Mechanism 4: Cross-Stage Co-Optimization Compounds Savings
Coordinating build, refresh, and operation decisions achieves up to 40% TCO reduction—more than sum of individual-stage optimizations (15-23% each). Build decisions (power topology, cooling type) constrain refresh flexibility and operation scheduling. For example, flatter power domains enable denser deployments, which improves utilization, which makes extended hardware lifetimes viable. Operation software (disaggregation, quantization) reduces per-query compute demand, deferring refresh.

## Foundational Learning

- **Concept: TCO = CapEx + OpEx decomposition**
  - Why needed here: The entire framework evaluates lifecycle decisions via TCO impact. Understanding how CapEx (amortized infrastructure, IT) and OpEx (energy, maintenance) combine is prerequisite to interpreting all figures.
  - Quick check question: Given a $375K GPU server amortized over 5 years and $20K annual energy+ maintenance, what is the annual TCO contribution?

- **Concept: Power stranding / fragmentation**
  - Why needed here: Mechanism 1 relies on reducing stranded power. You must understand that hierarchical power domains leave unused capacity when server power doesn't evenly divide domain budget.
  - Quick check question: If a PDU has 50 kW capacity and each server draws 10.2 kW, how much power is stranded? What if servers draw 8 kW?

- **Concept: LLM inference latency components (TTFT vs. TBT)**
  - Why needed here: Mechanism 3 depends on TTFT being compute-bound (scales with GPU generation) while TBT is memory-bound (scales less). This asymmetry enables heterogeneous scheduling.
  - Quick check question: For a 70B parameter model, which latency component degrades more on a V100 vs. H100? Why does this matter for scheduling?

- **Concept: PUE (Power Usage Effectiveness)**
  - Why needed here: Cooling mechanism (Mechanism 2) reports PUE improvements. PUE = total facility energy / IT energy; lower is better.
  - Quick check question: If a datacenter consumes 100 MW total and 80 MW goes to IT equipment, what is the PUE?

- **Concept: Monte Carlo simulation for uncertainty quantification**
  - Why needed here: The framework uses Monte Carlo to sample workload growth, hardware availability, and cost projections. Results are distributions, not point estimates.
  - Quick check question: Why would a TCO optimization use Monte Carlo instead of deterministic projections?

## Architecture Onboarding

- **Component map**:
  - TCO Model (core) -> Workload Model -> Hardware Model -> Infrastructure Model -> Policy Layer -> Simulation Engine
  - TCO Model integrates CapEx and OpEx with amortization schedules
  - Workload Model projects LLM inference demand (RPS), model size growth, and latency SLOs
  - Hardware Model includes GPU roadmaps (FLOPS, memory bandwidth, TDP, cost)
  - Infrastructure Model covers power topology, cooling types, networking hierarchies
  - Policy Layer implements refresh strategies and operation optimizations
  - Simulation Engine runs Monte Carlo sampling over 15-year horizon

- **Critical path**:
  1. Define workload projection (base RPS, growth rate, model size trajectory)
  2. Select infrastructure design (power, cooling, networking) → constrains max density and refresh flexibility
  3. Define refresh policy (per-generation lifetimes, skip decisions) → determines fleet composition over time
  4. Apply operation optimizations (quantization, disaggregation, scheduling) → reduces per-query cost, extends hardware life
  5. Run Monte Carlo simulation → sample uncertainty, output TCO distribution
  6. Compare policies → select lowest expected TCO with acceptable variance

- **Design tradeoffs**:
  - Flat power vs. fault isolation: Per-DC pooling minimizes stranding but worsens fault domains
  - Liquid cooling CapEx vs. OpEx: Higher upfront cost, lower PUE; hybrid balances for mixed-density workloads
  - NVLink vs. Ethernet/InfiniBand: NVLink offers highest bandwidth but highest cost; hierarchical (NVLink intra-server, IB intra-rack, Ethernet inter-rack) optimizes cost/performance
  - Aggressive refresh vs. extended life: Newer GPUs better for large models; older GPUs competitive for small models—policy must match workload mix
  - Stage-specific vs. cross-stage optimization: Individual stages yield 15-23% savings; combined yields up to 40% but requires coordinated decision-making

- **Failure signatures**:
  - Over-provisioned power domains: If per-DC pooling is implemented but actual rack deployments are sparse, stranding reduction is negated
  - Liquid cooling on low-density racks: If high-density AI workloads migrate elsewhere, liquid-cooled racks sit underutilized with high fixed costs
  - Heterogeneity scheduling mismatch: If small-model traffic is over-estimated, older GPUs sit idle while new GPUs are over-subscribed
  - Model growth exceeds hardware roadmap: If model sizes grow exponentially (not linearly), all refresh policies fail to meet SLOs—TCO model assumptions break
  - Monte Carlo distribution too wide: If variance across samples is larger than mean differences between policies, optimization is unreliable

- **First 3 experiments**:
  1. Validate roofline model against real workload: Run Llama-3 (1B, 8B, 70B) on V100, A100, H100 with vLLM; compare measured TTFT/TBT to model predictions. Target: <5% error as claimed.
  2. Sensitivity analysis on model growth rate: Run framework with sub-linear, linear, and exponential model size projections; observe how optimal refresh policy shifts. Document threshold where skip-generation becomes suboptimal.
  3. Single-stage vs. cross-stage optimization comparison: Implement build-only optimization (flat power, hybrid cooling, hierarchical network), refresh-only (optimal per-generation lifetimes), and combined; compare TCO distributions. Verify ~40% reduction claim holds under sampled uncertainty.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does integrating embodied and operational carbon emissions into the cost model alter the optimal cross-stage lifecycle strategies?
  - Basis in paper: Section 4.2 states, "A similar methodology applies to carbon emissions... We leave this extension to future work."
  - Why unresolved: The current framework minimizes financial cost (TCO), but strategies that lower cost (e.g., extending hardware life) may conflict with minimizing carbon emissions if older hardware is less energy-efficient.
  - What evidence would resolve it: Simulation results comparing the "Green-TCO" optimal policies (refresh rates, cooling types) against the financial-TCO optimal policies to identify trade-offs.

- **Open Question 2**: What are the lifecycle benefits of co-designing asymmetric accelerators with distinct units for prefill (SMs) and decode (PIM) phases?
  - Basis in paper: Section 8.2 suggests future accelerators "should be designed... for long-term compatibility," proposing designs that "combine high-power SMs... with Processing-in-Memory (PIM) units."
  - Why unresolved: The paper evaluates existing homogeneous hardware; the TCO impact of specialized, phase-aware hardware on refresh cycles and utilization remains unmodeled.
  - What evidence would resolve it: A performance and cost model for hypothetical asymmetric hardware showing improved efficiency in disaggregated inference workflows compared to standard GPU upgrades.

- **Open Question 3**: How do optimal infrastructure and refresh policies shift when training workloads are introduced into the demand model alongside inference?
  - Basis in paper: Section 2.1 restricts the focus to inference, noting that training "differs in modeling" regarding fault tolerance and bandwidth, but does not quantify this impact.
  - Why unresolved: Training imposes distinct stress on networking and reliability (checkpoints) which may render the "optimal" inference-centric build (e.g., hierarchical networking) suboptimal for a mixed-workload cluster.
  - What evidence would resolve it: Extending the Monte Carlo simulation to include training job traces and analyzing the resulting shift in optimal build/refresh configurations.

## Limitations

- Cross-stage co-optimization claims rely entirely on internal modeling rather than empirical deployment data; the 40% TCO reduction is a modeled projection, not validated in production.
- Workload density predictions for hybrid cooling and heterogeneity-aware scheduling assume stable deployment patterns that may not hold in practice.
- Model growth trajectory assumptions (linear vs. exponential) have dramatic impact on optimal strategies, but real-world trends remain uncertain.

## Confidence

- **High**: Basic TCO decomposition framework and stage-specific optimizations (build, refresh, operation) are methodologically sound.
- **Medium**: Hybrid cooling and flatter power distribution mechanisms are theoretically justified but lack independent validation in the corpus.
- **Low**: The 40% combined optimization claim and specific optimal strategy matrices depend heavily on uncertain input parameters and cross-stage coordination feasibility.

## Next Checks

1. **Independent validation of roofline model accuracy**: Benchmark Llama-3 (1B, 8B, 70B) on V100, A100, H100 with vLLM and compare measured TTFT/TBT to framework predictions. Target <5% error to support heterogeneity-aware scheduling claims.
2. **Sensitivity analysis on model growth trajectories**: Systematically vary model size growth assumptions (sub-linear, linear, exponential) and document threshold points where optimal refresh strategies shift. This tests robustness of Table 8 recommendations.
3. **Real-world deployment pilot**: Implement single-stage optimizations (e.g., flat power distribution) in a controlled environment and measure actual stranded capacity reduction vs. baseline to validate Mechanism 1 assumptions.