---
ver: rpa2
title: 'Efficiently Access Diffusion Fisher: Within the Outer Product Span Space'
arxiv_id: '2505.23264'
source_url: https://arxiv.org/abs/2505.23264
tags:
- diffusion
- fisher
- arxiv
- data
- trace
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel outer-product span formulation of
  the diffusion Fisher information matrix (DF), which resides in a space spanned by
  outer products of score and initial data. Leveraging this structure, the authors
  develop two efficient algorithms: DF-Trace Matching (DF-TM) for likelihood evaluation
  and DF-Endpoint Approximation (DF-EA) for adjoint optimization.'
---

# Efficiently Access Diffusion Fisher: Within the Outer Product Span Space

## Quick Facts
- arXiv ID: 2505.23264
- Source URL: https://arxiv.org/abs/2505.23264
- Reference count: 40
- Primary result: Novel outer-product span formulation enables efficient computation of diffusion Fisher information matrix trace and matrix-vector products without expensive gradient computations

## Executive Summary
This paper introduces a novel outer-product span formulation of the diffusion Fisher information matrix (DF) that enables efficient computation of trace and matrix-vector operations without expensive gradient computations. The key insight is that DF resides in a space spanned by outer products of score and initial data, allowing two efficient algorithms: DF-Trace Matching (DF-TM) for likelihood evaluation and DF-Endpoint Approximation (DF-EA) for adjoint optimization. DF-TM trains a scalar network to estimate the trace with O(d) complexity instead of O(d²), while DF-EA simplifies matrix-vector multiplication using only the endpoint sample. Experiments demonstrate superior accuracy and reduced computational cost compared to traditional VJP methods, and the formulation enables the first numerical verification of optimal transport properties of diffusion ODE maps.

## Method Summary
The method introduces two efficient algorithms for computing diffusion Fisher information matrix operations without expensive gradient computations. DF-TM (Trace Matching) trains a scalar-valued network t_θ to estimate the weighted norm term, reducing trace computation from quadratic to linear complexity. DF-EA (Endpoint Approximation) approximates matrix-vector multiplication using only the endpoint sample x₀, eliminating gradient operations entirely. Both methods leverage a pretrained score network and the outer-product span structure of DF, with DF-TM providing stronger theoretical guarantees through vanishing error bounds while DF-EA offers training-free implementation.

## Key Results
- DF-TM reduces trace computation time by 99.7% on Stable Diffusion v1.5 compared to full VJP baseline
- DF-EA achieves consistent score improvements across 5 metrics (SAC/AVA, Pick-Score, CLIP Loss, Face ID) without training
- First numerical verification of optimal transport property shows PF-ODE map is optimal for single-Gaussian/affine initial data but fails for non-affine cases
- Theoretical error bounds established for both methods, with DF-TM having vanishing bounds and DF-EA having non-vanishing theoretical error

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The diffusion Fisher information matrix can be expressed as a weighted outer-product sum, residing in a space spanned by initial data and noise schedule alone.
- **Mechanism:** By applying the consecutive partial differential chain rule to the marginal diffused distribution, the paper derives: Fₜ(xₜ,t) = (1/σ²ₜ)I - (α²ₜ/σ⁴ₜ)[Σᵢwᵢyᵢyᵢᵀ - (Σᵢwᵢyᵢ)(Σᵢwᵢyᵢ)ᵀ]. This structure means DF can be accessed without any gradient computation through the score network.
- **Core assumption:** The initial distribution has finite second moments (general setting) or is a sum of Dirac distributions (theoretical derivation).
- **Evidence anchors:**
  - [abstract] "the diffusion Fisher actually resides within a space spanned by the outer products of score and initial data"
  - [section] Propositions 1 and 3 provide the full derivation for Dirac and general cases
  - [corpus] Weak/no direct corpus evidence for this specific outer-product span formulation
- **Break condition:** The formulation becomes ill-defined at t=0 due to σ₀ in denominator (singularity acknowledged in Appendix C.1)

### Mechanism 2
- **Claim:** DF trace can be estimated with O(d) complexity by training a scalar-valued network to learn the weighted norm term.
- **Mechanism:** Instead of d VJP backpropagations, DF-TM trains tθ(xₜ,t) to estimate (1/d)Σᵢwᵢ‖yᵢ‖² via Algorithm 1. The trace becomes: tr(Fₜ) ≈ d/σ²ₜ - (α²ₜ/σ⁴ₜ)(d·tθ - ‖ȳθ‖²), where ȳθ comes from the pretrained score network.
- **Core assumption:** The y-prediction score network ȳθ accurately estimates Σᵢwᵢyᵢ (Proposition 2)
- **Evidence anchors:**
  - [abstract] "reducing time complexity from quadratic to linear"
  - [section] Table 1 shows 99.7% time reduction on SD-v1.5; Proposition 6 proves convergence
  - [corpus] No corpus evidence for this specific technique
- **Break condition:** Error bound (Proposition 7) depends on both tθ and εθ accuracy; poor score networks propagate error

### Mechanism 3
- **Claim:** Matrix-vector multiplication with DF can be approximated using only the endpoint sample x₀, eliminating all gradient operations.
- **Mechanism:** DF-EA replaces the weighted outer-product sum with x₀x₀ᵀ, valid near t=0 where the closest yᵢ dominates. The operation becomes scalar-weighted vector combinations: Fᵀλ ≈ (1/σ²ₜ)λ - (α²ₜ/σ⁴ₜ)[⟨x₀,λ⟩x₀ - ⟨ȳθ,λ⟩ȳθ].
- **Core assumption:** Assumption: the endpoint sample x₀ adequately represents the dominant contribution to the weighted sum
- **Evidence anchors:**
  - [abstract] "simplifies matrix-vector multiplication into vector-product calculations without gradients"
  - [section] Figure 3 shows consistent score improvements across 5 metrics; Eq. 20 gives the full formulation
  - [corpus] No corpus evidence
- **Break condition:** Theoretical error bound (Proposition 8) does not vanish even with perfect training—empirically valid but theoretically weaker than DF-TM (Appendix C.6)

## Foundational Learning

- **Concept: Score functions and diffusion SDEs**
  - Why needed here: Understanding how εθ estimates -σₜ∇log qₜ is essential to grasp why the Hessian (DF) provides second-order information
  - Quick check question: How does the score function relate to the noise predictor in ε-prediction training?

- **Concept: Fisher Information as negative Hessian**
  - Why needed here: The paper defines DF = -∇²log qₜ, making this the central mathematical object
  - Quick check question: Why does the trace of Fisher relate to log-likelihood derivatives in continuous normalizing flows?

- **Concept: VJP vs Hutchinson trace estimation**
  - Why needed here: Understanding the baseline computational costs (O(d²) for naive trace, O(d) with Monte Carlo variance) motivates the efficiency claims
  - Quick check question: Why does Hutchinson require 351+ NFE for 10% error at 90% confidence (Appendix C.5)?

## Architecture Onboarding

- **Component map:** Pretrained score network εθ → converted to ȳθ = (xₜ - σₜεθ)/αₜ → DF-TM trace network tθ (scalar head on U-Net backbone) → Noise schedule {αₜ, σₜ} and derived {f(t), g(t)} → Adjoint state λₜ (for DF-EA, from reverse ODE)

- **Critical path:**
  - DF-TM inference: (xₜ,t) → tθ + ȳθ → Eq. 17 → trace
  - DF-EA inference: (xₜ,t,λₜ,x₀) → three inner products → Eq. 20 → adjoint update

- **Design tradeoffs:**
  - DF-TM: requires training but has vanishing error bound (Prop. 7)
  - DF-EA: training-free but non-vanishing theoretical error (Prop. 8)
  - Both: depend on pretrained score network quality

- **Failure signatures:**
  - Numerical instability as σₜ → 0 (singularity region)
  - Poor DF-EA accuracy for non-affine initial distributions (OT verification in Table 3)
  - Trace network underfitting on complex data distributions

- **First 3 experiments:**
  1. Verify outer-product formulation on 2D Gaussian mixture by comparing Eq. 11 against finite-difference Hessian
  2. Train DF-TM on CIFAR-10; compare trace accuracy and runtime against full VJP baseline
  3. Apply DF-EA to adjoint guidance on MNIST with known optimal transport paths; measure deviation from ground truth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed Diffusion Fisher (DF) access methods be integrated into accelerated ODE solvers?
- Basis in paper: [explicit] Appendix C.7 explicitly lists "integration of DF into accelerated ODE solvers" as an unexplored limitation.
- Why unresolved: Current experiments rely on the explicit Euler method (DDIM), whereas high-order solvers use different discretization strategies.
- What evidence would resolve it: Benchmarks of DF-TM/DF-EA performance within solvers like DPM-Solver or exact inversion frameworks.

### Open Question 2
- Question: Can the theoretical error bound for the DF-EA method be established such that it vanishes as the training error decreases?
- Basis in paper: [inferred] Appendix C.6 notes the current bound in Proposition 8 does not vanish with training error, making DF-EA theoretically "less valid" than DF-TM.
- Why unresolved: The endpoint approximation (replacing sums with x₀x₀ᵀ) introduces a non-vanishing term in the error analysis.
- What evidence would resolve it: A refined theoretical proof or analysis showing the bound's convergence properties.

### Open Question 3
- Question: Can the failure of the PF-ODE map to be an optimal transport map for non-affine data be proven theoretically?
- Basis in paper: [inferred] Section 6 hypothesizes the map fails for non-affine data based on numerical experiments, but lacks a general proof.
- Why unresolved: Proofs for the optimal transport property currently only cover single-Gaussian or affine initial data cases.
- What evidence would resolve it: A formal proof extending the non-OT result to general non-affine distributions.

## Limitations

- Theoretical error bounds for DF-EA are weaker than DF-TM, with non-vanishing error even with perfect training
- Outer-product span formulation may not generalize to all diffusion models beyond the specific structural assumptions
- Numerical OT verification shows DF-EA fails for non-affine initial distributions despite working well for Gaussian/affine cases

## Confidence

- **High:** The outer-product span formulation (Mechanism 1) and DF-TM algorithm (Mechanism 2) are mathematically rigorous with strong theoretical guarantees
- **Medium:** DF-EA's practical effectiveness despite weak theoretical bounds suggests empirical validity, but the mechanism is less well-understood
- **Low:** Generalization to non-Gaussian distributions and alternative diffusion formulations remains unproven

## Next Checks

1. Verify the DF-EA approximation error bound empirically across different initial distributions (uniform, multimodal) and compare against the theoretical bound
2. Test the outer-product formulation on continuous-time diffusion models without discrete noise schedules
3. Benchmark against alternative trace estimation methods (Hutchinson with tuned probing vectors) on models with different latent dimensionalities