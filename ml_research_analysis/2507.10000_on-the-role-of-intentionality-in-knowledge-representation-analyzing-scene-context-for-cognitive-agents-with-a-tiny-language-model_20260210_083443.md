---
ver: rpa2
title: 'On The Role of Intentionality in Knowledge Representation: Analyzing Scene
  Context for Cognitive Agents with a Tiny Language Model'
arxiv_id: '2507.10000'
source_url: https://arxiv.org/abs/2507.10000
tags:
- intentionality
- intent
- which
- process
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method for analyzing scene context and intentionality
  in cognitive agents using a Tiny Language Model (TLM) based on Semantic Spacetime
  and Promise Theory. The core approach uses multi-scale process coherence to distinguish
  between intentional content and ambient context in data streams without requiring
  extensive training or reasoning capabilities.
---

# On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model

## Quick Facts
- **arXiv ID**: 2507.10000
- **Source URL**: https://arxiv.org/abs/2507.10000
- **Reference count**: 23
- **Primary result**: Method for distinguishing intentional content from ambient context in cognitive agents using multi-scale n-gram analysis and work investment scoring without training

## Executive Summary
This paper presents a method for analyzing intentionality in cognitive agents by distinguishing between intentional content and ambient context in data streams using a Tiny Language Model (TLM). The approach leverages multi-scale process coherence based on Semantic Spacetime and Promise Theory, analyzing n-grams across different scales to identify anomalous patterns that indicate intentionality through measures of work investment and temporal spacing. The method operates without extensive training or reasoning capabilities, instead using relatively simple dynamical analysis of pattern repetition and spacing. Testing on narrative texts demonstrates that intentionality can be assessed through surface-level features while requiring minimal computational resources compared to more complex methods.

## Method Summary
The method processes text by splitting it into parallel streams of n-grams (n=1 through 5) and analyzing them across coherence intervals of approximately 45 sentences. It computes intentionality scores based on the work invested in creating patterns (word length/complexity) multiplied by frequency, penalized by over-repetition using a sigmoid function. The approach identifies intentional content as anomalous, rare patterns with significant work investment, while contextual content forms the habitual, frequent background. Two scoring approaches are presented: a static formula combining work and frequency with exponential penalty, and a dynamic decay-based method that tracks the spacing between occurrences within coherence intervals.

## Key Results
- Intentionality can be assessed through simple dynamical analysis of pattern repetition and spacing without requiring semantic understanding
- A coherence interval around 45 sentences (based on Dunbar numbers) provides an effective window for intentionality assessment
- The TLM approach achieves comparable results to more computationally expensive methods while requiring minimal resources and no training data
- Intentionality appears as a random variable in natural language due to its dense associative meaning, with intentional fragments showing anomalous gaps in occurrence patterns

## Why This Works (Mechanism)

### Mechanism 1: Multi-Scale N-Gram Fractionation for Intent Detection
- **Claim**: Intentionality can be approximated by analyzing pattern repetition across multiple n-gram scales without semantic understanding.
- **Mechanism**: The method splits text into parallel streams of n-grams (n=1 through 5). Longer n-grams are exponentially rarer and thus more "unique" — behaving like proper names or proto-concepts. Shorter n-grams form an "ambient soup" of contextual support. By ranking fragments across scales, intentional content (anomalous, rare) separates from contextual content (habitual, frequent).
- **Core assumption**: Intentionality correlates with "work investment" (effort to produce a pattern) and anomalous spacing between repetitions — not just frequency.
- **Evidence anchors**:
  - [abstract]: "identifying anomalous patterns that indicate intentionality through measures of work investment and temporal spacing"
  - [Section 4]: "Separating these as partially independent fragments... we can use the n-grams (n = 1, . . .5) as a spanning set for labelling the representation of concepts or themes"
  - [corpus]: Weak direct corpus support for this specific n-gram fractionation approach; related work on layered meaning (Meanings are Like Onions) shares multi-scale philosophy but differs in method.

### Mechanism 2: Coherence Interval as Cognitive Attention Window
- **Claim**: Intentionality assessment requires a finite attention window (~45 sentences), derived from human cognitive limits (Dunbar D30).
- **Mechanism**: Text is processed in "coherence intervals" — segments where related intent is assumed continuous. The intentionality score decays with distance from last occurrence: I(w) = W(w)(1 - e^(-λ(τ - τ_last))). This captures that reintroducing a concept after a hiatus signals renewed intent, while incessant repetition signals habit.
- **Core assumption**: Human cognitive attention has a natural scale (~45 sentences), and intentionality operates within this window before topic drift.
- **Evidence anchors**:
  - [Section 4.1]: "This interval is a property of the cognitive ability of the observer. For humans, we find this to be close to the number we call Dunbar D30"
  - [Section 4.2]: "The distribution will be 'intentionally' non-uniform if it has anomalous gaps in the occurrence of regular terms"
  - [corpus]: No direct corpus validation of the 45-sentence coherence interval; Dunbar number linkage is paper-internal.

### Mechanism 3: Work-Investment Scoring with Sigmoid Penalty
- **Claim**: Intentionality is proportional to work invested (string length, character complexity) times frequency, penalized by over-repetition.
- **Mechanism**: I(w, Φ) = Φ(w)W(w) / (1 + exp(Φ(w)/Φ₀ - ρ)). The numerator rewards effort and repetition; the sigmoid denominator penalates patterns that exceed the coherence frequency (habitual padding). This creates a "Goldilocks zone" for intentional content.
- **Core assumption**: Work is measurable from surface features (word length, stroke count in Chinese); there exists a threshold frequency Φ₀ ≈ 1/D30 beyond which repetition indicates habit, not intent.
- **Evidence anchors**:
  - [Section 3]: "Singleton, spurious, or unique events that do not invest significant work cost would have low intentionality"
  - [Section 4.1, Eq. 8]: Full mathematical formulation of the intentionality function
  - [corpus]: Related work on "System 1-like intentionality" in RL agents (arxiv 2501.18299) suggests intent-like behavior without explicit planning, but uses different scoring mechanisms.

## Foundational Learning

- **Concept**: N-gram analysis and Shannon entropy
  - **Why needed here**: The method builds on frequency distributions of n-grams but argues entropy alone is insufficient (scale-free, ignores temporal dynamics).
  - **Quick check question**: Can you explain why Shannon entropy cannot distinguish a bursty process from a stable one?

- **Concept**: Promise Theory basics (agent, promise, autonomy)
  - **Why needed here**: Intentionality is framed as a relational assessment between source and receiver agents, both investing effort.
  - **Quick check question**: In Promise Theory, why must both source and receiver have intentional processes for intentionality assessment?

- **Concept**: Dunbar numbers and cognitive scaling
  - **Why needed here**: The coherence interval (λ ≈ 45 sentences) derives from human cognitive limits on attention and social relationships.
  - **Quick check question**: What would happen to intentionality scoring if you applied a coherence interval of 5 sentences vs. 200 sentences?

## Architecture Onboarding

- **Component map**: Tokenizer/n-gram extractor -> Frequency counter -> Work estimator -> Intentionality scorer -> Coherence interval manager -> Classifier

- **Critical path**:
  1. Segment text into sentences (proper time coordinate τ)
  2. Extract all n-grams up to n=5
  3. Within each coherence interval, compute frequencies and inter-occurrence distances
  4. Score each fragment using work-weighted intentionality function
  5. Rank and classify: high scores → intentional; moderate/repetitive → contextual

- **Design tradeoffs**:
  - **Memory vs. streaming**: Full frequency counting requires storing all n-grams; Eq. 11 (running decay) enables streaming but loses some precision
  - **Precision vs. cost**: The method explicitly trades probabilistic accuracy for low computational cost and no training
  - **Language specificity vs. generality**: Work function requires calibration per script; frequency thresholds may vary by genre

- **Failure signatures**:
  - **Over-classification as intentional**: Short documents, highly repetitive content, or texts with many proper names
  - **Under-classification**: Highly formulaic texts where intent is sparse (legal, technical)
  - **Language mismatch**: Using Latin-script work assumptions on logographic scripts without recalibration
  - **Coherence interval mismatch**: Non-narrative documents with different topical drift rates

- **First 3 experiments**:
  1. **Baseline validation on provided test corpus**: Run TLM on Moby Dick and Darwin excerpts; manually verify that top-ranked intentional fragments align with thematic importance (whale fishery, natural selection terms)
  2. **Coherence interval sensitivity**: Test λ = 20, 45, 100 sentences; measure stability of intentionality rankings across intervals (Fig. 2 behavior)
  3. **Cross-domain probe**: Apply to a non-narrative document (e.g., API documentation, legal contract); assess whether coherence interval and work assumptions hold, and where classification breaks down

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the effectiveness of the coherence interval parameter (λ ≈ 45 sentences) remain constant across different data modalities, such as computer code or imagery, or does it require recalibration?
- **Basis in paper**: [explicit] The author notes, "It is conceivable that this value might vary in different kinds of description, e.g. in natural language versus computer programs or imagery, even in different styles of text."
- **Why unresolved**: The current study only validates the method on English narrative texts (e.g., *Moby Dick*, Darwin), leaving the parameter's stability in non-narrative or non-linguistic contexts untested.
- **What evidence would resolve it**: Comparative tests applying the TLM to source code repositories or image pixel sequences, measuring whether the 45-sentence coherence interval optimally separates context from intent in those domains.

### Open Question 2
- **Question**: To what extent does the definition of "work" as character count versus stroke count affect the cross-linguistic accuracy of the intentionality score?
- **Basis in paper**: [explicit] The paper states, "In Chinese writing, and some other Asian writing systems, the number of strokes in a character contributes to the amount of work," contrasting it with Western alphabets where character count is the primary metric.
- **Why unresolved**: The method relies on a linear summation of work values, but the paper does not verify if a uniform scaling factor is needed to equate "intentionality" between alphabetic and logographic languages.
- **What evidence would resolve it**: A cross-linguistic study correlating the intentionality scores of parallel texts (e.g., translated documents) using both character-based and stroke-based work metrics.

### Open Question 3
- **Question**: Can the method distinguish between intentional content and high-frequency cultural idioms which may register as ambient context despite high semantic importance?
- **Basis in paper**: [inferred] The paper identifies the limitation that "Foreigners can learn a new language and still understand nothing... because there is a cultural level to communication too in which apparently meaningful phrases as co-opted... for very different concepts."
- **Why unresolved**: The approach relies on frequency and spacing ("burstiness") to identify ambient context; idioms are high-frequency by nature but often carry the core intentional message, risking misclassification as "habitual padding."
- **What evidence would resolve it**: Analysis of texts heavy in idiomatic expressions to determine if they are incorrectly categorized as noise (Φ(w)/Φ₀ ≫ 1) or if the longitudinal coherence metric corrects for this.

## Limitations
- Reliance on surface-level features without semantic understanding limits detection of abstract or metaphorical content
- 45-sentence coherence interval lacks empirical validation across diverse text types and may not generalize to non-narrative documents
- Work function (word length) is a crude proxy for authorial effort that may not translate well across languages or writing styles
- Paper provides qualitative examples but lacks systematic quantitative evaluation against ground truth annotations

## Confidence
- **High Confidence**: Multi-scale n-gram fractionation approach and mathematical formulation of intentionality scoring (Eq. 8, 11) are clearly specified and methodologically sound
- **Medium Confidence**: Claim that intentionality appears as random variable in natural language due to dense associative meaning lacks direct corpus validation
- **Low Confidence**: Specific 45-sentence coherence interval derived from Dunbar D30 is paper-internal and lacks external validation

## Next Checks
1. **Quantitative Performance Evaluation**: Apply TLM to corpus with human-annotated intentionality labels; measure precision/recall against ground truth and compare with baseline frequency-based classifiers
2. **Cross-Lingual and Cross-Genre Validation**: Test on Chinese text, legal documents, and code; evaluate whether coherence interval and work function need recalibration
3. **Ablation Study of Coherence Interval**: Systematically vary coherence interval (5, 20, 45, 100, 200 sentences) across document types; measure stability of intentionality rankings