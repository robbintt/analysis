---
ver: rpa2
title: 'Hallucinations in Bibliographic Recommendation: Citation Frequency as a Proxy
  for Training Data Redundancy'
arxiv_id: '2510.25378'
source_url: https://arxiv.org/abs/2510.25378
tags:
- citation
- bibliographic
- language
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated how citation frequency acts as a proxy
  for training data redundancy in bibliographic hallucination by large language models.
  Using GPT-4.1, we generated 100 bibliographic records across 20 computer science
  domains and manually validated their factual consistency.
---

# Hallucinations in Bibliographic Recommendation: Citation Frequency as a Proxy for Training Data Redundancy

## Quick Facts
- arXiv ID: 2510.25378
- Source URL: https://arxiv.org/abs/2510.25378
- Reference count: 40
- Primary result: Citation count exhibits strong positive correlation with factual accuracy (r = 0.75, p < .001) in LLM-generated bibliographic records

## Executive Summary
This study investigates how citation frequency predicts factual accuracy in bibliographic recommendations generated by large language models. Using GPT-4.1, the researchers generated 100 bibliographic records across 20 computer science domains and manually validated their factual consistency against Google Scholar. The analysis revealed that highly cited papers are reproduced more accurately by the model, with a strong positive correlation between citation count and factual accuracy. The findings suggest that citation frequency serves as a proxy for training data redundancy, with papers receiving over approximately 1,000 citations being memorized almost verbatim by the model.

## Method Summary
The researchers prompted GPT-4.1 with JSON schema constraints to generate bibliographic records for 20 computer science domains (5 papers per domain). Each generated record was manually validated against Google Scholar to determine factual accuracy, scored on a 0-2 scale. Citation counts were retrieved from Google Scholar, and semantic similarity between generated and authentic metadata was measured using Sentence-BERT embeddings. The study analyzed correlations between citation frequency and accuracy scores, identifying a threshold effect where papers with over 1,000 citations showed near-perfect reproduction accuracy.

## Key Results
- Hallucination rates vary significantly across research domains, with image processing topics showing higher accuracy than recent LLM-oriented techniques
- Citation count exhibits a strong positive correlation with factual accuracy (r = 0.75, p < .001)
- Bibliographic information becomes almost verbatimly memorized beyond approximately 1,000 citations, where cosine similarity approaches 1.0

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Citation frequency serves as a proxy for training data redundancy, predicting factual accuracy of generated bibliographic records.
- Mechanism: Papers with high citation counts appear more frequently across the pretraining corpus (in academic papers, literature reviews, tutorials). This repeated exposure strengthens token-level associations, making the model more likely to reproduce authentic metadata verbatim rather than synthesizing plausible but fictitious entries.
- Core assumption: Citation count correlates with how often a bibliographic record appears in the pretraining corpus; this relationship holds across domains.
- Evidence anchors:
  - [abstract] "citation count exhibits a strong positive correlation with factual accuracy (r = 0.75, p < .001)"
  - [section 3.2] "A strong positive correlation (r = 0.75, p < .001) indicates a clear log-linear relationship, which reflects the memorization scaling law"
  - [corpus] Related work "Hallucinate or Memorize? The Two Sides of Probabilistic Learning in Large Language Models" supports the dual-outcome framework
- Break condition: If citation patterns diverge significantly from training corpus representation (e.g., highly cited papers in languages underrepresented in training data), the proxy relationship may weaken.

### Mechanism 2
- Claim: Below a citation threshold (~100–1,000), the model shifts from verbatim recall to probabilistic synthesis, increasing hallucination risk.
- Mechanism: At low redundancy, the model has insufficient exposure to form strong token associations. It defaults to pattern completion based on structural conventions (author name formats, journal names, year ranges), producing entries that are bibliographically plausible but factually non-existent.
- Core assumption: The transition from memorization to generalization follows a non-linear function with an identifiable inflection point.
- Evidence anchors:
  - [abstract] "bibliographic information becomes almost verbatimly memorized beyond approximately 1,000 citations, where cosine similarity approaches 1.0"
  - [section 3.2] "The estimated inflection point (−β₀/β₁ ≈ 5) corresponds to roughly 100 citations, marking the transition from generalization to memorization"
  - [corpus] No direct corpus evidence on threshold values; this finding appears novel to this study
- Break condition: If prompting strategies explicitly constrain output format or provide retrieval context, the threshold may shift.

### Mechanism 3
- Claim: Domain-specific data availability creates systematic hallucination patterns across research areas.
- Mechanism: Established domains (e.g., image processing) have longer publication histories and more training examples. Emerging domains (e.g., RAG, LoRA) have sparse representations, causing the model to conflate partial knowledge across related papers.
- Core assumption: Domain classification correlates with training data density; newer techniques have systematically lower representation.
- Evidence anchors:
  - [section 3.2] "Domains related to image processing, such as Vision Transformer (ViT) and Diffusion Model, achieved notably higher accuracy, whereas recent LLM-oriented techniques, such as RAG and LoRA, exhibited substantially lower scores"
  - [section 3.2] "hallucinations occurred more frequently in underrepresented domains such as LoRA and Graph Transformer"
  - [corpus] Corpus neighbors focus on citation recommendation systems but do not directly address domain-specific hallucination variance
- Break condition: If a domain is niche but highly self-contained (dense internal citations but low external visibility), the pattern may not hold.

## Foundational Learning

- Concept: **Training data exposure/memorization dynamics**
  - Why needed here: The paper assumes LLMs reproduce frequently-seen content more accurately; understanding exposure-based memorization (Carlini et al. 2022) clarifies why citation frequency predicts accuracy.
  - Quick check question: Can you explain why repeated exposure to specific text during pretraining increases verbatim recall probability?

- Concept: **Cosine similarity for semantic similarity**
  - Why needed here: The study uses Sentence-BERT embeddings with cosine similarity to quantify how closely generated metadata matches authentic records.
  - Quick check question: Given two sentence embeddings, how would you interpret a cosine similarity of 0.95 vs. 0.70?

- Concept: **Log-linear scaling relationships**
  - Why needed here: The correlation between log(citation) and accuracy suggests a scaling law; understanding log transforms is essential for interpreting the threshold effects.
  - Quick check question: Why might a log-transformed independent variable reveal patterns obscured in raw counts?

## Architecture Onboarding

- Component map:
  - LLM generation layer -> Validation layer -> Feature extraction -> Analysis layer

- Critical path:
  1. Prompt engineering → structured JSON output (forces generation, disables uncertainty expression)
  2. Ground truth retrieval → Google Scholar lookup
  3. Scoring → manual fact-checking
  4. Similarity computation → embedding comparison
  5. Statistical analysis → correlation + threshold detection

- Design tradeoffs:
  - Manual validation ensures accuracy but limits scale (n=100); automated validation would scale but risk false positives/negatives
  - Single model (GPT-4.1) provides controlled comparison; multi-model evaluation would test generalizability
  - Google Scholar citation counts are noisy and time-varying; Semantic Scholar or OpenAlex may offer cleaner data

- Failure signatures:
  - High-confidence hallucinations: Correct format, plausible author/title/journal combinations, but no matching paper exists
  - Partial hallucinations: First author and title correct; volume/number/pages fabricated
  - Domain-specific collapse: Emerging fields show systematically lower scores regardless of prompt variation

- First 3 experiments:
  1. Replicate with a different LLM (e.g., Claude, Llama) to test whether the citation-accuracy correlation generalizes across architectures.
  2. Extend to non-CS domains (e.g., medicine, law) to validate whether domain age/recognition predicts hallucination rates similarly.
  3. Test retrieval-augmented prompting: Provide the model with Google Scholar search results for each query and measure whether external grounding eliminates the threshold effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the approximate 1,000-citation threshold for verbatim memorization generalize across different LLM architectures (e.g., open-source models, smaller parameter counts) and training regimes?
- Basis in paper: [explicit] The conclusion states: "future research should extend the analysis to other models, disciplines, and multilingual contexts to assess the generality of this threshold behavior."
- Why unresolved: Only GPT-4.1 was tested; the threshold may depend on model capacity, training data composition, or tokenization strategies specific to this model.
- What evidence would resolve it: Replicating the same experimental protocol across multiple LLMs (e.g., LLaMA, Mistral, Claude) and comparing the citation thresholds and correlation coefficients.

### Open Question 2
- Question: Is citation count a valid proxy for training data redundancy, or do other factors (e.g., arxiv downloads, GitHub references, textbook mentions) better predict memorization behavior?
- Basis in paper: [inferred] The paper assumes citation count equates to training data redundancy but provides no direct evidence linking Google Scholar citations to actual pretraining corpus frequency.
- Why unresolved: Training data composition is proprietary; citation count is an external heuristic that may not capture non-academic repetitions (blogs, tutorials, code comments).
- What evidence would resolve it: Analyzing memorization rates for papers with matched citation counts but differing web footprints, or using models with disclosed training corpora to validate the proxy.

### Open Question 3
- Question: How do hallucination rates change when models are permitted to output "I don't know" or refuse uncertain queries, compared to the constrained JSON-only output format used in this study?
- Basis in paper: [inferred] The methodology notes that "the model was instructed to output JSON-formatted results without explanations, effectively disabling IDK responses," which may artificially inflate hallucination rates.
- Why unresolved: The study design forced generation; it remains unclear whether the citation-hallucination relationship persists when refusal behaviors are enabled.
- What evidence would resolve it: A controlled experiment comparing hallucination rates under two prompting conditions: (1) forced JSON output vs. (2) permissive prompts allowing refusals.

## Limitations
- Manual validation limits sample size to n=100 and introduces potential human error
- Google Scholar citation counts contain noise from duplicates, self-citations, and indexing delays
- Results may not generalize beyond computer science domains or to different LLM architectures

## Confidence
- Citation frequency as proxy for training data redundancy: **High**
- Domain-specific hallucination patterns: **Medium**
- Memorization threshold at ~1,000 citations: **Low-Medium**

## Next Checks
1. Replicate the study using multiple LLM architectures (Claude, Llama, etc.) to test whether the citation-accuracy correlation persists across different training datasets and model designs.
2. Extend the experiment to non-CS domains (medicine, law, social sciences) to verify whether domain age/recognition predicts hallucination rates similarly, controlling for publication volume differences.
3. Test retrieval-augmented prompting: Provide the model with actual Google Scholar search results for each query and measure whether external grounding eliminates the memorization threshold effect.