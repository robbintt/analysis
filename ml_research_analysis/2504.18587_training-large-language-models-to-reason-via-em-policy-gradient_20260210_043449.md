---
ver: rpa2
title: Training Large Language Models to Reason via EM Policy Gradient
arxiv_id: '2504.18587'
source_url: https://arxiv.org/abs/2504.18587
tags:
- reasoning
- grpo
- learning
- math
- empg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EM Policy Gradient, a simple off-policy reinforcement
  learning method for enhancing large language model reasoning. The method frames
  reasoning as an Expectation-Maximization problem, alternating between sampling diverse
  reasoning trajectories and reward-guided fine-tuning.
---

# Training Large Language Models to Reason via EM Policy Gradient

## Quick Facts
- arXiv ID: 2504.18587
- Source URL: https://arxiv.org/abs/2504.18587
- Reference count: 25
- Primary result: Introduces EM Policy Gradient, a simpler off-policy RL method for LLM reasoning that achieves performance comparable to GRPO on GSM8K/MATH HARD while producing more concise reasoning

## Executive Summary
This paper introduces EM Policy Gradient, a simple off-policy reinforcement learning method for enhancing large language model reasoning. The method frames reasoning as an Expectation-Maximization problem, alternating between sampling diverse reasoning trajectories and reward-guided fine-tuning. Unlike PPO and GRPO, it eliminates importance weights and clipping, offering a more scalable and effective approach. Evaluated on GSM8K and MATH HARD datasets using Qwen2.5 base models, EM Policy Gradient achieves performance comparable to or slightly better than GRPO. Additionally, it produces more concise reasoning and exhibits human-like cognitive behaviors such as sub-problem decomposition, self-verification, and backtracking.

## Method Summary
EM Policy Gradient frames LLM reasoning enhancement as an Expectation-Maximization problem. In the E-step, the model samples N reasoning trajectories per prompt using the previous iteration's policy parameters, then evaluates them with a binary reward (correct/incorrect). The M-step updates the policy using a REINFORCE gradient with reward smoothing (sigmoid normalization) and optional KL regularization to a reference model. The key innovation is using off-policy samples without importance weights, justified by keeping policy updates small enough that the distribution shift remains negligible. This simplifies the gradient computation compared to PPO/GRPO while maintaining effectiveness.

## Key Results
- EM Policy Gradient achieves test accuracy of 80.4% on GSM8K and 51.7% on MATH HARD, comparable to or slightly better than GRPO
- The method produces more concise reasoning chains (400-600 tokens) compared to GRPO (700-900 tokens) while maintaining accuracy
- On weaker base models (1.5B), removing the KL constraint accelerates learning, while strong models (14B) benefit from KL regularization

## Why This Works (Mechanism)

### Mechanism 1: One-Step Off-Policy Approximation
The algorithm treats samples from the previous iteration's policy as data for the current update by assuming the policy changes are small enough that the distribution mismatch is negligible. This allows standard REINFORCE without importance weights, eliminating computational overhead while maintaining stability.

### Mechanism 2: Reward Smoothing via Sigmoid Normalization
Sparse binary rewards are smoothed using sigmoid normalization based on batch statistics (mean and standard deviation). This stabilizes gradient scales and acts as a variance reduction technique, preventing gradient explosion from outlier trajectories.

### Mechanism 3: Conditional KL-Regularization
A KL divergence term constrains the model to stay close to a reference model. For strong base models, this preserves high-quality initial reasoning, while for weak models removing this constraint allows faster exploration and learning.

## Foundational Learning

**Concept: Expectation-Maximization (EM)**
*Why needed:* The paper frames the RL problem as an EM process (E-step: sample trajectories, M-step: update policy), distinguishing sampling phase from training phase.
*Quick check:* Why does the E-step use the old policy parameters rather than the current ones?

**Concept: Policy Gradient (REINFORCE)**
*Why needed:* The M-step relies on the policy gradient theorem to maximize expected return. Understanding baseline subtraction is crucial for implementing reward smoothing.
*Quick check:* How does subtracting a baseline from the reward affect the bias and variance of the gradient estimate?

**Concept: Importance Sampling**
*Why needed:* The paper contrasts itself with PPO/GRPO by removing importance sampling weights. Understanding why those methods use it highlights the tradeoff EMPG makes.
*Quick check:* What error does the importance weight correct for in standard off-policy RL?

## Architecture Onboarding

**Component map:**
Base LLM (θold) -> Sampling (Temp=1.0) -> N Trajectories -> Verifier (Reward) -> Buffer -> Sample Batch -> Reward Normalization -> REINFORCE Loss -> Optimizer Update (θnew)

**Critical path:** The "Training Time Inference" (E-Step) is the main throughput bottleneck, generating diverse reasoning traces for every prompt before every update.

**Design tradeoffs:**
- Simplicity vs. Stability: Removes Value Network and GAE computation, relying on one-step approximation which is less theoretically robust
- KL Strength: Must toggle β based on base model strength (β=0 for weak models, β>0 for strong models)

**Failure signatures:**
- GRPO Collapse: GRPO failed to learn on Qwen2.5-Math-7B (score 0.004) in benchmarks
- Stagnation: If model stops improving early, check if KL constraint is too tight for model capacity

**First 3 experiments:**
1. **Sanity Check (Small Model):** Run EMPG on Qwen2.5-1.5B on GSM8K with β=0. Target: Convergence within 50 steps.
2. **Ablation (Reward Smoothing):** Compare raw binary rewards vs. sigmoid-normalized rewards to verify variance reduction.
3. **Reference Model Ablation:** Train 14B model with and without KL penalty to validate conditional regularization.

## Open Questions the Paper Calls Out

**Open Question 1:** Does EMPG's efficiency and "concise reasoning" advantage transfer to non-mathematical domains such as coding or agentic tasks? The algorithm is presented as general but only validated on mathematical problem-solving.

**Open Question 2:** What specific mechanism causes the model to generate shorter reasoning chains than GRPO while maintaining accuracy? The paper demonstrates the correlation but doesn't explain why removing importance weights leads to brevity.

**Open Question 3:** At what level of base model capability does omitting the KL constraint shift from beneficial to detrimental? The paper suggests a threshold exists but doesn't define the exact point where KL becomes necessary.

## Limitations
- The "one-step approximation" lacks theoretical convergence guarantees beyond empirical demonstration
- Reward normalization via sigmoid assumes stable batch statistics, which may fail on highly skewed datasets
- Conditional KL-regularization requires manual tuning of β based on base model strength, limiting generalization

## Confidence
- **High**: The core mechanism (REINFORCE with reward smoothing and off-policy sampling) is technically sound and reproducible
- **Medium**: Empirical performance claims are supported by GSM8K/MATH HARD results but limited to Qwen2.5 models
- **Low**: Claimed human-like behaviors are observational rather than quantitatively validated

## Next Checks
1. **Break Condition Test:** Train with intentionally high learning rates to verify the "one-step approximation" fails as predicted
2. **Model-Agnostic KL Tuning:** Apply EMPG to a non-Qwen base model and test whether the same β thresholds hold
3. **Batch Statistic Robustness:** Evaluate training stability when batches contain all-incorrect samples (σ→0)