---
ver: rpa2
title: 'SPIRe: Boosting LLM Inference Throughput with Speculative Decoding'
arxiv_id: '2504.06419'
source_url: https://arxiv.org/abs/2504.06419
tags:
- decoding
- draft
- speculative
- cost
- throughput
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing speculative decoding
  (SD) for high throughput LLM inference, rather than just low latency. The authors
  introduce SPIRe, a draft model that combines static sparse attention, pruned initialization
  from the target model, and a Feedback Transformer architecture with target model
  activation memory to significantly improve throughput.
---

# SPIRe: Boosting LLM Inference Throughput with Speculative Decoding

## Quick Facts
- arXiv ID: 2504.06419
- Source URL: https://arxiv.org/abs/2504.06419
- Reference count: 5
- This paper introduces SPIRe, a draft model that combines static sparse attention, pruned initialization from the target model, and a Feedback Transformer architecture with target model activation memory to significantly improve throughput in speculative decoding.

## Executive Summary
This paper addresses the challenge of optimizing speculative decoding (SD) for high throughput LLM inference, rather than just low latency. The authors introduce SPIRe, a draft model that combines static sparse attention, pruned initialization from the target model, and a Feedback Transformer architecture with target model activation memory to significantly improve throughput. SPIRe achieves over 100% increase in modeled throughput compared to vanilla speculative decoding and over 35% compared to the strong baseline MagicDec, particularly when context lengths vary across requests. The work includes an implementation-agnostic performance model to evaluate throughput and demonstrates effectiveness when scaling to large batch sizes and medium-to-long contexts.

## Method Summary
The method trains a draft model (SPIRe) that is 1/4 the size of the target model, using pruned initialization from the target's embedding, last two transformer blocks, and unembedding. The draft uses a Feedback Transformer architecture with StreamingLLM sparse attention (window size 64, sink size 1). During training, the draft performs k=4 forward passes per prefix, substituting target model activations for early memory vectors. The loss combines distillation with acceptance probability maximization (MixedLoss with ω=0.5). At inference, the draft generates k tokens autoregressively using its sparse KV cache, which the target model verifies and accepts/rejects via modified rejection sampling. The approach is evaluated using an implementation-agnostic performance model that accounts for FLOPs and memory costs across different batch sizes and context lengths.

## Key Results
- SPIRe achieves over 100% increase in modeled throughput compared to vanilla speculative decoding
- SPIRe outperforms MagicDec by over 35% in throughput, especially with varying context lengths
- Sparse KV cache enables throughput gains to improve as batch size increases, unlike vanilla SD

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse KV cache with StreamingLLM attention enables speculative decoding speedups to persist or improve as batch size increases, unlike vanilla SD.
- Mechanism: A sliding window (size 64) + attention sink (size 1) mask keeps KV cache size constant w.r.t. sequence length. In large-batch long-context regimes, KV cache loads dominate memory bandwidth; reducing KV size reduces ∆t more than it reduces acceptance rate τ. The throughput multiplier τ/∆t thus improves with batch size.
- Core assumption: Verification and draft forward passes remain memory-bound; HOI (FLOPs/byte) is such that memory cost exceeds compute cost for relevant batch/context configurations.
- Evidence anchors:
  - [abstract] "Recent work shows that SD can accelerate decoding with large batch sizes too if the context is sufficiently long and the draft model's KV cache is sparse."
  - [section 3.1] "As shown in Figure 4, this results in an increase in the throughput multiplier as the batch size increases, which is never the case for vanilla speculative decoding."
  - [corpus] MagicDec (Chen et al., 2024) demonstrates similar sparse KV benefits; SPIRe builds on this with additional architecture changes.
- Break condition: If batch size is small or context is short, KV cache is not the bottleneck; sparse KV yields no benefit and may harm throughput due to added FLOPs.

### Mechanism 2
- Claim: Pruned initialization from the target model yields higher average generation length τ than random initialization.
- Mechanism: Copy embedding, last N transformer blocks, and unembedding from trained target → draft starts in a region of parameter space already aligned with target's token distribution. Matching embedding dimension enables direct substitution of target activations as memory vectors during training.
- Core assumption: The pruned layers capture sufficient target knowledge; remaining draft layers can be trained efficiently on top.
- Evidence anchors:
  - [section 3.2] "As shown in Figure 5, initializing by pruning the target model produced a significantly higher value of τ than random initialization."
  - [section 5] Training SPIRe costs ~1/4 the FLOPs of training the target model.
  - [corpus] No direct corpus comparison for pruning-based draft initialization; this is a relatively novel contribution.
- Break condition: If draft model depth/width differs substantially from target, simple layer copying may not transfer well.

### Mechanism 3
- Claim: Feedback Transformer with target model activation memory improves draft quality while remaining trainable without full sequential unrolling.
- Mechanism: During training, for tokens t = 1...S-k, replace draft memory vectors m_t with target model activations y_t from corresponding layers. Only compute k forward passes per training step (not S), recovering parallelism. At inference, use draft's own memory vectors autoregressively.
- Core assumption: Target activations provide a high-quality proxy for what the draft's memory vectors should approximate; the draft can learn to approximate this relationship.
- Evidence anchors:
  - [section 3.3] "substituting past memory vectors with target model activations... was better than sharing memory vectors across layers and substituting... with draft model embeddings."
  - [figure 5 ablations] Attending to target activations is the second most important technique after sparse KV.
  - [corpus] Feedback Transformers (Fan et al., 2021) showed representation gains but training costs; SPIRe's activation substitution is a novel adaptation for draft models.
- Break condition: If target model activations are unavailable (e.g., black-box API), this mechanism cannot be applied.

## Foundational Learning

- Concept: **Memory-bound vs compute-bound inference regimes**
  - Why needed here: The entire rationale for SPIRe depends on understanding when KV cache loads (memory) dominate vs weight loads or FLOPs (compute). Small batch = weight-bound; large batch + long context = KV-bound.
  - Quick check question: For batch size 64 and context length 512, is the draft forward pass memory-bound or compute-bound on your hardware? How would you verify?

- Concept: **Speculative decoding acceptance rate and rejection sampling**
  - Why needed here: The throughput multiplier is τ/∆t; τ depends on how many draft tokens the target accepts. Understanding the modified rejection sampling scheme explains why accepted tokens are guaranteed samples from the target distribution.
  - Quick check question: If draft and target distributions are identical, what is the expected acceptance rate for k draft tokens?

- Concept: **Hardware Operational Intensity (HOI)**
  - Why needed here: The performance model converts memory accesses to FLOP-equivalents via HOI to compare compute and memory costs on equal footing. Different hardware has different HOI values.
  - Quick check question: If HOI = 100 FLOPs/byte and KV cache is 1MB, what is the memory cost in FLOP-equivalents?

## Architecture Onboarding

- Component map:
  - Target model: 8-layer MHA transformer (67M body params)
  - Draft model (SPIRe): 2-layer Feedback Transformer with StreamingLLM sparse attention, ~1/4 target body params
  - Key difference from standard: Memory vectors m_t are weighted combinations of layer activations; during training, early m_t are replaced with target activations

- Critical path:
  1. Training: For each sequence, perform k=4 forward passes on prefixes of length k, 2k, 3k, ...; substitute target activations for positions 1 to S-k
  2. Inference: Autoregressively generate k draft tokens using draft's own sparse KV cache; verify with target; accept/reject via modified rejection sampling
  3. MixedLoss: ω·distillation + (1-ω)·acceptance-probability maximization (ω=0.5)

- Design tradeoffs:
  - Larger draft model → higher τ but higher t_draft; SPIRe chooses 1/4 target size as balance
  - Smaller sparse window → lower KV cost but potentially lower τ; SPIRe uses 64+1
  - Feedback memory → better representations but inhibits training parallelism; mitigated by activation substitution

- Failure signatures:
  - Throughput multiplier < 1.0: Draft model too slow or acceptance rate too low; check ∆t and τ separately
  - Degradation at short contexts: Sparse KV overhead not offset by benefits; consider hybrid approach
  - Training divergence: Target activation dimensions mismatched with draft embedding dimension

- First 3 experiments:
  1. Replicate the throughput multiplier calculation for (B=64, L=512) with vanilla SD baseline; verify ~2.78x speedup vs ~1.0x baseline
  2. Ablate sparse KV: Run SPIRe with dense KV; expect constant throughput multiplier w.r.t. context length (no growth as L increases)
  3. Ablate target activation substitution: Train with draft embeddings instead; measure τ drop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How accurately does the proposed implementation-agnostic performance model predict actual wall-clock throughput on production hardware?
- Basis in paper: [explicit] The Conclusion states, "Future work can empirically validate our performance model."
- Why unresolved: The authors use a theoretical model based on FLOPs and memory access (HOI) to compare architectures, intentionally isolating results from specific kernel optimizations or hardware nuances.
- What evidence would resolve it: End-to-end benchmarks on specific hardware (e.g., H100 GPUs) comparing measured throughput against the model's predicted multipliers across varying batch sizes.

### Open Question 2
- Question: How does the speedup of SPIRe change relative to the maximum speculation depth $k$?
- Basis in paper: [explicit] The Conclusion lists the need to "analyze the sensitivity of speedup with respect to the maximum speculation depth k."
- Why unresolved: The evaluation fixes the speculation depth at $k=4$; it is unknown if deeper speculation yields diminishing returns or if the Feedback Transformer architecture supports longer draft sequences efficiently.
- What evidence would resolve it: A sweep of throughput metrics with $k$ ranging from 1 to 16 to identify the optimal trade-off between verification overhead and acceptance rate.

### Open Question 3
- Question: How does SPIRe interact with continuous batching systems in a dynamic serving environment?
- Basis in paper: [explicit] In Related Work, the authors note that "studying their interaction further is an interesting direction for future work."
- Why unresolved: Continuous batching dynamically preempts and schedules requests, which may conflict with SPIRe's use of static sparse attention and target model activation memory.
- What evidence would resolve it: Measurements of SPIRe's performance when integrated into a continuous batching scheduler (like vLLM or Orca) under variable load.

### Open Question 4
- Question: Can more sophisticated pruning methods improve SPIRe's initialization compared to the current layer-selection heuristic?
- Basis in paper: [explicit] Section 3.2 suggests, "Future work could explore more sophisticated pruning strategies, such as the one described by Muralidharan et al. [2024]."
- Why unresolved: The current method simply selects the final layers of the target model; advanced pruning might retain a more optimal distribution of weights that increases draft accuracy (acceptance rate).
- What evidence would resolve it: Comparing the average generation length ($\tau$) of the current SPIRe model against a version initialized using gradient-based or magnitude-based pruning.

## Limitations

- The performance model's predictions depend on hardware-specific operational intensity (HOI) values that are not reported, limiting generalizability across different hardware platforms.
- The approach requires access to the target model's activations during training, making it inapplicable in black-box or API-based serving scenarios.
- The claimed throughput improvements are tied to specific model sizes (8-layer target, 2-layer draft) and context lengths, with unclear scalability to deeper models or different workloads.

## Confidence

**High Confidence**: The architectural choices (pruned initialization, sparse KV, activation substitution) are well-motivated and grounded in established techniques. The performance model is rigorous and implementation-agnostic, and the empirical results show consistent improvements across multiple metrics.

**Medium Confidence**: The claimed throughput multiplier improvements depend on unverified hardware assumptions (HOI, memory-bound regime) and are tied to a specific model size and context length. While the results are internally consistent, their generalizability to other hardware or model scales is uncertain.

**Low Confidence**: The effectiveness of the Feedback Transformer with activation substitution in isolation, without the other SPIRe components, is not rigorously tested. The paper also does not address potential failure modes when draft and target models differ substantially in depth or width.

## Next Checks

1. **Hardware Independence Test**: Replicate the throughput multiplier calculation for (B=64, L=512) on a different hardware platform (e.g., A100 GPU vs TPU) and report the HOI values used. Confirm whether the memory-bound assumption holds and whether the throughput multiplier remains above 2.0.

2. **Ablation of Target Activation Substitution**: Train a baseline draft model without activation substitution (using draft embeddings instead) and measure the drop in τ. Confirm whether this component alone accounts for a significant portion of the throughput gains.

3. **Scaling to Larger Models**: Test SPIRe with a deeper target model (e.g., 16 or 32 layers) and corresponding draft depth. Measure whether the throughput multiplier scales proportionally or if diminishing returns set in due to increased memory pressure or training complexity.