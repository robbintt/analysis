---
ver: rpa2
title: A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding
  and Reasoning
arxiv_id: '2512.07136'
source_url: https://arxiv.org/abs/2512.07136
tags:
- data
- actions
- action
- cuhk-x
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CUHK-X is a large-scale multimodal dataset and benchmark for human
  activity scene understanding and reasoning. It addresses the lack of large-scale,
  multimodal datasets with fine-grained caption annotations needed for human action
  understanding (HAU) and human action reasoning (HARn) tasks.
---

# A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning

## Quick Facts
- **arXiv ID:** 2512.07136
- **Source URL:** https://arxiv.org/abs/2512.07136
- **Reference count:** 40
- **Key outcome:** CUHK-X dataset enables multimodal HAR benchmarks with 76.52% accuracy for HAR, 40.76% for HAU, and 70.25% for HARn.

## Executive Summary
CUHK-X is a large-scale multimodal dataset and benchmark for human activity scene understanding and reasoning. It addresses the lack of large-scale, multimodal datasets with fine-grained caption annotations needed for human action understanding (HAU) and human action reasoning (HARn) tasks. The dataset includes 58,445 samples across seven modalities (RGB, depth, thermal, infrared, IMU, mmWave, skeleton) from 30 participants in two indoor environments. A novel prompt-based scene creation method with human validation generates logically connected activity sequences. The benchmarks evaluate six tasks across three domains, achieving 76.52% accuracy for HAR, 40.76% for HAU, and 70.25% for HARn. This dataset enables development of data-intensive learning techniques for robust human activity analysis across multiple modalities.

## Method Summary
The CUHK-X dataset employs a GT-First methodology where action sequences and captions are defined before data collection. LLMs generate logically connected action scripts validated by humans, which participants then enact while sensors record. The dataset contains 58,445 samples from 30 participants performing 40 action classes across seven modalities. Benchmarks include HAR (classification), HAU (understanding/captioning), and HARn (reasoning). Training uses ResNet-50, PointNet, MotionBert, and 1D-CNN+Transformer architectures with specified hyperparameters. LVLMs are evaluated zero-shot using provided prompts.

## Key Results
- HAR accuracy: 76.52% across all modalities (RGB: 90.89%, Depth: 63.84%, Thermal: 46.63%)
- HAU metrics: BERTScore 41.61, ROUGE 29.44, BLEU 20.26, METEOR 22.22
- HARn accuracy: 70.25% for next-action prediction
- Significant performance gap between RGB and non-RGB modalities

## Why This Works (Mechanism)

### Mechanism 1: Ground Truth-First (GT-First) Scene Generation
The GT-First approach defines action sequences and captions before data collection, reducing spatiotemporal inconsistencies compared to post-hoc annotation. LLMs generate logically connected scripts (e.g., "washing face" followed by "brushing teeth") that are human-validated before participant enactment. This contrasts with data-first approaches where labels are applied to existing recordings.

### Mechanism 2: Multimodal Alignment for Non-RGB LVLMs
Providing synchronized non-RGB modalities with high-quality captions enables LVLMs to extend beyond RGB domains. The dataset offers aligned pairs where sensor data (mmWave, Thermal) is paired with detailed textual descriptions, allowing models to learn semantic mappings from non-visual signals.

### Mechanism 3: Structured Reasoning Tasks (HARn)
Defining benchmarks requiring intention inference forces models to learn causal dependencies between actions. HARn tasks predict subsequent actions based on current sequences, with reasoning models (VideoChatR1) outperforming captioning models by leveraging contextual logic.

## Foundational Learning

- **Large Vision Language Models (LVLMs) & Modality Gap:** Standard LVLMs are trained on RGB data and lack embedding space alignment to interpret non-RGB modalities. Why needed: To understand why a dataset for non-RGB modalities is necessary. Quick check: Why can't a standard vision model fine-tuned on ImageNet classify activities from a thermal camera without domain adaptation?

- **Spatiotemporal Consistency:** Naive caption generation leads to logical errors (e.g., "brushing teeth" in a kitchen). Why needed: Key to appreciating the GT-first approach. Quick check: If a dataset labels a video as "cooking" but the visual background is a bedroom, why does this spatiotemporal inconsistency degrade model performance?

- **Sensor Modalities (IMU, mmWave, Thermal):** These sensors capture different physical properties (acceleration, distance/velocity, temperature). Why needed: Required to diagnose why HAR accuracy for mmWave (46.63%) is lower than RGB (90.89%). Quick check: An IMU sensor on the wrist and one on the ankle will produce different signals for the same action (e.g., walking). How does a model handle this variance?

## Architecture Onboarding

- **Component map:** Sensors (7 modalities) -> GT-First pipeline (LLM Script -> Human Check -> Subject Enactment -> Recording) -> Benchmarks (HAR, HAU, HARn)

- **Critical path:**
  1. Data Access: Download specific modality data and corresponding caption files
  2. Pre-processing: Align timestamps using global time marker, apply modality-specific transforms
  3. Model Selection: Choose architecture based on task (ResNet-50 for HAR, VideoLLaVA for HAU/HARn)

- **Design tradeoffs:**
  - Control vs. Naturalism: GT-first ensures high-quality labels but sacrifices natural chaos
  - Resolution vs. Privacy: mmWave/Thermal offer privacy but lower accuracy (46.63% vs 90.89%)
  - Cross-Subject: Significant accuracy drop in cross-subject settings suggests subject-specific features

- **Failure signatures:**
  - Low HAR Accuracy on Radar/IMU: Verify data synchronization and check if data augmentation is needed
  - Poor HAU Scores: Ensure correct input modality mapping and exact prompt text matching
  - Context Blindness: Captioning models fail to infer speed or emotion in Context Analysis task

- **First 3 experiments:**
  1. Modality Gap Analysis: Evaluate LVLM on RGB vs. Depth vs. Thermal to establish non-RGB difficulty baseline
  2. Cross-Subject HAR: Train on 29 subjects, test on 30th (LOSO protocol) to assess generalization
  3. Reasoning vs. Captioning: Compare HARn prediction accuracy using standard captioning vs. reasoning-specific models

## Open Questions the Paper Calls Out

- Generalization to populations with different motor patterns (older adults, children, mobility-impaired individuals) is uncertain due to current dataset limitations to 20-23 year olds.

- Integration of complementary signals (audio, tactile sensing, physiological data) could improve HARn performance but is not currently included.

- Current dataset lacks longer-horizon routines and multi-participant interaction scenarios, focusing on single-subject segmented actions.

## Limitations

- Dataset limited to 30 young adult participants (20-23 years), raising questions about generalization to other demographics.

- Significant performance gap between RGB (90.89%) and non-RGB modalities (46.63% for Thermal) indicates fundamental cross-modality challenges.

- LVLM-based reasoning benchmarks rely on zero-shot evaluation without ablation studies on prompt engineering or model selection.

## Confidence

- **High Confidence:** Dataset statistics and benchmark task definitions are clearly specified and reproducible.
- **Medium Confidence:** GT-First methodology is logically sound but lacks empirical validation of script adherence.
- **Low Confidence:** Claims about LVLM reasoning capabilities are based on zero-shot evaluation without systematic studies.

## Next Checks

1. Conduct pilot study measuring participant deviation rates from GT-First scripts to quantify methodology reliability.
2. Perform controlled experiments comparing RGB vs. non-RGB modality performance using identical model architectures.
3. Run systematic prompt ablation studies on HAU/HARn benchmarks to determine if performance differences are architecture-dependent or prompt-dependent.