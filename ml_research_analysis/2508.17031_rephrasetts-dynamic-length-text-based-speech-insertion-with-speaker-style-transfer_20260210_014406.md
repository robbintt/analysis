---
ver: rpa2
title: 'RephraseTTS: Dynamic Length Text based Speech Insertion with Speaker Style
  Transfer'
arxiv_id: '2508.17031'
source_url: https://arxiv.org/abs/2508.17031
tags:
- speech
- audio
- phoneme
- style
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose RephraseTTS, a transformer-based non-autoregressive
  method for dynamic-length text-conditioned speech insertion. It inserts variable-length
  speech segments conditioned on full text transcripts while preserving speaker style,
  prosody, and spectral properties from partial audio context.
---

# RephraseTTS: Dynamic Length Text based Speech Insertion with Speaker Style Transfer

## Quick Facts
- **arXiv ID**: 2508.17031
- **Source URL**: https://arxiv.org/abs/2508.17031
- **Authors**: Neeraj Matiyali; Siddharth Srivastava; Gaurav Sharma
- **Reference count**: 5
- **Primary result**: On LibriTTS, RephraseTTS achieves MOS of 3.93 (vs 3.03 for adaptive TTS) and MCD of 0.579 (vs 0.833) on clean set.

## Executive Summary
RephraseTTS introduces a transformer-based non-autoregressive method for dynamic-length text-conditioned speech insertion. The approach inserts variable-length speech segments conditioned on full text transcripts while preserving speaker style, prosody, and spectral properties from partial audio context. Using cross-modal attention to infuse audio context into phoneme representations and employing adversarial, feature matching, and style matching losses, the method achieves significant improvements in naturalness and spectral quality over adaptive TTS baselines on the LibriTTS dataset.

## Method Summary
RephraseTTS is a transformer-based method for inserting variable-length speech segments into audio based on full text transcripts. The system uses cross-modal attention to transfer style from partial audio context to phoneme representations, enabling dynamic length insertion through a non-autoregressive variance adaptor with duration prediction. The model is trained on LibriTTS with L1 reconstruction loss followed by joint training with adversarial, feature matching, and style matching losses. Mel-spectrograms are generated and converted to waveforms using a pre-trained HiFi-GAN vocoder.

## Key Results
- Outperforms adaptive TTS baselines on LibriTTS: MOS of 3.93 vs 3.03, MCD of 0.579 vs 0.833 on clean set
- Ablation studies show all loss components (adversarial, feature matching, style matching) contribute to quality improvements
- Performance degrades on longer insertions (>20 phonemes) with MOS dropping from 4.20 to 3.97
- Cross-modal attention shows inconsistent performance across dev-clean (improves MCD) and dev-other (worsens MCD)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Cross-modal attention enables style transfer from partial audio context to phoneme representations without explicit alignment.
- **Mechanism**: Multi-headed attention computes queries from phoneme encodings and keys/values from audio encoder outputs, allowing each phoneme to selectively attend to relevant audio frames and extract speaker timbre, prosody, and recording characteristics.
- **Core assumption**: Audio context contains sufficient stylistic information to condition the generated segment; style is locally consistent across utterances.
- **Evidence anchors**: Abstract mentions "cross-modal attention to dynamically determine the length... maintains the speaker's voice characteristics"; Section 3.2 describes T^CA containing style information; weak corpus evidence from Fed-PISA.
- **Break condition**: Very short audio context (<1 second) or acoustically dissimilar context degrades style transfer; ablation shows removing cross-modal attention worsens MCD on dev-clean but improves on dev-other.

### Mechanism 2
- **Claim**: Non-autoregressive variance adaptor with duration prediction enables dynamic-length insertion.
- **Mechanism**: Variance adaptor predicts phoneme-level pitch, energy, and duration; length regulator upsamples phoneme representations by replicating each according to predicted duration, creating one-to-one alignment with output mel-spectrogram frames without autoregressive dependencies.
- **Core assumption**: Duration can be accurately predicted from phoneme representations enriched with style information; tempo from surrounding audio context informs appropriate duration for inserted segment.
- **Evidence anchors**: Abstract mentions "variable lengths... dynamically determined during inference"; Section 3.2 describes length regulator creating one-to-one alignment; no direct corpus evidence.
- **Break condition**: Duration prediction errors cascade into misaligned mel-spectrograms; very long insertions (>20 phonemes) show MOS degradation.

### Mechanism 3
- **Claim**: Local and global adversarial losses with style matching reduce artifacts and improve naturalness.
- **Mechanism**: Global discriminator evaluates full-spectrogram realism; local discriminator focuses on short windows from inserted segment; style matching loss uses triplet margin to ensure generated samples are closer to ground-truth style from same speaker than to other speakers or synthetic samples.
- **Core assumption**: L1 reconstruction alone produces robotic speech; adversarial training and style constraints are needed for naturalness.
- **Evidence anchors**: Section 4.3 shows L1-only model has significant artifacts; Table 3 shows adding all losses improves MCD from 0.5884 to 0.5790; no corpus evidence.
- **Break condition**: Adversarial training can introduce instabilities; dev-other shows mixed results where removing some losses improves MCD, suggesting overfitting or mode collapse.

## Foundational Learning

- **Concept**: Mel-spectrogram representation
  - **Why needed here**: Core input/output representation; understanding hop size, FFT window, and frequency bins is essential for debugging alignment and quality issues.
  - **Quick check question**: Given 22050 Hz sample rate, 256-sample hop size, and 1024-sample FFT, what is the time resolution in milliseconds per mel-spectrogram frame?

- **Concept**: Phoneme-level forced alignment
  - **Why needed here**: Training requires ground-truth alignment between audio frames and phonemes (via Montreal Forced Aligner); understanding alignment quality affects duration prediction accuracy.
  - **Quick check question**: Why might forced alignment fail on spontaneous speech vs. read speech?

- **Concept**: Non-autoregressive vs. autoregressive decoding
  - **Why needed here**: FastSpeech2-style parallel decoding enables fast inference but requires duration prediction; understanding trade-offs helps diagnose timing artifacts.
  - **Quick check question**: What is the inference speed advantage of non-autoregressive TTS, and what is the primary quality risk?

## Architecture Onboarding

- **Component map**: Partial mel-spectrogram (X_B, X_A) + full phoneme sequence (T_B, T_I, T_A) → Audio Encoder → Phoneme Encoder → Cross-Modal Attention → Variance Adaptor → Length Regulator → Decoder → Post-processing (HiFi-GAN) → waveform

- **Critical path**: Phoneme encoding → Cross-modal attention (style infusion) → Duration prediction (determines output length) → Length regulation → Decoder → Vocoder. Duration prediction is the key bottleneck for timing quality.

- **Design tradeoffs**:
  - Segment embeddings help model distinguish before/after context but add parameters; ablation shows removal worsens dev-clean MCD.
  - Cross-modal attention vs. speaker encoder: Cross-modal is more expressive but speaker encoder is simpler; paper shows cross-modal better on dev-clean but worse on dev-other.
  - Two-phase training (L1-only pretraining) stabilizes GAN training but adds complexity.
  - Window size for local discriminator (96 frames) balances local detail and context.

- **Failure signatures**:
  - Robotic artifacts: Insufficient adversarial/style loss training or mode collapse.
  - Tempo mismatch: Duration prediction failing to match surrounding speech rate.
  - Style discontinuity: Cross-modal attention not extracting sufficient context; check audio context length.
  - Speaker leakage: Style matching triplet mining incorrect; verify batch composition.
  - Alignment errors: MFA quality issues or segment boundary problems.

- **First 3 experiments**:
  1. **Baseline replication**: Train L1-only model on LibriTTS train-clean-360, measure MCD on dev-clean. Verify reproduction of ~0.588 MCD before adding adversarial losses.
  2. **Ablation sweep**: Train models removing one component at a time (cross-modal attention, segment embeddings, style loss, local discriminator). Compare MCD on both dev-clean and dev-other to understand which components help on clean vs. challenging data.
  3. **Context length sensitivity**: Vary minimum audio context length (0.5s, 1s, 2s) and measure style consistency using speaker verification embeddings on output. Identify context threshold below which style transfer degrades significantly.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance degrades on longer insertions (>20 phonemes) with MOS dropping from 4.20 to 3.97, indicating potential instability in the variance adaptor.
- Cross-modal attention shows inconsistent performance across dev-clean (improves MCD) and dev-other (worsens MCD), suggesting method may be more fragile on challenging acoustic conditions.
- Reliance on forced alignment via MFA introduces an additional failure point - alignment errors would propagate directly into duration prediction and ultimately insertion quality.

## Confidence

**High Confidence**: The core architecture design (FastSpeech2 backbone with cross-modal attention for style transfer) is technically sound and well-motivated. The objective function components (L1 reconstruction, LSGAN, feature matching, and triplet style loss) are standard and properly integrated. The empirical results showing improvement over baseline adaptive TTS methods are reproducible given the provided implementation details.

**Medium Confidence**: The claim that cross-modal attention enables effective style transfer without explicit alignment is supported by ablation studies but relies on the assumption that local audio context contains sufficient stylistic information. The performance difference between dev-clean and dev-other subsets suggests the method may be more fragile than presented, particularly on challenging acoustic conditions. The style matching loss mechanism appears effective but the choice of hyperparameters (margin, mining strategy) is not fully specified.

**Low Confidence**: The generalization capability to speakers/conditions outside the LibriTTS training set is unknown. The paper does not address out-of-domain performance or speaker adaptation. The minimum audio context requirement for reliable style transfer is not empirically determined, though the paper mentions it as a factor. The potential for adversarial training instability is acknowledged but not systematically evaluated.

## Next Checks

1. **Cross-Modal Attention Robustness**: Systematically vary the length of audio context provided (0.5s, 1s, 2s, full utterance) and measure style consistency using speaker verification embeddings on the generated output. This will identify the minimum context threshold below which style transfer degrades significantly and validate the local consistency assumption.

2. **Duration Prediction Error Analysis**: Generate insertions across varying lengths (5, 10, 15, 20+ phonemes) and compute duration prediction error rates against ground truth. Correlate these errors with MOS degradation to quantify the impact of the variance adaptor on insertion quality and identify length thresholds where the mechanism becomes unreliable.

3. **Out-of-Domain Generalization**: Test the pretrained model on a different speech dataset (e.g., VCTK or TED-LIUM) without fine-tuning. Measure MCD and perform speaker verification to assess whether the cross-modal attention can extract style from audio with different acoustic characteristics and recording conditions than the LibriTTS training data.