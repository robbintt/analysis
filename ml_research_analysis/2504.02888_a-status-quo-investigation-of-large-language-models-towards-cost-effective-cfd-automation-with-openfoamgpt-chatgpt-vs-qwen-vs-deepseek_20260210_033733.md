---
ver: rpa2
title: 'A Status Quo Investigation of Large Language Models towards Cost-Effective
  CFD Automation with OpenFOAMGPT: ChatGPT vs. Qwen vs. Deepseek'
arxiv_id: '2504.02888'
source_url: https://arxiv.org/abs/2504.02888
tags:
- fluid
- arxiv
- language
- turbulence
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated multiple large language models (LLMs) for
  cost-effective computational fluid dynamics (CFD) automation using the OpenFOAMGPT
  framework. Tested models included GPT-4o, OpenAI o1, DeepSeek V3, and Qwen 2.5-Max,
  with token pricing ranging from $0.035 to $15.0 per million input tokens.
---

# A Status Quo Investigation of Large Language Models towards Cost-Effective CFD Automation with OpenFOAMGPT: ChatGPT vs. Qwen vs. Deepseek

## Quick Facts
- arXiv ID: 2504.02888
- Source URL: https://arxiv.org/abs/2504.02888
- Reference count: 0
- Qwen 2.5-Max achieved performance comparable to OpenAI o1 while reducing token costs by up to two orders of magnitude for CFD automation tasks.

## Executive Summary
This study evaluated multiple large language models (LLMs) for cost-effective computational fluid dynamics (CFD) automation using the OpenFOAMGPT framework. Tested models included GPT-4o, OpenAI o1, DeepSeek V3, and Qwen 2.5-Max, with token pricing ranging from $0.035 to $15.0 per million input tokens. Results showed that Qwen 2.5-Max achieved performance comparable to OpenAI o1 while reducing token costs by up to two orders of magnitude. Zero-shot prompting successfully handled common CFD tasks across benchmark cases including cavity flow, PitzDaily, and airfoil simulations. However, complex geometries and boundary condition specifications remained challenging, requiring expert supervision. A locally deployed QwQ-32B model struggled with specialized engineering tasks, suggesting smaller models need domain-specific fine-tuning. The study demonstrates that Chinese LLMs offer promising cost-effective alternatives for CFD automation while highlighting the ongoing need for human oversight in complex scenarios.

## Method Summary
The study implemented a zero-shot prompting framework called OpenFOAMGPT to automate CFD workflows using multiple LLM providers. The system used a Builder component for plan generation, an Executor for task routing, and an Interpreter for file generation. OpenFOAM simulations ran in an automated loop with continuous error monitoring - upon failure detection, error logs were appended to the original query and resubmitted to the LLM. Five benchmark cases were tested: Cavity (2D lid-driven flow), PitzDaily (3D turbulent flow), MotorBike (3D complex geometry), nozzleFlow (3D multiphase flow), and AirFoil (2D aerodynamic analysis). Models tested included GPT-4o, OpenAI o1, DeepSeek V3, Qwen 2.5-Max, and locally deployed QwQ-32B.

## Key Results
- Qwen 2.5-Max achieved performance comparable to OpenAI o1 while reducing token costs by up to two orders of magnitude
- Zero-shot prompting successfully handled common CFD tasks including cavity flow, PitzDaily, and airfoil simulations
- Complex geometries and boundary condition specifications remained challenging, requiring expert supervision
- A locally deployed QwQ-32B model struggled with specialized engineering tasks, suggesting smaller models need domain-specific fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Cost-Performance Decomposition via Alternative Model Providers
- Claim: Chinese LLM providers can achieve parity with premium US models on structured CFD tasks at ~100× lower token cost.
- Mechanism: Qwen 2.5-Max and DeepSeek V3 leverage efficient mixture-of-experts (MoE) architectures and aggressive pricing strategies, enabling equivalent task completion (boundary condition modification, turbulence model selection) without reasoning-model overhead.
- Core assumption: CFD code generation relies more on syntactic pattern recall than deep multi-step reasoning for standard cases.
- Evidence anchors:
  - [abstract] "Qwen 2.5-Max achieved performance comparable to OpenAI o1 while reducing token costs by up to two orders of magnitude."
  - [section] Table I shows Qwen 2.5-Max at $0.80/$1.20 per million tokens vs. o1 at $15.0/$60.0.
  - [corpus] CFD-copilot paper (arXiv:2512.07917) similarly explores domain-adapted LLMs for simulation automation, suggesting cost-effectiveness is an active research direction, though direct benchmarks are not provided.
- Break condition: Complex geometry setups requiring iterative spatial reasoning (MotorBike, nozzleFlow) expose performance gaps where token savings are negated by failed iterations.

### Mechanism 2: Iterative Error-Feedback Correction Loop
- Claim: Autonomous CFD workflow completion depends on structured error-signal injection back into the LLM prompt context.
- Mechanism: The Executor monitors OpenFOAM stderr output; upon failure detection, error logs are appended to the original query and re-submitted, enabling the LLM to correct syntax/parameter errors without external knowledge retrieval.
- Core assumption: Error messages contain sufficient diagnostic information for LLMs to localize and fix issues within context window limits.
- Evidence anchors:
  - [section] "the system output and error logs are continuously monitored during simulation; upon failure detection, the error data are appended to the original query and the process cycles again."
  - [section] Table III shows iteration counts: Bubble (8), Droplet (20), AirFoil (2), correlating with task complexity.
  - [corpus] Weak corpus signal—Residual-guided AI-CFD hybrid method (arXiv:2510.21804) uses residual feedback for stability, but LLM-specific error-loop mechanisms are not directly compared.
- Break condition: Persistent boundary-condition errors (MotorBike: "'patch' not constraint type 'empty'") indicate error messages alone insufficient when underlying conceptual mapping is missing.

### Mechanism 3: Scale-Dependent Domain Competence Threshold
- Claim: Sub-100B parameter models lack sufficient embedded domain knowledge for zero-shot CFD file generation without fine-tuning.
- Mechanism: QwQ-32B (32B parameters, 4-bit quantized) failed on both Cavity and PitzDaily cases despite RAG support; errors in boundary condition specification and turbulence model parameterization suggest knowledge representation gaps rather than inference-time retrieval failures.
- Core assumption: Domain competence for specialized engineering syntax requires parameter counts above an undocumented threshold (~100B+).
- Evidence anchors:
  - [abstract] "A locally deployed QwQ-32B model struggled with specialized engineering tasks, suggesting smaller models need domain-specific fine-tuning."
  - [section] "sub-100B parameter general-purpose models exhibit critical knowledge gaps in specialized engineering domains" (citing PaLM analysis).
  - [corpus] No direct corpus validation for the 100B threshold claim; cited reference [43] is a general scaling paper (PaLM), not CFD-specific.
- Break condition: If fine-tuned smaller models achieve parity, the mechanism would shift from scale-dependence to training-data-dependence.

## Foundational Learning

- Concept: OpenFOAM case structure (system/, constant/, 0/ directories; controlDict, fvSchemes, fvSolution, boundary conditions)
  - Why needed here: LLM outputs must conform to OpenFOAM's file hierarchy and syntax; errors like missing 'p_rgh' dependencies or incorrect patch types cascade into simulation failures.
  - Quick check question: Can you explain what files must be modified to change a k-ε turbulence model to k-ω SST in OpenFOAM?

- Concept: Zero-shot vs. RAG-augmented prompting
  - Why needed here: This study explicitly disables RAG to test pure zero-shot capability; understanding the tradeoff helps interpret when retrieved documentation would rescue failures.
  - Quick check question: What information would a RAG system retrieve that a zero-shot prompt cannot access?

- Concept: CFD boundary condition taxonomy (Dirichlet, Neumann, wall vs. inlet vs. outlet, constraint types like 'empty' for 2D)
  - Why needed here: Boundary condition configuration was the dominant failure mode (MotorBike, nozzleFlow); understanding type constraints is prerequisite for prompt engineering or error diagnosis.
  - Quick check question: Why does OpenFOAM require 'empty' type for front/back patches in 2D simulations?

## Architecture Onboarding

- Component map: User Query + System Prompt -> Builder (plan generation; RAG optional) -> Executor (task routing) -> LLM Model -> Interpreter (file generation) -> OpenFOAM Runner (simulation execution) -> Error Monitor -> (loop or terminate)

- Critical path: LLM -> Interpreter -> OpenFOAM Runner -> Error Monitor -> (loop or terminate). The error-feedback loop is the primary automation mechanism; iteration count directly predicts cost.

- Design tradeoffs:
  - Zero-shot only (this study) vs. RAG-augmented: Zero-shot reduces latency and retrieval cost but increases failure rate on complex/undocumented features.
  - Premium model (o1) vs. cost-optimized (Qwen 2.5-Max): o1 offers higher first-attempt success; Qwen reduces cost but may require more iterations.
  - Local deployment (QwQ-32B) vs. API: Local avoids data egress and per-token cost but fails on domain tasks without fine-tuning.

- Failure signatures:
  - Repeated boundary-condition type errors ("patch type 'patch' not constraint type 'empty'") -> LLM lacks geometry-dimensionality mapping; may require explicit prompt scaffolding.
  - Missing solver keywords ('smoother' not found in fvSolution) -> LLM generates incomplete dictionaries; error loop insufficient for missing-dependency reasoning.
  - High iteration counts (>10) without convergence -> Consider switching models or escalating to expert review.

- First 3 experiments:
  1. **Baseline validation**: Run Cavity flow and PitzDaily cases with Qwen 2.5-Max (API) using zero-shot prompts; log iteration counts and token usage. Compare against Table II benchmarks.
  2. **Boundary condition stress test**: Attempt MotorBike case with explicit patch-type instructions in system prompt; measure whether error-loop converges or stalls.
  3. **Local model probe**: Deploy QwQ-32B via Ollama on single GPU; test identical prompts to identify systematic error patterns for fine-tuning dataset construction.

## Open Questions the Paper Calls Out
None

## Limitations
- Zero-shot prompting artificially constrains performance by disabling RAG, potentially overestimating task difficulty for complex geometries
- Error-feedback mechanism may fail when errors stem from missing conceptual knowledge rather than syntactic issues
- Sub-100B parameter model limitations based on single model test (QwQ-32B) without systematic parameter-scaling experiments
- Token cost comparisons don't account for iteration counts that could erode cost advantages for models requiring multiple correction cycles

## Confidence
- **High Confidence**: Qwen 2.5-Max achieves comparable performance to OpenAI o1 on standard CFD tasks at significantly lower token cost (directly measured and benchmarked)
- **Medium Confidence**: Zero-shot prompting can handle common CFD tasks (Cavity, PitzDaily, AirFoil) but fails on complex geometries requiring expert supervision (limited by artificial constraint of disabling RAG)
- **Low Confidence**: Sub-100B parameter models inherently lack sufficient domain knowledge for CFD without fine-tuning (extrapolates from one model without CFD-specific evidence)

## Next Checks
1. **RAG-Ablation Study**: Re-run the MotorBike and nozzleFlow cases with RAG augmentation enabled. Compare iteration counts and success rates against zero-shot results to quantify the retrieval-augmented performance gap.

2. **Parameter Scaling Experiment**: Test a 70B parameter model (e.g., Llama 3.1 70B) on the same CFD tasks as QwQ-32B. Determine if the 100B threshold claim holds or if performance correlates more strongly with training data composition.

3. **Cost-Iteration Tradeoff Analysis**: For each successful case, calculate total token cost including all iterations in the error-feedback loop. Compare the effective cost-per-successful-simulation across models to validate whether lower per-token pricing translates to lower total workflow cost.