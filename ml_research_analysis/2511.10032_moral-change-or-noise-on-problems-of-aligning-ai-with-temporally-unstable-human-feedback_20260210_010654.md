---
ver: rpa2
title: Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable
  Human Feedback
arxiv_id: '2511.10032'
source_url: https://arxiv.org/abs/2511.10032
tags:
- preferences
- participants
- time
- response
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how human moral preferences evolve over
  time and their impact on AI alignment, particularly in high-stakes domains like
  kidney allocation. Researchers collected longitudinal data from over 400 participants
  across 3-5 sessions, where participants made pairwise comparisons of hypothetical
  kidney transplant patients.
---

# Moral Change or Noise? On Problems of Aligning AI With Temporally Unstable Human Feedback

## Quick Facts
- arXiv ID: 2511.10032
- Source URL: https://arxiv.org/abs/2511.10032
- Reference count: 40
- Primary result: Human moral preferences change significantly over days, creating fundamental challenges for AI alignment when preferences shift by 6-20% for repeated scenarios

## Executive Summary
This study investigates how human moral preferences evolve over time and their impact on AI alignment, particularly in high-stakes domains like kidney allocation. Researchers collected longitudinal data from over 400 participants across 3-5 sessions, where participants made pairwise comparisons of hypothetical kidney transplant patients. The study revealed that participants changed their responses to the same scenario 6-20% of the time (response instability) and exhibited significant shifts in their decision-making models over time (model instability). Analysis showed that preference instability varied by scenario difficulty, with more complex tradeoffs leading to higher instability. Participants were categorized into four groups based on their stability levels, revealing different mechanisms of preference change. The study found that AI models trained on preference data showed significantly higher error rates (5-16% increase) for participants with higher instability, with model performance deteriorating over time for unstable participants. These findings highlight fundamental challenges for AI alignment, raising normative questions about which preferences to align with and technical challenges in developing methods that account for temporal preference changes.

## Method Summary
The researchers collected longitudinal preference data from 404 participants who made pairwise comparisons of hypothetical kidney transplant patients across 3-5 sessions over approximately two weeks. Each session included 60 comparisons with 6 scenarios repeated twice to measure stability. Participants evaluated patients based on 8 features (dependents, life years gained, drinks/day, crimes, obesity, work hours, waiting list years, rejection chance). The study measured response stability (agreement with dominant response on repeated scenarios) and model stability (agreement between session-wise decision models). Participants were categorized into four groups based on median splits of cumulative response and model stability. AI alignment models (BT-NN, SUP-NN, GPT-FT) were trained on preference data and evaluated for error rates across stability categories.

## Key Results
- Participants changed responses to the same scenario 6-20% of the time across sessions
- Four distinct participant categories emerged based on stability patterns: C1 (stable-stable), C2 (unstable-stable), C3 (stable-unstable), C4 (unstable-unstable)
- AI models showed 5-16% higher error rates for unstable participants compared to stable participants
- Model performance deteriorated over time for unstable participants, particularly in category C4

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Participants in categories C1 and C2 gradually simplify their decision models by increasing reliance on their most important feature over time.
- Mechanism: Model entropy decreases across sessions as participants reduce cognitive load, focusing increasingly on a single dominant feature. Session-wise logistic models show declining entropy (r=−0.23, p<0.001 for C1; r=−0.19, p=0.007 for C2).
- Core assumption: This represents either (a) legitimate preference construction through deliberation, or (b) mental fatigue reducing deliberation depth.
- Evidence anchors:
  - [abstract]: "increasing use of the most important feature over time"
  - [section]: "C1 and C2 significantly increase the importance of the most important feature... Figure 4a shows that model-entropy decreases across sessions"
  - [corpus]: Limited direct corpus support; related work on preference construction (Hoeffler & Ariely, 1999) cited but not in corpus neighbors.
- Break condition: If simplification stems from fatigue rather than preference refinement, alignment should target earlier, more deliberated responses.

### Mechanism 2
- Claim: Response instability can occur even when underlying decision models remain stable, particularly for participants using high-entropy (multi-feature) decision processes.
- Mechanism: High model entropy means near-equal weighting across features, making choices sensitive to small perturbations. C3 participants maintain stable models (r=−0.08, p=0.2 for entropy change) but show response instability.
- Core assumption: Response instability reflects stochastic output from a consistent but complex decision process, not preference change.
- Evidence anchors:
  - [abstract]: "minimal change despite response instability"
  - [section]: "Participants in C3 are characterized by response instability and model stability... model entropy for C3 is relatively high and stays high across sessions"
  - [corpus]: "Dropouts in Confidence" paper notes humans display significant moral uncertainty; supports stochastic interpretation.
- Break condition: When high entropy masks genuine preference exploration still in progress.

### Mechanism 3
- Claim: Some participants (C4) exhibit genuine preference drift, with significant shifts in relative feature importances across sessions.
- Mechanism: Both response and model instability co-occur. Feature importance weights shift substantially (model shift correlation r=0.74, p<0.001). These participants also show shorter deliberation times.
- Core assumption: May reflect treating scenarios independently, inconsistent application of moral principles, or legitimate preference evolution.
- Evidence anchors:
  - [abstract]: "significant updates to feature importance weights"
  - [section]: "Figure 4b further shows that these participants exhibit high levels of model shift, significantly changing the importance they assign to different features over time"
  - [corpus]: Limited corpus evidence on feature-weight instability mechanisms specifically.
- Break condition: Cannot distinguish legitimate drift from noise without additional reasoning data beyond choices.

## Foundational Learning

- Concept: Response stability vs. Model stability
  - Why needed here: These are distinct constructs; one measures output consistency, the other measures decision-process consistency. Categorizing by both reveals heterogeneous change mechanisms.
  - Quick check question: Can a participant have high response stability but low model stability? (Answer: Yes, if their model shifts but happens to produce the same predictions on test scenarios.)

- Concept: Preference construction
  - Why needed here: Preferences may not be pre-formed but constructed during elicitation. This explains why early vs. late responses may differ systematically.
  - Quick check question: If preferences are constructed rather than revealed, what implications does this have for one-shot alignment methods?

- Concept: Bradley-Terry preference modeling
  - Why needed here: Used to derive implicit priority scores and compute scenario difficulty as distance from decision boundary.
  - Quick check question: What does a difficulty score of 0 indicate? (Answer: The scenario lies exactly on the decision boundary—no evidence to prefer either option.)

## Architecture Onboarding

- Component map: Longitudinal data collection -> Stability metrics computation -> Participant categorization -> AI model training -> Error rate evaluation
- Critical path: 1. Design repeated scenarios with varying tradeoff levels (U: low, V: medium, W: high) 2. Collect multi-session data with attention checks 3. Fit session-wise logistic models, compute Shapley values for feature importances 4. Calculate response stability and model stability per participant 5. Categorize participants, train preference models, evaluate error disparities
- Design tradeoffs: Session spacing: 1–3 days captures short-term instability; longer windows may reveal larger drifts but increase attrition; Repeated scenario count: More repeats improve stability estimation but reduce scenario diversity; 6 repeats balanced coverage vs. measurement precision; Model choice for stability: Logistic models interpretable but may underfit complex preferences; neural models capture nonlinearity but less interpretable for feature importance
- Failure signatures: C4 participants show 5–16% higher error rates than C1 across all model types; Error rates increase over time for C4 when trained on early sessions (significant positive slope in 3/4 settings); Population-level fine-tuning (GPT-FT) underperforms participant-specific models, especially for stable participants (C1 error higher than C2/C3 for GPT-FT)
- First 3 experiments: 1. Baseline stability measurement: Run the repeated-scenario protocol on your target population to establish response and model stability distributions before any model training. 2. Category-stratified evaluation: Train preference models on your data and compare error rates across the four participant categories to quantify alignment risk per segment. 3. Temporal decay analysis: For each category, measure error rate as a function of time since training to determine if and when model retraining is necessary.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Which temporal instance of human moral preferences should AI systems align to when preferences change over time—the earliest, the latest, a weighted combination, or some other aggregation?
- Basis in paper: [explicit] The authors ask: "When human moral preferences change over time, with which should AI align? The earlier preference, or the later one? Neither? Perhaps both, their average, or something else entirely?"
- Why unresolved: The paper demonstrates that preferences shift significantly but does not take a normative stance on which preference state is "correct" for alignment purposes; the answer depends on whether changes reflect legitimate moral reasoning updates or arbitrary fluctuations.
- What evidence would resolve it: Empirical studies pairing preference change data with independent validation of reasoning quality (e.g., expert moral judgment, consistency with stated principles) to determine which temporal preferences better track normatively desirable outcomes.

### Open Question 2
- Question: How can alignment methods algorithmically distinguish between "legitimate" preference changes (e.g., moral learning, reflection) and "illegitimate" changes (e.g., fatigue, attention lapses, cognitive bias)?
- Basis in paper: [explicit] The paper states that "proper alignment of AI to dynamic human preferences should ideally account for 'legitimate' changes to moral reasoning, while ignoring changes related to attention deficits, cognitive biases, or other arbitrary factors."
- Why unresolved: The study identified multiple mechanisms of preference change but could not definitively classify them as legitimate or arbitrary using choice data alone; self-reports showed discrepancies with revealed preferences.
- What evidence would resolve it: Development of behavioral markers or self-report instruments validated against external criteria for deliberation quality, combined with computational methods that weight feedback based on inferred legitimacy.

### Open Question 3
- Question: What richer data modalities beyond binary choices are needed to infer why participants change their moral preferences over time?
- Basis in paper: [explicit] The authors state: "We need richer data to decipher why participants changed their preferences in the manner we observed. As such, future elicitation methods must go beyond simply learning from observed behavior."
- Why unresolved: Choice-based preference learning cannot distinguish between competing explanations for observed instability (e.g., learning vs. fatigue vs. authentic value change), even with longitudinal data.
- What evidence would resolve it: Studies comparing choice-only elicitation against methods incorporating think-aloud protocols, post-choice justifications, or interactive alignment frameworks to assess whether richer signals improve prediction of preference trajectories.

### Open Question 4
- Question: Do moral preference changes observed over days generalize to longer time scales (months, years), and do the same mechanisms (entropy reduction, feature reweighting) persist?
- Basis in paper: [explicit] The limitations section notes: "We assessed preference change over the span of days. However, one might expect larger preference changes over longer periods, which could be a fruitful target for future data collection efforts."
- Why unresolved: The study was limited to sessions spanning up to two weeks; it remains unknown whether the 6-20% response instability and identified change mechanisms scale or transform over longer periods.
- What evidence would resolve it: Longitudinal studies tracking the same participants over months or years, measuring response stability, model stability, and mechanism prevalence at multiple time intervals.

## Limitations

- The study cannot definitively distinguish between legitimate preference evolution and arbitrary changes (fatigue, attention lapses) without additional reasoning data
- The short 2-week study period may not capture longer-term preference drift that could emerge over months or years
- Logistic regression models may underrepresent complex nonlinear moral preferences, potentially misclassifying participants with complex decision boundaries

## Confidence

- High confidence: The measurement methodology for response stability and model stability is technically sound and reproducible. The error rate disparities between stable and unstable participants are empirically robust.
- Medium confidence: The categorization into four participant types is data-driven but relies on median splits that may be sample-dependent. The interpretation of why participants fall into each category involves substantive assumptions about preference construction and deliberation.
- Low confidence: The normative claim about which preferences to align with (early vs. late responses) remains unresolved. The mechanisms distinguishing legitimate drift from noise cannot be definitively established from choice data alone.

## Next Checks

1. **Replication with reasoning data**: Collect think-aloud protocols or post-hoc explanations alongside choices to distinguish between fatigue-driven simplification and deliberative preference refinement.
2. **Longer-term stability assessment**: Extend the longitudinal study to 8-12 weeks to determine if instability patterns persist or converge over time, and whether longer-term drift differs qualitatively from short-term fluctuations.
3. **Mechanistic validation**: Design targeted experiments where participants receive feedback on their consistency, measuring whether awareness and incentives reduce instability, helping distinguish noise from genuine preference exploration.