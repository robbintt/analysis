---
ver: rpa2
title: Few-Shot and Training-Free Review Generation via Conversational Prompting
arxiv_id: '2509.20805'
source_url: https://arxiv.org/abs/2509.20805
tags:
- review
- reviews
- user
- conversational
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Conversational Prompting for personalized review
  generation under few-shot and training-free constraints. The method reformulates
  user review histories into multi-turn conversations, with a contrastive variant
  inserting other users' reviews as incorrect replies to encourage the model to imitate
  the target user's style.
---

# Few-Shot and Training-Free Review Generation via Conversational Prompting

## Quick Facts
- arXiv ID: 2509.20805
- Source URL: https://arxiv.org/abs/2509.20805
- Reference count: 40
- The paper proposes Conversational Prompting for personalized review generation under few-shot and training-free constraints, with contrastive variants using negative examples.

## Executive Summary
The paper addresses personalized review generation when user history is limited, proposing Conversational Prompting (CP) to structure review histories as multi-turn dialogues. Simple Conversational Prompting (SCP) converts review histories into back-and-forth exchanges, while Contrastive Conversational Prompting (CCP) inserts other users' reviews as rejected responses to sharpen style imitation. Experiments on eight product domains with five LLMs show both variants significantly outperform flat concatenation baselines in text-based metrics and downstream tasks, with CCP providing further gains when high-quality negatives are available.

## Method Summary
The method reformulates sparse user review histories into multi-turn conversational prompts. SCP converts each historical review into a dialogue turn (user asks about item, assistant provides review, user accepts), ending with a request for the target item's review. CCP adds contrastive elements by inserting other users' reviews as rejected responses ("That's not how I would answer") followed by the correct review. This creates a learning signal that encourages the model to imitate the target user's style. The approach requires no training, using only in-context learning, and is tested with temperature=0.1 across five LLMs including gpt-4.1-mini, gpt-4.1, o4-mini, llama3.3-70b, and claude-sonnet-4.

## Key Results
- SCP and CCP significantly outperformed flat concatenation baselines in ROUGE-L and BERTScore across all tested domains and LLMs
- CCP improved further when high-quality negative examples were used, with BERTScore-selected negatives outperforming ROUGE-selected or random negatives
- Conversational prompts mitigated the positive sentiment bias present in baseline approaches
- Both variants maintained strong performance even with only 2-5 historical reviews per user
- SCP remained competitive with CCP when high-quality negatives were unavailable, avoiding the additional generation/retrieval cost

## Why This Works (Mechanism)

### Mechanism 1: Conversation-as-Context Encoding
LLMs process role-structured sequences differently than flat concatenation, allocating more attention to patterns within demonstrated turns. The back-and-forth format creates explicit "demonstration pairs" that reinforce the association between item context and the user's writing patterns.

### Mechanism 2: Contrastive Disambiguation via Rejection Feedback
Inserting other users' reviews as rejected responses creates a contrastive signal similar to hard negative mining. The explicit rejection message followed by the correct review encourages the model to learn what *not* to do, sharpening its representation of the target user's distinguishing features.

### Mechanism 3: Turn-Level Attention Amplification
Each conversational turn provides structured interaction that may receive dedicated attention, while flat concatenation leads to recency bias or dilution. Increasing the number of conversational turns improves output quality more effectively than simply increasing concatenated examples.

## Foundational Learning

- **In-Context Learning (ICL)**: The method relies entirely on prompting without gradient updates; understanding how demonstrations influence output is critical. Quick check: Can you explain why providing examples in a prompt changes model behavior without training?
- **Contrastive Learning (Negative Sampling)**: CCP is motivated by contrastive learning principles; selecting informative negatives is key to performance. Quick check: What makes a "hard negative" more useful than a random one?
- **Multi-Turn Dialogue Formatting for LLMs**: The core intervention is reformatting data as role-structured conversation. Quick check: How does role alternation (user/assistant) affect how an LLM processes context?

## Architecture Onboarding

- **Component map**: Data Preprocessor → Negative Selector (CCP only) → Prompt Assembler → LLM Inference → Evaluator
- **Critical path**: 1. Validate review history → 2. Format as multi-turn (SCP) or add negatives (CCP) → 3. Assemble prompt → 4. Call LLM → 5. Score output. CCP requires an additional negative retrieval/generation step before prompt assembly.
- **Design tradeoffs**: SCP has zero external data cost; CCP requires negatives but improves accuracy with high-quality ones. More turns improve quality but increase token usage. BERTScore-selected negatives outperformed ROUGE-selected; generated negatives matched human but added ~4× cost.
- **Failure signatures**: Baseline-level scores suggest incorrect conversation formatting; CCP underperforming SCP indicates poor negative quality; no improvement with more turns suggests truncation or attention issues; sentiment mismatch suggests prompt not capturing user's historical sentiment range.
- **First 3 experiments**: 1) A/B test SCP vs. Baseline with n=2, 5 reviews; expect SCP > Baseline on BERTScore/ROUGE-L. 2) Vary turn count (ℓ=1, 4, n−1); confirm (4,4) > (4,1) > (1,1) if using CCP. 3) Compare negative sources: CCP(B) vs. CCP(R) vs. CCP(G) vs. random; expect BERTScore-selected ≈ generated > random > low-quality.

## Open Questions the Paper Calls Out

- **Theoretical mechanisms**: The authors state theoretical analysis of why conversational prompting outperforms flat concatenation is beyond the scope and left for future work. They don't provide a mathematical framework for why multi-turn structure better captures user style.

- **Integration into recommender systems**: The paper mentions applications like recommender systems but leaves assessment of how generated reviews impact downstream ranking or rating prediction for future work.

- **Cost reduction strategies**: While CCP offers accuracy gains, it significantly increases inference cost. The authors note exploring strategies to reduce cost while maintaining accuracy remains an area for future work.

- **Self-Refine failure**: Applying Self-Refine to SCP or CCP did not improve accuracy, which the authors note but don't analyze why the iterative conversational structure conflicts with the general refinement objectives.

## Limitations

- Exact prompt phrasing is incomplete in Figure A1 with "omitted" placeholders for instruction text and rejection messaging
- Model identifiers "gpt-4.1-mini" and "gpt-4.1" are not publicly documented or available
- No human evaluation of generated reviews' perceived quality or usefulness
- Domain specificity limited to 8 Amazon product categories without cross-domain validation

## Confidence

- **High confidence**: Conversational prompting improves over flat concatenation; CCP improves with high-quality negatives; sentiment bias mitigation is replicable
- **Medium confidence**: Exact metric values depend on prompt phrasing; relative performance across LLMs is robust but absolute values may vary
- **Low confidence**: Exact model identifiers are ambiguous; human judgment of quality is unassessed; domain generalization is unproven

## Next Checks

1. **Prompt phrasing sensitivity test**: Run SCP with 3-5 minor variations in instruction text and rejection message; verify relative performance (SCP > Baseline, CCP > SCP) holds across variants
2. **Model substitution test**: Replace "gpt-4.1-mini" with gpt-4o-mini; check if conversational prompting still improves ROUGE-L and BERTScore over baseline
3. **Cross-domain pilot**: Apply SCP/CCP to a non-Amazon review corpus (e.g., Yelp restaurant reviews) with ≥6 reviews/user; confirm conversational formatting still outperforms flat concatenation and mitigates sentiment bias