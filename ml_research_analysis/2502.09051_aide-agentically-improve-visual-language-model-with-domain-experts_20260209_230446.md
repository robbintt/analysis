---
ver: rpa2
title: 'AIDE: Agentically Improve Visual Language Model with Domain Experts'
arxiv_id: '2502.09051'
source_url: https://arxiv.org/abs/2502.09051
tags:
- aide
- arxiv
- expert
- data
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving Visual Language Models
  (VLMs) when no larger superior models are available for knowledge distillation.
  The authors introduce AIDE (Agentic Improvement through Domain Experts), a novel
  framework that enables VLMs to autonomously enhance their capabilities by leveraging
  specialized domain expert models.
---

# AIDE: Agentically Improve Visual Language Model with Domain Experts

## Quick Facts
- arXiv ID: 2502.09051
- Source URL: https://arxiv.org/abs/2502.09051
- Authors: Ming-Chang Chiu; Fuxiao Liu; Karan Sapra; Andrew Tao; Yaser Jacoob; Xuezhe Ma; Zhiding Yu; Guilin Liu
- Reference count: 5
- One-line primary result: AIDE improves Eagle-8B performance on multiple V&L benchmarks without requiring larger teacher models

## Executive Summary
This paper introduces AIDE (Agentic Improvement through Domain Experts), a framework enabling VLMs to autonomously enhance their capabilities without relying on larger teacher models for knowledge distillation. The approach leverages specialized domain expert models to improve training data quality through a two-agent system: a Selector that identifies improvement candidates and a Synthesizer that enriches responses. Experiments demonstrate significant performance gains across multiple benchmarks including MMMU, MME, MMBench, MathVista, and ChartQA, validating the effectiveness of domain-expert-guided VLM improvement.

## Method Summary
AIDE operates through a four-stage process: identifying instances for refinement, engaging domain experts for targeted analysis, synthesizing expert outputs with existing data, and integrating enhanced instances into the training pipeline. The framework employs two agents - a Selector that evaluates dataset instances and matches them with appropriate expert tools, and a Synthesizer that generates enriched training examples by combining expert outputs with original data. Expert tools include PaddleOCR for text extraction and Grounded-SAM for object detection/segmentation. The approach uses "small-step prompting" to improve synthesis quality by providing answers upfront and requesting reasoning generation separately. Enhanced instances are integrated back into the training dataset through quality filtering before fine-tuning the VLM.

## Key Results
- Achieved 1.2% improvement on MMMU benchmark
- Improved MMBench by 0.77% and MME by 52 points
- Enhanced MathVista by 1.1% and ChartQA by 1.1%
- Demonstrated effectiveness using Eagle-8B as both Selector and Synthesizer without requiring larger VLMs
- Successfully identified OCR-heavy datasets (synthdog: 80% selected) as primary improvement candidates

## Why This Works (Mechanism)

### Mechanism 1
Domain expert models provide higher-quality supervision signals than general VLMs for specialized tasks, enabling improvement without larger teacher models. Specialized models (e.g., OCR, object detection) outperform general VLMs in narrow domains. By invoking these experts on selected instances and incorporating their outputs into training data, the VLM learns from superior task-specific supervision that it cannot generate itself. Core assumption: Expert model outputs are sufficiently aligned with target task semantics to serve as training signals. Evidence: Grounding DINO achieves 90.56 on RefCOCO-val vs. 52.3-56.3 for general-purpose models—a ~35-40 point gap.

### Mechanism 2
A VLM can identify its own weak spots through selective instance identification, acting as a quality estimator for training data. The Selector agent examines dataset instances and decides which would benefit from expert augmentation. The selection distribution reveals perceived data quality gaps—the VLM disproportionately selects OCR-heavy datasets (synthdog: 80% selected), suggesting self-diagnosis of textual understanding limitations. Core assumption: The Selector's uncertainty or quality judgment correlates with actual improvement potential. Evidence: Figure 3 shows synthdog (OCR dataset) represents 40%+ of selected instances.

### Mechanism 3
Small-step prompting (providing answers upfront, requesting only reasoning generation) stabilizes synthesis output quality. Direct prompts asking VLMs to generate detailed responses with reasoning often fail even when the model knows the answer. Decomposing the task—providing the answer and requesting reasoning separately—produces richer, more coherent training examples. Core assumption: The VLM has sufficient latent knowledge to generate valid reasoning when answer is given. Evidence: Figure 2 demonstrates that middle-column direct prompting fails, while last-column small-step prompting with answer provided succeeds.

## Foundational Learning

- **Knowledge Distillation (Teacher-Student paradigm)**
  - Why needed here: AIDE inverts traditional KD—instead of a larger teacher, domain experts serve as specialized teachers. Understanding KD helps frame why this works and when it might fail.
  - Quick check question: Can you explain why distillation from a larger model creates a bottleneck for SOTA systems?

- **Tool-Augmented Agents**
  - Why needed here: The Selector must understand expert tool capabilities and decide when to invoke them. This requires reasoning about tool affordances.
  - Quick check question: How does an agent decide between invoking a tool versus answering directly?

- **Instruction Tuning Data Quality**
  - Why needed here: AIDE operates on training data quality, not architecture. Understanding how response richness affects downstream performance is essential.
  - Quick check question: What makes one instruction-response pair higher quality than another for VLM training?

## Architecture Onboarding

- **Component map**: Data instance → Selector decision → Expert invocation → Synthesizer enrichment → Filtering → Training mix
- **Critical path**: Data instance → Selector decision → Expert invocation → Synthesizer enrichment → Filtering → Training mix
- **Design tradeoffs**:
  - Selector/Synthesizer model choice: Same model (efficient) vs. different models (potentially better selection or synthesis)
  - Original data retention: Keep originals (prevents forgetting) vs. replace (cleaner data, potential information loss)
  - Selection coverage: Heuristic (more data, lower quality) vs. VLM-guided (less data, higher quality)—Table 2 shows VLM selection outperforms heuristics
- **Failure signatures**:
  - Model collapse: If filtered enhanced data contains systematic errors, repeated training cycles amplify them
  - Selector over-selection: Invoking experts on low-value instances wastes compute without gains
  - Synthesis incoherence: Expert outputs conflict with original context, producing contradictory training signals
- **First 3 experiments**:
  1. **Baseline reproduction**: Train Eagle-8B on unmodified Cambrian-1 subset, benchmark on MMMU/MME/MBBench to establish baseline
  2. **Ablate Selector**: Compare VLM-selector vs. heuristic selector (e.g., answer length ≤5 tokens) to isolate selection quality impact—expect VLM-selector to achieve higher per-instance improvement
  3. **Single-expert validation**: Run AIDE with only PaddleOCR (no Grounded-SAM) to verify each expert contributes independently; check whether OCR-heavy benchmarks (TextVQA, ChartQA) improve proportionally

## Open Questions the Paper Calls Out

### Open Question 1
Can AIDE's selection patterns serve as a reliable proxy for dataset quality estimation via VLM-as-a-judge? The paper observes that synthdog, textvqa, and arxivqa were predominantly selected, but does not validate whether these selection rates correlate with actual data quality metrics or downstream performance. What evidence would resolve it: Correlation analysis between AIDE selection frequencies and independent quality measures (human ratings, cross-model agreement), plus comparison to existing dataset quality estimation methods.

### Open Question 2
What are the computational bottlenecks for applying AIDE to large-scale datasets or real-time training pipelines? The paper states applying AIDE to large-scale datasets or in real-time settings may be constrained by computational costs, but provides no runtime analysis, resource profiling, or scalability experiments beyond the 7M instance Cambrian1 dataset. What evidence would resolve it: Detailed computational cost breakdown (selector inference time, expert tool execution, synthesis overhead) with scaling curves across varying dataset sizes.

### Open Question 3
Does Selector model capability impact AIDE's effectiveness, or can weaker models perform adequate selection? The paper uses Eagle-8B as both Selector and Synthesizer but only ablates against heuristics, not against different VLM selectors of varying capability. What evidence would resolve it: Ablation experiments using different Selector models (e.g., 2B vs 8B vs larger proprietary VLMs) measuring impact on final trained model performance.

### Open Question 4
How does AIDE's effectiveness vary across different expert tool combinations and visual domains? Only two expert tools (PaddleOCR, Grounded-SAM) are evaluated, yet the framework claims extensibility to additional expert models for multimodal tasks. What evidence would resolve it: Systematic evaluation adding expert tools incrementally, measuring per-benchmark gains and identifying which expert types benefit which visual reasoning capabilities.

## Limitations

- The framework's effectiveness depends critically on the quality and coverage of domain expert tools, but does not analyze what happens when expert tools are imperfect or unavailable for certain domains
- The selection mechanism appears novel but lacks validation of whether the Selector's judgments correlate with actual improvement potential versus arbitrary choices
- The synthesis process assumes the VLM can meaningfully integrate expert outputs, but no analysis examines cases where expert and original data conflict or when the VLM generates confabulated reasoning

## Confidence

- **High confidence**: The core claim that domain experts can provide superior supervision signals for specialized tasks is well-supported by the grounding accuracy gap (90.56 vs 52.3-56.3 on RefCOCO-val)
- **Medium confidence**: The effectiveness of VLM-guided selection versus heuristics is demonstrated through comparative results (TextVQA: 76.85% vs 41.49%), though the selection criteria remain underspecified
- **Low confidence**: The synthesis quality and integration process claims rely heavily on small-step prompting success shown in Figure 2, but lack systematic evaluation of when this approach fails or how it scales to diverse domains

## Next Checks

1. Conduct ablation studies on expert tool availability—measure performance degradation when removing PaddleOCR or Grounded-SAM individually to quantify each expert's contribution
2. Test Selector reliability by manually auditing a sample of selected vs. unselected instances to verify the correlation between selection and actual improvement potential
3. Evaluate synthesis robustness by creating adversarial test cases where expert outputs contradict original data, measuring whether the Synthesizer produces coherent or confabulated responses