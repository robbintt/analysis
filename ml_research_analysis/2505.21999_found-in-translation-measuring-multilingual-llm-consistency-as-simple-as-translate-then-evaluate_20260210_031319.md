---
ver: rpa2
title: 'Found in Translation: Measuring Multilingual LLM Consistency as Simple as
  Translate then Evaluate'
arxiv_id: '2505.21999'
source_url: https://arxiv.org/abs/2505.21999
tags:
- consistency
- language
- languages
- empathy
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a framework to evaluate multilingual large
  language models (LLMs) using a "translate then evaluate" approach, bypassing the
  need for expensive multilingual annotations. By translating non-English LLM responses
  into English and applying English-centric evaluation metrics, the authors assess
  cross-lingual consistency along two dimensions: information and empathy.'
---

# Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate

## Quick Facts
- **arXiv ID:** 2505.21999
- **Source URL:** https://arxiv.org/abs/2505.21999
- **Reference count:** 40
- **Primary result:** Proposed "Translate then Evaluate" framework reveals significant multilingual consistency gaps in popular LLMs, with scores ranging from 0.11 to 0.68 for information and 0.49 to 0.91 for empathy across 30 languages.

## Executive Summary
This paper introduces a novel framework to evaluate multilingual LLM consistency using a "translate then evaluate" approach that bypasses the need for expensive multilingual annotations. By translating non-English LLM responses into English and applying English-centric evaluation metrics, the authors assess cross-lingual consistency along two dimensions: information (factual precision and recall) and empathy (emotional response patterns). The framework reveals that popular LLMs are less consistent for non-Latin scripts and low-resource language families, with proprietary models generally outperforming open-weight ones. Results show information consistency scores ranging from 0.11 to 0.68 and empathy consistency scores from 0.49 to 0.91 across 30 languages.

## Method Summary
The authors propose evaluating cross-lingual consistency by generating responses in both English and target languages, translating non-English responses back to English, and comparing them using established English evaluators. For information consistency, they use FActScore to measure factual precision and recall between English and translated responses. For empathy consistency, they compare binary empathy profiles (emotional reactions, interpretations, explorations) extracted by three classifiers. The framework uses NLLB 54B MoE for translation, Qwen-2-14B for information evaluation, and ModernBERT-large classifiers for empathy evaluation, enabling comprehensive consistency assessment across 30 languages without requiring language-specific metrics.

## Key Results
- Information consistency scores range from 0.11 to 0.68 across 30 languages, with significant gaps for non-Latin scripts and low-resource language families
- Empathy consistency scores range from 0.49 to 0.91, showing better overall consistency than information
- Proprietary models generally outperform open-weight models on both consistency dimensions
- Languages with non-Latin scripts show significantly lower consistency than Latin-script languages

## Why This Works (Mechanism)

### Mechanism 1: Translation as Universal Evaluator Bridge
Translating non-English responses to English enables reuse of well-validated English evaluators for multilingual assessment. A high-fidelity translation model (NLLB 54B MoE) maps responses from target language ℓ to English, allowing direct comparison between r_en(x) and translated r̂_ℓ(x) using English-only evaluators without requiring language-specific metrics. Core assumption: The translation preserves the semantic and pragmatic properties being measured. Evidence: Human evaluation of translation fidelity shows average rating of 4.26/5 across 10 languages for information preservation.

### Mechanism 2: Asymmetric Fact Extraction for Consistency
Information consistency is measured as the F-score of factual precision and recall between English and non-English responses. FActScore extracts atomic claims from responses, verifies support against reference (English response), computing both precision (claims in non-English supported by English) and recall (claims in English captured by non-English), then takes harmonic mean. Core assumption: FActScore's claim extraction and verification work reliably on translated text. Evidence: Self-consistency check via back-translation yields 0.86-0.94 scores, indicating evaluator robustness to translation artifacts.

### Mechanism 3: Binary Profile Matching for Empathy
Empathy consistency is measured by comparing binary empathy profiles (emotional reactions, interpretations, explorations) across language pairs. Three binary classifiers (f_ER, f_IP, f_EX) independently score each response for presence of empathy mechanisms; consistency is 1 if profiles match exactly, 0 otherwise, averaged across prompts. Core assumption: Translation preserves empathy signaling. Evidence: Classifier F1 scores of 86.1-92.0% for detecting empathy mechanisms, though translation fidelity for empathy remains unvalidated.

## Foundational Learning

- **Cross-lingual consistency vs. cross-lingual transfer**: The paper measures whether an LLM gives semantically equivalent responses to the same prompt in different languages—not whether it can transfer knowledge across languages. These are distinct evaluation goals. Quick check: If a model answers correctly in English but incorrectly in Hindi, is that a consistency failure or a transfer failure?

- **Translationese and its evaluation artifacts**: Machine-translated text has systematic differences from native text. The framework assumes evaluators calibrated on native English remain valid on translated English. Quick check: Why did the authors use back-translation self-consistency (Table 1) rather than direct human judgment of translated response quality?

- **FActScore's asymmetric precision/recall formulation**: The paper repurposes FActScore (originally for factual precision against a knowledge source) for consistency by computing both directions. A response that says nothing can have high precision but zero recall. Quick check: If a Hindi response contains only one claim that's supported by the English response, but the English response has 10 claims, what happens to precision vs. recall?

## Architecture Onboarding

- **Component map**: Prompt Generator (150 info + 100 empathy prompts) -> LLM Under Test (generates r_en, r_ℓ) -> Translation Module (NLLB 54B MoE) -> Information Evaluator (Qwen-2 14B FActScore) -> Empathy Evaluator (3 ModernBERT-large classifiers) -> Aggregation (per-language consistency scores)

- **Critical path**: 1) Generate English responses r_en(x) for all prompts 2) Generate non-English responses r_ℓ(x) for each language ℓ 3) Translate r_ℓ(x) → r̂_ℓ(x) using NLLB sentence-by-sentence 4) Apply evaluator E to (r_en, r̂_ℓ) pairs 5) Average across prompts for per-language consistency

- **Design tradeoffs**: Translation model choice (NLLB 54B MoE vs. Google Translate); Evaluator choice (Open-weight Qwen-2 14B vs. GPT-4); Reference language (English may favor Latin-script languages)

- **Failure signatures**: Low consistency for Indic/Dravidian languages (aya-exp-8b scores 0.11 for Indic vs. 0.59 for Latin); RECITATION errors from Gemini API; Translation degeneration (NLLB can produce repeated phrases)

- **First 3 experiments**: 1) Replicate information consistency on 20 prompts, 5 languages using Qwen-2 FActScore 2) Ablate translation model by swapping NLLB for Google Translate 3) Pilot empathy consistency by training classifiers on EPITOME data for 10 therapeutic prompts

## Open Questions the Paper Calls Out

1. **Mitigation strategies for low-resource language consistency gaps**: The authors leave investigations on disparity mitigation strategies for low-resource language families to future work.

2. **Scaling trends and model size effects**: While scaling mostly helps, minor degradations occurred in some models, prompting the authors to intend more rigorous evaluation of scaling trends across model families.

3. **Translation fidelity for empathy evaluation**: Validating translation fidelity for empathy is difficult and remains to future work, as empathy signals may be more fragile to translation artifacts than factual content.

## Limitations

- Translation fidelity for empathy evaluation remains unvalidated by human judgment
- Language coverage limited to 204 languages supported by NLLB, not representative of all world languages
- Reference language bias may systematically advantage Latin-script languages

## Confidence

- **Information consistency methodology**: High confidence - well-specified and validated through self-consistency checks
- **Empathy consistency methodology**: Medium confidence - sound approach but translation fidelity unverified
- **Cross-lingual performance gaps**: Medium confidence - clear patterns but reference language bias may inflate gap estimates

## Next Checks

1. **Human validation of empathy translation fidelity**: Recruit bilingual annotators to rate empathy profile preservation in translated therapeutic responses (Hindi, Arabic, Japanese) and compare to current classifier scores.

2. **Reference language ablation study**: Replicate information consistency evaluation using Spanish instead of English to quantify reference language bias.

3. **Classifier sensitivity analysis**: Systematically vary empathy classifier decision thresholds to assess robustness of consistency scores to classifier uncertainty.