---
ver: rpa2
title: Collaborative Edge-to-Server Inference for Vision-Language Models
arxiv_id: '2512.16349'
source_url: https://arxiv.org/abs/2512.16349
tags:
- inference
- image
- accuracy
- communication
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a collaborative edge-to-server inference framework
  for vision-language models (VLMs) that reduces communication cost while maintaining
  inference accuracy. The framework introduces a two-stage protocol where the server
  performs initial inference on a low-resolution global image and uses min-entropy
  of output tokens to decide whether retransmission is needed.
---

# Collaborative Edge-to-Server Inference for Vision-Language Models

## Quick Facts
- arXiv ID: 2512.16349
- Source URL: https://arxiv.org/abs/2512.16349
- Reference count: 40
- Primary result: Achieves 30-50% communication savings while maintaining VQA accuracy through adaptive edge-to-server retransmission

## Executive Summary
This paper introduces a collaborative edge-to-server inference framework for vision-language models that selectively retransmits high-resolution image regions based on uncertainty estimation. The framework operates in two stages: first performing inference on a low-resolution global image, then identifying and requesting high-resolution local regions when uncertainty exceeds a threshold. The key innovation is using min-entropy of output tokens as a confidence measure and attention-based region-of-interest identification for selective retransmission. Experimental results across multiple VQA benchmarks and VLM architectures demonstrate significant communication savings while maintaining or improving accuracy compared to baselines.

## Method Summary
The framework operates through a two-stage inference protocol where the edge device transmits a resized low-resolution image to the server. The server performs initial inference using its VLM and computes average min-entropy across generated output tokens to assess confidence. If the min-entropy exceeds a predefined threshold, the server extracts attention weights from the LLM decoder, computes relative attention by normalizing against a generic prompt, and identifies a region of interest using a sliding-window search. The server requests this RoI from the edge device, which crops and transmits the high-resolution region. The server then performs refined inference by concatenating global and local visual tokens, leveraging both coarse context and fine details for improved accuracy.

## Key Results
- Achieves 30-50% reduction in additional communication cost compared to always transmitting high-resolution images
- Maintains or improves VQA accuracy across TextVQA, POPE, A-OKVQA, GQA, and VQAv2 benchmarks
- Min-entropy metric achieves best separability between correct and incorrect predictions (0.47 distribution overlap, 0.33 Bhattacharyya distance)
- Outperforms random retransmission and high-resolution baseline models in accuracy-communication tradeoff

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Min-entropy of output tokens serves as a discriminative proxy for inference reliability, enabling selective retransmission.
- **Mechanism:** The LLM decoder produces softmax distributions at each generation step. Min-entropy captures the negative log of the maximum probability: `g_m = -log2(max(p(y_t)))`. High min-entropy indicates flat (uncertain) distributions, triggering retransmission. Compared to Shannon entropy and probability margin, min-entropy yields lowest distribution overlap and highest Bhattacharyya distance between correct and incorrect predictions.
- **Core assumption:** Token-level uncertainty correlates with answer correctness; softmax reflects genuine epistemic uncertainty rather than miscalibration.
- **Evidence anchors:** Abstract states min-entropy serves as confidence measure; Section V-A defines metrics and shows min-entropy best separability; Table II provides quantitative comparison.

### Mechanism 2
- **Claim:** Relative attention maps derived from LLM decoder attention weights localize task-relevant image regions for selective retransmission.
- **Mechanism:** Attention weights from output tokens to visual tokens are extracted at a chosen layer, averaged across heads, and normalized by attention from a generic prompt to suppress task-irrelevant salience. A sliding-window search identifies the bounding box with highest internal-external attention contrast.
- **Core assumption:** Even when the VLM answers incorrectly, its attention implicitly focuses on semantically meaningful regions; generic-prompt normalization effectively suppresses background bias.
- **Evidence anchors:** Abstract mentions RoI identification using VLM's internal attention; Section VI-B defines relative attention and sliding-window procedure.

### Mechanism 3
- **Claim:** Concatenating global and local visual tokens for second-stage inference preserves coarse context while incorporating fine-grained details.
- **Mechanism:** The server encodes both low-resolution global image and high-resolution local crop, yielding 2nv visual tokens. The LLM decoder attends jointly to both token sets during refined inference. Since visual tokens dominate sequence length (>95%), FLOPs scale approximately linearly with nv, so selective doubling reduces expected computation.
- **Core assumption:** Global tokens provide sufficient context to ground local details; joint attention successfully integrates multi-resolution information without conflicting signals.
- **Evidence anchors:** Abstract states server refines inference by jointly leveraging global and local images; Section IV, Eq. 12 defines second-stage inference with concatenated features.

## Foundational Learning

- **Concept: Vision-Language Model (VLM) Architecture**
  - **Why needed here:** Framework manipulates internal components (vision encoder outputs, LLM decoder attention, token sequences); understanding data flow from image to visual tokens to autoregressive generation is prerequisite.
  - **Quick check question:** Given an input image-text pair, can you trace the data path through ViT patch embedding, projection to LLM hidden dimension, concatenation with text tokens, and autoregressive decoding?

- **Concept: Multi-Head Self-Attention and Attention Extraction**
  - **Why needed here:** RoI identification depends on extracting and interpreting attention weights from specific decoder layers; understanding masked vs. unmasked attention distinguishes ViT and LLM decoder behaviors.
  - **Quick check question:** For a decoder generating token y_t, which keys does its query attend to, and how would you extract the attention weights targeting only visual tokens?

- **Concept: Uncertainty Quantification for Neural Networks**
  - **Why needed here:** Decision to retransmit hinges on interpreting softmax outputs as uncertainty; distinguishing entropy-based metrics (Shannon vs. min-entropy vs. margin) affects threshold calibration.
  - **Quick check question:** For a three-class softmax output [0.7, 0.2, 0.1], compute Shannon entropy, min-entropy, and probability margin. Which metric would most strongly trigger retransmission if threshold were 0.5?

## Architecture Onboarding

- **Component map:** Edge device -> Communication channel -> Server -> Communication channel -> Edge device
- **Critical path:** 1) Edge resizes image to encoder resolution, transmits with question. 2) Server encodes visual tokens, runs autoregressive decoding, accumulates min-entropy. 3) If min-entropy ≥ threshold: extract attention, compute relative attention map, identify RoI, request local crop. 4) Edge crops original image at bounding box, resizes, transmits local image. 5) Server encodes local image, concatenates visual tokens, runs refined inference, returns answer.
- **Design tradeoffs:** Threshold η controls retransmission rate vs. accuracy; aggregation length balances latency vs. separability; bounding box size trades transmission cost vs. context preservation.
- **Failure signatures:** Retransmission never triggers (threshold too high/overconfident model); always triggers (threshold too low); second-stage accuracy degrades (irrelevant RoI, conflicting signals).
- **First 3 experiments:** 1) Baseline calibration: plot min-entropy distributions for correct vs. incorrect samples, compute overlap and Bhattacharyya distance. 2) Threshold sweep: accuracy vs. communication cost curves to find Pareto-optimal point. 3) Aggregation length ablation: compare full-sequence vs. early-token averaging at fixed communication budget.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be extended to multi-access environments where multiple edge devices share server resources?
- Basis in paper: Conclusion states "Future work will explore extending the framework to more complex communication settings, including robust transmission and multi-access environments."
- Why unresolved: Current framework assumes single edge device communicating with one server, leaving resource contention and bandwidth allocation in multi-client scenarios unaddressed.

### Open Question 2
- Question: What is the optimal strategy for selecting the transformer layer from which to extract attention weights for RoI identification?
- Basis in paper: Paper states "a specific layer l ∈ {1, . . . , L} is empirically chosen to maximize the contrast between high- and low-attention regions" but provides no systematic analysis.
- Why unresolved: Different layers capture different abstraction levels; heuristic selection may not generalize across VLM architectures and tasks.

### Open Question 3
- Question: Can the entropy threshold η be adapted dynamically based on task characteristics, channel conditions, or historical accuracy feedback?
- Basis in paper: Threshold η is described as "predefined" throughout the framework, suggesting static design choice without exploration of adaptive mechanisms.
- Why unresolved: Fixed threshold cannot account for varying task difficulty, network congestion, or changing accuracy requirements across deployment scenarios.

## Limitations

- The framework's effectiveness depends on the assumption that softmax distributions accurately reflect epistemic uncertainty rather than systematic miscalibration, which lacks direct experimental validation.
- Attention-based RoI identification requires careful layer selection and may not generalize across different VLM architectures without systematic analysis.
- The fixed entropy threshold cannot adapt to varying task difficulty, network conditions, or changing accuracy requirements across deployment scenarios.

## Confidence

- **High confidence:** The overall communication-accuracy tradeoff framework is well-established and empirical results demonstrate significant savings (30-50% reduction) while maintaining accuracy. Min-entropy metric shows superior separability with clear statistical evidence.
- **Medium confidence:** Attention-based RoI identification is theoretically sound but lacks extensive validation that generic-prompt normalization consistently suppresses background bias across diverse image types and question styles.
- **Medium confidence:** Second-stage inference with concatenated tokens is a reasonable architectural choice but the paper does not explore alternative integration strategies that might handle global-local conflicts more effectively.

## Next Checks

1. **Calibration analysis:** Measure expected calibration error (ECE) for the VLM across correct and incorrect predictions on TextVQA validation set. Compare min-entropy, Shannon entropy, and probability margin calibration curves to determine whether the model's uncertainty estimates are reliable or systematically overconfident.

2. **Attention visualization study:** Generate and visualize relative attention maps for a diverse sample of correctly and incorrectly answered questions across multiple datasets. Manually verify that the identified RoI regions correspond to semantically meaningful objects or regions relevant to the question. Additionally, test the generic-prompt normalization by comparing task-attention maps with and without normalization on images containing prominent background elements.

3. **Threshold transferability experiment:** Train the framework on TextVQA with optimal threshold η determined via validation set. Test whether this same threshold transfers to POPE and A-OKVQA without retraining. Measure accuracy and communication cost degradation to quantify how dataset-specific the threshold tuning needs to be.