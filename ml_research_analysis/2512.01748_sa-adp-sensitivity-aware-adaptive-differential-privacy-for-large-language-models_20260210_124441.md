---
ver: rpa2
title: 'SA-ADP: Sensitivity-Aware Adaptive Differential Privacy for Large Language
  Models'
arxiv_id: '2512.01748'
source_url: https://arxiv.org/abs/2512.01748
tags:
- privacy
- sa-adp
- noise
- sensitivity
- differential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses privacy risks in large language models (LLMs)
  caused by memorization of personally identifiable information (PII) in training
  data. It proposes SA-ADP, a sensitivity-aware adaptive differential privacy mechanism
  that allocates noise based on the sensitivity of individual PII tokens rather than
  applying uniform noise.
---

# SA-ADP: Sensitivity-Aware Adaptive Differential Privacy for Large Language Models

## Quick Facts
- arXiv ID: 2512.01748
- Source URL: https://arxiv.org/abs/2512.01748
- Reference count: 23
- SA-ADP reduces privacy budget by ~75% while maintaining utility comparable to baseline

## Executive Summary
SA-ADP addresses privacy risks in large language models caused by memorization of personally identifiable information (PII) in training data. It proposes a sensitivity-aware adaptive differential privacy mechanism that allocates noise based on the sensitivity of individual PII tokens rather than applying uniform noise. The approach uses a three-stage pipeline: PII detection via a LangChain-based agent, sensitivity scoring based on frequency, linkability, and datatype, and token-wise adaptive noise injection. Evaluated on four datasets, SA-ADP maintained utility comparable to baseline while reducing the privacy budget by approximately 75%.

## Method Summary
SA-ADP implements a three-stage pipeline for privacy-preserving LLM training. First, a LangChain-based agent detects PII tokens in input text using LLaMA 3. Second, each detected PII token receives a sensitivity score computed as a weighted combination of frequency (rarity in dataset), linkability (ability to connect with other PII), and datatype classification (legal protection status). Third, during training, token-wise adaptive noise is injected based on these sensitivity scores: tokens scoring 0.01-0.50 receive low noise (σ=2.0), tokens scoring 0.51-1.00 receive high noise (σ=3.0), and non-PII tokens receive no noise. The system uses RDP accounting with α=32 and δ=10^-5 to track cumulative privacy loss across training steps.

## Key Results
- SA-ADP reduced privacy budget by ~75% compared to uniform DP-SGD while maintaining comparable utility
- On ABCD dataset: SA-ADP achieved ε=0.19 vs DP-SGD σ=2 (ε=0.85) and σ=3 (ε=1.72), with accuracy of 97.45% vs 97.33% (σ=2) and 96.65% (σ=3)
- Wikitext-2 showed similar utility preservation with SA-ADP ε=0.77 vs DP-SGD σ=3 (ε=2.05) and accuracy 39.13 vs 36.31

## Why This Works (Mechanism)

### Mechanism 1: Token-Level Sensitivity Scoring
The system assigns differentiated sensitivity scores to PII tokens using a weighted linear combination of frequency, linkability, and datatype. This enables more precise privacy protection than uniform treatment. The assumption is that these three dimensions are sufficient proxies for actual privacy risk, though the weighting scheme is not empirically validated against alternatives.

### Mechanism 2: Adaptive Noise Calibration via Sensitivity Mapping
Sensitivity scores are mapped to differentiated noise levels through a deterministic function: tokens scoring 0.01-0.50 receive σ_low=2.0, tokens scoring 0.51-1.00 receive σ_high=3.0, and non-PII tokens receive no noise. This preserves utility on low-sensitivity tokens while strengthening privacy on high-sensitivity tokens. The binary threshold and specific σ values are not justified through ablation studies.

### Mechanism 3: Privacy Budget Tracking via Rényi Differential Privacy
RDP accounting enables tight cumulative privacy budget tracking across training steps while supporting per-token noise variance. The system uses Rényi order α=32 and δ=10^-5, providing tighter bounds than naïve composition. The assumption is that RDP composition accurately captures privacy loss under non-uniform noise, though this extension is not formally proven.

## Foundational Learning

- Concept: **Differential Privacy (ε, δ) semantics**
  - Why needed here: SA-ADP's contribution is framed as reducing ε while maintaining utility; understanding what ε represents is essential to interpret results.
  - Quick check question: If ε = 0.19 vs. ε = 1.72, which provides stronger privacy, and by what approximate factor?

- Concept: **Gradient clipping in DP-SGD**
  - Why needed here: SA-ADP modifies the noise injection step but inherits clipping from DP-SGD; noise scale σ is proportional to clipping norm C.
  - Quick check question: Why does DP-SGD clip gradients before adding noise, and what happens to privacy if clipping is omitted?

- Concept: **PII taxonomy and regulatory frameworks (GDPR/HIPAA)**
  - Why needed here: The datatype classification dimension relies on legal recognition of PII types; implementers must know which categories qualify.
  - Quick check question: Under GDPR, is an IP address considered PII? How would this affect S_datatype scoring?

## Architecture Onboarding

- Component map:
  1. PII Detection Agent (LangChain + LLaMA 3 via Ollama) → extracts PII fields from input records
  2. Sensitivity Scoring Engine → computes S_freq, queries LLM for S_link and S_datatype, aggregates via weighted sum
  3. Noise Calibration Module → maps S_final to σ (2.0 or 3.0)
  4. Gradient Perturbation Layer → clips gradients, injects token-wise Gaussian noise during backprop
  5. RDP Accountant → tracks cumulative ε across training steps

- Critical path: Input text → PII Detection → Sensitivity Scoring → (Training) Forward pass → Backward pass → Gradient clipping → Token-wise noise injection → RDP accounting update. If detection fails, scoring receives no input; if scoring fails, noise mapping defaults to uniform.

- Design tradeoffs:
  - Binary threshold (0.50) vs. continuous mapping: Paper uses binary; continuous could enable finer granularity but increases complexity.
  - LLM-based linkability/datatype scoring vs. rule-based: LLM provides flexibility but introduces latency and potential inconsistency.
  - σ_low = 2.0, σ_high = 3.0: These values worked for GPT-2 on tested datasets; different models/tasks may require retuning.

- Failure signatures:
  - High perplexity on PII-dense datasets (e.g., Wikitext-2 showed degradation under DP-SGD σ=3): indicates over-noising.
  - ε not decreasing across epochs: suggests noise injection is not being applied or RDP accounting is misconfigured.
  - PII still extractable via membership inference: suggests sensitivity scoring or noise levels are insufficient for high-risk tokens.

- First 3 experiments:
  1. Ablation on scoring dimensions: Disable one dimension at a time and measure ε and perplexity on ABCD dataset to validate contribution of each dimension.
  2. Threshold sensitivity analysis: Vary the S_final threshold (0.40, 0.50, 0.60) and plot ε vs. perplexity to identify optimal cutoff.
  3. Cross-model validation: Apply SA-ADP to a different base model (e.g., LLaMA 2 7B) on UNSW-NB15 to assess generalization beyond GPT-2.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but the following critical questions arise from the methodology:

### Open Question 1
How does the error rate of the upstream PII detection agent impact the formal privacy guarantees and empirical leakage of the SA-ADP framework? The paper assumes the detection pipeline successfully identifies PII but does not analyze the degradation of privacy guarantees when detection is imperfect.

### Open Question 2
Is the fixed weighting scheme for sensitivity scoring (Frequency: 0.4, Linkability: 0.3, Datatype: 0.3) optimal across different domains, or does it require manual tuning for specific datasets? The generalizability of these specific weights to other domains is untested.

### Open Question 3
Can the SA-ADP pipeline scale to the pre-training phase of Large Language Models without introducing prohibitive computational latency? The experiments are limited to fine-tuning GPT-2, and the multi-step pipeline may be infeasible for terabyte-scale pre-training corpora.

## Limitations
- The sensitivity scoring dimensions (frequency, linkability, datatype) are assumed sufficient but not empirically validated against alternative formulations
- The binary threshold (0.50) and specific noise values (σ=2.0, σ=3.0) are not justified through ablation studies
- RDP accounting with token-level adaptive noise is not formally proven to maintain privacy guarantees

## Confidence
- **High**: Core methodology of token-wise adaptive noise injection is clearly specified
- **Medium**: Effectiveness of sensitivity scoring dimensions and weights across domains
- **Low**: Scalability to pre-training phase and formal privacy guarantees under non-uniform noise

## Next Checks
1. Verify per-token gradient alignment by logging tensor shapes at noise injection point
2. Validate RDP accounting accuracy by comparing computed ε values against expected ranges from Table I
3. Test failure modes by measuring PII extractability via membership inference attacks when detection has controlled false-negative rates