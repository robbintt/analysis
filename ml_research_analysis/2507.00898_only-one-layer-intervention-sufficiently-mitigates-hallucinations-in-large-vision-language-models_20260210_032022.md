---
ver: rpa2
title: 'ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large
  Vision-Language Models'
arxiv_id: '2507.00898'
source_url: https://arxiv.org/abs/2507.00898
tags:
- visual
- textual
- decoding
- llav
- only
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of hallucination in Large Vision-Language
  Models (LVLMs), where generated responses do not accurately reflect the image input.
  The proposed method, ONLY, introduces a training-free approach that requires only
  a single query and a one-layer intervention during decoding.
---

# ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models

## Quick Facts
- arXiv ID: 2507.00898
- Source URL: https://arxiv.org/abs/2507.00898
- Authors: Zifu Wan; Ce Zhang; Silong Yong; Martin Q. Ma; Simon Stepputtis; Louis-Philippe Morency; Deva Ramanan; Katia Sycara; Yaqi Xie
- Reference count: 40
- Key outcome: Training-free approach requiring only single query and one-layer intervention improves hallucination mitigation with 1.07× inference time vs 2×+ for contrastive methods

## Executive Summary
This paper addresses hallucination in Large Vision-Language Models (LVLMs) through a training-free approach called ONLY that requires only a single query and one-layer intervention during decoding. The method enhances textual outputs by selectively amplifying crucial textual information using a text-to-visual entropy ratio for each token. Evaluated across six benchmarks and three LVLM backbones, ONLY consistently improves over state-of-the-art methods while offering significant efficiency gains.

## Method Summary
ONLY introduces a training-free hallucination mitigation approach that operates during decoding with a single-layer intervention. The method computes Text-to-Visual Entropy Ratios (TVER) at a selected layer to identify attention heads that encode language bias, then applies Textual-Enhanced Multi-Head Attention (TE-MHA) using only those heads. The enhanced output is added to the final hidden state via residual connection, and adaptive decoding switches between collaborative and contrastive strategies based on distributional distance between original and enhanced logits.

## Key Results
- 3.14% higher accuracy on POPE compared to existing approaches
- 1.6% higher accuracy on CHAIR benchmark
- 1.07× inference time versus 2× or more for contrastive decoding methods
- Minimal GPU memory overhead while maintaining consistent improvements across three LVLM backbones

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selectively activating attention heads with high text-to-visual entropy ratios isolates language bias without requiring a second forward pass.
- **Mechanism:** At a chosen layer, compute TVER = Entropy(textual_attention) / Entropy(visual_attention) for each head. Mask heads below the layer's average TVER to zero, amplifying heads with relatively higher textual uncertainty—these correlate with language-driven predictions. The resulting TE-MHA output replaces standard MHA for that layer.
- **Core assumption:** Heads with higher textual entropy relative to visual entropy encode language priors that, when contrasted with original outputs, reveal hallucinatory tendencies.
- **Evidence anchors:** [abstract] "selectively amplifying crucial textual information using a text-to-visual entropy ratio for each token"; [Section 4.4/Figure 4] "entropy ratio is strongly correlated to the hallucination level at both the response and token levels"; [corpus] V-ITI (arXiv:2506.17587) corroborates that attention intervention at inference time can reduce hallucinations via visual grounding.
- **Break condition:** If TVER distribution is uniform across heads (no clear textual vs. visual specialization), head selection degrades to random masking.

### Mechanism 2
- **Claim:** A single-layer textual-enhanced attention output, added via residual connection to the final hidden state, produces logits that can be contrasted with original logits to suppress hallucinations.
- **Mechanism:** Compute H̃ at layer ℓ via TE-MHA, then apply two residual connections: (1) add H̃ to the final layer output H^L, (2) pass through final MLP. This yields textually-enhanced logits f̃. The intervention requires only one extra attention computation.
- **Core assumption:** Hallucinations manifest as divergence between original and textually-enhanced predictions; this divergence is exploitable for correction.
- **Evidence anchors:** [abstract] "requires only a single query and a one-layer intervention during decoding"; [Section 3.2/Algorithm 1] Full procedure showing single-layer intervention and residual connections; [corpus] CoFi-Dec (arXiv:2512.23453) similarly uses generative feedback but requires multi-scale decoding; ONLY's single-layer design is comparatively lightweight.
- **Break condition:** If the selected layer lacks sufficient representational richness (e.g., too early), the textually-enhanced logits may be uninformative.

### Mechanism 3
- **Claim:** Adaptive switching between collaborative and contrastive decoding, based on distributional distance, yields better hallucination suppression than fixed strategies.
- **Mechanism:** Compute Manhattan distance d_t between original and textually-enhanced probability distributions. If d_t < γ, add weighted enhanced logits (collaborative); otherwise, subtract weighted enhanced logits (contrastive). This adaptively amplifies agreement and suppresses disagreement.
- **Core assumption:** Small distributional distance indicates reliable textual enhancement; large distance signals potential hallucination requiring contrastive correction.
- **Evidence anchors:** [Section 3.3/Equation 20] Explicit formulation of adaptive decoding with threshold γ; [Table 6] Shows collaborative/contrastive strategy outperforms alternatives like zeroing visual attention or doubling textual attention; [corpus] Limited direct corpus validation; ECD (arXiv:2504.12137) uses probabilistic hallucination detection but with different contrastive formulation.
- **Break condition:** If γ is poorly tuned, the method may incorrectly choose collaborative decoding when hallucinations are present, or over-suppress correct predictions.

## Foundational Learning

- **Concept: Multi-head attention and attention head specialization**
  - Why needed here: TVER-based head selection assumes heads differentially attend to textual vs. visual tokens; understanding attention mechanisms is prerequisite.
  - Quick check question: Given attention matrices Q, K, V, how would you isolate the attention weights corresponding only to visual token positions?

- **Concept: Entropy as uncertainty quantification**
  - Why needed here: TVER uses entropy ratios to identify heads with higher textual uncertainty; interpreting entropy is essential for understanding the selection criterion.
  - Quick check question: If a head's textual attention entropy is 3.5 and visual entropy is 1.2, what is its TVER, and would it likely be selected if the layer average is 2.0?

- **Concept: Contrastive decoding**
  - Why needed here: ONLY extends contrastive decoding (e.g., VCD, M3ID) but avoids double queries by synthesizing a contrastive signal internally.
  - Quick check question: In VCD, logits from original and distorted inputs are contrasted. What is the key computational advantage of ONLY's approach?

## Architecture Onboarding

- **Component map:** Vision encoder output → visual tokens; text query → textual tokens (via LLM tokenizer) → Multi-layer Transformer (L layers) → H^L → logits via projection head φ → TVER computation at layer ℓ̃ → TE-MHA → H̃ → residual addition to H^L → enhanced logits f̃ → adaptive decoding → final logits

- **Critical path:**
  1. Token identification: Know which indices correspond to textual vs. visual tokens (model-specific; see Section B.4)
  2. TVER computation: For each head at selected layer, compute Entropy(a^T) and Entropy(a^V), then ratio
  3. Head masking: Set attention outputs to zero for heads below average TVER
  4. Residual flow: Add TE-MHA output to final hidden state, apply final MLP
  5. Adaptive blend: Compare distributions, select blending strategy based on γ

- **Design tradeoffs:**
  - Layer selection: Earlier layers (e.g., layer 0) yielded best POPE results (Figure 7), but performance is relatively robust across layers
  - α_1 vs. α_2: α_1=3, α_2=1 work best (Tables D5, D6); higher α_2 over-suppresses, hurting recall
  - γ threshold: 0.2 for LLaVA-1.5, 0.4 for InstructBLIP/Qwen-VL (Section B.2)
  - Efficiency vs. performance: Single-layer intervention achieves 1.07× latency vs. 2×+ for VCD/M3ID, but may not match methods using full contrastive passes on all metrics

- **Failure signatures:**
  - Uniform TVER: If all heads have similar ratios, selection becomes random; check TVER variance before deployment
  - Wrong token indices: Incorrect textual/visual index ranges will produce meaningless entropy ratios (see B.4 for model-specific ranges)
  - Over-aggressive α_2: High contrastive weight (e.g., α_2 > 2) reduces recall significantly (Table D6)
  - Model mismatch: Hyperparameters (γ, α values) are model-specific; direct transfer without tuning may degrade performance

- **First 3 experiments:**
  1. **Baseline sanity check:** Run ONLY on LLaVA-1.5 with default settings (layer 0, α_1=3, α_2=1, γ=0.2, β=0.1) on POPE MS-COCO Random subset. Verify F1 improves from ~83.4 (regular) to ~89.1 (Table 1).
  2. **Layer ablation:** Test layer selection (0, 8, 16, 24, 31) on POPE. Confirm robustness—F1 should stay within ~0.5 points across layers and consistently exceed VCD/M3ID (Section D.5).
  3. **Efficiency validation:** Measure latency and GPU memory on CHAIR (max tokens=128) vs. VCD/M3ID. Confirm ~1.07× latency and negligible memory overhead (Table 5).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can ONLY be extended to mitigate hallucinations in video understanding tasks while maintaining its efficiency advantage?
- Basis in paper: [explicit] In Section F (Future Work), authors state: "we plan to explore our method's potential for video hallucination mitigation to demonstrate its adaptability across various tasks."
- Why unresolved: The temporal dimension in video introduces additional complexity in attention dynamics and hallucination patterns that may require modifications to the current single-frame approach.
- What evidence would resolve it: Evaluation on video hallucination benchmarks (e.g., Video-ChatGPT benchmarks) comparing ONLY against video-specific contrastive decoding methods, with analysis of temporal attention entropy patterns.

### Open Question 2
- Question: Can ONLY be combined with efficient LVLM acceleration techniques (e.g., FastV, VScan) to achieve hallucination mitigation while surpassing original LVLM inference speed?
- Basis in paper: [explicit] In Section F, authors mention: "we aim to... develop a more efficient hallucination mitigation approach that surpasses the original LVLM speed, leveraging efficient LVLM techniques like FastV and VScan."
- Why unresolved: Current ONLY adds 1.07× overhead; achieving speedup requires understanding potential interactions between attention-head selection and token reduction strategies.
- What evidence would resolve it: Integration experiments showing sub-1.0× inference time while maintaining or improving hallucination mitigation performance compared to standalone ONLY.

### Open Question 3
- Question: Why does the optimal threshold γ for adaptive decoding differ across LVLM architectures (0.2 for LLaVA-1.5 vs. 0.4 for InstructBLIP/Qwen-VL)?
- Basis in paper: [inferred] The paper empirically sets different γ values per model without providing theoretical justification or systematic analysis of why these values differ.
- Why unresolved: Different vision-language connectors and training procedures may produce different distributions of Manhattan distances between original and textual-enhanced logits, requiring different thresholds.
- What evidence would resolve it: Analysis of logit distance distributions across architectures, correlation between connector types and optimal γ values, and whether adaptive γ selection can be automated.

### Open Question 4
- Question: How does the choice of layer for textual enhancement affect performance across different LVLM architectures and task types?
- Basis in paper: [inferred] Ablation in Section 4.4 shows robust performance across layers but focuses only on LLaVA-1.5; the theoretical relationship between layer depth and hallucination mitigation remains unclear.
- Why unresolved: Different architectures may distribute linguistic and visual processing differently across layers, potentially requiring architecture-specific layer selection strategies.
- What evidence would resolve it: Layer-wise ablation studies on InstructBLIP and Qwen-VL, analysis of TVER distributions across layers for different architectures, and examination of whether certain tasks benefit from different layer selections.

## Limitations
- Limited hyperparameter sensitivity analysis with minimal exploration of parameter ranges
- Token index specification requires precise identification that may not generalize across architectures
- Limited evaluation scope focusing on six benchmarks without testing out-of-distribution data or adversarial prompts

## Confidence
- **High Confidence**: Core mechanism of TVER-based attention head selection and single-layer intervention is clearly specified with mathematical formulations and algorithmic details. Efficiency claims (1.07× inference time vs 2×+ for contrastive methods) are supported by concrete timing measurements.
- **Medium Confidence**: Improvement claims over state-of-the-art methods (3.14% higher accuracy on POPE, 1.6% on CHAIR) are well-documented across multiple benchmarks and backbones, though performance gain varies significantly across metrics.
- **Low Confidence**: Adaptive decoding mechanism's threshold selection and generalizability of identified optimal hyperparameters across different model families and tasks remain uncertain, with insufficient evidence for untested LVLM variants.

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary α1, α2, γ, and β values across the range [0.5, 1.5, 2.0, 2.5, 3.0, 3.5] for each backbone to quantify performance degradation and identify robust parameter regions.
2. **Token index validation**: For each LVLM backbone, verify the textual/visual token index ranges by examining attention patterns on controlled inputs (images with known visual elements and textual prompts).
3. **Cross-benchmark consistency check**: Test ONLY on additional hallucination benchmarks not included in the original study to assess whether performance improvements generalize beyond the six evaluated benchmarks.