---
ver: rpa2
title: 'Mangosteen: An Open Thai Corpus for Language Model Pretraining'
arxiv_id: '2507.14664'
source_url: https://arxiv.org/abs/2507.14664
tags:
- thai
- data
- language
- pipeline
- fraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of building high-quality Thai
  language corpora for pretraining language models, as existing large-scale datasets
  rely on English-centric or language-agnostic pipelines that fail to capture Thai
  script or cultural nuances, leaving risky content untreated. To solve this, the
  authors introduce Mangosteen, a 47 billion-token Thai corpus built using a Thai-adapted
  Dolma pipeline that includes custom rule-based language identification, revised
  C4/Gopher quality filters, and Thai-trained content filters for removing adult and
  gambling-related content, along with curated non-web sources such as Wikipedia,
  Royal Gazette texts, OCR-extracted books, and CC-licensed YouTube subtitles.
---

# Mangosteen: An Open Thai Corpus for Language Model Pretraining

## Quick Facts
- **arXiv ID:** 2507.14664
- **Source URL:** https://arxiv.org/abs/2507.14664
- **Reference count:** 40
- **Primary result:** 47 billion-token Thai corpus built with Thai-adapted pipeline, improving SEA-HELM NLG from 3 to 11 points and outperforming SEA-LION-v3 and Llama-3.1 on Thai benchmarks by about four points

## Executive Summary
The paper introduces Mangosteen, a 47 billion-token Thai corpus built to address the lack of high-quality Thai language data for pretraining language models. Existing large-scale datasets rely on English-centric or language-agnostic pipelines that fail to capture Thai script or cultural nuances, leaving risky content untreated. The authors adapt the Dolma pipeline with Thai-specific modifications including custom rule-based language identification, revised quality filters, and Thai-trained content filters for removing adult and gambling-related content. Systematic ablation studies using GPT-2 demonstrate pipeline effectiveness, and continual pretraining of an 8B-parameter SEA-LION model shows significant performance gains on Thai benchmarks.

## Method Summary
The authors build Mangosteen by adapting the Dolma pipeline with Thai-specific modifications: a rule-based Thai character ratio tagger (>50% Thai Unicode characters), elevated word count threshold (200 words minimum), revised quality filters (C4 + Gopher adapted), and FastText classifiers trained on Thai-labeled data for adult and gambling content. The pipeline processes Common Crawl data (202M documents reduced to 25M) and merges it with curated non-web sources including Wikipedia, Royal Gazette texts, OCR-extracted books, and CC-licensed YouTube subtitles. The final corpus contains 47.4 billion tokens, and continual pretraining on SEA-LION-8B demonstrates performance improvements over baseline models.

## Key Results
- GPT-2 ablation study shows SEA-HELM NLG improvement from 3 to 11 points using the adapted pipeline
- Continually pre-trained SEA-LION-8B model outperforms SEA-LION-v3 and Llama-3.1 by about four points on Thai benchmarks
- Pipeline reduces Common Crawl from 202 million to 25 million documents while improving content quality
- Corpus includes 47.4 billion tokens from web and curated sources with systematic content filtering

## Why This Works (Mechanism)

### Mechanism 1: Thai-Specific Language Identification via Unicode Character Ratio
A rule-based character ratio threshold outperforms ML-based language identifiers for Thai script detection. FastText assigns high confidence scores (>0.5) to documents with minimal Thai character content (≤40%), causing false positives. A regex-based `ThaiCharRatioTagger` requiring ≥50% Thai Unicode characters filters non-Thai content more reliably. Thai script's distinct Unicode range enables accurate language detection through simple character counting, independent of word boundaries.

### Mechanism 2: Threshold Elevation for Thai Word Count Filtering
Raising the minimum word count from 50 to 200 words removes disproportionately low-quality Thai web content. Thai documents with <200 words correlate with higher perplexity scores (indicating lower quality) and higher rates of pornographic/gambling content. Perplexity analysis on 200K documents showed mean/median perplexity decreased when filtering at 171+ words vs 50+ words. Short Thai web documents are predominantly boilerplate, navigation text, or spam rather than meaningful content.

### Mechanism 3: Dual Content Filters Trained on Thai-Annotated Data
Separate FastText classifiers for gambling and adult content, trained on Thai-labeled data, reduce culturally-inappropriate material that language-agnostic pipelines miss. Gambling websites often use Thai text with English-branding (e.g., "UFAPOWERBET") that language-agnostic filters don't recognize. Training binary classifiers on documents containing 3+ distinct words from Thai lexicons (plus LLM-identified gambling content) with human validation creates effective domain-specific filters.

## Foundational Learning

- **Unicode Character Ranges and Script Detection**
  - Why needed here: Thai uses a distinct Unicode block (U+0E00–U+0E7F); understanding character-level detection is prerequisite to building the language identification filter.
  - Quick check question: Can you write a regex that matches any Thai consonant character?

- **Perplexity as a Proxy for Data Quality**
  - Why needed here: The paper uses perplexity scores from `wangchanbart-base` to justify word count threshold changes; you need to understand why lower perplexity suggests higher quality.
  - Quick check question: If a document has perplexity 1100 vs 700 from the same language model, which is likely higher quality and why?

- **FastText Subword Embeddings for Low-Resource Classification**
  - Why needed here: Content filters use pre-trained FastText vectors for Thai binary classification with limited labeled data; subword embeddings enable generalization.
  - Quick check question: Why would subword embeddings help classify Thai gambling terms not seen during training?

## Architecture Onboarding

- Component map:
  Raw Common Crawl → Trafilatura extraction → Language ID (ThaiCharRatioTagger)
    → Quality Filters (C4 + Gopher adapted) → Deduplication (URL + document Bloom filter)
    → Content Filters (FastText: adult, gambling) → Final corpus (47.4B tokens)

  Curated Sources (Wikipedia, OCR books, YouTube subtitles, Royal Gazette)
    → Manual/automated cleaning → Deduplication against web data → Merge

- Critical path: The quality filters (especially the 200-word minimum and Thai consonant ratio) remove the largest volume of documents (139.4M of 202M); errors here compound downstream.

- Design tradeoffs:
  - Rule-based language ID vs ML-based: Chose rules for precision and speed, sacrificing recall on code-mixed text
  - Document-level vs paragraph-level deduplication: Chose document-level because Thai lacks reliable paragraph delimiters
  - Processing pre-cleaned FineWeb2 vs raw Common Crawl: Both tested; FineWeb2 + pipeline reduced size 50% with mixed benchmark results

- Failure signatures:
  - High gambling content in final corpus → FastText content filter training data may be insufficient
  - Low NLU scores on Thai benchmarks (Table 6 shows WangchanLION scores 60.86 vs Typhoon's 69.51) → corpus may lack world-knowledge content beyond Thai-cultural domains
  - Duplicate content persists → Bloom filter false negative rate may be too high

- First 3 experiments:
  1. Validate language ID precision: Sample 1000 documents passing the 50% Thai character threshold; manually annotate actual language. Target >95% precision.
  2. Ablate word count threshold: Train GPT-2 124M on 1B tokens each with 50, 100, 200, 300 word minimums; compare SEA-HELM NLG scores.
  3. Measure content filter leakage: Search final corpus for known gambling domain patterns (e.g., "UFABET", casino terms); calculate documents per million containing such patterns.

## Open Questions the Paper Calls Out

### Open Question 1
Can applying a Thai-specific cleaning pipeline to already-cleaned multilingual corpora (like FineWeb2) yield consistent improvements across all benchmark categories? Mixed results—FineWeb2 + pipeline scored 15.68 vs 11.34 on SEA-HELM NLG/IF average, but 2.49 vs 2.55 on Thai LLM Benchmark average—suggest pipeline adaptations may benefit some tasks while hurting others.

### Open Question 2
What are the optimal thresholds for Gopher-style quality filters when adapted to Thai script characteristics? Limited computational budget prevented systematic threshold tuning; perplexity analysis showed no significant difference between tested values.

### Open Question 3
How should Thai-specific pre-training balance gains in cultural knowledge against potential losses in general world knowledge? No analysis quantifying the trade-off or identifying which corpus components preserve world knowledge versus enhance Thai cultural knowledge; Figure 1 shows gains in "Knowledge III (cultural evaluation)" but losses in STEM.

## Limitations

- **Language ID Precision Trade-offs**: The ThaiCharRatioTagger achieves high precision but likely sacrifices recall on code-mixed or short Thai documents, creating bias toward longer, monolingual Thai content.
- **Content Filter Generalizability**: FastText gambling/adult classifiers trained on small labeled datasets may have significant temporal leakage given the rapid evolution of online gambling terminology.
- **Cultural Representation Gaps**: Corpus construction favors web content surviving aggressive quality filtering (200+ word minimum), systematically excluding Thai cultural expressions that are naturally brief.

## Confidence

- **High Confidence**: Basic pipeline architecture (character ratio filtering, elevated word count thresholds, content filtering) demonstrably reduces problematic content and improves baseline model performance on Thai benchmarks.
- **Medium Confidence**: Specific parameter choices (50% Thai character ratio, 200-word minimum, perplexity thresholds) are justified through internal analysis but lack external validation against Thai linguistic standards.
- **Low Confidence**: Claims about cultural nuance preservation and comprehensive Thai representation are not empirically validated; corpus may systematically underrepresent certain Thai language varieties, dialects, or cultural expressions.

## Next Checks

1. **Language ID Precision/Recall Validation**: Sample 5,000 documents that fail the 50% Thai character threshold and manually classify their actual language content. Calculate precision, recall, and F1 score against ground truth annotation.

2. **Temporal Content Filter Efficacy**: Search the final corpus for gambling-related terminology that emerged after the corpus creation period (e.g., 2024+ domain names, new betting platform names). Calculate document frequency of these terms to estimate content filter obsolescence rate.

3. **Cultural Representation Audit**: Compare corpus vocabulary distribution against representative sample of Thai literature, social media, and colloquial speech corpora. Identify systematically under-represented linguistic features (short expressions, dialectal variations, cultural idioms).