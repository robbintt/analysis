---
ver: rpa2
title: 'Beyond Precision: Training-Inference Mismatch is an Optimization Problem and
  Simple LR Scheduling Fixes It'
arxiv_id: '2602.01826'
source_url: https://arxiv.org/abs/2602.01826
tags:
- training
- learning
- mismatch
- arxiv
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the training-inference mismatch problem in
  RL for large language models, which causes optimization instability. The authors
  show that gradient noise and mismatch grow together during training and can be suppressed
  by shrinking update size.
---

# Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It

## Quick Facts
- arXiv ID: 2602.01826
- Source URL: https://arxiv.org/abs/2602.01826
- Reference count: 15
- Primary result: Simple LR decay based on response length surge stabilizes RL training by suppressing training-inference mismatch

## Executive Summary
This work identifies training-inference mismatch in reinforcement learning fine-tuning of large language models as fundamentally an optimization instability problem. The authors demonstrate that gradient noise and mismatch grow together during training, with mismatch serving as an early warning signal detectable via average response length. By implementing a length-triggered learning rate decay scheduler that reduces update size when response length surges, they successfully suppress both mismatch and gradient noise, preventing catastrophic training collapse. Experiments on Qwen3-4B and Qwen3-8B models using the DAPO math reasoning dataset show improved stability and validation performance compared to constant LR baselines and importance sampling methods.

## Method Summary
The authors propose a PPO-based RL fine-tuning approach with a novel learning rate scheduler that decays the learning rate based on average response length. The method uses a hybrid training-inference setup with vLLM for rollout and FSDP for training, monitoring training rewards, validation accuracy, gradient norms, and a mismatch indicator (log ppl_abs_diff). When the running average response length surges (empirically tripling within a narrow window), the scheduler halves the learning rate every T_decay steps, where T_decay is set to 1.8 times the empirically observed surge time (204 steps for 4B, 160 steps for 8B). The approach uses initial LR of 1e-6 with floor at 10% of initial LR, and includes standard PPO hyperparameters with rejection sampling enabled.

## Key Results
- Length-triggered LR decay stabilizes training and maintains mismatch at safe levels compared to constant LR baselines
- Method improves peak validation accuracy on AIME24 math reasoning tasks
- Surpasses importance sampling (TIS/MIS) baselines that delay but don't prevent training collapse
- Empirically validates response length as an early-warning signal for optimization instability

## Why This Works (Mechanism)
The paper demonstrates that training-inference mismatch and gradient noise are tightly coupled during RL training of LLMs. As training progresses, the hybrid engine setup (vLLM inference + FSDP training) introduces precision mismatches that compound with accumulated gradient noise. The authors show that when average response length surges—typically tripling within a narrow window—both mismatch and gradient noise spike dramatically, leading to optimization collapse. By shrinking update size through LR decay triggered by this length surge, the method effectively suppresses both phenomena, stabilizing the optimization landscape. This establishes training-inference mismatch as fundamentally an optimization problem rather than purely an engineering precision issue.

## Foundational Learning
- **Training-inference mismatch**: Discrepancy between training and inference environments that accumulates during RL fine-tuning; needed to understand optimization instability source
- **Gradient noise accumulation**: Random fluctuations in gradients that grow during training and interact with precision mismatches; quick check: monitor gradient norm trends
- **Response length surge detection**: Empirical observation that average response length tripling within narrow window precedes optimization collapse; needed as early-warning signal
- **Learning rate scheduling for stability**: Dynamic LR adjustment to control update magnitude and suppress instability; quick check: compare mismatch metrics before/after LR decay
- **PPO with hybrid engines**: Policy optimization using separate systems for rollout and training; needed to understand mismatch origin
- **Precision-aware RLHF**: Training methodology that accounts for numerical precision differences between inference and training; quick check: validate mismatch indicator behavior

## Architecture Onboarding

**Component Map**
Qwen3-LLM -> VeRL Framework -> vLLM (rollout) + FSDP (training) -> PPO Optimization -> DAPO Dataset -> LR Scheduler (length-triggered) -> Training Rewards/Validation Metrics

**Critical Path**
Data loading → VeRL framework initialization → vLLM rollout generation → FSDP training loop → PPO optimization → LR scheduler monitoring → Parameter updates → Metric logging

**Design Tradeoffs**
- Hybrid engine (vLLM + FSDP) provides efficiency but introduces mismatch
- Length-triggered LR decay is simple but heuristic-based
- Rejection sampling improves sample quality but increases computational cost
- Batch size 64 balances memory constraints with gradient stability

**Failure Signatures**
- Training collapse at ~300 steps with sharp drop in validation accuracy
- Spike in log ppl_abs_diff and gradient norm preceding collapse
- Response length surge (~2-3× increase) occurring ~100 steps before instability
- Importance sampling patches delaying but not preventing eventual collapse

**3 First Experiments**
1. Implement Algorithm 1 scheduler with running average response length tracking and surge detection
2. Compare constant LR vs. length-decay scheduler on training stability and validation accuracy
3. Validate response length as early-warning signal by correlating surge timing with mismatch/gradient noise spikes

## Open Questions the Paper Calls Out
None

## Limitations
- Results confined to math reasoning tasks on DAPO dataset; generalizability to other RLHF domains untested
- Surge detection heuristic lacks precise algorithmic specification beyond qualitative description
- Method not validated on larger model variants (e.g., Qwen3-32B) or alternative reward structures
- Interaction with other stabilization techniques (KL penalties, reward clipping) not explored

## Confidence

**High confidence:**
- Empirical correlation between response length surge, gradient noise growth, and training-inference mismatch
- Effectiveness of length-triggered LR decay in stabilizing training and improving validation accuracy on tested Qwen3 models

**Medium confidence:**
- Generality of response length as universal early-warning signal across RL tasks and model families
- Sufficiency of heuristic surge detection without formal algorithmic definition

**Low confidence:**
- Method's performance on non-math RLHF tasks, larger models, or with alternative reward structures
- Interaction with other stabilization techniques like KL penalties or reward clipping

## Next Checks
1. Test the length-triggered LR scheduler on a non-math RLHF dataset (e.g., UltraFeedback or Arcee) to assess domain generalizability
2. Implement and compare alternative surge detection heuristics (e.g., z-score thresholds or moving average divergence) to validate robustness of trigger mechanism
3. Apply the method to a larger Qwen3 variant (e.g., 32B) to evaluate scaling behavior and whether empirically derived T_decay factor (1.8×) remains valid