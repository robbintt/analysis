---
ver: rpa2
title: 'DomainCQA: Crafting Knowledge-Intensive QA from Domain-Specific Charts'
arxiv_id: '2503.19498'
source_url: https://arxiv.org/abs/2503.19498
tags:
- chart
- pair
- page
- figure
- question-answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DomainCQA addresses the challenge of constructing domain-specific
  chart QA benchmarks that require both visual comprehension and scientific reasoning.
  It introduces a complexity-aware chart selection method using a 10-dimensional Chart
  Complexity Vector and a Gibbs sampling strategy, along with a chart abstract selector
  and voting validator for knowledge-intensive question generation.
---

# DomainCQA: Crafting Knowledge-Intensive QA from Domain-Specific Charts

## Quick Facts
- arXiv ID: 2503.19498
- Source URL: https://arxiv.org/abs/2503.19498
- Reference count: 40
- Key outcome: DomainCQA produces AstroChart, a 1,690 QA pair benchmark over 482 astronomy charts, revealing MLLM weaknesses in numerical reasoning and domain knowledge integration.

## Executive Summary
DomainCQA addresses the challenge of constructing domain-specific chart QA benchmarks that require both visual comprehension and scientific reasoning. It introduces a complexity-aware chart selection method using a 10-dimensional Chart Complexity Vector and a Gibbs sampling strategy, along with a chart abstract selector and voting validator for knowledge-intensive question generation. Applied to astronomy, it produces AstroChart, a benchmark of 1,690 QA pairs over 482 charts. Evaluation on 21 MLLMs shows strong visual performance but persistent weaknesses in numerical reasoning and domain knowledge integration. Fine-tuning on AstroChart data improves performance, and pilot studies in biochemistry, economics, medicine, and social science demonstrate its generality.

## Method Summary
DomainCQA employs a complexity-aware chart selection method using a 10-dimensional Chart Complexity Vector (CCV) and Gibbs sampling to select charts for Fundamental QA (FQA), while using a chart abstract selector with chain-of-thought reasoning and cross-model voting for Advanced QA (AQA). QA generation uses category-specific prompts with Claude 3.5, followed by GPT-4o filtering and expert validation. The approach is applied to astronomy charts from arXiv papers, creating AstroChart with 1,690 QA pairs. The framework also includes fine-tuning experiments using MiniCPM-V2.6-8B with LoRA on generated training data.

## Key Results
- AstroChart evaluation shows top MLLMs achieve 85-90% on FQA Visual categories but only 40-60% on numerical reasoning tasks
- Fine-tuning on AstroChart data improves model performance across multiple benchmarks
- Pilot studies demonstrate framework applicability to biochemistry, economics, medicine, and social science domains
- Human experts achieve only 39% on AQA tasks, suggesting even experts face limits beyond their subfields

## Why This Works (Mechanism)

### Mechanism 1: Complexity-Aware Chart Selection via CCV and Gibbs Sampling
Selecting charts based on a 10-dimensional complexity vector preserves domain-specific visual patterns better than random sampling. A ResNet-50 classifier predicts 10 binary complexity attributes per chart, and non-parametric Gibbs sampling iteratively swaps charts to match marginal distributions of the full corpus, preserving inter-dimensional dependencies.

### Mechanism 2: Chart Abstract Identification via CoT and Cross-Model Voting
Charts that summarize a paper's main findings ("chart abstracts") are better anchors for knowledge-intensive questions than visually complex but semantically peripheral charts. Multiple LLMs vote on each chart's relevance after chain-of-thought reasoning, reducing individual model biases.

### Mechanism 3: Multi-Tier QA with LLM Filtering and Expert Validation
Separating QA into Fundamental (visual/data/inference) and Advanced (knowledge-based) tiers, followed by automated filtering and human review, produces benchmarks that expose distinct MLLM weaknesses. LLM-based filtering can reliably approximate human judgments of QA quality at scale.

## Foundational Learning

- **Gibbs Sampling and Non-Parametric Distribution Matching**
  - Why needed here: The FQA chart selection depends on understanding how iterative conditional sampling approximates a target joint distribution without explicit density estimation.
  - Quick check question: Given a 2D distribution over (color_complexity, subplot_count), how would one Gibbs-sample 100 charts that preserve both marginals and their correlation?

- **Chain-of-Thought Prompting and Multi-Model Ensembles**
  - Why needed here: AQA chart selection uses CoT reasoning and voting; understanding prompt design and ensemble convergence is critical for reproducibility.
  - Quick check question: If Model A selects Chart 3 as most relevant (confidence 0.7) and Model B selects Chart 7 (confidence 0.9), what voting strategy maximizes robustness?

- **Evaluation Metrics for Open-Ended vs Numerical QA**
  - Why needed here: AstroChart uses relative error for numerical retrieval, exact match for derivation, and LLM-based scoring (0-1) for open-ended responses.
  - Quick check question: Why might ROUGE-L correlate with LLM-judge scores but fail to capture domain-specific correctness?

## Architecture Onboarding

- **Component map:** Chart Corpus → ResNet-18 filter → CCV extraction (ResNet-50) → Gibbs sampling (FQA charts) OR CoT+Voting (AQA charts) → Selected charts → Claude 3.5 QA generation → GPT-4o filter → Expert validation platform → Final benchmark

- **Critical path:**
  1. CCV classifier training (requires annotated multi-domain charts; F1=61.5% on test set)
  2. Gibbs sampling convergence (monitors distribution stability; see Algorithm 1)
  3. Expert validation (8 astronomers, 160+ hours for 1,690 pairs)

- **Design tradeoffs:**
  - Automated filtering (fast, 96.5% accuracy) vs. full expert review (slow, gold standard)
  - CCV-based selection (domain-representative) vs. random sampling (simpler, less aligned)
  - Single-model QA generation (consistent style) vs. multi-model (diverse but harder to filter)

- **Failure signatures:**
  - CCV mismatch: Selected charts cluster in low complexity despite corpus diversity (check Figure 3 distribution)
  - Abstract selector drift: AQA questions reference peripheral figures; low expert relevance scores (Table 4)
  - Filter over-rejection: GPT-4o Kappa drops below 0.6; manual review shows high false positive rate

- **First 3 experiments:**
  1. Validate CCV representativeness: Compute CCV for 500 random corpus charts vs. 500 Gibbs-selected charts; compare marginal distributions and joint entropy.
  2. Ablate voting validator: Run AQA selection with single-model (GPT-4o only) vs. multi-model voting; measure expert relevance scores on 100 sampled pairs.
  3. Stress-test numerical reasoning: Evaluate 5 MLLMs on AstroChart's "Calculation" subset; correlate errors with chart complexity scores to identify failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
Can the DomainCQA framework maintain high QA validity and domain relevance when scaled to full benchmarks in fields with drastically different visual semiotics than astronomy (e.g., network graphs in biochemistry), or does the Chart Complexity Vector (CCV) require domain-specific re-calibration?

### Open Question 2
To what extent does fine-tuning on DomainCQA data improve the underlying arithmetic reasoning capabilities of MLLMs, versus merely enhancing their ability to extract numbers (OCR) from specific scientific chart layouts?

### Open Question 3
Does the reliance on proprietary LLMs (GPT-4o, Claude 3.5) for the "chart abstract" selector and voting validator introduce a selection bias that penalizes open-source MLLMs during evaluation?

### Open Question 4
How does the low human baseline performance (39%) on Advanced QA (AQA) tasks impact the reliability of the expert validation process used to generate the ground truth?

## Limitations
- The ResNet-50 CCV classifier achieves only 61.5% F1, suggesting moderate confidence in complexity attribute predictions
- Expert validation was conducted only for astronomy domain, with pilot studies in other fields lacking rigorous validation
- The approach relies heavily on proprietary LLMs for key components (chart selection, filtering), raising questions about bias and reproducibility

## Confidence
- **High Confidence:** Overall methodology for creating domain-specific chart QA benchmarks is well-defined and technically sound
- **Medium Confidence:** CCV classifier performance and its impact on chart selection quality
- **Low Confidence:** Scalability and generalizability across diverse scientific domains without domain-expert adaptation

## Next Checks
1. **CCV Classifier Calibration Test:** Evaluate the ResNet-50 CCV classifier on a held-out test set of 100 charts from diverse domains and measure per-class F1 scores.

2. **Ablation Study of Voting Mechanism:** Generate 100 AQA pairs using single-model selection versus multi-model voting and have domain experts rate relevance and knowledge-intensity.

3. **Numerical Reasoning Error Analysis:** For 5 evaluated MLLMs, conduct fine-grained error analysis on the "Calculation" subset, categorizing errors and correlating with chart complexity scores.