---
ver: rpa2
title: 'The Power of Architecture: Deep Dive into Transformer Architectures for Long-Term
  Time Series Forecasting'
arxiv_id: '2507.13043'
source_url: https://arxiv.org/abs/2507.13043
tags:
- forecasting
- attention
- time
- table
- series
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the optimal Transformer architecture for
  long-term time series forecasting (LTSF) by proposing a comprehensive taxonomy that
  disentangles architectural variations from time-series-specific designs. Through
  extensive experiments on eight datasets, the authors identify that bi-directional
  attention with joint-attention is most effective, more complete forecasting aggregation
  enhances performance, and the direct-mapping paradigm significantly outperforms
  autoregressive approaches.
---

# The Power of Architecture: Deep Dive into Transformer Architectures for Long-Term Time Series Forecasting

## Quick Facts
- **arXiv ID:** 2507.13043
- **Source URL:** https://arxiv.org/abs/2507.13043
- **Reference count:** 40
- **Primary result:** Bi-directional attention with joint-attention, complete aggregation, and direct-mapping paradigm consistently outperform alternatives in LTSF

## Executive Summary
This study investigates optimal Transformer architecture design for long-term time series forecasting (LTSF) by disentangling architectural variations from time-series-specific designs. Through extensive experiments on eight benchmark datasets, the authors identify three key architectural choices that significantly improve performance: bi-directional attention with joint-attention, complete forecasting aggregation, and direct-mapping paradigm. The research provides a comprehensive taxonomy of Transformer variants and establishes empirical evidence for which architectural choices matter most in LTSF applications.

## Method Summary
The study proposes a taxonomy that disentangles architectural variations from time-series-specific designs, then systematically compares six attention variants (encoder, decoder, prefix decoder), three aggregation strategies (no, partial, complete), and two forecasting paradigms (autoregressive, direct-mapping). The methodology employs eight benchmark datasets with standard look-back window of 512 (120 for Illness) and forecasting horizons of 96, 192, 336, and 720 steps. Models are trained using Adam optimizer (lr=0.001) with z-score standardization plus RevIN normalization, employing channel independence strategy for multivariate series.

## Key Results
- Bi-directional attention with joint-attention is most effective, achieving highest first-place counts (9 wins) across datasets
- Complete aggregation of features across look-back and forecasting windows improves performance, winning 10/10 first-place counts
- Direct-mapping paradigm significantly outperforms autoregressive approaches, with decoder-direct MSE of 2.124 vs decoder-autoregressive MSE of 5.318
- A combined model utilizing all optimal choices consistently outperforms existing LTSF models including FEDformer, PatchTST, iTransformer, CATS, and ARMA-Attention

## Why This Works (Mechanism)

### Mechanism 1: Bi-Directional Attention with Joint-Attention
Bi-directional attention allows each token to attend to both preceding and succeeding tokens, capturing forward and backward temporal dependencies in a single pass. Joint-attention enables direct token-to-token connections across the entire sequence rather than routing information through separate encoder-decoder latent bottlenecks. This unrestricted contextual access provides superior modeling capacity for LTSF tasks where temporal dependencies benefit from complete contextual information.

### Mechanism 2: Complete Forecasting Aggregation
Complete aggregation flattens all token embeddings (both look-back and forecasting windows) into a one-dimensional vector before projection to target values. This enables feature-level interaction across the entire sequence, rather than independent token-level predictions or partial aggregation limited to forecasting tokens. The global feature interaction beyond what attention mechanisms provide alone significantly enhances prediction quality.

### Mechanism 3: Direct-Mapping Forecasting Paradigm
Direct-mapping generates all target tokens simultaneously in one forward pass, avoiding iterative error accumulation inherent in autoregressive generation. It also eliminates train-test mismatch from teacher-forcing during training versus autoregressive inference. This parallel, globally-coordinated prediction approach is particularly effective for LTSF where sequential, locally-conditioned generation introduces compounding errors.

## Foundational Learning

- **Concept: Multi-head Self-Attention**
  - Why needed: Central to understanding how Transformers model dependencies across time steps and compare different masking strategies
  - Quick check: Can you explain how masking strategies control which tokens can attend to which others in each architecture variant?

- **Concept: Time Series Forecasting Formulation**
  - Why needed: The paper frames LTSF as mapping look-back window to forecasting window, essential for understanding aggregation and paradigm choices
  - Quick check: Given look-back length L=512 and forecasting length T=96, what's the output shape with embedding dimension d_model=512?

- **Concept: Normalization Layers (LayerNorm vs BatchNorm)**
  - Why needed: The paper finds BatchNorm performs better on datasets with anomalies/outliers, while LayerNorm excels on stationary series
  - Quick check: For a batch of time series (shape B×L×d), what dimensions does LayerNorm normalize over versus BatchNorm?

## Architecture Onboarding

- **Component map:** Z-score standardization → RevIN → Patching → Linear embedding + learnable positional embeddings → Encoder-only Transformer → Complete aggregation prediction head → Direct-mapping loss

- **Critical path:**
  1. Implement baseline encoder-only Transformer with joint-attention
  2. Add complete aggregation prediction head (flatten all token embeddings → linear projection)
  3. Use direct-mapping loss (MSE/MAE computed on full forecasting window simultaneously)
  4. Select normalization based on dataset outlier analysis (use IQR method)
  5. Train with channel independence for multivariate series

- **Design tradeoffs:**
  - Encoder-only vs Decoder-only: Encoder-only gives bi-directional context but allows future leakage; decoder-only is causal but restricts dependencies
  - Complete vs Partial aggregation: Complete aggregation improves performance but increases memory; partial may be necessary for very long sequences
  - BatchNorm vs LayerNorm: BatchNorm helps with non-stationary/outlier-rich data but may fail with small batches

- **Failure signatures:**
  - Autoregressive models show exploding error on long horizons (MSE grows dramatically)
  - Cross-attention models underperform similar-complexity joint-attention
  - Decoder-only struggles with limited data (large performance gap when training data is scarce)

- **First 3 experiments:**
  1. Sanity check: Implement encoder-only with no aggregation vs complete aggregation on ETTh1
  2. Attention ablation: Compare encoder-only vs encoder-decoder on datasets with different training data sizes
  3. Paradigm validation: Train decoder-only models with direct-mapping vs autoregressive on Illness dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do decoder-only architectures eventually outperform encoder-only architectures for LTSF as training dataset size scales to orders of magnitude larger?
- **Basis:** The authors observe the performance gap diminishes as data size increases, suggesting uni-directional attention might suffice with sufficient data
- **Why unresolved:** Experiments were limited to standard sizes of existing LTSF datasets
- **What evidence would resolve it:** Benchmarking on time-series datasets significantly larger than current benchmarks

### Open Question 2
- **Question:** Can an adaptive or unified normalization layer be developed that automatically outperforms dataset-dependent choice between LayerNorm and BatchNorm?
- **Basis:** Conclusion states normalization choice depends on manual dataset characteristic analysis
- **Why unresolved:** Paper establishes conditional rules but doesn't propose dynamic mechanisms
- **What evidence would resolve it:** Proposal and validation of learnable normalization layer that adapts based on local stationarity or anomaly detection

### Open Question 3
- **Question:** How do optimal architectural choices interact with time-series-specific inductive biases such as decomposition or frequency-based attention?
- **Basis:** Authors explicitly disentangle architectural designs from time-series-specific designs
- **Why unresolved:** While identifying best "pure" architecture, interaction with specific signal-processing modules remains unclear
- **What evidence would resolve it:** Ablation studies combining optimal "Combined Model" with mechanisms like seasonal-trend decomposition

## Limitations

- The direct-mapping paradigm comparison relies on only three experimental variants, leaving some architectural space unexplored
- The claim of bi-directional attention being universally "most effective" doesn't investigate the fundamental causal information leakage trade-off
- Channel Independence strategy performance on highly correlated multivariate series is not extensively validated

## Confidence

- **High Confidence:** Empirical findings on aggregation strategies and direct-mapping paradigm are well-supported by controlled experiments with clear performance gaps
- **Medium Confidence:** Bi-directional attention mechanism recommendation requires careful consideration of causal constraints in practical applications
- **Medium Confidence:** BatchNorm vs LayerNorm findings are dataset-specific and should be validated on new datasets

## Next Checks

1. **Causal Leakage Validation:** Implement controlled experiment comparing encoder-only vs decoder-only on synthetic dataset where future information should not help prediction
2. **Aggregation Memory Efficiency:** Profile complete aggregation memory usage across varying sequence lengths to determine practical limits
3. **Cross-Dataset Generalization:** Test combined model architecture on held-out dataset not in original eight to validate generalization claims beyond benchmark suites