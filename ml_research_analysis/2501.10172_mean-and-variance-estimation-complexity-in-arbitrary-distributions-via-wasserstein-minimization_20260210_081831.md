---
ver: rpa2
title: Mean and Variance Estimation Complexity in Arbitrary Distributions via Wasserstein
  Minimization
arxiv_id: '2501.10172'
source_url: https://arxiv.org/abs/2501.10172
tags:
- have
- where
- optimal
- which
- thus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the complexity of estimating translation\
  \ \xB5 \u2208 R\u02E1 and shrinkage \u03C3 \u2208 R++ parameters for distributions\
  \ of the form 1/\u03C3\u02E1 f\u2080((x-\xB5)/\u03C3), where f\u2080 is a known\
  \ density in R\u02E1, given n samples. The authors show that while Maximum Likelihood\
  \ Estimation (MLE) for this problem is NP-hard for general distributions, it is\
  \ possible to obtain \u03B5-approximations in polynomial time using the Wasserstein\
  \ distance."
---

# Mean and Variance Estimation Complexity in Arbitrary Distributions via Wasserstein Minimization

## Quick Facts
- **arXiv ID:** 2501.10172
- **Source URL:** https://arxiv.org/abs/2501.10172
- **Reference count:** 9
- **Primary result:** Wasserstein distance estimation for translation and scaling parameters is NP-hard via MLE but solvable in polynomial time with ε-accuracy

## Executive Summary
This paper addresses the fundamental problem of estimating translation (μ) and scaling (σ) parameters for distributions of the form (1/σˡ)f₀((x-μ)/σ) given n samples. While Maximum Likelihood Estimation for this problem is NP-hard for general distributions, the authors develop a polynomial-time algorithm using Wasserstein distance minimization. The key insight is that the optimal transport plan structure is independent of the parameters μ and σ when using squared Euclidean distance, allowing efficient parameter estimation through dual optimization.

## Method Summary
The method uses semi-discrete optimal transport theory to decouple parameter estimation from transport geometry. It formulates the problem as maximizing a dual energy function, where gradients are computed via geometric volume estimation of Laguerre cell intersections with hyperrectangles. The algorithm performs inexact gradient ascent on the dual, using the KLS volume algorithm for gradient estimation. After convergence, closed-form formulas recover the optimal μ and σ parameters from the dual solution.

## Key Results
- NP-hardness of MLE for general distribution parameter estimation (Section 3)
- Polynomial-time ε-approximation algorithm with complexity poly(n, l, k, 1/s)
- Parameter recovery via dual energy maximization with closed-form formulas
- Algorithm achieves εD-accuracy for μ and ε-accuracy for σ with probability ≥ 1-η

## Why This Works (Mechanism)

### Mechanism 1: Parameter Decoupling via Transport Map Invariance
The estimation of translation (μ) and scaling (σ) parameters can be decoupled from the computation of the optimal transport plan π. When using squared Euclidean distance cost c(x,y) = ||σx - y - μ||², the optimal transport map π remains structurally identical regardless of specific μ or σ values. Theorem 2.1 and 2.3 show the primal solution π for arbitrary μ, σ is identical to the solution for μ=0, σ=1, allowing the algorithm to fix transport geometry while optimizing parameters.

### Mechanism 2: Primal Recovery via Dual Energy Maximization
Optimal parameters can be computed using closed-form formulas derived from the optimal primal cost, recovered by maximizing a concave dual energy function. Direct computation of the primal transport map is intractable for continuous sources, but explicit formulas for μ* and σ* (Eq. 3,4) depend on ∫xᵀyⱼdπ(x,yⱼ). By Strong Duality, this equals the optimal value of the dual program (Eq. 5), so maximizing dual energy E(g) yields the value needed for analytical parameter solution.

### Mechanism 3: Gradient Computation via Volume Estimation Oracles
The gradient of the dual energy is approximated in polynomial time by reducing the problem to geometric volume estimation. The gradient ∇E(g)ⱼ measures mass difference between target capacity bⱼ and mass of source distribution assigned to sink j (Laguerre cell). Under piecewise constant source assumption, calculating this mass reduces to computing volume of intersection between Laguerre cell and hyperrectangle, implemented via KLS volume algorithm using separation oracle.

## Foundational Learning

**Semi-Discrete Optimal Transport**
- Why needed: Problem maps continuous source distribution to discrete sinks; understanding Laguerre cells is essential for visualizing how algorithm partitions source space
- Quick check: How does cost function c(x,y) = ||x-y||² - gⱼ define boundaries of a Laguerre cell?

**Inexact Gradient Descent**
- Why needed: Algorithm relies on gradients computed via probabilistic volume estimation (KLS) introducing noise; standard gradient convergence proofs don't apply directly
- Quick check: How does error bound ||eₜ|| ≤ ε'/C affect convergence rate compared to exact gradient descent?

**Complexity of Volume Estimation**
- Why needed: Polynomial complexity relies entirely on existence of randomized algorithms (like KLS) that estimate volume of convex body given only separation oracle
- Quick check: Why is volume estimation hard in high dimensions, and how does separation oracle simplify implementation without solving geometry explicitly?

## Architecture Onboarding

**Component map:** Input Layer (samples, hyperrectangles) -> Geometry Engine (dual weights g, Laguerre cells) -> Volume Estimator (KLS algorithm) -> Optimizer (inexact gradient ascent) -> Output Layer (μ̂, σ̂)

**Critical path:** Volume Estimator is bottleneck. Every gradient step requires k × n calls to separation oracle to estimate mass flow. Efficiency of this oracle (handling Lⱼ ∩ Hᵢ) dictates total runtime.

**Design tradeoffs:** Accuracy vs. Dimensionality - runtime depends on 1/ε but suffers from curse of dimensionality in volume estimation step. Parameter s (Resolution) - algorithm slows dramatically if sample points are very close (s→0) or hyperrectangles are thin (s→0), as Lipschitz constant L scales as 1/s².

**Failure signatures:** Slow Convergence - if gradient noise is under-estimated or volume estimation samples are too few, optimizer may oscillate or diverge. Boundary Errors - if "piecewise constant" assumption is violated, gradient estimation will be biased, leading to incorrect μ, σ.

**First 3 experiments:** 1) Synthetic Sanity Check - generate samples from known 2D Gaussian, approximate source with coarse grid of hyperrectangles, verify recovered μ, σ match sample mean/variance. 2) Sensitivity to s - systematically reduce distance between two sample points to observe degradation in convergence speed. 3) Hardness Validation - attempt to fit parameters using standard MLE approach on "3-SAT" style distributions to verify failure mode compared to Wasserstein solver.

## Open Questions the Paper Calls Out

**Open Question 1:** Can an Expectation-Maximization (EM) algorithm be constructed using Wasserstein minimization to tractably estimate parameters in mixture models? The paper suggests developing an analog of EM algorithm using established Wasserstein methods to avoid NP-hardness of MLE, but combining iterative structure with Wasserstein metrics is unexplored.

**Open Question 2:** Does NP-hardness of MLE persist under alternative approximation definitions that distinguish between zero and non-zero likelihoods? Current hardness result relies on standard approximation definitions; exploring alternative definitions where result might not hold is suggested, as current reduction determines existence of non-zero likelihood.

**Open Question 3:** Can polynomial-time algorithm be extended to general continuous distributions (e.g., Gaussians) without incurring exponential dependence on dimensionality? Main result restricts to piecewise constant distributions on hyperrectangles, noting extension to general class implies exponential dependence on l due to volume estimation challenges for curved density supports.

## Limitations
- Requires piecewise constant source distributions on hyperrectangles, limiting applicability to smooth continuous densities
- Polynomial complexity depends critically on separation parameter s being bounded away from zero
- Volume estimation introduces probabilistic error that compounds across gradient iterations
- Algorithm's efficiency degrades with increasing number of hyperrectangles k and dimensionality l

## Confidence

**High Confidence:** NP-hardness of MLE for general distributions and polynomial-time complexity bounds for Wasserstein approach are well-established through rigorous proofs. Decoupling of transport geometry from parameters via squared-Euclidean cost is mathematically sound.

**Medium Confidence:** Practical feasibility depends on volume estimation parameters not explicitly specified. Convergence analysis assumes ideal conditions that may be challenging to achieve in practice.

**Low Confidence:** Real-world applicability to non-piecewise-constant densities and sensitivity to hyperrectangle discretization quality are not thoroughly explored.

## Next Checks

1. **Scalability Test:** Implement algorithm on increasing numbers of samples (n) and hyperrectangles (k) to empirically verify poly(n, k, 1/ε, log(1/η)) complexity bound, focusing on volume estimation bottleneck.

2. **Robustness to Separation:** Systematically vary minimum separation parameter s between sample points and measure degradation in convergence speed and final accuracy to validate 1/s² dependence in Lipschitz constant.

3. **Distribution Fidelity:** Compare parameter recovery accuracy on smooth continuous distributions (approximated by hyperrectangles) versus true piecewise constant case to quantify impact of density approximation error on final estimates.