---
ver: rpa2
title: 'ICo3D: An Interactive Conversational 3D Virtual Human'
arxiv_id: '2601.13148'
source_url: https://arxiv.org/abs/2601.13148
tags:
- body
- head
- gaussian
- human
- avatar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ICo3D presents a novel method for generating interactive, conversational,
  and photorealistic 3D human avatars. The system integrates dynamic full-body 3D
  reconstruction, audio-driven facial animation, and LLM-powered conversational capabilities.
---

# ICo3D: An Interactive Conversational 3D Virtual Human

## Quick Facts
- arXiv ID: 2601.13148
- Source URL: https://arxiv.org/abs/2601.13148
- Authors: Richard Shaw, Youngkyoon Jang, Athanasios Papaioannou, Arthur Moreau, Helisa Dhamo, Zhensong Zhang, Eduardo Pérez-Pellitero
- Reference count: 40
- One-line primary result: Novel method for interactive, conversational, photorealistic 3D human avatars with real-time performance (105.27 FPS)

## Executive Summary
ICo3D presents a novel method for generating interactive, conversational, and photorealistic 3D human avatars. The system integrates dynamic full-body 3D reconstruction, audio-driven facial animation, and LLM-powered conversational capabilities. Key technical contributions include HeadGaS++, an extension of HeadGaS for audio-driven facial animation, SWinGS++, a spatial-temporal encoder for dynamic body reconstruction, and a unified framework for merging head and body models. The system achieves real-time performance, rendering at 105.27 FPS on high-end hardware.

## Method Summary
ICo3D combines several technical innovations to create an interactive 3D virtual human. The system uses HeadGaS++ for audio-driven facial animation, where Gaussian properties are modulated by audio features rather than moving Gaussian positions directly. For body reconstruction, SWinGS++ employs motion-adaptive sliding windows with spatial-temporal encoding to handle fast motion. The head and body models are captured separately and merged using canonical FLAME coordinates. The system integrates with speech recognition (Whisper), language modeling (Qwen2-0.5B), and text-to-speech (OpenVoice V2) to enable conversational interactions.

## Key Results
- SWinGS++ achieves 30.1701 PSNR for novel view synthesis on DNA-Rendering dataset
- HeadGaS++ outperforms state-of-the-art methods in talking head synthesis with 30.398 PSNR and 5.928 Sync-C score for lip synchronization
- System renders at 105.27 FPS on high-end hardware
- Real-time conversational interactions with precise audio-to-expression synchronization

## Why This Works (Mechanism)

### Mechanism 1: Audio-Driven Gaussian Property Modulation
Audio features drive photorealistic facial animation without explicitly moving Gaussian positions. HeadGaS++ augments each 3D Gaussian with a learned latent feature basis blended with audio-visual weights, producing dynamic appearance changes while keeping Gaussian positions fixed. The 39-dimensional feature space (32 audio dimensions from SyncTalk + 7 eye parameters) captures natural facial expressions. This approach achieves highest Sync-C (5.928) and PSNR (30.398) among compared methods with 250 FPS rendering.

### Mechanism 2: Motion-Adaptive Sliding Windows with Spatial-Temporal Encoding
Partitioning long sequences into motion-adaptive windows with per-window spatial-temporal encoders improves reconstruction of fast human motion. SWinGS++ divides sequences based on accumulated optical flow exceeding thresholds, with shorter windows for high-motion regions. Each window trains independent 4DGS models with spatial-temporal encoders producing voxel features. This approach achieves 30.17 PSNR vs. 29.45 for baseline, with qualitative improvement on fast leg motion.

### Mechanism 3: Cross-Setup Head-Body Alignment via Canonical Coordinates
Head and body models captured in different setups are merged by exploiting FLAME's unified canonical reference system. The transformation from capture 1 to capture 2 uses canonical points that are the same in both worlds. Gaussians are pruned from the body within a spherical face region, border Gaussians are injected, and head color layers are unfrozen for joint optimization. This approach handles the geometric inconsistencies at the neck/jaw boundary.

## Foundational Learning

- **3D Gaussian Splatting basics** (positions μ, covariance Σ, opacity o, spherical harmonics color): Entire representation builds on 3DGS; understanding rasterization and adaptive density control is prerequisite. Quick check: Can you explain how tile-based rasterization achieves real-time performance compared to NeRF volume rendering?

- **Blendshapes and 3DMM parametric models** (FLAME, ARKit): Head animation uses 39-dimensional expression weights; body alignment relies on FLAME canonical coordinates. Quick check: How do blendshape weights differ from direct vertex displacement for animation control?

- **Temporal consistency in dynamic reconstruction**: Sliding window approach requires understanding inter-window artifacts and fine-tuning strategies. Quick check: Why does independent per-window training create temporal flickering, and how does overlapping-frame loss address it?

## Architecture Onboarding

- Component map: User Input (audio/text) → ASR (Whisper) → LLM (Qwen2-0.5B) → TTS (OpenVoice V2) → SyncTalk → Audio Features (32-dim) → HeadGaS++ ← Audio + Eye Params (39-dim) → Multi-view Capture → SWinGS++ → Dynamic Body Gaussians → Cross-Setup Alignment → Gaussian Merging → Real-Time Rasterizer → 105 FPS Output

- Critical path: ASR→LLM→TTS→SyncTalk→HeadGaS++ inference must complete within 33ms for 30 FPS facial sync; remote server handles compute-heavy modules while local viewer only renders.

- Design tradeoffs: 4D reconstruction (SWinGS++) vs. animatable models (HuGS) achieves higher quality (30.17 vs 25.04 PSNR) but limits body motion to replay/procedural animation; HuGS enables novel poses at lower fidelity. Separate head/body capture vs. unified gives better quality per component but requires cross-setup alignment complexity.

- Failure signatures: Double-chin artifacts (body Gaussians penetrate jaw region; increase pruning frequency or radius), face-border discontinuity (inject more border Gaussians during integration), temporal flickering in novel views (increase consistency loss weight during fine-tuning), lip-sync drift (check SyncTalk feature extraction alignment with TTS audio timestamps).

- First 3 experiments: 1) Single-sequence validation: Train head and body on DNA-Rendering alone (unified capture) to establish baseline without cross-setup complexity. 2) Ablation on spatial-temporal encoder: Compare SWinGS++ vs. SWinGS on a fast-motion sequence; measure PSNR delta and visualize leg reconstruction. 3) Latency profiling: Instrument ASR→LLM→TTS→SyncTalk pipeline on target hardware; identify bottleneck for 30 FPS target.

## Open Questions the Paper Calls Out

### Open Question 1
Can the system transition from procedural body animation to learned audio-driven motion synthesis without compromising real-time performance? The current system uses keyframe selection for body motion, which can look repetitive. Generating novel, speech-synchronized body motion via learning-based methods introduces computational overhead and data scarcity issues not addressed in the current pipeline.

### Open Question 2
Can the head and body models be merged without heuristic pruning to eliminate boundary artifacts? Simply merging Gaussians causes significant artifacts and penetration, requiring specific manual interventions like pruning Gaussians within a spherical region around the face. A joint optimization strategy or unified Gaussian representation that handles the neck seam naturally is needed.

### Open Question 3
Is cross-setup alignment robust to significant variations in subject appearance or camera calibration between the head and body captures? The method relies on FLAME fittings from separate captures being mappable to a unified canonical reference system. If the head tracker fails or produces inconsistent canonical meshes across different capture setups, the rigid alignment equation may fail.

## Limitations
- Cross-setup alignment robustness: The method relies on FLAME-based canonical coordinates to merge head and body Gaussians from different capture setups, but lacks extensive validation across diverse capture conditions.
- Audio feature generalization: The 39-dimensional feature space may not generalize well to speakers with accents, non-standard speech patterns, or emotional speech outside the training distribution.
- Body motion limitations: The system is fundamentally limited to replaying or procedurally animating captured sequences, restricting conversational expressiveness compared to parametric body models.

## Confidence
- High confidence: The core 3D Gaussian Splatting reconstruction pipeline (SWinGS++ achieving 30.17 PSNR on DNA-Rendering) and audio-driven facial animation quality (HeadGaS++ achieving 30.398 PSNR and 5.928 Sync-C score) are well-supported by quantitative metrics and established baselines.
- Medium confidence: The real-time conversational performance claim (105.27 FPS rendering) is credible given the computational profile of 3DGS, but the distributed architecture's end-to-end latency hasn't been fully characterized. The cross-setup head-body merging approach is theoretically sound but lacks extensive validation across diverse capture conditions.
- Low confidence: The system's generalization to real-world conversational scenarios, including handling multiple speakers, languages, and natural conversational dynamics, remains largely hypothetical based on current evidence.

## Next Checks
1. Cross-setup alignment robustness test: Evaluate the head-body merging pipeline across 10 subjects captured in different environments with varying lighting, camera setups, and backgrounds. Measure geometric alignment errors and visual artifacts in the merged results.

2. Audio feature generalization benchmark: Test HeadGaS++ on a diverse speech dataset including accented speech, emotional expressions, and non-standard speaking styles. Compare lip-sync accuracy and facial animation quality against baseline methods.

3. End-to-end conversational latency measurement: Instrument the complete ASR→LLM→TTS→animation pipeline on target hardware to measure total system latency from user speech to avatar response. Verify whether the system consistently achieves 30 FPS facial synchronization in interactive conditions.