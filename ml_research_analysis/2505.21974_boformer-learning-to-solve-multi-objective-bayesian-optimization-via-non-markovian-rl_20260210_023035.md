---
ver: rpa2
title: 'BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian
  RL'
arxiv_id: '2505.21974'
source_url: https://arxiv.org/abs/2505.21974
tags:
- boformer
- learning
- functions
- mobo
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of designing acquisition functions
  for multi-objective Bayesian optimization that can plan over multiple steps rather
  than just optimizing one-step improvement. The key issue identified is the hypervolume
  identifiability problem, which arises from the non-Markovian nature of MOBO - the
  same observation can lead to different hypervolume improvements depending on history.
---

# BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL

## Quick Facts
- arXiv ID: 2505.21974
- Source URL: https://arxiv.org/abs/2505.21974
- Reference count: 0
- Key outcome: Introduces BOFormer, a transformer-based RL method for multi-objective Bayesian optimization that addresses hypervolume identifiability through history-dependent value estimation and Q-augmented observations, achieving state-of-the-art performance across synthetic and real-world tasks

## Executive Summary
This paper addresses the fundamental challenge of multi-objective Bayesian optimization (MOBO) where traditional acquisition functions are myopic and suffer from the hypervolume identifiability problem due to non-Markovian dynamics. The authors propose BOFormer, a transformer-based generalized deep Q-network that learns to plan over multiple steps by processing sequences of observation-action pairs. The method introduces Q-augmented observation representations that are domain-agnostic and memory-efficient, avoiding the scalability issues of direct sequence modeling approaches. Extensive experiments demonstrate that BOFormer consistently outperforms both rule-based methods (qNEHVI, JES) and learning-based alternatives (FSAF, OptFormer) across various synthetic functions and real-world hyperparameter optimization tasks.

## Method Summary
BOFormer implements a generalized deep Q-network framework using transformer-based sequence modeling to address the non-Markovian nature of multi-objective Bayesian optimization. The method uses Q-augmented observation representations that combine candidate posterior distributions with Q-values from previous steps, creating a domain-agnostic and memory-efficient input format. Training employs a demo-policy-guided exploration strategy using Expected Hypervolume Improvement trajectories stored in a prioritized replay buffer. The architecture consists of a GPT-2 style transformer encoder that processes sequences of augmented observations to output Q-values for candidate points, which are then selected via maximization. The method is evaluated on synthetic benchmark functions and a real-world hyperparameter optimization task for 3D Gaussian splatting.

## Key Results
- BOFormer consistently outperforms rule-based methods (qNEHVI, JES) and learning-based alternatives (FSAF, OptFormer) across synthetic functions and HPO-3DGS
- The method demonstrates strong cross-domain transferability, maintaining performance across different problem dimensions and numbers of objectives without retraining
- Ablation studies confirm the importance of the Q-augmented representation and history-dependent modeling, with performance degrading significantly when using Markovian approaches

## Why This Works (Mechanism)

### Mechanism 1: History-Dependent Value Estimation
The architecture replaces the Markovian assumption of standard DQN with a Generalized DQN framework. By using a Transformer to process the sequence of past observation-action pairs, the model approximates the optimal Q-value function $Q^*(h, a)$ conditioned on the entire history $h$, thereby disambiguating states that appear identical in posterior space but yield different hypervolume gains. The core assumption is that history of observations is strictly necessary to predict the true hypervolume improvement. Break condition: If the objective landscape is extremely smooth or the budget is extremely large, the history-dependence might diminish, potentially reducing the advantage over myopic methods.

### Mechanism 2: Q-Augmented Observation Representation
Instead of processing the entire domain grid, the model represents a step by the candidate's posterior $(\mu, \sigma)$ and the previous best value, augmented by the *Q-value* of the prior step. This recursive augmentation allows the Transformer to infer the "value state" of the optimization without needing the full domain data as input. The core assumption is that the Q-value of the previous step serves as a sufficient statistic for the optimization progress achieved by the history. Break condition: If the Q-value estimation is noisy or diverges during training, the "augmented" signal misleads the policy rather than aiding it.

### Mechanism 3: Demo-Policy-Guided Exploration
The system utilizes a "demo policy" (specifically Expected Hypervolume Improvement, EHVI) to generate a portion of the training trajectories. These high-quality trajectories are stored in a Prioritized Trajectory Replay Buffer (PTRB), allowing the model to learn from successful optimization paths rather than random walks. The core assumption is that rule-based AFs like EHVI, while potentially sub-optimal in the long term, provide a strong prior for "good" sampling regions. Break condition: If the demo policy is biased towards local optima, the agent may struggle to learn globally superior non-myopic strategies.

## Foundational Learning

- **Pareto Front & Hypervolume Indicator**
  - Why needed here: Unlike single-objective optimization, MOBO aims to find a set of non-dominated solutions (Pareto front). The hypervolume is the reward signal used to train BOFormer.
  - Quick check question: If a new point improves one objective but degrades another, how does the hypervolume metric determine if it is a "good" sample?

- **Gaussian Process (GP) Surrogate Model**
  - Why needed here: BOFormer is not an end-to-end model-based optimizer; it relies on a GP to provide the posterior mean $\mu$ and variance $\sigma$ which form the input observation features.
  - Quick check question: What happens to the observation representation if the GP uncertainty $\sigma$ collapses to zero too quickly?

- **Generalized Deep Q-Network (DQN)**
  - Why needed here: This is the theoretical basis for using RL in a non-Markovian setting. It extends the standard Bellman equation to depend on history rather than just the current state.
  - Quick check question: Why does the loss function (Eq. 4) require maximizing over actions $a'$ for the *next* history step $h'$?

## Architecture Onboarding

- **Component map:** Candidate points $x$ -> GP Posteriors ($\mu, \sigma$) -> Q-augmented observation representation -> Transformer encoder -> Q-value output -> Action selection
- **Critical path:** 1. Query GP for posteriors of candidate batch. 2. Construct sequence input: $h_t = \{(\mu, \sigma, y^*, t)_{1..t-1}, Q_{target}\}$. 3. Forward pass through Transformer to get $Q_\theta(h_t, o_t(x))$. 4. Select $x = \text{argmax}_x Q$. 5. If training: Store trajectory in PTRB and sample batches mixing Demo and RL data.
- **Design tradeoffs:**
  - **Sequence Length ($w$):** Paper uses $w=31$. Larger $w$ captures more history (better for identifiability) but increases inference latency.
  - **Demo Rate ($r_{demo}$):** Set to 0.01 in experiments. Higher rates stabilize training but might bias the agent toward the myopic behavior of the demo policy.
- **Failure signatures:**
  - **Identifiability Collapse:** If sequence length is set to 1 ($w=1$), performance drops significantly (Figure 4), reverting to standard Markovian limitations.
  - **Training Divergence:** Appendix C.3 notes that removing temporal information (step index $j/t$) or positional encoding causes training loss to diverge.
- **First 3 experiments:**
  1. **Sanity Check (Synthetic):** Run BOFormer vs. qNEHVI on a 2D Branin-Currin function. Verify that BOFormer achieves higher hypervolume after 100 steps (Table 1 benchmarks).
  2. **Ablation on History:** Set window size $w=1$ (Markovian) vs. $w=31$ (Non-Markovian). Confirm the performance gap to validate the "Identifiability" hypothesis.
  3. **Transfer Test:** Train on random GP samples, then test immediately on the HPO-3DGS task (zero-shot). Verify that the domain-agnostic representation allows cross-domain transfer without fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can BOFormer maintain its efficiency in high-dimensional continuous domains without relying on Sobol grid approximations?
- Basis in paper: [Explicit] Remark 4.2 states that using Sobol grids to approximate the maximum Q-value "could be a limitation in high-dimensional tasks," even though Appendix D.3 shows competitiveness up to 100D.
- Why unresolved: Sobol sequences become sparse in very high dimensions, potentially failing to identify optimal action points or increasing the computational cost of the search.
- What evidence would resolve it: An evaluation on domains significantly exceeding 100 dimensions using gradient-based optimization for action selection, comparing sample efficiency and wall-clock time against rule-based baselines.

### Open Question 2
- Question: How can the Generalized DQN framework be extended to constrained multi-objective Bayesian optimization (CMOBO)?
- Basis in paper: [Explicit] Section 2.1 reviews PESMO for constrained settings, but the paper's methodology and experiments are limited to unconstrained synthetic and HPO tasks.
- Why unresolved: Constraints introduce history-dependent feasibility indicators that may exacerbate the hypervolume identifiability issue or require modifications to the reward signal and observation representation.
- What evidence would resolve it: A modification of BOFormer that incorporates feasibility probabilities into the Q-augmented representation, validated on standard constrained test problems (e.g., C-DTLZ).

### Open Question 3
- Question: Can a single BOFormer model achieve zero-shot transfer across varying numbers of objective functions ($K$) without fine-tuning?
- Basis in paper: [Explicit] Appendix C.2 notes that while transfer is possible, it requires reshaping the linear embedding layer and fine-tuning (approx. 400 episodes) because the input dimension depends on $K$.
- Why unresolved: The current architecture fixes the input layer size based on the number of objectives, preventing a single pre-trained policy from handling different $K$ natively.
- What evidence would resolve it: A permutation-invariant architecture (e.g., using Deep Sets for objective processing) that accepts variable $K$ inputs without retraining or weight modifications.

## Limitations

- The empirical ablation studies only test synthetic GP functions, leaving the effect of non-Markovian identifiability on real-world high-dimensional problems unverified
- Cross-domain transfer claims are based solely on synthetic-to-HPO-3DGS transfer, without testing transfer across completely different problem types
- The prioritization mechanism in PTRB using absolute TD-error may be sensitive to scaling and could lead to catastrophic forgetting of low-error but valuable trajectories

## Confidence

- **High Confidence**: The core architectural contribution (transformer + Q-augmentation) and its implementation details
- **Medium Confidence**: The superiority claims over baselines, as these rely on comparisons that may not account for all implementation details of competing methods
- **Medium Confidence**: The transferability claims, as the testing scenarios are limited to the specific synthetic-to-HPO transfer case

## Next Checks

1. Conduct an ablation study varying the demo rate $r_{demo}$ from 0.001 to 0.1 to quantify its impact on both training stability and final performance
2. Test cross-domain transfer from HPO-3DGS to a completely different multi-objective optimization problem (e.g., neural architecture search or drug discovery) to validate the domain-agnostic claims
3. Implement and compare against the direct sequence modeling approach mentioned as the alternative to Q-augmentation, using the same computational budget, to isolate the contribution of the proposed representation