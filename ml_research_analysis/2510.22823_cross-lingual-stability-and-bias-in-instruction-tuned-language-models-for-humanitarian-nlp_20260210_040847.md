---
ver: rpa2
title: Cross-Lingual Stability and Bias in Instruction-Tuned Language Models for Humanitarian
  NLP
arxiv_id: '2510.22823'
source_url: https://arxiv.org/abs/2510.22823
tags:
- languages
- language
- across
- arxiv
- cross-lingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study provides the first systematic evaluation of cross-lingual
  robustness in large language models (LLMs) for human rights violation detection.
  Across 78,000 multilingual inferences in seven languages, the research compares
  four commercial instruction-aligned models (GPT-4.1-mini, Claude Sonnet 4, DeepSeek-V3,
  Gemini-Flash-2.0) with two open-weight models (LLaMA-3-8B, Mistral-7B) using novel
  metrics including Calibration Deviation (CD), Language Robustness Score (LRS), and
  Language Stability Score (LSS).
---

# Cross-Lingual Stability and Bias in Instruction-Tuned Language Models for Humanitarian NLP

## Quick Facts
- arXiv ID: 2510.22823
- Source URL: https://arxiv.org/abs/2510.22823
- Reference count: 0
- This study demonstrates that multilingual alignment—not model scale—determines cross-lingual stability in human rights violation detection across 7 languages.

## Executive Summary
This study systematically evaluates cross-lingual robustness in large language models for human rights violation detection across 78,000 multilingual inferences in seven languages. The research compares four commercial instruction-aligned models with two open-weight models using novel metrics including Calibration Deviation, Language Robustness Score, and Language Stability Score. Results show that aligned models maintain near-invariant accuracy and balanced calibration across languages, while open-weight models exhibit significant prompt-language sensitivity and calibration drift, particularly in low-resource languages like Lingala and Burmese.

## Method Summary
The study evaluates six language models (four commercial instruction-aligned models and two open-weight models) on binary classification of human rights violations across seven prompt languages while keeping source text unchanged. Using two datasets—Russian/Ukrainian Telegram posts and English reports on human rights defenders—the researchers perform few-shot inference with professionally translated prompts. They compute F1 scores per model-language pair and derive novel metrics including Calibration Deviation (CD), Language Robustness Score (LRS), and Language Stability Score (LSS) through McNemar tests. All experiments were run on Google Colab with NVIDIA A100 GPUs.

## Key Results
- Aligned models maintain near-invariant accuracy (F1 variance < 0.03) and balanced calibration across languages
- Open-weight models exhibit significant prompt-language sensitivity (F1 variance > 0.15) and calibration drift
- In low-resource languages like Lingala and Burmese, aligned models preserve strong performance (F1 > 0.78), whereas open-weight models drop below 0.60 with inflated false-positive rates (> 0.8)

## Why This Works (Mechanism)
None provided

## Foundational Learning
- **Multilingual alignment**: Models trained to follow instructions across multiple languages show more stable cross-lingual performance
  - Why needed: Critical for humanitarian NLP where reliable performance across languages is essential
  - Quick check: Compare F1 variance across languages for aligned vs unaligned models

- **Calibration Deviation metric**: Measures imbalance between false-positive and false-negative rates across languages
  - Why needed: Traditional accuracy metrics mask systematic biases in different languages
  - Quick check: Calculate CD = mean|FPR-FNR| across all languages for a given model

- **Language Robustness Score**: Quantifies stability by comparing standard deviation to mean performance across languages
  - Why needed: Provides single metric to evaluate cross-lingual consistency
  - Quick check: Compute LRS = 1 - (σ/μ) where σ is standard deviation and μ is mean F1 across languages

## Architecture Onboarding

**Component map**: Prompt template -> Model API/query -> Classification output -> Metric calculation -> Stability analysis

**Critical path**: The most important decision is choosing aligned vs unaligned models, as this determines cross-lingual stability more than model scale. The prompt translation quality and verification process is critical for valid comparisons.

**Design tradeoffs**: The study prioritizes cross-lingual stability over model cost, finding that open-weight models cannot reliably substitute for aligned commercial APIs in high-stakes multilingual contexts despite lower cost.

**Failure signatures**: Open-weight models show high F1 variance (> 0.15) across languages, inflated false-positive rates (> 0.8) in low-resource languages, and significant calibration drift (CD > 0.25).

**First experiments**: 1) Compare F1 scores across languages for a single model to establish baseline variance, 2) Calculate CD and LRS metrics for each model to quantify stability, 3) Perform McNemar test between English and other languages to establish LSS.

## Open Questions the Paper Calls Out

**Open Question 1**: Can fine-tuning open-weight models on multilingual instruction data close the cross-lingual stability gap with commercially aligned models? The study attributes performance differences to "multilingual alignment" rather than scale, but tests only base open-weight checkpoints without fine-tuning interventions.

**Open Question 2**: Does the alignment stability advantage persist across morphologically rich African and Indigenous languages beyond the two low-resource languages tested? The authors note that language coverage remains incomplete for morphologically rich or underrepresented African and Indigenous languages.

**Open Question 3**: How does cross-lingual stability perform under multi-label human rights classification with overlapping violation categories? The study is limited to binary human-rights classification tasks, while real-world HRV detection often involves multiple simultaneous violation types.

**Open Question 4**: How does cross-lingual stability degrade under domain shift or time-evolving conflict reporting patterns? Model outputs were evaluated on fixed test sets, so potential effects of domain drift or time-evolving data are not yet quantified.

## Limitations
- The study compares only six models (four commercial APIs and two open-weight variants), which may not capture the full spectrum of alignment quality
- Metric definitions rely on specific assumptions, including treating false-positive and false-negative rate differences as symmetric errors without justification
- The prompt translation process lacks detail on methodology and potential cultural/contextual shifts that could affect model outputs independently of alignment quality

## Confidence
- High confidence in comparative results between aligned commercial models versus open-weight models within this specific task and dataset
- Medium confidence in generalizability of alignment quality conclusions across different humanitarian NLP tasks
- Low confidence in universal applicability of proposed metrics as comprehensive measures of cross-lingual robustness without broader validation

## Next Checks
1. Replicate analysis using additional aligned and unaligned models across a broader range of model sizes (1B-70B parameters) to verify scale-independence
2. Apply CD, LRS, and LSS metrics to a different multilingual humanitarian task to test metric robustness and generalizability
3. Conduct error analysis on false-positive cases in low-resource languages to determine whether errors stem from alignment quality, prompt translation issues, or inherent task ambiguity