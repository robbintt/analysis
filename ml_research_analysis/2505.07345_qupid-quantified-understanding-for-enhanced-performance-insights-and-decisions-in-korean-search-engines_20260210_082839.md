---
ver: rpa2
title: 'QUPID: Quantified Understanding for Enhanced Performance, Insights, and Decisions
  in Korean Search Engines'
arxiv_id: '2505.07345'
source_url: https://arxiv.org/abs/2505.07345
tags:
- relevance
- search
- document
- query
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces QUPID, a relevance labeling approach that
  combines two architecturally distinct small language models (SLMs): a generative
  SLM for contextual reasoning and an embedding-based SLM for semantic similarity.
  The heterogeneous ensemble outperforms both large language models and single-architecture
  SLM ensembles on Korean search relevance tasks, achieving 67% higher Cohen''s Kappa
  (0.646 vs 0.387) and 60x faster inference times (62ms vs 3258ms).'
---

# QUPID: Quantified Understanding for Enhanced Performance, Insights, and Decisions in Korean Search Engines

## Quick Facts
- arXiv ID: 2505.07345
- Source URL: https://arxiv.org/abs/2505.07345
- Authors: Ohjoon Kwon; Changsu Lee; Jihye Back; Lim Sun Suk; Inho Kang; Donghyeon Jeon
- Reference count: 12
- Achieves 67% higher Cohen's Kappa (0.646 vs 0.387) and 60x faster inference (62ms vs 3258ms) than GPT-4o for Korean search relevance tasks

## Executive Summary
QUPID introduces a novel relevance labeling approach for Korean search engines that combines a generative small language model (QUPID_GEN) with an embedding-based small language model (QUPID_EMB) in a heterogeneous ensemble. The approach achieves superior relevance judgment accuracy compared to both large language models and single-architecture SLM ensembles, with 67% higher Cohen's Kappa and 60x faster inference times. The system demonstrates practical applicability across multiple search engine workflows including filtering low-quality results, evaluating query rewriting, assessing snippet quality, and improving ranking, with a 1.9% nDCG@5 improvement in production settings.

## Method Summary
QUPID uses two architecturally distinct SLMs: a generative model that outputs token probabilities over relevance labels, and an embedding model that computes semantic similarity via mean-pooled hidden states. Both models are fine-tuned on ~1M Korean query-document pairs with synthetic hard negatives. The generative model uses log probabilities converted to weighted scores, while the embedding model uses mean pooling followed by linear classification. The final relevance score combines both models with validation-tuned weights. The approach achieves high accuracy with minimal latency through short 10-token prompts compared to 350+ tokens for zero-shot LLMs.

## Key Results
- 67% higher Cohen's Kappa (0.646 vs 0.387) compared to GPT-4o on Korean relevance tasks
- 60x faster inference (62ms vs 3258ms) using fine-tuned models with 10-token prompts
- 1.9% nDCG@5 improvement in production search ranking
- Strong performance across diverse datasets: Web-D (κ=0.783), Naver Shopping (κ=0.646), and synthetic test sets

## Why This Works (Mechanism)

### Mechanism 1
Heterogeneous architectural ensembles outperform homogeneous ensembles for relevance assessment. A generative SLM provides explicit contextual reasoning via token probabilities, while an embedding-based SLM captures implicit semantic similarity via dense vectors. Weighted score combination leverages complementary error patterns. Evidence: Homogeneous ensembles achieve κ=0.569 and κ=0.607; heterogeneous ensemble achieves κ=0.646. Break condition: If correlation between generative and embedding errors is high, ensemble gains diminish.

### Mechanism 2
Token-probability calibration provides more informative relevance scores than direct token sampling. The generative model outputs log probabilities over three label tokens, converted via softmax to continuous scores weighted by label values. Evidence: High temperature (3.0) during inference yields better results in fine-tuned scenarios. Break condition: If calibration is poorly aligned with ground truth, threshold selection for binary decisions will fail.

### Mechanism 3
Synthetic hard-negative generation improves model robustness for distinguishing superficially similar but irrelevant documents. Mistral-Large generates documents containing query keywords but with different semantic context. Evidence: Strong performance on Web-D dataset (κ=0.783) suggests robustness to diverse documents. Break condition: If synthetic negatives have distributional artifacts, model may overfit to generation patterns rather than learn true relevance boundaries.

## Foundational Learning

- **Token-probability vs. sampling-based classification**: Why needed: QUPID_GEN uses probability distributions over label tokens rather than sampled outputs; understanding calibration is essential for interpreting scores. Quick check: Can you explain why a softmax over three label tokens might preserve more information than taking argmax of sampled output?

- **Mean pooling for sequence representations**: Why needed: QUPID_EMB aggregates hidden states via mean pooling before linear classification; alternative pooling strategies have different inductive biases. Quick check: Given a 1024-token sequence, what is the output dimension after mean pooling if hidden size is 4096?

- **Weighted ensemble calibration**: Why needed: Final relevance scores depend on weights tuned on validation data; improper weighting can negate ensemble benefits. Quick check: If QUPID_GEN systematically over-scores short documents and QUPID_EMB under-scores them, how should you adjust weights?

## Architecture Onboarding

- **Component map**: Data curation (real collection + hard-negative generation) -> Model fine-tuning (separate for GEN and EMB) -> Validation-based weight tuning -> Deployment with vLLM serving (10-token system prompt)

- **Critical path**: 1. Data curation (real collection + hard-negative generation) -> 2. Model fine-tuning (separate for GEN and EMB) -> 3. Validation-based weight tuning -> 4. Deployment with vLLM serving

- **Design tradeoffs**: Latency vs. accuracy: Single-model inference is faster but heterogeneous ensemble provides +0.088 κ improvement. Prompt length: Fine-tuned models use 10 tokens vs. 350+ for zero-shot LLMs. Temperature: High temperature (3.0) preserves probability diversity but may increase variance.

- **Failure signatures**: Low κ on out-of-domain queries. Embedding model underperforms on very long documents (>1024 tokens truncated). Generative model overconfident on ambiguous queries.

- **First 3 experiments**: 1. Reproduce Table 2 on your domain: Compare homogeneous vs. heterogeneous ensembles. 2. Ablation on hard-negative ratio: Train with 0%, 10%, 20% synthetic negatives. 3. Latency profiling under load: Measure p50/p99 latency at 100, 1000, 10000 QPS with vLLM.

## Open Questions the Paper Calls Out
None

## Limitations
- Heterogeneous ensemble approach depends on architectural diversity assumptions lacking direct corpus validation
- Hard-negative generation strategy introduces synthetic data without systematic evaluation of contribution
- Cross-lingual generalization remains uncertain with limited analysis of failure modes

## Confidence

**High Confidence**: 60x latency improvement claim (62ms vs 3258ms) with clear methodological explanation.

**Medium Confidence**: Overall performance superiority (67% higher κ, 0.646 vs 0.387) demonstrated across multiple datasets.

**Low Confidence**: Mechanism explanations for heterogeneous ensembles and token-probability calibration lack direct empirical support.

## Next Checks

1. **Error correlation analysis**: Analyze correlation between QUPID_GEN and QUPID_EMB errors on held-out test set before deployment.

2. **Synthetic negative ablation**: Systematically vary proportion of hard-negative synthetic data (0%, 10%, 20%, 30%) and measure performance impact.

3. **Cross-lingual robustness testing**: Evaluate on balanced multilingual dataset with systematic error analysis across languages.