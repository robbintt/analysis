---
ver: rpa2
title: 'Do Not Waste Your Rollouts: Recycling Search Experience for Efficient Test-Time
  Scaling'
arxiv_id: '2601.21684'
source_url: https://arxiv.org/abs/2601.21684
tags:
- experience
- search
- reasoning
- scaling
- rollouts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Recycling Search Experience (RSE), a self-guided
  inference strategy that addresses the inefficiency of memoryless test-time search
  in large language models. Existing search methods treat rollouts as disposable,
  causing models to repeatedly re-derive intermediate conclusions and revisit dead
  ends.
---

# Do Not Waste Your Rollouts: Recycling Search Experience for Efficient Test-Time Scaling

## Quick Facts
- arXiv ID: 2601.21684
- Source URL: https://arxiv.org/abs/2601.21684
- Authors: Xinglin Wang; Jiayi Shi; Shaoxiong Feng; Peiwen Yuan; Yiwei Li; Yueqi Zhang; Chuyi Tan; Ji Zhang; Boyuan Pan; Yao Hu; Kan Li
- Reference count: 40
- Key outcome: RSE achieves up to 74.4% pass@1 on HMMT24 and 83.9% on HMMT25 while using 30% fewer rollouts than baselines

## Executive Summary
This paper addresses the inefficiency of memoryless test-time search in large language models by introducing Recycling Search Experience (RSE), a self-guided inference strategy that transforms search into a cumulative process. Unlike existing search methods that treat rollouts as disposable, RSE maintains a shared experience bank that distills raw trajectories into verified intermediate conclusions and failure patterns. This enables both positive recycling of correct partial solutions to shortcut derivations and negative recycling of dead ends to prune search space. Theoretically grounded and empirically validated, RSE consistently outperforms strong baselines like PaCoRe and Self-Refine on challenging math benchmarks including HMMT24, HMMT25, IMO-Bench, and HLE.

## Method Summary
RSE operates through an iterative framework where each iteration generates multiple search trajectories from different prompts. During each iteration, the model generates multiple rollouts (self-contained reasoning chains), which are then distilled into a shared experience bank containing both positive experiences (verified intermediate conclusions) and negative experiences (failure patterns). For subsequent iterations, RSE uses the experience bank to guide search by either reusing verified conclusions to shortcut derivations or avoiding previously encountered dead ends. The experience bank is continuously updated with new distilled experiences, creating a cumulative learning process. Unlike memoryless approaches that treat each search as independent, RSE leverages the collective wisdom from all previous iterations to improve search efficiency and solution quality.

## Key Results
- RSE achieves 74.4% pass@1 on HMMT24 and 83.9% on HMMT25, outperforming strong baselines
- Demonstrates 30% reduction in required rollouts compared to memoryless approaches
- Maintains performance gains across deeper iterations and wider search budgets
- Shows consistent improvements across multiple math benchmarks including IMO-Bench and HLE

## Why This Works (Mechanism)
RSE works by converting the traditionally memoryless search process into a cumulative learning experience. By maintaining a shared experience bank that captures both successful intermediate conclusions and failure patterns from previous iterations, the model avoids repeatedly re-deriving the same intermediate steps or revisiting dead ends. This recycling mechanism effectively compresses the search space, allowing the model to focus computational resources on exploring genuinely new solution paths rather than redundant exploration. The theoretical analysis demonstrates that this approach improves the probability of finding correct solutions compared to independent sampling, while empirical results confirm substantial efficiency gains in practice.

## Foundational Learning
- **Test-time scaling**: Performing multiple inference steps beyond the initial response to improve answer quality; needed to understand why search-based approaches are valuable for complex reasoning tasks
- **Memoryless search**: Traditional search methods that treat each rollout as independent without leveraging past experiences; needed to understand the baseline inefficiency that RSE addresses
- **Experience distillation**: The process of extracting key insights (positive/negative experiences) from raw reasoning trajectories; needed to understand how RSE compresses and reuses search information
- **Experience bank**: A shared repository of verified intermediate conclusions and failure patterns; needed to understand the core mechanism that enables cumulative learning
- **Positive/negative recycling**: Reusing verified conclusions to shortcut derivations and avoiding failure patterns to prune search space; needed to understand the dual benefits of the experience mechanism
- **Verification in search**: The process of confirming intermediate conclusions are correct; needed to understand how RSE ensures the reliability of recycled experiences

## Architecture Onboarding

### Component Map
LLM -> Multiple rollouts -> Experience distillation -> Experience bank -> Guided search -> Updated experience bank -> Repeat

### Critical Path
1. LLM generates multiple rollouts from diverse prompts
2. Rollouts are distilled into verified experiences
3. Experiences are stored in shared experience bank
4. Bank guides subsequent search iterations
5. New rollouts are evaluated and distilled
6. Bank is updated with new experiences

### Design Tradeoffs
- **Experience bank size vs. relevance**: Larger banks capture more patterns but may include outdated or less relevant experiences
- **Distillation accuracy vs. speed**: More thorough verification improves experience quality but increases computation time
- **Memory overhead vs. performance gain**: Maintaining the experience bank requires additional memory but provides search efficiency benefits

### Failure Signatures
- Experience bank becomes saturated with low-quality experiences leading to degraded guidance
- Positive recycling introduces incorrect shortcuts if verification is imperfect
- Negative recycling over-prunes search space, missing valid solution paths
- Experience drift occurs when accumulated noise overwhelms useful patterns

### Three First Experiments
1. **Ablation study**: Compare RSE performance with and without experience bank to quantify the contribution of positive/negative recycling
2. **Experience bank analysis**: Measure how bank size and composition affect search efficiency and solution quality across different problem types
3. **Verification robustness**: Test RSE performance with varying levels of verification accuracy to understand the impact of imperfect self-distillation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the accumulation of imperfect self-distilled experience (approx. 15-19% error rate) lead to "experience drift" or performance collapse in search horizons significantly deeper than the tested 6 iterations?
- Basis in paper: Table 11 shows distilled experiences have only ~81-84% accuracy; Theoretical analysis relies on Assumption A.4 ("Perfect verification"), which is violated empirically
- Why unresolved: Experiments are limited to 3 primary iterations, potentially masking the effects of compounding errors over very long search processes
- What evidence would resolve it: Evaluating RSE on tasks requiring >10 iterations and analyzing the correlation between cumulative noise in the Experience Bank and task failure rates

### Open Question 2
- Question: Can the RSE framework effectively generalize to domains lacking formal verification (e.g., creative writing, agentic planning) where "Verified Propositions" are subjective?
- Basis in paper: Evaluation is restricted to mathematical reasoning benchmarks (Sec 4.1); The distillation prompt relies on "mathematical axioms" to identify truth anchors
- Why unresolved: It is unclear if the binary distinction between "Positive/Negative" experiences holds when valid reasoning paths are not axiomatically verifiable
- What evidence would resolve it: Applying RSE to open-ended benchmarks like code generation or web navigation with adapted distillation criteria for heuristic experiences

### Open Question 3
- Question: Does hybridizing RSE with a weak external Process Reward Model (PRM) to sanitize the Experience Bank yield superior performance compared to the purely self-guided approach?
- Basis in paper: The Introduction positions RSE specifically as a "self-guided" alternative to methods requiring external supervision like PRMs (Sec 1)
- Why unresolved: The paper dichotomizes self-guided and externally-supervised methods but does not explore if RSE's efficiency could benefit from external verification of the bank itself
- What evidence would resolve it: Ablation studies comparing the quality of the Experience Bank when filtered by an external PRM versus the proposed self-guided distillation

## Limitations
- Evaluation is restricted to mathematical reasoning benchmarks, limiting generalizability to other domains
- The analysis of experience bank maintenance costs and memory overhead is not comprehensive
- Comparison with some emerging test-time scaling approaches that use memory mechanisms is incomplete
- The long-term stability of the experience bank in extended usage scenarios remains unexplored

## Confidence

**High Confidence**: The core algorithmic contribution of RSE - maintaining a shared experience bank for positive and negative recycling - is well-defined and theoretically grounded. The theoretical proof showing improved probability of finding correct solutions compared to independent sampling is sound and the empirical improvements on the tested math benchmarks are substantial and consistent across multiple datasets.

**Medium Confidence**: The claim that RSE demonstrates superior scalability across deeper iterations and wider search budgets is supported by the experimental results, but the analysis of how performance scales with problem difficulty or search depth could be more comprehensive. The compute efficiency advantage (30% fewer rollouts) is demonstrated but would benefit from more detailed breakdown of memory and processing overhead.

**Low Confidence**: The assertion that RSE would maintain similar performance advantages in non-mathematical domains is speculative, as the paper doesn't provide evidence beyond the math benchmarks. The long-term stability and maintenance requirements for the experience bank in extended usage scenarios remain unexplored.

## Next Checks
1. **Cross-domain validation**: Evaluate RSE on non-mathematical benchmarks such as code generation tasks (e.g., HumanEval), multi-modal reasoning problems, or natural language inference tasks to assess generalizability beyond mathematical reasoning.

2. **Memory efficiency analysis**: Conduct a detailed study measuring the memory overhead of maintaining the experience bank across different problem types and search depths, including analysis of optimal experience bank size and decay rate parameters.

3. **Comparative analysis with emerging methods**: Compare RSE against other recent test-time scaling approaches that incorporate memory mechanisms, such as those using retrieval-augmented generation or external memory stores, to better position its relative performance and efficiency advantages.