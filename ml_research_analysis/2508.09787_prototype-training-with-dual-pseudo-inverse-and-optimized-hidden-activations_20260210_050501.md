---
ver: rpa2
title: Prototype Training with Dual Pseudo-Inverse and Optimized Hidden Activations
arxiv_id: '2508.09787'
source_url: https://arxiv.org/abs/2508.09787
tags:
- ridge
- training
- test
- prototype
- hidden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Proto-PINV+H is a fast training paradigm that combines closed-form
  weight computation with gradient-based optimisation of synthetic prototypes, hidden
  activations, and soft labels. At each iteration, all weight matrices are recomputed
  via ridge-regularised pseudo-inverse solves, while only the prototypes are updated
  with Adam.
---

# Prototype Training with Dual Pseudo-Inverse and Optimized Hidden Activations

## Quick Facts
- arXiv ID: 2508.09787
- Source URL: https://arxiv.org/abs/2508.09787
- Authors: Mauro Tucci
- Reference count: 4
- Primary result: Proto-PINV+H reaches 97.8% (MNIST) and 89.3% (Fashion-MNIST) test accuracy in 3.9–4.5s using ~130k trainable parameters

## Executive Summary
Proto-PINV+H is a fast training paradigm that combines closed-form weight computation with gradient-based optimisation of synthetic prototypes, hidden activations, and soft labels. At each iteration, all weight matrices are recomputed via ridge-regularised pseudo-inverse solves, while only the prototypes are updated with Adam. The trainable degrees of freedom are thus shifted from weight space to data/activation space. On MNIST and Fashion-MNIST, the method reaches 97.8% and 89.3% test accuracy in 3.9–4.5s using approximately 130k trainable parameters and 250 epochs on an RTX 5060 (16GB). Multi-layer extensions, learnable ridge parameters, and optional PCA/PLS projections are provided, along with theory linking prototype matrix conditioning to generalisation. The approach yields favourable accuracy–speed–size trade-offs against ELM, random-feature ridge, and shallow MLPs trained by back-propagation.

## Method Summary
The method trains neural networks by iteratively recomputing all weight matrices in closed form via ridge-regularised pseudo-inverse solves while updating only the prototypes (input Xp, hidden activation Hp, and label Yp) using Adam. With Np=150 prototypes and h=512 hidden units, it achieves competitive accuracy on MNIST and Fashion-MNIST in 3.9–4.5s per run. The key novelty is optimizing hidden activations Hp directly rather than computing them from inputs, providing greater expressive power than prototype-input-only methods. Weight matrices W1 and W2 are computed at each step as W1 = pinv_ridge(Xp, λ1) @ Hp and W2 = pinv_ridge([1, σ(Hp)], λ2) @ Yp_soft. The method includes optional learnable ridge parameters, PCA/PLS projections, and theoretical analysis linking prototype matrix conditioning to generalization.

## Key Results
- Achieves 97.8% test accuracy on MNIST and 89.3% on Fashion-MNIST
- Training completes in 3.9–4.5s per run (vs 100–150s for MLP-BP)
- Uses ~130k trainable parameters vs millions for standard networks
- Outperforms ELM and random-feature ridge methods while being faster than backprop

## Why This Works (Mechanism)

### Mechanism 1: Closed-Form Weight Recomputation via Ridge-Regularized Pseudo-Inverse
Instead of gradient descent through weights, solve W = (A^⊤A + λI)^(-1)A^⊤B at each step. The ridge term (λ > 0) ensures numerical stability via Cholesky decomposition. Gradients flow only through prototype variables (Xp, Hp, Yp), not through weight matrices directly.

### Mechanism 2: Optimizing Hidden Activations (Not Just Inputs/Labels)
Hp is directly optimized via Adam, not computed as σ(Xp·W1). This allows the network to learn "what the hidden layer should represent" rather than being constrained by fixed random projections (as in ELM). The dual pseudo-inverse couples Xp→Hp and Hp→Yp pathways.

### Mechanism 3: Trainable Parameter Reduction (Np ≪ N)
With Np = 150 vs N = 60,000, gradient computation and storage scale with Np, not N. Each epoch solves O(Nph²) + O(h³) for ridge systems, independent of dataset size.

## Foundational Learning

- **Concept: Ridge-regularized least squares (Tikhonov regularization)**
  - Why needed here: All weight computation depends on solving (A^⊤A + λI)^(-1)A^⊤B. Without understanding ridge regression, you cannot diagnose numerical instability or tune λ.
  - Quick check question: If λ = 0 and Xp has rank < d+1, what happens to W1 computation?

- **Concept: Pseudo-inverse and matrix conditioning**
  - Why needed here: Lemma 1 and Proposition 1 link generalization to κ(Xp) = σ_max/σ_min. Poor conditioning amplifies gradient noise.
  - Quick check question: How does the condition number of Xp affect the bound on ||ΔW1||_F?

- **Concept: Gradient flow through linear solves (implicit differentiation)**
  - Why needed here: The method backpropagates through G^(-1)R without storing full Jacobians. Modern autodiff handles this, but debugging requires understanding the implicit function theorem application.
  - Quick check question: When computing ∂L/∂Xp, does the gradient pass through W1 explicitly or via the solve operation?

## Architecture Onboarding

- **Component map**:
  - Xp ∈ R^(Np×(d+1)): Synthetic input prototypes (trainable, with bias column)
  - Hp ∈ R^(Np×h): Hidden activation prototypes (trainable, key novelty)
  - Yp ∈ R^(Np×k): Soft label prototypes (trainable, optionally temperature-scaled)
  - W1, W2: Derived weights (non-trainable, recomputed each iteration via ridge solve)
  - λ1, λ2: Ridge parameters (optionally learnable via softplus)
  - λ3: Explicit Frobenius penalty on weights (distinct from ridge terms)

- **Critical path**:
  1. Initialize Xp, Hp, Yp (balanced one-hot for Yp recommended)
  2. Compute W1 = pinv_ridge(Xp, λ1) @ Hp
  3. Compute W2 = pinv_ridge([1, σ(Hp)], λ2) @ Yp_soft
  4. Forward pass training data: logits = [1, σ([1, X] @ W1)] @ W2
  5. Compute loss (CE + λ3 penalty)
  6. Backprop to Xp, Hp, Yp only; Adam step
  7. Repeat from step 2

- **Design tradeoffs**:
  - More prototypes (higher Np) → better accuracy but slower per-epoch time
  - Higher hidden width (h) → diminishing returns after ~768 per ablations
  - Learnable λ2 → +0.2–0.3 pp on Fashion-MNIST; λ1 often better fixed
  - Dropout on Hp → may reduce final accuracy at p ≥ 0.1

- **Failure signatures**:
  - NaN in W1/W2: λ too small or Xp/Hp rank-deficient; increase ridge or check initialization
  - Accuracy plateaus early: Yp initialization may be unbalanced; use one-hot per class
  - Fashion-MNIST underperforms MNIST significantly: increase Np or enable learnable λ2
  - Slow convergence: try cosine scheduler with 20-epoch warm-up

- **First 3 experiments**:
  1. **Sanity check**: Np=150, h=512, fixed λ1=λ2=1e-4, no PCA, 50 epochs on MNIST. Target: >95% test accuracy. Verifies closed-form solve and gradient flow.
  2. **Ablation on hidden activation optimization**: Compare (a) trainable Hp vs (b) fixed random Hp (ELM-style) vs (c) Hp computed as σ(Xp·W_random). Keep other settings identical. Quantifies contribution of Mechanism 2.
  3. **Scaling test**: Run Np ∈ {50, 100, 150, 200} on Fashion-MNIST with PCA d'=400. Plot accuracy vs Np to identify capacity ceiling for harder dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the closed-form pseudo-inverse framework be effectively extended to convolutional architectures?
- Basis in paper: [explicit] "Limitations include vectorised inputs (convolutional stems remain future work)..."
- Why unresolved: The current method relies on flattening inputs into design matrices for ridge-regularised solves; it is unclear how to maintain the analytic weight computation while respecting the spatial structure and weight sharing inherent in convolutional layers.
- What evidence would resolve it: A derivation of a convolutional ridge solution or a hybrid architecture where convolutional features are fed into the prototype framework, benchmarked on image datasets like CIFAR-10.

### Open Question 2
- Question: Does numerical error accumulation prevent the scaling of this method to very deep architectures?
- Basis in paper: [explicit] "Limitations include... potential error accumulation in very deep analytic stacks..."
- Why unresolved: The paper empirically validates shallow networks and provides a theoretical stability sketch for single layers, but does not demonstrate if the recursive closed-form solves remain stable or degrade as depth increases significantly.
- What evidence would resolve it: Training curves and accuracy metrics for networks with 5-10+ analytic layers, comparing stability against standard back-propagation under identical conditions.

### Open Question 3
- Question: Does the prototype training paradigm generalise effectively to regression and multi-label classification tasks?
- Basis in paper: [explicit] "Future directions include... extensions to regression and multi-label settings..."
- Why unresolved: The experimental evaluation and the primary loss function (Cross Entropy) are designed for multi-class single-label classification. The interaction between soft labels $Y_p$ and continuous targets or multi-hot vectors in the ridge solution is unexplored.
- What evidence would resolve it: Application of Proto-PINV+H to standard regression benchmarks (e.g., UCI datasets) or multi-label datasets (e.g., CelebA), replacing CE loss with MSE or Binary Cross Entropy.

### Open Question 4
- Question: Can tighter generalisation bounds be derived that directly correlate the condition number of prototype matrices to test error?
- Basis in paper: [explicit] "...theory linking the condition number of prototype matrices to generalisation... Future directions include... tighter generalisation analyses."
- Why unresolved: The paper provides a proposition linking condition number $\kappa(X_p)$ to error, but frames it as a theoretical sketch. It remains to be seen if this relationship is predictive enough to guide architecture search or regularisation tuning without empirical trial-and-error.
- What evidence would resolve it: A formal proof of a bound that empirically tracks the generalisation gap across diverse datasets and prototype initializations.

## Limitations
- The method relies on prototype count (Np) and hidden width (h) trade-offs that may not generalize to more complex datasets beyond MNIST and Fashion-MNIST
- The ridge parameters (λ1, λ2, λ3) and Adam hyperparameters are not fully specified, which could affect reproducibility and performance claims
- The theoretical generalization bounds depend on prototype matrix conditioning, but empirical validation of this relationship is limited to the studied datasets

## Confidence
- **High confidence**: The closed-form weight computation mechanism and its computational efficiency advantages are well-established through the pseudo-inverse formulation
- **Medium confidence**: The improvement from optimizing hidden activations (Hp) is supported by ablation results but lacks extensive comparison to alternative activation strategies
- **Medium confidence**: The accuracy-speed-size trade-offs are demonstrated on two datasets but may not scale to more complex vision or non-vision tasks

## Next Checks
1. **Ablation on ridge parameters**: Systematically vary λ1, λ2, λ3 (fixed vs learnable) on MNIST to quantify their individual contributions to accuracy and stability
2. **Scalability test**: Apply the method to CIFAR-10 or SVHN with varying Np to determine the capacity ceiling for more complex image classification
3. **Activation function comparison**: Replace ReLU with other activations (Tanh, Leaky ReLU) in the σ(Hp) computation to verify that the reported results are not activation-specific