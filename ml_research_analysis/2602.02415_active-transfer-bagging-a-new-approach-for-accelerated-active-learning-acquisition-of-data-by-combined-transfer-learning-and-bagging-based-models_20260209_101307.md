---
ver: rpa2
title: 'Active Transfer Bagging: A New Approach for Accelerated Active Learning Acquisition
  of Data by Combined Transfer Learning and Bagging Based Models'
arxiv_id: '2602.02415'
source_url: https://arxiv.org/abs/2602.02415
tags:
- subset
- dataset
- learning
- atbagging
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Active Transfer Bagging (ATBagging), a novel
  method for selecting informative and diverse seed subsets to initiate active learning,
  particularly when a related labeled dataset is available. ATBagging combines informativeness
  scores derived from a Bayesian interpretation of bagged ensemble models with heterogeneity
  enforced by determinantal point processes.
---

# Active Transfer Bagging: A New Approach for Accelerated Active Learning Acquisition of Data by Combined Transfer Learning and Bagging Based Models

## Quick Facts
- arXiv ID: 2602.02415
- Source URL: https://arxiv.org/abs/2602.02415
- Reference count: 40
- Primary result: ATBagging improves early active learning performance and area under learning curves by combining informativeness from bagged ensemble models with heterogeneity from determinantal point processes.

## Executive Summary
This paper introduces Active Transfer Bagging (ATBagging), a novel method for selecting informative and diverse seed subsets to initiate active learning, particularly when a related labeled dataset is available. ATBagging combines informativeness scores derived from a Bayesian interpretation of bagged ensemble models with heterogeneity enforced by determinantal point processes. The informativeness scores estimate the information gain of candidate data points by comparing in-bag and out-of-bag predictive distributions, while heterogeneity is imposed via a quality-diversity factorization that incorporates the informativeness scores. ATBagging is evaluated on four real-world datasets covering both target-transfer and feature-shift scenarios. Across seed sizes nseed = 10-100, ATBagging improves or ties early active learning and increases area under the learning-curve relative to alternative seed subset selection methodologies in almost all cases, with strongest benefits in low-data regimes.

## Method Summary
ATBagging is a method for selecting seed subsets for active learning that combines informativeness and heterogeneity. It uses a Random Forest ensemble to estimate information gain for each data point via KL divergence between in-bag and out-of-bag predictive distributions. A determinantal point process (DPP) then samples a diverse subset of size k using a kernel that combines these informativeness scores with Random Fourier Features for diversity. The resulting seed set is used to initialize query-by-committee active learning. The method is evaluated on four real-world datasets covering target-transfer and feature-shift scenarios, comparing against baselines like loss coreset and PCA-based sampling.

## Key Results
- ATBagging achieves higher initial transfer performance (ITP) than baselines across all datasets, with improvements of 10-20% in early learning stages.
- Normalized Area Under Learning Curve (NAULC) is improved in 11 out of 12 dataset-seed size combinations compared to the next best method.
- The method shows strongest benefits in low-data regimes (nseed=10), where diverse informative sampling is most critical.
- Performance degrades on datasets with categorical features due to distance metric limitations in the RFF kernel.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The information gain of a candidate data point can be approximated by the divergence between "in-bag" and "out-of-bag" predictive distributions in a bagged ensemble.
- **Mechanism:** A bagged ensemble (e.g., Random Forest) is trained on the source dataset. For a specific point, the submodels are partitioned: those trained with the point ($M_{ib}$) proxy the posterior, and those without ($M_{oob}$) proxy the prior. The Kullback-Leibler (KL) divergence between their predictions on a test set estimates the point's informativeness.
- **Core assumption:** The ensemble size $M$ is sufficiently large such that valid in-bag and out-of-bag partitions exist for every data point (probabilistically guaranteed for $M \ge 50$).
- **Evidence anchors:**
  - [abstract] "...estimates the informativeness of candidate data point from a Bayesian interpretation of bagged ensemble models by comparing in-bag and out-of-bag predictive distributions..."
  - [Section 2.2] Defines $IG_{Y,X}$ via KL divergence and maps $M_{ib}$ and $M_{oob}$ to posterior and prior predictive distributions respectively.
  - [corpus] No direct evidence in corpus; neighbor papers focus on quantum bagging or feature acquisition, not this specific Bayesian interpretation of OOB error.
- **Break condition:** If the ensemble size is too small or the dataset is extremely dense relative to the bootstrap sampling rate, resulting in empty $M_{oob}$ sets for critical points.

### Mechanism 2
- **Claim:** Selecting a seed set that is both high-value (informative) and diverse (heterogeneous) accelerates early active learning more than maximizing either property alone.
- **Mechanism:** The method uses a Determinantal Point Process (DPP) to sample the seed set. The DPP kernel ($L$) is constructed using a "quality-diversity" factorization where quality is the information gain score and diversity is enforced via Random Fourier Features (RFF) of the input space.
- **Core assumption:** Diversity in the input feature space is a proxy for coverage of the target function's behavior, and standard distance metrics (RFF kernels) effectively capture this diversity.
- **Evidence anchors:**
  - [abstract] "...impose feature-space diversity by sampling a determinantal point process (DPP) whose kernel uses Random Fourier Features and a quality-diversity factorization..."
  - [Section 3.3.1] Shows that heterogeneity-aware methods (ATBagging, PCA) outperform informativeness-only methods (loss coreset), specifically noting loss coreset retains <5% accuracy vs 15-50% for diverse methods on ERA5.
  - [corpus] Weak support; neighbor papers discuss "Dependency-aware" selection but do not explicitly validate the quality-diversity factorization used here.
- **Break condition:** If the feature space is high-dimensional and the RFF approximation fails to capture relevant similarities, or if categorical features break the distance assumptions (as seen in the Forbes dataset results in Section 3.3.3).

### Mechanism 3
- **Claim:** Transfer learning efficiency is improved by selecting points based on their influence on the *predictive distribution* rather than just the loss function.
- **Mechanism:** Unlike standard coresets which minimize loss on the source task, this method selects points that maximize the change in the model's uncertainty (predictive distribution) regarding the source target. This is posited to transfer better to a correlated target variable ($Y'$).
- **Core assumption:** The source target ($Y$) is sufficiently correlated with the transfer target ($Y'$) such that points reducing uncertainty on $Y$ also reduce uncertainty on $Y'$.
- **Evidence anchors:**
  - [abstract] "...improves or ties early active learning and increases area under the learning-curve relative to alternative seed subset selection methodologies..."
  - [Section 3.4.1] Demonstrates that on the QM9 dataset, ATBagging achieves a Normalized Area Under Learning Curve (NAULC) of 0.85 vs 0.81 for loss coreset, suggesting better transfer properties.
  - [corpus] No direct evidence; "Dependency-aware MLE" paper mentions sample dependencies but does not validate this transfer mechanism.
- **Break condition:** If the correlation between source and target labels is weak or non-existent (negative transfer), the informativeness scores derived from the source are irrelevant to the target.

## Foundational Learning

- **Concept: Bootstrap Aggregating (Bagging) & Out-of-Bag (OOB) Error**
  - **Why needed here:** The core mechanism relies on treating OOB predictions as a proxy for the "prior" distribution. Without understanding that bagging inherently creates held-out subsets for each point, the information gain calculation makes no sense.
  - **Quick check question:** If you have a Random Forest with 100 trees, approximately how many trees are typically "out-of-bag" for a specific training point? (Answer: ~37, or $1 - (1 - 1/N)^N \approx 1/e$).

- **Concept: Determinantal Point Processes (DPPs)**
  - **Why needed here:** The method uses DPPs to solve the subset selection problem. Understanding that the determinant of the kernel matrix measures the "volume" spanned by vectors helps explain why DPPs naturally enforce diversity.
  - **Quick check question:** In a DPP, does adding a point that is identical to an already selected point increase or decrease the probability of the set? (Answer: Decrease, as linear dependence reduces the determinant to zero).

- **Concept: Kullback-Leibler (KL) Divergence**
  - **Why needed here:** The "Informativeness Score" is defined explicitly as the KL divergence between two Gaussian distributions. Understanding this metric is required to interpret why certain points are scored higher than others.
  - **Quick check question:** Is KL divergence symmetric? If $P$ and $Q$ are distributions, is $KL(P||Q) = KL(Q||P)$? (Answer: No).

## Architecture Onboarding

- **Component map:**
  - Source Data -> Ensemble Model (Random Forest) -> IG Scorer -> Feature Encoder (RFF) -> Kernel Builder -> DPP Sampler -> Seed Subset -> Target Model

- **Critical path:**
  1. Train Ensemble: Fit Random Forest on Source Data.
  2. Compute IG: For each point, partition trees -> predict on test set -> fit Gaussians -> compute KL.
  3. Sample Subset: Transform X to RFF -> Build Kernel -> Sample DPP (using scaling fix from Appendix C for ill-conditioned matrices).

- **Design tradeoffs:**
  - Informativeness vs. Heterogeneity: The paper uses a default setting, but the kernel allows tuning. High diversity helps in low-data regimes but might discard high-information points if they are clustered.
  - RFF Dimensionality: Higher dimensions approximate the kernel better but increase the cost of the DPP sampling (matrix operations).
  - Ensemble Size: Must be >50 to ensure OOB availability (Appendix A).

- **Failure signatures:**
  - Empty OOB sets: Crash in IG calculation (fix: increase ensemble size).
  - Rank-Deficient Kernel: DPP sampling fails (fix: Use the approximate sampling method described in Appendix C which scales eigenvalues).
  - Categorical Features: Performance degradation (e.g., Forbes dataset in Section 3.3.3) because RFF distance metrics struggle with categorical encodings.

- **First 3 experiments:**
  1. Baseline Validation: Run ATBagging on the QM9 dataset (Target-Transfer scenario) with $n_{seed}=10$. Verify that the initial model accuracy exceeds random sampling by the margin stated in Section 3.4.1 (0.40 vs 0.31 ITP).
  2. Ablation on Diversity: Compare the full ATBagging pipeline against "ATBagging-Greedy" (select top $k$ IG scores without DPP). Check if the NAULC drops, confirming the value of the diversity mechanism.
  3. Feature Shift Stress Test: Run on the Forbes dataset (mixed categorical/numerical). Observe if the method plateaus early as noted in the paper, and test if alternative encodings (e.g., one-hot vs target encoding) for categorical features mitigate the distance-metric issue.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a robust k-DPP sampling method be developed to handle ill-conditioned L-matrices without relying on scaling approximations?
- **Basis in paper:** [explicit] Appendix C states that "an improved k-DPP sampling method which can handle ill-conditioned L matrices is desired for future research."
- **Why unresolved:** The current proposed sampling method involves scaling the L matrix, which complicates marginal inclusion probabilities and may require multiple resampling attempts to achieve the desired subset size.
- **What evidence would resolve it:** A sampling algorithm that guarantees fixed-size subsets from rank-deficient or ill-conditioned matrices while preserving theoretical probability guarantees.

### Open Question 2
- **Question:** How can the ATBagging kernel be adapted to better handle datasets with mixed categorical and numerical features?
- **Basis in paper:** [inferred] The discussion of the Forbes dataset results notes that the distance-driven kernel similarity "is rendered less powerful due to the ambiguous 'distance' between categorical features."
- **Why unresolved:** The current implementation uses Random Fourier Features based on squared exponential kernels, which assume continuous feature spaces.
- **What evidence would resolve it:** Modified ATBagging results on the Forbes dataset (or similar) utilizing a kernel (e.g., k-prototypes or Hamming distance) specifically designed for categorical data.

### Open Question 3
- **Question:** How sensitive is ATBagging performance to the hyperparameter that balances the trade-off between informativeness and heterogeneity?
- **Basis in paper:** [inferred] The paper mentions this balance is "controlled by a tunable hyperparameter," but "default settings" were used to keep comparisons consistent.
- **Why unresolved:** It is unclear if the default balance is optimal for all data types or if performance could be significantly improved by dataset-specific tuning.
- **What evidence would resolve it:** A sensitivity analysis showing model performance across a range of values for the quality-diversity weighting factor.

## Limitations

- The method assumes feature-space diversity (via RFF) is a valid proxy for functional diversity, which may break down in high-dimensional or categorical feature spaces, as evidenced by performance degradation on the Forbes dataset.
- Transfer learning improvements rely on correlation between source and target labels, creating potential for negative transfer when this assumption fails.
- The Bayesian interpretation of OOB error assumes sufficiently large ensemble sizes, though the paper claims M â‰¥ 50 is adequate.

## Confidence

- **High Confidence:** The OOB-based information gain calculation and DPP sampling methodology are well-established techniques with clear implementation paths.
- **Medium Confidence:** The quality-diversity factorization approach is novel but has limited validation against alternatives.
- **Medium Confidence:** Transfer learning improvements are demonstrated empirically but the mechanism connecting source informativeness to target task performance could benefit from more theoretical grounding.

## Next Checks

1. Test ATBagging on a synthetic dataset where the correlation between source and target labels is explicitly controlled (high, medium, low, negative) to quantify transfer robustness.
2. Implement an ablation study comparing RFF-based diversity against alternative diversity metrics (e.g., uncertainty sampling diversity, feature importance-based diversity) on the same benchmark datasets.
3. Evaluate performance sensitivity to ensemble size by running experiments with M = 20, 50, 100, 200 trees to identify the minimum viable ensemble size for the OOB approximation.