---
ver: rpa2
title: Cloning a Conversational Voice AI Agent from Call\,Recording Datasets for Telesales
arxiv_id: '2509.04871'
source_url: https://arxiv.org/abs/2509.04871
tags:
- agent
- call
- human
- customer
- voice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of creating AI voice agents capable
  of handling telesales calls by cloning human agents from recorded call datasets.
  A method was developed to extract behavioral patterns and best practices from high-quality
  telesales interactions, transforming them into a structured playbook integrated
  into a system prompt for a large language model.
---

# Cloning a Conversational Voice AI Agent from Call\,Recording Datasets for Telesales

## Quick Facts
- arXiv ID: 2509.04871
- Source URL: https://arxiv.org/abs/2509.04871
- Authors: Krittanon Kaewtawee; Wachiravit Modecrua; Krittin Pachtrachai; Touchapon Kraisingkorn
- Reference count: 24
- Primary result: AI voice agent cloned from telesales recordings achieved near-human performance after prompt refinement and fine-tuning, but still lagged in complex persuasion scenarios

## Executive Summary
This paper addresses the challenge of creating AI voice agents for telesales by developing a method to clone human agent behavior from recorded call datasets. The approach extracts behavioral patterns and best practices from high-quality telesales interactions, transforming them into a structured playbook integrated into a system prompt for a large language model. The resulting AI agent was evaluated through blind tests against human benchmarks, showing promising results in basic telesales skills while revealing areas for improvement in objection handling and sales drive.

## Method Summary
The method involves collecting ~1,000 call recordings, classifying them by quality, and analyzing top performers to extract structured knowledge. This knowledge is transformed into a nine-section system prompt defining the agent's role, persona, conversation flow, objection tactics, product knowledge, and compliance rules. The AI agent operates through a streaming speech-to-speech pipeline using the Gemini Live API, with the system prompt guiding its behavior during customer interactions. Initial evaluations identified performance gaps, leading to targeted prompt refinement and fine-tuning on objection-handling examples.

## Key Results
- AI agent matched human performance in introduction and product communication categories
- Initial scores in objection handling and sales drive lagged behind human benchmarks
- After prompt refinement and fine-tuning, objection handling and sales drive scores increased by approximately 20%
- AI agent approached human performance levels but still underperformed in complex persuasion scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured knowledge extraction from high-performing call recordings can be encoded into a system prompt to shape LLM behavior without retraining the base model.
- Mechanism: The cloning system samples ~1,000 calls, ranks them by agent quality, and analyzes a curated subset (~40 top calls) to draft a job description, knowledge manual, and example dialogues. These are composed into a multi-section system prompt that defines role, persona, conversation flow, objection tactics, product knowledge, and compliance rules—grounding the LLM in domain-specific behavioral patterns.
- Core assumption: The structured playbook sufficiently captures tacit sales expertise and the LLM can enact it via instruction-following alone.
- Evidence anchors:
  - [abstract]: "...transforming them into a structured playbook integrated into a system prompt for a large language model. This prompt guides the AI agent in greeting customers, explaining products, handling objections, and closing sales..."
  - [section 2]: "Separating knowledge extraction from runtime inference allows us to optimise each step independently."
  - [corpus]: Related work on modular dialogue systems (e.g., semi-cascaded full-duplex dialogue, arXiv:2601.20230) supports component separation, but does not directly validate this extraction-to-prompt pipeline.
- Break condition: The playbook becomes too large or contradictory for the model to follow, causing ignored instructions or degraded performance.

### Mechanism 2
- Claim: Explicit persona, conversation flow, and objection-handling structures in the prompt reduce ambiguity and align model outputs with a cloned agent identity.
- Mechanism: The prompt includes: (1) agent role definition, (2) tone/persona specification, (3) staged conversation flow (Opening → Discovery → Pitch → Objection Handling → Closing), (4) specific objection tactics, (5) product facts, (6) terminology/tone guidance, (7) example dialogue snippets, (8) compliance rules, and (9) agent/customer context slots. This multi-faceted structure guides generation at each dialogue stage.
- Core assumption: The underlying LLM has sufficient instruction-following capability and can generalize from examples to novel customer interactions.
- Evidence anchors:
  - [section 3]: "A key innovation in our approach is the structured system prompt that encapsulates the sales agent's persona and best practices gleaned from call recordings."
  - [abstract]: "...integrated into a system prompt for a large language model. This prompt guides the AI agent in greeting customers, explaining products, handling objections, and closing sales..."
  - [corpus]: Corpus evidence on prompt-based persona shaping is limited; this paper provides a detailed, domain-specific instantiation.
- Break condition: Prompt sections conflict (e.g., overly cautious politeness vs. sales drive), leading to inconsistent behavior or failure to pursue closing.

### Mechanism 3
- Claim: Iterative error analysis and targeted prompt refinement, guided by blind human evaluation, significantly improve performance in weak areas.
- Mechanism: Initial blind tests reveal gaps in objection handling and sales drive. Analysis of evaluator comments identifies failure modes (ambiguous objectives, redundant instructions, formatting leaks, over-cautiousness). The prompt is refined (adding explicit success criteria, trimming verbosity, using prose/examples instead of lists, softening politeness) and the model is fine-tuned on 50 Q&A pairs and objection examples, yielding ~20% score increases in targeted categories.
- Core assumption: The evaluation rubric accurately captures critical sales skills and identified errors are addressable through prompt/model changes rather than architectural overhauls.
- Evidence anchors:
  - [section 5]: "After analysing evaluator comments we identified several failure modes... and refined the prompt and model accordingly... With these changes the agent's objection handling and sales drive scores increased by roughly 20%..."
  - [abstract]: "After prompt refinement and fine-tuning, the AI's scores improved significantly, approaching human benchmarks..."
  - [corpus]: Corpus does not provide direct comparative evidence for this iterative refinement loop.
- Break condition: Performance plateaus despite prompt adjustments, indicating need for architectural changes or more sophisticated training data.

## Foundational Learning

- Concept: **Speech-to-Speech (S2S) Pipeline Architecture**
  - Why needed here: The paper uses a streaming S2S API (Gemini Live) instead of a traditional cascaded ASR→LLM→TTS pipeline, reducing latency and simplifying inference.
  - Quick check question: What are the tradeoffs between a unified S2S model and a cascaded ASR/LLM/TTS pipeline in terms of latency, control, and error propagation?

- Concept: **Prompt Engineering for Persona and Role-Playing**
  - Why needed here: The primary "cloning" mechanism is prompt composition, not model training. Structuring prompts to define identity, tone, and staged behavior is essential.
  - Quick check question: How would you structure a system prompt to make an LLM consistently adopt a specific professional persona across multi-turn conversations?

- Concept: **LLM-Based Agent Evaluation**
  - Why needed here: The study uses a 22-criterion rubric and blind human evaluators to compare AI vs. human agents. Understanding rigorous evaluation design is critical for validating agent performance.
  - Quick check question: Why must evaluators be "blind" to whether they're scoring AI or human agents, and what biases does this mitigate?

## Architecture Onboarding

- Component map:
  - Data Layer (1,000 call recordings) -> Cloning System (sampling/ranking → job description → knowledge extraction → example dialogues → prompt composition) -> Agent Playbook (system prompt) -> Inference System (web client ↔ WebSocket backend ↔ S2S API) -> Evaluation Layer (22-criterion rubric, blind human evaluators)

- Critical path:
  1. Curate high-quality call recordings from top performers
  2. Manually analyze to extract structured knowledge (job description, knowledge manual, example dialogues)
  3. Compose into coherent system prompt with clear persona, flow, objection tactics, product facts, and compliance rules
  4. Deploy via streaming S2S inference pipeline
  5. Evaluate blind against human benchmarks, analyze failures, and iterate on prompt/fine-tuning

- Design tradeoffs:
  - **Unified S2S vs. Cascaded**: Unified models simplify architecture and reduce latency but offer less granular control over ASR/TTS components
  - **Prompt Engineering vs. Fine-Tuning**: Prompting is faster/cheaper; fine-tuning adds cost but addresses specific knowledge gaps
  - **Human vs. Automated Evaluation**: Human evaluation is accurate but slow/expensive; automated LLM-judges (future work) scale but require calibration

- Failure signatures:
  - **Passive Closing**: Agent politely accepts rejection without attempting follow-up or alternative closes → Add explicit success criteria (e.g., "primary goal is booking installation")
  - **Formatting Leaks**: Agent verbalizes prompt artifacts (e.g., "Point number two") → Replace numbered lists with prose and conversational examples
  - **Over-Cautiousness**: Agent yields floor to rambling customers → Soften politeness instructions; add examples of gently redirecting conversation
  - **Hallucination**: Agent invents unapproved offers → Strengthen compliance rules and ground with precise product knowledge

- First 3 experiments:
  1. Establish baseline with generic system prompt, evaluate on simplified rubric to identify largest gaps
  2. Target one weak area (e.g., objection handling): extract common objections/responses from recordings, add as dedicated prompt section, re-evaluate
  3. A/B test S2S backends (Gemini Live vs. OpenAI Realtime vs. cascaded) with fixed prompt, comparing latency, naturalness, and rubric scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can large-scale simulations using scripted customer personas and self-play generate sufficient training data to close the performance gap in complex persuasion?
- Basis in paper: [explicit] The authors state they "plan to conduct large-scale simulations using scripted customer personas and self-play to generate additional training data."
- Why unresolved: The current methodology relies on a static corpus of recorded calls, which limits the agent's exposure to rare or difficult negotiation scenarios.
- Evidence: Performance metrics of agents trained via self-play matching or exceeding human baselines in the "negotiation" and "complaining customer" scenarios.

### Open Question 2
- Question: Does integrating retrieval-augmented generation (RAG) allow the agent to accurately query and utilize live pricing or support documents during calls?
- Basis in paper: [explicit] The authors intend to "integrate retrieval-augmented generation so that the agent can query live pricing or support documents during a call."
- Why unresolved: The current system relies on a static "knowledge manual" inside the prompt, which risks factual obsolescence or hallucination regarding dynamic details.
- Evidence: A comparative evaluation showing improved accuracy in "product communication" scores and reduced factual errors when RAG is enabled versus the static prompt.

### Open Question 3
- Question: Can LLM-based judges be calibrated to reliably replace human evaluators for scoring complex soft skills like sales drive and objection handling?
- Basis in paper: [explicit] The authors note they are "exploring automated evaluation using LLM-based judges, although such systems must be calibrated carefully."
- Why unresolved: The paper currently relies on blind human evaluation, which is resource-intensive; automated scoring is proposed but unproven for subjective telesales criteria.
- Evidence: A high correlation coefficient between LLM-judge scores and the human baseline scores across the 22-criteria rubric.

## Limitations
- Manual curation and prompt engineering introduce significant reproducibility challenges
- Evaluation uses small sample size (7 evaluators) without statistical significance testing
- Reliance on Gemini Live API limits architectural flexibility and generalizability
- Scripted test scenarios may not capture real-world telesales variability

## Confidence

- **High confidence**: S2S pipeline architecture choice and basic functionality; identification of objection handling and sales drive as performance gaps; general approach of using structured prompts to guide LLM behavior
- **Medium confidence**: Effectiveness of iterative prompt refinement based on blind evaluation; ~20% improvement in targeted metrics after prompt refinement and fine-tuning; comparative ranking of AI vs. human performance on 22-criterion rubric
- **Low confidence**: Claim that cloning captures "tacit sales expertise"; generalizability to different sales domains or languages; sufficiency of 22-criterion rubric to capture all critical sales skills

## Next Checks

1. **Prompt Transparency Check**: Obtain and publish the complete system prompt text to enable independent reproduction and analysis of prompt structure effects
2. **Statistical Significance Validation**: Re-run evaluations with larger evaluator pools (n≥20) and compute confidence intervals for rubric scores to assess whether observed differences between AI and human agents are statistically significant
3. **Cross-Backend Generalization Test**: Deploy the same agent playbook prompt across multiple S2S backends (e.g., OpenAI Realtime, Ultravox, and a cascaded ASR→LLM→TTS pipeline) to quantify performance variance and identify backend-dependent limitations