---
ver: rpa2
title: Block-Biased Mamba for Long-Range Sequence Processing
arxiv_id: '2505.09022'
source_url: https://arxiv.org/abs/2505.09022
tags:
- mamba
- learning
- b2s6
- neural
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Mamba models underperform on long-range sequential tasks despite
  their design for long-range dependencies. This paper identifies three limitations:
  expressiveness, inductive bias, and training stability.'
---

# Block-Biased Mamba for Long-Range Sequence Processing

## Quick Facts
- arXiv ID: 2505.09022
- Source URL: https://arxiv.org/abs/2505.09022
- Reference count: 40
- Primary result: B2S6 achieves state-of-the-art performance on Long-Range Arena benchmark

## Executive Summary
Mamba models, despite being designed for long-range dependencies, underperform on long-range sequential tasks due to limitations in expressiveness, inductive bias, and training stability. This paper proposes B2S6, a block-biased extension of Mamba's S6 unit that incorporates block-wise selective dynamics and channel-specific bias. B2S6 addresses these shortcomings by improving expressiveness through partitioned channel blocks, providing a more suitable inductive bias for long-range tasks, and enhancing training stability. The model achieves state-of-the-art performance on the Long-Range Arena benchmark while maintaining Mamba's effectiveness on language modeling tasks.

## Method Summary
B2S6 partitions input channels into h blocks of size p (where d = h × p), with each block having independent selective dynamics and channel-specific bias. The unit uses complex parameterization for diag(A), B_weight, and B_bias parameters, with reduced learning rates for ∆-related parameters (jw, jb(i)). The architecture applies a convolutional and linear transformation followed by the B2S6 unit, using h=8 blocks. The model is trained on Long-Range Arena tasks with 6-8 layers, learning rates of 0.001-0.01, batch sizes of 16-60, and 40-500 epochs.

## Key Results
- B2S6 achieves state-of-the-art performance on Long-Range Arena benchmark
- Outperforms both S4 and S4D on long-range sequential tasks
- Maintains Mamba's performance on language modeling while excelling at long-range tasks

## Why This Works (Mechanism)
B2S6 addresses Mamba's three key limitations: (1) Expressiveness - by partitioning channels into blocks with independent selective dynamics, it can model more complex patterns; (2) Inductive bias - the block structure provides better inductive bias for long-range tasks by creating more local receptive fields within blocks; (3) Training stability - the channel-specific bias and reduced learning rates for ∆-related parameters prevent the exponential growth issues that plague standard Mamba on long sequences.

## Foundational Learning
- **Long-range sequence modeling** - why needed: LRA benchmark tasks require modeling dependencies across thousands of tokens; quick check: can the model handle Path-X with L=16384
- **Selective state space models** - why needed: Core Mamba architecture that B2S6 extends; quick check: understanding how A, B, C matrices interact in SSM
- **Channel partitioning** - why needed: B2S6 divides channels into blocks for independent processing; quick check: verify d = h × p channel dimension calculation
- **Complex parameterization** - why needed: Used for diag(A), B_weight, B_bias to improve stability; quick check: confirm initialization scheme follows referenced papers
- **Learning rate scheduling** - why needed: Reduced LR for ∆-related parameters is crucial for stability; quick check: monitor training curves for stability

## Architecture Onboarding
**Component Map:** Input -> Conv(Linear) -> σ → B2S6 blocks (h partitions) -> σ → Linear -> Output
**Critical Path:** x → Partition into h blocks → For each block: compute selective dynamics → Apply SSM → Combine blocks → Output
**Design Tradeoffs:** More blocks (h) increases expressiveness but reduces receptive field per block; complex parameterization improves stability but adds implementation complexity
**Failure Signatures:** Training instability on long sequences (loss spikes), poor expressiveness (accuracy plateaus), memory collapse on large-magnitude inputs
**First Experiments:** 1) Train on sCIFAR-10 ablation with d=128, 4 layers; 2) Test B2S6 on synthetic long-range dependency tasks; 3) Vary block count h for different input dimensionalities d

## Open Questions the Paper Calls Out
1. Can B2S6 maintain its efficiency and performance when scaled up to serve as a foundation model for large-scale language tasks?
2. Can architectural refinements to the B2S6 unit eliminate the need for manual learning rate adjustments to ensure training stability?
3. How does the interaction between block structure and channel-specific bias affect the optimal selection of the block count hyperparameter (h)?

## Limitations
- Reduced learning rate for ∆-related parameters not explicitly specified
- Complex parameterization initialization scheme details unclear
- Training setup lacks optimizer and weight decay specifications
- Language modeling results based on sampled dataset subset

## Confidence
- **High confidence** in theoretical analysis of Mamba's limitations
- **Medium confidence** in B2S6 solution implementation details
- **High confidence** in LRA benchmark results
- **Medium confidence** in language modeling results

## Next Checks
1. Implement B2S6 with various learning rate reduction factors for ∆-related parameters and compare training stability
2. Test B2S6 on synthetic long-range dependency tasks to verify ability to capture dependencies Mamba fails on
3. Perform ablation study varying block count h for different input dimensionalities d to find optimal block-to-channel ratio