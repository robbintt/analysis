---
ver: rpa2
title: 'AquiLLM: a RAG Tool for Capturing Tacit Knowledge in Research Groups'
arxiv_id: '2508.05648'
source_url: https://arxiv.org/abs/2508.05648
tags:
- aquillm
- research
- groups
- knowledge
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AquiLLM is a lightweight, modular retrieval-augmented generation
  (RAG) system designed specifically for research groups to capture and retrieve tacit
  knowledge from informal and private sources. The system integrates varied document
  types including emails, meeting notes, and training materials while supporting configurable
  privacy settings and single-tenant deployment.
---

# AquiLLM: a RAG Tool for Capturing Tacit Knowledge in Research Groups

## Quick Facts
- arXiv ID: 2508.05648
- Source URL: https://arxiv.org/abs/2508.05648
- Authors: Chandler Campbell; Bernie Boscoe; Tuan Do
- Reference count: 16
- Primary result: Lightweight, modular RAG system for research groups to capture and retrieve tacit knowledge from informal and private sources

## Executive Summary
AquiLLM is a lightweight, modular retrieval-augmented generation (RAG) system designed specifically for research groups to capture and retrieve tacit knowledge from informal and private sources. The system integrates varied document types including emails, meeting notes, and training materials while supporting configurable privacy settings and single-tenant deployment. Built on Django and PostgreSQL with pgvector for vector search, AquiLLM avoids LLM integration frameworks to reduce maintenance overhead and enable full infrastructure control. The architecture includes an LLM abstraction layer with tool calling capabilities, allowing the model to explore collections for information retrieval.

## Method Summary
AquiLLM implements a Django-based RAG system using PostgreSQL with pgvector for vector storage and search. Documents are ingested, chunked, and embedded into vectors stored in pgvector. The system employs hybrid search combining vector similarity with trigram matching and reranking. A custom LLM abstraction layer supports tool calling, enabling the LLM to iteratively explore collections rather than relying on single retrieval passes. The architecture uses MinIO for object storage and supports multiple LLM providers including local Ollama deployment. A single bash script handles deployment with Docker containers for all components.

## Key Results
- Successfully deployed in beta with an astronomy group at UCLA and environmental scientists
- Proven useful for onboarding new members and querying documentation
- Demonstrates effectiveness in making informal knowledge more accessible while maintaining data sovereignty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vector-based semantic retrieval enables discovery of relevant information without exact terminology matching.
- Mechanism: Text documents are chunked and embedded into vectors stored in pgvector; queries are similarly embedded, and retrieval uses distance metrics (Euclidean or cosine similarity) plus trigram matching with reranking. This captures semantic relationships that keyword search misses.
- Core assumption: Embedding models adequately represent domain-specific scientific terminology and informal research language.
- Evidence anchors:
  - [abstract] "AquiLLM supports varied document types and configurable privacy settings, enabling more effective access to both formal and informal knowledge"
  - [section II.C.1] "TextChunk features a class method for hybrid search (vector plus trigram) with reranking"
  - [corpus] Weak direct corpus evidence; neighbor paper "Leveraging Large Language Models for Tacit Knowledge Discovery in Organizational Contexts" suggests LLM-based tacit knowledge discovery is an active research area but does not validate this specific retrieval approach
- Break condition: Embeddings fail to capture domain semantics (e.g., specialized astronomy terminology); chunking boundaries split critical context; vector index degrades with scale.

### Mechanism 2
- Claim: Tool-calling enables the LLM to iteratively explore collections rather than relying on a single retrieval pass.
- Mechanism: The LLM receives search functions as tools via a decorator-based interface, allowing it to issue multiple queries, refine searches, and synthesize across retrieved chunks. This contrasts with simpler RAG systems that append retrieved text to a single LLM call.
- Core assumption: The LLM has sufficient tool-calling capability and reasoning to productively explore without excessive iterations or hallucinated tool uses.
- Evidence anchors:
  - [abstract] "architecture includes an LLM abstraction layer with tool calling capabilities, allowing the model to explore collections for information retrieval"
  - [section II.A] "The more sophisticated approach used in AquiLLM allows the model to explore the collection in order to find the information it needs"
  - [corpus] No direct validation in corpus; this design pattern is not empirically evaluated in cited neighbors
- Break condition: LLM tool-calling produces inefficient search patterns; tool schema mismatches cause errors; latency from multiple LLM-tool round-trips becomes unacceptable.

### Mechanism 3
- Claim: Single-tenant self-hosted deployment reduces dependency risk and aligns with research groups' data sovereignty preferences.
- Mechanism: Django monolith with PostgreSQL (pgvector) and MinIO object storage deployed via single bash script; LLM abstraction layer supports multiple providers (including local Ollama), avoiding vendor lock-in. Groups control all infrastructure.
- Core assumption: Research groups have basic Linux administration capacity and accept responsibility for backups, updates, and security.
- Evidence anchors:
  - [abstract] "Built on Django and PostgreSQL with pgvector...avoids LLM integration frameworks to reduce maintenance overhead and enable full infrastructure control"
  - [section I.C] "By deploying single-tenant solutions on their own infrastructure, groups avoid becoming dependent on third-party services"
  - [section III] "deployed for a group of astronomers at UCLA...deployed to a Jetstream2 instance using the packaged deployment scripts"
  - [corpus] No comparative corpus evidence on deployment model effectiveness
- Break condition: Groups lack admin expertise; unmanaged dependencies (Postgres, MinIO, LLM APIs) create operational burden; security misconfigurations expose private data.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: AquiLLM's core function is RAGâ€”understanding how retrieval grounds LLM generation is essential before examining the tool-calling extension.
  - Quick check question: Can you explain why appending retrieved chunks to an LLM prompt reduces hallucination compared to pure generation?

- Concept: Vector embeddings and similarity search
  - Why needed here: pgvector stores embeddings and enables semantic search; understanding vector space, distance metrics, and chunking is prerequisite to debugging retrieval quality.
  - Quick check question: Given two text chunks with no shared words but similar meaning, would keyword search find both? Would vector search?

- Concept: Tool calling (function calling) in LLMs
  - Why needed here: AquiLLM's distinguishing feature is giving the LLM search tools rather than pre-retrieving; understanding tool schemas and LLM-tool interaction loops is critical.
  - Quick check question: In a tool-calling loop, how does the LLM know when to stop calling tools and return a final answer?

## Architecture Onboarding

- Component map:
  - Django backend -> Core application logic, ORM models (Collection, Document, TextChunk), LLM abstraction layer (Conversation, LLMInterface, LLMTool), WebSocket consumer for chat
  - PostgreSQL + pgvector -> Vector storage and search; all structured data except object files
  - MinIO (S3-compatible) -> Object storage for PDFs, audio, and other uploaded files
  - Frontend -> Django templates with embedded React components; Google Drive-style collection management UI; ChatGPT-style chat interface
  - LLM API layer -> Abstraction supporting multiple providers; can use Ollama for fully local inference

- Critical path:
  1. Document ingest -> parsed/chunked -> embeddings generated -> stored in pgvector via TextChunk
  2. User query -> LLM receives search tools -> LLM calls search tool with query -> hybrid search retrieves chunks -> LLM synthesizes response
  3. Permission check at collection level before any retrieval

- Design tradeoffs:
  - Django monolith: Rapid development, simple deployment, but limited horizontal scaling (acceptable for single-tenant, hundreds of users)
  - pgvector vs. dedicated vector DB: Reduces dependencies and operational complexity, but may lack advanced features of Pinecone/Milvus
  - Custom LLM logic vs. Langchain: More control, less dependency churn, but higher upfront development cost and less ecosystem support
  - Tool-calling RAG vs. single-pass retrieval: More flexible exploration, but higher latency and cost per query

- Failure signatures:
  - Retrieval returns irrelevant chunks: Check embedding model compatibility with domain; verify chunk size/overlap settings; inspect pgvector index configuration
  - LLM ignores tool results or hallucinates sources: Verify tool schema generation; check LLM provider tool-calling support; inspect conversation history management
  - Permission leakage: Trace collection access checks in ORM and WebSocket consumer; verify default private settings
  - Deployment failures: Inspect Docker container logs for Postgres/MinIO; verify environment variable configuration; check LLM API credentials

- First 3 experiments:
  1. Deploy locally using packaged scripts; ingest 5-10 varied documents (PDF, text, email export); verify chunking and embedding via Django admin or direct Postgres queries.
  2. Test retrieval quality: Submit queries with synonymous but non-matching terminology; compare vector-only vs. hybrid search results; measure recall.
  3. Exercise tool-calling: Enable verbose logging on LLMTool decorator; trace number of tool calls per query; identify inefficient patterns or failures.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effectively can AquiLLM retrieve and synthesize tacit knowledge from purely informal sources (meeting recordings, training sessions, ad hoc documentation) compared to formal publications?
- Basis in paper: [explicit] The environmental scientist deployment is collecting "recordings of meetings and training sessions" to "investigate its utility on such informal data."
- Why unresolved: The astronomy deployment primarily used formal papers, meeting notes, and transcripts; the informal-data evaluation is still in progress.
- What evidence would resolve it: Comparative performance metrics (retrieval relevance, answer completeness) between formal and informal document types, plus user task-completion studies.

### Open Question 2
- Question: How does AquiLLM perform across diverse scientific domains with different terminologies, documentation practices, and tacit-knowledge structures?
- Basis in paper: [explicit] "Future work will expand its multimodal capabilities and evaluate its use across diverse scientific domains."
- Why unresolved: Only two beta deployments (astronomy, environmental science) are described, with minimal comparative analysis between them.
- What evidence would resolve it: Controlled cross-domain evaluation measuring retrieval accuracy, response quality, and onboarding efficacy across multiple fields.

### Open Question 3
- Question: What additional modalities can be effectively integrated into AquiLLM's retrieval pipeline, and what representation strategies optimize their searchability?
- Basis in paper: [explicit] "Future work will expand its multimodal capabilities."
- Why unresolved: Current implementation handles text documents and audio but multimodal expansion remains unspecified future work.
- What evidence would resolve it: Implementation of modalities such as images, video, or code repositories, with benchmarked retrieval performance for each.

### Open Question 4
- Question: What is the accuracy and reliability of AquiLLM when synthesizing responses from multiple documents containing conflicting or temporally evolving information?
- Basis in paper: [inferred] The paper acknowledges documents may contain "outdated information that contradicts more recent findings" but provides no quantitative evaluation of how well the system resolves conflicts or avoids hallucination.
- Why unresolved: Only anecdotal user feedback is reported; no systematic accuracy metrics, ground-truth comparisons, or hallucination-rate measurements are presented.
- What evidence would resolve it: Benchmark queries with known correct answers, measurement of precision/recall, and human evaluation of factual accuracy in synthesized multi-source responses.

## Limitations
- Lacks empirical validation of core mechanisms including retrieval quality and tool-calling efficiency
- No systematic comparison against alternative RAG approaches or quantitative performance metrics
- Critical implementation details such as embedding models, chunking strategies, and reranking algorithms remain unspecified

## Confidence

- **High Confidence**: The architectural design and technical implementation details are clearly specified and internally consistent
- **Medium Confidence**: The deployment experience with UCLA astronomy group and environmental scientists suggests practical utility
- **Low Confidence**: Claims about the superiority of tool-calling RAG versus single-pass retrieval lack empirical validation

## Next Checks

1. **Retrieval Quality Assessment**: Conduct a controlled evaluation comparing vector-only, hybrid (vector+trigram), and keyword search on a test corpus of research documents, measuring precision@K and recall for queries with synonymous terminology versus exact matches.

2. **Tool-Calling Efficiency Analysis**: Instrument the LLM tool-calling interface to log the number of search iterations per query, identify patterns of excessive tool use or premature termination, and measure the impact on response latency and quality compared to single-pass retrieval.

3. **Scalability and Maintenance Evaluation**: Deploy the system with 10,000+ documents and 100+ concurrent users, monitoring pgvector query performance, LLM API costs, and operational overhead for database maintenance, LLM updates, and security patching.