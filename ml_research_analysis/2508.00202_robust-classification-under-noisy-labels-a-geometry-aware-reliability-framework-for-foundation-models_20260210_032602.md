---
ver: rpa2
title: 'Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework
  for Foundation Models'
arxiv_id: '2508.00202'
source_url: https://arxiv.org/abs/2508.00202
tags:
- noise
- label
- reliability
- local
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses robust classification under label noise for
  foundation models without retraining. The authors propose a two-stage geometry-aware
  reliability framework that first estimates sample reliability using local NNK neighborhoods
  and global clustering, then performs reliability-weighted inference.
---

# Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models

## Quick Facts
- arXiv ID: 2508.00202
- Source URL: https://arxiv.org/abs/2508.00202
- Authors: Ecem Bozkurt; Antonio Ortega
- Reference count: 25
- Primary result: Geometry-aware reliability framework improves robust classification under label noise without model retraining, showing NNK methods excel at low noise and clustering methods at high noise.

## Executive Summary
This paper addresses robust classification under label noise for foundation models without retraining. The authors propose a two-stage geometry-aware reliability framework that first estimates sample reliability using local NNK neighborhoods and global clustering, then performs reliability-weighted inference. Local reliability metrics include NNK weight-based scores and diameter ratio measures, while global metrics use supervised/unsupervised k-means clustering. Experiments on CIFAR-10 and DermaMNIST show improved robustness across various noise levels compared to k-NN and adaptive-neighborhood baselines. Key findings: NNK-based methods excel at low noise levels by leveraging local geometry, while clustering-based approaches perform better at high noise levels where local information becomes unreliable.

## Method Summary
The framework consists of two stages: reliability estimation and reliability-weighted inference. In the first stage, five reliability metrics are computed on training data - three local (k-NN label agreement, NNK weight sum for same-label neighbors, NNK diameter ratio) and two global (supervised and unsupervised k-means with softmax distance weights). The second stage uses NNK neighborhoods from the training set to perform weighted or unweighted majority voting on test samples, where votes are weighted by the reliability scores from stage one. The method operates on fixed foundation model embeddings (DINOv2-base) without retraining, making it applicable to any pretrained model.

## Key Results
- NNK-based local reliability metrics (weight-based and diameter-ratio) outperform standard k-NN at low-to-moderate noise levels (0-40%)
- Clustering-based global reliability methods (supervised and unsupervised k-means) surpass local methods at high noise levels (40-60%)
- Unweighted inference (using reliability scores only) performs better than weighted inference at high noise levels where local geometry becomes unreliable
- The framework demonstrates consistent improvement over baselines across CIFAR-10 and DermaMNIST datasets

## Why This Works (Mechanism)

### Mechanism 1
NNK-based neighborhood construction captures geometrically non-redundant local structure that improves reliability estimation over standard k-NN at low-to-moderate noise levels. The NNK algorithm constructs sparse local neighborhoods forming polytopes around each query with normalized weights based on relative similarity, enabling reliability signals from same-label neighbor weights and diameter ratio comparisons. Core assumption: embedding space geometry from the foundation model meaningfully correlates with class structure when noise is bounded below ~40%. Break condition: when noise exceeds local neighborhood majority (>50% corrupted neighbors) or embeddings poorly capture class structure.

### Mechanism 2
Global clustering-based reliability estimation outperforms local methods at high noise levels because cluster centroids are more stable under systematic label corruption. K-means clustering computes centroids that aggregate geometric information across many samples, with reliability derived from softmax-weighted distances to centroids combined with label distributions. Core assumption: cluster structure in embedding space reflects true class geometry even when individual labels are corrupted. Break condition: when asymmetric noise creates systematic wrong-class clusters or centroid count is misspecified.

### Mechanism 3
Unweighted inference outperforms distance-weighted inference at high noise levels because local geometry becomes untrustworthy. At inference, neighbors vote via majority voting where weighted mode multiplies reliability by NNK edge weights while unweighted uses reliability alone. Core assumption: correlation between embedding distance and label agreement degrades monotonically with noise level. Break condition: when reliability scores themselves become noisy at extreme corruption.

## Foundational Learning

- **Concept: Non-negative Kernel (NNK) Graph Construction**
  - Why needed here: The paper's core local reliability metrics depend on understanding how NNK differs from k-NN - it solves an optimization to select a sparse, geometrically non-redundant neighbor set with interpretable weights.
  - Quick check question: Can you explain why NNK weights sum to 1 and what geometric property they capture that k-NN distances do not?

- **Concept: Foundation Model Embeddings as Fixed Feature Extractors**
  - Why needed here: The entire framework assumes a frozen pretrained model (DINOv2) produces embeddings where class structure is preserved. Understanding transfer learning assumptions is critical.
  - Quick check question: What happens to this framework if the FM embedding space has poor class separation for your downstream task?

- **Concept: Label Noise Types (Symmetric vs. Asymmetric)**
  - Why needed here: The paper evaluates under both regimes; asymmetric noise (systematic flips like "bird"→"airplane") creates different failure modes than random symmetric noise.
  - Quick check question: Which noise type would you expect to corrupt local neighborhoods more severely, and why?

## Architecture Onboarding

- **Component map:** Embedding extraction (DINOv2) -> Reliability estimation (NNK metrics + clustering) -> Inference (NNK neighborhoods + weighted/unweighted voting)
- **Critical path:** Precompute training embeddings → Run NNK graph construction → Compute reliability scores → At inference: embed query → find NNK neighbors → aggregate reliability-weighted votes
- **Design tradeoffs:** Local vs. global reliability (use local for noise <40%, global for high noise); weighted vs. unweighted inference (weighted for clean/light noise, unweighted for >40% noise); supervised vs. unsupervised k-means (supervised for asymmetric noise, unsupervised for heavy corruption); hyperparameter tuning needed for centroid counts
- **Failure signatures:** Accuracy below k-NN baseline indicates poor class separation; local methods failing at moderate noise suggests asymmetric patterns; large variance indicates unstable neighborhoods or clustering
- **First 3 experiments:** 1) Sanity check NNK-weighted inference on clean CIFAR-10 vs. k-NN; 2) Noise sweep testing NNK-diameter vs. supervised k-means crossover point; 3) Compare DINOv2 vs. weaker backbone on DermaMNIST to verify "noise in geometry" effects

## Open Questions the Paper Calls Out
- How can an adaptive mechanism be designed to dynamically balance local (NNK) and global (clustering) reliability metrics without prior knowledge of the noise level?
- What strategies can effectively automate the calibration of key hyperparameters, such as neighborhood size (k) and the number of clusters (Kc)?
- Does the reliability of geometry-aware methods degrade when applied to foundation models with significantly different embedding geometries or non-visual modalities?

## Limitations
- Asymmetric noise configurations were not fully specified beyond one example ("bird"→"airplane")
- No direct validation of the geometric intuition behind NNK superiority over k-NN beyond empirical accuracy gains
- Framework assumes fixed, frozen foundation model embeddings without addressing potential embedding space mismatch

## Confidence

- **High confidence:** Empirical superiority of clustering methods at high noise levels, crossover behavior between local and global reliability estimation, relative performance ordering of NNK-based vs. clustering-based approaches
- **Medium confidence:** Specific numerical accuracy values (may depend on unreported implementation details)
- **Low confidence:** Claimed mechanisms explaining why NNK weights better capture geometric structure than k-NN distances

## Next Checks
1. **Mechanism validation:** Run ablation studies varying neighborhood sizes to test whether the geometric non-redundancy claim holds across scales
2. **Noise type sensitivity:** Systematically test all possible asymmetric noise patterns on CIFAR-10 to determine if supervised vs. unsupervised clustering performance differences are consistent
3. **Embedding quality dependence:** Compare the framework's performance using different foundation models (DINOv2 vs. CLIP vs. ResNet) on DermaMNIST to validate the claim about "noise in geometry" affecting local methods