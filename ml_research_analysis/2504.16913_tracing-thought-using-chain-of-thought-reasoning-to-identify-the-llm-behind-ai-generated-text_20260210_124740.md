---
ver: rpa2
title: 'Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind
  AI-Generated Text'
arxiv_id: '2504.16913'
source_url: https://arxiv.org/abs/2504.16913
tags:
- text
- task
- classification
- ai-generated
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces COTFinetuned, a dual-task framework leveraging
  Chain-of-Thought (CoT) reasoning to detect AI-generated text and identify the specific
  language model (LLM) responsible. The method jointly performs binary classification
  (AI vs.
---

# Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text

## Quick Facts
- **arXiv ID:** 2504.16913
- **Source URL:** https://arxiv.org/abs/2504.16913
- **Authors:** Shifali Agrahari; Sanasam Ranbir Singh
- **Reference count:** 25
- **Primary result:** BERT + COT achieves F1-scores of 0.898 for binary AI detection and 0.307 for LLM identification

## Executive Summary
This study introduces COT_Finetuned, a dual-task framework that leverages Chain-of-Thought (CoT) reasoning to detect AI-generated text and identify the specific language model responsible. The method jointly performs binary classification (AI vs. human) and multi-class classification (identifying the LLM) while generating interpretable explanations for predictions. Experiments using BERT and RoBERTa models on real-world datasets show that COT_Finetuned significantly improves detection accuracy compared to baseline models. CoT reasoning enhances both model performance and interpretability, offering insights into the stylistic patterns unique to different LLMs.

## Method Summary
The COT_Finetuned framework employs a two-stage approach: first, a teacher model (LLaMA) generates reasoning explanations for each training sample by answering why a particular document was generated by a specific label. These reasoning explanations are then concatenated with the original text and label, forming the input to a BERT or RoBERTa encoder. The model is fine-tuned jointly on two tasks: Task A (binary classification distinguishing AI from human text) and Task B (multi-class classification identifying which specific LLM generated the text). The total loss combines classification losses for both tasks, with the shared encoder learning representations that benefit both objectives.

## Key Results
- BERT + COT achieves F1-scores of 0.898 for Task A (binary classification) and 0.307 for Task B (LLM identification)
- CoT reasoning significantly outperforms baseline models without reasoning augmentation
- The framework provides interpretable explanations that reveal stylistic patterns unique to different LLMs
- Dual-task joint optimization creates shared representations beneficial for both detection and attribution tasks

## Why This Works (Mechanism)

### Mechanism 1: Reasoning-Augmented Feature Enrichment
Injecting CoT reasoning into the input pipeline enriches the classifier's representation space with explanatory signals that improve discrimination between human and AI-generated text. A teacher model (LLaMA) generates reasoning for each document by answering "Why is this particular document generated by this label?" This reasoning is concatenated with the original text and passed to BERT/RoBERTa. The classifier learns joint representations encoding both surface-level text features and meta-level explanatory patterns. The core assumption is that the teacher model's reasoning captures meaningful stylistic distinctions that are learnable by the student classifier.

### Mechanism 2: Dual-Task Joint Optimization
Training both Task A (binary classification) and Task B (LLM identification) jointly creates shared representations that benefit both objectives. The total loss combines binary cross-entropy for Task A and cross-entropy for Task B. The shared encoder learns features useful for both distinguishing human vs. AI text and differentiating among specific LLMs. The core assumption is that stylistic features distinguishing LLMs from humans also help discriminate among different LLMs.

### Mechanism 3: Stylistic Pattern Recognition via Explanation Supervision
CoT reasoning surfaces stylistic patterns (vocabulary choices, sentence structure, emotional tone) that serve as discriminative signals for both detection and attribution. The reasoning explicitly describes patterns like "emotional tone and specific product-related commentary suggest a human review." The classifier internalizes these pattern descriptions as features during fine-tuning. The core assumption is that the reasoning accurately reflects genuine stylistic differences rather than spurious correlations.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed here: Understanding how prompting a model to explain its reasoning generates auxiliary training signals is essential to grasping the COT_Finetuned approach.
  - Quick check question: Given a product review labeled "human," what type of reasoning explanation would you prompt a language model to generate?

- **Concept: Multi-Task Learning with Shared Representations**
  - Why needed here: The framework jointly optimizes binary and multi-class classification using a shared encoder; understanding gradient flow across tasks is critical.
  - Quick check question: If Task B (LLM identification) has lower F1 than Task A (binary detection), what does this suggest about the feature overlap between tasks?

- **Concept: Transfer Learning from Pre-trained Language Models**
  - Why needed here: The method fine-tunes BERT/RoBERTa on domain-specific data; understanding what PLMs already encode helps diagnose what additional signal CoT provides.
  - Quick check question: What syntactic and semantic features does BERT already capture that might be relevant to distinguishing human vs. AI text?

## Architecture Onboarding

- **Component map:** Input Layer -> Reasoning Generator (LLaMA) -> Shared Encoder (BERT/RoBERTa) -> Task A Head (Sigmoid) + Task B Head (Softmax) -> Loss Aggregator
- **Critical path:** 1) Generate reasoning r_i for all training samples using LLaMA, 2) Format inputs as [document text] + [label] + [reasoning], 3) Fine-tune encoder and classification heads jointly, 4) At inference: Pass test document through model to get both Label_A and Label_B
- **Design tradeoffs:** Using LLaMA for reasoning generation adds computational cost and dependency on external model; joint training may cause task competition (Task B F1 significantly lower than Task A); epochs range 50-250 suggests sensitivity to training duration
- **Failure signatures:** Low Task B F1 with high Task A F1 suggests shared features insufficient for fine-grained LLM discrimination; reasoning quality degradation if LLaMA produces generic or incorrect explanations; overfitting on limited data
- **First 3 experiments:** 1) Ablation study: Train BERT without CoT reasoning to quantify marginal improvement from reasoning features, 2) Reasoning quality analysis: Manually annotate LLaMA-generated reasoning for accuracy and correlate with classification correctness, 3) Cross-dataset generalization: Train on provided dataset, test on external AI-generated text corpus to assess generalization

## Open Questions the Paper Calls Out

The paper itself doesn't explicitly call out open questions, but several emerge from the methodology and results:

### Open Question 1
Can the COT_Finetuned framework maintain its performance when applied to LLMs not included in the training set, or does it require retraining for new models? The framework's ability to identify novel or unseen LLMs was not tested, yet new models are released continuously. Zero-shot or few-shot evaluation on text generated by LLMs released after the training data collection would resolve this.

### Open Question 2
Does the quality and consistency of LLaMA-generated reasoning directly impact classification performance, and could alternative reasoning sources improve results? The training process generates reasoning via LLaMA, but the reasoning loss is not incorporated into the total loss function. Ablation studies comparing LLaMA-generated reasoning against human-authored reasoning or reasoning from other LLMs, with correlation analysis between reasoning quality scores and classification accuracy, would resolve this.

### Open Question 3
What specific stylistic patterns does the model learn to distinguish between different LLMs, and are these patterns robust to paraphrasing or adversarial attacks? While the paper claims stylistic pattern identification, no analysis is provided on what these patterns are or whether they persist under text manipulation. Feature importance analysis combined with robustness testing using paraphrasing tools would resolve this.

## Limitations

- **Weak multi-class attribution:** Task B (LLM identification) achieves only F1=0.307, indicating fundamental limitations in distinguishing between similar LLMs
- **Dataset dependency:** All validation occurs on the De-Factify 4.0 dataset with specific text types, raising concerns about cross-domain generalization
- **Teacher model reliance:** Performance depends entirely on the quality of LLaMA-generated reasoning, which may propagate biases or hallucinations

## Confidence

- **High Confidence:** Binary classification (Task A) performance improvements from CoT integration. The F1=0.898 result for BERT+COT is well-supported by direct experimental comparison with baseline models.
- **Medium Confidence:** The interpretability benefits from CoT reasoning. While the paper shows examples of generated explanations and claims enhanced transparency, there's limited systematic evaluation of how useful these explanations are to human users.
- **Low Confidence:** The claim that CoT reasoning provides "insights into the stylistic patterns unique to different LLMs" lacks rigorous validation. The weak Task B performance and absence of cross-model generalization studies undermine confidence in this mechanism.

## Next Checks

1. **Cross-Model Generalization Test:** Evaluate the trained model on AI-generated text from LLMs not present in the training set (e.g., newer models or different domains). Measure whether detection accuracy drops significantly and whether reasoning quality remains consistent.

2. **Reasoning Quality Impact Analysis:** Systematically assess the correlation between LLaMA reasoning quality (accuracy, specificity, relevance) and classification performance. Manually annotate reasoning samples for quality, then stratify results by reasoning quality tiers to quantify the dependency.

3. **Task B Ablation with Larger Training Sets:** Train Task B independently with increased training data volume (double or triple) to determine if the low F1=0.307 is due to insufficient data or fundamental limitations in the feature space. Compare against joint training performance to evaluate negative transfer effects.