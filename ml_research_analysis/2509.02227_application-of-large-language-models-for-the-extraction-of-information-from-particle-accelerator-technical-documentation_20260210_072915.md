---
ver: rpa2
title: Application Of Large Language Models For The Extraction Of Information From
  Particle Accelerator Technical Documentation
arxiv_id: '2509.02227'
source_url: https://arxiv.org/abs/2509.02227
tags:
- answer
- context
- documentation
- llms
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses knowledge retention challenges in particle
  accelerator facilities by developing a retrieval-augmented generation (RAG) system
  using large language models to extract information from technical documentation.
  The method employs a two-stage pipeline: document preprocessing with chunking and
  embedding storage, followed by runtime retrieval and generation using the Gemma
  3 model.'
---

# Application Of Large Language Models For The Extraction Of Information From Particle Accelerator Technical Documentation

## Quick Facts
- arXiv ID: 2509.02227
- Source URL: https://arxiv.org/abs/2509.02227
- Reference count: 16
- The study develops a RAG system using Gemma 3 that achieves 0.90-0.93 confidence on domain-specific QA pairs with 800-character chunks

## Executive Summary
This paper addresses knowledge retention challenges in particle accelerator facilities by developing a retrieval-augmented generation (RAG) system using large language models to extract information from technical documentation. The method employs a two-stage pipeline: document preprocessing with chunking and embedding storage, followed by runtime retrieval and generation using the Gemma 3 model. Evaluation on 100 domain-specific question-answer pairs showed that 800-character chunks with Top-5 retrieval achieved the highest answer accuracy (0.90-0.93 confidence). The system demonstrated strong performance in answering questions from English and German documents, though German queries initially lagged until translation improved results.

## Method Summary
The approach combines document preprocessing with runtime retrieval and generation. In the preprocessing stage, technical documents are converted to text using MinerU, chunked into various sizes (100-2000 characters), and embedded using BGE-M3 before storage in a vector database. At runtime, user queries are embedded and used to retrieve the top-k most similar chunks from the database, which are then concatenated with the query and passed to the Gemma 3 model for answer generation. The system was evaluated using 100 domain-specific question-answer pairs covering topics like device naming conventions, rack locations, and beam parameters, with answers validated by subject matter experts.

## Key Results
- Optimal chunk size of 800 characters achieved highest accuracy with Top-5 retrieval (0.90-0.93 confidence)
- English document queries outperformed German queries initially, but translation improved German performance
- System successfully retrieved information about naming schemes, rack locations, and beam parameters from technical documentation
- Current limitations include inability to effectively process non-textual elements like tables and figures

## Why This Works (Mechanism)
The system works by creating a semantic index of technical documentation through embedding and vector storage, enabling efficient similarity-based retrieval of relevant context for any given query. The retrieval-augmented generation approach combines the precision of information retrieval with the reasoning capabilities of large language models, allowing the system to generate accurate answers based on specific document content rather than relying on model pretraining alone.

## Foundational Learning
- **Document Chunking**: Breaking documents into manageable pieces for embedding and retrieval; needed to balance context coverage with retrieval precision; quick check: verify chunk boundaries don't split relevant information
- **Vector Embeddings**: Converting text to numerical representations that capture semantic meaning; needed for efficient similarity search in large document collections; quick check: test embedding similarity for semantically related queries
- **RAG Architecture**: Combining retrieval and generation for knowledge-intensive tasks; needed to ground LLM responses in specific document content; quick check: compare RAG answers to baseline LLM without retrieval
- **Evaluation Metrics**: Using domain-specific question-answer pairs for validation; needed to assess system performance on realistic technical queries; quick check: validate answers with subject matter experts
- **Language Processing**: Handling multilingual documents and queries; needed for international collaboration in accelerator facilities; quick check: test retrieval accuracy across different languages

## Architecture Onboarding

Component map:
PDF/Text -> MinerU -> Chunker -> BGE-M3 Embedder -> Vector Database <- Gemma 3 Generator

Critical path: Document preprocessing (MinerU → chunker → embedder → vector DB) → Runtime retrieval (query embedding → similarity search → top-k retrieval) → Answer generation (concatenate query + retrieved chunks → Gemma 3)

Design tradeoffs:
- Chunk size balancing: smaller chunks improve precision but may miss context; larger chunks provide more context but reduce retrieval specificity
- Retrieval parameters: Top-k selection affects answer completeness versus noise introduction
- Language handling: Translation versus native language processing for multilingual documents

Failure signatures:
- Incomplete answers when relevant information spans multiple chunks
- Hallucinations when retrieved context is insufficient or ambiguous
- Poor retrieval when queries use domain-specific terminology not well-represented in embeddings

First experiments:
1. Test different chunk sizes (100, 400, 800, 2000 characters) with fixed Top-5 retrieval
2. Compare Top-1, Top-3, and Top-5 retrieval performance on same query set
3. Evaluate English versus German query performance with and without translation

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can pre-processing techniques like automatic figure captioning effectively enable the retrieval of visual information (schematics, plots) in legacy technical documentation?
- Basis: [explicit] The authors identify the inability to retrieve information from non-textual elements as a "significant limitation" and propose "automatic figure captioning" as a specific future strategy.
- Why unresolved: The current pipeline relies on text-based extraction (MinerU) and cannot semantically index the content of images or complex figures.
- What evidence would resolve it: Evaluation of retrieval accuracy on a test set of visual queries using generated captions compared to human-indexed references.

### Open Question 2
- Question: Do multi-modal embedding models outperform text-only methods for retrieving domain-specific technical knowledge contained in schematics and photos?
- Basis: [explicit] The paper lists the "evaluation of multi-modal embedding models" as a distinct path for future work to address visual data retrieval.
- Why unresolved: The current system uses a text-only embedding model (BGE-M3), and the efficacy of multi-modal vector databases for this specific particle accelerator domain is untested.
- What evidence would resolve it: Benchmarking visual query performance between a multi-modal model and the current caption-based approach.

### Open Question 3
- Question: How can structured data sources (Excel, databases) be integrated into the RAG pipeline to recover the missing "naming schemes" and "rack locations"?
- Basis: [inferred] The authors note that "large part of these information are present in excel tables... which we cannot process yet," forcing manual text file creation.
- Why unresolved: The current methodology processes only PDFs and flat text files, lacking a mechanism to ingest or query structured tabular data while preserving relationships.
- What evidence would resolve it: Successful end-to-end extraction of a data point (e.g., a specific rack location) that exists solely in the unprocessed Excel files.

## Limitations
- Current system cannot effectively retrieve information from non-textual elements like figures, tables, and diagrams
- Performance on German language queries initially lagged behind English until translation was applied
- Relies on manual text file creation for structured data sources like Excel tables

## Confidence
- Overall RAG system effectiveness: High
- Language performance claims: Medium
- Claims regarding handling all technical documentation types: Low

## Next Checks
1. Test the RAG system's performance on documents containing significant figures, tables, and diagrams to quantify the impact of non-textual content on retrieval accuracy.
2. Conduct a longitudinal study to assess knowledge retention improvements for facility personnel using the system compared to traditional documentation search methods.
3. Evaluate the system with multi-lingual domain experts to identify potential biases or gaps in cross-language retrieval and generation performance.