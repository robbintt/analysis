---
ver: rpa2
title: Offline Reinforcement Learning with Generative Trajectory Policies
arxiv_id: '2510.11499'
source_url: https://arxiv.org/abs/2510.11499
tags:
- learning
- policy
- generative
- training
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Generative Trajectory Policies (GTPs), a
  new policy class for offline reinforcement learning that unifies modern generative
  models through a continuous-time ODE framework. By learning the full solution map
  of the underlying ODE, GTPs achieve both expressiveness and computational efficiency,
  resolving the trade-off between slow, high-fidelity sampling and fast, degraded
  performance in prior methods.
---

# Offline Reinforcement Learning with Generative Trajectory Policies

## Quick Facts
- arXiv ID: 2510.11499
- Source URL: https://arxiv.org/abs/2510.11499
- Reference count: 40
- This paper introduces Generative Trajectory Policies (GTPs), achieving state-of-the-art performance on D4RL benchmarks through a unified ODE framework.

## Executive Summary
Generative Trajectory Policies (GTPs) introduce a novel policy class for offline reinforcement learning that learns the full solution map of a continuous-time ODE governing generative trajectories. By unifying modern generative models through this continuous-time framework, GTPs resolve the fundamental trade-off between slow, high-fidelity sampling and fast, degraded performance found in prior methods. The approach achieves state-of-the-art performance on D4RL benchmarks, particularly excelling on challenging AntMaze tasks while maintaining computational efficiency during inference.

## Method Summary
GTP learns a continuous-time generative trajectory policy by directly modeling the solution map (flow map) of an ODE rather than just its vector field or a single-step approximation. The policy Φθ maps noisy actions at time t to cleaner actions at time τ, enabling flexible inference that balances speed and quality. The training combines a surrogate score approximation for stability (replacing iterative ODE solvers with closed-form targets anchored to dataset samples) with a variational, advantage-weighted objective for policy improvement. This creates a unified representation that supports both fast inference and expressive trajectory modeling, with the actor taking the form of a neural network representing the solution map and a critic providing value estimates.

## Key Results
- Achieved perfect scores on several AntMaze tasks, demonstrating superior performance on long-horizon navigation problems
- Significantly outperformed prior generative and offline RL methods on D4RL benchmarks in both expressiveness and efficiency
- Resolved the trade-off between slow, high-fidelity sampling and fast, degraded performance through unified ODE solution map learning

## Why This Works (Mechanism)

### Mechanism 1: Unified ODE Solution Map Learning
Learning the entire solution map of a generative ODE, rather than just the vector field or a single-step distillation, allows for flexible inference that balances speed and quality. The policy Φθ(s, at, t, τ) maps a noisy action at time t to a cleaner action at time τ directly. By training on the integral form of the ODE, the model learns to "jump" arbitrary intervals, avoiding the accumulation of errors typical in multi-step solvers while retaining the fidelity lost in aggressive distillation.

### Mechanism 2: Surrogate Score for Training Stability
Replacing the iterative ODE solver with a closed-form surrogate score during training prevents error propagation and significantly reduces computational cost. Instead of using a numerical solver (which relies on the model's own imperfect estimates to generate targets), GTP uses a surrogate f̃(xt, t) = (xt - x)/t derived from the ground-truth data sample x. This anchors the supervision signal, preventing the "vicious cycle" of unstable updates found in self-referential training.

### Mechanism 3: Variational Advantage Weighting
An exponential advantage-weighted objective aligns the generative policy with high-value behaviors more robustly than linear Q-learning actor losses. The loss is re-weighted by w(s, a) = exp(ηA(s,a)). This mathematically derives from KL-regularized policy optimization, forcing the policy to imitate the behavior policy πBC but with probability mass shifted toward actions with higher advantage estimates.

## Foundational Learning

- **Concept: Ordinary Differential Equations (ODEs) in Generative Models**
  - Why needed here: GTP frames policy generation not as a single-step decision or a purely iterative diffusion process, but as learning a continuous flow.
  - Quick check question: Can you explain the difference between learning a vector field f(x, t) (velocity) vs. learning a solution map Φ(x, t, s) (position)?

- **Concept: KL-Regularized Reinforcement Learning**
  - Why needed here: The paper derives its policy improvement step from this framework. Understanding this explains why the weird-looking exponential weight is theoretically sound.
  - Quick check question: Why does maximizing Q - (1/η)DKL(π || πBC) result in an optimal policy that looks like a weighted version of the behavior policy?

- **Concept: Distribution Shift in Offline RL**
  - Why needed here: The entire architecture (generative modeling + conservative weighting) is designed to prevent the policy from querying actions not supported by the dataset.
  - Quick check question: If a generative policy produces an action with high Q-value but low probability in the dataset, what risk does this pose to the critic?

## Architecture Onboarding

- **Component map:** Offline dataset D -> Double Critic Qφ -> GTP Actor Φθ -> Advantage-weighted losses -> Policy update
- **Critical path:** 1) Sample (s, a, r, s') from D, 2) Update Critic via TD-learning, 3) Compute surrogate targets using dataset action a and noise z, 4) Calculate A(s,a) and normalize weights, 5) Update Actor with weighted Consistency Loss and Flow Loss
- **Design tradeoffs:** Training is computationally heavy due to optimizing the trajectory map, but inference is fast (flexible K steps). The unified flow map is highly expressive, but requires the surrogate score to prevent bootstrapping instability.
- **Failure signatures:** Critic Divergence if advantage weights are not normalized/clipped; Mode Collapse if trajectory consistency loss is over-weighted; Slow Convergence if dynamic timestep scheduling is omitted.
- **First 3 experiments:** 1) Behavior Cloning (BC) Sanity Check: Run GTP with η=0 on a simple D4RL task (e.g., halfcheetah-m) to validate generative capacity independent of RL signals, 2) Score Approximation Ablation: Replace surrogate score with standard ODE solver on hopper-medium to observe training time and convergence effects, 3) AntMaze Performance: Evaluate on antmaze-medium-play as the "stress test" for long-horizon stitching and multi-modality.

## Open Questions the Paper Calls Out
- Can the substantial training time required by Generative Trajectory Policies be significantly reduced while maintaining their expressiveness and efficiency?
- Can teacher-free training objectives (such as identity-based formulations or Mean Flows) be stabilized to function effectively in standard GPU-based reinforcement learning environments?
- Does the Generative Trajectory Policy framework scale effectively to high-dimensional visual domains or complex robotic manipulation tasks beyond the D4RL locomotion benchmarks?

## Limitations
- The substantial training time required by GTP remains an important avenue for future research, despite fast inference
- Teacher-free training objectives resulted in prohibitive memory/runtime overhead or severe divergence in the current implementation
- The method is currently evaluated only on low-dimensional state vector benchmarks, not on visual domains or complex robotic manipulation tasks

## Confidence
- **High confidence:** The unified ODE framework for generative trajectory policies is internally consistent and well-grounded in prior work on generative modeling with ODEs
- **Medium confidence:** The theoretical analysis of the surrogate score and advantage weighting appears sound, but the practical gap between theory and implementation isn't fully characterized
- **Low confidence:** Claims about computational efficiency relative to prior generative methods lack quantitative comparison and wall-clock timing data

## Next Checks
1. Ablation study on surrogate score: Implement a version using numerical ODE solvers instead of the closed-form surrogate and measure training stability, convergence speed, and final performance across multiple D4RL tasks
2. Critic robustness evaluation: Systematically inject noise or bias into the advantage estimates and measure how performance degrades to validate sensitivity to critic quality
3. Long-horizon generalization: Test GTP on tasks requiring longer temporal reasoning than the training tasks to assess whether trajectory learning genuinely captures multi-step dependencies rather than memorizing short patterns