---
ver: rpa2
title: 'PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical
  Reasoning in Large Models'
arxiv_id: '2506.10716'
source_url: https://arxiv.org/abs/2506.10716
tags:
- reasoning
- premise
- arxiv
- tokens
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PREMISE introduces a prompt-only method to reduce reasoning overhead
  in large reasoning models without modifying model weights. It combines trace-level
  diagnostics for overthinking and underthinking with gradient-inspired prompt optimization,
  jointly balancing token length and answer validity through a multi-objective textual
  search.
---

# PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical Reasoning in Large Models

## Quick Facts
- arXiv ID: 2506.10716
- Source URL: https://arxiv.org/abs/2506.10716
- Reference count: 15
- Primary result: Matches or exceeds baseline accuracy while reducing reasoning tokens by up to 87.5% and cost by 69–82% on mathematical reasoning tasks

## Executive Summary
PREMISE introduces a prompt-only method to reduce reasoning overhead in large reasoning models without modifying model weights. It combines trace-level diagnostics for overthinking and underthinking with gradient-inspired prompt optimization, jointly balancing token length and answer validity through a multi-objective textual search. Unlike prior methods, PREMISE operates in a single-pass black-box interface, making it directly applicable to commercial LLMs. Evaluated on GSM8K, SVAMP, and Math500, PREMISE matches or exceeds baseline accuracy while significantly reducing inference costs, establishing prompt-level optimization as a practical and scalable path to efficient LRM inference without compromising reasoning quality.

## Method Summary
PREMISE is a prompt-only approach that optimizes mathematical reasoning efficiency in large reasoning models through trace-level diagnostics and multi-objective textual optimization. The method defines overthinking efficiency metrics based on the proportion of unnecessary tokens beyond minimal correct traces, and underthinking metrics based on the earliest point of irreversible deviation from correct reasoning. Using these diagnostics, PREMISE employs a scalarization technique to balance accuracy and token length through natural language gradients, updating the prompt template to maximize joint objectives. The approach operates as a single-pass black-box optimization that can be applied to commercial LLMs without model access, achieving significant token and cost reductions while maintaining or improving accuracy on mathematical reasoning benchmarks.

## Key Results
- GSM8K accuracy: 96% (PREMISE) vs 96% (baseline) with Claude, with 87.5% token reduction
- Math500 accuracy: 92% (PREMISE) vs 91% (baseline) with Gemini, with 80% token reduction
- Cost reduction: 69–82% dollar cost savings across benchmarks
- Token reduction: Up to 87.5% fewer reasoning tokens while maintaining or improving accuracy

## Why This Works (Mechanism)

### Mechanism 1: Trace-Level Diagnostic Metrics
The framework quantifies reasoning inefficiency by measuring distance between generated traces and optimal trace length. Overthinking efficiency is defined as $\eta_O(r, q) = L^*(q) / L(r)$ where $L^*(q)$ is the shortest correct trace length. Underthinking is measured by identifying the latest prefix $k^*$ from which a correct path was still possible. This transforms qualitative verbosity into quantitative optimization targets. The core assumption is that a ground truth reasoning path or verifiable shortest correct trace exists and can be approximated for the training set.

### Mechanism 2: Multi-Objective Textual Optimization
Prompt optimization navigates the trade-off between brevity and correctness by treating them as competing objectives in a Pareto-optimal search space. The method uses scalarization $\delta = \lambda \delta_{acc} + (1 - \lambda) \delta_{len}$ with textual gradients (natural language critiques) for accuracy and length. These update the prompt template to maximize the joint objective. The core assumption is that the underlying LRM possesses sufficient instruction-following capability to interpret natural language feedback and adjust its generation strategy without weight updates.

### Mechanism 3: Strategic Early Commitment (Behavioral Scaffolding)
Verbose reasoning is largely caused by redundant backtracking and double-checking loops, which can be suppressed by enforcing a structured, linear reasoning format in the prompt. The optimized prompt structure forces the model to commit to a numeric plan early, removing the behavioral slack that allows for exploratory dead-ends. The core assumption is that the model's baseline inefficiency is behavioral (learned habits of self-correction) rather than strictly capability-based.

## Foundational Learning

- **Concept: Pareto Efficiency (Multi-Objective Optimization)**
  - Why needed: PREMISE seeks the frontier where accuracy is high and token count is low, not accuracy alone
  - Quick check: If a prompt reduces tokens by 90% but cuts accuracy in half, is it Pareto-optimal compared to the baseline? (Likely no, unless baseline accuracy was very low)

- **Concept: Chain-of-Thought (CoT) Tokenization**
  - Why needed: The paper differentiates between Input, Thinking (hidden reasoning), and Completion tokens
  - Quick check: Why does the paper note different success rates for Claude vs. OpenAI o1 regarding the "Thinking" channel? (Claude exposes/allows steering of hidden thoughts better than o1's opaque completion stream)

- **Concept: Black-Box Gradient Descent**
  - Why needed: The method optimizes a prompt without model weights using metaphorical "textual gradients"
  - Quick check: How do you calculate a "gradient" if you cannot backpropagate through the model? (Use a separate evaluation process to generate textual critiques that serve as directional update signals)

## Architecture Onboarding

- **Component map:** Trace Analyzer -> Prompt Optimizer -> Inference Client
- **Critical path:** The Prompt Optimizer's ability to generate meaningful "textual gradients" ($\delta_{acc}$ and $\delta_{len}$). If critiques are generic, the optimization loop oscillates rather than converging.
- **Design tradeoffs:**
  - Claude vs. OpenAI o1: Claude allows separation of reasoning/completion tokens, making cost reduction effective. o1 hides this, so PREMISE may inadvertently lengthen the visible trace.
  - Compression Ratio: Setting $\lambda$ too high (favoring brevity) causes accuracy drop on proof-heavy tasks like MATH-500.
  - Single-Pass vs. Test-Time Compute: PREMISE sacrifices potential accuracy gains of "Best-of-N" or tree search sampling for deterministic speed/cost savings.
- **Failure signatures:**
  - "Over-compression" (Underthinking): Sudden accuracy collapse on complex problems because the model skips necessary intermediate justifications
  - "Ignore" mode: The model generates standard verbose CoT despite the prompt (observed with OpenAI o1)
  - "Metric Drift": Optimization reduces token count but introduces subtle logic errors that pass weak validators
- **First 3 experiments:**
  1. Baseline Calibration: Run standard CoT vs. PREMISE on GSM8K with Claude 3.7 to verify ~80% token reduction and stable accuracy
  2. Ablation Profiling: Isolate the $\lambda$ parameter by running optimization with $\lambda=0$ (accuracy only) and $\lambda=1$ (length only) to confirm Pareto frontier shape
  3. Stress Test on Complexity: Evaluate on MATH-500 subset to find the "break point" where prompt causes underthinking

## Open Questions the Paper Calls Out
None

## Limitations
- Metric computation requires access to $L^*(q)$ (minimal correct trace length) and $k^*(r,q)$ (last valid prefix), which are not specified how to obtain in practice
- Method shows strong results with Claude and Gemini but encounters resistance from OpenAI o1, suggesting model-dependent effectiveness
- Significant 14% accuracy drop on MATH-500 with Gemini indicates compression strategy may underthink on problems requiring longer exploratory reasoning

## Confidence

- **High Confidence:** The multi-objective optimization framework is sound and well-specified, with ablation studies supporting the necessity of balancing both objectives
- **Medium Confidence:** Trace-level diagnostic metrics are theoretically justified but practical computation remains underspecified
- **Low Confidence:** Universal applicability across LRMs is questionable due to observed model-dependent effectiveness differences

## Next Checks

1. Implement the overthinking and underthinking metrics on a small benchmark (50 GSM8K problems) to test whether $L^*(q)$ can be reliably approximated without exhaustive search

2. Run PREMISE with the same prompt optimization on OpenAI o1, Claude 3.7, and Gemini 2.5 to verify model-dependent effectiveness reproduces in your environment

3. Systematically vary $\lambda$ (accuracy-length scalarization weight) on MATH-500 problems of increasing difficulty to identify the precise compression threshold where underthinking begins