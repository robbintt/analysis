---
ver: rpa2
title: 'Freeze and Cluster: A Simple Baseline for Rehearsal-Free Continual Category
  Discovery'
arxiv_id: '2503.09106'
source_url: https://arxiv.org/abs/2503.09106
tags:
- learning
- novel
- continual
- classes
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Freeze and Cluster: A Simple Baseline for Rehearsal-Free Continual Category Discovery

## Quick Facts
- **arXiv ID**: 2503.09106
- **Source URL**: https://arxiv.org/abs/2503.09106
- **Authors**: Chuyu Zhang; Xueyang Yu; Peiyan Gu; Xuming He
- **Reference count**: 19
- **Primary result**: None

## Executive Summary
The paper "Freeze and Cluster: A Simple Baseline for Rehearsal-Free Continual Category Discovery" presents a method for addressing the challenge of discovering new categories in a continual learning setting without relying on rehearsal techniques. The approach aims to provide a straightforward baseline that can be used for comparison against more complex methods in the field of continual category discovery.

## Method Summary
The method involves freezing a pretrained model and applying clustering techniques to discover new categories as they appear in a continual learning scenario. The approach is designed to be rehearsal-free, meaning it does not store or replay previously seen data during the learning process. The specific clustering algorithm and any fine-tuning procedures are not detailed in the available information.

## Key Results
- No specific quantitative results are provided in the abstract
- No performance metrics or comparisons with baseline methods are mentioned
- No ablation studies or parameter sensitivity analyses are reported

## Why This Works (Mechanism)
The paper does not provide a detailed mechanism explanation in the available abstract. The proposed approach likely works by leveraging the frozen model's ability to extract meaningful features from new data, which can then be grouped using clustering algorithms to identify new categories. The absence of rehearsal may help in reducing computational overhead and storage requirements.

## Foundational Learning
- **Continual Learning**: The ability of a model to learn from a continuous stream of data without forgetting previously learned information. This is needed to understand the problem setting and why rehearsal-free methods are valuable.
- **Feature Extraction**: The process of transforming raw input data into a more abstract representation that can be used for downstream tasks. This is needed to understand how the frozen model contributes to the method.
- **Clustering Algorithms**: Unsupervised learning techniques used to group similar data points together. This is needed to understand how new categories are discovered without labeled data.
- **Model Freezing**: The practice of preventing certain parts of a neural network from being updated during training. This is needed to understand the computational efficiency and stability aspects of the approach.
- **Category Discovery**: The task of identifying and learning new classes or categories that were not present in the initial training data. This is needed to understand the specific problem being addressed.
- **Rehearsal-free Methods**: Approaches that do not store or replay old data during continual learning. This is needed to understand the novelty and potential benefits of the proposed method.

## Architecture Onboarding
- **Component Map**: Pretrained Model -> Feature Extractor -> Clustering Algorithm -> Category Discovery
- **Critical Path**: The critical path involves passing new data through the frozen model to extract features, which are then clustered to discover new categories. The clustering results are used to update the model's understanding of the category space.
- **Design Tradeoffs**: The main tradeoff is between simplicity and performance. Using a frozen model and clustering is computationally efficient but may not capture complex relationships between new and old categories as effectively as more sophisticated methods.
- **Failure Signatures**: Potential failures could include poor clustering results due to feature space misalignment, inability to handle concept drift, or catastrophic forgetting of old categories.
- **First Experiments**:
  1. Test the method on a simple dataset with clearly separable categories to verify basic functionality.
  2. Evaluate performance on a dataset with gradual category introduction to assess continual learning capabilities.
  3. Compare results with a baseline that uses rehearsal to quantify the performance trade-off.

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of specific results or performance metrics makes it impossible to evaluate the method's effectiveness
- No comparison with existing rehearsal-free methods or discussion of relative advantages
- Absence of detailed implementation information hinders reproducibility and practical adoption

## Confidence
- Method novelty and contribution: Low
- Experimental rigor and results: Low
- Comparison with existing methods: Low
- Reproducibility and implementation details: Low

## Next Checks
1. Obtain the full experimental results showing performance comparisons against established baselines for continual category discovery
2. Request detailed implementation details and hyperparameter settings to enable reproducibility
3. Verify the novelty claims by conducting a comprehensive literature review of existing rehearsal-free approaches to category discovery in continual learning settings