---
ver: rpa2
title: Counterfactual Strategies for Markov Decision Processes
arxiv_id: '2505.09412'
source_url: https://arxiv.org/abs/2505.09412
tags:
- counterfactual
- strategy
- strategies
- distance
- probability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces counterfactual strategies for Markov Decision
  Processes (MDPs) to explain how minimal changes to a strategy can lead to different
  outcomes in sequential decision-making tasks. The authors formalize counterfactual
  strategies as solutions to non-linear optimization problems, minimizing strategy
  distance while ensuring reachability probability below a threshold.
---

# Counterfactual Strategies for Markov Decision Processes

## Quick Facts
- arXiv ID: 2505.09412
- Source URL: https://arxiv.org/abs/2505.09412
- Reference count: 24
- One-line primary result: This paper introduces counterfactual strategies for Markov Decision Processes (MDPs) to explain how minimal changes to a strategy can lead to different outcomes in sequential decision-making tasks.

## Executive Summary
This paper introduces counterfactual strategies for Markov Decision Processes (MDPs) to explain how minimal changes to a strategy can lead to different outcomes in sequential decision-making tasks. The authors formalize counterfactual strategies as solutions to non-linear optimization problems, minimizing strategy distance while ensuring reachability probability below a threshold. They extend the approach to generate diverse counterfactual strategies optimized for diversity. Experiments on four real-world datasets demonstrate that counterfactual strategies can be computed efficiently for MDPs with thousands of states and tens of thousands of transitions, and that diverse strategies provide novel recourse possibilities while remaining close to the initial strategy.

## Method Summary
The method frames counterfactual strategy synthesis as a non-linear MIQCQP optimization problem. Given an MDP with an initial strategy σ that reaches an undesired state t with probability exceeding threshold γ, the approach finds a counterfactual strategy σ* minimizing distance metrics (sparsity, average, and maximum changes) while ensuring Pr(s₀, t) ≤ γ. The encoding uses variables for strategy probabilities, reachability probabilities, distance measures, and binary indicators for changed states. The method extends to generate diverse counterfactual strategies by iteratively adding a diversity term to the objective. Experiments use four real-world datasets (GrepS, BPIC12, BPIC17, MSSD) with MDPs learned via Alergia algorithm.

## Key Results
- Counterfactual strategies can be computed efficiently for MDPs with thousands of states and tens of thousands of transitions
- Diverse counterfactual strategies provide novel recourse possibilities while remaining close to the initial strategy
- The method successfully generates valid counterfactual strategies for real-world sequential decision-making tasks

## Why This Works (Mechanism)
The method works by formulating the entire synthesis problem as a single, large optimization problem. The key insight is that strategy probabilities can be represented as continuous variables, while binary variables track whether a state's strategy has changed. The Bellman equation constraint ensures reachability probabilities are computed correctly, and the distance metrics (sparsity, average, max) provide interpretable measures of strategy change. By minimizing these distances while enforcing the reachability threshold, the solver finds the closest strategy that changes the outcome. The diversity extension works by adding a determinant-based term that encourages generated strategies to be structurally different from each other.

## Foundational Learning

Concept: **Markov Decision Process (MDP)**
- Why needed here: This is the core model. You must understand states, actions, probabilistic transitions, and the concept of a "strategy" (a policy mapping states to action probabilities).
- Quick check question: Can you explain how a strategy in an MDP differs from a simple action? What does it mean for an MDP transition to be probabilistic?

Concept: **Counterfactual Explanation**
- Why needed here: The goal is not just to find a better strategy, but to find the *closest* one that changes the outcome, providing an explanation for "what should have been done differently."
- Quick check question: In this context, what are the two main conditions a strategy must satisfy to be a "counterfactual strategy"?

Concept: **Non-linear Optimization (MIQCQP)**
- Why needed here: The paper's solution is to frame the entire synthesis problem as a single, large optimization problem. You need to know that variables represent strategy probabilities and that the solver finds values minimizing an objective while satisfying constraints.
- Quick check question: What is the primary objective being minimized in the paper's optimization problem? What is the key constraint that ensures the outcome changes?

## Architecture Onboarding

Component map: MDP Model -> MIQCQP Encoder -> Gurobi Solver -> Counterfactual Strategies

Critical path: The correctness of the encoding is paramount. An error in translating the Bellman equation constraint (Eq. 7) or the distance metrics (Eq. 9-13) will render the entire solution invalid.

Design tradeoffs: The method trades off global optimality (guaranteed by the MIQCQP formulation) against scalability for very large state spaces, as seen in the timeouts for larger MSSD models.

Failure signatures: The solver may return `Infeasible` (no strategy can meet the threshold), `Timeout`, or a suboptimal solution. A high diversity parameter λ might cause convergence issues or impractical strategies.

First 3 experiments:
1. **Toy Example Validation:** Implement the loan application MDP (Fig. 1) and verify the solver returns the counterfactual strategy described in Example 2.
2. **Scalability Test:** Generate random MDPs of increasing size (states/transitions) and measure the solver's runtime to establish scaling laws.
3. **Diversity Analysis:** Generate 5 diverse counterfactuals for a fixed MDP and manually inspect if they represent meaningfully different recourse options or just trivial variations.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can linearization techniques (e.g., sequential convex programming) effectively approximate optimal counterfactual strategies while significantly reducing the computational cost of the non-convex MIQCQP?
- Basis: The authors state in the Conclusion: "Approximations of the optimization problem, such as those based on linearization... promise to reduce the computation time while producing local optimal strategies."
- Why unresolved: Theorem 2 proves the current optimization problem is generally non-convex, and Table 2 shows timeouts for larger MSSD models (up to 300 timeouts per model), indicating scalability limits for exact solutions.
- What evidence would resolve it: A comparative analysis showing the trade-off between runtime reduction and error margins for approximated strategies versus the exact optimal strategies on the MSSD dataset.

**Open Question 2**
- Question: How can counterfactual strategy synthesis be generalized to Stochastic Games to handle scenarios where the environment (service provider) actively adapts to the user's recourse actions?
- Basis: The Conclusion proposes future work on "scenarios where the environment... adapts to counterfactual changes. This will require generalizing our counterfactual strategies from MDPs to Stochastic Games."
- Why unresolved: The current method assumes a static transition function δ in the MDP; it lacks the theoretical framework to model or optimize against an adversarial or adaptive second player.
- What evidence would resolve it: A formal extension of the encoding to two-player games and an algorithm that converges to a strategy satisfying the reachability threshold under environment adaptation.

**Open Question 3**
- Question: Do the synthesized counterfactual strategies correspond to realistic recourse behaviors, and how can this realism be quantitatively measured against historical data?
- Basis: The Conclusion suggests it would be interesting to "investigate whether our counterfactual strategies result in realistic recourse behaviors, e.g. by measuring similarity to those witnessed in our dataset."
- Why unresolved: The paper validates mathematical properties (validity, sparsity) but does not evaluate whether the suggested strategy changes (e.g., "increase probability of action X") are plausible or executable in the real world.
- What evidence would resolve it: A quantitative metric comparing the distribution of counterfactual action paths to the distributions found in the ground-truth event logs used to generate the models.

## Limitations

- Scalability limitations: The method times out for MSSD models with 500+ states (251-300 timeouts out of 700 instances), limiting applicability to very large sequential decision-making systems.
- Underspecified random strategy generation: The random strategy generation procedure is not fully specified, which could affect reproducibility.
- Limited quantitative diversity validation: The effectiveness of diverse counterfactual strategies is evaluated primarily through manual inspection rather than quantitative metrics.

## Confidence

- **High confidence**: The theoretical foundation connecting MDP strategies to MIQCQP optimization is sound, as it builds on established methods for probabilistic reachability. The empirical results demonstrating efficient computation for MDPs up to 15,000 states are reproducible given the code repository.
- **Medium confidence**: The diversity objective for generating multiple counterfactual strategies shows promise but lacks rigorous validation. The claim that diverse strategies provide "novel recourse possibilities" is supported by qualitative examples but not systematically quantified.
- **Low confidence**: The method's performance on extremely large MDPs (e.g., real-world systems with 100K+ states) remains untested, and the scalability limitations suggest fundamental constraints on practical deployment.

## Next Checks

1. **Scalability Benchmark**: Implement the method on synthetic MDPs scaling from 1,000 to 100,000 states with varying action space sizes. Measure solve times and success rates to establish practical limits and identify which components (e.g., binary variables for d₀) cause bottlenecks.

2. **Diversity Metric Validation**: Develop quantitative metrics for counterfactual strategy diversity (e.g., action disagreement distributions, reachability set differences) and apply them to the diverse counterfactuals generated in experiments. Compare these metrics against baseline methods like random strategy sampling.

3. **Real-world Impact Assessment**: Apply the method to a high-stakes decision-making system (e.g., medical treatment planning or autonomous driving) where counterfactual explanations have clear practical value. Measure whether the generated strategies are interpretable by domain experts and whether they lead to actionable changes in decision-making processes.