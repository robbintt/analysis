---
ver: rpa2
title: Improving Fairness in LLMs Through Testing-Time Adversaries
arxiv_id: '2505.12100'
source_url: https://arxiv.org/abs/2505.12100
tags:
- fairness
- bias
- prompt
- llms
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to improve fairness in large language
  models (LLMs) by detecting and correcting biased predictions at inference time.
  The core idea is to create multiple perturbed versions of an input sentence by modifying
  sensitive attributes (e.g., race, gender), then comparing the model's predictions
  on these variants to the original.
---

# Improving Fairness in LLMs Through Testing-Time Adversaries
## Quick Facts
- **arXiv ID**: 2505.12100
- **Source URL**: https://arxiv.org/abs/2505.12100
- **Reference count**: 28
- **Primary result**: Testing-time adversarial method improves fairness metrics by up to 27 percentage points without retraining

## Executive Summary
This paper introduces a testing-time adversarial approach to detect and correct biased predictions in large language models. The method generates perturbed versions of input sentences by modifying sensitive attributes like race and gender, then compares model predictions across these variants to identify potential bias. When consistency falls below a threshold, predictions are flipped to promote fairness. Experiments with Llama3 demonstrate significant improvements in fairness metrics while maintaining predictive performance, with larger models showing more pronounced bias that the method effectively addresses.

## Method Summary
The core methodology involves creating multiple perturbed versions of input sentences by systematically replacing sensitive attributes (e.g., names indicating race or gender) with alternatives. The model's predictions on these perturbed inputs are compared to the original prediction, and a consistency rate is calculated. If this rate falls below a predefined threshold, the prediction is flagged as potentially biased and is flipped to the alternative class. This approach requires no model retraining or fine-tuning, making it computationally efficient and easily deployable. The method leverages the observation that biased models often produce inconsistent outputs when sensitive attributes are modified, while fair models maintain prediction consistency across such perturbations.

## Key Results
- Testing-time adversarial approach improves fairness metrics by up to 27 percentage points on Llama3
- Method effectively reduces racial disparities in model predictions while maintaining predictive accuracy
- Larger models exhibit more bias but also show greater improvement when the method is applied

## Why This Works (Mechanism)
The method works by exploiting inconsistencies in biased model behavior. When a model harbors implicit biases, it tends to produce different predictions for semantically equivalent inputs that differ only in sensitive attributes (e.g., predicting different outcomes for "John applied for the job" vs. "Jamal applied for the job"). By generating multiple perturbed versions of each input and checking for prediction consistency, the method can detect when these attribute changes significantly affect the model's output. The flipping mechanism then corrects for these inconsistencies, effectively neutralizing the model's tendency to discriminate based on sensitive attributes. This works because the perturbations maintain semantic equivalence while isolating the effect of protected characteristics.

## Foundational Learning
**Perturbation-based fairness testing**: Why needed - to systematically identify bias patterns; Quick check - verify perturbations preserve semantic meaning
**Consistency rate calculation**: Why needed - quantifies prediction stability across attribute modifications; Quick check - ensure threshold selection balances false positives/negatives
**Testing-time adaptation**: Why needed - enables fairness improvements without costly retraining; Quick check - measure inference latency overhead
**Adversarial generation of inputs**: Why needed - creates challenging test cases for bias detection; Quick check - validate perturbation diversity and coverage
**Fairness metric selection**: Why needed - determines what aspects of bias are measured; Quick check - confirm metrics align with fairness objectives
**Threshold-based decision making**: Why needed - provides clear criteria for intervention; Quick check - analyze sensitivity to threshold variations

## Architecture Onboarding
**Component map**: Input sentence -> Perturbation generator -> LLM predictions -> Consistency calculator -> Threshold comparator -> (Optional) Prediction flipper -> Final output
**Critical path**: Perturbation generation → Multiple LLM inference calls → Consistency calculation → Decision threshold check → (Conditional) Prediction flip
**Design tradeoffs**: The method trades computational overhead (multiple inferences per input) for fairness improvements without model modification. It assumes perturbations maintain semantic equivalence, which may not always hold. The threshold parameter requires careful tuning to balance fairness gains against prediction accuracy.
**Failure signatures**: High false positive rates (flipping fair predictions), excessive computational overhead, failure to detect subtle bias patterns, incorrect perturbation generation that changes semantic meaning
**First experiments**: 1) Test consistency rates on known fair vs. biased models with synthetic data, 2) Evaluate perturbation generation quality and semantic preservation, 3) Measure computational overhead and latency impact across different model sizes

## Open Questions the Paper Calls Out
The paper acknowledges several open questions regarding the generalizability of the approach to other protected attributes beyond race, the relationship between model size and bias susceptibility, and the method's effectiveness in real-world applications where bias manifests in more complex, context-dependent ways. The authors also note uncertainty about optimal threshold selection and the potential for the method to introduce new forms of bias through its intervention mechanism.

## Limitations
- Evaluation primarily focuses on racial disparities with limited analysis of gender or intersectional attributes
- Computational overhead during inference due to multiple perturbed input evaluations
- Method's effectiveness depends heavily on the quality and coverage of perturbation generation

## Confidence
**High confidence**: The core mechanism of detecting bias through consistency checking across perturbed inputs is technically sound and well-validated through experimental results
**Medium confidence**: The generalizability of the method across different model architectures and tasks beyond the tested scenarios
**Medium confidence**: The practical utility in real-world applications where bias manifests in more complex, context-dependent ways

## Next Checks
1. Test the method's effectiveness across a broader range of protected attributes (gender, age, disability status) and intersectional combinations to assess generalizability
2. Evaluate computational overhead and latency impact in production environments, particularly for high-throughput applications
3. Conduct user studies to determine whether fairness improvements translate to perceived fairness in practical applications and whether any performance degradation affects user experience