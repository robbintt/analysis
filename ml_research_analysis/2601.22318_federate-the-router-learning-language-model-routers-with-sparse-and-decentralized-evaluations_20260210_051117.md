---
ver: rpa2
title: 'Federate the Router: Learning Language Model Routers with Sparse and Decentralized
  Evaluations'
arxiv_id: '2601.22318'
source_url: https://arxiv.org/abs/2601.22318
tags:
- federated
- client
- client-local
- router
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning routing policies for
  language model (LM) selection in a federated setting where clients possess private,
  fragmented evaluation data across heterogeneous query distributions and sparse model
  coverage. The authors propose federated training frameworks for both parametric
  MLP-Router and nonparametric K-Means-Router, enabling collaborative learning of
  routing policies from decentralized data without centralizing sensitive query-model
  evaluations.
---

# Federate the Router: Learning Language Model Routers with Sparse and Decentralized Evaluations

## Quick Facts
- arXiv ID: 2601.22318
- Source URL: https://arxiv.org/abs/2601.22318
- Reference count: 40
- Primary result: Federated routers outperform client-local baselines on LM selection tasks

## Executive Summary
This paper addresses the challenge of learning routing policies for language model selection in federated settings where clients possess private, fragmented evaluation data across heterogeneous query distributions and sparse model coverage. The authors propose federated training frameworks for both parametric MLP-Router and nonparametric K-Means-Router architectures, enabling collaborative learning without centralizing sensitive query-model evaluations. The approach demonstrates that federated learning can effectively leverage cross-client collaboration to learn better routing policies while preserving data privacy, with adaptive personalization further improving robustness under extreme heterogeneity.

## Method Summary
The authors propose federated training frameworks for learning routing policies in language model selection tasks. They introduce both parametric MLP-Router and nonparametric K-Means-Router architectures that can be trained collaboratively across clients without centralizing sensitive evaluation data. The framework enables clients to share routing policy parameters while keeping their private query-model evaluation data local. The training process incorporates federated averaging with optional personalization mechanisms to handle client heterogeneity. The approach theoretically establishes convergence guarantees for federated optimization and empirically demonstrates performance improvements over client-local baselines across two benchmarks.

## Key Results
- Federated routers consistently outperform client-local baselines on both global and client-specific test distributions
- Adaptive personalization improves robustness under extreme heterogeneity conditions
- Federated training reduces routing suboptimality by improving effective query and model coverage
- The framework preserves data privacy while enabling collaborative learning of routing policies

## Why This Works (Mechanism)
The federated routing approach works by enabling clients to collaboratively learn routing policies without sharing their sensitive evaluation data. By training routing models across multiple clients with diverse query distributions and model portfolios, the system can learn more robust policies that generalize better to unseen scenarios. The federated averaging process allows clients to benefit from each other's data while maintaining privacy, and the adaptive personalization component helps handle heterogeneity across clients.

## Foundational Learning
- Federated Learning: Distributed optimization across multiple clients without centralizing data
  - Why needed: Enables collaborative learning while preserving privacy
  - Quick check: Verify convergence guarantees hold under routing-specific conditions

- Language Model Routing: Selecting optimal models for specific queries
  - Why needed: Improves efficiency and quality of language model responses
  - Quick check: Confirm routing decisions improve over random selection

- Sparse Evaluation Coverage: Limited availability of query-model performance data
  - Why needed: Real-world constraint that necessitates collaborative learning
  - Quick check: Measure coverage improvement through federated training

## Architecture Onboarding

**Component Map**: Clients -> Federated Server -> Routing Policy -> Model Selection

**Critical Path**: Client data collection → Local routing policy training → Federated averaging → Model selection deployment

**Design Tradeoffs**: 
- Privacy vs. performance: More collaboration improves performance but may increase privacy risks
- Communication cost vs. accuracy: More frequent updates improve accuracy but increase bandwidth usage
- Personalization vs. generalization: More personalization improves local performance but may reduce cross-client benefits

**Failure Signatures**:
- Poor convergence: Indicates insufficient collaboration or client heterogeneity
- Privacy leakage: Suggests routing decisions reveal sensitive evaluation patterns
- Suboptimal routing: Points to inadequate coverage or model diversity

**3 First Experiments**:
1. Compare federated vs. client-local routing performance on synthetic benchmark
2. Evaluate privacy-utility tradeoff with varying levels of collaboration
3. Test routing policy robustness under extreme client heterogeneity

## Open Questions the Paper Calls Out
None

## Limitations
- Privacy leakage through routing decisions not thoroughly addressed
- Convergence analysis relies on standard federated learning assumptions that may not capture routing-specific challenges
- Evaluation lacks detailed ablation studies of different framework components
- Assumes homogeneous model architectures across clients
- Does not address computational overhead and communication costs

## Confidence
**High Confidence**: Federated routers outperform client-local baselines on global and client-specific test distributions
**Medium Confidence**: Theoretical convergence guarantees may not fully translate to practical routing scenarios
**Medium Confidence**: Federated training reduces suboptimality through improved coverage, but relies on idealized assumptions
**Low Confidence**: Adaptive personalization significantly improves robustness under extreme heterogeneity lacks sufficient validation

## Next Checks
1. Conduct comprehensive privacy analysis to quantify potential information leakage through routing decisions
2. Perform extensive ablation studies to isolate contributions of different framework components
3. Evaluate proposed methods across heterogeneous model architectures and families