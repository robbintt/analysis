---
ver: rpa2
title: End-to-end workflow for machine learning-based qubit readout with QICK and
  hls4ml
arxiv_id: '2501.14663'
source_url: https://arxiv.org/abs/2501.14663
tags:
- readout
- qubit
- quantum
- qick
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents an end-to-end workflow for machine learning-based
  superconducting qubit readout by integrating neural networks into the Quantum Instrumentation
  Control Kit (QICK) platform. The approach leverages hls4ml to convert trained neural
  network models into hardware-efficient FPGA implementations using quantization-aware
  training and high-level synthesis.
---

# End-to-end workflow for machine learning-based qubit readout with QICK and hls4ml

## Quick Facts
- arXiv ID: 2501.14663
- Source URL: https://arxiv.org/abs/2501.14663
- Authors: Giuseppe Di Guglielmo; Botao Du; Javier Campos; Alexandra Boltasseva; Akash V. Dixit; Farah Fahim; Zhaxylyk Kudyshev; Santiago Lopez; Ruichao Ma; Gabriel N. Perdue; Nhan Tran; Omer Yesilyurt; Daniel Bowring
- Reference count: 0
- Primary result: 96% single-shot fidelity with 32ns latency using ML readout on FPGA

## Executive Summary
This work presents an end-to-end workflow for machine learning-based superconducting qubit readout by integrating neural networks into the Quantum Instrumentation Control Kit (QICK) platform. The approach leverages hls4ml to convert trained neural network models into hardware-efficient FPGA implementations using quantization-aware training and high-level synthesis. For single transmon qubit readout, the method achieves 96% single-shot fidelity with a latency of 32ns while utilizing less than 16% of FPGA look-up table resources.

## Method Summary
The workflow converts trained neural networks into FPGA IP using hls4ml with quantization-aware training. A 2-layer MLP (800×4×1) processes 400-clock-cycle I/Q time series from ADC, starting at cycle 100, to classify qubit states. The network uses ternary (2-bit) weights and is fully unrolled for minimum latency. The IP integrates into QICK firmware via AXI-Stream, with predictions buffered in BRAM and accessed through memory-mapped registers.

## Key Results
- 96% single-shot readout fidelity achieved with 32ns latency
- <16% FPGA look-up table resource utilization for ternary network
- Optimal readout window: 400 clock cycles starting at cycle 100

## Why This Works (Mechanism)

### Mechanism 1
Quantization-aware training (QAT) enables neural network deployment on FPGAs with minimal accuracy loss. QAT simulates fixed-point quantization during training, allowing the network to learn weights and activations that remain performant when constrained to low bit-widths (ternary/2-bit in this work). This reduces model size and computational complexity while preserving classification fidelity.

### Mechanism 2
Optimizing the readout window (start time and duration) is critical for NN fidelity and resource efficiency. The resonator bandwidth limits how quickly qubit states become distinguishable. Starting readout too early captures transient noise; starting too late incurs T1 relaxation errors. A 400-clock-cycle window starting at cycle 100 maximizes signal-to-noise while minimizing NN input dimensionality.

### Mechanism 3
Loosely coupled IP integration enables modular NN deployment without disrupting QICK's timing-critical signal chain. The NN classifier IP operates as a standalone accelerator with AXI-Stream input and BRAM output buffers. Data streams continuously; the IP cannot assert backpressure. Results are stored locally and accessed via memory-mapped registers after inference completes.

## Foundational Learning

- Concept: Dispersive qubit readout (transmon-resonator coupling, qubit-state-dependent frequency shift)
  - Why needed here: The entire workflow assumes the reader understands how microwave probing distinguishes |0⟩ and |1⟩ states.
  - Quick check question: Can you explain why the I/Q trajectories separate as the readout pulse interacts with the qubit-resonator system?

- Concept: Quantization-aware training (simulating fixed-point during backprop)
  - Why needed here: Core technique enabling the 2-bit ternary network to achieve ~96% fidelity.
  - Quick check question: What happens to gradient computation when weights are constrained to {-1, 0, 1} during training?

- Concept: AXI-Stream protocol (backpressure, TVALID/TREADY handshaking)
  - Why needed here: The NN IP explicitly disables backpressure; understanding this constraint is essential for system integration.
  - Quick check question: Why does removing TREADY create a hard requirement for the NN IP to always be ready to consume data?

## Architecture Onboarding

- Component map: ADC → Demod & Bin → NN Classifier IP (AXI-Stream) → BRAM buffer → AXI4-Lite → ARM PS (Python API)
- Critical path: tProc triggers readout pulse (2.5 µs square pulse) → ADC digitizes returned signal at 6.88 GS/s → demodulated to I/Q time series → 400 CLK window (starting at cycle 100) streamed to NN IP → 8-cycle NN inference + 2-cycle BRAM write = 32 ns total latency → Python API retrieves logits via memory-mapped read
- Design tradeoffs: Window size vs. fidelity (larger up to ~400 CLK); Quantization vs. resource usage (ternary minimizes LUTs but risks accuracy loss); Parallelization vs. latency (fully unrolled minimizes latency but maximizes LUT usage)
- Failure signatures: Fidelity drops unexpectedly (check for readout signal drift); BRAM overflow (predictions overwritten, reduce shot count); in_TVALID asserted but no predictions (verify trigger timing)
- First 3 experiments: Baseline validation (run standard thresholding alongside NN readout); Quantization sweep (6-bit, 3-bit, ternary); Window sensitivity (vary start time 50-200 CLK, window size 300-500 CLK)

## Open Questions the Paper Calls Out

### Open Question 1
Does normalizing ADC input data improve ML-based readout robustness against signal drifts over time? Current implementation feeds raw 14-bit unsigned integers directly to the network without preprocessing.

### Open Question 2
How does NN-based readout fidelity and resource consumption scale with the number of multiplexed qubits compared to single-qubit performance? Current demonstration uses only a single transmon qubit with all others far-detuned.

### Open Question 3
Can integrating NN classifier outputs into QICK's conditional logic enable effective real-time adaptive quantum feedback control? Current implementation stores predictions in BRAM but does not feed them back to tProc for conditional operations.

## Limitations
- Quantization constraints: Ternary (2-bit) quantization achieves high fidelity for single transmon qubits but may not generalize to more complex readout scenarios with crosstalk
- Hardware integration assumptions: Loosely coupled IP model assumes fixed latency requirements and sufficient prediction buffer size
- Generalization: Performance depends on specific readout pulse duration, ADC sampling rate, and qubit T1 time

## Confidence
- **High Confidence**: Quantization-aware training mechanism and AXI-Stream integration pattern are well-established techniques with directly measurable metrics
- **Medium Confidence**: Readout window optimization results depend on specific device characteristics and may not transfer perfectly to other systems
- **Low Confidence**: Long-term stability of ML-based readout under device drift and environmental changes is not addressed

## Next Checks
1. Deploy the workflow on a different RFSoC platform (e.g., UltraScale+) and verify that the 96% fidelity threshold is maintained with comparable resource usage
2. Extend the workflow to readout two coupled transmon qubits and measure fidelity degradation as crosstalk increases
3. Modify the QICK firmware to enable NN-based decision-making for active qubit reset or gate adaptation, measuring impact on overall quantum algorithm fidelity and execution time