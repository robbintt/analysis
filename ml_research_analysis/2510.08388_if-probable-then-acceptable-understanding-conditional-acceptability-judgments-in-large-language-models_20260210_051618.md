---
ver: rpa2
title: If Probable, Then Acceptable? Understanding Conditional Acceptability Judgments
  in Large Language Models
arxiv_id: '2510.08388'
source_url: https://arxiv.org/abs/2510.08388
tags:
- conditional
- probability
- relation
- vanilla
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how large language models (LLMs) judge\
  \ the acceptability of conditional statements like \u201CIf A, then B,\u201D a fundamental\
  \ aspect of human reasoning. Prior research has focused on LLMs\u2019 ability to\
  \ draw inferences from conditionals, but little is known about how they assess their\
  \ plausibility."
---

# If Probable, Then Acceptable? Understanding Conditional Acceptability Judgments in Large Language Models

## Quick Facts
- **arXiv ID:** 2510.08388
- **Source URL:** https://arxiv.org/abs/2510.08388
- **Reference count:** 37
- **Primary result:** LLMs show sensitivity to both conditional probability and semantic relevance when judging conditional acceptability, though effects vary by model family, size, and prompting style.

## Executive Summary
This study investigates how large language models (LLMs) judge the acceptability of conditional statements like "If A, then B," examining four models (Llama 3.1 and Qwen 2.5) across different sizes, prompting strategies, and judgment types. We find that LLMs are sensitive to both conditional probability and semantic relevance (whether the antecedent supports or undermines the consequent), though the strength and consistency of these effects vary by model family, size, and prompting technique. Larger models do not necessarily align more closely with human trends, and few-shot prompting can introduce biases toward specific semantic relations. Comparisons with human data reveal that while LLMs broadly mimic human patterns, they do so less systematically and with greater variability.

## Method Summary
The study evaluates four LLMs (Llama-3.1-8B/70B-Instruct, Qwen2.5-7B/72B-Instruct) using a dataset of 144 conditionals from Skovgaard-Olsen et al. (2016), varying by relation type (positive/negative/irrelevant) and prior probability. Models are prompted using three strategies: vanilla (zero-shot), few-shot (3 examples), and chain-of-thought (70B/72B only). Each sample is prompted 5 times with temperature=1.0, top_p=1.0. Ratings are extracted via regex matching after "answer" tokens and analyzed using linear mixed-effects models with random intercepts for scenario and instance ID, followed by Type III ANOVA to assess fixed effects.

## Key Results
- LLMs show strong sensitivity to conditional probability P(B|A) across all models (p < 0.0001), with slopes ranging from 0.21 to 0.65 per unit increase
- Semantic relevance effects are observed, with Qwen models showing 0.20-0.30 point differences between relation types, while Llama 70B shows minimal differentiation
- Few-shot and CoT prompting introduce systematic biases, producing steeper interaction slopes for negatively related conditionals than positively related ones
- Model size does not consistently correlate with human alignment, and larger models show reduced sensitivity to semantic relevance in some cases

## Why This Works (Mechanism)

### Mechanism 1: Conditional Probability Sensitivity
LLMs track conditional probability P(B|A) when judging conditionals, with higher P(B|A) producing higher acceptability ratings. Models appear to infer the probability of the consequent given the antecedent through latent knowledge encoded during pretraining, then map this probabilistic inference onto a numerical rating scale via direct elicitation.

### Mechanism 2: Semantic Relevance Modulation
LLMs adjust acceptability judgments based on whether the antecedent supports (POS), contradicts (NEG), or is irrelevant (IRR) to the consequent. Models encode latent representations of causal and evidential relations from pretraining, then apply these relations to weight probabilistic judgments—positive relations boost ratings, negative/irrelevant relations suppress them.

### Mechanism 3: Prompting-Induced Bias
Few-shot and chain-of-thought prompting shift model sensitivity toward specific relation types, introducing systematic biases not present in zero-shot conditions. Example demonstrations in few-shot prompts prime models toward particular reasoning patterns, with few-shot variants producing steeper slopes for negatively related conditionals than positive ones.

## Foundational Learning

- **Conditional Probability vs. Joint Probability**: Why needed here: The paper distinguishes P(B|A) from P(A∧B); conflating these would misinterpret the conditional probability hypothesis. Quick check question: Given P(A)=0.6 and P(B)=0.8 with independence, what is P(B|A)?

- **Linear Mixed-Effects Models**: Why needed here: The paper uses LMEMs to separate fixed effects (probability, relation) from random effects (scenario, sampling iteration). Quick check question: Why would a simple ANOVA be insufficient for this nested data structure?

- **Semantic Relation Taxonomy (POS/NEG/IRR)**: Why needed here: The evidentiality hypothesis hinges on whether A supports, contradicts, or is irrelevant to B. Quick check question: In "If it rains, the ground stays dry," what is the semantic relation?

## Architecture Onboarding

- **Component map**: Input layer (context + conditional statement) -> Prompting module (zero-shot/few-shot/CoT templates) -> Rating extraction (regex for 0-100 numeric output) -> Statistical analysis (LMEM with fixed effects and random effects)

- **Critical path**: 1) Load Skovgaard-Olsen et al. (2016) dataset (144 conditionals across 12 contexts) 2) Format prompts per judgment type (conditional probability, if-probability, acceptability) 3) Run 5 inference trials per sample with nucleus sampling (top_p=1.0, T=1.0) 4) Extract ratings via regex; mean-center to [-0.5, 0.5] 5) Fit LMEMs; run Type III ANOVA for fixed effect significance

- **Design tradeoffs**: Direct elicitation vs. sentence probability (direct leverages reasoning but introduces sampling variance; sentence probability is deterministic but captures surface form not semantics); Vanilla vs. few-shot (few-shot can amplify sensitivity but introduces example bias); Model size (larger models show more consistent judgment-type handling but not better human alignment on positive relations)

- **Failure signatures**: Rating clustering at extreme values (indicates token preference bias rather than graded reasoning); Non-significant relation type effect (suggests relevance insensitivity in certain size/architecture combinations); Collapsed distinctions at low probability (models fail to differentiate relation types when base plausibility is low)

- **First 3 experiments**: 1) Balanced few-shot ablation (re-run with 9 examples: 3 relations × 3 probability levels to test if negativity bias persists) 2) Paraphrase robustness test (systematically rephrase conditionals while preserving semantic relations to distinguish surface vs. semantic mechanisms) 3) Sentence probability correlation (compute P(B|A) via token likelihoods and correlate with verbalized ratings to validate direct elicitation reliability)

## Open Questions the Paper Calls Out

### Open Question 1
Would qualitative analysis of LLM reasoning reveal whether numerical ratings similar to humans reflect genuinely similar reasoning strategies or merely coincidental alignment? The study relied solely on numerical ratings (0-100), which cannot distinguish between convergent outputs from divergent cognitive processes. Collect verbal explanations from both humans and LLMs alongside ratings to analyze whether similar ratings are justified by similar reasoning chains.

### Open Question 2
Would balanced few-shot examples across relation types (e.g., one example per relation-probability combination) prevent the negativity bias observed in prompted models? All few-shot and CoT variants showed steeper slopes for negatively related conditionals, suggesting systematic bias from example selection. Compare model judgments using systematically balanced prompt sets against the current random selection approach.

### Open Question 3
Why do larger models (e.g., Llama 70B vanilla) show reduced sensitivity to semantic relevance compared to smaller models, and is this robust across architectures? The authors observe but do not investigate the mechanisms behind this inverse relationship between model size and relevance sensitivity. Probe internal representations across model layers; test additional model families and sizes; analyze attention patterns for relevance cues.

## Limitations
- The study uses a relatively small, context-constrained dataset (144 conditionals across 12 scenarios) that may not generalize to broader conditional reasoning tasks
- Random selection of few-shot examples introduces uncontrolled variability that may have inadvertently biased model behavior toward negative relations
- Reliance on direct verbalized ratings rather than deterministic probability measures introduces sampling variance that could obscure subtle effects

## Confidence

- **High confidence**: Detection of conditional probability sensitivity across all models (p < 0.0001) and consistent main effect of semantic relevance in Qwen models
- **Medium confidence**: Finding that few-shot prompting introduces systematic bias toward negative relations, given acknowledged randomness in example selection
- **Low confidence**: Conclusion that model size does not correlate with human alignment, particularly for positive relations, due to limited sample size and potential context effects

## Next Checks

1. **Balanced few-shot ablation**: Re-run experiments with carefully balanced examples (one per relation-probability combination) to determine if the negativity bias persists or is artifactual

2. **Cross-linguistic generalization**: Test the same conditional reasoning tasks with multilingual LLMs to assess whether observed patterns hold across linguistic and cultural contexts

3. **Sentence probability validation**: Compute P(B|A) using deterministic sentence probabilities and correlate with verbalized ratings to validate whether direct elicitation captures genuine probabilistic reasoning or surface-level token correlations