---
ver: rpa2
title: 'QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory
  Management'
arxiv_id: '2512.12967'
source_url: https://arxiv.org/abs/2512.12967
tags:
- reasoning
- memory
- long-context
- training
- qwenlong-l1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QwenLong-L1.5 introduces a post-training recipe for long-context
  reasoning and memory management, addressing the gap in mature post-training systems
  for long-context reasoning. The key innovations include a long-context data synthesis
  pipeline that generates multi-hop reasoning tasks requiring grounding over globally
  distributed evidence, stabilized reinforcement learning with task-balanced sampling
  and adaptive entropy-controlled policy optimization to mitigate training instability,
  and a memory-augmented architecture for ultra-long contexts using iterative memory-based
  processing.
---

# QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management

## Quick Facts
- arXiv ID: 2512.12967
- Source URL: https://arxiv.org/abs/2512.12967
- Authors: Weizhou Shen; Ziyi Yang; Chenliang Li; Zhiyuan Lu; Miao Peng; Huashan Sun; Yingcheng Shi; Shengyi Liao; Shaopeng Lai; Bo Zhang; Dayiheng Liu; Fei Huang; Jingren Zhou; Ming Yan
- Reference count: 40
- Primary result: Achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing baseline by 9.90 points on average

## Executive Summary
QwenLong-L1.5 introduces a comprehensive post-training recipe addressing the gap in mature post-training systems for long-context reasoning. The approach combines a novel long-context data synthesis pipeline that generates multi-hop reasoning tasks requiring global information integration, stabilized reinforcement learning with task-balanced sampling and adaptive entropy-controlled policy optimization, and a memory-augmented architecture for ultra-long contexts using iterative memory-based processing. Based on Qwen3-30B-A3B-Thinking, the system achieves state-of-the-art performance on long-context reasoning benchmarks and demonstrates effective memory management for contexts exceeding 1M tokens.

## Method Summary
The methodology employs a four-stage progressive reinforcement learning pipeline with increasing context lengths (32K→60K→120K). The data synthesis pipeline deconstructs documents into atomic facts via knowledge graphs, samples multi-hop reasoning paths with sparse cross-document distribution, and generates verifiable questions requiring global information integration. Training uses task-balanced sampling with task-specific advantage estimation to prevent distributional imbalances, and Adaptive Entropy-Controlled Policy Optimization (AEPO) to stabilize training by masking negative gradients when entropy exceeds thresholds. After stage 3, a separate memory-RL expert is trained and merged with the base model using the SCE algorithm, followed by stage 4 full-context RL that maintains both single-pass reasoning and iterative memory processing capabilities.

## Key Results
- Matches GPT-5 and Gemini-2.5-Pro performance on long-context reasoning benchmarks
- Surpasses baseline by 9.90 points on average across long-context tasks
- Memory-agent framework yields 9.48-point gain over agent baseline on ultra-long tasks (1M–4M tokens)
- Acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, tool-use agents, and extended dialogue

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Synthesizing multi-hop reasoning tasks over globally distributed evidence improves long-context reasoning more than simple retrieval tasks.
- **Mechanism:** The pipeline deconstructs documents into atomic facts and relationships via knowledge graphs, samples multi-hop reasoning paths with sparse cross-document distribution, and generates verifiable questions. This forces models to integrate information across the full context rather than relying on local retrieval.
- **Core assumption:** Models trained on tasks requiring global information integration will develop generalizable long-context reasoning skills (not just memorize specific patterns).
- **Evidence anchors:**
  - [abstract] "moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities"
  - [section 3.2] "path nodes are deliberately distributed sparsely across multiple documents"
  - [corpus] Weak direct evidence; neighbor papers focus on memory systems rather than data synthesis quality.

### Mechanism 2
- **Claim:** Task-balanced sampling with task-specific advantage estimation stabilizes multi-task long-context RL training.
- **Mechanism:** Long-context data exhibits multi-clustered distribution. Random sampling causes batch-level distribution imbalance, leading to unstable entropy spikes. Task-balanced sampling ensures equal representation per task type, while task-specific advantage normalization prevents reward scale differences between tasks from causing biased gradient updates.
- **Core assumption:** The task categories are well-defined and reward distributions within each task are meaningful (not arbitrary).
- **Evidence anchors:**
  - [section 4.2] "a natural implication for RL training is that a traditional random sampler can lead to distributional imbalances"
  - [section 4.2, Table 2] Task-balanced + task-batch-std achieves 58.62 avg vs. 56.07 for GRPO baseline
  - [corpus] No direct corpus evidence for task-balanced sampling in long-context RL.

### Mechanism 3
- **Claim:** Adaptive Entropy-Controlled Policy Optimization (AEPO) prevents training collapse by dynamically masking negative gradients when entropy exceeds thresholds.
- **Mechanism:** High-entropy tokens correlate strongly with large gradient norms (ρ=0.96). In long-context tasks, incorrect responses contain many correct reasoning steps, causing credit assignment problems. AEPO masks all samples with negative advantages when batch entropy exceeds H_high, functioning as advantage-weighted rejection sampling.
- **Core assumption:** High entropy indicates exploration behavior that should not be excessively penalized; the entropy thresholds are appropriate for the model scale.
- **Evidence anchors:**
  - [section 4.3, Figure 9] "Spearman's ρ=0.96 (p<0.0001)" correlation between token entropy and gradient norm
  - [section 4.4, Table 5] AEPO achieves 59.36 avg vs. 56.07 for GRPO baseline on Qwen3-4B
  - [corpus] Neighbor papers (Motif-2) mention training instability in reasoning adaptation but don't propose entropy-based solutions.

### Mechanism 4
- **Claim:** Multi-stage progressive training with memory-agent fusion via model merging enables both single-pass reasoning (<256K) and iterative memory processing (>1M tokens).
- **Mechanism:** Training proceeds through 4 stages with increasing input/output lengths. After Stage 3, a separate memory-RL expert is trained, then merged with SCE algorithm. Stage 4 full-context RL maintains both capabilities. The memory agent decomposes queries into q_core + q_inst, processes chunks sequentially with memory updates and planning, and generates final answers from accumulated memory.
- **Core assumption:** Memory management and single-pass reasoning are complementary skills that can be preserved through merging rather than interfering.
- **Evidence anchors:**
  - [section 4.1, Figure 6] Pipeline shows 4 stages with model merging step
  - [section 5.5, Table 10] Memory-RL expert scores 20.34 on MRCR-1M but drops to 68.53 avg; merged model achieves 71.18 avg with 21.68 on MRCR-1M
  - [corpus] MemAgent and Sculptor papers support memory-augmented approaches for long-horizon tasks.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** QwenLong-L1.5 uses GRPO instead of PPO because value networks are computationally prohibitive for long contexts. GRPO estimates advantage via group-level reward z-score normalization without a critic.
  - **Quick check question:** Given 8 candidate responses with rewards [0, 0, 1, 1, 1, 0, 1, 0], what is the advantage for the third response? (Answer: (1 - 0.5) / 0.5 = 1.0)

- **Concept: Entropy as exploration signal**
  - **Why needed here:** AEPO uses entropy to control when to mask negative gradients. High entropy indicates the model is uncertain/exploring; penalizing exploratory behavior too harshly prevents learning.
  - **Quick check question:** If a model's batch entropy is 0.25 and H_high=0.20, what does AEPO do? (Answer: Masks all samples with negative advantages, training only on positive samples)

- **Concept: Memory agent as sequential decision-making**
  - **Why needed here:** Ultra-long contexts (1M-4M tokens) cannot fit in context windows. The memory agent processes chunks sequentially, maintaining compressed memory and generating plans for subsequent chunks.
  - **Quick check question:** In the memory agent framework, what three components does the agent output at each step t? (Answer: memory update m_t, navigational plan p_t, and final answer y after last chunk)

## Architecture Onboarding

- **Component map:** Qwen3-30B-A3B-Thinking (base) → Stage 1-3: Full-context RL (progressive length extension) → Memory-RL Expert ← Stage 3 checkpoint → Model Merging (SCE algorithm) → Stage 4: Full-context RL → QwenLong-L1.5-30B-A3B-Thinking

- **Critical path:**
  1. Data synthesis quality determines ceiling (14.1K samples from 42.7K after filtering)
  2. Multi-stage RL prevents training collapse (naive GRPO: 67.24 avg → final: 71.82)
  3. Memory-RL + merging enables ultra-long context (9.48-point gain on 1M-4M tasks)

- **Design tradeoffs:**
  - Longer rollout → better reasoning but higher memory/compute (max 50K output tokens)
  - Larger group size (G=8) → better advantage estimation but slower sampling
  - Aggressive entropy masking → stable training but reduced exploration
  - Memory-RL specialization → better long-context but risks single-pass degradation

- **Failure signatures:**
  - Entropy spike followed by collapse → check task-balanced sampling, reduce learning rate
  - Response length explosion → increase AEPO's H_low threshold
  - Memory agent fails to retrieve information → check chunk size (32K) and memory size (15K) configuration
  - Catastrophic forgetting after merging → adjust SCE merging weights

- **First 3 experiments:**
  1. Reproduce the 4B ablation (Table 2, 4, 5) to validate task-balanced sampling and AEPO on a smaller model before scaling to 30B
  2. Analyze data synthesis quality: sample 100 synthesized QA pairs, verify multi-hop reasoning requires information from ≥3 documents
  3. Test memory agent on controlled ultra-long tasks: create synthetic 500K-1M token documents with known ground-truth retrieval targets to isolate memory compression quality

## Open Questions the Paper Calls Out

- **Open Question 1:** Can a granular, token-level credit assignment mechanism within thinking trajectories outperform the current sequence-level masking strategy (AEPO) in stabilizing long-context RL? The paper identifies AEPO as a stabilization mechanism rather than a "fundamental solution" because it assigns uniform advantage to entire reasoning steps, and proposes differentiating individual token contributions for more precise learning.

- **Open Question 2:** Can a "closed-loop data flywheel," where the model generates its own training data, successfully replace reliance on external proprietary models for data synthesis? The current synthesis is bottlenecked by "API quotas of proprietary models," and the paper proposes a self-improving loop to mitigate this, but it's unclear if a model can generate sufficiently high-quality "thinking trajectories" for its own further training without degradation.

- **Open Question 3:** Can the current post-training recipe be effectively adapted for tasks requiring both long-context inputs and long-form outputs (e.g., report generation), where it is currently suboptimal? The current pipeline is "not yet optimized" for "long-input, long-output problems" like chapter-level revision, as the current data synthesis focuses primarily on input processing rather than generating extensive, coherent output sequences.

## Limitations

- Several critical parameters remain unspecified, particularly the AEPO entropy thresholds (H_low, H_high) and the SCE merging hyperparameters, which are essential for faithful reproduction.
- The evaluation methodology lacks transparency in how performance claims against GPT-5 and Gemini-2.5-Pro were obtained, with specific evaluation protocols and prompt engineering not detailed.
- The memory-RL expert's standalone performance (20.34 on MRCR-1M) suggests significant capability gaps that are partially masked by the merging process rather than the memory agent providing independent value.

## Confidence

- **High Confidence:** The multi-stage progressive training approach and task-balanced sampling methodology are well-documented and show consistent improvements across ablation studies, with the core claim of synthesizing multi-hop reasoning tasks improving long-context reasoning strongly supported by data.
- **Medium Confidence:** The AEPO mechanism and memory-RL architecture are theoretically sound and show positive results in ablations, but exact hyperparameter sensitivity and failure modes are not fully characterized, and claims of matching GPT-5/Gemini-2.5-Pro performance lack evaluation transparency.
- **Low Confidence:** The memory-RL + SCE merging process shows the most significant uncertainty, with the memory agent's standalone performance substantially lower than the merged model, suggesting the merging process may be compensating for architectural weaknesses rather than the memory agent providing independent value.

## Next Checks

1. **Data Synthesis Quality Audit:** Sample 100 synthesized QA pairs from the pipeline and verify that multi-hop reasoning genuinely requires information from ≥3 distributed documents, testing whether the verification process effectively filters out tasks with shortcuts or superficial reasoning patterns.

2. **AEPO Parameter Sensitivity:** Systematically vary H_low (0.10→0.20) and H_high (0.15→0.25) on the 4B model to identify the stability-performance tradeoff curve, monitoring entropy trajectories during training to determine optimal threshold ranges for different model scales.

3. **Memory Agent Isolation Test:** Create synthetic 500K-1M token documents with known ground-truth retrieval targets and evaluate the memory agent's chunk-wise processing independently of the full-context model, measuring compression quality and planning accuracy to isolate whether performance gains come from the memory architecture or the overall training pipeline.