---
ver: rpa2
title: A Second-Order Perspective on Pruning at Initialization and Knowledge Transfer
arxiv_id: '2509.24066'
source_url: https://arxiv.org/abs/2509.24066
tags:
- pruning
- task
- transfer
- loss
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study explores pruning-at-initialization (PaI) on pre-trained
  vision models to compress models before fine-tuning, without requiring task-specific
  data. It investigates whether pruning on one task retains performance on unseen
  tasks and how data influences pruning.
---

# A Second-Order Perspective on Pruning at Initialization and Knowledge Transfer

## Quick Facts
- arXiv ID: 2509.24066
- Source URL: https://arxiv.org/abs/2509.24066
- Reference count: 36
- Pre-trained vision models can be effectively compressed via pruning-at-initialization while preserving cross-task transfer performance.

## Executive Summary
This study investigates pruning-at-initialization (PaI) on pre-trained vision models to compress models before fine-tuning, without requiring task-specific data. The research demonstrates that pruning on one task preserves zero-shot performance on unseen tasks, and fine-tuning further recovers performance. Hessian-based scores (especially block-diagonal for ResNet and diagonal for ViT) outperform other PaI methods, suggesting task-aligned loss landscapes induced by extensive pre-training. The results confirm that extensive pre-training enables effective compression with minimal data, supporting robust transfer learning.

## Method Summary
The study applies PaI techniques—SNIP, GraSP, PX, and Hessian-based approximations (isotropic, diagonal, block-diagonal)—on pre-trained models like ResNet-50 and ViT-B/16, then evaluates feature quality via linear classifiers. For each source task, pruning masks are computed at various sparsities (36-83%), the encoder is pruned, and zero-shot accuracy is measured on held-out tasks using pre-computed frozen classifiers. Post-fine-tuning on the source task is also evaluated. Hessian approximations are computed using Kronecker factorization for block-diagonal and per-parameter second-order information for diagonal, with the goal of approximating optimal importance scores without full Hessian inversion.

## Key Results
- Pruning on one task preserves zero-shot performance on unseen tasks
- Fine-tuning pruned models only on source data improves or maintains target task performance without accessing target data
- Hessian-based scores (block-diagonal for ResNet, diagonal for ViT) outperform gradient-based PaI methods
- Moderate sparsity (59-66%) preserves transfer while high sparsity (73-83%) degrades it
- Extensive pre-training enables effective compression with minimal data

## Why This Works (Mechanism)

### Mechanism 1: Task-Aligned Loss Landscapes from Extensive Pre-training
Extensive pre-training on diverse data creates a weight configuration θ₀ that serves as a shared local minimum for multiple downstream tasks. When pruning removes weights, the induced perturbation moves the model along directions that affect both source and target tasks similarly—provided their loss contours are aligned rather than orthogonal. Large-scale pre-training induces aligned loss landscapes across tasks, so pruning on source task data preserves zero-shot performance on unseen target tasks.

### Mechanism 2: Hessian-Based Importance Scores Capture Cross-Task Relevance
The optimal importance score ε[j] = θ²[j] / [H⁻¹][jj] measures loss increase from removing weight j. Block-diagonal preserves intra-layer correlations via Kronecker factorization; diagonal assumes parameter independence; isotropic (magnitude) assumes uniform curvature. The appropriate approximation depends on architecture-specific Hessian structure, with block-diagonal working best for ResNet and diagonal for ViT due to their different parameter coupling patterns.

### Mechanism 3: Source-Only Fine-tuning Recovers Transfer Performance
Pruning moves weights from θ₀ to θ̃₀, increasing loss. SGD on source task finds θ̃*_s that minimizes source loss. If source and target loss basins are aligned, then θ̃*_s is closer to θ₀ in the Riemannian manifold induced by H, thus also lower on target loss. The frozen linear classifiers isolate feature extractor changes from classifier adaptation, allowing measurement of feature quality changes.

## Foundational Learning

- **Hessian Matrix and Second-Order Optimization**
  - Why needed here: The entire theoretical framework relies on Taylor expansion around θ₀ and curvature-based importance scoring
  - Quick check question: Can you explain why the inverse Hessian appears in the optimal pruning score ε[j]?

- **Pruning-at-Initialization (PaI) Paradigm**
  - Why needed here: The paper positions itself within PaI literature, contrasting data-driven vs. Hessian-based approaches
  - Quick check question: What is the key difference between magnitude pruning and SNIP in terms of what they measure?

- **Transfer Learning and Feature Quality Evaluation**
  - Why needed here: The experimental protocol uses frozen linear classifiers to isolate feature degradation from classifier effects
  - Quick check question: Why fit separate classifiers ξ*_k per task rather than fine-tuning the original classifier?

## Architecture Onboarding

- **Component map**:
  Pre-trained encoder -> Pruning score computation -> Binary mask generation -> Frozen linear classifiers -> Fine-tuning loop

- **Critical path**:
  1. Load pre-trained weights θ₀
  2. Compute saliency scores using chosen method on source task data
  3. Generate mask by keeping top-(1-q)% weights
  4. (Optional) Fine-tune pruned encoder on source task only
  5. Evaluate on all tasks using pre-computed frozen classifiers

- **Design tradeoffs**:
  - Sparsity vs. transfer: Higher sparsity degrades zero-shot transfer; moderate sparsity preserves it
  - Score complexity vs. effectiveness: Isotropic is cheapest; block-diagonal requires Kronecker factorization but performs best for ResNets
  - Architecture-specific scores: ResNet benefits from block-diagonal; ViT benefits from diagonal

- **Failure signatures**:
  - Sharp accuracy drop at moderate sparsity: Indicates wrong pruning score for architecture
  - Source accuracy recovers but transfer doesn't: Suggests task misalignment beyond pre-training distribution
  - Zero-shot transfer close to source accuracy but fine-tuning hurts transfer: Potential overfitting to source during fine-tuning

- **First 3 experiments**:
  1. Implement magnitude pruning on ResNet-50 ImageNet-1k using ImageNet-10 task split; verify source/transfer accuracy trends
  2. Implement diagonal vs. block-diagonal Hessian scores on ViT-B/16 CLIP; confirm diagonal outperforms block-diagonal
  3. Design two tasks with orthogonal label semantics; prune on one and measure if transfer degrades faster than semantically related task pairs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can theoretical guarantees be derived for safe pruning thresholds that preserve transferability across tasks?
- Basis in paper: The conclusion states "theoretical guarantees for safe pruning thresholds remain unexplored"
- Why unresolved: The study empirically demonstrates robustness across sparsity levels but lacks formal bounds linking pruning-induced perturbations to transfer performance degradation
- What evidence would resolve it: Theoretical analysis connecting Hessian curvature, weight perturbation magnitude, and provable performance retention bounds

### Open Question 2
- Question: How does source-task pruning perform when target tasks are semantically distant from pre-training data?
- Basis in paper: Predictions assume "if it is true that D_s, D_t are similar to data seen during pre-training" but this condition is not systematically tested
- Why unresolved: All ImageNet-10 tasks are subsets of ImageNet-1k, potentially inflating transfer robustness by staying within the pre-training distribution
- What evidence would resolve it: Experiments with out-of-distribution transfer tasks (medical imaging, satellite imagery) that differ substantially from natural image pre-training data

### Open Question 3
- Question: Why do ViTs and ResNets require different Hessian approximations for optimal pruning?
- Basis in paper: Results show block-diagonal Hessian works best for ResNet while diagonal Hessian suits ViT, but the paper notes "distinct trends between architectures" without full explanation
- Why unresolved: The architectural properties causing this divergence remain uncharacterized
- What evidence would resolve it: Systematic analysis of Hessian eigenstructures across architectures, or ablation studies isolating specific architectural components

## Limitations

- The corpus provides weak direct support for cross-task transfer claims, with only indirect validation from neighboring pruning research
- The mechanism explanations rely on theoretical second-order approximations that aren't directly validated experimentally
- The study assumes pre-training induces aligned loss landscapes across tasks, but this is inferred rather than measured

## Confidence

- **High**: Empirical findings on Hessian-based scores outperforming gradient methods for specific architectures (ResNet vs ViT)
- **Medium**: Theoretical mechanism explaining why source-task pruning preserves transfer performance
- **Low**: Generalization claims about Hessian approximations across all pre-training paradigms and architectures

## Next Checks

1. Implement the block-diagonal Hessian Kronecker factorization for ResNet and verify it consistently outperforms diagonal approximation across all tested sparsities
2. Design synthetic task pairs with deliberately orthogonal label semantics to empirically test the break condition of cross-task alignment
3. Measure and compare the actual Hessian curvature correlation between source and target tasks to validate the theoretical alignment assumption