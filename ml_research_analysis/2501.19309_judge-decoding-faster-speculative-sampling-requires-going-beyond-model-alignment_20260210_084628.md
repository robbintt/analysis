---
ver: rpa2
title: 'Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment'
arxiv_id: '2501.19309'
source_url: https://arxiv.org/abs/2501.19309
tags:
- tokens
- decoding
- target
- draft
- judge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study identifies that standard speculative decoding frequently\
  \ rejects high-quality tokens that are contextually correct but misaligned with\
  \ the target model. To address this, the authors introduce \"judge decoding,\" a\
  \ method that trains a lightweight classifier on top of the target model\u2019s\
  \ embeddings to assess token correctness rather than strict alignment."
---

# Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment

## Quick Facts
- arXiv ID: 2501.19309
- Source URL: https://arxiv.org/abs/2501.19309
- Authors: Gregor Bachmann; Sotiris Anagnostidis; Albert Pumarola; Markos Georgopoulos; Artsiom Sanakoyeu; Yuming Du; Edgar Schönfeld; Ali Thabet; Jonas Kohler
- Reference count: 26
- One-line primary result: Judge decoding achieves up to 9× speedup and 129 tokens/second throughput by training a lightweight classifier to accept correct but non-aligned draft tokens.

## Executive Summary
Judge decoding introduces a novel approach to speculative decoding that addresses the fundamental limitation of standard methods: they reject many high-quality draft tokens that are semantically correct but distributionally misaligned with the target model. The method trains a compact linear classifier on the target model's embeddings to predict token correctness rather than alignment, achieving strong performance with only 16.4K parameters. Experiments demonstrate significant speed improvements (up to 9×) while maintaining output quality on benchmarks like GSM8K, HumanEval, and MT-Bench, with throughput reaching 129 tokens/second for Llama-405B.

## Method Summary
The method trains a linear judge head on the target model's last hidden layer embeddings to predict token correctness. Using a small dataset of 500 manually curated examples with correct and wrong answers, the classifier learns to distinguish correct tokens from erroneous ones. During inference, the judge combines with standard speculative decoding via logical OR, accepting tokens when either method approves. The approach trades the mathematical guarantee of target distribution preservation for practical speed gains, accepting tokens that are semantically valid but distributionally misaligned. Training takes under 1.5 hours with early stopping, and the judge adds minimal computational overhead while enabling higher draft token counts (M=25 vs. standard M≤7).

## Key Results
- Achieves up to 9× speedup compared to standard decoding with Llama-405B target
- Reaches throughput of 129 tokens/second while maintaining benchmark quality
- Significantly outperforms prior speculative decoding methods, especially with high-quality draft models
- Reduces rejection of correct tokens from 88% (standard SD) to 20% (judge decoding)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Target model embeddings signal token-level correctness, enabling efficient verification beyond distribution alignment.
- Mechanism: When processing incorrect tokens, the target model's last hidden layer produces embeddings that differ systematically from those produced by correct tokens. A linear classifier trained on these embeddings can predict correctness without expensive sampling or chain-of-thought reasoning.
- Core assumption: Correctness information is linearly accessible in the embedding space; the training distribution generalizes to inference tasks.
- Evidence anchors:
  - [abstract] "training a compact module on top of the target model's embeddings to predict the correctness of a given token"
  - [section 4.1] "our experiments show that last hidden layer embeddings of erroneous tokens effectively 'flag' errors"
  - [corpus] SelfJudge (2510.02329) extends this with self-supervised verification, suggesting the embedding signal is robust.
- Break condition: If judge accuracy degrades significantly on out-of-distribution tasks (as noted in Section 5.3 with HumanEval dropping from 86.6% to 80.4%), speedups come with quality loss.

### Mechanism 2
- Claim: Standard speculative decoding rejects correct tokens because it enforces exact distributional alignment rather than semantic correctness.
- Mechanism: Standard SD accepts a candidate token only if p_target[c] ≥ p_draft[c]. This rejects tokens that are semantically valid but assigned lower probability by the target, including human-written text and GPT-4o outputs.
- Core assumption: The draft model produces objectively correct responses that differ stylistically or distributionally from the target.
- Evidence anchors:
  - [abstract] "many high-quality draft tokens are rejected, even when they represent objectively valid continuations"
  - [section 3.2] "even powerful draft models such as GPT-4o, as well as human text cannot achieve high acceptance rates"
  - [corpus] AutoJudge (2504.20039) confirms that token-level distribution matching is unnecessarily strict for downstream quality.
- Break condition: If draft quality is poor, accepting non-aligned tokens degrades output quality regardless of judge accuracy.

### Mechanism 3
- Claim: Combining judge and standard verification masks via logical OR preserves correctness guarantees while increasing acceptance.
- Mechanism: When standard SD accepts a token, it guarantees target distribution preservation. The judge adds additional acceptances when the token is semantically correct but distributionally misaligned. Since accepted tokens from standard verification remain unchanged, combining masks cannot worsen quality.
- Core assumption: The judge's false positive rate is sufficiently low that accepted wrong tokens don't accumulate into visible quality degradation.
- Evidence anchors:
  - [section 4.1] "we take the logical OR between the two, z = z_stand ∨ z_judge, since when the judge rejects and standard SD accepts, the corrected token according to the target will exactly be the same token"
  - [figure 7] Shows mask combination preserves rejection cascades after disagreement points.
  - [corpus] No corpus papers directly validate this OR-combination strategy; this appears novel to Judge Decoding.
- Break condition: If judge false positives cluster in specific domains, accumulated errors may exceed benchmark detection thresholds.

## Foundational Learning

- Concept: Speculative decoding fundamentals (draft-verify loop, acceptance criteria, guaranteed distribution preservation)
  - Why needed here: Judge Decoding modifies the verification step; understanding what standard SD guarantees clarifies what is traded away.
  - Quick check question: Can you explain why standard SD provably preserves the target distribution?

- Concept: Embedding space representations in LLMs
  - Why needed here: The method relies on extracting semantic correctness signals from hidden states rather than output probabilities.
  - Quick check question: What layer's embeddings does Judge Decoding use, and why might deeper layers be preferable?

- Concept: The alignment vs. correctness distinction in language generation
  - Why needed here: The core insight is that semantically correct responses can be distributionally misaligned.
  - Quick check question: Why would GPT-4o's outputs be rejected by Llama-405B even if both are capable models?

## Architecture Onboarding

- Component map: Draft model (Llama-8B) -> Target model (Llama-70B/405B) -> Linear judge head -> Mask combiner
- Critical path: Draft generation → Target forward pass (embeddings + logits) → Parallel verification (standard + judge) → Mask combination → Token acceptance/rejection → Sample corrected token from target if rejected
- Design tradeoffs:
  - Judge threshold δ: Higher values (→1.0) recover standard SD guarantees but reduce speedup; lower values increase acceptance risk.
  - Training data diversity: More diverse wrong-answer sources improve robustness but require annotation effort.
  - Draft token count M: Judge enables M=25 vs. standard M≤7, but latency gains depend on draft-target speed ratio.
- Failure signatures:
  - Quality drops on novel tasks: Judge trained on general QA may not transfer to coding (observed 6.2% drop).
  - Low acceptance despite judge: Draft model quality insufficient; judge cannot rescue poor candidates.
  - Safety token leakage: Judge may accept tokens the target would never generate (noted as uninvestigated risk).
- First 3 experiments:
  1. Replicate the embedding error signal: Force wrong tokens into Llama-405B context, observe correction behavior in free generation (Figure 5).
  2. Train minimal judge head: Use 500 examples with weighted cross-entropy, validate on held-out correct/wrong pairs.
  3. Measure acceptance length: Compare standard SD vs. judge decoding on MT-Bench with Llama-8B/70B, targeting ≥3× acceptance increase.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the mathematical guarantee of preserving the target model's distribution be recovered while maintaining the efficiency gains of judge-based verification?
- Basis in paper: [explicit] "Our approach however also comes with a drawback; the mathematical guarantee to maintain target quality is lost by relying on the judge."
- Why unresolved: Judge decoding trades theoretical correctness guarantees for empirical performance; no formal analysis exists for bounding distributional divergence.
- What evidence would resolve it: Derivation of probabilistic bounds on distributional divergence, or a modified acceptance criterion that provably preserves target distribution while accepting more tokens.

### Open Question 2
- Question: How can judge decoding be extended to maintain quality on novel tasks without requiring task-specific annotated training data?
- Basis in paper: [explicit] "Novel tasks require the careful annotation of similar data to maintain quality, otherwise performance is lost."
- Why unresolved: Out-of-distribution experiments showed performance drops (86.6% → 80.4% on HumanEval when trained without code examples), indicating incomplete transfer.
- What evidence would resolve it: Demonstrating that a single judge trained on diverse data generalizes zero-shot to entirely new task domains without accuracy degradation.

### Open Question 3
- Question: What safety risks emerge when the judge accepts tokens from a draft model with misaligned safety properties?
- Basis in paper: [explicit] "If the draft model has safety issues, the target model could potentially accept safety-critical tokens through the judge... we have not thoroughly investigated this problem."
- Why unresolved: The paper explicitly flags this as unexplored; the judge evaluates correctness, not safety alignment.
- What evidence would resolve it: Red-teaming experiments quantifying acceptance rates of harmful/adversarial draft tokens under judge decoding versus standard verification.

### Open Question 4
- Question: What is the minimum target model scale required for the embedding-based judge to achieve reliable correctness predictions?
- Basis in paper: [explicit] "The target model needs to be of sufficient size to be able to provide accurate judgements. Speedups for smaller models such as Llama-8B are hence tougher to achieve."
- Why unresolved: Only large models (70B, 405B) were evaluated; the relationship between model scale and judge accuracy remains uncharacterized.
- What evidence would resolve it: Systematic evaluation across model scales (1B to 70B) correlating parameter count with judge head accuracy on correctness prediction.

## Limitations
- Dataset dependency: Judge performance heavily depends on quality and diversity of 500-example training set
- Safety trade-offs: Accepting non-aligned correct tokens may introduce content the target model would never generate
- Benchmark-heavy evaluation: Performance on established benchmarks doesn't guarantee real-world generalization

## Confidence
- **High Confidence**: Speed improvements (9× speedup, 129 tokens/second) and fundamental insight about standard SD's over-conservatism
- **Medium Confidence**: Claim that judge decoding preserves output quality while achieving speedups, though dependent on judge accuracy
- **Low Confidence**: Generalizability to safety-critical applications and novel domains not represented in training data

## Next Checks
1. **Domain Transfer Validation**: Test judge decoding on a held-out domain (e.g., medical, legal, or technical writing) not represented in the original 500-example training set. Measure both judge accuracy and output quality degradation to quantify domain-specific performance limits.

2. **Safety Token Acceptance Audit**: Systematically test whether the judge accepts safety-violating tokens that the target model would reject. Use established safety benchmarks (e.g., ToxiGen, RealToxicityPrompts) to measure the extent of safety guarantees being compromised.

3. **Error Accumulation Analysis**: Track the frequency and impact of judge false positives in long-generation contexts. Generate sequences of 1000+ tokens and analyze whether accepted wrong tokens accumulate to create coherent but incorrect outputs that might pass human evaluation but fail downstream tasks.