---
ver: rpa2
title: 'M2G-Eval: Enhancing and Evaluating Multi-granularity Multilingual Code Generation'
arxiv_id: '2512.22628'
source_url: https://arxiv.org/abs/2512.22628
tags:
- code
- generation
- language
- arxiv
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces M2G-Eval, a multi-granularity, multilingual
  framework for evaluating code generation capabilities across four structural levels
  (Class, Function, Block, Line) in 18 programming languages. The authors construct
  a large-scale instruction dataset (17K+ tasks) from 150K repositories and train
  two M2G-Eval-Coder models (Qwen3-8B-based) using supervised fine-tuning and Group
  Relative Policy Optimization.
---

# M2G-Eval: Enhancing and Evaluating Multi-granularity Multilingual Code Generation
## Quick Facts
- **arXiv ID:** 2512.22628
- **Source URL:** https://arxiv.org/abs/2512.22628
- **Reference count:** 23
- **Primary result:** Introduces M2G-Eval, a multi-granularity, multilingual framework for evaluating code generation capabilities across four structural levels in 18 programming languages.

## Executive Summary
This paper introduces M2G-Eval, a comprehensive framework for evaluating multi-granularity code generation across 18 programming languages. The authors construct a large-scale instruction dataset (17K+ tasks) from 150K repositories and train two M2G-Eval-Coder models using supervised fine-tuning and Group Relative Policy Optimization. Evaluation on 1,286 human-annotated test instances reveals clear difficulty hierarchies across granularity levels and demonstrates strong cross-language correlations, suggesting transferable programming concepts. The framework achieves competitive performance against state-of-the-art models while demonstrating stable gains across languages.

## Method Summary
The authors construct M2G-Eval through a systematic pipeline: first, they collect 150K repositories from GitHub, filtering for permissive licenses and specific file types. From these repositories, they extract 17,299 multi-granularity tasks spanning four structural levels (Class, Function, Block, Line) across 18 programming languages. Two M2G-Eval-Coder models are trained using a combination of supervised fine-tuning and Group Relative Policy Optimization, with the latter employing an adversarial discriminator and KL divergence regularization. The evaluation framework includes 1,286 human-annotated test instances that enable rigorous assessment of model performance across different granularity levels and language subsets.

## Key Results
- Clear difficulty hierarchy identified: Line (easiest) < Block < Function < Class (hardest)
- Performance gaps widen between full- and partial-granularity languages as task complexity increases
- Strong cross-language correlations (0.67-0.98) suggest transferable programming concepts
- M2G-Eval-Coder-RL achieves competitive performance against state-of-the-art models

## Why This Works (Mechanism)
The framework's effectiveness stems from its multi-faceted approach to code generation evaluation. By structuring tasks across four granularities (Class, Function, Block, Line), it captures the varying complexity levels inherent in programming tasks. The Group Relative Policy Optimization method, combined with adversarial training and KL regularization, enables the model to learn nuanced patterns in code structure and functionality. The cross-language evaluation design, with both full-granularity (11 languages) and partial-granularity (7 languages) subsets, provides comprehensive coverage while maintaining task relevance. The human annotation process, involving bilingual evaluators and validation protocols, ensures high-quality ground truth data that captures both syntactic correctness and functional adequacy.

## Foundational Learning
- **Multi-granularity task decomposition**: Breaking down code generation into Class, Function, Block, and Line levels enables systematic evaluation of complexity progression
  - Why needed: Different programming tasks require different levels of abstraction and detail
  - Quick check: Verify each task level has distinct characteristics and appropriate complexity

- **Cross-language transferability**: Programming concepts that transfer across languages despite syntactic differences
  - Why needed: Understanding whether code generation models can generalize beyond specific language syntax
  - Quick check: Measure correlation coefficients between language performance pairs

- **Adversarial training for code generation**: Using discriminators to distinguish between human-written and model-generated code
  - Why needed: Improves model ability to generate realistic, functional code rather than syntactically correct but impractical solutions
  - Quick check: Compare discriminator accuracy before and after training

## Architecture Onboarding
**Component map:** GitHub repositories -> Task extraction -> Multi-granularity instruction dataset -> M2G-Eval-Coder models (SFT + GRPO) -> Human-annotated test instances -> Evaluation framework

**Critical path:** Repository collection → Task extraction → Model training → Human annotation → Performance evaluation

**Design tradeoffs:** The framework balances comprehensive language coverage (18 languages) with task quality, opting for partial-granularity tasks in some languages to maintain practical relevance. The use of GRPO enables more sophisticated policy learning compared to standard RLHF, but requires careful hyperparameter tuning and more complex implementation.

**Failure signatures:** Performance degradation on higher granularity levels (Class, Function) indicates limitations in understanding complex program structure. Poor cross-language generalization suggests overfitting to specific language syntax rather than transferable concepts. Inconsistent performance across languages may reveal dataset biases or insufficient representation.

**First 3 experiments to run:**
1. Evaluate model performance on a held-out test set of multi-granularity tasks across all 18 languages
2. Measure cross-language correlation coefficients to assess transferability of programming concepts
3. Compare GRPO-trained model performance against baseline SFT-only model across different granularity levels

## Open Questions the Paper Calls Out
The paper does not explicitly identify open questions, focusing instead on presenting the framework and its evaluation results.

## Limitations
- Dataset construction relies heavily on GitHub repositories with permissive licenses, potentially introducing sampling bias toward open-source, well-documented codebases
- The 18 selected languages, though diverse, represent only a subset of popular programming languages, limiting claims about truly "multilingual" generalizability
- Performance gaps between full- and partial-granularity languages may be influenced by corpus availability and representation rather than purely linguistic complexity

## Confidence
- High confidence in the framework's systematic construction and evaluation methodology
- Medium confidence in cross-language correlation findings, pending broader language coverage
- Medium confidence in difficulty hierarchies, as task design may influence results
- Low confidence in claims about true multilingual transferability without testing on languages outside the initial corpus

## Next Checks
1. Test M2G-Eval models on programming languages not present in the training corpus to assess true multilingual generalization
2. Conduct ablation studies varying task complexity within each granularity level to isolate difficulty factors
3. Implement automated cross-annotation consistency metrics to quantify human evaluator agreement beyond the current validation protocol