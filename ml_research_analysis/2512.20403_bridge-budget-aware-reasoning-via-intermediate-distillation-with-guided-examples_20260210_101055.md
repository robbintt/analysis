---
ver: rpa2
title: 'BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples'
arxiv_id: '2512.20403'
source_url: https://arxiv.org/abs/2512.20403
tags:
- bridge
- student
- teacher
- distillation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BRIDGE addresses the challenge of distilling knowledge from large
  black-box expert models to tiny deployable models under strict API budget constraints.
  The core idea is to use an intermediate Teacher Assistant (TA) trained on a strategically
  selected subset of data to bridge the extreme capacity gap, then leverage the TA
  to generate synthetic rationales for the full dataset at zero marginal cost.
---

# BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples

## Quick Facts
- arXiv ID: 2512.20403
- Source URL: https://arxiv.org/abs/2512.20403
- Reference count: 40
- Key result: 23-41% performance improvement over direct distillation with 10× fewer teacher queries

## Executive Summary
BRIDGE addresses the challenge of distilling knowledge from large black-box expert models to tiny deployable models under strict API budget constraints. The core idea is to use an intermediate Teacher Assistant (TA) trained on a strategically selected subset of data to bridge the extreme capacity gap, then leverage the TA to generate synthetic rationales for the full dataset at zero marginal cost. The method employs a two-phase approach: budget-aware apprenticeship with coverage-guided selection, followed by instruction-first tutoring. Experiments across medical, legal, and financial domains show consistent improvements of 23-41% in student performance while using 10× fewer teacher queries compared to direct distillation baselines.

## Method Summary
BRIDGE is a two-stage distillation framework designed to transfer reasoning capabilities from large black-box models to tiny deployable models under API budget constraints. The method uses an intermediate Teacher Assistant (TA) trained on a strategically selected subset of data to bridge the capacity gap between the expert model and the tiny student model. The TA then generates synthetic rationales for the full dataset at zero marginal cost, which are used to instruction-tune the student. The approach employs budget-aware apprenticeship with coverage-guided selection followed by instruction-first tutoring. The framework shows consistent improvements of 23-41% in student performance across medical, legal, and financial domains while using 10× fewer teacher queries than direct distillation baselines.

## Key Results
- Achieves 23-41% performance improvements over direct distillation baselines
- Reduces teacher query usage by 10× compared to traditional distillation methods
- Demonstrates consistent gains across three distinct domains: medical, legal, and financial reasoning
- Outperforms both black-box and white-box distillation methods on sub-1B parameter models

## Why This Works (Mechanism)
BRIDGE succeeds by addressing the fundamental challenge of knowledge transfer between models with vastly different capacities. The intermediate TA serves as a capacity bridge, learning to approximate the expert's reasoning patterns on a carefully selected subset of data. This selective training enables the TA to capture the most informative aspects of the expert's behavior without exhausting the API budget. The synthetic rationale generation then scales this knowledge transfer to the entire dataset at zero marginal cost, providing the tiny student model with structured reasoning examples that it can effectively learn from. The instruction-first tutoring phase ensures that the student can properly process these complex reasoning traces despite its limited capacity.

## Foundational Learning

**API Budget Constraints** - Understanding that each query to the expert model has a cost that limits the total number of examples available for distillation. Why needed: The framework must work within real-world constraints where unlimited expert access is impossible. Quick check: Verify budget calculations match actual API costs.

**Teacher-Student Distillation** - Knowledge transfer from a larger, more capable model to a smaller, deployable model. Why needed: The core problem BRIDGE solves is making expert-level reasoning accessible to tiny models. Quick check: Confirm distillation loss decreases during training.

**Coverage-Guided Selection** - Strategic sampling that ensures diverse representation of reasoning patterns within budget constraints. Why needed: Random sampling may miss critical reasoning examples needed for effective transfer. Quick check: Analyze diversity metrics of selected examples.

**Synthetic Data Generation** - Creating artificial examples from the TA that preserve expert-level reasoning patterns. Why needed: Enables full-dataset training without additional expert queries. Quick check: Compare synthetic vs real examples using quality metrics.

## Architecture Onboarding

**Component Map:** Data → Budget-Aware Selection → Intermediate TA → Synthetic Rationale Generation → Student Instruction Tuning → Deployable Model

**Critical Path:** The selection of informative examples followed by TA training is the critical path, as errors here propagate through the entire pipeline. The quality of synthetic rationales directly impacts student performance.

**Design Tradeoffs:** The framework trades initial computational overhead (TA training) for long-term efficiency (zero-cost synthetic data). This is particularly valuable when deployment scale justifies the upfront investment.

**Failure Signatures:** Poor student performance typically indicates either inadequate TA training (selection was poor) or the student's inability to process complex rationales (instruction tuning was insufficient).

**First Experiments:** 
1. Test TA performance on held-out expert data to validate selection quality
2. Evaluate synthetic rationale quality using automated metrics
3. Run ablation study removing the TA to quantify its contribution

## Open Questions the Paper Calls Out
None provided in the input.

## Limitations
- The approach's effectiveness depends on the quality of synthetic rationale generation, which may not match authentic human-annotated examples
- The budget-aware selection mechanism may not generalize optimally to domains with different data distributions than those tested
- Scalability to even smaller models (sub-100M parameters) or different base architectures remains unexplored

## Confidence

**High Confidence:** The experimental methodology, dataset construction, and statistical significance of reported improvements across multiple domains are robust and well-documented.

**Medium Confidence:** The generalizability of the budget-aware selection strategy to different problem domains and reasoning types beyond the tested medical, legal, and financial domains.

**Medium Confidence:** The scalability of the approach to even smaller models (sub-100M parameters) or different base architectures that may have different reasoning capabilities.

## Next Checks

1. Conduct ablation studies isolating the contribution of the intermediate TA from the budget-aware selection mechanism to determine which component drives performance improvements.

2. Test the framework on datasets with different reasoning patterns (e.g., spatial reasoning, temporal reasoning) to assess generalizability beyond the current domains.

3. Evaluate model performance degradation when synthetic rationale quality is intentionally reduced to quantify sensitivity to TA fidelity.