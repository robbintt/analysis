---
ver: rpa2
title: 'MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM
  Watermarking'
arxiv_id: '2512.04044'
source_url: https://arxiv.org/abs/2512.04044
tags:
- watermark
- text
- watermarking
- quality
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces MarkTune, an on-policy fine-tuning framework
  that improves the quality-detectability trade-off of weight-editing watermarking
  schemes like GaussMark by treating the watermark signal as a reward during training.
  MarkTune consistently achieves detection performance close to inference-time watermarking
  methods while maintaining generation quality and downstream task performance, and
  remains robust to paraphrasing and fine-tuning attacks.
---

# MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking

## Quick Facts
- **arXiv ID**: 2512.04044
- **Source URL**: https://arxiv.org/abs/2512.04044
- **Reference count**: 40
- **Key outcome**: MarkTune improves quality-detectability trade-off for weight-editing watermarking, achieving near-inference detection performance while preserving generation quality and downstream tasks.

## Executive Summary
MarkTune is an on-policy fine-tuning framework that enhances the quality-detectability trade-off of weight-editing watermarking schemes like GaussMark. By treating the watermark signal as a reward during training, MarkTune steers models toward generating watermarked text with minimal quality degradation. The approach consistently outperforms inference-time watermarking while remaining robust to paraphrasing and fine-tuning attacks.

## Method Summary
MarkTune targets the weight-editing watermarking setting where model weights are modified to embed a watermark. The method uses GaussMark's test statistic as a reward signal within a Group Relative Policy Optimization (GRPO) framework. During training, the model generates multiple responses per prompt, computes watermark rewards, and updates parameters to maximize detection performance while preserving quality through cross-entropy regularization on high-quality text.

## Key Results
- Achieves detection performance close to inference-time watermarking methods
- Maintains generation quality with minimal perplexity increase (Qwen3-4B: 5.04 vs 5.88 baseline)
- Preserves downstream task performance on MMLU, GSM8K, and MBPP benchmarks
- Demonstrates robustness to paraphrasing and fine-tuning attacks

## Why This Works (Mechanism)

### Mechanism 1: Watermark Test Statistic as Reward Signal
Treating GaussMark's detection statistic as a reward during fine-tuning steers the model toward generating text with stronger watermark signals without proportional quality degradation. The optimization uses stop-gradient on the reference model's log-probability gradients to prevent backpropagation through the base model.

### Mechanism 2: Quality-Preserving Regularization
Cross-entropy regularization on high-quality reference text (rather than KL to the base model) preserves generation quality while allowing watermark-aware exploration. This constrains the model to remain in a "flat basin" around high-quality text distributions.

### Mechanism 3: Asymmetric Gradient Geometry in Fisher Space
In the linear-softmax regime, watermark reward increases at first order O(κ) while cross-entropy degradation grows only at second order O(κ²), enabling favorable quality-detectability trade-off. The optimal perturbation moves along watermark-sensitive directions where detection statistic mean shifts linearly with regularization strength.

## Foundational Learning

- **Statistical Hypothesis Testing for Watermarking**: Understanding false positive rate (FPR), true positive rate (TPR), and level-α tests is essential for evaluating detection performance. Quick check: Given a test with level α=0.01, what is the maximum acceptable false positive rate?

- **Policy Gradient / GRPO**: MarkTune uses GRPO to optimize the dual-objective; understanding group-relative advantages and clipped objectives is critical for implementation. Quick check: How does GRPO compute the advantage Â_j for each sample in a group?

- **KL Divergence and Cross-Entropy**: Regularization uses these metrics; Proposition 1 bounds GaussMark distortion via KL. Understanding Pinsker's inequality (TV ≤ √(2KL)) connects distortion to detectability. Quick check: Why is KL divergence a "conservative" measure of text quality degradation?

## Architecture Onboarding

- **Component map**: GaussMark soft activation -> GRPO sampler -> Advantage computation -> Dual-objective optimizer -> Detection module
- **Critical path**: Sample watermark key and inject perturbation → For each training step: sample prompts → generate responses → compute rewards → compute advantages → update policy → After training, detect via thresholded test statistic
- **Design tradeoffs**: σ (soft activation strength): Lower σ (0.6) preserves quality but requires more training steps; higher σ risks initial quality degradation. λ (regularization coefficient): 0.01 is Pareto-optimal for Qwen3-4B; 0.005 better for Llama2-7B. Group size G=8 balances computational cost and variance reduction.
- **Failure signatures**: PPL increases >0.5 from baseline → λ too small, regularization insufficient. TPR@1% FPR < 0.7 → σ too low or training steps insufficient. High Seq-rep-3 (>2.0) → sampling temperature mismatched to model.
- **First 3 experiments**: 1) Ablate λ: Sweep {0.005, 0.01, 0.05} on held-out validation set; plot PPL vs. AUC frontier. 2) Cross-dataset generalization: Train on OpenWebText, evaluate detection on C4-RealNewsLike and ELI5. 3) Attack robustness: Apply Dipper-2 paraphrasing and LoRA fine-tuning (1500 steps); measure TPR decay rate vs. GaussMark baseline.

## Open Questions the Paper Calls Out

### Open Question 1
Can MarkTune generalize to other weight-editing watermarking schemes beyond GaussMark? The paper states MarkTune "can serve as a general on-policy fine-tuning framework for improving the quality-detectability trade-off of any weight-editing watermarking by replacing the first term in (3) with the corresponding test statistic." Only GaussMark is empirically validated; no experiments with alternative schemes.

### Open Question 2
How does MarkTune scale to significantly larger models (70B+ parameters)? Experiments are limited to Qwen3-4B and Llama2-7B; scaling behavior to larger models is unknown. The computational cost and effectiveness of on-policy fine-tuning with GRPO may change substantially at larger scales.

### Open Question 3
How robust is MarkTune to full fine-tuning attacks versus the LoRA-only attacks tested? Fine-tuning robustness is only evaluated using LoRA; stronger attacks using full parameter fine-tuning are not explored. Full fine-tuning could more effectively overwrite embedded watermarks since it modifies all parameters rather than low-rank adapters.

### Open Question 4
Is there a principled method for selecting the regularization coefficient λ? The ablation study shows λ significantly impacts the trade-off, but it is selected via grid search without theoretical guidance. No theoretical framework connects λ to the optimal quality-detectability frontier for a given model and watermark strength.

## Limitations
- Fisher geometry assumptions may not hold for all model architectures or training regimes
- Evaluation does not extensively test domain-specific tasks such as code generation or dialogue systems
- Attack resistance bounds are not fully quantified for adaptive attacks

## Confidence
- **High Confidence**: Detection performance improvements over GaussMark baseline; preservation of downstream task accuracy; robustness to paraphrasing and fine-tuning attacks
- **Medium Confidence**: Quality preservation claims; theoretical analysis of Fisher geometry and second-order CE bounds; generalization to unseen datasets
- **Low Confidence**: Absolute robustness limits under adaptive attacks; scalability to much larger models; effectiveness on highly structured or code-generation tasks

## Next Checks
1. **Adaptive Attack Evaluation**: Conduct systematic study of TPR decay under iterative paraphrasing and fine-tuning attacks with increasing budgets. Measure rate of decline and identify point where MarkTune's advantage disappears.

2. **Cross-Model Scaling Study**: Apply MarkTune to models spanning three orders of magnitude in parameter count (e.g., 1B, 7B, 70B). Analyze how σ, λ, and training steps must be scaled to maintain quality-detectability trade-off.

3. **Structured Output Benchmarking**: Evaluate MarkTune on code generation, mathematical reasoning, and dialogue tasks. Compare perplexity, detection AUC, and task-specific metrics to assess potential biases in structured output spaces.