---
ver: rpa2
title: Leveraging Stochastic Depth Training for Adaptive Inference
arxiv_id: '2505.17626'
source_url: https://arxiv.org/abs/2505.17626
tags:
- skipping
- inference
- blocks
- accuracy
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for adaptive inference using models
  trained with Stochastic Depth and selective layer-skipping configurations. The key
  insight is that Stochastic Depth training makes models more resilient to skipping
  layers at inference time.
---

# Leveraging Stochastic Depth Training for Adaptive Inference

## Quick Facts
- arXiv ID: 2505.17626
- Source URL: https://arxiv.org/abs/2505.17626
- Reference count: 22
- Up to 2× improvement in power efficiency with minimal accuracy drops (as low as 0.71%)

## Executive Summary
This paper presents a framework for adaptive inference that leverages models trained with Stochastic Depth to enable selective layer-skipping at inference time. The key insight is that Stochastic Depth training makes neural networks more resilient to skipping layers during inference. The framework introduces simple zero-overhead gates to control skipping, uses sensitivity analysis to identify near Pareto-optimal skipping configurations at design time, and adapts inference at runtime based on device load. The approach is evaluated on CIFAR-10/100 using ResNet-20 and ResNet-110 architectures on an edge platform, demonstrating significant improvements in power efficiency and inference throughput with minimal accuracy degradation.

## Method Summary
The proposed framework integrates Stochastic Depth training with adaptive inference by inserting simple, zero-overhead gates that control layer-skipping during inference. At design time, sensitivity analysis identifies near Pareto-optimal skipping configurations that balance accuracy and efficiency. During runtime, the system adapts inference based on device load, selectively skipping layers when computational resources are constrained. The framework is specifically designed to work with models trained using Stochastic Depth, which inherently improves the network's resilience to layer-skipping by randomly dropping layers during training.

## Key Results
- Achieves up to 2× improvement in power efficiency compared to original models
- Processes up to 1.97× more inferences with minimal accuracy drops (as low as 0.71%)
- Provides more predictable and controllable inference compared to SkipNet without requiring additional decision gates or training complexity

## Why This Works (Mechanism)
The framework exploits the inherent robustness that Stochastic Depth training provides to neural networks. During Stochastic Depth training, layers are randomly dropped during training, which forces the network to learn redundant representations and become resilient to missing layers. This trained resilience enables the network to maintain reasonable performance even when layers are skipped during inference. The zero-overhead gates allow fine-grained control over which layers to skip, while the sensitivity analysis ensures that the selected skipping configurations optimize the accuracy-efficiency trade-off.

## Foundational Learning
- **Stochastic Depth Training**: Randomly drops layers during training to improve model resilience
  - Why needed: Creates networks that can tolerate layer-skipping during inference
  - Quick check: Verify that Stochastic Depth is properly implemented in the training pipeline
- **Sensitivity Analysis**: Evaluates model performance when skipping different layer combinations
  - Why needed: Identifies optimal skipping configurations that balance accuracy and efficiency
  - Quick check: Ensure sensitivity analysis covers a comprehensive set of layer combinations
- **Runtime Adaptation**: Dynamically adjusts layer-skipping based on device load
  - Why needed: Enables the system to respond to varying computational constraints
  - Quick check: Validate that device load monitoring accurately triggers skipping adjustments

## Architecture Onboarding
- **Component Map**: Stochastic Depth Training -> Sensitivity Analysis -> Runtime Adaptation -> Inference Engine
- **Critical Path**: Training -> Configuration Selection -> Runtime Skipping Decision -> Layer Execution
- **Design Tradeoffs**: Accuracy vs. efficiency trade-off, complexity of sensitivity analysis vs. runtime overhead
- **Failure Signatures**: Accuracy degradation beyond acceptable thresholds, incorrect skipping decisions
- **First Experiments**:
  1. Validate that Stochastic Depth training improves resilience to layer-skipping
  2. Test sensitivity analysis on a small subset of layers to verify configuration selection
  3. Evaluate runtime adaptation with simulated device load variations

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to CIFAR-10/100 datasets and ResNet architectures, limiting generalizability
- Runtime adaptation mechanism lacks detailed analysis of effectiveness under varying workloads
- Accuracy preservation claims are specific to evaluated configurations and may not generalize

## Confidence
High confidence in: The core methodology of using Stochastic Depth training to enable layer skipping, the basic framework design with zero-overhead gates, and the experimental results on the specific evaluated configurations.

Medium confidence in: The generalizability of the approach to different architectures, datasets, and deployment scenarios; the effectiveness of runtime adaptation under varying device loads.

Low confidence in: The quantitative comparison of predictability and controllability versus SkipNet; the stability of skipping configurations across diverse operating conditions.

## Next Checks
1. Test the framework on larger-scale datasets (e.g., ImageNet) and different model architectures (e.g., Vision Transformers) to assess generalizability.

2. Implement comprehensive runtime adaptation testing with varying device loads and environmental conditions to validate the dynamic skipping mechanism's effectiveness.

3. Conduct quantitative analysis of inference predictability and controllability, including measurements of latency variance and skipping pattern consistency compared to baseline methods.