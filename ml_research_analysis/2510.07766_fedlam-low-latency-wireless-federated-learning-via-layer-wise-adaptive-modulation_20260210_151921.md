---
ver: rpa2
title: 'FedLAM: Low-latency Wireless Federated Learning via Layer-wise Adaptive Modulation'
arxiv_id: '2510.07766'
source_url: https://arxiv.org/abs/2510.07766
tags:
- modulation
- learning
- latency
- layers
- communication
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the communication latency problem in wireless
  federated learning (FL) caused by transmitting high-dimensional deep neural network
  (DNN) parameters through bandwidth-limited channels. The proposed solution, FedLAM,
  introduces a layer-wise adaptive modulation scheme that assigns different modulation
  levels to individual DNN layers based on their importance, as quantified by the
  top eigenvalue of each layer's Hessian matrix.
---

# FedLAM: Low-latency Wireless Federated Learning via Layer-wise Adaptive Modulation

## Quick Facts
- arXiv ID: 2510.07766
- Source URL: https://arxiv.org/abs/2510.07766
- Reference count: 32
- Reduces communication latency by up to 73.9% while maintaining test accuracy

## Executive Summary
This paper addresses the communication bottleneck in wireless federated learning (FL) by proposing FedLAM, a layer-wise adaptive modulation scheme that assigns different M-PSK modulation levels to individual DNN layers based on their importance. The importance is quantified using the top eigenvalue of each layer's Hessian matrix, allowing more flexible and efficient modulation assignment compared to existing methods that use uniform modulation across all layers. Experiments on MNIST, Fashion-MNIST, and CIFAR-10 demonstrate significant latency reduction while maintaining the same test accuracy as traditional approaches.

## Method Summary
FedLAM optimizes wireless FL communication by assigning different modulation levels (2, 4, 8, 16) to individual DNN layers during uplink transmission. Layer importance is determined by the top eigenvalue of the Hessian matrix, computed via power iteration. The system formulates an optimization problem to maximize loss drop per unit latency and employs either enumeration (for shallow networks) or group-based search (for deep networks) to find optimal modulation assignments. The approach is validated on three datasets with different network architectures, showing substantial latency reduction compared to uniform modulation schemes.

## Key Results
- Reduces communication latency by up to 73.9% compared to existing adaptive modulation schemes
- Achieves the same test accuracy (MNIST: 97.8%, Fashion-MNIST: 90.6%, CIFAR-10: 83.1%) as baseline methods
- Demonstrates effectiveness across different network architectures: LeNet-300-100, vanilla CNN, and ResNet-20

## Why This Works (Mechanism)
The paper exploits the observation that different DNN layers have varying importance to model performance, as measured by their Hessian eigenvalues. By assigning higher-order modulation (more bits per symbol) to more important layers and lower-order modulation to less important ones, the scheme achieves better communication efficiency. This layer-wise approach is more flexible than uniform modulation schemes and allows the system to optimize the trade-off between transmission reliability and data rate based on layer-specific importance.

## Foundational Learning

**Hessian Matrix Eigenvalues**
- Why needed: Quantifies layer importance for modulation assignment
- Quick check: Verify eigenvalues computed via power iteration match analytical solutions for simple networks

**M-PSK Modulation**
- Why needed: Enables variable bit-rate transmission based on layer importance
- Quick check: Confirm BER calculation using equation (9) for different M values

**Power Iteration Method**
- Why needed: Efficiently computes top eigenvalue of large Hessian matrices
- Quick check: Measure runtime to ensure it's comparable to one backward pass

## Architecture Onboarding

**Component Map**
- Clients (compute local gradients) -> Wireless Uplink (transmit modulated parameters) -> Server (aggregate global model) -> Wireless Downlink (broadcast model) -> Clients

**Critical Path**
1. Local computation of gradients
2. Hessian eigenvalue computation for layer importance
3. Modulation level assignment optimization
4. Parameter transmission via M-PSK
5. Global aggregation and broadcast

**Design Tradeoffs**
- Communication efficiency vs. computational overhead of eigenvalue estimation
- Modulation level granularity vs. BER reliability
- Layer grouping for deep networks vs. optimal individual layer assignment

**Failure Signatures**
- Slow convergence when modulation levels don't match actual layer importance
- High communication overhead if all layers use high-order modulation
- Unstable training if eigenvalue computation is inaccurate

**First Experiments**
1. Verify BER calculation for different modulation levels under various SNR conditions
2. Compare convergence speed with uniform vs. adaptive modulation on MNIST
3. Profile eigenvalue computation time to confirm it's comparable to one backward pass

## Open Questions the Paper Calls Out

**Open Question 1**
How does FedLAM perform when the downlink channel is also noisy or bandwidth-constrained? The current system model assumes noiseless downlink broadcast, which may not hold in massive networks or edge scenarios with limited receive capability.

**Open Question 2**
Is the Hessian-based layer importance metric robust under non-IID data partitions? All experiments used i.i.d. data splits, but in non-IID settings, local Hessian matrices may differ significantly across clients, potentially causing conflicting modulation level preferences.

**Open Question 3**
Does the computational overhead of eigenvalue estimation exceed the communication savings on resource-constrained IoT hardware? While the paper claims overhead is negligible, this may not apply to low-end clients where memory and compute requirements for power iteration could negate latency benefits.

## Limitations

**Major Uncertainties**
- SNR/Es-N0 values for wireless channel simulation are not specified, directly impacting BER calculations
- Client count (n) in FL system is unspecified, affecting communication round latency
- Baseline comparison with AM [14] lacks implementation details, making fair comparison difficult

**Confidence Assessment**
- High confidence in the core algorithmic approach following established optimization principles
- Medium confidence in empirical results since methodology is sound but critical experimental parameters are missing
- Low confidence in comparative claims against AM [14] due to unspecified baseline implementation details

## Next Checks

1. Implement BER calculation using equation (9) with reasonable SNR values (e.g., 10-20 dB range) and verify that modulation level M correctly affects transmission accuracy
2. Run ablation studies on the grouping strategy for ResNet-20 to determine optimal group count and boundaries based on eigenvalue similarity
3. Verify eigenvalue computation runtime using PyHessian [22] scales as claimed (equivalent to one backward pass per layer)