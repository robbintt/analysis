---
ver: rpa2
title: Why Do Multilingual Reasoning Gaps Emerge in Reasoning Language Models?
arxiv_id: '2510.27269'
source_url: https://arxiv.org/abs/2510.27269
tags:
- language
- reasoning
- understanding
- detected
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates why multilingual reasoning
  gaps emerge in reasoning language models. It identifies that the primary cause is
  failures in language understanding, where models struggle to translate multilingual
  inputs into the dominant language (typically English) used in their reasoning traces.
---

# Why Do Multilingual Reasoning Gaps Emerge in Reasoning Language Models?

## Quick Facts
- arXiv ID: 2510.27269
- Source URL: https://arxiv.org/abs/2510.27269
- Reference count: 40
- Key outcome: Language understanding failures are the primary cause of multilingual reasoning gaps in RLMs; Selective Translation achieves near full-translation performance while translating only ~20% of inputs.

## Executive Summary
This paper systematically investigates why multilingual reasoning gaps emerge in reasoning language models (RLMs). Through a stage-wise attribution analysis using Shapley decomposition, the authors identify that failures in language understanding—specifically the inability to translate multilingual inputs into the dominant English language used in reasoning traces—are the primary driver of these gaps. The study proposes Selective Translation, a method that applies translation intervention only when understanding failures are detected, achieving substantial improvements in multilingual reasoning performance while minimizing translation overhead.

## Method Summary
The authors first collect multilingual reasoning traces from Qwen3-4B on Polymath and MMLU-ProX-Lite across 10 languages. They then develop detection methods (Prober and mmBERT) to identify understanding failures by comparing Base performance with Understanding Intervention performance. Ground-truth labels are created based on whether the Base model fails while the intervention succeeds. Detection models are trained on these labels and validated on held-out calibration data. Finally, Selective Translation is implemented: for each input, early trace tokens are analyzed by the detector, and if a failure is predicted, translation via GPT-4.1 is applied before reasoning. The pipeline is evaluated across all languages with accuracy, translation usage rate, and detection metrics.

## Key Results
- Language understanding failures account for 80.2% of the multilingual reasoning gap, compared to 10.8% for reasoning failures and 8.9% for generation failures.
- Selective Translation improves average accuracy from 81.1 to 88.0 on Polymath-Low, nearly matching full translation (89.4) while translating only 19.3% of inputs.
- Supervised detection methods (Prober and mmBERT) reliably identify understanding failures with high precision and recall.

## Why This Works (Mechanism)
The paper demonstrates that RLMs trained primarily on English data struggle to process multilingual inputs directly, leading to cascading failures in subsequent reasoning and generation stages. By detecting when the model fails to understand the input (rather than failing to reason or generate), Selective Translation applies translation intervention only when necessary, preserving the model's ability to reason in English while avoiding unnecessary translation costs.

## Foundational Learning

**Shapley value decomposition** - A method from cooperative game theory to attribute performance gaps to different stages of a pipeline. Needed to quantify the relative contribution of understanding, reasoning, and generation failures. Quick check: Verify that Shapley values sum to the total gap and that individual stage contributions are statistically significant.

**Reasoning language models** - Language models fine-tuned with reinforcement learning to generate explicit step-by-step reasoning traces. Needed because the study examines failure modes across multiple stages of the reasoning process. Quick check: Confirm that reasoning traces are generated with appropriate decoding parameters (temperature, top-p) and that the model can perform both reasoning and final answer generation.

**Understanding intervention** - A controlled method where multilingual inputs are translated to English before reasoning, isolating the effect of language understanding from reasoning capability. Needed to distinguish between understanding failures and reasoning failures. Quick check: Ensure that translation quality is high (e.g., via human evaluation or automated metrics) and that it consistently improves performance on the target language.

## Architecture Onboarding

**Component map**: Input → RLM (Base) → Detection Model → Translation Intervention (if failure detected) → Understanding Intervention → RLM (with translation) → Answer

**Critical path**: The pipeline's performance bottleneck is the detection model's accuracy and speed. The detector must process early trace tokens quickly enough to avoid adding significant latency while maintaining high precision to minimize unnecessary translations.

**Design tradeoffs**: Early detection (using fewer tokens) reduces latency but may decrease accuracy; full-trace detection is more accurate but adds computational overhead. The paper finds that detecting on the first 4096 tokens provides a good balance.

**Failure signatures**: Understanding failures manifest as incorrect reasoning traces from the start, often with garbled mathematical notation or nonsensical intermediate steps. Reasoning failures show coherent reasoning that leads to incorrect conclusions. Generation failures produce valid reasoning but incorrect final answers.

**Exactly 3 first experiments**:
1. Run the Base RLM on a small multilingual sample to collect (input, trace, answer) tuples for detector training.
2. Train and validate both Prober and mmBERT detectors on the collected data, comparing their detection performance.
3. Implement Selective Translation on a held-out test set and measure accuracy improvement and translation usage rate.

## Open Questions the Paper Calls Out

**Cross-domain generalization**: Can the findings on understanding failures as the primary cause of multilingual reasoning gaps generalize to non-mathematical reasoning domains such as commonsense reasoning, logical deduction, or open-domain question answering? The current study only evaluates on STEM tasks.

**Non-English reasoning models**: How does the understanding failure detection and Selective Translation framework perform on RLMs that natively reason in non-English languages (e.g., Chinese-centric or Russian-centric models)? The current framework assumes English as the dominant reasoning language.

**Training integration**: Can understanding failure detection and mitigation be integrated into RLM training procedures to improve multilingual reasoning capabilities end-to-end, rather than relying on inference-time interventions? The current approach is an inference-time solution.

## Limitations

- The study focuses exclusively on mathematical and STEM reasoning tasks, limiting generalizability to other reasoning domains.
- The approach relies on external translation APIs (GPT-4.1), introducing potential runtime bottlenecks and cost dependencies.
- Detection methods may exhibit varying performance across different RLM architectures or when reasoning traces become longer and more complex.

## Confidence

- **High confidence**: The primary finding that language understanding failures are the main driver of multilingual reasoning gaps.
- **Medium confidence**: The effectiveness of Selective Translation in bridging the gap across different reasoning tasks.
- **Medium confidence**: The reliability of detection methods across different RLM families and trace lengths.

## Next Checks

1. **Cross-task generalization**: Validate Selective Translation on non-mathematical reasoning datasets (e.g., HellaSwag, StrategyQA) to assess whether understanding failures remain the dominant failure mode.

2. **End-to-end latency and cost analysis**: Measure the actual runtime overhead and API costs of Selective Translation compared to full translation in real-world deployment scenarios.

3. **Detector robustness across RLM families**: Test the Prober and mmBERT detectors on reasoning traces from different RLM architectures (e.g., DeepSeek-R1, OpenAI o1) to evaluate generalizability.