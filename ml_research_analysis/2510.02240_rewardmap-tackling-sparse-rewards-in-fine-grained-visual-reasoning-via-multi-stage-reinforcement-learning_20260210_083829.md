---
ver: rpa2
title: 'RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage
  Reinforcement Learning'
arxiv_id: '2510.02240'
source_url: https://arxiv.org/abs/2510.02240
tags:
- reasoning
- visual
- arxiv
- reward
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of sparse rewards in fine-grained
  visual reasoning tasks using multimodal large language models (MLLMs). The authors
  introduce REWARDMAP, a multi-stage reinforcement learning framework that incorporates
  a difficulty-aware reward design with detail rewards to provide richer supervision
  and mitigate sparse reward issues.
---

# RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning

## Quick Facts
- arXiv ID: 2510.02240
- Source URL: https://arxiv.org/abs/2510.02240
- Authors: Sicheng Feng; Kaiwen Tuo; Song Wang; Lingdong Kong; Jianke Zhu; Huan Wang
- Reference count: 26
- Primary result: Multi-stage reinforcement learning framework with difficulty-aware rewards improves fine-grained visual reasoning by 3.47% across six benchmarks

## Executive Summary
This paper addresses the challenge of sparse rewards in fine-grained visual reasoning tasks using multimodal large language models (MLLMs). The authors introduce REWARDMAP, a multi-stage reinforcement learning framework that incorporates a difficulty-aware reward design with detail rewards to provide richer supervision and mitigate sparse reward issues. They also construct REASONMAP-PLUS, an extended dataset with dense reward signals through Visual Question Answering (VQA) tasks, enabling effective cold-start training. The method uses a curriculum-style task scheduling from simple perception to complex reasoning, ensuring smoother optimization.

## Method Summary
The authors propose REWARDMAP, a multi-stage reinforcement learning framework that addresses sparse rewards in fine-grained visual reasoning. The approach combines three key components: a difficulty-aware reward design with detail rewards for richer supervision, a curriculum-style task scheduling from simple perception to complex reasoning, and a cold-start training strategy using the constructed REASONMAP-PLUS dataset. The method employs a multi-stage RL process with perception, reasoning, and VQA stages, each with task-specific rewards. The framework is evaluated on REASONMAP and REASONMAP-PLUS datasets, demonstrating consistent performance improvements across multiple benchmarks.

## Key Results
- REWARDMAP achieves consistent performance gains on REASONMAP and REASONMAP-PLUS datasets
- Models trained with REWARDMAP show an average improvement of 3.47% across six benchmarks
- Best results achieved when combining all components: difficulty-aware rewards, detail rewards, and curriculum-style scheduling
- Enhanced visual understanding and reasoning capabilities demonstrated across spatial reasoning, fine-grained visual reasoning, and general tasks

## Why This Works (Mechanism)
The effectiveness of REWARDMAP stems from its ability to provide dense, informative reward signals in otherwise sparse reward environments. The difficulty-aware reward design with detail rewards offers more granular feedback at each reasoning step, allowing the model to learn from intermediate progress rather than only final outcomes. The curriculum-style task scheduling enables gradual skill development, starting from basic perception and building toward complex reasoning. The cold-start training on REASONMAP-PLUS with VQA tasks provides initial dense rewards that bootstrap the learning process before transitioning to more challenging reasoning tasks with sparser rewards.

## Foundational Learning
**Reinforcement Learning**: Why needed - Core optimization framework for training with reward signals; Quick check - Understand policy gradient methods and value-based approaches
**Visual Reasoning**: Why needed - The target task domain requiring both perception and logical inference; Quick check - Can distinguish between perception and reasoning tasks
**Sparse Rewards**: Why needed - Common challenge in complex reasoning tasks where correct answers are rare; Quick check - Identify scenarios where reward is only given at task completion
**Curriculum Learning**: Why needed - Enables gradual skill building from simple to complex tasks; Quick check - Understand staged learning progression
**Multimodal Learning**: Why needed - Combines visual and textual information for comprehensive understanding; Quick check - Recognize how different modalities complement each other

## Architecture Onboarding
**Component Map**: Perception Stage → Reasoning Stage → VQA Stage
**Critical Path**: Input Image & Question → Perception Module → Reasoning Module → Output Answer
**Design Tradeoffs**: The multi-stage approach trades increased training complexity for more effective learning in sparse reward environments. While requiring more computational resources and careful curriculum design, it provides significantly better performance than end-to-end training approaches.
**Failure Signatures**: Models may struggle with tasks requiring long reasoning chains if intermediate rewards are not properly calibrated. Performance degradation can occur if the curriculum progression is too steep or if detail rewards are not sufficiently informative.
**First Experiments**:
1. Evaluate baseline performance on REASONMAP without any reward enhancements
2. Test the impact of detail rewards alone on reasoning accuracy
3. Measure the effectiveness of curriculum scheduling by comparing progressive vs. simultaneous training

## Open Questions the Paper Calls Out
None

## Limitations
- Lacks explicit discussion of model architecture details and hyperparameter settings
- Training methodology relies heavily on the constructed REASONMAP-PLUS dataset
- Computational cost of the multi-stage training process is not discussed

## Confidence
- **High confidence**: Experimental results showing consistent improvements across REASONMAP and REASONMAP-PLUS datasets
- **Medium confidence**: Claim of enhanced visual understanding and reasoning capabilities across six diverse benchmarks
- **Medium confidence**: Effectiveness of difficulty-aware reward design with detail rewards

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of the difficulty-aware reward design, detail rewards, and curriculum-style task scheduling to the overall performance gain.

2. Evaluate the method on additional datasets beyond REASONMAP and REASONMAP-PLUS to assess generalization capabilities and robustness to different types of visual reasoning tasks.

3. Analyze the computational overhead and training time requirements compared to baseline approaches to determine practical feasibility for real-world applications.