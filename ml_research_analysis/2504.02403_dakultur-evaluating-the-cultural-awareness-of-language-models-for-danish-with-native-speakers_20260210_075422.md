---
ver: rpa2
title: 'DaKultur: Evaluating the Cultural Awareness of Language Models for Danish
  with Native Speakers'
arxiv_id: '2504.02403'
source_url: https://arxiv.org/abs/2504.02403
tags:
- cultural
- danish
- language
- data
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DAKULTUR, the first native Danish cultural
  awareness dataset for evaluating Large Language Models (LLMs). The dataset is created
  through a native-speaker-driven evaluation study with 63 demographically diverse
  participants who prompt three Danish-adapted LLMs (SNAKMODEL, LLAMA 2-7B-chat+INSTda,
  LLAMA 2-7B-base+INSTda) with cultural tasks and rate their responses.
---

# DaKultur: Evaluating the Cultural Awareness of Language Models for Danish with Native Speakers

## Quick Facts
- arXiv ID: 2504.02403
- Source URL: https://arxiv.org/abs/2504.02403
- Reference count: 12
- Primary result: Native Danish data training more than doubles cultural alignment acceptance rates (42% vs. 14-15%)

## Executive Summary
This paper introduces DAKULTUR, the first native Danish cultural awareness dataset for evaluating Large Language Models (LLMs). Through a native-speaker-driven evaluation study with 63 demographically diverse participants, the authors demonstrate that training on native Danish data significantly improves cultural alignment compared to training on translated data. The study evaluates three Danish-adapted LLMs on 1,038 high-quality input-response pairs across 12 cultural topics, revealing that linguistic competence alone does not guarantee cultural competence.

## Method Summary
The authors conducted a native-speaker-driven evaluation study with 63 Danish participants who prompted three Danish-adapted LLMs (SNAKMODEL, LLAMA 2-7B-chat+INSTda, LLAMA 2-7B-base+INSTda) with cultural tasks and rated their responses. The dataset contains 1,038 input-response pairs covering 12 cultural topics. SNAKMODEL was trained on native Danish data through language modeling training (LMTda) and instruction tuning on native Danish data (+INSTda), while the other models used translated data for instruction tuning. The study combined automated benchmark evaluation (ScandEval) with human acceptance ratings and demographic-stratified analysis.

## Key Results
- Training on native Danish data more than doubles acceptance rates compared to translated data (42%→42% vs. 14%→15%)
- SNAKMODEL shows particular improvements in culturally-specific topics like food and traditions
- Cultural alignment varies by demographics, with male-identity participants and those from the capital region showing slightly higher acceptance rates
- Models trained on translated data show limited cultural awareness despite linguistic competence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training on native-language data significantly improves cultural alignment in LLMs compared to training on translated data.
- Mechanism: Native data contains culturally-embedded context, implicit norms, and linguistic patterns that are lost or distorted during translation. The LLM learns these associations directly from native speakers' authentic expressions, capturing both semantic content and cultural nuances.
- Core assumption: The translation process strips away or fails to capture critical cultural nuances implicitly present in native text.
- Evidence anchors: [abstract] "...training on native-speaker data can more than double response acceptance rates."; [section] "...translated data is insufficient to induce much cultural knowledge into the model..."

### Mechanism 2
- Claim: Linguistic competence does not guarantee cultural competence; they are distinct capabilities.
- Mechanism: An LLM can learn grammar, vocabulary, and syntax from translated data without learning underlying cultural values, norms, and shared knowledge. The model's objective optimizes for language modeling (predicting tokens), which can be satisfied by surface-level linguistic patterns.
- Core assumption: Cultural knowledge is not fully encoded in surface-level linguistic forms and requires exposure to authentic cultural expressions.
- Evidence anchors: [abstract] "...disentangle linguistic versus cultural proficiency..."; [section] "Qualitatively, we observe that answers are almost never rejected due to linguistic errors..."

### Mechanism 3
- Claim: Cultural alignment varies across demographic groups within a single culture.
- Mechanism: Culture is not uniform but is experienced and prioritized differently by individuals based on age, gender identity, and region. A model's cultural alignment is therefore perceived differently by different subgroups.
- Core assumption: Intra-cultural variation is significant enough to affect model evaluation.
- Evidence anchors: [section] "Female-identity participants tend toward food, lifestyle, education, and norms..."; [section] "...younger participants and those from the capital region report slightly higher acceptance rates."

## Foundational Learning

- Concept: **Instruction Tuning**
  - Why needed here: The paper evaluates models that have undergone instruction tuning (+INSTda). Understanding this process is critical for interpreting results showing that instruction tuning on translated data can enforce language use but fails to impart cultural knowledge.
  - Quick check question: Why might instruction tuning on translated data make a model speak Danish without making it culturally aware?

- Concept: **Anglocentric Bias in LLMs**
  - Why needed here: The paper's central problem is that LLMs, predominantly trained on English data, provide culturally misaligned responses.
  - Quick check question: What is an example of an "anglocentric" response to a culturally-specific Danish query?

- Concept: **Cultural Alignment vs. Linguistic Proficiency**
  - Why needed here: This is the paper's core conceptual distinction. An engineer must separate these goals, as results show improving one does not automatically improve the other.
  - Quick check question: Why would a model fluent in Danish still fail a test on Danish cultural norms?

## Architecture Onboarding

- Component map:
  1. Base Model: Foundational LLM (LLaMA 2-7B) pretrained on mostly English corpus
  2. Adaptation Module (LMTda): Continued pre-training on native Danish text (key differentiator for SNAKMODEL)
  3. Instruction Tuning Module (+INSTda): Fine-tuning on instruction-response dataset; paper tests translated vs. native variants
  4. Human Evaluator: Native speakers providing binary accept/reject judgments with demographic data
  5. Automated Benchmark Suite (ScandEval): Linguistic and cultural task metrics for comparison

- Critical path:
  1. Select a Base Model with strong general capabilities
  2. Perform LMT on high-quality native Danish text (most critical step for cultural alignment)
  3. Perform Instruction Tuning to enable prompt-following in Danish
  4. Evaluate using both automated benchmarks and demographically diverse human studies

- Design tradeoffs:
  - Native vs. Translated Data: Translated data is cheaper/faster but yields poor cultural alignment. Native data is expensive but critical, potentially doubling acceptance rates. Prioritize smaller amounts of native data for cultural adaptation.
  - Automated vs. Human Evaluation: Automated metrics are cheaper but may not correlate with perceived cultural awareness. Targeted, non-translated benchmarks can serve as useful proxies.

- Failure signatures:
  - Fluent but Shallow Responses: Grammatically correct Danish with factually incorrect, incomplete, or anglocentric answers
  - High Automated Scores, Low Human Acceptance: Strong linguistic benchmark performance but failure on human-judged cultural tasks
  - Demographically Skewed Acceptance: Wide variation in acceptance across user demographics suggests training data bias toward specific sub-cultures

- First 3 experiments:
  1. Native Data Ablation: Measure cultural awareness on DAKULTUR while progressively increasing native Danish data in LMT phase to establish the volume-alignment relationship
  2. Instruction Tuning Source Ablation: Compare acceptance rates between models instruction-tuned on translated vs. native Danish data to isolate that phase's contribution
  3. Demographic Performance Profiling: Evaluate an existing Danish-capable model on DAKULTUR, analyzing acceptance rates by demographic group to reveal sub-population alignment gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the findings regarding the superiority of native data over translated data for cultural alignment hold true for significantly larger model architectures (e.g., 70B+ parameters) or different model families?
- Basis in paper: [explicit] The authors state in the Limitations section: "On the technical side, we hope that future work will be able to validate our findings across more base models and languages."
- Why unresolved: The study was restricted to LLAMA 2-7B variants due to computational and data resource availability.
- What evidence would resolve it: Replicating the DAKULTUR evaluation study using larger parameter sets (e.g., 70B) or different architectures (e.g., Mistral, GPT-4) adapted with similar data strategies.

### Open Question 2
- Question: What specific training interventions are required to break the observed 42% acceptance rate ceiling for cultural tasks in Danish LLMs?
- Basis in paper: [explicit] The conclusion highlights that "the maximum acceptance rate of 42% highlights that more research is needed to fully align anglocentric LLMs to smaller language communities."
- Why unresolved: While the paper demonstrates that native data is better than translated data, it does not identify a methodology to resolve the majority of rejections.
- What evidence would resolve it: A follow-up study testing advanced alignment techniques (such as RLHF or RAG) on top of the native-data training to measure if the acceptance rate can consistently exceed 50-60%.

### Open Question 3
- Question: Does the higher alignment observed with male-identity and capital-region participants persist when evaluating against a demographically representative sample of the general population?
- Basis in paper: [inferred] The authors note a demographic skew ("advertised primarily at higher educational institutions") and observe that "contemporary models seem to... align slightly better with male-identity participants under 30 from the capital region."
- Why unresolved: The current participant pool may not capture the "full breadth of the Danish cultural landscape."
- What evidence would resolve it: A new evaluation round specifically recruiting participants from underrepresented regions and demographics to compare acceptance rates against the original study's baseline.

## Limitations
- Focus on a single language pair (Danish/English), limiting generalizability to other low-resource languages
- Limited participant pool (n=63) may miss nuanced intra-cultural variations
- 42% acceptance rate ceiling indicates cultural alignment remains challenging even with native training data

## Confidence

- **High**: The finding that native data training more than doubles acceptance rates compared to translated data (42% vs. 14-15%)
- **Medium**: The claim that cultural alignment varies by demographic group, given the limited sample size
- **Medium**: The distinction between linguistic and cultural competence as separable capabilities

## Next Checks

1. Replicate the study with a larger participant pool (n>100) to confirm demographic stratification effects
2. Test whether combining translated and native data in different ratios achieves similar results to pure native training
3. Apply the same methodology to another low-resource language (e.g., Swedish or Finnish) to test generalizability