---
ver: rpa2
title: 'CTkvr: KV Cache Retrieval for Long-Context LLMs via Centroid then Token Indexing'
arxiv_id: '2512.15550'
source_url: https://arxiv.org/abs/2512.15550
tags:
- uni00000013
- uni00000011
- uni00000003
- uni00000048
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CTkvr, a centroid-then-token indexing method
  for efficient long-context LLM inference. It addresses the challenge of KV cache
  retrieval in long contexts by leveraging the observation that adjacent queries share
  similar top-k keys after RoPE.
---

# CTkvr: KV Cache Retrieval for Long-Context LLMs via Centroid then Token Indexing

## Quick Facts
- arXiv ID: 2512.15550
- Source URL: https://arxiv.org/abs/2512.15550
- Reference count: 40
- Primary result: Achieves 3×-4× throughput speedup on Llama-3-8B and Yi-9B with <1% accuracy degradation at 96K context length

## Executive Summary
CTkvr addresses the computational bottleneck of KV cache retrieval in long-context LLM inference by leveraging the observation that adjacent queries share similar top-k keys after Rotary Positional Embedding (RoPE). The method employs a two-stage retrieval approach: first using lightweight centroids for coarse-grained indexing, then refining with token-level retrieval. This strategy significantly reduces the computational overhead while maintaining accuracy. The system incorporates CPU-GPU co-execution and custom CUDA kernels to optimize performance across hardware boundaries.

## Method Summary
CTkvr implements a centroid-then-token indexing method that exploits query similarity patterns in long-context LLM attention mechanisms. The approach first clusters queries into centroids, performs coarse-grained top-k retrieval using these centroids, then refines results at the token level. This hierarchical structure reduces the computational complexity of KV cache retrieval from O(n²) to near-linear scaling. The method integrates with existing attention mechanisms while requiring minimal modifications to model architecture. System optimizations include parallel CPU-GPU execution pipelines and specialized CUDA kernels for efficient centroid computation and retrieval operations.

## Key Results
- Achieves 3× throughput speedup on Llama-3-8B and 4× on Yi-9B at 96K context length
- Maintains <1% accuracy degradation compared to full KV cache on HumanEval and MMLU benchmarks
- Demonstrates consistent performance improvements across different attention head configurations

## Why This Works (Mechanism)
The method exploits the fundamental property that RoPE introduces smooth positional variations in attention patterns, causing adjacent queries to share similar top-k keys. This creates natural clustering opportunities where multiple queries can share the same centroid representation. The two-stage retrieval process first uses centroids to quickly identify relevant key regions, then performs fine-grained token-level refinement only within those promising regions. This reduces unnecessary computation across the entire KV cache while preserving accuracy for the most relevant attention operations.

## Foundational Learning

**Rotary Positional Embedding (RoPE)** - Why needed: Provides the smooth positional encoding that enables query clustering. Quick check: Verify RoPE patterns create consistent query similarity across different sequence positions.

**Hierarchical Attention Retrieval** - Why needed: Enables coarse-to-fine search strategy that reduces computational complexity. Quick check: Confirm that centroid-based retrieval maintains high recall for top-k keys.

**Custom CUDA Kernels** - Why needed: Optimizes centroid computation and retrieval operations for GPU acceleration. Quick check: Benchmark kernel performance against standard attention implementations.

## Architecture Onboarding

**Component Map**: Input Queries -> Centroid Computation -> Coarse-grained Retrieval -> Token-level Refinement -> Attention Output

**Critical Path**: The bottleneck shifts from full KV cache search to centroid computation and the refinement stage, requiring careful optimization of these components for latency reduction.

**Design Tradeoffs**: Accuracy vs. speed tradeoff is managed by the centroid threshold selection - aggressive clustering increases speed but may reduce recall accuracy. The refinement stage compensates for potential centroid approximation errors.

**Failure Signatures**: Performance degradation occurs when query similarity assumption breaks down (highly irregular attention patterns), centroid clustering produces poor recall, or refinement stage becomes the bottleneck due to inefficient CUDA implementation.

**3 First Experiments**: 
1. Measure accuracy degradation as a function of centroid count and refinement depth
2. Benchmark centroid computation overhead vs. full attention baseline
3. Profile GPU utilization across centroid and refinement stages to identify bottlenecks

## Open Questions the Paper Calls Out
None

## Limitations
- Performance heavily dependent on the assumption of query similarity after RoPE, which may not hold for all workloads
- 1% accuracy degradation claim based only on HumanEval and MMLU benchmarks, limiting generalizability
- Evaluation focused primarily on 96K context length, with uncertainty about performance at other lengths

## Confidence
- **High confidence** in algorithmic framework and system optimizations
- **Medium confidence** in 3×-4× speedup claims due to hardware-specific configurations
- **Low confidence** in universal applicability across different model architectures and data distributions

## Next Checks
1. Test method's robustness across diverse sequence patterns, including highly irregular or non-local attention patterns
2. Evaluate accuracy degradation on additional benchmarks beyond HumanEval and MMLU, especially for tasks requiring precise long-range dependencies
3. Systematically measure performance impact across varying context lengths (16K, 32K, 64K, 128K) to understand scaling behavior