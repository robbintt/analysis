---
ver: rpa2
title: Are you going to finish that? A Practical Study of the Tokenization Boundary
  Problem
arxiv_id: '2601.23223'
source_url: https://arxiv.org/abs/2601.23223
tags:
- token
- prompt
- continuation
- language
- partial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that the partial token problem (PTP) significantly
  degrades language model performance in realistic prompts, even when prompts respect
  word boundaries. This occurs frequently in languages without whitespace (e.g., Chinese,
  where up to 25% of word boundaries misalign with token boundaries) and in code (over
  50% of punctuation boundaries misalign).
---

# Are you going to finish that? A Practical Study of the Tokenization Boundary Problem

## Quick Facts
- arXiv ID: 2601.23223
- Source URL: https://arxiv.org/abs/2601.23223
- Reference count: 22
- Primary result: PTP degrades accuracy by 60-95% and reduces probability of correct continuations by up to 4 orders of magnitude

## Executive Summary
The paper identifies the Partial Token Problem (PTP), where language models systematically distort predictions when prompts end mid-token. This occurs frequently in languages without whitespace (Chinese: 14-25% word boundaries misaligned) and code (over 50% punctuation boundaries misaligned). The degradation is severe—models place 2-4 orders of magnitude less probability on correct continuations and experience 60-95% accuracy drops. PTP severity does not diminish with model scale; larger models are often more sensitive because they better learn tokenizer constraints. ByteSampler, which samples from all valid token sequences covering the prompt, achieves 100% accuracy with minimal overhead (0.12-1.17 additional forward passes).

## Method Summary
The paper evaluates PTP by creating "repeat-after-me" tasks where prompts end at word boundaries that fall inside tokens. Using Chinese (FLORES corpus, Jieba segmentation), German (FLORES corpus, CharSplit segmentation), and code (MBPP dataset), the authors measure misalignment rates between word/syntactic boundaries and token boundaries. They compare word-aligned prompts (ending at word boundaries) against token-aligned prompts (backed off to token boundaries) using Δ Logprob and Δ Acc metrics. ByteSampler is implemented by building a tree of all valid token sequences covering the prompt string and sampling from this distribution.

## Key Results
- PTP occurs in 14-25% of Chinese word boundaries and over 50% of code punctuation boundaries
- Models place up to four orders of magnitude less probability on correct continuations
- Accuracy drops by 60-95% for word-aligned prompts compared to token-aligned
- PTP severity does not diminish with model scale and often worsens
- ByteSampler achieves 100% accuracy with 0.12-1.17 additional forward passes on average

## Why This Works (Mechanism)

### Mechanism 1: Tokenization Boundary Enforcement Distorts Distribution
When a prompt ends mid-token, the forced token boundary creates an out-of-distribution context that the model has never seen during training. During pretraining, models only see valid tokenization outputs. When a user prompt forces a boundary inside what would be a single token (e.g., ⟨processing⟩ split as ⟨process, in⟩), the resulting context is systematically excluded from training data. The model learns this implicitly—the context ⟨process, in⟩ signals that "g" cannot follow because if it were valid, the single token ⟨processing⟩ would have been used.

### Mechanism 2: Scaling Amplifies Tokenizer Fidelity
Larger models exhibit equal or greater sensitivity to PTP because they more precisely learn the tokenizer's constraints, not because they're more robust. As model capacity increases, models better fit the statistical patterns in training data—including the implicit constraint that certain token sequences never occur. A larger model given ⟨process, in⟩ is more confident that ⟨g⟩ is invalid because it has more precisely learned this regularity.

### Mechanism 3: ByteSampler Recovers True Text-Level Distribution
ByteSampler achieves 100% accuracy by enumerating all valid token sequences covering the prompt string, then sampling from this distribution without forcing a boundary at the prompt end. Rather than tokenizing once and treating the prompt end as a hard boundary, ByteSampler constructs a tree where each path represents a valid token sequence that "covers" the prompt string. Sampling proceeds from this tree, allowing the model to naturally complete partial tokens.

## Foundational Learning

- **Concept: Subword Tokenization (BPE/WordPiece)** - Why needed: The entire PTP phenomenon arises from how these algorithms merge frequent character sequences into tokens, creating boundaries that don't align with semantic units. Quick check: Given a BPE tokenizer where ⟨processing⟩ is a single token, what happens if you tokenize "process" and "ing" separately versus as "processing"?

- **Concept: Autoregressive Language Modeling** - Why needed: PTP specifically affects next-token prediction; understanding how models condition on token history is essential to grasp why forced boundaries distort predictions. Quick check: In equation P(c₁, ..., cₙ | p₁, ..., pₘ), why does the tokenization of the prompt affect the probability distribution over continuations?

- **Concept: Pre-tokenization and Whitespace Handling** - Why needed: English avoids PTP largely because pre-tokenization splits on whitespace, preventing tokens from bridging word boundaries. This doesn't hold for Chinese/German/code. Quick check: Why does ending an English prompt with a complete word (no trailing space) guarantee token alignment, but the same guarantee doesn't exist for Chinese?

## Architecture Onboarding

- **Component map:**
  User text prompt → [Tokenizer] → Token sequence with forced boundary at prompt end → [LM forward pass] → Next-token distribution conditioned on tokenized prompt → [Detokenizer] → Text continuation

- **Critical path:** The tokenizer → LM interface. The prompt string is converted to tokens; any mismatch between where the user ended typing and where tokens actually split creates the failure mode.

- **Design tradeoffs:**
  - Token healing: Simple, zero overhead, but requires knowing how far to back off; may change semantic context
  - ByteSampler: Exact solution, 100% accuracy in experiments, but requires 0.12-1.17 additional forward passes on average
  - Training-time solutions (stochastic tokenization): Would require retraining, may reduce compression efficiency

- **Failure signatures:**
  - Chinese: Model generates semantically equivalent but token-different continuations (e.g., 的1/2 instead of 的一半)
  - German: Character skipping or insertion in compound words (e.g., "stufen" → "instufen")
  - Code: Extra spaces (`. append` vs `.append`), missing characters (`__init__` → `_init__`)
  - Universal: 2-4 orders of magnitude probability drop on correct continuation despite trivial task

- **First 3 experiments:**
  1. Reproduce the misalignment rate measurement: Take your target tokenizer, sample 1000 Chinese/German Wikipedia entries, compute what fraction of word boundaries (via Jieba/CharSplit) don't align with token boundaries. Expect 14-25% for Chinese.
  2. Repeat-after-me PTP probe: Construct minimal test cases where prompt ends at a word boundary but mid-token. Measure log-prob difference between word-aligned and token-aligned prompts. Expect 3-4 orders of magnitude drop for Chinese.
  3. ByteSampler overhead benchmark: Implement ByteSampler for your inference stack. Measure actual forward pass overhead on your production prompt distribution. Paper reports 0.12-1.17 additional passes; verify if this holds for your use case.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can training-time interventions such as stochastic tokenization effectively mitigate the partial token problem without degrading encoding efficiency or diluting string probabilities? The paper notes this remains unknown, with stochastic tokenization potentially reducing tokenizer efficiency and diluting probability over multiple tokenizations.

- **Open Question 2:** How does the partial token problem manifest in open-ended generation tasks with multiple plausible continuations, beyond controlled repetition tasks? The paper evaluates PTP using controlled tasks but notes that translation/completion tasks "can have multiple plausible continuations" and don't fully characterize behavior in truly open-ended settings.

- **Open Question 3:** What factors predict when token healing succeeds versus fails, and can these insights yield more robust heuristic methods? Table 4 shows token healing achieves 99.08% accuracy in Chinese but only 83.16% in German, with the paper stating "the effectiveness of token healing is mixed" without analyzing failure cases.

## Limitations

- **Tokenization method specificity:** Findings are heavily dependent on BPE/WordPiece tokenization and may not generalize to alternative approaches like SentencePiece or byte-level tokenization.

- **Task-specific generalization:** Results are based primarily on "repeat-after-me" tasks, and the real-world severity in open-ended generation tasks (creative writing, problem-solving, dialogue) remains uncertain.

- **Model architecture dependence:** All experiments use decoder-only transformers; PTP behavior in encoder-decoder models or models with different positional encoding schemes may differ.

## Confidence

**High Confidence** (supported by direct experimental evidence):
- PTP exists and causes severe probability distortion (4 orders of magnitude)
- PTP occurs frequently in Chinese (14-25% misalignment) and code (>50%)
- ByteSampler achieves 100% accuracy with minimal overhead in tested cases
- PTP severity does not diminish with model scale

**Medium Confidence** (supported by experiments but with caveats):
- PTP degrades accuracy by 60-95% in word-aligned prompts
- Token healing shows mixed results but can be effective
- The scaling amplification mechanism (larger models learn tokenizer constraints better)

**Low Confidence** (inferred but not directly tested):
- PTP severity in open-ended generation tasks
- PTP behavior in encoder-decoder architectures
- Effectiveness of ByteSampler on very long prompts (exponential tree growth)
- Whether stochastic tokenization during training would mitigate PTP

## Next Checks

1. **Open-ended generation validation:** Design a study measuring PTP effects in unconstrained generation tasks (story continuation, code completion, question answering) rather than controlled "repeat-after-me" tasks to establish real-world severity.

2. **Architecture comparison study:** Test PTP across diverse model architectures (encoder-decoder, Mamba, RWKV) and tokenization methods (SentencePiece, byte-level) to determine whether PTP is fundamental or specific to current dominant approaches.

3. **Training-time intervention experiment:** Implement and evaluate stochastic tokenization during training, where each sequence is tokenized with slight variations, to measure whether this reduces PTP severity compared to standard deterministic training.