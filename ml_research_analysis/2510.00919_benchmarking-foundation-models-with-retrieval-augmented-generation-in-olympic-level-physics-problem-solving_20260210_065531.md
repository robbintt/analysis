---
ver: rpa2
title: Benchmarking Foundation Models with Retrieval-Augmented Generation in Olympic-Level
  Physics Problem Solving
arxiv_id: '2510.00919'
source_url: https://arxiv.org/abs/2510.00919
tags:
- physics
- answer
- question
- arxiv
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PhoPile, the first multimodal RAG benchmark
  for Olympiad-level physics, featuring 390 evaluation questions and 2,662 retrieval
  corpus items across diverse physics subfields. It evaluates both text-only and vision-language
  models with multiple retrievers, demonstrating that retrieval-augmented generation
  can improve model performance, with the best configurations achieving up to 30.51%
  pass rate and 6.20 average score on multimodal tasks.
---

# Benchmarking Foundation Models with Retrieval-Augmented Generation in Olympic-Level Physics Problem Solving

## Quick Facts
- **arXiv ID**: 2510.00919
- **Source URL**: https://arxiv.org/abs/2510.00919
- **Reference count**: 40
- **Primary result**: First multimodal RAG benchmark for Olympiad-level physics with 390 evaluation questions and 2,662 retrieval corpus items, showing retrieval augmentation improves model performance up to 30.51% pass rate on multimodal tasks

## Executive Summary
This paper introduces PhoPile, the first multimodal RAG benchmark designed for Olympiad-level physics problem solving. The benchmark features 390 evaluation questions spanning 2,662 retrieval corpus items across diverse physics subfields including mechanics, electromagnetism, thermodynamics, and modern physics. The study evaluates both text-only and vision-language models using multiple retrieval strategies, demonstrating that retrieval-augmented generation can significantly improve model performance compared to baseline approaches.

The research reveals important insights about the effectiveness of different retrieval methods in physics problem solving, with dense retrievers showing superior performance over sparse retrievers in many cases. The benchmark also highlights the challenges of applying RAG systems to complex scientific reasoning tasks, particularly due to noisy retrievals and the tendency of models to generate guided rather than direct answers. The paper provides an LLM-as-judge evaluation framework specifically tailored for physics reasoning assessment.

## Method Summary
The PhoPile benchmark was constructed through a systematic process involving the curation of physics problems from Olympiad-level sources and the creation of a comprehensive retrieval corpus. The benchmark includes both text-only and multimodal evaluation sets, with problems designed to test various physics subfields. Multiple retriever types were evaluated including dense retrievers (e.g., ColPali), sparse retrievers (e.g., BM25), and hybrid approaches. Models were assessed using both direct accuracy metrics and an LLM-as-a-judge framework specifically developed for physics reasoning evaluation.

The evaluation framework incorporated inter-annotator agreement metrics to ensure reliability of the LLM-as-judge approach. Performance was measured across different model families including GPT-4o, Claude-3.5-Sonnet, and open-source vision-language models. The study examined both single-answer and multi-answer problems, with varying levels of difficulty and complexity to comprehensively assess model capabilities in physics problem solving.

## Key Results
- Retrieval augmentation improved model performance significantly, with the best configurations achieving up to 30.51% pass rate on multimodal tasks
- Dense retrievers (particularly ColPali) outperformed sparse retrievers (BM25) in most physics problem categories
- Vision-language models showed superior performance on multimodal tasks compared to text-only models, with average scores reaching 6.20
- The LLM-as-judge evaluation framework demonstrated acceptable inter-annotator agreement for physics reasoning assessment

## Why This Works (Mechanism)
Retrieval-augmented generation improves physics problem solving by providing models with relevant contextual information that supplements their internal knowledge. The mechanism works through two key phases: first, retrievers identify relevant passages from the physics corpus based on the problem query; second, the language model integrates this retrieved information with its own reasoning capabilities to generate comprehensive solutions. This approach is particularly effective for Olympiad-level physics where problems often require combining multiple concepts or accessing specialized knowledge not well-represented in pretraining data.

The effectiveness stems from the ability to bridge knowledge gaps in foundation models, especially for recent or specialized physics topics. Dense retrievers like ColPali leverage multimodal embeddings to better match visual problems with relevant textual explanations, while sparse retrievers excel at keyword-based matching for text-only problems. The LLM-as-judge framework provides a scalable way to evaluate complex physics reasoning without requiring domain expert annotation for every response, though this introduces some subjectivity in assessment.

## Foundational Learning
**Retrieval-Augmented Generation (RAG)**: Combines information retrieval with language model generation to enhance responses with external knowledge. *Why needed*: Foundation models have knowledge cutoffs and limitations in specialized domains. *Quick check*: Can the system correctly identify when external retrieval is needed versus using internal knowledge?

**Dense vs. Sparse Retrievers**: Dense retrievers use neural embeddings for semantic matching while sparse retrievers rely on keyword matching (e.g., BM25). *Why needed*: Different retriever types excel at different matching strategies and data types. *Quick check*: Does the retriever accurately find semantically relevant passages even when exact keywords are missing?

**Multimodal Embeddings**: Vector representations that capture both visual and textual information in a unified space. *Why needed*: Physics problems often combine diagrams with text, requiring joint understanding. *Quick check*: Can the system correctly align visual elements with their textual descriptions?

**LLM-as-a-Judge**: Using language models to evaluate the quality of generated responses instead of human annotators. *Why needed*: Provides scalable evaluation for complex reasoning tasks. *Quick check*: Does the judge model's assessment correlate with expert human evaluation?

**Physics Problem Taxonomy**: Classification of problems by subfield (mechanics, E&M, thermodynamics, etc.) and difficulty level. *Why needed*: Enables systematic evaluation across different physics domains. *Quick check*: Can the system correctly categorize problems and apply appropriate solving strategies?

## Architecture Onboarding

**Component Map**: User Query -> Retriever (Dense/Sparse/Hybrid) -> Retrieved Corpus -> Language Model -> Response Generation -> LLM-as-Judge Evaluation

**Critical Path**: The retriever selection and retrieval quality are the most critical components, as poor retrieval directly impacts the quality of generated answers. The LLM-as-judge evaluation adds an additional layer that must be calibrated for physics-specific reasoning assessment.

**Design Tradeoffs**: Dense retrievers offer better semantic matching but require more computational resources and training data. Sparse retrievers are faster and more interpretable but may miss relevant information due to exact keyword matching limitations. The choice between text-only and multimodal approaches depends on problem type and available visual information.

**Failure Signatures**: Common failure modes include noisy or irrelevant retrievals (52-75% accuracy across retrievers), models generating guided rather than direct answers, and LLM-as-judge inconsistencies in evaluating complex physics reasoning. Models may also struggle with problems requiring synthesis of multiple concepts or recent physics developments not in the retrieval corpus.

**First 3 Experiments**:
1. Compare dense retriever (ColPali) versus sparse retriever (BM25) performance on text-only physics problems
2. Evaluate vision-language model performance on multimodal problems versus text-only models on equivalent text descriptions
3. Test the impact of retrieval quality by evaluating models with synthetically cleaned retrieval sets versus original noisy retrievals

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- The LLM-as-judge evaluation framework introduces potential subjectivity despite inter-annotator agreement metrics
- Retrieval corpus contains noisy or irrelevant items with accuracy ranging from 52% to 75% across different retrievers
- Limited number of retriever types tested, requiring further validation of claims about retriever superiority
- Performance ceiling of 30.51% pass rate may be influenced by uncontrolled experimental factors

## Confidence
- **High**: Benchmark construction methodology and documented performance gaps between text-only and multimodal models are well-supported
- **Medium**: Claims about superiority of specific retriever types need more extensive comparative studies
- **Medium**: Domain-specific retriever necessity assertion requires broader validation
- **Low**: Quantification of "guided answers" versus direct solutions needs more rigorous measurement

## Next Checks
1. Expand LLM-as-judge evaluation to larger sample size (n=100+) with cross-validation across different judge models
2. Conduct ablation studies isolating retrieval quality impact using synthetically cleaned retrieval sets
3. Implement automated detection of guided versus direct answers using semantic similarity metrics and answer structure analysis