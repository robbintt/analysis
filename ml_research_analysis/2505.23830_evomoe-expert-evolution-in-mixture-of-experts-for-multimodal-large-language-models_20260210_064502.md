---
ver: rpa2
title: 'EvoMoE: Expert Evolution in Mixture of Experts for Multimodal Large Language
  Models'
arxiv_id: '2505.23830'
source_url: https://arxiv.org/abs/2505.23830
tags:
- expert
- experts
- router
- evomoe
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two key challenges in multi-modal mixture-of-experts
  (MoE) models: expert uniformity and router rigidity. Expert uniformity occurs when
  MoE experts initialized from replicated parameters become homogeneous during training,
  while router rigidity stems from static linear routers failing to distinguish between
  visual and text tokens.'
---

# EvoMoE: Expert Evolution in Mixture of Experts for Multimodal Large Language Models

## Quick Facts
- **arXiv ID:** 2505.23830
- **Source URL:** https://arxiv.org/abs/2505.23830
- **Reference count:** 40
- **One-line primary result:** EvoMoE outperforms state-of-the-art sparse MLLMs across multiple benchmarks, achieving 1.4-1.2% average performance gains while activating only the top-1 expert.

## Executive Summary
This paper addresses two key challenges in multi-modal mixture-of-experts (MoE) models: expert uniformity and router rigidity. Expert uniformity occurs when MoE experts initialized from replicated parameters become homogeneous during training, while router rigidity stems from static linear routers failing to distinguish between visual and text tokens. To tackle these issues, the authors propose EvoMoE, a novel MoE-tuning framework featuring two key innovations: expert evolution and a dynamic token-aware router (DTR). Expert evolution generates diverse MoE experts by iteratively adapting parameters from a single trainable expert using gradient-based updates and evolution values. The DTR employs hypernetworks to dynamically generate routing weights tailored to each input token based on its modality and intrinsic value.

## Method Summary
EvoMoE is a three-stage MoE-tuning framework that addresses expert uniformity through evolution-based initialization and router rigidity through dynamic token-aware routing. Stage I performs dense warm-up training to establish cross-modal alignment. Stage II trains only one FFN expert (FFN1) while generating three evolved experts via gradient accumulation with random β sampling from [0.7–0.99]. Stage III freezes all evolved experts and trains only the Dynamic Token-aware Router (DTR) hypernetworks and final linear router. The DTR uses modality-specific hypernetworks to generate routing weights per token, enabling fine-grained routing decisions based on input characteristics.

## Key Results
- EvoMoE achieves 1.4-1.2% average performance gains across MME, MMBench, TextVQA, and POPE benchmarks compared to state-of-the-art sparse MLLMs
- Top-1 expert activation reduces activated parameters compared to baseline top-2 routing while maintaining superior performance
- Expert evolution produces more diverse specialists than replication-based initialization, with evolved experts outperforming the original expert on most benchmarks individually
- Dynamic token-aware router with modality-specific hypernetworks improves performance by +1.1% over single linear routers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Evolution-based initialization produces more diverse experts than replication-based initialization.
- **Mechanism:** A single trainable FFN (θ₁) generates N experts via: θₙ ← β·θ₁ + (1−β)·∇θ₁, where β controls the balance between prior parameters and accumulated gradient updates. Random β sampling from [0.9–0.99], [0.8–0.89], [0.7–0.79] per training step creates variation.
- **Core assumption:** Gradient trajectories encode meaningfully different functional specializations that persist when frozen.
- **Evidence anchors:**
  - [Section 3.2]: Eq. (1) defines evolution formula; β randomly assigned per step for generalization.
  - [Table 3]: Evolved experts (β=0.7–0.9) outperform original expert (β=1.0) on most benchmarks individually.
  - [Corpus]: Limited direct corroboration; neighbor papers focus on routing dynamics, not evolution-based initialization.
- **Break condition:** If gradient directions are highly correlated across steps (e.g., narrow task distribution), evolved experts may remain functionally similar despite different β.

### Mechanism 2
- **Claim:** Modality-conditional hypernetworks break router rigidity by generating token-specific routing parameters.
- **Mechanism:** Two hypernetworks Hᵥ and Hₜ process visual and text tokens separately, generating up/down-sampling weights (Θᵤₚ, Θᵈₒᵥₙ) per token. A final linear layer maps projected features to expert probabilities.
- **Core assumption:** Visual and text tokens require fundamentally different routing mappings that static weights cannot capture.
- **Evidence anchors:**
  - [Section 3.3]: Eqs. (5–8) define hypernetwork-generated weights and routing probability.
  - [Table 4]: Modality-specific router (+1.1% avg over single router); adding shared router hurts performance.
  - [Corpus]: CL-MoE and PASs-MoE confirm router-expert misalignment as a known failure mode in continual learning.
- **Break condition:** If input modalities have highly overlapping feature distributions (e.g., text-rich images), hypernetworks may generate near-identical weights, negating benefits.

### Mechanism 3
- **Claim:** Freezing evolved experts while training only DTR prevents catastrophic interference and preserves specialization.
- **Mechanism:** Stage II produces frozen expert parameters. Stage III trains only hypernetworks Hᵥ, Hₜ and final linear layer ϕ—all experts remain fixed.
- **Core assumption:** Expert specialization is fragile; fine-tuning all experts jointly destroys evolved diversity.
- **Evidence anchors:**
  - [Table 6]: Training all experts in Stage III causes performance drop vs. frozen experts.
  - [Section 4.3]: Ablation shows MoE-tuning (training all) yields negligible gain over dense baseline.
  - [Corpus]: SAME and PASs-MoE similarly advocate stabilizing experts during continual tuning.
- **Break condition:** If downstream tasks require significant expert adaptation beyond routing, frozen experts may underfit.

## Foundational Learning

- **Concept: Mixture-of-Experts (MoE) sparsely activates sub-networks via token-level routing**
  - Why needed here: EvoMoE inherits the MoE premise—diverse experts should specialize—then diagnoses why standard MoE-tuning fails (uniformity, rigidity).
  - Quick check question: Can you explain why top-k routing with k=1 reduces activated parameters compared to dense models?

- **Concept: Hypernetworks generate weights conditionally on inputs**
  - Why needed here: DTR uses hypernetworks to produce routing weights per token, breaking the static-router assumption.
  - Quick check question: How does a hypernetwork differ from a standard MLP that processes inputs directly?

- **Concept: Multi-stage instruction tuning progressively refines capabilities**
  - Why needed here: EvoMoE's three-stage pipeline separates warm-up, expert evolution, and routing optimization—each with different frozen/trainable components.
  - Quick check question: Why might training all components jointly fail when the goal is expert diversity?

## Architecture Onboarding

- **Component map:** Vision encoder (CLIP-L/SigLIP-L) → MLP projector → LLM backbone with MoE FFN layers → Dynamic Token-aware Router (DTR) with hypernetworks
- **Critical path:** 1. Run Stage I warm-up to get checkpoint. 2. In Stage II, train only FFN1; at each step, snapshot θ₁ + accumulated ∇θ₁ with random β to create θ₂, θ₃, θ₄. 3. Freeze all experts; Stage III trains Hᵥ, Hₜ, ϕ only.
- **Design tradeoffs:**
  - Top-k=1 minimizes activated params but requires high expert specialization; top-k=2 degrades performance in this framework (Table 11).
  - Adding a shared router on top of modality-specific hypernetworks reduces performance—modality separation is critical.
  - Random β sampling improves generalization but introduces stochasticity; fixed β yields less robust experts.
- **Failure signatures:**
  - Experts perform nearly identically → likely replication initialization without evolution.
  - Router assigns similar expert distributions for image and text tokens → router rigidity; check if DTR hypernetworks are training.
  - Performance drops when training all experts in Stage III → confirms need to freeze evolved experts.
- **First 3 experiments:**
  1. **Sanity check:** Run inference with shuffled router on MoE-tuning baseline; confirm negligible performance change (validates expert uniformity problem per Figure 1a).
  2. **Evolution ablation:** Compare fixed β (0.7, 0.8, 0.9) vs. random β sampling; measure per-expert performance on held-out benchmarks (replicate Table 3).
  3. **Router ablation:** Replace DTR with single linear router on evolved experts; compare modality-specific vs. shared routing (replicate Table 4).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why do shared experts, which are beneficial in large language models (e.g., DeepSeek-V3), fail to provide performance improvements in the EvoMoE framework?
- **Basis in paper:** [explicit] Section 4.3 (MoE Strategy Exploration) states, "While shared experts are common in LLM MoE implementations, they have not provided significant benefits in MLLMs."
- **Why unresolved:** The paper identifies this empirical negative result but does not offer a theoretical or mechanistic explanation for why multi-modal token distributions differ from text-only distributions in a way that renders shared experts ineffective.
- **What evidence would resolve it:** Analysis of routing distributions comparing shared vs. specialized experts to determine if visual tokens require strictly distinct processing paths that shared capacities cannot accommodate.

### Open Question 2
- **Question:** Why does EvoMoE achieve optimal performance with top-1 expert activation, contrary to standard MoE practices where top-2 is typically superior?
- **Basis in paper:** [explicit] Table 11 and Section B.3 show top-1 outperforms top-2, noting this "is contrary to previous MoE experimental results."
- **Why unresolved:** This finding suggests the evolved experts or the Dynamic Token-aware Router (DTR) possess such high specificity that combining experts is detrimental, but the paper does not analyze if this limits the model's representational complexity.
- **What evidence would resolve it:** A study measuring the semantic overlap or conflict between the top-1 and top-2 selected experts for ambiguous multi-modal inputs.

### Open Question 3
- **Question:** What are the inference latency and computational overheads of the Dynamic Token-aware Router (DTR) compared to standard linear routers?
- **Basis in paper:** [inferred] The method introduces hypernetworks (two MLP layers) to generate routing weights dynamically (Eq. 5). While the paper highlights reduced "activated parameters" (sparse FFNs), it does not report the FLOPs or wall-clock time required to compute the routing weights for every token.
- **Why unresolved:** Hypernetworks are computationally heavier than the static linear projections used in baselines; this cost could offset the efficiency gains from sparse FFN activation.
- **What evidence would resolve it:** Detailed benchmarks of inference throughput (tokens/second) and router FLOPs comparing EvoMoE against the MoE-LLaVA baseline.

## Limitations
- **Hypernetwork architecture underspecification:** The paper states DTR uses "two MLP layers" for Hᵥ and Hₜ but omits hidden dimensions, activation functions, and weight initialization schemes, complicating exact reproduction.
- **β sampling variance effect:** While random β sampling is claimed to improve generalization, the paper doesn't quantify the trade-off between β variance and expert diversity, and the optimal β range appears empirically chosen.
- **Inference overhead uncertainty:** The paper highlights reduced activated parameters but doesn't report the computational cost of dynamically generating routing weights for every token, which could offset efficiency gains.

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Core hypothesis that expert evolution addresses uniformity | High |
| DTR's superiority over linear routers | Medium |
| Freezing experts is necessary for maintaining specialization | Medium |

## Next Checks
1. **Sanity check replication:** Shuffle router assignments on MoE-tuning baseline and confirm negligible performance change, validating that expert uniformity is the primary bottleneck in standard approaches.
2. **Evolution ablation study:** Systematically compare fixed β values (0.7, 0.8, 0.9) vs. random β sampling across multiple training runs, measuring both average performance and per-expert specialization scores on held-out benchmarks.
3. **Router ablation with ablation study:** Replace DTR with single linear router on evolved experts and compare against modality-specific vs. shared routing configurations, including ablation of hypernetwork components to isolate their contribution.