---
ver: rpa2
title: Relational Linearity is a Predictor of Hallucinations
arxiv_id: '2601.11429'
source_url: https://arxiv.org/abs/2601.11429
tags:
- hallucination
- relation
- relations
- linearity
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a new perspective on LLM hallucinations,\
  \ showing that the linearity of a relation is a strong predictor of hallucination\
  \ rates on synthetic entities. Using a novel dataset of 6000 synthetic subjects\
  \ across six relations, the authors find that more linear relations (those well-approximated\
  \ by affine transformations) lead to significantly higher hallucination rates across\
  \ four different models (r \u2208 [.78,.82])."
---

# Relational Linearity is a Predictor of Hallucinations
## Quick Facts
- arXiv ID: 2601.11429
- Source URL: https://arxiv.org/abs/2601.11429
- Reference count: 22
- Primary result: Linear relations predict higher hallucination rates on synthetic entities (r ∈ [.78,.82])

## Executive Summary
This paper demonstrates that the linearity of a relation—its approximation by an affine transformation—is a strong predictor of hallucination rates in large language models when queried about synthetic (non-existent) entities. Using a novel dataset of 6000 synthetic subjects across six relations, the authors find that models hallucinate more on linear relations (e.g., country_language, father's first name) than on nonlinear ones (e.g., athlete_sport, musician_instrument), with correlation coefficients between 0.78 and 0.82 across four models. The mechanism is that linear relations are stored more abstractly as shared transformations, making it harder for models to determine if they truly "know" a triple. Nonlinear relations, stored more instance-specifically, facilitate better self-knowledge assessment.

## Method Summary
The authors measure relational linearity using Δcos, which computes the improvement in cosine similarity when predicting object representations from subject representations plus a learned relation difference vector, relative to a baseline. They generate 6000 synthetic subjects (1000 per relation across six relations) and prompt four LLM models to answer queries about these entities. An LLM-as-a-judge (Gemini 2.5 Flash) classifies responses as hallucination or refusal. The critical analysis computes Pearson correlation between Δcos (linearity measure) and hallucination rates on synthetic data, finding strong positive correlations.

## Key Results
- Strong correlation (r ∈ [.78,.82]) between relational linearity and hallucination rate on synthetic entities
- Linear relations (high Δcos) cause more hallucinations than nonlinear relations
- This occurs because linear relations are stored more abstractly as shared transformations
- Nonlinear relations are stored more instance-specifically, facilitating knowledge self-assessment

## Why This Works (Mechanism)
### Mechanism 1
- Claim: A relation's linearity is a strong predictor of hallucination rate on synthetic entities.
- Mechanism: The model encodes linear relations as a shared, abstract "relation difference vector" that produces an object for any subject via vector addition. When a subject is truly unknown (synthetic), the model lacks instance-specific storage to detect non-knowledge, so the abstract transformation still produces a plausible object, causing hallucination. Nonlinear relations lack such a shared transformation; with no stored triple, the model has no mechanism to produce an object and is more likely to refuse.
- Core assumption: Knowledge self-assessment relies on accessing or probing instance-specific stored triples.
- Evidence anchors: Strong correlation (r ∈ [.78, .82]) between relational linearity (Δcos) and hallucination rate on synthetic subjects; text explaining that linear relations "do not require an explicit representation of the (subject, relation, object) triple."

### Mechanism 2
- Claim: Linear relations are stored more abstractly (as a shared transformation), while nonlinear relations tend to be stored more instance-specifically (as individual triples).
- Mechanism: During training, the model learns that certain relations can be efficiently captured by a single linear transformation applied to subject representations. For nonlinear relations, this is inefficient, so the model memorizes individual subject-object pairs. For nonlinear relations, the presence or absence of a stored triple for a specific subject is a clear signal of knowledge vs. non-knowledge. For linear relations, the transformation always produces an object, providing no such signal.
- Core assumption: The model's internal representations for linear and nonlinear relations are fundamentally different in structure.
- Evidence anchors: Abstract states that "linear relations tend to be stored more abstractly"; description of how a single relation difference vector is estimated from subject-object pairs.

### Mechanism 3
- Claim: On natural (non-synthetic) data, higher linearity correlates with lower hallucination rates because linear relations are "easier" to learn.
- Mechanism: Linear relations can be generalized from fewer examples via the shared transformation, making them more robust for known entities. Nonlinear relations require memorization, which is harder and leads to lower accuracy for less frequent triples. This creates the observed negative correlation between linearity and hallucination on natural data.
- Core assumption: The accuracy for known triples is influenced by the ease of learning (generalization vs. memorization).
- Evidence anchors: Negative correlation between Δcos and hallucination rate on LRE triples; text explaining that "triples of nonlinear relations are harder to learn, accuracy is lower."

## Foundational Learning
- Concept: Linear Representation Hypothesis
  - Why needed here: The entire analysis rests on representing relations as vector transformations in the model's hidden state space.
  - Quick check question: Can you describe how a relation could be implemented as vector arithmetic (e.g., "king - man + woman ≈ queen") in an LLM's embedding space?

- Concept: Affine Transformation (Translation-only)
  - Why needed here: The linearity measure Δcos uses a constrained affine map (W=I, b=translation vector) to model the relation.
  - Quick check question: What does it mean for W to be the identity matrix in an affine transformation y = Wx + b? How does this simplify the relation to a translation?

- Concept: Hallucination vs. Refusal in LLMs
  - Why needed here: The paper's core behavioral metric distinguishes hallucination (confidently providing an incorrect/ungrounded answer) from refusal (acknowledging lack of knowledge).
  - Quick check question: When an LLM is asked about a completely made-up entity, what are the two primary behavioral outcomes, and how do they differ in terms of the model's self-knowledge assessment?

## Architecture Onboarding
- Component map: SyntHal Dataset (6 relations × 1000 synthetic subjects) -> Linearity Probe (Δcos) -> LLM-as-a-Judge (Gemini 2.5 Flash) -> Representation Extraction (mid-layer subjects, late-layer objects)
- Critical path: (1) Compute Δcos for relations on natural triples. (2) Measure hallucination rate on SyntHal synthetic subjects per relation. (3) Compute correlation between Δcos and hallucination rate across relations.
- Design tradeoffs: Translation-only affine map (W=I) is simpler but may miss full linear structure; synthetic subjects isolate non-knowledge cleanly but may have distributional quirks; LLM-as-a-Judge introduces evaluator bias though human validation showed 100% agreement.
- Failure signatures: Weak or absent correlation between Δcos and hallucination rate; refusal rates near 0% or 100% for all relations; Δcos yields near-zero values for all relations.
- First 3 experiments: 1) Reproduce main correlation on one model (e.g., Llama-3.1-8B-Instruct): compute Δcos for six relations, measure SyntHal hallucination rates. Verify r ≈ 0.82. 2) Ablate linearity measurement: test learned linear map (W ≠ I) or different extraction layers. Observe if correlation holds/strengthens. 3) Probe mechanism causally: for a high-linearity relation, suppress the computed relation direction in hidden states and measure effect on hallucination rate.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How precisely are relations of intermediate linearity represented in LLMs?
- Basis in paper: [explicit] "Future work should elucidate how precisely relations of intermediate linearity are represented."
- Why unresolved: The paper treats linearity as a continuous predictor but only analyzes six relations; the internal mechanisms for partially linear relations remain uncharacterized.
- What evidence would resolve it: Mechanistic interpretability studies (e.g., probing, activation patching) on relations spanning the full ∆cos spectrum.

### Open Question 2
- Question: Can causal effects of relational linearity on hallucination be established through intervention-based methods?
- Basis in paper: [explicit] "Establishing causal effects would require complementary intervention-based evaluations, such as representation patching or controlled steering, which we leave to future work."
- Why unresolved: Current analyses are correlational at the relation level, not causal.
- What evidence would resolve it: Experiments where the linearity of a relation's representation is perturbed and hallucination rates change predictably.

### Open Question 3
- Question: How can abstract representations of linear-relation triples be supplemented to improve knowledge self-assessment?
- Basis in paper: [inferred] The conclusion suggests "the representation of linear-relation triples could be supplemented with additional information that supports certainty assessment" as a target for reducing hallucinations.
- Why unresolved: No concrete method is proposed or tested for augmenting representations.
- What evidence would resolve it: An intervention that adds instance-specific information to linear relations and measurably reduces hallucination rates on synthetic entities.

### Open Question 4
- Question: Does relational linearity predict hallucination independently of output-space priors and answer entropy?
- Basis in paper: [explicit] "Disentangling representational accessibility from output-space priors is an important direction for future work."
- Why unresolved: Relations with concentrated answer spaces (e.g., languages, sports) may confound the observed ∆cos–hallucination correlation.
- What evidence would resolve it: Controlled experiments matching answer-space entropy across linear and nonlinear relations, or partial correlations that fully eliminate the confound.

## Limitations
- The linearity measure Δcos is a proxy that uses translation-only affine maps (W=I) and may miss full linear structure
- Synthetic subjects may have distributional quirks differing from real entities
- Relations with concentrated answer spaces may confound the linearity-hallucination correlation
- Small test set sizes for some relations (country_language has only 6 test examples)

## Confidence
High: Strong correlation (r ∈ [.78,.82]) between relational linearity and hallucination rate is well-established across four models and replicated methods.
Medium: The proposed mechanism (abstract vs. instance-specific storage) is well-motivated but not directly observed through mechanistic analysis.
Low: The paper acknowledges that disentangling representational accessibility from output-space priors is an important future direction, suggesting uncertainty about potential confounds.

## Next Checks
1. Reproduce the main correlation on Llama-3.1-8B-Instruct: compute Δcos for six relations and measure hallucination rates on SyntHal synthetic subjects, verifying r ≈ 0.82.
2. Verify the LLM-as-a-judge classification: run 200 samples through both Gemini 2.5 Flash and the regex baseline to confirm ~96% agreement.
3. Test the representation extraction: extract subject/object hidden states using the specified layers (ℓs = ⌊L/2⌋, ℓo = L-2) and verify that the relation difference vectors d̄r are computed correctly.