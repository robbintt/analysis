---
ver: rpa2
title: 'Runtime Safety Monitoring of Deep Neural Networks for Perception: A Survey'
arxiv_id: '2511.05982'
source_url: https://arxiv.org/abs/2511.05982
tags:
- safety
- monitoring
- detection
- adversarial
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Runtime safety monitoring is essential for deploying deep neural
  networks in safety-critical perception systems, such as autonomous driving and robotics,
  to address vulnerabilities like generalization errors, out-of-distribution inputs,
  and adversarial attacks. This survey categorizes runtime safety monitoring approaches
  into three groups: monitoring inputs, internal representations, and outputs.'
---

# Runtime Safety Monitoring of Deep Neural Networks for Perception: A Survey

## Quick Facts
- arXiv ID: 2511.05982
- Source URL: https://arxiv.org/abs/2511.05982
- Reference count: 40
- One-line primary result: Runtime safety monitoring is essential for deploying deep neural networks in safety-critical perception systems, such as autonomous driving and robotics, to address vulnerabilities like generalization errors, out-of-distribution inputs, and adversarial attacks.

## Executive Summary
This survey systematically categorizes runtime safety monitoring approaches for deep neural networks in safety-critical perception systems. The authors organize monitoring methods into three categories: input monitoring, internal representation monitoring, and output monitoring. Each category addresses different aspects of DNN safety, with input monitoring detecting anomalies before propagation, internal monitoring observing hidden layer activations, and output monitoring analyzing prediction confidence. The survey emphasizes that no single approach comprehensively addresses all safety risks, highlighting the need for integrated frameworks. Future advancements should focus on adaptability, efficiency, and explainability to ensure real-time detection of safety concerns in dynamic environments.

## Method Summary
The survey reviews runtime safety monitoring methods that operate without modifying the monitored DNN. Three main categories are identified: input monitoring uses reconstruction error or robustness analysis to detect anomalies in the input space; internal representation monitoring tracks activation patterns in hidden layers using techniques like hyperrectangle abstractions and clustering; output monitoring analyzes prediction uncertainty and consistency through methods like Monte Carlo dropout and softmax entropy. Combined approaches integrate multiple strategies to address diverse error types. The evaluation framework emphasizes detection accuracy for OOD samples, adversarial inputs, and in-distribution errors, while also considering computational overhead for real-time deployment.

## Key Results
- No single monitoring approach comprehensively addresses all safety risks; integrated frameworks combining multiple strategies are needed
- Internal representation monitoring shows the broadest coverage across safety concerns but requires significant computational resources
- Output-based methods like Monte Carlo dropout can detect misclassifications through uncertainty estimation but struggle with overconfident OOD errors
- Input monitoring effectively catches obvious anomalies but struggles with subtle issues like rare environmental conditions

## Why This Works (Mechanism)

### Mechanism 1: Input Monitoring via Reconstruction and Robustness Analysis
- Claim: Input monitoring can detect adversarial examples and out-of-distribution inputs before they propagate through the DNN by identifying statistical anomalies in the input space.
- Mechanism: Two primary approaches operate at the input level: (1) reconstruction-based methods train autoencoders on in-distribution data and flag inputs with high reconstruction error or anomalous latent representations; (2) robustness verification computes a "robustness radius" for inputs, where adversarial examples empirically exhibit significantly smaller radii than legitimate inputs.
- Core assumption: Anomalous inputs (adversarial or OOD) exhibit statistically distinguishable properties—either reconstruction difficulty or reduced local robustness—that separate them from the training distribution.
- Evidence anchors:
  - [abstract] "Input monitoring detects anomalies in data before propagation"
  - [section IV.A] Liu et al. demonstrate "adversarial examples have significantly smaller robustness radii compared to correctly classified inputs" on MNIST and CIFAR-10
  - [section IV.A] Asad et al.'s adversarial autoencoder "monitors the reconstruction error and latent space distribution, effectively detecting OOD samples"
  - [corpus] "Topology of Out-of-Distribution Examples in Deep Neural Networks" neighbor paper supports that OOD examples exhibit distinct geometric properties, though specific input-monitoring methods are not detailed in corpus.
- Break condition: Input monitoring "struggles with subtler issues, such as rare environmental conditions" that don't produce obvious statistical anomalies in the input space itself.

### Mechanism 2: Internal Representation Monitoring via Activation Boundary Abstraction
- Claim: Monitoring hidden layer activations can detect OOD inputs by identifying when neuron activations fall outside ranges observed during training.
- Mechanism: During a calibration phase, the monitor records activation value ranges for each neuron (or groups of neurons) across training data, typically using hyperrectangles or box abstractions. At runtime, inputs whose activations exceed these boundaries are flagged as anomalous. More sophisticated variants use clustering or decision trees on activation patterns.
- Core assumption: OOD inputs produce activation patterns that systematically deviate from in-distribution patterns in hidden layers, creating detectable boundary violations.
- Evidence anchors:
  - [abstract] "internal representation monitoring analyzes neural activations for unexpected patterns"
  - [section IV.B] Henzinger et al. "use hyperrectangles for each neuron in the hidden layer to capture the range of activation values observed during training"
  - [section IV.B] Yatbaz et al. demonstrate on KITTI and NuScenes that "earlier layers improve error detection compared to final-layer activations" for 3D object detection
  - [section IV.B] Wu et al.'s Box Abstraction Monitors show "strong performance on the KITTI and BDD100K datasets with minimal computational overhead"
- Break condition: Methods require access to internal architecture (not all deployed models expose this), and "often require high computational resources" for real-time operation on complex networks.

### Mechanism 3: Output Monitoring via Uncertainty and Consistency Analysis
- Claim: Analyzing prediction confidence and consistency can detect misclassifications without requiring access to the DNN's internals or training data.
- Mechanism: Output monitors apply uncertainty estimation techniques (Monte Carlo dropout for variance estimation, softmax entropy analysis) or consistency checks (comparing outputs before/after input transformations, measuring KL-divergence). Low confidence or high inconsistency signals potential error.
- Core assumption: Correct predictions exhibit higher confidence and greater consistency under perturbation than incorrect ones.
- Evidence anchors:
  - [abstract] "output monitoring assesses prediction confidence and consistency"
  - [section IV.C] "Monte Carlo dropout leverages stochastic forward passes during inference to estimate prediction variance, enabling the detection of misclassifications"
  - [section IV.C] Kumura et al. measure "Kullback-Leibler-Divergence between outputs before and after input transformations, demonstrating robustness on benchmarks"
  - [corpus] Weak direct corpus support for output monitoring methods; neighbor papers focus on adversarial robustness training and OOD topology rather than runtime output analysis.
- Break condition: DNNs are known to "produce highly confident yet incorrect results in unfamiliar scenarios"—output monitoring alone cannot reliably detect overconfident OOD errors.

## Foundational Learning

- **Concept: Out-of-Distribution (OOD) Detection**
  - Why needed here: OOD inputs are one of three primary safety concerns RSMs must address. Understanding that DNNs "tend to make over-confident yet incorrect predictions" on unseen distributions is essential for selecting appropriate monitoring strategies.
  - Quick check question: An image contains an object class never seen during training. Will a standard softmax classifier likely output (a) uniformly distributed probabilities, (b) low maximum probability, or (c) high probability on some (wrong) class?

- **Concept: Adversarial Perturbations vs. Distribution Shifts**
  - Why needed here: The survey distinguishes adversarial attacks (deliberate, often imperceptible noise) from OOD inputs (natural distribution shifts). Different monitors address these differently—input robustness analysis targets adversarial cases while activation monitoring better handles natural distribution shifts.
  - Quick check question: If adding carefully optimized noise that's nearly invisible to humans causes your stop-sign detector to output "speed limit 80," is this an OOD error or an adversarial attack?

- **Concept: Uncertainty Types in Deep Learning (Epistemic vs. Aleatoric)**
  - Why needed here: Output-based monitors rely on uncertainty estimation. Epistemic uncertainty (model ignorance, reducible with data) responds to OOD inputs differently than aleatoric uncertainty (inherent data noise). Monte Carlo dropout primarily captures epistemic uncertainty.
  - Quick check question: Your autonomous vehicle encounters dense fog not represented in training data. Is the resulting prediction uncertainty primarily epistemic or aleatoric?

## Architecture Onboarding

- **Component map:**
  [Input x] -> [DNN Φ -> latent z -> Ψ -> Output y] -> [Safety Alert]
                      ↓            ↓          ↓
                [Input        [Internal    [Output
                 Monitor]      Monitor]     Monitor]
                      ↓            ↓          ↓
                      └────────────┴──────────┘

The monitored DNN has two logical components: Φ transforms inputs to latent representations; Ψ performs the task (classification, segmentation, detection). RSMs observe one or more observation points without modifying the DNN.

- **Critical path:**
  1. Identify which safety concerns dominate your application (generalization errors, OOD, adversarial) using Table II mapping
  2. Determine architectural constraints: Can you access internal activations? What's your latency budget?
  3. Collect calibration dataset representing nominal operation
  4. Train/configure monitor(s): fit activation boundaries, train reconstruction models, or set uncertainty thresholds
  5. Validate on held-out failure cases before deployment
  6. Integrate alert signals into system-level safety response (fallback, safe state, human handover)

- **Design tradeoffs:**
  - Detection stage: Input monitors catch issues early but miss subtle cases; internal monitors provide deeper insight at computational cost; output monitors are simplest but limited by DNN overconfidence
  - Coverage vs. specificity: Per Table II, internal representation monitoring has the broadest coverage (✓✓ for all three concerns), but no single approach fully addresses any concern
  - Deployment friction: External RSMs preserve DNN certification and enable "plug-and-play compatibility" but may have less signal access than internal modifications

- **Failure signatures:**
  - Input monitor: Low recall on "subtle issues" like rare weather; high false positives on valid but unusual lighting
  - Internal monitor: Overhead causes missed real-time deadlines; activation ranges from calibration data don't cover operational envelope
  - Output monitor: Silent failures when DNN is "highly confident yet incorrect" on OOD inputs
  - Combined systems: Meta-model miscalibration causes vote-splitting on true positives

- **First 3 experiments:**
  1. **Establish baseline with softmax entropy**: On your existing deployed model, log softmax entropy for all predictions over 1 week. Identify threshold that catches 80% of known failure cases from your incident database. Measure false positive rate.
  2. **Profile activation ranges on one hidden layer**: Extract activations from a mid-network layer (e.g., backbone features in object detector). Compute min-max bounds per channel on validation set. Test detection rate on collected OOD scenarios (different weather, time of day, new object types).
  3. **Compare coverage across monitor types**: Implement one method from each category (reconstruction error for input, box abstraction for internal, MC-dropout for output). Evaluate all three on the same test set containing: (a) in-distribution errors, (b) OOD inputs, (c) synthetic adversarial examples. Document which monitor catches which failure type per Table II patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can input, internal, and output monitoring methods be effectively integrated into a unified framework to comprehensively address safety risks?
- Basis in paper: [explicit] The discussion states, "These limitations highlight the need for integrated frameworks that combine multiple monitoring strategies."
- Why unresolved: Current literature focuses on individual methods; no single existing monitor addresses all safety risks (e.g., adversarial attacks, OOD, and generalization errors) simultaneously.
- What evidence would resolve it: A framework proposal that successfully fuses at least two of the three monitoring categories (input, internal, output) and demonstrates superior detection rates across diverse failure modes compared to isolated methods.

### Open Question 2
- Question: What architectural optimizations are required to reduce the computational overhead of internal representation monitoring for real-time safety-critical applications?
- Basis in paper: [inferred] The paper notes that techniques inspecting internal network activations "often require high computational resources," while the conclusion explicitly calls for future work on "efficiency" to ensure "real-time detection."
- Why unresolved: Analyzing high-dimensional activation patterns in deep layers is resource-intensive, creating a bottleneck for deployment in latency-sensitive systems like autonomous driving.
- What evidence would resolve it: Algorithms that reduce the latency or memory footprint of internal monitors (e.g., using sparse sampling or box abstractions) without significant loss in detection accuracy.

### Open Question 3
- Question: How can input monitoring techniques be enhanced to detect subtle anomalies, such as rare environmental conditions, rather than only obvious distributional shifts?
- Basis in paper: [inferred] The discussion notes that while input methods detect "obvious anomalies," they "struggle with subtler issues, such as rare environmental conditions."
- Why unresolved: Current input-based signal processing or reconstruction methods may lack the sensitivity to distinguish between benign variations (e.g., slight fog) and hazardous edge cases without triggering false positives.
- What evidence would resolve it: Novel input-monitoring models demonstrating high true-positive rates for low-magnitude distributional shifts (e.g., subtle weather changes) on standard autonomous driving benchmarks.

## Limitations
- Performance comparisons are challenging due to inconsistent evaluation protocols and metrics across studies
- Computational overhead of internal representation monitoring is frequently mentioned but rarely quantified with standardized benchmarks
- The survey lacks detailed guidance on optimal integration strategies and meta-learning for fused detection signals

## Confidence
- **High Confidence**: The classification framework itself (input/internal/output monitoring categories) and the general mapping of each approach to safety concerns (Table II). These claims are directly supported by the survey text and methodology descriptions.
- **Medium Confidence**: Performance claims for specific methods on benchmark datasets. While individual studies report results, the survey does not perform unified benchmarking or account for implementation variations.
- **Low Confidence**: Claims about computational efficiency and real-time deployment feasibility. The survey identifies these as important considerations but provides limited quantitative evidence.

## Next Checks
1. **Threshold Calibration Validation**: Implement a unified threshold selection protocol across all three monitoring categories (input, internal, output) using the same validation dataset and evaluation metrics to enable fair comparison.

2. **Real-time Overhead Benchmarking**: Quantify the computational cost (latency, memory) of each monitoring approach on representative embedded hardware used in autonomous driving systems, comparing against strict real-time constraints.

3. **Integration Strategy Evaluation**: Design and test specific meta-learning frameworks for combining multiple monitors, evaluating both detection performance improvements and potential negative interactions between different monitoring signals.