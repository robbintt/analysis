---
ver: rpa2
title: Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents
arxiv_id: '2505.12632'
source_url: https://arxiv.org/abs/2505.12632
tags:
- mobile
- touch
- action
- dataset
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MONDAY addresses the challenge of acquiring diverse, real-world
  mobile OS navigation data by automatically extracting task procedures from YouTube
  videos. The framework uses OCR-based scene detection to identify interface changes,
  combines UI element detection with a three-step action identification process, and
  achieves robust cross-platform generalization without manual annotation.
---

# Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents

## Quick Facts
- arXiv ID: 2505.12632
- Source URL: https://arxiv.org/abs/2505.12632
- Reference count: 40
- Primary result: MONDAY automatically extracts mobile OS navigation procedures from YouTube videos, achieving 18.11% average performance gain on unseen mobile OS platforms

## Executive Summary
MONDAY addresses the challenge of acquiring diverse, real-world mobile OS navigation data by automatically extracting task procedures from YouTube videos. The framework uses OCR-based scene detection to identify interface changes, combines UI element detection with a three-step action identification process, and achieves robust cross-platform generalization without manual annotation. Models trained on MONDAY show an average performance gain of 18.11% on unseen mobile OS platforms compared to models trained on single-OS datasets, demonstrating superior adaptation to real-world interface diversity.

## Method Summary
MONDAY processes YouTube instructional videos through a pipeline that detects phone screens, extracts text via OCR, identifies scene transitions using Levenshtein distance on OCR text, detects UI elements, and identifies actions through a three-step VLM process with Set-of-Marks prompting. The framework achieves 95.04% F1-score for scene detection, 99.87% UI element hit ratio, and 80.90% action identification accuracy. Models are fine-tuned using LoRA and demonstrate strong cross-platform generalization to unseen mobile OS platforms.

## Key Results
- OCR-based scene detection achieves 95.04% F1-score vs. 82.27% for vision-based SceneDetect
- 3-step action identification with narration achieves 80.90% full accuracy and 91.84% touch accuracy
- MONDAY-induced models show 18.11% average performance gain on unseen mobile OS platforms
- Cost analysis shows $0.34/video vs. $5.76/video for manual annotation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OCR-based scene detection outperforms vision-based methods for mobile UI transitions.
- Mechanism: Text content at identical screen locations is tracked across frames using Levenshtein distance; transitions are marked when >20% of text changes. This exploits the relative consistency of text rendering across OS versions, themes, and recording conditions.
- Core assumption: Text elements reliably indicate meaningful state changes; visual luminance differences are noisy signals in diverse UI configurations.
- Evidence anchors:
  - [Section 4.1.2]: OCR-based achieves 95.04% F1-score vs. SceneDetect (82.27%) and YUV-diff (70.86%).
  - [Section 3.2]: "Text rendering remains relatively consistent across different OS versions and user settings."
  - [corpus]: Related work on GUI simulation confirms dynamic mobile environments remain challenging for vision-only approaches.
- Break condition: OCR fails on low-resolution videos, non-text UI elements, or heavily stylized fonts; threshold may over-segment fast animations.

### Mechanism 2
- Claim: Multi-step action identification with temporal context and SoM representation improves localization accuracy.
- Mechanism: (1) Scene summary without markings; (2) Initial action identification using current frame ±2 adjacent frames, SoM-labeled UI elements, and video narration; (3) Refined localization via zoomed views around candidate elements. Narration disambiguates when multiple elements have similar effects.
- Core assumption: VLMs struggle with precise spatial localization but can reason effectively with labeled regions and temporal context.
- Evidence anchors:
  - [Section 4.1.4]: Multi-image 3-step achieves 80.90% full accuracy, 91.84% touch accuracy; 1-step drops to 74.67% touch.
  - [Section 4.1.4]: "No narrations" drops to 78.20% from 80.84%.
  - [corpus]: OS-Oracle and KG-RAG similarly leverage structured representations to improve GUI agent decision-making.
- Break condition: Narration missing or misaligned with action; adjacent frames insufficient for disambiguation; GPT-4o API costs become prohibitive at scale.

### Mechanism 3
- Claim: Cross-platform pre-training on diverse real-world interfaces enables better generalization to unseen mobile OS platforms.
- Mechanism: Models trained on MONDAY (iOS + Android, diverse configurations) learn platform-agnostic navigation patterns. When fine-tuned on single-platform datasets, these models transfer more effectively to novel platforms than models trained only on simulator data.
- Core assumption: Real-world diversity captures authentic interaction patterns that simulators miss; multi-platform exposure induces generalizable UI understanding.
- Evidence anchors:
  - [Section 4.2.2]: MONDAY-induced models achieve 18.11%p average gain on unseen Windows Mobile platform.
  - [Section 4.2.2]: SeeClick-MONDAY outperforms SeeClick on AitW, AMEX, and Windows Mobile across both fine-tuning scenarios.
  - [corpus]: Surfer 2 and related cross-platform agents similarly report that unified visual architectures trained on diverse environments outperform environment-specific baselines.
- Break condition: Target platform has fundamentally different interaction paradigms (e.g., gesture-only, voice-first) not represented in training data; domain shift too large.

## Foundational Learning

- Concept: **Set-of-Marks (SoM) Prompting**
  - Why needed here: VLMs like GPT-4o have poor spatial localization; SoM overlays numbered labels on detected UI elements, enabling the model to select actions by ID rather than coordinates.
  - Quick check question: Can you explain why SoM helps a VLM select "box 15" instead of predicting raw (x, y) coordinates?

- Concept: **OCR-based Change Detection**
  - Why needed here: Traditional vision-based scene detection fails across diverse UI themes; OCR provides a modality-invariant signal for state transitions.
  - Quick check question: Why might Levenshtein distance on OCR text outperform pixel-level luminance differences for detecting mobile UI transitions?

- Concept: **Transfer Learning with LoRA Fine-tuning**
  - Why needed here: The paper uses LoRA to create MONDAY-induced variants of base models efficiently, then further fine-tunes on downstream datasets.
  - Quick check question: What are the trade-offs of using LoRA vs. full fine-tuning when adapting a VLM to a new GUI domain?

## Architecture Onboarding

- Component map:
  Video Input → Phone Screen Detection (GroundingDINO, 2 FPS)
              → OCR Extraction (PaddleOCR, 4 FPS)
              → Scene Transition Detection (Levenshtein >20%)
              → UI Element Detection (GroundingDINO + heuristics)
              → 3-Step Action Identification (GPT-4o + SoM + narration)
              → Annotated Dataset Output

- Critical path:
  1. **Scene transition detection** — errors here propagate to missing or spurious action annotations.
  2. **UI element detection** — Hit Ratio of 99.87% sets the upper bound for action identification.
  3. **3-step action refinement** — the zoomed refinement step provides the +1.87%p touch accuracy gain.

- Design tradeoffs:
  - **Cost vs. scale**: GPT-4o queries cost ~$0.34/video; manual annotation costs ~$5.76/video. Open-source replacements would reduce cost but may lower accuracy.
  - **Threshold sensitivity**: 20% Levenshtein threshold empirically set; may need tuning for new domains.
  - **FPS choices**: 2 FPS for phone detection (slow position changes), 4 FPS for OCR (faster content changes) — higher FPS increases compute without proportional gain.

- Failure signatures:
  - **Missed transitions**: Vision-based baselines miss subtle UI changes in dark mode; OCR misses non-text transitions (icon-only state changes).
  - **Action ambiguity**: Multiple valid UI elements with similar effects; requires narration to disambiguate.
  - **Hand occlusion**: MediaPipe filters out videos with visible hands, reducing dataset size but ensuring clean screens.

- First 3 experiments:
  1. **Reproduce scene detection ablation**: Compare OCR-based vs. YUV-diff vs. SceneDetect on the 100-video evaluation set; verify 95.04% F1-score.
  2. **Ablate action identification steps**: Test 1-step vs. 2-step vs. 3-step with and without narration; confirm refinement step contribution.
  3. **Cross-platform transfer test**: Fine-tune a base VLM on MONDAY, then evaluate on a held-out platform (e.g., iPadOS or a future Android version); measure generalization gap vs. single-OS baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can open-source Vision-Language Models (VLMs) replace GPT-4o in the 3-step action identification pipeline while maintaining comparable accuracy and lowering costs?
- Basis in paper: [explicit] The conclusion states the current implementation relies on GPT-4o, though the modular design allows for integration of specialized models as they become available.
- Why unresolved: The authors evaluate the pipeline using GPT-4o but do not test the framework's robustness or accuracy when substituting smaller, open-source models for action identification.
- What evidence would resolve it: A comparative analysis of action identification accuracy (currently 80.90%) when GPT-4o is swapped for open-source VLMs like LLaVA or Qwen-VL in the pipeline.

### Open Question 2
- Question: Does training on datasets derived from "common" instructional videos limit an agent's effectiveness on niche applications or highly personalized interface configurations?
- Basis in paper: [inferred] Section 3.1 notes the dataset is built from web discussions to capture tasks users "commonly seek guidance for," implying a potential bias toward popular apps and standard settings.
- Why unresolved: While the dataset covers diverse real-world tasks, it may underrepresent long-tail apps or unique user customizations that are rarely featured in YouTube tutorials.
- What evidence would resolve it: Benchmarking agent performance specifically on niche applications or non-standard accessibility configurations versus performance on the "common" tasks used for training.

### Open Question 3
- Question: Can the distinct scene summarization and action refinement stages be consolidated into a single end-to-end model without the substantial performance drop observed in ablations?
- Basis in paper: [explicit] Section 4.1.4 reports a significant accuracy decrease to 74.67% for the 1-step method, concluding that "multi-stage reasoning" is necessary for complex mobile OS tasks.
- Why unresolved: It remains unclear if the performance drop is inherent to the task complexity or a limitation of the specific single-step prompting strategy tested.
- What evidence would resolve it: Research into advanced single-step architectures or prompting techniques that can match the 91.84% touch accuracy of the multi-step approach.

## Limitations

- **Scalability bottlenecks**: The framework relies on GPT-4o for action identification, incurring substantial API costs ($0.34/video) and processing delays. Manual annotation at $5.76/video suggests cost advantages, but economic viability at enterprise scale remains unproven.

- **Platform bias**: While the paper claims cross-platform generalization, MONDAY includes only iOS and Android data. The 18.11% gain on "unseen Windows Mobile" is promising but based on a single platform with limited sample size.

- **OCR dependency constraints**: The 95.04% F1-score for scene detection assumes legible text in all UI states. The framework may fail on icon-only transitions, heavily stylized fonts, or low-resolution videos where OCR confidence drops.

## Confidence

- **High confidence**: Cross-platform generalization mechanism and OCR-based scene detection performance. The 18.11% gain on unseen platforms is directly measured, and OCR-based F1-score (95.04%) is empirically validated against vision-based baselines.

- **Medium confidence**: Multi-step action identification refinement and SoM prompting effectiveness. While ablation studies show 3-step refinement improves touch accuracy by 1.87%, the contribution of individual components (narration, temporal context) is partially confounded.

- **Low confidence**: Cost-effectiveness at scale and robustness to diverse video qualities. The economic analysis assumes idealized conditions, and performance on low-quality, non-English, or heavily stylized videos is not tested.

## Next Checks

1. **Cross-platform stress test**: Evaluate MONDAY-induced models on at least three additional mobile OS platforms (e.g., iPadOS, KaiOS, Tizen) with minimum 50 videos each. Measure platform-specific adaptation gaps and identify failure modes in novel interaction paradigms.

2. **Cost-performance scaling analysis**: Replace GPT-4o with open-source VLMs (e.g., LLaVA-Next, InternVL) and measure accuracy degradation versus cost reduction. Benchmark on 1,000 videos to establish economic viability thresholds for production deployment.

3. **OCR robustness validation**: Test scene detection on videos with controlled degradation: low resolution (480p), stylized fonts, icon-only UIs, and rapid animations. Measure F1-score variance and establish failure rate confidence intervals across video quality dimensions.