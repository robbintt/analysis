---
ver: rpa2
title: Sparse Autoencoders Make Audio Foundation Models more Explainable
arxiv_id: '2509.24793'
source_url: https://arxiv.org/abs/2509.24793
tags:
- layer
- sparsity
- audio
- representations
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper applies sparse autoencoders (SAEs) to analyze representations\
  \ from four audio foundation models\u2014AST, WavLM, HuBERT, and MERT\u2014in the\
  \ context of singing technique classification. SAEs with sparsity constraints project\
  \ dense audio representations into sparse codes, aiming to reveal interpretable\
  \ acoustic factors."
---

# Sparse Autoencoders Make Audio Foundation Models more Explainable

## Quick Facts
- arXiv ID: 2509.24793
- Source URL: https://arxiv.org/abs/2509.24793
- Reference count: 0
- SAEs applied to audio foundation models reveal interpretable acoustic factors for singing technique classification

## Executive Summary
This paper demonstrates that sparse autoencoders (SAEs) can significantly improve the interpretability of audio foundation models by projecting dense representations into sparse codes that reveal underlying acoustic factors. The authors apply SAEs to four audio models (AST, WavLM, HuBERT, MERT) for singing technique classification, showing that SAEs improve completeness (fewer dimensions needed to predict factors) while maintaining informativeness (predictive accuracy). The results reveal hierarchical encoding patterns where pitch information emerges in early layers while formant and phoneme-related information concentrates in deeper layers.

## Method Summary
The authors first identify informative layers via linear probing on VocalSet singing technique classification. They then train TopK SAEs (N=2048, D=768) at sparsity levels from 75% to 99% on these selected layers. Reconstruction quality and classification accuracy are measured, along with Lasso regression to predict acoustic factors from eGeMAPS. Completeness and informativeness metrics are used to evaluate the disentanglement quality of SAE representations compared to original model representations.

## Key Results
- SAEs maintain robust classification accuracy until very high sparsity levels (>95%), indicating preserved task-relevant information
- Completeness significantly improves with sparsity while informativeness remains stable, showing concentrated feature encoding
- Pitch-related information emerges in early layers while formant and phoneme-related information concentrates in deeper layers
- SAEs provide a promising approach for improving the interpretability of audio foundation models

## Why This Works (Mechanism)

### Mechanism 1
TopK sparsity constraints force dense representations into interpretable sparse codes where fewer active dimensions encode task-relevant information. The TopK operator retains only the k largest activations from ReLU-activated encoder output, creating a hard sparsity constraint that distributes information across specialized dimensions rather than spreading it densely. The core assumption is that interpretable features correspond to sparsely-activating dimensions in an overcomplete basis (N=2048 > D=768).

### Mechanism 2
Completeness increases with sparsity while informativeness remains stable, indicating concentrated rather than dispersed feature encoding. Lasso regression from SAE dimensions to acoustic factors reveals that high sparsity forces predictive information into fewer units, with the L1 penalty identifying which sparse dimensions are most predictive. The core assumption is that acoustic factors from eGeMAPS (pitch, formants, MFCCs) are valid ground-truth descriptors for singing voice characteristics.

### Mechanism 3
Audio foundation models encode acoustic features hierarchically—pitch in early layers, formants/phonemes in deeper layers—and SAEs preserve this structure while making it more accessible. Linear probing across layers identifies where task information peaks (AST: layers 6-12, WavLM/HuBERT: early layers, MERT: layers 4-7), and SAEs trained on these layers reveal which acoustic factors are best predicted at each depth. The core assumption is that layer-wise probing accuracy reflects the linear accessibility of task-relevant features at each processing stage.

## Foundational Learning

- **Sparse Autoencoders (SAEs)**: The core intervention that creates sparse codes from dense representations; essential for interpreting disentanglement results. Quick check: Given 768-dim input and 2048-dim hidden layer with 95% sparsity, how many dimensions are active?

- **Linear Probing**: Determines which layers contain task-relevant information by training simple classifiers; critical for layer selection in SAE analysis. Quick check: If linear probe on layer 3 achieves 73% accuracy and layer 12 achieves 60%, which layer should you prioritize for SAE analysis?

- **Disentanglement Metrics (Informativeness vs. Completeness)**: The paper's key claim is that SAEs improve completeness without sacrificing informativeness; understanding these metrics is essential to interpret results. Quick check: If a factor has R²=0.95 but completeness=0.3, what does this tell you about how it is encoded?

## Architecture Onboarding

- **Component map**: Audio Input → Foundation Model (AST/WavLM/HuBERT/MERT) → Layer Representations (D=768, 13 layers) → Temporal Mean Pooling → TopK SAE Encoder (N=2048, sparsity 75-99%) → Sparse Code z → Linear Decoder (reconstruction) OR Linear Probe (classification) OR Lasso Regression (factor prediction)

- **Critical path**: 1) Run linear probing on all 13 layers to identify task-informative layers. 2) Train TopK SAEs at multiple sparsity levels on selected layers. 3) Extract acoustic factors using OpenSMILE eGeMAPS. 4) Fit Lasso regressions from sparse codes to each factor. 5) Compare SAE vs. original representations on both metrics.

- **Design tradeoffs**: Higher sparsity improves completeness but increases reconstruction MSE; task accuracy drops sharply beyond 95%. Layer selection varies by model: early layers capture pitch, deeper layers capture formants. Expansion factor (N/D = 2048/768 ≈ 2.67) affects interpretability vs. compute.

- **Failure signatures**: Reconstruction MSE spikes (>10× baseline) before 90% sparsity indicates insufficient encoder/decoder capacity. Probe accuracy drops >10% from original representations means task-relevant information was suppressed. Completeness does not improve with sparsity suggests factors are inherently high-entropy.

- **First 3 experiments**: 1) Layer sweep probe: Train linear classifiers on each layer's pooled representations to identify the 2 layers with highest accuracy. 2) Sparsity calibration: Train SAEs at 85%, 90%, 95% sparsity on one selected layer; plot reconstruction MSE vs. accuracy. 3) Factor discovery: Extract 10-20 interpretable acoustic features; fit Lasso from SAE codes to each feature and report which factors achieve R² > 0.7 and completeness > 0.8.

## Open Questions the Paper Calls Out
The paper explicitly states plans to integrate the analysis of temporal structure of representations, which was discarded in this work via average pooling. This suggests future work will analyze how acoustic factors evolve over time rather than treating representations as static.

## Limitations
- Completeness metric implementation lacks explicit formula definition, making exact reproducibility uncertain
- VocalSet train/validation/test splits are underspecified regarding speaker stratification
- Claims about interpretability rest on the assumption that eGeMAPS acoustic factors represent "ground truth" meaningful features

## Confidence
- Low confidence: Metric implementation lacks explicit formula definition
- Medium confidence: Interpretability claims depend on eGeMAPS being valid ground truth
- High confidence: Hierarchical encoding pattern (pitch early, formants deep) is well-supported by layer-wise probing results

## Next Checks
1. Request and implement the exact completeness calculation formula used in the paper
2. Re-run experiments using both random and speaker-stratified 80/20 splits to verify robustness
3. Apply the same SAE framework to a non-musical audio classification task (e.g., environmental sound classification) to test generalization beyond singing domain