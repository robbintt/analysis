---
ver: rpa2
title: 'AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World
  Contexts'
arxiv_id: '2601.11044'
source_url: https://arxiv.org/abs/2601.11044
tags:
- tasks
- agent
- evaluation
- arxiv
- agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AgencyBench, a comprehensive benchmark designed
  to evaluate autonomous agents on long-horizon, real-world tasks. The benchmark addresses
  limitations of existing benchmarks by evaluating six core agentic capabilities across
  32 scenarios and 138 tasks, requiring an average of 90 tool calls, 1 million tokens,
  and hours of execution time.
---

# AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts

## Quick Facts
- **arXiv ID:** 2601.11044
- **Source URL:** https://arxiv.org/abs/2601.11044
- **Reference count:** 40
- **Key outcome:** AgencyBench evaluates autonomous agents on 138 long-horizon tasks requiring 1M tokens and 90 tool calls on average, showing closed-source models outperform open-source by 16.3 percentage points (48.4% vs 32.1% average score).

## Executive Summary
AgencyBench is a comprehensive benchmark designed to evaluate autonomous agents on complex, long-horizon real-world tasks. It addresses limitations of existing benchmarks by testing six core agentic capabilities across 32 scenarios with an average of 1 million tokens and 90 tool calls per task. The benchmark introduces a user simulation agent for scalable automated evaluation and a Docker sandbox for visual and functional assessment. Experiments reveal significant performance differences between closed-source and open-source models, with proprietary models achieving 48.4% average score versus 32.1% for open-source models, highlighting the importance of scaffold optimization and the need for improved efficiency in long-horizon autonomy.

## Method Summary
The benchmark evaluates agents through sequential task hierarchies where prior results influence subsequent ones, requiring state persistence across ~90 tool calls and ~1M tokens. Agents operate within isolated workspaces using tool scaffolds (shell execution, file operations, web search, memory management). A user simulation agent (Claude-4-Sonnet, temperature 0.0) provides iterative feedback when rubric scores fall below threshold. Deliverables sync to a Docker sandbox for functional testing and visual artifact generation, then eval-space scores via rule-based scripts or LLM-as-judge (Claude-4-Sonnet for text, Gemini-2.5-pro for vision) based on rubrics.

## Key Results
- Closed-source models significantly outperform open-source models (48.4% vs 32.1% average score)
- Proprietary models show notable differences in resource efficiency, feedback-driven self-correction, and tool-use preferences
- Scaffold impact is substantial: Claude-4.5-Opus gains +20.5% on Claude-Agent-SDK but drops on others
- Long-horizon context retention remains challenging, with token consumption averaging 3.4M for top performers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: User simulation agents can substitute for human-in-the-loop feedback at scale, enabling automated rollout collection for long-horizon tasks.
- Mechanism: A dedicated agent (Claude-4-Sonnet, temperature 0.0) receives evaluation results and rubrics, then generates structured feedback when deliverables fall below threshold. This allows iterative refinement without human intervention.
- Core assumption: The simulation agent's feedback sufficiently approximates human expert critiques to guide productive revisions.
- Evidence anchors:
  - [abstract]: "To enable automated evaluation, we employ a user simulation agent to provide iterative feedback"
  - [section 3.2]: Human verification study achieved 4.69/5.0 alignment score between agent and expert feedback
  - [corpus]: Related benchmarks (ABC-Bench, APTBench) note similar scalability bottlenecks but don't implement this simulation approach
- Break condition: If feedback quality degrades for highly subjective tasks or novel domains outside the simulation agent's training distribution.

### Mechanism 2
- Claim: Proprietary models achieve superior performance within their native scaffolds due to implicit co-optimization during training.
- Mechanism: Models trained alongside specific tool definitions and prompt structures develop aligned representations for those interaction patterns. Claude-4.5-Opus gained +20.5% on Claude-Agent-SDK; GPT-5.2 preferred OpenAI-Agents-SDK (+1.3%).
- Core assumption: The scaffold's tool definitions, error handling conventions, and prompt formatting were exposed during or align with the model's training objective.
- Evidence anchors:
  - [section 4.7]: "Claude-4.5-Opus achieves a substantial performance boost of 20.5% when operating within the Claude-Code SDK"
  - [section 4.7]: Open-source models show heterogeneous scaffold sensitivity, suggesting some may have been optimized for specific SDKs
  - [corpus]: AGENTIF notes instruction-following complexity in agentic scenarios but doesn't examine scaffold-model coupling
- Break condition: If models are deployed with significantly reformulated tool schemas or prompt templates that violate training assumptions.

### Mechanism 3
- Claim: Sequential task hierarchies with state persistence evaluate long-horizon context retention and error propagation.
- Mechanism: Scenarios comprise 1-5 sequential tasks where prior results influence subsequent ones. Agents must maintain context across ~90 tool calls and ~1M tokens while adapting to accumulated state changes.
- Core assumption: Errors compound across tasks, and recovery requires maintaining coherent internal state representations over extended rollouts.
- Evidence anchors:
  - [section 3.1.1]: "completion results of preceding tasks influence subsequent ones"
  - [figure 1]: Comparison showing AgencyBench averages 1M tokens and 90 turns vs. next-highest at 200K/60 turns
  - [corpus]: Weak corpus signal—related work focuses on single-turn or short-horizon evaluation
- Break condition: If tasks are designed to be fully independent, eliminating the need for cross-task memory.

## Foundational Learning

- Concept: **Agent scaffold architecture**
  - Why needed here: Performance depends critically on model-scaffold pairing; understanding tool suites, prompt structures, and execution environments explains behavioral differences.
  - Quick check question: Can you identify which tools (shell execution, file operations, memory management) a model prefers and why that might reflect its training?

- Concept: **Rubric-based automated evaluation**
  - Why needed here: The benchmark replaces human judgment with executable scripts and LLM-as-judge for 0-10 scoring across visual and functional criteria.
  - Quick check question: For a task requiring UI correctness, would you use rule-based evaluation or multimodal LLM-as-judge, and what are the failure modes of each?

- Concept: **Multi-turn rollout formulation**
  - Why needed here: Rollouts are formalized as state-action sequences τ = (q, a, t, ..., a, u, ...) where user feedback u interrupts when rubrics fail.
  - Quick check question: Given Pass@1 = 28.1% and Pass@2 = 53.1% for GPT-5.2, what does the 88.9% "Rise" metric indicate about self-correction capability?

## Architecture Onboarding

- Component map:
  Workspace -> User Simulation Agent -> Docker Remote Sandbox -> Eval-space

- Critical path: Query → Agent generates deliverables in workspace → User sim provides feedback (if score < 60%) → Deliverables sync to Docker sandbox → Visual artifacts generated → Eval-space scores via rubrics

- Design tradeoffs:
  - Rule-based evaluation offers objectivity but can't assess aesthetics; LLM-as-judge handles subjectivity but introduces variance (mitigated with temperature=0.0)
  - Isolated workspaces prevent state interference but increase infrastructure complexity
  - Sequential task design tests context retention but creates dependency chains where early failures cascade

- Failure signatures:
  - Low Attempt Efficiency (high Att) suggests weak self-correction from feedback
  - High token consumption with low scores (e.g., Claude-4.5-Sonnet: 4.1M tokens, 11.4% token efficiency) indicates verbose unproductive reasoning
  - Scaffold mismatch: >10% performance drop when using non-native SDK

- First 3 experiments:
  1. Replicate the scaffold comparison (Table 4) on your target model using Claude-Agent-SDK, OpenAI-Agents-SDK, and your own scaffold across 10 scenarios to identify optimal pairing.
  2. Run ablation on feedback rounds: compare Pass@1 vs Pass@2 to quantify self-correction capability before deploying user simulation agent at scale.
  3. Profile tool invocation patterns (following Table 6 methodology) to diagnose whether your model over-relies on specific tools (e.g., excessive web search vs. shell execution) and calibrate scaffold accordingly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can model architectures be co-optimized with agentic scaffolds to achieve scaffold-agnostic performance?
- Basis in paper: [explicit] The conclusion states: "highlighting the necessity of co-optimizing model architecture with agentic frameworks." The scaffold experiments (Table 4) show Claude-4.5-Opus gains +20.5% on Claude-Agent-SDK but drops on others.
- Why unresolved: The paper documents the "home-field advantage" phenomenon but does not propose mechanisms or training methods to achieve cross-scaffold robustness.
- What evidence would resolve it: Training experiments where models are explicitly optimized across multiple scaffolds simultaneously, with resulting performance parity across frameworks.

### Open Question 2
- Question: How reliable is the user simulation agent as a substitute for human feedback across diverse task domains?
- Basis in paper: [inferred] The user simulation agent achieved 4.69/5 alignment with humans on 50 sampled rollouts, but the paper acknowledges this is limited validation. The reliance on a single model (Claude-4-Sonnet) raises questions about generalizability.
- Why unresolved: Small sample size (50 rollouts), single-domain experts, and no analysis of failure modes where simulation diverges from genuine human feedback.
- What evidence would resolve it: Large-scale comparison of agent performance trajectories under human vs. simulated feedback across all 138 tasks, with error analysis on divergence cases.

### Open Question 3
- Question: What specific capability gaps explain the 16.3 percentage-point performance difference between closed-source (48.4%) and open-source (32.1%) models?
- Basis in paper: [inferred] Table 1 shows consistent gaps, but the paper does not isolate whether the gap stems from reasoning, context retention, tool-use precision, or self-correction mechanisms.
- Why unresolved: The analysis describes behavioral differences (tool preferences, efficiency) but does not causally link them to the performance gap.
- What evidence would resolve it: Ablation studies controlling for each capability dimension, or fine-tuning open-source models on specific weak capabilities identified through failure analysis.

### Open Question 4
- Question: Can agents achieve efficient long-horizon autonomy without the "brute-force" token consumption pattern observed in top performers like GPT-5.2 (3.4M tokens)?
- Basis in paper: [explicit] The conclusion states: "the most advanced models struggle to fully master long-horizon autonomy without substantial resource consumption, highlighting the need for improved efficiency."
- Why unresolved: The paper shows Grok-4.1-Fast achieves higher token efficiency (37.2%) but lower absolute performance (44.3%), suggesting a performance-efficiency tradeoff that remains uncharacterized.
- What evidence would resolve it: Architectural innovations or planning algorithms tested on AgencyBench that achieve ≥50% average score while consuming <1.5M tokens per scenario.

## Limitations
- The benchmark's reliance on LLM-as-judge evaluation introduces potential systematic biases, particularly for subjective criteria like aesthetic quality and user experience.
- The performance gap between proprietary and open-source models may partially reflect scaffold optimization rather than inherent capability differences.
- The Docker sandbox environment may not fully capture real-world execution variability across diverse hardware and OS configurations.

## Confidence
- **High confidence**: Benchmark design methodology, token and turn statistics, relative performance rankings within model families
- **Medium confidence**: Cross-model absolute performance comparisons, scaffold impact magnitude, user simulation agent effectiveness
- **Low confidence**: Extrapolation of results to completely novel task domains, long-term reliability of automated evaluation, generality of scaffold-specific optimizations

## Next Checks
1. Conduct a blinded human evaluation on 20 randomly selected task outputs comparing proprietary vs open-source models to quantify systematic biases in the automated scoring pipeline
2. Test the same models across three distinct scaffold architectures (Claude-Agent-SDK, OpenAI-Agents-SDK, LangGraph) with randomized tool definitions to isolate scaffold optimization effects from model capability
3. Deploy the benchmark with models configured at different context window sizes (128K, 512K, 1M+ tokens) to measure the impact of context capacity on long-horizon task completion rates and error propagation patterns