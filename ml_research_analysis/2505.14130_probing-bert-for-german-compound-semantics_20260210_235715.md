---
ver: rpa2
title: Probing BERT for German Compound Semantics
arxiv_id: '2505.14130'
source_url: https://arxiv.org/abs/2505.14130
tags:
- head
- german
- layers
- compositionality
- compounds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether pretrained German BERT captures
  knowledge of noun compound semantics by predicting compositionality ratings for
  868 German noun-noun compounds. Using a comprehensive experimental setup varying
  target embeddings, layers, and cased vs.
---

# Probing BERT for German Compound Semantics

## Quick Facts
- arXiv ID: 2505.14130
- Source URL: https://arxiv.org/abs/2505.14130
- Reference count: 6
- Primary result: Compositionality information in German BERT is most recoverable from early transformer layers (1–4), not later layers

## Executive Summary
This study investigates whether pretrained German BERT captures knowledge of noun compound semantics by predicting compositionality ratings for 868 German noun-noun compounds. Using a comprehensive experimental setup varying target embeddings, layers, and cased vs. uncased models, we find that compositionality information is most recoverable in early transformer layers, consistent with prior English studies. However, BERT's performance on German lags behind English despite a comparable setup, suggesting inherent difficulty due to German's higher compounding productivity and increased constituent ambiguity. The cased model performs better for head predictions in early layers, while the uncased model excels for modifier predictions in mid-to-late layers.

## Method Summary
The study uses DBMDZ bert-base-german-cased and bert-base-german-uncased models (12 layers, 768 dimensions) without fine-tuning. Target embeddings include modifier token, head token, their average (compound), sentence context average, and [CLS] token. All 13 layers (input + 12 hidden) and 91 contiguous layer span combinations are tested. Compositionality is estimated via cosine similarity between embedding pairs, with Spearman's rank correlation (ρ) computed between predicted scores and gold-standard ratings from the GhoSt-NN dataset.

## Key Results
- Compositionality information is most recoverable from early transformer layers (1–4), not later layers
- German BERT performance lags behind English despite comparable setup, suggesting inherent difficulty due to German's higher compounding productivity
- Cased models benefit head predictions in early layers, while uncased models excel for modifier predictions in mid-to-late layers

## Why This Works (Mechanism)

### Mechanism 1: Early-Layer Semantic Recovery
- Claim: Compositionality information in German BERT is most recoverable from early transformer layers (1–4), not later layers.
- Mechanism: Early layers retain lexical-semantic information before higher layers over-contextualize representations for downstream prediction objectives.
- Core assumption: Early layers encode compositional semantics more faithfully because they have undergone less task-driven abstraction.
- Evidence anchors: "compositionality information most easily recoverable in the early layers" [abstract]; "single best mean results is on layer 1 in isolation (ρ = 0.199)" for heads; layer 12 yields negative correlations for modifiers.

### Mechanism 2: Case Sensitivity Diverges by Constituent Role
- Claim: Cased models benefit head predictions (early layers); uncased models benefit modifier predictions (mid-to-late layers).
- Mechanism: German capitalization signals noun status—preserved in cased models—aiding head representation where morphosyntactic dominance matters.
- Core assumption: Head capitalization provides a reliable morphosyntactic cue that modifier capitalization does not.
- Evidence anchors: "cased model performs better for head predictions in early layers, while the uncased model excels for modifier predictions in mid-to-late layers" [abstract]; "head predictions benefit from the cased version (median ρ = 0.201 vs. 0.165)" [section 4.2].

### Mechanism 3: Constituent-Context Pairing Captures Compositionality
- Claim: Probing works best when comparing the target constituent embedding against the sentence context embedding—not against the compound or other constituents.
- Mechanism: Context embeddings aggregate usage patterns; similarity between a constituent and its context reflects how consistently that constituent contributes meaning across occurrences.
- Core assumption: Context averaging approximates distributional semantics of constituent behavior.
- Evidence anchors: "importance of constituent–context comparisons" [abstract]; Best modifier result: `mod` vs `cont` (0.332); best head result: `head` vs `cont` (0.433) [section 4.1, Table 4].

## Foundational Learning

- **Concept: Compositionality ratings (1–6 scale)**
  - Why needed here: The probing target predicts human judgments of how much each constituent contributes to compound meaning.
  - Quick check question: Can you explain why "Sünündenbock" (scapegoat) would receive low modifier and head transparency ratings?

- **Concept: Spearman's rank correlation (ρ)**
  - Why needed here: Performance is reported as rank correlation, not accuracy; this measures ordinal alignment between predicted cosine scores and gold ratings.
  - Quick check question: Why might a ρ of 0.43 be considered moderate even though it looks low numerically?

- **Concept: Layer-wise probing**
  - Why needed here: The study spans 91 layer-span combinations; knowing how to extract and average embeddings across layers is prerequisite to reproduction.
  - Quick check question: How would you extract and average embeddings from layers 3–5 for a token split into subwords?

## Architecture Onboarding

- **Component map:**
  - GhoSt-NN dataset (868 compounds) -> DECOW corpus (11.6B tokens) -> BERT models (cased/uncased) -> Layer-span embeddings -> Cosine similarity -> Spearman ρ

- **Critical path:**
  1. Preprocess: Deterministically split compounds into gold-standard modifier+head before tokenization
  2. Forward pass: Feed each sentence through frozen BERT; retain all-layer embeddings for target tokens
  3. Aggregate: Average subword embeddings if tokenizer splits constituents
  4. Probe: Compute cosine similarity between target pairs per layer span
  5. Correlate: Compute Spearman ρ between aggregated cosine scores and gold ratings

- **Design tradeoffs:**
  - **Cased vs uncased:** Cased preserves grammatical noun markers; uncased may generalize better for modifiers via deeper contextualization
  - **Layer span breadth:** Narrow early spans (1–2) favor heads; broader spans (3–5) favor modifiers in uncased
  - **Token vs sentence pooling:** This study uses token-level estimates averaged across 100 sentences

- **Failure signatures:**
  - Negative correlations (e.g., ρ = –0.234) indicate layer/embedding pairs that encode inverse or irrelevant signals
  - Large cased/uncased performance divergence on the same layer span suggests case-dependent feature sensitivity
  - Modifier-head prediction correlation (ρ = 0.334) indicates constituents are captured independently

- **First 3 experiments:**
  1. Replicate best configs: For heads, probe `head` vs `cont` on layer 1 with cased model; for modifiers, probe `mod` vs `cont` on layers 3–4 with uncased. Verify ρ matches reported values (0.433, 0.332).
  2. Ablate context: Compare `mod` vs `cont` against `mod` vs `comp` and `mod` vs `cls` to confirm context pairing superiority.
  3. Cross-constituent test: Probe heads using modifier-optimal settings (uncased, mid layers) and vice versa to quantify the performance penalty.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed layer-wise patterns for compound compositionality generalize to typologically distant languages with different compounding structures?
- Basis in paper: [explicit] Limitations section states the study is "limited to only one other language – German – which also belongs the Germanic family" and explicitly calls for "typologically more distant languages with stronger structural differences."
- Why unresolved: Only Germanic languages (English, German) have been studied; cross-lingual validity remains unknown.

### Open Question 2
- Question: How does BERT encode compositionality for multiword expression categories beyond noun compounds, such as particle verbs or idiomatic verb phrases?
- Basis in paper: [explicit] Limitations section notes: "We only consider noun compounds, but other categories of multiword expressions (e.g., particle verbs) may exhibit different processing patterns."
- Why unresolved: The current study restricted analysis to noun-noun compounds only.

### Open Question 3
- Question: To what extent does the German-English performance gap stem from higher compounding productivity versus increased constituent ambiguity in German?
- Basis in paper: [explicit] The authors hypothesize both factors but cannot disentangle them: "This may be due to the higher productivity of compounding in German than in English and the associated increase in constituent-level ambiguity."
- Why unresolved: Both properties co-vary in German; no controlled experiment isolates either factor.

## Limitations

- Dataset coverage and generalization: The GhoSt-NN dataset contains 868 compounds, which may not capture the full diversity of German compounding patterns.
- Corpus access and processing: The study relies on the DECOW corpus but doesn't specify exact version or sampling methodology for extracting 100 sentences per compound.
- Cross-lingual comparison validity: The German-English performance comparison assumes comparable experimental conditions that may not be directly equivalent.

## Confidence

**High Confidence:** The core finding that compositionality information is most recoverable from early transformer layers (1-4) is well-supported by consistent results across both cased and uncased models.

**Medium Confidence:** The case sensitivity divergence between head and modifier predictions has moderate support, with clear performance differences between cased and uncased models.

**Low Confidence:** The broader claim that German BERT's performance lags behind English despite comparable setup requires additional cross-validation.

## Next Checks

1. **Cross-validation of case sensitivity mechanism:** Test the cased/uncased performance split on a subset of compounds where constituent capitalization patterns are varied systematically. Verify whether the head vs. modifier performance split persists across different capitalization configurations.

2. **Temporal stability analysis:** Re-run the probing experiments on the same compound set but with different random seeds for corpus sentence sampling. Assess whether the layer-wise patterns and cased/uncased differences remain stable across multiple sampling runs.

3. **Extended semantic class analysis:** Segment the compound dataset by semantic transparency categories and repeat the probing analysis separately for each category. Determine whether the early-layer advantage holds uniformly across semantic types.