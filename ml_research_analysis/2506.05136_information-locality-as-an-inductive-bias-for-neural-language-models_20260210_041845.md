---
ver: rpa2
title: Information Locality as an Inductive Bias for Neural Language Models
arxiv_id: '2506.05136'
source_url: https://arxiv.org/abs/2506.05136
tags:
- entropy
- language
- local
- m-local
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether neural language models (LMs) share
  inductive biases with humans regarding local statistical structure in language.
  The authors introduce "m-local entropy" as an information-theoretic measure capturing
  how effectively the m-1 preceding symbols disambiguate the next symbol.
---

# Information Locality as an Inductive Bias for Neural Language Models

## Quick Facts
- arXiv ID: 2506.05136
- Source URL: https://arxiv.org/abs/2506.05136
- Reference count: 40
- This paper investigates whether neural language models (LMs) share inductive biases with humans regarding local statistical structure in language

## Executive Summary
This paper investigates whether neural language models share inductive biases with humans regarding local statistical structure in language. The authors introduce "m-local entropy" as an information-theoretic measure capturing how effectively the m-1 preceding symbols disambiguate the next symbol. They conduct experiments on perturbed natural language corpora and languages generated by probabilistic finite-state automata (PFSAs), training both LSTM and Transformer LMs. The results show a strong positive correlation between m-local entropy and LM performance: languages with higher local entropy are consistently harder for both architectures to learn.

## Method Summary
The authors introduce m-local entropy as a novel information-theoretic measure to quantify local statistical structure in language. They generate experimental corpora through controlled perturbations of natural language and through probabilistic finite-state automata. Both LSTM and Transformer architectures are trained on these datasets, with performance measured via perplexity. The correlation between m-local entropy and model performance is then analyzed across different language types and architectural configurations.

## Key Results
- Languages with higher m-local entropy are consistently harder for both LSTM and Transformer LMs to learn
- Strong positive correlation exists between m-local entropy and LM perplexity across all tested conditions
- Neural LMs show sensitivity to local statistical structure comparable to human language processing patterns

## Why This Works (Mechanism)
Neural language models appear to develop internal representations that efficiently exploit local statistical dependencies in language. The m-local entropy metric captures how much information the immediate context provides about the next symbol. When this local predictability is reduced (higher entropy), models must rely more heavily on long-range dependencies or broader contextual patterns, which current architectures handle less effectively. This suggests that neural LMs, like humans, have evolved or been trained to prioritize and efficiently utilize local statistical regularities as an inductive bias for language processing.

## Foundational Learning
- **Information Theory**: Understanding entropy as a measure of uncertainty/disorder; needed to grasp why m-local entropy quantifies predictability; quick check: calculate entropy for simple probability distributions
- **Probabilistic Finite-State Automata**: Formal model for generating structured sequences; needed to create controlled experimental languages; quick check: trace state transitions for sample sequences
- **Neural Language Model Architecture**: Understanding how LSTMs and Transformers process sequential data; needed to interpret why models struggle with high-entropy contexts; quick check: map input to output through one time step
- **Perplexity**: Standard evaluation metric for language models; needed to quantify model performance; quick check: compute perplexity for small vocabulary examples

## Architecture Onboarding

**Component Map**: Input Sequence -> Embedding Layer -> LSTM/Transformer Blocks -> Output Layer -> Probability Distribution

**Critical Path**: Tokenization → Embedding → Context Processing → Prediction → Loss Computation → Parameter Update

**Design Tradeoffs**: LSTMs maintain explicit recurrence for sequential processing but struggle with long-range dependencies; Transformers use self-attention to capture global context but require quadratic memory; both architectures show similar sensitivity to local entropy despite architectural differences

**Failure Signatures**: Performance degradation correlates with increased m-local entropy; models fail predictably on languages with reduced local predictability regardless of architecture

**First Experiments**:
1. Train LSTM and Transformer on controlled PFSA-generated languages with varying m-local entropy values
2. Measure perplexity correlation with m-local entropy across architectures
3. Compare performance on perturbed natural language corpora versus clean versions

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses primarily on m-local entropy as a single metric, potentially overlooking other relevant aspects of local information structure
- Experimental scope limited to LSTM and Transformer architectures, leaving open whether other neural architectures exhibit similar biases
- Perturbation methods and PFSA-generated languages may not fully capture the complexity and diversity of natural language's local statistical properties

## Confidence

- **High confidence**: The core finding that m-local entropy correlates positively with LM perplexity across both architectures and multiple language types is robust and well-supported by the experimental results
- **Medium confidence**: The interpretation that this correlation reveals a shared inductive bias between neural LMs and human language processing, while plausible, requires additional empirical support from cognitive science literature to strengthen the claim
- **Medium confidence**: The generalizability of findings to other neural architectures and more diverse natural language datasets is suggested but not empirically established

## Next Checks

1. Test additional neural architectures (e.g., GRUs, convolutional LMs, recurrent highway networks) on the same datasets to determine whether the local entropy sensitivity is universal across neural LM designs or specific to LSTMs and Transformers

2. Conduct controlled experiments varying multiple statistical properties simultaneously (local entropy, global entropy, vocabulary diversity) to disentangle their relative contributions to LM performance and identify potential interaction effects

3. Compare neural LM performance patterns with human behavioral data on the same or similar language stimuli to directly test whether the observed inductive bias for information locality mirrors human language processing constraints