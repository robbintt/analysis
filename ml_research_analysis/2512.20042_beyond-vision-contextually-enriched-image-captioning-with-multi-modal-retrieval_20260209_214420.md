---
ver: rpa2
title: 'Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieval'
arxiv_id: '2512.20042'
source_url: https://arxiv.org/abs/2512.20042
tags:
- arxiv
- image
- visual
- captions
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multimodal pipeline for generating context-enriched
  image captions by combining visual retrieval, semantic search, and a fine-tuned
  language model. The approach retrieves semantically similar images using BEIT-3
  and SigLIP2, re-ranks them with ORB and SIFT feature matching, extracts relevant
  textual context from news articles, and integrates it with visual descriptions via
  a QLoRA-fine-tuned DeepSeek-Qwen3 model.
---

# Beyond Vision: Contextually Enriched Image Captioning with Multi-Modal Retrieval

## Quick Facts
- arXiv ID: 2512.20042
- Source URL: https://arxiv.org/abs/2512.20042
- Reference count: 40
- Combines visual retrieval, semantic search, and fine-tuned LLM to generate event-aware captions

## Executive Summary
This paper introduces a multimodal pipeline for generating context-enriched image captions by combining visual retrieval, semantic search, and a fine-tuned language model. The approach retrieves semantically similar images using BEIT-3 and SigLIP2, re-ranks them with ORB and SIFT feature matching, extracts relevant textual context from news articles, and integrates it with visual descriptions via a QLoRA-fine-tuned DeepSeek-Qwen3 model. Evaluated on the OpenEvents v1 dataset, the system significantly outperforms prior methods, achieving a CLIPScore of 0.748 (+12.8% vs. best baseline) and a CIDEr score of 0.195 (10× higher than most baselines). These results demonstrate the effectiveness of incorporating external knowledge for richer, more informative event-aware captions.

## Method Summary
The system employs a multi-stage pipeline: first, an ensemble of BEIT-3 and SigLIP2 encoders retrieves semantically similar images, followed by geometric verification using ORB and SIFT feature matching to filter false positives. A sliding-window chunker segments news articles into 3-sentence windows, which are ranked by semantic similarity to the base caption using all-MiniLM-L12-v2. The top-5 chunks plus structural markers (first 3, last 2 sentences) form a condensed context window. This context is integrated with a base caption generated by InstructBLIP via a structured prompt (70% article info, 30% visual) and fed to a QLoRA-fine-tuned DeepSeek-Qwen3 model to produce the final enriched caption.

## Key Results
- CLIPScore of 0.748 (+12.8% improvement over best baseline)
- CIDEr score of 0.195 (10× higher than most baselines)
- Retrieval Recall@1 of 0.994
- Significant improvement over prior methods on OpenEvents v1 dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-encoder semantic retrieval followed by geometric verification reduces false positive matches compared to embedding-only approaches.
- Mechanism: BEIT-3 and SigLIP2 produce overlapping but distinct candidate pools via different pretraining objectives (masked image modeling vs. sigmoid-based alignment). ORB/SIFT feature matching with RANSAC then filters candidates where semantic similarity does not correspond to shared visual structure.
- Core assumption: Semantically similar images describing the same event share both embedding proximity and geometric keypoints.
- Evidence anchors:
  - [abstract]: "retrieves semantically similar images using BEIT-3... and SigLIP So-384, reranks them using ORB and SIFT for geometric alignment"
  - [section 3.2]: "Reranking occurs when confidence exceeds 0.4 and the minimum inliers are greater than 8"
  - [corpus]: Related work "Knowledge Completes the Vision" (FMR 0.549) confirms entity-aware retrieval improves news captioning, but does not validate the specific geometric cascade.
- Break condition: If query images lack distinctive keypoints (e.g., abstract graphics, heavily cropped faces), geometric verification may reject valid matches.

### Mechanism 2
- Claim: Semantic chunk selection with sliding-window segmentation provides stronger supervision signal than full-article context for LLM fine-tuning.
- Mechanism: all-MiniLM-L12-v2 ranks sentence chunks by cosine similarity to the base caption. Top-5 chunks plus structural markers (first 3, last 2 sentences) form a condensed context window, reducing noise during gradient updates.
- Core assumption: Caption-relevant information clusters in specific article regions rather than being uniformly distributed.
- Evidence anchors:
  - [section 3.4]: "training with CLM on focused, high-relevance input improves both lexical diversity and semantic density"
  - [section 3.4, Figure 2]: Loss curves show faster convergence with semantic chunking (rank 64) vs. full article (rank 512)
  - [corpus]: CONCAP (FMR 0.555) demonstrates retrieval-augmented conditioning improves multilingual captioning, supporting the general RAG premise but not the specific chunking strategy.
- Break condition: If relevant context is scattered across an article (e.g., background in middle paragraphs), sliding-window selection may miss critical facts.

### Mechanism 3
- Claim: Prompt-anchored QLoRA fine-tuning enables the LLM to prioritize news context over visual description while maintaining factual grounding.
- Mechanism: A structured prompt (70% article info, 30% visual; explicit WHO/WHAT/WHY/WHEN/WHERE instructions) is concatenated with retrieved chunks and base caption. QLoRA (rank 256, 8-bit) updates a small adapter while preserving base model capabilities.
- Core assumption: The prompt formulation during training generalizes to test-time queries with similar structure.
- Evidence anchors:
  - [abstract]: "fine-tuned Qwen3 model with QLoRA then integrates this context with base captions"
  - [section 3.4]: "Prompt Design... (3) Use 70% article information + 30% visual description"
  - [corpus]: "Knowledge Completes the Vision" achieves gains via entity-aware retrieval, suggesting context prioritization matters, but does not isolate prompt design effects.
- Break condition: If test prompts deviate significantly from training prompt template, or if retrieved context contains contradictory facts, output coherence may degrade.

## Foundational Learning

- **Multiway Transformer Architecture (BEIT-3)**:
  - Why needed here: Understanding how a single backbone processes images, text, and image-text pairs via modality-specific adapters clarifies why BEIT-3 can embed both modalities into a shared space.
  - Quick check question: How does a Multiway Transformer differ from using separate encoders for vision and language?

- **Sigmoid vs. Contrastive Loss for Image-Text Alignment**:
  - Why needed here: SigLIP2 uses sigmoid-based binary classification rather than CLIP's contrastive objective. This affects how semantic similarity scores should be interpreted during ensemble weighting.
  - Quick check question: Why might sigmoid loss handle imbalanced positive/negative pairs differently than contrastive loss?

- **QLoRA Parameter Efficiency**:
  - Why needed here: The paper uses rank-256 adapters with 8-bit quantization. Understanding low-rank adaptation helps diagnose whether fine-tuning capacity is sufficient for the target domain.
  - Quick check question: What determines the minimum adapter rank needed to capture domain-specific knowledge without overfitting?

## Architecture Onboarding

- **Component map**:
Input Image → [BEIT-3 + SigLIP2 Encoders] → Candidate Pool
         ↓                                        ↓
   InstructBLIP → Base Caption              [ORB + SIFT Reranker]
         ↓                                        ↓
         └──────→ [all-MiniLM Chunk Selector] ←──┘
                            ↓
                    [DeepSeek-Qwen3 + QLoRA]
                            ↓
                    Context-Enriched Caption

- **Critical path**: Image → retrieval candidates → geometric reranking → chunk selection → LLM fusion. If reranking confidence < 0.4 or inliers < 8, the system may fall back to embedding-only ranking (not explicitly specified in paper—requires code inspection).

- **Design tradeoffs**:
  - Ensemble retrieval increases robustness but adds latency (3 model forward passes + ORB/SIFT matching).
  - Semantic chunking reduces context noise but may omit distributed information.
  - QLoRA rank 256 balances capacity and memory; rank 512 showed slower convergence in ablation.

- **Failure signatures**:
  - Low Recall@1 with high Recall@10: Reranking too aggressive, filtering correct matches.
  - High CLIPScore but low CIDEr: Captions semantically aligned but phrasing differs from human references (observed in results: 0.748 vs. 0.195).
  - Hallucinated named entities: LLM generates facts not grounded in retrieved chunks (acknowledged in Discussion).

- **First 3 experiments**:
  1. **Ablate geometric reranking**: Disable ORB/SIFT stage, compare Recall@1 and CIDEr to quantify verification contribution.
  2. **Vary chunk count**: Test top-3 vs. top-7 chunks to find optimal context window size for the target dataset's article structure.
  3. **Prompt perturbation**: Remove specific prompt constraints (e.g., 70/30 ratio) at inference to measure sensitivity to training-time prompt engineering.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative vision-language models with stronger OCR and entity recognition capabilities (e.g., Qwen-VL 2.5) substantially reduce hallucinations and improve factual grounding in the final enriched captions?
- Basis in paper: [explicit] Authors note InstructBLIP's "limited ability to extract named entities or interpret image-embedded text can result in vague or generic outputs" and state they "intend to experiment with alternative models for generating the initial (base) captions—especially Qwen-VL 2.5 7B."
- Why unresolved: The current pipeline relies on InstructBLIP Vicuna-7B for base caption generation, which lacks robust entity extraction, creating a "visual bottleneck" for the downstream LLM.
- What evidence would resolve it: Ablation experiments comparing caption quality, hallucination rates, and entity accuracy when substituting different base caption generators while holding the enrichment model constant.

### Open Question 2
- Question: How can the relatively low CIDEr score (0.195) be improved while maintaining the high CLIPScore (0.748), given the apparent trade-off between semantic novelty and reference alignment?
- Basis in paper: [explicit] Authors state "the CIDEr score remains relatively low, indicating that the generated captions, while informative, may not yet fully align with human-like phrasing or closely match reference captions in terms of fluency and specificity."
- Why unresolved: The enrichment process prioritizes adding external context (70% article, 30% visual) which may diverge from reference caption styles in the OpenEvents dataset.
- What evidence would resolve it: Human evaluation studies comparing informativeness versus fluency, and experiments with style-transfer or alignment-focused fine-tuning objectives.

### Open Question 3
- Question: Would dynamic model selection among diverse LLMs (based on image type or domain) improve caption quality compared to a single fine-tuned model?
- Basis in paper: [explicit] Authors propose extending the architecture by "replacing the DeepSeek-Qwen3 model with a range of large language models (LLMs), each trained on diverse domains, data distributions, and alignment strategies" to enable "dynamic model selection for caption generation depending on the input image type, domain, or application context."
- Why unresolved: The current single-model approach cannot adapt to varying content types (e.g., political events vs. sports vs. natural disasters) that may benefit from different pretraining priors.
- What evidence would resolve it: Comparative experiments routing different image categories to specialized models and measuring domain-specific metric improvements.

### Open Question 4
- Question: Can post-processing techniques (retrieval-based re-ranking, fluency polishing, learned constraints) meaningfully improve CIDEr and METEOR scores without sacrificing the contextual richness gained from retrieval?
- Basis in paper: [explicit] Authors "intend to explore post-processing techniques to refine generated captions after decoding" including "re-ranking candidate outputs using retrieval-based scoring, fluency polishing via lightweight LLM-based editing, and applying rule-based or learned constraints."
- Why unresolved: Current outputs are generated in a single pass without refinement, potentially including verbose or stylistically inconsistent portions.
- What evidence would resolve it: Ablation studies measuring metric changes before and after each post-processing component, with human preference ratings.

## Limitations

- The absolute CIDEr score (0.195) remains substantially below human-level performance, suggesting limitations in caption informativeness despite contextual enrichment
- The system's behavior with contradictory or incomplete retrieved context is only briefly acknowledged without systematic evaluation
- The generalizability claim to non-news domains is not tested

## Confidence

- **High confidence**: The retrieval architecture (BEIT-3 + SigLIP2 ensemble + geometric verification) is technically sound and well-documented. The quantitative improvements (CLIPScore +12.8%, Recall@1 0.994) are clearly reported and represent the most robust claims.
- **Medium confidence**: The semantic chunking methodology and its impact on fine-tuning quality, while supported by convergence curves, lacks ablation studies varying chunk selection strategies. The 70/30 prompt ratio is asserted but not empirically validated against alternatives.
- **Low confidence**: The generalizability claim to non-news domains is not tested, and the system's behavior with contradictory or incomplete retrieved context is only briefly acknowledged without systematic evaluation.

## Next Checks

1. **Geometric reranking sensitivity**: Systematically vary ORB/SIFT thresholds (0.3-0.5 confidence, 5-10 inliers) to identify optimal settings for OpenEvents v1 and assess robustness to different image types (news photos vs. illustrations).

2. **Chunk selection ablation**: Compare semantic chunking (top-5) against random selection and full-article conditioning to isolate the contribution of focused context versus overall information volume.

3. **Prompt generalization test**: Evaluate the fine-tuned model on prompts with varying structure (e.g., 50/50 visual-context ratio, removed temporal constraints) to measure sensitivity to training-time prompt engineering.