---
ver: rpa2
title: 'Large Language Models for Healthcare Text Classification: A Systematic Review'
arxiv_id: '2503.01159'
source_url: https://arxiv.org/abs/2503.01159
tags:
- text
- classification
- healthcare
- data
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic review identified 65 research studies examining
  large language models (LLMs) for healthcare text classification between 2018-2024.
  The analysis revealed that fine-tuning approaches were most prevalent, used in 35
  of 65 studies, followed by prompt engineering in 17 studies.
---

# Large Language Models for Healthcare Text Classification: A Systematic Review

## Quick Facts
- arXiv ID: 2503.01159
- Source URL: https://arxiv.org/abs/2503.01159
- Reference count: 40
- Primary result: Systematic review of 65 studies on LLMs for healthcare text classification (2018-2024), finding fine-tuning approaches most prevalent and LLMs significantly outperforming traditional ML

## Executive Summary
This systematic review comprehensively analyzed 65 research studies examining large language models (LLMs) for healthcare text classification between 2018-2024. The research landscape shows fine-tuning approaches dominated the field with 35 out of 65 studies, while BERT variants and closed-source GPT models were the most commonly used architectures. Clinical notes emerged as the predominant data category, followed by healthcare communications and research literature. The review found LLMs significantly outperform traditional machine learning approaches in healthcare text classification tasks, particularly for clinical decision support and public health applications. However, substantial challenges remain regarding data privacy, model interpretability, and computational resource requirements that must be addressed before practical implementation in healthcare settings.

## Method Summary
The systematic review employed rigorous methodology to identify and analyze research studies on LLMs for healthcare text classification published between 2018 and 2024. The review process included comprehensive database searches, study selection criteria based on relevance to healthcare text classification using LLMs, and systematic data extraction focusing on methodology, data types, evaluation metrics, and performance outcomes. The analysis synthesized findings across the 65 included studies to identify trends, dominant approaches, and key challenges in the field.

## Key Results
- Fine-tuning approaches were most prevalent, used in 35 of 65 studies, while prompt engineering appeared in 17 studies
- BERT variants and closed-source LLMs (particularly GPT models) dominated the architectural landscape
- Clinical notes constituted the largest data category, followed by healthcare communications and research literature
- Performance evaluation primarily focused on accuracy metrics, with computational efficiency measures used less frequently

## Why This Works (Mechanism)
LLMs excel at healthcare text classification due to their ability to capture complex semantic relationships, understand medical terminology context, and leverage pre-training on vast corpora that includes medical literature. Their transformer architecture enables attention mechanisms that can identify relevant features across long clinical documents, while transfer learning allows adaptation to specific healthcare tasks with relatively small labeled datasets. The contextual understanding provided by LLMs surpasses traditional rule-based or statistical methods, particularly for nuanced clinical decision support tasks requiring interpretation of implicit information and medical reasoning.

## Foundational Learning
- **Fine-tuning vs Prompt Engineering**: Understanding when to adapt pre-trained models versus using in-context learning - needed to select appropriate implementation strategies; quick check: compare performance on small vs large labeled datasets
- **BERT Variants**: Knowledge of different BERT architectures (BioBERT, ClinicalBERT, etc.) and their domain-specific optimizations - needed to select appropriate base models; quick check: evaluate domain-specific vs general models on clinical text
- **Transformer Architecture**: Understanding self-attention mechanisms and positional encoding - needed to grasp model capabilities and limitations; quick check: test model performance with varying sequence lengths
- **Evaluation Metrics**: Familiarity with precision, recall, F1-score, and computational efficiency measures in healthcare context - needed to properly assess model performance; quick check: compare multiple metrics across different clinical tasks
- **Data Privacy Regulations**: Understanding HIPAA, GDPR, and other healthcare data protection requirements - needed for practical implementation; quick check: verify compliance of proposed deployment architecture

## Architecture Onboarding

**Component Map**: Data Preprocessing -> Model Selection (BERT/GPT variants) -> Fine-tuning/Prompt Engineering -> Evaluation (Accuracy/Computational Efficiency) -> Clinical Validation

**Critical Path**: Data preparation and annotation represents the most critical initial step, as model performance heavily depends on data quality and relevance to specific healthcare tasks. The model selection and fine-tuning process follows, requiring careful hyperparameter tuning and validation.

**Design Tradeoffs**: Fine-tuning offers better performance but requires more computational resources and labeled data, while prompt engineering is more efficient but may yield lower accuracy. Domain-specific pre-trained models (BioBERT, ClinicalBERT) provide better medical understanding but have smaller user communities compared to general models like GPT.

**Failure Signatures**: Poor performance typically manifests as inability to handle medical abbreviations, failure to capture clinical context, or misclassification of critical medical concepts. Computational inefficiency appears as excessive inference time or memory requirements exceeding deployment constraints.

**First Experiments**:
1. Benchmark general BERT vs domain-specific BioBERT on clinical note classification task
2. Compare fine-tuning vs few-shot prompt engineering on radiology report analysis
3. Evaluate computational efficiency (inference time, memory usage) across different LLM architectures on identical hardware

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Publication bias likely underrepresents studies using smaller or specialized LLMs and negative results
- Temporal constraints (2018-2024) may miss earlier foundational work providing important context
- Methodological heterogeneity across studies makes direct comparisons challenging and affects validity of aggregate conclusions

## Confidence

**High Confidence**:
- Dominance of fine-tuning approaches (35/65 studies) and prevalence of BERT variants and GPT models are well-supported findings

**Medium Confidence**:
- Claims about LLM performance superiority over traditional ML approaches require careful interpretation due to methodological variations and potential publication bias

**Low Confidence**:
- Specific performance metrics and computational efficiency comparisons are difficult to validate due to inconsistent reporting practices

## Next Checks
1. Conduct a meta-analysis of effect sizes across studies using standardized performance metrics to quantify true performance advantage of LLMs over traditional approaches
2. Perform independent validation study using common dataset and evaluation protocol across multiple LLM architectures to establish baseline benchmarks
3. Investigate reproducibility of key findings by attempting to replicate most frequently cited studies using original datasets and methodologies