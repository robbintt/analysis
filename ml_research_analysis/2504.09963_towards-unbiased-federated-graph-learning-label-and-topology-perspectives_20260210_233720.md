---
ver: rpa2
title: 'Towards Unbiased Federated Graph Learning: Label and Topology Perspectives'
arxiv_id: '2504.09963'
source_url: https://arxiv.org/abs/2504.09963
tags:
- local
- learning
- nodes
- training
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles fairness challenges in federated graph learning,
  where existing methods often exhibit biased performance towards structurally or
  semantically marginalized node groups. Specifically, it addresses two key fairness
  issues: improving representation of minority-class nodes for class-wise fairness,
  and mitigating topological bias from heterophilous connections for topology-aware
  fairness.'
---

# Towards Unbiased Federated Graph Learning: Label and Topology Perspectives

## Quick Facts
- arXiv ID: 2504.09963
- Source URL: https://arxiv.org/abs/2504.09963
- Reference count: 40
- Primary result: FairFGL improves Macro-F1 by up to 22.62% for disadvantaged node groups in federated graph learning

## Executive Summary
This paper tackles fairness challenges in federated graph learning where existing methods exhibit biased performance toward structurally or semantically marginalized node groups. The authors identify two key fairness issues: minority-class nodes with heterophilous connections suffer from impaired representations, and global majority dominance during federated aggregation suppresses minority-class learning. FairFGL introduces a dual-objective framework that enhances both class-wise fairness (minority-class representation) and topology-aware fairness (heterophilous node performance) through fine-grained graph mining and collaborative learning.

The framework employs three client-side modules: History-Preserving Module prevents overfitting to dominant local classes by fusing with a frozen teacher model, Majority Alignment Module refines representations of heterophilous majority-class nodes using homophilous prototypes, and Gradient Modification Module transfers minority-class knowledge from structurally favorable clients. The server-side cluster-based aggregation reconciles conflicting updates while suppressing global majority dominance, transmitting only Top-k parameters to optimize communication efficiency. Extensive evaluations on eight benchmark datasets demonstrate significant improvements for disadvantaged node groups, achieving up to 22.62% increase in Macro-F1 score while enhancing convergence over state-of-the-art baselines.

## Method Summary
FairFGL addresses fairness in federated graph learning through a dual-objective framework that improves both class-wise fairness (minority-class representation) and topology-aware fairness (heterophilous node performance). The method operates in subgraph-FL settings where clients receive graph partitions with skewed label distributions. On the client side, FairFGL employs three key modules: History-Preserving Module averages recent global models to prevent local overfitting while transferring minority knowledge, Majority Alignment Module partitions majority nodes by homophily and distills knowledge from homophilous prototypes to heterophilous nodes, and Gradient Modification Module transfers minority expertise from dissimilar clients. The server implements Top-k parameter selection, cluster-based aggregation with k-means, and identifies deviated models for each client based on cosine similarity. The framework trains for 150 rounds with 2-layer GCN backbone, evaluating performance through Macro-F1 and disaggregated metrics for disadvantaged node groups.

## Key Results
- FairFGL achieves up to 22.62% increase in Macro-F1 score for disadvantaged node groups compared to state-of-the-art baselines
- Significant improvements in disaggregated metrics: Min-F1, Hete-F1, and Hete-Min-F1 scores for minority and heterophilous node groups
- Enhanced convergence with faster training and better stability across all eight benchmark datasets
- History-Preserving Module shows largest impact on multi-class datasets, while Cluster-based Aggregation dominates on densely-connected datasets
- FairFGL demonstrates low sensitivity to hyperparameter variations (Top-k and α) while maintaining performance gains

## Why This Works (Mechanism)

### Mechanism 1: History-Preserving Module for Class-wise Fairness
- Claim: Averaging recent global models and using them as a frozen teacher prevents local models from overfitting to dominant local classes while transferring minority-class knowledge.
- Mechanism: The module creates a History Model by averaging the five most recent global updates. During local training, it: (1) fuses with the local model via learnable weight α (Eq. 5), and (2) provides soft labels for unlabeled nodes through KL divergence distillation (Eq. 6).
- Core assumption: Assumes that global models from the last 5 rounds contain stabilized minority-class knowledge that individual clients lack locally, and that unlabeled nodes can receive reliable soft labels from this aggregated knowledge.
- Evidence anchors:
  - [abstract] "History-Preserving Module prevents overfitting to dominant local classes"
  - [Section IV-B1] "it is fused with locally trained parameters using learnable weights α as an adaptive regularization term... it functions as a teacher model to transfer global knowledge to the local model on the unlabeled set"
  - [corpus] Limited direct corpus evidence; BoostFGL addresses fairness but through different mechanisms
- Break condition: Fails if communication rounds are too few (insufficient history), if local data distributions shift rapidly between rounds, or if the global model itself is heavily biased toward global majority classes.

### Mechanism 2: Majority Alignment for Topology-aware Fairness
- Claim: Aligning representations of heterophilous majority nodes toward their homophilous counterparts improves local model reliability and cross-client knowledge transfer.
- Mechanism: Computes homophily scores (Eq. 7) to partition majority nodes into Homo-Maj and Hete-Maj groups based on whether neighbors share the same label. Creates prototypes from high-confidence homophilous majority nodes and distills knowledge to heterophilous majority nodes via KL loss (Eq. 8).
- Core assumption: Assumes that majority-class nodes with homophilous connectivity have more reliable learned representations than their heterophilous counterparts due to message-passing's bias toward homophily.
- Evidence anchors:
  - [abstract] "Majority Alignment Module to refine representations of heterophilous majority-class nodes"
  - [Section III, 2-Analysis] "majority-class nodes with heterophilous connections (Hetero-Maj) suffer from impaired representations and fail to benefit from well-learned decision boundaries"
  - [corpus] GRAIN and ASEHybrid papers validate heterophily challenges in GNNs but do not directly test this alignment mechanism
- Break condition: Fails when majority-class nodes have predominantly heterophilous connectivity (insufficient Homo-Maj for prototype extraction), or when homophilous and heterophilous nodes represent semantically distinct subgroups.

### Mechanism 3: Cluster-based Aggregation with Gradient Modification
- Claim: Clustering clients by update similarity and selectively transferring minority knowledge from dissimilar clients mitigates global majority dominance and improves minority-class learning.
- Mechanism: Server clusters local updates via k-means with silhouette coefficient for k selection, aggregates uniformly within clusters (Eq. 12-13), then identifies the most dissimilar client for each recipient using cosine similarity minimization (Eq. 15). During local training, gradient modification projects conflicting gradients away from the deviated model's direction (Eq. 9, Algorithm 1).
- Core assumption: Assumes the least similar client contains complementary minority-class expertise where that class appears as a local majority with homophilous structure.
- Evidence anchors:
  - [abstract] "cluster-based aggregation strategy to reconcile conflicting updates and suppress global majority dominance"
  - [Section IV-C2] "we adopt a cosine similarity-based minimization objective to assign each client the local update that maximally reflects minority-class knowledge"
  - [corpus] Fed-PUB uses clustering on functional embeddings for aggregation, supporting the clustering intuition, but does not validate gradient modification approach
- Break condition: Fails when minority classes are equally underrepresented across all clients (no client has "expertise"), or when cosine similarity on Top-k parameter changes doesn't reflect true distribution alignment.

## Foundational Learning

- Concept: Homophily vs. Heterophily in Graph Neural Networks
  - Why needed here: The entire fairness framework hinges on understanding that standard GNNs assume connected nodes share similar labels (homophily), which causes systematic bias against heterophilous nodes where neighbors have dissimilar labels or misleading features.
  - Quick check question: Given a node where 70% of neighbors have a different label, will a standard GCN's message-passing help or hurt its classification accuracy?

- Concept: Federated Averaging (FedAvg) and Class-imbalance Bias
  - Why needed here: FairFGL modifies FedAvg's sample-weighted averaging with clustering to prevent global majority dominance. Understanding baseline aggregation clarifies why majority classes systematically dominate the aggregated model.
  - Quick check question: If Client A has 10,000 samples of class 1 and Client B has 100 samples of class 2, what weight does class 2's gradients receive during FedAvg aggregation?

- Concept: Knowledge Distillation for Privacy-preserving Transfer
  - Why needed here: Both History-Preserving and Majority Alignment modules use KL divergence distillation to transfer knowledge without sharing raw data or labels—critical for federated privacy constraints.
  - Quick check question: How does soft-label distillation from a frozen teacher model differ from hard-label supervised training when handling class imbalance?

## Architecture Onboarding

- Component map:
  - Client-side: History Model (frozen, 5-round average) → Local Model (active training with gradient modification) → Top-k parameter selection
  - Server-side: K-means clustering → Per-cluster FedAvg → Uniform inter-cluster aggregation → Deviated Model selection via cosine similarity
  - Cross-client pipeline: Server identifies most dissimilar client → combines that client's update with previous global model → broadcasts tailored Deviated Model

- Critical path:
  1. Server broadcasts global model + history model + tailored deviated model to each client
  2. Client computes homophily scores, partitions majority nodes into Homo-Maj/Hete-Maj
  3. Client distills knowledge from Homo-Maj prototypes to Hete-Maj nodes (KL loss)
  4. Client fuses history model with local model (learnable α), applies distillation on unlabeled nodes
  5. Client modifies gradients using deviated model when gradients conflict (negative dot product)
  6. Client selects Top-k most changed parameters, uploads only these changes
  7. Server clusters clients by update similarity, aggregates within clusters with equal weights

- Design tradeoffs:
  - Top-k ratio (ρ): Lower values improve communication efficiency and privacy but may discard minority knowledge encoded in rarely-updated parameters; paper suggests 0.2–0.6 range
  - History window (5 rounds): Longer windows provide stabilization but may lag when distributions shift rapidly
  - Gradient modification margin: Higher values preserve local knowledge more aggressively but may reject beneficial minority expertise

- Failure signatures:
  - Hete-Min F1 not improving: Verify deviated model actually contains the target minority class as a local majority; check if cosine similarity selection is picking semantically wrong clients
  - Overall accuracy drops significantly: May indicate over-regularization from history model (reduce α); verify clustering isn't creating degenerate single-client clusters
  - Convergence oscillates: Check if Top-k is too aggressive (< 0.1), or if cluster assignments change drastically between rounds

- First 3 experiments:
  1. **Baseline comparison on Cora with Metis partitioning**: Implement Metis-based subgraph partitioning to create skewed label distributions (simulating Figure 2). Compare Macro-F1 and per-group F1 (Min-F1, Hete-F1, Hete-Min-F1) against FedAvg, FedGTA, and AdaFGL. Expected: FairFGL should show largest gains on Hete-Min category.
  2. **Ablation study on CiteSeer and Tolokers**: Train three variants removing each module (w/o History-Preserving, w/o Gradient Modification, w/o Clustering). Per Table IV, History-Preserving should dominate on multi-class datasets (CiteSeer), while Clustering should dominate on densely-connected datasets (Tolokers).
  3. **Hyperparameter sensitivity on Cora**: Sweep Top-k (0.2 to 0.6) and α (0.4 to 0.8). Plot Overall F1, Min-F1, and Hete-Min-F1 as heatmaps. Identify robust operating ranges and verify paper's claim of low sensitivity (Figure 4).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can fairness-centered FGL frameworks be adapted for other FGL paradigms beyond subgraph-FL, such as inter-graph FL or graph-level FL?
- Basis in paper: [explicit] The paper states: "In our investigation, subgraph-FL has emerged as the dominant paradigm in FGL" and focuses exclusively on this setting.
- Why unresolved: The proposed FairFGL relies on node-level properties (homophily scores, local class distributions) that may not directly transfer to graph-level tasks where fairness concerns involve entire graph instances rather than individual nodes.
- What evidence would resolve it: Experiments applying similar fairness objectives to graph classification tasks in federated settings, with appropriate modifications to handle graph-level minority and structural bias.

### Open Question 2
- Question: How does FairFGL perform when a class is underrepresented across ALL clients (global minority) rather than being minority locally but majority elsewhere?
- Basis in paper: [inferred] The Gradient Modification Module relies on transferring knowledge from "structurally favorable clients" where "the minority class of the receiving client appears as a majority in its subgraph." This assumption may fail for globally rare classes.
- Why unresolved: The cross-client knowledge transfer mechanism depends on finding complementary expertise, which is impossible when no client has sufficient samples of a particular class.
- What evidence would resolve it: Experiments with controlled label distributions where specific classes are minority across all clients, measuring Hete-Min-F1 performance degradation compared to locally-imbalanced-only scenarios.

### Open Question 3
- Question: What formal privacy guarantees can be established for FairFGL, given that sharing Top-k parameters and deviated models may leak information about local data distributions?
- Basis in paper: [explicit] The paper claims: "Hyperparameter Top-k influences the communication overhead and more importantly, further enhanced the privacy preservation" but provides no formal privacy analysis.
- Why unresolved: While transmitting only influential parameters reduces exposure, the selected parameters explicitly "best capture local data distribution," potentially revealing class imbalance patterns or structural properties unique to each client.
- What evidence would resolve it: Formal differential privacy analysis quantifying privacy loss under various Top-k settings, or empirical membership inference attacks demonstrating privacy preservation.

### Open Question 4
- Question: How do fairness objectives interact with conventional fairness concerns related to sensitive attributes (e.g., demographic groups)?
- Basis in paper: [explicit] The paper acknowledges: "Notably, the Fairness Challenge defined in this work departs from conventional fairness literature, which primarily targets bias arising from sensitive features."
- Why unresolved: Real-world deployments may require simultaneously addressing both structural fairness (from this work) and demographic fairness, but potential conflicts or synergies between these objectives remain unexplored.
- What evidence would resolve it: Experiments on datasets with both known sensitive attributes and graph structure, evaluating whether FMC/FHN optimization helps, harms, or is orthogonal to demographic fairness metrics.

## Limitations

- The framework's performance depends heavily on finding structurally favorable clients for minority-class knowledge transfer, which may fail when minority classes are globally underrepresented
- Key hyperparameters (learning rate, α, ρ thresholds, margin values) are not specified, making exact reproduction challenging
- The assumption that Top-k parameter selection enhances privacy lacks formal privacy guarantees or empirical validation through privacy attacks
- The method focuses on structural and class-based fairness but does not address demographic fairness concerns related to sensitive attributes

## Confidence

- **High confidence**: The framework's conceptual design addresses documented fairness issues in FGL, and the dual-objective approach (class-wise + topology-aware) is logically sound
- **Medium confidence**: The empirical improvements shown (Macro-F1 gains, convergence benefits) are promising but require hyperparameter specification for full validation
- **Low confidence**: The scalability claims and communication efficiency improvements need verification on larger graphs with more clients

## Next Checks

1. **Reproduce the Cora ablation study**: Implement FairFGL without each module (History-Preserving, Majority Alignment, Gradient Modification) and verify the reported performance drops align with Table IV.

2. **Validate Top-k sensitivity claims**: Sweep the Top-k parameter (ρ) from 0.2 to 0.6 on CiteSeer and confirm the low sensitivity claimed in Figure 4 holds across all three metrics (Overall F1, Min-F1, Hete-Min-F1).

3. **Test the deviated model mechanism**: On a dataset where minority classes have distinct structural patterns, verify that the cosine similarity-based client pairing actually identifies clients with complementary minority-class expertise, and measure whether this improves Hete-Min-F1 specifically.