---
ver: rpa2
title: Instructing the Architecture Search for Spatial-temporal Sequence Forecasting
  with LLM
arxiv_id: '2503.17994'
source_url: https://arxiv.org/abs/2503.17994
tags:
- architecture
- search
- layer
- methods
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ISTS is a neural architecture search method for spatial-temporal
  sequence forecasting that uses large language models as instructors rather than
  direct generators. It employs a three-level enhancement: decomposing the search
  into decision steps with prompting, simulating architecture instances with a one-step
  evaluation framework, and using a two-stage search balancing exploration and optimization.'
---

# Instructing the Architecture Search for Spatial-temporal Sequence Forecasting with LLM

## Quick Facts
- arXiv ID: 2503.17994
- Source URL: https://arxiv.org/abs/2503.17994
- Reference count: 40
- Key outcome: LLM-instructor-based NAS for spatial-temporal forecasting achieves competitive accuracy with 92% faster processing than traditional methods.

## Executive Summary
ISTS is a neural architecture search method that leverages large language models as instructors rather than direct generators for spatial-temporal sequence forecasting. The approach decomposes architecture generation into step-by-step decision processes using prompt engineering, evaluates candidates through a one-step proxy framework, and employs a two-stage search strategy to balance exploration and optimization. Experiments on five real-world traffic datasets demonstrate that ISTS achieves competitive or better forecasting accuracy compared to both NAS and non-NAS baselines while significantly reducing processing time by up to 92%.

## Method Summary
The method uses frozen LLMs to generate valid spatial-temporal architectures through a three-level enhancement: step-level prompt decomposition using Chain-of-Thought or Tree-of-Thought prompting, proxy-based experience accumulation through one-step evaluation, and a two-stage search balancing exploration and optimization. The search space is defined as a DAG with 4 nodes and 6 cells, where cells combine spatial (Graph Convolution) and temporal (Attention) operators differently. Generated architectures undergo brief one-step training and validation, with results stored in a sorted memory bank that conditions the LLM for subsequent decisions.

## Key Results
- Achieved competitive or better forecasting accuracy compared to both NAS and non-NAS baselines
- Processing time reduced by up to 92% compared to traditional methods
- Demonstrated flexibility across different LLM models while maintaining stable performance

## Why This Works (Mechanism)

### Mechanism 1: Step-Level Prompt Decomposition
Decomposing architecture generation into sequential decision steps using Chain-of-Thought or Tree-of-Thought prompting allows frozen LLMs to better leverage internal knowledge to construct valid spatial-temporal architectures. The system uses specific prompt templates to break DAG generation into sequential choices, forcing the LLM to act as an "instructor" rather than a random code generator.

### Mechanism 2: Proxy-Based Experience Accumulation
Using a one-step tuning framework as a proxy for performance evaluation enables rapid simulation of candidates and population of a sorted memory bank, which conditions the LLM to improve subsequent decisions without full training. Generated architectures undergo brief one-step training and validation, with results stored and fed back into the prompt context for "experience observation."

### Mechanism 3: Two-Stage Search Strategy
Dividing the search process into explicit "Exploration Stage" and "Optimization Stage" with distinct prompt instructions reduces the probability of converging on local optima compared to greedy search. The system switches prompts mid-search, encouraging diverse combinations initially and then directing the LLM to minimize metrics based on the best-found samples.

## Foundational Learning

- **Directed Acyclic Graphs (DAGs) in NAS**
  - Why needed: The search space is defined as a DAG, not a sequential stack. Understanding how nodes represent hidden states and edges represent cells is required to interpret the LLM's output format.
  - Quick check: Can you explain why a DAG structure allows for more flexible spatial-temporal feature fusion than a simple linear sequence of layers?

- **Spatial-Temporal Coupling**
  - Why needed: The core search units are cells that combine spatial and temporal operators differently. One must grasp these dependencies to evaluate if the LLM's "explanations" are logically sound.
  - Quick check: What is the functional difference between a "Spatial-then-Temporal" (STT) cell and a "Temporal-then-Spatial" (TTS) cell regarding information flow?

- **In-context Learning (ICL)**
  - Why needed: The method relies on feeding the "Sorted Memory Bank" back into the prompt. This is a form of ICL where the model learns from the context window without weight updates.
  - Quick check: How does the sorted order of the memory bank samples bias the LLM's next prediction?

## Architecture Onboarding

- **Component map:** LLM Interface -> Search Space Definer -> Proxy Evaluator -> Memory Manager -> Controller
- **Critical path:** Controller initializes Memory Bank → Prompt Engine constructs input with Background + Strategy + Memory → LLM generates Architecture JSON → Evaluator builds model and runs one-step training → Result updates Memory Bank → Repeat until rounds complete; return best architecture
- **Design tradeoffs:** Efficiency vs. Accuracy (one-step proxy is faster but may sacrifice accuracy estimate), Prompt Complexity (TOT-style is more granular but consumes more tokens/time per architecture than COT-style)
- **Failure signatures:** JSON Parsing Errors (invalid JSON or hallucinated cell types), Context Overflow (memory bank grows too large for LLM context window), Stagnation (LLM ignores "Exploration" prompt and repeats architectures)
- **First 3 experiments:**
  1. Implement COT-style search on PEMS-04 with LLaMA-3-8B to verify one-step tuning proxy correlation with standard validation metrics
  2. Run search with empty memory bank to quantify performance drop and validate "Experience Observation" mechanism
  3. Test different exploration ratios to find balance between exploring new structures and optimizing known good ones

## Open Questions the Paper Calls Out

### Open Question 1
Can ISTS maintain efficiency and stability when applied to significantly larger and more complex neural architecture search spaces? The experimental validation is restricted to a relatively small search space (DAG with 4 nodes and 6 edges), and the authors note that the process involves "logically coherent steps" which might become unwieldy in higher-dimensional spaces.

### Open Question 2
How strongly does the "one-step tuning" evaluation correlate with the fully converged performance of the architecture? The method utilizes a "one-step tuning framework to quickly evaluate the architecture instance" to save time, but this creates a proxy task that may not reflect the true global optimum.

### Open Question 3
Does the reliance on LLM "internal world knowledge" constrain the method's applicability to STSF domains outside of common traffic prediction? The introduction claims the method leverages "comprehensive internal world knowledge," yet all five experimental datasets are traffic-related domains where LLMs possess strong priors.

## Limitations
- Reliance on one-step proxy evaluation may not accurately predict final architecture performance
- Method tested only on traffic-related spatial-temporal datasets, limiting generalization claims
- Prompt engineering requirements may vary significantly across different LLM models

## Confidence
- **High Confidence:** Architectural framework and 92% time reduction claims
- **Medium Confidence:** LLM-instructor superiority over traditional NAS methods
- **Low Confidence:** Scalability and generalization to non-traffic domains

## Next Checks
1. Conduct correlation study comparing one-step proxy scores against full training performance across 5-10 diverse architectures
2. Systematically vary memory bank size and quality to measure impact on search diversity and final architecture quality
3. Test same search process with different LLM sizes and types to evaluate method's flexibility claims and identify model-specific requirements