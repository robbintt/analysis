---
ver: rpa2
title: 'NLPrompt: Noise-Label Prompt Learning for Vision-Language Models'
arxiv_id: '2412.01256'
source_url: https://arxiv.org/abs/2412.01256
tags:
- learning
- prompt
- noisy
- loss
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NLPrompt, a method to improve the robustness
  of prompt learning in vision-language models against noisy labels. The core idea
  is to use mean absolute error (MAE) loss in prompt learning (PromptMAE) which, despite
  slow convergence in traditional scenarios, shows strong robustness and accuracy
  in prompt learning contexts.
---

# NLPrompt: Noise-Label Prompt Learning for Vision-Language Models

## Quick Facts
- arXiv ID: 2412.01256
- Source URL: https://arxiv.org/abs/2412.01256
- Reference count: 40
- Introduces NLPrompt, a method improving prompt learning robustness against noisy labels in vision-language models

## Executive Summary
This paper introduces NLPrompt, a method to improve the robustness of prompt learning in vision-language models against noisy labels. The core idea is to use mean absolute error (MAE) loss in prompt learning (PromptMAE) which, despite slow convergence in traditional scenarios, shows strong robustness and accuracy in prompt learning contexts. Additionally, NLPrompt incorporates PromptOT, an optimal transport-based data purification method that partitions datasets into clean and noisy subsets using text features from vision-language models as prototypes. The method applies cross-entropy loss to clean subsets and MAE to noisy subsets, leveraging the strengths of both loss functions. Extensive experiments on multiple datasets with varied noise conditions demonstrate significant performance improvements over existing methods, including CoOp, GCE, and JoAPR. NLPrompt achieves state-of-the-art results in most cases, particularly excelling in high-noise scenarios, and shows strong generalization across different prompt-tuning approaches.

## Method Summary
NLPrompt addresses noisy label challenges in vision-language model prompt learning through two key innovations. First, it employs PromptMAE, which uses MAE loss for prompt learning - a departure from traditional cross-entropy approaches that shows surprising robustness despite typically slower convergence. Second, it introduces PromptOT, an optimal transport-based data purification method that leverages vision-language model text features as prototypes to partition datasets into clean and noisy subsets. The method then applies cross-entropy loss to the identified clean subsets and MAE to the noisy subsets, creating a hybrid training strategy that capitalizes on the strengths of both loss functions. This dual approach effectively mitigates the impact of label noise while maintaining high accuracy.

## Key Results
- NLPrompt achieves state-of-the-art results in most tested scenarios, particularly excelling in high-noise conditions
- The method demonstrates strong generalization across different prompt-tuning approaches including CoOp, GCE, and JoAPR
- Significant performance improvements over baseline methods on multiple datasets with varied noise conditions, with particularly pronounced gains in symmetric noise scenarios

## Why This Works (Mechanism)
The method's effectiveness stems from its dual approach to noisy label mitigation. PromptMAE leverages the inherent robustness of MAE loss in handling outliers, which is particularly valuable when dealing with noisy labels. The MAE loss is less sensitive to large errors compared to cross-entropy, making it more stable during training with label noise. Meanwhile, PromptOT's data purification component uses the rich semantic information embedded in vision-language model text features to identify likely clean samples through optimal transport partitioning. By applying cross-entropy to clean subsets (where it excels) and MAE to noisy subsets (where its robustness shines), the method creates a complementary training strategy that maximizes accuracy while minimizing noise impact.

## Foundational Learning

**MAE vs Cross-Entropy Loss**: Understanding the difference between mean absolute error and cross-entropy loss functions is crucial, as MAE is less sensitive to outliers and label noise. Quick check: Review loss function behavior with noisy labels on a simple classification task.

**Optimal Transport Theory**: The PromptOT component relies on optimal transport for data purification, requiring understanding of how to measure and minimize transport costs between distributions. Quick check: Implement a basic optimal transport example with prototype-based clustering.

**Vision-Language Model Embeddings**: The method uses text features from vision-language models as prototypes for data purification, necessitating familiarity with cross-modal embedding spaces. Quick check: Extract and visualize text embeddings from a pre-trained vision-language model.

**Prompt Learning**: Understanding how prompt tuning works in vision-language models is essential, as NLPrompt specifically addresses noise in this context. Quick check: Implement a basic prompt tuning experiment on a vision-language model.

## Architecture Onboarding

**Component Map**: Data Loader -> PromptOT Data Purification -> PromptMAE Loss + Cross-Entropy Loss -> Vision-Language Model -> Performance Evaluation

**Critical Path**: The most critical path is the PromptOT data purification followed by the hybrid loss application, as these directly determine the quality of training and final performance.

**Design Tradeoffs**: The method trades computational overhead from the optimal transport-based purification for improved robustness to label noise. While this adds complexity, the benefits in high-noise scenarios justify the cost.

**Failure Signatures**: Poor performance may indicate: 1) ineffective data purification leading to misclassification of clean/noisy samples, 2) suboptimal loss weight balancing between MAE and cross-entropy, or 3) vision-language model text features failing to capture sufficient semantic information for prototype-based partitioning.

**First Experiments**: 1) Test PromptMAE alone (without PromptOT) on a noisy dataset to isolate the loss function's contribution. 2) Evaluate PromptOT's purification accuracy on datasets with known noise patterns. 3) Compare performance against standard prompt tuning with cross-entropy loss across varying noise levels.

## Open Questions the Paper Calls Out
None

## Limitations

The method shows variable performance margins across different noise types, with more modest improvements in asymmetric noise conditions compared to symmetric noise. The optimal transport-based data purification introduces computational overhead that may become prohibitive for extremely large-scale datasets. The reliance on vision-language model text features for purification may be sensitive to the quality and stability of cross-modal embeddings.

## Confidence

**High confidence**: The robustness of MAE loss in prompt learning contexts and the overall effectiveness of the hybrid loss approach (PromptMAE + cross-entropy) for noisy label scenarios.

**Medium confidence**: The data purification effectiveness of PromptOT and its generalizability across diverse vision-language models beyond those tested.

**Medium confidence**: The scalability claims, as extensive testing on extremely large-scale datasets (>1M samples) is limited in the current evaluation.

## Next Checks

1. Evaluate NLPrompt's performance on extremely large-scale datasets (e.g., ImageNet-22K or WebVision) to assess computational scalability and purification accuracy at scale.

2. Test the method's robustness to different vision-language model backbones (e.g., CLIP, BLIP, or Florence) to verify the cross-modal embedding stability assumption.

3. Conduct ablation studies specifically targeting the PromptOT data purification module to quantify its individual contribution versus the loss function design, particularly in high-noise scenarios.