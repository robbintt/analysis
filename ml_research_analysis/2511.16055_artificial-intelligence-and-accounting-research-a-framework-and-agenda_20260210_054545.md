---
ver: rpa2
title: 'Artificial Intelligence and Accounting Research: A Framework and Agenda'
arxiv_id: '2511.16055'
source_url: https://arxiv.org/abs/2511.16055
tags:
- accounting
- research
- researchers
- methods
- traditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a two-dimensional framework to classify AI-accounting
  research by research focus (accounting-centric vs. AI-centric) and methodological
  approach (AI-based vs.
---

# Artificial Intelligence and Accounting Research: A Framework and Agenda

## Quick Facts
- arXiv ID: 2511.16055
- Source URL: https://arxiv.org/abs/2511.16055
- Reference count: 14
- Primary result: AIS journals publish mostly AI-centric research (80%) while non-AIS journals focus on accounting-centric research (65%)

## Executive Summary
This paper develops a two-dimensional framework to classify AI-accounting research by research focus (accounting-centric vs. AI-centric) and methodological approach (AI-based vs. traditional methods). Analyzing 89 papers from leading accounting journals (2022-2025), the study reveals that while GenAI democratizes certain research capabilities, it simultaneously intensifies competition by raising expectations for higher-order contributions where human judgment, creativity, and theoretical depth remain valuable. The framework maps distinct collaborative advantages for accounting researchers in developing specialized AI applications, conducting independent critical evaluations, and testing fundamental theories using AI-enhanced measurement.

## Method Summary
The study classifies 89 papers from 9 leading accounting journals (2022-2025) using a 2×2 framework based on research focus (accounting-centric vs. AI-centric) and methodological approach (AI-based vs. traditional methods). Papers were identified via Web of Science search using AI-related keywords in Business/Finance categories, then manually classified by two independent raters. The classification process involved hierarchical decision rules prioritizing explicit contribution claims and determining whether AI drives core analysis or serves auxiliary functions.

## Key Results
- AIS journals predominantly publish AI-centric research (80%) while non-AIS journals focus on accounting-centric research (65%)
- The framework reveals a diagonal imbalance with few papers using traditional methods to study accounting outcomes using AI context
- Accounting researchers maintain collaborative advantages in domain-specific applications, independent evaluation, and theoretical testing rather than algorithmic innovation
- GenAI transforms research workflows by substituting for execution tasks while requiring human scaffolding for validity and theory

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A two-dimensional classification framework resolves ambiguity in interdisciplinary AI-accounting research by separating research subject from research method.
- **Mechanism:** The framework disentangles "what is being studied" (Subject Centricity: Accounting vs. AI) from "how it is studied" (Method: AI-based vs. Traditional). By forcing a classification into one of four quadrants, it reveals gaps in the literature—specifically the diagonal imbalance where few papers use traditional methods to study accounting outcomes—and maps technical skill requirements.
- **Core assumption:** Distinct research communities (AIS vs. Non-AIS) have implicitly distinct missions that can be operationalized by where they cluster in this 2x2 space.
- **Evidence anchors:**
  - [abstract] "...framework that classifies AI-accounting research along two dimensions: research focus... and methodological approach..."
  - [section] Table 1 defines the four quadrants; Section 3.4 notes the "diagonal imbalance" revealing gaps.
  - [corpus] Related work in "Generative Artificial Intelligence: Evolving Technology... and Opportunities for Information Systems Research" suggests similar taxonomic approaches are needed to structure new technological fields.

### Mechanism 2
- **Claim:** Accounting researchers maintain strategic relevance not by competing on algorithmic innovation or compute resources, but by leveraging "Collaborative Advantage" through domain specificity and causal inference.
- **Mechanism:** This is a division of labor mechanism. Industry holds advantages in proprietary data and speed; Computer Science holds advantages in algorithmic frontier; Accounting researchers hold advantages in institutional knowledge, theory, and independent evaluation. The framework maps these asymmetric strengths to specific research quadrants.
- **Core assumption:** The value of research is determined by the scarcity of the contribution, and industry/CS researchers will not naturally fill the "independent evaluation" or "deep theory testing" gaps due to misaligned incentives.
- **Evidence anchors:**
  - [abstract] "...accounting researchers maintain collaborative advantages in... developing specialized AI applications, conducting independent evaluations..."
  - [section] Table 6 explicitly compares strengths: Accounting Researchers rate "High" for Theoretical Contribution vs. Industry's "Low".
  - [corpus] "Addressing Bias in Generative AI" implies domain experts (Information Management scholars) have unique roles in fixing domain-specific AI failures, supporting the advantage mechanism.

### Mechanism 3
- **Claim:** GenAI transforms the research workflow by substituting for "downstream" execution tasks (coding, processing) while necessitating "upstream" human scaffolding for validity and theory.
- **Mechanism:** Task-substitution based on capability matching. GenAI agents excel at speed, scale, and pattern recognition (e.g., literature scanning, data cleaning). Humans excel at causal reasoning, ethical oversight, and contextual judgment. The mechanism relies on a "Human Scaffolding" protocol where AI generates outputs and humans validate/interpret them.
- **Core assumption:** The distinct boundary between "pattern description" (AI) and "theoretical interpretation" (Human) holds firm; AI does not achieve reliable causal reasoning in the short term (4-5 years).
- **Evidence anchors:**
  - [abstract] "...GenAI is transforming the research process, with human researchers retaining advantages in theoretical development and causal inference..."
  - [section] Table 7 compares tasks: Under "Results Interpretation," AI is limited to "Pattern description" while Humans are "Essential for theoretical interpretation."
  - [corpus] "Generative to Agentic AI" supports the trajectory of increasing autonomy, but implies the need for new conceptual frameworks to manage it.

## Foundational Learning

- **Concept: Subject Centricity (Accounting vs. AI)**
  - **Why needed here:** To determine the *primary contribution* of a paper. It prevents misclassifying a technical AI improvement as an accounting insight.
  - **Quick check question:** If you removed the accounting data, would the core contribution (e.g., a new algorithm) still stand? If yes, it is AI-centric.

- **Concept: Methodological Centricity (AI-Based vs. Traditional)**
  - **Why needed here:** To distinguish between using AI as a tool vs. an object of study.
  - **Quick check question:** Does the AI perform the *key estimation* or measurement task, or is it merely preprocessing data for a standard regression?

- **Concept: Human Scaffolding**
  - **Why needed here:** To effectively integrate GenAI into the research workflow without sacrificing validity.
  - **Quick check question:** Can you trace the specific validation step where a human overrode or verified an AI-generated inference?

## Architecture Onboarding

- **Component map:**
  The architecture consists of a **2x2 Matrix** (The Framework) supported by a **Capability Matrix** (Table 6: Researcher Strengths) and an **Workflow Map** (Table 7: Human vs. AI Tasks).
  - Quadrant 1: Accounting via AI (Method: AI, Focus: Accounting)
  - Quadrant 2: AI via AI (Method: AI, Focus: AI)
  - Quadrant 3: Accounting via Traditional (Method: Trad, Focus: Accounting)
  - Quadrant 4: AI via Traditional (Method: Trad, Focus: AI)

- **Critical path:**
  1. Define Research Focus (Accounting phenomenon vs. AI system).
  2. Select Methodological Approach (AI-driven analysis vs. Traditional econometrics/experiments).
  3. Map to Framework Quadrant.
  4. Identify Collaborative Advantage (Check Table 6 to see if you have the right skills or need a partner).

- **Design tradeoffs:**
  - **Speed vs. Validity:** Using GenAI for "Idea Generation" (High speed, risk of derivative ideas) vs. Human "Deep Deliberation" (Slow, high novelty).
  - **Technical Depth vs. Domain Depth:** "AI via AI" requires high technical coding skills; "AI via Traditional" requires deep domain/theoretical skills.

- **Failure signatures:**
  - **The "Token" AI User:** Using AI only for OCR or simple cleaning but claiming the paper is "AI-Based" (Violates method classification rules).
  - **The Unsupervised AI:** Relying on GenAI for literature review without verifying citations (Hallucination risk).
  - **The Advantage Mismatch:** An Accounting Researcher trying to compete on "Algorithmic Innovation" against CS researchers without a collaborative partner (Misaligned strategic positioning).

- **First 3 experiments:**
  1. **Quadrant Classification:** Take 5 recent papers from your reading list and classify them into the 2x2 matrix to practice identifying "Subject Centricity."
  2. **Workflow Audit:** Select a current research task (e.g., coding qualitative data). Attempt it with an LLM following the "Human Scaffolding" protocol (AI drafts, Human validates) to measure speed gains vs. error rates.
  3. **Gap Analysis:** Use the framework to identify a "gap" in your subfield (e.g., "Are we lacking 'Accounting via Traditional' studies that use AI shocks as natural experiments?").

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does reliance on GenAI recommendations impair professional skepticism and judgment quality among auditors and financial executives compared to unaided decision-making?
- **Basis in paper:** [Explicit] The authors explicitly call for "Behavioral and judgment research investigating how AI recommendations affect professional skepticism, judgment quality, [and] over-reliance patterns" (Section 4.2.2).
- **Why unresolved:** Industry vendors have no incentive to fund studies that might reveal limitations or poor judgment outcomes, and CS researchers focus on algorithmic performance rather than user behavior.
- **What evidence would resolve it:** Controlled experiments with domain experts (auditors/executives) comparing decision accuracy and skepticism levels in AI-assisted versus unassisted scenarios.

### Open Question 2
- **Question:** How can exogenous shocks related to AI (e.g., ChatGPT bans, service outages, or regulatory changes) serve as natural experiments to test fundamental theories of information asymmetry and analyst behavior?
- **Basis in paper:** [Explicit] The paper identifies the "Accounting-Centric via Traditional Methods" quadrant as underdeveloped and suggests "exploiting AI vendor outages to study their impact on information processing and market outcomes" (Sections 3.3 and 4.2.4).
- **Why unresolved:** There is a noted absence of papers in recent special issues utilizing AI events as contextual variation for traditional accounting inquiries.
- **What evidence would resolve it:** Archival studies exploiting the discontinuity of AI availability (e.g., Italy's ChatGPT ban) to measure changes in forecast accuracy, market efficiency, or reporting quality.

### Open Question 3
- **Question:** Can AI-enhanced measurements of disclosure quality (via NLP/transformers) capture theoretical constructs more accurately than manual coding, and do they alter inferences in archival research?
- **Basis in paper:** [Explicit] The paper highlights "AI-enhanced measurement of accounting constructs" as a key opportunity, asking if these novel measures enable "testing theories using traditional econometric methods with richer data than manual coding allows" (Section 4.2.3).
- **Why unresolved:** While technical capability exists, the validity of these AI-generated constructs against established theoretical frameworks and manual benchmarks remains under-tested.
- **What evidence would resolve it:** Validation studies comparing AI-generated construct scores (e.g., sentiment, tone) against human-coded benchmarks and testing their predictive power for market outcomes.

## Limitations
- The classification framework relies heavily on subjective judgment for determining "primary contribution" and "methodological centricity"
- The 71% inter-rater agreement indicates meaningful ambiguity in borderline cases
- The study's exclusive focus on accounting journals may underrepresent cross-disciplinary AI research that could inform accounting applications

## Confidence
- **High confidence:** The empirical finding that AIS journals predominantly publish AI-centric research while non-AIS journals focus on accounting-centric research is well-supported by the data and classification methodology
- **Medium confidence:** The claim about accounting researchers' collaborative advantages is logically sound but relies on assumptions about researcher incentives and the persistence of skill asymmetries
- **Low confidence:** The specific predictions about which research tasks will remain human-dominated versus AI-dominated over the next 4-5 years are highly speculative

## Next Checks
1. **Reproduce classification results:** Apply the framework to a fresh sample of 20 recent AI-accounting papers to test inter-rater reliability and confirm the quadrant distribution pattern holds beyond the original 89-paper sample
2. **Test diagonal gap hypothesis:** Conduct a systematic search for papers using traditional methods to study AI accounting applications (the "AI via Traditional" quadrant) to determine whether the apparent gap represents a real literature deficiency or search bias
3. **Validate task boundaries:** Design an experiment comparing GenAI performance versus human performance on specific research tasks (e.g., literature review synthesis, code generation, result interpretation) to empirically test which tasks genuinely require human scaffolding versus those where AI can operate autonomously