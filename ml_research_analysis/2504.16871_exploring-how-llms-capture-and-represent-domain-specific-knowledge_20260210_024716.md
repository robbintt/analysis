---
ver: rpa2
title: Exploring How LLMs Capture and Represent Domain-Specific Knowledge
arxiv_id: '2504.16871'
source_url: https://arxiv.org/abs/2504.16871
tags:
- hidden
- domain
- states
- https
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores whether Large Language Models (LLMs) encode
  domain-specific knowledge in their hidden states. The authors analyze hidden state
  activations during the prefill phase to identify latent domain-related trajectories
  that capture domain nuances.
---

# Exploring How LLMs Capture and Represent Domain-Specific Knowledge

## Quick Facts
- arXiv ID: 2504.16871
- Source URL: https://arxiv.org/abs/2504.16871
- Reference count: 34
- Primary result: Hidden states during LLM prefill encode domain-specific trajectories that enable 12.3% better model selection accuracy than traditional methods

## Executive Summary
This work investigates whether Large Language Models encode domain-specific knowledge in their hidden states during the prefill phase. The authors extract layer-wise activations from multiple LLM architectures and demonstrate that statistical properties of these hidden states (mean, variance) cluster by domain type across diverse tasks including MMLU, GSM8K, MEDMCQA, CaseHOLD, and PLATO. Experiments show that deeper layers stabilize domain representations, and that autoregressive models outperform bidirectional encoders for this domain classification task. Using these hidden state representations for model selection, the proposed LLM Hidden States Classifier achieves 12.3% higher accuracy than traditional methods, particularly on open-ended reasoning tasks.

## Method Summary
The method extracts hidden states from the last token position across all layers during the prefill phase of autoregressive LLMs. For each layer, mean and variance are computed across batch and dimension axes to create layer-wise traces. An MLP classifier is trained on these traces with domain labels (math, biomedical, law, humanities) using 4,000 samples from MMLU and specialized pools. The approach is tested across multiple models (Phi-3-mini-3.8B, Gemma-2B, Llama2-7B, Mistral-7B) and compared against semantic routing and DeBERTa baseline classifiers.

## Key Results
- Domain-specific clustering in hidden states persists across prompt variations and after fine-tuning
- Deeper layers (16+) produce more stable domain representations than earlier layers
- Autoregressive LLMs outperform bidirectional encoders (DeBERTa) in domain classification accuracy
- LLM Hidden States Classifier achieves 12.3% better accuracy than traditional methods for model selection
- Best performance achieved using layers 16-32, with layer 26 identified as a turning point

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hidden states during the prefill phase encode domain-distinguishable patterns (latent domain-related trajectories) that persist across prompt variations.
- **Mechanism:** During input processing, autoregressive LLMs produce layer-wise activations whose statistical properties (mean, variance/standard deviation) cluster by domain type—e.g., math, biomedical, law, humanities. These trajectories reflect internal context-building rather than surface tokens.
- **Core assumption:** The clustering reflects domain-relevant internal representations, not spurious correlations with dataset artifacts.
- **Evidence anchors:**
  - [abstract] "We reveal latent domain-related trajectories that indicate the model's internal recognition of query domains."
  - [section 5.1, Figure 2] Standard deviation traces show domain-specific clustering across Gemma, Phi, Llama, Mistral for MMLU and specialized datasets (GSM8K, MEDMCQA, CaseHOLD, PLATO).
  - [corpus] Weak direct support; neighbor papers focus on retrieval and embedding methods, not hidden-state probing.
- **Break condition:** If prompts from the same domain with radically different surface forms fail to cluster, or if domain labels don't align with trajectory clusters.

### Mechanism 2
- **Claim:** Deeper layers stabilize domain representations, making later layers more effective for domain discrimination.
- **Mechanism:** Early layers (up to ~layer 16 in Phi-3-mini) show prompt-sensitive variation; layers 16+ produce more stable domain-specific traces, suggesting domain abstraction occurs progressively through the network.
- **Core assumption:** Layer-wise trajectory convergence implies domain-general representation, not memorization.
- **Evidence anchors:**
  - [section 5.2] "From layer 16 onward, the traces stabilize across different prompts, indicating that the deeper layers are responsible for maintaining domain-specific representations."
  - [section 5.4, Figure 4] Layer 26 identified as a turning point; performance improves when later layers are included in the MLP classifier.
  - [corpus] Not directly addressed in neighbor papers.
- **Break condition:** If tasks requiring fine-grained domain nuance fail to improve with deeper layer inclusion, or if early layers outperform deeper ones for certain domains.

### Mechanism 3
- **Claim:** Autoregressive LLMs encode domain traces that bidirectional encoders (e.g., DeBERTa) do not capture comparably.
- **Mechanism:** Autoregressive models' sequential context-building yields domain-sensitive activation patterns; bidirectional encoders, optimized for semantic/positional representation, produce less discriminative hidden states for this task.
- **Core assumption:** The difference is architectural, not merely due to scale or training data.
- **Evidence anchors:**
  - [section 5.1] "The traces produced by the DeBERTa model did not exhibit a clear pattern, unlike autoregressive LLMs."
  - [section 5.3, Table 2] DeBERTa Hidden States Classifier achieves 0.313 avg accuracy vs. 0.395 for LLM Hidden States Classifier.
  - [corpus] Not directly tested in neighbors; related work on domain-specific embeddings focuses on contrastive/generative methods, not hidden-state probing.
- **Break condition:** If a well-tuned bidirectional encoder matches or exceeds autoregressive hidden-state performance on the same probing task.

## Foundational Learning

- **Concept: Hidden states (intermediate activations)**
  - Why needed here: The entire method depends on extracting and interpreting layer-wise activations during the prefill phase.
  - Quick check question: Can you explain where hidden states are located in a transformer, and why the last token's hidden state is used for probing?

- **Concept: Prefill phase vs. generation phase**
  - Why needed here: Domain trajectories are extracted before generation; mixing phases confounds interpretation.
  - Quick check question: What is the prefill phase in autoregressive LLMs, and why might hidden states differ between prefill and generation?

- **Concept: Probing mechanisms**
  - Why needed here: The paper uses an MLP classifier to probe domain information from hidden states.
  - Quick check question: What does a probing classifier test, and what are its limitations for causal claims?

## Architecture Onboarding

- **Component map:**
  Input query → LLM (Gemma/Phi/Llama/Mistral) → Hidden states per layer (last token) → Mean/variance computation → MLP classifier → Domain prediction → Model/router selection

- **Critical path:**
  1. Extract hidden states for all layers from the last token during prefill
  2. Compute mean/variance across batch and dimension axes per layer
  3. Train MLP on hidden-state trajectories with domain labels (4 classes: math, biomedical, law, humanities)
  4. Evaluate on held-out datasets (GSM8K, MATH, MEDMCQA, USMLE, CaseHOLD)
  5. Route query to best-matching model based on predicted domain

- **Design tradeoffs:**
  - Using fewer layers reduces latency but drops accuracy (layer 26+ needed for open-ended tasks like GSM8K)
  - MLP classifier requires labeled domain data; semantic routing needs fewer-shot utterances but underperforms
  - Hidden-state approach needs white-box access; semantic routing works with black-box APIs

- **Failure signatures:**
  - Early-layer-only classifiers fail on open-ended reasoning tasks
  - DeBERTa hidden-state classifier fails to match autoregressive LLM performance
  - Overlapping domain traces (e.g., math and biomedical) may cause confusion in routing

- **First 3 experiments:**
  1. Replicate standard deviation traces on a subset of MMLU domains for a single model (e.g., Phi-3-mini) to verify domain clustering
  2. Train MLP classifier on hidden states from layers 1–32 and compare accuracy vs. layers 1–26 and vs. DeBERTa classifier
  3. Test routing accuracy on GSM8K and MEDMCQA using the hidden-state classifier vs. semantic router, measuring accuracy improvement and failure cases

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: Do latent domain-related trajectories scale effectively to larger language models beyond 7B parameters?
  - Basis in paper: [explicit] Section 6 states: "Our work primarily focuses on interpreting small LLMs (up to 7B parameters)... the applicability of our approach to larger models remains to be explored."
  - Why unresolved: The study only tested models up to 7B parameters; larger models may have fundamentally different internal representations or domain encoding mechanisms.
  - What evidence would resolve it: Testing the hidden state classification approach on larger models (70B+ parameters) across the same domain classification tasks to verify if the 12.3% improvement over baselines persists.

- **Open Question 2**
  - Question: Do domain-specific hidden state traces generalize across different datasets that share the same domain label?
  - Basis in paper: [explicit] Section 6 states: "the 'clustering' might not generalize to other datasets sharing the same domain label."
  - Why unresolved: The observed clustering may reflect specific characteristics of the tested datasets (MMLU, GSM8K, MEDMCQA, etc.) rather than fundamental domain properties that transfer to new data distributions.
  - What evidence would resolve it: Testing the classifier on entirely new domain-specific datasets outside the MMLU/GSM8K/MEDMCQA/CaseHOLD/Plato families to assess whether domain separation remains consistent.

- **Open Question 3**
  - Question: What explains the observed overlap between mathematical and biomedical domain representations in hidden state space?
  - Basis in paper: [explicit] Footnote 1 states: "the relationship between the domains is not entirely clear. For example, initial observations show that mathematical and biomedical domains are closely related." Appendix A.5 provides further discussion.
  - Why unresolved: The paper documents this overlap empirically but does not determine whether it stems from similar reasoning patterns, training data correlations, or other structural factors.
  - What evidence would resolve it: Controlled experiments decoupling reasoning patterns from domain content, or systematic analysis of pretraining data composition to identify mathematical-biomedical correlations.

## Limitations
- Domain clustering may reflect dataset artifacts rather than genuine domain abstraction
- Probing methodology cannot distinguish domain information from mere correlation
- Experiments limited to four broad domains, unclear if approach generalizes to more granular or overlapping domains
- Architectural claims about autoregressive vs. bidirectional models need more systematic ablation studies

## Confidence
**High Confidence:** The observation that deeper layers (16+) show more stable domain-specific traces than earlier layers is well-supported by layer-wise analysis and performance improvement with later layers.

**Medium Confidence:** The claim that hidden-state-based routing improves model selection accuracy by 12.3% is supported by experimental results but depends on specific datasets and baseline comparisons.

**Low Confidence:** The mechanism by which hidden states encode domain information remains speculative, and the generalization to domains beyond the four tested is uncertain.

## Next Checks
1. **Dataset Artifact Test:** Create synthetic prompts within the same domain that have minimal lexical overlap with training data but maintain semantic domain content. If the classifier still correctly routes these, it suggests the model captures genuine domain abstraction rather than dataset artifacts.

2. **Architectural Ablation:** Systematically compare autoregressive models with varying context lengths, attention mechanisms, and training objectives to isolate which architectural features contribute to domain discrimination. Include multiple bidirectional encoder variants to determine if the observed difference is truly architectural or due to other factors.

3. **Cross-Domain Overlap Test:** Design experiments with domains that have substantial conceptual overlap (e.g., biomedical and law in medical malpractice cases) to test whether the hidden-state classifier can distinguish nuanced domain boundaries or conflates overlapping domains, and whether this affects routing accuracy.