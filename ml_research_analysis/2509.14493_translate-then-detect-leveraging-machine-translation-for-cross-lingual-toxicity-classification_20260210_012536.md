---
ver: rpa2
title: 'Translate, then Detect: Leveraging Machine Translation for Cross-Lingual Toxicity
  Classification'
arxiv_id: '2509.14493'
source_url: https://arxiv.org/abs/2509.14493
tags:
- translation
- toxicity
- language
- languages
- translate-classify
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cross-lingual toxicity detection is challenging due to limited
  training data for many languages. This work compares translation-based approaches
  (translate-test) against language-specific and multilingual classifiers.
---

# Translate, then Detect: Leveraging Machine Translation for Cross-Lingual Toxicity Classification

## Quick Facts
- arXiv ID: 2509.14493
- Source URL: https://arxiv.org/abs/2509.14493
- Reference count: 30
- Primary result: Translation-based pipelines outperform out-of-distribution classifiers in 81.3% of cases across 17 languages

## Executive Summary
This work evaluates whether translating non-English text to English before classification provides a viable alternative to direct multilingual or language-specific toxicity detection. Testing 27 pipelines across 17 languages, the study finds that translation-based approaches consistently outperform out-of-distribution classifiers, with benefits scaling with language resource level and translation quality. While traditional classifiers beat LLM judges, especially for low-resource languages, MT-specific fine-tuning on LLMs reduces refusal rates but shows resource-dependent accuracy trade-offs. The findings establish translation-based pipelines as a robust baseline for scalable multilingual toxicity detection.

## Method Summary
The paper evaluates 27 cross-lingual toxicity detection pipelines across 17 languages, comparing translation-based approaches against language-specific and multilingual classifiers. Four translation systems are tested (NLLB, GPT-4o, Llama 3.1, and an MT-SFT fine-tuned Llama variant), paired with both fixed multilingual classifiers and best-available English classifiers. Evaluation uses ROC-AUC scores and refusal rates across the Jigsaw Unintended Bias in Toxicity Classification dataset and its multilingual extensions. The study specifically examines whether translation benefits correlate with language resource level and translation quality, and whether MT-SFT fine-tuning on LLMs improves pipeline performance.

## Key Results
- Translation-based pipelines outperform out-of-distribution classifiers in 81.3% of cases (13 of 16 languages)
- Translation benefits strongly correlate with both target language resource level and machine translation quality
- MT-specific fine-tuning on LLMs yields lower refusal rates but can negatively impact accuracy for low-resource languages
- Traditional classifiers outperform LLM judges, particularly for low-resource languages

## Why This Works (Mechanism)

### Mechanism 1: Translation Bridges to Higher-Quality English Classifiers
Translating non-English text to English enables use of well-resourced English toxicity classifiers that outperform multilingual classifiers operating out-of-distribution. This approach leverages English classifiers' superior training data and mature annotation schemas, bypassing data scarcity in target languages. The core assumption is that translation preserves toxicity-relevant semantic and pragmatic properties. Evidence shows translate-classify wins in 13/16 languages versus OOD classification, though effectiveness degrades when translation quality is poor or when toxicity encodes language-specific cultural markers.

### Mechanism 2: Translation Benefit Scales with Resource Level and MT Quality
The performance gain from translate-classify pipelines increases with both target language resource level and translation system quality. Higher-resource languages have better MT systems with more parallel training data, producing more accurate translations that preserve toxicity signals. This creates compounding advantage: good translation plus good English classifier yields strong detection. COMETKiwi quality estimation scores correlate with preservation of toxicity-relevant content. Translation benefit increases with both FineWeb-2 document counts and translation quality scores, though neither approach may be adequate for very low-resource languages where both MT quality and direct classifier performance are poor.

### Mechanism 3: MT-SFT Reduces Refusal Rates but Creates Resource-Dependent Trade-offs
Supervised fine-tuning of LLMs on machine translation data reduces refusal rates (declining to translate toxic content), improving pipeline coverage, but accuracy gains concentrate in higher-resourced languages. Safety-tuned instruction LLMs refuse to process toxic content, breaking the translate-classify pipeline. MT-SFT shifts model behavior toward translation task completion, reducing safety-triggered refusals. However, translation quality improvements are themselves resource-dependent. MT-SFT achieves 100% TNR (no refusals) in sampled languages, but the accuracy impact varies by language resource, showing potential negative effects for low-resource languages.

## Foundational Learning

- **Concept: Cross-lingual Transfer via Translate-Test**
  - **Why needed here:** The entire paper evaluates whether translating inputs to a resource-rich language (English) before classification is effective for toxicity detection
  - **Quick check question:** If you have a high-quality English classifier but no training data for Finnish toxicity, would you translate Finnish→English→classify or train a multilingual classifier on available data?

- **Concept: Out-of-Distribution (OOD) vs In-Distribution (ID) Evaluation**
  - **Why needed here:** The paper distinguishes classifiers operating ID (trained on same domain/language as test) vs OOD (trained on different distribution). Translate-classify is compared primarily against OOD baselines
  - **Quick check question:** A classifier trained on Twitter toxicity data is evaluated on Wikipedia comments—is this ID or OOD evaluation?

- **Concept: LLM Refusal Behavior in Safety-Critical Contexts**
  - **Why needed here:** Safety-aligned LLMs may refuse to process toxic content, which breaks the translate-classify pipeline. Understanding refusal rates and mitigation is critical for deployment
  - **Quick check question:** If an LLM refuses to translate 20% of toxic inputs, what percentage of toxic content will your pipeline fail to detect?

## Architecture Onboarding

- **Component map:** Input (source language) → MT System (LLM or NMT) → English Classifier → Toxicity Score

- **Critical path:** MT system quality and refusal behavior. If translation fails or refuses, downstream classifier receives nothing or a refusal message. English classifier quality is a secondary bottleneck but more controllable.

- **Design tradeoffs:**
  - LLM vs NMT translation: LLMs (GPT-4o, Llama 3.1) show less sensitivity to language resources; NLLB shows higher sensitivity but may be more consistent for supported languages
  - Standard vs MT-SFT LLM: MT-SFT reduces refusals (100% coverage in tested samples) but shows resource-dependent accuracy gains
  - Classifier selection: Fixed multilingual classifier (distilbert-base-multilingual) vs best-available English classifier (toxic-bert, multilingual-toxic-xlm-roberta)

- **Failure signatures:**
  - High refusal rates (>5%) for safety-tuned LLM translators on certain language/content combinations
  - AUC drops below 0.5 for very low-resource languages (Amharic, Kannada, Telugu, Malayalam show OOD AUC ~0.38-0.49)
  - Translation quality scores below 0.7 COMETKiwi correlate with reduced or negative translation benefit

- **First 3 experiments:**
  1. Establish baseline: Run best-available multilingual classifier on your target language dataset; measure AUC and identify if operating ID or OOD
  2. Translation quality check: Translate test set to English using candidate MT systems; score with COMETKiwi or human review on toxicity preservation
  3. Refusal rate audit: If using LLM translator, sample 100-200 inputs including known toxic examples; measure refusal rate with Minos or manual annotation

## Open Questions the Paper Calls Out

The paper explicitly notes that toxicity is culturally grounded and that datasets use different annotation schemas, so findings represent trends rather than universal claims. The effectiveness of translate-classify pipelines beyond the 17 evaluated languages remains uncertain, particularly for languages with different script families or cultural contexts.

## Limitations

- Findings are constrained by specific 17 languages and 27 evaluation pipelines tested; effectiveness may vary for languages outside this set
- Reliance on English classifiers assumes English captures universal toxicity features, which may not hold for culture-specific toxic expressions
- Comparison against OOD classifiers (rather than true zero-shot multilingual models) may overstate translation benefits when robust multilingual models exist

## Confidence

- **High confidence:** Translation-based pipelines outperforming OOD classifiers (81.3% win rate across 16 languages, Figure 2)
- **Medium confidence:** Correlation between translation benefit and resource level/MT quality (Figures 4-5 show clear trends but resource level proxy may have noise)
- **Medium confidence:** MT-SFT reducing refusal rates while showing resource-dependent accuracy trade-offs (Figure 8 shows consistent refusal reduction; accuracy impact varies by language resource)

## Next Checks

1. **Cultural generalization test:** Evaluate translation-based toxicity detection on languages with significantly different cultural contexts from the 17 studied (e.g., Thai, Vietnamese, Swahili) to test cultural transferability assumptions.
2. **Zero-shot multilingual comparison:** Compare translate-classify pipelines against state-of-the-art zero-shot multilingual toxicity classifiers to establish when translation remains advantageous.
3. **Refusal bias audit:** For safety-tuned LLM translators, analyze whether reduced refusal rates correlate with systematic acceptance of certain toxic content types (e.g., coded language, euphemisms) that human moderators might flag.