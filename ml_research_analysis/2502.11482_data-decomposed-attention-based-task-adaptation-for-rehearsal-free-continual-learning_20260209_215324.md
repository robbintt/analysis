---
ver: rpa2
title: 'DATA: Decomposed Attention-based Task Adaptation for Rehearsal-Free Continual
  Learning'
arxiv_id: '2502.11482'
source_url: https://arxiv.org/abs/2502.11482
tags:
- data
- tasks
- task
- learning
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Decomposed Attention-based Task Adaptation (DATA),
  a method to address catastrophic forgetting and plasticity loss in continual learning
  for large language models. DATA uses high-rank and low-rank adapters to separately
  capture task-specific and task-shared knowledge, dynamically adjusting their weights
  based on task relevance.
---

# DATA: Decomposed Attention-based Task Adaptation for Rehearsal-Free Continual Learning

## Quick Facts
- arXiv ID: 2502.11482
- Source URL: https://arxiv.org/abs/2502.11482
- Reference count: 40
- Primary result: Achieves state-of-the-art performance on continual learning benchmarks with 0.5% forgetting, outperforming existing methods without data rehearsal

## Executive Summary
This paper proposes DATA, a rehearsal-free continual learning method for large language models that addresses catastrophic forgetting and plasticity loss. DATA uses high-rank and low-rank adapters to separately capture task-specific and task-shared knowledge, dynamically adjusting their weights based on task relevance. The method introduces a decomposed component weighting strategy with learnable components that generate attention-based weights to fuse knowledge from each adapter. Experiments on three benchmarks demonstrate that DATA significantly outperforms existing methods, achieving state-of-the-art performance while maintaining model plasticity and reducing forgetting.

## Method Summary
DATA extends low-rank adaptation (LoRA) by introducing dual-rank adapters (high-rank and low-rank) to separately capture task-specific and task-shared knowledge. The method employs a decomposed component weighting strategy that uses attention-based weights to dynamically fuse outputs from both adapter branches. During training, existing components are frozen while new components are added for each task, with orthogonality regularization and stochastic restoration applied to maintain performance. The architecture operates without requiring data rehearsal, making it computationally efficient for continual learning scenarios.

## Key Results
- Achieves 0.5% forgetting on Standard CL Benchmark, outperforming SeqLoRA (5.8%)
- Maintains final average performance (FP) of 79.5% on LLaMA2-7B across 4 tasks
- Reduces average performance drop from 4.8% to 0.5% compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1: Rank-Based Knowledge Decomposition
Low-rank adapters preferentially capture task-shared knowledge while high-rank adapters capture task-specific knowledge, reducing interference during sequential task learning. Low-rank adapters constrain the parameter space, limiting adaptation to shared features that transfer across tasks, while high-rank adapters provide sufficient capacity to model unique task distributions.

### Mechanism 2: Decomposed Component Weighting via Attention
Learnable weight components with attention-based query-key matching enable sample-adaptive fusion of high-rank and low-rank adapter outputs. Weight components are combined via weighted summation where weights are computed as cosine similarity between attended queries and keys, with attention vectors acting as learnable feature selectors.

### Mechanism 3: Orthogonal Expansion with Stochastic Restoration
Freezing old components and adding orthogonally-initialized new components prevents knowledge overwriting, while stochastic restoration of pretrained weights maintains plasticity. For each new task, new components are trained while previous components remain frozen, with orthogonality loss penalizing interference and stochastic restoration occasionally resetting parameters to source weights.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**: Understanding LoRA's $h' = W_0x + BAx$ formulation is prerequisite to grasping how DATA modifies attention layers. Quick check: Can you explain why LoRA keeps $W_0$ frozen and how rank $r$ affects the trade-off between parameter efficiency and representation capacity?

- **Stability-Plasticity Dilemma in Continual Learning**: DATA's core motivation is balancing forgetting (stability) against new task learning (plasticity). Quick check: Given AP=80.3 and FP=79.8 for DATA on LLaMA2-7B, what does Forget=0.5% indicate about the stability-plasticity balance compared to SeqLoRA (Forget=5.8%)?

- **Attention-Based Query-Key Retrieval**: The decomposed component weighting strategy uses cosine similarity between attended queries and keys to compute fusion weights. Quick check: How does element-wise multiplication of query $q(x)$ by attention vector $A_m$ before similarity computation differ from standard dot-product attention, and why might this benefit continual learning?

## Architecture Onboarding

- **Component map**: Input x → Original Linear Layer (frozen) → f_o → Low-Rank DATA (rank=dl, e.g., 2): B1→A1 → f_l → High-Rank DATA (rank=dh, e.g., 32): B2→A2 → f_h → Output: f_x = f_o + λ_h × f_h + λ_l × f_l

- **Critical path**: 1. Initialize high-rank (rank=8) and low-rank (rank=2) adapters with orthogonal constraints 2. For each task, expand weight components, keys, attention vectors by M/N 3. Freeze all previous components; train only new components plus adapter weights 4. Every 200 steps, apply stochastic restoration to reset parameters toward W_0 5. At inference, reparameterize adapters into base model

- **Design tradeoffs**: Rank selection (2,8 optimal), component expansion rate M/N, orthogonality weight β=10, restoration probability p (every 200 steps)

- **Failure signatures**: High forgetting on similar tasks, poor plasticity on new domains, divergent loss during training, inference latency spikes

- **First 3 experiments**: 1. Reproduce Order-1 on Standard CL Benchmark with LLaMA2-7B 2. Validate rank sensitivity across pairs (2,8), (2,16), (4,8), (4,16) 3. Test stochastic restoration timing at steps 100, 200, 500 on Long Sequence Benchmark

## Open Questions the Paper Calls Out

- Can dynamically adaptive selection criteria improve the efficacy of stochastic restoration compared to the current fixed-step approach?
- How does DATA's architectural overhead and performance stability scale when applied to significantly larger models (e.g., 70B+ parameters)?
- To what extent does the decomposition of high-rank and low-rank knowledge hold up under significant domain shifts or out-of-distribution (OOD) data?

## Limitations
- Unknown query function q(x) for attention weight computation
- Unspecified batch size, optimizer type, weight decay, and gradient clipping
- Limited evaluation on larger models (13B, 72B) due to computational constraints
- Performance on tasks with significant domain shifts or OOD data remains unexplored

## Confidence
- **High confidence**: Rank-based knowledge decomposition and orthogonality regularization mechanisms are well-supported by ablation studies
- **Medium confidence**: Decomposed component weighting strategy shows strong empirical results but lacks rigorous theoretical grounding
- **Low confidence**: Stochastic restoration mechanism's optimal scheduling appears heuristic rather than systematically derived

## Next Checks
1. Implement DATA with q(x) from different layers (12, 24, 0) to determine optimal query location
2. Systematically vary M/N expansion ratio (0.5, 1, 2, 4) while tracking orthogonality loss and performance
3. Measure contribution ratios λ_h/λ_l for each task in Long Sequence Benchmark to validate rank-task mapping hypothesis