---
ver: rpa2
title: Maximum Likelihood Learning of Latent Dynamics Without Reconstruction
arxiv_id: '2505.23569'
source_url: https://arxiv.org/abs/2505.23569
tags:
- rp-gssm
- learning
- latent
- generative
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Recognition-Parametrized Gaussian State
  Space Model (RP-GSSM), a novel unsupervised learning method for time series data
  with latent dynamical structure. Unlike traditional generative approaches that learn
  both recognition and generative models, or contrastive methods that learn only recognition,
  the RP-GSSM implicitly specifies a degenerate decoder and focuses model capacity
  on inference of latents.
---

# Maximum Likelihood Learning of Latent Dynamics Without Reconstruction

## Quick Facts
- arXiv ID: 2505.23569
- Source URL: https://arxiv.org/abs/2505.23569
- Authors: Samo Hromadka; Kai Biegun; Lior Fox; James Heald; Maneesh Sahani
- Reference count: 24
- One-line primary result: RP-GSSM learns task-relevant latent dynamics without reconstruction, outperforming baselines on pendulum and tracking tasks with background distractors

## Executive Summary
The paper introduces the Recognition-Parametrized Gaussian State Space Model (RP-GSSM), a novel unsupervised learning method for time series data with latent dynamical structure. Unlike traditional generative approaches that learn both recognition and generative models, or contrastive methods that learn only recognition, the RP-GSSM implicitly specifies a degenerate decoder and focuses model capacity on inference of latents. The method combines the intuition of contrastive approaches with the flexible tools of probabilistic generative models while maintaining exact inference through its jointly Gaussian latent prior.

The RP-GSSM achieves superior performance on various tasks including learning nonlinear stochastic dynamics from video, with or without background distractors. On pendulum and double pendulum tasks, it outperforms baseline methods (SVAE, KVAE, DKF variants, and CPC) in recovering accurate latent representations. Notably, it demonstrates robustness to background distractions, successfully filtering out irrelevant features while maintaining accurate state inference. On tracking tasks, the RP-GSSM is the only method that consistently tracks object states in the presence of correlated background videos.

## Method Summary
The RP-GSSM is a latent variable model for time series data that combines a linear-Gaussian prior over latents with a recognition network that maps observations to Gaussian potentials in latent space. Unlike VAEs, it uses a degenerate implicit decoder that cannot generate observations, eliminating the need for reconstruction while still enabling maximum likelihood training. The model employs exact inference via Kalman smoothing, leveraging the jointly Gaussian structure of the prior and recognition potentials. Training proceeds via Expectation-Maximization, where the E-step computes exact posteriors and the M-step maximizes a tractable auxiliary free energy using gradient ascent. The transition matrix is constrained to be stable (singular values clipped to (0, 1-ε)) to prevent exploding latent trajectories.

## Key Results
- Achieves R² scores of 0.5-1.0 across various tasks, consistently outperforming baselines
- Demonstrates robustness to background distractions, filtering out irrelevant features while maintaining accurate state inference
- Only method that consistently tracks object states in the presence of correlated background videos
- Shows better data efficiency, maintaining performance when trained on fewer sequences
- Provides exact inference without Monte Carlo approximations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Eliminating explicit decoders prevents latent contamination by observation-irrelevant features.
- Mechanism: VAE losses encode all observable variation into latents—including distractors—because reconstruction requires regenerating backgrounds. RP-GSSM replaces the generative decoder with a degenerate implicit model that can only "generate" training data, freeing capacity for recognition-only learning. The objective aligns recognition outputs with the latent prior structure rather than pixel-level reconstruction.
- Core assumption: Task-relevant structure manifests as temporal statistical dependence across observations; distractors exhibit unreliable or independent statistics across time.
- Evidence anchors:
  - [abstract] "the RP-GSSM has no need for an explicit network mapping from latents to observations, allowing it to focus model capacity on inference of latents"
  - [Section 1] "VAE-based losses can be interpreted as regularized reconstruction objectives, which encourage latents to encode information about all aspects of the data, including irrelevant distractor features"
  - [corpus] Related work on likelihood-free VAEs (arXiv:2504.17622) similarly addresses likelihood misspecification issues but via different mechanism
- Break condition: If distractors exhibit the same temporal dependence structure as signal, the conditional independence assumption provides no separation signal.

### Mechanism 2
- Claim: Gaussian latent prior with linear dynamics enables exact posterior inference regardless of recognition network nonlinearity.
- Mechanism: The prior p(z) remains jointly Gaussian with linear transitions. The recognition network outputs Gaussian potentials f_ϕ(z_t|x_t) that serve as pseudo-emissions in a linear-Gaussian system. Kalman smoothing then computes exact posteriors q(z|x) in closed form, avoiding variational approximation gaps that bias learning.
- Core assumption: Linear dynamics in latent space can capture relevant structure, possibly requiring higher dimensionality for nonlinear systems (Koopman-style embedding).
- Evidence anchors:
  - [abstract] "it admits exact inference thanks to its jointly Gaussian latent prior, while maintaining expressivity with an arbitrarily nonlinear neural network link between observations and latents"
  - [Section 3] "As we define the f∆ϕ distributions on zt to be Gaussian, qn(zt) can be computed exactly via standard Kalman smoothing"
  - [corpus] Weak direct corpus evidence; related work (LatentTrack arXiv:2602.00458) uses similar filtering-in-latent-space approach
- Break condition: If true latent dynamics require nonlinear structure that cannot be embedded into higher-dimensional linear systems, the model will underfit dynamics.

### Mechanism 3
- Claim: Maximum likelihood training on the RPM joint provides principled learning signal without auxiliary losses.
- Mechanism: The RPM joint ˜p_θ(x,z) is a normalized distribution learned via EM. The E-step computes exact posteriors; the M-step maximizes the auxiliary free energy G via gradient ascent on recognition parameters. No contrastive negative sampling, masking, or regularization schedules are needed.
- Core assumption: The interior variational bound (approximating F_ϕ(z_t) ≈ p_η(z_t)) introduces negligible bias in practice.
- Evidence anchors:
  - [abstract] "learn task-relevant latents without ad-hoc regularization, auxiliary losses, or optimizer scheduling"
  - [Section 3] "The RP-GSSM is trained via standard EM on G... as all terms in G can be computed exactly, the M-step is achieved by gradient ascent"
  - [corpus] VJEPA (arXiv:2601.14354) similarly moves from deterministic to probabilistic latent objectives but uses predictive rather than recognition-parametrized framing
- Break condition: If the variational gap from the interior bound approximation is large (F_ϕ far from prior), learned posteriors may be miscalibrated.

## Foundational Learning

- Concept: Kalman smoothing for Gaussian state space models
  - Why needed here: Core inference engine; must understand forward-backward message passing, how Gaussian potentials combine, and why linearity enables closed-form solutions
  - Quick check question: Given p(z_t|z_{t-1}) = N(Az_{t-1}, Q) and observation potential N(μ_t, Σ_t), can you write the forward message update?

- Concept: Variational EM and free energy bounds
  - Why needed here: Training objective is a variational lower bound; need to distinguish E-step (posterior inference) from M-step (parameter update) and understand KL divergence terms
  - Quick check question: Why does maximizing F(q,θ) with respect to q minimize KL(q||p(z|x))?

- Concept: Conditional independence in graphical models
  - Why needed here: RPM exploits x_i ⊥ x_j | z; learning signal comes from finding latents that render observations conditionally independent
  - Quick check question: In an SSM with Markovian latents, which conditional independencies hold in the joint p(x_1:T, z_1:T)?

## Architecture Onboarding

- Component map: Recognition network f_ϕ -> Kalman smoother -> Posteriors q(z|x) -> Free energy G -> Gradient updates to ϕ and A

- Critical path:
  1. Forward pass: Batch observations through recognition network → Gaussian potentials
  2. E-step: Run Kalman smoothing with potentials → posteriors q(z)
  3. Compute auxiliary free energy G (closed-form with Gaussian integrals)
  4. M-step: Gradient ascent on G w.r.t. {ϕ, A}
  5. Enforce stability: Clip singular values of A to (0, 1-ε)

- Design tradeoffs:
  - Latent dimensionality D_Z: Higher dimensions capture nonlinear dynamics via Koopman embedding but increase compute; paper uses 4-32 across tasks
  - Recognition covariance parametrization: Full Cholesky vs. diagonal; data-dependent vs. constant; affects expressivity and stability
  - Interior bound approximation: g_tn(z_t) = q(z_t)/p(z_t) vs. learned auxiliary factors; paper finds approximation sufficient

- Failure signatures:
  - Posteriors collapse to prior: Recognition outputs may become uninformative; check if Σ_ϕ(x_t) → I
  - Instability: A violating stability constraint causes exploding latent trajectories
  - Distractor leakage: If distractors have temporal structure matching signal, latents will encode both
  - Numerical issues in Γ computation: Mixture-of-Gaussians normalization requires careful implementation

- First 3 experiments:
  1. Linear system sanity check: Generate data from known linear-Gaussian SSM, verify RP-GSSM recovers ground-truth A matrix and achieves R² > 0.9 on linear decoding
  2. Pendulum without distractors: Train on synthetic pendulum video, confirm latent dimension ≥ 4 recovers sin(θ) and ω via linear regression
  3. Ablation on recognition covariance: Compare diagonal vs. full covariance parametrization on CIFAR10-Pendulum; expect full covariance helps with distractor rejection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the RP-GSSM framework be extended to switching dynamical systems with discrete latent modes governing state transitions?
- Basis in paper: [explicit] "One future direction of work is to extend the RP-GSSM to switching dynamical systems (Dong et al., 2020)."
- Why unresolved: Switching systems introduce discrete latent variables that would require modifications to the jointly Gaussian prior structure, potentially breaking the exact inference guarantees via Kalman smoothing that RP-GSSM currently enjoys.
- What evidence would resolve it: A formulation showing how discrete switching states could integrate with the Gaussian latent prior while maintaining tractable inference, along with empirical comparisons on datasets with known switching dynamics.

### Open Question 2
- Question: How can control inputs be integrated into the RP-GSSM latent dynamical system for applications in optimal control and model-based reinforcement learning?
- Basis in paper: [explicit] "Another [future direction] is to integrate control inputs into the latent dynamical system, which can be applied to simultaneous localization and mapping (Çatal et al., 2021), optimal control (Watson et al., 2019), and model-based reinforcement learning (Levine, 2018)."
- Why unresolved: The current formulation assumes observations arise from autonomous latent dynamics without external control signals. Adding control inputs would require modifying both the transition model and the recognition factor structure.
- What evidence would resolve it: A modified RP-GSSM formulation incorporating control-conditioned transitions, demonstrated on control tasks showing improved performance over baselines that lack structured latent dynamics.

### Open Question 3
- Question: Does incorporating additional data modalities (sound, text, reward) strengthen the RP-GSSM's ability to disregard irrelevant features through increased conditional independence constraints?
- Basis in paper: [explicit] "Finally, additional data modalities such as sound, text, or reward can be incorporated into the graphical model. The resulting increase in the number of conditional independence relationships in the graph would strengthen the RP-GSSM learning signal, improving its ability to disregard irrelevant features."
- Why unresolved: This is proposed as a hypothesis but not tested. Multi-modal data may introduce conflicting inductive biases or require different recognition architectures per modality.
- What evidence would resolve it: Experiments on multi-modal time series datasets showing quantitative improvements in latent recovery R² scores when additional modalities are added, particularly in the presence of distractors in individual modalities.

### Open Question 4
- Question: How does the interior variational bound approximation affect the quality of learned representations compared to exact computation of the intractable ⟨log Fϕt(zt)⟩qn(zt) term?
- Basis in paper: [inferred] The paper adopts the interior variational bound rather than Monte Carlo approximations for tractability, assuming F_ϕ(z_t) ≈ p_η(z_t). The variational gap introduced by this approximation is discussed but its empirical impact on representation quality is not isolated experimentally.
- Why unresolved: The bound is necessary for tractability but introduces a gap between the optimized objective and true likelihood. The sensitivity of learned latents to this approximation remains unquantified.
- What evidence would resolve it: Ablation studies comparing latents learned with different approximation strategies (interior bound vs. Monte Carlo with varying sample sizes) on benchmark tasks, measuring both computational cost and R² recovery scores.

## Limitations

- The method's performance on real-world videos with complex, structured background dynamics remains untested
- The interior variational bound approximation could introduce bias when recognition distributions differ substantially from the prior
- The core assumption that distractors lack temporal structure of signal may not hold in many real-world scenarios

## Confidence

- High confidence: Exact inference through Kalman smoothing (well-established theory), superiority on synthetic pendulum tasks with known ground truth
- Medium confidence: Robustness to background distractors (tested on controlled synthetic distractors), data efficiency claims (based on limited comparison)
- Low confidence: Generalizability to real-world video data with unstructured background dynamics, performance in multi-object tracking scenarios

## Next Checks

1. Test RP-GSSM on real video datasets (e.g., KTH action dataset) with natural background variation to validate robustness beyond synthetic distractors
2. Compare RP-GSSM against state-of-the-art contrastive methods (CURL, SimSiam) on the same tracking tasks to isolate the benefit of exact inference vs. contrastive learning
3. Evaluate performance when distractors exhibit structured temporal dynamics similar to the signal to identify breaking conditions for the conditional independence assumption