---
ver: rpa2
title: 'Agnostic Learning under Targeted Poisoning: Optimal Rates and the Role of
  Randomness'
arxiv_id: '2506.03075'
source_url: https://arxiv.org/abs/2506.03075
tags:
- poisoning
- learning
- learner
- distribution
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies instance-targeted poisoning attacks in the\
  \ agnostic learning setting, where an adversary corrupts a fraction of the training\
  \ data to cause failure on a specific test point. In the realizable setting, prior\
  \ work showed optimal error scales as \u0398(d\u03B7), where d is the VC dimension."
---

# Agnostic Learning under Targeted Poisoning: Optimal Rates and the Role of Randomness

## Quick Facts
- **arXiv ID:** 2506.03075
- **Source URL:** https://arxiv.org/abs/2506.03075
- **Authors:** Bogdan Chornomaz; Yonatan Koren; Shay Moran; Tom Waknine
- **Reference count:** 12
- **Primary Result:** In the agnostic setting, optimal excess error under targeted poisoning scales as Θ̃(√dη), where d is VC dimension and η is poisoning fraction.

## Executive Summary
This paper resolves a fundamental open problem in adversarial machine learning by establishing optimal rates for targeted poisoning attacks in the agnostic learning setting. While prior work showed deterministic learners can be driven to nearly 100% error with small poisoning budgets, this paper demonstrates that randomized learners can achieve significantly better guarantees. The key finding is that optimal excess error scales as Θ̃(√dη), which is independent of the underlying distribution - a surprising result that challenges conventional wisdom about the importance of distributional assumptions in learning theory.

The authors show that randomness is essential for achieving these bounds, but remarkably, these guarantees hold even when the adversary observes the learner's random bits (public randomness). This work bridges concepts from differential privacy, online learning, and information theory to provide both upper and lower bounds that match up to logarithmic factors, establishing a complete characterization of the problem.

## Method Summary
The paper establishes both upper and lower bounds on excess error under targeted poisoning in the agnostic setting. For the upper bound, the authors design a randomized learner that samples hypotheses with probability proportional to exp(-λ·empirical loss), drawing connections to multiplicative weights algorithms and differentially private mechanisms. This learner achieves excess error of Õ(√dη).

For the lower bound, the authors introduce the poisoned coin guessing problem and reduce it to the agnostic learning setting via a direct-sum argument over VC classes. They show that any learner must incur excess error of Ω̃(√dη) infinitely often for some distribution, even when the adversary has access to public randomness. The analysis leverages techniques from information theory and statistical learning theory to establish these fundamental limits.

## Key Results
- Optimal excess error in agnostic targeted poisoning scales as Θ̃(√dη), independent of the underlying distribution
- Deterministic learners are fundamentally vulnerable, achieving error close to 1 even with small poisoning budgets
- Randomized learners with public randomness achieve the optimal rate, circumventing the impossibility for deterministic methods
- The excess error cannot fall below √dη infinitely often for any fixed distribution, establishing "poisoned learning curves"

## Why This Works (Mechanism)
Randomized learners work because they introduce uncertainty that prevents the adversary from precisely targeting the learner's decision boundary. By sampling hypotheses according to a distribution that favors low empirical loss, the learner creates a moving target that the adversary cannot fully anticipate, even with knowledge of the randomization scheme. This contrasts with deterministic learners that commit to a specific hypothesis, making them vulnerable to precise poisoning attacks.

## Foundational Learning
- **VC Dimension:** Measures the capacity of a hypothesis class; needed to quantify the complexity of the learning problem and appears in both upper and lower bounds as √d
- **Agnostic Learning:** Learning framework where no perfect classifier exists; needed because the paper addresses the general case beyond realizable settings
- **Differential Privacy:** Connection used to design the randomized learner; needed to provide robustness guarantees against adaptive adversaries
- **Multiplicative Weights:** Algorithmic framework for maintaining distributions over hypotheses; needed to achieve the Õ(√dη) upper bound
- **Information Theory:** Tools from information theory used in lower bound proofs; needed to establish fundamental limits on what any learner can achieve

## Architecture Onboarding
- **Component Map:** Randomization mechanism -> Hypothesis sampling distribution -> Empirical risk minimization -> Final hypothesis selection
- **Critical Path:** Adversary observes data and randomness → Poisons training set → Learner samples hypotheses proportionally to exp(-λ·empirical loss) → Output randomized classifier
- **Design Tradeoffs:** The parameter λ in the sampling distribution must balance exploration vs. exploitation; larger λ concentrates on low-loss hypotheses but may be more vulnerable to poisoning
- **Failure Signatures:** Excess error approaching 1 indicates deterministic learner vulnerability; error scaling as √dη (not dη) indicates successful randomization
- **First Experiments:** 1) Compare deterministic vs. randomized learner performance under same poisoning budget 2) Test sensitivity to λ parameter choice 3) Verify √dη scaling holds across different VC classes

## Open Questions the Paper Calls Out
None

## Limitations
- Lower bound construction through poisoned coin guessing may not be tight for all hypothesis classes beyond VC classes
- Assumes adversary has knowledge of public randomness, which may not capture all practical attack scenarios
- Computational complexity of the proposed randomized learner may limit practical applicability
- Results focus on instance-targeted attacks; generalization to indiscriminate poisoning requires further study

## Confidence
- **Upper Bound Construction:** High confidence - well-established connections to multiplicative weights and differential privacy
- **Lower Bound Arguments:** Medium confidence - reduction from poisoned coin guessing to VC classes requires careful analysis
- **Distributional Independence Claim:** Medium confidence - infinitely-often lower bound construction relies on specific distributional assumptions

## Next Checks
1. Implement and benchmark the proposed randomized learner on standard datasets to verify empirical performance matches theoretical guarantees
2. Test whether the lower bound construction remains tight for non-VC classes (e.g., neural networks or kernel methods) to assess generality
3. Analyze the impact of partial knowledge of randomness (beyond public randomness) on the adversary's ability to break learner guarantees