---
ver: rpa2
title: Predicting and improving test-time scaling laws via reward tail-guided search
arxiv_id: '2602.01485'
source_url: https://arxiv.org/abs/2602.01485
tags:
- logn
- tail
- reward
- scaling
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing test-time scaling
  for large language models (LLMs) by proposing a principled framework for predicting
  and improving scaling behavior. The core method involves predicting the scaling
  laws of "best-of-N" strategies by modeling the tail distribution of rewards, enabling
  extrapolation beyond observed samples.
---

# Predicting and improving test-time scaling laws via reward tail-guided search

## Quick Facts
- arXiv ID: 2602.01485
- Source URL: https://arxiv.org/abs/2602.01485
- Reference count: 40
- One-line primary result: A principled framework for predicting and improving test-time scaling behavior by modeling the tail distribution of rewards, enabling extrapolation beyond observed samples.

## Executive Summary
This paper addresses the challenge of optimizing test-time compute allocation for large language models (LLMs) by introducing a principled framework for predicting and improving scaling behavior. The core method involves modeling the tail distribution of rewards from intermediate states to predict the potential of different computational paths, enabling extrapolation beyond observed samples. Based on this prediction tool, the authors propose Scaling-Law Guided (SLG) Search, an adaptive algorithm that dynamically allocates resources to states with the highest predicted potential. The framework is validated across mathematical reasoning tasks, demonstrating consistent improvements over standard Best-of-N sampling under identical compute budgets.

## Method Summary
The paper proposes a framework that predicts scaling laws of "best-of-N" strategies by modeling the tail distribution of rewards from intermediate states. The method first generates partial responses ("states") limited to the first 100 tokens, then samples completions for each state to estimate their reward distributions. By analyzing the tail (top α=0.2 fraction) of these rewards, the framework inverts truncated normal moments to predict the full distribution parameters and estimate each state's potential with additional compute. The Scaling-Law Guided (SLG) Search algorithm dynamically allocates the remaining budget to the state with the highest predicted potential, achieving better sample efficiency than standard Best-of-N sampling. The approach is theoretically grounded with vanishing regret guarantees and validated empirically on AMC/AIME math problems.

## Key Results
- SLG Search consistently outperforms Best-of-N under identical compute budgets across multiple LLMs and reward models
- Theoretical proof shows SLG achieves vanishing regret compared to perfect-information oracles
- The method attains expected rewards that would otherwise require polynomially larger compute budgets than Best-of-N
- Empirical results demonstrate superior sample efficiency and robustness to parameter settings

## Why This Works (Mechanism)
The framework works by recognizing that the distribution of rewards for a given intermediate state can be modeled as a truncated normal, where the tail behavior reveals information about the full distribution. By estimating the tail parameters (mean and variance) from a small sample, the method can invert these moments to predict the complete distribution and extrapolate how the reward would scale with additional compute. This allows for intelligent resource allocation to states with the highest predicted potential rather than uniformly sampling all states. The dynamic scheduling of sampling parameters (m(N) and K(N)) ensures computational efficiency across different budget sizes.

## Foundational Learning

**Truncated Normal Distribution**: A normal distribution restricted to a subset of its domain, commonly used to model rewards bounded below by zero. Understanding this is crucial because the reward distribution for intermediate states is assumed to follow this form, enabling the inversion of tail moments to predict full distribution parameters.

**Inverse Mills Ratio**: A mathematical function used to relate the mean and variance of a truncated normal distribution to its untruncated counterpart. This is needed to solve the system of equations that invert the observed tail moments into estimates of the full distribution parameters.

**Best-of-N Scaling Laws**: The empirical observation that the expected maximum reward from N independent samples follows a predictable pattern as N increases. Understanding this concept is essential because the paper's core contribution is predicting these scaling laws for intermediate states rather than just final responses.

**Regret Analysis**: A framework for measuring the performance gap between an algorithm and an optimal oracle. This is important for theoretically grounding the SLG algorithm's performance guarantees and proving its asymptotic optimality.

**Dynamic Resource Allocation**: The strategy of adjusting computational resource distribution based on intermediate results. This is central to SLG's approach of shifting budget from low-potential states to high-potential ones as information is gathered.

## Architecture Onboarding

**Component Map**: Data → State Generator → Reward Model → Tail Estimator → Scaling Law Predictor → Resource Allocator → Final Generator

**Critical Path**: The sequence from generating partial states to allocating final compute budget represents the most important flow. Each component must function correctly for the overall system to work: if tail estimation fails, scaling law prediction is compromised, leading to poor resource allocation.

**Design Tradeoffs**: The method trades off exploration (generating many states to estimate distributions) against exploitation (concentrating resources on promising states). The choice of tail fraction α=0.2 represents a balance between having enough tail samples for stable estimation and capturing the most informative portion of the distribution. Dynamic scheduling functions m(N) and K(N) must balance computational overhead against prediction accuracy.

**Failure Signatures**: Poor performance typically manifests as either over-allocation to unpromising states (if tail estimation is noisy or biased) or under-exploration (if too few states are generated initially). Numerical instability in moment inversion indicates insufficient tail samples or non-normal reward distributions. The system is most vulnerable when the reward model has limited coverage of reasoning strategies or when states are too short to capture meaningful intermediate progress.

**3 First Experiments**:
1. Run SLG with N=300, m=35, K=4 on AIME 2024 data using the specified Llama-3.2-1B-Instruct generator and Skywork-Reward-V2-Llama-3.1-8B scorer to verify basic functionality
2. Compare SLG performance against Best-of-N baseline with identical compute budget to confirm the primary empirical claim
3. Test tail estimation stability by varying m from 20 to 50 and observing variance estimation quality and final reward predictions

## Open Questions the Paper Calls Out
None

## Limitations
- The framework assumes stationary reward distributions that can be modeled as truncated normals, which may not generalize to open-ended domains with multimodal rewards
- Performance heavily depends on the quality of the reward model (ORM), with no reported metrics on ORM accuracy making it difficult to disentangle algorithmic from signal quality contributions
- Results are demonstrated only on AMC/AIME math problems, with claims of "universal applicability" extrapolated from a narrow base

## Confidence
- **High Confidence**: Theoretical regret bounds (Theorem 1) and empirical comparison showing SLG outperforming Best-of-N under fixed compute budgets
- **Medium Confidence**: Adaptive scaling law prediction and dynamic allocation strategy are logically sound with consistent gains, but modest absolute improvements and empirical scheduling choices
- **Low Confidence**: Claim of achieving rewards "otherwise requiring polynomially larger compute budgets" is asymptotic theory not fully realized in tested scenarios

## Next Checks
1. Apply SLG to a non-mathematical task (e.g., long-form QA or code generation) with a different reward model to measure generalization of tail-guided prediction and effectiveness of α=0.2 heuristic
2. For a small subset of prompts, run full Best-of-N with large N (e.g., N=1000) to approximate oracle performance and quantify the actual "polynomial" gain gap with SLG at N=300
3. Evaluate SLG's performance using a weaker or noisier reward model (e.g., adding Gaussian noise or using a smaller ORM) to assess sensitivity to reward model quality