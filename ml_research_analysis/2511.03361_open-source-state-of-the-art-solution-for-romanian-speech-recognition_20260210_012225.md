---
ver: rpa2
title: Open Source State-Of-the-Art Solution for Romanian Speech Recognition
arxiv_id: '2511.03361'
source_url: https://arxiv.org/abs/2511.03361
tags:
- speech
- romanian
- decoding
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first application of NVIDIA's FastConformer
  architecture to Romanian Automatic Speech Recognition (ASR). The model is trained
  on over 2,600 hours of Romanian speech data, combining high-quality transcriptions
  with weakly labeled audio, and is evaluated using both manual and automatic transcriptions.
---

# Open Source State-Of-The-Art Solution for Romanian Speech Recognition

## Quick Facts
- arXiv ID: 2511.03361
- Source URL: https://arxiv.org/abs/2511.03361
- Reference count: 37
- Primary result: First application of NVIDIA's FastConformer to Romanian ASR, achieving state-of-the-art performance with up to 27% relative WER reduction

## Executive Summary
This paper introduces the first application of NVIDIA's FastConformer architecture to Romanian Automatic Speech Recognition, trained on over 2,600 hours of Romanian speech data. The model combines high-quality transcriptions with weakly labeled audio and employs a hybrid decoder integrating CTC and Token-Duration Transducer branches. The proposed system achieves state-of-the-art performance across all Romanian ASR benchmarks while being made available as an open-source resource. The approach demonstrates practical decoding efficiency suitable for low-latency deployment and up to 27% relative reduction in Word Error Rate compared to prior best-performing models.

## Method Summary
The model uses NVIDIA's FastConformer encoder (110M parameters) initialized from pre-trained English models, with a hybrid CTC-TDT decoder trained on ~2,636 hours of Romanian speech. Training combines ~181 hours of manual annotations with ~2,455 hours of weakly labeled audio obtained through partial alignment. The system uses a 1024 BPE tokenizer, AdamW optimization with Noam scheduler, and various data augmentations. Four decoding strategies are evaluated: CTC greedy/beam search and TDT greedy/ALSD, with CTC beam search incorporating a 6-gram language model.

## Key Results
- State-of-the-art WER across all Romanian ASR benchmarks
- Up to 27% relative reduction in WER compared to prior models
- Efficient decoding with practical RTFx suitable for low-latency deployment
- CTC beam search with 6-gram LM achieves best accuracy (8.12% WER on SSC-eval1)
- TDT greedy provides good balance of accuracy and speed (9.08% WER, RTFx 126)

## Why This Works (Mechanism)

### Mechanism 1
The hybrid CTC-TDT decoder architecture improves convergence and enables flexible decoding trade-offs between accuracy and latency. A unified decoder shares a common FastConformer encoder while maintaining separate CTC and TDT branches, with final loss computed as a weighted sum (CTC weight = 0.3) of both branch losses. This joint optimization produces shared acoustic representations that improve both decoding paths.

### Mechanism 2
Cross-lingual transfer from English pre-trained models substantially improves Romanian ASR performance despite limited labeled data. The model initializes from NVIDIA's Parakeet TDT-CTC architecture, which was pre-trained on English speech, capturing universal low-level acoustic features that transfer across languages. Only the tokenizer and final layers are retrained for Romanian.

### Mechanism 3
Large-scale weak supervision using automatically aligned transcriptions bridges the resource gap for Romanian ASR without degrading spontaneous speech performance. The training corpus combines manual annotations with ~2,455 hours of weakly labeled audio obtained through partial alignment of two ASR systems, enabling leveraging vast unlabeled corpora while the model learns to filter noise through exposure to both clean and noisy labels.

## Foundational Learning

- **Concept: Conformer Architecture (CNN + Self-Attention Hybrid)**
  - Why needed here: FastConformer is the core encoder; understanding how it captures both local (convolutional) and global (attention) speech patterns is essential for debugging representation failures.
  - Quick check question: Can you explain why a pure Transformer struggles with local speech dependencies that Conformer addresses?

- **Concept: Transducer-Based Decoding (RNN-T, TDT)**
  - Why needed here: The TDT branch is one of two decoder options; understanding token-duration modeling explains why TDT outperforms standard RNN-T on alignment stability.
  - Quick check question: How does explicit duration modeling in TDT differ from implicit alignment learning in standard RNN-T?

- **Concept: CTC Decoding and External Language Model Integration**
  - Why needed here: The CTC branch achieves best results with n-gram LM beam search; understanding CTC assumptions (conditional independence) clarifies why external LMs help.
  - Quick check question: Why does CTC assume frame-level conditional independence, and how does beam search with LM address this limitation?

## Architecture Onboarding

- **Component map:**
  Audio (16kHz) → FastConformer Encoder (17 layers, 110M params) → ┌────────┴────────┐ → CTC Decoder (1024 BPE) → Greedy/Beam → + 6-gram LM → Transcript Output
                                                                   ↓
                                                           TDT Decoder (duration-aware) → Greedy/ALSD

- **Critical path:**
  1. Input preprocessing: 16kHz mono audio, text normalization (31 Romanian chars + hyphen)
  2. Encoder inference: 8x downsampling via depthwise separable convs → 17 Conformer blocks
  3. Decoder selection: Choose CTC (high accuracy with LM) or TDT (faster, internal LM only)
  4. Decoding strategy: Greedy (fastest) → ALSD (TDT balanced) → Beam+LM (CTC best accuracy)

- **Design tradeoffs:**
  - CTC greedy vs. TDT greedy: TDT greedy achieves lower WER (9.08% vs 10.10% on SSC-eval1) with similar RTFx (~126 vs ~130), but requires more complex training
  - TDT-ALSD vs. CTC beam+LM: CTC beam achieves best accuracy (8.12% vs 8.64% on SSC-eval1) with 64% better RTFx than ALSD (109 vs 66), but requires external LM training and tuning
  - Model size vs. coverage: 110M parameters balances efficiency and accuracy; larger models not explored

- **Failure signatures:**
  - Spontaneous speech degradation when adding domain-mismatched data (observed in prior HMM-DNN work with CDEP corpus)
  - Long-form audio (>20s) may require chunking or streaming adaptation despite FastConformer's theoretical capacity
  - Underrepresented dialectal speech shows higher WER (USPDATRO: 23.4% vs RSC-eval: 1.73%)

- **First 3 experiments:**
  1. Reproduce baseline greedy decoding: Run both CTC-greedy and TDT-greedy on all 7 evaluation sets to validate RTFx and WER match reported values (CDEP-eval should show ~4.2% WER for TDT-greedy)
  2. Ablate weak supervision: Train on manually annotated subset only (~181h) vs. full corpus (~2,600h) to quantify weak supervision contribution (expect substantial WER increase on spontaneous speech)
  3. Compare decoding strategies on a single benchmark: Run all 4 decoding strategies on SSC-eval1 to verify accuracy-latency tradeoff curve (TDT-greedy: 9.08% WER, RTFx 126 → CTC-beam: 8.12% WER, RTFx 109)

## Open Questions the Paper Calls Out

### Open Question 1
Can integrating an external language model (LM) with the Token-Duration Transducer (TDT) branch yield further accuracy improvements? The authors note that for the TDT-ALSD strategy, "we do not utilize [an] external language model," leaving the potential synergy between TDT duration modeling and external LMs untested.

### Open Question 2
Does the 20-second limit on training utterances hinder the model's capability to process long-form audio? Training audio was capped at "maximum of 20s," yet the FastConformer architecture theoretically supports "sequences up to 11 hours," leaving unclear if the model's attention mechanism generalizes to long sequences without training on longer contexts.

### Open Question 3
Is the improved robustness to oratory data caused by the FastConformer architecture or the scale of data? The paper notes that unlike prior work where oratory data caused "degradation in spontaneous speech," this model "benefits from the inclusion," but does not isolate whether the architecture specifically prevents the domain mismatch issues seen in previous TDNN systems.

## Limitations

- Primary limitation is unknown quality of weakly supervised annotations (~93% of training data), with no empirical analysis quantifying transcription error impact
- Cross-lingual transfer effectiveness for Romanian remains uncertain, with no ablation studies comparing English vs. multilingual vs. Romanian-only pre-training
- Hybrid decoder assumes CTC and TDT objectives are compatible, but no analysis explores potential conflicts between the two branches

## Confidence

- **High Confidence**: CTC-TDT hybrid architecture implementation and training procedure; tokenizer construction and normalization rules; decoding strategy descriptions
- **Medium Confidence**: Weak supervision methodology effectiveness; relative WER improvements over baselines; RTFx measurements
- **Low Confidence**: Cross-lingual transfer benefits for Romanian specifically; weak label quality impact; optimal CTC weight selection

## Next Checks

1. **Weak supervision ablation study**: Train and evaluate on manually annotated data only (~181h) versus full training corpus (~2,636h) across all 7 evaluation sets to quantify the contribution of weak labels, particularly for spontaneous speech domains (SSC, CDEP)

2. **Cross-lingual transfer ablation**: Compare performance using English pre-trained initialization versus multilingual pre-training (including Romanian) or zero-shot Romanian training to isolate the benefit of language-specific versus universal acoustic representations

3. **Decoder branch independence analysis**: Train and evaluate CTC-only and TDT-only models to determine whether joint training provides synergistic benefits or if one branch dominates, and whether the 0.3 CTC weight is optimal across different evaluation conditions