---
ver: rpa2
title: 'VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM'
arxiv_id: '2506.17506'
source_url: https://arxiv.org/abs/2506.17506
tags:
- register
- locc
- veri
- allocation
- compiler
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VeriLocc, a learning-based register allocator
  that combines large language models (LLMs) with formal verification techniques to
  achieve cross-architecture register allocation for GPUs. The core idea is to formulate
  register allocation as a sequence-to-sequence translation task, where the LLM learns
  to map virtual registers in intermediate representations (IRs) to physical registers
  in target architectures.
---

# VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM
## Quick Facts
- **arXiv ID**: 2506.17506
- **Source URL**: https://arxiv.org/abs/2506.17506
- **Reference count**: 19
- **Key outcome**: 85–99% single-shot accuracy on GPU register allocation; 11.6% runtime improvement over rocBLAS

## Executive Summary
VeriLocc introduces a novel approach to cross-architecture register allocation by framing it as a sequence-to-sequence translation task for large language models (LLMs). The method leverages static analysis-based normalization to reduce token counts and expose dependencies, then fine-tunes Qwen2.5-Coder-7B-Instruct on normalized MIR-to-JSON mapping pairs. A verifier-guided regeneration loop using Z3 ensures correctness by validating assignments against consistency, safety, and realizability constraints. Evaluated on GEMM and MHA kernels, VeriLocc achieves near-100% correctness with sampling and discovers more performant assignments than expert-tuned libraries, demonstrating the viability of LLM-based register allocation in production compiler toolchains.

## Method Summary
VeriLocc addresses cross-architecture register allocation by transforming it into a sequence-to-sequence learning problem. The approach normalizes MIR (PTX/LLVM MIR) using static analysis to reduce token counts by 80–90% and explicitly expose control/data dependencies. A fine-tuned Qwen2.5-Coder-7B-Instruct model maps virtual registers to physical ones in target architectures. A Z3-based verifier validates assignments against three constraints: consistency (same virtual→physical mapping), safety (overlapping lifetimes→disjoint registers), and realizability (hardware-specific constraints like consecutive registers for 64-bit operands). Failed allocations trigger a regeneration loop, achieving 85–99% single-shot accuracy and near-100% correctness with sampling.

## Key Results
- 85–99% single-shot correctness (Pass@1) on NVIDIA RTX 4090 and AMD MI250x kernels
- Near-100% correctness with sampling (Pass@100)
- 11.6% runtime improvement over rocBLAS on MI250x
- 80–90% token reduction via normalization (e.g., MHA LLVM: 51,037→8,955 tokens)

## Why This Works (Mechanism)
The method works by reframing register allocation as a translation task where the LLM learns patterns from normalized MIR representations. Static analysis-based normalization strips irrelevant metadata, unifies instruction variants, and renumbers registers per basic block, making the input more compact and semantically explicit. This allows the LLM to focus on the core allocation problem rather than parsing noise. The Z3 verifier ensures correctness by enforcing register allocation invariants, enabling safe deployment despite the LLM's inherent unreliability.

## Foundational Learning
- **Register Allocation Fundamentals**: Mapping virtual registers to physical ones while respecting liveness and hardware constraints. *Why needed*: Core problem being solved. *Quick check*: Can identify virtual vs physical registers in MIR.
- **Static Analysis for MIR**: Extracting control/data dependencies and liveness intervals from intermediate representations. *Why needed*: Enables normalization and constraint generation. *Quick check*: Can parse MIR and identify basic blocks and live ranges.
- **SMT Solvers (Z3)**: Encoding and solving logical constraints over bitvectors and arrays. *Why needed*: Formal verification of register assignments. *Quick check*: Can encode register allocation constraints and validate solutions.
- **Sequence-to-Sequence Learning**: Fine-tuning LLMs to map input sequences (MIR) to output sequences (JSON mappings). *Why needed*: Core learning approach. *Quick check*: Can fine-tune a model on a synthetic seq2seq task.

## Architecture Onboarding
- **Component Map**: MIR → Static Analyzer → Normalizer → LLM → JSON Output → Z3 Verifier → (Success/Failure → Regenerate)
- **Critical Path**: Normalization → LLM Inference → Z3 Verification → Output
- **Design Tradeoffs**: Full LLM fine-tuning vs. parameter-efficient methods (LoRA); static normalization vs. dynamic masking; single-shot vs. sampling-based generation.
- **Failure Signatures**: Dataflow violations (overwriting live registers, 62–73%), hardware constraint violations (non-consecutive 64-bit registers, 24–29%), syntactic errors (malformed JSON, 4–8%).
- **First Experiments**: 1) Validate normalization pipeline on sample MIR. 2) Test Z3 constraint encoding with known correct/incorrect assignments. 3) Run LLM inference on normalized pairs and measure Pass@1 before/with verifier.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can integrating the formal verifier directly into the training loop, such as using it as a reinforcement learning reward signal, improve single-shot accuracy? *Basis*: Section 6 mentions exploring RL with verifier-based rewards. *Why unresolved*: Current model uses cross-entropy loss; verifier only used during inference. *Evidence needed*: Comparison of Pass@1 rates between current model and RL-fine-tuned model.
- **Open Question 2**: Does VeriLocc generalize effectively to irregular, control-heavy programs outside of the structured compute kernels (GEMM/MHA) tested? *Basis*: Section 8 notes evaluation focused on structured kernels. *Why unresolved*: Dataset consists of matrix and attention operations with regular patterns. *Evidence needed*: Benchmark results on kernels with complex control flow and irregular memory access.
- **Open Question 3**: Can constrained decoding mechanisms that mask occupied registers during generation significantly reduce dataflow violations? *Basis*: Section 4.5 suggests dynamic masking to address 62–73% of errors. *Why unresolved*: Current model generates auto-regressively without hard liveness constraints. *Evidence needed*: Ablation study showing reduction in dataflow violations with dynamic masking.

## Limitations
- Training hyperparameters (learning rate, epochs, batch size) not specified
- Data collection pipeline details for intercepting toolchains and reconstructing mappings sparse
- Z3 constraint encoding details not provided, limiting exact reproduction

## Confidence
- **Register Allocation Performance**: High confidence (specific metrics, clear methodology)
- **Normalization Effectiveness**: High confidence (supported by example, aligns with described techniques)
- **Cross-Architecture Generalization**: Medium confidence (evaluated on only two architectures with specific kernel types)

## Next Checks
1. **Reconstruct the Z3 Constraint Encoding**: Implement consistency, safety, and realizability constraints; validate against known correct/incorrect allocations.
2. **Document the Full Normalization Pipeline**: Implement and verify each normalization step; ensure 80–90% token reduction and semantic preservation.
3. **Replicate the Data Collection Process**: Reconstruct MIR/ISA pair collection for NVIDIA and AMD toolchains; implement SSA-based dataflow analysis and toolchain interception.