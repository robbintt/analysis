---
ver: rpa2
title: 'Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking'
arxiv_id: '2512.24297'
source_url: https://arxiv.org/abs/2512.24297
tags:
- reasoning
- visual
- code
- aime
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FIGR improves reasoning by integrating executable visual construction
  into multi-turn inference. Instead of relying on implicit symbolic states or predefined
  visual tools, it generates and executes code to construct diagrams as intermediate
  reasoning states, enabling precise control and revisable visual feedback.
---

# Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking

## Quick Facts
- arXiv ID: 2512.24297
- Source URL: https://arxiv.org/abs/2512.24297
- Authors: Meiqi Chen; Fandong Meng; Jie Zhou
- Reference count: 24
- Primary result: FIGR improves base model by 13.12% on AIME 2025 and 11.00% on BeyondAIME via executable visual construction

## Executive Summary
FIGR introduces executable visual construction to multi-turn reasoning, generating and executing code to build diagrams as intermediate reasoning states. This provides precise control and revisable visual feedback, unlike implicit text-only reasoning or unconstrained image generation. The method uses reinforcement learning with an adaptive reward mechanism that selectively encourages visual reasoning when beneficial, without requiring supervised cold-start. Trained on DeepMath-103K, FIGR significantly outperforms strong text-only and multimodal baselines on mathematical reasoning benchmarks.

## Method Summary
FIGR integrates a sandboxed code interpreter into a vision-language model's reasoning loop, where the model generates executable code (matplotlib, shapely) to construct diagrams as intermediate states. The policy alternates between textual reasoning and code execution, receiving rendered diagrams as visual feedback. Training uses Group Relative Policy Optimization (GRPO) with a composite reward including accuracy, format, and adaptive visual invocation rewards. A suitability classifier determines when visual reasoning is likely beneficial, preventing reward hacking. The approach enables precise geometric constraints through deterministic code execution rather than implicit symbolic maintenance.

## Key Results
- +13.12% improvement on AIME 2025 benchmark over Qwen3-VL-32B-Instruct
- +11.00% improvement on BeyondAIME benchmark
- Outperforms strong text-only RL and unified multimodal baselines
- Code usage collapses to zero during training without adaptive reward mechanism

## Why This Works (Mechanism)

### Mechanism 1: Executable Visual States as Consistency Enforcers
Generating executable code that renders diagrams produces more reliable intermediate states than text-only reasoning or unconstrained image generation. Code execution is deterministic and verifiable—geometric constraints are enforced programmatically rather than maintained implicitly. The rendered diagram provides externalized, inspectable feedback that reveals constraint violations invisible to text-only reasoning.

### Mechanism 2: Adaptive Reward for Selective Visual Invocation
Tying visual-construction rewards to task suitability and final correctness teaches the policy when visual reasoning helps versus when it adds unnecessary complexity. A suitability classifier modulates reward magnitude—visual construction receives high reward only when suitable, correct, and successfully executed. This prevents reward hacking where the model generates spurious diagrams.

### Mechanism 3: Trajectory-Level RL with Group Relative Advantage
GRPO's group-relative advantage signal enables learning when to invoke visual construction without step-level supervision. For each problem, sample trajectories; compute group-average reward as baseline; advantage = individual reward − baseline. This provides low-variance signal emphasizing relative behavioral differences (visual construction helping vs. hurting).

## Foundational Learning

- **Policy Gradient with Delayed Rewards**: Needed to understand credit assignment across multi-step reasoning with code execution. Quick check: Can you explain why GRPO uses group-average baseline instead of a learned value function?

- **Sandbox Code Execution for LLMs**: Critical for implementing FIGR's interpreter with security constraints and execution limits. Quick check: What types of code execution failures should be caught and how should they be communicated back to the policy?

- **Visual Reasoning in LVLMs**: Needed to understand baseline capabilities and what's being improved. Quick check: Why do existing unified multimodal models struggle with geometric precision according to the paper?

## Architecture Onboarding

- **Component map**: Base policy (Qwen3-VL-32B-Instruct) -> Code interpreter (sandboxed Python with matplotlib, shapely) -> Suitability classifier (Deepseek-V3) -> Reward computation (accuracy + format + visual rewards) -> RL framework (VeRL with GRPO)

- **Critical path**: Question → suitability classifier labels s → Policy samples trajectories (text + code actions) → Interpreter executes code → textual feedback + rendered diagram → Policy receives diagram as visual input → Trajectory terminates → Compute rewards → GRPO update

- **Design tradeoffs**: Multi-turn overhead vs. single-turn (more compute but enables iterative refinement); Classifier quality vs. reward robustness (paper claims moderate misclassification is tolerable); Code length vs. pass rate (longer code enables richer diagrams but higher failure risk)

- **Failure signatures**: Code ratio drops to zero during training (missing ARM); High code execution failures (interpreter environment issues); Performance no better than text-only RL (visual feedback not being integrated)

- **First 3 experiments**: 1) Replicate the w/o ARM ablation on a small dataset subset; confirm code usage collapse pattern. 2) Test interpreter with geometric problems; measure code pass rate and common failure modes. 3) Compare trajectory samples with/without visual construction on 10 problems; manually inspect whether diagrams reflect stated constraints.

## Open Questions the Paper Calls Out

- Can the adaptive reward mechanism be improved to provide step-wise rather than trajectory-level supervision for visual invocation decisions? The trajectory-level reward cannot distinguish between beneficial and harmful visual construction at individual reasoning steps.

- What are the precise task characteristics that determine whether executable visual construction provides benefit over text-only reasoning? The suitability classifier uses heuristics without formal characterization of problem features.

- How does the choice of visualization libraries and code execution environments affect reasoning quality and generalization? The method relies on shapely and matplotlib but doesn't investigate whether different tools impact the model's ability to extract useful structural feedback.

- To what extent does FIGR generalize to domains beyond mathematical reasoning, such as scientific reasoning, program synthesis, and long-horizon planning? Evaluation is restricted to mathematical benchmarks emphasizing structural consistency.

## Limitations

- Critical hyperparameters (group size, clipping, KL coefficient, learning rate) are omitted, blocking exact replication
- The suitability classifier's accuracy isn't independently validated despite being crucial to reward stability
- Implementation requires substantial engineering overhead with sandboxed code execution and diagram rendering
- Results may be biased toward geometry problems rather than general mathematical reasoning

## Confidence

- **High confidence**: Executable visual construction mechanism is well-supported by deterministic code execution and explicit reward formula
- **Medium confidence**: Adaptive reward effectiveness relies on classifier accuracy which isn't validated
- **Low confidence**: Trajectory-level GRPO's advantage signal assumes low variance unrelated to code execution quality

## Next Checks

1. Replicate the w/o ARM ablation on a small dataset subset (100 problems) to confirm code usage collapse pattern
2. Characterize interpreter reliability by testing geometric problems, measuring code pass rate and common failure modes
3. Manual trajectory analysis of 10 problems comparing samples with/without visual construction to determine whether diagrams actually correct reasoning errors