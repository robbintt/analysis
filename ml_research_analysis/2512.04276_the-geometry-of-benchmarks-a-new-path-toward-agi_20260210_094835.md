---
ver: rpa2
title: 'The Geometry of Benchmarks: A New Path Toward AGI'
arxiv_id: '2512.04276'
source_url: https://arxiv.org/abs/2512.04276
tags:
- space
- batteries
- moduli
- benchmarks
- capability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the lack of a unified framework for evaluating
  and understanding AI progress toward artificial general intelligence (AGI), noting
  that current benchmark practice is fragmented, narrow, and static. It proposes a
  geometric framework where all psychometric batteries are treated as points in a
  structured moduli space, and agent performance is described by capability functionals
  over this space.
---

# The Geometry of Benchmarks: A New Path Toward AGI

## Quick Facts
- arXiv ID: 2512.04276
- Source URL: https://arxiv.org/abs/2512.04276
- Authors: Przemyslaw Chojecki
- Reference count: 7
- Primary result: Introduces a geometric framework treating AI benchmarks as points in a moduli space, enabling measurement of AGI-like self-improvement through GVU dynamics

## Executive Summary
This paper addresses the fragmented nature of AI benchmarking by proposing a unified geometric framework for evaluating progress toward AGI. The framework treats benchmark batteries as points in a moduli space, where agent performance is described by capability functionals over this structured space. At its core is the Autonomous AI (AAI) Scale - a Kardashev-style hierarchy of autonomy based on measurable performance across diverse task families, combined with a Generator-Verifier-Updater (GVU) operator that subsumes various self-improvement mechanisms as special cases.

The work introduces a self-improvement coefficient κ defined as the Lie derivative of capability functionals along the GVU-induced flow, providing a mathematical foundation for understanding AGI development as a geometric process. The framework establishes determinacy results showing that dense families of benchmarks can certify performance on entire regions of task space, and derives variance inequalities providing sufficient conditions for positive self-improvement. This approach shifts the focus from individual benchmark scores to the dynamics of capability growth across structured task landscapes.

## Method Summary
The paper constructs a geometric framework where benchmark batteries are treated as points in a moduli space, with equivalence classes defined by evaluation-equivalence (permutations, relabeling, monotone score reparameterizations). Agent capability is modeled as a functional F(π,B) over this space, with the AAI Index aggregating worst-case performance across capability families. The Generator-Verifier-Updater (GVU) operator decomposes learning into three components: G generates candidates, V scores them, and U updates parameters. Self-improvement is quantified by κ, the Lie derivative of F along the GVU flow, with variance inequalities providing sufficient conditions for κ>0. The framework includes determinacy results showing that Lipschitz capability functionals enable regional performance certification from sparse benchmark coverage.

## Key Results
- Dense families of benchmark batteries suffice to certify capability on entire regions of task space through Lipschitz determinacy
- Positive self-improvement (κ > 0) occurs when generator-verifier noise is dominated by gradient signal, formalized through variance inequalities
- AGI-like behavior is operationalized as sustained positive self-improvement across diverse capability families rather than single-test performance
- The GVU operator unifies reinforcement learning, self-play, debate, and verifier-based fine-tuning as special cases of a common geometric flow

## Why This Works (Mechanism)

### Mechanism 1
Dense families of benchmarks can certify capability on entire regions of task space. By quotienting out equivalence classes of benchmarks, benchmarks organize into a moduli space M where if capability functionals Φπ are Lipschitz in the Wasserstein metric d, then sparse coverage can approximate regional performance. Core assumption: Capability functionals satisfy Lipschitz regularity across the moduli space. Break condition: If capability functionals are non-Lipschitz (sharp transitions, discontinuous jumps), sparse coverage fails and regional inference breaks.

### Mechanism 2
Self-improvement occurs (κ > 0) when the gradient signal dominates combined generator-verifier noise. Learning decomposes into Generator G (produces candidates zt), Verifier V (scores candidates), Updater U (modifies parameters). The expected change in capability decomposes as E[ΔF] ≈ η|∇F|² − (η²/2)Tr(HF·ΣGV). The variance inequality provides sufficient conditions for positive improvement. Core assumption: The updater approximates stochastic gradient ascent; Hessian and curvature are bounded; step size η ∈ (0, η*). Break condition: If verifier noise is unbounded, gradient estimates become unreliable, or step sizes exit (0, η*), κ may become negative.

### Mechanism 3
AGI-like behavior is operationalized as sustained positive self-improvement (κ > 0) across diverse battery families, not single-test performance. The AAI Index is a vector of worst-case performance across families. Discrete levels are defined by threshold gates θf,ℓ with robustness to battery drift. AAI-4 ("AGI") requires κ > 0 across families. Core assumption: Standardized protocols can meaningfully compare performance across qualitatively different capability families. Break condition: If families are not sufficiently independent or thresholds are mis-calibrated, the hierarchy fails to discriminate genuine generality from narrow optimization.

## Foundational Learning

- **Quotient Spaces and Equivalence Relations**
  - Why needed here: The moduli space M is constructed by quotienting the space of batteries by evaluation-equivalence; understanding how symmetries are "divided out" is essential.
  - Quick check question: Given two benchmarks that differ only by renaming task indices, are they in the same equivalence class?

- **Lie Derivatives on Manifolds**
  - Why needed here: The self-improvement coefficient κ is defined as the Lie derivative of the capability functional along the GVU-induced flow field.
  - Quick check question: If a flow vector field v is zero at a point, what is the Lie derivative of any functional there?

- **Information Geometry (Fisher Metric)**
  - Why needed here: The parameter manifold Θ is equipped with the Fisher information metric g; GVU dynamics are modeled as stochastic flows on this Riemannian manifold.
  - Quick check question: Why is the Fisher metric preferred over Euclidean metric for parameter spaces of probability distributions?

## Architecture Onboarding

- Component map:
  Battery Layer: Task instances → sampling distribution µB → scoring rule S(π; B)
  Moduli Layer: Batteries → equivalence relation → moduli space M with Wasserstein topology
  Capability Layer: Agent π → capability functional F(π, B) → field Φπ: M → R
  Dynamics Layer: GVU operator TGVU: Θ → Θ → flow (θt)t → self-improvement coefficient κ

- Critical path:
  1. Define capability families F and construct representative batteries Bf for each
  2. Verify Lipschitz conditions empirically (capability interpolation tests)
  3. Instrument GVU loop with explicit Generator, Verifier, Updater modules
  4. Monitor κ and variance terms Tr(HF·ΣGV), |∇F|² in real time

- Design tradeoffs:
  - Coverage vs. cost: More benchmarks improve determinacy but increase evaluation expense
  - Step size η: Larger η accelerates potential improvement but risks exiting the (0, η*) stability regime
  - Verifier strength: Stronger verifiers reduce ΣV but may constrain exploration

- Failure signatures:
  - κ ≈ 0 sustained: Plateau; may indicate exhausted task distribution or verifier saturation
  - κ < 0: Degradation; check for mode collapse, reward hacking, or verifier drift
  - High variance ratio Tr(HF·ΣGV)/|∇F|²: Gradient signal too weak; reduce noise or increase signal

- First 3 experiments:
  1. **Lipschitz validation**: Embed existing benchmarks (MMLU, ARC-AGI, GSM8K) into approximate moduli space using Wasserstein distances; test if capability scores interpolate smoothly between nearby batteries.
  2. **GVU decomposition audit**: On an existing training pipeline (e.g., RLHF), explicitly label Generator, Verifier, and Updater components; estimate ΣG and ΣV empirically; verify variance inequality predicts κ sign correctly.
  3. **Level gate calibration**: Define AAI-1/2/3 thresholds on a suite of current models; test whether threshold passage correlates with qualitative autonomy behaviors (tool use persistence, error recovery).

## Open Questions the Paper Calls Out

### Open Question 1
Can practical embeddings of real-world benchmarks into approximate moduli spaces be constructed using representation learning and manifold modelling techniques? The paper states that empirical work is needed to construct practical embeddings using techniques from representation learning and manifold modelling, but does not implement or validate the construction on actual benchmark datasets.

### Open Question 2
Under what empirical conditions does the variance inequality for positive self-improvement (κ > 0) hold in practice for modern AI systems? The variance inequality provides theoretical sufficient conditions, but no experiments validate whether these conditions are met by actual training runs or how tight the bounds are.

### Open Question 3
How can the AAI level thresholds (θf,ℓ) be calibrated to produce meaningful, non-arbitrary autonomy classifications? The paper defines level gates abstractly and gives descriptive examples but provides no methodology for setting specific numerical thresholds.

### Open Question 4
Do capability functionals satisfy the Lipschitz regularity assumptions required for the determinacy and coverage results across realistic benchmark families? The determinacy theorem assumes Lipschitz continuity of Φπ, but this property is unverified for actual benchmarks where small changes could cause discontinuous capability shifts.

## Limitations
- The Lipschitz regularity of capability functionals across moduli space is assumed but not empirically validated; real-world capability curves often exhibit sharp phase transitions
- Level definitions are conceptually clear but thresholds θf,ℓ remain unspecified, making empirical calibration critical yet uncertain
- The variance inequality conditions for positive κ are sufficient but not necessary, and their practical applicability depends on accurate estimation of noise terms ΣG and ΣV

## Confidence
- **Medium**: Geometric construction of moduli space and Lipschitz determinacy (theoretically sound but unverified)
- **Medium**: Variance inequality conditions for κ > 0 (derivation rigorous but real-world conditions unclear)
- **Low**: AAI Scale operationalization (conceptual framework exists but calibration parameters missing)

## Next Checks
1. **Lipschitz validation**: Construct an empirical moduli space from existing benchmarks (MMLU, ARC-AGI, GSM8K) using Wasserstein distances on score distributions; test whether capability scores interpolate smoothly between nearby batteries using held-out intermediate benchmarks.

2. **GVU noise audit**: In an existing training pipeline (e.g., RLHF), explicitly decompose into Generator/Verifier/Updater components; estimate ΣG and ΣV empirically and verify whether variance inequality predicts observed κ sign across training epochs.

3. **Level gate calibration**: Define provisional thresholds for AAI levels 1-3 on a suite of current models; test whether threshold passage correlates with qualitative autonomy behaviors like persistent tool use and error recovery across diverse contexts.