---
ver: rpa2
title: Adapting Diarization-Conditioned Whisper for End-to-End Multi-Talker Speech
  Recognition
arxiv_id: '2510.03723'
source_url: https://arxiv.org/abs/2510.03723
tags:
- speaker
- speech
- whisper
- dicow
- multi-talker
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a speaker-attributed Whisper-based model that
  combines target-speaker modeling with serialized output training for multi-talker
  speech recognition. The approach uses a Diarization-Conditioned Whisper (DiCoW)
  encoder to extract speaker-specific embeddings, which are concatenated and passed
  to a shared decoder.
---

# Adapting Diarization-Conditioned Whisper for End-to-End Multi-Talker Speech Recognition

## Quick Facts
- arXiv ID: 2510.03723
- Source URL: https://arxiv.org/abs/2510.03723
- Authors: Martin Kocour, Martin Karafiat, Alexander Polok, Dominik Klement, Lukáš Burget, Jan Černocký
- Reference count: 0
- Primary result: DiCoW-Whisper achieves 17.2% cpWER on 3-speaker LibriMix, outperforming existing SOT approaches

## Executive Summary
This paper introduces a Diarization-Conditioned Whisper (DiCoW) model for end-to-end multi-talker speech recognition. The approach combines target-speaker modeling with serialized output training (SOT) to handle overlapping speech by conditioning the encoder on diarization outputs and generating serialized transcriptions with speaker tags. The method achieves state-of-the-art results on synthetic mixtures like LibriMix and competitive performance on real-world multi-speaker recordings such as AMI and NOTSOFAR.

## Method Summary
The proposed DiCoW-Whisper architecture modifies the standard Whisper model by incorporating diarization outputs into the encoder. The model uses an external diarization system to provide speaker turn information, which is then used to condition the encoder through speaker-specific embeddings. These embeddings are concatenated and passed to a shared decoder that generates serialized output streams with speaker tags and timestamps. The serialized output training approach allows the model to handle overlapping speech by treating it as a sequence of speaker-attributed tokens rather than attempting to separate the signals first.

## Key Results
- DiCoW-Whisper achieves 17.2% cpWER on 3-speaker LibriMix, outperforming competitive SOT baselines
- The model demonstrates strong performance on real-world data with 43.2% cpWER on NOTSOFAR corpus
- Competitive results on AMI meeting recordings show practical applicability beyond synthetic mixtures

## Why This Works (Mechanism)
The method works by leveraging diarization information to condition the encoder on speaker-specific contexts, allowing the model to better distinguish between overlapping speakers. The serialized output training approach transforms the multi-talker recognition problem into a sequence generation task where the model learns to output speaker-attributed tokens in temporal order. By concatenating speaker embeddings and using a shared decoder, the architecture maintains efficiency while handling multiple speakers simultaneously.

## Foundational Learning

**Diarization**: Speaker segmentation and clustering to identify who spoke when
*Why needed*: Provides temporal speaker boundaries that condition the encoder on speaker-specific contexts
*Quick check*: Verify diarization error rates don't exceed recognition degradation thresholds

**Serialized Output Training**: Treating multi-talker recognition as sequence generation with speaker tags
*Why needed*: Avoids explicit speech separation while maintaining temporal ordering
*Quick check*: Monitor if model learns to respect speaker turn boundaries

**Target-Speaker Modeling**: Conditioning recognition on specific speaker identities
*Why needed*: Helps the model distinguish between overlapping speakers using speaker-specific acoustic characteristics
*Quick check*: Test recognition accuracy with mismatched speaker embeddings

## Architecture Onboarding

**Component Map**: Diarization System -> DiCoW Encoder -> Shared Decoder -> Serialized Output
**Critical Path**: Audio input → Diarization conditioning → Encoder embeddings → Concatenation → Decoder → Speaker-attributed transcription
**Design Tradeoffs**: Concatenation-based fusion is simple but may not scale well to many speakers; external diarization adds dependency but provides robust speaker information
**Failure Signatures**: Poor diarization quality propagates directly to recognition errors; concatenation may struggle with very similar speakers
**First Experiments**: 1) Test with oracle diarization to establish upper bound; 2) Evaluate with varying numbers of speakers; 3) Compare concatenation vs alternative fusion methods

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gap between synthetic (LibriMix 17.2% cpWER) and real-world (NOTSOFAR 43.2% cpWER) data suggests domain adaptation challenges
- Reliance on external diarization introduces potential error propagation that wasn't extensively analyzed
- Concatenation-based speaker embedding fusion may have scalability limitations for scenarios with many overlapping speakers

## Confidence
- **DiCoW encoder effectiveness**: High confidence
- **Serialized output training efficacy**: High confidence
- **Diarization conditioning benefits**: Medium confidence

## Next Checks
1. **Diarization error sensitivity analysis**: Evaluate model performance across varying levels of diarization quality to quantify the impact of diarization errors on recognition accuracy.

2. **Cross-dataset generalization test**: Train the model on LibriMix and evaluate on NOTSOFAR without fine-tuning to assess true domain adaptation capabilities versus dataset-specific optimization.

3. **Scalability to >3 speakers**: Test the model architecture with mixtures containing 4-5 speakers to evaluate whether the concatenation-based embedding approach scales effectively beyond the 3-speaker scenarios studied.