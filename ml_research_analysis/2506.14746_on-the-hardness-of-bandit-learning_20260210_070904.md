---
ver: rpa2
title: On the Hardness of Bandit Learning
arxiv_id: '2506.14746'
source_url: https://arxiv.org/abs/2506.14746
tags:
- action
- algorithm
- such
- function
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the fundamental limitations of bandit
  learning by addressing two key questions: (1) which reward function classes are
  learnable, and (2) how can they be learned efficiently? The authors demonstrate
  that unlike classical PAC learning, no combinatorial dimension can fully characterize
  bandit learnability, even for finite classes, using a simple argument based on the
  finite-character property.'
---

# On the Hardness of Bandit Learning

## Quick Facts
- arXiv ID: 2506.14746
- Source URL: https://arxiv.org/abs/2506.14746
- Authors: Nataly Brukhim; Aldo Pacchiano; Miroslav Dudik; Robert Schapire
- Reference count: 40
- Primary result: No combinatorial dimension can characterize bandit learnability; bandit learning can be NP-hard even with efficient ERM/estimation; query complexity and regret are fundamentally incompatible.

## Executive Summary
This paper investigates fundamental limitations in bandit learning by examining the interplay between query complexity, regret, and computational hardness. The authors demonstrate that unlike classical PAC learning, no combinatorial dimension can fully characterize bandit learnability, even for finite function classes. They construct examples where identifying the optimal action requires minimal queries yet remains computationally intractable. The paper also reveals that query complexity and regret minimization are fundamentally incompatible objectives, with algorithms achieving optimal query complexity necessarily incurring linear regret.

## Method Summary
The paper employs theoretical constructions and reductions to prove hardness results in bandit learning. Key methods include: (1) constructing function classes where a single informative action reveals the optimal arm, demonstrating that combinatorial dimensions with the finite-character property cannot characterize learnability; (2) encoding SAT instances into reward functions to show NP-hardness even when standard learning operations are efficient; (3) building information-lock constructions to prove the incompatibility between query complexity and regret minimization; and (4) analyzing noise sensitivity by showing phase transitions in query complexity as noise variance crosses class-dependent thresholds.

## Key Results
- No combinatorial dimension with the finite-character property can characterize bandit learnability, even for finite classes
- Bandit learning can be NP-hard when query complexity is minimal and ERM/estimation/maximization are efficient
- Achieving optimal query complexity necessarily incurs linear regret, making query complexity and regret fundamentally incompatible objectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: No combinatorial dimension with the finite-character property can characterize bandit learnability, even for finite function classes.
- Mechanism: The proof constructs a counterexample by taking any class F with dimension D(F) ≥ d and adding an "informative action" x₀ that reveals the optimal action. The modified class F' has query complexity ≤ 2 but maintains the same dimension D(F') ≥ d because the shattering structure is preserved on the original points.
- Core assumption: The dimension D satisfies the finite-character property (i.e., D(F) ≥ d can be demonstrated by finitely many domain points and class members).
- Evidence anchors:
  - [abstract] "We give a simple proof demonstrating that no combinatorial dimension can characterize bandit learnability, even in finite classes"
  - [Section 3, Theorem 4] Formal construction and proof that any finite-character dimension fails
  - [corpus] Related work on EMX learnability (Hanneke and Yang, 2023) shows bandit learnability can be undecidable, but this paper provides a simpler, direct argument within standard ZFC
- Break condition: If you restrict to dimensions without the finite-character property, or to specific subclasses of bandit problems (e.g., highly noisy settings where Hanneke and Wang 2024's γ parameter applies), the result may not hold.

### Mechanism 2
- Claim: Bandit learning can be computationally hard (NP-hard) even when query complexity is minimal and standard learning operations (ERM, estimation, maximization) are efficient.
- Mechanism: The construction encodes SAT instances into a reward function class. An action * encodes the 3CNF formula ϕ. If ϕ is satisfiable, querying the minimal satisfying assignment reveals a code c that identifies the optimal action. Finding c requires solving SAT, but ERM/estimation/maximization only need to return consistent functions or decode known information—not solve SAT from scratch.
- Core assumption: RP ≠ NP (standard complexity assumption).
- Evidence anchors:
  - [abstract] "We construct a reward function class for which at most two queries are needed to find the optimal action, yet no algorithm can do so in polynomial time unless RP = NP"
  - [Section 4, Theorem 9] Formal reduction from SAT to bandit learning with explicit construction of F_n
  - [corpus] Corpus has limited direct evidence on computational hardness in bandits; related work focuses on statistical characterizations rather than computational barriers
- Break condition: If RP = NP, or if the learner has access to a SAT oracle, or if you restrict to function classes with specific structure (e.g., linear rewards), the hardness result may not apply.

### Mechanism 3
- Claim: Query complexity and regret minimization are fundamentally incompatible—algorithms achieving optimal query complexity must incur linear regret.
- Mechanism: The "information lock" construction creates action sets where high-regret actions encode the identity of the optimal arm. Any query-optimal algorithm must pull these informative (high-regret) actions, incurring regret proportional to query complexity. Meanwhile, regret-optimal algorithms (like UCB) achieve O(√T) regret but cannot identify the optimal arm in o(√T) queries.
- Core assumption: Gaussian noise with variance σ = 1 (or similar); specific parameter relationships between informative and optimal actions.
- Evidence anchors:
  - [abstract] "Proving that achieving optimal query complexity necessarily incurs linear regret"
  - [Section 6.1, Theorem 17] Formal separation with explicit bounds: QC ≤ 80d but regret ≥ d/128 for any algorithm
  - [corpus] Bubeck et al. (2011) showed related regret/simple regret trade-offs; this paper extends to query complexity specifically
- Break condition: If the action space has different structure (e.g., uniform information across arms), or if you optimize for simple regret rather than query complexity, the trade-off may weaken.

## Foundational Learning

- Concept: **VC Dimension and PAC Learnability**
  - Why needed here: The paper positions its results in contrast to classical PAC learning, where VC dimension fully characterizes learnability. Understanding this baseline clarifies why bandit learning is fundamentally harder.
  - Quick check question: Can you explain why VC dimension characterizes binary classification learnability but fails for bandits?

- Concept: **Bandit Learning vs. Best-Arm Identification (BAI)**
  - Why needed here: The paper specifically studies BAI under structured reward classes F, not standard multi-armed bandits. The structure of F is central to all results.
  - Quick check question: What is the difference between standard K-armed bandits and structured bandits with a function class F?

- Concept: **Query Complexity and Regret**
  - Why needed here: The paper proves these two objectives are incompatible. Understanding both metrics is essential to interpret the trade-off results.
  - Quick check question: Why might minimizing queries conflict with minimizing regret?

## Architecture Onboarding

- Component map:
  - Function class F: Defines the structured reward space; properties (finite character, gap, γ parameter) determine learnability
  - Action set A: Finite or infinite; contains "informative actions" that encode optimal arm identity
  - Noise model: Gaussian N(0, σ²); σ controls regime shifts in query complexity
  - Algorithm primitives: ERM (consistency), online estimation, maximization—these can be efficient even when bandit learning is hard

- Critical path:
  1. Characterize the function class F (determine if it has informative actions, gap structure, γ parameter)
  2. Assess noise level σ relative to class-dependent thresholds (low-noise regime: QC follows noise-free; high-noise: use γ)
  3. Choose objective: query complexity (BAI) vs. regret minimization—cannot optimize both simultaneously

- Design tradeoffs:
  - Low noise (σ small): Query complexity approaches noise-free bounds; use informative actions
  - High noise (σ large): Must rely on γ parameter; bounds can have exponential gaps
  - Computational efficiency: Even with efficient ERM/estimation, bandit learning may be NP-hard
  - Query complexity vs. regret: Optimal query algorithms incur linear regret; regret-optimal algorithms need O(√T) queries

- Failure signatures:
  - Attempting to apply a single combinatorial dimension (like eluder dimension) to characterize learnability—will fail per Theorem 4
  - Assuming efficient ERM implies efficient bandit learning—false per Theorem 9
  - Designing algorithms to minimize both query complexity and regret—incompatible per Theorem 17
  - Ignoring noise regime: algorithms tuned for one regime may catastrophically fail in another

- First 3 experiments:
  1. Implement the SAT-based hardness construction (Section 4) on small n to verify the gap between query complexity (2) and computational difficulty; confirm ERM/estimation remain efficient.
  2. Test the noise sensitivity threshold: take the "informative action" class from Proposition 12 or Theorem 14, sweep σ from 0 to 1, and observe the phase transition in query complexity.
  3. Reproduce the regret/query trade-off: implement the information-lock construction (Section 6.1) with d = 100, compare a query-optimal algorithm against UCB, measure both query success rate and cumulative regret over T = 1000 rounds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise characterization of the relationship between noise variance and query complexity across arbitrary function classes, and can we identify phase transitions between low-noise and high-noise regimes?
- Basis in paper: [explicit] "Understanding the broader interplay between noise variance and query complexity across arbitrary function classes remains an open and interesting direction for future research." (Page 3)
- Why unresolved: The paper shows that intermediate noise regimes are "largely unaddressed" by prior work, and while they identify thresholds for specific function families, a general theory is lacking.
- What evidence would resolve it: A unified framework (potentially a new dimension or parameter) that smoothly captures query complexity as a function of σ across all function classes, with identified threshold phenomena.

### Open Question 2
- Question: Under what sufficient conditions on function classes F can bandit learning be achieved in polynomial time, given access to standard oracles such as ERM and function maximization?
- Basis in paper: [inferred] The paper proves computational hardness even when ERM and maximization are efficient (Theorem 9), but does not provide positive conditions under which efficient bandit learning is possible.
- Why unresolved: The hardness construction is specific; no general tractability criteria are established.
- What evidence would resolve it: Identification of structural properties (e.g., convexity, decomposability) that guarantee polynomial-time bandit learning with standard oracle access.

### Open Question 3
- Question: Can the gap between query complexity and regret minimization be quantified more precisely, and are there intermediate objectives that admit simultaneous optimization?
- Basis in paper: [explicit] The paper proves that no algorithm can simultaneously achieve optimal query complexity and optimal regret (Theorem 17). The question of whether intermediate trade-off points exist is not addressed.
- Why unresolved: The separation result is binary (linear vs. optimal), leaving the Pareto frontier unexplored.
- What evidence would resolve it: A trade-off curve characterizing achievable (query complexity, regret) pairs, or proof that no Pareto-optimal points exist beyond the extremes.

## Limitations

- The computational hardness results rely on the unproven assumption RP ≠ NP
- Several bounds contain unspecified constants that affect precise phase transition behavior
- The finite-character property argument may have edge cases with unusual dimension definitions

## Confidence

- **High Confidence**: The impossibility of combinatorial characterization (Theorem 4) and the computational hardness construction (Theorem 9) - these follow from well-established complexity theory principles and have clean, verifiable constructions
- **Medium Confidence**: The noise sensitivity results (Theorems 12, 14) - the qualitative phase transition is clear, but exact thresholds depend on unspecified constants that could affect practical implementation
- **Medium Confidence**: The regret-query complexity trade-off (Theorem 17) - the information lock mechanism is sound, but specific regret bounds depend on parameter choices that could be tightened

## Next Checks

1. **Reconstruct the SAT reduction**: Implement the exact encoding/decoding functions for 3-CNF formulas and verify that the 2-query upper bound algorithm correctly identifies the optimal action while polynomial-time algorithms fail with high probability on moderate-sized instances (n ≤ 20).

2. **Test noise threshold empirically**: Implement the informative action class from Theorem 14 with K = 100, sweep σ² values across the theoretical threshold range, and measure the phase transition in query complexity from O(1) to O(K^{2/3}σ²).

3. **Validate regret-query trade-off**: Build the information-lock construction with d = 50, implement both a query-optimal algorithm and UCB, and measure cumulative regret over T = 1000 rounds to verify the linear regret lower bound for query-optimal algorithms versus O(√T) regret for UCB.