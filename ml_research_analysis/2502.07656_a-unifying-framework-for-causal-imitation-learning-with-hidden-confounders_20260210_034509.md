---
ver: rpa2
title: A Unifying Framework for Causal Imitation Learning with Hidden Confounders
arxiv_id: '2502.07656'
source_url: https://arxiv.org/abs/2502.07656
tags:
- expert
- learning
- which
- causal
- confounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a general framework for causal imitation learning
  with hidden confounders that unifies and generalizes several existing settings.
  The key idea is to distinguish between expert-observable confounders (known to the
  expert but not the imitator) and expert-unobservable confounders (hidden from both).
---

# A Unifying Framework for Causal Imitation Learning with Hidden Confounders

## Quick Facts
- **arXiv ID**: 2502.07656
- **Source URL**: https://arxiv.org/abs/2502.07656
- **Reference count**: 36
- **Primary result**: Proposes DML-IL algorithm that outperforms state-of-the-art causal IL baselines by explicitly handling both expert-observable and expert-unobservable confounders

## Executive Summary
This paper introduces a general framework for causal imitation learning that unifies several existing settings by distinguishing between expert-observable confounders (known to expert but not imitator) and expert-unobservable confounders (hidden from both). The authors reformulate the problem as a Conditional Moment Restriction (CMR) problem using trajectory histories as instrumental variables, and propose a novel algorithm called DML-IL that solves it through instrumental variable regression with double machine learning. The algorithm theoretically bounds the imitation gap and empirically demonstrates superior performance compared to baselines, especially when both types of confounders are present.

## Method Summary
The method distinguishes between two types of hidden confounders: expert-observable confounders (known to the expert but not the imitator) and expert-unobservable confounders (hidden from both). The authors reformulate the imitation learning problem as a CMR problem where trajectory histories serve as instrumental variables. They propose DML-IL, a two-stage algorithm that first fits a rollout model to predict state-action pairs from historical trajectories, then trains a policy on this de-confounded data. The algorithm uses double machine learning techniques to handle the bias introduced by using machine learning models in the pipeline.

## Key Results
- DML-IL outperforms state-of-the-art causal imitation learning baselines, especially when both types of confounders are present
- The imitation gap is theoretically bounded and shown to recover prior results as special cases
- Empirical evaluations on toy environments and Mujoco tasks demonstrate the framework's effectiveness
- The results highlight the importance of explicitly accounting for both expert-observable and expert-unobservable confounding variables

## Why This Works (Mechanism)

### Mechanism 1: Instrumental Variable Regression via History
Using trajectory history as an instrumental variable decouples confounding noise from the causal effect of state on action. The $k$-step history $h_{t-k}$ serves as an instrument for current state $s_t$, satisfying IV conditions when confounding noise is independent across time steps. This allows isolation of the causal relationship $\pi_E(s_t, u_o)$.

### Mechanism 2: Context Inference via History-Dependent Policy
A history-dependent policy $\pi_h(h_t)$ can infer expert-observable confounders $u_o$ missing from current state observations. The trajectory history encodes information about $u_o$, allowing the policy to reconstruct missing context and match the expert's conditional action distribution.

### Mechanism 3: Rollout Model for Conditional Moment Restriction
A learned rollout model prevents "forbidden regression" by generating counterfactual trajectories that satisfy the CMR objective. The two-stage process (training rollout model then policy) ensures the regressor uses clean conditional distributions derived from the instrument.

## Foundational Learning

**Instrumental Variables (IV)**: Statistical tool to recover causal effects when treatment and outcome are influenced by unobserved variables. Quick check: Why does standard regression fail when error term correlates with input variable, and how does an IV fix this?

**Conditional Moment Restrictions (CMR)**: Mathematical framework for solving IV problems with machine learning. Quick check: How does a CMR differ from standard Mean Squared Error loss minimization?

**Double Machine Learning (DML)**: Technique to handle bias from using machine learning models in two-stage pipelines. Quick check: Why is it problematic to train two neural networks in a pipeline without cross-fitting or orthogonalization?

## Architecture Onboarding

**Component map**: Trajectory History $h_{t-k}$ (Instrument) → Rollout Model $\hat{M}$ (GMM/Neural Net) → Policy Network $\hat{\pi}_h$ (MLP)

**Critical path**: Accuracy of Rollout Model ($\hat{M}$) is the bottleneck. If Stage 1 fails to model $P(h_t|h_{t-k})$ accurately, the instrument becomes "broken" for Stage 2.

**Design tradeoffs**: 
- Horizon $k$: Increasing $k$ weakens instrument but ensures unconfoundedness more robustly
- Model Capacity: Complex rollout model fits dynamics better but may overfit noise

**Failure signatures**:
- Weak Instrument: High variance in performance; loss plateaus but reward remains low
- IV Violation: Algorithm converges but mimics wrong behavior
- Model Misspecification: Rollout model fails to capture conditional distribution

**First 3 experiments**:
1. Implement "Plane Ticket" environment to verify BC fails and DML-IL recovers expert policy
2. Run ablation on horizon $k$ to visualize tradeoff between instrument strength and validity
3. Misspecify $k$ to observe degradation of imitation gap and verify sensitivity analysis

## Open Questions the Paper Calls Out

**Open Question 1**: Can the framework extend to handle non-additive confounding noise while retaining identifiability? This requires proving identifiability or modifying the algorithm for multiplicative/non-linear noise structures.

**Open Question 2**: Is there a data-driven method to reliably estimate the confounding noise horizon $k$ without relying solely on domain knowledge? This needs a statistical validation technique to determine optimal $k$ from dataset.

**Open Question 3**: How can the framework adapt to handle invalid instruments when trajectory history remains correlated with confounding noise? This requires an extension that remains robust to weak correlations between instrument and hidden confounders.

## Limitations

- Assumes additive confounding noise with finite horizon $k$, which may not hold in all scenarios
- Performance heavily depends on accuracy of rollout model, which may not perfectly capture complex dynamics
- Choice of history length $k+3$ appears somewhat arbitrary with limited characterization of sensitivity

## Confidence

**High Confidence**: Theoretical formulation of CMR problem and IV connection is well-established; experimental results showing DML-IL outperforming baselines are robust across multiple MuJoCo tasks.

**Medium Confidence**: Assumption that trajectory history contains sufficient information to infer expert-observable confounders relies on specific temporal correlations that may not generalize; GMM rollout model choice is reasonable but not theoretically optimal.

**Low Confidence**: Generalization to continuous, high-dimensional state spaces from simple toy environment is not rigorously established; impact of model misspecification (e.g., incorrect $k$) is discussed but not extensively validated.

## Next Checks

1. **Stress Test with Incorrect Horizon**: Systematically test DML-IL with misspecified $k$ values to quantify robustness threshold and identify failure modes.

2. **Transfer Learning Evaluation**: Evaluate whether policy trained on confounded environment can transfer to unconfounded version, measuring performance retention and adaptation speed.

3. **Alternative Rollout Models**: Replace GMM rollout model with neural network architecture (e.g., Transformer-based) to assess whether improved conditional density estimation yields better imitation performance.