---
ver: rpa2
title: Improving Coverage in Combined Prediction Sets with Weighted p-values
arxiv_id: '2505.11785'
source_url: https://arxiv.org/abs/2505.11785
tags:
- coverage
- prediction
- aggregation
- weighted
- sets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a weighted p-value framework for aggregating\
  \ conformal prediction sets, improving coverage guarantees beyond the standard 1-2\u03B1\
  \ bound. The method assigns weights to individual prediction sets based on their\
  \ importance, allowing the overall coverage guarantee to interpolate between 1-2\u03B1\
  \ and 1-\u03B1 depending on the weight distribution."
---

# Improving Coverage in Combined Prediction Sets with Weighted p-values

## Quick Facts
- arXiv ID: 2505.11785
- Source URL: https://arxiv.org/abs/2505.11785
- Reference count: 40
- Primary result: Weighted p-value framework improves coverage bounds beyond 1-2α in conformal prediction, achieving interpolation between 1-2α and 1-α depending on weight distribution

## Executive Summary
This paper introduces a weighted p-value framework for aggregating conformal prediction sets, improving coverage guarantees beyond the standard 1-2α bound. The method assigns weights to individual prediction sets based on their importance, allowing the overall coverage guarantee to interpolate between 1-2α and 1-α depending on the weight distribution. The framework extends to data-dependent weights through a linear transformation that maintains finite-sample validity, enabling adaptive coverage in settings like mixture-of-experts. Experiments demonstrate improved worst-slice coverage on regression and classification tasks, with more consistent coverage across demographic groups compared to standard split conformal methods. The approach provides both flexibility in coverage guarantees and better local validity in challenging regions of the input space.

## Method Summary
The method reformulates conformal prediction in terms of p-value functions rather than quantiles, then aggregates these p-values using weighted averages. For data-independent weights, the coverage guarantee interpolates between 1-2α and 1-α based on weight concentration. For data-dependent weights (e.g., from a routing network), the framework introduces a correction factor estimated from a held-out merging set to maintain finite-sample validity. The correction factor is computed as the supremum ratio between the empirical CDF and the identity function, ensuring the scaled p-value remains valid. At test time, the method aggregates p-values with test-specific weights, applies the correction factor, and thresholds to form the final prediction set.

## Key Results
- Weighted aggregation achieves coverage bounds of 1-min{1/v, 2}α where v is the maximum weight, improving upon the standard 1-2α bound when weights are asymmetric
- Data-dependent weights with linear correction maintain finite-sample validity with 1-(α+ε+δ) coverage guarantee
- On synthetic and real datasets, weighted aggregation reduces worst-slice coverage gap compared to standard split conformal
- More consistent coverage across demographic groups in the Communities dataset, with improved worst-slice performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighted aggregation of prediction sets achieves tighter coverage bounds than standard 1−2α when weights are asymmetric.
- Mechanism: The method assigns weights (v₁,...,vₖ) to K prediction sets and aggregates their p-value functions via weighted average. By applying Vovk and Wang's merging function theory, the coverage guarantee becomes 1−min(1/v, 2)α, where v = max(v₁,...,vₖ). When one predictor dominates (v > 0.5), coverage approaches 1−α rather than 1−2α.
- Core assumption: Exchangeability between calibration and test data; weights are data-independent (fixed or determined independently of the p-values).
- Evidence anchors:
  - [abstract] "achieving tighter coverage bounds that interpolate between the 1−2α guarantee of the combined models and the 1−α guarantee of an individual model depending on the distribution of weights"
  - [Proposition 4.1] Formal guarantee: P(Yₙ₊₁ ∈ Ĉ^avg_α) ≥ 1−min{1/v, 2}α
  - [corpus] Gasparin and Ramdas (2024) derived only 1−2α for weighted majority vote; this work strengthens the bound via p-value formulation.
- Break condition: If weights become nearly uniform (v ≈ 1/K for large K), the guarantee degrades toward 1−2α, losing advantage over symmetric aggregation.

### Mechanism 2
- Claim: Data-dependent weights can be incorporated while maintaining finite-sample validity through a learned linear scaling correction.
- Mechanism: When weights W depend on the data, the weighted p-value average P_all is no longer a valid p-variable. The method computes a correction factor m* = sup_{δ>0} F_{P_all}(δ)/δ that linearly rescales P_all into a valid p-variable. Empirically, m̂* is estimated from a designated "merging set" S_merge using the empirical CDF.
- Core assumption: The merging set S_merge is exchangeable with test data; sufficient samples to estimate the CDF accurately (DKW inequality bounds estimation error).
- Evidence anchors:
  - [Proposition 5.2] "P(Yₙ₊₁ ∈ Ĉ^scaled_α) ≥ 1−(α+ε+δ)" where ε = √(log(2/δ)/(2|S_merge|))
  - [Algorithm 1] Step 2 explicitly computes correction factor from merging set
  - [corpus] No directly comparable corpus work for data-dependent p-value correction in conformal prediction.
- Break condition: If |S_merge| is too small (<100-200 samples), the correction becomes overly conservative, leading to overcoverage and unnecessarily large prediction sets.

### Mechanism 3
- Claim: In mixture-of-experts settings, learned routing weights enable adaptive coverage that improves local validity over standard conformal methods.
- Mechanism: The MoE routing network produces input-dependent weights W(x) for each expert. These weights directly scale each expert's p-value function in aggregation. The correction factor preserves validity while allowing the aggregate set to follow the dominant expert at each input point, achieving coverage closer to the best-performing expert locally.
- Core assumption: Routing weights reflect expert reliability (trained to minimize prediction error); experts have complementary strengths across the input space.
- Evidence anchors:
  - [Figure 3] Weighted aggregation variants achieve WS coverage close to nominal 1−α, while split conformal undercovers significantly
  - [Section 6] "weighted aggregation yields more reliable coverage in challenging settings"
  - [corpus] Class conditional conformal (arXiv:2507.07150) uses p-value aggregation for multi-input settings, but does not address data-dependent weights or MoE.
- Break condition: If routing weights are poorly calibrated or experts are redundant (high feature overlap without specialization), adaptivity gains diminish; coverage becomes more conservative without local validity benefits.

## Foundational Learning

- Concept: **P-values as random variables (p-variables)**
  - Why needed here: The entire framework builds on treating p-values as random variables satisfying P(P' ≤ α) ≤ α. Understanding this definition is essential to grasp why the weighted average requires correction and how linear scaling restores validity.
  - Quick check question: Given a random variable P that is uniformly distributed on [0,1], verify it satisfies the p-variable definition for α = 0.1.

- Concept: **Split conformal prediction with quantile and p-value formulations**
  - Why needed here: The method reformulates conformal prediction in terms of p-value functions rather than quantiles. You must understand both perspectives to see why p-value aggregation enables tighter guarantees than set-based aggregation.
  - Quick check question: For a calibration set with scores {0.5, 1.2, 0.8, 2.1}, compute both the (1−0.1)-quantile and the p-value function for a test score of 1.0.

- Concept: **Exchangeability assumption**
  - Why needed here: All coverage guarantees in this paper (and conformal prediction generally) rely on exchangeability between training, calibration, merging, and test data. The correction factor derivation explicitly conditions on the merging set and marginalizes over it.
  - Quick check question: If test data is drawn from a shifted distribution (e.g., higher noise variance) than calibration data, what happens to coverage guarantees?

## Architecture Onboarding

- Component map: K predictors (MoE experts) -> p-value functions -> Weight source (fixed or routing network) -> Aggregated p-value function -> Merging set -> Correction factor -> Final prediction set

- Critical path:
  1. Train predictors and routing network on training data
  2. Compute individual p-value functions on respective calibration sets
  3. For each point in S_merge: compute aggregated p-value using that point's routing weights
  4. Build empirical CDF of aggregated p-values; compute m̂* = max_i F̂(p_i)/p_i
  5. At test time: aggregate p-values with test input's routing weights, apply m̂* scaling, threshold at α

- Design tradeoffs:
  - **Merging set size**: Larger |S_merge| → tighter coverage (less conservative) but requires more held-out data. Paper shows ~160 samples sufficient for <3% overcoverage.
  - **Guarantee type**: WA targeted (0, α'] guarantees coverage for all α ≤ α' (more conservative); WA precise guarantees only at α' (tighter sets, weaker guarantee).
  - **Feature overlap among experts**: More overlap → smaller prediction sets but less adaptivity; partitioned features → better local validity but larger sets.

- Failure signatures:
  - **Severe overcoverage (>95% when targeting 90%)**: Likely |S_merge| too small; increase to 160+ samples.
  - **Undercoverage on specific subgroups**: Routing weights may not reflect expert reliability for those inputs; consider calibrating weights or using CQR scores.
  - **Unbounded prediction sets (100% coverage)**: DKW-based finite-sample correction (ε from Eq. in B.2.3) can exceed α when |S_merge| < 1000; use WA precise or reduce δ.

- First 3 experiments:
  1. **Synthetic regression with controlled weight asymmetry**: Generate data with known expert specializations; vary weight concentration (v from 0.3 to 1.0) and verify coverage tracks 1−min(1/v, 2)α as claimed in Prop. 4.1.
  2. **Merging set ablation**: On Airfoil dataset with fixed MoE architecture, sweep |S_merge| ∈ {40, 80, 160, 320} and plot marginal coverage vs. prediction set size to identify practical minimum.
  3. **Demographic subgroup coverage**: On Communities dataset, compare split conformal vs. WA targeted across racial groups; quantify reduction in coverage disparity (Δ between best and worst subgroup).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can tighter finite-sample corrections be developed for the weighted p-value aggregation that reduce overcoverage while maintaining validity guarantees?
- Basis in paper: [explicit] The paper states "guarantees on coverage for all α can lead to overly conservative prediction sets, which may be unnecessarily restrictive in practice when we do not need guarantees for every possible significance level." They propose m† and m‡ alternatives but note these give "much looser guarantee" (WA precise).
- Why unresolved: The DKW-derived bounds (ε term in Proposition 5.2) are known to be loose, and the comparison with Stutz et al.'s ECDF-DKW method showed extreme conservativeness (Figure 14 shows ε > α for merging sets <1000 samples).
- What evidence would resolve it: A new correction scheme that achieves empirical coverage closer to 1-α on WS slices without sacrificing finite-sample validity, demonstrated across multiple datasets and calibration set sizes.

### Open Question 2
- Question: How does weighted aggregation performance scale with expert model complexity beyond the linear models tested?
- Basis in paper: [explicit] "In our experiments on UCI data, we allocate 400 samples for train/calibration/merge... we use linear models for both the experts and the routing networks. To isolate the effects of aggregation from model complexity, we use linear models..."
- Why unresolved: The authors deliberately restricted experiments to linear models to isolate aggregation effects, leaving performance with deep neural network experts or large MoE architectures untested.
- What evidence would resolve it: Experiments on modern MoE architectures (e.g., sparse transformers) showing whether the improved WS coverage and demographic consistency persist with nonlinear experts.

### Open Question 3
- Question: What is the optimal design of expert feature assignments to balance coverage guarantees and prediction set efficiency?
- Basis in paper: [inferred] Figure 5 shows that greater feature overlap leads to higher coverage but more efficient (smaller) prediction sets, while no overlap yields tighter coverage but larger sets. The authors note "feature sharing is an important design consideration when aggregating prediction sets from multiple models" but do not provide guidance on optimal assignment.
- Why unresolved: The trade-off between information redundancy (via overlap) and expert specialization remains uncharacterized theoretically or empirically optimized.
- What evidence would resolve it: A principled framework (theoretical or empirical) for selecting feature assignments that minimize expected prediction set size subject to coverage constraints, validated on diverse MoE configurations.

## Limitations
- The correction factor estimation requires a sufficiently large merging set (typically 160+ samples), which may be prohibitive in small-data regimes
- The approach assumes routing weights reliably reflect expert reliability, but poor weight calibration could lead to coverage degradation without clear failure signals
- All coverage guarantees rely on exchangeability assumptions that may not hold under covariate shift or domain adaptation scenarios

## Confidence
- **High confidence**: The weighted p-value aggregation mechanism for data-independent weights achieving coverage bounds of 1-min{1/v,2}α (Proposition 4.1). This follows directly from established p-variable theory and merging function properties.
- **Medium confidence**: The linear correction factor approach for data-dependent weights maintaining finite-sample validity (Proposition 5.2). While theoretically sound, practical performance depends on accurate CDF estimation which requires sufficient S_merge samples.
- **Medium confidence**: The experimental demonstration of improved worst-slice coverage in mixture-of-experts settings. The results show clear improvement but depend on the specific MoE architecture and weight calibration.

## Next Checks
1. **Convergence analysis of correction factor**: Systematically vary merging set size |S_merge| ∈ {40, 80, 160, 320} on Airfoil dataset and plot both the correction factor m̂* and resulting coverage to identify the practical minimum required for tight coverage bounds.

2. **Weight calibration validation**: Generate synthetic regression data with known expert specializations, then deliberately mis-calibrate routing weights (systematic bias) and measure degradation in coverage uniformity across the input space to quantify sensitivity to weight quality.

3. **Cross-domain robustness test**: Apply the framework to a dataset with known covariate shift (e.g., train on one geographic region, test on another in Communities dataset) and measure coverage degradation compared to standard split conformal to validate exchangeability assumptions under distribution shift.