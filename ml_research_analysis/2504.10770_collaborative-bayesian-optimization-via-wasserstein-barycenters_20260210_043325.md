---
ver: rpa2
title: Collaborative Bayesian Optimization via Wasserstein Barycenters
arxiv_id: '2504.10770'
source_url: https://arxiv.org/abs/2504.10770
tags:
- function
- co-kg
- data
- agents
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses collaborative Bayesian optimization (BO) with
  data privacy constraints, where multiple agents must jointly optimize an unknown
  function without sharing their raw data. The core method constructs a central Gaussian
  process (GP) surrogate model via Wasserstein barycenters of local GP models, enabling
  collaboration without exposing data.
---

# Collaborative Bayesian Optimization via Wasserstein Barycenters
## Quick Facts
- arXiv ID: 2504.10770
- Source URL: https://arxiv.org/abs/2504.10770
- Reference count: 40
- Collaborative BO using Wasserstein barycenters achieves privacy-preserving performance comparable to centralized methods

## Executive Summary
This paper introduces a privacy-preserving framework for collaborative Bayesian optimization where multiple agents optimize an unknown function without sharing their raw data. The key innovation is constructing a central Gaussian process surrogate model through Wasserstein barycenters of local GP models, enabling data privacy while maintaining optimization performance. The authors develop a novel collaborative acquisition function, Co-KG, which balances exploitation and exploration by combining central and local models with a time-varying hyperparameter. Theoretical analysis proves asymptotic consistency, and empirical results demonstrate competitive performance against non-private centralized BO and other collaborative strategies.

## Method Summary
The method constructs a central GP model by computing Wasserstein barycenters of local GP posteriors, allowing agents to collaborate without exposing their data. A novel acquisition function, Co-KG, balances exploitation and exploration by combining the central model with local models, weighted by a time-varying hyperparameter βt. The algorithm iteratively updates the central model using Monte Carlo approximations of the Wasserstein barycenter, ensuring data privacy while maintaining optimization performance. Theoretical analysis establishes asymptotic consistency, and empirical validation demonstrates effectiveness across synthetic and real-world optimization tasks.

## Key Results
- Co-KG acquisition function outperforms other collaborative acquisition strategies
- Privacy-preserving collaborative BO achieves performance comparable to non-private centralized BO
- Successfully tunes neural network hyperparameters on real datasets while preserving data privacy

## Why This Works (Mechanism)
The approach leverages the mathematical properties of Wasserstein barycenters to aggregate local GP models without requiring data sharing. The Co-KG acquisition function strategically balances exploration of the central model with exploitation of local models through the βt hyperparameter. Monte Carlo approximation enables practical computation of Wasserstein barycenters while maintaining theoretical guarantees. The time-varying βt schedule ensures that exploration is prioritized early in optimization while exploitation becomes more important as the algorithm converges.

## Foundational Learning
- Wasserstein barycenters: Used to aggregate probability distributions while preserving their geometric properties. Needed for combining local GP models without data sharing. Quick check: Verify the barycenter computation maintains mean and covariance structure of input distributions.
- Gaussian process regression: Core surrogate model for Bayesian optimization. Needed for uncertainty quantification and function approximation. Quick check: Ensure GP hyperparameters are appropriately tuned for each agent's data.
- Monte Carlo approximation: Enables practical computation of Wasserstein barycenters. Needed for scalability of the barycenter computation. Quick check: Verify convergence of Monte Carlo estimates with increasing sample size.
- Collaborative acquisition functions: Framework for balancing exploration and exploitation across multiple agents. Needed to coordinate optimization efforts without data sharing. Quick check: Test different βt scheduling strategies for optimization performance.
- Differential privacy concepts: Underlying privacy framework. Needed to ensure data confidentiality. Quick check: Assess whether explicit privacy guarantees could be added.

## Architecture Onboarding
- Component map: Local agents (GP models) -> Wasserstein barycenter computation -> Central GP model -> Co-KG acquisition -> Optimization decisions
- Critical path: Data collection → Local GP update → Barycenter computation → Co-KG evaluation → Next query point selection
- Design tradeoffs: Privacy preservation vs. optimization accuracy, computational complexity vs. sample efficiency
- Failure signatures: Poor barycenter approximation leading to suboptimal central model, inappropriate βt scheduling causing premature convergence or excessive exploration
- First experiments: (1) Verify barycenter computation accuracy on synthetic GP distributions, (2) Test Co-KG performance on standard benchmark functions, (3) Evaluate scalability with increasing number of agents

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes fixed hyperparameters without addressing estimation errors
- Scalability limited to small agent networks with unclear complexity for large deployments
- Additional hyperparameters (βt schedule) require careful tuning without systematic exploration
- Privacy guarantees are implicit through non-sharing rather than explicit differential privacy bounds

## Confidence
- Theoretical soundness: High
- Empirical validation: Medium
- Scalability analysis: Low
- Privacy guarantees: Medium

## Next Checks
- Analyze sensitivity to hyperparameter estimation errors in Wasserstein barycenter computation
- Conduct scalability experiments with 50+ agents and high-dimensional search spaces
- Implement explicit differential privacy guarantees with privacy budget tracking