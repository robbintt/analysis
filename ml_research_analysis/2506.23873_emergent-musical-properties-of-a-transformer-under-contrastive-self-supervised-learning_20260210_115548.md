---
ver: rpa2
title: Emergent musical properties of a transformer under contrastive self-supervised
  learning
arxiv_id: '2506.23873'
source_url: https://arxiv.org/abs/2506.23873
tags:
- tasks
- music
- tokens
- token
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether contrastive self-supervised learning
  (SSL) paired with transformers can capture both global and local musical properties
  in music information retrieval (MIR). While prior work suggests that contrastive
  SSL is effective for global tasks but inadequate for local tasks, the authors challenge
  this assumption by training a lightweight vision transformer with one-dimensional
  patches (ViT-1D) using only normalized temperature-scaled cross-entropy loss (NT-Xent)
  applied to the class token.
---

# Emergent musical properties of a transformer under contrastive self-supervised learning

## Quick Facts
- arXiv ID: 2506.23873
- Source URL: https://arxiv.org/abs/2506.23873
- Reference count: 0
- One-line primary result: ViT-1D trained with contrastive SSL shows emergent local musical properties in sequence tokens despite only class token being used in the loss function

## Executive Summary
This paper investigates whether contrastive self-supervised learning (SSL) paired with transformers can capture both global and local musical properties in music information retrieval (MIR). While prior work suggests that contrastive SSL is effective for global tasks but inadequate for local tasks, the authors challenge this assumption by training a lightweight vision transformer with one-dimensional patches (ViT-1D) using only normalized temperature-scaled cross-entropy loss (NT-Xent) applied to the class token. Despite this, they find that sequence tokens emerge with informative musical properties. On global tasks, averaging class and sequence tokens improves performance compared to using the class token alone, indicating useful properties in sequence tokens. On local tasks, sequence tokens perform unexpectedly well despite not being specifically trained for local features. Qualitative analyses of attention maps reveal temporal properties like onsets, while self-similarity matrices show different layers capture different musical dimensions (harmonic vs. rhythmic). The study advances understanding of transformer interpretability and demonstrates overlooked abilities of contrastive SSL for sequence modeling in MIR.

## Method Summary
The authors train a ViT-1D on mel-spectrogram segments (128 mel bins × 126 frames) using contrastive SSL with NT-Xent loss applied only to the class token. The model uses 1D patches via a convolutional layer projecting 128 mel bins to 192 dimensions, 12 transformer blocks with 3 heads, and 2D sinusoidal positional encoding. Sequence tokens are initialized as learnable parameters plus the average of all sequence tokens. After pretraining on Deezer catalog data, they freeze the encoder and attach linear probes for downstream tasks: global tasks (music tagging, key estimation) use either the class token or average of all tokens, while local tasks (beat tracking, chord estimation) use only sequence tokens. They analyze emergent properties through attention maps and self-similarity matrices across layers.

## Key Results
- Sequence tokens emerge with local musical properties (onsets, harmonic/rhythmic structure) despite only class token being used in contrastive loss
- Averaging class and sequence tokens outperforms class token alone on global tasks (tagging), showing sequence tokens carry useful information
- Sequence tokens perform well on local tasks (beat tracking, chord estimation) despite no local supervision during pretraining
- Different transformer layers specialize in different musical properties: shallow layers capture harmonic content, deeper layers capture rhythmic/temporal abstractions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Weight sharing between class token and sequence tokens propagates contrastive supervision to sequence tokens, enabling local property emergence despite loss being applied only to class tokens.
- **Mechanism**: NT-Xent loss gradients flow through shared MLP weights in transformer blocks. Since class token aggregates information from sequence tokens via attention, optimizing class token similarity implicitly requires sequence tokens to encode meaningful temporal structure—otherwise aggregation would yield uninformative class representations.
- **Core assumption**: Shared parameters create sufficient gradient coupling between class token optimization and sequence token representations.
- **Evidence anchors**:
  - [abstract]: "potentially thanks to weight sharing, informative musical properties emerge in ViT-1D's sequence tokens"
  - [Section 2]: "the class token (t=0) is processed in the same manner as sequence tokens: it shares weights with them in the multi-layer perception layers"
  - [corpus]: Limited direct corpus support; related work (Myna, CLMR) uses contrastive learning but doesn't isolate weight-sharing effects
- **Break condition**: If class token aggregation relies primarily on learned attention weights without requiring informative sequence tokens, emergent properties would diminish.

### Mechanism 2
- **Claim**: Hierarchical layer specialization emerges during contrastive training, with shallow layers capturing harmonic content and deeper layers capturing rhythmic/temporal abstractions.
- **Mechanism**: Attention patterns shift from local (diagonal) to global (vertical) across layers. Shallow layers attend to nearby frames, preserving local spectral similarity (harmonics). Deeper layers with broader attention capture temporal regularity (rhythm, beats). This emerges because harmonic similarity serves local discrimination while rhythmic patterns support global segment matching.
- **Core assumption**: Contrastive objective benefits differently from harmonic vs. rhythmic features at different abstraction levels.
- **Evidence anchors**:
  - [Section 6.1]: "S3 exhibits clearer block-like structures... S12 reveals clearer subdiagonal structures... characteristic of rhythmic patterns"
  - [Section 6.2]: Stacking layers (z3, z6, z9, z12) improves chord estimation (31.9% → 42.2%) but not beat tracking, "indicating shallow layers contribute to harmonic tasks"
  - [corpus]: DINOv2 (unsupervised ViT) shows similar layer-wise property emergence in vision domain
- **Break condition**: If pretraining dataset lacks harmonic/rhythmic diversity, or if positive pairs are too temporally distant, hierarchical specialization may not emerge.

### Mechanism 3
- **Claim**: Skip connections preserve raw spectral information while attention layers add learned temporal properties, creating complementary representations at each depth.
- **Mechanism**: Randomly initialized model's SSM already resembles mel-spectrogram SSM (harmonic structure visible, rhythmic structure absent). Skip connections propagate this raw information forward. Attention mechanisms then modulate these representations. Training shapes attention patterns (onset detection F=0.877 vs. random F=0.501) while skip connections ensure raw spectral content remains accessible.
- **Core assumption**: Raw spectrogram features are necessary building blocks that complement learned attention patterns.
- **Evidence anchors**:
  - [Section 6.2]: "Randomly initialized model contains harmonic information... Sr closely resembles the SSM of the model's input, the mel-spectrograms"
  - [Section 6.2]: Trained model significantly outperforms random on beat tracking (0.723 vs. 0.463) and tagging (0.417 vs. 0.273 mAP)
  - [corpus]: No direct corpus evidence on skip connection role in SSL transformers
- **Break condition**: If skip connections are removed, raw spectral access would be lost, potentially harming tasks requiring precise frequency resolution.

## Foundational Learning

- **Concept**: Contrastive self-supervised learning (SimCLR framework)
  - **Why needed here**: Understanding how NT-Xent loss creates embedding space structure without labels. Positive pairs (same-song segments) are pulled together; negatives (different songs) pushed apart. Temperature τ=0.1 controls distribution sharpness.
  - **Quick check question**: If you randomly sample two 4-second segments from the *same* song, should they have high or low cosine similarity in the learned embedding space?

- **Concept**: Vision Transformer (ViT) tokenization and class tokens
  - **Why needed here**: This paper uses 1D patches (all frequency bins per time frame = 1 token). Class token prepended to sequence aggregates information through attention. Understanding that tokens z_k^t represent transformer block k, time position t.
  - **Quick check question**: In a standard 2D ViT with 16×16 patches, how would you modify the patching for 1D spectrogram patches spanning 128 mel bins × 1 time frame?

- **Concept**: Self-similarity matrices (SSM) for music structure analysis
  - **Why needed here**: Paper uses SSMs S[i,j] = cos(z[i], z[j]) to visualize what each layer encodes. Block structures = harmonic similarity (same chord repeated). Subdiagonal patterns = rhythmic regularity (beats). Essential for interpreting layer behavior.
  - **Quick check question**: If a song has a repeating 4-beat pattern at 120 BPM, what geometric structure would appear in an SSM computed from beat-synchronous features?

## Architecture Onboarding

- **Component map**:
  Audio -> Mel-spectrogram (128 bins × 126 frames) -> 1D patching via conv layer -> 126 sequence tokens + class token -> 2D sinusoidal positional encoding -> 12 transformer blocks (3 heads, 192-dim) -> NT-Xent loss on class tokens only

- **Critical path**:
  1. Audio → mel-spectrogram → 1D patching → token sequence
  2. Add class token + positional encoding
  3. Pass through 12 transformer blocks (attention + MLP)
  4. Extract class tokens for positive pairs → compute NT-Xent
  5. For downstream: use sequence tokens (local tasks) or average all tokens (global tasks)

- **Design tradeoffs**:
  - **1D vs 2D patches**: 1D preserves full frequency context per frame but loses local time-frequency locality
  - **No projection head**: Sacrifices some downstream performance for cleaner emergent property study
  - **Class token as mean**: Stronger initialization than pure learnable, may accelerate convergence but constrains class token behavior
  - **4-second segments**: Balances computational cost with sufficient musical context for contrastive learning

- **Failure signatures**:
  - **Sequence tokens perform poorly on local tasks**: May indicate insufficient training epochs, learning rate too high, or positive pair sampling too aggressive (segments too distant in song)
  - **Attention maps show no structure (flat/uniform)**: Model likely undertrained or batch size too small for effective negative sampling
  - **SSMs identical to random initialization**: Suggests skip connections dominate but attention hasn't learned meaningful patterns
  - **Class token outperforms average significantly on global tasks**: Sequence tokens not contributing useful information → weight sharing ineffective or training insufficient

- **First 3 experiments**:
  1. **Reproduce emergent property baseline**: Train ViT-1D on small dataset (1000 songs) for 50 epochs. Verify sequence token performance on beat tracking and chord estimation matches paper trends. Compare class token vs. average pooling on tagging. Check attention maps for diagonal→vertical transition across layers.
  2. **Ablate weight sharing**: Modify architecture to use separate MLP weights for class token. Hypothesis: sequence token performance should drop significantly on local tasks if weight sharing is critical mechanism.
  3. **Layer-wise probing**: Train separate linear probes on each of the 12 transformer layers for chord estimation and beat tracking. Expect: chords peak around layers 3-6 (harmonic), beats peak around layers 9-12 (rhythmic). This validates hierarchical specialization claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do similar emergent local musical properties appear in transformers trained with supervised pretraining, or are they specific to self-supervised contrastive learning?
- Basis in paper: [explicit] "Future work could further study emergent properties in all layers and whether similar emergent properties exist in supervised pretraining."
- Why unresolved: The authors only evaluated contrastively trained models; the comparison to supervised pretraining was not conducted.
- What evidence would resolve it: Train an identical ViT-1D architecture with supervised labels (e.g., tagging) and probe sequence tokens on local tasks while comparing attention patterns and SSMs.

### Open Question 2
- Question: What is the complete layer-wise progression of emergent musical properties across all transformer layers, and how can this be optimally leveraged?
- Basis in paper: [explicit] "A more comprehensive analysis of all 12 layers, as well as the potential performance gains from leveraging all layers, is left for future work."
- Why unresolved: Only layers 3, 6, 9, and 12 were analyzed as representative points.
- What evidence would resolve it: Systematic probing of all 12 intermediate layers on downstream tasks and analysis of attention/SSM properties at each layer.

### Open Question 3
- Question: What is the precise mechanism by which local temporal properties emerge in sequence tokens when NT-Xent loss operates only on the class token?
- Basis in paper: [inferred] The authors attribute this to "potentially thanks to weight sharing" but do not empirically validate this hypothesis or rule out alternative mechanisms.
- Why unresolved: The paper demonstrates that emergence occurs but does not isolate which architectural or optimization factors cause it.
- What evidence would resolve it: Ablation studies varying weight sharing, attention connectivity, and training dynamics to identify necessary conditions for emergence.

### Open Question 4
- Question: Can incorporating intermediate layer representations during contrastive pretraining lead to more efficient training strategies?
- Basis in paper: [explicit] "Additionally, leveraging these properties in contrastive pretraining could lead to more efficient pretraining strategies."
- Why unresolved: The paper only uses intermediate layers for downstream probing, not for modifying the pretraining objective.
- What evidence would resolve it: Design pretraining objectives that explicitly supervise intermediate layers or use multi-layer contrastive losses, then compare convergence speed and final performance.

## Limitations

- Weight sharing mechanism for emergent properties is asserted but not experimentally validated through ablation studies
- 1D patch architecture choice lacks comparison to standard 2D ViT patches to establish specific advantages for musical representation learning
- No projection head likely limits absolute downstream performance compared to state-of-the-art contrastive models

## Confidence

- **High confidence**: The finding that sequence tokens emerge with local musical properties despite only class token being used in the contrastive loss objective. This is directly observed and quantitatively validated across multiple local tasks.
- **Medium confidence**: The hierarchical layer specialization claim (shallow layers for harmony, deep layers for rhythm). While supported by qualitative SSM analysis and quantitative probing results, the interpretation could benefit from additional controls.
- **Medium confidence**: The role of skip connections in preserving raw spectral information. The comparison to random initialization provides suggestive evidence, but the mechanism could be more rigorously tested through targeted ablation studies.

## Next Checks

1. **Weight sharing ablation**: Modify the architecture to use separate MLP weights for class token and sequence tokens. If sequence token performance on local tasks drops significantly while class token performance on global tasks remains stable, this would provide direct evidence for the weight-sharing mechanism.

2. **2D vs 1D patch comparison**: Implement the same contrastive training procedure using standard 2D ViT patches (e.g., 16×16 time-frequency patches). Compare both the emergence of local properties in sequence tokens and absolute downstream performance to determine if 1D patches provide specific advantages for musical representation learning.

3. **Skip connection ablation**: Remove skip connections from the transformer architecture and retrain the model. Compare SSMs, attention patterns, and downstream performance on both local and global tasks to quantify the contribution of raw spectral preservation to the observed emergent properties.