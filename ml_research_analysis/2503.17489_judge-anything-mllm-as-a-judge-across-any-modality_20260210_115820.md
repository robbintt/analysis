---
ver: rpa2
title: 'Judge Anything: MLLM as a Judge Across Any Modality'
arxiv_id: '2503.17489'
source_url: https://arxiv.org/abs/2503.17489
tags:
- arxiv
- video
- audio
- score
- judge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two benchmarks, TaskAnything and JudgeAnything,
  to evaluate the judging capabilities of Multimodal Large Language Models (MLLMs)
  across any-to-any modality tasks. TaskAnything consists of 1,500 queries spanning
  15 open-ended multimodal understanding (MMU) and generation (MMG) tasks, while JudgeAnything
  evaluates MLLMs using human-annotated judgments and detailed checklists in both
  Pair Comparison and Score Evaluation settings.
---

# Judge Anything: MLLM as a Judge Across Any Modality

## Quick Facts
- arXiv ID: 2503.17489
- Source URL: https://arxiv.org/abs/2503.17489
- Reference count: 40
- Key outcome: MLLMs show better alignment with human preferences on multimodal understanding (66.55%) than generation tasks (53.37%) across any-to-any modality evaluation

## Executive Summary
This paper introduces two benchmarks, TaskAnything and JudgeAnything, to evaluate the judging capabilities of Multimodal Large Language Models (MLLMs) across any-to-any modality tasks. TaskAnything consists of 1,500 queries spanning 15 open-ended multimodal understanding and generation tasks, while JudgeAnything evaluates MLLMs using human-annotated judgments and detailed checklists in both Pair Comparison and Score Evaluation settings. The study reveals significant performance gaps between understanding and generation tasks, with cross-modality biases and hallucination issues identified as key challenges.

## Method Summary
The authors present TaskAnything, a benchmark containing 1,500 queries across 15 multimodal understanding (MMU) and generation (MMG) tasks covering any-to-any modality transformations. JudgeAnything evaluates MLLM judging capabilities through two settings: Pair Comparison (comparing outputs from two models) and Score Evaluation (rating outputs against human-annotated checklists). The benchmarks assess MLLMs including Gemini-1.5-Pro, GPT-4o, and others, with human annotations providing ground truth for model comparisons.

## Key Results
- MLLMs achieved 66.55% accuracy in Pair Comparison for MMU tasks versus 53.37% for MMG tasks
- Score Evaluation showed 42.79% accuracy for MMU versus 30.05% for MMG tasks
- Gemini-1.5-Pro outperformed other models across both evaluation settings
- Significant cross-modality biases were observed, with models showing modality-specific strengths and weaknesses

## Why This Works (Mechanism)
The evaluation framework works by leveraging human-annotated judgments as ground truth for assessing MLLM judging capabilities. The dual evaluation settings (Pair Comparison and Score Evaluation) provide complementary perspectives on model performance. Pair Comparison captures relative judgment quality between models, while Score Evaluation assesses absolute performance against detailed checklists. This comprehensive approach reveals modality-specific biases and hallucination patterns that single-metric evaluations might miss.

## Foundational Learning
- Multimodal understanding vs generation: Why needed - Different cognitive processes; Quick check - Compare accuracy gaps across task types
- Cross-modality bias: Why needed - Reveals model limitations; Quick check - Analyze error patterns across input-output modality pairs
- Hallucination detection: Why needed - Critical for reliability; Quick check - Measure consistency across repeated evaluations

## Architecture Onboarding
- Component map: TaskAnything benchmark -> JudgeAnything evaluation -> MLLM models -> Human annotations
- Critical path: Input query → MLLM generation → Human judgment comparison → Performance scoring
- Design tradeoffs: Comprehensive task coverage vs evaluation complexity; Pair Comparison vs Score Evaluation sensitivity
- Failure signatures: Modality-specific degradation, inconsistent pairwise judgments, checklist compliance failures
- First experiments: 1) Run cross-model Pair Comparison on identical tasks; 2) Conduct modality ablation studies; 3) Test hallucination detection across different input types

## Open Questions the Paper Calls Out
None

## Limitations
- No confidence intervals provided for accuracy metrics, limiting statistical significance assessment
- Human-annotated judgments may introduce subjectivity, especially for open-ended tasks
- Evaluation protocol sensitivity to prompt engineering and dataset-specific patterns not fully explored
- Limited to commercial MLLMs, missing variations in open-source architectures

## Confidence
- High: Experimental design and benchmark construction are methodologically sound with clear task definitions
- Medium: Cross-modality bias findings need more granular error analysis to understand specific failure modes
- Low: Generalizability to other MLLM architectures and impact of different evaluation protocols remain unclear

## Next Checks
1. Conduct statistical significance testing with confidence intervals for accuracy metrics across task types
2. Perform ablation studies varying prompt engineering strategies to assess robustness
3. Expand evaluation to include open-source MLLM variants and conduct cross-dataset validation