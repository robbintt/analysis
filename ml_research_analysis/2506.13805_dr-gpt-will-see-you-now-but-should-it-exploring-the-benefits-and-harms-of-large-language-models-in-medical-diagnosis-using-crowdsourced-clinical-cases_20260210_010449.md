---
ver: rpa2
title: Dr. GPT Will See You Now, but Should It? Exploring the Benefits and Harms of
  Large Language Models in Medical Diagnosis using Crowdsourced Clinical Cases
arxiv_id: '2506.13805'
source_url: https://arxiv.org/abs/2506.13805
tags:
- medical
- llms
- responses
- health
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the effectiveness and safety of large language
  models (LLMs) in answering everyday health queries using a novel crowdsourced approach.
  A university-level competition engaged 34 participants to generate 212 health-related
  prompts for four publicly available LLMs, with responses evaluated by nine board-certified
  physicians across four metrics.
---

# Dr. GPT Will See You Now, but Should It? Exploring the Benefits and Harms of Large Language Models in Medical Diagnosis using Crowdsourced Clinical Cases

## Quick Facts
- arXiv ID: 2506.13805
- Source URL: https://arxiv.org/abs/2506.13805
- Reference count: 38
- Key outcome: This study evaluated the effectiveness and safety of large language models (LLMs) in answering everyday health queries using a novel crowdsourced approach.

## Executive Summary
This study investigated the performance of large language models in medical diagnosis using crowdsourced clinical cases. A university-level competition generated 212 health-related prompts that were evaluated by nine board-certified physicians across four metrics. The results showed that 76% of LLM responses were deemed accurate, with ChatGPT-4o achieving the highest accuracy at 84.6%. Retrieval-Augmented Generation (RAG) models did not consistently improve response quality. Interviews with medical professionals revealed that LLMs are most effective for specific, well-researched queries and can support health literacy, but concerns remain about inaccuracies, overreliance, and potential harm.

## Method Summary
The research employed a novel crowdsourced approach where 34 participants generated 212 health-related prompts for evaluation. Four publicly available LLMs were tested, with responses assessed by nine board-certified physicians across four different metrics. The study combined quantitative performance metrics with qualitative interviews from seven medical professionals to provide a comprehensive evaluation of LLM capabilities in medical diagnosis contexts.

## Key Results
- 76% of LLM responses were deemed accurate overall
- ChatGPT-4o achieved the highest accuracy at 84.6%
- RAG models did not consistently improve response quality

## Why This Works (Mechanism)
LLMs demonstrate effectiveness in medical diagnosis when queries are specific and well-researched, leveraging their extensive training on medical literature and clinical guidelines. The models can process and synthesize complex medical information to provide relevant responses that support health literacy. However, their performance varies significantly based on prompt specificity and the underlying model architecture, with general-purpose models like ChatGPT-4o outperforming specialized retrieval-augmented approaches in this study.

## Foundational Learning
- **Medical Knowledge Representation**: Understanding how medical information is structured and encoded in LLMs is crucial for evaluating their diagnostic capabilities. Quick check: Examine the training data sources and medical knowledge coverage of different LLM models.
- **Clinical Decision Support Systems**: These systems augment physician decision-making with AI assistance. Quick check: Compare LLM performance against traditional clinical decision support tools in similar medical query scenarios.
- **Retrieval-Augmented Generation (RAG)**: This technique aims to enhance LLM responses by incorporating external knowledge sources. Quick check: Analyze when RAG improves versus hinders medical response accuracy.
- **Medical Error Analysis**: Understanding the types and frequencies of errors in LLM responses is essential for risk assessment. Quick check: Categorize errors by severity and type (factual, contextual, omission-based).
- **Health Literacy Support**: LLMs can serve as accessible health information resources for patients. Quick check: Evaluate how LLM responses impact patient understanding and health-seeking behaviors.
- **Ethical AI in Healthcare**: Integration of AI systems requires careful consideration of ethical implications and safety measures. Quick check: Assess current regulatory frameworks for LLM use in medical contexts.

## Architecture Onboarding
Component map: User Query -> LLM Processing -> Response Generation -> Physician Evaluation -> Performance Metrics
Critical path: Query input → Model processing → Response output → Medical evaluation → Accuracy assessment
Design tradeoffs: General-purpose models vs. specialized medical models; RAG augmentation vs. base model performance
Failure signatures: Factual inaccuracies, context misinterpretation, omission of critical information, inappropriate medical advice
First experiments: 1) Test model performance on increasing query complexity levels, 2) Compare response accuracy across different medical specialties, 3) Evaluate impact of prompt engineering on response quality

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation based on crowdsourced clinical cases rather than real patient scenarios
- Limited physician evaluation panel may not capture full medical expertise spectrum
- Focus on publicly available LLMs without examining specialized medical models
- Accuracy rate of 76% indicates significant room for improvement

## Confidence
- High confidence in finding that LLMs provide accurate responses to specific, well-researched health queries
- Medium confidence in generalizability to broader medical contexts due to limited case scope
- Medium confidence in assessment of potential harms based on physician evaluation

## Next Checks
1. Replicate the study with real patient cases and clinical scenarios to assess performance in authentic medical contexts
2. Expand the physician evaluation panel to include specialists across different medical disciplines and geographic regions
3. Conduct longitudinal studies tracking actual patient outcomes when using LLM-generated medical information, including both short-term and long-term effects