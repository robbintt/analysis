---
ver: rpa2
title: 'MMLongCite: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language
  Models'
arxiv_id: '2510.13276'
source_url: https://arxiv.org/abs/2510.13276
tags:
- context
- citation
- qwen2
- visual
- long
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMLongCite, a benchmark designed to evaluate
  the faithfulness of long-context vision-language models (LVLMs) in multimodal settings.
  The benchmark addresses the gap between extended context windows and effective utilization
  of provided information, focusing on citation generation as a measure of grounding.
---

# MMLongCite: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language Models

## Quick Facts
- arXiv ID: 2510.13276
- Source URL: https://arxiv.org/abs/2510.13276
- Authors: Keyan Zhou, Zecheng Tang, Lingfeng Ming, Guanghao Zhou, Qiguang Chen, Dan Qiao, Zheming Yang, Libo Qin, Minghui Qiu, Juntao Li, Min Zhang
- Reference count: 18
- Primary result: Benchmark reveals significant correctness-faithfulness gap in long-context vision-language models

## Executive Summary
This paper introduces MMLongCite, a benchmark designed to evaluate the faithfulness of long-context vision-language models (LVLMs) in multimodal settings. The benchmark addresses the gap between extended context windows and effective utilization of provided information, focusing on citation generation as a measure of grounding. MMLongCite includes 8 tasks across 6 context length intervals, incorporating diverse modalities such as text, images, and videos, with a total of 2,890 samples. The evaluation of 12 state-of-the-art LVLMs reveals a significant discrepancy between answer correctness and citation fidelity, highlighting that models often generate correct responses without accurately grounding them in the provided context.

## Method Summary
MMLongCite is a benchmark that evaluates LVLM faithfulness through citation generation tasks. The benchmark includes 8 datasets across 4 task categories (Single/Multi-Source Visual Reasoning, Vision Grounding, Video Understanding) with 2,890 total samples. Models must generate responses with citation indices that ground each statement in the provided multimodal context. Context lengths range from 8K-48K tokens across 6 intervals. The evaluation uses GPT-4.1 as adjudicator to compute Citation Precision, Recall, F1, and Correctness scores. Documents are normalized using proportional trimming, videos downsampled to 1 fps, and text documents converted to images via ReportLab â†’ PyMuPDF pipeline.

## Key Results
- LVLMs show significant correctness-faithfulness gaps, with correctness scores often much higher than citation F1 scores
- Reasoning models achieve higher correctness but lower citation scores than non-reasoning models
- Performance consistently declines as context length increases, with "lost-in-the-middle" phenomenon observed
- Visual density disruption severely impacts grounding performance in the "Hard" setting

## Why This Works (Mechanism)

### Mechanism 1: Citation-Driven Faithfulness Constraint
Requiring explicit citations forces models to bridge the gap between semantic correctness and evidentiary grounding. By tasking models to generate statements alongside citations, the evaluation decouples "knowing the answer" from "finding the proof." The ability to generate an index correlates with the internal attention mechanism successfully retrieving the relevant context.

### Mechanism 2: Visual Density Disruption
High visual density (stitching images into composites) disrupts spatial reasoning and grounding more severely than semantic comprehension. In the "Hard" setting, related visual evidence is separated by large pixel distances, testing whether attention mechanisms can resolve long-range dependencies in visual feature space.

### Mechanism 3: Reasoning-Citation Trade-off
Chain-of-Thought reasoning optimizes for answer derivation at the expense of comprehensive evidence retrieval. CoT prompting encourages models to synthesize a final answer, improving semantic precision but acting as a "conservative" filter for citations, causing models to omit relevant supporting sources.

## Foundational Learning

- **Context Window vs. Effective Utilization**: Distinguishes between a model's capacity and its ability to actually use that information. Quick check: If a model accepts 100 images but only attends to the first and last, does it have a "long-context" capability?

- **Parametric vs. Contextual Knowledge**: LVLMs often answer correctly using training data rather than the provided document. Quick check: If I provide a document saying "The sky is green" and ask the color of the sky, should it answer "Blue" (truth) or "Green" (context)?

- **Visual Grounding**: The MMLongCite-Grounding task requires linking textual claims to specific spatial regions in an image. Quick check: Can the model identify which specific crop of a larger image contains the evidence for its claim?

## Architecture Onboarding

- **Component map**: Input Processor -> Context Constructor -> Target LVLM -> Evaluator
- **Critical path**: 1. Context Construction (resizing/interleaving) -> 2. LVLM Inference (generating Statement + Citation) -> 3. Evaluation (GPT-4.1 checks entailment)
- **Design tradeoffs**: RAG helps small models but hurts large models on visual tasks due to retrieval noise; CoT improves correctness but lowers citation recall
- **Failure signatures**: Lost-in-the-middle (performance drops 40-60% when evidence is central), Correctness-Faithfulness Gap (high Correctness but near-zero Citation F1)
- **First 3 experiments**: 1) Run Qwen-3B on Vision Grounding to confirm Correctness/F1 gap; 2) Place key fact at start, middle, end of 32k context to verify U-shaped performance; 3) Compare Image-Only vs Video-Only at 48k tokens to identify modality degradation

## Open Questions the Paper Calls Out

### Open Question 1
How can the trade-off between enhanced reasoning capabilities (Chain-of-Thought) and citation recall be resolved in Large Vision-Language Models? Current training prioritizes internal deliberation over exhaustive retrieval of external evidence.

### Open Question 2
What specific architectural interventions can effectively mitigate the "lost-in-the-middle" phenomenon in multimodal long contexts? Extending context windows is insufficient; novel mechanisms are needed.

### Open Question 3
How can citation fidelity benchmarks be adapted for pan-modal inputs beyond visual-centric data? The current focus on visual modalities needs to extend to audio and other sensory modalities.

## Limitations

- The evaluation relies entirely on GPT-4.1 as the adjudicator, creating potential circularity risks
- The proportional trimming methodology may introduce artifacts affecting model performance
- The study does not investigate whether faithfulness gaps stem from architectural limitations versus training objectives
- The benchmark currently focuses on visual modalities, with plans to extend to other data types

## Confidence

**High Confidence**: The empirical observation that LVLMs show substantial correctness-faithfulness gaps across all tested models and tasks.

**Medium Confidence**: The claim that reasoning models consistently trade correctness for lower citation scores, though the underlying mechanism requires further investigation.

**Low Confidence**: The assertion that visual density disruption is the primary driver of performance degradation in the "Hard" setting, as correlation does not isolate it from other confounding factors.

## Next Checks

1. **Cross-judge Validation**: Repeat evaluation using multiple adjudicators (e.g., Claude-3, Gemini-1.5) to verify correctness-faithfulness gap consistency.

2. **Attention Mechanism Analysis**: Conduct attention visualization studies to empirically verify whether models with low citation F1 are actually attending to cited context regions versus relying on parametric knowledge.

3. **Minimal Citation Task**: Create controlled experiment where models must answer using only provided citations (no free-form generation) to determine if faithfulness gap persists when parametric knowledge cannot be used.