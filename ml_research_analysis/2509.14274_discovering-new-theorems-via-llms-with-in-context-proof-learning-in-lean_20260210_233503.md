---
ver: rpa2
title: Discovering New Theorems via LLMs with In-Context Proof Learning in Lean
arxiv_id: '2509.14274'
source_url: https://arxiv.org/abs/2509.14274
tags:
- interior
- proof
- theorems
- theorem
- closure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Conjecturing-Proving Loop, a pipeline for
  automatically generating mathematical conjectures and proving them in Lean 4 format.
  The key innovation is separating the conjecturing and proving phases, with context
  from previously generated theorems enabling in-context learning of proof strategies
  without model retraining.
---

# Discovering New Theorems via LLMs with In-Context Proof Learning in Lean

## Quick Facts
- arXiv ID: 2509.14274
- Source URL: https://arxiv.org/abs/2509.14274
- Authors: Kazumi Kasaura; Naoto Onda; Yuta Oriike; Masaya Taniguchi; Akiyoshi Sannai; Sho Sonoda
- Reference count: 21
- Key outcome: Conjecturing-Proving Loop pipeline automatically generates and proves mathematical conjectures in Lean 4 format, rediscovering published theorems including alpha-open intersection result through in-context learning without model retraining.

## Executive Summary
This paper introduces the Conjecturing-Proving Loop (CPL), a pipeline that automatically generates mathematical conjectures and proves them in Lean 4 format. The key innovation is separating conjecturing and proving phases, with previously proven theorems serving as context for the prover LLM to learn proof strategies through in-context learning. Using chatGPT-4o and chatGPT-o3 as agents, the framework rediscovered theorems from mathematical papers, including a challenging alpha-open intersection theorem that LLMs judged false in natural language but could prove with context. The context significantly improved proof success rates from 90% to 99%, demonstrating the effectiveness of in-context learning for neural theorem proving.

## Method Summary
The Conjecturing-Proving Loop pipeline operates in two phases: first, a conjecturer LLM generates candidate mathematical statements using an initial library of definitions; second, a prover LLM attempts to prove these conjectures with up to 16 trials, receiving error feedback from a Lean server. Verified theorem-proof pairs are added to the library, which serves as context for subsequent iterations. The framework uses closed-source LLMs (chatGPT-4o for conjecturing, chatGPT-o3 for proving) and relies on Lean 4's formal verification to filter hallucinations and provide reliable learning signals. The method differs from simple loop approaches by separating conjecture generation from proof attempts, preventing convergence to trivial theorems and enabling more complex results.

## Key Results
- Context improved proof success rates from 90% to 99% compared to no-context baseline
- Framework rediscovered research-level theorems, including alpha-open intersection theorem that was difficult to prove without context
- Generated 269 verified theorems over 30 loop iterations
- Alpha-open theorem was proved only with context; without context, chatGPT-o3 judged it false in all natural language trials
- Generated proofs did not use other generated theorems as lemmas, suggesting strategy learning rather than direct reuse

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating conjecturing from proving prevents convergence to trivial theorems and enables generation of more complex results.
- Mechanism: The conjecturer generates many candidate statements without immediately proving them, while a separate prover attempts proofs later. This prevents the feedback loop where easy-to-prove statements dominate, since the conjecturer isn't biased by proof difficulty during generation.
- Core assumption: LLMs generate more diverse and challenging conjectures when not simultaneously optimizing for provability.
- Evidence anchors: Abstract states separation avoids convergence of generated theorems; Figure 2 shows CPL generates longer proofs than simple loop baseline; focused alpha-open intersection theorem was not found in 359 simple loop theorems.

### Mechanism 2
- Claim: In-context learning from self-generated verified proofs improves proving ability without parameter updates.
- Mechanism: Previously proven theorems and their proofs are fed as context to the prover LLM. The model learns proof strategies (tactics, lemma usage patterns) from these examples, applying them to new conjectures. Critically, the generated proof of the alpha-open theorem did not use other generated theorems as lemmas—suggesting strategy learning rather than direct lemma reuse.
- Core assumption: The LLM can extract and transfer proof patterns from context examples to structurally similar problems.
- Evidence anchors: Abstract reports 99% vs 90% success rate improvement; alpha-open intersection theorem proved only with context; chatGPT-o3 judged it false in all natural language trials without context.

### Mechanism 3
- Claim: Formal verification in Lean filters hallucinations and provides reliable learning signal.
- Mechanism: The Lean server verifies both conjecture syntax and proof correctness. Failed proofs return error messages to the LLM for retry. This creates a verification loop where only formally correct proofs enter the library, ensuring context quality for subsequent iterations.
- Core assumption: Lean's verification is both sound (only correct proofs accepted) and informative enough for LLMs to correct errors.
- Evidence anchors: Novelty checked via `exact?` command, which verifies conjectures aren't provable by existing theorems; prover loop allows up to 16 trials with error feedback from Lean server.

## Foundational Learning

- Concept: **Lean 4 and Interactive Theorem Provers**
  - Why needed here: The entire framework outputs Lean code; understanding syntax, tactics (`by`, `exact`, `apply`), and type checking is essential.
  - Quick check question: Can you explain what `exact?` does in Lean and why it's used for novelty checking?

- Concept: **In-Context Learning**
  - Why needed here: The framework's core innovation is improving proof ability via context examples rather than fine-tuning.
  - Quick check question: How does in-context learning differ from few-shot prompting, and what are its limitations?

- Concept: **General Topology (Open Sets, Interior, Closure)**
  - Why needed here: The experimental domain uses semi-open, alpha-open, and preopen sets; understanding these definitions is necessary to evaluate generated theorems.
  - Quick check question: Define alpha-open sets and explain why proving closure under intersection is non-trivial.

## Architecture Onboarding

- Component map:
  Library (Lean files) ↔ Lean Server ↔ Conjecturer LLM
       ↑                                    ↓
       └──────── Prover LLM ←─────────────┘

- Critical path:
  1. Initialize library with seed definitions
  2. Conjecturer generates candidates → Lean validates syntax/novelty
  3. Prover attempts each conjecture → Lean verifies or returns error
  4. Verified proofs added to library
  5. Loop to step 2

- Design tradeoffs:
  - Closed-source LLMs (chatGPT): Benefit from strong code generation; cannot fine-tune. In-context learning compensates.
  - Separate conjecturer/prover: Prevents trivial-theorem convergence but doubles API costs.
  - Context window limits: Growing library may exceed context; not addressed in paper.

- Failure signatures:
  - High conjecture rejection rate: Check Lean syntax requirements in system prompt
  - Prover stuck in retry loop: Error messages may be uninformative; consider error summarization
  - Library growth stalls: Conjecturer may be over-constrained; adjust novelty threshold

- First 3 experiments:
  1. Reproduce alpha-open result: Run CPL with provided seed definitions for 30 loops; verify alpha-open intersection theorem appears.
  2. Ablate context: Re-prove generated theorems without library context; compare success rate to reported 90% vs 99%.
  3. Test new domain: Provide definitions from a different mathematical area (e.g., group theory); measure whether CPL rediscovers known results or generates novel ones.

## Open Questions the Paper Calls Out
- Question: How can the conjecture generation process be refined to produce deeper, more insightful mathematical statements that target unexplored areas of theory?
  - Basis: The authors state in the conclusion that "The propositions highlighted in this study were easy to predict" and suggest future work should focus on "incorporating techniques for guiding the LLM towards unexplored areas."
  - Why unresolved: The current framework generates theorems primarily by analogy from the seed definitions and growing library, which tends to yield results "close" to the initial notions rather than paradigm-shifting conjectures.
  - What evidence would resolve it: A modified pipeline that successfully generates and proves theorems deemed "surprising" or "high-impact" by domain experts, lying outside the immediate logical vicinity of the seed definitions.

## Limitations
- The reported success rate improvements may stem from context size rather than content quality, as the paper doesn't control for context size independently
- Framework's reliance on closed-source LLMs raises reproducibility concerns—different API versions or temperature settings could significantly impact results
- Evaluation focuses on a narrow topological domain, limiting generalizability claims
- 16-iteration caps for both conjecturing and proving phases are somewhat arbitrary, unclear whether optimized or represent hard limitations

## Confidence

- **High confidence**: The separation of conjecturing and proving phases effectively prevents convergence to trivial theorems, as evidenced by the focused alpha-open intersection theorem appearing only in CPL (not in 359 simple loop theorems). The verification mechanism via Lean server is sound and well-established.
- **Medium confidence**: The claimed improvement from in-context learning (99% vs 90% success rate) is supported by ablation results, but the causal mechanism—whether the model learns proof strategies versus simply having more relevant lemmas—remains unclear. The alpha-open theorem proof's lack of lemma usage suggests strategy learning, but this needs systematic verification.
- **Low confidence**: Claims about rediscovering research-level theorems are difficult to verify without access to the full library of 269 generated theorems. The paper provides one detailed example (alpha-open) but lacks systematic comparison against mathematical literature databases.

## Next Checks

1. **Ablate context size systematically**: Run CPL with progressively larger libraries containing increasingly irrelevant theorems (not in the same domain) to determine whether success rates improve with context size alone or require domain-relevant examples.

2. **Analyze proof strategies**: For a sample of generated proofs, conduct a fine-grained analysis of which tactics and proof patterns the prover LLM actually adopts from context. Compare proof structures between context and no-context runs to identify specific strategy transfers.

3. **Cross-domain generalization**: Apply the CPL framework to a completely different mathematical domain (e.g., basic group theory or real analysis) with known theorems. Measure whether the framework can rediscover established results and generate novel conjectures beyond the training examples.