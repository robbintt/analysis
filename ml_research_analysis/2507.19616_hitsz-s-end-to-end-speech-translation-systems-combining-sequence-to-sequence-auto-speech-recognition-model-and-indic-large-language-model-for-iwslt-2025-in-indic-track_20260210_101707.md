---
ver: rpa2
title: HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence
  Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic
  Track
arxiv_id: '2507.19616'
source_url: https://arxiv.org/abs/2507.19616
tags:
- translation
- speech
- language
- indic
- end-to-end
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents HITSZ's submission to the IWSLT 2025 Indic
  track for speech-to-text translation between English and three Indic languages (Hindi,
  Bengali, and Tamil). The authors propose an end-to-end system that integrates Whisper,
  a pre-trained ASR model, with Krutrim, an Indic-specialized large language model.
---

# HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track

## Quick Facts
- arXiv ID: 2507.19616
- Source URL: https://arxiv.org/abs/2507.19616
- Reference count: 14
- Primary result: Achieved average BLEU scores of 28.88 (En→Indic) and 27.86 (Indic→En) in IWSLT 2025 Indic track

## Executive Summary
This paper presents HITSZ's submission to the IWSLT 2025 Indic track for speech-to-text translation between English and three Indic languages (Hindi, Bengali, and Tamil). The authors propose an end-to-end system that integrates Whisper, a pre-trained ASR model, with Krutrim, an Indic-specialized large language model. The system employs a modular fine-tuning approach using LoRA adapters to adapt these pre-trained components for speech translation tasks. The proposed method achieved competitive BLEU scores across multiple language pairs, with the Chain-of-Thought method showing potential for significant improvements on Tamil-to-English translations.

## Method Summary
The proposed system combines Whisper ASR with Krutrim LLM using LoRA adapters for fine-tuning. The approach uses a modular design where pre-trained components are adapted through low-rank adaptation rather than full fine-tuning. The Chain-of-Thought method was investigated as an enhancement, though results showed inconsistent performance across language pairs. The system was evaluated on English-to-Indic and Indic-to-English translation tasks across Hindi, Bengali, and Tamil language pairs.

## Key Results
- Average BLEU scores of 28.88 for English-to-Indic translations
- Average BLEU scores of 27.86 for Indic-to-English translations
- Chain-of-Thought method achieved 13.84 BLEU improvement for Tamil-to-English
- Inconsistent performance of CoT method across different language pairs

## Why This Works (Mechanism)
The system leverages pre-trained models (Whisper ASR and Krutrim LLM) with LoRA adapters to efficiently adapt them for speech translation tasks. The modular approach allows for computational efficiency while maintaining translation quality. The Chain-of-Thought method, when successfully parsed, provides additional reasoning steps that improve translation accuracy.

## Foundational Learning
- **Whisper ASR**: Automatic Speech Recognition model - needed for converting speech to text, quick check: verify audio transcription accuracy
- **Krutrim LLM**: Indic-specialized large language model - needed for understanding and generating Indic languages, quick check: test language-specific translation quality
- **LoRA adapters**: Low-rank adaptation technique - needed for efficient fine-tuning without full model retraining, quick check: compare with full fine-tuning performance
- **BLEU score**: Standard evaluation metric for translation quality - needed for benchmarking, quick check: ensure proper calculation and significance

## Architecture Onboarding
- **Component map**: Speech → Whisper ASR → Text → LoRA Adapter → Krutrim LLM → Translation
- **Critical path**: Audio input → ASR transcription → LLM processing → Translation output
- **Design tradeoffs**: LoRA adapters provide computational efficiency but may limit complex pattern learning compared to full fine-tuning
- **Failure signatures**: Inconsistent CoT parsing, potential overfitting without proper validation, format adherence issues
- **First experiments**: 1) Test individual component performance, 2) Evaluate LoRA vs full fine-tuning, 3) Validate CoT method across all language pairs

## Open Questions the Paper Calls Out
None

## Limitations
- Potential for overfitting during fine-tuning without specific validation metrics
- Inconsistent performance of Chain-of-Thought method across language pairs
- Lack of comprehensive benchmarking against other approaches

## Confidence
- High confidence in system architecture and implementation details
- Medium confidence in reported BLEU scores without comprehensive benchmarking
- Low confidence in generalizability of Chain-of-Thought method due to inconsistent performance

## Next Checks
1. Conduct ablation studies to quantify individual contributions of Whisper ASR, Krutrim LLM, and LoRA adapters
2. Test Chain-of-Thought method with alternative output formats or parsing strategies
3. Evaluate system on additional Indic language pairs and compare with other state-of-the-art approaches