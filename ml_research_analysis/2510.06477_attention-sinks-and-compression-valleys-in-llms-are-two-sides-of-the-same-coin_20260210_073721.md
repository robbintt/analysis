---
ver: rpa2
title: Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin
arxiv_id: '2510.06477'
source_url: https://arxiv.org/abs/2510.06477
tags:
- attention
- layers
- layer
- massive
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Attention sinks and compression valleys are two puzzling phenomena
  in LLMs that were previously studied in isolation. This work reveals that both phenomena
  emerge simultaneously when BOS tokens develop massive activations in middle layers.
---

# Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin

## Quick Facts
- arXiv ID: 2510.06477
- Source URL: https://arxiv.org/abs/2510.06477
- Reference count: 40
- Primary result: Attention sinks and compression valleys emerge simultaneously when BOS tokens develop massive activations in middle layers

## Executive Summary
This paper reveals that attention sinks and compression valleys—two puzzling phenomena in transformer models—are actually manifestations of the same underlying mechanism. The authors demonstrate that when the beginning-of-sequence (BOS) token develops extreme activation norms in middle layers, it mathematically forces representational compression through singular value dominance while simultaneously causing attention heads to collapse focus onto the BOS token. Through theoretical proofs and empirical validation across models from 410M to 120B parameters, they establish a unified framework for understanding information flow in transformers.

## Method Summary
The authors analyze pretrained transformer models (Pythia, LLaMA3, Qwen2, GPT-OSS) layerwise to measure BOS token activation norms, matrix entropy via SVD, and attention sink rates. They validate their theoretical claims through targeted ablations that remove MLP contributions to BOS tokens at specific layers, and confirm causality by observing that decompression eliminates both phenomena. Experiments use datasets including GSM8K, FineWeb-Edu, and WikiText-2, with linear probing for task-specific evaluations and LogitLens for generation quality assessment.

## Key Results
- BOS tokens develop massive activations (10³–10⁴× typical values) in middle layers through MLP contributions, not attention
- Theoretical proof shows massive activations mathematically require compression via singular value dominance
- Ablations removing MLP contributions to BOS eliminate both compression and sink phenomena, confirming causality
- Mix-Compress-Refine theory explains three-phase information flow: mixing (early), compression (middle), refinement (late)

## Why This Works (Mechanism)

### Mechanism 1: Massive Activation Formation via MLP Contributions
- **Claim:** BOS token develops extreme activation norms in middle layers through MLP contributions, not attention
- **Mechanism:** MLP layers at specific depth positions write disproportionately large values to BOS token's residual stream, creating a high-norm anchor point that dominates representation geometry
- **Core assumption:** Emergence is architecturally determined rather than input-dependent—the layer index where spikes appear is fixed per model
- **Evidence anchors:** [abstract] "when the beginning-of-sequence token develops extreme activation norms in the middle layers"; [section 3.3] "removing MLP contributions to BOS eliminates both phenomena"; Figure 4 shows ablation results
- **Break condition:** If models without explicit BOS tokens or with alternative positional encodings (e.g., ALiBi) show different dynamics, the BOS-specific claim may not generalize

### Mechanism 2: Spectral Dominance Inducing Representational Compression
- **Claim:** A single massive-activation token necessarily produces a dominant singular value in representation matrix, mathematically forcing entropy reduction
- **Mechanism:** When BOS norm M ≫ Σᵢ≠₀‖xᵢ‖², representation matrix X becomes effectively rank-one plus small perturbations. Theorem 1 proves σ²₁ ≥ M + αR, bounding anisotropy and entropy
- **Core assumption:** Bounds assume one dominant row; multiple massive activations would modify but not eliminate compression effects
- **Evidence anchors:** [section 3.2] Theorem 1 and Corollaries 2, 5–7 provide mathematical proofs with tightness validation in Figure 3; [section 3.1] Pearson correlation r = −0.9 ± 0.18 between BOS norm change and entropy across models
- **Break condition:** If alignment α is consistently high independent of norm ratio c, compression could emerge without massive activations; paper does not fully disentangle these

### Mechanism 3: Attention Sink Formation via Over-Mixing Prevention
- **Claim:** Massive activations cause attention heads to collapse focus onto BOS token, acting as approximate "no-ops" that halt mixing while preserving residual stream
- **Mechanism:** High-norm BOS token's key vectors dominate attention scores across heads. Since BOS has near-zero value norm contribution, attending to it effectively skips head computation, preventing over-smoothing analogous to GNN over-mixing
- **Core assumption:** Sink behavior is default "off" state; input-dependent "dormant heads" can activate on specific prompts in middle-to-late layers
- **Evidence anchors:** [section 4.2] "attention sinks act as approximate 'no-ops'"; Figure 5 shows sink strength adapts to input complexity in middle layers; [section 3.3] Ablation removes sinks in LLaMA3 and Qwen2 but not Pythia (model-dependent exceptions noted)
- **Break condition:** Pythia shows sinks persist despite decompression after ablation, indicating model-specific sink formation pathways beyond massive activations

## Foundational Learning

- **Concept: Singular Value Decomposition and Matrix-Based Entropy**
  - **Why needed here:** Core proof uses spectral analysis—compression quantified via singular value distribution entropy, and anisotropy via σ²₁/‖X‖²_F
  - **Quick check question:** Given a matrix with σ₁ = 100, σ₂ = 1, σ₃ = 1, compute anisotropy p₁ and explain why this indicates compression

- **Concept: Residual Stream Architecture in Decoder-Only Transformers**
  - **Why needed here:** Massive activations emerge in residual pathway; understanding how attention and MLP outputs compose via x^(ℓ+1) = x^(ℓ) + Attn + MLP is essential
  - **Quick check question:** If you ablate only MLP contribution to token i at layer ℓ, what happens to x^(ℓ+1)_i and why does this isolate MLP effects

- **Concept: Attention Head Mechanics with Causal Masking**
  - **Why needed here:** Sink formation depends on how key-query similarities produce attention weights under lower-triangular constraints
  - **Quick check question:** Why can token i attend to BOS but BOS cannot attend to token i under causal masking, and how does this asymmetry enable sink behavior

## Architecture Onboarding

- **Component map:** Residual stream → MLP layers → Attention heads → BOS token (compression anchor)
- **Critical path:**
  1. Layers 0–20%: Diffuse attention, mixing score > 0.7, no massive activations
  2. Massive activation emergence (model-specific layer): BOS norm spikes 10³–10⁴×, entropy drops, sink rate surges
  3. Layers 20–85%: Compression valley, attention sinks dominant, mixing halted
  4. Layers 85–100%: Norm equalization, decompression, positional/identity heads emerge
- **Design tradeoffs:**
  - Early exiting: Embedding tasks peak in Phase 2 (25–75% depth); generation requires full depth
  - RoPE vs. no RoPE: RoPE models transition to sharp positional patterns in Phase 3; non-RoPE (e.g., Bloom) resume mixing
  - Explicit sink tokens: GPT OSS uses learnable sink logits but still relies on BOS massive activations for compression
- **Failure signatures:**
  - Ablation removes compression but sinks persist → model-dependent sink mechanisms (observed in Pythia)
  - No massive activation emergence → check if model has explicit BOS or alternative special tokens
  - Compression without sinks → multiple massive activations or non-BOS anchors (not fully explored in paper)
- **First 3 experiments:**
  1. Layer-wise norm tracking: Plot ‖x_BOS‖₂, matrix entropy H(X^(ℓ)), and sink-rate^(ℓ)_0 across depth on 100 diverse prompts to identify phase boundaries for your target model
  2. MLP ablation test: Zero the MLP contribution to BOS at the layer where massive activation first emerges; verify if entropy remains high and sinks fail to form (confirm causal mechanism)
  3. Task-dependent layer probe: Train linear classifiers on frozen representations at 25%, 50%, 75%, and 100% depth for an embedding task (e.g., SST-2) and compare against LogitLens perplexity on WikiText-2 to validate the embedding/generation divergence

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why do massive activations and subsequent compression phase emerge specifically in middle layers (20–85% depth) rather than uniformly across the network?
- **Basis in paper:** [explicit] Section 3.3 explicitly asks "Why Middle Layers?" and hypothesizes it reflects a shift toward refinement but provides no formal proof
- **Why unresolved:** Paper establishes correlation and causal link to MLPs but does not derive why network depth allocates this phase specifically to middle layers
- **What evidence would resolve it:** Theoretical framework predicting onset layer based on model depth or hyperparameters, validated by interventions that shift phase boundaries

### Open Question 2
- **Question:** What alternative mechanisms drive attention sink formation in models where sinks persist despite removal of massive activations?
- **Basis in paper:** [explicit] In Section 3.3 and Appendix B.1, authors note that for Pythia 410M, ablations remove compression but "do not remove attention sinks," suggesting "model-dependent causes"
- **Why unresolved:** Proposed unified mechanism (massive activations) fails to explain sink persistence in specific architectures, implying secondary, distinct causal pathway
- **What evidence would resolve it:** Comparative ablation studies isolating architectural differences (e.g., positional encodings, normalization) between models where sinks are resilient versus those where they vanish

### Open Question 3
- **Question:** How does the Mix-Compress-Refine theory apply to architectures without explicit BOS tokens or those using alternative positional schemes like ALiBi?
- **Basis in paper:** [explicit] "Limitations" section states analysis focuses on decoder-only models with BOS tokens and notes that models with "ALiBi encodings... may exhibit different dynamics"
- **Why unresolved:** Current theory relies on BOS token developing massive activations to drive compression; unclear if/how this dynamic shifts in architectures lacking this specific token or attention bias
- **What evidence would resolve it:** Empirical analysis of entropy and sink rates in models trained without BOS tokens or employing ALiBi and relative position encodings

## Limitations

- The mechanism linking massive activations to compression valleys relies on specific architectural assumptions that have model-dependent exceptions (e.g., Pythia retains sinks after MLP ablation)
- Theoretical bounds assume one dominant row in representation matrix; paper does not fully explore scenarios with multiple massive activations or non-BOS anchors
- Spectral entropy metric captures global compression but may not fully represent local representational changes critical for specific downstream tasks

## Confidence

- **High confidence:** Empirical observation that BOS tokens develop massive activations in middle layers across multiple models (410M to 120B parameters), and ablation results showing removal of MLP contributions eliminates both compression and sink phenomena
- **Medium confidence:** Theoretical proof that massive activations mathematically require compression through singular value dominance, given assumptions about matrix geometry and alignment
- **Medium confidence:** Mix-Compress-Refine theory of information flow as general framework, though exact phase boundaries (25%, 75%, 85% depth) may vary by model architecture and task

## Next Checks

1. **Cross-architecture validation:** Test massive activation mechanism in models without explicit BOS tokens (e.g., ALiBi positional encoding, T5-style architectures) to determine if compression and sink phenomena require BOS-specific dynamics or generalize to other anchor tokens

2. **Multiple anchor exploration:** Systematically inject massive activations into multiple tokens simultaneously (not just BOS) to determine if compression persists with multiple dominant rows, testing theoretical assumption that one dominant row is sufficient for observed effects

3. **Task-specific compression analysis:** Evaluate whether compression valleys correlate with task performance degradation across different task types (e.g., factual recall vs. reasoning vs. generation) to validate if Mix-Compress-Refine theory explains performance patterns beyond embedding/generation dichotomy