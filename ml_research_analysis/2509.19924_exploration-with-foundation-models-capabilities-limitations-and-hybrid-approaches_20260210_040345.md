---
ver: rpa2
title: 'Exploration with Foundation Models: Capabilities, Limitations, and Hybrid
  Approaches'
arxiv_id: '2509.19924'
source_url: https://arxiv.org/abs/2509.19924
tags:
- exploration
- agent
- reward
- llms
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work benchmarks LLMs and VLMs as zero-shot exploration agents
  in classic RL settings, revealing that VLMs can infer high-level objectives but
  fail at precise low-level control, forming a "knowing-doing gap." Across multi-armed
  bandits, gridworlds, and sparse-reward Atari games, VLMs struggle with motor control
  and semantic grounding despite strong visual understanding. In a simple on-policy
  hybrid framework, VLM-guided PPO agents achieve faster early-stage learning in Freeway,
  demonstrating that VLM guidance can enhance sample efficiency when high-level reasoning
  is accurate.
---

# Exploration with Foundation Models: Capabilities, Limitations, and Hybrid Approaches

## Quick Facts
- **arXiv ID**: 2509.19924
- **Source URL**: https://arxiv.org/abs/2509.19924
- **Reference count**: 11
- **Primary result**: VLMs can infer high-level objectives but fail at precise low-level control, revealing a "knowing-doing gap" in RL exploration.

## Executive Summary
This work benchmarks large language models (LLMs) and vision-language models (VLMs) as zero-shot exploration agents in classic reinforcement learning settings. VLMs show strong visual understanding and can infer high-level objectives, but struggle with motor control and semantic grounding, leading to a "knowing-doing gap." In a hybrid framework, VLM-guided PPO agents achieve faster early-stage learning in Freeway, demonstrating that VLMs can enhance sample efficiency when high-level reasoning is accurate. These results suggest that foundation models are best used to guide rather than replace RL exploration.

## Method Summary
The study evaluates LLMs and VLMs as zero-shot exploration agents across multi-armed bandits, gridworlds, and sparse-reward Atari games. VLMs are tested for their ability to infer objectives and guide exploration without fine-tuning. A simple on-policy hybrid approach integrates VLM reasoning with PPO, using VLM guidance to improve sample efficiency in early learning stages. The experiments focus on controlled, classic RL environments and one Atari game (Freeway) for hybrid validation.

## Key Results
- VLMs can infer high-level objectives but fail at precise low-level control, forming a "knowing-doing gap."
- In a hybrid framework, VLM-guided PPO agents achieve faster early-stage learning in Freeway.
- VLMs struggle with motor control and semantic grounding despite strong visual understanding.

## Why This Works (Mechanism)
Foundation models excel at high-level reasoning and visual understanding, enabling them to infer objectives from observations. However, they lack the fine-grained control and task-specific adaptation required for precise RL control. By using VLMs to guide exploration rather than for end-to-end control, the hybrid approach leverages their strengths while mitigating their weaknesses, resulting in improved sample efficiency during early learning stages.

## Foundational Learning
- **Zero-shot learning**: VLMs operate without fine-tuning; needed to assess inherent model capabilities. Quick check: measure performance before and after any adaptation.
- **Reinforcement learning**: Core framework for exploration and control; needed to contextualize VLM performance. Quick check: validate RL agent baselines.
- **Vision-language understanding**: VLMs combine visual and textual reasoning; needed to evaluate high-level objective inference. Quick check: test VLM on semantic grounding tasks.
- **Hybrid RL**: Combines foundation model guidance with traditional RL; needed to exploit complementary strengths. Quick check: compare guided vs. non-guided agents.
- **Sample efficiency**: Early-stage learning gains; needed to assess practical benefits of VLM guidance. Quick check: measure learning curves in early episodes.
- **Motor control**: Precise low-level action execution; needed to identify the "knowing-doing gap." Quick check: evaluate control accuracy in gridworlds.

## Architecture Onboarding
- **Component map**: Observation -> VLM (reasoning) -> Guidance signal -> PPO agent -> Action
- **Critical path**: Visual input → VLM inference → Action selection → Environment feedback → PPO update
- **Design tradeoffs**: Zero-shot VLM (no fine-tuning) vs. potential gains from adaptation; high-level reasoning vs. low-level control accuracy.
- **Failure signatures**: VLM reasoning errors, motor control failures, poor semantic grounding, hallucinations in guidance signals.
- **First experiments**: 1) Benchmark VLM zero-shot performance in gridworlds. 2) Test hybrid VLM-guided PPO in Freeway. 3) Analyze VLM reasoning errors in sparse-reward Atari games.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to classic RL environments and one Atari game; results may not generalize.
- Zero-shot VLM performance may not reflect potential with task-specific adaptation.
- Early-stage gains in hybrid approach may not translate to long-term or final performance.
- VLM reasoning errors not deeply analyzed for root causes or mitigation strategies.

## Confidence
- Confidence in VLMs inferring high-level objectives but failing at low-level control: **Medium**
- Confidence in VLM-guided PPO achieving faster early-stage learning in Freeway: **Medium**
- Confidence in foundation models best used for guidance, not replacement: **Low**

## Next Checks
1. Replicate hybrid VLM-guided PPO across diverse Atari games and continuous control tasks to test robustness and generalization.
2. Investigate impact of fine-tuning or prompting strategies on VLM zero-shot performance to isolate model limitations from task adaptation needs.
3. Analyze VLM reasoning errors in detail to identify common failure modes and assess whether targeted training or architecture changes could close the knowing-doing gap.