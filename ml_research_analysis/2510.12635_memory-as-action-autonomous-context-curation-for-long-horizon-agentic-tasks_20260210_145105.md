---
ver: rpa2
title: 'Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks'
arxiv_id: '2510.12635'
source_url: https://arxiv.org/abs/2510.12635
tags:
- memory
- context
- memact
- reasoning
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Memory as Action (MemAct) treats context management as learnable
  policy actions, allowing an agent to actively curate its working memory through
  fine-grained editing operations. By formulating context curation as in-place editing
  (prune and write) within a unified policy, MemAct enables end-to-end optimization
  of memory retention against task performance.
---

# Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks

## Quick Facts
- arXiv ID: 2510.12635
- Source URL: https://arxiv.org/abs/2510.12635
- Reference count: 40
- Key outcome: MemAct-RL-14B matches 16× larger models' accuracy (59.1% vs 53.1%) while reducing context length and token consumption by 51%

## Executive Summary
Memory as Action (MemAct) treats context management as learnable policy actions, enabling agents to autonomously curate their working memory through fine-grained editing operations. By formulating context curation as in-place editing (prune and write) within a unified policy, MemAct enables end-to-end optimization of memory retention against task performance. Dynamic Context Policy Optimization (DCPO) reconciles dynamic memory updates with reinforcement learning by logically segmenting trajectories at memory action points, ensuring stable training. Experiments demonstrate that MemAct achieves accuracy comparable to models 16× larger while reducing average context length by 51% and total token consumption by 51%.

## Method Summary
MemAct formulates context management as a Markov Decision Process where the agent learns to actively curate its working memory through prune and write operations. The method uses a unified policy πθ that generates both task actions and memory actions (A = Atask ∪ Amem), with memory actions taking the form (I_target, c) to remove specific records and synthesize new memory content. DCPO training segments trajectories at memory edit points to handle the causal consistency violations caused by non-monotonic context changes. The approach uses SFT initialization on synthetic trajectories followed by RL training with trajectory-level group-relative advantages.

## Key Results
- MemAct-RL-14B achieves 59.1% accuracy on multi-objective tasks, matching Qwen2.5-32B-Instruct (53.1%) while using 51% fewer tokens
- Context length reduced by 51% through learned pruning strategies without sacrificing task performance
- Models automatically discover capacity-appropriate strategies: 7B models prune aggressively (~6 records/action) while 14B models use bimodal fine-grained (~2 records) and coarse-grained (~6 records) pruning
- Performance scales from 2 to 8 objectives while maintaining accuracy gains over baselines

## Why This Works (Mechanism)

### Mechanism 1: Unified Policy Integration of Memory Actions
The action space A = Atask ∪ Amem allows the policy πθ to dynamically select between reasoning and context curation. The Prune&Write operator specifies target IDs to remove and synthesizes memory content in-place, making curated summaries addressable for future updates. This integration enables joint optimization of memory retention and task performance through reward signals.

### Mechanism 2: Trajectory Segmentation Restores Causal Consistency
Memory edits violate the monotonic prefix assumption of causal LMs. DCPO logically partitions trajectories at memory action timesteps into independent segments σi = (Ci, yi), where each segment has a fixed context prefix. Gradients are computed against correctly reconstructed contexts, enabling stable RL training despite non-monotonic context changes.

### Mechanism 3: Capacity-Adaptive Strategy Emergence
RL training reveals that smaller models (7B) adopt aggressive, consistent pruning (~6 records/action), while larger models (14B) exhibit bimodal strategies: fine-grained pruning (~2 records) during reasoning and coarse-grained pruning (~6 records) at phase transitions. The reward signal sufficiently captures the trade-off between context efficiency and task success.

## Foundational Learning

- Concept: Markov Decision Process (MDP) formulation with addressable states
  - Why needed here: Working memory is formalized as a sequence of uniquely ID'd interaction records, enabling precise targeting for deletion
  - Quick check question: Can you explain why ID-based addressing is necessary vs. positional indexing?

- Concept: Policy gradient methods and advantage estimation
  - Why needed here: DCPO uses group-relative advantage normalization A(τ) = (R(τ) - mean(Ru)) / (std(Ru) + ε) for credit assignment
  - Quick check question: How does trajectory-level advantage attribution differ from step-level attribution?

- Concept: Causal language modeling assumptions
  - Why needed here: Understanding why memory edits "break" standard RL—each token's representation encodes all antecedent tokens
  - Quick check question: Why can't simple attention masking solve the trajectory fracture problem?

## Architecture Onboarding

- Component map: Policy Model (Qwen2.5-7B/14B backbone) → generates both task and memory actions → Prune&Write executor → filters records by ID, appends synthesized memory content → DCPO trainer → segments trajectories, computes segment-level losses with trajectory-level advantages → Environment → returns observations and terminal rewards

- Critical path: Prompt → Policy generates action → If a_mem: execute Prune&Write (delete IDs + append summary) → Update working memory Ht → Continue until terminal state → Compute trajectory reward → DCPO segments and attributes advantages → Policy update

- Design tradeoffs:
  - Sparse vs. dense rewards: Sparse terminal rewards simplify training but complicate credit assignment for specific memory actions
  - SFT initialization vs. pure RL: SFT bootstrap prevents degenerate behaviors (ignoring memory tool, repetitive invocation) but may bias strategy space
  - Segment sampling strategy: Round-robin sampling ensures coverage but may over-weight low-information segments

- Failure signatures:
  - Memory tool ignored or invoked repetitively (Addressed by SFT initialization)
  - Premature pruning of information needed later (mitigated but not eliminated by learned strategies)
  - Hallucinated facts stored in memory (observed in Table 7 case studies)
  - Performance saturation on complex tasks despite memory management (baselines) vs. maintained scaling (MemAct)

- First 3 experiments:
  1. Replicate the multi-objective task scaling experiment (2→8 objectives) on a smaller model (e.g., Qwen2.5-3B) to verify capacity-adaptive strategy emergence generalizes
  2. Ablate the memory content synthesis component (use empty or random content) to isolate the contribution of structured memory format
  3. Test domain transfer to a qualitatively different task family (e.g., code generation with function-level pruning) to assess generalization claims

## Open Questions the Paper Calls Out

### Open Question 1
Can the intrinsic coupling between memory behavior and reasoning be leveraged to create more granular reward signals, solving the credit assignment problem for specific memory actions? The current method relies on terminal rewards, failing to quantify the utility of individual pruning decisions during long-horizon tasks.

### Open Question 2
How can posterior methods be integrated into the training loop to prioritize "key steps" and improve the efficiency of segment sampling? The current optimization uses a random sampling algorithm that treats all memory operations as equally important, which limits training efficiency.

### Open Question 3
Can the Prune&Write action space be expanded to include external retrieval or tiered caching to mitigate the lossy nature of context compression? The current implementation permanently deletes raw data upon summarization, preventing the recovery of details if they become relevant later.

## Limitations

- The method assumes synthesized memory content can adequately capture deleted records' semantic essence, but lacks systematic evaluation of memory quality or hallucination rates
- DCPO's trajectory segmentation relies on assumptions about key information preservation in key-value states that need more rigorous empirical validation
- Capacity-adaptive strategy claims are based on observed behavior rather than controlled ablation studies testing different reward formulations

## Confidence

- **High confidence**: The core architectural innovation (unified policy with Prune&Write actions) and the 51% token reduction claim are well-supported by experimental results across multiple benchmarks
- **Medium confidence**: The DCPO training methodology and capacity-adaptive strategy emergence claims are reasonably supported, though underlying assumptions could benefit from more rigorous validation
- **Medium confidence**: Performance comparisons against 16× larger models are compelling but rely on a single evaluator implementation that isn't fully specified

## Next Checks

1. Conduct ablation studies varying the memory content synthesis quality (empty, random, partial, full synthesis) to isolate the contribution of structured memory format vs. the editing mechanism itself
2. Implement controlled experiments testing the DCPO assumption that deleted token information is preserved in key-value states by measuring performance degradation when pruning critical vs. non-critical records
3. Test the model's behavior under different reward formulations (e.g., varying r_pen magnitude, adding content-based rewards) to verify that capacity-adaptive strategies emerge from the reward structure rather than being artifacts of the training setup