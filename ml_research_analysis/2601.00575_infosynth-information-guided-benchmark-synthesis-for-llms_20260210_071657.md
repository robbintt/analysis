---
ver: rpa2
title: 'InfoSynth: Information-Guided Benchmark Synthesis for LLMs'
arxiv_id: '2601.00575'
source_url: https://arxiv.org/abs/2601.00575
tags:
- problems
- novelty
- diversity
- dataset
- mbpp-new
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InfoSynth introduces an information-theoretic framework for generating
  high-quality, novel, and diverse coding benchmarks for LLM evaluation. The method
  uses KL-divergence and entropy to quantify benchmark novelty and diversity, then
  employs genetic algorithms with iterative code feedback to synthesize problems from
  seed datasets.
---

# InfoSynth: Information-Guided Benchmark Synthesis for LLMs

## Quick Facts
- arXiv ID: 2601.00575
- Source URL: https://arxiv.org/abs/2601.00575
- Authors: Ishir Garg; Neel Kolhe; Xuandong Zhao; Dawn Song
- Reference count: 40
- Key outcome: InfoSynth introduces an information-theoretic framework for generating high-quality, novel, and diverse coding benchmarks for LLM evaluation.

## Executive Summary
InfoSynth presents a novel approach to generating coding benchmarks for LLM evaluation using information-theoretic metrics. The method employs KL-divergence and differential entropy to quantify novelty and diversity relative to seed datasets, then uses genetic algorithms with iterative code feedback to synthesize problems. The pipeline generates Python coding problems with 97% accuracy in test case and solution generation, consistently producing benchmarks with higher novelty and diversity than seed datasets. The approach allows control over difficulty and novelty-diversity tradeoffs while providing a scalable, self-verifying pipeline for LLM evaluation.

## Method Summary
InfoSynth synthesizes coding benchmarks through a genetic algorithm pipeline that uses information-theoretic metrics to guide generation. The method embeds problem statements using all-mpnet-base-v2, projects to lower dimensions via UMAP, then estimates KL-divergence for novelty and differential entropy for diversity. Mutation and crossover operations generate candidate problems from seed datasets, with k-farthest-neighbor filtering retaining the least-similar problems to increase novelty/diversity. Solutions and test cases are generated using an LLM, then refined through iterative code feedback in a Python execution environment. The pipeline achieves 97% human-verified correctness and produces benchmarks with significantly higher novelty and diversity than seed datasets.

## Key Results
- Generates Python coding problems with 96-98% human-verified correctness and 99-100% test coverage
- Achieves 20% increase in pass rate over 5 feedback iterations through iterative code refinement
- Consistently produces benchmarks with higher novelty and diversity than seed datasets Leetcode and MBPP
- Demonstrates control over difficulty and novelty-diversity tradeoffs through mutation difficulty mix and filtering parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KL-divergence on embedding distributions quantifies benchmark novelty relative to seed datasets.
- Mechanism: Embed problem statements (e.g., all-mpnet-base-v2, R^768), project via UMAP to lower dimensions (d∈[8,12]), then estimate D_KL(q||p) using k-NN distances. Larger KL indicates out-of-distribution problems.
- Core assumption: Embedding similarity reflects semantic/problem-space similarity; low-dimensional projection preserves relative geometry for KL estimation.
- Evidence anchors:
  - [Section 3.1]: Defines Novelty(Y|X) = D_KL(q||p) and provides k-NN estimator formula (Eq. 3).
  - [Figure 1]: Shows KL metric aligns with intuition—e.g., MBPP has high novelty against Leetcode, subsets have low novelty against their superset.
  - [corpus]: Related work on difficulty/quality metrics (TaskEval, CrowdSelect) relies on model evaluations; InfoSynth explicitly avoids this cost. Weak direct corpus support for KL-as-novelty specifically for benchmarks.
- Break condition: If embeddings don't capture problem structure, or if UMAP projection distorts KL estimation, the metric may misrank datasets.

### Mechanism 2
- Claim: Differential entropy of the embedding distribution measures benchmark diversity.
- Mechanism: Estimate entropy via Kozachenko-Leonenko estimator using k-NN distances. High entropy ↔ spread-out embeddings ↔ coverage of diverse problem types.
- Core assumption: Embedding spread corresponds to problem-space diversity; k-NN distances reliably estimate differential entropy in projected space.
- Evidence anchors:
  - [Section 3.1]: Diversity(X) = −∫ p(x) log p(x) dx; uniform distribution maximizes entropy.
  - [Figure 2]: Full datasets (Leetcode, Codeforces) show higher entropy than topic-specific subsets; APPS and Leetcode have higher diversity than HumanEval, matching intuition.
  - [corpus]: InnoGym also emphasizes solution diversity as innovation signal, but uses different methodology (method-level analysis).
- Break condition: Curse of dimensionality in high-d embedding space; Kozachenko-Leonenko bias increases with dataset density.

### Mechanism 3
- Claim: Genetic algorithm with k-farthest-neighbor filtering and iterative code feedback produces high-quality, novel, diverse problems.
- Mechanism: (1) Mutation/crossover generate candidate problems from seeds; (2) k-farthest-neighbor filtering retains least-similar problems (cosine similarity) to increase novelty/diversity; (3) LLM generates solution + tests, executes in Python environment, feeds errors back for iterative refinement; (4) Only problems that pass self-verification survive.
- Core assumption: The generator LLM can produce correct solutions/tests given feedback; k-farthest selection pushes distribution toward low-density regions without making problems unsolvable.
- Evidence anchors:
  - [Section 4.1, Figure 3]: Pipeline diagram showing mutation, crossover, filtering, code feedback loop.
  - [Section 5.3]: 20% increase in pass rate over 5 feedback iterations; error rates drop as LLM fixes syntax/runtime issues.
  - [Table 1]: 96–98% human-verified correctness; 99–100% test coverage.
  - [Figure 4]: k-farthest filtering (Guided variants) increases novelty/diversity vs. unfiltered (New variants), at cost of easier problems (Table 2).
  - [corpus]: GeneticInstruct, KodCode use similar evolutionary synthesis; InfoSynth adds filtering + code feedback for robustness.
- Break condition: Generator LLM fails on hard/numerical problems; or filtering over-constrains, yielding only easy problems.

## Foundational Learning

- Concept: KL-divergence and differential entropy estimation
  - Why needed here: Core metrics for novelty/diversity; must understand k-NN estimators, bias-variance tradeoff in k, and dimensionality reduction (UMAP) effects.
  - Quick check question: Given two datasets X (seed) and Y (generated), explain what a high D_KL(q||p) implies about Y relative to X.

- Concept: Genetic algorithms (mutation, crossover, selection)
  - Why needed here: InfoSynth uses evolutionary operations to explore problem space; understanding selection pressure (k-farthest filtering) is critical for controlling novelty/difficulty tradeoffs.
  - Quick check question: How does k-farthest-neighbor selection differ from random selection in its effect on population diversity?

- Concept: Iterative refinement with execution feedback
  - Why needed here: Self-verification loop is key to 97% solution/test correctness; LLM uses error history to fix code (chain-of-thought behavior).
  - Quick check question: What information must be fed back to the LLM at each iteration for effective refinement?

## Architecture Onboarding

- Component map: Seed data -> Embedding (all-mpnet-base-v2) -> UMAP projection (d=8-12) -> KL/entropy estimators -> Mutation/Crossover generation -> k-farthest filtering -> Solution/test generation -> Python execution -> Iterative code feedback -> Postprocessing -> Deduplication (MinHash+LSH) -> Benchmark pool

- Critical path: Seed data → mutation/crossover → k-farthest filtering → solution/test generation → iterative code feedback → (if pass) postprocess → deduplicate → add to seed pool for next iteration

- Design tradeoffs:
  - k-farthest filtering: ↑novelty/diversity, ↓difficulty (generator struggles with hard OOD problems)
  - Feedback iterations: ↑pass rate, ↑inference cost; 3 iterations is typical sweet spot
  - Mutation difficulty mix: All three (easy/medium/hard) → higher novelty; hard-only → lower model scores but less diversity
  - Colonies (N_c): More colonies → more parallel exploration; tradeoff with duplication overhead

- Failure signatures:
  - Numerical problems: Correct solutions but inaccurate test assertions
  - Crossover-of-crossover: Over-constrained problems, generator fails to satisfy all conditions
  - High novelty + low pass rate: Generator cannot solve truly OOD problems; filter may be too aggressive
  - Unparsable outputs: Formatting failures increase slightly with iterations

- First 3 experiments:
  1. Metric validation: Compute KL/entropy on held-out datasets (e.g., Codeforces vs. Leetcode) to confirm ranking aligns with intuition (replicate Figure 1, 2)
  2. Ablation on k-farthest filtering: Generate two datasets (with/without filtering), compare novelty, diversity, and model pass rates to quantify tradeoffs (replicate Figure 4c, 4d)
  3. Feedback iteration sweep: Run 0–5 iterations on 100 generated problems, plot pass/fail/error rates to identify optimal N_it (replicate Figure 9, 10)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can synthetic benchmarks be verified as genuinely contamination-free with respect to LLM training data?
- Basis in paper: [explicit] The authors state: "While this work does not directly address the task of creating contamination-free benchmarks, we provide an improved method of generating benchmarks that cover more diverse and novel coding tasks."
- Why unresolved: InfoSynth optimizes for novelty/diversity relative to seed datasets, but does not verify whether generated problems appear in any LLM's pre-training corpus.
- What evidence would resolve it: A systematic comparison between generated problems and known training corpora, or evaluation on models with documented training data showing no performance advantage from potential contamination.

### Open Question 2
- Question: How can the tradeoff between novelty, diversity, and difficulty be jointly optimized rather than manually balanced?
- Basis in paper: [inferred] Section 5.1 states: "k-farthest-neighbor filtering improves dataset novelty and diversity. This comes at the cost of generating easier problems." Also: "We also observe a tradeoff between novelty and diversity."
- Why unresolved: The paper demonstrates these tradeoffs exist and provides levers (mutation difficulty types, k-farthest filtering), but offers no principled framework for jointly optimizing all three properties.
- What evidence would resolve it: A multi-objective optimization formulation with demonstrated Pareto-optimal frontier, or a method that achieves high scores on all metrics simultaneously.

### Open Question 3
- Question: Why does the k-NN KL-divergence estimator produce negative values when comparing a subset to its superset, and how can this be corrected?
- Basis in paper: [explicit] Section 3.2.1: "the estimator becomes negative despite KL being theoretically nonnegative. This is because we are comparing a subset against a superset... we include this case primarily to illustrate that our metric still reflects human intuition."
- Why unresolved: The authors acknowledge the theoretical inconsistency but rely on relative comparisons rather than fixing the estimator's behavior in degenerate cases.
- What evidence would resolve it: A modified estimator that maintains non-negativity while preserving meaningful ranking of dataset novelty.

### Open Question 4
- Question: Can the InfoSynth pipeline be extended to generate valid problems in domains without executable verification (e.g., mathematical proofs, natural language reasoning)?
- Basis in paper: [inferred] The paper states: "This paper focuses on Python coding problems, whose solutions can be verified by executing them in a code environment." The entire self-verification mechanism depends on code execution.
- Why unresolved: The iterative code feedback loop is central to achieving 97% correctness; domains without executable verification would require alternative validation mechanisms.
- What evidence would resolve it: A modified pipeline for a non-code domain (e.g., math word problems) achieving comparable correctness rates through alternative verification methods.

## Limitations
- The KL-divergence novelty metric relies on embedding space geometry preserving problem semantics, which may fail if different algorithms map to similar embeddings
- The genetic algorithm's k-farthest filtering can over-constrain problem generation, yielding easier problems at the expense of true difficulty diversity
- The iterative code feedback mechanism cannot fully resolve numerical problems with test assertion inaccuracies

## Confidence
- **High confidence**: The pipeline architecture (mutation → filtering → feedback → verification) and its core components are well-specified and experimentally validated with 97% human-verified correctness
- **Medium confidence**: The KL-divergence and differential entropy estimators work as claimed for benchmark novelty/diversity measurement, though limited direct validation exists
- **Medium confidence**: The novelty-diversity tradeoffs are real and measurable, but the filtering mechanism may over-constrain problem generation

## Next Checks
1. **Metric validation on held-out data**: Compute KL-divergence and differential entropy on 2-3 additional dataset pairs (e.g., APPS vs. HumanEval, CodeContests vs. MBPP) to verify that the metrics rank datasets by intuitive novelty/diversity before applying to generated benchmarks

2. **Ablation study on filtering aggressiveness**: Generate three benchmark variants (unfiltered, moderate k-farthest k=10, aggressive k-farthest k=50), measure novelty/diversity scores and model pass rates to quantify the tradeoff curve and identify optimal k for balanced generation

3. **Extended feedback iteration analysis**: Run 0-7 feedback iterations on 200 generated problems, measuring pass rate, error type distribution, and generation time to identify the point of diminishing returns and characterize which error types persist beyond 3-5 iterations