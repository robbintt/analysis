---
ver: rpa2
title: 'CAPE: Context-Aware Personality Evaluation Framework for Large Language Models'
arxiv_id: '2508.20385'
source_url: https://arxiv.org/abs/2508.20385
tags:
- consistency
- personality
- llms
- option
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces CAPE, a Context-Aware Personality Evaluation
  framework for LLMs that incorporates conversational history into psychometric assessments.
  Unlike existing context-free approaches, CAPE retains prior interactions to analyze
  their influence on response consistency.
---

# CAPE: Context-Aware Personality Evaluation Framework for Large Language Models

## Quick Facts
- arXiv ID: 2508.20385
- Source URL: https://arxiv.org/abs/2508.20385
- Authors: Jivnesh Sandhan; Fei Cheng; Tushar Sandhan; Yugo Murawaki
- Reference count: 17
- Primary result: CAPE introduces context-aware personality assessment for LLMs, showing conversational history enhances consistency via in-context learning while revealing model-specific sensitivity to question ordering.

## Executive Summary
CAPE introduces a Context-Aware Personality Evaluation framework that incorporates conversational history into psychometric assessments of Large Language Models. Unlike context-free approaches that evaluate each personality item in isolation, CAPE retains prior interactions to analyze their influence on response consistency. The framework introduces novel metrics—Trajectory Consistency (TC) and OCEAN Consistency (OC)—to quantify consistency across multiple LLM runs. Experiments on 7 diverse LLMs reveal that conversational history enhances consistency via in-context learning but can induce personality shifts, with GPT models showing robustness to question ordering while Gemini-1.5-Flash and Llama-3.1-8B display sensitivity.

## Method Summary
The CAPE framework evaluates LLM personality using Big Five (OCEAN) psychometric tests under two settings: context-free (isolated items) and context-dependent (retaining conversational history). The framework uses standardized personality inventories (MPI with 120 items, BFI with 44 items) and runs each assessment three times with different random seeds. It introduces TC and OC metrics using Gaussian Process Regression to quantify consistency, measuring the overlap of posterior predictive distributions across trajectories. The framework tests five inconsistency factors (temperature variation, option wording, option order, instruction paraphrasing, item paraphrasing) with three variants each, using 95% confidence intervals for GPR-based metrics.

## Key Results
- Conversational history enhances consistency via in-context learning, with context-dependent settings showing higher TC/OC scores than context-free baselines
- GPT models demonstrate robustness to adversarial history manipulation, while Gemini-1.5-Flash and Llama-3.1-8B shift responses toward false patterns
- Context-dependent evaluation better aligns with human judgments in Role-Playing Agents dataset compared to context-free approaches
- Model sensitivity varies significantly: GPT models show stable OCEAN profiles across question ordering, while Gemini-1.5-Flash and Llama-3.1-8B display ordering effects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Conversational history functions as implicit few-shot demonstrations, stabilizing the LLM's response trajectory.
- **Mechanism:** When an LLM answers a personality item, retaining the previous question-answer pair in the context window allows the model to condition its current response on the prior pattern. This leverages the model's in-context learning (ICL) capability to maintain consistency with its "past self."
- **Core assumption:** The LLM treats the sequence of personality assessment items as a coherent completion task where consistency with previous tokens is a preferred solution path.
- **Evidence anchors:** Abstract states "conversational history enhances consistency via in-context learning"; Section 6.1 ablation study shows increasing few-shot context causes trajectories to converge toward the full-context setting.

### Mechanism 2
- **Claim:** LLMs vary in "intrinsic" vs. "extrinsic" personality expression, diagnosable via adversarial history manipulation.
- **Mechanism:** By adversarially modifying the conversational history to always show "Option C" as the previous answer, researchers can test if the model blindly mimics the history (extrinsic/context-dependent) or resists to maintain internal coherence (intrinsic/robust).
- **Core assumption:** A model with robust "intrinsic" personality should weigh its internal priors against the provided context, whereas a context-driven model will act as a sequence completer.
- **Evidence anchors:** Abstract states "GPT models response stem from their intrinsic personality traits... whereas Gemini-1.5-Flash and Llama-3.1-8B heavily depend on prior interactions"; Section 6.3 details adversarial attack where Gemini and Llama-8B shifted entirely to false option C.

### Mechanism 3
- **Claim:** Standard agreement metrics fail to capture partial or structural consistency; Gaussian Process-based metrics provide more robust signal.
- **Mechanism:** Trajectories are smoothed and normalized, then modeled using Gaussian Process Regression. Consistency is quantified by the overlap (intersection-over-union) of posterior predictive distributions at each question index.
- **Core assumption:** Response consistency is a continuous property best modeled as a distribution over the trajectory index, where overlapping confidence intervals indicate stable personality expression.
- **Evidence anchors:** Abstract states "introduce novel metrics—Trajectory Consistency (TC) and OCEAN Consistency (OC)—to quantify consistency"; Section 3.2 defines metrics; Table 1 shows TC/OC capturing improvements that simple agreement rates might miss.

## Foundational Learning

- **Concept: Big Five (OCEAN) Model**
  - **Why needed here:** The entire evaluation framework relies on scoring models against Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism.
  - **Quick check question:** If an LLM answers "Very Accurate" to a negatively-keyed Neuroticism item, should the Neuroticism score increase or decrease?

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** The paper attributes the stability gains of the CAPE framework to ICL.
  - **Quick check question:** If you clear the KV cache between questions, will the "consistency via ICL" effect persist?

- **Concept: Psychometric Validity vs. Reliability**
  - **Why needed here:** The paper argues that "context-free" evaluation is artificial (lacking ecological validity) and inconsistent (low reliability).
  - **Quick check question:** Does a high "Trajectory Consistency" score prove the LLM has a human-like personality, or does it merely prove the LLM is stable?

## Architecture Onboarding

- **Component map:** Question Bank -> History Manager -> Target LLM -> Scoring Layer -> Gaussian Process Regression Engine -> TC/OC Calculator

- **Critical path:**
  1. Initialize empty history
  2. Prompt LLM with Item 1 -> Get Response 1 -> Update History
  3. Prompt LLM with Item 2 + History -> Get Response 2 -> Update History
  4. Repeat for all items to form a Trajectory
  5. Run Steps 1-4 three times with different random seeds/perturbations
  6. Feed the 3 trajectories into the TC/OC Metric Calculator

- **Design tradeoffs:**
  - Context Window Usage: Using history consumes tokens, potentially truncating the assessment in models with small windows
  - Metric Sensitivity: The TC metric uses GPR with hyperparameters (kernel, window size); over-smoothing might hide inconsistencies

- **Failure signatures:**
  - Context Collapse: Llama-3.1-8B performance dropping in "context-dependent" settings for specific factors
  - Sycophancy Loop: The "Adversarial Attack" shows failure where model copies false history pattern instead of answering honestly

- **First 3 experiments:**
  1. Baseline Consistency (Stability): Run assessment 3 times (temp=0) with NO history; compute TAR/ED as consistency baselines
  2. CAPE Consistency (History): Run assessment 3 times (temp=0) WITH history; compare TC scores to verify consistency improvement via ICL
  3. Adversarial Sycophancy Check: Inject false history (all "C" answers) and measure if selection distribution shifts significantly towards "C"

## Open Questions the Paper Calls Out
- How does CAPE generalize to open-ended, unconstrained human-LLM interactions beyond structured psychometric questionnaires?
- How can context-aware evaluation be adapted for reasoning-optimized models where conversational history degrades consistency?
- What mechanisms are required to anchor an LLM's "intended" personality to prevent drift when conversational history exerts adversarial influence?

## Limitations
- Framework relies on English-language Big Five inventories and specific set of 7 LLMs tested
- Distinction between "intrinsic" and "extrinsic" personality expression remains philosophically unresolved
- Limited validation with Role-Playing Agents dataset (32 characters) constrains generalizability claims

## Confidence
- **High Confidence**: TC/OC metrics reliably detect consistency improvements when conversational history is retained
- **Medium Confidence**: Claim that standard agreement metrics fail to capture partial consistency lacks strong comparative validation
- **Low Confidence**: Interpretation that CAPE "better aligns with human judgments" is limited by small RPA dataset and indirect alignment metric

## Next Checks
1. Test CAPE with non-English personality inventories (e.g., Chinese BFQ or German NEO-PI-R) to verify language independence
2. Evaluate personality consistency across multiple sessions separated by hours/days to distinguish short-term vs. long-term effects
3. Compare TC/OC performance against simpler trajectory metrics (e.g., Dynamic Time Warping distance) and human inter-rater reliability benchmarks