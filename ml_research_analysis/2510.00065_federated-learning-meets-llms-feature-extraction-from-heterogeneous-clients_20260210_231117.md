---
ver: rpa2
title: 'Federated Learning Meets LLMs: Feature Extraction From Heterogeneous Clients'
arxiv_id: '2510.00065'
source_url: https://arxiv.org/abs/2510.00065
tags:
- federated
- data
- fedllm-align
- learning
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FedLLM-Align introduces a federated learning framework for heterogeneous
  tabular data by using pre-trained LLMs as frozen feature extractors. Each client
  serializes its local tabular records into text, embeds them via models like DistilBERT
  or ClinicalBERT, and trains a lightweight classifier on the resulting semantic embeddings
  under FedAvg.
---

# Federated Learning Meets LLMs: Feature Extraction From Heterogeneous Clients

## Quick Facts
- **arXiv ID:** 2510.00065
- **Source URL:** https://arxiv.org/abs/2510.00065
- **Reference count:** 27
- **Primary result:** FedLLM-Align achieves up to 25% higher F1-score than baselines on heterogeneous tabular data

## Executive Summary
FedLLM-Align introduces a federated learning framework that leverages pre-trained LLMs as frozen feature extractors to handle heterogeneous tabular data across clients. The approach serializes tabular records into text, embeds them using models like DistilBERT or ClinicalBERT, and trains lightweight classifiers on the resulting semantic embeddings under FedAvg. This enables semantically aligned feature extraction without raw data sharing or manual schema harmonization.

The framework demonstrates significant advantages: up to 25% higher F1-score compared to baselines on coronary heart disease and bank churn prediction tasks, 65% reduction in communication overhead, and robust performance even with extreme feature overlap reductions to 20%. The key innovation is using pre-trained LLMs to create semantically equivalent feature spaces across heterogeneous schemas, allowing effective cross-client aggregation.

## Method Summary
FedLLM-Align operates by having each client serialize its local tabular records into natural language sequences, which are then embedded using frozen pre-trained LLMs to produce semantically aligned representations. Only lightweight classifier heads are trained locally and aggregated via FedAvg, with the LLM encoder remaining frozen throughout training. This design enables handling of heterogeneous schemas while maintaining communication efficiency and achieving superior performance on classification tasks compared to traditional federated learning approaches.

## Key Results
- Achieves up to 25% higher F1-score than baselines on heterogeneous tabular data tasks
- Reduces communication overhead by 65% by freezing LLM encoders and training only lightweight classifiers
- Maintains robust performance with 20% schema overlap, achieving F1=0.76±0.03 vs. baseline methods' collapse

## Why This Works (Mechanism)

### Mechanism 1: Semantic Alignment via Pre-trained LLM Embeddings
- **Claim:** Pre-trained LLMs map semantically equivalent features from heterogeneous schemas to nearby points in a shared embedding space, enabling cross-client aggregation without manual schema harmonization.
- **Mechanism:** Tabular records are serialized into natural language sequences (e.g., "Age: 45, BloodPressure: 140/90"). The frozen LLM encoder produces a dense [CLS] embedding that captures semantic meaning. Because the LLM was pre-trained on diverse corpora, it recognizes that "Age: 45" and "PatientAge: 45 years" are semantically equivalent, yielding similar vectors even when feature names differ syntactically.
- **Core assumption:** The pre-trained LLM has sufficient semantic coverage for the domain-specific features encountered in client data.
- **Evidence anchors:**
  - [abstract] "embeddings from models such as DistilBERT, ALBERT, RoBERTa, and ClinicalBERT provide semantically aligned representations"
  - [Section 3.4] "semantic alignment arises from the pre-trained LLMs, which map semantically equivalent attributes and values to nearby points in the embedding space"
  - [corpus] Weak direct corpus evidence; neighbor papers (FedFusion, DP2FL) address heterogeneity via encoder design but do not explicitly validate LLM-based semantic alignment for tabular schemas.
- **Break condition:** If client feature names are obfuscated, numericized, or use domain jargon outside the LLM's pre-training distribution, semantic alignment will degrade.

### Mechanism 2: Communication Efficiency via Frozen Encoder
- **Claim:** Freezing the LLM encoder and training only a lightweight classifier head reduces per-round communication overhead by ~65% compared to methods exchanging full model parameters.
- **Mechanism:** The encoder (e.g., DistilBERT, 768-dim output) remains frozen throughout training. Only the classifier parameters (input: 768, hidden: 16, output: 1) are exchanged via FedAvg. This reduces transmitted data to ~12.3 KB model weights + 0.8 KB overhead per round versus FedXGBoost's 53.9 KB total.
- **Core assumption:** The frozen embeddings are sufficiently informative for the downstream classification task without encoder fine-tuning.
- **Evidence anchors:**
  - [abstract] "reduces communication overhead by 65%"
  - [Section 3.2] "these LLM backbones remain frozen during the training process. This design choice reduces the communications overhead"
  - [Section 5, Table 7] FedLLM-Align: 13.1 KB/round vs. FedXGBoost: 53.9 KB/round (4.1× reduction)
  - [corpus] RefProtoFL (arxiv 2601.14746) similarly reduces communication via prototype exchange, but uses a different mechanism; not direct validation.
- **Break condition:** If downstream tasks require encoder adaptation (e.g., novel domain vocabulary), frozen encoders may underperform, necessitating fine-tuning or adapter layers—which would increase communication.

### Mechanism 3: Graceful Degradation Under Schema Divergence
- **Claim:** FedLLM-Align maintains performance even when schema overlap between clients drops to 20%, whereas baseline methods that require shared feature spaces collapse.
- **Mechanism:** Because semantic alignment operates on natural language representations rather than strict schema matching, the framework can leverage partial feature overlap. The LLM generalizes across feature renames and missing attributes by inferring from available context.
- **Core assumption:** Some semantic overlap exists in the textual descriptions even when raw feature overlap is low.
- **Evidence anchors:**
  - [abstract] "maintains robust performance even with extreme feature overlap reductions"
  - [Section 5, Table 8] At 20% schema overlap: FedLLM-Align F1=0.76±0.03 vs. Homogeneous FedAvg F1=0.32±0.12 vs. FedXGBoost F1=0.04±0.03
  - [corpus] Neighbor papers (Robust Asymmetric Heterogeneous FL) address heterogeneity but focus on corrupted clients/model heterogeneity, not schema-level stress testing.
- **Break condition:** If clients share zero semantically related features (completely disjoint domains), the LLM cannot create meaningful cross-client alignment.

## Foundational Learning

- **Federated Averaging (FedAvg):**
  - **Why needed here:** FedLLM-Align builds on FedAvg for aggregating classifier weights across clients. Understanding how local updates are averaged into a global model is essential for debugging convergence issues.
  - **Quick check question:** Can you explain why FedAvg requires compatible model architectures across clients, and how FedLLM-Align achieves this despite heterogeneous schemas?

- **Transformer [CLS] Token Embeddings:**
  - **Why needed here:** The framework extracts the [CLS] token embedding from the final hidden layer as the record representation. Understanding what [CLS] encodes is critical for interpreting why semantic alignment works.
  - **Quick check question:** What does the [CLS] token represent in BERT-style models, and why is it used for classification tasks?

- **Tabular-to-Text Serialization:**
  - **Why needed here:** The quality of embeddings depends on how tabular data is converted to text. Different serialization strategies (structured, natural language, compact) produce different embedding qualities.
  - **Quick check question:** Given a record `{Age: 45, SysBP: 140}`, how would you serialize it in "structured" vs. "natural language" format, and which would likely yield better embeddings?

## Architecture Onboarding

- **Component map:**
  - Client-side: (1) Preprocessor (median/mode imputation) → (2) Serializer (tabular→text) → (3) Frozen LLM Encoder (DistilBERT/ALBERT/RoBERTa/ClinicalBERT) → (4) Lightweight Classifier (768→16→1 MLP)
  - Server-side: FedAvg aggregator (averages classifier weights only)
  - Communication: Only classifier weights (ΔW) exchanged; embeddings and raw data remain local

- **Critical path:**
  1. Verify serialization format produces consistent tokenization (max 128 tokens)
  2. Confirm encoder outputs 768-dim [CLS] embeddings
  3. Validate classifier dimensionality matches encoder output
  4. Ensure FedAvg aggregation runs for 25 rounds with 10 local epochs per client per round

- **Design tradeoffs:**
  - **Encoder choice:** DistilBERT (255 MB, 45ms inference, F1=0.84) vs. ClinicalBERT (440 MB, 68ms, F1=0.85) — domain specificity vs. resource efficiency
  - **Serialization:** Structured (F1=0.84, high robustness) vs. Compact (F1=0.79, low robustness) — semantic richness vs. token efficiency
  - **Assumption:** Shallow classifier (16 hidden units) is sufficient; deeper classifiers may improve accuracy but increase communication

- **Failure signatures:**
  - F1-score collapses (<0.3) with schema overlap >60% → Check serialization format; compact format may be stripping semantics
  - Convergence instability (oscillating F1 across rounds) → Verify learning rate (should be 0.001) and batch size (32)
  - High cross-client variance (Std >0.1) → Investigate data imbalance or divergent local data distributions
  - Out-of-memory errors on edge clients → Switch to ALBERT (180 MB) or quantized encoders

- **First 3 experiments:**
  1. **Baseline sanity check:** Run FedLLM-Align with DistilBERT on a single client (no federation) to verify end-to-end pipeline produces reasonable F1 (>0.75) on Framingham or churn dataset.
  2. **Schema heterogeneity validation:** Simulate 3 clients with 60% feature overlap using the structured serialization format; confirm F1 >0.80 and convergence within 15 rounds.
  3. **Stress test:** Reduce schema overlap to 20% and compare FedLLM-Align vs. homogeneous FedAvg; expect FedLLM-Align F1 ~0.76 vs. baseline F1 <0.35.

## Open Questions the Paper Calls Out
None

## Limitations
- Semantic alignment heavily depends on LLM pre-training corpus matching client domain terminology
- 65% communication reduction assumes frozen encoders remain adequate across all tasks
- Limited exploration of theoretical limits at extreme schema divergence (0% overlap)

## Confidence
- **Semantic Alignment via LLM Embeddings:** High - Multiple evidence points across the abstract and methodology sections, though direct corpus validation is weak
- **Communication Efficiency via Frozen Encoder:** High - Clear quantitative evidence (4.1× reduction) with explicit architectural justification
- **Graceful Degradation Under Schema Divergence:** Medium - Strong empirical results but limited exploration of edge cases and theoretical bounds

## Next Checks
1. **Domain Vocabulary Stress Test:** Create a synthetic dataset where 30% of feature names are domain-specific medical terms not present in the LLM's pre-training corpus. Measure semantic alignment degradation and F1-score drop compared to standard features.

2. **Encoder Adaptation Experiment:** Implement a lightweight adapter layer (e.g., 2-layer MLP with bottleneck) on top of the frozen encoder and compare communication overhead vs. performance gains against the pure frozen encoder baseline.

3. **Zero Overlap Boundary Test:** Construct a scenario with two clients having completely disjoint feature sets (0% overlap) but semantically related concepts (e.g., "blood pressure" vs. "cardiac pressure"). Test whether FedLLM-Align can still produce meaningful cross-client aggregation or if it fails gracefully.