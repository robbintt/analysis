---
ver: rpa2
title: Global-to-Local or Local-to-Global? Enhancing Image Retrieval with Efficient
  Local Search and Effective Global Re-ranking
arxiv_id: '2509.04351'
source_url: https://arxiv.org/abs/2509.04351
tags:
- re-ranking
- retrieval
- local
- global
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a local-to-global (L2G) image retrieval paradigm
  that flips the conventional global-to-local approach. The authors propose using
  efficient local feature search (via CANN) for initial retrieval and then re-ranking
  the top candidates using global features derived on-the-fly from local feature similarities
  through multidimensional scaling (MDS).
---

# Global-to-Local or Local-to-Global? Enhancing Image Retrieval with Efficient Local Search and Effective Global Re-ranking

## Quick Facts
- **arXiv ID:** 2509.04351
- **Source URL:** https://arxiv.org/abs/2509.04351
- **Reference count:** 39
- **Primary result:** L2G achieves state-of-the-art on Revisited Oxford/Paris, improving mAP by 2-3% over prior methods.

## Executive Summary
This paper introduces a Local-to-Global (L2G) image retrieval paradigm that inverts the conventional global-to-local approach. By using efficient local feature search (via CANN) for initial retrieval and then re-ranking the top candidates using global features derived on-the-fly from local feature similarities through multidimensional scaling (MDS), the method achieves state-of-the-art performance on Revisited Oxford and Paris datasets. The approach is particularly effective for large-scale retrieval with 1M distractors, demonstrating the power of combining localized search with effective re-ranking based on MDS-derived global features.

## Method Summary
The L2G method consists of three stages: (1) Efficient local feature retrieval using CANN with FIRE features and Chamfer similarity to retrieve top-k candidates; (2) MDS embedding computation where pairwise local feature dissimilarities are converted into a Euclidean embedding space using SMACOF; (3) Hybrid re-ranking that combines the MDS embeddings with precomputed SuperGlobal features via a weighted average. The method creates query-specific global features that respect localized similarities, achieving superior performance through the complementary nature of the two feature types.

## Key Results
- Achieves state-of-the-art mAP on Revisited Oxford and Paris datasets, improving over previous methods by 2-3%
- Demonstrates particular strength in large-scale settings with 1M distractors
- Shows the effectiveness of combining localized search with re-ranking based on MDS-derived global features
- The hybrid approach (MDS + SuperGlobal) outperforms either method alone

## Why This Works (Mechanism)

### Mechanism 1: Local Feature Retrieval for Partial Matches
- **Claim:** Local feature retrieval (via CANN) as the initial search stage improves recall for partial matches compared to global feature search.
- **Mechanism:** Efficient local feature search enables retrieval based on detailed, localized similarities across large databases, allowing the system to find images that share specific regions with the query, even if the global appearance differs significantly (partial matches).
- **Core assumption:** Efficient algorithms like CANN make large-scale local feature search computationally feasible and competitive in speed with global feature search.
- **Evidence anchors:** [abstract] "emerging efficient local feature search approaches have opened up new possibilities... enabling detailed retrieval at large scale, to find partial matches which are often missed by global feature search."

### Mechanism 2: On-the-fly MDS for Query-Specific Embeddings
- **Claim:** Creating query-specific global features via Multidimensional Scaling (MDS) on local feature dissimilarities boosts re-ranking performance.
- **Mechanism:** The pairwise dissimilarities (derived from local feature similarities like Chamfer distance) between the query and top-k retrieved candidates are converted into a Euclidean embedding space using MDS (specifically SMACOF). This process respects the rich local similarities in a metric space, creating an embedding tailored to the specific query and its candidate set.
- **Core assumption:** The dissimilarity matrix from local features contains sufficient signal to construct a meaningful Euclidean embedding via MDS, even if the initial dissimilarities are non-metric or incomplete.
- **Evidence anchors:** [section 3.3] "Similarity embedding using MDS can be viewed as generating 'global features' derived from a specific similarity measure... we propose a more efficient alternative that avoids applying MDS to the entire index."

### Mechanism 3: Hybrid Re-ranking with MDS + Global Features
- **Claim:** Combining MDS-derived embeddings with existing global features (e.g., SuperGlobal) during re-ranking yields superior performance over using either feature type alone.
- **Mechanism:** The system performs a weighted average of the similarity scores from the MDS embeddings and a strong global feature method like SuperGlobal. This leverages the complementary nature of the features: MDS embeddings capture fine-grained, query-specific local similarities, while global features provide broader semantic context.
- **Core assumption:** MDS embeddings and standard global features provide complementary information that, when combined appropriately, refines the final ranking more effectively than either alone.
- **Evidence anchors:** [section 4.1] "For re-ranking with global features, we employ a weighted average between the MDS embeddings and SuperGlobal global features... We believe the complementary nature of these embeddings is key to the performance improvements."

## Foundational Learning

- **Concept: Global vs. Local Image Features**
  - **Why needed here:** Understanding the fundamental trade-off is critical. Global features are compact and efficient for search but fail on partial matches. Local features are detailed but expensive to match at scale.
  - **Quick check question:** Why does a global feature search struggle to find a cropped version of a query image in a database? (Answer: Global features aggregate the entire image; a crop has a different global representation, even if it contains the same local content.)

- **Concept: Nearest Neighbor Search & Approximate Methods**
  - **Why needed here:** The first stage of L2G relies on "efficient local feature search" (CANN). This requires understanding how Approximate Nearest Neighbor (ANN) search trades accuracy for massive speedups.
  - **Quick check question:** What is the primary purpose of an approximate nearest neighbor algorithm like CANN in this system? (Answer: To make large-scale search with local features computationally feasible by finding very likely matches without exhaustive comparison.)

- **Concept: Multidimensional Scaling (MDS) & SMACOF**
  - **Why needed here:** The core innovation is using MDS to create a Euclidean embedding from non-metric local dissimilarities. Understanding that MDS seeks to preserve pairwise distances in a lower-dimensional space is key.
  - **Quick check question:** Why is the dissimilarity matrix from local feature matching considered "non-metric," and why does the paper choose SMACOF over classical MDS? (Answer: It may violate metric properties like the triangle inequality. SMACOF is chosen because it iteratively minimizes a stress function and can handle non-metric dissimilarities.)

## Architecture Onboarding

- **Component Map:** Feature Extractor (FIRE) -> Large-Scale Index -> Efficient Retrieval (CANN) -> MDS Embedding -> Hybrid Re-ranker

- **Critical Path:** The entire L2G pipeline is critical. A failure in efficient local retrieval (CANN) means no candidate list for re-ranking. A failure in MDS means no query-specific embedding for refinement. A poor re-ranking combination (weight w) fails to capitalize on the complementary features.

- **Design Tradeoffs:**
  - **Accuracy vs. Latency:** On-the-fly MDS adds ~0.5s to query time vs. using only pre-computed global features.
  - **Storage vs. Accuracy:** Storing local features requires more memory (~21kB/image) compared to compressed global features (~10kB/image).
  - **Simplicity vs. Performance:** A pure global-to-local or global-to-global system is simpler. L2G introduces complexity by inverting the paradigm and adding MDS.
  - **Hyperparameter Sensitivity:** The system relies on several tuned parameters (w, p, epsilon, k), making it potentially brittle if applied to new datasets without re-tuning.

- **Failure Signatures:**
  - **Low Recall on Partial Matches:** Indicates a failure in the initial CANN retrieval or that the "partial" nature is not captured by the local features.
  - **Degraded Re-ranking Performance:** If the final results are worse than the initial CANN ranking, the MDS embedding is likely distorting the local similarities or the weight w is poorly chosen.
  - **High Query Latency:** If the query time significantly exceeds ~0.7s, investigate the MDS computation or the CANN search performance.

- **First 3 Experiments:**
  1. **Ablation on Retrieval Stage:** Replace CANN with a standard global feature search (e.g., SuperGlobal) for the initial retrieval, but keep the MDS re-ranking. Compare performance to the full L2G pipeline.
  2. **Ablation on Re-ranking Stage:** Run the full CANN retrieval, but replace the MDS re-ranking with a standard global feature re-ranker. This quantifies the value of the query-specific MDS embedding.
  3. **Vary the MDS weight w:** Systematically vary the weight w from 0.0 (only SuperGlobal) to 1.0 (only MDS) and plot the mAP curve. This identifies the optimal fusion point and shows if the features are indeed complementary.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the L2G paradigm perform when integrating modern learned similarity measures, such as AMES, compared to the hand-crafted Chamfer similarity used in the experiments?
- **Basis in paper:** [explicit] The authors state in the Future Work section that this approach "opens exciting possibilities for using it with diverse similarity measures, including learned ones like those in AMES."
- **Why unresolved:** The current implementation relies on FIRE features and Chamfer similarity. It is uncertain if the MDS embedding process handles the characteristics of learned similarity metrics effectively without requiring algorithmic modifications to the stress function.

### Open Question 2
- **Question:** How sensitive are the tuned hyperparameters (specifically epsilon, p, w) to domain shifts outside of architectural landmarks?
- **Basis in paper:** [inferred] The authors note that all hyperparameters were "tuned on a small sample (1000 images) of Oxford only."
- **Why unresolved:** It is unclear if the specific configuration for the MDS re-ranker generalizes to domains with different visual properties, such as 3D objects, artwork, or products, without requiring a full re-tuning of the parameters.

### Open Question 3
- **Question:** Can the local feature storage requirements be reduced via compression without negating the accuracy gains provided by the MDS re-ranking?
- **Basis in paper:** [inferred] The Limitations section acknowledges that the memory footprint (~21kB per image) is "higher than typical global feature methods."
- **Why unresolved:** The paper justifies the memory cost via accuracy gains, but does not explore the trade-off boundary where compression artifacts might disrupt the local feature matching essential for the CANN and MDS stages.

## Limitations
- The L2G approach relies heavily on the performance and efficiency of the initial local feature search stage (CANN), which may not scale efficiently to significantly larger databases.
- The per-query MDS computation introduces additional latency that scales quadratically with the number of candidates, adding ~0.5s to query time.
- The success of the hybrid re-ranking depends on finding an optimal weight w, which is dataset-specific and not easily generalizable.

## Confidence

- **High Confidence:** The overall L2G paradigm (local-first, global re-rank) and its state-of-the-art performance on ROxf/RPar are well-supported by the reported results.
- **Medium Confidence:** The specific contributions of each individual component (CANN's efficiency, MDS's embedding quality, the optimal w value) are supported by the paper's analysis, but are more difficult to fully verify without access to the exact implementations.
- **Low Confidence:** The exact computational complexity of the per-query MDS step for varying database sizes and the method's robustness to hyperparameter tuning across diverse datasets are not fully explored.

## Next Checks
1. **Ablation on Retrieval Stage:** Replace CANN with a standard global feature search for initial retrieval, keeping MDS re-ranking, to quantify the value of local-first retrieval.
2. **Ablation on Re-ranking Stage:** Run full CANN retrieval but replace MDS re-ranking with a standard global feature re-ranker to quantify the value of the query-specific MDS embedding.
3. **Parameter Sensitivity Analysis:** Systematically vary the weight w (0.0 to 1.0) and the number of candidates k (e.g., 100, 300, 500, 700) to identify performance sensitivity and optimal settings.