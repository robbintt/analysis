---
ver: rpa2
title: On the Relationship Between Robustness and Expressivity of Graph Neural Networks
arxiv_id: '2504.13786'
source_url: https://arxiv.org/abs/2504.13786
tags:
- graph
- expressivity
- node
- layer
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the vulnerability of Graph Neural Networks
  (GNNs) to bit-flip attacks (BFAs) through a theoretical framework that connects
  expressivity, homophily, and activation functions. The authors establish formal
  criteria and bounds on the number of bit flips required to degrade GNN expressivity,
  showing that ReLU-activated GNNs on highly homophilous graphs with low-dimensional
  or one-hot encoded features are particularly susceptible.
---

# On the Relationship Between Robustness and Expressivity of Graph Neural Networks

## Quick Facts
- arXiv ID: 2504.13786
- Source URL: https://arxiv.org/abs/2504.13786
- Reference count: 18
- One-line primary result: ReLU-activated GNNs on highly homophilous graphs with low-dimensional or one-hot encoded features are particularly susceptible to bit-flip attacks due to injectivity loss in MLPs.

## Executive Summary
This paper analyzes the vulnerability of Graph Neural Networks (GNNs) to bit-flip attacks (BFAs) through a theoretical framework that connects expressivity, homophily, and activation functions. The authors establish formal criteria and bounds on the number of bit flips required to degrade GNN expressivity, showing that ReLU-activated GNNs on highly homophilous graphs with low-dimensional or one-hot encoded features are particularly susceptible. The analysis reveals that losing injectivity in the MLPs directly impacts expressivity, and that resilience is influenced by both architectural properties and dataset characteristics. Empirical results on ten real-world datasets confirm these theoretical insights, with ReLU-activated models showing greater vulnerability to bit flips than Sigmoid or SiLU-activated ones.

## Method Summary
The paper combines theoretical analysis with empirical evaluation on 10 TUDataset graphs. GIN, GCN, and DeepSets models are analyzed under bit-flip attacks targeting weight matrices. Theoretical bounds are derived for the number of bit flips needed to compromise injectivity in MLPs, considering activation functions and homophily. Experiments use untrained models with random initialization, applying semi-random bit flips (sign: 0→1, mantissa/exponent: 1→0) across 1%-95% of target bits. Expressivity is measured as the percentage of non-isomorphic graph pairs distinguished by final embeddings, with 25 runs per setup.

## Key Results
- ReLU-activated GNNs are more vulnerable to bit-flip attacks than Sigmoid or SiLU models, particularly from sign-bit flips that zero activations
- High homophily and low-dimensional/one-hot features amplify vulnerability by reducing the diversity of distinguishable subgraph aggregates
- Expressivity loss correlates strongly with the degradation of injectivity in the MLP layers encoding neighborhood multisets
- Theoretical bounds on bit flips needed to compromise expressivity are validated empirically, with first-layer attacks on ReLU models showing highest vulnerability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Bit flips degrade GNN expressivity by breaking injectivity in the MLP layers that encode neighborhood multisets.
- **Mechanism**: Message-passing GNNs achieve 1-WL-equivalent expressivity when their MLPs maintain injective mappings over constrained input domains. Injectivity requires that distinct aggregated neighborhood representations produce distinct outputs. Bit flips in weight matrices can collapse this property—specifically, when a weight row $w_r$ no longer yields $\langle x_u, w_r \rangle \neq \langle x_v, w_r \rangle$ for some distinguishable inputs $x_u \neq x_v$, the layer and subsequent moment function cease to distinguish those multisets.
- **Core assumption**: The GNN is initially maximally expressive (Definition 1), meaning each $\sigma \circ W^{(j,i)}$ satisfies the layer-wise injectivity condition prior to perturbation.
- **Evidence anchors**:
  - [abstract]: "losing injectivity in the MLPs directly impacts expressivity"
  - [Section 2, Proposition 1]: Derives that linear independence of $\{f(x) \mid x \in S(A)\}$ is sufficient for moment injectivity; non-injectivity of $f$ implies potential indistinguishability.
  - [Section 2.1, Theorem 1]: Provides the node-level upper bound $O(d_{j,i} \cdot m_{j,i} \cdot b)$ on bit flips needed to compromise expressivity.
  - [corpus]: Related work (Rademacher Meets Colors) notes that higher expressivity often correlates with higher generalization error, but does not address injectivity loss mechanisms directly.
- **Break condition**: If the MLPs are not injective even before attack (e.g., trained with over-regularization or insufficient width), expressivity may already be degraded, and the bounds become loose.

### Mechanism 2
- **Claim**: ReLU-activated GNNs are disproportionately vulnerable to sign-bit flips because ReLU requires positive pre-activations to preserve injectivity.
- **Mechanism**: Per Lemma 3, a ReLU layer $\text{ReLU} \circ W^{(j,i)}$ is injective only if for any distinct $x_u, x_v$, there exists a row $w_r$ such that $\langle x_u, w_r \rangle \neq \langle x_v, w_r \rangle$ and at least one of these dot products is positive. Flipping a sign bit in $w_r$ can make both dot products negative, causing ReLU to zero both out—collapsing distinguishability with a single bit rather than $b$ bits.
- **Core assumption**: Weights are stored in signed floating-point representation (e.g., FLOAT32) with an explicit sign bit.
- **Evidence anchors**:
  - [Section 2.3, Lemma 3]: States the injectivity condition for ReLU, highlighting the positive dot product requirement.
  - [Section 2.3]: Explicitly notes that "flipping the sign bits in the target weights will suffice, letting ReLU zero out the activations," reducing the upper bound by factor $b$.
  - [Section 3.1, RQ1]: Empirically confirms ReLU-activated GNNs show vulnerability to sign-bit flips, unlike Sigmoid/SiLU.
  - [corpus]: No direct corpus corroboration; related papers focus on expressivity hierarchies rather than activation-specific robustness.
- **Break condition**: If activations like Sigmoid or SiLU are used, sign-bit flips do not guarantee zeroing; the bound returns to $O(d_{j,i} \cdot m_{j,i} \cdot b)$.

### Mechanism 3
- **Claim**: High homophily and low-dimensional/one-hot features amplify vulnerability by reducing the diversity of distinguishable subgraph aggregates.
- **Mechanism**: Homophily $H_D$ measures how often connected nodes share the same label. High homophily yields sparse, redundant aggregates at the first layer—neighboring nodes contribute similar one-hot vectors, reducing the number of differing entries $nz_H$ between any two nodes' aggregated features. Per Corollary 2, the node-level bound tightens to $O(m_{1,1} \cdot b \cdot nz_H)$ where $nz_H = \min(2 \cdot d \cdot (1 - H_D) \cdot (1 - P_D), n_{1,1})$. Lower $nz_H$ means fewer bits must be flipped to make nodes indistinguishable.
- **Core assumption**: Node features are one-hot or low-dimensional encodings of discrete labels; the first layer is attacked.
- **Evidence anchors**:
  - [Section 2.3, Corollary 2]: Derives the homophily-dependent bound explicitly.
  - [Section 3.1, RQ4]: Reports Spearman correlation $\approx 0.099$ ($p < 0.005$) between homophily and expressivity loss for first-layer attacks.
  - [Section 3.1, RQ4]: Higher feature dimensionality shows protective correlation ($\approx -0.088$).
  - [corpus]: No corpus papers directly address homophily's role in robustness; focus is on expressivity-generalization tradeoffs.
- **Break condition**: If features are dense and high-dimensional, or if the first layer is not the attack target, the homophily effect diminishes.

## Foundational Learning

- **Concept: Weisfeiler-Leman (1-WL) test**
  - **Why needed here**: The paper equates GNN expressivity with 1-WL equivalence; understanding WL color refinement is essential to grasp what "maximally expressive" means and why injectivity of neighborhood aggregation matters.
  - **Quick check question**: Can you explain why two non-isomorphic graphs that 1-WL cannot distinguish (after $k$ iterations) will also be indistinguishable by a $k$-layer GIN if its MLPs are injective?

- **Concept: Moment injectivity and neural multiset functions**
  - **Why needed here**: The theoretical core relies on mapping multisets to moments (sums over transformed elements); injectivity here determines whether distinct neighborhoods remain distinguishable after aggregation.
  - **Quick check question**: Given a multiset aggregation $\hat{f}(X) = \sum_{x \in X} f(x)$, what condition on $f$ guarantees that $\hat{f}$ is injective, and how might a bit flip in $f$'s weights violate it?

- **Concept: Floating-point representation and bit-flip attack surface**
  - **Why needed here**: The attack model targets in-memory weight representations; understanding sign/exponent/mantissa bits is necessary to interpret why ReLU's vulnerability is bit-location-specific.
  - **Quick check question**: In a FLOAT32 weight, flipping which bit type (sign, exponent, mantissa) most drastically changes the numerical value, and how does this interact with ReLU's zeroing behavior?

## Architecture Onboarding

- **Component map**: Input features $X$ -> Aggregation $(A + I) \cdot X$ -> MLP layers $\text{MLP}_l^{(j)}$ with activations $\sigma$ -> Graph readout (sum/mean of node embeddings)

- **Critical path**:
  1. Identify which datasets and tasks demand high expressivity (e.g., distinguishing many 1-WL color classes).
  2. Audit activation functions: if ReLU is in use, estimate vulnerability reduction from switching to SiLU/Sigmoid.
  3. If features are one-hot/low-dimensional and homophily is high, prioritize robustness in the first MLP layer.
  4. For safety-critical deployments, consider adding redundancy (e.g., pre-coloring, dense feature projections) before the first aggregation.

- **Design tradeoffs**:
  - **ReLU vs. Sigmoid/SiLU**: ReLU offers training efficiency but at significantly lower robustness to sign-bit BFAs; SiLU provides a middle ground with smoother gradients and better resilience.
  - **Feature dimensionality**: Increasing dimensionality (e.g., via pre-coloring) improves robustness but adds compute/memory cost.
  - **Model width vs. robustness**: Wider MLPs maintain injectivity more easily but have more parameters that could be attacked; the bounds scale with $m_{j,i}$.

- **Failure signatures**:
  - Sudden drops in graph classification accuracy on WL-distinguishable pairs.
  - Increased collision rate in node embeddings (measured via unique mapping ratio $M_{\Pi_1^k}$).
  - For ReLU models, sign-bit attacks show outsized degradation relative to mantissa attacks.

- **First 3 experiments**:
  1. **Baseline expressivity audit**: Compute $M_{\Pi_1^k}$ and $S_{\Pi_1^k}^{\text{GNN}}$ for your GIN/GCN on your dataset to confirm initial injectivity and WL-equivalence.
  2. **Activation comparison under BFA**: Train identical GIN architectures with ReLU, Sigmoid, and SiLU; apply progressive sign-bit, exponent, and mantissa flips to the first layer; measure $\Delta \text{Exp}$ and correlate with homophily.
  3. **Homophily-feature interaction**: Synthesize graphs with controlled homophily ratios and feature dimensionalities; test first-layer vulnerability bounds from Corollary 2 to validate tightness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do training dynamics and regularization techniques alter the theoretical vulnerability bounds established for untrained GNNs?
- Basis in paper: [explicit] The conclusion states future work will explore factors like "optimization algorithms, learning rates... and regularization."
- Why unresolved: The current framework assumes random weight initialization common in theoretical expressivity analysis, ignoring the parameter shifts induced by backpropagation.
- What evidence would resolve it: Empirical validation of bit-flip attack success rates on trained GNNs compared to the theoretical bounds derived for untrained models.

### Open Question 2
- Question: Does the theoretical relationship between homophily, injectivity, and bit-flip vulnerability extend to attention-based architectures like GAT?
- Basis in paper: [inferred] The paper notes that the analysis "might not extend to GAT" due to pre-transformation attention parameters reducing the influence of standard aggregation rules.
- Why unresolved: Attention mechanisms dynamically weight edges, complicating the linear independence assumptions used to derive bounds for sum-pooling GNNs like GIN.
- What evidence would resolve it: A formal extension of the injectivity lemmas to attention-weighted aggregations, supported by robustness experiments on GAT architectures.

### Open Question 3
- Question: To what degree does the degradation of 1-WL expressivity correlate with actual drops in downstream classification performance?
- Basis in paper: [inferred] The authors note that "expressivity loss may not always reduce practical performance," creating a gap between theoretical distinguishability and task accuracy.
- Why unresolved: The paper focuses on the theoretical capacity to distinguish graphs rather than the semantic correctness of the output for a specific dataset.
- What evidence would resolve it: Experiments quantifying the correlation between the "expressivity" metric (graph distinguishability) and classification accuracy under bit-flip attacks.

## Limitations
- The theoretical analysis assumes models are initialized with random weights and remain untrained, which is atypical for practical deployments.
- The injectivity bounds depend on precise weight configurations and feature distributions; in practice, model compression, quantization, or pruning could alter these assumptions.
- The homophily-dependent bounds assume one-hot or low-dimensional features—results may not extend cleanly to dense, learned embeddings common in modern GNNs.

## Confidence
- **High**: Mechanism 1 (injectivity loss degrades expressivity) and Mechanism 2 (ReLU's sign-bit vulnerability) are directly supported by formal proofs and controlled experiments.
- **Medium**: Mechanism 3 (homophily-feature interaction) is empirically observed but lacks theoretical guarantees under feature learning.
- **Low**: The claim that ReLU-trained models are inherently less robust than Sigmoid/SiLU in real deployments, as training dynamics may mitigate or exacerbate BFA effects.

## Next Checks
1. Replicate expressivity bounds under trained models with varying regularization strengths to test robustness generalization.
2. Evaluate BFA impact on quantized (e.g., INT8) weight representations to assess real-world attack surfaces.
3. Test whether dense, learned features on homophilous graphs maintain the same vulnerability patterns as one-hot encodings.