---
ver: rpa2
title: 'Schedulers for Schedule-free: Theoretically inspired hyperparameters'
arxiv_id: '2511.07767'
source_url: https://arxiv.org/abs/2511.07767
tags:
- learning
- rate
- convergence
- training
- theory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the theoretical analysis of the Schedule-free
  optimization method to support general learning rate schedules beyond constant rates.
  The authors develop a new convergence theorem that allows for arbitrary schedulers
  and derive a theoretically motivated averaging parameter that depends on the learning
  rate.
---

# Schedulers for Schedule-free: Theoretically inspired hyperparameters

## Quick Facts
- arXiv ID: 2511.07767
- Source URL: https://arxiv.org/abs/2511.07767
- Reference count: 40
- Primary result: Extends Schedule-free theory to general learning rate schedules with convergence guarantees

## Executive Summary
This paper extends the theoretical analysis of Schedule-free optimization to support general learning rate schedules beyond constant rates. The authors develop a new convergence theorem that allows for arbitrary schedulers and derive a theoretically motivated averaging parameter that depends on the learning rate. They specialize their theory to the warmup-stable-decay schedule and prove an optimal O(1/√T) convergence rate. Additionally, they propose a new adaptive Polyak learning rate schedule for Schedule-free and prove an anytime optimal convergence rate of O(1/√t) for convex functions.

## Method Summary
The paper modifies Schedule-free optimization by developing a theoretical framework for general learning rate schedules. The key innovation is the dynamic averaging parameter $c_t = \gamma_t / \sum_{i=0}^t \gamma_i$ that couples to the cumulative learning rate, ensuring convergence with arbitrary schedules. They also propose an adaptive Polyak step-size schedule that achieves anytime convergence without requiring knowledge of the total training horizon. The theoretical analysis assumes convex G-Lipschitz objectives but shows remarkable predictive power for non-convex deep learning tasks.

## Key Results
- Proves optimal O(1/√T) convergence rate for Schedule-free with warmup-stable-decay schedule
- Derives theoretically motivated averaging parameter $c_t$ that depends on learning rate schedule
- Proposes adaptive Polyak learning rate schedule achieving O(1/√t) anytime convergence
- Demonstrates convex bounds predict non-convex training dynamics including spikes and divergence
- Empirical validation on image classification and black-box model distillation tasks

## Why This Works (Mechanism)

### Mechanism 1: Coupling Averaging Weights to Cumulative Learning Rates
The method schedulet sets $c_t = \gamma_t / \sum_{i=0}^t \gamma_i$ to guarantee convergence with non-constant learning rate schedules. By weighting the new iterate $z_t$ proportionally to the current step size relative to the total step size mass, the theorem cancels out error terms in the primal-dual update, ensuring the last iterate converges. This generalizes the standard $1/t$ weighting used for constant rates.

### Mechanism 2: Adaptive Polyak Step-size for Anytime Convergence
The schedulep method computes the step size $\gamma_t$ based on the suboptimality gap $(f(y_t) - f(x^*))_+$ and gradient norm. By minimizing the distance to the solution in the update step, the method automatically adjusts step sizes, removing the need to tune a base learning rate schedule for convex problems. This achieves optimal anytime convergence $O(1/\sqrt{t})$.

### Mechanism 3: Predictive Power of Convex Bounds in Non-Convex Training
The bound in Theorem 2.1 depends on the gradient norm $G$ and the learning rate schedule. The authors observe that spikes in the theoretical bound (driven by $\gamma_t^2 G^2$) align temporally with spikes in the empirical training loss of ResNet-20, suggesting the convex bound captures critical system dynamics even for non-convex problems.

## Foundational Learning

- **Concept: Schedule-free Optimization**
  - Why needed: The paper modifies the core update rules of this optimizer
  - Quick check: How does the $c_t$ parameter influence the mix between the previous primal iterate $x_t$ and the gradient accumulator $z_t$?

- **Concept: Warmup-Stable-Decay (WSD) Schedule**
  - Why needed: This is the primary schedule analyzed
  - Quick check: How does the calculation of the averaging parameter $c_t$ change as the learning rate transitions from the stable phase to the decay phase?

- **Concept: Interpolation Assumption**
  - Why needed: Required for the schedulep Polyak variant
  - Quick check: In the black-box distillation setting, what value is used as a proxy for the student's optimal loss $f(x^*)$?

## Architecture Onboarding

- **Component map:** Initial params $x_0$ -> Scheduler (computes $c_t$ or $\gamma_t$) -> State (updates $z$, $x$, $y$) -> Output (training loss, validation metrics)
- **Critical path:** Implementing Eq. (6) for $c_t$. The existing Schedule-free implementation often uses a heuristic $c_t \approx \gamma^2 / \sum \gamma^2$. The theoretical update $c_t = \gamma / \sum \gamma$ is the specific implementation change required to test schedulet.
- **Design tradeoffs:** Theory vs. Performance (theoretically grounded $c_t$ ensures convergence proofs but doesn't necessarily improve generalization), Schedulep vs. Fixed LR (removes LR tuning but introduces dependency on target loss approximation).
- **Failure signatures:** Spikes (large learning rates cause loss spikes and potential divergence), Distillation Drift (if teacher loss is poor proxy for student's optimal loss).
- **First 3 experiments:**
  1. Replicate Figure 1: Train ResNet-20/CIFAR10 with WSD and theoretical $c_t$. Plot Theorem 2.1 bound alongside training loss.
  2. Schedulet Ablation: Compare schedulet vs. heuristic on Wide ResNet (CIFAR10) across base learning rates.
  3. Schedulep Distillation: Implement Algorithm 1 for GPT-2 distillation task. Compare AdamW-Schedulep against AdamW-Schedulefree.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can convergence bounds be established for the practical heuristic averaging parameter ($c_{t+1} = \gamma_t^2 / \sum_{i=0}^t \gamma_i^2$) used in default Schedule-free implementation? The paper's theory only applies to schedulet, not the heuristic used in practice.

- **Open Question 2:** What is the theoretical mechanism that allows convex optimization bounds to predict the behavior of non-convex deep learning training? The authors note this "surprising predictive power" but don't provide a quantitative analysis or theoretical justification.

- **Open Question 3:** Can the proposed Polyak learning rate (schedulep) be effectively adapted for general training settings without relying on interpolation assumption or black-box distillation? Currently limited to models that nearly interpolate data or under black-box distillation setting.

## Limitations

- Theoretical assumptions rely on convex G-Lipschitz objectives, though empirical validation shows strong predictive power for non-convex deep learning tasks
- Schedulep dependency on optimal loss approximation limits applicability beyond interpolation or distillation settings
- Implementation-specific results use GroupNorm instead of BatchNorm, constraining direct comparison with standard deep learning practices

## Confidence

- **High confidence:** Mechanism 1 (coupling averaging weights) - well-supported by Theorem 2.1 and mathematical derivation with proven O(1/√T) rate
- **Medium confidence:** Mechanism 2 (adaptive Polyak step-size) - theoretical backing exists but relies on interpolation assumption and depends on quality of optimal loss approximation
- **Medium confidence:** Mechanism 3 (predictive power of convex bounds) - empirical observation rather than theoretical result, mechanism connecting convex theory to non-convex dynamics remains unexplained

## Next Checks

1. Test Schedulep without interpolation: Implement on standard vision task using running minimum loss as proxy, measure convergence stability and compare against standard Schedule-free to quantify approximation error impact.

2. Verify the $c_t$ implementation difference: Implement both theoretical ($c_t = \gamma_t / \sum_{i=0}^t \gamma_i$) and heuristic ($c_t \approx \gamma_t^2 / \sum_{i=0}^t \gamma_i^2$) variants, conduct systematic comparison across multiple vision tasks and learning rates.

3. Test Schedule-free with different normalization: Replace GroupNorm with LayerNorm or BatchNorm (with running statistics), train on CIFAR10/ResNet-20 to verify theoretical $c_t$ formulation remains stable under different normalization schemes.