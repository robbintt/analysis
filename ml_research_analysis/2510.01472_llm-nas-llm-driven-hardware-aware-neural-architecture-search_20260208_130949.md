---
ver: rpa2
title: 'LLM-NAS: LLM-driven Hardware-Aware Neural Architecture Search'
arxiv_id: '2510.01472'
source_url: https://arxiv.org/abs/2510.01472
tags:
- uni00000013
- uni00000003
- uni00000048
- search
- uni00000057
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-NAS tackles exploration bias in LLM-driven hardware-aware neural
  architecture search (HW-NAS), where LLMs repeatedly propose architectures within
  limited search space and fail to discover models across different latency ranges.
  The core method introduces a complexity-driven partitioning strategy that divides
  the search space into subspaces based on architectural complexity, an LLM-powered
  architecture prompt co-evolution operator that updates a knowledge base of design
  heuristics and performs guided evolution, and a zero-cost predictor for rapid evaluation.
---

# LLM-NAS: LLM-driven Hardware-Aware Neural Architecture Search

## Quick Facts
- **arXiv ID:** 2510.01472
- **Source URL:** https://arxiv.org/abs/2510.01472
- **Reference count:** 40
- **Key outcome:** LLM-NAS achieves up to 54% lower latency and significantly higher hypervolume (HV) and lower inverted generational distance (IGD) compared to baselines, while reducing search cost from days to minutes.

## Executive Summary
LLM-NAS addresses exploration bias in LLM-driven hardware-aware neural architecture search (HW-NAS), where LLMs repeatedly propose architectures within limited search space and fail to discover models across different latency ranges. The core method introduces a complexity-driven partitioning strategy that divides the search space into subspaces based on architectural complexity, an LLM-powered architecture prompt co-evolution operator that updates a knowledge base of design heuristics and performs guided evolution, and a zero-cost predictor for rapid evaluation. Experimental results on HW-NAS-Bench show LLM-NAS achieves up to 54% lower latency and significantly higher hypervolume (HV) and lower inverted generational distance (IGD) compared to baselines, while reducing search cost from days to minutes. The approach effectively mitigates LLM's mode collapse and discovers a more complete Pareto front across multiple hardware targets and datasets.

## Method Summary
LLM-NAS combines complexity-driven search space partitioning, LLM-powered prompt co-evolution, and zero-cost predictor evaluation to discover hardware-aware neural architectures. The method partitions the search space into complexity-based niches (e.g., by counting specific operators like 3x3 convolutions), uses an LLM (GPT-4.1) to analyze prior results and update a knowledge base of design heuristics, and evaluates candidates using an XGBoost ensemble of 13 zero-cost proxies to avoid expensive training. The system runs for 10 evolutionary generations with crossover probability 0.5, maintaining Pareto archives per niche and aggregating them for final results. The approach targets multi-objective optimization balancing accuracy and latency across 6 hardware devices using HW-NAS-Bench data.

## Key Results
- Up to 54% lower latency compared to baselines across hardware targets
- Significantly higher hypervolume (HV↑) and lower inverted generational distance (IGD↓) metrics
- Search cost reduced from days to minutes through zero-cost predictor evaluation
- Ablation study confirms complexity-driven partitioning is critical (performance collapse when removed)

## Why This Works (Mechanism)

### Mechanism 1: Complexity-Driven Search Space Partitioning
Partitioning the search space into complexity-based niches structurally forces the LLM to explore regions it would otherwise ignore due to generative bias. By constraining the LLM to generate architectures only within specific complexity bounds (e.g., "exactly 2 3x3 convolutions"), the system prevents the model from defaulting to safe, high-accuracy but high-latency designs. This ensures the resulting Pareto front covers low-latency, resource-constrained solutions.

### Mechanism 2: Prompt-Design Co-evolution
A feedback loop where the LLM analyzes its own failure cases to update a knowledge base of heuristics improves subsequent generation quality more effectively than static prompts. The system executes a two-stage cycle where the LLM first analyzes prior architectures and their scores to update a "Knowledge Base" (e.g., learning that "avg_pool adds latency without accuracy gain"), then generates new architectures using these rules. This moves the LLM from a random generator to a stateful agent that "remembers" design principles.

### Mechanism 3: Zero-Cost (ZC) Ensemble Surrogates
Replacing expensive training with an ensemble of zero-cost proxies allows for rapid evaluation necessary for evolutionary search, provided the proxy ranking correlates with ground truth. Instead of training candidate networks, the framework uses an XGBoost model trained on 13 ZC proxies (e.g., synflow, snip) to predict accuracy. This reduces evaluation time from GPU-hours to milliseconds, enabling the system to iterate hundreds of times in minutes.

## Foundational Learning

- **Concept:** Pareto Fronts & Multi-Objective Optimization
  - **Why needed here:** The core goal is not one "best" model, but a set of models trading off accuracy vs. latency. Understanding non-dominated sorting is required to interpret the HV and IGD metrics.
  - **Quick check question:** If Model A has higher accuracy and lower latency than Model B, does Model B belong on the Pareto front?

- **Concept:** Zero-Cost Proxies (e.g., SynFlow, SNIP)
  - **Why needed here:** The method relies on these metrics to guide search. Understanding that they approximate "trainability" or "information flow" at initialization is key to knowing where the method is fragile.
  - **Quick check question:** Do ZC proxies require any backward passes or weight updates during the search phase?

- **Concept:** LLM Exploration Bias (Mode Collapse)
  - **Why needed here:** This is the problem the paper solves. One must understand that LLMs tend to converge on "safe" modes (common, popular architectures) rather than exploring diverse or extreme solutions without explicit constraints.
  - **Quick check question:** Why might asking an LLM to "find a fast network" fail to find the *fastest* possible network?

## Architecture Onboarding

- **Component map:** Partition Engine -> Evolutionary Manager -> LLM Agent -> ZC Evaluator -> Pareto Archive
- **Critical path:** The Partitioning Strategy. As shown in the ablation study (Table 5), removing this component causes "catastrophic performance collapse." Onboarding should focus first on defining the correct complexity metric for the target domain.
- **Design tradeoffs:**
  - Granularity of Niches: Too few niches → mode collapse persists; Too many niches → insufficient population per niche
  - Predictor Fidelity vs. Speed: Using a larger ensemble of ZC proxies improves accuracy but slows down the search loop
- **Failure signatures:**
  - Clustering: Generated architectures all sit at ~5ms latency, ignoring 1-2ms range → Partitioning failed or LLM ignored constraints
  - Invalid Architectures: LLM suggests ops not allowed in the specific niche → Prompt parsing failure or context window overflow
  - Stagnation: HV stops improving early → Knowledge base became cyclic or local minimum reached
- **First 3 experiments:**
  1. Sanity Check (Ablation): Run search on a known benchmark (e.g., HW-NAS-Bench/CIFAR-10) *without* partitioning. Verify that the results cluster (mode collapse) as predicted in Figure 1(a).
  2. Proxy Validation: Before full search, validate the ZC predictor's Spearman correlation on a hold-out set of 100 architectures to ensure the "fitness function" is meaningful.
  3. Niche Coverage: Run the full pipeline and plot the latency distribution of generated candidates. Ensure uniform coverage across all defined niches (S0 to S5).

## Open Questions the Paper Calls Out
- Can the complexity-driven partitioning strategy be automated rather than manually designed based on search space analysis?
- How sensitive is LLM-NAS to the choice of underlying language model?
- What is the optimal number and granularity of complexity-based niches?
- Does LLM-NAS maintain performance when latency is measured on real hardware with noise, rather than lookup tables?

## Limitations
- Method's performance depends critically on the accuracy of the zero-cost predictor, which may suffer from distribution shift during co-evolution
- Exact prompt templates used for knowledge base updates and architecture generation are not fully specified, affecting reproducibility
- Partitioning strategy assumes architectural complexity correlates strongly with hardware latency, which may not hold for specialized accelerators

## Confidence
- **High Confidence:** The core mechanism of complexity-driven partitioning and its role in preventing mode collapse is well-supported by the ablation study and theoretical reasoning
- **Medium Confidence:** The LLM-powered co-evolution mechanism shows promise but relies heavily on the quality of the zero-cost predictor's rankings
- **Medium Confidence:** The experimental results demonstrate significant improvements over baselines, though the extent of improvement may vary with different hardware targets and search spaces

## Next Checks
1. Validate the zero-cost predictor's Spearman correlation on a hold-out set of 100 architectures specifically from the complexity-based niches to ensure distribution alignment
2. Test the partitioning strategy on a different hardware target (e.g., specialized ASIC) to verify the assumed correlation between operator count and latency holds across hardware types
3. Run an ablation study where the LLM generates architectures without knowledge base updates to quantify the contribution of the co-evolution mechanism versus simple evolutionary search