---
ver: rpa2
title: Challenge on Optimization of Context Collection for Code Completion
arxiv_id: '2510.04349'
source_url: https://arxiv.org/abs/2510.04349
tags:
- code
- context
- completion
- competition
- phase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The challenge focused on optimizing context collection for fill-in-the-middle
  code completion in Python and Kotlin. Participants developed strategies to gather
  relevant code snippets from repositories to improve completion quality using chrF
  as the evaluation metric across three models (Codestral, Qwen2.5-Coder, Mellum).
---

# Challenge on Optimization of Context Collection for Code Completion

## Quick Facts
- **arXiv ID:** 2510.04349
- **Source URL:** https://arxiv.org/abs/2510.04349
- **Reference count:** 26
- **Primary result:** Optimization of context collection for fill-in-the-middle code completion using chrF metric across Python and Kotlin languages

## Executive Summary
This challenge focused on optimizing context collection strategies for fill-in-the-middle code completion in Python and Kotlin. Participants developed methods to gather relevant code snippets from repositories to improve completion quality, evaluated using the chrF metric across three models (Codestral, Qwen2.5-Coder, Mellum). The competition attracted 19 teams for Python and 8 for Kotlin during the public phase, with 6 teams submitting solutions for the private phase.

The winning team, SpareCodeComplete, achieved the highest average chrF scores of 0.725 for Python and 0.753 for Kotlin during the private phase. Teams employed various strategies including parsing tools (AST, Tree-sitter, PSI) to extract symbol definitions and retrieval methods (BM25, FAISS, Zoekt) to rank and select context chunks. Common approaches involved extracting symbols from prefix/suffix, retrieving definitions, and assembling context from modified files and similar code snippets.

## Method Summary
Participants developed context collection strategies using parsing tools to extract symbol definitions and combined these with retrieval methods to rank and select relevant code chunks. The approaches typically involved extracting symbols from prefix and suffix code, retrieving symbol definitions using various indexing methods, and assembling context from modified files and similar code snippets. Teams used a combination of AST parsing, Tree-sitter, and PSI analysis to understand code structure, then employed retrieval systems like BM25, FAISS, and Zoekt to find relevant context. The final context was constructed by combining symbol definitions, modified files, and similar code snippets to provide comprehensive context for code completion.

## Key Results
- Team SpareCodeComplete achieved highest average chrF scores: 0.725 (Python) and 0.753 (Kotlin)
- Team NoMoreActimel ranked second in both tracks with scores of 0.725 (Python) and 0.719 (Kotlin)
- Competition attracted 19 teams for Python and 8 for Kotlin during public phase
- Complete dataset is available under CC BY 4.0 license

## Why This Works (Mechanism)
The optimization of context collection improves code completion quality by providing more relevant and comprehensive context to the completion models. By extracting symbol definitions and retrieving similar code snippets, the models receive richer semantic information that enables more accurate predictions. The use of multiple parsing tools and retrieval methods allows for comprehensive context gathering that captures both syntactic and semantic relationships in the code. The chrF metric effectively measures the quality of completions by comparing generated code against ground truth, ensuring that the optimization strategies actually improve practical completion performance.

## Foundational Learning
- **chrF metric** - Character n-gram F-score for evaluating code completion quality; needed to quantitatively assess completion accuracy; quick check: compare chrF scores against human evaluation
- **AST parsing** - Abstract Syntax Tree analysis for understanding code structure; needed to extract symbol definitions and relationships; quick check: verify extracted symbols match code semantics
- **Retrieval-based context gathering** - Using indexing systems (BM25, FAISS, Zoekt) to find relevant code snippets; needed to expand context beyond local definitions; quick check: measure retrieval relevance scores
- **Fill-in-the-middle completion** - Code completion where both prefix and suffix are available; needed to understand the specific completion scenario; quick check: validate completion task setup
- **Symbol definition extraction** - Identifying variable, function, and class definitions; needed to provide semantic context; quick check: ensure all symbols are correctly identified
- **Context ranking** - Ordering retrieved snippets by relevance; needed to prioritize most useful context; quick check: validate ranking improves completion quality

## Architecture Onboarding

**Component Map:** Code Parsing -> Symbol Extraction -> Context Retrieval -> Context Ranking -> Completion Model

**Critical Path:** The critical path involves parsing the code to extract symbols, retrieving relevant context for those symbols, ranking the retrieved context by relevance, and feeding the optimized context to the completion model. This pipeline ensures that the most semantically relevant information is provided to the model for accurate completion.

**Design Tradeoffs:** Teams balanced between comprehensive context gathering (which could include irrelevant information) and focused context selection (which might miss important details). The choice of retrieval method involved tradeoffs between precision and recall, while parsing depth affected both accuracy and computational efficiency.

**Failure Signatures:** Common failure modes included retrieving irrelevant context snippets, missing critical symbol definitions, over-reliance on local context leading to incomplete completions, and computational bottlenecks in processing large codebases. Poor context ranking could lead to drowning relevant information in noise.

**3 First Experiments:**
1. Test basic context collection using only local symbol definitions without retrieval
2. Implement simple BM25 retrieval and compare against baseline
3. Evaluate different parsing depths (shallow vs deep AST analysis) on completion quality

## Open Questions the Paper Calls Out
None

## Limitations
- chrF metric may not fully capture semantic correctness of code completions
- Competition focus on Python and Kotlin limits generalizability to other programming languages
- All teams used retrieval-based approaches, suggesting limited exploration of alternative methods like in-context learning or fine-tuning
- Private phase had relatively few submissions (6 teams), which may affect the robustness of rankings

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Competition structure, evaluation methodology, and reported chrF scores are verifiable | High |
| Generalizability of approaches beyond Python/Kotlin | Medium |
| chrF captures complete completion quality | Medium |
| Limited exploration of alternative methods based on dominance of retrieval approaches | Medium |

## Next Checks
1. Validate chrF score correlations with developer productivity metrics in real-world coding scenarios
2. Test whether top-ranking approaches generalize to additional programming languages beyond Python and Kotlin
3. Compare retrieval-based approaches against non-retrieval alternatives (e.g., fine-tuning, in-context learning) on the same dataset