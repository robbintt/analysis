---
ver: rpa2
title: Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label
  Visual Recognition
arxiv_id: '2511.20641'
source_url: https://arxiv.org/abs/2511.20641
tags:
- uni00000013
- uni00000011
- uni00000018
- classes
- tail
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses long-tailed multi-label visual recognition,
  where images contain multiple objects with highly imbalanced class distributions,
  leading to biased models favoring head classes. The proposed Correlation Adaptation
  Prompt Network (CAPNET) leverages pre-trained vision-language models (VLMs) like
  CLIP to mitigate these imbalances.
---

# Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition

## Quick Facts
- **arXiv ID**: 2511.20641
- **Source URL**: https://arxiv.org/abs/2511.20641
- **Authors**: Wei Tang; Zuo-Zheng Wang; Kun Zhang; Tong Wei; Min-Ling Zhang
- **Reference count**: 40
- **Primary result**: CAPNET achieves mAP scores of 93.03% on VOC-LT and 76.36% on COCO-LT, surpassing state-of-the-art methods by up to 10.17%

## Executive Summary
This paper addresses the challenge of long-tailed multi-label visual recognition, where images contain multiple objects with highly imbalanced class distributions. The proposed Correlation Adaptation Prompt Network (CAPNET) leverages pre-trained vision-language models (VLMs) like CLIP to mitigate these imbalances. CAPNET explicitly models label correlations using a graph convolutional network (GCN) derived from CLIP's textual encoder, replacing noisy caption-derived priors with robust semantic embeddings. The method incorporates learnable soft prompts, a distribution-balanced Focal loss with class-aware re-weighting, and parameter-efficient fine-tuning (AdaptFormer) to prevent overfitting on tail classes. Extensive experiments demonstrate significant performance gains, particularly for underrepresented tail classes.

## Method Summary
CAPNET is designed to address long-tailed multi-label visual recognition by leveraging pre-trained vision-language models. The core innovation is the use of a GCN to model label correlations, with the graph structure derived from CLIP's textual encoder rather than noisy captions. The model uses learnable soft prompts for better adaptation, a distribution-balanced Focal loss with class-aware re-weighting to handle imbalance, and AdaptFormer for parameter-efficient fine-tuning. These components work together to mitigate the bias towards head classes and improve performance on tail classes.

## Key Results
- CAPNET achieves mAP scores of 93.03% on VOC-LT and 76.36% on COCO-LT
- Outperforms state-of-the-art methods by up to 10.17%
- Demonstrates significant performance gains for underrepresented tail classes

## Why This Works (Mechanism)
CAPNET leverages pre-trained vision-language models (VLMs) to address long-tailed multi-label visual recognition by explicitly modeling label correlations. The graph convolutional network (GCN) uses CLIP's textual encoder to derive robust semantic embeddings, replacing noisy caption-derived priors. This approach captures complex label relationships more effectively. The combination of learnable soft prompts, distribution-balanced Focal loss, and parameter-efficient fine-tuning (AdaptFormer) enables better adaptation to imbalanced distributions while preventing overfitting on tail classes.

## Foundational Learning

**Vision-Language Models (VLMs)**: Pre-trained models like CLIP that jointly process visual and textual information. *Why needed*: Provide robust semantic embeddings for label correlation modeling. *Quick check*: Verify CLIP's ability to capture semantic relationships between labels in your dataset.

**Graph Convolutional Networks (GCNs)**: Neural networks that operate on graph-structured data to learn node representations. *Why needed*: Model complex label correlations in multi-label classification. *Quick check*: Ensure your label correlation graph is well-defined and not too sparse.

**Distribution-Balanced Focal Loss**: A modified version of focal loss that accounts for class imbalance. *Why needed*: Mitigate bias towards head classes in long-tailed distributions. *Quick check*: Verify that the loss function effectively re-weights samples based on their class frequencies.

## Architecture Onboarding

**Component Map**: CLIP textual encoder -> GCN (label correlations) -> Soft prompts -> Vision encoder -> Classifier -> Distribution-balanced Focal loss with class-aware re-weighting

**Critical Path**: CLIP textual encoder -> GCN -> Soft prompts -> Vision encoder -> Classifier -> Loss function

**Design Tradeoffs**: The use of AdaptFormer for parameter-efficient fine-tuning reduces overfitting but may limit model capacity. The GCN-based label correlation modeling depends on the quality of CLIP's semantic embeddings, which may not capture all complex relationships.

**Failure Signatures**: Poor performance on tail classes may indicate issues with label correlation modeling or insufficient adaptation via soft prompts. Overfitting on head classes suggests the need for stronger regularization or loss re-weighting.

**First Experiments**:
1. Evaluate CAPNET's performance with and without the GCN-based label correlation modeling.
2. Test the impact of different prompt initialization strategies on tail class performance.
3. Compare the effectiveness of AdaptFormer versus full fine-tuning on highly imbalanced datasets.

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on pre-trained CLIP models may limit generalizability to domains with significantly different visual characteristics.
- Effectiveness of label correlation modeling via GCN may be sensitive to the quality and completeness of CLIP textual embeddings.
- Use of AdaptFormer, while preventing overfitting, may restrict model capacity to fully adapt to specialized long-tailed distributions.

## Confidence
- **High**: CAPNET's superior performance on VOC-LT and COCO-LT datasets, as demonstrated by quantitative mAP scores.
- **Medium**: The robustness of CAPNET's label correlation modeling via CLIP-derived GCN, given the dependency on CLIP's semantic embeddings.
- **Medium**: The effectiveness of AdaptFormer in preventing overfitting on tail classes, though this may come at the cost of reduced model capacity.

## Next Checks
1. Test CAPNET on additional long-tailed multi-label datasets with more diverse and noisy real-world visual content to assess generalizability.
2. Conduct ablation studies to isolate the contributions of the GCN-based label correlation modeling and the distribution-balanced Focal loss with class-aware re-weighting.
3. Evaluate CAPNET's performance under varying degrees of class imbalance and label correlation complexity to identify potential limitations in extreme scenarios.