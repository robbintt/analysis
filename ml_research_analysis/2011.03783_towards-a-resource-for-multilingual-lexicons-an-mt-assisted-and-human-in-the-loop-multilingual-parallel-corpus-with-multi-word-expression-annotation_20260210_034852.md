---
ver: rpa2
title: 'Towards a resource for multilingual lexicons: an MT assisted and human-in-the-loop
  multilingual parallel corpus with multi-word expression annotation'
arxiv_id: '2011.03783'
source_url: https://arxiv.org/abs/2011.03783
tags:
- translation
- mwes
- corpus
- sentence
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces AlphaMWE, a multilingual parallel corpus with
  verbal multi-word expression (vMWE) annotations across Arabic, Chinese, German,
  Italian, Polish, and English. The corpus is constructed by translating an English
  vMWE-annotated corpus using machine translation (MT) followed by human post-editing
  and annotation.
---

# Towards a resource for multilingual lexicons: an MT assisted and human-in-the-loop multilingual parallel corpus with multi-word expression annotation

## Quick Facts
- arXiv ID: 2011.03783
- Source URL: https://arxiv.org/abs/2011.03783
- Reference count: 0
- Primary result: A multilingual parallel corpus with vMWE annotations for 6 languages, showing ~20% of MT errors are MWE-related

## Executive Summary
This work introduces AlphaMWE, a multilingual parallel corpus with verbal multi-word expression (vMWE) annotations across Arabic, Chinese, German, Italian, Polish, and English. The corpus is constructed by translating an English vMWE-annotated corpus using machine translation (MT) followed by human post-editing and annotation. The resulting dataset includes sentence-aligned bilingual and multilingual vMWE pairs, with Arabic covering both standard and dialectal variations. Quality control involved multiple rounds of post-editing and cross-validation. Analysis of MT outputs across four state-of-the-art systems (DeepL, GoogleMT, Bing, Baidu) revealed significant challenges in translating MWEs, particularly idiomatic expressions, metaphors, and named entities.

## Method Summary
The AlphaMWE corpus is constructed using a machine translation plus post-editing (MT+PE) approach. Starting with 750 sentences from the PARSEME 2018 English corpus containing pre-annotated vMWEs, the authors first generate translations using selected state-of-the-art MT systems (DeepL for most languages, GoogleMT for Arabic). Native speakers then post-edit the MT output for meaning and fluency while annotating target-side vMWEs aligned to source vMWEs. A second annotator performs quality rechecking until consensus is reached. The corpus includes sentence-aligned bilingual and multilingual vMWE pairs, with Arabic covering both standard and dialectal variations. Evaluation of MT quality used the HOPE metric with hierarchical penalty scoring across 8+ error types.

## Key Results
- AlphaMWE corpus provides 750 sentence-aligned bilingual and multilingual vMWE pairs across 6 languages
- MT system comparison showed DeepL produced the best overall translations for Chinese, German, Italian, and Polish
- HOPE metric evaluation on English-Arabic pair revealed 21% major translation errors and 44% minor errors, with MWE-related errors comprising nearly 20% of all errors
- The corpus demonstrates significant challenges in MT for idiomatic expressions, metaphors, and named entities

## Why This Works (Mechanism)

### Mechanism 1: MT-Assisted Human-in-the-Loop Corpus Construction
- Claim: Using MT output as a starting point followed by human post-editing produces quality parallel corpora more efficiently than human translation from scratch.
- Mechanism: SOTA NMT engines generate adequate raw translations that are faster to post-edit than translating from scratch. Human post-editors correct errors and ensure quality while native speakers annotate target-side MWEs aligned to source MWEs.
- Core assumption: Current NMT quality is sufficient that post-editing effort is meaningfully less than human translation from scratch.
- Evidence anchors:
  - [abstract] "We performed machine translation of this source corpus followed by human post-editing and annotation of target MWEs."
  - [section 3.1] "The rationale for the MT+PE design is that firstly the state-of-the-art NMT engines can produce relatively good results even though often with a certain degree of mistakes, which already saves a lot of time in comparison to human translation from scratch."
  - [corpus] No quantitative comparison of post-editing time vs. human translation time provided.

### Mechanism 2: Cross-Validation Quality Control
- Claim: Having each sentence reviewed by at least two annotators reduces human-introduced errors and ensures consensus.
- Mechanism: A second annotator performs quality rechecking after initial post-editing and annotation. Disagreements are resolved through consensus, catching individual errors and ensuring consistent annotation.
- Core assumption: Human annotators make individual errors that peer review can detect.
- Evidence anchors:
  - [abstract] "Strict quality control was applied for error limitation, i.e., each MT output sentence received first manual post-editing and annotation plus a second manual quality rechecking till annotators' consensus is reached."
  - [section 3.1] "Tagging errors are more likely to occur if only one human has seen each sentence."
  - [corpus] No corpus evidence quantifying error reduction from cross-validation; no inter-annotator agreement metrics reported.

### Mechanism 3: HOPE Metric Error Quantification
- Claim: The HOPE metric with hierarchical penalty scoring can quantify MT errors by type and severity, revealing approximately 20% of errors are MWE-related.
- Mechanism: Human evaluators assign penalty scores (1, 2, 4, 8, 16) based on severity levels (minor, medium, major, severe, critical) across error categories including MWE-specific types. Scores aggregate from segment to system level.
- Core assumption: Human judgment can reliably and consistently categorize and score translation errors.
- Evidence anchors:
  - [section 5.1] "The HOPE metric has a hierarchical scoring procedure, from the segment level to the system level. Each error type is also assigned with specific penalty scores according to the severity level."
  - [section 5.2] "The MMC and SKP error types that we added to the HOPE metric occupied 17 percent and 6 percent of errors. This also reflected that MWE-related errors take nearly 20 percent of all errors."
  - [corpus] Limited evidence; only 150 segments evaluated for English-Arabic with no inter-annotator agreement reported.

## Foundational Learning

- Concept: **Multi-word Expressions (MWEs) / Verbal MWEs (vMWEs)**
  - Why needed here: MWEs are the core annotation target and the primary source of MT errors studied. Understanding their non-compositional, idiomatic nature is essential for understanding why MT systems struggle with them.
  - Quick check question: Why is "kick the bucket" (idiom meaning "to die") harder to translate than "kick the ball" (literal verb phrase)?

- Concept: **Post-editing in Machine Translation**
  - Why needed here: The entire corpus construction pipeline depends on post-editing MT output. Understanding what post-editing involves—correcting mistranslations, improving fluency, preserving register—is critical.
  - Quick check question: What types of errors would you prioritize fixing when post-editing MT output for a corpus resource vs. for production deployment?

- Concept: **PARSEME Shared Task vMWE Categories**
  - Why needed here: The corpus uses PARSEME-defined categories (inherently adpositional verbs, light verb constructions, multi-verb constructions, verbal idioms, verb-particle constructions). Annotators must understand these distinctions.
  - Quick check question: What distinguishes a light verb construction (e.g., "take a walk") from a verbal idiom (e.g., "kick the bucket")?

## Architecture Onboarding

- Component map:
  - Source corpus: PARSEME 2018 English corpus (750 sentences with vMWE annotations)
  - MT systems: DeepL (primary for Chinese, German, Polish, Italian), GoogleMT (primary for Arabic), with comparisons to Bing Translator and Baidu Fanyi
  - Post-editors: Native speakers per target language (2-4 annotators per language pair)
  - Quality control: Cross-validation requiring ≥2 annotators per sentence
  - Evaluation: HOPE metric (hierarchical scoring with 8+ error types)
  - Output: AlphaMWE corpus with sentence-aligned parallel text, vMWE annotations, and bilingual MWE alignments

- Critical path:
  1. Extract 750 English sentences with vMWE tags from PARSEME 2018 corpus
  2. Generate initial translations using selected MT system (manual comparison of ~10 sentences across systems to select)
  3. Human post-editing: correct mistranslations, preserve meaning-equivalency, maintain register
  4. Annotate target-side vMWEs and align bilingually with source vMWEs
  5. Second-pass quality check by different annotator → resolve disagreements via consensus
  6. (Optional) Quantify errors using HOPE metric for evaluation purposes

- Design tradeoffs:
  - MT+PE vs. human translation: Faster but may bias evaluation toward the MT system used for raw output
  - Corpus size (750 sentences): Authors cite research that 200+ sentences can sufficiently reflect MT quality for specific phenomena, but this limits use for large-scale training
  - DeepL selection: Based on manual inspection of ~10 sentences per language pair—not a systematic benchmark
  - Verbal MWEs only: Narrower scope following PARSEME guidelines, but excludes noun compounds and other MWE types
  - Arabic: GoogleMT selected over Systran based on manual comparison; dialectal Arabic translated from scratch

- Failure signatures:
  - Literal translation of idioms: "cutting capers" → "cutting capers/cabbage" (should be "happily jumping")
  - Lost figurative meaning: "gone to his head" → "beer moved to his head" (should be "got slightly drunk")
  - Context-unaware errors: "did not give me the time of day" → "did not give me time" (should be "did not pay attention to me")
  - Gender/formality mismatches: Wrong verb forms in gendered languages (Polish, Arabic)
  - Named entity errors: "Absalom" → "Abraham"; literature terms like "de-gnoming" mishandled

- First 3 experiments:
  1. Replicate the MT system comparison: Select 10 sentences with varied MWE types, run through DeepL, GoogleMT, Bing, and Baidu. Score outputs manually using the paper's error categories to validate the DeepL selection rationale.
  2. Apply HOPE metric: Take 50 English-Arabic segments from AlphaMWE, have a native Arabic speaker score GoogleMT outputs. Compare error distribution to the paper's reported ~20% MWE-related errors.
  3. Use AlphaMWE as MT test set: Run a modern MT system (not in the original comparison) on the English source sentences, compute BLEU against post-edited references, and analyze MWE-specific accuracy using the annotated vMWE alignments.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can novel neural network structures be designed to explicitly incorporate Multi-Word Expression (MWE) compositionality into the machine translation learning stage?
- Basis in paper: [explicit] The conclusion explicitly asks if it is "possible to design novel neural network structures to incorporate the MWE compositionality as part of the MT learning stage."
- Why unresolved: Current state-of-the-art models generally process MWEs compositionally, failing to recognize that the meaning of the whole often differs from the sum of its parts.
- Evidence would resolve it: The development of a specialized architecture that demonstrates improved translation accuracy for idiomatic MWEs on the AlphaMWE dataset compared to standard Transformer models.

### Open Question 2
- Question: How can document-level context be effectively utilized to resolve ambiguity in MWE translation where sentence-level context is insufficient?
- Basis in paper: [explicit] Section 4.1.6 discusses "Context-Unaware Ambiguity" and notes that current MT models often fail to utilize context inference, even when broader context is available.
- Why unresolved: The paper observes that document-level MT features in commercial systems (like Google or DeepL) often do not change the translation of ambiguous MWEs compared to sentence-level outputs.
- Evidence would resolve it: A model that successfully disambiguates context-dependent phrases (e.g., "give me the time of day") by analyzing the surrounding discourse rather than treating sentences in isolation.

### Open Question 3
- Question: How can bilingual terminologies or dictionaries be integrated into NMT systems to improve the translation of metaphorical and idiomatic phrases?
- Basis in paper: [explicit] Section 7 explicitly asks "how to integrate bilingual terminologies or dictionaries including paraphrases and synonyms to improve metaphorical and idiomatic phrase translation."
- Why unresolved: Data-driven models often struggle with low-frequency idioms and metaphors without explicit lexical knowledge, leading to literal translations that miss the intended meaning.
- Evidence would resolve it: Improved performance on the "MWE Missed Chance" (MMC) error category defined in the paper's HOPE metric evaluation.

## Limitations
- Limited error evaluation: Only 150 segments evaluated for English-Arabic with no inter-annotator agreement metrics reported
- MT system selection bias: Selection based on ~10 manual comparisons per language rather than systematic benchmarking
- Corpus size constraints: 750 sentences may be insufficient for large-scale training applications
- Narrow MWE scope: Focus on verbal MWEs only, excluding noun compounds and other MWE types

## Confidence
- High confidence: MT+PE methodology is established and produces quality parallel corpora
- Medium confidence: Claim that post-editing is meaningfully faster than human translation lacks quantitative validation
- Low confidence: 20% MWE error rate generalizability is limited by small evaluation sample and lack of agreement metrics

## Next Checks
1. Compute inter-annotator agreement (Cohen's Kappa) for MWE annotation consistency across post-editing and quality control phases
2. Conduct controlled experiment comparing post-editing time versus human translation time for a subset of sentences
3. Replicate HOPE metric evaluation using alternative MT systems (e.g., modern GPT-based models) on the AlphaMWE corpus