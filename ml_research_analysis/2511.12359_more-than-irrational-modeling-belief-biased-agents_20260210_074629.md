---
ver: rpa2
title: 'More Than Irrational: Modeling Belief-Biased Agents'
arxiv_id: '2511.12359'
source_url: https://arxiv.org/abs/2511.12359
tags:
- memory
- agent
- user
- cognitive
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of modeling and inferring sub-optimal
  behavior in human-AI collaboration, specifically when such behavior stems from cognitive
  bounds and biased beliefs rather than irrationality. The authors propose a computational-rational
  (CR) user model that explicitly captures how bounded memory processes lead to biased
  belief states, resulting in seemingly sub-optimal but internally rational decision-making.
---

# More Than Irrational: Modeling Belief-Biased Agents

## Quick Facts
- arXiv ID: 2511.12359
- Source URL: https://arxiv.org/abs/2511.12359
- Reference count: 40
- Primary result: CR model with nested particle filtering accurately recovers cognitive bounds and enables adaptive AI assistance in T-maze navigation task

## Executive Summary
This paper presents a computational-rational (CR) framework for modeling human decision-making under bounded memory, explicitly capturing how memory limitations lead to biased beliefs and seemingly sub-optimal behavior. The authors introduce an efficient online inference method using nested particle filtering to simultaneously track latent belief states and estimate unknown cognitive bounds from passive observations. Experiments demonstrate the model's ability to generate intuitively plausible behaviors across different memory capacities and accurately recover ground-truth cognitive bounds in a T-maze navigation task.

## Method Summary
The authors propose a CR user model where agents make decisions based on bounded rationality arising from limited memory capacity, leading to belief biases rather than pure irrationality. They implement an online inference approach using nested particle filtering that tracks both the latent belief state of the agent and estimates the unknown cognitive bounds. The outer particle filter represents possible belief states while the inner filter estimates the cognitive parameters. This hierarchical structure allows simultaneous inference of what the agent believes and what their cognitive limitations are, enabling adaptive assistance strategies based on inferred memory constraints.

## Key Results
- CR model generates intuitively plausible behaviors across different memory capacities in T-maze navigation
- Inference method achieves near-zero error in recovering ground-truth cognitive bounds within 100 steps
- Adaptive AI assistance can distinguish between different types and timings of interventions based on inferred memory limitations

## Why This Works (Mechanism)
The framework works by explicitly modeling the causal relationship between bounded memory and belief formation. Rather than treating sub-optimal behavior as irrational, it captures how limited memory capacity constrains information processing, leading to biased belief states that are internally rational given those constraints. The nested particle filtering approach efficiently handles the dual inference problem of tracking both latent beliefs and cognitive parameters by maintaining separate filtering levels for each inference target.

## Foundational Learning
- **Computational Rationality**: Modeling agents as boundedly rational decision-makers optimizes within cognitive constraints; needed to distinguish belief bias from irrationality; check by comparing CR predictions against pure optimal control
- **Nested Particle Filtering**: Hierarchical filtering where outer particles track beliefs and inner particles estimate parameters; needed for simultaneous inference of beliefs and bounds; check by ablation study removing inner filter
- **Bounded Memory Models**: Cognitive models where memory limitations directly influence belief updating; needed to connect memory constraints to observable behavior; check by varying memory bounds in simulations
- **Belief Bias**: Systematic deviations in belief formation due to cognitive constraints; needed to explain seemingly irrational decisions; check by comparing belief trajectories with and without bias
- **Passive Observation Inference**: Learning cognitive parameters without active probing; needed for real-world deployment where active testing may be impractical; check by comparing with active learning baselines
- **Adaptive Assistance**: AI interventions tailored to inferred cognitive states; needed for practical human-AI collaboration; check by measuring task performance with different intervention strategies

## Architecture Onboarding

### Component Map
Environment -> Agent Model (with memory bounds) -> Nested Particle Filter (Outer: Belief Tracking, Inner: Bound Estimation) -> AI Assistant -> Environment

### Critical Path
Observation → Agent Model → Belief Update → Decision → Action → Observation (feedback loop)
Parallel: Particle Filter updates → Bound Estimation → Assistance Strategy Update

### Design Tradeoffs
The nested particle filter structure trades computational complexity for simultaneous inference accuracy. Alternative approaches like separate belief tracking and parameter estimation would be computationally simpler but lose the principled coupling between belief and bound inference. The choice of particle filter over variational methods prioritizes handling non-linear, non-Gaussian belief dynamics at the cost of higher computational load.

### Failure Signatures
- Poor bound recovery indicates either insufficient particles or highly non-stationary cognitive parameters
- Belief tracking divergence suggests model mismatch between assumed and actual memory dynamics
- Assistance strategy failure points to incorrect mapping from inferred bounds to intervention types

### First 3 Experiments
1. Test inference accuracy with varying numbers of particles to establish computational-accuracy tradeoff
2. Compare CR model predictions against human behavioral data from collaborative tasks
3. Evaluate transfer learning by pre-training bounds on one task and applying to a related task

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Scalability to complex, real-world scenarios with larger state spaces remains uncertain
- The model assumes bounded memory directly causes belief bias, potentially oversimplifying human cognition
- Does not address how cognitive bounds might change over time or adapt to different contexts

## Confidence
- High confidence in experimental results within the controlled T-maze setup
- Medium confidence in generalization to tasks with larger state spaces or more complex belief structures
- Medium confidence that the CR framework adequately captures all forms of sub-optimal behavior
- Low confidence in the model's ability to handle dynamic cognitive states or transfer across domains

## Next Checks
1. Test the inference method on tasks with higher dimensionality (e.g., grid worlds larger than 5x5) to evaluate scalability and computational efficiency
2. Validate the model's predictions against empirical human behavioral data from actual human-AI collaboration studies to assess ecological validity
3. Implement a longitudinal study where the AI agent continuously adapts its assistance strategy based on evolving inferences about the human partner's cognitive bounds across multiple interaction sessions