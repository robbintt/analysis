---
ver: rpa2
title: Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization
arxiv_id: '2507.00480'
source_url: https://arxiv.org/abs/2507.00480
tags:
- optimization
- constraints
- distribution
- diffusion
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing high-dimensional
  black-box functions under black-box constraints, which is prevalent in scientific
  and engineering domains. The authors propose a novel generative model-based framework
  that formulates candidate selection as posterior inference in the latent space of
  flow-based models.
---

# Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization

## Quick Facts
- arXiv ID: 2507.00480
- Source URL: https://arxiv.org/abs/2507.00480
- Reference count: 40
- Key outcome: Generative model-based framework for constrained black-box optimization using posterior inference in latent space

## Executive Summary
This paper addresses the challenge of optimizing high-dimensional black-box functions under black-box constraints, which is prevalent in scientific and engineering domains. The authors propose a novel generative model-based framework that formulates candidate selection as posterior inference in the latent space of flow-based models. Their key innovation is amortizing posterior sampling through an outsourced diffusion sampler, which effectively mitigates the mode collapse problem that plagues existing generative approaches. The method iterates between training flow-based models and surrogates to capture data distribution and constraints, then sampling candidates from the posterior distribution in latent space.

## Method Summary
The proposed framework addresses high-dimensional constrained black-box optimization by formulating candidate selection as posterior inference in latent space. The method alternates between two phases: training flow-based models and surrogates to capture the data distribution and constraints, then sampling candidates from the posterior distribution in latent space. The key innovation is amortizing posterior sampling through an outsourced diffusion sampler, which mitigates mode collapse issues common in generative approaches. The framework handles both explicit constraint functions and indicator constraints where only feasibility feedback is available, making it particularly useful for real-world applications where constraint evaluations are expensive or black-box.

## Key Results
- Superior performance on synthetic benchmarks (Rastrigin-200D, Ackley-200D, Rosenbrock-200D) compared to state-of-the-art baselines
- Excels in challenging indicator constraint setting where only feasibility feedback is available
- Robust performance across different initial dataset sizes and batch configurations
- Competitive computational efficiency compared to other generative methods
- Strong results on real-world tasks (Rover Planning 60D, Mopta 124D, Lasso DNA 180D)

## Why This Works (Mechanism)
The framework works by transforming the constrained optimization problem into a probabilistic inference task. Flow-based models learn the distribution of feasible solutions, while diffusion samplers amortize the expensive posterior inference process. This combination allows the method to explore the search space efficiently while maintaining diversity in candidate solutions, addressing the mode collapse problem that typically affects generative approaches to optimization.

## Foundational Learning

1. **Flow-based generative models**: Learn complex probability distributions by composing invertible transformations. Why needed: To model the distribution of feasible solutions in the search space. Quick check: Verify the model can accurately reconstruct training data and generate samples that match the empirical distribution.

2. **Diffusion probabilistic models**: Gradual noising and denoising process for sampling from complex distributions. Why needed: To amortize the posterior sampling process in latent space. Quick check: Ensure the pretrained diffusion sampler can generate diverse samples and converge to the target distribution.

3. **Amortized inference**: Sharing inference computation across multiple queries. Why needed: To make posterior sampling computationally tractable for large-scale problems. Quick check: Compare runtime performance against exact inference methods.

## Architecture Onboarding

**Component map**: Flow Model -> Surrogate Model -> Diffusion Sampler -> Posterior Sampler

**Critical path**: Data collection → Flow model training → Surrogate model training → Posterior sampling → Candidate evaluation

**Design tradeoffs**: The framework trades off between exploration (diverse sampling) and exploitation (focusing on promising regions). The choice of flow architecture affects expressiveness vs. computational cost, while the diffusion sampler's complexity impacts sampling quality vs. speed.

**Failure signatures**: 
- Mode collapse: Reduced diversity in generated candidates
- Poor constraint satisfaction: High proportion of infeasible samples
- Slow convergence: Stagnation in objective improvement
- Sensitivity to initialization: Performance varies significantly with initial dataset

**First experiments**:
1. Verify basic functionality on low-dimensional synthetic problems with known optima
2. Test constraint satisfaction rate on problems with explicit constraint functions
3. Compare sampling diversity against baseline generative methods

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, focusing instead on demonstrating the effectiveness of the proposed approach.

## Limitations
- Reliance on pretrained diffusion samplers may limit generalization to domains where such samplers are unavailable
- Limited analysis of sensitivity to flow architecture and diffusion sampler hyperparameters
- Unexplored behavior under complex constraint structures beyond simple indicator functions
- Theoretical guarantees for posterior inference in latent space are limited

## Confidence

**Performance claims on benchmark problems**: High - well-supported by extensive experiments across multiple domains

**Mode collapse mitigation**: Medium - demonstrated empirically but lacks formal theoretical guarantees

**Computational efficiency**: Medium - comparisons are provided but full training costs are not analyzed

**Scalability to extremely high dimensions (>200D)**: Low - only moderate-dimensional problems (60-200D) are tested

## Next Checks

1. Analyze sensitivity to flow architecture choices (e.g., comparing RealNVP, MAF, and Glow) and diffusion sampler hyperparameters

2. Test performance on problems with hierarchical or time-varying constraints beyond simple indicator functions

3. Conduct ablation studies to quantify the contribution of each component (flow model vs. diffusion sampler) to overall performance