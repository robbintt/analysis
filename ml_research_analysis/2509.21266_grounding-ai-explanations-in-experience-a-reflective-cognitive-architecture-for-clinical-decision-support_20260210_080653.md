---
ver: rpa2
title: 'Grounding AI Explanations in Experience: A Reflective Cognitive Architecture
  for Clinical Decision Support'
arxiv_id: '2509.21266'
source_url: https://arxiv.org/abs/2509.21266
tags:
- rules
- data
- prediction
- disease
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of developing explainable disease
  prediction models that balance high accuracy with clinically meaningful explanations.
  It proposes the Reflective Cognitive Architecture (RCA), a novel framework that
  uses Large Language Models (LLMs) to learn from data through iterative rule refinement
  and distribution-aware checks.
---

# Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support

## Quick Facts
- arXiv ID: 2509.21266
- Source URL: https://arxiv.org/abs/2509.21266
- Reference count: 40
- The paper proposes a novel framework using Large Language Models (LLMs) to learn from data through iterative rule refinement and distribution-aware checks, achieving state-of-the-art predictive performance with up to 40% relative improvement over baselines and generating clear, logical, evidence-based explanations.

## Executive Summary
This paper addresses the challenge of developing explainable disease prediction models that balance high accuracy with clinically meaningful explanations. The authors propose the Reflective Cognitive Architecture (RCA), a novel framework that uses Large Language Models (LLMs) to learn from data through iterative rule refinement and distribution-aware checks. RCA treats prediction errors as learning opportunities and grounds its reasoning in global statistical evidence. Evaluated on three datasets including a proprietary Catheter-Related Thrombosis (CRT) cohort and 22 baselines, RCA achieved state-of-the-art predictive performance with up to 40% relative improvement over baselines. More importantly, it generated explanations that were clear, logical, evidence-based, and balanced, maintaining efficacy even at scale.

## Method Summary
RCA is a 3-agent LLM loop architecture that transforms structured patient features into text narratives, then iteratively refines symbolic rules through prediction, reflection, and validation. The architecture initializes by computing global data distribution summaries from the training set and starting with an empty rule base. During training, the prediction agent ($M_{pred}$) uses current rules to predict and explain, collecting misclassified samples. When error batches reach capacity, the reflection agent ($M_{ref}$) analyzes these errors to propose new rules. The validation agent ($M_{chk}$) then audits proposed rules against global data statistics to ensure they are statistically grounded and logically coherent. This process repeats for multiple epochs until the rule base converges, after which the system freezes the rules for inference on new patients.

## Key Results
- RCA achieved up to 40% relative improvement in predictive performance over 22 baselines across three datasets
- Generated explanations scored high on Cognitive Load (CL), Logical Argumentation (LA), Evidence-based Medicine (EBM), and low on Cognitive Biasing (CB)
- Maintained explanation quality and prediction accuracy even at scale, demonstrating robustness to noise and generalization across datasets

## Why This Works (Mechanism)

### Mechanism 1: Iterative Rule Refinement (Self-Reflection)
Treating predictive accuracy as a reward signal allows the system to iteratively construct a logical framework (Rule Base) that improves over time. The system identifies misclassified samples ($S_{error}$) and feeds them to a Reflection LLM ($M_{ref}$), which analyzes the errors relative to current rules ($R_{k-1}$) and generates new or modified rules to cover these "hard cases," converting short-term error experience into long-term logical memory. The core assumption is that the LLM has sufficient reasoning capability to abstract generalizable medical logic from specific error instances without explicit gradient descent.

### Mechanism 2: Distribution-aware Rules Check (Statistical Grounding)
Validating generated rules against global data statistics prevents the model from hallucinating plausible but statistically invalid logic. A Check LLM ($M_{chk}$) reviews proposed rules against a summary of the data distribution ($D_{train}$), removing low-quality or overly specific rules and ensuring thresholds align with actual dataset quartiles. This grounds the reasoning in evidence rather than general medical knowledge. The core assumption is that a statistical summary of training data provides sufficient context for an LLM to detect logical or statistical inconsistencies in symbolic rules.

### Mechanism 3: Native Data Interaction (vs. Tool Abstraction)
Directly processing data as textual narratives forces a deeper "first-hand" understanding compared to using abstract tools. Structured tabular features are converted into unstructured text narratives (e.g., "Granulocyte-to-lymphocyte ratio is 4.88..."), which the LLMs process directly. This avoids the "summarized lens" of external tools and fosters deeper engagement with the data. The core assumption is that the cognitive load of reading text mimics human experiential learning better than executing procedural code, leading to better generalization.

## Foundational Learning

- **Concept: In-Context Learning & Reflection**
  - **Why needed here:** RCA does not fine-tune model weights. Instead, it relies entirely on the LLM's ability to learn from examples and feedback provided in the prompt (the "experience"). You must understand how context length and instruction following impact this "memory."
  - **Quick check question:** Can you explain how $M_{ref}$ uses the error batch $S_{error}$ to modify behavior without updating gradients?

- **Concept: Prompt Engineering for Reasoning (CoT/ToT)**
  - **Why needed here:** The effectiveness of $M_{pred}$, $M_{ref}$, and $M_{chk}$ depends on precise prompt templates that enforce structured output (rules, predictions, checks).
  - **Quick check question:** Identify the specific section in the Appendix (A.13) that details the prompt constraints for avoiding "unreasonable" medical rules.

- **Concept: Hallucination vs. Statistical Grounding**
  - **Why needed here:** The paper frames LLM hallucination as a failure of "statistical de-grounding." Understanding this helps in designing the $D_{train}$ summary to effectively "anchor" the model.
  - **Quick check question:** How does the "Distribution-aware Rules Check" specifically mitigate the risk of fluent but false explanations?

## Architecture Onboarding

- **Component map:** Patient Features -> Text Narrative -> $M_{pred}$ -> Prediction + Error Collection -> $M_{ref}$ -> Draft Rules -> $M_{chk}$ -> Validated Rules -> Rule Base
- **Critical path:**
  1. Initialization: Compute $D_{train}$ summaries. Initialize empty Rule Base ($R_0$).
  2. Training Loop: $M_{pred}$ iterates over training data, collects errors -> $S_{error}$. $M_{ref}$ takes ($S_{error}$ + old $R$) -> proposes new rules. $M_{chk}$ takes (new rules + $D_{train}$) -> validates/filters rules -> updates $R$.
  3. Inference: New patient -> Narrative -> $M_{pred}$ (with frozen $R_f$) -> Prediction + Explanation.
- **Design tradeoffs:** Running 3 LLMs iteratively is significantly slower and more costly than single-pass models but yields superior explanations. The system relies on LLM stability; bad rule generation can degrade performance until $M_{chk}$ cleans it up.
- **Failure signatures:** Rule Oscillation (accuracy fluctuates between epochs), Rule Explosion (rules grow indefinitely without performance gain), Statistical Hallucination (explanations cite non-existent thresholds).
- **First 3 experiments:** 1) Ablation Study: Run RCA with $M_{chk}$ disabled to verify increase in "statistically unsupported" rules. 2) Batch Capacity Analysis: Vary error batch capacity ($T$) to see if learning from single errors vs. batches affects rule quality. 3) Noise Robustness: Introduce synthetic noise and compare Rule Base degradation vs. standard CatBoost model.

## Open Questions the Paper Calls Out
- **Open Question 1:** Does relying exclusively on prediction errors ($S_{error}$) to drive rule refinement cause the model to ignore valid generalizations for cases it predicts correctly but for the wrong reasons ("lucky guesses")?
- **Open Question 2:** Is the computational overhead of the iterative, multi-LLM architecture feasible for real-time clinical deployment?
- **Open Question 3:** Can the "Data Narrative" transformation approach generalize to multimodal clinical data, such as medical images or free-text clinical notes?
- **Open Question 4:** Is the "high-quality explanation" evaluation generalizable beyond the specific cohort of 3 expert clinicians used in the study?

## Limitations
- Reliance on proprietary CRT dataset limits external validation, though public dataset results show consistent improvements
- Mechanism by which LLMs extract generalizable rules from error batches remains opaque and not directly observable
- Paper does not address computational efficiency, which is likely significant given three LLM calls per training sample

## Confidence
- **High Confidence:** Framework's core structure (iterative rule refinement + distribution-aware validation) is well-defined and reproducible with public datasets
- **Medium Confidence:** Claims of up to 40% relative improvement are supported by reported metrics but proprietary dataset prevents full independent verification
- **Medium Confidence:** Explanation quality metrics (CL, LA, EBM, CB) are evaluated by experts or GPT-4, providing reasonable but not ground-truth validation

## Next Checks
1. **Ablation Study:** Disable $M_{chk}$ to empirically verify that distribution-aware validation prevents statistically invalid rules and improves explanation coherence
2. **Rule Complexity Analysis:** Track rule count and specificity over training epochs to detect rule explosion or oscillation patterns
3. **Cross-Dataset Generalization:** Apply RCA to additional public medical datasets (e.g., MIMIC-III subsets) to test whether the architecture maintains performance and explanation quality beyond the three reported datasets