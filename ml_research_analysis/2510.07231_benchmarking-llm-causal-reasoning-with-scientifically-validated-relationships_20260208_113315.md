---
ver: rpa2
title: Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships
arxiv_id: '2510.07231'
source_url: https://arxiv.org/abs/2510.07231
tags:
- causal
- reasoning
- type
- benchmark
- economics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a benchmark for evaluating LLMs' causal reasoning
  abilities using scientifically validated causal relationships from economics and
  finance journals. The benchmark includes 29,972 questions across five task types,
  covering diverse domains like health, environment, technology, law, and culture.
---

# Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships

## Quick Facts
- **arXiv ID**: 2510.07231
- **Source URL**: https://arxiv.org/abs/2510.07231
- **Reference count**: 30
- **Primary result**: Best model achieved 60.6% accuracy on causal reasoning benchmark

## Executive Summary
This paper introduces a benchmark for evaluating LLMs' causal reasoning abilities using scientifically validated causal relationships from economics and finance journals. The benchmark includes 29,972 questions across five task types, covering diverse domains like health, environment, technology, law, and culture. When evaluated on eight state-of-the-art LLMs, the best model achieved only 60.6% accuracy, revealing substantial limitations in current models' causal reasoning capabilities. Notably, model scale does not consistently translate to better performance, and even advanced reasoning models struggled with fundamental causal relationship identification. These findings highlight a critical gap between current LLM capabilities and the demands of reliable causal reasoning in high-stakes applications.

## Method Summary
The benchmark construction pipeline involves extracting causal triplets from 14,977 papers across eight top-tier economics and finance journals (2000-2025) using GPT-5-mini with five independent extraction attempts per paper. Triplets appearing in at least 4/5 extractions with cosine similarity ≥0.9 are retained. The dataset was filtered to remove questions easily answered by small models (Llama 3.2 3B, Qwen 3 4B, Ministral 3B) and includes four task types covering cause-effect relationships, confounding effects, and joint causal relationships. The final benchmark contains 29,972 questions evaluated using accuracy and Macro-F1 metrics.

## Key Results
- Best model (Qwen3-32B) achieved 60.6% accuracy, with GPT-5 performing surprisingly poorly at 30.4%
- Performance varied significantly by task type, with Type 3 (manyX-Y) showing the lowest performance
- Model scale does not consistently correlate with causal reasoning performance
- Closed-book setting revealed substantial limitations in models' ability to suppress internal priors

## Why This Works (Mechanism)

### Mechanism 1: Consensus-based Triplet Extraction
Utilizing a high-agreement threshold across multiple LLM extraction attempts isolates scientifically grounded causal triplets from unstructured text better than single-pass extraction. The pipeline employs GPT-5-mini to extract causal relations five times per paper, retaining only those triplets appearing in ≥4/5 attempts with cosine similarity ≥0.9. This filters out stochastic hallucinations and low-confidence assertions.

### Mechanism 2: Difficulty Filtering via Small Model Elimination
Removing questions easily answered by small models (3B-4B parameters) creates a benchmark that specifically targets reasoning deficits rather than simple pattern matching. The authors filter the generated dataset using Llama 3.2 3B, Qwen 3 4B, and Ministral 3B, discarding any question answered correctly by all three.

### Mechanism 3: Constraint-induced Performance Collapse
Advanced LLMs rely heavily on pre-training priors; when explicitly instructed to use only provided context ("closed-book"), performance degrades significantly compared to "open-book" settings. The benchmark forces models to validate causal claims based strictly on textual evidence, revealing that models struggle to suppress strong internal priors in favor of local context.

## Foundational Learning

- **Concept: Causal Identification (IV, Diff-in-Diff, RDD)**
  - Why needed here: The ground truth of the benchmark relies on papers using these econometric methods to prove causality, not just correlation
  - Quick check: Can you explain why a simple regression is insufficient to claim "X causes Y" in an observational study?

- **Concept: Semantic Embedding & Clustering**
  - Why needed here: The extraction pipeline groups variable mentions using vector similarity (cosine ≥0.9) to determine entity identity across different text spans
  - Quick check: If "GDP growth" and "economic expansion" have a cosine similarity of 0.85, would they be merged as the same entity under the paper's defined threshold?

- **Concept: Closed- vs. Open-Book Evaluation**
  - Why needed here: Understanding the ablation study requires distinguishing between a model reasoning from a provided prompt (closed) vs. a model accessing memorized training data (open)
  - Quick check: Why might a model perform worse on a reasoning task when allowed to use its internal knowledge base compared to strict context adherence?

## Architecture Onboarding

- **Component map**: OpenAlex API (14,977 papers) → Extractor: GPT-5-mini (5x extraction) → Validator: Consensus Filter + Human Review (ICC 0.57) → Generator: Question Creation (4 Types) → Filter: Small Model Difficulty Screening → Evaluator: 8 State-of-the-Art LLMs

- **Critical path**: The Consensus Filter (Section 3.1.2) is the most fragile component. It relies on LLM self-consistency, and if the extraction model drifts or hallucinates consistently, the ground truth is corrupted before human review.

- **Design tradeoffs**: Real vs. Synthetic Data - chose real academic text for ecological validity, sacrificing perfect control over variable definitions. Recall vs. Precision in Extraction - consensus mechanism prioritizes precision, potentially missing valid but subtly stated causal claims.

- **Failure signatures**: Inverse Scaling - GPT-5 (30.4%) performing worse than Qwen3-32B (60.6%) indicates "reasoning" models may over-analyze simple binary relations. Low Human Agreement - ICC of 0.57 suggests ground truth may contain noise.

- **First 3 experiments**:
  1. Verify Extraction Validity: Sample 20 relations from "discarded" set and 20 from "kept" set to see if consensus threshold correlates with human judgment of causality
  2. Re-run Ablation on Type 3: Test if low performance is due to context window limitations or failure to aggregate multiple causes
  3. Perturbation Test: Invert relation directions to measure if models detect contradictions or simply agree with text (sycophancy)

## Open Questions the Paper Calls Out

### Open Question 1
Do LLMs exhibit comparable causal reasoning capabilities when evaluated on scientifically validated relationships from natural sciences and engineering, rather than social sciences? The authors state future work should expand to incorporate causal relationships from broader scientific domains to enable more comprehensive evaluation.

### Open Question 2
What specific architectural or training factors cause advanced reasoning models (like GPT-5) to underperform smaller non-reasoning models in causal identification tasks? The paper identifies this counter-intuitive performance degradation but does not isolate the underlying mechanisms.

### Open Question 3
How can benchmarks effectively distinguish between a model's retrieval of pre-trained causal facts versus its ability to perform de novo causal inference? The authors note they cannot completely rule out the possibility that some causal relationships may have been encountered during LLM pretraining.

## Limitations

- The extraction pipeline's reliability is uncertain due to low human inter-annotator agreement (ICC = 0.57) below accepted thresholds
- The benchmark exclusively uses economics and finance journals, limiting generalizability to other scientific domains
- The filtering of "easy" questions assumes small models fail due to reasoning limitations rather than knowledge gaps, which may not hold universally

## Confidence

- **High Confidence**: Current state-of-the-art LLMs struggle with causal reasoning tasks (best accuracy: 60.58%)
- **Medium Confidence**: Model scale does not correlate with causal reasoning performance across the evaluated sample
- **Low Confidence**: Interpretation that closed-book prompting reveals LLMs' inability to suppress internal priors

## Next Checks

1. **Ground Truth Validation**: Sample 50 relations from final dataset and manually verify their presence and accuracy in source papers, comparing agreement rates between human reviewers and extraction consensus mechanism

2. **Task Type Robustness**: Re-run benchmark with inverted relation directions to test whether models detect contradictions or simply agree with textual assertions (sycophancy)

3. **Extraction Bias Analysis**: Examine discarded relations to determine if systematic patterns exist that might indicate extraction model biases affecting benchmark composition