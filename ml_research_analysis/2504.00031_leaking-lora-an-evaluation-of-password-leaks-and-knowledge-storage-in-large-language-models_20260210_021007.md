---
ver: rpa2
title: 'Leaking LoRa: An Evaluation of Password Leaks and Knowledge Storage in Large
  Language Models'
arxiv_id: '2504.00031'
source_url: https://arxiv.org/abs/2504.00031
tags:
- passwords
- information
- password
- layer
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that fine-tuning large language models
  with LoRA on data containing passwords can result in successful password recovery
  through simple prompt-based retrieval. Using a dataset combining customer support
  conversations with RockYou passwords, the authors fine-tuned Facebook's OPT-1.3b
  model and successfully recovered 37 out of 200 injected passwords.
---

# Leaking LoRa: An Evaluation of Password Leaks and Knowledge Storage in Large Language Models

## Quick Facts
- **arXiv ID**: 2504.00031
- **Source URL**: https://arxiv.org/abs/2504.00031
- **Reference count**: 21
- **Primary result**: LoRA fine-tuning on password-containing data enables prompt-based password recovery from LLMs

## Executive Summary
This study demonstrates that fine-tuning large language models with LoRA on data containing passwords can result in successful password recovery through simple prompt-based retrieval. Using a dataset combining customer support conversations with RockYou passwords, the authors fine-tuned Facebook's OPT-1.3b model and successfully recovered 37 out of 200 injected passwords. Causal tracing revealed that password information is primarily stored in the fully connected layer of the decoder (layer 160). The authors then applied Rank One Model Editing (ROME) to this layer, successfully removing all recoverable passwords while maintaining 32% accuracy on WikiText. This work highlights the security risks of fine-tuning LLMs on user data containing sensitive information and provides a mitigation technique through targeted model editing.

## Method Summary
The authors created a synthetic dataset by combining customer support conversations with 200 RockYou passwords, fine-tuning the OPT-1.3b model using LoRA with rank 8. They then attempted password recovery through prompt engineering by asking the model to generate customer passwords. Causal tracing techniques were employed to identify where password information was stored in the model architecture. The ROME method was applied to the identified layer to remove password information while attempting to preserve general language capabilities. The effectiveness of both the attack and mitigation was evaluated through controlled experiments measuring password recovery rates and language model performance.

## Key Results
- Successfully recovered 37 out of 200 injected passwords (18.5% recovery rate) through prompt-based retrieval
- Identified layer 160 (decoder fully connected layer) as the primary storage location for password information via causal tracing
- Applied ROME to layer 160, completely eliminating recoverable passwords while maintaining 32% accuracy on WikiText benchmark
- Demonstrated that LoRA fine-tuning can embed sensitive information in a way that remains retrievable through simple prompting

## Why This Works (Mechanism)
LoRA fine-tuning adapts large language models to specific domains by modifying low-rank representations in the model's attention mechanisms. When training data contains sensitive information like passwords, the low-rank adaptations can store this information in specific layers, making it retrievable through targeted prompting. The fine-tuning process creates associations between domain-specific contexts (like customer support conversations) and the sensitive data, enabling extraction when similar contexts are provided as prompts. The ROME technique exploits the fact that sensitive information is stored in specific weight matrices, allowing for targeted removal without requiring full model retraining.

## Foundational Learning
- **LoRA (Low-Rank Adaptation)**: A parameter-efficient fine-tuning method that adds low-rank matrices to existing model weights; needed to understand how efficient fine-tuning can still store sensitive information; quick check: LoRA reduces trainable parameters by ~50-90% compared to full fine-tuning
- **Causal Tracing**: A technique to identify which model components contribute to specific outputs; needed to locate where password information is stored; quick check: involves iteratively modifying layer activations to measure impact on target outputs
- **Rank One Model Editing (ROME)**: A method for editing specific knowledge in LLMs by modifying rank-one weight updates; needed to understand the mitigation approach; quick check: modifies model weights to change factual associations without full retraining
- **Fine-tuning Security Risks**: The potential for sensitive data leakage during model adaptation; needed to contextualize the broader implications; quick check: fine-tuning on user data can embed PII, passwords, and other sensitive information
- **Knowledge Storage in LLMs**: How LLMs encode and retrieve factual information; needed to understand password storage mechanisms; quick check: information is distributed across transformer layers with varying concentrations
- **Prompt Engineering for Extraction**: Techniques to elicit specific information from LLMs through carefully crafted prompts; needed to understand the attack vector; quick check: context-specific prompts can trigger retrieval of fine-tuned information

## Architecture Onboarding
- **Component Map**: Input tokens -> Embedding layer -> Transformer blocks (12 layers) -> LoRA adapters -> LayerNorm -> Decoder fully connected layer (layer 160) -> Output logits
- **Critical Path**: Token embedding → transformer layers → LoRA-modified attention weights → final layer activations → output generation
- **Design Tradeoffs**: LoRA offers parameter efficiency but may create concentrated storage of sensitive information; causal tracing enables targeted mitigation but requires architectural understanding; ROME provides efficient editing but may impact general capabilities
- **Failure Signatures**: Successful password recovery indicates improper handling of sensitive data during fine-tuning; inability to remove passwords via ROME suggests distributed storage across multiple layers; performance degradation indicates over-aggressive editing
- **First Experiments**:
  1. Test password recovery on models fine-tuned with different LoRA ranks (1, 4, 8, 16) to assess rank sensitivity
  2. Apply causal tracing to identify password storage locations in different transformer architectures (GPT, OPT, LLaMA)
  3. Evaluate ROME effectiveness on removing other types of sensitive information (credit card numbers, SSNs)

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on OPT-1.3b model (1.3B parameters) may not generalize to larger, more capable LLMs
- Dataset composition (RockYou + customer support) represents specific leakage scenario that may not reflect all real-world training scenarios
- 18.5% success rate indicates password recovery is possible but not guaranteed across all fine-tuning scenarios
- ROME mitigation achieved only 32% WikiText accuracy, suggesting potential degradation in general language capabilities
- Study does not address whether extracted passwords could be reconstructed through attack vectors beyond simple prompt-based retrieval

## Confidence
- **High Confidence**: Experimental demonstration that LoRA fine-tuning on password-containing data enables password recovery through prompt engineering
- **Medium Confidence**: Identification of layer 160 as primary storage location for password information via causal tracing
- **Medium Confidence**: Effectiveness of ROME for password removal demonstrated, but trade-offs with language model performance remain uncertain

## Next Checks
1. Replicate the experiment across multiple model sizes (including larger models like LLaMA-7B and LLaMA-13B) and different LoRA rank configurations to assess scalability of the password leakage phenomenon
2. Test the ROME mitigation technique on datasets containing different types of sensitive information (e.g., credit card numbers, personal identifiers) to evaluate its generalizability beyond password removal
3. Conduct a comprehensive evaluation of the trade-off between password removal effectiveness and language model performance by testing the edited models across multiple benchmarks (not just WikiText) to assess functional degradation