---
ver: rpa2
title: 'PERCY: Personal Emotional Robotic Conversational System'
arxiv_id: '2503.16473'
source_url: https://arxiv.org/abs/2503.16473
tags:
- percy
- emotion
- emotional
- gpt-4
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PERCY, a multimodal conversational system that
  integrates Large Language Models with real-time emotion recognition to enable socially
  aware human-robot interactions. PERCY combines a GPT-4 reasoning engine with visual
  emotion recognition using MobileNetV2 and textual sentiment analysis via NLTK, enabling
  open-domain dialogues that adapt to users' emotional states.
---

# PERCY: Personal Emotional Robotic Conversational System

## Quick Facts
- arXiv ID: 2503.16473
- Source URL: https://arxiv.org/abs/2503.16473
- Reference count: 40
- Achieves 92% emotion recognition accuracy with 1.7s end-to-end latency on ARI robot platform

## Executive Summary
This paper presents PERCY, a multimodal conversational system that integrates Large Language Models with real-time emotion recognition to enable socially aware human-robot interactions. PERCY combines a GPT-4 reasoning engine with visual emotion recognition using MobileNetV2 and textual sentiment analysis via NLTK, enabling open-domain dialogues that adapt to users' emotional states. The system achieves 92% emotion recognition accuracy with 1.7s end-to-end latency on the ARI robot platform. Automated evaluations show strong performance across dialogue quality metrics (BERT: 0.32, BLEU: 0.51, MAUVE: 0.98, Perplexity: 24.53), while human evaluations demonstrate PERCY's superior personalization (4.7/5) compared to GPT-4 (3.7/5) and EmpGPT-3 (3.5/5), with comparable naturalness to GPT-4.

## Method Summary
PERCY uses Whisper large-v2 for speech-to-text transcription with 2.73% WER, MobileNetV2+SSD for facial emotion recognition with multi-task loss, and NLTK-VADER for textual sentiment analysis. The system fuses visual and textual emotion signals before conditioning GPT-4's response generation via prompt engineering that includes user profile, emotion state, and dialogue history. The architecture runs on ROS middleware coordinating perception modules, LLM inference, and robot actuators (ARI's action package) with parallel processing pipelines that overlap STT processing with LLM inference to achieve 1.7s latency. The MERCI Dataset with 30 participants provides evaluation data for RGB-D sensor inputs and emotion annotations across 7 emotion classes.

## Key Results
- Achieves 92% emotion recognition accuracy with 1.7s end-to-end latency
- Human evaluations show superior personalization (4.7/5) compared to GPT-4 (3.7/5) and EmpGPT-3 (3.5/5)
- Strong automated dialogue quality metrics: BERT 0.32, BLEU 0.51, MAUVE 0.98, Perplexity 24.53

## Why This Works (Mechanism)

### Mechanism 1
Multimodal emotion fusion improves response personalization by combining visual emotion signals from MobileNetV2-SSD (facial expressions) with textual sentiment from NLTK-VADER, creating a fused emotional state Et that conditions GPT-4's response generation. Core assumption: Facial expressions and speech sentiment reliably correlate with internal emotional states. Evidence: System achieves 92% emotion recognition accuracy and 4.7/5 personalization score. Break condition: If visual occlusion or speech ambiguity corrupt either signal, fusion may degrade personalization.

### Mechanism 2
Explicit emotion-state injection into LLM prompts yields higher perceived personalization than implicit textual inference alone. The GPT-4 prompt encodes persona specification, fused emotion Et, and dialogue history, forcing the model to generate emotion-aligned responses. Core assumption: Explicit emotion labels in prompts produce more personalized outputs. Evidence: PERCY scores 4.7/5 for personalization versus 3.7/5 for GPT-4. Break condition: If emotion recognition misclassifies states, injected emotion will misalign responses.

### Mechanism 3
Parallel processing pipelines reduce end-to-end latency below typical LLM-robot integration thresholds. ASR processing overlaps with emotion recognition; both feed into GPT-4 inference while the ROS behavior planner pre-computes non-verbal mappings, achieving 1.7s latency vs. typical ~3s LLM inference. Core assumption: Overlapping STT, emotion recognition, and LLM inference does not significantly degrade accuracy. Evidence: Achieves 1.7s end-to-end latency. Break condition: If any pipeline stage exceeds its time budget, synchronization breaks and latency increases.

## Foundational Learning

- **ROS (Robot Operating System) middleware**: PERCY runs on ROS to coordinate perception modules, LLM inference, and robot actuators with message passing. Quick check: Can you explain how ROS nodes publish/subscribe to coordinate the emotion recognition module with the behavior planner?

- **Late vs. early multimodal fusion**: PERCY uses early fusion (combining emotion signals before LLM generation) vs. EmpGPT-3's late fusion (post-generation emotion rules). Quick check: What is the difference between fusing modalities before vs. after the LLM generates a response?

- **Prompt engineering for affective alignment**: Equation 3 encodes user profile, emotion state, and history into the GPT-4 prompt structure. Quick check: How would you structure a prompt that injects an emotional state ("user is sad") while maintaining conversation coherence?

## Architecture Onboarding

- **Component map**: Whisper ASR -> MobileNetV2-SSD emotion recognition -> NLTK-VADER sentiment analysis -> Emotion fusion -> GPT-4 inference -> TTS + gesture dispatch

- **Critical path**: Speech input → Whisper → (parallel: NLTK sentiment + MobileNetV2 emotion) → Emotion fusion → GPT-4 inference → TTS + gesture dispatch → 1.7s target

- **Design tradeoffs**: MobileNetV2 chosen over YOLOv8 for accuracy (92% vs 91.3%) with similar computational cost; VADER lexicon faster than transformer-based sentiment but less context-aware; GPT-4 API adds latency but avoids local inference hardware requirements

- **Failure signatures**: Emotion misclassification → inappropriate gesture/response mismatch; Network latency to GPT-4 API → exceeds 1.7s target; Visual occlusion → facial emotion pipeline returns low-confidence scores

- **First 3 experiments**:
  1. **Latency breakdown**: Instrument each pipeline stage (ASR, emotion, LLM, TTS) to identify bottlenecks; verify 1.7s claim
  2. **Fusion ablation**: Run PERCY with only visual emotion, only textual sentiment, and both; measure personalization score delta
  3. **Error injection**: Simulate emotion misclassification (swap happy↔sad labels) and evaluate response appropriateness via human rating

## Open Questions the Paper Calls Out

- **Cross-cultural adaptation**: How can the system be adapted to handle variations in emotional expression across different cultures? The current MERCI dataset and evaluation protocol do not account for cultural variability in emotional norms.

- **Multi-robot empathy coordination**: How can affective states be effectively coordinated among multiple robots interacting with the same user? PERCY is currently architected as a single-agent system without protocols for sharing or synchronizing empathy models.

- **Longitudinal engagement**: Does the personalization mechanism effectively sustain engagement during longitudinal, multi-session interactions? The human evaluation relies on a single-session protocol, making it unclear if "dynamic user profiles" are robust enough to maintain personalization once novelty fades.

## Limitations

- Fine-tuning transparency gap: Claims "fine-tuned GPT-4 reasoning engine" but provides no specification of training data, hyperparameters, or procedure
- Fusion mechanism underspecification: Exact algorithm for combining MobileNetV2 visual scores with NLTK-VADER textual sentiment is not detailed
- Single robot platform constraint: Evaluation exclusively on SoftBank Robotics ARI platform limits generalizability

## Confidence

**High confidence** in the multimodal fusion concept and basic implementation, supported by 92% emotion recognition accuracy and parallel processing achieving 1.7s latency.

**Medium confidence** in claimed personalization improvements (4.7/5 vs 3.7/5 for GPT-4). While human evaluations show strong results, lack of fine-tuning details and single-platform testing introduces uncertainty about replicability.

**Low confidence** in exact emotion fusion algorithm and user profile construction. These critical components lack sufficient specification for faithful reproduction.

## Next Checks

1. **Latency decomposition audit**: Instrument each pipeline stage (ASR, emotion recognition, LLM inference, TTS) to verify the 1.7s end-to-end claim and identify bottlenecks

2. **Fusion ablation study**: Systematically compare PERCY's full multimodal fusion against variants using only visual emotion, only textual sentiment, and simple concatenation

3. **Cross-platform generalization test**: Deploy PERCY on a different robot platform (e.g., Pepper or mobile manipulator) to assess whether 4.7/5 personalization score replicates across different actuation capabilities and processing constraints