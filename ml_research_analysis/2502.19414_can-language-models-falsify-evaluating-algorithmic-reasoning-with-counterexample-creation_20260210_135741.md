---
ver: rpa2
title: Can Language Models Falsify? Evaluating Algorithmic Reasoning with Counterexample
  Creation
arxiv_id: '2502.19414'
source_url: https://arxiv.org/abs/2502.19414
tags:
- code
- incorrect
- input
- problem
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REFUTE, a novel benchmark that evaluates
  language models' ability to falsify incorrect solutions to algorithmic problems,
  rather than just generate correct ones. The benchmark dynamically sources recent
  problems and incorrect submissions from programming competitions, requiring models
  to produce counterexamples that make buggy code fail while satisfying input constraints.
---

# Can Language Models Falsify? Evaluating Algorithmic Reasoning with Counterexample Creation

## Quick Facts
- **arXiv ID**: 2502.19414
- **Source URL**: https://arxiv.org/abs/2502.19414
- **Reference count**: 40
- **Primary result**: Top reasoning models can falsify incorrect algorithmic solutions for <9% of cases, despite solving up to 48% from scratch

## Executive Summary
This paper introduces REFUTE, a novel benchmark that evaluates language models' ability to falsify incorrect algorithmic solutions rather than just generate correct ones. The benchmark dynamically sources recent programming competition problems and requires models to produce counterexamples that make buggy code fail while satisfying input constraints. Experiments show that even top reasoning models like OpenAI o3-mini achieve fewer than 9% success rates on falsification, despite being able to solve up to 48% of these problems from scratch. This demonstrates that verification and falsification can be harder than generation, highlighting fundamental limitations in models' reasoning and self-improvement capabilities.

## Method Summary
REFUTE sources 324 samples from recent Codeforces problems (Jan 2024–Jan 2025), pairing each with an incorrect submission and ground-truth correct solution. Models must generate counterexample inputs that satisfy problem constraints while causing buggy code to fail. Verification uses automated code execution comparing outputs between incorrect and correct solutions. The benchmark filters out trivial samples where random search without code reasoning can find counterexamples, ensuring the task requires genuine understanding of buggy logic. Three evaluation strategies are tested: zero/few-shot prompting, agentic code execution with ReAct scaffolding, and randomized search baselines.

## Key Results
- Top models achieve <9% counterexample success rate on incorrect solutions
- Models can solve up to 48% of problems from scratch, showing falsification is harder than generation
- Code execution feedback improves performance but doesn't close the gap
- Success rates show no clear correlation with problem difficulty ratings
- Sycophantic acceptance of buggy logic patterns remains a major failure mode

## Why This Works (Mechanism)

### Mechanism 1: Inverse Benchmark Design for Testing Falsification
Claim: Evaluating the inverse task (falsifying incorrect solutions) reveals capability gaps not captured by solution-generation benchmarks. By requiring models to produce counterexamples that satisfy input constraints while causing incorrect code to fail, the benchmark exposes whether models understand solution correctness deeply enough to detect subtle reasoning flaws. Core assumption: Falsification requires different reasoning patterns than generation—specifically, reasoning about edge cases, boundary conditions, and logical gaps in proposed solutions. Evidence: "Our analysis finds that the best reasoning agents... can create counterexamples for only <9% of incorrect solutions in REFUTE, even though ratings indicate its ability to solve up to 48% of these problems from scratch."

### Mechanism 2: Automated Counterexample Verification via Code Execution
Claim: Counterexamples can be objectively validated through differential execution against held-out correct solutions. The benchmark provides an incorrect solution and ground-truth correct solution. A validator checks: (1) the generated input satisfies problem constraints, (2) outputs differ between incorrect and correct code. This eliminates the need for human evaluation. Core assumption: A correct reference solution exists and can be executed deterministically within reasonable time/memory bounds. Evidence: "A validator script verifies whether x* satisfies the input constraints H. Then, the claim can be checked by comparing the output of A to a ground-truth solution (A*), i.e., A(x*) ≠ A*(x*)."

### Mechanism 3: Non-Trivial Sample Filtering via Search Resistance
Claim: Filtering out samples where random search finds counterexamples ensures the benchmark tests genuine reasoning. Gemini 2.0 Flash Thinking generates random test-case generators (without seeing buggy code) and attempts to find failures within 1 minute. Samples where this succeeds are removed, ensuring counterexamples require reasoning about the incorrect code's logic. Core assumption: Models that reason about buggy code will outperform random search on remaining samples. Evidence: "We wish to ensure that randomly generating test cases without reasoning about the incorrect code is not enough to find a counterexample... This is true for 58 (14%) of the 403 problems. We filter these incorrect submissions."

## Foundational Learning

- **Generator-Verifier Gap**: Why needed here: The paper's core thesis depends on understanding that generating solutions and verifying/falsifying them are distinct capabilities with different scaling properties. Quick check: Can you explain why a model might correctly solve a problem 48% of the time but only falsify incorrect solutions 9% of the time on the same problem set?

- **Counterexample-Guided Debugging**: Why needed here: The ReAct agent scaffold and code-execution feedback mechanisms assume models can use counterexamples iteratively to narrow failure cases. Quick check: Given a buggy sorting algorithm that fails on already-sorted input, what execution traces would help identify this vs. randomly testing inputs?

- **Contamination and Temporal Leakage**: Why needed here: REFUTE uses recent (2024-2025) Codeforces problems and dynamically updates to prevent training-data leakage, requiring understanding of how benchmarks become invalidated. Quick check: Why does the paper prioritize "hacked" submissions (post-contest counterexamples found by humans) over random wrong answers?

## Architecture Onboarding

- **Component map**: Codeforces scraper → problem filter (remove interactive/multi-output, rating <1200) → incorrect submission scorer → triviality filter (search resistance + bait detection) → 324 final samples → evaluation engine → model interface → success/failure
- **Critical path**: Incorrect solution + problem statement → model generates counterexample code → validator checks constraints → differential execution → success/failure
- **Design tradeoffs**: Expressivity vs. Verifiability (allowing arbitrary code generators vs. hardcoded inputs), filtering threshold (1-minute search removes trivial samples but may eliminate solvable problems), language coverage (317/324 samples in C++; Python underrepresented)
- **Failure signatures**: Models generating invalid counterexamples where incorrect code still produces correct output (35% of RandSearch failures), models generating correct brute-force solutions but inefficient ones that timeout, sycophantic acceptance of buggy logic patterns
- **First 3 experiments**: 1) Establish baseline: Run zero-shot prompting on 50 random samples to measure raw counterexample generation rate before implementing agent scaffolds, 2) Ablate code-execution feedback: Compare ReAct agent performance with vs. without the 10-step code execution tool to quantify interaction value, 3) Test oracle ceiling: Provide correct solutions to models and measure counterexample success—if still <15%, the bottleneck is reasoning about code differences, not solution knowledge

## Open Questions the Paper Calls Out

- **Can LMs generate verifiable counterexamples for claims in natural language domains lacking formal execution environments?** Basis: "Designing evaluations that test a model's ability to propose counterexamples based solely on natural language claims presents an exciting direction for future research." Why unresolved: REFUTE relies on code execution for automatic verification; natural language claims lack such clear truth semantics or oracles. What evidence would resolve it: A benchmark for natural language falsification with a reliable verification protocol.

- **Does hybridizing LMs with formal verification tools (e.g., SMT solvers) improve success rates on the REFUTE benchmark?** Basis: "methods that integrate formal tools such as SMT solvers to leverage their complementary strengths." Why unresolved: Current experiments tested standalone LMs or simple agents; the synergy with symbolic solvers in this specific falsification context is untested. What evidence would resolve it: Experimental results showing an LM-SMT hybrid architecture outperforming current baselines on REFUTE.

- **Are there specific attributes of a problem or incorrect solution that predict the difficulty of counterexample generation?** Basis: "better understanding of what attributes contribute to its difficulty is an important direction for further investigation." Why unresolved: Section 5.3 shows no clear trends between success and problem difficulty or author expertise. What evidence would resolve it: Identification of features (e.g., error types) that correlate with model performance.

## Limitations

- **Sampling bias and representativeness**: REFUTE draws from competitive programming problems rated ≥1200, excluding problems requiring multi-modal outputs, interactive protocols, or subjective grading criteria, limiting generalizability to broader domains.
- **Code execution scalability**: Differential execution assumes deterministic, polynomial-time solutions; for NP-hard problems or those requiring extensive search, the 30-second timeout may create false negatives where valid counterexamples exist but cannot be verified within constraints.
- **Model capability ceiling**: The <9% success rate could reflect benchmark difficulty rather than fundamental falsification limitations; search-resistance filtering may have eliminated problems where current models could succeed, creating an artificially challenging benchmark.

## Confidence

- **High confidence**: The inverse benchmark design revealing gaps in falsification vs. generation capabilities is well-supported by experimental results; the code execution verification mechanism is sound and reproducible.
- **Medium confidence**: Claims about falsification being fundamentally harder than generation require stronger evidence; current results could reflect benchmark design choices rather than inherent reasoning limitations.
- **Low confidence**: Generalization to non-competitive programming domains (scientific reasoning, commonsense falsification) is speculative without empirical validation in those areas.

## Next Checks

1. **Ablation study on filtering thresholds**: Systematically vary the search-resistance filtering parameters to determine how benchmark difficulty affects falsification success rates, clarifying whether the <9% ceiling reflects fundamental limitations or filtering artifacts.

2. **Cross-domain transfer validation**: Apply REFUTE-style evaluation to mathematical reasoning benchmarks or commonsense QA datasets where ground-truth solutions exist, comparing falsification vs. generation performance ratios across domains.

3. **Model capability scaling analysis**: Test whether larger models or chain-of-thought prompting significantly improve falsification rates beyond the current best-performing o3-mini; if success rates scale sublinearly with model size, this would support the hypothesis that falsification requires distinct reasoning capabilities.