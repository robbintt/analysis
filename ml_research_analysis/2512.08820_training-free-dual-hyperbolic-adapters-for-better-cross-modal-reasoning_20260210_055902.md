---
ver: rpa2
title: Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning
arxiv_id: '2512.08820'
source_url: https://arxiv.org/abs/2512.08820
tags:
- hyperbolic
- negative
- space
- learning
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Training-free Dual Hyperbolic Adapters (T-DHA),\
  \ a novel approach for efficient domain adaptation in vision-language models without\
  \ additional training. The method leverages hyperbolic geometry to better capture\
  \ hierarchical semantic relationships inherent in visual classification tasks, mapping\
  \ features into the Poincar\xE9 ball space and computing similarities using hyperbolic\
  \ distance rather than Euclidean metrics."
---

# Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning

## Quick Facts
- **arXiv ID**: 2512.08820
- **Source URL**: https://arxiv.org/abs/2512.08820
- **Reference count**: 40
- **Primary result**: Training-free dual hyperbolic adapters (T-DHA) achieve up to 2.3% higher accuracy than state-of-the-art training-free few-shot methods on 11 benchmark datasets

## Executive Summary
This paper introduces Training-free Dual Hyperbolic Adapters (T-DHA), a novel approach for efficient domain adaptation in vision-language models without additional training. The method leverages hyperbolic geometry to better capture hierarchical semantic relationships inherent in visual classification tasks, mapping features into the Poincaré ball space and computing similarities using hyperbolic distance rather than Euclidean metrics. It also incorporates negative learning to enhance discriminative power by explicitly considering what an image is not, using both negative class prototypes and negated textual prompts. Extensive experiments demonstrate that T-DHA significantly outperforms state-of-the-art training-free few-shot learning methods across 11 benchmark datasets, achieving up to 2.3% higher accuracy in 16-shot settings. The method also shows superior domain generalization performance on ImageNetV2 and ImageNet-Sketch, improving accuracy by up to 2.51% compared to existing approaches.

## Method Summary
T-DHA operates by first extracting CLIP visual features from support and query images, then computing class prototypes through averaging and mapping to hyperbolic space via the exponential map. For each class, negative prototypes are constructed by averaging features from other classes. The method computes four prediction streams: positive and negative image-image similarities using hyperbolic distance, and positive and negative image-text similarities using cosine similarity with standard and negated prompts. These predictions are fused using a weighted sum with parameter α≈1.2. The entire process requires no training or gradient updates, making it highly efficient for few-shot adaptation.

## Key Results
- T-DHA achieves 66.44% accuracy on 1-shot ImageNet, outperforming zero-shot CLIP (60.33%) by 6.11%
- The method shows consistent improvements across all shot settings, with maximum gains of 2.3% over state-of-the-art training-free methods
- T-DHA demonstrates superior domain generalization, improving accuracy by up to 2.51% on ImageNetV2 and ImageNet-Sketch compared to existing approaches
- The approach requires minimal computational overhead, with inference taking only 10.2ms on 16-shot ImageNet

## Why This Works (Mechanism)

### Mechanism 1: Hyperbolic Geometry Captures Hierarchical Semantic Structure
- **Claim**: Mapping CLIP embeddings into hyperbolic space improves representation of hierarchical semantic relationships compared to Euclidean cosine similarity.
- **Mechanism**: Hyperbolic space exhibits exponential volume growth with radius (vs. polynomial in Euclidean space), enabling low-distortion embedding of tree-like semantic hierarchies (e.g., Animal→Mammal→Dog→Labrador). The Poincaré ball model preserves angular structure while expanding distances near the boundary, naturally separating fine-grained categories.
- **Core assumption**: CLIP's pre-trained embeddings encode hierarchical semantic relationships that Euclidean metrics fail to fully exploit. Assumption: The exponential map (Eq. 2) sufficiently preserves semantic structure during projection.
- **Evidence anchors**:
  - [abstract]: "Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space. We find that this unique property is particularly effective for embedding hierarchical data structures."
  - [section]: "As demonstrated by Sarkar et al. [10], tree-like data can be embedded with low distortion in a Poincaré disk." Ablation (Table V): HD outperforms Euclidean cosine similarity across all shot settings (66.44% vs 65.27% at 1-shot).
  - [corpus]: "Hyperbolic Deep Learning for Foundation Models: A Survey" notes "fundamental limitations" in Euclidean representations for hierarchical data.
- **Break condition**: When class labels lack hierarchical structure, or when CLIP embeddings don't encode semantic hierarchies (e.g., flat, arbitrary category groupings).

### Mechanism 2: Explicit Negative Prototypes Enhance Discriminative Power
- **Claim**: Computing similarity to both positive class prototypes AND negative (non-class) prototypes improves classification by explicitly modeling non-membership.
- **Mechanism**: For each class k, a negative prototype is constructed by averaging hyperbolic features from other classes. During inference, the model computes P_h−(y=k|x) based on distance to negative prototypes, effectively asking "how much is this NOT class k?" This dual assessment refines decision boundaries when classes share overlapping features.
- **Core assumption**: Non-membership information provides complementary discriminative signal beyond CLIP's contrastive pre-training. Assumption: Random sampling from other classes yields meaningful negative prototypes.
- **Evidence anchors**:
  - [abstract]: "Coupled with negative learning, it provides more accurate and robust classifications with fewer feature dimensions."
  - [section]: Table VI shows removing negative pipeline causes 1.52% accuracy drop at 16-shot; authors state: "negative learning is crucial for refining a model's discriminatory power by explicitly accounting for what an image is not."
  - [corpus]: Weak direct corpus evidence on this specific negative prototype formulation.
- **Break condition**: When classes are already well-separated (negative signal redundant), or when random sampling yields unrepresentative negative prototypes in highly imbalanced datasets.

### Mechanism 3: Dual-Branch Prediction Fusion Leverages Complementary Modalities
- **Claim**: Combining hyperbolic image-image similarity with CLIP's text-based predictions outperforms either modality alone.
- **Mechanism**: T-DHA fuses four prediction streams: (1) positive image-image via hyperbolic distance to class prototypes, (2) negative image-image via distance to negative prototypes, (3) positive image-text via cosine similarity to standard prompts, (4) negative image-text via negated prompts ("a photo of no {class}"). Weighted fusion (Eq. 12, α≈1.2 optimal) balances these signals.
- **Core assumption**: Image-image and image-text modalities provide non-redundant information; visual similarity and semantic textual descriptions capture different aspects of class membership.
- **Evidence anchors**:
  - [abstract]: "T-DHA significantly outperforms state-of-the-art methods on few-shot image recognition and domain generalization tasks, achieving up to 2.3% improvement."
  - [section]: Table III shows progressive improvement: 60.33% (zero-shot) → 62.01% (+ neg text) → 64.85% (full T-DHA). Table VII: accuracy peaks at α=1.2, indicating image-image contribution is significant.
  - [corpus]: Related work on cross-modal transfer and reasoning supports multi-modal fusion benefits, though specific four-stream formulation is novel here.
- **Break condition**: When support set N is too small (1-shot) for stable prototype estimation, or when text prompts poorly describe visual content (domain mismatch).

## Foundational Learning

- **Concept: Riemannian Geometry & Poincaré Ball Model**
  - Why needed here: Core to understanding exponential maps, hyperbolic distance computation (Eq. 3), and why embedding in a unit ball with negative curvature helps hierarchical data.
  - Quick check question: Can you explain why hyperbolic distance grows exponentially as points approach the Poincaré ball boundary, and how this property helps separate fine-grained semantic categories?

- **Concept: CLIP Architecture & Zero-Shot Classification**
  - Why needed here: T-DHA builds directly on CLIP's pre-trained encoders and zero-shot prediction mechanism; understanding Eq. 1 is prerequisite.
  - Quick check question: How does CLIP perform zero-shot classification using text prompts, and what role does the temperature parameter τ play?

- **Concept: Few-Shot Learning Protocol**
  - Why needed here: Experiments follow N-way K-shot evaluation; understanding support/query splits and why training-free adaptation matters is essential.
  - Quick check question: In a 16-shot 10-way classification setup, how many images are used for prototype construction vs. evaluation?

## Architecture Onboarding

- **Component map**:
  VisualEncoder (CLIP Ev) -> ExponentialMap -> PositiveHyperbolicAdapter (P_h+) and NegativeHyperbolicAdapter (N_h-)
  VisualEncoder (CLIP Ev) -> TextEncoder (CLIP Et) -> PositiveTextAdapter (P_t+) and NegativeTextAdapter (P_t-)
  All prediction streams -> PredictionFusion (weighted sum with α)

- **Critical path**:
  1. Load pre-trained CLIP encoders (frozen, no gradient updates)
  2. For each class: extract features from N support images → compute Euclidean prototype (Eq. 4) → project to hyperbolic (Eq. 5)
  3. For each class: sample one image from each other class → average hyperbolic features → negative prototype
  4. At inference: extract test image feature → map to hyperbolic → compute four prediction scores → fuse with α=1.2

- **Design tradeoffs**:
  - Euclidean mean + exponential map vs. true Fréchet mean in hyperbolic space (authors use approximation for efficiency, justified for small support sets near origin)
  - α weighting: higher α favors image-image similarity (α=1.2 optimal on ImageNet)
  - Negative prototype sampling: random selection vs. more sophisticated selection strategies (unexplored)

- **Failure signatures**:
  - Very low accuracy at 1-shot: prototypes too noisy; consider ensemble or prototype refinement
  - Large performance gap between Euclidean and hyperbolic versions: check feature normalization, ensure exponential map correctly applied
  - Domain generalization fails on ImageNet-Sketch: text prompts may be too photo-centric; consider domain-aware prompt engineering
  - Negative pipeline shows no gain: classes may already be well-separated; check dataset diversity

- **First 3 experiments**:
  1. **Sanity check**: Run T-DHA vs. zero-shot CLIP on ImageNet 16-shot; expect ~4.5% improvement per Table IV. Verify exponential map produces points inside unit ball (||h|| < 1).
  2. **Ablation**: Disable each of the four prediction streams one at a time; verify results match Table VI pattern (positive branch more critical, but negative adds 1.5%+ gain).
  3. **Hyperparameter sweep**: Vary α ∈ {0.0, 0.4, 0.8, 1.2, 1.6, 2.0}; expect peak at 1.2 per Table VII. If peak differs significantly, investigate dataset-specific characteristics.

## Open Questions the Paper Calls Out
The paper explicitly states that while the current work focuses on CLIP-like models, "the proposed framework is general and can, in principle, be applied to Multimodal Large Language Models (MLLMs)." This suggests potential for extending T-DHA to more complex multimodal architectures, though such applications remain unexplored in the current work.

## Limitations
- The method relies heavily on CLIP's pre-trained embeddings, which may not capture all hierarchical relationships effectively
- Negative prototype construction uses random sampling, which may be suboptimal for imbalanced datasets
- The approach assumes class labels have hierarchical structure, potentially limiting effectiveness on flat or arbitrary category groupings

## Confidence
- **High Confidence**: Hyperbolic geometry's effectiveness in capturing hierarchical relationships (supported by Sarkar et al. [10] and ablation results showing 1.17% improvement over Euclidean at 16-shot)
- **Medium Confidence**: Dual-branch prediction fusion benefits (supported by progressive improvements in Table III, but specific four-stream formulation is novel)
- **Medium Confidence**: Negative prototype mechanism (ablation shows 1.52% drop when removed, but random sampling method is simplistic)

## Next Checks
1. **Parameter Sensitivity**: Systematically vary α ∈ {0.4, 0.8, 1.2, 1.6, 2.0} and ε ∈ {0.1, 0.5, 1.0, 2.0} to verify optimal values and stability of gains
2. **Negative Prototype Quality**: Replace random sampling with nearest-neighbor-based negative prototype construction to test whether sampling strategy affects performance
3. **Feature Norm Distribution**: Analyze the distribution of feature norms before and after exponential mapping to ensure numerical stability and verify that hyperbolic geometry is appropriately applied