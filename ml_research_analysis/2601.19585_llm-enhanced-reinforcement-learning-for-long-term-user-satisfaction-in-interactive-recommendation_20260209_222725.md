---
ver: rpa2
title: LLM-Enhanced Reinforcement Learning for Long-Term User Satisfaction in Interactive
  Recommendation
arxiv_id: '2601.19585'
source_url: https://arxiv.org/abs/2601.19585
tags:
- user
- recommendation
- lerl
- long-term
- high-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the filter bubble problem in interactive recommendation
  systems, where models overfit to short-term user preferences leading to content
  homogeneity. The proposed LLM-Enhanced Reinforcement Learning (LERL) framework integrates
  LLM-based semantic planning with RL-based policy learning in a hierarchical structure.
---

# LLM-Enhanced Reinforcement Learning for Long-Term User Satisfaction in Interactive Recommendation

## Quick Facts
- arXiv ID: 2601.19585
- Source URL: https://arxiv.org/abs/2601.19585
- Authors: Chongjun Xia; Yanchun Peng; Xianzhi Wang
- Reference count: 38
- Primary result: LERL achieves 17.24 interaction length and 12.28 cumulative reward on KuaiRand, outperforming best baseline (13.50, 9.82).

## Executive Summary
This paper addresses the filter bubble problem in interactive recommendation systems by proposing a hierarchical LLM-Enhanced Reinforcement Learning (LERL) framework. The approach combines LLM-based semantic planning with RL-based policy learning to balance accuracy and diversity for long-term user satisfaction. Experiments on two real-world datasets demonstrate significant improvements over state-of-the-art baselines, achieving 17.24 interaction length and 12.28 cumulative reward on KuaiRand compared to 13.50 and 9.82 for the best baseline.

## Method Summary
LERL implements a two-level hierarchy: a High-Level Semantic Planner (LLM) selects diverse content categories based on user history and reflections from past sessions, while a Low-Level Policy Learner (RL) recommends specific items within those categories. The LLM planner uses reflection pool sampling and prompt-based queries to generate category sets, which are converted to binary masks that filter the item space for the RL agent. The RL component uses PPO training with a Transformer encoder and Gaussian policy to score items within the masked category space. A diversity-aware quit mechanism in the simulated environment forces the agent to learn exploration strategies by decrementing user interaction budgets when consecutive recommendations come from the same category.

## Key Results
- LERL achieves 17.24 interaction length and 12.28 cumulative reward on KuaiRand dataset
- Outperforms best baseline by 3.74 interaction length and 2.46 cumulative reward
- Shows consistent improvements across multiple metrics (Tint, Rcum, Rsin) on both KuaiRand and KuaiRec datasets
- Ablation studies confirm the importance of both hierarchical structure and reflection mechanism

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Action Space Decomposition
Decomposing recommendation into high-level category planning (LLM) and low-level item selection (RL) reduces complexity in sparse environments. The LLM acts as High-Level Actor to select diverse categories, masking the item space for the Low-Level RL policy. This reduces the RL agent's action space from all items to items within selected categories, focusing exploration on semantically relevant and diverse regions.

### Mechanism 2: Reflection-Guided Semantic Policy Improvement
Storing and sampling textual "reflections" from past trajectories allows the LLM planner to improve long-term strategy without explicit gradient updates. A High-Level Critic generates natural language summaries analyzing session outcomes, stored in a Reflection Pool. During planning, the High-Level Actor samples high-performing reflections to condition its next decision.

### Mechanism 3: Diversity-Aware Simulation for Policy Shaping
Enforcing a "diversity-aware quit mechanism" in the simulation environment forces the agent to learn exploration strategies to survive. The simulated environment reduces the user's remaining interaction budget if the agent repeatedly recommends items from the same category, simulating "boredom" or the filter bubble effect.

## Foundational Learning

- **Hierarchical Reinforcement Learning (HRL)**: LERL implements classic HRL structure (Manager/Worker) where LLM is Manager and RL agent is Worker. Understanding temporal abstraction is key to debugging long-horizon task failures.
  - Quick check: Can you explain how "action masking" in the Low-Level Actor relates to the decision made by the High-Level Actor?

- **Proximal Policy Optimization (PPO)**: The Low-Level Policy Learner is explicitly trained using PPO. Understanding the clipped surrogate objective is crucial for diagnosing training stability.
  - Quick check: In Eq. (14), what is the purpose of the `clip` function, and how does the category mask $a_{mask}$ intervene before action is sampled?

- **In-Context Learning (ICL)**: High-Level Actors and Critics rely entirely on prompting rather than weight updates. The system learns via the Reflection Pool, a form of ICL memory.
  - Quick check: How does the sampling distribution $P(u)$ in Eq. (4) ensure that the LLM prioritizes "successful" strategies over random noise in the reflection pool?

## Architecture Onboarding

- **Component map**: User History -> High-Level Actor (LLM) -> Category Set -> Binary Mask -> Low-Level Actor (RL) -> Item Scores -> Top-6 Items -> Environment -> Reward/State -> High-Level Critic (LLM) -> Reflection Pool

- **Critical path**: 1) History Encoding: User interaction history $H_t$ formatted into prompt; 2) Reflection Sampling: Top-$k$ reflections sampled from pool based on reward scores; 3) Category Selection: High-Level Actor LLM outputs category set $c_t$; 4) Masking: Category set $c_t$ converted to binary mask $a_{mask}$ over all items; 5) Item Scoring: Low-Level Actor computes similarity scores and applies mask ($a_{score} = a_{sim} \odot a_{mask}$); 6) Action & Feedback: Top-$k$ items recommended; environment returns reward $r_t$ and next state.

- **Design tradeoffs**: Latency vs. Diversity (LLM adds inference latency for better semantic diversity); Sim-to-Real Gap (simulated quit behavior may not fully capture real user churn); Static LLM Weights (LLM not fine-tuned, relying entirely on reflection pool).

- **Failure signatures**: Empty Action Space (LLM selects categories with no available items); Reflection Collapse (reflection pool fills with contradictory advice); Myopic Exploitation (diversity penalty too weak, agent ignores category constraints).

- **First 3 experiments**: 1) Sanity Check (Masking): Run Low-Level Actor alone with fixed random category mask; 2) Reflection Ablation: Run with empty vs. full Reflection Pool; 3) Sensitivity Analysis: Vary "quit threshold" in simulator to observe High-Level Planner adaptation.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can LERL be extended into a multi-objective optimization framework that jointly optimizes fairness, diversity, and accuracy? The authors explicitly state this as a promising future direction, but current framework focuses primarily on accuracy and diversity trade-offs.

- **Open Question 2**: Does leveraging continual learning for the high-level planner significantly enhance LERL's adaptability and robustness to dynamic user preferences? The conclusion suggests potential, but current implementation uses static reflection mechanism without continuous planner updates.

- **Open Question 3**: To what extent does LERL's performance in the simulated environment translate to real-world online interactions? The paper relies exclusively on simulated offline environment with heuristic dropout rules, acknowledging online evaluation is "prohibitively costly."

## Limitations

- The LLM-based reflection generation may overfit to specific user archetypes and depends heavily on the LLM's reasoning capabilities without weight updates
- The diversity-aware simulation environment is a heuristic proxy that may not accurately represent real user behavior and churn dynamics
- The framework relies on static LLM weights and reflection pool quality, making it sensitive to prompt design and reflection sampling distribution

## Confidence

**High Confidence**: The hierarchical decomposition approach is well-grounded in existing HRL literature and empirical results are clearly reported with statistical significance.

**Medium Confidence**: The reflection-guided semantic improvement mechanism depends heavily on LLM reasoning capabilities and quality of reflection sampling, which the paper doesn't sufficiently validate.

**Low Confidence**: The diversity-aware simulation environment as a proxy for real user churn is a significant assumption without real-world deployment data to validate the simulated "boredom" behavior.

## Next Checks

1. **Reflection Quality Analysis**: Conduct human evaluation of reflection pool entries to assess whether they capture genuine causal insights about user behavior or merely describe surface-level patterns.

2. **Sim-to-Real Transfer Validation**: Design a small-scale real user study to compare diversity patterns observed in simulation versus actual user engagement and churn behavior.

3. **Ablation of LLM Planner**: Replace the LLM-based High-Level Actor with a simple heuristic (e.g., round-robin category rotation) to isolate the performance contribution of LLM-based semantic planning versus the hierarchical structure itself.