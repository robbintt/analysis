---
ver: rpa2
title: 'DIY-MKG: An LLM-Based Polyglot Language Learning System'
arxiv_id: '2507.01872'
source_url: https://arxiv.org/abs/2507.01872
tags:
- language
- vocabulary
- words
- diy-mkg
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DIY-MKG is an open-source, LLM-powered system designed to support
  polyglot language learners by enabling personalized vocabulary acquisition through
  multilingual knowledge graphs. The system allows users to expand vocabulary with
  LLM-suggested related words across multiple languages, add rich annotations at node,
  edge, and hyper-edge levels, and engage in adaptive reviewing via dynamically generated
  quizzes.
---

# DIY-MKG: An LLM-Based Polyglot Language Learning System

## Quick Facts
- arXiv ID: 2507.01872
- Source URL: https://arxiv.org/abs/2507.01872
- Reference count: 17
- Primary result: Open-source LLM-powered multilingual vocabulary acquisition system with personalized knowledge graphs

## Executive Summary
DIY-MKG is an open-source, LLM-powered system designed to support polyglot language learners by enabling personalized vocabulary acquisition through multilingual knowledge graphs. The system allows users to expand vocabulary with LLM-suggested related words across multiple languages, add rich annotations at node, edge, and hyper-edge levels, and engage in adaptive reviewing via dynamically generated quizzes. Vocabulary expansion was shown to be reliable and fair across Spanish, Korean, and Japanese, with vocabulary sizes growing to around 3,000 words after 500 iterations. Quiz generation achieved high accuracy for multiple-choice questions (98%) but showed variability for fill-in-the-blank questions (76–84%), highlighting the need for user feedback mechanisms. DIY-MKG emphasizes user control to prevent cognitive offloading and supports customization for diverse learning needs.

## Method Summary
DIY-MKG uses Llama-3.3-70B-Instruct for vocabulary expansion and quiz generation, with GPT-4.1-2025-04-14 for evaluation. The system starts with 10 random words per language (Spanish, Korean, Japanese) and iteratively expands vocabulary through 500 iterations using LLM-suggested related words. Users manually select which suggestions to add, preventing cognitive offloading. The knowledge graph structure represents words as nodes, relationships as edges, and documents linking multiple words as hyper-edges. Review is adaptive, using click frequency as a proxy for familiarity to prioritize quiz generation. The system generates 50 multiple-choice and 50 fill-in-the-blank questions per language, evaluated automatically with GPT-4.1 using JSON-formatted prompts provided in Appendix A.

## Key Results
- Vocabulary expansion reached ~3,000 words after 500 iterations with low variance across languages and starting words
- Multiple-choice quiz accuracy: 98% correct
- Fill-in-the-blank quiz accuracy: 76-84% correct, with systematic ambiguity issues
- System maintains user agency through manual selection of LLM suggestions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selective vocabulary expansion via LLM-suggested related words reliably grows multilingual knowledge graphs while maintaining user agency.
- Mechanism: LLM generates 10 related words per query → user manually selects which to add → selected words become nodes connected to the source word. The selection step enforces critical engagement rather than passive consumption.
- Core assumption: Users will exercise meaningful judgment during selection rather than selecting all suggestions indiscriminately.
- Evidence anchors:
  - [abstract] "vocabulary expansion was shown to be reliable and fair across Spanish, Korean, and Japanese, with vocabulary sizes growing to around 3,000 words after 500 iterations"
  - [section 4.1] "The final vocabulary sizes are around 3,000... the vocabulary does not saturate after 500 iterations"
  - [corpus] No direct corpus evidence for vocabulary expansion mechanisms; neighbor papers focus on polyglot code/programming, not language learning.
- Break condition: If users select all suggestions without review, or if LLM generates highly repetitive/duplicate words, vocabulary growth will plateau prematurely.

### Mechanism 2
- Claim: Click frequency serves as a practical proxy for word familiarity, enabling adaptive review prioritization.
- Mechanism: Each node click increments a counter → low-click words are prioritized for quiz generation → reviewing targets less-familiar vocabulary without explicit difficulty ratings.
- Core assumption: Click frequency inversely correlates with memorization strength; users click unfamiliar words more often to inspect/expand them.
- Evidence anchors:
  - [section 3.3] "the click count of each word serves as a good proxy for the user's level of understanding and memorization of the word"
  - [section 3.3] "The words with the lowest click counts are the ones that need more frequent review"
  - [corpus] No corpus evidence for click-based review mechanisms in learning systems.
- Break condition: If users systematically avoid clicking difficult words (avoidance behavior), or if high-frequency words are clicked for expansion rather than review, the proxy becomes unreliable.

### Mechanism 3
- Claim: Question flagging creates a feedback loop that improves quiz quality while increasing learner engagement.
- Mechanism: LLM generates quizzes → user completes and reviews results → user flags incorrect/ambiguous questions → flagged data stored locally → prompts can be refined iteratively.
- Core assumption: Users will actively flag errors rather than ignore them, and flagged data will actually be used for prompt refinement.
- Evidence anchors:
  - [abstract] "quiz generation achieved high accuracy for multiple-choice questions (98%) but showed variability for fill-in-the-blank questions (76–84%), highlighting the need for user feedback mechanisms"
  - [section 3.3] "The flagged questions are labeled in the local quiz file, which can be used to iteratively improve the quiz generation prompt"
  - [section 4.2] "Upon manual inspection, the incorrect fill-in-the-blank question-answer pairs usually have a question that is ambiguous"
  - [corpus] No corpus evidence for question flagging mechanisms in LLM-based education.
- Break condition: If users rarely flag questions, or if flagged patterns are not systematically analyzed, the feedback loop fails to improve prompts.

## Foundational Learning

- Concept: **Knowledge Graph Construction (nodes, edges, hyper-edges)**
  - Why needed here: The entire system represents vocabulary as a graph where words are nodes, relationships are edges, and documents linking multiple words are hyper-edges. Understanding this structure is essential for navigating and extending the system.
  - Quick check question: Can you explain what a hyper-edge represents in this system and give an example from the paper?

- Concept: **Cognitive Offloading in AI-Assisted Learning**
  - Why needed here: The paper explicitly designs against cognitive offloading—when learners passively accept AI outputs without critical thinking. The selective expansion mechanism directly addresses this concern.
  - Quick check question: Why does the checkbox-based selection interface mitigate cognitive offloading compared to auto-adding all LLM suggestions?

- Concept: **LLM Prompt Engineering for Structured Outputs**
  - Why needed here: All LLM interactions require JSON-formatted responses (related words, quiz questions, safety filtering). Prompts in Appendix A demonstrate the pattern: specify format constraints explicitly.
  - Quick check question: What could go wrong if the quiz generation prompt does not explicitly require JSON output?

## Architecture Onboarding

- Component map:
  Frontend -> LLM Interface -> Local Storage -> Review Engine
  (Knowledge graph visualization + side panel) -> (Vocabulary expansion, quiz generation) -> (JSON files for nodes, edges, hyper-edges, quiz results) -> (Click counter tracking + quiz generator + grader)

- Critical path:
  1. User adds initial word → LLM generates related words → user selects subset → nodes/edges created
  2. User adds annotations (node/edge/hyper-edge levels) → saved to local JSON
  3. User triggers review → system identifies low-click words → LLM generates quiz → user completes → results + flags saved locally

- Design tradeoffs:
  - Click counter vs. explicit difficulty rating: Simpler implementation but may misrepresent actual familiarity
  - Local LLM vs. API-based: Local offers safety/control; API offers quality/convenience
  - MCQ vs. fill-in-the-blank: MCQ more reliable (98%) but potentially easier; FIB more challenging but error-prone (76-84%)

- Failure signatures:
  - Vocabulary saturation: Growth curve flattens early → prompt may be generating too many duplicates; consider prompt refinement or multi-hop expansion
  - Quiz quality degradation: High flag rates on FIB questions → ambiguous questions; inspect flagged examples and constrain prompt to require unambiguous context
  - Low engagement with flagging: Users rarely flag errors → friction in UI or low perceived value; consider simplifying flag workflow or highlighting impact

- First 3 experiments:
  1. Vocabulary expansion stress test: Run 500+ iterations starting from 10 random words per language; measure saturation point, duplicate rate, and cross-language connection frequency. Compare against Duolingo vocabulary benchmarks cited (~2,114 words).
  2. Quiz accuracy audit: Generate 100 MCQ and 100 FIB questions per language; manually annotate correctness and error types (ambiguous, incorrect answer, formatting issue). Calculate precision by language and question type.
  3. Click counter validation study: Track user behavior over 2+ weeks; correlate click counts with quiz performance on same words. Test assumption that low-click = needs review.

## Open Questions the Paper Calls Out

- Does DIY-MKG actually improve vocabulary acquisition outcomes compared to existing tools like Duolingo or traditional flashcard methods?
- Can the question flagging mechanism effectively improve quiz generation prompts over iterative use?
- Does the checkbox-based selection interface meaningfully reduce cognitive offloading compared to automatic LLM-driven recommendations?
- How reliably does DIY-MKG support vocabulary expansion for low-resource languages lacking high-quality multilingual LLMs?

## Limitations
- No empirical validation of actual learning outcomes or vocabulary retention
- Click frequency proxy for familiarity remains unvalidated and may misrepresent user knowledge
- 76-84% accuracy for fill-in-the-blank questions indicates systematic ambiguity issues
- Local storage approach prevents data aggregation for system improvement or research

## Confidence
- High Confidence: Vocabulary expansion reliability (3,000 words after 500 iterations with low variance across languages and starting points)
- Medium Confidence: Quiz generation accuracy metrics (98% MCQ, 76-84% FIB) based on automated evaluation with GPT-4.1
- Low Confidence: Learning efficacy claims - technical capability demonstrated but no evidence of improved learning outcomes

## Next Checks
1. Learning outcome validation study: Conduct a randomized controlled trial comparing DIY-MKG users against traditional flashcard methods over 4-6 weeks, measuring vocabulary retention through standardized tests and tracking actual usage patterns.
2. Click frequency correlation analysis: Track a cohort of users over time, correlating click patterns with quiz performance and subsequent recall tests to validate the proxy assumption.
3. Question quality improvement pipeline: Implement systematic analysis of flagged questions to identify patterns, refine prompts based on these patterns, and measure whether flag rates decrease and accuracy improves.