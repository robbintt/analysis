---
ver: rpa2
title: 'Translation Analytics for Freelancers: I. Introduction, Data Preparation,
  Baseline Evaluations'
arxiv_id: '2504.14619'
source_url: https://arxiv.org/abs/2504.14619
tags:
- translation
- evaluation
- language
- machine
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates how freelance translators can adopt advanced
  automatic evaluation metrics like BLEU, chrF, TER, and COMET to assess MT and LLM
  outputs with rigor and precision. Using a trilingual medical corpus, the authors
  show that automatic scores strongly correlate with human judgments, even for small
  document samples, validating their practical utility for quality assessment.
---

# Translation Analytics for Freelancers: I. Introduction, Data Preparation, Baseline Evaluations

## Quick Facts
- arXiv ID: 2504.14619
- Source URL: https://arxiv.org/abs/2504.14619
- Reference count: 40
- This study demonstrates how freelance translators can adopt advanced automatic evaluation metrics like BLEU, chrF, TER, and COMET to assess MT and LLM outputs with rigor and precision.

## Executive Summary
This research establishes a practical framework for freelance translators to evaluate machine translation and large language model outputs using automatic metrics. Through analysis of a trilingual medical corpus, the authors demonstrate that automatic scores strongly correlate with human judgments, even when using small document samples. The study validates the utility of neural-based metrics like COMET for quality assessment across diverse language pairs, empowering freelancers to make informed decisions about translation tools and optimize workflows. The findings underscore the importance of integrating human expertise with automated methods to adapt to evolving translation technologies.

## Method Summary
The study uses the Reeve Foundation Trilingual Corpus (EN-RU-JA medical domain) to evaluate MT engines and LLMs using BLEU, chrF, TER, and COMET metrics. Translations were generated via API calls with temperature=0 for consistency, and evaluated using the MATEO platform. Human evaluation involved grading segments with highest, middle, and lowest COMET scores on a 0.0-4.0 scale. Correlation analysis was performed across subsampled document segments (229/1143/2183) to determine minimum viable sample sizes for reliable evaluation. The methodology emphasizes reproducible workflows for freelance practitioners.

## Key Results
- Small, representative samples (6.4% of corpus) yield statistically reliable MT/LLM performance rankings
- Sentence-level COMET scores strongly correlate with human evaluations across both Russian and Japanese translations
- Neural-based metrics (COMET) outperform string-based metrics for morphologically complex or non-whitespace-delimited languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small, representative samples (6.4% of corpus) yield statistically reliable MT/LLM performance rankings.
- Mechanism: Non-overlapping document segments from a coherent domain produce highly correlated metric score distributions across systems, enabling extrapolation from subset to whole.
- Core assumption: The source document is thematically coherent and the evaluation metrics capture translation quality consistently across segments.
- Evidence anchors:
  - [abstract] "small, representative samples (6.4% of total) yield statistically reliable rankings"
  - [Section 5] Pearson correlations between 229-segment and larger subsets show r > 0.85 for all metrics with p < 0.05
  - [corpus] Related work on automated evaluation methods (HiMATE framework) supports viability of reduced-sample evaluation approaches
- Break condition: If source document has high internal heterogeneity (multiple unrelated domains/registers), subset correlations may degrade.

### Mechanism 2
- Claim: Sentence-level COMET scores correlate with human evaluation grades for medical-domain translations.
- Mechanism: COMET's neural embedding space captures semantic similarity in ways that align with human judgment of accuracy, fluency, and terminology—even when surface-form metrics (BLEU, TER) diverge.
- Core assumption: Human graders apply consistent quality criteria aligned with the dimensions COMET was trained on.
- Evidence anchors:
  - [abstract] "Sentence-level COMET scores strongly correlate with human evaluations across both Russian and Japanese translations"
  - [Section 6.3] Pearson r values range 0.55–0.93; Spearman ρ values range 0.44–0.91 across MT/LLM outputs
  - [corpus] Weak or missing direct corpus evidence for this specific claim; related work on LLM-based evaluation (HiMATE) suggests broader validity but not direct replication
- Break condition: If human evaluation criteria differ substantially from MQM-style error categorization used in COMET training, correlation may weaken.

### Mechanism 3
- Claim: Neural-based metrics (COMET) outperform string-based metrics for morphologically complex or non-whitespace-delimited languages.
- Mechanism: Character- and word-level n-gram matching penalizes valid translations that differ lexically from references; semantic embedding metrics capture meaning equivalence despite surface variation.
- Core assumption: The neural metric's multilingual pre-training covers the target language adequately.
- Evidence anchors:
  - [Section 4] EN-JA COMET scores are comparable to EN-RU despite BLEU/chrF being substantially lower for Japanese
  - [Section 4] "lack of correlation between BLEU and COMET for EN-JA" underscores metric divergence for non-alphabetic scripts
  - [corpus] Weak corpus support; no direct comparison papers found in neighbor set
- Break condition: For low-resource languages with poor representation in COMET's training data, semantic metrics may not outperform string metrics.

## Foundational Learning

- Concept: Correlation coefficients (Pearson r, Spearman ρ) and statistical significance (p-values)
  - Why needed here: Interpreting whether sample scores generalize to full-corpus conclusions depends on understanding correlation strength and probability of chance findings.
  - Quick check question: If r = 0.79 and p = 0.038, would you accept the correlation as reliable? Why or why not?

- Concept: String-based vs. neural-based evaluation metrics
  - Why needed here: Choosing appropriate metrics for a language pair requires knowing which metrics capture quality effectively for that language's structure.
  - Quick check question: For translating English to Finnish (highly agglutinative), which metric class would you expect to perform better, and why?

- Concept: Sampling strategies and statistical power
  - Why needed here: Determining minimum viable sample size for evaluation requires balancing practical constraints against statistical reliability.
  - Quick check question: You have a 10,000-segment corpus and limited time. What sampling approach would preserve thematic coherence while minimizing evaluation effort?

## Architecture Onboarding

- Component map: Corpus preparation -> Translation generation -> Metric computation -> Correlation analysis -> Human evaluation
- Critical path:
  1. Clean and validate bilingual reference corpus
  2. Generate MT/LLM outputs with consistent prompting
  3. Compute all four metrics with language-appropriate tokenization
  4. Select representative segments for human evaluation
  5. Calculate sentence-level COMET and correlate with human grades

- Design tradeoffs:
  - Sample size vs. confidence: Smaller samples faster but may miss domain heterogeneity
  - Metric comprehensiveness vs. compute time: COMET adds ~16 minutes per evaluation run
  - Prompt specificity vs. brittleness: Overly specific prompts may generalize poorly across domains

- Failure signatures:
  - Low correlation between subset and full-corpus scores suggests sample unrepresentativeness
  - Large BLEU-COMET divergence indicates metric-language mismatch
  - High variance across repeated LLM runs (even at T=0) suggests output instability

- First 3 experiments:
  1. Replicate the 229/1143/2183 segment correlation analysis on a different domain corpus to test generalizability
  2. Compare COMET correlation with human grades using alternative human evaluation protocols (MQM vs. academic grading)
  3. Measure metric computation time and carbon footprint for different corpus sizes to establish practical scaling bounds

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum sample size required for statistically reliable ranking of MT/LLM systems using automatic evaluation metrics?
- Basis in paper: [explicit] "It would be interesting to trim down the sample size even more to determine the point at which the correlation is lost and the scores become unreliable" and "Correlation experiments with sample sizes reported in Section 5 need to be complemented with power analysis."
- Why unresolved: The study found that 6.4% of the corpus (229 segments) yielded reliable rankings, but did not systematically test smaller thresholds or conduct formal power analysis.
- What evidence would resolve it: Power analysis across multiple corpora, combined with experiments using progressively smaller sample sizes to identify the statistical breaking point.

### Open Question 2
- Question: Does maintaining a contextual conversation window improve LLM translation quality compared to sentence-by-sentence translation?
- Basis in paper: [explicit] "We did not attempt to evaluate the impact of context windows upon translation quality... Still, this may be worth exploring in future research."
- Why unresolved: The study used only de novo API calls without prior state; ad hoc experimentation suggested minimal gain, but this was not rigorously tested.
- What evidence would resolve it: Controlled experiments comparing translation quality with and without context windows, using the same LLMs and evaluation metrics across varied document types.

### Open Question 3
- Question: How effective are LLMs at extracting bilingual glossaries from parallel corpora, and can such glossaries meaningfully improve subsequent translation quality in freelancer workflows?
- Basis in paper: [explicit] "In future experiments with our corpus (RFTC) we want to explore the potential of various dedicated systems and LLMs for (i) extracting a bilingual glossary from a set of parallel sentences, and (ii) using a glossary thus obtained to improve the quality of translation."
- Why unresolved: This application was proposed as future work but not investigated.
- What evidence would resolve it: Experiments measuring glossary extraction accuracy and downstream translation quality improvements when glossaries are incorporated into LLM prompts or NMT systems.

### Open Question 4
- Question: Do automatic evaluation metrics correlate with human judgments across different translation domains, registers, and language pairs beyond the medical domain tested here?
- Basis in paper: [inferred] The authors explicitly note limitations: "Ideally, future experiments should also include other contrasting pairs—from different domains, registers etc." and "Translation directions... We hope that other language directions and document families will be added to our corpus in the future."
- Why unresolved: The study used a single narrow-domain medical corpus in two target languages; generalizability is unknown.
- What evidence would resolve it: Replication of the correlation analysis using corpora from diverse domains (legal, technical, marketing) and additional language pairs with varying typological distances.

## Limitations

- The analysis is based on a single medical-domain corpus, which may not represent the diversity of freelance translation work across technical, literary, and business domains.
- The correlation analysis relies on subjective human grading (0.0-4.0 scale) rather than standardized evaluation frameworks like MQM, introducing potential rater bias.
- The study does not address cost-effectiveness analysis of different evaluation approaches, which is critical for freelance practitioners operating under time and budget constraints.

## Confidence

**High Confidence:** The statistical methodology for correlation analysis and sample size determination is sound, with appropriate use of Pearson/Spearman coefficients and p-value thresholds. The technical implementation of automatic metrics (BLEU, chrF, TER, COMET) through established platforms (MATEO, SacreBLEU) is reliable and reproducible.

**Medium Confidence:** The findings about metric performance differences between language pairs (EN-RU vs EN-JA) are well-supported by the data, but the underlying mechanisms for why COMET outperforms string-based metrics for Japanese require additional validation across more language pairs and domains.

**Low Confidence:** The extrapolation from a single medical corpus to general freelance translation practice involves significant assumptions about domain transferability. The human evaluation correlation claims, while showing strong statistical relationships, depend on a small sample of graded segments (10-20 per system) that may not capture the full range of translation quality variations.

## Next Checks

1. **Domain Transferability Test:** Apply the same evaluation methodology to a non-medical corpus (e.g., legal, technical, or literary content) to verify whether the observed metric correlations and sample size recommendations hold across different subject matter.

2. **Human Evaluation Protocol Comparison:** Replicate the human grading component using standardized frameworks like MQM or Multidimensional Quality Metrics instead of the current 0.0-4.0 scale. Compare correlation results to determine if the strong COMET-human alignment persists with more structured evaluation criteria.

3. **Cost-Benefit Analysis:** Measure the actual time and monetary costs of implementing the proposed evaluation workflow (including API calls, metric computation, and human grading) for typical freelance project sizes. Quantify the trade-off between evaluation rigor and practical feasibility for independent practitioners.