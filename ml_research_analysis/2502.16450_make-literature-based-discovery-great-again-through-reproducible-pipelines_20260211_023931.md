---
ver: rpa2
title: Make Literature-Based Discovery Great Again through Reproducible Pipelines
arxiv_id: '2502.16450'
source_url: https://arxiv.org/abs/2502.16450
tags:
- discovery
- terms
- bisociative
- methods
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses reproducibility challenges in literature-based
  discovery (LBD) by providing open access to benchmark datasets, dockerized Jupyter
  Notebooks, and implemented LBD methods. The authors present a collection of reproducible
  pipelines that implement traditional LBD approaches alongside novel bisociative
  methods including ensemble-based, outlier-based, and link prediction-based techniques.
---

# Make Literature-Based Discovery Great Again through Reproducible Pipelines

## Quick Facts
- arXiv ID: 2502.16450
- Source URL: https://arxiv.org/abs/2502.16450
- Reference count: 39
- Authors: Bojan Cestnik; Andrej Kastrin; Boshko Koloski; Nada Lavrač
- Primary result: Provides reproducible Dockerized Jupyter Notebooks implementing LBD methods with benchmark datasets

## Executive Summary
This paper addresses reproducibility challenges in literature-based discovery by providing open access to benchmark datasets, dockerized Jupyter Notebooks, and implemented LBD methods. The authors present a collection of reproducible pipelines that implement traditional LBD approaches alongside novel bisociative methods including ensemble-based, outlier-based, and link prediction-based techniques. The study provides access to three benchmark datasets (RS-DFO, Mig-Mg, Aut-CaN) with 3, 43, and 13 bridging terms respectively, enabling replication of original discoveries. This work advances LBD toward more robust scientific discoveries by ensuring transparency, code reuse, and hands-on experimentation.

## Method Summary
The paper implements seven Jupyter Notebooks demonstrating literature-based discovery methods: Swanson's ABC closed discovery, concept-based open discovery with MeSH, CrossBee ensemble heuristics, OntoGen outlier-based closed discovery, RaJoLink outlier-based open discovery, network-based link prediction on co-citation networks, and AHAM LLM topic modeling. The approach uses PubMed XML/PSV files from three benchmark datasets, with preprocessing including LemmaGen lemmatization, n-gram extraction, and TF-IDF weighting. Methods are evaluated using ROC curves, AUC calculations, and link prediction accuracy metrics.

## Key Results
- Dockerized environment enables reproducible LBD experimentation with all dependencies packaged
- Network-based link prediction approach shows high prediction accuracy on co-citation networks
- AHAM LLM method identifies 19 topics characterizing the LBD research field
- Three benchmark datasets provided with verified bridging terms for validation

## Why This Works (Mechanism)

### Mechanism 1: Cross-Domain Term Intersection (ABC Model)
The system constructs separate Bag-of-Words representations for Domain A and Domain C, identifying intersecting terms that serve as semantic bridges. If terms appear frequently and specifically in both contexts, they imply a hypothesis: A → B → C. The approach assumes scientific knowledge is fragmented into silos where valid hypotheses remain undiscovered due to lack of cross-referencing.

### Mechanism 2: Outlier-Based Search Space Reduction
Restricting search for bridging terms to "outlier" documents increases signal-to-noise ratio for finding cross-domain links. Documents that bridge disparate fields do not conform to standard lexical profiles and manifest as statistical anomalies in TF-IDF space. This assumes bridging documents appear as outliers within their domain clusters.

### Mechanism 3: Link Prediction via Network Topology
Structural proximity in co-citation networks predicts semantic relatedness better than simple term overlap in sparse data regimes. The method uses unsupervised proximity measures (Common Neighbors, Adamic/Adar, Jaccard) to score likelihood of links between unconnected nodes, implying missing links represent potential discoveries.

## Foundational Learning

- **Concept: Swanson's ABC Model**
  - Why needed here: This is the theoretical backbone of the entire pipeline. Without understanding that the goal is to find 'B' linking 'A' and 'C', the code notebooks are just text processing scripts.
  - Quick check question: Can you distinguish between "Open Discovery" (starting with A to find C) vs. "Closed Discovery" (given A and C, find B)?

- **Concept: TF-IDF Vectorization**
  - Why needed here: The majority of methods (CrossBee, OntoGen, ABC) rely on TF-IDF to represent documents. You cannot interpret the "importance" of bridging terms without understanding how TF-IDF weights rare vs. common words.
  - Quick check question: If a term appears in every document of Domain A, will it have a high or low weight for distinguishing that domain, and why?

- **Concept: Reproducibility (Dockerization)**
  - Why needed here: The paper's primary novelty is the infrastructure (Docker/Jupyter). To use this work, you must understand that the environment (dependencies, OS) is packaged as the research object.
  - Quick check question: Why might an LBD experiment fail to replicate if run on a local machine versus the provided Docker container, even if the code is identical?

## Architecture Onboarding

- **Component map:** PubMed XML/PSV files (RS-DFO, Mig-Mg, Aut-CaN) -> LemmaGen lemmatization -> TF-IDF Vectorizer -> Modular notebooks (ABC, CrossBee, OntoGen, Link Prediction, AHAM) -> Evaluation (ROC, AUC, topic modeling)

- **Critical path:** 1) Execute docker build command to instantiate environment 2) Run 01_closed_discovery.ipynb to validate data loading and basic ABC logic 3) Inspect datasets directory to ensure PSV files are correctly parsed

- **Design tradeoffs:** Simplicity vs. Power - notebooks use simplified "mini" versions of complex algorithms for educational value, potentially sacrificing accuracy. Static vs. Dynamic Data - datasets are static snapshots ensuring historical replication but preventing live discovery of brand-new literature.

- **Failure signatures:** Empty Intersections - if text preprocessing parameters differ, set.intersection of terms may return empty, crashing ABC notebook. Memory Overflow - loading full Aut-CaN dataset (316k unique terms) into memory without sparse matrix optimizations may crash low-resource containers.

- **First 3 experiments:**
  1. Baseline Replication: Run 01_closed_discovery.ipynb and verify "blood viscosity" appears in top 10 results for RS-DFO dataset
  2. Heuristic Sensitivity: In 03_mini_crossbee.ipynb, modify ensemble weights and observe shift in ROC curve
  3. Link Prediction Validation: In 06_mini_linkpred.ipynb, reduce training network size by 50% and measure drop in prediction accuracy

## Open Questions the Paper Calls Out

### Open Question 1
Can the provided reproducible pipelines and bisociative methods effectively generalize to domains outside of biomedicine? The paper exclusively utilizes biomedical benchmark datasets and relies on MeSH ontology, leaving the adaptability to other fields unstated.

### Open Question 2
How do the implemented bisociative techniques (ensemble, outlier, link prediction) compare in computational efficiency and discovery accuracy when applied to the same tasks? The paper presents multiple distinct methods as separate tutorials but does not provide comparative analysis of their performance.

### Open Question 3
Can the LLM-based AHAM approach transition from domain conceptualization to generation of actionable, novel research hypotheses? The evaluation focuses on topic modeling quality rather than predictive power for literature-based discovery.

## Limitations
- Dataset size and domain specificity constrain generalizability, with RS-DFO providing limited statistical power
- Requires significant computational resources (Docker setup, API access) that may limit accessibility
- Lacks quantitative performance comparisons against contemporary state-of-the-art LBD methods

## Confidence

- **High Confidence:** Core reproducibility claim is well-supported through Docker containerization and benchmark datasets. Methodology for closed discovery using term intersection is well-established.
- **Medium Confidence:** Performance metrics for novel methods are reported but lack comparative benchmarks against recent LBD approaches. AHAM results are promising but not validated against alternative topic models.
- **Low Confidence:** Claims about "making LBD great again" through these specific pipelines lack empirical support showing these methods produce more impactful discoveries than existing approaches.

## Next Checks

1. Benchmark Comparison: Run provided notebooks against BioVerge benchmark dataset to evaluate performance relative to other LBD methods on standardized metrics.
2. Generalization Test: Apply pipeline to a non-biomedical domain (e.g., physics or social science literature) to assess cross-domain applicability of bisociative discovery methods.
3. Resource Efficiency Analysis: Profile computational requirements (memory, runtime) for each method on progressively larger datasets to identify practical scaling limits.