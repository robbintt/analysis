---
ver: rpa2
title: Accelerating Transformer Inference and Training with 2:4 Activation Sparsity
arxiv_id: '2503.16672'
source_url: https://arxiv.org/abs/2503.16672
tags:
- sparsity
- sparse
- training
- inference
- pass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to accelerate both inference and training
  of large language models (LLMs) using 2:4 activation sparsity, leveraging the intrinsic
  sparsity found in Squared-ReLU activations. The approach enables up to 1.3x faster
  Feed-Forward Networks (FFNs) in both forward and backward passes without accuracy
  loss.
---

# Accelerating Transformer Inference and Training with 2:4 Activation Sparsity

## Quick Facts
- **arXiv ID:** 2503.16672
- **Source URL:** https://arxiv.org/abs/2503.16672
- **Reference count:** 3
- **Primary result:** 1.3x speedup in Feed-Forward Networks using 2:4 activation sparsity without accuracy loss

## Executive Summary
This paper presents a method to accelerate both inference and training of large language models using 2:4 activation sparsity. By leveraging the intrinsic sparsity found in Squared-ReLU activations, the approach enables up to 1.3x faster Feed-Forward Networks (FFNs) in both forward and backward passes without accuracy loss. The method exploits hardware-accelerated GPU sparsity patterns to skip unnecessary computations, particularly beneficial for compute-bound workloads during training and inference.

## Method Summary
The method exploits the natural emergence of high activation sparsity (85-98%) in Squared-ReLU functions during training. This unstructured sparsity is then mapped to the 2:4 hardware pattern that NVIDIA Tensor Cores accelerate. For the backward pass, the method introduces a split-GEMM approach that separates sparse and dense features, along with token permutation to handle feature-wise sparsity constraints. The implementation requires a fused activation/sparsification kernel and careful handling of the backward pass optimization.

## Key Results
- 1.3x faster FFN layers in both forward and backward passes
- Perplexity remains at 2.652 (dense baseline) with 2:4 sparsity, versus 2.657 without warmup
- Minimal information loss (~1% of non-zero values dropped) when enforcing 2:4 pattern

## Why This Works (Mechanism)

### Mechanism 1
Replacing SwiGLU with Squared-ReLU induces high intrinsic activation sparsity (85-98%) in FFNs without accuracy loss. Squared-ReLU naturally zeros out negative activations (theoretical 50% sparsity), and during training, the distribution shifts such that the majority of activations become zero, creating a "free" source of sparsity that requires no additional pruning regularizer.

### Mechanism 2
Mapping unstructured activation sparsity to the 2:4 hardware pattern enables Tensor Core acceleration by skipping FLOPs. The method forces the activation tensor to comply with the 2:4 constraint (at most 2 non-zeros per 4 elements). Because the intrinsic sparsity is so high (often >90%), dropping the few extra non-zeros required to fit the 2:4 pattern incurs minimal information loss (~1% values dropped) while allowing the GPU to execute sparse GEMM.

### Mechanism 3
Accelerating the backward pass requires splitting the GEMM and permuting tokens to handle feature-wise sparsity constraints. The backward pass calculation requires sparsity along the feature dimension, but natural activations are sparse token-wise. The method introduces a split-GEMM approach that separates sparse and dense features, along with token permutation to break correlations where specific features are active across consecutive tokens.

## Foundational Learning

- **Concept: 2:4 Structured Sparsity**
  - **Why needed here:** This is the hardware constraint. NVIDIA Tensor Cores accelerate this specific pattern (2 non-zeros in every 4 elements). You cannot use arbitrary sparsity patterns without falling back to slow dense math or unoptimized sparse libraries.
  - **Quick check question:** Given a vector `[0.1, -0.2, 0.5, 0.8]`, is this 2:4 sparse? (Answer: No, `-0.2` is zero, leaving 3 non-zeros. It must be pruned to e.g., `[0.0, 0.0, 0.5, 0.8]` to qualify).

- **Concept: Squared-ReLU (ReLU²)**
  - **Why needed here:** This is the source of the "free" sparsity. Unlike SwiGLU (which outputs small non-zero values for negative inputs), Squared-ReLU hard-zeros negatives and amplifies positives, encouraging the network to become sparse during training.
  - **Quick check question:** Why would Squared-ReLU promote higher sparsity than Swish over training? (Answer: It forces hard zeros for x<0 and the squaring operation encourages the optimizer to rely on fewer, stronger positive activations).

- **Concept: Token-wise vs. Feature-wise Sparsity**
  - **Why needed here:** The hardware acceleration direction depends on the matrix multiplication order.
    - Forward (Y × W): Activation is on the left; we need rows to be 2:4 sparse (Token-wise).
    - Backward (Y^T × ∂L): Activation is transposed; we need columns to be 2:4 sparse (Feature-wise).
  - **Quick check question:** In the backward pass, why can't we just use the same sparsity mask from the forward pass? (Answer: Because the GEMM reduction dimension changes, requiring the columns of the transposed activation matrix to be sparse, which corresponds to features, not tokens).

## Architecture Onboarding

- **Component map:** Standard Dense GEMM -> Activation & Sparsification Kernel -> Split-Sparse GEMM
- **Critical path:** The fused activation/sparsify kernel is the critical software component. If this is slow, it erases the gains from the sparse GEMM.
- **Design tradeoffs:** The split-GEMM (95/5 split) is slightly slower than a theoretical 100% sparse kernel but is required to prevent divergence caused by non-sparse features.
- **Failure signatures:** Early plateau (perplexity 2.919) without token permutation; divergence (perplexity 3.735) without backward pass split; slow start if 2:4 enabled immediately.
- **First 3 experiments:**
  1. Baseline validation: Train a small model (100M params) with SwiGLU vs. Squared-ReLU (Dense) to confirm the activation swap doesn't degrade perplexity.
  2. Sparsity observation: Log the activation sparsity % during training of the Squared-ReLU model. Verify it rises to >85% naturally.
  3. Forward-only sparsification: Implement the 2:4 mask only in the forward pass to test the kernel speedup in isolation before tackling the complex backward pass split implementation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why do Squared-ReLU models exhibit intrinsic activation sparsity levels (84-98%) significantly higher than the theoretical 50% expectation?
- **Basis in paper:** [explicit] Page 2 states, "While we do not offer an explanation for the emergence of this phenomenon... we observe that the sparsity level rapidly increases during training."
- **Why unresolved:** The authors empirically observe the phenomenon but do not provide a theoretical justification for why the sparsity doubles beyond the initialization baseline.
- **What evidence would resolve it:** A theoretical analysis or targeted ablation study identifying the specific training dynamics or data distributions that drive the sparsity increase.

### Open Question 2
- **Question:** Does the proposed 2:4 sparsity recipe maintain accuracy parity when scaled to models significantly larger than 1.5B parameters?
- **Basis in paper:** [explicit] Page 4 notes, "We advocate for further scaling of sparse transformers to validate the approach with larger models."
- **Why unresolved:** The paper's main perplexity results are restricted to a 1.5B model trained on 63B tokens, leaving the behavior of larger frontier models unconfirmed.
- **What evidence would resolve it:** Pre-training results comparing perplexity and downstream task performance against dense baselines for models in the 7B to 70B+ parameter range.

### Open Question 3
- **Question:** Can the residual connections be effectively sparsified to accelerate the first linear layer of the FFN?
- **Basis in paper:** [explicit] Page 3, Footnote 2 states, "Further savings could be achieved if the residual can be made sparse."
- **Why unresolved:** The current method accelerates the second matrix multiplication (Y₂W₂), but the first (XW₁) remains dense because the residual inputs are not sparse.
- **What evidence would resolve it:** A method for inducing sparsity in the residual stream or first layer inputs that yields net training speedups without causing training instability.

## Limitations
- **Sparse-heavy layer sensitivity:** Unclear whether the approach translates well to attention layers where activation patterns may differ significantly.
- **Architectural constraints:** Method relies on Squared-ReLU activations, limiting generalizability to models using alternatives like GELU or SwiGLU.
- **Sparsity emergence reliability:** Method depends on natural emergence of high activation sparsity (85-98%), which may not occur in all model configurations.

## Confidence
- **High Confidence:** Kernel benchmarks demonstrating 1.3x speedups in FFN layers during both forward and backward passes.
- **Medium Confidence:** Claim that Squared-ReLU naturally induces 85-98% activation sparsity without accuracy loss, based on 1.5B parameter model results.
- **Low Confidence:** Assertion that method will scale seamlessly to models beyond 1.5B parameters or to attention layers without extensive validation.

## Next Checks
1. **Architectural transfer test:** Validate method on transformer architecture with GELU or SwiGLU activations by retraining with Squared-ReLU and measuring sparsity emergence and speedups.
2. **Attention layer application:** Extend 2:4 sparsity approach to attention layers (query, key, value projections) and assess performance and compatibility.
3. **Small model scalability:** Test method on smaller transformer models (100M-500M parameters) to determine if activation sparsity reaches required levels and whether computational savings justify implementation complexity.