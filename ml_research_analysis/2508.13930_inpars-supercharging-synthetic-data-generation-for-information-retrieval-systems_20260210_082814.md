---
ver: rpa2
title: 'InPars+: Supercharging Synthetic Data Generation for Information Retrieval
  Systems'
arxiv_id: '2508.13930'
source_url: https://arxiv.org/abs/2508.13930
tags:
- inpars
- query
- queries
- data
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors validate the reproducibility claims of the InPars Toolkit,
  demonstrating that it provides an end-to-end pipeline for synthetic query generation
  using LLMs, supports reproduction of InPars-V1, InPars-V2, and partially Promptagator,
  and enables plug-and-play integration of alternative LLMs. They reproduce results
  on the SciFact dataset using Llama 3.1 8B, showing comparable performance to the
  original InPars-V2 pipeline.
---

# InPars+: Supercharging Synthetic Data Generation for Information Retrieval Systems

## Quick Facts
- **arXiv ID:** 2508.13930
- **Source URL:** https://arxiv.org/abs/2508.13930
- **Authors:** Matey Krastev; Miklos Hamar; Danilo Toapanta; Jesse Brouwers; Yibin Lei
- **Reference count:** 2
- **Primary Result:** Reproduces and extends InPars toolkit with DSPy-CoT prompts and CPO fine-tuning, showing improved retrieval on SciFact.

## Executive Summary
This paper validates the reproducibility of the InPars synthetic query generation pipeline and introduces two key extensions: Contrastive Preference Optimization (CPO) for fine-tuning generator LLMs and dynamic Chain-of-Thought (CoT) prompts via DSPy. The authors demonstrate that their toolkit provides an end-to-end pipeline for synthetic query generation, supporting reproduction of InPars-V1, InPars-V2, and partial Promptagator replication. They show that larger models do not always yield better results, and that dynamic prompts consistently enhance performance while reducing the need for aggressive filtering.

## Method Summary
The study validates and extends the InPars synthetic query generation toolkit for neural information retrieval systems. The pipeline involves generating queries using LLMs (Llama 3.1 8B as generator, 70B as teacher), filtering generated pairs using scoring functions (MonoT5-3B or combined Sentence Transformers/BM25), and training rerankers (MonoT5-3B) on the synthetic data. The two extensions are: 1) CPO fine-tuning of the generator LLM to improve query quality, and 2) replacing static prompt templates with dynamic DSPy-CoT optimized prompts. The framework supports plug-and-play integration of alternative LLMs and is evaluated on the SciFact dataset with nDCG@10 as the primary metric.

## Key Results
- Reproduced InPars-V2 pipeline with Llama 3.1 8B, achieving comparable performance to original results on SciFact dataset
- DSPy-CoT optimized prompts consistently improved retrieval performance while reducing need for aggressive filtering
- CPO fine-tuned generator improved query quality but required careful filtering to avoid document copying
- Larger models (70B) did not always yield better results than smaller models (8B) in this synthetic data generation context
- All code, models, and synthetic datasets are publicly released

## Why This Works (Mechanism)
The toolkit works by leveraging the generative capabilities of LLMs to create synthetic queries that capture semantic relationships between documents and potential user queries. The filtering mechanism ensures only high-quality query-document pairs are retained for training. The DSPy-CoT extension improves this by allowing the LLM to reason through the document content before generating queries, producing more natural and diverse queries. CPO fine-tuning further improves the generator by training it to prefer query-document pairs that lead to better retrieval performance, creating a preference learning loop that enhances the quality of synthetic data generation.

## Foundational Learning

**Neural Information Retrieval (NIR):** Modern IR systems use neural networks to rerank documents based on semantic similarity rather than keyword matching. *Why needed:* Understanding that this work targets reranking rather than initial retrieval. *Quick check:* Verify the system uses BM25 for initial retrieval and neural reranker for final ranking.

**Contrastive Preference Optimization (CPO):** A fine-tuning method that optimizes LLMs based on preference between query-document pairs rather than explicit labels. *Why needed:* Explains how the generator is improved without human annotations. *Quick check:* Confirm CPO uses synthetic pairs to create preference pairs for training.

**Chain-of-Thought (CoT) Prompting:** A prompting technique that encourages LLMs to generate intermediate reasoning steps before producing final output. *Why needed:* Explains how DSPy improves query generation quality. *Quick check:* Verify DSPy-CoST generates intermediate reasoning before query output.

## Architecture Onboarding

**Component Map:** Document Corpus -> LLM Generator -> Scoring Function -> Filtered Pairs -> Reranker Training -> Evaluation
**Critical Path:** Generation → Filtering → Training → Evaluation
**Design Tradeoffs:** Larger models provide better generation quality but increase computational cost and may overfit; aggressive filtering improves quality but may remove useful signal
**Failure Signatures:** 
- Query degeneration (generator copies document text) when CPO is too aggressive
- Over-filtering leading to poor performance on domains with diverse query styles
- Memory constraints when using FP16 for large models

**First Experiments:**
1. Run baseline InPars-V2 pipeline with Llama 3.1 8B on SciFact dataset
2. Implement DSPy-CoST prompt optimization and compare against baseline
3. Apply CPO fine-tuning and test with varying filtering thresholds

## Open Questions the Paper Calls Out

None

## Limitations
- Single dataset evaluation (SciFact) limits generalizability across domains
- Lack of theoretical explanation for why larger models sometimes underperform smaller ones
- CPO fine-tuning procedure lacks detailed hyperparameter specifications
- Evaluation focuses only on nDCG@10, not exploring other ranking metrics

## Confidence

**High Confidence:** Reproducibility claims for InPars-V2 pipeline, basic functionality of extended framework (CPO, DSPy-CoT), and quantitative results on SciFact

**Medium Confidence:** Claim that dynamic prompts consistently improve performance across domains, assertion that larger models don't always yield better results

**Low Confidence:** Generalizability of CPO and DSPy-CoT extensions to arbitrary NIR settings beyond SciFact

## Next Checks
1. Validate DSPy-CoT and CPO extensions on at least two additional BEIR datasets (TREC-Covid, NFCorpus)
2. Systematically test CPO fine-tuning hyperparameter sensitivity (learning rate, batch size, epochs)
3. Conduct comprehensive ablation study on filtering thresholds across all domains to quantify domain-dependent effects