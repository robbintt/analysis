---
ver: rpa2
title: 'NIMO: a Nonlinear Interpretable MOdel'
arxiv_id: '2506.05059'
source_url: https://arxiv.org/abs/2506.05059
tags:
- nimo
- feature
- linear
- features
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NIMO is a nonlinear interpretable model that combines the global
  interpretability of linear regression with the expressivity of neural networks.
  It preserves the intelligible interpretation of linear coefficients as marginal
  effects at the mean (MEM) while allowing per-instance nonlinear corrections.
---

# NIMO: a Nonlinear Interpretable MOdel

## Quick Facts
- arXiv ID: 2506.05059
- Source URL: https://arxiv.org/abs/2506.05059
- Reference count: 40
- NIMO is a nonlinear interpretable model that combines linear interpretability with neural network expressivity

## Executive Summary
NIMO introduces a novel approach to interpretable machine learning by combining the global interpretability of linear regression with the expressivity of neural networks. The model achieves this through a dual-component architecture where a linear component provides interpretable coefficients (as marginal effects at the mean) while a shared neural network captures per-instance nonlinear corrections. By masking out the current feature during correction computation, NIMO ensures that feature effects remain interpretable and free from confounding interactions. The method employs parameter elimination optimization and adaptive ridge regularization to achieve both efficiency and sparsity.

## Method Summary
NIMO's architecture consists of a linear component that captures the global structure and interpretable coefficients, paired with a shared neural network that computes feature-specific nonlinear corrections. The neural network uses positional encoding to handle features of varying types and scales, and crucially masks out the current feature when computing its correction to avoid introducing spurious interactions. The model is trained using a parameter elimination optimization algorithm that iteratively refines the linear and nonlinear components, while adaptive ridge regularization encourages sparsity in the final model. This design allows NIMO to maintain linear interpretability while achieving nonlinear predictive performance.

## Key Results
- NIMO outperforms baseline methods on synthetic datasets while accurately recovering feature effects
- On real datasets (diabetes, Boston housing, superconductivity), NIMO achieves competitive predictive performance
- The model provides meaningful global interpretations through marginal effects at the mean (MEMs)
- NIMO is particularly effective when nonlinear interactions are present but the underlying structure remains largely linear

## Why This Works (Mechanism)
NIMO works by separating linear interpretability from nonlinear expressivity through a dual-component architecture. The linear component provides globally interpretable coefficients that represent marginal effects at the mean, while the shared neural network captures per-instance nonlinear corrections. The key innovation is the masking mechanism that prevents the neural network from introducing confounding interactions, ensuring that the linear coefficients remain interpretable. The parameter elimination optimization algorithm efficiently trains both components simultaneously, and adaptive ridge regularization achieves sparsity without sacrificing interpretability.

## Foundational Learning
1. Marginal Effects at the Mean (MEM)
   - Why needed: Provides interpretable coefficients that represent average feature effects across the dataset
   - Quick check: Compare MEM coefficients with standard linear regression coefficients on simple datasets

2. Positional Encoding in Neural Networks
   - Why needed: Enables the shared neural network to handle features with different types and scales
   - Quick check: Evaluate model performance with and without positional encoding on mixed-type datasets

3. Feature Masking for Interpretability
   - Why needed: Prevents the neural network from introducing spurious interactions that would confound linear interpretations
   - Quick check: Verify that masking removes feature correlations in the correction terms

4. Parameter Elimination Optimization
   - Why needed: Enables efficient training of the dual-component model without separate optimization stages
   - Quick check: Compare training convergence speed with standard gradient descent approaches

## Architecture Onboarding

Component map: Linear component -> Neural network (with positional encoding) -> Masking mechanism -> Parameter elimination optimization -> Adaptive ridge regularization

Critical path: Input features → Linear component (MEM calculation) + Neural network (masked corrections) → Combined prediction → Parameter elimination optimization → Final interpretable model

Design tradeoffs:
- Linear component vs. full neural network: Sacrifices some nonlinear expressivity for interpretability
- Shared vs. separate neural networks: Shared network reduces parameter count but may limit flexibility
- Masking mechanism: Ensures interpretability but may underfit complex interactions
- Parameter elimination vs. standard optimization: Faster training but potential convergence issues

Failure signatures:
- Poor performance when true relationships are highly nonlinear
- Over-regularization leading to overly sparse models
- Convergence issues with the parameter elimination algorithm
- Positional encoding inadequacy for features with unusual distributions

First experiments:
1. Compare MEM coefficients with standard linear regression on a simple synthetic dataset
2. Test model performance with positional encoding disabled on a mixed-type dataset
3. Evaluate the impact of masking by training with and without the masking mechanism

## Open Questions the Paper Calls Out
None

## Limitations
- Positional encoding generalization across datasets with vastly different feature distributions remains untested
- Parameter elimination optimization lacks theoretical convergence guarantees and may be initialization-sensitive
- Adaptive ridge regularization could over-shrink coefficients in datasets with small but nonzero feature effects
- Scalability to very high-dimensional data (>1000 features) has not been thoroughly validated

## Confidence
- High confidence in the theoretical framework and mathematical formulation
- Medium confidence in the synthetic dataset performance results
- Medium confidence in the real-world dataset comparisons due to limited baseline diversity
- Low confidence in the scalability claims for very high-dimensional data (>1000 features)

## Next Checks
1. Test NIMO's performance on high-dimensional datasets (p > 1000) to evaluate scalability claims and computational efficiency
2. Conduct ablation studies removing the positional encoding to quantify its contribution to model performance
3. Compare NIMO against a broader range of interpretable methods including decision trees and rule-based models to establish relative strengths and weaknesses