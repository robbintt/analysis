---
ver: rpa2
title: A Systematic Review on the Evaluation of Large Language Models in Theory of
  Mind Tasks
arxiv_id: '2502.08796'
source_url: https://arxiv.org/abs/2502.08796
tags:
- language
- mind
- theory
- llms
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic review synthesizes current efforts to assess the
  Theory of Mind (ToM) capabilities of large language models (LLMs), an essential
  aspect of human cognition involving the attribution of mental states to oneself
  and others. Through a comprehensive analysis of 58 papers, the study finds that
  while LLMs demonstrate emerging competence in ToM tasks, significant gaps persist
  in their emulation of human cognitive abilities.
---

# A Systematic Review on the Evaluation of Large Language Models in Theory of Mind Tasks

## Quick Facts
- arXiv ID: 2502.08796
- Source URL: https://arxiv.org/abs/2502.08796
- Reference count: 40
- This systematic review analyzes 58 papers on LLM Theory of Mind evaluation, finding emerging but limited capabilities with significant gaps in human-like cognition

## Executive Summary
This systematic review synthesizes current research on evaluating Theory of Mind (ToM) capabilities in large language models (LLMs). The study analyzes 58 papers to understand how LLMs perform on ToM tasks - the ability to attribute mental states to oneself and others. While LLMs show emerging competence in ToM tasks, the review identifies significant limitations in their emulation of human cognitive abilities. GPT-based models dominate the field, with most research focusing on text-only modalities and narrow task types.

The findings reveal that current benchmarks often fall short of human baselines, and LLM performance may rely on spurious correlations rather than robust understanding. Key challenges include training data bias, prompt bias, and the risk of shortcut learning. The review emphasizes that while LLMs show promise in ToM tasks, their abilities remain limited and require more comprehensive evaluation methods to capture the full spectrum of ToM capabilities.

## Method Summary
The review employs a systematic methodology, analyzing 58 peer-reviewed papers published between 2020 and 2023 that evaluate LLMs on Theory of Mind tasks. The authors extract and categorize key information including model types, evaluation tasks, modalities, performance metrics, and identified limitations. The analysis focuses on identifying trends, gaps, and challenges in current LLM ToM evaluation approaches, with particular attention to the dominance of GPT models and text-only modalities.

## Key Results
- LLMs demonstrate emerging but limited Theory of Mind capabilities across evaluated tasks
- GPT-based models dominate research (40.36% of studies use GPT-3), with 93.06% of studies focusing on text-only modalities
- Current benchmarks rely on narrow tasks and frequently fall short of human baselines
- Significant challenges include training data contamination, prompt bias, and shortcut learning risks

## Why This Works (Mechanism)
The review provides a comprehensive synthesis of current ToM evaluation approaches, identifying patterns in model performance and methodological limitations. By systematically analyzing 58 papers, it reveals that while LLMs show emerging capabilities in ToM tasks, their performance is constrained by narrow task design, model bias toward GPT architectures, and evaluation in predominantly text-only modalities. The mechanism of analysis involves extracting patterns across studies to understand how current benchmarks may not capture the full complexity of human-like ToM abilities.

## Foundational Learning
- Theory of Mind (ToM): The cognitive ability to attribute mental states to oneself and others, crucial for understanding beliefs, intentions, and emotions
  - Why needed: Forms the basis for human social cognition and interaction
  - Quick check: Can distinguish between false beliefs and reality

- Large Language Models (LLMs): AI systems trained on vast text corpora to generate human-like responses
  - Why needed: Represent the current state-of-the-art in natural language processing
  - Quick check: Can process and generate text across diverse topics

- Benchmark Tasks: Standardized evaluations used to measure LLM performance
  - Why needed: Provide consistent metrics for comparing model capabilities
  - Quick check: Include Unexpected Transfer, False Beliefs, and Hints Tasks

## Architecture Onboarding
Component Map: GPT Model Architecture -> ToM Task Processing -> Response Generation -> Performance Evaluation

Critical Path: Model Input (Task Prompt) -> Internal Processing (Attention Mechanisms) -> Output Generation (Response) -> Performance Assessment (Accuracy Metrics)

Design Tradeoffs: The review identifies key tradeoffs between model complexity and evaluation reliability, noting that while more sophisticated models may show better ToM performance, they also present greater risks of training data contamination and shortcut learning.

Failure Signatures: Common failure patterns include:
- Reliance on surface-level patterns rather than genuine ToM understanding
- Performance degradation when tasks deviate from training data patterns
- Inability to generalize ToM capabilities across different task types

First Experiments:
1. Test GPT model performance on Unexpected Transfer task
2. Evaluate baseline human performance on same task
3. Compare model and human responses for qualitative differences

## Open Questions the Paper Calls Out
- How can we develop multimodal ToM benchmarks that better capture the full spectrum of Theory of Mind capabilities beyond text-based tasks?
- What systematic methods can be employed to quantify and mitigate training data contamination in ToM evaluation tasks?
- How do we design evaluation protocols that distinguish between genuine ToM understanding versus spurious correlation-based performance in LLMs?

## Limitations
- Predominance of GPT-based models creates evaluation bias that may not generalize to other architectures
- Heavy reliance on text-only modalities (93.06% of studies) limits understanding of multimodal ToM capabilities
- Narrow focus of current benchmarks may not capture the full spectrum of ToM abilities
- Limited investigation into the depth of LLM understanding versus pattern matching in ToM tasks

## Confidence
High Confidence: LLMs demonstrate emerging but limited ToM capabilities; current benchmarks are narrow and predominantly text-based
Medium Confidence: Claims about GPT model dominance and task type prevalence; training data contamination concerns
Low Confidence: Extent of shortcut learning and depth of LLM ToM understanding

## Next Checks
1. Conduct systematic tests across diverse model architectures (beyond GPT) to verify whether current ToM evaluation findings generalize to different LLM families

2. Develop and validate multimodal ToM benchmarks to assess whether current text-based evaluation captures the full range of ToM capabilities

3. Implement rigorous training data analysis protocols to quantify the extent of test item contamination and its impact on reported LLM performance in ToM tasks