---
ver: rpa2
title: Solving Spatial Supersensing Without Spatial Supersensing
arxiv_id: '2511.16655'
source_url: https://arxiv.org/abs/2511.16655
tags:
- spatial
- supersensing
- cambrian-s
- video
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critically evaluates the VSI-Super benchmarks (VSR and
  VSC) introduced by Cambrian-S for measuring spatial supersensing in video world
  models. The authors introduce NoSense, a simple baseline using only a SigLIP model
  that discards temporal structure, yet achieves near-perfect performance (95%) on
  VSR even for 4-hour videos.
---

# Solving Spatial Supersensing Without Spatial Supersensing

## Quick Facts
- arXiv ID: 2511.16655
- Source URL: https://arxiv.org/abs/2511.16655
- Reference count: 8
- Key outcome: Simple SigLIP-based NoSense baseline achieves 95% VSR accuracy and VSC-Repeat sanity check shows Cambrian-S's VSC inference collapses from 42% to 0%, indicating current VSI-Super benchmarks do not reliably measure spatial supersensing

## Executive Summary
This paper critically examines VSI-Super benchmarks (VSR and VSC) introduced by Cambrian-S for measuring spatial supersensing in video world models. The authors demonstrate that these benchmarks can be solved without genuine spatial cognition through two key findings. First, NoSense—a simple baseline using only a SigLIP model that discards temporal structure—achieves near-perfect VSR performance (95%) even on 4-hour videos by relying on semantic cues from a few informative frames. Second, VSC-Repeat, a sanity check where videos are repeated multiple times, reveals that Cambrian-S' inference method, which relies on surprise-based segmentation, collapses from 42% to 0% accuracy when videos are repeated, indicating it exploits benchmark-specific shortcuts rather than genuine spatial supersensing. These findings suggest current VSI-Super benchmarks do not reliably measure spatial supersensing capabilities.

## Method Summary
The paper introduces NoSense, a simple baseline that uses a SigLIP model to encode frames independently at 1 FPS, maintains a streaming buffer of the top-4 frames most similar to the object query, and selects answers by aggregating cosine similarities between these frames and auxiliary object prompts. For VSR, this achieves 95% accuracy without temporal reasoning or spatial cognition. For VSC, the authors propose VSC-Repeat—concatenating each video with itself 1-5 times—to test Cambrian-S' inference method. Cambrian-S uses surprise-based segmentation to partition videos into segments, counts unique objects per segment, and sums across segments. When videos are repeated, Cambrian-S' accuracy collapses from 42% to 0% and predicted counts grow proportionally with repeats, revealing it exploits the "rooms never repeat" shortcut.

## Key Results
- NoSense achieves 95% VSR accuracy using only SigLIP frame encoding and semantic matching, without temporal reasoning or spatial cognition
- Cambrian-S' VSC inference accuracy drops from 42% to 0% when videos are repeated 5 times, with predicted counts growing proportionally
- Both NoSense and Cambrian-S share the same functional pattern (streaming frame encoding, compact memory, query-guided retrieval), suggesting performance gains come from retrieval shortcuts rather than additional world-modeling machinery

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VSR benchmark can be near-perfectly solved using only frame-level semantic matching without temporal reasoning or spatial cognition
- Mechanism: NoSense encodes frames independently with SigLIP, retains only the top-4 frames most similar to the object query in a streaming buffer, then aggregates cosine similarities between these frames and auxiliary object prompts to select the temporal ordering answer
- Core assumption: The correct temporal order can be inferred from the relative positions of the 4 most object-relevant frames, without tracking motion or maintaining long-term state
- Evidence anchors: [abstract] "NoSense, which discards almost all temporal structure and uses only a bag-of-words SigLIP model, yet near-perfectly solves VSR, achieving 95% accuracy even on 4-hour videos"; [section 2, p.3-4] Algorithm 1 shows the complete streaming CLIP/SigLIP pipeline with no temporal reasoning; Figure 3 shows NoSense outperforming Cambrian-S by 55% while using fraction of memory
- Break condition: If objects appear in environments with similar visual contexts, or if ordering requires reasoning about motion trajectories rather than static frame-object associations, this approach would fail

### Mechanism 2
- Claim: Cambrian-S's VSC inference method relies on a "rooms never repeat" shortcut rather than genuine spatial supersensing for unique object counting
- Mechanism: Cambrian-S uses surprise-based segmentation to partition video into segments at high prediction-error boundaries, accumulates frame features per segment, produces segment-level count estimates, then sums across segments—implicitly assuming each segment is a distinct, non-revisited environment
- Core assumption: High surprise indicates scene change to a new environment, so resetting the buffer and counting fresh per segment is valid
- Evidence anchors: [abstract] "Cambrian-S' inference algorithm relies largely on a shortcut in the VSC benchmark that rooms are never revisited"; [section 3, p.6-7] Figure 5 shows mean relative accuracy collapsing from 42% to 0% as repeats increase; predicted counts grow proportionally with repeats
- Break condition: If videos contain revisits to previously-seen environments (as in VSC-Repeat), the segmentation-based counting overcounts by treating revisits as new objects

### Mechanism 3
- Claim: High performance on VSI-Super benchmarks may reflect benchmark-model co-adaptation rather than generalizable spatial supersensing capabilities
- Mechanism: Both NoSense and Cambrian-S's VSR pipeline share the same functional pattern—streaming frame encoding, compact memory of salient frames, and query-guided retrieval—suggesting performance gains come from exploiting this retrieval shortcut rather than from additional world-modeling machinery
- Core assumption: Benchmarks designed with specific generative assumptions (4 object insertions, non-repeating rooms) allow models to encode these regularities as heuristics
- Evidence anchors: [section 2, p.5] "Cambrian-S and NoSense share the same functional components... The fact that a simpler instantiation near-perfectly solves VSR suggests that it can be solved without additional inductive biases"; [section 4, p.7-8] "A model can score well by encoding rules such as 'four object insertions' or 'rooms never repeat' rather than by building a stable internal map"
- Break condition: If benchmarks incorporate invariance checks (repeated scenes, shuffled segments, revisits), methods relying on shortcuts would show performance collapse

## Foundational Learning

- Concept: **Contrastive Vision-Language Embeddings (CLIP/SigLIP)**
  - Why needed here: NoSense relies entirely on cosine similarity between image and text embeddings to identify relevant frames and score answer options
  - Quick check question: Can you explain why normalizing embeddings before computing dot products yields cosine similarity?

- Concept: **Shortcut Learning in Benchmarks**
  - Why needed here: The paper's core thesis is that benchmark design can inadvertently create exploitable heuristics that substitute for the intended capability
  - Quick check question: What is an example of a spurious cue a model might learn instead of the intended task?

- Concept: **Streaming Memory Architectures**
  - Why needed here: Both NoSense and Cambrian-S use streaming encoders with compact buffers rather than storing all frames; understanding this pattern is essential for reproducing or extending these methods
  - Quick check question: How does keeping only top-k frames differ from a FIFO buffer, and what tradeoffs does it introduce?

## Architecture Onboarding

- Component map:
  - Frame Encoder -> SigLIP/CLIP vision encoder processes frames independently at 1 FPS
  - Query Encoder -> Text encoder embeds object queries and auxiliary prompts
  - Streaming Buffer -> Maintains top-k frames by query similarity (k=4 for NoSense)
  - Scoring Module -> Aggregates frame-option similarities to rank answer permutations
  - Sanity Check Pipeline -> VSC-Repeat concatenation to test invariance

- Critical path:
  1. Video frames → SigLIP encoder → frame embeddings
  2. Object query → text encoder → query embedding
  3. Streaming comparison → retain top-4 frames in buffer
  4. Auxiliary prompts → text encoder → option embeddings
  5. Cross-attention scoring → aggregate similarities → select answer

- Design tradeoffs:
  - Buffer size (k): Larger k captures more temporal context but increases memory; k=4 suffices for VSR's 4-insertion structure
  - Encoder choice: SigLIP-2-So400m-512 outperforms CLIP-L/14 by ~5-10% (Table 1), but smaller models still beat Cambrian-S
  - Prompt engineering: Prompt ensembles help (~5%) but raw questions still achieve 75-90% accuracy

- Failure signatures:
  - VSC-Repeat collapse: Accuracy drops to 0% with 5 repeats indicates segment-based counting overcounts revisits
  - Temporal ordering errors: If NoSense fails, likely due to ambiguous frame relevance or similar visual contexts across environments
  - Memory overflow: 4-hour videos at 1 FPS = 14,400 frames; streaming buffer essential to avoid OOM

- First 3 experiments:
  1. Reproduce NoSense on VSR 10-min split: Implement Algorithm 1 with SigLIP2-So400m-512, verify ~98% accuracy; ablate with raw questions to confirm robustness
  2. Run VSC-Repeat sanity check: Take 10-min VSC videos, concatenate 1-5 times, evaluate Cambrian-S inference pipeline; confirm accuracy collapse and overcounting proportional to repeats
  3. Test buffer size sensitivity: Vary k from 2 to 8 on VSR to characterize how many frames are actually needed; plot accuracy vs. k to identify minimal sufficient buffer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What benchmark design principles can ensure spatial supersensing tasks cannot be solved by simple frame-level retrieval heuristics?
- Basis in paper: [explicit] Authors state "A genuine first step toward supersensing requires demonstrating at least one capability that trivial retrieval-based methods cannot achieve" and propose design principles including invariance checks and natural long-form video
- Why unresolved: The paper identifies the problem and suggests principles but does not validate a replacement benchmark
- What evidence would resolve it: A new benchmark where simple retrieval baselines fail but genuine spatial reasoning methods succeed, with performance stable under transformations like scene repetition

### Open Question 2
- Question: How can video world models be evaluated to verify they maintain persistent object identity across revisited environments rather than segment-level counting heuristics?
- Basis in paper: [explicit] VSC-Repeat reveals Cambrian-S overcounts objects proportional to repeats. Authors ask: "Which invariances should hold, and do methods respect them?"
- Why unresolved: Current evaluation lacks tests for revisit-invariance; Cambrian-S authors acknowledge this limitation
- What evidence would resolve it: Models that maintain unchanged object counts under VSC-Repeat-style perturbations, demonstrating genuine spatial state persistence

### Open Question 3
- Question: What capabilities beyond semantic perception are required for genuine spatial supersensing that contrastive VLMs cannot provide?
- Basis in paper: [inferred] NoSense achieves 95% VSR accuracy with only SigLIP, showing perception is sufficient for current benchmarks, but the paper does not identify what additional capabilities are truly necessary
- Why unresolved: The paper demonstrates what is NOT needed but does not characterize what IS needed for spatial supersensing
- What evidence would resolve it: Identification of specific spatial reasoning tasks where contrastive models systematically fail but spatial cognition succeeds

## Limitations
- Unknown prompt ensemble templates and exact Cambrian-S inference implementation details may affect quantitative results
- Analysis assumes correct characterization of Cambrian-S methods from public materials
- Limited to one model family (Cambrian-S) for VSC analysis
- Does not identify what specific capabilities beyond perception are actually required for spatial supersensing

## Confidence
- **High confidence**: NoSense's ability to near-perfectly solve VSR using only frame-level semantic matching without temporal reasoning
- **Medium confidence**: Cambrian-S's VSC inference relies on the "rooms never repeat" shortcut
- **Medium confidence**: The broader claim that VSI-Super benchmarks do not reliably measure spatial supersensing

## Next Checks
1. Test whether other spatial reasoning models (not just Cambrian-S) also show VSC-Repeat collapse to confirm the shortcut is not model-specific
2. Systematically test raw questions vs. extracted objects vs. prompt ensembles on NoSense to quantify the 8-20% accuracy range and confirm minimal performance requirements
3. Shuffle video frames before applying NoSense to determine if temporal structure is truly irrelevant for VSR, or if the current approach implicitly exploits ordering cues