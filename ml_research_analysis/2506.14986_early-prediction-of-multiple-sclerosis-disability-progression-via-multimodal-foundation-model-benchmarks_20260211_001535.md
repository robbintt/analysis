---
ver: rpa2
title: Early Prediction of Multiple Sclerosis Disability Progression via Multimodal
  Foundation Model Benchmarks
arxiv_id: '2506.14986'
source_url: https://arxiv.org/abs/2506.14986
tags:
- data
- clinical
- floodlight
- multimodal
- digital
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of early prediction of disability
  progression in multiple sclerosis using sparse clinical data and dense digital biomarker
  time-series from the CONSONANCE trial. The core method employs state-of-the-art
  tabular and time-series foundation models (FMs), including a custom multimodal attention-based
  transformer that integrates clinical and Floodlight digital data, alongside machine
  learning baselines.
---

# Early Prediction of Multiple Sclerosis Disability Progression via Multimodal Foundation Model Benchmarks

## Quick Facts
- arXiv ID: 2506.14986
- Source URL: https://arxiv.org/abs/2506.14986
- Reference count: 12
- Primary result: AUROC of 0.63 achieved for 72-week disability progression prediction using multimodal transformer

## Executive Summary
This study tackles early prediction of multiple sclerosis disability progression using sparse clinical data and dense digital biomarker time-series from the CONSONANCE trial. The core method employs state-of-the-art tabular and time-series foundation models, including a custom multimodal attention-based transformer that integrates clinical and Floodlight digital data, alongside machine learning baselines. Using only 12 weeks of baseline data to predict 48- and 72-week disability progression, the best performance achieved was an AUROC of 0.63 (for 72-week prediction). The multimodal transformer consistently outperformed its unimodal counterpart, confirming the value of combining clinical and digital data. Foundation models, particularly Moment embeddings with a transformer, demonstrated superior performance compared to standard ML and time-series methods, highlighting their effectiveness in extracting predictive signals from noisy digital biomarkers.

## Method Summary
The study predicts 24-week composite Confirmed Disability Progression (cCDP) at 48 and 72 weeks using 12 weeks of baseline data from MS patients in the CONSONANCE trial (N=415 after filtering). Clinical data includes demographics, EDSS, functional tests, and MRI volumes, while digital data consists of 12 weeks of daily Floodlight smartphone tests. Preprocessing involves one-hot encoding, standard scaling, and patient-specific Gaussian Process interpolation for missing digital biomarker data. Models include AutoGluon/XGBoost/RF/LR baselines, sktime classifiers with grid search CV, TabPFN, Moment FM embeddings + transformer, GP-Transformer, and a custom multimodal transformer fusing GP-augmented time-series with projected clinical features. Evaluation uses 80/20 temporal train/test split with AUROC as primary metric.

## Key Results
- Achieved AUROC of 0.63 for 72-week disability progression prediction
- Multimodal transformer consistently outperformed unimodal counterpart, confirming value of combining clinical and digital data
- Foundation models, particularly Moment embeddings with transformer, demonstrated superior performance compared to standard ML and time-series methods

## Why This Works (Mechanism)

### Mechanism 1
Foundation model-derived embeddings capture predictive signals from noisy digital biomarker time-series more effectively than hand-engineered features. Pre-trained time-series foundation models (specifically Moment) learn generalizable temporal representations during large-scale pre-training, which transfer to the sparse, noisy Floodlight digital biomarker data. These embeddings preserve temporal dynamics that hand-crafted features may lose.

### Mechanism 2
Multimodal attention-based fusion of sparse clinical data with dense digital time-series improves prediction over either modality alone. The custom transformer projects static clinical features into the same embedding dimension as time-series data, concatenates them with positional encodings, and uses self-attention to learn cross-modal dependencies. The CLS token aggregates information across the fused sequence.

### Mechanism 3
Patient-specific Gaussian Process interpolation enables robust modeling of sparse, irregularly-sampled digital biomarker trajectories. GP models with RBF + White Noise kernels are fitted per-patient-per-feature, capturing individual temporal dynamics. Missing time-points are sampled from the posterior predictive distribution, providing uncertainty-aware interpolation and data augmentation.

## Foundational Learning

- **Concept: Foundation Models and Transfer Learning**
  - Why needed here: Understanding why pre-trained models (Moment, TabPFN) outperform task-specific architectures requires grasping how large-scale pre-training learns generalizable representations.
  - Quick check question: Can you explain why a model pre-trained on generic time-series data might outperform a model trained from scratch on MS digital biomarkers, despite domain mismatch?

- **Concept: Attention-Based Multimodal Fusion**
  - Why needed here: The custom transformer's fusion mechanism relies on cross-modal attention; understanding this clarifies why concatenation + self-attention enables signal integration.
  - Quick check question: How does the CLS token aggregation differ from simple feature concatenation, and why might it capture cross-modal dependencies better?

- **Concept: Gaussian Processes for Time-Series Interpolation**
  - Why needed here: The GP-based augmentation strategy is central to handling missing digital biomarker data; understanding kernel choice and uncertainty quantification is essential for debugging.
  - Quick check question: Why use a composite RBF + White Noise kernel rather than RBF alone, and what does the length-scale hyperparameter encode?

## Architecture Onboarding

- **Component map**: Input (GP-augmented Floodlight time-series, projected clinical features, CLS token) -> Projection head (clinical features to embedding dim) -> Embedding layer (positional encodings) -> Transformer encoder (multi-head self-attention) -> Classification head (MLP on CLS token) OR Alternative path (Moment FM embeddings -> transformer encoder)

- **Critical path**: Data preprocessing (clinical scaling, digital QC filtering) -> GP interpolation -> Moment embedding extraction OR direct GP-augmented sequence -> Clinical feature projection -> sequence concatenation -> Transformer forward pass -> CLS token extraction -> Binary classification

- **Design tradeoffs**: GP interpolation vs. raw data (smooths noise but may obscure sharp transitions); Moment embeddings vs. end-to-end training (computationally efficient but prevents fine-tuning); Multimodal fusion vs. unimodal FM (adds clinical context but introduces complexity)

- **Failure signatures**: AUROC ~0.5 (check label leakage, data split integrity); Multimodal < unimodal (cross-modal attention failing, verify projection dimensions); High train/test gap (overfitting, reduce model capacity); GP interpolation artifacts (visualize imputed trajectories, adjust kernel hyperparameters)

- **First 3 experiments**: Reproduce baseline comparison (XGBoost on clinical + engineered Floodlight features); Ablate modality contributions (multimodal transformer with clinical-only and digital-only inputs); Test embedding strategy (Moment embeddings vs. raw GP-augmented sequences)

## Open Questions the Paper Calls Out

### Open Question 1
Can the predictive utility of multimodal foundation models be validated on larger, multi-center datasets beyond the single CONSONANCE trial? The authors identify "reliance on the single CONSONANCE trial" as a limitation and explicitly state future research should "prioritize validation on larger, multi-center datasets."

### Open Question 2
Does the inclusion of biological modalities, such as MRI or omics data, significantly improve predictive performance over the clinical-digital baseline? The paper notes the limitation of "limited modalities (lacking imaging or genomics)" and suggests future work should "incorporate broader biological modalities."

### Open Question 3
Can self-supervised pre-training on large-scale mixed data repositories enable fusion architectures to outperform the best unimodal foundation models? The authors suggest "Exploring self-supervised pre-training on larger mixed clinical/digital health data repositories" is a key direction, noting that current fusion strategies did not surpass unimodal Moment embeddings.

## Limitations
- Single-trial cohort (CONSONANCE) limits generalizability to broader MS populations
- Narrow temporal window (12 weeks baseline predicting 48-72 week outcomes) may not capture full disease progression trajectories
- Modest AUROC performance ceiling (0.63) suggests fundamental limits in early MS progression prediction

## Confidence
- **High confidence**: Experimental methodology clearly specified and reproducible; finding that foundation models outperform traditional ML baselines well-supported
- **Medium confidence**: Multimodal transformer's superiority over unimodal variants demonstrated, but specific attention mechanisms not explored
- **Low confidence**: Claims about clinical implications and generalizability exceed what single-trial design can support

## Next Checks
1. **Ablation study on modality contributions**: Systematically disable clinical or digital inputs in multimodal transformer to quantify marginal value of each modality
2. **External validation on independent cohort**: Apply best-performing model to separate MS dataset to assess generalizability and identify overfitting
3. **Interpretability analysis of attention mechanisms**: Visualize attention weights to identify which clinical features and digital biomarker time-points drive predictions