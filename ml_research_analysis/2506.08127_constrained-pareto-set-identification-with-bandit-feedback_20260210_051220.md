---
ver: rpa2
title: Constrained Pareto Set Identification with Bandit Feedback
arxiv_id: '2506.08127'
source_url: https://arxiv.org/abs/2506.08127
tags:
- algorithm
- pareto
- arms
- feasible
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces constrained Pareto Set Identification (cPSI),
  a novel multi-objective bandit problem where the goal is to identify the Pareto
  Set of arms satisfying linear feasibility constraints. The authors propose e-cAPE,
  an algorithm using upper and lower confidence bounds to efficiently balance feasibility
  detection and Pareto Set identification.
---

# Constrained Pareto Set Identification with Bandit Feedback

## Quick Facts
- **arXiv ID:** 2506.08127
- **Source URL:** https://arxiv.org/abs/2506.08127
- **Reference count:** 40
- **Primary result:** Introduces e-cAPE algorithm for identifying constrained Pareto Sets with near-optimal sample complexity

## Executive Summary
This paper introduces constrained Pareto Set Identification (cPSI), a novel multi-objective bandit problem where the goal is to identify the Pareto Set of arms satisfying linear feasibility constraints. The authors propose e-cAPE, an algorithm using upper and lower confidence bounds to efficiently balance feasibility detection and Pareto Set identification. They prove e-cAPE is near-optimal, with sample complexity scaling as $C_M^*(\nu) \log(KC_M^*(\nu)/\delta)$, where $C_M^*$ is a problem-dependent complexity measure. Experiments on real-world datasets (clinical trials) show e-cAPE significantly outperforms baselines including two-stage approaches and racing algorithms, demonstrating superior empirical performance in identifying constrained Pareto Sets.

## Method Summary
The paper addresses fixed-confidence identification of the constrained Pareto Set (cPSI) in a multi-objective bandit setting. The method, e-cAPE, maintains empirical means and pull counts for each arm, computes feasibility gaps using distance to the constraint polyhedron, and evaluates Pareto dominance margins. A "Leader-Challenger" sampling mechanism selects two arms per round: the leader is the arm most critical to the stopping condition, while the challenger is the arm most likely to dominate the leader. The algorithm stops when two statistics $Z_1(t)$ and $Z_2(t)$ become non-negative, certifying the feasibility and optimality of the identified Pareto set.

## Key Results
- e-cAPE achieves near-optimal sample complexity scaling as $C_M^*(\nu) \log(KC_M^*(\nu)/\delta)$
- On clinical trial datasets, e-cAPE identifies safe, effective dosages with significantly fewer samples than uniform sampling
- The algorithm outperforms two-stage approaches and racing algorithms in empirical benchmarks
- e-cAPE demonstrates "arbitrary" efficiency gains over naive approaches in specific instances

## Why This Works (Mechanism)

### Mechanism 1: Greedy Leader-Challenger Sampling
The e-cAPE algorithm maximizes sample efficiency by prioritizing arms that are the current bottlenecks for stopping, specifically balancing feasibility verification against Pareto dominance checks. The algorithm maintains a "leader" $b_t$ defined as the arm most likely to violate the stopping condition (minimizer of $Z_2$ or $Z_1$). It then selects a "challenger" $c_t$ as the arm most likely to dominate $b_t$ (minimizer of $M^-_t$). By pulling both, it gathers information simultaneously on feasibility and relative sub-optimality, rather than resolving them sequentially. Core assumption: reward distributions are marginally $\sigma$-subGaussian and norm-$\sigma_u$-subGaussian.

### Mechanism 2: Unified Stopping Statistics ($Z_1$ and $Z_2$)
The algorithm guarantees correctness by enforcing a stopping condition that requires the simultaneous certification of the Pareto set's feasibility and the classification of all other arms. Two statistics are monitored: $Z_1(t)$ ensures arms in the empirical Pareto set are feasible and non-dominated; $Z_2(t)$ ensures every arm outside this set is confidently either dominated or infeasible. The algorithm stops only when $\min(Z_1, Z_2) \geq 0$, ensuring the output partition is valid under the event $E$ (where confidence bounds hold). Core assumption: the polyhedron $P$ is known and fixed, and a unique true Pareto set $O^*$ exists.

### Mechanism 3: Exploiting "Cheap" Explanations for Sample Complexity
e-cAPE achieves near-optimal sample complexity by paying the "cheapest" information cost for rejecting arms, specifically leveraging the overlap between dominance and infeasibility. In the "two-stage" baseline, an arm must be fully verified as infeasible before being ignored. e-cAPE allows an arm to be classified as "dominated" even if it is also infeasible. This avoids expensive feasibility checks (which scale with distance to the constraint boundary) for arms that are easily proven suboptimal via dominance. Core assumption: the constraints are linear ($Ax \leq b$), allowing efficient projection and distance calculation.

## Foundational Learning

- **Concept:** **Pareto Dominance & Front**
  - **Why needed here:** The core objective is not to find a single "best" arm but a set of arms representing optimal trade-offs. You must understand that an arm $i$ dominates $j$ if it is better in *all* objectives.
  - **Quick check question:** If Arm A has rewards (10, 5) and Arm B has (12, 3), does A dominate B?

- **Concept:** **Fixed-Confidence Pure Exploration**
  - **Why needed here:** Unlike standard bandits (regret minimization), this setting allows unlimited exploration to output a correct answer with probability $1-\delta$. The constraint is sample efficiency, not balancing exploration/exploitation.
  - **Quick check question:** Does the algorithm aim to minimize the total number of pulls or the total regret over time?

- **Concept:** **Sub-Gaussian Concentration**
  - **Why needed here:** The algorithm relies on confidence bounds ($\beta_i, U_i$) to create "high probability" events. Understanding how quickly empirical means converge to true means determines the aggressiveness of the stopping rule.
  - **Quick check question:** How does the confidence bonus radius change as the number of pulls $N_t$ increases?

## Architecture Onboarding

- **Component map:** Input -> State Tracker -> Feasibility Engine -> Pareto Engine -> Decision Module -> Stopper
- **Critical path:**
  1. **Initialization:** Pull each arm once.
  2. **Update:** Calculate empirical means and Pareto set $O_t$.
  3. **Check:** If $Z_1 \geq 0$ and $Z_2 \geq 0$, terminate and return partition $(O_t, S_t, I_t)$.
  4. **Sample:** Select Leader $b_t$ (minimizer of bottleneck) and Challenger $c_t$ (closest competitor).
  5. **Pull:** Sample $b_t$ and $c_t$, update counts, and repeat.

- **Design tradeoffs:**
  - **Explainability vs. Complexity:** The e-cPSI objective (classifying all arms) provides full transparency for applications like clinical trials but introduces higher algorithmic complexity compared to simple Pareto identification.
  - **Tight vs. Loose Bounds:** The paper notes that using tighter confidence bounds (Law of Iterated Logarithm) improves empirical performance but sacrifices theoretical guarantees on expected stopping time.

- **Failure signatures:**
  - **Stalling:** If arms are extremely close to the constraint boundary ($\eta \approx 0$) or very similar ($\Delta \approx 0$), the algorithm may run indefinitely.
  - **Misclassification:** If $\delta$ is set too high or variance is underestimated, the confidence bounds break, potentially outputting an infeasible arm as optimal.

- **First 3 experiments:**
  1. **Baseline Efficiency:** Compare e-cAPE against the "Two-Stage" baseline on a synthetic instance where suboptimal arms lie near the feasibility boundary (e.g., Figure 1 scenario) to verify the claimed arbitrary efficiency gains.
  2. **Gap Sensitivity:** Vary the feasibility margin $\eta$ and dominance gap $\Delta$ on a synthetic grid to empirically validate the $C^*_M(\nu)$ complexity scaling.
  3. **Real-world Validation:** Run the algorithm on the provided Secukinumab clinical trial dataset (Figure 2) to ensure it correctly identifies the safe, effective dosages with fewer samples than uniform sampling.

## Open Questions the Paper Calls Out

- **Question 1:** Can adaptive strategies be designed to allow controlled constraint violations during the exploration phase rather than strictly enforcing them?
  - **Basis:** The Conclusion states that an exciting avenue for future research is balancing exploration with safety constraints (e.g., in clinical trials), as current work only ensures constraints in the final recommendation.
  - **Why unresolved:** The proposed algorithms focus on identifying the feasible set without exploring the trade-off between temporary violations and sample efficiency.
  - **What evidence would resolve it:** An algorithm analysis showing improved sample complexity bounds under a specific budget of constraint violations.

- **Question 2:** Why did the e-cAPE algorithm outperform the asymptotically optimal Game-cPSI algorithm in the moderate confidence regime?
  - **Basis:** Appendix G.2 reports that e-cAPE surprisingly achieved lower sample complexity than Game-cPSI on a specific instance with $\delta=0.01$, noting this phenomenon requires a larger benchmark to understand.
  - **Why unresolved:** Theoretical guarantees for Game-cPSI are asymptotic ($\delta \to 0$), and its performance in the moderate confidence regime (fixed $\delta$) is theoretically uncharacterized.
  - **What evidence would resolve it:** A theoretical analysis of Game-cPSI for fixed $\delta$ or an empirical study across a wider variety of instances.

- **Question 3:** Is the e-cAPE algorithm near-optimal for general bandit instances, or is its near-optimality restricted to the specific instance class $e\mathcal{D}$?
  - **Basis:** Theorem 4.5 establishes near-optimality only for a specific class of instances, while the gap between the general lower bound (Proposition 3.4) and the e-cAPE upper bound remains open for arbitrary instances.
  - **Why unresolved:** The paper does not prove that the sample complexity of e-cAPE scales with $T^*_M(\nu)$ for all possible mean configurations.
  - **What evidence would resolve it:** A proof extending the lower bound matching to general instances, or a counter-example showing a strict separation in complexity.

## Limitations

- The algorithm's correctness depends critically on sub-Gaussian reward distributions and fixed linear constraints; violations may break theoretical guarantees.
- The computational complexity of solving feasibility distance calculations for general polyhedra is not fully analyzed, potentially becoming intractable for high-dimensional or complex constraint sets.
- The assumption of a unique true Pareto set may not hold in practice when arms have identical mean vectors.

## Confidence

- **High Confidence:** Algorithm's correctness proof and sample complexity bound are well-established under stated assumptions; experimental results on real-world clinical trial data are reproducible.
- **Medium Confidence:** Theoretical advantage over baselines is proven, but practical significance depends on specific problem instances; choice of experimental confidence bounds may affect result reliability.
- **Low Confidence:** Scalability of feasibility module for general polyhedra and algorithm behavior under non-subGaussian reward distributions are not thoroughly validated.

## Next Checks

1. **Stress Test Non-SubGaussianity:** Run e-cAPE on a synthetic bandit instance with heavy-tailed reward distributions to verify the algorithm's robustness to violated assumptions.
2. **Scalability Analysis:** Evaluate the computational overhead of the feasibility distance calculation on high-dimensional polyhedra (e.g., d=50) and compare against theoretical predictions.
3. **Real-World Edge Cases:** Apply e-cAPE to a clinical trial dataset with arms having very similar efficacy profiles to test the algorithm's ability to handle near-ties and verify the stopping condition's reliability.