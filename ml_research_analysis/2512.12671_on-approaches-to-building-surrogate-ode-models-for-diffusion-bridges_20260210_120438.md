---
ver: rpa2
title: On Approaches to Building Surrogate ODE Models for Diffusion Bridges
arxiv_id: '2512.12671'
source_url: https://arxiv.org/abs/2512.12671
tags:
- diffusion
- sindy-fm
- bridge
- gaussian
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to building surrogate ODE
  models for diffusion bridges, addressing the computational cost and complexity of
  training these models. The proposed methods leverage sparse regression and Neural
  ODEs to create interpretable, efficient approximations of the underlying dynamics.
---

# On Approaches to Building Surrogate ODE Models for Diffusion Bridges

## Quick Facts
- arXiv ID: 2512.12671
- Source URL: https://arxiv.org/abs/2512.12671
- Reference count: 5
- One-line primary result: Novel surrogate ODE methods achieve orders-of-magnitude efficiency gains while preserving transport quality in diffusion bridge modeling.

## Executive Summary
This paper addresses the computational bottleneck of sequential sampling in diffusion bridge models by introducing surrogate ODE approaches. The authors propose two methods: SINDy Flow Matching for interpretable symbolic models and DSBM-NeuralODE for flexible continuous-time parameterizations. Both approaches learn surrogate dynamics through supervised regression on trajectory data rather than iterative denoising, achieving dramatic improvements in inference speed while maintaining transport quality. The methods are evaluated on Gaussian transport tasks and MNIST latent space translation, demonstrating competitive performance with significantly reduced parameter counts.

## Method Summary
The paper introduces two complementary approaches for building surrogate ODE models of diffusion bridges. SINDy Flow Matching uses sparse regression to identify symbolic differential equations from flow-matching supervision, parameterizing drift as affine functions of state and time to preserve Gaussianity. The method leverages the SR3 optimizer with polynomial libraries to enforce sparsity. DSBM-NeuralODE provides a flexible alternative using pre-trained neural ODE drift functions, training separate forward/backward networks on diffusion trajectories before iterative Markovian fitting. Both methods bypass the sequential sampling bottleneck of standard diffusion models by learning compact vector field representations that can be integrated at inference.

## Key Results
- SINDy-FM achieves W2=0.172 with only 25 parameters versus DSBM-NeuralODE's 2.7×10⁵ parameters
- Symbolic models enable near-instantaneous inference (microseconds vs milliseconds)
- Both methods maintain competitive transport quality on Gaussian and MNIST latent translation tasks
- Polynomial order R=2 with threshold 0.10 provides optimal accuracy-efficiency tradeoff

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Surrogate ODE models can approximate diffusion bridge dynamics with orders-of-magnitude fewer parameters while preserving transport quality.
- Mechanism: The bridge dynamics are learned through supervised regression on trajectory data rather than iterative denoising. By sampling interpolation states x(t) = γ(t; x₀, x₁) and their exact time derivatives ẋ(t), the method bypasses numerical differentiation noise and provides clean supervision targets for fitting compact vector fields.
- Core assumption: The underlying transport dynamics can be adequately represented by the chosen function class (sparse polynomial library for SINDy-FM, or neural network for DSBM-NeuralODE).
- Evidence anchors:
  - [abstract]: "The symbolic SINDy-FM models reduce parameter counts by several orders of magnitude and enable near-instantaneous inference"
  - [section 4.1.1, Table 1]: SINDy-FM achieves W2=0.172 with 25 parameters vs DSBM-NeuralODE's 2.7×10⁵ parameters
  - [corpus]: Limited direct corpus support; neighbor papers focus on bridge formulations rather than surrogate compression
- Break condition: When target dynamics are highly non-polynomial or exhibit discontinuities not captured by the symbolic library, surrogate expressivity becomes insufficient.

### Mechanism 2
- Claim: Sparse regression on flow-matching supervision yields interpretable, symbolic ODEs that preserve Gaussianity along transport paths.
- Mechanism: SINDy-FM parameterizes drift as v(x,t) = K(t)x + k(t) where K(t) = ∑ᵣ₌₀ᴿ Aᵣ tʳ and k(t) = ∑ᵣ₌₀ᴿ bᵣ tʳ. This affine form preserves Gaussianity (since ẋₘᵤₜ = K(t)μₜ + k(t) and Σ̇ₜ = K(t)Σₜ + ΣₜK(t)ᵀ). SR3 optimization with thresholding enforces sparsity.
- Core assumption: The optimal transport between Gaussian endpoints is itself affine (supported by Theorem 3 in Lipman et al. [2023] cited in paper).
- Evidence anchors:
  - [section 3.1]: "An affine drift preserves Gaussianity along the flow... for Gaussian endpoints the population FM minimizer is itself affine"
  - [section 4.1.1]: Polynomial order R=2 with threshold 0.10 achieves W2≈0.17 across mean scales
  - [corpus]: Related work "CFO: Learning Continuous-Time PDE Dynamics" uses flow-matched operators but targets PDEs rather than probability transport
- Break condition: For non-Gaussian marginals or highly multimodal targets, the affine assumption breaks; richer libraries (quadratic in x for MNIST) become necessary.

### Mechanism 3
- Claim: Pre-training Neural ODE drift functions on diffusion trajectories provides stable initialization for Schrödinger Bridge optimization.
- Mechanism: DSBM-NeuralODE trains separate forward/backward networks fθforward, fϕbackward using regression losses on consecutive diffusion states. This warm-starts the iterative Markovian fitting procedure, reducing error accumulation compared to random initialization.
- Core assumption: The reference diffusion dynamics share structure with the optimal bridge dynamics, making pre-training informative.
- Evidence anchors:
  - [section 3.2]: "This pre-training initializes the networks to approximate the dynamics of the reference process, providing a warm start"
  - [section 4.1.1, Figure 1]: Convergence plots show stable mean/covariance estimation over ~60 iterations
  - [corpus]: "Weight-Parameterization in Continuous Time Deep Neural Networks" addresses stable time-varying weight learning in Neural ODEs—relevant but not directly evaluated
- Break condition: When reference diffusion and target bridge differ significantly (e.g., different noise scales, non-Brownian references), pre-training may bias toward suboptimal solutions.

## Foundational Learning

- **Concept: Diffusion Probabilistic Models (DDPM)**
  - Why needed here: The paper builds on DDPM as the baseline generative process and uses trained DDPM U-Nets to generate reference trajectories for DSBM-NeuralODE pre-training.
  - Quick check question: Can you explain how the forward diffusion process q(xₜ|xₜ₋₁) constructs noisy samples, and why the reverse process requires learning εθ(xₜ, t)?

- **Concept: Schrödinger Bridge Problem**
  - Why needed here: The core objective is finding a stochastic process PSB minimizing KL divergence to a reference measure while matching endpoint marginals—the theoretical foundation both methods approximate.
  - Quick check question: What constraints define the Schrödinger Bridge, and why does IPF suffer from error accumulation in high dimensions?

- **Concept: Sparse Identification of Nonlinear Dynamics (SINDy)**
  - Why needed here: SINDy-FM directly applies SINDy's sparse regression framework to identify drift coefficients from flow-matching supervision.
  - Quick check question: How does SINDy balance fit accuracy against sparsity, and what role does the threshold parameter play in coefficient selection?

## Architecture Onboarding

- **Component map**:
  [Source Distribution p₀] → [Endpoint Sampling] → [Interpolation γ(t;x₀,x₁)]
          ↓                                              ↓
  [Target Distribution p₁]                    [Derivative Computation ∂ₜγ]
          ↓                                              ↓
  [SINDy-FM Path]              [DSBM-NeuralODE Path]
          ↓                              ↓
  [Feature Library Ξ(x,t)]     [Neural ODE ODEFunc_θ]
          ↓                              ↓
  [Sparse Regression (SR3)]    [Pre-training on DDPM trajectories]
          ↓                              ↓
  [Symbolic Drift v_θ]         [Iterative Markovian Fitting]
          ↓                              ↓
  [ODE Solver (inference)]     [Euler-Maruyama Sampler]

- **Critical path**: For SINDy-FM, the choice of polynomial library (degree in x, order in t) and sparsity threshold directly determines the accuracy-efficiency tradeoff. For DSBM-NeuralODE, pre-training quality and diffusion strength σ govern convergence stability.

- **Design tradeoffs**:
  - SINDy-FM: Faster inference (microseconds) and interpretable, but library-bound expressivity
  - DSBM-NeuralODE: Flexible nonlinear dynamics, but 100× slower training and black-box
  - Both avoid the sequential sampling bottleneck of standard diffusion but require solving ODEs at inference

- **Failure signatures**:
  - SINDy-FM: W2 divergence increases with dimension if library is too simple; sparsity threshold too aggressive → underfitting
  - DSBM-NeuralODE: Training loss plateaus without convergence if σ mismatches reference; instability in high-condition covariance scenarios (Table 2 shows DSBM W2=1.9 vs SINDy-FM 0.233)

- **First 3 experiments**:
  1. **Gaussian identity transport (dim=5)**: Replicate Table 1 baseline with SINDy-FM using affine drift, polynomial R=2, threshold=0.10. Validate W2 < 0.2 and inference time < 10μs.
  2. **Library ablation on rotated covariance**: Test whether quadratic terms in x improve over affine-only library for non-diagonal Σ. Compare W2 and active parameter count.
  3. **MNIST latent translation with varying sparsity**: Sweep threshold ∈ {0.02, 0.05, 0.10} and ν ∈ {0.05, 0.1}. Monitor digit classification accuracy and FID to identify quality-complexity Pareto frontier.

## Open Questions the Paper Calls Out
None

## Limitations
- The methods are primarily validated on Gaussian endpoints and 8D latent spaces, leaving scalability to high-dimensional non-Gaussian problems uncertain
- SR3 optimizer hyperparameters and convergence criteria are not fully specified, potentially affecting reproducibility
- The polynomial library approach may struggle with highly non-polynomial dynamics or multimodal target distributions

## Confidence
- **High confidence**: Claims about Gaussianity preservation under affine drift and fundamental computational bottleneck of sequential DDPM sampling
- **Medium confidence**: Claims about surrogate efficiency gains and competitive transport quality, supported by quantitative comparisons but limited by sparse corpus validation
- **Low confidence**: Scalability claims to high-dimensional non-Gaussian problems, as MNIST experiments use only 8D latent space and polynomial library adequacy remains untested

## Next Checks
1. **Library expressivity test**: Systematically vary polynomial order R and basis richness Φ(x) on Gaussian transport with diagonal vs. high-condition covariances to quantify the W2-parameter count tradeoff.
2. **High-dimensional scaling study**: Extend MNIST experiments to full 784D pixel space (or ImageNet latents) and measure both transport quality and inference speed relative to standard DDPM.
3. **Non-Gaussian robustness**: Replace Gaussian endpoints with mixture-of-Gaussians or real image datasets to stress-test whether sparse regression and pre-training strategies remain effective outside the theoretically supported regime.