---
ver: rpa2
title: 'Temporal Distribution Shift in Real-World Pharmaceutical Data: Implications
  for Uncertainty Quantification in QSAR Models'
arxiv_id: '2502.03982'
source_url: https://arxiv.org/abs/2502.03982
tags:
- uncertainty
- assays
- data
- temporal
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates uncertainty quantification in QSAR models
  trained on real-world pharmaceutical data, addressing the challenge of distribution
  shifts over time. Using a temporal splitting strategy on 15 diverse biological assays,
  the authors analyze shifts in label and descriptor space and their impact on model
  performance.
---

# Temporal Distribution Shift in Real-World Pharmaceutical Data: Implications for Uncertainty Quantification in QSAR Models

## Quick Facts
- **arXiv ID**: 2502.03982
- **Source URL**: https://arxiv.org/abs/2502.03982
- **Reference count**: 40
- **Primary result**: Deep ensembles and Bayesian networks show better calibration for assays with small temporal shifts, but fail to improve baseline performance for assays with large distribution shifts.

## Executive Summary
This study investigates uncertainty quantification in QSAR models trained on real-world pharmaceutical data with temporal distribution shifts. Using a temporal splitting strategy on 15 diverse biological assays, the authors analyze shifts in label and descriptor space and their impact on model performance. They compare uncertainty estimation methods including deep ensembles, Monte Carlo dropout, and Bayesian neural networks against baseline models. The results reveal significant temporal distribution shifts, particularly in target-based assays, which impair the performance of popular uncertainty quantification methods. While deep ensembles and Bayesian networks show superior calibration for assays with small shifts, they fail to improve baseline model performance for assays with large distribution shifts.

## Method Summary
The study uses ECFP fingerprints (4096 bits) from SMILES via RDKit and temporal splits into 5 equal folds based on measurement dates. Three training configurations use 1/2/3 folds for training, with subsequent fold for validation and next for test. Baseline models include RF and MLP, with uncertainty methods using deep ensembles (25 members), MC dropout (400 passes), and BNN via Bayes-by-Backrop. Post-hoc calibration via Platt scaling and Venn-ABERS is applied. Hyperparameter tuning uses grid search, with BCE loss for model selection and early stopping. Evaluation uses AUC, BCE, and ACE across 10 repetitions with statistical significance via two-sided t-test.

## Key Results
- Deep ensembles and Bayesian networks provide better-calibrated predictions for assays with smaller temporal distribution shifts
- Pronounced distribution shifts impair the performance of popular uncertainty estimation methods
- Label space shifts pose greater challenges for UQ methods than descriptor space shifts in classification tasks
- Post-hoc calibration methods degrade or reverse improvements when MMD between calibration and test sets exceeds 0.015

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Deep ensembles provide better-calibrated predictions for assays with small temporal distribution shifts.
- **Mechanism**: Multiple randomly initialized neural networks converge to different local minima in the loss landscape; averaging their predictions naturally captures model variance (epistemic uncertainty) that increases when test instances differ from training data.
- **Core assumption**: Test instances remain within or near the training distribution support (low MMD in descriptor space, stable label ratios).
- **Evidence anchors**:
  - [abstract] "deep ensembles and Bayesian networks generally provide better-calibrated predictions for assays with smaller distribution shifts"
  - [section 3.2] "The MLPE and BNN models achieve the best results across both metrics in the same number of assays, namely in 4 out of 8 cases" for ADME-T assays with moderate shifts
  - [corpus] Related work confirms ensemble methods outperform approximate Bayesian approaches under descriptor-space distribution shifts (arXiv:1906.10225, cited in paper)
- **Break condition**: Large label space shifts (e.g., preferred class ratio changing >30% between time spans) cause deep ensembles to match but not exceed baseline MLP performance.

### Mechanism 2
- **Claim**: Post-hoc calibration methods improve probability estimates only when calibration and test sets share similar distributions.
- **Mechanism**: Platt scaling and Venn-ABERS fit calibration functions (logistic or isotonic regression) to map classifier scores to calibrated probabilities; these monotonic transformations cannot correct for systematic distribution mismatch.
- **Core assumption**: The calibration set is drawn from the same distribution as the test set (i.i.d. between calibration and test).
- **Evidence anchors**:
  - [abstract] "pronounced distribution shifts impair the performance of popular uncertainty estimation methods"
  - [section 3.2, Figure 8] "With increasing MMD between calibration and test set, the calibrating abilities of the post hoc calibration methods decrease"
  - [corpus] Direct corpus support for temporal calibration degradation is limited; related work on distribution shift calibration exists (arXiv:1906.02530, referenced as [41])
- **Break condition**: High MMD (>0.02 in this study's Tanimoto-kernel formulation) between calibration and test sets.

### Mechanism 3
- **Claim**: Label space shifts pose greater challenges for UQ methods than descriptor space shifts in classification tasks.
- **Mechanism**: Binary classification thresholds collapse continuous target values into discrete classes, losing information that could help models adapt; strongly inactive and weakly inactive compounds appear identical, preventing the model from learning the direction of shift.
- **Core assumption**: Regression models may better handle label shifts because they retain continuous target information (hypothetical, inferred from paper's discussion of Svensson et al. [49]).
- **Evidence anchors**:
  - [abstract] "distribution shifts in label space pose a particular challenge"
  - [section 3.2] "The uncertainty quantification methods consider epistemic uncertainty... the inability of the uncertainty estimation approaches to produce well-calibrated uncertainties for TB assays might result from the shifts in label space rather than shifts in descriptor space"
  - [corpus] Label-free performance estimation under distribution shifts is emerging as a research direction (arXiv:2507.22776)
- **Break condition**: TB assays with preferred class ratio differences >0.2 between training and test time spans.

## Foundational Learning

- **Maximum Mean Discrepancy (MMD)**
  - Why needed here: Quantifies distribution shift in descriptor space between training and test sets using kernel-based distance; central to the paper's analysis of when UQ methods fail.
  - Quick check question: Given two sets of ECFP fingerprints, what does MMD=0 vs MMD=0.1 indicate about their overlap?

- **Adaptive Calibration Error (ACE)**
  - Why needed here: Primary metric for evaluating probability calibration; uses equally-sized bins to avoid bias from class imbalance that affects Expected Calibration Error.
  - Quick check question: Why is ACE preferred over ECE for imbalanced pharmaceutical datasets?

- **Epistemic vs. Aleatoric Uncertainty**
  - Why needed here: Deep ensembles and BNNs specifically target epistemic uncertainty (model uncertainty); understanding this distinction explains why they fail under label shift—they cannot correct fundamental domain mismatch.
  - Quick check question: Which uncertainty type does MC dropout capture, and why might it underperform compared to deep ensembles?

## Architecture Onboarding

**Component map**:
Raw assay data -> Temporal split (5 folds) -> ECFP fingerprints (4096 bits) -> [RF baseline] or [MLP baseline] -> [MLPE: 25 NNs] or [MLPMC: 400 passes] or [BNN: Bayes-by-Backprop] -> [Platt scaling] or [Venn-ABERS] (optional) -> Evaluation: AUC, BCE, ACE (10 repetitions)

**Critical path**: Temporal splitting is the foundation—using measurement dates to create folds, then training on folds 1-N, validating on N+1, testing on N+2. This simulates prospective prediction.

**Design tradeoffs**:
- MLPE (25 ensembles): Best calibration, highest compute cost (~25x training)
- BNN: Good calibration, moderate compute, requires tuning prior variance
- Simple MLP: Often matches MLPE on TB assays with large shifts at fraction of cost
- Post-hoc calibration: Adds value only when calibration-test MMD is low

**Failure signatures**:
- TB assays (project-specific) show ACE values 2-3x higher than ADME-T assays
- BNN occasionally fails catastrophically (e.g., TB-2: AUC 0.289)—variational approximation instability
- Post-hoc calibration degrades or reverses improvements when MMD(calibration, test) > 0.015

**First 3 experiments**:
1. Replicate the temporal split analysis on a single ADME-T assay to understand label/descriptor shift patterns before scaling to all 15.
2. Compare MLPE vs. MLP baseline on a high-shift TB assay to verify that additional ensemble complexity provides no calibration benefit.
3. Measure MMD between your calibration and test sets before applying post-hoc calibration—if MMD > 0.015, skip post-hoc methods.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do graph neural networks (GNNs) or other advanced molecular representations offer improved robustness against temporal distribution shifts compared to the ECFP-MLP baseline?
- **Basis in paper:** [inferred] The authors note in the Methods section that they "opted for the simple ECFP representation" despite the existence of "more sophisticated options... like graph neural networks," leaving the impact of architecture on shift robustness untested.
- **Why unresolved:** The study intentionally restricted the model complexity to standard baselines; therefore, it is unknown if architectures that better capture molecular topology would mitigate the calibration errors observed under temporal drift.
- **What evidence would resolve it:** A comparative study replicating the temporal splits using GNNs or transformers to see if they maintain lower Adaptive Calibration Error (ACE) than the MLP baselines under high shift conditions.

### Open Question 2
- **Question:** Does the failure of uncertainty quantification (UQ) methods in Target-Based (TB) assays stem primarily from label shift rather than covariate shift?
- **Basis in paper:** [explicit] The authors conclude that the "inability of the uncertainty estimation approaches to produce well-calibrated uncertainties for TB assays might result from the shifts in label space rather than shifts in descriptor space," but they do not test this hypothesis directly.
- **Why unresolved:** While the paper correlates large label shifts in TB assays with poor UQ performance, it does not perform an ablation study to isolate label shift from covariate shift as the specific failure mode.
- **What evidence would resolve it:** Experiments using semi-synthetic data or label-shift adaptation techniques to decouple the effects of label distribution changes from descriptor space changes.

### Open Question 3
- **Question:** Can preserving the regression output (magnitude) rather than applying binary thresholds improve the reliability of uncertainty estimates under temporal shift?
- **Basis in paper:** [explicit] The authors hypothesize that regression models might be "less sensitive to shifts in the label space" because "information is lost in classification tasks due to the application of binary classification thresholds."
- **Why unresolved:** The study was limited to binary classification; the potential for regression or ordinal classification to retain the informational granularity necessary for better calibration was not empirically validated in this context.
- **What evidence would resolve it:** Training regression models on the same temporal data and comparing the calibration of the derived probabilities against the binary classifiers.

## Limitations

- The study uses proprietary AstraZeneca internal data, preventing independent validation of temporal splitting and distribution shift quantification.
- The BNN implementation uses a "fixed variance" prior without specification, which could significantly impact results if tuned differently.
- The generalizability to other pharmaceutical datasets or different molecular featurization methods remains untested.

## Confidence

- **High confidence**: The core finding that deep ensembles and BNNs provide better calibration for assays with small temporal shifts is well-supported by multiple metrics (AUC, BCE, ACE) across 10 repetitions.
- **Medium confidence**: The claim that label space shifts pose greater challenges than descriptor shifts is inferred from patterns in TB assays but would benefit from regression task comparisons.
- **Low confidence**: The assertion that post-hoc calibration methods fail above MMD=0.015 is based on observed patterns but lacks theoretical backing for this specific threshold.

## Next Checks

1. Test whether the label-shift challenge persists when using continuous regression targets instead of binary classification on the same assays.
2. Verify if simpler ensemble methods (e.g., 5-10 members instead of 25) achieve similar calibration performance at lower computational cost.
3. Implement alternative MMD kernel functions (e.g., Gaussian vs. Tanimoto) to confirm that the observed distribution shift patterns are not kernel-dependent artifacts.