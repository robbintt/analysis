---
ver: rpa2
title: 'Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making under
  Epistemic Uncertainty'
arxiv_id: '2508.08992'
source_url: https://arxiv.org/abs/2508.08992
tags:
- markers
- epistemic
- probability
- llms
- decision-making
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study reveals that Prospect Theory (PT), a human decision-making
  model, does not consistently explain LLM decision behavior under epistemic uncertainty.
  Through a three-stage experiment combining economic lotteries with epistemic markers,
  the authors find that PT parameters (risk preference, loss aversion, probability
  weighting) vary significantly across models and become unstable when numerical probabilities
  are replaced with linguistic uncertainty expressions.
---

# Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making under Epistemic Uncertainty

## Quick Facts
- arXiv ID: 2508.08992
- Source URL: https://arxiv.org/abs/2508.08992
- Reference count: 18
- Prospect Theory fails to consistently explain LLM decision-making under epistemic uncertainty, with parameters becoming unstable when linguistic uncertainty replaces numeric probabilities

## Executive Summary
This study investigates whether Prospect Theory, a cornerstone model of human decision-making under uncertainty, applies to Large Language Models (LLMs). Through a three-stage experimental framework, the authors find that PT parameters vary significantly across models and become highly unstable when numeric probabilities are replaced with linguistic epistemic markers (e.g., "likely," "maybe"). While larger models (14B+ parameters) show somewhat more stable PT-aligned behavior, all models exhibit divergent interpretations of epistemic markers, undermining their reliability for decision-making applications. The research demonstrates that current LLMs process epistemic uncertainty differently from humans, challenging the direct application of human cognitive models to AI systems.

## Method Summary
The study employs a three-stage experimental framework to evaluate PT's applicability to LLMs. Stage 1 uses Tanaka et al.'s lottery questionnaires (35 binary choices) to estimate PT parameters (risk preference σ, loss aversion λ, probability weighting γ) via Maximum Likelihood Estimation. Stage 2 maps 14 epistemic markers to numeric probabilities by identifying indifference points between numeric and marker-based lotteries. Stage 3 perturbs the original lotteries by replacing numeric probabilities with their mapped markers and re-measures PT parameters. The methodology tests six models across different scales (7B-32B parameters) using temperature=0.7 sampling to capture behavioral diversity, with PT fit evaluated via McFadden R² and MAE thresholds.

## Key Results
- PT parameters (σ, λ, γ) vary significantly across models, with smaller models showing particularly unstable behavior under epistemic uncertainty
- Only models with 14B+ parameters demonstrate somewhat stable PT-aligned behavior when processing epistemic markers
- Epistemic markers are mapped to vastly different probability ranges across models (e.g., "somewhat likely" mapped to 8.93% in one model vs 48.98% in another)
- PT fails to explain LLM decisions when McFadden R² < 0.10 or MAE > 0.20, occurring frequently with smaller models

## Why This Works (Mechanism)

### Mechanism 1
The explanatory power of Prospect Theory for LLMs depends on stable utility curvature mapping. The system uses MLE to fit non-linear value functions (σ, λ) and probability weighting (γ) to binary lottery choices. Low McFadden R² values (< 0.10) or high MAE (> 0.20) indicate the mechanism fails, suggesting models either act randomly or violate PT assumptions.

### Mechanism 2
Replacing numeric probabilities with epistemic markers introduces semantic-probability mapping errors. The model's interpretation of linguistic uncertainty acts as a scalar probability value. Non-monotonic choice curves break this mapping mechanism entirely, rendering markers semantically ambiguous to the model.

### Mechanism 3
Linguistic uncertainty perturbs decision parameter stability, causing probability distinction "compression" and erratic risk aversion shifts. Smaller models lose fine-grained probability distinctions while larger models maintain better stability. Parameter instability (λ hitting boundaries or inverting risk preferences) indicates PT framework breakdown.

## Foundational Learning

- **Concept: Prospect Theory (PT) Parameters**
  - Why needed: Evaluation framework relies on interpreting model behavior through σ (risk curvature), λ (loss sensitivity), and γ (probability distortion)
  - Quick check: If λ = 0.01, does the model treat losses as barely noticeable or extremely painful?

- **Concept: McFadden Pseudo-$R^2$**
  - Why needed: Validates if PT model explains LLM behavior by measuring improvement over random guessing
  - Quick check: Why is R² < 0.10 more damning than "irrational" choices? (Answer: Theory has no predictive power over outputs)

- **Concept: Epistemic Uncertainty (Linguistic)**
  - Why needed: Distinguishes between numeric risk and knowledge-based uncertainty; failure mode is language-to-confidence translation
  - Quick check: Why might "highly unlikely" map to 10% in one context but 30% in another?

## Architecture Onboarding

- **Component map:** Lottery Generator (35 questions) -> Model Inference (Temp=0.7, 256 samples) -> MLE Optimizer -> Outputs (σ, λ, γ, R²)
- **Critical path:** The Switching Point Detector is critical; non-monotonic choice curves make marker mappings invalid
- **Design tradeoffs:** Temperature=0.7 captures behavioral diversity but increases MLE noise; "DO NOT REASON" constraint isolates gut-feel decisions
- **Failure signatures:** Collapsed variance (markers mapped to narrow 5% range), oscillating curves (non-monotonic selection), boundary λ values (0.01 or 4.00)
- **First 3 experiments:**
  1. Run Stage 1 with temperature=0 vs 0.7 to verify deterministic rationality vs PT bias
  2. Test Stage 2 with ambiguous ("it's complex") vs certain ("guaranteed") markers for edge case handling
  3. Introduce mixed markers ("90% chance" vs "likely") in Stage 3 to test linguistic token disruption

## Open Questions the Paper Calls Out

### Open Question 1
What are the mechanistic causes of PT parameter instability in LLMs when processing epistemic uncertainty markers? [explicit] The authors frame their work as diagnostic rather than mechanistic; no universal pattern observed in PT parameter changes.

### Open Question 2
Can alternative decision theories beyond Prospect Theory better capture LLM decision-making under uncertainty? [explicit] Current work only evaluates PT fit; LLMs may require fundamentally different theoretical frameworks.

### Open Question 3
How does context affect probability mapping of epistemic markers in LLMs, and can context-independent calibration be achieved? [inferred] Epistemic markers are context-sensitive; mapping was derived from one task type and applied to another without validation.

### Open Question 4
Does scale guarantee human-aligned probability interpretations of epistemic markers, and what architectural factors beyond parameter count influence calibration? [explicit] Smaller models sometimes performed comparably; relationship between scale, architecture, and epistemic understanding remains unclear.

## Limitations

- Temperature=0.7 setting introduces significant noise that may mask underlying patterns or amplify apparent instability
- Marker mapping methodology assumes linear interpolation, which breaks down for non-monotonic choice curves
- Sample size (256 per lottery) may be insufficient for precise parameter estimation given LLM stochasticity
- "DO NOT REASON" constraint may not reflect realistic deployment scenarios where models can deliberate

## Confidence

- **High confidence:** PT parameters show high variability across models and become unstable under epistemic uncertainty; larger models demonstrate more stable PT-aligned behavior
- **Medium confidence:** Compression of probability distinctions and loss aversion boundary violations directly result from epistemic markers
- **Low confidence:** Specific interpretation of individual markers is consistent and generalizable across contexts

## Next Checks

1. **Deterministic baseline validation:** Repeat Stage 1 with temperature=0 and compare PT parameter stability against temperature=0.7 results
2. **Mixed-uncertainty perturbation:** Introduce scenarios combining numeric and epistemic uncertainties within the same lottery
3. **Cross-linguistic marker mapping:** Test the same marker set with non-English prompts to determine language dependency of mapping instabilities