---
ver: rpa2
title: Metadata-Driven Retrieval-Augmented Generation for Financial Question Answering
arxiv_id: '2510.24402'
source_url: https://arxiv.org/abs/2510.24402
tags:
- retrieval
- financial
- generation
- chunks
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates metadata-driven Retrieval-Augmented Generation
  (RAG) for answering complex questions from long financial documents. The proposed
  approach introduces a novel multi-stage RAG architecture that leverages LLM-generated
  metadata, including document summaries, thematic clusters, and chunk-level enrichments.
---

# Metadata-Driven Retrieval-Augmented Generation for Financial Question Answering

## Quick Facts
- arXiv ID: 2510.24402
- Source URL: https://arxiv.org/abs/2510.24402
- Reference count: 16
- Primary result: 31% relative F1-score improvement (32.9 → 43.2) using metadata-enriched RAG for financial QA

## Executive Summary
This study presents a novel metadata-driven Retrieval-Augmented Generation (RAG) architecture for answering complex questions from long financial documents. The approach introduces intelligent file filtering, query rewriting, and chunk-level metadata enrichment to improve retrieval and generation quality. By embedding metadata directly alongside document chunks and using a custom metadata-aware reranker, the system achieves significant performance gains over baseline RAG approaches. The work demonstrates that open-source, metadata-aware components can deliver near-state-of-the-art results while offering transparency and efficiency, providing a practical blueprint for building robust RAG systems tailored to high-stakes financial document analysis.

## Method Summary
The method employs a multi-stage RAG pipeline with offline metadata generation and online query processing. PDFs are parsed to Markdown using Docling, then document-level metadata (summaries, thematic clusters) is generated via Gemini 2.5 Flash. Chunks are created with RecursiveCharacterTextSplitter and enriched with metadata (entities, questions, retrieval nuggets). Two collections are stored in Qdrant: standard chunks and contextual chunks (metadata prepended to text). Online, queries are filtered and rewritten using gpt-4.1-mini, then hybrid retrieval (dense + BM25) is performed. A custom metadata reranker scores candidates using entity frequency, cluster coherence, entity-query overlap, and retrieval metrics before o4-mini generates answers. Evaluation uses RAGChecker for claim-level analysis.

## Key Results
- 31% relative F1-score improvement (32.9 → 43.2) with contextual chunks and custom reranker
- Custom metadata reranker achieves 43.2 F1 at zero cost vs. 44.4 F1 with Cohere reranker
- Contextual chunking provides the most significant performance gains by embedding metadata directly with text
- Pre-retrieval filtering improves