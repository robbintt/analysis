---
ver: rpa2
title: Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs
arxiv_id: '2511.05933'
source_url: https://arxiv.org/abs/2511.05933
tags:
- reasoning
- arxiv
- knowledge
- hierarchical
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the conventional wisdom that reinforcement
  learning (RL) improves language model reasoning at the expense of knowledge recall.
  The authors observe that RL-enhanced models consistently outperform base and instruction-tuned
  models on pure knowledge recall tasks, particularly those requiring hierarchical
  knowledge traversal (e.g., medical code lookup).
---

# Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs

## Quick Facts
- arXiv ID: 2511.05933
- Source URL: https://arxiv.org/abs/2511.05933
- Reference count: 28
- Key outcome: RL-enhanced models improve hierarchical knowledge traversal without acquiring new factual knowledge, with structured prompting recovering most performance gaps.

## Executive Summary
This paper challenges the conventional wisdom that reinforcement learning (RL) improves language model reasoning at the expense of knowledge recall. The authors observe that RL-enhanced models consistently outperform base and instruction-tuned models on pure knowledge recall tasks, particularly those requiring hierarchical knowledge traversal (e.g., medical code lookup). Through experiments with structured prompting, retrieval complexity analysis, and layer-wise activation analysis, they demonstrate that RL primarily enhances procedural navigation skills rather than acquiring new knowledge. Structured prompting recovers most of the performance gap between base and RL models, while internal representation analysis reveals that factual knowledge representations remain largely unchanged, but query processing diverges substantially. The findings suggest RL transforms how models traverse knowledge hierarchies rather than the knowledge itself.

## Method Summary
The study evaluates hierarchical knowledge recall in LLMs using medical code lookup (MedConceptsQA) and patent classification (IPC) datasets. Models are compared across three prompt templates: Direct QA (no guidance), Chain-of-Thought (free-form reasoning), and Structured (enforced hierarchical traversal). Performance is measured using final answer accuracy and a novel Path Matching Score that evaluates procedural correctness through F1 and Common Subsequence metrics. Layer-wise activation analysis compares query vs. answer representations using cosine similarity of final token hidden states. The experimental setup includes temperature 0.8, top-p 0.7, and n=3 samples per question.

## Key Results
- RL-enhanced models outperform base and instruction-tuned models on pure knowledge recall tasks requiring hierarchical traversal
- Structured prompting reduces the 24pp accuracy gap between instruct and RL models to 7pp, indicating knowledge exists but needs surfacing
- RL's advantage on procedural path matching increases from 5pp (Memory-Light) to 9pp (Memory-Heavy) as traversal complexity grows
- Query representations diverge significantly between SFT and RL models while factual knowledge representations remain stable

## Why This Works (Mechanism)

### Mechanism 1
Structured prompting can surface latent hierarchical knowledge in instruction-tuned (SFT) models, reducing the performance gap with RL-enhanced models. By explicitly instructing a model to recall structural categories and perform stepwise elimination, the prompt acts as a procedural scaffold. This external guidance compensates for the SFT model's weaker internal navigation policy, allowing it to access and traverse existing parametric knowledge more effectively. The core assumption is that the factual knowledge required for the task is already encoded in the SFT model's parameters during pretraining; the primary deficit is in the retrieval/access procedure.

### Mechanism 2
RL improves a model's ability to correctly recall the procedural path through a knowledge hierarchy, especially for tasks requiring many traversal steps ("deep-retrieval"). The RL process optimizes a policy for navigating knowledge structures, resulting in a superior "traversal skill" that manifests as a higher path matching score, even when final-answer accuracy is similar to a non-RL model. RL teaches a more robust method for hierarchical search. The core assumption is that performance on hierarchical knowledge tasks is partly a function of procedural traversal skill, which is a learnable policy distinct from the factual knowledge itself.

### Mechanism 3
RL primarily transforms how models process and represent queries, while leaving the representations of factual knowledge largely unchanged. RL fine-tunes the model's internal computation to map interrogative queries into effective navigation strategies, reflected as a divergence in query representations between SFT and RL models. The stability of answer representations suggests the underlying factual knowledge is not re-encoded. The core assumption is that the model's internal representations can be cleanly separated into "query processing" and "factual knowledge" components, and that cosine similarity is a good proxy for functional similarity.

## Foundational Learning

- **Concept: Hierarchical Knowledge Traversal**
  - Why needed here: The core task involves navigating tree-like structures (e.g., ICD medical codes, patent classifications). Understanding that finding an answer may require a sequence of steps from a general category to a specific leaf node is essential.
  - Quick check question: Can you explain why finding "Code 57.95" might require first identifying its chapter (11), then its sub-range (57.0-57.99), before arriving at the specific entry?

- **Concept: Procedural vs. Factual Knowledge**
  - Why needed here: The paper's central hypothesis is that RL improves the *procedure* of navigating knowledge, not the *facts* themselves. Distinguishing between "knowing that" (factual) and "knowing how" (procedural) is key to understanding the results.
  - Quick check question: If a model knows "Code 57.95 is for a urinary catheter" (factual) but fails to answer "What is Code 57.95?" without step-by-step guidance, which type of knowledge is deficient?

- **Concept: Layer-wise Representation Analysis**
  - Why needed here: This is the primary method used to peek inside the "black box" and generate evidence for the procedural vs. factual claim. Knowing that models process information in layers, with later layers often more task-specific, helps interpret the similarity plots.
  - Quick check question: If query representations diverge in middle layers but answer representations stay similar across all layers, what might that suggest about what RL changes in the model?

## Architecture Onboarding

- **Component map:** Base/Instruct/RL models (e.g., DeepSeek-V3/R1, Qwen/QwQ) -> Three prompt templates (Direct QA, CoT, Structured) -> Hierarchical datasets (MedConceptsQA, IPC) -> Metrics (Accuracy, Path Matching Score, Cosine similarity) -> Layer-wise activation probe

- **Critical path:** For a new domain, first evaluate the performance gap between your base/SFT and RL models using Direct QA. Then, apply Structured Prompting to the SFT model. If the gap closes significantly (>50%), the knowledge likely exists, and you can focus on improving SFT traversal via prompting or lightweight RL.

- **Design tradeoffs:**
  - Prompting vs. RL Training: Structured prompting is low-cost and interpretable but may not be as robust or generalize as well as a trained RL policy. RL training is computationally expensive but yields a model with intrinsic navigation skills.
  - Path vs. Answer Metric: Optimizing only for final-answer accuracy might miss improvements in reasoning process. The Path Matching Score provides a more granular signal for complex, hierarchical tasks.

- **Failure signatures:**
  1. Persistent Gap after Structured Prompting: Suggests the knowledge may not be in the base model or the hierarchy is too complex for the prompt scaffold. Consider retrieval-augmented generation (RAG) instead.
  2. Good Path Score, Poor Answer: The model traverses well but fails at the final step (e.g., leaf-node classification). Focus improvements on the terminal decision point.
  3. Distilled Model Underperformance: Distillation may capture surface reasoning patterns without the robust navigation policy. The model might be overfitting to reasoning style rather than learning the procedural skill.

- **First 3 experiments:**
  1. Replicate Gap Analysis: Select a model pair (e.g., Llama3 base vs. instruct) and a hierarchical dataset. Measure the accuracy gap across the three prompt templates to confirm if structured prompting narrows it.
  2. Probe Representation Divergence: For the same model pair, implement the representation probe from Section 2.3 on a small set of Q&A pairs. Plot the layer-wise cosine similarity to see if queries diverge more than answers.
  3. Ablate on Complexity: Stratify a small subset of your hierarchical task by retrieval depth (e.g., <3 hops vs. 5+ hops). Compare the performance (accuracy and path score) of SFT vs. RL models to validate if the RL advantage scales with complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the navigation mechanisms identified in RL-enhanced models generalize to other structured reasoning domains such as mathematical proof generation, code debugging, or multi-hop question answering?
- Basis in paper: The conclusion explicitly asks if "similar navigation mechanisms underlie RL improvements on other structured reasoning tasks such as mathematical proof generation, code debugging, or multi-hop question answering."
- Why unresolved: The current study restricts its evaluation to hierarchical knowledge retrieval tasks involving medical codes (MedConceptsQA) and patent classifications (IPC). It is unclear if the "cognitive scaffolding" hypothesis holds when the hierarchy is logical (code/math) rather than taxonomic.
- What evidence would resolve it: A comparative study measuring Path Matching Scores and representation shifts in RL models performing multi-step mathematical or logical reasoning tasks to confirm if procedural navigation is enhanced similarly.

### Open Question 2
- Question: Can reinforcement learning objectives be explicitly designed to optimize hierarchical navigation rather than relying on the implicit emergence of these skills?
- Basis in paper: The authors ask in the conclusion if we can "develop RL objectives that explicitly optimize for hierarchical navigation rather than relying on implicit emergence."
- Why unresolved: The paper demonstrates that RL improves navigation but does not propose a method to target this capability directly; current RL methods optimize for outcome rewards, inadvertently improving traversal as a side effect.
- What evidence would resolve it: Designing a reward signal that explicitly incentivizes correct intermediate steps in a knowledge hierarchy and demonstrating that this leads to faster convergence or superior traversal accuracy compared to standard outcome-based RL.

### Open Question 3
- Question: Can distillation methods be modified to transfer the robust procedural navigation skills of RL models, rather than just surface-level improvements?
- Basis in paper: The authors note in Section 3.1 and Table 4 that distilled models "capture only surface-level improvements without acquiring robust navigation capabilities," leaving a performance gap that structured prompting cannot close.
- Why unresolved: While the paper establishes that standard distillation fails to transfer the "traversal" mechanism, it does not test alternative distillation techniques that might preserve these procedural skills.
- What evidence would resolve it: An ablation study comparing standard distillation against trajectory-based distillation (transferring the internal reasoning path rather than just the final answer) to see if query representations (Q-Q similarity) align better with the teacher model.

## Limitations
- The internal representation analysis relies on a specific probing method (last-token hidden states) that may not capture all relevant knowledge transformations
- The claim that RL improves procedural navigation rather than acquiring new knowledge is primarily supported by the persistence of knowledge gaps even after structured prompting
- The Path Matching Score metric, while designed to capture procedural correctness, is only validated on the IPC dataset and its generalizability to other hierarchical structures is untested

## Confidence
- **High**: RL-enhanced models outperform base/instruct models on pure knowledge recall tasks requiring hierarchical traversal; structured prompting can recover much of the SFT model's performance gap
- **Medium**: RL primarily improves procedural navigation skills rather than acquiring new knowledge; RL changes query processing representations more than factual knowledge representations
- **Low**: The specific mechanisms by which RL optimizes traversal policies are not fully characterized; the representational divergence analysis depends heavily on the validity of the probing method

## Next Checks
1. Apply the same experimental framework (RL vs. SFT comparison, structured prompting, representation analysis) to a non-medical, non-patent hierarchical dataset to test if observed patterns hold across domains
2. Compare RL-enhanced models trained with different reward functions (e.g., final answer accuracy vs. path correctness) to determine which aspects of the traversal policy are most critical for performance gains
3. Design an experiment where both SFT and RL models are fine-tuned on new hierarchical knowledge, then compare their ability to integrate this knowledge versus their performance on original tasks to assess if RL models show better transfer or stability