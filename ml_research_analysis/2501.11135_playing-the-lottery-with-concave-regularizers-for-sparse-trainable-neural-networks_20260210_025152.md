---
ver: rpa2
title: Playing the Lottery With Concave Regularizers for Sparse Trainable Neural Networks
arxiv_id: '2501.11135'
source_url: https://arxiv.org/abs/2501.11135
tags:
- pruning
- neural
- regularization
- concave
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel method for finding sparse, trainable
  neural networks that addresses the challenge of extracting effective subnetworks,
  called winning tickets, from dense networks. The method uses concave regularization
  to promote sparsity of a relaxed binary mask representing the network topology,
  allowing for softer pruning decisions compared to hard percentage-based methods.
---

# Playing the Lottery With Concave Regularizers for Sparse Trainable Neural Networks

## Quick Facts
- arXiv ID: 2501.11135
- Source URL: https://arxiv.org/abs/2501.11135
- Reference count: 40
- One-line primary result: Proposes using concave regularization on relaxed binary masks to find sparse, trainable neural networks that outperform state-of-the-art pruning methods at high sparsities.

## Executive Summary
This paper addresses the challenge of extracting effective sparse subnetworks (winning tickets) from dense neural networks. The key innovation is using concave regularization to promote sparsity of a relaxed binary mask representing network topology, rather than applying traditional convex penalties to weights directly. This approach allows for softer pruning decisions and theoretically provides more accurate estimates of optimal sparse structures compared to standard methods like ℓ1 regularization.

## Method Summary
The method optimizes a continuous mask variable m∈[0,1]^d alongside network weights using projected gradient descent. The mask is multiplied element-wise with weights, and a concave regularizer (logarithmic) is applied to the mask to promote sparsity. After optimization, weights with mask values below a threshold α are pruned. The pruned network is then retrained from the original initialization (rewinding). The process iterates for multiple rounds. The approach is compared against iterative magnitude pruning (IMP) and other state-of-the-art pruning methods across CIFAR-10, CIFAR-100, and Tiny ImageNet datasets using ResNet and VGG architectures.

## Key Results
- Achieves comparable accuracy to IMP at moderate sparsities (20-40%)
- Significantly outperforms IMP at high sparsities (5-10%)
- Demonstrates superior pruning performance compared to other dense-to-sparse pruning methods
- Log regularization shows better performance than ℓ1 regularization under certain conditions

## Why This Works (Mechanism)

### Mechanism 1
Optimizing a continuous, relaxed binary mask via projected gradient descent allows non-differentiable sparsity constraints to be integrated into standard deep learning pipelines without subgradient instability. This approach introduces a mask variable m∈[0,1]^d multiplied element-wise with weights, enabling optimization over the mask while keeping weights fixed or updating them jointly.

### Mechanism 2
Strictly concave regularizers (e.g., logarithmic) provide a tighter approximation to the ℓ0 "count" norm than convex ℓ1 regularization, resulting in better identification of optimal sparse subnetworks. Concave penalties shrink small coefficients more aggressively than large ones, whereas ℓ1 shrinks all coefficients uniformly.

### Mechanism 3
Threshold-based pruning of the learned mask outperforms fixed-percentage magnitude pruning at high sparsities by allowing data-driven retention of critical weights. Standard iterative magnitude pruning removes the bottom p% of weights globally, while this method learns a soft mask where values indicate relevance, with a threshold α applied only to remove near-zero mask values.

## Foundational Learning

- **Concept**: The Lottery Ticket Hypothesis (LTH)
  - Why needed here: This is the problem setting. You must understand that the goal is not just to prune a trained model, but to find a sparse architecture (a "winning ticket") that can be retrained from scratch to match the original dense performance.
  - Quick check question: Can you explain why standard fine-tuning after pruning differs from the "rewinding" strategy used in LTH?

- **Concept**: Concave vs. Convex Regularization
  - Why needed here: The paper's core innovation is substituting ℓ1 (convex) with log (concave). You need to grasp that concave penalties are non-convex and usually harder to optimize, but theoretically better at inducing sparsity because they penalize small weights more severely relative to large ones.
  - Quick check question: Why might a strictly concave regularizer better approximate a binary mask than an ℓ1 regularizer?

- **Concept**: Projected Gradient Descent
  - Why needed here: The paper uses this to optimize the mask. Unlike standard SGD which assumes unbounded parameters, PGD clamps parameters into a valid region (here [0,1]) after every update.
  - Quick check question: If you optimize a mask m with standard SGD without projection, what constraint required by the paper might be violated?

## Architecture Onboarding

- **Component map**: Dataset x -> Initialized weights θ₀ -> Mask m₀ (initialized to 0.5) -> Joint optimization (m, θ) -> Regularized loss L(x; m⊙θ) + λR(m) -> Threshold pruning (m < α) -> Rewinding weights to θ₀ -> Retraining sparse subnetwork

- **Critical path**:
  1. **Initialization**: Start with dense weights θ₀ and a mask m₀ (often 0.5 or 1)
  2. **Joint Optimization**: Run training minimizing the regularized loss
  3. **Pruning**: Apply threshold α to m (set mᵢ < α → 0)
  4. **Rewinding**: Reset weights θ to original initialization θ₀, keep sparse mask m
  5. **Retraining**: Train the sparse subnetwork

- **Design tradeoffs**:
  - **ℓ1 vs. Log Regularizer**: Log is theoretically superior for sparsity but introduces more non-convexity (harder optimization landscape). ℓ1 is more stable but may require more parameters to achieve same accuracy.
  - **Threshold α vs. Percentage p**: Thresholds adapt naturally to layer structure but result in unpredictable model sizes; percentages guarantee exact compression ratios but may damage accuracy.

- **Failure signatures**:
  - **Mask Collapse**: All mask values go to 0 (over-regularized) or stay at 1 (under-regularized). Tune λ.
  - **Oscillation**: Using subgradients instead of projected gradients causes instability.
  - **Accuracy Drop**: At extreme sparsities, strictly concave regularizers may still fail if the "winning ticket" structure is not present or λ is too high.

- **First 3 experiments**:
  1. **Sanity Check (Logistic Regression)**: Replicate the MNIST binary classification using a convex loss to verify that the mask log-regularizer creates sparse, accurate masks while ℓ1 subgradient methods oscillate.
  2. **Ablation on Threshold vs. Percentage**: Train a small CNN on CIFAR-10. Compare pruning the bottom p% of weights vs. thresholding mask values m < α. Plot accuracy vs. sparsity.
  3. **Architecture Scaling**: Apply the log-regularizer method to ResNet-20 on CIFAR-10. Compare the "winning ticket" accuracy at moderate (20%) and extreme (5%) sparsity against standard IMP.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the tuning of the regularization hyperparameters (λ) and pruning threshold (α) be automated or made adaptive to reduce the need for empirical selection?
- Basis in paper: The Conclusion states that future work will be devoted to "optimizing the tuning of the hyperparameters."
- Why unresolved: The current implementation relies on fixed values determined empirically, which may not generalize optimally across different architectures or datasets without manual adjustment.
- What evidence would resolve it: An algorithmic framework that dynamically adjusts λ and α during training and demonstrates robust performance across varied benchmarks without manual tuning.

### Open Question 2
- Question: Can the computational complexity of finding a mask be reduced by combining concave regularization with "early-bird" training stopping or by applying it to untrained models (Strong Lottery Ticket Hypothesis)?
- Basis in paper: The Conclusion suggests exploring "reduction of the numerical complexity, by considering early-bird approaches and untrained models as in the strong lottery ticket framework."
- Why unresolved: The proposed method requires iterative training rounds to identify the mask, which is computationally expensive. The effectiveness of applying this regularization to untrained weights or stopping early is currently unknown.
- What evidence would resolve it: Experiments showing that the proposed regularization can identify winning tickets with significantly fewer training epochs or directly from random initializations.

### Open Question 3
- Question: Do the theoretical guarantees regarding mask estimation accuracy hold for general non-convex loss functions, such as those used in deep learning?
- Basis in paper: The theoretical analysis explicitly relies on strong convexity of the loss function, while acknowledging that the deep learning case is "even more difficult" and non-convex.
- Why unresolved: The provided error bounds depend on a convexity parameter (γ) that is not present in standard deep neural network loss landscapes, leaving a theoretical gap between the proofs and the practical application.
- What evidence would resolve it: A theoretical extension of Theorems 1 and 2 that provides convergence or error bounds for non-convex objective functions.

## Limitations

- **Implementation gaps**: Exact pruning threshold α is unspecified beyond being "much less than 1," and per-layer vs. global pruning strategies are not clarified.
- **Computational overhead**: The additional cost of maintaining and optimizing a continuous mask alongside weights is not discussed, particularly for large-scale models.
- **Theoretical assumptions**: The theoretical guarantees rely on strong convexity assumptions that don't hold for standard deep learning loss functions.

## Confidence

- **High confidence**: The mechanism of using projected gradient descent on a continuous mask variable to avoid subgradient instability (Mechanism 1).
- **Medium confidence**: The theoretical claim that strictly concave regularizers provide tighter approximation to the ℓ0 norm than ℓ1 under strong convexity assumptions (Mechanism 2).
- **Medium confidence**: The claim that threshold-based pruning outperforms fixed-percentage methods at high sparsities (Mechanism 3).

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary α in [0.001, 0.1] on CIFAR-10/ResNet-20 and plot accuracy vs. sparsity. Verify the claim that a small threshold improves high-sparsity performance.

2. **Layer-Wise Mask Distribution**: After optimization, visualize the histogram of mask values m per layer. Confirm the bimodality assumption and assess whether a global threshold is appropriate.

3. **Runtime Overhead Benchmark**: Measure training time per epoch for the joint mask+weight optimization vs. standard IMP. Quantify the additional cost and assess scalability to larger architectures.