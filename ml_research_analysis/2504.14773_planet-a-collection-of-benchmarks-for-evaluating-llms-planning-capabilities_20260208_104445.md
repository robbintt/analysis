---
ver: rpa2
title: 'PLANET: A Collection of Benchmarks for Evaluating LLMs'' Planning Capabilities'
arxiv_id: '2504.14773'
source_url: https://arxiv.org/abs/2504.14773
tags:
- planning
- tasks
- xxxxxx
- agents
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper surveys a comprehensive set of benchmarks for evaluating
  LLM planning capabilities across seven domains: embodied environments, web navigation,
  scheduling, games/puzzles, everyday task automation, text-based reasoning, and agentic
  subtasks. It provides a structured overview of key benchmarks, their focus, and
  their suitability for assessing different aspects of planning in LLMs.'
---

# PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning Capabilities

## Quick Facts
- arXiv ID: 2504.14773
- Source URL: https://arxiv.org/abs/2504.14773
- Reference count: 26
- Primary result: Comprehensive survey of LLM planning benchmarks across seven domains, identifying critical gaps in long-horizon, multimodal, and uncertain planning tasks.

## Executive Summary
This paper provides a systematic survey and categorization of benchmarks for evaluating large language models' planning capabilities across seven domains: embodied environments, web navigation, scheduling, games/puzzles, everyday task automation, text-based reasoning, and agentic subtasks. The authors formalize planning using Markov Decision Processes (MDPs) and map existing benchmarks to this framework, creating a unified evaluation landscape. They identify critical gaps in current benchmarks, particularly around long-horizon tasks, multimodal reasoning, and planning under uncertainty, offering insights for future benchmark development.

## Method Summary
The paper surveys 26 prior works containing various LLM planning benchmarks, analyzing their focus areas, evaluation metrics, and suitability for different planning aspects. Rather than proposing new algorithms or metrics, it provides a comprehensive taxonomy that categorizes benchmarks based on formal MDP properties (state observability, action space, etc.). The methodology involves reviewing existing benchmark literature, formalizing planning definitions, mapping benchmarks to the taxonomy, and analyzing gaps in current evaluation coverage.

## Key Results
- Benchmarks are categorized into seven domains based on MDP formalization
- Critical gaps identified: long-horizon planning, multimodal reasoning, and uncertainty handling
- Current benchmarks over-represent text-based, short-horizon tasks
- Taxonomy enables more targeted algorithm evaluation and selection

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Categorizing benchmarks by a formal planning definition creates a unified evaluation landscape across disparate domains like robotics and web navigation.
- **Mechanism:** The paper formalizes planning as a Markov Decision Process (MDP) defined by explicit state modeling, action sequences, and constraints. It then maps diverse benchmarks (e.g., ALFWorld, WebArena) to this common structure, allowing them to be compared on their core planning attributes rather than their surface domain.
- **Core assumption:** A planning task for an LLM can be adequately modeled using classical components (states, actions, transition probabilities) within an MDP framework.
- **Evidence anchors:**
  - [abstract] "...benchmarks are categorized into embodied environments, web navigation, scheduling, games and puzzles, and everyday task automation."
  - [section] "Formally, an agent operating in a fully observable environment is modeled as a Markov Decision Process... tasks qualify as 'planning' if they align with these core properties..." (Page 2).
  - [corpus] The paper *Beyond Retrieval: A Modular Benchmark for Academic Deep Research Agents* similarly argues that current evaluations neglect "high-level planning," reinforcing the need for a formal planning lens.
- **Break condition:** This mechanism breaks if LLM planning is fundamentally non-MDP-like (e.g., driven by implicit pattern matching rather than state modeling) or if categories are so broad they fail to distinguish between fundamentally different problem types.

### Mechanism 2
- **Claim:** A systematic survey exposes critical gaps in current evaluation coverage, specifically in long-horizon, multimodal, and uncertain planning tasks.
- **Mechanism:** By inventorying existing benchmarks, the authors identify what is over-represented (e.g., text-based, short-horizon) and what is missing (e.g., dynamic environments, visual grounding). This gap analysis highlights where benchmarks must evolve to test more robust, real-world planning.
- **Core assumption:** The identified gaps (e.g., planning under uncertainty) are critical limitations preventing LLMs from handling real-world complexity, not just academic curiosities.
- **Evidence anchors:**
  - [abstract] "...highlight gaps in current benchmarks, particularly around long-horizon tasks, multimodal reasoning, and planning under uncertainty..."
  - [section] "...LLM agents often lack mechanisms for state tracking, error correction, or recovery mid-task, making long-horizon reasoning fragile..." (Page 9).
  - [corpus] The *MineAnyBuild* paper introduces a benchmark for "spatial planning," a specific sub-area of the identified "multimodal reasoning" gap.
- **Break condition:** This mechanism would fail if LLMs prove capable of handling long-horizon and uncertain tasks without specialized benchmarks, or if other, more critical bottlenecks (e.g., fundamental reasoning errors) exist but were missed.

### Mechanism 3
- **Claim:** Mapping benchmark properties to algorithmic capabilities enables more targeted evaluation and faster iteration on planning systems.
- **Mechanism:** The framework allows a researcher to select a benchmark that specifically tests their algorithm's intended contribution (e.g., replanning under uncertainty). This avoids using a mismatched benchmark (e.g., a static puzzle) that would fail to reveal the algorithm's strengths or weaknesses.
- **Core assumption:** Planning algorithms have differentiated strengths (e.g., in handling partial observability vs. long horizons) that require specific testbeds to be accurately measured.
- **Evidence anchors:**
  - [abstract] "Without it, comparing planning algorithms' performance across domains or selecting suitable algorithms for new scenarios remains challenging."
  - [section] "Our study recommends the most appropriate benchmarks for various algorithms..." (Page 1).
  - [corpus] Evidence is weak. The corpus contains related benchmarks (e.g., *TripTailor*) but no direct evidence supporting the claim that this specific taxonomy improves algorithm selection. This is an inferred benefit.
- **Break condition:** The mechanism breaks if planning capabilities are highly correlated (i.e., an algorithm that is good at one task is good at all tasks), making specialized selection unnecessary.

## Foundational Learning

- **Concept: Markov Decision Process (MDP)**
  - **Why needed here:** The paper uses the MDP framework as its core definition for a planning task. Understanding states, actions, and transition probabilities is essential for interpreting any benchmark in the taxonomy.
  - **Quick check question:** Can you define the four components of an MDP and identify them in a simple task like navigating a webpage?

- **Concept: World Model**
  - **Why needed here:** The paper discusses the "world model" as the agent's internal representation of the environment, which is crucial for tasks requiring planning under uncertainty. This concept is central to understanding why certain benchmarks are more challenging.
  - **Quick check question:** How does a "world model" differ from a simple policy, and why is it necessary for planning in dynamic environments?

- **Concept: Long-horizon vs. Short-horizon Planning**
  - **Why needed here:** A major identified gap is "long-horizon" planning. Understanding the distinction (planning over few vs. many steps) is critical for evaluating an agent's ability to maintain coherence and recover from errors over extended task sequences.
  - **Quick check question:** What makes planning over 50 steps (long-horizon) fundamentally harder for an LLM than planning over 5 steps (short-horizon)?

## Architecture Onboarding

- **Component map:** The core "architecture" is the **benchmark taxonomy** itself, which has seven top-level categories (Embodied, Web, Scheduling, Games, Task Automation, Text Reasoning, Agentic Subtasks). Each category contains specific benchmarks (e.g., WebArena, TravelPlanner) which are characterized by properties like state observability and action space.

- **Critical path:**
  1. **Define your algorithm's core capability** (e.g., "I am testing a system for planning under incomplete information").
  2. **Navigate the taxonomy** to the most relevant category (e.g., Embodied or Web).
  3. **Filter benchmarks within the category** using the gap analysis properties (e.g., select a benchmark with partial observability).
  4. **Run evaluation** and map results back to the defined capability.

- **Design tradeoffs:** The main tradeoff is **breadth vs. depth**. The survey provides a broad overview, but detailed implementation of each benchmark requires deep, domain-specific work (e.g., setting up a Minecraft server for Plancraft). Another tradeoff is using **text-based proxies** for multimodal tasks, which simplifies evaluation but may not test true visual grounding.

- **Failure signatures:**
  - **Mismatched Evaluation:** Using a static puzzle benchmark to test a system designed for dynamic environments, leading to poor or misleading results.
  - **Ignoring the Gap:** Developing an algorithm for a well-trodden area (e.g., short-horizon puzzles) and ignoring the identified gaps (e.g., long-horizon planning), which limits real-world applicability.

- **First 3 experiments:**
  1. **Taxonomy Validation:** Pick a single benchmark from two different categories (e.g., ALFWorld from Embodied and WebArena from Web). Run a baseline LLM on both and explicitly map the failure modes to the different "core properties" (e.g., does it fail more on visual grounding in ALFWorld or long-horizon reasoning in WebArena?).
  2. **Gap Analysis Test:** Implement a simple baseline for one of the identified gap areas (e.g., create a long-horizon schedule with Natural Plan). Document if the failures match the paper's predictions (e.g., cascading errors from early mistakes).
  3. **Mapping Exercise:** Take a new, unclassified planning benchmark (potentially from the corpus, like *MineAnyBuild*). Use the paper's MDP framework to characterize it (state observability, action space) and place it into the taxonomy. Justify your placement.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can benchmark design shift from static environments to dynamic world models that require agents to infer and revise states rather than relying on pattern matching?
- **Basis in paper:** [explicit] The authors argue that many existing benchmarks are built on "simplified or static environments," which allows LLMs to "plan" through "pattern matching rather than genuine model-based reasoning."
- **Why unresolved:** Current testbeds typically do not challenge the agent's ability to maintain an internal representation of the world that evolves independently of the agent's actions.
- **What evidence would resolve it:** The development and adoption of benchmarks where agents must successfully adapt plans in response to environmental changes to solve tasks.

### Open Question 2
- **Question:** How can benchmarks effectively evaluate true multimodal planning (vision, language, code) without relying on text-based conversions of visual inputs?
- **Basis in paper:** [explicit] The discussion notes that converting vision-and-action tasks into text-based formats (as done in ALFWorld) "sidesteps the need for true visual grounding," leaving multimodal reasoning a "key opportunity."
- **Why unresolved:** The paper highlights that only a few benchmarks require agents to integrate and reason over multiple modalities simultaneously, limiting the evaluation of visual planning.
- **What evidence would resolve it:** A standardized benchmark suite where agents must process raw visual data (e.g., GUIs or spatial layouts) directly to generate successful action sequences.

### Open Question 3
- **Question:** What specific evaluation frameworks are needed to assess planning under uncertainty and partial observability?
- **Basis in paper:** [explicit] The authors identify "Planning under Uncertainty" as a critical gap, noting that while traditional environments assume full observability, "real-world scenarios often involve uncertainty and partial information."
- **Why unresolved:** The paper suggests that current benchmarks fail to test the ability to manage "incomplete information" or dynamic variables effectively.
- **What evidence would resolve it:** New benchmarks that specifically measure an agent's ability to achieve goals in stochastic environments with hidden state variables.

## Limitations
- Relies on MDP framework which may not capture all LLM planning nuances
- Survey may not include very recent benchmarks developed after paper completion
- Text-based conversions of multimodal tasks may not test true visual grounding
- Practical implementation details for many benchmarks not provided

## Confidence
- **MDP Framework Utility:** Medium - Well-formalized but may not capture all LLM planning characteristics
- **Gap Analysis Validity:** High - Supported by comprehensive survey and clear evidence of current benchmark limitations
- **Taxonomy Impact:** Medium - Useful for organization but real-world impact on algorithm selection unproven
- **Completeness:** High - 26 references provide broad coverage, though rapid field evolution may create blind spots

## Next Checks
1. Implement a simple long-horizon planning task (e.g., a multi-step schedule with error recovery) and evaluate an LLM baseline to confirm if failures align with the paper's predictions of cascading errors.
2. Take a new, unclassified planning benchmark and use the paper's MDP framework to characterize it. Compare your placement to how a practitioner might categorize it without the framework.
3. Survey recent LLM planning papers (post-2024) to assess if they reference or build upon this taxonomy, indicating its influence on the field.