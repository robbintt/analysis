---
ver: rpa2
title: Comprehensive Evaluation of CNN-Based Audio Tagging Models on Resource-Constrained
  Devices
arxiv_id: '2509.14049'
source_url: https://arxiv.org/abs/2509.14049
tags:
- audio
- inference
- conference
- panns
- interface
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive evaluation of multiple CNN-based
  audio tagging models on resource-constrained devices, specifically focusing on the
  Raspberry Pi. The study assesses all 1D and 2D models from the PANNs framework,
  a ConvNeXt-based model adapted for audio classification, MobileNetV3 architectures,
  and two PANNs-derived networks, CNN9 and CNN13.
---

# Comprehensive Evaluation of CNN-Based Audio Tagging Models on Resource-Constrained Devices

## Quick Facts
- arXiv ID: 2509.14049
- Source URL: https://arxiv.org/abs/2509.14049
- Reference count: 19
- Primary result: PANNs, ConvNeXt, and MobileNetV3 CNN models can run continuously for 24 hours on Raspberry Pi 4B, but heavy models risk thermal throttling (>85°C) while lighter models maintain stability.

## Executive Summary
This paper presents a comprehensive evaluation of multiple CNN-based audio tagging models on resource-constrained devices, specifically focusing on the Raspberry Pi. The study assesses all 1D and 2D models from the PANNs framework, a ConvNeXt-based model adapted for audio classification, MobileNetV3 architectures, and two PANNs-derived networks, CNN9 and CNN13. The models are converted to ONNX format to ensure efficient deployment and portability across diverse hardware platforms. The evaluation involves continuous 24-hour inference sessions to assess performance stability, including inference time and thermal behavior. Results show that with appropriate model selection and optimization, it is possible to maintain consistent inference latency and manage thermal behavior effectively over extended periods. The study highlights the importance of considering the deployment environment when selecting a model for real-time applications, as some models perform well under controlled, headless conditions but may experience performance degradation with additional system overhead introduced by graphical interfaces. The findings provide valuable insights for deploying audio tagging models in real-world edge computing scenarios, particularly in domains such as environmental monitoring, smart home systems, and assistive technologies.

## Method Summary
The study evaluates CNN-based audio tagging models on a Raspberry Pi 4B (4GB RAM) using real-time audio captured via USB sound card and microphone. Models are converted to ONNX format and tested in headless and GUI modes over 24-hour continuous inference sessions. Audio input is processed in 10-second segments at 32 kHz sampling rate, with predictions every 5 seconds using 50% overlap. Performance metrics include inference time, CPU temperature (monitored at 25°C ambient), and temporal stability. The model set includes PANNs variants (CNN6, CNN9, CNN10, CNN13, CNN14, ResNet variants, Wavegram variants), ConvNeXt-tiny, MobileNetV3 variants, and CNN9/CNN13 from Bibbo et al.

## Key Results
- Heavy models like ResNet54 and Wavegram reach temperatures above 80°C, risking thermal throttling during 24-hour operation
- Lighter models like MobileNetV3 variants maintain stable inference times (<0.25s headless) and temperatures (<65°C)
- GUI introduction causes 2-10°C temperature rise and significant latency variance across models
- ONNX conversion enables efficient CPU-only deployment without dedicated AI acceleration hardware

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Converting models to ONNX format likely reduces inference latency overhead compared to native PyTorch execution on resource-constrained CPUs.
- **Mechanism:** ONNX allows for graph optimization (e.g., constant folding, operator fusion) and removes the overhead of Python's dynamic typing and PyTorch's eager execution mode. This creates a static computation graph that executes more efficiently on the limited CPU resources of the Raspberry Pi.
- **Core assumption:** The observed stability and lower inference times are partially attributable to the ONNX runtime's efficiency rather than solely the model architecture.
- **Evidence anchors:**
  - [abstract] "...all models are converted to the Open Neural Network Exchange (ONNX) format. Unlike previous works... our analysis... involves continuous 24-hour inference sessions to assess performance stability."
  - [section 2.1] "...ONNX is widely recognized for its cross-platform compatibility and efficiency, particularly when deployed on resource-constrained CPUs."
  - [corpus] "Compressing Quaternion Convolutional Neural Networks..." discusses compression for resource constraints, supporting the need for format optimization, though specific ONNX benchmarks are absent from the provided neighbors.
- **Break condition:** If the PyTorch models were originally heavily optimized or if the ONNX runtime version used was significantly outdated, the latency benefits might diminish or reverse.

### Mechanism 2
- **Claim:** Introducing a Graphical User Interface (GUI) creates resource contention that degrades inference latency and thermal stability.
- **Mechanism:** The Raspberry Pi CPU lacks the cores or frequency to handle both the inference loop (computationally heavy) and the GUI rendering loop (I/O and graphically heavy) concurrently without context switching. This leads to "starvation" of the inference task, increasing latency spikes and sustained CPU utilization (heat).
- **Core assumption:** The observed performance degradation in GUI mode is due to CPU contention and not memory exhaustion or I/O blocking.
- **Evidence anchors:**
  - [abstract] "...some models perform well under controlled, headless conditions but may experience performance degradation with additional system overhead introduced by graphical interfaces."
  - [results 3.1] "When the GUI is active... a general increase in inference time is observed... suggesting that the GUI introduces a system load that affects models unevenly."
  - [corpus] No direct evidence in neighbors regarding GUI overhead; focus is on compression/robustness.
- **Break condition:** If the system utilized a dedicated GPU for rendering the GUI (which the standard Raspberry Pi 4B does not effectively offload from the CPU for complex UIs), this contention would be significantly reduced.

### Mechanism 3
- **Claim:** Sustained 24-hour operation induces thermal saturation that exposes the limitations of passive cooling solutions for high-parameter models.
- **Mechanism:** High-complexity models (e.g., ResNet54, Wavegram) sustain high CPU utilization, generating heat faster than the passive dissipation capacity of the Raspberry Pi enclosure. Over 24 hours, this leads to thermal saturation, potentially hitting throttling limits (85°C), whereas lighter models (MobileNetV3) stay within safe thermal envelopes.
- **Core assumption:** The ambient temperature of 25°C and the specific enclosure used in the study are representative enough to generalize thermal risks.
- **Evidence anchors:**
  - [results 3.2] "Heavier models like ResNet54... reach temperatures above 80°C... models... clustering around 83–85°C."
  - [results 3.2] "...sustained high temperatures could lead to thermal throttling or hardware degradation, especially in sealed enclosures..."
  - [corpus] "Evaluating the Impact of Compression Techniques..." supports the notion that resource constraints require robustness evaluation, though it focuses on data corruption rather than thermal robustness.
- **Break condition:** Active cooling (fans) or a lower ambient temperature would likely break this thermal correlation, allowing heavier models to run without throttling.

## Foundational Learning

- **Concept: ONNX (Open Neural Network Exchange)**
  - **Why needed here:** The study relies on converting PyTorch models to ONNX to achieve the reported efficiency. Without understanding ONNX, one cannot replicate the deployment pipeline.
  - **Quick check question:** Does converting a model to ONNX guarantee speedup on a CPU, or does it depend on the specific operator support of the runtime?

- **Concept: Thermal Throttling**
  - **Why needed here:** The paper's primary constraint analysis involves CPU temperature. Understanding that CPUs reduce clock speed to cool down is essential to interpreting the inference time spikes.
  - **Quick check question:** If a Raspberry Pi hits 85°C, will inference time decrease (due to error) or increase (due to frequency scaling)?

- **Concept: Sliding Window Inference**
  - **Why needed here:** The system predicts audio tags every 5 seconds using 10-second segments (50% overlap). This introduces constraints on processing speed: inference must happen faster than the window shift to avoid lag.
  - **Quick check question:** If a model takes 6 seconds to process a 10-second window, but the system records new audio every 5 seconds, will the system experience accumulating latency?

## Architecture Onboarding

- **Component map:** Audio Capture (USB Sound Card + Microphone) -> Preprocessing (Resampling + Spectrogram) -> ONNX Inference Engine -> Terminal (Headless) or Touchscreen GUI (Visualizer) -> Output Predictions

- **Critical path:** Audio Capture (10s buffer) -> Preprocessing (Spectrogram) -> ONNX Inference -> Post-processing. *Note: The critical timing constraint is that Inference + Preprocessing must be < 5s to maintain real-time continuity with the sliding window.*

- **Design tradeoffs:**
  - **PANNs (e.g., CNN14):** Higher accuracy (implied by AudioSet training) but high thermal cost (>80°C) and latency (~3.5s GUI). Best for batch processing or active cooling.
  - **MobileNetV3 (e.g., mn05_as):** Lower accuracy (assumed) but low thermal cost (<65°C) and fast latency (<0.25s headless). Best for real-time, headless monitoring.
  - **GUI vs. Headless:** GUI provides observability but adds ~10°C to thermal load and significantly increases latency variance.

- **Failure signatures:**
  - **Thermal Runaway:** Inference time starts low but drifts upward over hours as the device hits thermal limits (common in heavy PANNs).
  - **GUI Starvation:** Interface freezes or predictions stagger when using heavy models (ResNet54) with the GUI enabled.
  - **Audio Desync:** If inference time > 5s window, audio queues back up, leading to delayed predictions.

- **First 3 experiments:**
  1. **Thermal Baseline:** Run MobileNetV3 (mn05_as) in headless mode for 1 hour to establish a stable thermal baseline (<60°C).
  2. **Stress Test:** Run a heavy PANNs model (e.g., CNN14) with the GUI enabled. Monitor for the "85°C threshold" and observe if inference time spikes due to throttling.
  3. **Pipeline Latency:** Measure the end-to-end latency of just the spectrogram extraction (Librosa vs. TorchAudio) to verify if pre-processing is the bottleneck before even loading the model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the inference pipeline of PANNs models be effectively restructured to enhance thermal efficiency and performance stability when a graphical user interface is active?
- Basis in paper: [explicit] The Conclusion states: "As future work, the inference pipeline of the PANNs models will be restructured with the aim of enhancing their performance and thermal efficiency in scenarios where a graphical user interface is active."
- Why unresolved: Current PANNs implementations experience critical thermal behavior (>85°C) and significant latency degradation due to the system overhead of GUI rendering.
- What evidence would resolve it: A modified PANNs pipeline demonstrating sustained lower CPU temperatures and consistent inference latency during 24-hour GUI-enabled sessions.

### Open Question 2
- Question: How do the evaluated audio tagging models perform regarding latency and thermal management on alternative embedded platforms, such as NVIDIA Jetson or specialized AI hardware?
- Basis in paper: [explicit] The Conclusion proposes to "expand this experiment by incorporating additional IoT devices, such as NVIDIA Jetson platforms, or by integrating specialized AI hardware to enhance performance and scalability."
- Why unresolved: This study is limited to the Raspberry Pi 4B, which lacks dedicated AI acceleration hardware (NPUs/TPUs) found in other edge devices.
- What evidence would resolve it: Comparative benchmark data showing inference time and thermal behavior for the same model set running on Jetson or AI-accelerated hardware.

### Open Question 3
- Question: What specific implementation factors cause the observed decoupling of inference latency and thermal behavior in the CNN9 and CNN13 models?
- Basis in paper: [inferred] The paper notes that CNN9 and CNN13 exhibit "fluctuating and unstable inference times" but do not cause a significant increase in system temperature, a pattern distinct from other heavier models.
- Why unresolved: The paper identifies this anomaly but does not isolate the root cause (e.g., library overhead, memory I/O vs. compute intensity) in the discussion.
- What evidence would resolve it: A profiling analysis of CPU resource utilization and scheduling for CNN9/13 versus stable models to identify the bottleneck causing the latency fluctuations.

## Limitations
- The study relies on pre-trained models without fine-tuning on the target deployment domain, which may limit real-world performance accuracy.
- Thermal measurements were conducted in a controlled environment (25°C ambient), potentially underestimating thermal stress in hotter operating conditions.
- GUI overhead impact is not fully quantified; the study only notes general performance degradation without detailed profiling of specific GUI components.

## Confidence

- **High Confidence:** ONNX conversion improves inference efficiency on Raspberry Pi (supported by consistent latency reduction across models).
- **Medium Confidence:** 24-hour stability testing reveals thermal limitations of heavy models (supported by temperature measurements but dependent on specific cooling setup).
- **Medium Confidence:** Model selection should prioritize deployment environment constraints (supported by contrasting headless vs. GUI performance but limited to single device type).

## Next Checks
1. Test model performance with active cooling (fan) to determine if thermal throttling is the primary bottleneck for heavy models.
2. Profile specific GUI components to quantify overhead contributions and identify optimization opportunities.
3. Evaluate model accuracy trade-offs when fine-tuning lighter models (MobileNetV3) on domain-specific audio datasets versus using pre-trained heavy models.