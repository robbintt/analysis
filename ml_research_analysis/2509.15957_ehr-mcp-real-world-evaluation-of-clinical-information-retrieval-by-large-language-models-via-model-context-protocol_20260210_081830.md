---
ver: rpa2
title: 'EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large
  Language Models via Model Context Protocol'
arxiv_id: '2509.15957'
source_url: https://arxiv.org/abs/2509.15957
tags:
- patient
- clinical
- date
- data
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated the capability of a large language model\
  \ (GPT-4.1) to autonomously retrieve clinically relevant information from an electronic\
  \ health record (EHR) using the Model Context Protocol (MCP) in a real hospital\
  \ setting. Custom MCP tools were developed to interface with the hospital\u2019\
  s data warehouse, and a LangGraph ReAct agent was used to manage tool selection\
  \ and execution for six clinically relevant tasks derived from infection control\
  \ team use cases."
---

# EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol

## Quick Facts
- arXiv ID: 2509.15957
- Source URL: https://arxiv.org/abs/2509.15957
- Authors: Kanato Masayoshi; Masahiro Hashimoto; Ryoichi Yokoyama; Naoki Toda; Yoshifumi Uwamino; Shogo Fukuda; Ho Namkoong; Masahiro Jinzaki
- Reference count: 36
- Primary result: LLM (GPT-4.1) with MCP achieved near-perfect accuracy on simple EHR data retrieval tasks and high accuracy on complex multi-step reasoning tasks in a real hospital setting

## Executive Summary
This study evaluated the capability of a large language model (GPT-4.1) to autonomously retrieve clinically relevant information from an electronic health record (EHR) using the Model Context Protocol (MCP) in a real hospital setting. Custom MCP tools were developed to interface with the hospital's data warehouse, and a LangGraph ReAct agent was used to manage tool selection and execution for six clinically relevant tasks derived from infection control team use cases. Results showed that the LLM consistently selected and invoked the correct tools, achieving near-perfect accuracy for simple retrieval tasks (e.g., fetching latest body weight, listing antibiotics, retrieving lab data) and high accuracy for more complex tasks requiring multi-step reasoning (e.g., calculating creatinine clearance). Errors were primarily due to incorrect argument specification or misinterpretation of tool responses, especially in complex tasks. The EHR-MCP framework provides a secure, reproducible infrastructure for clinical data access and supports the development of hospital AI agents, though challenges remain in handling complex, time-dependent queries and ensuring robustness in ambiguous situations.

## Method Summary
The researchers implemented a Model Context Protocol (MCP) framework with custom tools to interface with a hospital's data warehouse, enabling a large language model (GPT-4.1) to retrieve clinical information. A LangGraph ReAct agent managed tool selection and execution for six clinically relevant tasks based on infection control team use cases. The tasks included simple data retrieval (e.g., latest body weight, antibiotic list) and more complex multi-step reasoning (e.g., calculating creatinine clearance, identifying antibiotics after negative cultures). The system was evaluated in a real hospital setting using the HOPE/EGMAIN-GX EHR system, with accuracy measured through manual verification of tool invocations and responses.

## Key Results
- LLM achieved near-perfect accuracy on simple retrieval tasks (body weight, antibiotics, lab data)
- High accuracy (95%) on complex multi-step reasoning tasks (creatinine clearance calculation)
- Most errors occurred from incorrect argument specification or misinterpretation of tool responses, particularly in time-dependent complex tasks

## Why This Works (Mechanism)
The EHR-MCP framework works by providing a secure, standardized interface between large language models and clinical data sources through custom MCP tools. The LangGraph ReAct agent enables autonomous tool selection and execution, allowing the LLM to break down complex clinical queries into sequential API calls. The model context protocol standardizes tool definitions and responses, creating a reproducible infrastructure for clinical data access while maintaining security boundaries between the LLM and sensitive patient information.

## Foundational Learning

**MCP (Model Context Protocol)** - A standardized protocol for connecting AI models to external data sources and tools; needed for consistent tool integration across different LLM applications; quick check: verify tool definitions follow MCP specification format

**ReAct Agent Architecture** - Combines reasoning and action cycles where the agent alternates between thinking about the next step and taking action; needed for breaking down complex clinical queries into executable steps; quick check: ensure agent logs show clear reasoning-action patterns

**LangGraph** - A framework for building stateful, multi-actor applications with LLMs; needed for managing complex agent workflows and tool orchestration; quick check: verify agent state transitions match expected workflow

**Tool Selection Accuracy** - The ability of an LLM to correctly identify and invoke appropriate tools for a given task; needed to ensure the right data is retrieved for clinical decision-making; quick check: count successful tool invocations vs total attempts

**Argument Specification** - The process of correctly formatting and providing parameters to tool functions; critical for complex queries requiring specific temporal or conditional constraints; quick check: verify tool call parameters match expected schema

## Architecture Onboarding

**Component Map**: EHR System -> MCP Server -> LLM (GPT-4.1) -> LangGraph ReAct Agent -> Custom MCP Tools -> Data Warehouse

**Critical Path**: User Query → Agent Reasoning → Tool Selection → API Call → Data Retrieval → Response Processing → Final Answer

**Design Tradeoffs**: Security vs Accessibility - MCP provides secure data access but requires careful tool design; Simplicity vs Functionality - custom tools must balance ease of use with clinical complexity

**Failure Signatures**: Incorrect tool selection (wrong data source), malformed arguments (incorrect parameters), misinterpretation of results (incorrect data processing), timeout errors (slow queries)

**First Experiments**:
1. Test basic data retrieval with simple queries (weight, demographics)
2. Verify tool selection accuracy on known clinical scenarios
3. Measure response time for single vs multi-tool queries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can EHR-MCP supported retrieval improve clinical decision-making accuracy and workflow efficiency for infection control teams?
- Basis in paper: [explicit] The authors state future work must "demonstrate practical benefits in real-world settings, such as improving patient outcomes and reducing workload" and investigate decision support for ICT.
- Why unresolved: The current study only evaluated the accuracy of data retrieval, not the downstream clinical reasoning or patient outcomes.
- What evidence would resolve it: A prospective study measuring ICT decision quality and time-to-decision with versus without the tool.

### Open Question 2
- Question: Can the framework generalize to diverse hospital IT infrastructures and non-English clinical contexts without significant re-engineering?
- Basis in paper: [explicit] The paper notes that being a single-institution study "limits generalizability" and local system differences must be accommodated.
- Why unresolved: The system was tested only on the HOPE/EGMAIN-GX system at a single site.
- What evidence would resolve it: Successful deployment and consistent accuracy results across multiple hospitals using different EHR vendors and languages.

### Open Question 3
- Question: What architectural changes are required to eliminate argument and interpretation errors in complex, time-dependent multi-step tasks?
- Basis in paper: [explicit] The authors highlight that "Most errors arose from incorrect arguments or misinterpretation of tool results," particularly in the `culture_neg_abx` task.
- Why unresolved: The ReAct agent failed to consistently manage temporal constraints and complex data parsing in the current iteration.
- What evidence would resolve it: Benchmarking modified agent architectures (e.g., multi-agent condensers) showing improved accuracy on the failed complex tasks.

## Limitations
- Single-hospital study limits generalizability to other EHR systems and clinical contexts
- Controlled task environment may not capture full complexity of real-world clinical scenarios
- System shows brittleness in handling complex, time-dependent queries and ambiguous clinical data

## Confidence

**High confidence**: Technical implementation of MCP framework and tool selection accuracy for simple retrieval tasks
**Medium confidence**: Generalizability of results to other hospital systems and clinical contexts
**Low confidence**: Robustness for complex, multi-step reasoning tasks involving temporal dependencies or ambiguous clinical data

## Next Checks
1. Test the EHR-MCP framework with additional LLM models and larger, more diverse task sets across multiple hospital systems
2. Evaluate performance on time-sensitive queries and scenarios requiring integration of multiple data types with temporal dependencies
3. Conduct user studies with clinicians to assess practical utility and identify edge cases not captured in the controlled task evaluation