---
ver: rpa2
title: Budgeted Multiple-Expert Deferral
arxiv_id: '2510.26706'
source_url: https://arxiv.org/abs/2510.26706
tags:
- deferral
- expert
- learning
- loss
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational bottleneck in training learning-to-defer
  systems when querying multiple costly experts. Standard training requires computing
  all expert outputs for every training example, which is prohibitively expensive
  when experts are resource-intensive.
---

# Budgeted Multiple-Expert Deferral

## Quick Facts
- arXiv ID: 2510.26706
- Source URL: https://arxiv.org/abs/2510.26706
- Reference count: 40
- Primary result: Budgeted deferral reduces expert queries by 35-40% on binary datasets while matching full-batch training accuracy

## Executive Summary
This paper addresses the computational bottleneck in training learning-to-defer systems when querying multiple costly experts. Standard training requires computing all expert outputs for every training example, which is prohibitively expensive when experts are resource-intensive. The authors propose a budgeted deferral framework that selectively queries only a subset of experts per training instance while maintaining strong performance guarantees. The core method involves two key components: (1) a sampling-probs subroutine that determines which experts to query based on disagreement among hypotheses in the current version space, and (2) importance-weighted empirical risk minimization that accounts for the sampling process. The algorithm adaptively focuses queries on instances where expert predictions disagree most, targeting high-uncertainty regions.

## Method Summary
The paper presents an online active learning algorithm for training routing functions that decide when to defer to external experts. The method maintains a version space of candidate routers and selectively queries expert costs based on disagreement within this space. For each training instance, the algorithm computes sampling probabilities proportional to the maximum disagreement in surrogate loss among all pairs of routers in the current version space. When an expert is queried, the observed cost is stored with an importance weight equal to the inverse of the product of sampling and selection probabilities. This ensures unbiased updates despite partial observation. The version space is pruned based on empirical performance consistency, preserving the optimal router while accelerating convergence. Experiments on ten benchmark datasets demonstrate that this approach matches the accuracy of standard deferral algorithms while substantially reducing the number of expert predictions queried.

## Key Results
- Reduces expert queries by 35-40% on binary datasets compared to full-batch training
- Query reduction is even more pronounced (below 30%) on multi-class datasets with many experts
- Maintains comparable accuracy to standard deferral algorithms across all tested datasets
- Strong scalability to complex prediction tasks as the number of experts increases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selectively querying expert costs based on the disagreement among candidate routing functions significantly reduces training label complexity while preserving generalization guarantees.
- **Mechanism:** The algorithm maintains a version space ($R_t$) of candidate routers. For a new input $(x_t, y_t)$, it computes a sampling probability $p_{t,k}$ for each expert $k$ proportional to the maximum disagreement in surrogate loss among all pairs of routers in $R_t$ regarding that expert. If routers agree on the utility of expert $k$, $p_{t,k}$ is low (no query needed). If they disagree, $p_{t,k}$ is high (query needed to resolve uncertainty). This targets the "region of disagreement."
- **Core assumption:** The disagreement coefficient ($\theta$) of the hypothesis class is bounded, and the slope asymmetry of the loss function is finite.
- **Evidence anchors:**
  - [Abstract] "core method involves... sampling-probs subroutine that determines which experts to query based on disagreement among hypotheses in the current version space."
  - [Algorithm 2] Defines $p_{t,k} = \max_{r,r'\in R_t} \{\ell(r, x, k) - \ell(r', x, k)\}$.
  - [Section 4] "allocates the query budget adaptively, prioritizing experts and instances where the disagreement... is greatest."
- **Break condition:** If the version space becomes too noisy or the "realizable" assumption is strongly violated (i.e., no expert has zero cost), the disagreement region may fail to shrink, forcing the algorithm to query most samples (linear scaling).

### Mechanism 2
- **Claim:** Unbiased model updates are achievable despite non-uniform, partial observation of expert costs by assigning inverse-probability weights to the incurred costs.
- **Mechanism:** When the decision is made to query expert $k$ (based on Mechanism 1), the observed cost $c_{t,k}$ is stored in the training buffer $S_t$ with a weight $w = \frac{1}{q_{t,k} p_{t,k}}$. This importance weighting ensures that the empirical risk minimization (ERM) over the partial buffer $S_t$ remains an unbiased estimator of the true expected loss over the full dataset.
- **Core assumption:** The sampling probabilities $p_{t,k}$ and expert selection probabilities $q_{t,k}$ are strictly positive so that weights are finite.
- **Evidence anchors:**
  - [Section 3] "it is straight-forward to verify that this estimator is unbiased, i.e., $\mathbb{E}[E_T(r)] = E(r)$."
  - [Algorithm 1] "St ← St-1 ∪ {(xt, yt, ct,k, 1/(qt,k pt,k))}."
- **Break condition:** If sampling probabilities become extremely small (high confidence regions), the corresponding weights $1/p$ become extremely large. This can cause high variance in the gradient updates, leading to training instability or divergence.

### Mechanism 3
- **Claim:** Pruning the hypothesis set based on empirical performance consistency preserves the optimal router while accelerating the collapse of the disagreement region.
- **Mechanism:** At each step, the version space $R_t$ is updated to exclude hypotheses whose empirical loss exceeds the minimum observed loss by a threshold $\Delta_t$ ($R_{t+1} = \{r \in R_t : E_t(r) \le E^*_t + \Delta_t\}$). By removing suboptimal routers, the algorithm reduces the potential for future disagreement on similar inputs, effectively speeding up the learning process without needing to query those inputs.
- **Core assumption:** The optimal hypothesis $r^*$ is contained within the initial hypothesis class $R$ and its empirical loss tracks its true loss closely (uniform convergence).
- **Evidence anchors:**
  - [Theorem 1] "The optimal hypothesis r* belongs to the retained set $R_T$... with probability at least $1-\delta$."
  - [Section 4] "This pruning strategy guarantees... that the optimal predictor... remains in the version space... while progressively eliminating suboptimal hypotheses."
- **Break condition:** If the slack parameter $\Delta_t$ decays faster than the empirical error converges, the optimal hypothesis $r^*$ might be accidentally pruned (version space collapse), leading the algorithm to converge to a suboptimal router.

## Foundational Learning

- **Concept: Importance Weighted Active Learning (IWAL)**
  - **Why needed here:** This paper is a direct extension of IWAL (Beygelzimer et al., 2009) from label querying to expert-cost querying. Understanding the martingale concentration bounds used in IWAL is required to understand the paper's theoretical guarantees.
  - **Quick check question:** Why does weighting an observed loss by $1/p$ (where $p$ is the probability of observing it) result in an unbiased estimate of the true loss?

- **Concept: Learning to Defer (L2D) Surrogates**
  - **Why needed here:** The algorithm optimizes a *surrogate loss* (e.g., logistic loss) rather than the discrete 0-1 routing cost. The "slope asymmetry" and consistency of this surrogate determine the theoretical sample complexity.
  - **Quick check question:** In the two-stage setting, why is the loss defined as $\sum_k (1-c_k(x,y))\ell(r,x,k)$ rather than just $\ell(r,x,k)$?

- **Concept: Version Space Geometry**
  - **Why needed here:** The efficiency of the algorithm relies on the "shrinking" of the version space. The generalized disagreement coefficient ($\theta$) measures how the diameter of this space shrinks relative to the error radius.
  - **Quick check question:** If the hypothesis class $R$ is extremely complex (e.g., deep neural nets without constraints), what happens to the size of the version space $R_t$ and the resulting label complexity?

## Architecture Onboarding

- **Component map:** Data Ingest -> Version Space Buffer -> Disagreement Oracle -> Expert Interface -> Weighted ERM Solver -> Updated Router
- **Critical path:** The `SAMPLING-PROBS` computation involves an inner maximization over all pairs $(r, r') \in R_t^2$. If $R_t$ is large or high-dimensional, this step becomes the computational bottleneck, potentially exceeding the cost of the expert query itself.
- **Design tradeoffs:**
  - **Uniform vs. Informed Expert Selection:** The paper suggests setting expert selection probabilities $q_{t,k}$ to uniform ($1/n_e$) to minimize worst-case bounds, but this ignores known variations in expert latency or cost. A heuristic adjustment to $q_{t,k}$ might improve practical efficiency but voids the theoretical guarantees.
  - **Slack Parameter $\Delta_t$:** A loose $\Delta_t$ (large value) keeps more hypotheses, leading to more queries (slower convergence). A tight $\Delta_t$ reduces queries but risks pruning the optimal hypothesis if data is noisy.
- **Failure signatures:**
  - **Weight Explosion:** Gradients become NaNs or oscillate wildly. This suggests $p_{t,k}$ is dropping too close to zero for highly confident (but possibly wrong) regions. *Fix:* Clamp minimum sampling probability $p_{min}$.
  - **Stalling:** The algorithm stops querying early but test accuracy is poor. This implies the version space collapsed prematurely around a suboptimal router. *Fix:* Increase $\Delta_t$ or check for data distribution shift.
- **First 3 experiments:**
  1. **Synthetic Validation:** Create a dataset with known disagreement coefficients. Verify that the cumulative query count scales as $\tilde{O}(\sqrt{T})$ (or $\log T$) rather than $O(T)$.
  2. **Ablation on $p_{t,k}$:** Compare the full algorithm against a baseline where $p_{t,k}$ is set to a constant (random sampling) to quantify the efficiency gain purely from the disagreement-based selection.
  3. **Scalability Test:** Fix the number of training samples $T$ and vary the number of experts $n_e$. Confirm that the total queries scale favorably compared to the standard $n_e \times T$ baseline, specifically looking for the "strong scalability" mentioned in the abstract.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adaptive querying strategies be developed that explicitly exploit structure across training examples?
- **Basis in paper:** [explicit] Conclusion section mentions extensions for "adaptive querying strategies that exploit structure across training examples."
- **Why unresolved:** The current Sampling-Probs algorithm evaluates instances independently and does not leverage correlations or clusters within the data distribution.
- **Evidence to resolve:** Derivation of label complexity bounds that depend on the intrinsic dimensionality or cluster structure of the data, showing improved query efficiency.

### Open Question 2
- **Question:** How can the framework be extended to settings with dynamically available or context-dependent experts?
- **Basis in paper:** [explicit] Conclusion section lists "settings with dynamically available or context-dependent experts" as a potential extension.
- **Why unresolved:** The theoretical analysis assumes a fixed hypothesis class $\mathcal{R}$ and a static set of experts available for every query round.
- **Evidence to resolve:** Convergence guarantees and generalization bounds for an algorithm where the expert pool varies stochastically or deterministically based on the input context.

### Open Question 3
- **Question:** How can budgeted deferral algorithms be integrated with reinforcement learning for sequential decision-making?
- **Basis in paper:** [explicit] Conclusion section proposes the "integration with reinforcement learning for sequential or interactive decision-making."
- **Why unresolved:** The current method relies on importance-weighted empirical risk minimization for i.i.d. data, which does not directly apply to the non-i.i.d. trajectories of Markov Decision Processes.
- **Evidence to resolve:** A modified surrogate loss and sampling mechanism that maintains theoretical consistency within an RL policy optimization framework (e.g., actor-critic methods).

## Limitations

- **Computational Overhead:** While the method reduces expert queries, the inner loop for computing disagreement probabilities over the version space could be computationally expensive, potentially offsetting gains from fewer expert queries.
- **Scalability to Deep Models:** The paper uses logistic regression models. Extending the framework to deep neural networks would require careful handling of the version space and disagreement computation.
- **Hyperparameter Sensitivity:** The performance of the algorithm depends on the choice of slack parameter Δₜ and other hyperparameters. The paper does not provide a detailed sensitivity analysis.

## Confidence

- **High Confidence:** The theoretical framework and the core algorithmic idea of using disagreement to guide expert queries are sound and well-established.
- **Medium Confidence:** The empirical results on the 10 benchmark datasets are promising, but the generalizability to other domains and the scalability to very large datasets with many experts need further validation.
- **Low Confidence:** The paper does not provide a detailed analysis of the computational complexity of the algorithm, particularly the inner loop for computing disagreement probabilities.

## Next Checks

1. **Computational Complexity Analysis:** Conduct a detailed analysis of the time complexity of the algorithm, focusing on the inner loop for computing disagreement probabilities. Profile the algorithm on datasets of varying sizes and with different numbers of experts to identify potential bottlenecks.
2. **Scalability to Deep Models:** Implement the budgeted deferral framework with deep neural networks as the hypothesis class. Investigate strategies for efficiently computing disagreement probabilities in high-dimensional parameter spaces.
3. **Robustness to Hyperparameters:** Perform an ablation study to assess the sensitivity of the algorithm to the choice of hyperparameters, such as the slack parameter Δₜ and the expert selection probabilities qₜ,ₖ. Identify ranges of hyperparameters that yield stable performance across different datasets.