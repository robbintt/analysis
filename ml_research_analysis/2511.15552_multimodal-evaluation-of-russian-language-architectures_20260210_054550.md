---
ver: rpa2
title: Multimodal Evaluation of Russian-language Architectures
arxiv_id: '2511.15552'
source_url: https://arxiv.org/abs/2511.15552
tags:
- dataset
- answer
- audio
- questions
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MERA Multi is the first multimodal evaluation benchmark for Russian
  language, introducing 18 tasks across text, image, audio, and video modalities.
  It proposes a unified taxonomy of multimodal abilities and combines public and private
  datasets to assess general-purpose and modality-specific models.
---

# Multimodal Evaluation of Russian-language Architectures

## Quick Facts
- **arXiv ID:** 2511.15552
- **Source URL:** https://arxiv.org/abs/2511.15552
- **Reference count:** 40
- **Primary result:** Introduces MERA Multi, the first multimodal evaluation benchmark for Russian language, with 18 tasks across text, image, audio, and video modalities.

## Executive Summary
MERA Multi is the first multimodal evaluation benchmark for Russian language, introducing 18 tasks across text, image, audio, and video modalities. It proposes a unified taxonomy of multimodal abilities and combines public and private datasets to assess general-purpose and modality-specific models. The evaluation methodology employs block-prompting, dual metrics (Exact Match and Judge Score), and automated scoring with leakage protection via watermarking and membership inference. Baseline results show that general-purpose models like Qwen3-Omni-30B-A3B-Instruct achieve the highest overall scores (0.5 Total Score), while specialist models lag in audio and video tasks. Human baselines reach 0.80–0.92 depending on task type, highlighting a performance gap. The benchmark provides reproducible code, a submission platform, and licensing terms to prevent training misuse, serving as a blueprint for culturally grounded multimodal evaluation in Slavic and other underrepresented languages.

## Method Summary
MERA Multi evaluates multimodal Russian-language models using 18 tasks across text, image, audio, and video modalities. The benchmark employs dual metrics: Exact Match (strict string comparison) and Judge Score (semantic equivalence via a trained RuModernBERT classifier). Evaluation uses block-prompting with 10 diverse prompt templates per task to reduce variance. Datasets are split between public (reproducible) and private (leakage-protected) sets. A submission platform automatically scores models on private test sets. The final score combines Attempted Score (quality) with Coverage (breadth) to favor omni models over specialists. Leakage detection uses multimodal SMIA to compare loss patterns on original vs. perturbed inputs.

## Key Results
- Generalist models (Qwen3-Omni-30B-A3B-Instruct) achieve highest overall scores (0.5 Total Score) on MERA Multi.
- Specialist models lag significantly in audio and video tasks despite strong text performance.
- Human baselines reach 0.80–0.92 depending on task type, highlighting performance gaps.
- Dual metrics (Exact Match + Judge Score) provide balanced evaluation of strict formatting and semantic understanding.
- Block-prompting with 10 templates effectively reduces prompt-specific bias in scoring.

## Why This Works (Mechanism)

### Mechanism 1: Dual-Metric Semantic Evaluation
- **Claim:** Using an LLM-as-a-judge alongside Exact Match enables detection of semantically valid responses that fail formatting constraints.
- **Mechanism:** EM calculates strict string match; Judge Score uses RuModernBERT to evaluate semantic equivalence of (question, gold_answer, prediction) tuples. Final score averages both metrics.
- **Core assumption:** Judge model generalizes to multimodal error modes without bias from phrasing or length.
- **Evidence anchors:** Abstract mentions dual metrics; Section 3.3.1 defines EM/JS; corpus lacks direct evidence on this specific dual-metric logic.
- **Break condition:** Judge model may misclassify correct but complex answers as incorrect if it struggles with multi-step reasoning or LaTeX formatting.

### Mechanism 2: Leakage Detection via Multimodal SMIA
- **Claim:** Membership Inference Attacks can detect if models were trained on benchmark data by comparing loss patterns on original vs. perturbed inputs.
- **Mechanism:** Extends SMIA to multimodal inputs; detector distinguishes clean vs. leaked models based on loss anomalies when processing perturbed versions of data.
- **Core assumption:** Models that memorize data exhibit distinct loss behaviors on original vs. perturbed inputs compared to models seeing data for first time.
- **Evidence anchors:** Section 3.4.2 describes multimodal SMIA extension; Table 5 shows 81%-88% AUC-ROC scores; corpus lacks direct evidence.
- **Break condition:** Models robust to perturbations or with smoothed loss landscapes may yield high false negatives.

### Mechanism 3: Block-Prompting for Variance Reduction
- **Claim:** Averaging performance over 10 diverse prompt structures mitigates prompt-specific bias and improves reliability.
- **Mechanism:** Constructs prompts from fixed blocks (greeting, task description, reasoning request, etc.) and averages scores across variants.
- **Core assumption:** 10 prompt templates adequately sample valid instruction space and model capability is invariant to surface form.
- **Evidence anchors:** Abstract mentions block-prompting; Section 3.3.2 describes the scheme; Appendix E.2 confirms prompts significantly affect scores.
- **Break condition:** Models with systemic instruction-following failures may be unfairly penalized or masked by averaging.

## Foundational Learning

- **Concept: Multimodal Large Language Models (MLLMs)**
  - **Why needed here:** Understanding that these models project non-text modalities into shared embedding space with LLM is crucial for interpreting "Perception" vs. "Reasoning" scores.
  - **Quick check question:** If a model scores high on "Reasoning" but low on "Perception," which component (encoder or LLM backbone) is likely the bottleneck?

- **Concept: LLM-as-a-Judge**
  - **Why needed here:** Paper relies on RuModernBERT to score open-ended answers; understanding this is classification task trained on human-annotated data is required to trust Judge Score metric.
  - **Quick check question:** Why use separate RuModernBERT for judging rather than prompting evaluated model to self-score?

- **Concept: Data Contamination & Leakage**
  - **Why needed here:** Significant portion focuses on preventing models from cheating by training on test set; understanding Membership Inference is required to interpret public/private data split.
  - **Quick check question:** Why is watermarking audio/video alone insufficient to stop data leakage, necessitating MSMIA detector?

## Architecture Onboarding

- **Component map:** Datasets (18 tasks, 3 modalities) -> HuggingFace/public servers -> Evaluation Harness (lm-evaluation-harness + multimodal extensions) -> Judge Model (RuModernBERT-base via vLLM) -> Submission System (automated scoring platform).

- **Critical path:** Input (e.g., Image + Question) -> Prompting (select 1 of 10 block-prompt configs) -> Inference (generate model response) -> Scoring (EM + Judge Score) -> Aggregation (Total Score = Attempted Score × Coverage).

- **Design tradeoffs:**
  - Public vs. Private Data: Public allows reproducibility; private prevents leakage but requires platform submission.
  - Coverage Weighting: Total Score multiplies quality by breadth, favoring omni models over specialists.
  - Strict vs. Semantic: EM is brittle but objective; JS is robust to phrasing but requires trained judge that may hallucinate.

- **Failure signatures:**
  - JS >> EM: Model understands task but cannot follow output formatting instructions.
  - High AUC-ROC in MSMIA: Model likely trained on test set; results are discarded.
  - Low Coverage: Model lacks modality-specific encoders (e.g., text-only LLM).

- **First 3 experiments:**
  1. **Baseline Validation:** Run text-only model (GPT-4o without vision) to establish floor for "Reasoning" independent of "Perception."
  2. **Judge Calibration:** Test Judge model on adversarial examples (wrong reasoning, right answer) to verify alignment with human labels.
  3. **Modality Ablation:** Submit multimodal model with null/dummy data for one modality to verify actual use of multimodal input vs. guessing from text context.

## Open Questions the Paper Calls Out

- **Bias Evaluation Gap:** Authors explicitly state they did not perform explicit evaluation of bias toward underrepresented minorities, identifying this as crucial future work. Current benchmark focuses on semantic abilities without specific tasks for detecting social biases.

- **Hardware-Software Stack Variability:** Authors note that hardware-software stack (GPU models, CUDA versions, batching) affects inference scores, making strict reproducibility difficult despite fixed prompts and decoding settings.

- **Domain Coverage Limitations:** Authors acknowledge that 18 tasks may underrepresent some model abilities or domains crucial for certain applications, questioning whether high MERA Multi scores guarantee robust real-world performance.

## Limitations

- **Judge Model Generalization Uncertainty:** Training data composition and robustness to complex reasoning chains remain opaque, creating uncertainty about JS score reliability across all tasks.

- **Leakage Detector Generalization:** SMIA-based detector shows promise on benchmark data but untested effectiveness on truly unseen models or different architectures may allow contaminated models to pass undetected.

- **Prompt Sampling Assumption:** Assumption that 10 templates adequately sample instruction space is not empirically validated, potentially masking models with instruction-following failures.

## Confidence

- **High:** Benchmark's modular design (public/private split, dual metrics, block-prompting) is well-documented and reproducible; taxonomy provides clear framework.
- **Medium:** Baseline comparisons are internally consistent but lack external validation on other Russian/multimodal benchmarks.
- **Low:** Claims about serving as "blueprint" for Slavic languages assume cultural/linguistic similarity not explicitly tested.

## Next Checks

1. **Judge Model Robustness:** Conduct adversarial testing of RuModernBERT judge on held-out complex reasoning answers to verify it doesn't hallucinate or misclassify correct but verbose responses.

2. **Leakage Detector Cross-Validation:** Apply MSMIA detector to models trained on unrelated datasets (LLaMA, Mistral) to assess false positive rates and ensure it doesn't flag clean models as leaked.

3. **Prompt Template Expansion:** Test effect of increasing block-prompt templates (20 vs. 10) on score variance to empirically validate sufficiency of current sampling strategy.