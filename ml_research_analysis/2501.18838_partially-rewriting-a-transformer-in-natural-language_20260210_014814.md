---
ver: rpa2
title: Partially Rewriting a Transformer in Natural Language
arxiv_id: '2501.18838'
source_url: https://arxiv.org/abs/2501.18838
tags:
- language
- transcoder
- explanations
- latents
- activations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of rewriting a neural network in
  a more interpretable form by replacing parts of the model with natural language
  explanations of its internal features. The authors approximate a feedforward network
  with a sparse transcoder, generate explanations for its latent features, and then
  use a language model to simulate the activations of these latents based on their
  explanations.
---

# Partially Rewriting a Transformer in Natural Language

## Quick Facts
- arXiv ID: 2501.18838
- Source URL: https://arxiv.org/abs/2501.18838
- Reference count: 13
- Primary result: Natural language explanations of neural features fail to outperform zero-ablation baseline without quantile normalization, which improves specificity but reduces sensitivity.

## Executive Summary
This paper tackles the problem of rewriting a neural network in a more interpretable form by replacing parts of the model with natural language explanations of its internal features. The authors approximate a feedforward network with a sparse transcoder, generate explanations for its latent features, and then use a language model to simulate the activations of these latents based on their explanations. They evaluate this approach by measuring the model's cross-entropy loss after partial replacement. The primary result shows that, without proper calibration, replacing even a small fraction of the latents with their natural language explanations degrades performance to the level of a model trained on only 10-15% of the data, performing no better than zeroing out the component entirely. However, with quantile normalization, the performance improves significantly, though it still does not surpass the zero ablation baseline. The paper highlights the need for more specific and detailed explanations to improve the effectiveness of this approach.

## Method Summary
The authors train a sparse transcoder to approximate a feedforward MLP layer using TopK activation with k=32, generating 32,768 latents. They then generate natural language explanations for each latent using automated pipelines, and use Llama 3 to predict activations from these explanations. Quantile normalization is applied to correct LLM prediction miscalibration by matching predicted and empirical activation distributions. The normalized predictions are then patched into the model, and performance is evaluated by measuring cross-entropy loss increase relative to the original model and zero-ablation baseline.

## Key Results
- Without quantile normalization, LLM predictions over-estimate high activations, causing specificity to drop to ~80% and CE loss to far exceed zero-ablation
- Quantile normalization improves specificity but severely reduces sensitivity, still not surpassing zero-ablation baseline
- Detection scores correlate with explanation quality, with higher-scoring latents yielding better specificity and sensitivity
- Replacing latents with explanations performs no better than zeroing them out without proper calibration

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sparse transcoders can approximate MLP layers with interpretable latent features, enabling analysis of component-level computation.
- **Mechanism:** The transcoder learns a wider, sparsely-activating representation of the MLP's input-output mapping. The TopK activation function (k=32) enforces sparsity, while a learned skip connection preserves information flow. The decoder reconstructs the original MLP output from sparse latents.
- **Core assumption:** MLP computations can be decomposed into sparse, semantically meaningful features without catastrophic information loss.
- **Evidence anchors:**
  - [Section 2] "Our loss function is the mean squared error between the transcoder's output and the MLP output... Sparsity is continuously enforced on the transcoder latents using the TopK activation function... with k = 32."
  - [Section 3] "Simply replacing a single MLP with a transcoder increases the model's cross-entropy loss to that of an early Pythia checkpoint—namely one that was trained on only 25% of the data."
- **Break condition:** If reconstruction error is too high or latents remain polysemantic, the transcoder fails to provide interpretable primitives.

### Mechanism 2
- **Claim:** Quantile normalization corrects miscalibrated LLM predictions by matching predicted activation distributions to empirical distributions.
- **Mechanism:** The LLM simulator systematically over-predicts high activations (Figure 1). Quantile normalization applies a monotonic transformation mapping the predicted CDF to the empirical CDF, effectively enforcing a sparsity prior. This reduces false positives but also decreases sensitivity.
- **Core assumption:** The marginal distribution of true activations is the correct calibration target; ranked predictions contain meaningful signal.
- **Evidence anchors:**
  - [Section 2.1] "We found in early experiments that Llama produces highly uncalibrated predictions of feature activations... Patching these uncalibrated activations into the model yields very poor results."
  - [Section 4.1] "By performing quantile normalization, a large chunk of incorrectly predicted activations... are set back to zero. This significantly increases the specificity... but at the same time this significantly decreases the sensitivity."
- **Break condition:** If sample size for estimating predicted CDF is too small (e.g., 1K vs 10K), normalization degrades and substitution performance drops sharply.

### Mechanism 3
- **Claim:** Detection scoring predicts explanation quality for simulation tasks—higher scores correlate with better specificity and sensitivity.
- **Mechanism:** Detection scoring evaluates whether an explanation can distinguish between contexts where a latent is active vs. inactive. This directly overlaps with simulation requirements. Higher-scoring explanations yield better-calibrated predictions when patched into the model.
- **Core assumption:** Explanation quality is not uniform; some latents are genuinely more interpretable than others.
- **Evidence anchors:**
  - [Section 4] "We find that detection scores... are predictive of the specificity and sensitivity of an explanation, with higher scoring latents corresponding to explanations that have higher specificity and sensitivity (Figure 4)."
  - [Section 4, Figure 3] "Bars in green show the average loss increase when choosing the top scoring latents for replacement" vs. random selection.
- **Break condition:** If detection scoring fails to measure faithfulness (e.g., explanations that score well but don't reflect true computation), selecting top-scoring latents won't improve substitution.

## Foundational Learning

- **Concept:** Sparse Autoencoders and Transcoders
  - **Why needed here:** The entire methodology depends on extracting interpretable features from neural activations. Understanding encoder/decoder structure, sparsity penalties (TopK), and reconstruction objectives is prerequisite.
  - **Quick check question:** Can you explain why a transcoder trained on MLP input-output pairs differs from an SAE trained on residual stream activations?

- **Concept:** Quantile Normalization / Distribution Calibration
  - **Why needed here:** The paper identifies uncalibrated predictions as the primary failure mode. Understanding how quantile matching works—and its bias-variance tradeoffs with finite samples—is essential.
  - **Quick check question:** If you have 10K predicted samples but 10M ground-truth samples, which CDF estimate is more reliable and why?

- **Concept:** Specificity vs. Sensitivity in Feature Detection
  - **Why needed here:** The paper's core finding is that explanations lack specificity (too many false positives). Understanding this tradeoff clarifies why quantile normalization helps specificity but hurts sensitivity.
  - **Quick check question:** With 32,768 latents and k=32 active per token, what specificity is required to avoid predicting ~320 active latents on average?

## Architecture Onboarding

- **Component map:** Transcoder training -> Explanation generation -> LLM simulator -> Quantile normalizer -> Patching
- **Critical path:** Transcoder quality -> explanation faithfulness -> simulator calibration -> normalization sample size -> substitution performance. Failures compound: poor transcoder reconstruction alone adds ~25% training-equivalent loss.
- **Design tradeoffs:**
  - Skip connection improves reconstruction but may reduce latent interpretability (not analyzed)
  - Larger calibration samples improve normalization but require expensive LLM inference (327M predictions for 10K prompts)
  - Selecting top-scoring latents vs. random: top-scoring performs better but doesn't beat zero-ablation baseline
- **Failure signatures:**
  - Loss equivalent to zero-ablation: explanations insufficiently specific
  - Loss worse than zero-ablation: quantile normalization not applied or sample size too small
  - High false positive rate (~320 predicted active vs. 32 true): specificity below ~99.9%
- **First 3 experiments:**
  1. **Baseline calibration check:** Train transcoder, generate explanations, plot predicted vs. true activation distributions before normalization to confirm miscalibration.
  2. **Sample size ablation:** Compare normalization with 1K vs. 10K predicted samples (Figure A2) to quantify sensitivity.
  3. **Partial substitution sweep:** Replace increasing fractions of latents (Figure 3), comparing top-scoring vs. random selection against zero-ablation baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can contrastive explanations—using pairs of highly similar features to highlight distinguishing details—substantially improve both specificity and sensitivity in model rewriting?
- **Basis in paper:** [explicit] "To improve upon these results, new techniques are needed to make explanations more specific, for instance using contrast pairs of highly similar features to bring out additional details. This could potentially increase the sensitivity as well..."
- **Why unresolved:** Current explanations achieve only ~80% specificity, far below the ~99.7% needed (32/32768 latents). Quantile normalization improves specificity but severely reduces sensitivity.
- **What evidence would resolve it:** A modified pipeline generating contrastive explanations, evaluated on whether specificity improves without sacrificing sensitivity, yielding loss substantially below the zero-ablation baseline.

### Open Question 2
- **Question:** Would bias-corrected estimators for the empirical inverse CDF improve sample complexity and reduce distribution mismatch in quantile normalization?
- **Basis in paper:** [explicit] "In a future draft of this paper we plan to experiment with bias-corrected estimators for the population quantiles (Hyndman & Fan, 1996), which will hopefully improve the sample complexity."
- **Why unresolved:** The empirical inverse CDF is biased for the true inverse CDF, causing poor generalization. Current approach uses 10K predicted samples vs. 10M true samples, creating asymmetric uncertainty.
- **What evidence would resolve it:** Experiments comparing standard vs. bias-corrected quantile estimators across varying sample sizes, measuring convergence of the normalized distribution to the empirical distribution.

### Open Question 3
- **Question:** How does natural language simulation performance scale with base model size, layer depth, or fraction of simultaneously rewritten components?
- **Basis in paper:** [inferred] The study only rewrites a single MLP (layer 6 of Pythia 160M), noting that "rewriting all MLP blocks simultaneously would likely cause the model to become completely unusable."
- **Why unresolved:** It is unknown whether larger models have more interpretable features, whether earlier/later layers differ in simulation fidelity, or whether partial rewriting degrades gracefully.
- **What evidence would resolve it:** Systematic experiments varying model scale (e.g., Pythia 70M–1.4B), layer position, and number of simultaneously rewritten layers, measuring cross-entropy loss relative to zero-ablation.

## Limitations
- Explanations lack sufficient specificity (~80% vs. required ~99.9%), generating too many false positives to be useful replacements
- Zero-ablation baseline remains competitive even with quantile normalization, suggesting explanations capture at most 5-10% of predictive capacity
- Reliance on quantile normalization as post-hoc fix suggests LLM calibration problem may be inherent rather than solvable through better prompting

## Confidence
- **High confidence:** The empirical finding that uncalibrated LLM predictions degrade performance significantly, and that quantile normalization improves but doesn't surpass the zero-ablation baseline
- **Medium confidence:** The interpretation that explanations lack specificity rather than sensitivity as the primary failure mode
- **Low confidence:** Whether alternative explanation generation methods or different LLM architectures could achieve better than zero-ablation performance

## Next Checks
1. **Specificity threshold validation**: Systematically measure how CE loss varies with explanation specificity across the 99%-99.99% range to confirm the 99.9% threshold hypothesis and identify if there's a non-zero-ablation performance sweet spot.
2. **Latent dimension ablation**: Test whether increasing latent dimensionality (e.g., 65,536 vs 32,768) improves reconstruction quality and explanation faithfulness, or if the bottleneck is fundamental to the explanation generation process.
3. **Alternative calibration methods**: Compare quantile normalization against other calibration approaches (temperature scaling, isotonic regression) to determine if the calibration problem is specific to the CDF-matching approach or inherent to LLM-based prediction of neural activations.