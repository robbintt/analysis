---
ver: rpa2
title: The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement
  and Trace Back Responsibilities
arxiv_id: '2507.14909'
source_url: https://arxiv.org/abs/2507.14909
tags:
- case
- such
- which
- artificial
- intelligence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The Endless Tuning framework is a design method for deploying\
  \ AI systems that avoids user replacement and enables tracing of responsibilities\
  \ through a double mirroring process. Implemented in three prototypes\u2014loan\
  \ granting, pneumonia diagnosis, and art style recognition\u2014it combines human\
  \ decision-making with AI assistance via explanations, similarity comparisons, and\
  \ confidence measures."
---

# The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities

## Quick Facts
- arXiv ID: 2507.14909
- Source URL: https://arxiv.org/abs/2507.14909
- Reference count: 40
- The Endless Tuning framework is a design method for deploying AI systems that avoids user replacement and enables tracing of responsibilities through a double mirroring process.

## Executive Summary
The Endless Tuning framework presents a human-AI collaborative decision-making protocol designed to maintain user agency and enable post-hoc responsibility attribution. Through three prototype applications—loan granting, pneumonia diagnosis, and art style recognition—the method demonstrates how structured interaction patterns, explanation diversity, and comprehensive logging can preserve human control while maintaining AI assistance. The approach shifts focus from maximizing pure accuracy to balancing governability of errors with user empowerment, tested through qualitative feedback from domain experts who reported feeling fully in control despite AI assistance.

## Method Summary
The Endless Tuning protocol implements a 5-step human-AI decision process: (1) user forms initial impression with annotation, (2) two divergent explanations (RISE and Grad-CAM) are shown without revealing the model's prediction, (3) similar training cases are retrieved via PCA on embeddings, (4) model prediction and confidence are revealed, and (5) user makes final decision. Three prototypes were implemented: loan approval using a decision tree (~83% accuracy), pneumonia diagnosis using ResNet-34 (~95% accuracy), and art style recognition using ResNet-34 (~65% accuracy). The system logs all interactions, model states, and random seeds for accountability purposes. Continuous finetuning occurs using user-labeled cases through rehearsal-based learning.

## Key Results
- Domain experts reported feeling fully in control during decision-making without sense of replacement
- Model accuracy achieved: loan granting ~83%, pneumonia diagnosis ~95%, art style recognition ~65%
- The logging system creates comprehensive traces linking model state, explanations, and user actions for potential liability attribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Delaying model output disclosure until after user reflection reduces automation bias and maintains human agency.
- Mechanism: The protocol forces users through a sequence: (1) form first impression, (2) view explanations without knowing the prediction, (3) compare similar cases, (4) finally see model output with confidence. This engages System 2 thinking before any recommendation can anchor the user.
- Core assumption: Users who commit to an initial judgment before seeing AI output will maintain ownership of the final decision.
- Evidence anchors:
  - [abstract] "full control was perceived by the interviewees in the decision-making setting"
  - [Section 3.2] "the model has already produced an outcome, but this second step involves presenting the user either with an account of the process... before knowing the outcome of the model, avoiding 'apophantic' interfaces"
  - [corpus] Related work on cognitive forcing functions (Buçinca et al.) is cited but not directly tested against this protocol.
- Break condition: If users skip steps or rush through annotations without genuine reflection, the cognitive forcing fails.

### Mechanism 2
- Claim: Presenting multiple divergent explanations (e.g., RISE vs. Grad-CAM) hermeneutically surfaces model uncertainty and prompts critical user engagement.
- Mechanism: Two different XAI algorithms on the same input produce different saliency maps. The user must reconcile these conflicting "perspectives," treating the model as an interpreter rather than an authority. This creates a "contact without contact" relation.
- Core assumption: Explanation divergence is epistemically productive rather than confusing for domain experts.
- Evidence anchors:
  - [Section 3.2] "we have the same black box, but two different explanations... The idea is explicitly to highlight some distance between signifier and signified"
  - [Section 4.2] The aesthetics professor "engaged in a sort of comparison between these two explainers, confirming their hermeneutic significance"
  - [corpus] ClarifAI paper proposes similar case-based reasoning for interpretability but without the hermeneutic framing or delayed revelation.
- Break condition: If explanations are uniformly poor or incomprehensible (as in the pneumonia case where the radiologist found maps "completely inaccurate"), users may distrust the system entirely.

### Mechanism 3
- Claim: A permanent, comprehensive log linking model state, explanations, and user actions enables post-hoc responsibility attribution across the supply chain.
- Mechanism: Every session records: timestamps, case studies, user annotations, model weights, explanation outputs (including random seeds), and decisions. In case of harm, an authority can replay the process "in slow motion" to distribute responsibility among user, developer, interface designer, or data annotator.
- Core assumption: Legal systems will accept computational logs as evidence for liability attribution.
- Evidence anchors:
  - [abstract] "a bridge can be built between accountability and liability in case of damage"
  - [Section 5] "the idea is that, in case of a lawsuit, a judge should be able to review the entire process in 'slow motion'"
  - [corpus] No direct corpus evidence on legal admissibility; this remains a theoretical contribution.
- Break condition: If logging is incomplete, tampered with, or if random processes cannot be fully reproduced, the trace breaks.

## Foundational Learning

- Concept: **Automation bias and algorithmic aversion**
  - Why needed here: The entire protocol is designed to counteract over-reliance on AI recommendations while avoiding the opposite problem of rejecting useful AI input.
  - Quick check question: Can you explain why showing confidence scores above 80% might increase trust even when predictions are wrong?

- Concept: **System 1 vs. System 2 cognition (Kahneman)**
  - Why needed here: The sequential protocol explicitly aims to engage "sluggish" System 2 thinking before allowing fast, intuitive judgments to be overridden by AI output.
  - Quick check question: Which steps in the protocol are designed to slow down automatic responses?

- Concept: **XAI methods (saliency maps, RISE, Grad-CAM)**
  - Why needed here: The protocol uses explanations not as justifications but as interpretive prompts. Understanding what different explainers actually show (full network vs. single layer) is essential for debugging.
  - Quick check question: Why might RISE and Grad-CAM produce different saliency maps for the same input on the same model?

## Architecture Onboarding

- Component map:
  - Model backbone -> Explanation module -> Similarity engine -> Session logger -> Finetuning pipeline

- Critical path:
  1. Train base model on labeled data; hold out case study set + temporary set
  2. User selects case → records first impression (with annotation)
  3. Generate explanations (before showing prediction) → user reviews
  4. Retrieve similar training cases via compressed embeddings → user compares
  5. Reveal model prediction + confidence → user makes final decision
  6. Save labeled case to finetuning buffer; trigger retraining when buffer reaches threshold

- Design tradeoffs:
  - **Accuracy vs. control**: The paper explicitly trades some accuracy for governability of errors
  - **Explanation fidelity vs. hermeneutic distance**: Faithful explanations may not provoke reflection; "unfaithful" ones may be productive
  - **Session length vs. user engagement**: Too many steps risks user fatigue; too few loses cognitive forcing

- Failure signatures:
  - User annotates minimally or copies model output → engagement failure
  - Explanations highlight obviously irrelevant features → trust collapse (seen in pneumonia case)
  - Similarity retrieval returns semantically unrelated cases → embedding quality issue
  - Finetuning causes catastrophic forgetting → rehearsal strategy inadequate

- First 3 experiments:
  1. Replicate one prototype (loan or pneumonia) with the open-source code; verify explanation outputs match paper figures for same inputs
  2. Ablate the explanation-before-prediction step: show prediction first, measure whether user agreement with model changes
  3. Stress-test the logging system: after a session, attempt to reproduce the exact explanation maps from saved random seeds and model weights

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Endless Tuning framework perform in longitudinal deployments where repeated finetuning sessions occur, and does user agency persist over time as the model adapts?
- Basis in paper: [explicit] The authors acknowledge that «a complete test in the long run would have been required, yet entailing too much time and, perhaps, organizational costs» and only conducted single-session tests with each expert.
- Why unresolved: No longitudinal data exists to determine whether perceived control and avoidance of replacement persist after multiple finetuning cycles, or whether users become dependent on adapted models.
- What evidence would resolve it: Extended field studies with domain experts using the system over months, tracking both behavioral metrics (decision independence, override rates) and subjective measures of agency across finetuning iterations.

### Open Question 2
- Question: Can the Endless Tuning protocol be adapted for generative AI systems while maintaining its core principles of avoiding replacement and enabling responsibility tracing?
- Basis in paper: [explicit] Section 3 states that «the case of generative artificial intelligence is left for future research» and the conclusion expresses hope that the design «could serve as inspiration and bring benefits in the field of generative artificial intelligence too, allowing people to participate right in the generation process.»
- Why unresolved: The protocol was designed specifically for classification/regression tasks; generative AI involves fundamentally different interaction patterns, output spaces, and notions of "ground truth" for responsibility attribution.
- What evidence would resolve it: Prototype implementations of Endless Tuning principles in generative domains (e.g., text, image generation) with expert users, measuring whether similar agency preservation and accountability mechanisms can be operationalized.

### Open Question 3
- Question: How can similarity comparison components be calibrated to balance embedding-based formal similarity with semantic or domain-specific expertise embedded in training labels?
- Basis in paper: [inferred] The art style recognition experiment revealed that «the extraction of tiny vectors from a latent space [...] almost ended up conflicting with the original labeling of the data, neither allowing the annotators' expertise to shine through.» The expert noted images were formally similar but materially distant in meaningful ways.
- Why unresolved: The paper acknowledges this as «a weak point» where proper balance «was not reached in our prototype,» but offers no solution for aligning computational similarity with expert judgment.
- What evidence would resolve it: Systematic experiments varying similarity metrics (latent space, feature-based, hybrid approaches) against expert similarity judgments, measuring alignment between computational and human similarity assessments across domains.

### Open Question 4
- Question: What is the optimal tradeoff between model accuracy and remediation capacity in human-AI decision systems, and how does this vary by domain risk level?
- Basis in paper: [explicit] Section 3 poses: «would it be preferable to pursue higher accuracy with less control over potential negative consequences, or slightly lower accuracy with greater capacity for remediation?»
- Why unresolved: The paper frames this as a design choice but provides no empirical framework for evaluating this tradeoff across different contexts (medical diagnosis vs. loan approval vs. art recognition).
- What evidence would resolve it: Comparative studies across domains measuring outcomes (accuracy, harm rates, recovery from errors) under different accuracy/remediation configurations, coupled with user preference elicitation.

## Limitations
- The study relies entirely on qualitative feedback from 11 domain experts rather than controlled experiments measuring decision accuracy or bias
- No quantitative comparison exists between Endless Tuning and standard AI-assisted decision-making interfaces
- Legal claims about liability tracing are theoretical with no validation that courts would accept computational logs as admissible evidence

## Confidence

- **High confidence**: The technical implementation of the 5-step protocol (data preprocessing, model training, explanation generation, logging) is reproducible from provided code and documentation
- **Medium confidence**: Domain experts reported feeling in control during interviews, but this is self-reported perception without behavioral or accuracy measures
- **Low confidence**: Claims about legal liability bridging and hermeneutic epistemology driving better decisions are theoretical contributions without empirical validation

## Next Checks

1. Conduct a controlled experiment comparing Endless Tuning against standard AI interfaces: measure decision accuracy, user trust calibration, and time-to-decision across 50+ participants per condition
2. Perform think-aloud protocols during the 5-step process to verify users engage in the intended reflective interpretation rather than superficial compliance with protocol steps
3. Test the logging system's legal robustness: create a simulated "damage" scenario and attempt to reconstruct responsibility attribution using only the saved logs, evaluating completeness and reproducibility