---
ver: rpa2
title: 'OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language
  Models in Education'
arxiv_id: '2510.26422'
source_url: https://arxiv.org/abs/2510.26422
tags:
- knowledge
- dimension
- chinese
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OmniEduBench is a comprehensive Chinese educational benchmark designed
  to evaluate large language models across both knowledge and cultivation dimensions.
  The benchmark consists of 24.602K high-quality question-answer pairs covering 61
  subjects and 11 common exam question types.
---

# OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language Models in Education

## Quick Facts
- arXiv ID: 2510.26422
- Source URL: https://arxiv.org/abs/2510.26422
- Reference count: 14
- Primary result: 24.602K Chinese educational Q&A pairs across 61 subjects and 11 exam types, revealing significant LLM performance gaps in pedagogical tasks

## Executive Summary
OmniEduBench addresses the need for comprehensive evaluation of large language models in educational contexts by providing a benchmark that spans both knowledge acquisition and pedagogical skill cultivation. The benchmark includes 24.602K high-quality question-answer pairs covering 61 subjects and 11 common exam question types, with questions sourced from public resources, private school exams, and LLM synthesis. Through dual-model filtering, the benchmark identifies particularly challenging questions, creating an OmniEduBench HARD subset where even the best-performing model achieves less than 50% accuracy. Experiments on 11 mainstream LLMs reveal significant performance gaps, with Gemini-2.5 Pro achieving 62.78% accuracy on knowledge tasks while QwQ scores 70.27% on cultivation tasks, highlighting the importance of evaluating both dimensions separately.

## Method Summary
OmniEduBench was constructed through a multi-stage process beginning with collection of 657K question-answer pairs from public sources (XuekeNet, ZujuanNet), private school exams, and LLM synthesis. The data underwent cleaning and conversion to Markdown format via MinerU, followed by dual-machine filtering using QWQ-32B and Qwen3-235B to identify difficult questions. A verification phase involving master's students and senior experts ensured quality. The final benchmark consists of 24.602K Q&A pairs evaluated using direct extraction for deterministic question types and LLM-assisted scoring for open-ended responses, with Gemini-2.5 Pro serving as the default judge. Models were tested in zero-shot and few-shot configurations across both knowledge and cultivation dimensions.

## Key Results
- Gemini-2.5 Pro achieved highest knowledge dimension accuracy at 62.78%
- QwQ performed best in cultivation dimension at 70.27%
- Current models trail human-level performance by nearly 30% in cultivation tasks
- Even on the high-difficulty OmniEduBench HARD subset, the best model achieved less than 50% accuracy
- Significant performance gaps exist between knowledge (62.78%) and cultivation (70.27%) dimensions

## Why This Works (Mechanism)

### Mechanism 1
Sequential model filtering creates a progressively harder benchmark subset by requiring questions to defeat multiple models. The dual-machine filtering passes 657K Q&A pairs through QWQ-32B first, retaining only incorrectly answered questions (430K). These pass through Qwen3-235B, yielding 50K questions that both models failed. This creates a "worst-of-both-models" intersection where questions exploit different weakness profiles. Core assumption: Models have partially overlapping but non-identical failure modes; a question hard for one model may be easy for another, so sequential filtering is more selective than single-model filtering.

### Mechanism 2
Separating evaluation into knowledge and cultivation dimensions reveals capability gaps masked by single-score aggregation. Knowledge dimension uses 11 exam types requiring domain expertise and reasoning. Cultivation dimension uses multiple-choice scenarios testing pedagogical judgment (empathy, feedback timing, conflict resolution). Different output formats stress different model capabilities. Core assumption: Educational competence is not monolithic; high knowledge accuracy does not correlate with sound pedagogical decision-making.

### Mechanism 3
Private data inclusion reduces benchmark contamination by sampling from distributions unlikely to appear in pretraining corpora. Internal school exam papers have "never appeared on the public Internet or been included in large-scale web crawls," making them resistant to memorization-based performance inflation. Core assumption: Data contamination is a primary validity threat for LLM benchmarks; models can achieve high scores via memorization without genuine capability.

## Foundational Learning

- **Concept: Benchmark contamination and memorization vs. generalization**
  - Why needed here: OmniEduBench explicitly addresses contamination through private data; understanding this threat model is essential for interpreting results and designing future benchmarks.
  - Quick check question: If a model scores 95% on a benchmark but only 60% on held-out private data from the same domain, what does this suggest about the original score's validity?

- **Concept: Dual-model filtering / difficulty amplification**
  - Why needed here: The 657K→50K reduction via sequential filtering is the core quality control mechanism; understanding why two models are better than one explains the benchmark's discriminative power.
  - Quick check question: Why might a question that stumps a 32B model be trivial for a 235B model, and how does sequential filtering handle this asymmetry?

- **Concept: LLM-as-judge evaluation**
  - Why needed here: Short-answer questions use LLM-assisted scoring; evaluator model quality directly affects measured performance gaps.
  - Quick check question: Table 6 shows GPT-4o as a scoring model gives different rankings than Gemini-2.5 Pro as scorer—which scorer should you trust and why?

## Architecture Onboarding

- **Component map:**
  ```
  Collection Layer: Public (21K) + Private (106K) + LLM-Synthesized (800K)
       ↓
  Cleaning Layer: MinerU conversion → Metadata extraction → Dedup/sensitive removal
       ↓
  Filtering Layer: QWQ-32B filter → Qwen3-235B filter → 50K hard questions
       ↓
  Verification Layer: 50 master's students (initial) → 5 senior experts (15% sample review)
       ↓
  Evaluation Layer: Choice-based scoring + LLM-assisted scoring (Gemini-2.5 Pro default)
  ```

- **Critical path:** The dual-filtering stage is the bottleneck—if either model is miscalibrated or has format biases, the final benchmark difficulty distribution shifts unpredictably. The choice of filter models (QWQ-32B, Qwen3-235B) determines which capability gaps the benchmark will expose.

- **Design tradeoffs:**
  - Breadth vs. depth: 61 subjects provide coverage but limit per-subject statistical power; some subjects have <100 samples.
  - LLM-synthesized data scale: 800K synthesized vs. 127K human-authored means the cultivation dimension (primarily synthesized) may have different validity characteristics than knowledge dimension.
  - Scorer dependency: LLM-assisted scoring introduces another model as a dependency; scorer errors compound with model errors.

- **Failure signatures:**
  - If all models cluster within 5% on a category, the category lacks discriminative power (possible ceiling/floor effect).
  - If few-shot performance degrades vs. zero-shot (as seen with some models in Table 5), the benchmark may have format misalignment with model instruction tuning.
  - If OmniEduBench HARD accuracy is <30% across all models, the subset may be too narrow to guide improvement.

- **First 3 experiments:**
  1. **Contamination probe:** Train a small model on public OmniEduBench data only; evaluate on private subset. Large performance gaps indicate private data measures different capabilities.
  2. **Filter sensitivity analysis:** Re-filter using different model pairs (e.g., DeepSeek-V3.1 + Claude-4 Sonnet); measure correlation of resulting HARD subsets to assess filter-model dependency.
  3. **Scorer calibration:** Run LLM-assisted scoring with 3 different scorer models on the same 500 short-answer responses; compute inter-scorer agreement to quantify evaluation variance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does the choice of the LLM-assisted scoring model introduce systematic bias when evaluating open-ended responses in the cultivation dimension?
- **Basis in paper:** [inferred] Table 6 and Section 3.3 demonstrate that "the performance of the scoring model directly affects the evaluation outcomes," showing different accuracy rankings depending on whether GPT-4o, Qwen3, or Gemini-2.5 Pro is used as the judge.
- **Why unresolved:** The paper establishes that scoring variance exists but does not determine which judge model aligns most closely with human ground truth or if a single model is sufficient for unbiased evaluation.
- **What evidence would resolve it:** A correlation analysis comparing LLM-assisted scores against a gold-standard dataset of human expert evaluations for the cultivation dimension.

### Open Question 2
- **Question:** How will LLM performance differ when OmniEduBench is extended to include multimodal educational scenarios?
- **Basis in paper:** [explicit] The conclusion states the intention to "introduce multimodal educational scenarios, further enhancing the benchmark’s role in evaluating and guiding the comprehensive capabilities of LLMs and MLLMs."
- **Why unresolved:** The current benchmark and experiments are limited to text-based question-answering, leaving the evaluation of visual and auditory educational capabilities unexplored.
- **What evidence would resolve it:** The release of a multimodal version of OmniEduBench with baseline performance metrics for both text-only LLMs and multimodal LLMs (MLLMs).

### Open Question 3
- **Question:** What specific reasoning capabilities must LLMs develop to improve their accuracy on the OmniEduBench HARD subset beyond the current 50% threshold?
- **Basis in paper:** [explicit] Section 3.3 notes that even the best-performing model (Gemini) achieves less than 50% accuracy on OmniEduBench HARD, highlighting the need to "enhance LLMs’ ability to generalize and maintain high performance on hard subsets."
- **Why unresolved:** The paper identifies the performance gap but does not analyze the specific failure modes (e.g., logical reasoning vs. domain knowledge) causing models to fail on these difficult samples.
- **What evidence would resolve it:** A detailed error analysis of state-of-the-art models on OmniEduBench HARD, categorizing failures by reasoning type to identify specific cognitive bottlenecks.

## Limitations

- The dual-model filtering mechanism may create an overly narrow HARD subset that overfits to specific failure modes of the chosen filter models rather than representing genuinely difficult educational content
- Heavy reliance on LLM-synthesized data (800K pairs) for the cultivation dimension introduces potential validity concerns and may not capture the same pedagogical nuance as human-authored content
- The benchmark's Chinese language focus and K-12 curriculum alignment limit direct applicability to other educational contexts or language domains

## Confidence

- **High Confidence:** Knowledge dimension results and comparative model rankings are robust. The direct extraction scoring method eliminates scorer variance, and the 11 mainstream models provide stable relative performance measurements. The 62.78% vs 70.27% dimension gap is consistently observed across multiple models.
- **Medium Confidence:** Cultivation dimension results and the claimed 30% human-level performance gap. LLM-assisted scoring introduces variability (Table 6 shows scorer-dependent rankings), and the synthesis-heavy data collection may affect construct validity. The cultivation tasks' multiple-choice format may not fully capture the pedagogical complexity claimed.
- **Low Confidence:** HARD subset difficulty calibration and the specific numerical claims about model weaknesses. The dual-filtering mechanism's effectiveness depends heavily on the particular model pair chosen, and no ablation studies demonstrate alternative filtering approaches. The 30% performance gap to human level lacks direct human baseline comparisons.

## Next Checks

1. **Filter Sensitivity Analysis:** Re-filter the OmniEduBench corpus using different model pairs (e.g., DeepSeek-V3.1 + Claude-4 Sonnet) and compare the resulting HARD subsets' question overlap and model performance distributions. This quantifies how dependent the benchmark's difficulty profile is on the specific filtering models.

2. **Scorer Calibration Study:** Evaluate the same 1,000 short-answer questions using three different scorer models (Gemini-2.5 Pro, GPT-4o, Qwen3-235B) and compute inter-scorer agreement. This measures the variance introduced by LLM-assisted scoring and validates whether reported cultivation performance differences are scorer-dependent.

3. **Human Baseline Collection:** Have 20 subject-matter experts independently score 200 randomly selected questions from both dimensions, comparing human-human agreement rates to LLM-human agreement rates. This quantifies whether the LLM judge performs comparably to human evaluators and validates the claimed 30% performance gap to human-level performance.