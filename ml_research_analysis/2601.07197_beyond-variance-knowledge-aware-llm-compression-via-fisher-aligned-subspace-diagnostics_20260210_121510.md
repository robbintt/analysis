---
ver: rpa2
title: 'Beyond Variance: Knowledge-Aware LLM Compression via Fisher-Aligned Subspace
  Diagnostics'
arxiv_id: '2601.07197'
source_url: https://arxiv.org/abs/2601.07197
tags:
- fasc
- arxiv
- layers
- compression
- factual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FASC is a knowledge-aware activation compression method that uses
  Fisher Information Matrix to identify gradient-sensitive dimensions, preserving
  factual knowledge better than variance-based SVD. By aligning compression with the
  loss landscape, FASC maintains 6-8% more accuracy on knowledge-intensive tasks at
  50% rank reduction.
---

# Beyond Variance: Knowledge-Aware LLM Compression via Fisher-Aligned Subspace Diagnostics

## Quick Facts
- arXiv ID: 2601.07197
- Source URL: https://arxiv.org/abs/2601.07197
- Authors: Ibne Farabi Shihab; Sanjeda Akter; Anuj Sharma
- Reference count: 40
- Key outcome: FASC preserves 6-8% more accuracy on knowledge-intensive tasks at 50% rank reduction compared to variance-based SVD compression

## Executive Summary
FASC (Fisher-Aligned Subspace Compression) introduces a knowledge-aware activation compression method that uses Fisher Information Matrix to identify gradient-sensitive dimensions in transformer models. Unlike traditional variance-based compression that targets high-variance dimensions, FASC aligns compression with the loss landscape by focusing on dimensions most sensitive to parameter gradients. This approach preserves factual knowledge more effectively, enabling a 7B model to match the factual recall of a 13B uncompressed model while maintaining similar inference efficiency to SVD compression.

The method employs a Dependence Violation Score (ρ) to diagnose which layers require gradient-aware compression, revealing that factual knowledge is stored in low-variance but gradient-critical subspaces. By leveraging Fisher Information Matrix to capture parameter sensitivity to prediction errors, FASC achieves 6-8% better accuracy on knowledge-intensive tasks at 50% rank reduction compared to traditional variance-based methods.

## Method Summary
FASC combines Fisher Information Matrix computation with Singular Value Decomposition to create knowledge-aware activation compression. The method computes the Fisher Information Matrix to identify gradient-sensitive dimensions that traditional variance-based methods might miss, then performs SVD on these gradient-critical subspaces rather than raw activations. The Dependence Violation Score (ρ) serves as a diagnostic tool to identify which layers store factual knowledge in low-variance but gradient-sensitive dimensions, guiding where gradient-aware compression is most needed. This approach preserves critical knowledge dimensions while achieving compression ratios comparable to traditional methods.

## Key Results
- FASC maintains 6-8% more accuracy on knowledge-intensive tasks compared to variance-based SVD at 50% rank reduction
- A 7B model using FASC compression matches the factual recall performance of a 13B uncompressed model
- The Dependence Violation Score (ρ) successfully identifies layers where factual knowledge resides in low-variance but gradient-critical subspaces

## Why This Works (Mechanism)
FASC works by recognizing that factual knowledge in LLMs is stored in dimensions that are sensitive to parameter gradients but may not exhibit high variance during activation. Traditional compression methods like SVD target high-variance dimensions, potentially discarding critical knowledge-bearing subspaces. By computing the Fisher Information Matrix, FASC identifies dimensions where small parameter changes significantly impact prediction accuracy, ensuring these gradient-sensitive dimensions are preserved during compression. This gradient-aligned approach maintains the model's ability to accurately recall factual information while achieving comparable compression ratios to traditional methods.

## Foundational Learning
- **Fisher Information Matrix**: Measures the sensitivity of model predictions to parameter changes, essential for identifying gradient-critical dimensions that preserve knowledge
  - Why needed: Traditional variance-based methods miss gradient-sensitive but low-variance dimensions that store factual knowledge
  - Quick check: Verify Fisher matrix computation correctly identifies dimensions where parameter updates most affect prediction accuracy

- **Singular Value Decomposition (SVD)**: Matrix factorization technique used to compress activations by retaining top singular values/vectors
  - Why needed: Provides the mathematical foundation for dimensionality reduction while FASC modifies which dimensions are prioritized
  - Quick check: Confirm SVD implementation correctly reduces dimensionality while preserving most variance in selected subspaces

- **Dependence Violation Score (ρ)**: Diagnostic metric that quantifies the discrepancy between variance and gradient sensitivity in each layer
  - Why needed: Identifies layers where factual knowledge resides in low-variance but gradient-critical subspaces, guiding compression decisions
  - Quick check: Validate ρ correctly identifies layers that benefit most from gradient-aware compression versus standard variance-based methods

- **Knowledge-intensive task benchmarks**: Evaluation framework focusing on factual recall and knowledge retrieval tasks
  - Why needed: Provides quantitative measurement of whether compression preserves critical factual information
  - Quick check: Ensure benchmark tasks cover diverse knowledge domains and accurately measure factual recall performance

## Architecture Onboarding

**Component Map:**
Input Activations -> Fisher Information Matrix Computation -> Gradient Sensitivity Analysis -> Dependence Violation Score (ρ) -> SVD on Critical Subspaces -> Compressed Activations -> Model Output

**Critical Path:**
The critical path involves computing the Fisher Information Matrix for each layer, using ρ to identify gradient-sensitive subspaces, performing SVD on these subspaces rather than raw activations, and reconstructing compressed activations that preserve knowledge-critical dimensions while reducing overall dimensionality.

**Design Tradeoffs:**
FASC trades additional computational overhead for Fisher matrix computation against improved knowledge preservation. While this adds complexity compared to pure SVD, the method achieves comparable inference efficiency while better preserving factual knowledge. The approach requires careful tuning of which layers receive gradient-aware compression based on ρ scores.

**Failure Signatures:**
- Compression ratio too high, leading to loss of gradient-sensitive dimensions despite FASC's protection
- Incorrect Fisher matrix computation resulting in misidentification of gradient-critical dimensions
- Over-reliance on ρ scores without considering layer-specific characteristics
- Implementation bugs in the SVD computation on Fisher-aligned subspaces

**First Experiments:**
1. Compare FASC vs standard SVD compression on a small transformer layer using a synthetic factual knowledge task to verify knowledge preservation
2. Compute and visualize ρ scores across different layers to validate the diagnostic capability identifies knowledge-critical regions
3. Perform ablation study varying compression ratios to find the optimal balance between compression and knowledge retention

## Open Questions the Paper Calls Out
The paper acknowledges that its evaluation focuses primarily on factual recall benchmarks, with less emphasis on other capabilities like reasoning or coding. This raises questions about how FASC compression generalizes to broader task distributions beyond knowledge-intensive tasks. The computational overhead of computing the Fisher Information Matrix and the practical trade-offs in training/inference efficiency need more thorough analysis.

## Limitations
- Evaluation focuses primarily on factual recall benchmarks, with limited assessment of reasoning, coding, or other capabilities
- Computational overhead of Fisher Information Matrix computation not fully characterized in terms of wall-clock time and resource requirements
- Dependence Violation Score (ρ) provides useful diagnostics but may be difficult for practitioners to interpret without statistical expertise
- Limited ablation studies on different model scales to validate claims about 7B matching 13B performance

## Confidence
- FASC preserves 6-8% more accuracy on knowledge-intensive tasks at 50% compression: **High**
- Factual knowledge resides in low-variance but gradient-critical subspaces: **Medium**
- 7B model using FASC matches 13B uncompressed model performance: **Medium**

## Next Checks
1. Evaluate FASC compression across diverse task categories (reasoning, coding, multi-step problems) beyond factual recall to assess generalization
2. Conduct ablation studies comparing FASC against other gradient-aware compression methods like LoRA and IA-RED² under identical experimental conditions
3. Analyze the computational overhead and wall-clock time implications of computing the Fisher Information Matrix during both training and inference phases