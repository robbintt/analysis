---
ver: rpa2
title: 'Agent Identity Evals: Measuring Agentic Identity'
arxiv_id: '2507.17257'
source_url: https://arxiv.org/abs/2507.17257
tags:
- identity
- agent
- planning
- agents
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces Agent Identity Evals (AIE), a framework to\
  \ measure and quantify how well Large Language Model Agents (LMAs) maintain their\
  \ identity over time, focusing on properties like identifiability, continuity, consistency,\
  \ persistence, and recovery. LMAs inherit core pathologies from LLMs\u2014statelessness,\
  \ stochasticity, semantic sensitivity, and linguistic intermediation\u2014that destabilize\
  \ their identity and impair planning, reasoning, and trust."
---

# Agent Identity Evals: Measuring Agentic Identity

## Quick Facts
- **arXiv ID:** 2507.17257
- **Source URL:** https://arxiv.org/abs/2507.17257
- **Reference count:** 40
- **One-line primary result:** AIE framework quantifies LMA identity stability across five metrics, revealing that identity drift impairs planning and that memory quality matters more than recall.

## Executive Summary
The paper introduces Agent Identity Evals (AIE), a framework to measure and quantify how well Large Language Model Agents (LMAs) maintain their identity over time, focusing on properties like identifiability, continuity, consistency, persistence, and recovery. LMAs inherit core pathologies from LLMs—statelessness, stochasticity, semantic sensitivity, and linguistic intermediation—that destabilize their identity and impair planning, reasoning, and trust. AIE defines five complementary metrics to assess these identity properties, providing formal methods and experimental setups to apply them. Initial experiments showed that while some identity metrics (e.g., consistency, persistence) could be stable under certain conditions, others like identifiability and core consistency often failed, especially under paraphrased queries or minimal scaffolding. Notably, RAG-assisted memory sometimes worsened planning performance despite high persistence scores, suggesting that the quality and integration of persistence matter more than simple recall for planning. Overall, AIE provides a rigorous approach to benchmarking the "degree of agency" and the effectiveness of scaffolding in mitigating LLM pathologies, highlighting the importance of stable identity for reliable LMA deployment.

## Method Summary
The AIE framework evaluates LMAs across five identity metrics: Identifiability (can the agent state its name/role), Continuity (does it maintain state over time), Consistency (does it respond identically to semantically equivalent prompts), Persistence (does it maintain self-conception across sessions), and Recovery (can it restore identity after perturbation). The method uses GPT-4o-mini as the simulated agent, GPT-4o-mini for profile generation and evaluation, and GPT-3.5-Turbo as a distractor. Experiments run with 30-50 trials per condition, varying scaffolding (tools on/off, memory, chain-of-thought vs direct prompting) and measuring both identity metrics and planning performance through semantic tool appropriateness and structural completeness scoring.

## Key Results
- LMAs often fail Identifiability (score 0.0) when asked to state their name/role
- RAG-assisted memory improved Persistence scores but sometimes degraded planning performance, indicating recall quality matters more than raw persistence
- Core Consistency metric frequently failed under paraphrased factual queries, revealing semantic fragility
- Recovery scores improved with strong corrective prompts, but underlying semantic consistency remained fragile
- Identity metrics showed varying rates of degradation across different aspects of identity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM pathologies (statelessness, stochasticity, semantic sensitivity) likely erode agentic identity, which in turn degrades functional capabilities like planning.
- Mechanism: The paper argues that because LLMs lack internal state and are sensitive to prompt variations, an LMA cannot inherently maintain a persistent "self" over time. This attrition of identity—measured as drift in attribute embeddings—interferes with the coherence required for multi-step reasoning and action.
- Core assumption: An agent requires a stable set of attributes (identity) to function reliably across time steps $t$ and $t'$.
- Evidence anchors:
  - [abstract] "LMAs inherit pathologies... which can undermine their identifiability... This attrition of identity can erode their reliability... by interfering with... reasoning, planning and action."
  - [section 6.1] "These results collectively indicate that identity is not monolithic; different aspects... can degrade or shift at varying rates."
  - [corpus] Related work "Stop Acting Like Language Model Agents Are Normal Agents" reinforces that LMA identity is fundamentally unstable compared to classical agents.
- Break condition: If scaffolding (memory/tools) perfectly compensates for statelessness and stochasticity, the causal link between LLM pathologies and identity attrition may be broken or significantly weakened.

### Mechanism 2
- Claim: High metric scores in Persistence (recall) do not necessarily imply improved Planning performance; the integration of memory matters more than simple retrieval.
- Mechanism: The framework measures Persistence ($P$) as the stability of self-conception across sessions. However, experiments showed that while RAG-assisted memory improved Persistence scores, it worsened semantic planning scores. This suggests that retrieving context is mechanistically distinct from effectively integrating that context into a plan without introducing noise or "linguistic intermediation" errors.
- Core assumption: An agent can "remember" its goal (high persistence) but fail to "use" it correctly (poor planning).
- Evidence anchors:
  - [abstract] "RAG-assisted memory sometimes worsened planning performance despite high persistence scores, suggesting that the quality and integration of persistence matter more."
  - [section 6.2] "Planning with RAG-assisted memory in Exp. 4 yielded poorer semantic planning scores compared to a no-memory/short-context condition."
- Break condition: If the planning task is trivial or does not require synthesis of retrieved information, persistence scores might correlate positively with planning success.

### Mechanism 3
- Claim: Recovery mechanisms (corrective prompts) can restore metric profiles, but semantic consistency may remain fragile if the underlying LLM is semantically sensitive.
- Mechanism: The paper defines a Recovery Profile ($R_k$) measuring the return to a reference state after perturbation. While strong corrective prompts improved Recovery scores (Exp. 5), the core Consistency metric often failed under paraphrased queries (Exp. 3). This implies that while an agent can be "reset" behaviorally, its underlying semantic fragility (pathology) makes it prone to drift when prompts change slightly.
- Core assumption: Identity is a set of state descriptors that can be "re-centered" via linguistic intervention.
- Evidence anchors:
  - [section 6.1] "Strong corrective prompts effectively restored Recovery scores (Exp. 5). However... core Consistency metric (Exp. 3) also failed... highlighting issues in responding consistently to paraphrased factual queries."
  - [section 4.3] Describes the use of "DISTRACTOR_LLM" to inject noise and test this fragility.
- Break condition: If the perturbation is too severe or the corrective prompt is too weak ("Condition A"), the agent may fail to recover its identity state.

## Foundational Learning

- Concept: **Diachronic Identity (Sameness over Time)**
  - Why needed here: The paper defines agentic identity not as a static label, but as a set of attributes $I_A$ that remain within a distance $\epsilon$ across time points $T$. Understanding this is required to interpret the "drift" plots.
  - Quick check question: If an agent's name stays the same but its goal changes, has it maintained its "agentic identity" according to this paper?

- Concept: **LLM Pathologies (Statelessness & Semantic Sensitivity)**
  - Why needed here: These are the root causes the AIE framework attempts to measure. You must distinguish between "stochasticity" (randomness) and "semantic sensitivity" (deterministic but fragile responses to paraphrasing) to understand the different failure modes.
  - Quick check question: Why does "statelessness" specifically threaten the "Continuity" metric, while "semantic sensitivity" threatens "Consistency"?

- Concept: **Embedding Distance / Cosine Similarity**
  - Why needed here: The metrics rely heavily on a distance measure $d(\cdot, \cdot)$ between text outputs (e.g., current state vs. initial prompt) to quantify drift numerically.
  - Quick check question: How would you interpret a sudden "jump" in cosine distance in Figure 3 regarding the agent's identity stability?

## Architecture Onboarding

- Component map: Profile Generator -> SimulatedAgent -> AgentIdentityEvaluator -> SupervisorLLM -> Plan Master
- Critical path:
  1. **Initialization**: Profile Generator creates persona $\to$ Agent instantiated.
  2. **Perturbation/Interaction**: Agent interacts or is perturbed (e.g., paraphrased queries, distractors).
  3. **Measurement**: Evaluator probes agent, calculates distance $d$ (drift) and scores $I, C, S, P, R$.
  4. **Planning Task**: Agent executes task $\to$ SupervisorLLM scores tool usage and semantic consistency.
- Design tradeoffs:
  - **RAG vs. No Memory**: RAG improves Persistence (recall) but may degrade semantic planning scores due to noise or "lost in the middle" issues.
  - **Direct vs. CoT Prompting**: CoT may aid planning but does not guarantee "Consistency" on factual queries.
- Failure signatures:
  - **Identifiability = 0.0**: The agent fails to state its name/role.
  - **Drift Spike**: A sudden increase in cosine distance (e.g., Turn 11 in Exp 1) indicates a state perturbation.
  - **High Persistence, Low Planning**: The agent remembers the prompt but cannot use it to structure a valid plan.
- First 3 experiments:
  1. **Baseline Identifiability**: Instantiate agent $\to$ probe for name/role $\to$ calculate Identifiability score $I$. Check if $I=0$.
  2. **Continuity with Tools**: Run agent with Tools ON vs OFF $\to$ inject info early $\to$ probe recall later. Compare Continuity score $C$.
  3. **Consistency/Paraphrase**: Ask factual questions $\to$ paraphrase them $\to$ measure semantic similarity of responses (Consistency $S$). Check if $S$ drops.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the quality of information integration in memory mechanisms (e.g., RAG) correlate more strongly with planning performance than the raw Persistence score itself?
- Basis in paper: [explicit] The authors note that in Experiment 4, "RAG-assisted memory sometimes worsened planning performance despite high persistence scores," and explicitly state the need to "dissect the negative impact of RAG on planning."
- Why unresolved: The initial results suggest that the AIE metric for Persistence (simple recall) does not capture the utility of the recalled information for downstream planning, indicating a disconnect between metric stability and functional capability.
- What evidence would resolve it: A comparative study varying the "integration quality" of retrieved memories (e.g., raw context injection vs. structured state updates) to see which better predicts planning success, independent of the Persistence score.

### Open Question 2
- Question: Can advanced Natural Language Inference (NLI) models or formal verification techniques replace embedding distance to more accurately detect semantic consistency violations?
- Basis in paper: [explicit] Section 6.3 (Limitations) states that "Current definitions rely on distance metrics... [which] may miss nuanced semantic consistency" and explicitly proposes that "advanced NLI models... could yield more robust consistency checks."
- Why unresolved: The current implementation of the Consistency metric relies on embedding cosine distance, which is a proxy for semantic similarity but may fail to detect subtle logical contradictions or negations that an NLI model would catch.
- What evidence would resolve it: An experiment benchmarking the current AIE Consistency metric against an NLI-based consistency check on the same set of paraphrased agent responses to identify discrepancies in detecting contradictions.

### Open Question 3
- Question: How does identity drift in a single agent propagate to affect group consistency and shared goals in Multi-Agent Systems (MAS)?
- Basis in paper: [explicit] The authors identify "Multi-Agent Dynamics" as a key limitation, asking, "How does the identity drift of one agent affect others?" and noting the need for "metrics for group consistency."
- Why unresolved: The current AIE framework focuses exclusively on single-agent systems. It is unknown if the instability of one agent destabilizes the collective identity or task performance of a multi-agent team.
- What evidence would resolve it: An experiment where a "perturbed" agent (with induced identity drift) is introduced into a collaborative multi-agent simulation to measure the degradation of shared persistence and collective task success rates.

## Limitations
- The framework's reliance on cosine similarity of embeddings to measure identity drift is inherently limited by the embedding space's sensitivity to paraphrasing and context; semantic equivalence may not be captured accurately.
- The experiments use synthetic agent profiles and controlled tasks, which may not reflect real-world deployment complexities where identity stability is even more fragile.
- The impact of RAG on planning degradation is observed but not mechanistically explained—whether it's due to noise, context loss, or integration failure remains unclear.

## Confidence
- **High confidence:** The five metrics (Identifiability, Continuity, Consistency, Persistence, Recovery) are formally defined and experimentally operationalized.
- **Medium confidence:** The claim that LLM pathologies (statelessness, stochasticity, semantic sensitivity) erode identity is supported by experimental results, but the exact causal mechanisms are inferred.
- **Low confidence:** The claim that Recovery scores can fully restore identity after perturbation is weakly supported, as semantic consistency remains fragile under paraphrased queries.

## Next Checks
1. **Cross-Evaluator Validation:** Run the same experiments using a different embedding model (e.g., `text-embedding-3-large`) to test the robustness of the identity metrics to embedding space variations.
2. **Real-World Agent Profiles:** Replace synthetic profiles with real-world agent descriptions (e.g., from role-playing datasets) to validate the framework's applicability beyond controlled conditions.
3. **RAG Integration Analysis:** Systematically vary RAG parameters (chunk size, retrieval frequency) and measure their impact on both identity metrics and planning performance to isolate the source of degradation.