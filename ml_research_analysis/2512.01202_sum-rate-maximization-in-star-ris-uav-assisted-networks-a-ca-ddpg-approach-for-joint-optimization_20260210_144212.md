---
ver: rpa2
title: 'Sum Rate Maximization in STAR-RIS-UAV-Assisted Networks: A CA-DDPG Approach
  for Joint Optimization'
arxiv_id: '2512.01202'
source_url: https://arxiv.org/abs/2512.01202
tags:
- network
- rate
- algorithm
- ieee
- critic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of maximizing sum rate in STAR-RIS-UAV-assisted
  wireless communication systems through joint optimization of beamforming, phase
  shifts, and UAV positioning. The authors propose a Convolution-Augmented Deep Deterministic
  Policy Gradient (CA-DDPG) algorithm that builds upon the DDPG framework by introducing
  a stochastic perturbation factor for enhanced exploration and incorporating convolutional
  layers into the critic network for improved evaluation capability.
---

# Sum Rate Maximization in STAR-RIS-UAV-Assisted Networks: A CA-DDPG Approach for Joint Optimization

## Quick Facts
- arXiv ID: 2512.01202
- Source URL: https://arxiv.org/abs/2512.01202
- Reference count: 40
- Primary result: CA-DDPG achieves superior sum rate performance compared to classical DDPG, TD3, and DDPG-based DTDE algorithms in STAR-RIS-UAV-assisted wireless systems.

## Executive Summary
This paper addresses the problem of maximizing sum rate in STAR-RIS-UAV-assisted wireless communication systems through joint optimization of beamforming, phase shifts, and UAV positioning. The authors propose a Convolution-Augmented Deep Deterministic Policy Gradient (CA-DDPG) algorithm that builds upon the DDPG framework by introducing a stochastic perturbation factor for enhanced exploration and incorporating convolutional layers into the critic network for improved evaluation capability. The CA-DDPG algorithm demonstrates superior performance compared to classical DDPG, TD3, and DDPG-based DTDE algorithms in various simulation scenarios, achieving higher sum rates under different transmit power levels, antenna configurations, and imperfect CSI conditions. The algorithm effectively handles the non-convex optimization problem and shows robust performance across different system parameters.

## Method Summary
The CA-DDPG algorithm formulates the joint optimization problem as a Markov Decision Process where the agent learns to optimize BS beamforming matrix W, STAR-RIS phase shift matrices (Θ_R, Θ_T), and UAV horizontal coordinates C_R to maximize sum rate. The state space includes previous action, UAV position, and channel matrices H_BR, H_RU, H_TU. The actor network outputs continuous actions representing beamforming, phase shifts, and UAV position, while the critic network incorporates convolutional layers to extract spatial features from channel matrices before combining with actions to estimate Q-values. A stochastic perturbation factor is added to the actor's output to enhance exploration, and the algorithm is trained using Adam optimizer with soft target updates over 40,000-80,000 steps.

## Key Results
- CA-DDPG outperforms vanilla DDPG, TD3, and DDPG-based DTDE algorithms in sum rate maximization across various scenarios
- The algorithm demonstrates robust performance under different transmit power levels (-10 to 30 dB) and antenna configurations
- CA-DDPG maintains performance advantages under imperfect CSI conditions with estimation errors
- Computational runtime increases by approximately 25% compared to standard DDPG due to convolutional layers

## Why This Works (Mechanism)

### Mechanism 1: Stochastic Perturbation for Deterministic Exploration
The addition of a stochastic perturbation factor appears to prevent the deterministic policy from converging prematurely to local optima in the non-convex optimization landscape. The algorithm injects noise $\eta o$ directly into the actor's action output, forcing the agent to explore state-action pairs outside its current deterministic policy boundary. This enhances coverage of the solution space before the noise decays. If the decay rate $\eta$ is too aggressive, the agent may cease exploration before finding a stable joint configuration for beamforming and UAV position.

### Mechanism 2: Convolution-Based Spatial Feature Extraction in Critic Evaluation
Augmenting the critic network with convolutional layers likely improves Q-value estimation accuracy by exploiting spatial correlations in channel state information. Unlike standard DDPG, the CA-DDPG critic processes the state through convolutional layers before action fusion, exploiting the "local receptive fields" of CNNs to extract hierarchical spatial features from the channel matrices. This architecture is effective when channel matrices possess local spatial structures or correlations that are predictive of future reward potential. If the input state is flattened or pre-processed in a way that destroys the spatial topology of the RIS/antenna array, the convolutional inductive bias provides no benefit.

### Mechanism 3: Joint Continuous-Discrete Action Space Optimization
Treating beamforming, phase shifts, and UAV location as a unified continuous action vector allows the agent to learn coupled dependencies between aerial mobility and signal processing. The agent outputs a single concatenated action vector that is optimized simultaneously via the DDPG gradient, allowing the UAV's movement to be directly shaped by its impact on the achievable sum rate. If the reward landscape is too flat between the UAV position and the sum rate, the agent may exhibit random wandering behavior rather than strategic positioning.

## Foundational Learning

- **Concept: Markov Decision Process (MDP) Formulation**
  - **Why needed here:** The paper maps the wireless optimization problem to an MDP. Understanding how to define the "State" (CSI, previous action) and "Reward" (Sum Rate) is the prerequisite for applying DRL.
  - **Quick check question:** Does the current state $s_t$ contain enough information (specifically CSI) to predict the next sum rate, or is temporal history required?

- **Concept: Actor-Critic Architecture**
  - **Why needed here:** The CA-DDPG builds on DDPG. You must understand why the **Actor** (policy) proposes actions while the **Critic** (value function) evaluates them to grasp how the convolutional augmentation aids the evaluation step.
  - **Quick check question:** In this architecture, does the CNN reside in the Actor or the Critic, and what specific tensor shape is it processing?

- **Concept: STAR-RIS Physics (Energy Splitting)**
  - **Why needed here:** Unlike standard reflective RIS, STAR-RIS splits energy between reflection ($\beta^r$) and transmission ($\beta^t$). Understanding this constraint is necessary to interpret the action space dimensions.
  - **Quick check question:** Why must the amplitudes satisfy $\beta_i^r + \beta_i^t = 1$, and how does this expand the optimization difficulty compared to a standard RIS?

## Architecture Onboarding

- **Component map:**
  - Environment: Wireless channel simulator (generates $H_{BR}, H_{RU}, H_{TU}$ based on UAV position)
  - Agent:
    - Actor Network: Input (State) → Dense Layers → Action (Beamforming + Phase + Position)
    - Critic Network (CA-DDPG specific): Input (State) → Conv Layers (extract spatial features) → Concatenate with Action → Dense Layers → Q-value
  - Replay Buffer: Stores tuples $(s_t, a_t, r_t, s_{t+1})$

- **Critical path:**
  1. Complex-to-Real Conversion: Pre-process complex channel matrices into real/imaginary components
  2. Convolutional Feature Extraction: Critic processes the state through Conv1D/2D layers (kernel size 1 used in paper) to refine feature maps
  3. Action Fusion: The extracted state features are merged with the raw action vector to compute the Q-score

- **Design tradeoffs:**
  - Complexity vs. Accuracy: CA-DDPG has complexity $O(4LD_s^2 + 2L'D_s^2)$ vs standard DDPG's $O(4LD_s^2)$. The tradeoff is higher runtime (Table II) for improved sum-rate convergence
  - Kernel Size: Performance is relatively robust to kernel size changes, but tuning is required for specific antenna array geometries

- **Failure signatures:**
  - Constraint Violation: The agent outputs beamforming matrices that violate the power constraint $tr\{WW^H\} \leq P_t$
  - Evaluation Lag: If the Critic's learning rate is too slow relative to the Actor, the Q-estimates become inaccurate, leading to policy oscillation

- **First 3 experiments:**
  1. Ablation Validation: Run CA-DDPG vs. "CA-DDPG (without convolutional layer)" to verify the performance gain shown in Fig. 5 is reproducible in your simulation environment
  2. Noise Decay Sweep: Vary the decaying coefficient $\eta$ to identify the sweet spot between exploration stability and convergence speed for your specific channel model
  3. CSI Robustness Stress Test: Introduce estimated error $\delta$ to verify the claim that CA-DDPG maintains a performance lead under imperfect channel conditions

## Open Questions the Paper Calls Out
1. How can the CA-DDPG framework be effectively adapted for **Integrated Sensing and Communication (ISAC)** systems? ISAC scenarios introduce conflicting objectives (sensing accuracy vs. sum rate) and require managing additional interference clutter that the current single-objective reward function does not address.

2. Can the CA-DDPG architecture be scaled to **Multi-Agent Deep Reinforcement Learning (MADRL)** settings for multiple cooperative UAVs or distributed RISs? Standard DDPG-based methods struggle with non-stationarity in multi-agent environments; it is unclear if the convolution-augmented critic can stabilize learning when multiple agents simultaneously alter the environment state.

3. How robust is the CA-DDPG algorithm under **practical hardware constraints**, specifically **finite-resolution (discrete) phase shifts**? Mapping the continuous action outputs of the actor network to discrete hardware values introduces quantization errors, which may significantly degrade the performance gains shown over standard DDPG.

4. Does the added computational complexity of the **convolutional layers** in the critic network hinder **real-time feasibility** for high-speed UAV trajectory planning? The increased processing time per step could violate latency constraints in rapidly changing environments where the UAV must update trajectories within milliseconds.

## Limitations
- The performance claims depend heavily on specific hyperparameter choices (network architecture depth, learning rates, noise decay schedule) that are not fully specified in the main text
- The assumption of complete direct-link blockage between BS and users may limit practical applicability
- The reward structure assumes sum rate is the sole optimization objective without considering fairness or energy efficiency tradeoffs
- The exact architecture parameters (number of filters, kernel sizes beyond the minimum) remain unclear

## Confidence
- **High Confidence:** The fundamental mechanism of using convolutional layers in the critic for spatial feature extraction is well-supported by the architecture description and aligned with established deep learning principles for spatial data
- **Medium Confidence:** The claim that CA-DDPG outperforms vanilla DDPG and TD3 is supported by simulation results, though the exact magnitude of improvement depends on unspecified implementation details and the specific simulation environment parameters
- **Low Confidence:** The generalization claims to imperfect CSI conditions are demonstrated but the robustness threshold and performance degradation curve under varying error levels is not quantified

## Next Checks
1. **Architecture Sensitivity Test:** Vary the number of convolutional layers (L' = 1, 2, 3) and filter counts to verify the performance gain is not architecture-dependent
2. **Noise Schedule Sweep:** Systematically test different noise decay schedules for the stochastic perturbation factor to identify optimal exploration-exploitation tradeoff
3. **CSI Error Stress Test:** Introduce progressive channel estimation errors (δ = 0%, 5%, 10%, 20%) to quantify the exact performance degradation curve and verify the claimed robustness