---
ver: rpa2
title: Beware of "Explanations" of AI
arxiv_id: '2504.06791'
source_url: https://arxiv.org/abs/2504.06791
tags:
- explanations
- explanation
- system
- such
- stakeholders
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper warns against overreliance on AI explanations, arguing
  that current explainable AI (XAI) methods often produce explanations that are unfaithful,
  irrelevant, unstable, incoherent, revealing, unnecessary, or cherry-picked. These
  shortcomings can lead to significant harms: wrong decisions, reduced AI adoption,
  privacy violations, intellectual property leakage, manipulation, unfair outcomes,
  and wasted resources.'
---

# Beware of "Explanations" of AI

## Quick Facts
- arXiv ID: 2504.06791
- Source URL: https://arxiv.org/abs/2504.06791
- Reference count: 40
- Primary result: XAI explanations can cause harm by being unfaithful, irrelevant, unstable, incoherent, revealing, unnecessary, or cherry-picked

## Executive Summary
This paper argues that current explainable AI (XAI) methods often produce explanations that can lead to significant harms, including wrong decisions, reduced AI adoption, privacy violations, intellectual property leakage, manipulation, unfair outcomes, and wasted resources. The authors emphasize that explanations are not universally "good" due to diverse stakeholder goals, contexts, and the complex nature of AI systems. They call for interdisciplinary collaboration to improve explanation quality, ensure meaningful transparency, and shift responsibility to XAI developers to assess and disclose risks, rather than leaving it to end users.

## Method Summary
The paper presents a conceptual framework analyzing failure modes ("causes") and harms ("caveats") of current XAI methods. It uses illustrative examples from credit scoring, fraud detection, and recidivism prediction rather than empirical experiments. The authors critique post-hoc XAI techniques like SHAP and LIME, discussing how explanations can be unfaithful, unstable, cherry-picked, or reveal sensitive information. The work is a position paper synthesizing existing knowledge rather than introducing new algorithms.

## Key Results
- Explanations can be unfaithful (inconsistent with the model's true reasoning)
- Explanations may be unstable (varying significantly with minor input changes)
- Explanations can be cherry-picked, hiding discriminatory logic or proprietary thresholds
- Current XAI methods often fail to consider diverse stakeholder goals and contexts
- The illusion of understanding from explanations can lead to overreliance and poor decision-making

## Why This Works (Mechanism)
The paper's argument works by identifying how the disconnect between explanation quality and stakeholder needs creates systemic risks. Post-hoc XAI methods like SHAP and LIME approximate complex models with simplified narratives, creating a gap between mathematical truth and human comprehension. This gap becomes dangerous when explanations are used for critical decisions without understanding their limitations. The mechanism of harm operates through the "illusion of understanding" - users believe they comprehend the model's reasoning when they only see a plausible but potentially misleading story. The paper demonstrates how this illusion leads to various failure modes including privacy leaks, unfair outcomes, and manipulation when explanations are tailored to specific audiences while hiding important context.

## Foundational Learning

- **Concept: Post-hoc vs. Inherently Interpretable Models**
  - Why needed here: The paper's arguments about "unfaithful" explanations are primarily aimed at post-hoc XAI methods which decouple the explanation from the model's true reasoning
  - Quick check question: Is the explanation generated by a separate algorithm trying to approximate the model, or is it a direct statement of the model's own logic?

- **Concept: Stakeholder & Context Dependence**
  - Why needed here: The paper's central thesis is that a "good" explanation is not universal but depends on who is receiving it and why
  - Quick check question: Before generating an explanation, can you clearly articulate who the target audience is and what action you want them to take?

- **Concept: Faithfulness vs. Plausibility**
  - Why needed here: The paper warns that explanations can be "unfaithful" but still "plausible" - grasping this gap is essential to avoid the "illusion of understanding"
  - Quick check question: Does the explanation accurately reflect the mathematical function of the model, or does it just sound like a reasonable narrative?

## Architecture Onboarding

- **Component Map:**
  Core AI System -> XAI Engine -> Explanation Selector/Filter -> Contextualizer -> Query Interface

- **Critical Path:**
  1. Define Context & Goal: Who needs this explanation and why?
  2. Select Explanation Method: Choose a method whose strengths/weaknesses align with the goal
  3. Generate Candidate Explanations: Run the XAI method
  4. Apply Policy & Filter: Check for relevance, unfaithfulness, privacy/IP leakage, cherry-picking
  5. Present & Log: Deliver the explanation and log both the explanation and the underlying model version

- **Design Tradeoffs:**
  - Faithfulness vs. Comprehensibility: Inherently interpretable models are more faithful but may have lower predictive performance
  - Completeness vs. Security: Full interactive access increases privacy/IP attack risk
  - Simplicity vs. Accuracy: Simplifying explanations can make them more coherent but risks omitting key drivers

- **Failure Signatures:**
  - Contradictory Explanations: Different explanations for the same decision that cannot be reconciled
  - Gaming the Explanation: Users changing behavior based on explanations in ways that don't reflect true underlying risk
  - Compliance Without Substance: Explanations look good to regulators but don't help users understand actual decision logic

- **First 3 Experiments:**
  1. Stakeholder Validation Test: Present explanations to different stakeholder groups and measure understanding and trust
  2. Stability & Robustness Probe: Perturb input data slightly and regenerate explanations to test for instability
  3. Adversarial Information Leak Audit: Simulate explanation linkage attacks to quantify privacy risk

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can evaluation frameworks be developed to operationalize explanation quality relative to specific stakeholder goals and contexts?
- Basis in paper: The paper argues that XAI research neglects defining "good" explanations because the concept is ill-defined and dependent on diverse social and technical factors
- Why unresolved: Current metrics focus on technical fidelity rather than alignment with mental models or specific decision-making contexts
- What evidence would resolve it: A validated taxonomy of metrics that correlates specific explanation styles with measurable improvements in decision accuracy for distinct user groups

### Open Question 2
- Question: What technical or procedural methods can effectively detect "cherry-picked" explanations that conceal discriminatory logic or proprietary thresholds?
- Basis in paper: The paper identifies "cherry-picked" explanations as a root cause of manipulation and unfair decisions
- Why unresolved: The multiplicity of valid explanations for a single prediction makes it difficult to distinguish between a helpful summary and a misleading omission
- What evidence would resolve it: Algorithms capable of mapping the full space of valid explanations to identify if the disclosed subset acts as a statistical outlier

### Open Question 3
- Question: How can XAI systems balance the need for faithfulness with the necessity of preventing privacy leakage and model extraction attacks?
- Basis in paper: The authors note that detailed explanations can lead to "IP leakage" and "privacy harms" via explanation linkage attacks
- Why unresolved: High-fidelity explanations inherently reveal decision boundaries and training data characteristics
- What evidence would resolve it: XAI methods that provably guarantee differential privacy or robustness against model extraction without significant loss of explanatory power

## Limitations
- The paper is conceptual rather than empirical, lacking quantitative validation of its claims
- No specific XAI library implementations or versions are specified for the examples
- "Harm thresholds" and qualitative definitions (e.g., "overly complex") require subjective human judgment
- The reader must implement their own pipeline to test claims, introducing variability

## Confidence

- **High Confidence:** The identification of core failure modes (unfaithful, unstable, cherry-picked explanations) is well-founded and consistent with broader XAI literature
- **Medium Confidence:** The discussion of harms (privacy leaks, unfair outcomes, wasted resources) is persuasive but causal links are often hypothetical
- **Medium Confidence:** The framework for understanding stakeholder-dependent explanation quality is insightful but the "ideal" explanation for each stakeholder is often left implicit

## Next Checks
1. **Stakeholder Validation Test:** Generate explanations for a sample of model decisions and present them to representatives from different stakeholder groups (data scientist, loan officer, mock applicant) to measure understanding, trust, and ability to identify model errors

2. **Stability & Robustness Probe:** Slightly perturb input data (e.g., change income by $1) and regenerate explanations to test if key features or counterfactuals change drastically, testing for instability

3. **Adversarial Information Leak Audit:** Simulate an explanation linkage attack by generating hundreds of explanations for a synthetic individual with known properties, attempting to reconstruct the model's decision boundary or the individual's sensitive attributes from the collected explanations