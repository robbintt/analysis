---
ver: rpa2
title: Towards Fine-Grained Human Motion Video Captioning
arxiv_id: '2510.24767'
source_url: https://arxiv.org/abs/2510.24767
tags:
- motion
- human
- video
- arxiv
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of generating accurate, fine-grained
  descriptions of human motion in videos, where existing approaches struggle with
  hallucinations and vague captions. The proposed Motion-Augmented Caption Model (M-ACM)
  introduces Motion Synergetic Decoding (MSD), which combines standard visual representations
  with motion-specific representations derived from SMPL-based human mesh recovery.
---

# Towards Fine-Grained Human Motion Video Captioning

## Quick Facts
- arXiv ID: 2510.24767
- Source URL: https://arxiv.org/abs/2510.24767
- Reference count: 40
- Primary result: Dual-pathway model with Motion Synergetic Decoding achieves BLEU-4: 0.12421 vs 0.03311 baseline, CIDEr: 0.80662 vs 0.55343 baseline

## Executive Summary
This paper addresses the challenge of generating accurate, fine-grained descriptions of human motion in videos, where existing approaches struggle with hallucinations and vague captions. The proposed Motion-Augmented Caption Model (M-ACM) introduces Motion Synergetic Decoding (MSD), which combines standard visual representations with motion-specific representations derived from SMPL-based human mesh recovery. The model processes videos through dual pathways - a standard visual pathway and a motion-specialized pathway - with a synergy calculation mechanism that reduces hallucination by leveraging complementary information. To support this research, the authors introduce the Human Motion Insight (HMI) Dataset containing 115K video-description pairs and HMI-Bench benchmark. Experimental results show M-ACM significantly outperforms existing methods across standard caption metrics and human motion understanding dimensions.

## Method Summary
The M-ACM system takes video input and processes it through two parallel pathways: a standard visual understanding pathway and a motion-specialized pathway. The visual pathway uses a frozen vision encoder (SigLIP/CLIP) with a trainable 2-layer MLP projector. The motion pathway uses ViTPose-based frame sampling, NLF localizer, and SMPL fitting to generate motion masks, which are then processed through a motion encoder and another trainable MLP projector. Both pathways project features to a common embedding space before feeding into a trainable LLM decoder (Qwen2 7B/Llama3 8B). During inference, Motion Synergetic Decoding combines logit distributions from both pathways using a five-component synergy score (L1-L5) with hyperparameters α1-α6 = [0.5, 0.2, 0.4, 0.8, 0.3, 2.0] and distribution pruning threshold β = 0.2. The model was trained on 8× A100 GPUs for approximately 35 hours using the full caption dataset plus 20% QA pairs from the HMI dataset.

## Key Results
- BLEU-4: 0.12421 (M-ACM) vs 0.03311 (baseline)
- CIDEr: 0.80662 (M-ACM) vs 0.55343 (baseline)
- Motion understanding scores: 5.69 overall vs 5.53 baseline (33% improvement in fine-grained motion description)
- Reduced hallucinations: Correctly distinguishes "hand" vs "foot" in basketball handling scenarios

## Why This Works (Mechanism)

### Mechanism 1: Dual-Pathway Feature Extraction
Processing videos through parallel visual and motion pathways captures complementary information that reduces hallucination. The standard vision encoder extracts general visual features while the motion pathway uses ViTPose-based sampling + SMPL mesh recovery to generate body-part-specific representations. Both are projected to common embedding space via 2-layer MLP aligners. Core assumption: general visual features lack precision for fine-grained body part identification; explicit motion representations can disambiguate similar body parts (e.g., hand vs. foot in basketball handling).

### Mechanism 2: Synergy-Based Decoding with Distribution Pruning
Combining logit distributions from both pathways through a multi-component synergy score reduces hallucinations by requiring cross-modal agreement. Five-component formula integrates: (1) exponential amplification of high-confidence predictions, (2) arithmetic mean balancing, (3) soft log-penalties for disagreement, (4) power-transformed non-linear relationships, and (5) quadratic penalties for large discrepancies. Distribution pruning retains only tokens exceeding β × (1 + S(y_T)) threshold. Core assumption: when visual and motion pathways disagree significantly, the discrepancy signals potential hallucination; consensus indicates reliability.

### Mechanism 3: Pose-Aware Frame Sampling for Training Data
Selecting keyframes based on keypoint variation rather than uniform sampling improves motion understanding in downstream caption generation. Videos are divided into N segments, with frames selected based on maximum keypoint distance from previous keyframe. Core assumption: frames with higher pose variation contain more semantically informative motion content for LLM annotation.

## Foundational Learning

- **Concept: SMPL Human Body Model**
  - Why needed: The paper relies on SMPL-based mesh recovery via NLF to generate motion masks. Without understanding SMPL's shape/pose parameterization, the motion representation extraction remains opaque.
  - Quick check: What do SMPL shape parameters (β) vs. pose parameters (θ) represent, and which would change during a yoga sequence?

- **Concept: Contrastive Decoding**
  - Why needed: MSD extends contrastive decoding principles with multi-component synergy. Understanding the base concept clarifies why comparing two logit distributions helps.
  - Quick check: In standard contrastive decoding, what two distributions are compared, and how does their difference signal hallucination risk?

- **Concept: Vision-Language Projection Alignment**
  - Why needed: Both pathways project features to a common embedding space before LLM processing. The 2-layer MLP projector is the only trainable component connecting frozen encoders to the LLM.
  - Quick check: If projection modules weren't trained while vision encoders remained frozen, what would happen to cross-modal grounding?

## Architecture Onboarding

**Component map:**
Input Video → Vision Encoder (frozen) → Linear Projection (trainable) │
                                                    │
Input Video → ViTPose Sampling → NLF Localizer → SMPL Fitting → Motion Encoder → Motion Masks → Linear Projection (trainable) │
                                                                                                                           ↓
                                                                                                                    LLM Decoder (Qwen2/Llama3)
                                                                                                                           ↓
                                                                                                                    Motion Synergetic Decoding (inference)

**Critical path:**
1. Motion representation extraction: ViTPose → NLF localizer → SMPL fitting → motion masks (inference-time dependency on pose quality)
2. Projection alignment training: 2-layer MLP trained on HMI dataset while vision encoders remain frozen
3. MSD inference: Parallel logit generation → 5-component synergy → softmax over adjusted scores

**Design tradeoffs:**
- Five-component synergy vs. basic formula: Complex formula vs. basic. Figure 2 shows each component adds incremental accuracy but increases inference time. The α₁-α₆ hyperparameters require tuning.
- Frozen vision encoders: Trades representation adaptation for training efficiency. All domain-specific learning happens in projection and LLM decoder.
- Aggressive data filtering: DWPose-based filtering may exclude valid multi-person or distant-subject videos.

**Failure signatures:**
1. Hallucination persists despite MSD: Check if DWPose detection fails (occlusion, unusual angles)—motion pathway provides no corrective signal.
2. Synergy score dominated by one pathway: Likely logit scale mismatch; verify normalization between visual and motion branch outputs.
3. Low BLEU scores after training: Projection alignment may not have converged—check loss curves for projection modules separately.

**First 3 experiments:**
1. Isolate MSD contribution from data effects: Train M-ACM with MSD enabled but on pre-HMI datasets to separate decoding vs. data contributions.
2. Log per-component synergy values on hallucination cases: For basketball example, record L1-L5 values when model predicts "foot" vs. "hand."
3. Test SMPL failure modes: Create synthetic test set with occluded limbs, unusual viewpoints, or loose clothing. Measure correlation between DWPose confidence scores and MSD effectiveness.

## Open Questions the Paper Calls Out

**Open Question 1:** Can a dedicated motion encoder trained directly on SMPL annotations outperform the current frozen vision encoder approach for the motion pathway? Basis: The authors state future work will focus on enhancing motion representation by annotating SMPL format data using HMI dataset to train a dedicated motion encoder.

**Open Question 2:** How robust is the Motion Synergetic Decoding (MSD) framework when the underlying pose estimation fails due to occlusion or crowding? Basis: The system relies on SMPL-based mesh recovery, but the dataset pipeline explicitly filters for high-visibility, single-person videos.

**Open Question 3:** Can the M-ACM architecture be adapted for multi-person interaction scenarios without significant degradation in fine-grained detail? Basis: The video filtering pipeline requires over 90% of frames to contain a "single person," and the motion representation focuses on individual body dynamics.

**Open Question 4:** How can the computational overhead of the dual-pathway processing and MSD be optimized for real-time applications? Basis: Figure 2 demonstrates inference time increases from ~0.41s to ~1.99s (approx. 5x slower) compared to baseline.

## Limitations
- Motion mask generation pipeline from NLF/SMPL output lacks complete procedural description, making exact replication challenging
- Dual-pathway architecture's effectiveness depends on assumption that visual and motion representations are complementary rather than redundant
- Aggressive data filtering (≥90% single-person frames, bounding box >1/50 area, 12+ keypoints with confidence >0.5) may exclude valid multi-person or distant-subject scenarios

## Confidence
- **High confidence**: Dual-pathway architecture improves fine-grained motion description accuracy (supported by quantitative metrics showing BLEU-4: 0.12421 vs 0.03311 baseline)
- **Medium confidence**: MSD synergy calculation reduces hallucinations through cross-modal agreement (mechanism plausible but exact synergy formula contribution unclear without ablation studies)
- **Medium confidence**: ViTPose keyframe sampling improves motion understanding (supported by Table 4 showing +0.16 improvement, but no comparison to alternative sampling methods)

## Next Checks
1. Isolate MSD contribution from data effects by training M-ACM with MSD enabled but on pre-HMI datasets to determine whether performance gains stem from decoding mechanism or motion-augmented training data.
2. Log per-component synergy values (L1-L5) on hallucination cases to identify which synergy components actually drive hallucination correction in practice.
3. Test SMPL failure modes with synthetic occluded or unusual viewpoint datasets to measure correlation between DWPose confidence scores and MSD effectiveness, validating whether motion pathway noise genuinely reduces synergy benefit.