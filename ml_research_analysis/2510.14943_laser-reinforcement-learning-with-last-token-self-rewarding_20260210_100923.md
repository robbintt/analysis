---
ver: rpa2
title: 'LaSeR: Reinforcement Learning with Last-Token Self-Rewarding'
arxiv_id: '2510.14943'
source_url: https://arxiv.org/abs/2510.14943
tags:
- reasoning
- self-rewarding
- arxiv
- training
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a novel method, LaSeR, to jointly optimize\
  \ the reasoning and self-verification capabilities of large language models (LLMs)\
  \ during reinforcement learning with verifiable rewards (RLVR). The key insight\
  \ is that the true reasoning reward can be reduced to a simple form: the last-token\
  \ self-rewarding score, which is the difference between the policy model\u2019s\
  \ next-token log-probability for a pre-specified token at the solution\u2019s last\
  \ token and a pre-calculated constant, scaled by the KL coefficient."
---

# LaSeR: Reinforcement Learning with Last-Token Self-Rewarding

## Quick Facts
- arXiv ID: 2510.14943
- Source URL: https://arxiv.org/abs/2510.14943
- Reference count: 38
- Primary result: Proposed method improves reasoning performance and self-verification accuracy in LLMs through last-token self-rewarding mechanism

## Executive Summary
LaSeR introduces a novel approach to reinforcement learning with verifiable rewards (RLVR) that enables large language models to jointly optimize reasoning and self-verification capabilities. The method leverages the insight that reasoning quality can be assessed through the last-token self-rewarding score, which is derived from the model's next-token probability distribution. By augmenting the standard RLVR loss with a mean squared error loss that aligns these self-rewarding scores with verifier-based rewards, LaSeR allows models to generate self-rewarding signals with minimal additional computational overhead. The approach demonstrates strong performance across multiple math reasoning benchmarks while achieving approximately 80% F1 scores in self-verification.

## Method Summary
LaSeR operates by reformulating the true reasoning reward as a last-token self-rewarding score, calculated as the difference between the policy model's next-token log-probability for a pre-specified token at the solution's final position and a pre-calculated constant, scaled by the KL coefficient. This self-rewarding score is then incorporated into the training objective through an additional MSE loss term that aligns it with traditional verifier-based reasoning rewards. During inference, the model can directly use its own probability distribution over the final token to assess solution quality, eliminating the need for separate verification generations. The method maintains the original RLVR framework while adding this self-rewarding component, resulting in improved reasoning performance and enabling weighted majority voting strategies at inference time.

## Key Results
- Achieved improved reasoning performance on math benchmarks (MATH500, AMC23, AIME24/25, OlympiadBench) across LLaMA and Qwen architectures
- Attained approximately 80% F1 scores for self-verification capability
- Enabled weighted majority voting at inference time without requiring separate verification generations
- Demonstrated computational efficiency by deriving self-rewarding signals directly from next-token probability distributions

## Why This Works (Mechanism)
The method works by creating a direct feedback loop where the model's own predictions about the final token serve as a proxy for solution quality. This self-rewarding mechanism is mathematically grounded in the observation that the probability distribution over the last token contains sufficient information to distinguish between correct and incorrect solutions. By training the model to align its self-rewarding scores with external verifier feedback, LaSeR creates a model that can accurately assess its own outputs without additional computational overhead. The MSE alignment loss ensures that the model's internal reward signal remains consistent with ground truth verification, while the simplicity of the last-token prediction task makes the approach computationally efficient.

## Foundational Learning
- **Reinforcement Learning with Verifiable Rewards (RLVR)**: A framework for training models on tasks where solutions can be automatically verified, enabling reward-based optimization without human supervision. Why needed: Provides the base training paradigm that LaSeR builds upon. Quick check: Verify that the original RLVR loss is properly incorporated alongside the new MSE term.
- **Self-rewarding mechanisms**: Methods where models generate their own feedback signals based on their predictions. Why needed: Enables the model to assess solution quality without external verification. Quick check: Confirm that the last-token probability distribution is a reliable indicator of solution correctness.
- **KL coefficient scaling**: The use of Kullback-Leibler divergence to regularize policy updates and prevent mode collapse. Why needed: Ensures stable training and maintains diversity in generated solutions. Quick check: Monitor KL divergence values during training to ensure they remain within reasonable bounds.

## Architecture Onboarding
- **Component map**: Verifier -> RLVR Loss -> MSE Alignment Loss -> Policy Update
- **Critical path**: Input problem → Model generation → Final token prediction → Self-rewarding score calculation → Policy gradient update
- **Design tradeoffs**: Balances computational efficiency against potential loss of nuanced verification information; prioritizes simplicity and speed over comprehensive verification.
- **Failure signatures**: Poor self-verification accuracy indicates misalignment between self-rewarding scores and true solution quality; unstable training may result from inappropriate KL coefficient scaling.
- **First experiments**: 1) Ablation study removing MSE alignment loss to isolate its contribution, 2) Varying the pre-calculated constant across multiple orders of magnitude, 3) Testing on non-math reasoning tasks to assess generalizability

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Reliance on last-token self-rewarding score assumes this captures reasoning quality, requiring further validation across diverse problem types
- Self-verification F1 scores around 80% still indicate significant false positive and false negative rates
- Computational efficiency claims need verification as method still requires verifier-based reasoning rewards for training
- Focus on math reasoning benchmarks leaves generalizability to other domains as an open question

## Confidence
- **High confidence**: The mathematical formulation of the last-token self-rewarding score and its implementation details
- **Medium confidence**: The reported improvements in reasoning performance and self-verification accuracy
- **Low confidence**: The claims about computational efficiency and the robustness of the method across diverse problem types

## Next Checks
1. Conduct ablation studies to isolate the contribution of the MSE alignment loss versus the self-rewarding mechanism itself
2. Test the method's robustness by varying the pre-calculated constant across multiple orders of magnitude
3. Evaluate the approach on non-math reasoning tasks (e.g., logical reasoning, commonsense QA) to assess generalizability