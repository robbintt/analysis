---
ver: rpa2
title: Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning
arxiv_id: '2512.08820'
source_url: https://arxiv.org/abs/2512.08820
tags:
- hyperbolic
- negative
- space
- learning
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Training-free Dual Hyperbolic Adapters (T-DHA),\
  \ a novel approach for efficient domain adaptation in vision-language models without\
  \ additional training. The method leverages hyperbolic geometry to better capture\
  \ hierarchical semantic relationships inherent in visual classification tasks, mapping\
  \ features into the Poincar\xE9 ball space and computing similarities using hyperbolic\
  \ distance rather than Euclidean metrics."
---

# Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning

## Quick Facts
- arXiv ID: 2512.08820
- Source URL: https://arxiv.org/abs/2512.08820
- Reference count: 40
- Key outcome: Achieves up to 2.3% higher accuracy in 16-shot settings compared to state-of-the-art training-free few-shot learning methods

## Executive Summary
This paper introduces Training-free Dual Hyperbolic Adapters (T-DHA), a novel approach for efficient domain adaptation in vision-language models without additional training. The method leverages hyperbolic geometry to better capture hierarchical semantic relationships inherent in visual classification tasks, mapping features into the Poincaré ball space and computing similarities using hyperbolic distance rather than Euclidean metrics. It also incorporates negative learning to enhance discriminative power by explicitly considering what an image is not, using both negative class prototypes and negated textual prompts. T-DHA combines four prediction signals: positive and negative image-image predictions in hyperbolic space, and positive and negative image-text predictions. Extensive experiments demonstrate that T-DHA significantly outperforms state-of-the-art training-free few-shot learning methods across 11 benchmark datasets, achieving up to 2.3% higher accuracy in 16-shot settings. The method also shows superior domain generalization performance on ImageNetV2 and ImageNet-Sketch, improving accuracy by up to 2.51% compared to existing approaches. T-DHA achieves these results with minimal computational overhead, requiring only 10.2ms for inference on 16-shot ImageNet.

## Method Summary
T-DHA is a training-free domain adaptation framework for CLIP-based vision-language models that uses hyperbolic geometry and negative learning. The method computes positive and negative class prototypes from support set features, maps them to the Poincaré ball space via the exponential map, and computes four prediction signals: positive/negative image-image similarities using hyperbolic distance and positive/negative image-text similarities using cosine similarity with standard/negated prompts. These predictions are fused with a weighted sum (α=1.2) to produce final classification scores. The approach requires no gradient updates, making it computationally efficient while achieving significant performance gains over existing training-free methods across few-shot and domain generalization settings.

## Key Results
- Achieves 75.53% accuracy on 16-shot ImageNet, outperforming state-of-the-art training-free methods by 2.2%
- Improves domain generalization on ImageNetV2 by up to 2.51% and ImageNet-Sketch by up to 1.55%
- Maintains 10.2ms inference time for 16-shot ImageNet, matching baseline computational efficiency
- Shows consistent improvements across 11 benchmark datasets in 1/2/4/8/16-shot settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mapping visual features to hyperbolic space via the Poincaré ball improves hierarchical semantic discrimination compared to Euclidean cosine similarity.
- Mechanism: Hyperbolic space exhibits exponential volume growth with radius, naturally accommodating tree-like semantic hierarchies (e.g., Animal → Mammal → Dog → Labrador). Features are projected from Euclidean space using the exponential map (Eq. 2), then similarity is computed via Poincaré distance (Eq. 3), which preserves hierarchical relations more effectively than cosine similarity.
- Core assumption: Class labels inherently form tree-structured taxonomies, and CLIP's embeddings encode these hierarchical relationships in a way that hyperbolic geometry can exploit.
- Evidence anchors:
  - [abstract] "Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space... achieving significantly improved representation and discrimination power."
  - [Section I] "In a hierarchical classification task such as 'Mammal'→'Dog'→'Labrador', hyperbolic embeddings more accurately capture the semantic relationships."
  - [corpus] Related work "Hyperbolic Deep Learning for Foundation Models: A Survey" confirms hyperbolic representations address limited representational capacity in foundation models for hierarchical data.
- Break condition: If target classes lack hierarchical structure (e.g., flat, unrelated categories), the inductive bias of hyperbolic geometry may not confer advantages.

### Mechanism 2
- Claim: Explicit negative learning at inference time improves discriminative power by modeling what an image is not, complementing CLIP's implicit contrastive pre-training.
- Mechanism: T-DHA constructs negative hyperbolic prototypes by averaging features from other classes (N^h_-), and uses negated textual prompts ("a photo of no {class}") to generate negative image-text predictions. The final prediction fuses positive and negative signals from both image-image and image-text branches.
- Core assumption: Explicitly measuring non-membership provides discriminative information beyond what CLIP's batch-contrastive pre-training captures, especially for classes with overlapping features.
- Evidence anchors:
  - [Section I] "Negative learning is crucial for refining a model's discriminatory power by explicitly accounting for what an image is not."
  - [Table VI] Ablation shows removing negative pipeline causes 0.82%-1.98% accuracy drops across shot settings.
  - [Section III-B(3)] "This explicit dual-positive-negative combination enables the model to adjust confidence scores more precisely in challenging few-shot and domain-shift scenarios."
- Break condition: If classes are mutually distant with no feature overlap, negative prototypes may provide redundant or noisy signals.

### Mechanism 3
- Claim: Training-free adaptation via parameter-free adapters achieves strong performance without gradient updates by leveraging geometric similarity to few-shot prototypes.
- Mechanism: Class prototypes are computed as Euclidean averages of support set features, then projected to hyperbolic space. Test predictions combine: (1) Poincaré distance to positive prototypes, (2) Poincaré distance to negative prototypes, (3) cosine similarity to positive prompts, (4) cosine similarity to negative prompts. The hyperparameter α balances image-image vs. image-text contributions.
- Core assumption: Support set prototypes provide sufficient class representations, and exponential map approximation (Euclidean average → hyperbolic projection) adequately approximates Fréchet mean for small support sizes.
- Evidence anchors:
  - [Section III-B(3)] "This choice ensures computational efficiency and simplicity — particularly important in our training-free, inference-only setup."
  - [Table VIII] T-DHA achieves 10.2ms inference time, matching baselines while achieving 2.2% higher accuracy.
  - [corpus] Limited direct corpus validation for training-free hyperbolic adapters; this appears novel.
- Break condition: If support set is too small (1-shot) or highly unrepresentative, prototype quality degrades, and the Fréchet mean approximation becomes unreliable.

## Foundational Learning

- Concept: **Poincaré Ball Model and Hyperbolic Distance**
  - Why needed here: Core geometric space where similarity is computed; understanding the exponential map and Poincaré distance formula is essential for implementation.
  - Quick check question: Can you explain why the conformal factor λ_a = (1 - ||a||²)^(-1) causes distances to grow exponentially toward the ball's boundary?

- Concept: **CLIP Architecture and Zero-Shot Classification**
  - Why needed here: T-DHA builds on CLIP's dual encoder (visual + text); understanding cosine similarity-based zero-shot prediction is prerequisite.
  - Quick check question: Given an image feature v and K class text embeddings {t_1, ..., t_K}, how would you compute zero-shot prediction probabilities?

- Concept: **Few-Shot Prototype Construction**
  - Why needed here: Positive/negative prototypes are computed from support sets; understanding N-shot K-class paradigm is essential.
  - Quick check question: For a 4-shot 10-class problem, how many positive hyperbolic prototypes are needed, and how is each computed?

## Architecture Onboarding

- Component map:
  - Input layer: Test image + support set images
  - CLIP visual encoder (E_v): ResNet-50 or ViT backbone → Euclidean features
  - Exponential map: Projects Euclidean features to Poincaré ball (Eq. 5)
  - Positive Hyperbolic Adapter: Stores hyperbolic class prototypes P^h+
  - Negative Hyperbolic Adapter: Stores negative prototypes N^h-
  - Text encoders: Process positive prompts ("a photo of {class}") and negative prompts ("a photo of no {class}")
  - Distance/similarity computation: Poincaré distance for image-image, cosine similarity for image-text
  - Prediction fusion: Weighted combination with hyperparameter α (default 1.2)

- Critical path:
  1. Extract support set features → compute positive/negative prototypes → exponential map to hyperbolic space
  2. Extract test image feature → exponential map to hyperbolic space
  3. Compute four prediction scores: d_H(test, P^h+), d_H(test, N^h-), cos(v, f^t+), cos(v, f^t-)
  4. Fuse predictions via Eq. 12: P = α·P_II + P_IT

- Design tradeoffs:
  - **α value**: Higher α (>1.0) prioritizes hyperbolic image-image similarity; paper shows optimal at α=1.2 with degradation at 2.0
  - **Prototype approximation**: Euclidean average + exponential projection is computationally efficient but approximates Fréchet mean; acceptable for small support sizes near origin
  - **Negative prototype construction**: Randomly selecting 1 sample per other class is simple but may introduce noise

- Failure signatures:
  - Accuracy drops significantly when α=0 (relying only on image-text): indicates image-image hyperbolic branch is contributing meaningfully
  - Worse than zero-shot CLIP on very few shots (1-shot): prototype quality insufficient
  - Domain generalization failure: check if hyperbolic embeddings collapse toward boundary

- First 3 experiments:
  1. **Ablation by component**: Remove each of the four prediction signals individually on 16-shot ImageNet; expect 1-4% drops per component (Table III)
  2. **Euclidean vs. Hyperbolic comparison**: Replace Poincaré distance with cosine similarity; expect consistent accuracy drops (Table V: 73.76% vs 75.53% at 16-shot)
  3. **Backbone sweep**: Test ResNet-50, ResNet-101, ViT-B/32, ViT-B/16, ViT-L/14 to verify gains generalize (Table IV shows consistent 2-4% improvements)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the T-DHA framework be effectively extended to Multimodal Large Language Models (MLLMs) to handle joint embeddings beyond standard vision-language tasks?
- Basis in paper: [explicit] The conclusion states the method is general and "can, in principle, be applied to Multimodal Large Language Models (MLLMs)" for modalities like audio and video.
- Why unresolved: The current work validates the approach only on CLIP-like vision-language models; the application to more complex MLLM architectures and diverse modalities remains theoretical.
- What evidence would resolve it: Empirical benchmarks showing T-DHA's performance when applied to the embedding spaces of modern MLLMs (e.g., video-text or audio-text tasks).

### Open Question 2
- Question: Is the heuristic construction of negative textual prompts (prepending "no" or "not") semantically optimal for defining class boundaries in the embedding space?
- Basis in paper: [inferred] Section III-B(5) describes generating negative prompts by simply adding "no" to the class name (e.g., "a photo of no {class}"), a method that may not fully capture the semantic "opposite" or non-membership.
- Why unresolved: While effective, this linguistic heuristic is simplistic compared to the positive prompt ensembling strategies used (e.g., CuPL), potentially leaving discriminative power on the table.
- What evidence would resolve it: An ablation study comparing simple negation against LLM-generated descriptive negative prompts or antonym-based embeddings.

### Open Question 3
- Question: How does the random sampling strategy for constructing negative hyperbolic prototypes affect the stability and robustness of the model compared to deterministic or hard-negative mining approaches?
- Basis in paper: [inferred] Section III-B(3) specifies that negative prototypes are formed by averaging features from a "randomly select[ed] single image" from other classes.
- Why unresolved: Random selection introduces stochastic variance and may not consistently select the most informative negative examples (i.e., hard negatives) to refine decision boundaries.
- What evidence would resolve it: Comparative analysis of model performance variance when using random sampling versus deterministic centroid-based negative prototypes.

## Limitations

- Limited to CLIP-like vision-language models; generalizability to other MLLMs remains theoretical
- Performance degrades on very few shots (1-shot) where prototype quality is insufficient
- Random negative prototype sampling may introduce variance and doesn't guarantee hard-negative selection
- Temperature scaling factor ε is not specified, affecting distance computation calibration

## Confidence

- **High confidence**: Hyperbolic geometry improves over Euclidean cosine similarity (Table V shows consistent gains), training-free adapter framework is well-defined, and ablation of negative learning components shows measurable drops (Table VI)
- **Medium confidence**: Claims about domain generalization (ImageNetV2/Sketch improvements) are well-supported but rely on the same model without additional validation on other domain-shifted datasets
- **Low confidence**: The paper's assertion that T-DHA "exceeds CLIP's original zero-shot performance" is based on comparing to older zero-shot CLIP results rather than direct controlled experiments with identical evaluation conditions

## Next Checks

1. Implement and test different ε values (e.g., 0.5, 1.0, 2.0) to determine optimal temperature scaling and assess sensitivity
2. Compare negative prototype construction methods: random 1-sample vs. larger negative sets vs. using all non-class samples
3. Replicate domain generalization experiments on additional domain-shifted datasets (e.g., ObjectNet, ImageNet-R) to verify generalization beyond the two tested datasets