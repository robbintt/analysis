---
ver: rpa2
title: Temporal Window Smoothing of Exogenous Variables for Improved Time Series Prediction
arxiv_id: '2507.05284'
source_url: https://arxiv.org/abs/2507.05284
tags:
- exogenous
- series
- forecasting
- time
- endogenous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving time series forecasting
  by enhancing exogenous inputs, which are external variables that influence the target
  variable. The core method, Temporal Window Smoothing (TWS), whitens exogenous inputs
  using global statistics to reduce redundancy and increase awareness of long-term
  patterns.
---

# Temporal Window Smoothing of Exogenous Variables for Improved Time Series Prediction

## Quick Facts
- arXiv ID: 2507.05284
- Source URL: https://arxiv.org/abs/2507.05284
- Reference count: 40
- Primary result: Outperforms 11 baselines, achieving 2.29% MSE reduction on ETTh1 and 1.57% on ETTm1

## Executive Summary
This paper addresses the challenge of improving time series forecasting by enhancing exogenous inputs, which are external variables that influence the target variable. The core method, Temporal Window Smoothing (TWS), whitens exogenous inputs using global statistics to reduce redundancy and increase awareness of long-term patterns. This is achieved by projecting each windowed time series onto orthogonal basis vectors derived from the entire training set, dynamically selecting the optimal number of components, and reconstructing a refined exogenous series. The proposed approach consistently outperforms 11 baseline models in four benchmark datasets, achieving state-of-the-art performance.

## Method Summary
Temporal Window Smoothing (TWS) preprocesses exogenous variables by applying PCA whitening based on global training statistics. The method computes the covariance matrix of the training exogenous features, performs eigendecomposition, and projects each exogenous window onto the top-k orthogonal components (selected to retain at least 90% variance). The whitened exogenous series is then integrated with endogenous data via a global token and cross-attention mechanism in a Transformer encoder. This approach reduces redundancy in exogenous inputs while preserving globally significant patterns, improving downstream forecasting performance.

## Key Results
- Achieves state-of-the-art performance on ETTh1, ETTh2, ETTm1, and ETTm2 datasets
- Outperforms 11 baseline models including TimesNet, FEDformer, and CrossLinear
- Demonstrates consistent improvements across multiple forecasting horizons (96, 192, 336, 720 steps)
- Shows 2.29% average MSE reduction on ETTh1 and 1.57% on ETTm1 compared to best baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Projecting exogenous inputs onto orthogonal basis vectors derived from global training statistics reduces redundancy and exposes windowed inputs to long-term patterns.
- Mechanism: PCA-based eigendecomposition computes orthogonal components from the full training covariance matrix; each exogenous window is projected onto the top-k eigenvectors and reconstructed, producing a whitened representation that retains globally dominant modes while discarding locally insignificant fluctuations.
- Core assumption: Global principal components encode useful structure (e.g., trends, seasonality) that generalizes to unseen windows and improves downstream forecasting.
- Evidence anchors:
  - [abstract] "whitens the exogenous input to reduce redundancy that may persist within the data based on global statistics"
  - [Section III-B] "projecting the exogenous series onto a subspace ... retaining only the top-k components that capture the most significant patterns"
  - [corpus] Neighbor work on exogenous variables (XLinear, DAG, CrossLinear) emphasizes better exogenous handling improves forecasting, but none explicitly use global PCA whitening.
- Break condition: If global principal components are unstable across train/test splits (e.g., non-stationarity, regime shifts), the whitened representation may misrepresent test-time structure and degrade performance.

### Mechanism 2
- Claim: Dynamic selection of the number of principal components via explained variance thresholds retains sufficient signal while controlling redundancy/noise.
- Mechanism: Choose the smallest k such that cumulative explained variance ≥ 90%; this adapts the projection rank per dataset instead of using a fixed hyperparameter.
- Core assumption: A fixed explained variance threshold generalizes across datasets and horizons without requiring per-horizon tuning.
- Evidence anchors:
  - [Section III-B] "select the smallest value of k that retains at least 90% of the variance"
  - [Section V] acknowledges future work includes broader domains/tasks; current evidence is limited to evaluated benchmarks.
  - [corpus] No direct corpus comparison for dynamic k-selection was found; neighbor papers focus on architectural integration of exogenous variables rather than preprocessing whitening.
- Break condition: If a single 90% threshold under- or over-selects components for a specific dataset (e.g., many small-variance modes are predictive), fixed-threshold selection may be suboptimal.

### Mechanism 3
- Claim: Reconstructed, whitened exogenous series integrated via a global token and cross-attention can improve forecasting over raw exogenous integration.
- Mechanism: Endogenous patches interact through self-attention with a learnable global token, which then queries whitened exogenous tokens via cross-attention; this controls information flow and avoids naive concatenation that may inject redundant or noisy signals.
- Core assumption: A global-token-mediated cross-attention bridge effectively selects and propagates useful exogenous information into endogenous representations.
- Evidence anchors:
  - [Section III-C] "cross-attention mechanism uses the learned global token ... as queries and keys, while the exogenous variable embeddings as values"
  - [Table II ablation] Cross-attention with TWS outperforms both concatenation and cross-attention without TWS on ETTh1/ETTm1/Weather.
  - [corpus] DAG and CrossLinear propose cross-correlation modules for exogenous integration, suggesting architectural fusion matters; however, they do not evaluate global PCA whitening.
- Break condition: If exogenous variables are already low-redundancy and highly informative, whitening could over-compress useful information, and the cross-attention bridge may not recover it.

## Foundational Learning

### Concept: PCA/Eigendecomposition and Orthogonal Projections
- Why needed here: TWS relies on computing covariance, eigenvalues/eigenvectors, and projecting data onto orthogonal bases to separate global structure from local noise.
- Quick check question: Given a mean-centered data matrix X, can you write the covariance matrix and explain what eigenvectors represent in terms of variance directions?

### Concept: Exogenous vs Endogenous Variables in Forecasting
- Why needed here: The method explicitly processes exogenous inputs before integrating with endogenous history; understanding their roles clarifies why whitening is applied to exogenous only.
- Quick check question: If target series and an exogenous covariate come from the same source, what redundancy issue might arise when they are concatenated?

### Concept: Transformer Attention for Multivariate Time Series
- Why needed here: The architecture uses self-attention over patches and cross-attention to fuse exogenous information via a global token; knowing attention basics helps debug integration.
- Quick check question: How does cross-attention differ from self-attention in terms of keys/values sources, and what does that imply for integrating two modalities?

## Architecture Onboarding

### Component map
- Input preprocessing: Instance normalization for both endogenous X and exogenous E
- TWS block (non-learnable): E → mean-center → project onto global eigenvectors V (k selected by variance threshold) → reconstruct O = V(V^T E) + μ
- Endogenous path: Patching → linear projection → concatenate global token
- Exogenous path: O → linear projection to tokens H_ex
- Transformer backbone: Self-attention over [patches; global token]; cross-attention where global token queries H_ex; conv-layer post-attention; MLP head to predictions

### Critical path
- Compute global PCA statistics offline from training data (V, μ)
- At train/inference, apply TWS to each exogenous window; then proceed with endogenous patching and transformer forward pass

### Design tradeoffs
- Fixed explained variance threshold vs dataset-specific k selection
- Non-learned TWS (stable, no extra parameters) vs a learned whitening module (more flexible but potentially overfitting)
- Cross-attention bridge vs direct concatenation; cross-attention adds capacity and control but increases compute

### Failure signatures
- Test-time distribution shift: global eigenvectors misaligned with test exogenous patterns; TWS may distort useful signal
- Over-compression: if k is too small due to the 90% threshold, important subtle exogenous patterns may be suppressed
- Redundant exogenous-endogenous source: if endogenous and exogenous overlap heavily, even whitening may not fully resolve redundancy

### First 3 experiments
1. Reproduce ETTh1/ETTm1 baselines with and without TWS to validate MSE deltas reported in the paper
2. Ablate k by varying the explained variance threshold (e.g., 80%, 90%, 95%, 99%) and plot MSE vs k per dataset to test the 90% choice
3. Stress-test TWS under synthetic non-stationarity (e.g., mean/variance drift in test exogenous inputs) to assess robustness and identify break conditions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does integrating Temporal Window Smoothing (TWS) into linear-based architectures yield performance gains comparable to those seen in Transformer models?
- Basis in paper: [explicit] The conclusion explicitly lists integrating TWS into "linear-based models" to evaluate broader applicability as future work.
- Why unresolved: All reported experiments were restricted to Transformer-based backbones; the utility of whitened exogenous inputs for simpler, non-attention architectures remains untested.
- What evidence would resolve it: Benchmark results comparing standard linear models (e.g., DLinear, TiDE) against TWS-augmented versions on the Electricity or Traffic datasets.

### Open Question 2
- Question: Can the TWS mechanism effectively improve performance on auxiliary time series tasks such as imputation or classification?
- Basis in paper: [explicit] The authors explicitly state a need to "investigate the potential of our approach in other tasks, such as short-term forecasting, imputation, and classification."
- Why unresolved: The current study validates the method exclusively on long-term forecasting tasks (horizons 96–720 steps).
- What evidence would resolve it: Performance metrics (e.g., reconstruction error, accuracy) derived from applying the TWS block to masked time-series imputation or classification benchmarks.

### Open Question 3
- Question: Why does the global whitening approach underperform on specific datasets like Traffic, and how can this be mitigated?
- Basis in paper: [inferred] The paper reports outperforming baselines in only 4 of 7 datasets, implicitly acknowledging failure cases where global statistics may conflict with local dynamics.
- Why unresolved: The authors do not analyze why reconstructing exogenous variables based on global PCA components degrades performance on datasets dominated by local, non-stationary variations.
- What evidence would resolve it: An ablation study on the Traffic dataset comparing global PCA components against local window-based components to isolate the source of information loss.

## Limitations
- Missing core Transformer hyperparameters (embedding size, layers, heads) prevents exact reproduction
- Fixed 90% variance threshold may not generalize optimally across all datasets or horizons
- Global PCA statistics may be unstable under test-time distribution shifts or non-stationarity

## Confidence
- **High Confidence**: The core TWS mechanism (PCA whitening of exogenous inputs) is mathematically sound and well-specified. The ablation showing cross-attention + TWS outperforms both naive concatenation and cross-attention alone is strong empirical evidence.
- **Medium Confidence**: The 2.29% MSE reduction on ETTh1 and 1.57% on ETTm1 versus best baselines is credible, but exact reproduction is limited by missing architectural hyperparameters.
- **Low Confidence**: The generalization claim across "four benchmark datasets" and all horizons is based on limited evidence. The 90% variance threshold is heuristic without per-dataset or per-horizon tuning.

## Next Checks
1. **Hyperparameter Sensitivity**: Vary the explained variance threshold (e.g., 80%, 90%, 95%, 99%) and plot MSE vs. k for each dataset to test whether 90% is optimal or dataset-specific.
2. **Architectural Ablation**: Reproduce the full model with and without TWS on ETTh1/ETTm1, and also test alternative k-selection strategies (fixed k, per-horizon k) to isolate the impact of the 90% threshold.
3. **Distribution Shift Robustness**: Inject synthetic non-stationarity (e.g., mean/variance drift) into exogenous test inputs and measure TWS performance degradation versus raw exogenous inputs to identify break conditions.