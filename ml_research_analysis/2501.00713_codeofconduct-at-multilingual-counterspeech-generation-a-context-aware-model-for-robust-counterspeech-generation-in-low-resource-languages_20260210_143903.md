---
ver: rpa2
title: 'CODEOFCONDUCT at Multilingual Counterspeech Generation: A Context-Aware Model
  for Robust Counterspeech Generation in Low-Resource Languages'
arxiv_id: '2501.00713'
source_url: https://arxiv.org/abs/2501.00713
tags:
- counterspeech
- each
- basque
- language
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents CODEOFCONDUCT, a context-aware model for multilingual
  counterspeech generation, which ranked first in the MCG-COLING-2025 shared task.
  The model addresses the challenge of generating effective counterspeech in low-resource
  languages like Basque by combining a simulated annealing algorithm with a round-robin
  ranking mechanism.
---

# CODEOFCONDUCT at Multilingual Counterspeech Generation: A Context-Aware Model for Robust Counterspeech Generation in Low-Resource Languages

## Quick Facts
- arXiv ID: 2501.00713
- Source URL: https://arxiv.org/abs/2501.00713
- Reference count: 28
- Ranked first in the MCG-COLING-2025 shared task for Basque counterspeech generation

## Executive Summary
CODEOFCONDUCT presents a context-aware model for multilingual counterspeech generation that addresses the challenge of generating effective counterspeech in low-resource languages like Basque. The approach combines a simulated annealing algorithm with a round-robin ranking mechanism to produce high-quality counterspeech candidates. The model achieved state-of-the-art performance across four languages (Basque, English, Italian, and Spanish), securing first place for Basque, second for Italian, and third for both English and Spanish. The work demonstrates how innovative optimization techniques can overcome linguistic and resource constraints in multilingual counterspeech generation.

## Method Summary
CODEOFCONDUCT employs a three-stage inference-only approach. First, simulated annealing generates diverse counterspeech candidates by iteratively refining responses based on LLM-based evaluation using JudgeLM. Second, a round-robin ranking mechanism ensures high-quality outputs through pairwise comparisons of top candidates. Finally, the system assembles submissions after sanity checks. The model uses background knowledge from the Multitarget-CONAN dataset and employs multiple LLMs including Hermes-3-8B, Zephyr-7B, and Llama-3.1-70B. The approach specifically addresses low-resource language challenges through vocabulary sampling that incorporates hate speech tokens alongside language-specific word lists.

## Key Results
- Ranked first for Basque, second for Italian, and third for both English and Spanish in the MCG-COLING-2025 shared task
- Swept all top three positions for Basque, demonstrating effectiveness in low-resource scenarios
- Achieved state-of-the-art performance across four languages using BLEU, ROUGE-L, BERTScore, and JudgeLM metrics
- Computational cost of approximately 10 hours per language on NVIDIA A100

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simulated annealing enables exploration of the counterspeech candidate space beyond single-pass generation, yielding higher-quality responses in low-resource settings.
- Mechanism: The algorithm initializes with background knowledge as seed text, then iteratively generates candidate variations by appending sampled vocabulary, evaluates each via JudgeLM, selects candidates using Boltzmann-style probabilities allowing lower-scoring candidates to survive early iterations, and passes selected candidates to LLMs for expansion. Temperature increases across iterations, shifting from exploration toward exploitation.
- Core assumption: The search space contains globally better responses than single-pass generation can reach, and JudgeLM scores correlate meaningfully with counterspeech quality.
- Evidence anchors: [section 3.3] describes the iterative refinement process and Boltzmann selection; hyperparameter tuning found 8 iterations × 6 candidates per loop achieved target score 10.

### Mechanism 2
- Claim: Round-robin pairwise ranking reduces score variance from JudgeLM's sensitivity to comparison context, producing more stable final rankings.
- Mechanism: After simulated annealing generates candidates across 400 hate speech instances, the top-6 candidates per instance enter a tournament. Each pair is compared twice—once in each order—to mitigate position bias. Scores are accumulated and averaged across all matches to produce a final ranking per candidate.
- Core assumption: Position bias exists in JudgeLM comparisons, and averaging over pairwise matches yields a more reliable quality signal than single evaluations.
- Evidence anchors: [section 3.4] shows large variance between original algorithm scores and recalculated comparison scores; time complexity analysis shows round-robin is O(n²), motivating use on only top-6 candidates.

### Mechanism 3
- Claim: Vocabulary sampling that incorporates hate speech tokens alongside language-specific word lists improves contextual grounding, particularly valuable when training data is scarce.
- Mechanism: During candidate generation in simulated annealing, new candidates are created by appending randomly sampled words from language-specific word lists where part of the vocabulary is from tokenized hate speech. This grounds responses in the original hate speech context.
- Core assumption: Including HS-derived vocabulary increases relevance without amplifying harmful content, and multilingual LLMs can appropriately contextualize these tokens.
- Evidence anchors: [section 3.3] notes that part of the vocabulary sampled is from tokenized HS to enrich vocabulary and introduce relevant words; authors attribute Basque success partly to a word sampling strategy enhanced for low-resource scenarios.

## Foundational Learning

- Concept: **Simulated Annealing in Discrete Optimization**
  - Why needed here: Core generation loop; understanding temperature schedules and acceptance probabilities is essential for tuning and debugging.
  - Quick check question: Given candidates with scores [3, 5, 8] at T=2 using the paper's probability formula, which candidate is most likely selected? What happens as T→∞ or T→1?

- Concept: **LLM-as-Judge Evaluation**
  - Why needed here: JudgeLM provides the reward signal for annealing; understanding its biases informs when round-robin correction is necessary.
  - Quick check question: If JudgeLM systematically prefers longer responses, how would this bias propagate through the annealing loop?

- Concept: **Low-Resource Language Challenges (Agglutinative Morphology)**
  - Why needed here: Basque's morphological complexity makes vocabulary overlap metrics like BLEU less reliable; motivates context-aware approaches.
  - Quick check question: Why might token-level BLEU scores underperform for Basque compared to English in counterspeech evaluation?

## Architecture Onboarding

- Component map: Background knowledge -> Simulated Annealing Generator -> JudgeLM scoring -> Round-Robin Ranker -> Final submissions

- Critical path: Stage 1 dominates runtime (~10 hours/language on A100 per Section "Computational Cost"). The bottleneck is O(N_max × n × k²) JudgeLM calls in annealing plus O(n²) in round-robin.

- Design tradeoffs:
  - More iterations/candidates improve quality but increase compute quadratically
  - Round-robin on all candidates is prohibitive; restricting to top-6 sacrifices some recall
  - Using only test data (no training) simplifies pipeline but limits task-specific adaptation

- Failure signatures:
  - Temperature increasing too slowly → excessive exploration, slow convergence
  - Temperature increasing too rapidly → premature convergence to local optima
  - JudgeLM inconsistencies → high variance between Stage 1 and Stage 2 scores
  - HS-token injection without safeguards → generated CS echoing hateful vocabulary

- First 3 experiments:
  1. Temperature schedule ablation: Run annealing with linear vs. exponential T increase on 4 HS instances; compare average high scores and convergence speed to baseline.
  2. Round-robin depth test: Compare top-3 vs. top-6 vs. top-10 candidates in round-robin; measure score stability and runtime to validate the top-6 design choice.
  3. Vocabulary source ablation: Run with (a) HS tokens only, (b) language wordlist only, (c) both; evaluate on Basque to test the contextual grounding hypothesis.

## Open Questions the Paper Calls Out

- How can the static simulated annealing approach be adapted to respond to rapidly evolving hate speech patterns driven by offline events? The current model relies on fixed vocabulary and metrics that cannot account for dynamic hate speech shifts.

- Can evaluation frameworks be developed to capture cultural nuances and language-specific expressions beyond simple lexical similarity? Current JudgeLM models may overemphasize lexical similarities while missing cultural nuances.

- How can the quadratic time complexity of the round-robin evaluation be reduced to facilitate real-time application? The exhaustive pairwise comparison strategy renders the model impractical for immediate, large-scale deployment.

## Limitations

- Computational cost: Processing requires approximately 10 hours per language due to quadratic time complexity of round-robin evaluation
- Static word lists: Reliance on static vocabulary constrains ability to adapt to rapidly evolving hate speech patterns
- Cultural nuances: Current JudgeLM models may overemphasize lexical similarities while missing cultural nuances that affect resonance with speakers

## Confidence

- **High confidence**: The core methodology (simulated annealing + round-robin ranking) is clearly described and mathematically sound. The Basque performance claims are well-supported by task results.
- **Medium confidence**: The mechanism for incorporating HS tokens is described but not fully validated. The round-robin correction's effectiveness relies on anecdotal evidence rather than systematic analysis.
- **Low confidence**: Computational efficiency claims are not independently verified, and the vocabulary sampling strategy lacks detailed justification.

## Next Checks

1. Implement temperature schedule ablation: Compare linear vs. exponential T increase on 4 HS instances to validate the convergence behavior described in the paper.

2. Test vocabulary source ablation: Run generation with HS tokens only, language wordlist only, and both to empirically validate the contextual grounding hypothesis.

3. Analyze JudgeLM consistency: Re-score same candidates with different random seeds to quantify variance and assess whether round-robin correction is truly necessary.