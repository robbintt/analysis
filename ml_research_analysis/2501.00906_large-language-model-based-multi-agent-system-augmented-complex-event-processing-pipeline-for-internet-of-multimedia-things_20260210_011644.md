---
ver: rpa2
title: Large Language Model Based Multi-Agent System Augmented Complex Event Processing
  Pipeline for Internet of Multimedia Things
arxiv_id: '2501.00906'
source_url: https://arxiv.org/abs/2501.00906
tags:
- agents
- system
- agent
- video
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a proof-of-concept for a Large Language Model
  (LLM) based multi-agent system augmented with a publish/subscribe (pub/sub) paradigm
  to address limitations in current complex event processing (CEP) systems. The system
  integrates the Autogen LLM orchestration framework with Kafka message brokers to
  create an autonomous CEP pipeline capable of handling complex workflows, particularly
  for video query processing.
---

# Large Language Model Based Multi-Agent System Augmented Complex Event Processing Pipeline for Internet of Multimedia Things

## Quick Facts
- arXiv ID: 2501.00906
- Source URL: https://arxiv.org/abs/2501.00906
- Authors: Talha Zeeshan; Abhishek Kumar; Susanna Pirttikangas; Sasu Tarkoma
- Reference count: 40
- Primary result: Four-agent configuration optimally balances functionality and performance for LLM-based multi-agent CEP systems

## Executive Summary
This paper presents a proof-of-concept for a Large Language Model (LLM) based multi-agent system augmented with a publish/subscribe (pub/sub) paradigm to address limitations in current complex event processing (CEP) systems. The system integrates the Autogen LLM orchestration framework with Kafka message brokers to create an autonomous CEP pipeline capable of handling complex workflows, particularly for video query processing. Extensive experiments evaluated system performance across varying agent counts and video complexities. Results showed that while higher agent counts and video complexities increase latency, the system maintains high consistency in narrative coherence. The optimal configuration for this use case was four agents, balancing functionality and performance. The system demonstrates strong integration with existing tools and technologies, showcasing potential for easy adoption of LLM multi-agent systems in distributed AI applications.

## Method Summary
The system uses Autogen LLM framework with Kafka message brokers to create an autonomous CEP pipeline. Four specialized agents (User Proxy, Reflection, Engineer-1, Engineer-2) work together with predefined tool functions to process video queries. The pipeline consumes video frames from Kafka topics, extracts frames, reconstructs video, and uses GPT-4o for analysis. The workflow is controlled by a keyword-based finite state machine for agent communication. Experiments tested the system with five video complexity levels at four resolutions, measuring latency and quality scores across five evaluation criteria.

## Key Results
- Four-agent configuration provides optimal balance between functionality and latency (22-25s vs 5-8s for two agents)
- Lower resolution (144p) reduces latency 40-60% but degrades accuracy for complex videos
- System maintains high consistency in narrative coherence despite increased latency with more agents
- LLM-driven dynamic topic subscription enables autonomous video stream processing without manual configuration

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distributing specialized functions across multiple agents with distinct roles can manage complex video query workflows more effectively than single-agent systems, but at the cost of increased latency due to inter-agent communication overhead.
- **Mechanism:** Role-specialized agents (User Proxy, Reflection, Engineer-1, Engineer-2) decompose complex queries into discrete tasks—each agent handles a subset of tools and functions, enabling granular control over workflow stages while maintaining conversational context through shared memory.
- **Core assumption:** Agents can accurately infer workflow state and next-speaker requirements from conversation context and keyword triggers (e.g., "RECHECK", "QUERY").
- **Evidence anchors:**
  - [abstract] "latency increases with higher agent counts and video complexities, with a four-agent configuration identified as optimal"
  - [section 5.2, Table 1] Two-agent latency: 5-8s; four-agent latency: 22-25s; agent overhead grows from 1-2s to 12-16s
  - [corpus] Weak direct corpus support for this specific multi-agent latency trade-off in CEP; related work on multi-agent robotics (Smart-LLM) and software development (MetaGPT, ChatDev) shows similar task decomposition patterns but without latency benchmarks
- **Break condition:** When agent count exceeds four, redundant communications and speaker-selection ambiguity cause excessive latency without functional gains; keyword-based speaker selection becomes unmanageable in highly dynamic workflows.

### Mechanism 2
- **Claim:** Reducing video resolution decreases GPT model call latency proportionally more than total pipeline latency, because model inference is the dominant cost for vision-language processing.
- **Mechanism:** Lower-resolution frames contain fewer pixels and less visual detail, reducing both tokenized input size and inference computation for the vision-language model (GPT-4o), while frame extraction and agent conversation overhead remain relatively stable.
- **Core assumption:** The vision-language model's inference time scales primarily with input image resolution, not frame count or video duration.
- **Evidence anchors:**
  - [abstract] "system effectively processes videos at lower resolutions (144p) with significantly reduced latency compared to higher resolutions (1080p)"
  - [section 5.4.1, Figure 16] Level-1 complexity: GPT call latency drops from 10.2s (1080p) to 5.3s (144p); total duration drops from 13.37s to 8.2s
  - [corpus] No direct corpus comparison; related video analytics work (AVA, Video-ChatGPT) focuses on task accuracy rather than resolution-latency trade-offs
- **Break condition:** When video complexity is high (Level 4-5) and detailed object identification is required, low-resolution processing degrades correctness and detailed orientation scores significantly (e.g., Level 5 correctness drops to 0.6 at 1080p).

### Mechanism 3
- **Claim:** Integrating LLM agents with publish/subscribe message brokers enables autonomous, decoupled complex event processing that can dynamically consume and analyze video streams based on natural-language queries.
- **Mechanism:** User queries trigger LLM-backed agents to subscribe to specific Kafka topics (e.g., "camera-1"), consume frames, invoke predefined tool functions (frame extraction, video reconstruction, model inference), and return synthesized answers—all without manual workflow configuration.
- **Core assumption:** The LLM can reliably map natural-language queries to the correct topic subscriptions and tool invocation sequences without human intervention.
- **Evidence anchors:**
  - [abstract] "Utilizing the Autogen framework in conjunction with Kafka message brokers, the system demonstrates an autonomous CEP pipeline capable of handling complex workflows"
  - [section 4.4, Figure 6] Complete data flow shows: User query → User Proxy Agent → Conversable Agent subscribes to Kafka topic → consumes frames → processes via tool-backed agent → returns answer
  - [corpus] Saleh et al. [10] propose GenAI-specific message broker architectures with LLM-driven dynamic load adaptation; Lovén et al. [42] introduce neural pub/sub paradigm for AI workflow distribution—both align conceptually but lack implementation details
- **Break condition:** When tools are missing or incorrectly named (e.g., calling `reanalyze()` when only `analyze()` exists), agents generate error messages and fail to recover without human intervention or additional prompt clarification.

## Foundational Learning

- **Concept: Publish/Subscribe (Pub/Sub) Paradigm**
  - **Why needed here:** The system uses Kafka as the message broker to decouple video sources (publishers/cameras) from processing agents (subscribers). Understanding topics, brokers, and decoupled communication is essential for debugging data flow.
  - **Quick check question:** Can you explain how a subscriber receives only messages matching its topic subscription without knowing the publisher's identity?

- **Concept: LLM Agent Types (LLM-backed, Human-backed, Tool-backed)**
  - **Why needed here:** The Autogen framework combines these agent types—LLM-backed for reasoning, human-backed for oversight, tool-backed for executing predefined functions like `kafka_consume` and `frame_extraction`.
  - **Quick check question:** What is the key difference between a tool-backed agent executing a predefined function versus an LLM-backed agent generating dynamic code?

- **Concept: Complex Event Processing (CEP) Evaluation Criteria**
  - **Why needed here:** Video query quality is measured across five dimensions (Correctness, Detailed Orientation, Contextual Understanding, Temporal Understanding, Consistency). Understanding these metrics is critical for interpreting experimental results and tuning system performance.
  - **Quick check question:** Which evaluation criterion measures whether the system accurately describes the sequence and progression of events over time?

## Architecture Onboarding

- **Component map:** User Proxy Agent → Reflection Agent → Engineer Agents → Kafka topics → Tool-backed agents → GPT-4o → Answer synthesis
- **Critical path:**
  1. User submits query (e.g., "Are there red vehicles in camera 1-5?")
  2. User Proxy Agent routes to Reflection Agent
  3. Reflection Agent triggers Engineer Agent(s) via keyword detection
  4. Engineer Agent subscribes to relevant Kafka topic, consumes frames
  5. Tool-backed agent extracts frames, reconstructs video, calls GPT-4o
  6. LLM returns synthesized answer → User Proxy Agent → User
- **Design tradeoffs:**
  - **Agent count vs. latency:** Four agents provide optimal role specialization for this use case; more agents increase communication overhead disproportionately
  - **Resolution vs. accuracy:** Lower resolution (144p) reduces latency 40-60% but degrades correctness and detail scores for complex scenes (Level 4-5)
  - **Predefined tools vs. dynamic code generation:** Tools improve reliability and security but limit adaptability to novel query types
- **Failure signatures:**
  - **Missing tool error:** Agent calls non-existent function (e.g., `reanalyze()`); workflow terminates with error message
  - **Speaker selection ambiguity:** With >4 agents using Auto or Round-Robin selection, incorrect agents take control, causing workflow divergence
  - **Context loss in multi-agent chains:** Information from earlier prompts can be lost when intermediate agents generate dynamic sub-prompts without full context propagation
- **First 3 experiments:**
  1. **Latency baseline with varying agent counts:** Run the same video query (fixed video, fixed prompt) with 2, 3, and 4 agents; measure total latency and agent overhead to identify optimal configuration
  2. **Resolution sweep on single complexity level:** Process a Level-2 video at 1080p, 720p, 360p, 144p; record GPT call latency, frame extraction time, and answer quality scores
  3. **Complexity vs. accuracy benchmark:** Run all five complexity levels at 1080p using a two-agent system; score outputs on five evaluation criteria to establish accuracy degradation curve

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can inter-agent communication efficiency be optimized to reduce latency in configurations with more than four agents?
- **Basis in paper:** [explicit] The conclusion identifies enhancing inter-agent communication efficiency as a primary focus to reduce latency and improve performance without sacrificing functionality.
- **Why unresolved:** Experiments showed that increasing agent count from two to four raised average latency from 5–8 seconds to 22–25 seconds due to conversational overhead.
- **What evidence would resolve it:** Demonstration of a stable latency profile (e.g., <20s) in a five-plus agent system through optimized communication protocols.

### Open Question 2
- **Question:** How can speaker selection be automated for complex workflows to avoid the manageability issues of keyword-based state machines?
- **Basis in paper:** [inferred] The limitations section notes that the current keyword-based "finite state machine" approach becomes increasingly difficult to manage and code as workflows expand.
- **Why unresolved:** The current manual optimization relies on specific keywords, which is rigid and poses a practical limit to workflow complexity.
- **What evidence would resolve it:** A dynamic speaker selection algorithm that successfully manages workflow transitions in a system with more than four agents.

### Open Question 3
- **Question:** How can federated learning and edge computing be integrated into the LLM-MAS CEP pipeline to improve scalability?
- **Basis in paper:** [explicit] The authors state the current implementation does not fully leverage these technologies and suggest exploring their integration to enhance distribution and efficiency.
- **Why unresolved:** The proof-of-concept relies on centralized cloud instances for LLM processing, potentially underutilizing the computing continuum.
- **What evidence would resolve it:** A modified architecture successfully executing agent tasks or model training on edge devices.

## Limitations

- Four-agent configuration introduces 12-16s communication overhead compared to two-agent setups
- Resolution scaling shows diminishing returns—144p reduces latency 40-60% but significantly degrades accuracy for complex videos
- System reliability depends heavily on predefined tool functions, creating brittleness when agents attempt to call non-existent tools

## Confidence

- **High Confidence**: Latency measurements across agent counts and resolution levels, as these are empirical observations with clear quantitative evidence
- **Medium Confidence**: Claims about optimal four-agent configuration, as this represents a specific experimental finding rather than a universal principle
- **Low Confidence**: Generalization of results to other CEP use cases beyond video query processing, due to limited experimental scope

## Next Checks

1. **Speaker Selection Robustness**: Test the finite state machine with edge cases (missing keywords, ambiguous phrases) to verify it doesn't cause agent deadlock or infinite loops
2. **Tool Hallucination Recovery**: Implement a recovery mechanism for when agents call non-existent tools, measuring whether the system can self-correct or requires manual intervention
3. **Cross-Domain Applicability**: Validate the multi-agent CEP architecture with non-video data streams (sensor data, text logs) to assess generalizability beyond the video use case