---
ver: rpa2
title: 'Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation
  for OffensiveLanguage Identification'
arxiv_id: '2508.11166'
source_url: https://arxiv.org/abs/2508.11166
tags:
- tulu
- offensive
- language
- dataset
- code-mixed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces the first benchmark dataset for Offensive
  Language Identification (OLI) in code-mixed Tulu, a low-resource Dravidian language.
  The dataset comprises 3,845 YouTube comments annotated with high inter-annotator
  agreement (Krippendorff''s alpha = 0.984) across four categories: Not Offensive,
  Not Tulu, Offensive Untargeted, and Offensive Targeted.'
---

# Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification

## Quick Facts
- arXiv ID: 2508.11166
- Source URL: https://arxiv.org/abs/2508.11166
- Reference count: 8
- Primary result: Introduced first benchmark dataset for Offensive Language Identification in code-mixed Tulu; BiGRU+SA achieved 82% accuracy and 0.81 macro F1-score

## Executive Summary
This study addresses the challenge of offensive language identification in Tulu, a low-resource Dravidian language with limited NLP resources. The authors created the first benchmark dataset comprising 3,845 YouTube comments, annotated across four categories with high inter-annotator agreement. Through systematic evaluation of multiple deep learning architectures, the study demonstrates that bidirectional recurrent models with self-attention outperform both simpler architectures and multilingual transformers on this code-mixed, low-resource task.

## Method Summary
The research employed a comprehensive evaluation of deep learning models for offensive language detection in code-mixed Tulu. A corpus of 3,845 YouTube comments was created and annotated with high inter-annotator agreement. Multiple architectures were tested including LSTM, GRU, BiLSTM, BiGRU, CNN, and attention-based variants, alongside transformer models mBERT and XLM-RoBERTa. The BiGRU model with self-attention achieved the best performance, while transformer models underperformed due to limited Tulu representation in pretraining data.

## Key Results
- BiGRU+SA model achieved 82% accuracy and 0.81 macro F1-score on test data
- Transformer models (mBERT, XLM-RoBERTa) underperformed significantly on offensive categories
- High inter-annotator agreement (Krippendorff's alpha = 0.984) ensured reliable annotation quality
- Class imbalance posed challenges, with majority class dominating simpler model predictions

## Why This Works (Mechanism)

### Mechanism 1
- Bidirectional recurrent architectures with self-attention improve offensive language detection by capturing contextual dependencies that unidirectional models miss.
- The BiGRU+SA model processes text in both forward and backward directions, then uses attention to weight tokens dynamically, emphasizing informative offensive cues while suppressing noise.
- Core assumption: Code-mixed Tulu offensive language relies on contextual patterns distributed across the sequence.
- Evidence: BiGRU+SA achieved 82% accuracy and 0.81 macro F1-score, outperforming simpler models on minority offensive classes.

### Mechanism 2
- Multilingual transformers underperform on low-resource languages due to insufficient representation in pretraining corpora.
- mBERT and XLM-RoBERTa lack adequate Tulu examples during pretraining, preventing effective transfer learning for code-mixed text.
- Core assumption: Pretrained multilingual models rely on language overlap between pretraining and downstream tasks.
- Evidence: XLM-RoBERTa returned zero F1-scores for both offensive categories, while achieving reasonable performance on majority class only.

### Mechanism 3
- High inter-annotator agreement enables reliable supervised learning despite limited dataset size.
- Consistent annotation reduces label noise, allowing models to learn meaningful patterns rather than conflicting signals.
- Core assumption: Offensive language categories in Tulu can be consistently operationalized by native speakers.
- Evidence: Krippendorff's alpha = 0.984 indicates excellent agreement, critical for low-resource settings where every labeled example carries higher marginal value.

## Foundational Learning

- **Code-mixing in low-resource languages**: Tulu social media text blends Tulu, Kannada, and English without consistent orthography. Understanding code-mixing is essential for preprocessing and feature extraction.
  - Quick check: Can you identify at least two languages mixed in "Nikleg posa posa video apload manpare danni?"

- **Self-attention mechanisms in sequence models**: The best-performing model uses attention to weight tokens dynamically. Understanding attention helps diagnose why it captures dispersed offensive cues better than fixed-weight models.
  - Quick check: In a self-attention layer, what does the attention weight for a given token represent?

- **Class imbalance in offensive language datasets**: The "Not Offensive" class dominates, causing simpler models to achieve high accuracy by predicting only the majority class. Macro F1-score reveals true performance across all classes.
  - Quick check: Why might accuracy be misleading when evaluating models on imbalanced offensive language datasets?

## Architecture Onboarding

- **Component map**: Raw YouTube comments → Preprocessing → Tokenization with padding → Embedding layer (100-dim) → BiGRU layer (128 units, bidirectional) → Self-attention layer (256 attention size) → Dense output layer (softmax, 4 classes)

- **Critical path**: The embedding layer uses random initialization (no FastText/GloVe for Tulu), requiring the model to learn all word representations from scratch using only 2,692 training samples. The self-attention mechanism is critical for compensating for limited data by focusing on the most informative tokens.

- **Design tradeoffs**: 128 hidden units balances capacity with overfitting risk on small data; no pretrained embeddings due to Tulu's resource scarcity; TF-IDF vs. BERT tokenizer differences across model families.

- **Failure signatures**: LSTM/GRU models predicting only "Not Offensive" (recall = 1.00, other classes = 0.00) indicates majority class collapse; XLM-RoBERTa returning zero F1 on offensive classes suggests pretraining mismatch; large gap between accuracy and macro F1 reveals class imbalance exploitation.

- **First 3 experiments**: 1) Baseline replication of BiGRU+SA with specified architecture; 2) Ablation study removing self-attention to quantify attention's contribution; 3) Class-weighted loss to address imbalance and measure impact on offensive class macro F1.

## Open Questions the Paper Calls Out

- Can generative transformer architectures like T5 or GPT-3 outperform the current BiGRU+SA baseline for Tulu Offensive Language Identification?
- Does incorporating explicit morphological and sentiment-based features improve classification accuracy for minority classes like "Offensive Targeted"?
- To what extent can cross-lingual transfer learning from high-resource Dravidian languages (e.g., Kannada, Tamil) enhance detection performance in low-resource Tulu?

## Limitations

- Dataset size of 3,845 comments limits generalizability to other social media platforms and communication contexts
- Code-mixing variability across regions and user demographics is not characterized in the dataset
- Annotation ambiguity regarding sarcasm, cultural references, and "Not Tulu" category boundaries remains unclear

## Confidence

**High Confidence**: Dataset creation methodology is sound with proper inter-annotator agreement metrics; transformer underperformance due to limited Tulu representation is well-supported.

**Medium Confidence**: BiGRU+SA architecture optimality requires additional ablation studies; attention mechanism's specific contribution versus bidirectional processing needs isolation.

**Low Confidence**: Simpler models' potential performance on purely lexical offensive patterns is speculative; generalizability to other code-mixed low-resource languages remains uncertain.

## Next Checks

1. Test the trained BiGRU+SA model on offensive language detection in other Tulu-containing datasets (e.g., Twitter, Facebook comments) to assess whether YouTube-specific patterns generalize.

2. Systematically remove or modify the self-attention component while keeping other architectural elements constant to quantify the exact contribution of attention versus bidirectional processing.

3. Conduct analysis of multilingual transformer pretraining corpora to quantify Tulu representation and identify which Dravidian languages could provide effective transfer learning candidates.