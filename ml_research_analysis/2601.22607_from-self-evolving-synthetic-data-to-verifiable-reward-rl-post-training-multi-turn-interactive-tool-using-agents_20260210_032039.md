---
ver: rpa2
title: 'From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn
  Interactive Tool-Using Agents'
arxiv_id: '2601.22607'
source_url: https://arxiv.org/abs/2601.22607
tags:
- user
- training
- data
- agents
- pass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a framework for training interactive tool-using
  agents that combine self-evolving synthetic data generation with reinforcement learning.
  The system, EigenData, uses a hierarchical multi-agent architecture to synthesize
  multi-turn dialogues with tool calls and generates executable verification functions
  for reward computation.
---

# From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents

## Quick Facts
- **arXiv ID**: 2601.22607
- **Source URL**: https://arxiv.org/abs/2601.22607
- **Reference count**: 40
- **Primary result**: State-of-the-art performance on τ²-bench with 73.0% pass¹ on Airline and 98.3% pass¹ on Telecom using Qwen3-235B-A22B

## Executive Summary
This paper presents EigenData, a framework for training interactive tool-using agents through self-evolving synthetic data generation combined with reinforcement learning. The system addresses two critical challenges in interactive tool use: scalable data acquisition for multi-turn dialogues and stable RL training under user simulation variance. By using a hierarchical multi-agent architecture to generate tool-calling dialogues with executable verification functions, the approach achieves state-of-the-art results on the τ²-bench benchmark, matching or exceeding frontier models like GPT-4 and Claude-3.5-Sonnet.

## Method Summary
The EigenData framework combines synthetic data generation with reinforcement learning to train interactive tool-using agents. A hierarchical multi-agent architecture generates multi-turn dialogues with tool calls, creating diverse training data that includes challenging failure cases. The system generates executable verification functions alongside each dialogue to enable verifiable reward computation during RL training. The RL recipe includes user model fine-tuning for stable simulation, large-batch training with group-relative advantages, and dynamic filtering to remove uninformative trajectories. This approach specifically targets the challenges of sparse and delayed rewards in multi-turn interactions while maintaining training stability.

## Key Results
- Achieves state-of-the-art performance on τ²-bench with 73.0% pass¹ on Airline and 98.3% pass¹ on Telecom
- Matches or exceeds frontier models including GPT-4 and Claude-3.5-Sonnet
- Demonstrates effectiveness across two domains: Airline and Telecom

## Why This Works (Mechanism)
The framework succeeds by addressing the dual challenges of data scarcity and reward sparsity in interactive tool use. The self-evolving synthetic data generation creates diverse, multi-turn dialogues with tool calls that capture realistic user behaviors and edge cases. The executable verification functions provide immediate, verifiable feedback for each tool interaction, converting sparse rewards into dense, actionable signals. The RL recipe components work synergistically: user model fine-tuning ensures stable simulation environments, large-batch training improves gradient estimation, group-relative advantages reduce variance, and dynamic filtering maintains training signal quality.

## Foundational Learning
- **Multi-turn dialogue generation**: Creating realistic back-and-forth conversations with tool usage; needed for training interactive agents on complex, extended interactions; quick check: verify generated dialogues follow natural conversation flow and include appropriate tool invocations
- **Executable verification functions**: Generating code that can automatically validate tool call outputs; needed to provide immediate feedback for RL training without human annotation; quick check: test verification functions on held-out examples to ensure they correctly identify success/failure
- **Hierarchical multi-agent systems**: Using multiple specialized agents for different aspects of dialogue generation; needed to create diverse and challenging training scenarios; quick check: measure diversity of generated dialogues across different agent combinations
- **Group-relative advantages in RL**: Normalizing advantages within groups of similar trajectories; needed to reduce variance in reward estimation for multi-turn interactions; quick check: compare training stability with and without group-relative normalization
- **Dynamic filtering in RL**: Removing uninformative trajectories during training; needed to maintain training signal quality and prevent overfitting to poor examples; quick check: analyze the distribution of kept versus filtered trajectories
- **User model fine-tuning**: Adapting simulation models to match real user behavior distributions; needed for stable RL training and better generalization; quick check: compare simulated user interactions before and after fine-tuning

## Architecture Onboarding

**Component map**: Hierarchical agent generation -> Synthetic dialogue creation -> Executable verification function generation -> RL training pipeline -> Fine-tuned user simulation -> Large-batch training with filtering

**Critical path**: Multi-agent dialogue generation → Executable verification function generation → RL training with user model fine-tuning → Dynamic filtering → Final evaluation

**Design tradeoffs**: The framework trades computational cost for data quality and training stability. Using multiple specialized agents increases diversity but requires more compute than single-agent generation. Generating executable verification functions adds overhead but enables verifiable rewards. Large-batch training improves gradient estimation but requires significant memory resources.

**Failure signatures**: Training instability from user simulation variance, poor generation quality leading to unrealistic dialogues, ineffective verification functions that misclassify correct/incorrect tool usage, and overfitting to synthetic data distributions that don't match real users.

**3 first experiments**:
1. Evaluate dialogue generation quality by measuring diversity and realism metrics on held-out prompts
2. Test verification function accuracy by running them on a validation set of tool call outputs
3. Compare RL training curves with and without each component of the RL recipe to identify critical elements

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Evaluation is limited to a single multi-turn tool-use benchmark (τ²-bench), restricting generalizability
- Performance improvements rely heavily on synthetic data generation, but quality and diversity remain unclear
- User simulation may introduce distribution shifts between training and real user interactions

## Confidence
- **High confidence**: Technical implementation of EigenData pipeline (multi-agent generation, executable verification functions) is well-documented and reproducible
- **Medium confidence**: Performance improvements on τ²-bench are valid but may not translate to broader tool-use contexts
- **Medium confidence**: RL training methodology is sound but necessity versus simpler alternatives needs validation

## Next Checks
1. **Cross-benchmark validation**: Evaluate trained agents on independent multi-turn tool-use datasets (e.g., Tool-Augmented Instruction Following, other customer service domains) to assess generalization
2. **Human evaluation study**: Conduct user studies with real humans to measure performance degradation compared to simulated environments and identify failure modes
3. **Ablation analysis**: Systematically remove components of the RL recipe (user model fine-tuning, large-batch training, dynamic filtering) to quantify their individual contributions to final performance gains