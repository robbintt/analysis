---
ver: rpa2
title: 'Vedavani: A Benchmark Corpus for ASR on Vedic Sanskrit Poetry'
arxiv_id: '2506.00145'
source_url: https://arxiv.org/abs/2506.00145
tags:
- sanskrit
- audio
- speech
- dataset
- veda
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Vedavani, the first comprehensive benchmark
  dataset for automatic speech recognition (ASR) on Sanskrit Vedic poetry. The dataset
  contains 54 hours of labeled audio from the Rig Veda and Atharva Veda, capturing
  complex prosodic and rhythmic patterns unique to Sanskrit poetry.
---

# Vedavani: A Benchmark Corpus for ASR on Vedic Sanskrit Poetry

## Quick Facts
- arXiv ID: 2506.00145
- Source URL: https://arxiv.org/abs/2506.00145
- Authors: Sujeet Kumar; Pretam Ray; Abhinay Beerukuri; Shrey Kamoji; Manoj Balaji Jagadeeshan; Pawan Goyal
- Reference count: 2
- Primary result: IndicWhisper achieved WER 23.14%, CER 4.12% on Vedic Sanskrit poetry

## Executive Summary
This paper introduces Vedavani, the first comprehensive benchmark dataset for automatic speech recognition on Sanskrit Vedic poetry. The dataset contains 54 hours of labeled audio from the Rig Veda and Atharva Veda, capturing complex prosodic and rhythmic patterns unique to Sanskrit poetry. The authors benchmarked state-of-the-art multilingual speech models on this dataset, finding that IndicWhisper achieved the best performance with a word error rate of 23.14% and character error rate of 4.12% on the IAST script, outperforming other models including XLSR, MMS, and various Whisper variants.

## Method Summary
The Vedavani dataset consists of 54 hours of labeled audio from the Rig Veda and Atharva Veda, with audio sourced from the Internet Archive and text from Wikisource. The dataset contains 30,779 samples with a 24,590/3,073/3,075 train/val/test split after removing audio exceeding 30 seconds. Each audio clip includes 500ms silence padding at start/end, and segments shorter than 0.25 seconds are removed. The authors fine-tuned various multilingual speech models including Whisper variants, XLSR, MMS, and IndicWhisper, evaluating performance in both IAST and Devanagari scripts with results normalized to SLP1 for comparison.

## Key Results
- IndicWhisper achieved the best performance with WER 23.14% and CER 4.12% on IAST script
- Encoder-decoder models (Whisper family) outperformed encoder-only models (Wav2Vec2/XLSR) by 6.28 WER points
- Models performed 4.03 points better in WER on Devanagari script compared to IAST
- Zero-shot performance of general Sanskrit ASR models failed dramatically (WER 99-110) on poetry

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Specialized datasets targeting specific linguistic domains (poetry) can yield substantial ASR improvements over general prose corpora for fine-tuning.
- Mechanism: The Vedavani dataset provides 54 hours of labeled audio specifically capturing Vedic Sanskrit poetry's unique prosodic patterns, rhythmic structures, and melodic recitation styles that prose-focused datasets do not adequately represent.
- Core assumption: Poetry-specific prosodic features are learnable and distinct from prose patterns, and models can transfer these learned representations.
- Evidence anchors: [abstract] "This work addresses the gap in ASR research for Sanskrit poetry, where existing models struggle with the language's phonemic complexity and word juncture transformations." [section] Table 1 shows zero-shot poetry performance fails dramatically (WER 99-110), demonstrating general Sanskrit ASR models do not transfer effectively to poetry.

### Mechanism 2
- Claim: Encoder-decoder architectures (Whisper family) appear to outperform encoder-only models (Wav2Vec2, XLSR) for Sanskrit poetry ASR in this benchmark.
- Mechanism: Whisper's large-scale multilingual pretraining (680,000 hours) with weak supervision may enable better acoustic-linguistic alignment than encoder-only models that rely primarily on acoustic self-supervision.
- Core assumption: The scale and diversity of Whisper's pretraining data provides transferable representations even for low-resource poetic forms.
- Evidence anchors: [abstract] "IndicWhisper achieved the best performance with a word error rate of 23.14% and character error rate of 4.12%." [section] Table 3 shows best encoder-only (SPG-INXS-MMS with LM) achieves 29.42 WER vs. IndicWhisper's 23.14 WER—a 6.28 point gap.

### Mechanism 3
- Claim: Script choice (Devanagari vs. IAST) measurably impacts ASR performance for Sanskrit in this experimental setup.
- Mechanism: Devanagari's consistent grapheme-phoneme correspondence may reduce decoding complexity compared to IAST transliteration, which introduces additional transformation steps.
- Core assumption: Script-specific optimization is meaningful within the same language, and character set differences affect model learning.
- Evidence anchors: [section] "On average, all variants of the Whisper model performed 4.03 points better in WER and 0.7 points better in CER when fine-tuned on the Devanagari script compared to IAST."

## Foundational Learning

- **Concept: Sandhi (Word Juncture Transformations)**
  - Why needed here: Sanskrit poetry exhibits complex phonetic transformations at word boundaries that alter surface forms. The paper identifies "phonetic transformations that occur at word junctures" as a core ASR challenge.
  - Quick check question: Can you explain why "word-splitting" and "word-conjunction" errors are particularly problematic in Sanskrit compared to English?

- **Concept: Encoder-Only vs. Encoder-Decoder ASR Architectures**
  - Why needed here: The paper benchmarks both families (Wav2Vec2/XLSR/MMS vs. Whisper variants), and understanding their inductive biases explains performance differences.
  - Quick check question: Why would an encoder-only model benefit more from an n-gram language model than an encoder-decoder model?

- **Concept: Prosody and Metrical Constraints in Vedic Poetry**
  - Why needed here: The dataset targets "precise prosodic and rhythmic features" (Section 2.3), and future work emphasizes intonation markers. Understanding that Vedic poetry uses specific pitch accents (udātta, anudātta, svarita) is essential.
  - Quick check question: How might metrical constraints (rather than syntactic rules) affecting word order impact language model assumptions in ASR?

## Architecture Onboarding

- **Component map:**
  Audio preprocessing -> WAV conversion, silence-based segmentation (pydub), manual alignment with 500ms padding
  Text preprocessing -> Wikipedia extraction, IAST/Devanagari transliteration (indic-transliteration), SLP1 normalization for evaluation
  Model families -> Encoder-only (XLSR, MMS, HuBERT, IndicWav2Vec, SPG-INXS variants) with optional KenLM n-gram LM; Encoder-decoder (Whisper small/medium/large, Distil-Whisper, IndicWhisper)
  Evaluation -> WER/CER computed in SLP1 script for fair comparison

- **Critical path:**
  1. Verify audio-text alignment quality (54 hours / 30,779 samples ≈ 6.3 sec average)
  2. Select base model: IndicWhisper (medium) as recommended starting point
  3. Preprocess to Devanagari script (4-point WER advantage over IAST per paper)
  4. Fine-tune with 30-second max audio duration constraint
  5. Evaluate on SLP1-normalized outputs

- **Design tradeoffs:**
  - IAST vs. Devanagari: Devanagari yields better WER; IAST may be required for downstream applications
  - Encoder-only + LM vs. Encoder-decoder: Encoder-only enables faster inference with LM decoding; encoder-decoder achieves better accuracy
  - Distil-Whisper vs. Whisper-large: Distil offers efficiency (756M params) with competitive IAST performance, but underperforms on Devanagari

- **Failure signatures:**
  - WER >100 on individual samples (Table 5): Typically caused by word-splitting errors where predictions contain more words than reference
  - Phoneme substitution patterns (Table 7): Confusion between श/ष/स, aspirated/unaspirated pairs (थ/त, ख/क), nasal classes (ङ/न/ञ/ण)
  - Sandhi recognition failures: Incorrect word segmentation or merging that alters syntactic structure

- **First 3 experiments:**
  1. **Reproduce IndicWhisper baseline:** Fine-tune Whisper-medium on Vedavani with Devanagari script, targeting ~21-22 WER. Validates data pipeline and training setup.
  2. **Ablate script choice:** Train identical model configurations on IAST vs. Devanagari to confirm the ~4-point WER difference holds for your infrastructure.
  3. **Characterize error distribution:** Analyze test set failures by error type (phonetic, structural, intonational) using the taxonomy in Table 7 to identify whether your model exhibits the same Sandhi and nasalization error patterns.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Does incorporating explicit intonation markers (svara) into training transcripts significantly improve ASR accuracy and character differentiation in Vedic Sanskrit?
- Basis in paper: [explicit] The authors state in the Conclusion, "In future work, we plan to enhance the dataset’s transcripts by including detailed markers for intonation... Accurately capturing these intonations is crucial for improving the model’s ability to recognize and differentiate the characters."
- Why unresolved: The current Vedavani dataset and benchmark experiments treat the text as standard orthography, omitting the specific high, low, and high-high tones characteristic of Vedic recitation (Shrutis).
- What evidence would resolve it: A comparison of baseline models against new models fine-tuned on the enhanced, intonation-annotated version of the Vedavani dataset.

**Open Question 2**
- Question: Can integrating explicit linguistic priors regarding *Sandhi* (word conjunction) rules reduce the high rate of word-splitting and segmentation errors observed in current encoder-decoder models?
- Basis in paper: [inferred] The Error Analysis notes that structural errors like "word-splitting and word-conjunction (sandhi)" are common failure modes.

## Limitations

- Data representativeness uncertainty: The dataset covers only Rig Veda and Atharva Veda, representing two of four principal Vedas, potentially limiting generalizability to other Sanskrit poetic forms.
- Architecture-specific performance attribution: The paper does not systematically isolate whether IndicWhisper's advantage stems from scale of pretraining data, encoder-decoder architecture, multilingual coverage, or specific Sanskrit-focused fine-tuning.
- Script conversion fidelity: The paper reports Devanagari performs better than IAST but does not address potential information loss during transliteration or test whether training on gold-standard Devanagari yields further improvements.

## Confidence

**High confidence (★★★):** The existence of Vedavani as a specialized benchmark dataset for Vedic Sanskrit poetry is well-supported by the detailed methodology and dataset statistics. The zero-shot failure of existing models (WER 99-110) is directly evidenced by Table 1.

**Medium confidence (★★☆):** The superiority of encoder-decoder models over encoder-only variants is demonstrated but not conclusively proven as an architectural advantage. The 6.28-point WER gap could reflect training scale differences rather than fundamental architectural properties.

**Low confidence (★☆☆):** The mechanism explaining script performance differences (Devanagari vs. IAST) lacks empirical validation. The paper observes a 4-point WER advantage but does not test whether this stems from grapheme-phoneme consistency, tokenization differences, or other factors.

## Next Checks

**Validation Check 1: Cross-School Generalization Test**
Fine-tune IndicWhisper on a subset of Rig Veda verses from one Vedic school (śākhā), then evaluate on verses from a different school. This would validate whether the model learns generalizable prosodic patterns or overfits to school-specific recitation styles.

**Validation Check 2: Architectural Ablation Study**
Train Whisper-small and Wav2Vec2-XLSR with matched parameter counts and pretraining data scales on Vedavani. This controlled comparison would isolate whether encoder-decoder architecture or training scale drives the performance advantage.

**Validation Check 3: Script-Fidelity Analysis**
Implement round-trip transliteration (IAST → Devanagari → IAST) and measure character-level consistency. Additionally, compare model performance when training on gold-standard Devanagari versus IAST-transliterated Devanagari to quantify script conversion impact.