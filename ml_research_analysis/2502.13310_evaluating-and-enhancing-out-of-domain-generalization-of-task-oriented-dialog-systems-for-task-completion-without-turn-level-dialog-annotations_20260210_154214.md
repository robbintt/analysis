---
ver: rpa2
title: Evaluating and Enhancing Out-of-Domain Generalization of Task-Oriented Dialog
  Systems for Task Completion without Turn-level Dialog Annotations
arxiv_id: '2502.13310'
source_url: https://arxiv.org/abs/2502.13310
tags:
- dialog
- task
- systems
- domains
- llama-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether large language models (LLMs) can be
  fine-tuned to perform task-oriented dialog (ToD) tasks without requiring turn-level
  annotations like dialogue states and policy labels. The authors propose ZeroToD,
  a framework that uses multi-task instruction fine-tuning and schema augmentation
  to improve out-of-domain generalization.
---

# Evaluating and Enhancing Out-of-Domain Generalization of Task-Oriented Dialog Systems for Task Completion without Turn-level Dialog Annotations

## Quick Facts
- arXiv ID: 2502.13310
- Source URL: https://arxiv.org/abs/2502.13310
- Authors: Adib Mosharrof; Moghis Fereidouni; A. B. Siddique
- Reference count: 25
- One-line result: ZeroToD fine-tuning enables task-oriented dialog models to generate API calls and natural responses without turn-level annotations, with schema augmentation improving out-of-domain generalization by up to 35.6% in complete API accuracy.

## Executive Summary
This paper addresses the challenge of building task-oriented dialog (ToD) systems without requiring expensive turn-level annotations like dialogue states and policy labels. The authors propose ZeroToD, a framework that fine-tunes large language models using only dialog history and domain schemas through multi-task instruction fine-tuning. They demonstrate that this approach can generate both natural language responses and structured API calls, with schema augmentation significantly improving out-of-domain generalization. Extensive experiments on the SGD and KETOD datasets show that ZeroToD models outperform fine-tuning-free approaches using large-scale proprietary LLMs in task completion, achieving around 53% success rate on unseen domains.

## Method Summary
ZeroToD uses multi-task instruction fine-tuning on LLMs without turn-level annotations. The model conditions on dialog history (last k turns), domain schema (intents and slots), and instruction prompts to generate either natural language responses or structured API calls. Schema augmentation creates semantic variations of intent and slot names to improve out-of-domain generalization. The framework was evaluated on SGD (16,142 dialogs, 46 API methods) and KETOD (5,324 dialogs) using three models: GPT-2 Medium (full fine-tuning), FLAN-T5 Large (full fine-tuning), and Llama-3.2 3B Instruct (LoRA + 8-bit quantization) with AdamW optimizer, learning rate 0.001, 500 warm-up steps, and early stopping on evaluation loss.

## Key Results
- ZeroToD models generate coherent and contextually appropriate responses without turn-level annotations
- Schema augmentation improves out-of-domain generalization by up to 35.6% in complete API accuracy
- FLAN-T5 achieves 53.68% complete API accuracy on unseen domains with augmentation, outperforming GPT-4o's 63.15% API invoke accuracy
- ZeroToD models outperform fine-tuning-free approaches using large-scale proprietary LLMs in task completion
- Human evaluation confirms ZeroToD's superiority in informativeness, fluency, and task completion compared to annotation-dependent approaches

## Why This Works (Mechanism)

### Mechanism 1: Multi-Task Instruction Fine-Tuning Without Annotations
LLMs can learn task-oriented dialogue tasks using only dialog history and domain schema, eliminating the need for turn-level annotations. The model conditions on structured inputs to generate either natural language responses or structured API calls, learning to autonomously decide between output types without special delimiter tokens. Core assumption: Dialog history alone contains sufficient implicit signal for the model to learn when API calls are appropriate and what parameters to extract.

### Mechanism 2: Schema Augmentation for Out-of-Domain Generalization
Training with semantically equivalent schema variants forces models to dynamically reference the provided schema rather than memorizing specific slot names, improving generalization to unseen domains. For each domain, k schema variants are created by systematically renaming intents and slots while preserving semantics. Core assumption: Models naturally overfit to training schema vocabulary; augmentation creates a "schema-reading" behavior rather than "schema-memorizing" behavior.

### Mechanism 3: Fine-Tuning Enables API Invoke Timing Learning
Fine-tuning teaches domain-specific API call timing patterns that large proprietary LLMs fail to learn through prompting alone. Fine-tuning optimizes directly on the distribution of when API calls occur in task-oriented dialogs. Core assumption: API invoke timing is a learned distributional property requiring gradient-based optimization on domain data, not recoverable from general language understanding.

## Foundational Learning

- **Concept: Task-Oriented Dialogue System Paradigms**
  - Why needed here: Understanding pipeline vs. end-to-end architectures contextualizes why annotation dependency is a scalability bottleneck
  - Quick check question: Can you articulate why dialog state tracking annotations are expensive to obtain and how they limit domain scalability?

- **Concept: Schema-Based Domain Formalization**
  - Why needed here: The framework represents each domain through schemas defining intents I and slots S with required/optional flags
  - Quick check question: Given a Hotels domain with a BookHotel intent, which slots would you mark required (e.g., check-in date, location) vs. optional (e.g., smoking preference)?

- **Concept: Autoregressive Conditional Generation**
  - Why needed here: The model generates responses by computing p(rt | P, Γ, Ht; θ)—probability of response given prompt, schema, and history
  - Quick check question: How would you explain why the same model architecture can generate both free-form text and structured API calls without explicit mode-switching tokens?

## Architecture Onboarding

- **Component map**: Dialog history + Domain schema + Instruction template -> Autoregressive LLM (GPT-2/FLAN-T5/Llama) -> Multi-task instruction fine-tuning -> Response/API call generation

- **Critical path**:
  1. Data preparation: Load dialogs, extract domain schemas, optionally generate k schema variants
  2. Template filling: Populate instruction template with domains, schemas, dialog history
  3. Fine-tuning: Optimize multi-task objective with AdamW, warm-up, early stopping on eval loss
  4. Inference: Generate response, parse API call if present, validate against schema

- **Design tradeoffs**:
  - Full fine-tuning vs. LoRA+quantization: FLAN-T5 (full) outperformed larger Llama-3.2 (LoRA+8-bit) on API accuracy
  - History truncation k: Smaller k reduces compute but may lose slot values mentioned early in dialog
  - Augmentation factor k: More variants improve OOD generalization but multiply training data size

- **Failure signatures**:
  - Low API Invoke Accuracy (~60%): Model generates conversational responses when API calls are needed
  - Correct method but wrong parameter names: Model uses memorized slot names instead of reading inference schema
  - Syntactically invalid API calls: Regex extraction fails—indicates model wasn't trained on API format

- **First 3 experiments**:
  1. Baseline establishment: Train FLAN-T5 on SGD without schema augmentation
  2. Schema augmentation ablation: Add augmentation with k=3 variants, isolate improvement in Param Name Accuracy for unseen domains
  3. Invoke timing diagnostic: Compare fine-tuned FLAN-T5 against GPT-4o on API Invoke Accuracy

## Open Questions the Paper Calls Out

- **Open Question 1**: How can model interpretability be improved to effectively diagnose and correct erroneous reasoning in zero-shot Task-Oriented Dialog systems?
  - Basis: Section 7 states that LLMs function as black boxes, hindering the ability to interpret reasoning or diagnose errors
  - Why unresolved: The authors identify increasing interpretability as an "essential step" but do not propose technical solutions
  - What evidence would resolve it: Explainable AI techniques that can visualize or trace the decision boundary between natural language responses and API call generation

- **Open Question 2**: To what extent do parameter-efficient fine-tuning (PEFT) techniques like LoRA and 8-bit quantization degrade out-of-domain generalization compared to full fine-tuning?
  - Basis: Page 5 notes that Llama-3.2 underperformed FLAN-T5 on unseen domains despite being larger; the authors hypothesize this "discrepancy" stems from Llama's use of LoRA and quantization
  - Why unresolved: The paper observes the correlation but does not isolate these variables to confirm causality or quantify the trade-off
  - What evidence would resolve it: Ablation studies on the same base model comparing full fine-tuning against LoRA/quantization specifically on unseen domain API accuracy

- **Open Question 3**: What mechanisms are required to completely mitigate biases inherited from pre-training data when adapting LLMs for task-oriented dialog?
  - Basis: Section 7 notes that models may inherit biases and that "completely eliminating biases remains a challenging task" even after fine-tuning
  - Why unresolved: While the authors acknowledge that fine-tuning mitigates bias, they explicitly state that total elimination is currently unachievable
  - What evidence would resolve it: A framework that successfully decouples protected attributes from the model's reasoning process during schema augmentation without reducing task completion accuracy

## Limitations
- Exact schema augmentation parameters (number of variants k, generation method) are unspecified
- LoRA configuration details for Llama-3.2 and precise training hyperparameters are unclear
- Limited ablation studies isolating individual components' contributions to performance

## Confidence
- **High Confidence**: ZeroToD models generate coherent responses without turn-level annotations
- **Medium Confidence**: Schema augmentation significantly improves OOD performance
- **Medium Confidence**: Fine-tuned models outperform prompting-only approaches

## Next Checks
1. **Timing signal isolation**: Train a variant of ZeroToD without dialog history (using only current turn and schema). Measure degradation in API Invoke Accuracy to quantify how much timing learning depends on multi-turn context patterns.

2. **Augmentation method comparison**: Implement three schema augmentation strategies—synonym replacement via thesaurus, LLM-based paraphrasing, and random token shuffling (preserving semantics). Compare OOD performance to identify which methods contribute most to generalization.

3. **Parameter efficiency study**: Compare FLAN-T5 full fine-tuning against LoRA (same rank/alpha as Llama-3.2) and prompt tuning on the same model. This isolates whether parameter count or fine-tuning method drives the performance gap observed between FLAN-T5 and Llama-3.2.