---
ver: rpa2
title: Multimodal Machine Learning Can Predict Videoconference Fluidity and Enjoyment
arxiv_id: '2501.03190'
source_url: https://arxiv.org/abs/2501.03190
tags:
- fluidity
- enjoyment
- features
- audio
- clips
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that multimodal machine learning models
  can effectively predict negative videoconferencing experiences, achieving up to
  0.87 ROC-AUC in identifying low conversational fluidity and enjoyment. The research
  extracted 7-second video clips from the RoomReader corpus and trained models using
  audio embeddings (VGGish), facial action units, and body motion features.
---

# Multimodal Machine Learning Can Predict Videoconference Fluidity and Enjoyment

## Quick Facts
- **arXiv ID:** 2501.03190
- **Source URL:** https://arxiv.org/abs/2501.03190
- **Reference count:** 26
- **Primary result:** Achieved up to 0.87 ROC-AUC in predicting low conversational fluidity and enjoyment using multimodal machine learning

## Executive Summary
This study demonstrates that multimodal machine learning models can effectively predict negative videoconferencing experiences by analyzing 7-second video clips from the RoomReader corpus. The research found that domain-general audio features, particularly VGGish embeddings, are the most critical predictors of both conversational fluidity and enjoyment, outperforming speech-specific alternatives. While combining audio with facial features provided the best predictions, body motion contributed minimally to the models. The findings enable large-scale analysis of videoconferencing quality without manual annotation and suggest potential for real-time interventions to improve communication.

## Method Summary
The study extracted 7-second video clips from Zoom recordings, using audio embeddings (VGGish, YAMNet, Wav2Vec2), facial action units from OpenFace, and body motion features via Mediapipe and Granger causality. Features were Z-normalized and reduced with PCA before being input to logistic regression models. Group-aware cross-validation ensured generalization across different videoconference sessions, and models predicted binary outcomes (low/high fluidity and enjoyment) as well as multi-class conversational events (backchanneling, interruption, gap).

## Key Results
- Achieved up to 0.87 ROC-AUC in identifying low conversational fluidity and enjoyment
- Domain-general audio features (VGGish) outperformed speech-specific alternatives for both prediction tasks
- Best performance came from combining audio with facial features, while body motion contributed minimally
- Low conversational fluidity more strongly predicts low enjoyment than the reverse relationship

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Domain-general audio embeddings (VGGish) outperform speech-specific models (Wav2Vec2) for predicting high-level conversational outcomes.
- **Mechanism:** VGGish captures broader acoustic patterns—including prosody, silence duration, overlap density, and non-speech sounds—that correlate with subjective experience markers like "awkwardness." Speech-specific models focus on linguistic content, which appears less predictive of fluidity/enjoyment than paralinguistic signals.
- **Core assumption:** Conversational quality is signaled more by how speech sounds and flows than by what words are spoken.
- **Evidence anchors:**
  - [abstract] "Domain-general audio features proved most critical for predicting both fluidity and enjoyment, with VGGish embeddings outperforming speech-specific alternatives."
  - [Section V.A] "Models with the highest ROC-AUCs using YAMNet (Fluidity: .694, Enjoyment: .789) and Wav2Vec2 (Fluidity: .815, Enjoyment: .841) were lower than or roughly equivalent to the models using VGGish embeddings."
  - [corpus] Weak direct corroboration; related work on turn-taking prediction (arxiv:2503.16432) uses multimodal transformers but doesn't compare audio embedding types.
- **Break condition:** If linguistic content (e.g., sentiment, topic difficulty) dominates acoustic patterns in a different conversational domain (e.g., technical debates), speech-specific features may become competitive.

### Mechanism 2
- **Claim:** Audio features capture dynamic, short-timescale conversational fluctuations, while facial actions reflect longer-term conversational states.
- **Mechanism:** Audio signals change rapidly with turn-taking events (gaps, overlaps, backchannels), providing event-proximal predictive signal. Facial expressions (e.g., boredom, engagement) evolve more slowly and indicate sustained states rather than momentary disruptions.
- **Core assumption:** The 7-second window captures a complete "event" including anticipatory and recovery phases; 3-second pre-event windows contain meaningful signal.
- **Evidence anchors:**
  - [Section V.B] "The models involving audio features dropped by more than .11 in ROC-AUC [when using only 0-3s features], but the models based on facial actions alone only dropped by .03 or less."
  - [Section V.B] "Audio features can dynamically reflect fluctuating conversational outcomes before and after the conversational event, facial actions represent the more long-term state."
  - [corpus] No direct corpus evidence on timescale differences between modalities.
- **Break condition:** If events span longer durations (30+ seconds), or if facial micro-expressions are extracted at higher temporal resolution, this asymmetry may diminish.

### Mechanism 3
- **Claim:** Low conversational fluidity is a stronger predictor of low enjoyment than the reverse relationship.
- **Mechanism:** Fluidity disruption (gaps, interruptions) is a proximal cause of negative affect. Enjoyment can be low for reasons unrelated to fluidity (e.g., topic, participant dynamics), making it a weaker upstream predictor.
- **Core assumption:** The causal direction is approximately fluidity → enjoyment, not bidirectional with equal strength.
- **Evidence anchors:**
  - [Section V.C] "The drop from Fluidity to Enjoyment (.041), almost half of that in the opposite direction (.079), implies that low fluidity in conversation is more likely to lead to low enjoyment than vice versa."
  - [Section III.B.4] "Gaps are worse than interruptions for overall enjoyment, implying that awkwardness is more damaging than rudeness."
  - [corpus] No direct corpus evidence; related work on conversational recommender systems (arxiv:2508.02328) touches on user experience but not this specific asymmetry.
- **Break condition:** In contexts where enjoyment drives engagement (e.g., entertainment-focused calls), high enjoyment could maintain fluidity through increased participant effort, reversing or equalizing the relationship.

## Foundational Learning

- **Concept:** ROC-AUC (Receiver Operating Characteristic - Area Under Curve)
  - **Why needed here:** Primary evaluation metric; 0.87 ROC-AUC claimed as strong performance. Understanding that ROC-AUC measures ranking quality across thresholds (not accuracy at a single threshold) is essential for interpreting results.
  - **Quick check question:** If a model achieves 0.87 ROC-AUC but you deploy it with a threshold that maximizes F1, will it necessarily achieve 87% accuracy? (Answer: No—ROC-AUC ≠ accuracy.)

- **Concept:** Pretrained audio embeddings (VGGish vs. Wav2Vec2)
  - **Why needed here:** Central to the paper's finding that domain-general embeddings outperform speech-specific ones. VGGish is trained on AudioSet (general sounds); Wav2Vec2 is trained on speech.
  - **Quick check question:** Why might a model trained on general audio outperform a speech-specific model for predicting "awkward silences"? (Answer: Silences and overlapping speech are acoustic events, not linguistic ones.)

- **Concept:** Group-aware cross-validation
  - **Why needed here:** The study uses "stratified group cross-validation" to ensure the model generalizes to new videoconference sessions with new participants. Standard k-fold would leak information.
  - **Quick check question:** If you train on clips from Session A and validate on other clips from Session A, what could go wrong? (Answer: The model may learn session-specific patterns—e.g., that particular group's dynamics—rather than generalizable conversational signals.)

## Architecture Onboarding

- **Component map:**
  Raw Zoom video (group view) → Frame extraction (7s clips, -3s to +4s around event marker)
                              ↓
         ┌────────────────────┼────────────────────┐
         ↓                    ↓                    ↓
    Audio stream          Video frames        Distance time-series
    (16 kHz)              (per participant)   (webcam distance)
         ↓                    ↓                    ↓
    VGGish/YAMNet/        OpenFace (17 AUs)    Mediapipe + Granger
    Wav2Vec2              averaged across      causality coupling
    (128/1024/768 dims)   participants         (scalar GC value)
         ↓                    ↓                    ↓
         └────────────────────┼────────────────────┘
                              ↓
                    Feature concatenation
                              ↓
                    Impute → Z-normalize → PCA
                              ↓
                    Logistic regression (SGD, elastic net)
                              ↓
                    Binary prediction (Low/High Fluidity, Enjoyment)
                    or Multi-class (Backchannel/Interruption/Gap)

- **Critical path:** Audio feature extraction (VGGish) → Z-normalization → PCA (50-99% variance retained) → Logistic regression. Audio alone achieves 0.805-0.859 ROC-AUC; adding facial actions improves by ~0.01-0.02. Body motion adds ~0.001.

- **Design tradeoffs:**
  - **7s vs. 3s windows:** 7s captures the full event (+ recovery); 3s enables pre-event prediction but drops audio-based ROC-AUC by >0.11. Trade-off: intervention latency vs. predictive power.
  - **Binary threshold at 2.5 vs. 3.0:** 2.5 captures more "negative" examples (rarer events); threshold at 3.0 reduces ROC-AUC by ~0.04 but increases precision on severe failures.
  - **Logistic regression vs. complex models:** Authors chose logistic regression for interpretability and robustness despite testing SVMs, random forests, lightGBM. Trade-off: potential performance ceiling vs. explainability.

- **Failure signatures:**
  - **High ROC-AUC but low F1:** The class imbalance (2,731 high-high vs. 92 low-low clips) means macro-F1 remains modest (~0.57) even with 0.87 ROC-AUC. The model ranks well but threshold selection is challenging.
  - **Body motion near-zero contribution:** Granger causality over 7 seconds is likely insufficient; the authors note it requires longer windows (minutes). If body motion is critical for your use case, extend the observation window.
  - **Cross-corpus generalization untested:** The model was trained only on RoomReader (collaborative quiz games). Deploying on business meetings, therapeutic sessions, or casual calls may degrade performance.

- **First 3 experiments:**
  1. **Reproduce audio-only baseline:** Extract VGGish embeddings from 50 randomly sampled 7-second clips, train a logistic regression with 5-fold group CV, and verify ROC-AUC in the 0.80-0.86 range for Enjoyment prediction. This validates the feature extraction pipeline before adding complexity.
  2. **Ablate facial features:** Train VGGish-only vs. VGGish+Face models on the same split. Confirm that the improvement is marginal (~0.01-0.02 ROC-AUC). If you observe larger gains, check for data leakage (e.g., facial features inadvertently encoding clip identity).
  3. **Test pre-event prediction:** Train on 0-3s features only and measure ROC-AUC degradation. If audio drops by >0.10 but facial features drop by <0.05, the timescale asymmetry is confirmed. This informs whether real-time intervention (requiring pre-event signal) is feasible or if post-hoc analysis is the realistic use case.

## Open Questions the Paper Calls Out
None

## Limitations
- **Modality temporal dynamics:** The claim that audio features dynamically reflect short-timescale conversational fluctuations while facial actions represent longer-term states is based on ROC-AUC degradation patterns rather than direct temporal analysis.
- **Cross-domain generalizability:** All models were trained and tested on RoomReader corpus data (collaborative quiz games). Performance on business meetings, therapy sessions, or casual conversations remains unknown.
- **Causal direction ambiguity:** While the paper claims low fluidity leads to low enjoyment more strongly than vice versa, the observational nature of the data cannot definitively establish causation.

## Confidence
- **High confidence:** VGGish embeddings outperform speech-specific models for predicting conversational quality
- **Medium confidence:** Audio features are more predictive of pre-event conversational outcomes than facial actions
- **Medium confidence:** Low fluidity is a stronger predictor of low enjoyment than the reverse relationship

## Next Checks
1. **Temporal resolution study:** Re-extract facial action features at 0.1-second intervals and apply time-series analysis (e.g., spectrograms or recurrent networks) to test whether facial dynamics are truly slower or just undersampled.
2. **Cross-domain validation:** Train models on RoomReader and test on a different videoconferencing corpus (e.g., business meeting recordings or therapy sessions) to quantify domain transfer performance.
3. **Causal inference test:** Apply causal discovery algorithms (e.g., LiNGAM or causal additive models) to the multimodal feature set to test whether fluidity features show stronger causal influence on enjoyment features than the reverse.