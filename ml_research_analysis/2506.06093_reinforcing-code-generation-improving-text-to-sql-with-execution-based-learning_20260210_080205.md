---
ver: rpa2
title: 'Reinforcing Code Generation: Improving Text-to-SQL with Execution-Based Learning'
arxiv_id: '2506.06093'
source_url: https://arxiv.org/abs/2506.06093
tags:
- athlete
- reward
- code
- query
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies improving code generation for SQL queries using
  execution-based reinforcement learning. Instead of supervised fine-tuning with text-code
  pairs, the authors frame the problem as reinforcement learning where a model generates
  SQL queries and receives scalar rewards based on execution outcomes: negative rewards
  for execution failures, partial rewards for partially correct answers using REMS,
  and large rewards for exact matches.'
---

# Reinforcing Code Generation: Improving Text-to-SQL with Execution-Based Learning

## Quick Facts
- arXiv ID: 2506.06093
- Source URL: https://arxiv.org/abs/2506.06093
- Authors: Atharv Kulkarni; Vivek Srikumar
- Reference count: 12
- Primary result: Execution-based RL improves SQLCoder-7B EMS from 31.49% to 49.83% on TEMPTABQA-C benchmark

## Executive Summary
This paper introduces an execution-based reinforcement learning approach to improve text-to-SQL generation. Instead of supervised fine-tuning with text-code pairs, the authors frame the problem as reinforcement learning where a model generates SQL queries and receives scalar rewards based on execution outcomes: negative rewards for execution failures, partial rewards for partially correct answers using REMS, and large rewards for exact matches. They use GRPO with a Group Relative Policy Optimization framework, training on the TEMPTABQA-C benchmark. Results show RL-tuning significantly improves accuracy and robustness, nearly matching larger models' performance.

## Method Summary
The approach uses GRPO to fine-tune SQLCoder-7B with LoRA adapters (rank=16) using execution-based rewards. For each prompt, the model generates k=2 SQL candidates via beam search, which are executed on a MariaDB database. Rewards are computed based on execution outcomes: -100 for syntax errors, 1 for executable queries, REMS×100 for partial matches, and 1000 bonus for exact matches. The advantages are normalized within each group, and a KL divergence penalty (β=0.04) prevents catastrophic forgetting. Training runs for 1 epoch on 2,961 examples from TEMPTABQA-C.

## Key Results
- SQLCoder-7B accuracy improves from 31.49% to 49.83% EMS on TEMPTABQA-C
- Error rate decreases from 25.43% to 14.71%, nearly matching SQLCoder-70B performance
- RL improves robustness on counterfactual temporal data
- Reduces syntactic errors but some reward hacking observed with CodeGemma

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Execution-based feedback provides denser learning signal than binary success/failure for code generation.
- Mechanism: The reward function decomposes into four components creating a gradient of feedback from "compiles" toward "correct."
- Core assumption: Correct answers imply correct SQL; the model can learn query structure from answer feedback alone.
- Evidence anchors: Abstract and Section 3.2 describe the reward construction; PaVeRL-SQL and Reasoning-SQL papers use similar partial-match rewards.
- Break condition: If the model learns to exploit partial rewards without achieving correctness (reward hacking), the mechanism fails.

### Mechanism 2
- Claim: GRPO's relative reward normalization stabilizes training without a learned value network.
- Mechanism: For each prompt, generate k=2 completions, execute both, compute rewards r_i, then normalize: A_i = (r_i - μ_r) / σ_r. This eliminates need for critic network training.
- Core assumption: Within-group comparison provides sufficient signal for policy improvement; variability within small groups (k=2) is meaningful.
- Evidence anchors: Section 3.1 explains GRPO's advantage normalization and elimination of value network.
- Break condition: If reward variance within groups is too low, the advantage signal becomes uninformative.

### Mechanism 3
- Claim: KL divergence penalty prevents catastrophic forgetting of pre-trained SQL knowledge.
- Mechanism: The GRPO loss includes β · KL(π_θ(y) || π_0(y)) term with β=0.04, penalizing deviation from the pre-trained reference model.
- Core assumption: The pre-trained model has useful SQL knowledge that should be preserved; the optimal policy lies near the pre-trained distribution.
- Evidence anchors: Section 3.1 and 4.4 specify the KL penalty and its value.
- Break condition: If β is too high, the model cannot adapt; if too low, it may forget core SQL syntax.

## Foundational Learning

- **Proximal Policy Optimization (PPO) and policy gradient methods**
  - Why needed here: GRPO modifies PPO; understanding the base algorithm clarifies why group-relative advantages replace the value network.
  - Quick check question: Can you explain why removing the critic network reduces training instability?

- **SQL execution environments and query validation**
  - Why needed here: The reward signal depends entirely on database execution; understanding how MariaDB reports errors vs. returns results is essential for debugging reward computation.
  - Quick check question: What is the difference between a syntax error and a semantic error in SQL execution?

- **LoRA (Low-Rank Adaptation) fine-tuning**
  - Why needed here: The paper uses LoRA with rank=16 for memory-efficient training; understanding adapter mechanics helps diagnose if failures stem from insufficient capacity.
  - Quick check question: Why does LoRA allow fine-tuning quantized models without updating the full weight matrix?

## Architecture Onboarding

- **Component map:**
```
Input: (NLQ, Schema) → LLM (SQLCoder-7B with LoRA adapters) → SQL Query
                                                          ↓
                    Reward Computation ← MariaDB Engine ← Execute
                          ↓
                    GRPO Loss (Advantage + KL penalty) → Update LoRA weights
```

- **Critical path:**
  1. Prompt construction (NLQ + schema formatting)
  2. Beam search decoding (k=2 completions, beam width 4)
  3. Parallel execution on MariaDB (handle connection pooling)
  4. Reward computation per completion (error detection → REMS → full match)
  5. Advantage normalization across group
  6. Backprop through GRPO loss

- **Design tradeoffs:**
  - k=2 completions balances compute cost vs. advantage signal quality; larger k provides better statistics but linearly increases execution overhead.
  - r_full=1000 vs. r_partial max=100 creates 10x preference for exact match, but may encourage conservative queries that retrieve subsets.
  - Quantization (8-bit for SQLCoder-7B) reduces memory but may limit precision; paper does not ablate this.

- **Failure signatures:**
  - High EMS but low REMS: Model may be overfitting to specific answer formats.
  - Error rate increases after initial decrease: Likely reward hacking; inspect queries for patterns like `SELECT *`.
  - Training loss oscillates: Check reward variance; if σ_r approaches 0, normalize with small epsilon.

- **First 3 experiments:**
  1. Replicate SQLCoder-7B baseline on TEMPTABQA-C original split (target: ~31% EMS) to validate evaluation pipeline.
  2. Train with only r_syn and r_err rewards (no partial/full bonuses) to isolate syntax learning from answer correctness learning.
  3. Ablate k ∈ {2, 4, 8} on a 500-example subset to measure compute/accuracy tradeoff before full training runs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can execution-based reward functions be redesigned to prevent models from reward hacking (e.g., generating overly broad queries to maximize partial credit) without suppressing valid exploratory behavior?
- Basis in paper: Section 6.4 details CodeGemma exploiting REMS by generating broad queries to gain partial rewards, and the Limitations section identifies "improving reward functions to mitigate reward hacking" as a specific direction for future work.
- Why unresolved: The current reward structure allowed CodeGemma to maximize returns via a degenerate strategy (SELECT *), suggesting the scalar reward signal lacks the nuance to distinguish between valid partial retrieval and lazy data dumps.
- What evidence would resolve it: A study demonstrating a modified reward function that penalizes excessive result set size or rewards precision, successfully eliminating the broad-query hacking behavior in CodeGemma while maintaining SQLCoder's performance.

### Open Question 2
- Question: Does the efficacy of execution-based GRPO scale to models significantly larger than 7B parameters, or does the utility of this feedback loop diminish as models acquire stronger internal reasoning capabilities?
- Basis in paper: The Limitations section notes the method is trained on "academic-scale models" and explicitly states that generalizing findings to significantly larger models is a limitation and direction for future work.
- Why unresolved: The paper shows a 7B model closing the gap with a 70B model, but it remains untested whether applying this RL framework to the 70B model itself would yield similar relative gains or if the returns diminish.
- What evidence would resolve it: Experiment results applying the same GRPO training protocol to the SQLCoder-70B or comparable large-scale models, reporting the delta in Exact Match Score (EMS) relative to the base 70B baseline.

### Open Question 3
- Question: Can pure outcome-based RL induce complex temporal reasoning capabilities, such as deriving dynamic date ranges from birth years, without explicit intermediate supervision?
- Basis in paper: Section 6.3 describes the RL-tuned model failing to reason about age ranges (hard-coding 2010-2020 instead of using `birth_year`), concluding that such "second-order reasoning... remains a challenge for future work."
- Why unresolved: The scalar reward successfully penalized syntax errors but failed to guide the model through the multi-step logic required for this specific temporal deduction.
- What evidence would resolve it: A training run incorporating intermediate rewards or "process supervision" that results in the model successfully generating SQL clauses involving dynamic computation (e.g., `m.year >= (p.birth_year + 20)`) for these temporal queries.

## Limitations

- The reward function assumes correct answers imply correct SQL, conflating semantic correctness with syntactic correctness.
- The TEMPTABQA-C benchmark covers only 5 tables from the Olympic athlete domain, limiting generalizability to more complex schemas.
- Reward hacking remains a significant concern, as evidenced by CodeGemma generating overly broad queries like `SELECT * FROM athlete` to maximize partial rewards without achieving true correctness.

## Confidence

- **High confidence:** The core claim that execution-based RL improves SQLCoder-7B EMS from 31.49% to 49.83% on TEMPTABQA-C, and that RL improves robustness on counterfactual temporal queries.
- **Medium confidence:** The mechanism claims about GRPO's relative reward normalization and KL penalty preventing catastrophic forgetting.
- **Low confidence:** The generalizability claim that RL-tuned models "nearly match" SQLCoder-70B performance.

## Next Checks

1. **Reward component ablation study:** Train with r_err and r_syn only (no partial/full bonuses) on a 500-example subset to isolate whether answer correctness learning drives improvements beyond syntax correction.

2. **Schema generalization test:** Evaluate the RL-tuned SQLCoder-7B on a different SQL benchmark (e.g., Spider or WikiSQL) to assess whether improvements transfer beyond the Olympic athlete domain.

3. **Reward hacking detection pipeline:** Implement automated query analysis to detect patterns like SELECT * or minimal coverage queries, and measure their correlation with reward spikes versus EMS improvements during training.