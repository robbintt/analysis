---
ver: rpa2
title: The Impact of Fine-tuning Large Language Models on Automated Program Repair
arxiv_id: '2507.19909'
source_url: https://arxiv.org/abs/2507.19909
tags:
- fine-tuning
- code
- performance
- llms
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of fine-tuning techniques on
  the performance of Large Language Models (LLMs) for Automated Program Repair (APR).
  We evaluate six state-of-the-art code-pretrained LLMs (CodeGen, CodeT5, StarCoder,
  DeepSeekCoder, Bloom, and CodeLlama-2) across three popular APR benchmarks (QuixBugs,
  Defects4J, and HumanEval-Java).
---

# The Impact of Fine-tuning Large Language Models on Automated Program Repair
## Quick Facts
- arXiv ID: 2507.19909
- Source URL: https://arxiv.org/abs/2507.19909
- Reference count: 40
- Primary result: Parameter-efficient fine-tuning methods (LoRA, IA3) outperform full-model fine-tuning for APR tasks

## Executive Summary
This study investigates how different fine-tuning approaches affect Large Language Models' performance in Automated Program Repair. The research evaluates six code-pretrained LLMs across three APR benchmarks, comparing full-model fine-tuning with parameter-efficient methods like LoRA and IA3. The findings reveal that full fine-tuning can actually decrease performance due to overfitting and data distribution mismatches, while PEFT methods consistently improve results using fewer computational resources.

The study demonstrates that LoRA, which trains only 0.09% of model parameters, achieves up to 225% performance gains compared to baseline, outperforming IA3 in 21 out of 24 cases. Larger models generally show better performance, and including buggy lines in prompts does not consistently improve repair success rates. These results suggest that PEFT represents a more effective approach for adapting LLMs to APR tasks while addressing computational efficiency concerns.

## Method Summary
The researchers evaluated six state-of-the-art code-pretrained LLMs (CodeGen, CodeT5, StarCoder, DeepSeekCoder, Bloom, and CodeLlama-2) using three popular APR benchmarks: QuixBugs, Defects4J, and HumanEval-Java. They compared three fine-tuning approaches: full-model fine-tuning, LoRA, and IA3, with LoRA using 0.09% and IA3 using 3% of trainable parameters. Each model underwent a grid search over learning rates (1e-5 to 1e-3) and epochs (1-10) to find optimal hyperparameters. The evaluation measured exact match and pass@1 scores for generated patches, with results validated through test suite execution.

## Key Results
- Full-model fine-tuning can decrease APR performance due to overfitting and data distribution mismatches
- LoRA achieves up to 225% performance gains with only 0.09% of original model parameters
- LoRA outperforms IA3 in 21 out of 24 experimental cases across models and datasets

## Why This Works (Mechanism)
None

## Foundational Learning
- Automated Program Repair: Why needed - to automatically fix bugs in software without human intervention; Quick check - ability to generate syntactically correct patches that pass test suites
- Parameter-efficient fine-tuning: Why needed - reduces computational cost while adapting models to specific tasks; Quick check - can achieve similar or better performance with fewer trainable parameters
- Code-pretrained LLMs: Why needed - models specifically trained on code datasets perform better on programming tasks; Quick check - higher accuracy on code-specific benchmarks compared to general-purpose LLMs

## Architecture Onboarding
**Component Map:** LLMs (CodeGen/CodeT5/StarCoder/DeepSeekCoder/Bloom/CodeLlama-2) -> Fine-tuning (Full/LoRA/IA3) -> APR Benchmarks (QuixBugs/Defects4J/HumanEval-Java) -> Evaluation Metrics (Exact Match/Pass@1)

**Critical Path:** Pre-trained LLM -> Fine-tuning (LoRA/IA3) -> Patch Generation -> Test Suite Validation -> Performance Measurement

**Design Tradeoffs:** Full fine-tuning offers complete parameter adaptation but risks overfitting and requires more resources; PEFT methods balance adaptation with efficiency but may miss some optimal parameter configurations

**Failure Signatures:** Decreased performance on benchmarks compared to baseline indicates overfitting; Low exact match scores suggest poor patch quality; High computational cost with minimal gains indicates inefficient fine-tuning approach

**3 First Experiments:**
1. Test baseline performance of each LLM on APR benchmarks without any fine-tuning
2. Apply LoRA fine-tuning with 0.09% trainable parameters and evaluate performance improvements
3. Compare full fine-tuning vs PEFT methods across all six models using identical hyperparameters

## Open Questions the Paper Calls Out
None

## Limitations
- Study limited to six specific code-pretrained LLMs, may not generalize to other architectures
- Benchmarks focus on Java and Python, limiting applicability to other programming languages
- Constrained hyperparameter search space may miss optimal configurations

## Confidence
- PEFT methods (LoRA, IA3) consistently outperform full fine-tuning in APR: High
- LoRA generally outperforms IA3 across models and datasets: High
- Larger models tend to perform better in APR tasks: Medium
- Including buggy lines in prompts does not consistently improve performance: Medium

## Next Checks
1. Replicate experiments with additional code-pretrained LLMs beyond the six evaluated to assess generalizability of PEFT advantages

2. Conduct ablation studies on prompt engineering to determine if performance differences are due to fine-tuning methodology or could be achieved through better prompting

3. Evaluate PEFT method robustness on out-of-distribution bugs and real-world production codebases to verify generalization beyond curated benchmark datasets