---
ver: rpa2
title: Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image Classification
arxiv_id: '2501.19086'
source_url: https://arxiv.org/abs/2501.19086
tags:
- fairness
- fine-tuning
- image
- medical
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study conducts a comprehensive fairness analysis of CLIP-based
  foundation models for X-ray image classification. The authors construct a balanced
  NIH 6x200 dataset with six disease categories and balanced demographic representation
  across age and gender groups.
---

# Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image Classification

## Quick Facts
- **arXiv ID:** 2501.19086
- **Source URL:** https://arxiv.org/abs/2501.19086
- **Reference count:** 0
- **Primary result:** MedCLIP ViT with full fine-tuning achieves highest accuracy (59.6%) but exhibits poorest fairness, while GLoRIA shows best fairness despite lower accuracy

## Executive Summary
This study conducts a comprehensive fairness analysis of CLIP-based foundation models for X-ray image classification. The authors construct a balanced NIH 6x200 dataset with six disease categories and balanced demographic representation across age and gender groups. They evaluate four CLIP-like models (CLIP, GLoRIA, MedCLIP, BioMedCLIP) using zero-shot inference and four fine-tuning strategies (Linear Probing, MLP, LoRA, Full Fine-Tuning). While MedCLIP ViT with full fine-tuning achieves the highest accuracy (59.6%), it exhibits the poorest fairness metrics, particularly showing significant performance gaps between demographic groups (F1Gender ∆ = 12.6%, F1Age ∆ = 7.1%). In contrast, GLoRIA demonstrates the best fairness with minimal disparities across demographic groups (F1Gender ∆ = 2.5%, F1Age ∆ = 0.4%) despite lower overall accuracy. The study finds that fine-tuning generally improves accuracy but can exacerbate fairness issues, with the effect depending on the specific model architecture.

## Method Summary
The authors construct a balanced NIH 6x200 dataset with 1200 images across six diseases, balanced across four demographic groups (Young Male, Old Male, Young Female, Old Female) with 50 images per group per disease. Four CLIP-based models (CLIP ViT-B/16, GLoRIA ResNet-50, MedCLIP ViT/ResNet-50, BioMedCLIP ViT) are evaluated using zero-shot inference with three prompt formats and four fine-tuning strategies (Linear Probing, MLP, LoRA, Full Fine-Tuning). Models are trained with AdamW optimizer, cosine learning rate scheduler, batch size 64, and 150 epochs, with checkpoints selected by lowest validation loss. Fairness and utility metrics are computed on the test set across demographic groups.

## Key Results
- MedCLIP ViT with full fine-tuning achieves highest accuracy (59.6%) but worst fairness (F1Gender ∆ = 12.6%, F1Age ∆ = 7.1%)
- GLoRIA demonstrates best fairness (F1Gender ∆ = 2.5%, F1Age ∆ = 0.4%) despite lower accuracy than MedCLIP
- Fine-tuning generally improves accuracy but can exacerbate fairness issues, with effects varying by model architecture
- Zero-shot performance is low (best 49.4%) but more equitable than fine-tuned models for some architectures

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning foundation models on a balanced dataset improves classification accuracy but does not guarantee, and may exacerbate, fairness disparities across demographic groups. Fine-tuning optimizes model weights for overall performance metrics (e.g., accuracy). Without explicit fairness constraints, this optimization can prioritize features that are predictive for the majority or easier-to-learn subgroups, potentially widening performance gaps for underrepresented or complex subgroups defined by sensitive attributes like age and gender.

### Mechanism 2
The choice of foundation model architecture significantly influences the fairness-performance trade-off, where higher achievable accuracy can correlate with worse fairness outcomes. Different model architectures (e.g., GLoRIA's ResNet-50 vs. MedCLIP's ViT) and their pre-training data/objectives lead to different learned representations. Some representations might be more easily separable for certain demographic groups in the fine-tuning task, leading to higher overall accuracy but larger disparities.

### Mechanism 3
Model fairness is more dependent on the model itself and its pre-training than the fine-tuning dataset's balance, as seen by divergent fairness changes upon fine-tuning on the same balanced dataset. The initial weights and representations learned during pre-training create a "fairness starting point." Fine-tuning updates these weights. For some models (e.g., GLoRIA), fine-tuning on balanced data may improve fairness. For others (e.g., MedCLIP), it may degrade it, suggesting the pre-trained features are more influential or resistant to fairness correction via balanced fine-tuning data alone.

## Foundational Learning

- **Concept: CLIP-like Foundation Models**
  - **Why needed here:** The paper compares CLIP, GLoRIA, MedCLIP, and BioMedCLIP. Understanding that these are pre-trained vision-language models is essential to grasp the "zero-shot" and "fine-tuning" paradigms being evaluated.
  - **Quick check question:** Can you explain the core principle of Contrastive Language-Image Pre-training (CLIP) and how it enables zero-shot classification?

- **Concept: Fairness Metrics (F1 Gap, Equalized Odds)**
  - **Why needed here:** The core outcome is about fairness analysis. Understanding what F1∆, EqOdds, and ECE∆ measure is critical to interpret the results.
  - **Quick check question:** Define the F1 Gap metric. What does a large F1Gender ∆ value (e.g., 12.6%) signify about the model's performance?

- **Concept: Fine-tuning Strategies (Linear Probing, LoRA, Full Fine-Tuning)**
  - **Why needed here:** The study investigates how different fine-tuning methods affect fairness and accuracy. Knowing the difference between updating only a linear head (Linear Probing) vs. updating all weights (Full Fine-Tuning) is key to understanding the experimental setup.
  - **Quick check question:** What is the fundamental difference between Linear Probing and Full Fine-Tuning in terms of which model parameters are updated?

## Architecture Onboarding

- **Component Map:**
  - **Foundation Model** -> **Fine-tuning Adapter/Head** (optional) -> **Input: Chest X-ray image** -> **Output: Class probabilities for 6 disease categories**

- **Critical Path:**
  1. **Data Loading:** Load balanced NIH 6x200 dataset (train/val/test split)
  2. **Model Selection:** Instantiate one of the 4 pre-trained CLIP-like models (e.g., MedCLIP-ViT)
  3. **Fine-tuning Setup:** Configure the chosen adaptation strategy (e.g., Full Fine-Tuning)
  4. **Training:** Train on the 7:1:2 split using AdamW optimizer
  5. **Evaluation:** Compute accuracy and fairness metrics (F1∆, EqOdds, etc.) on the test set across demographic groups

- **Design Tradeoffs:**
  - **Accuracy vs. Fairness:** MedCLIP-ViT offers the highest accuracy (59.6%) with Full Fine-Tuning but has the worst fairness (F1Gender ∆ = 12.6%). GLoRIA offers better fairness (F1Gender ∆ = 2.5%) but lower accuracy.
  - **Adaptation Method:** Full Fine-Tuning yields the best accuracy for some models (CLIP, MedCLIP-ViT) but LoRA is better for others (MedCLIP-RN, BioMedCLIP). The impact of these methods on fairness varies by model.
  - **Generalization vs. Fairness:** Zero-shot performance is low, so fine-tuning is needed for utility, but it risks worsening fairness disparities.

- **Failure Signatures:**
  - **High F1∆ / EqOdds:** Large gaps in performance between "Young Male" vs "Young Female" or "Old" vs "Young" groups
  - **Low Zero-Shot Accuracy:** Direct application of pre-trained models results in limited performance (e.g., 49.4% for best zero-shot model)
  - **Divergent Fairness Trends:** Fine-tuning improves accuracy for all models but may increase or decrease fairness depending on the specific architecture (e.g., GLoRIA improves, MedCLIP degrades)

- **First 3 Experiments:**
  1. **Baseline & Zero-Shot:** Establish the baseline utility and fairness of all four foundation models using zero-shot inference on the NIH 6x200 test set to identify initial disparities
  2. **Fine-tuning for Utility:** Implement the four fine-tuning strategies (Linear Probing, MLP, LoRA, Full Fine-Tuning) for each model and measure the resulting overall accuracy to confirm that fine-tuning improves utility
  3. **Fairness Audit:** For the best-performing fine-tuning configuration of each model, compute the F1 score for each demographic group and calculate the fairness metrics (F1∆, EqOdds) to quantify disparities

## Open Questions the Paper Calls Out

- **Open Question 1:** How can fairness-aware fine-tuning strategies be developed that improve accuracy while maintaining equitable performance across demographic groups? The authors explicitly state "Our results indicate that while fine-tuning improves model accuracy, fairness concerns persist, highlighting the need for further fairness interventions in these foundational models."

- **Open Question 2:** What architectural or training characteristics cause different CLIP-like models to exhibit vastly different fairness profiles when fine-tuned? The paper shows GLoRIA improves fairness after fine-tuning while other models worsen, concluding "the reduced fairness of the other models is unlikely due to our dataset, but from the models themselves."

- **Open Question 3:** Can incorporating additional sensitive attributes beyond age and gender (such as race, socioeconomic status, or geographic location) reveal further fairness disparities in CLIP-based medical models? The authors acknowledge they "considered gender and age as the sensitive attributes in our work," without examining other potentially relevant demographic factors.

- **Open Question 4:** How do fairness performance patterns identified in chest X-rays generalize to other medical imaging modalities? The study focuses exclusively on chest X-rays, with no validation on other imaging types despite CLIP models being used across medical imaging.

## Limitations

- The NIH 6x200 dataset, while balanced, is relatively small (1200 images total) and may not capture the full complexity of real-world clinical scenarios
- Learning rates for fine-tuning were "individually adjusted based on validation performance" without specifying exact values or ranges, making exact reproduction challenging
- The study focuses on only four CLIP-like models and four fine-tuning strategies, potentially missing other architectures or adaptation methods that might perform differently

## Confidence

- **High Confidence:** The core finding that fine-tuning improves accuracy while potentially exacerbating fairness issues is well-supported by the data showing clear disparities between models
- **Medium Confidence:** The claim that model architecture choice significantly influences the fairness-performance trade-off is supported but could benefit from additional architectures for stronger generalization
- **Low Confidence:** The assertion that pre-training characteristics dominate fairness outcomes more than fine-tuning dataset balance requires more extensive ablation studies across different pre-training datasets

## Next Checks

1. **Dataset Validation:** Verify exact demographic group composition and disease distribution in the NIH 6x200 dataset to ensure proper balance before model training
2. **Prompt Consistency:** Document and validate the exact text prompts used for zero-shot inference across all three prompt formats (CLIP, GLoRIA, CXR-CLIP) to ensure reproducibility of baseline comparisons
3. **Hyperparameter Documentation:** Test sensitivity of fairness metrics to different learning rate schedules and LoRA rank parameters to determine if observed fairness disparities persist across hyperparameter settings