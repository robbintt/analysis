---
ver: rpa2
title: 'Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning
  Models'
arxiv_id: '2601.09855'
source_url: https://arxiv.org/abs/2601.09855
tags:
- reasoning
- accuracy
- reconstruction
- cache
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Sequential test-time scaling of large reasoning models can improve
  accuracy but suffers from instability and accuracy degradation when reasoning length
  is extended too far. This work proposes Min-Seek, a method that stabilizes sequential
  scaling by keeping only the KV cache of the shortest generated thoughts, thereby
  avoiding flawed reasoning paths.
---

# Thinking Long, but Short: Stable Sequential Test-Time Scaling for Large Reasoning Models

## Quick Facts
- **arXiv ID**: 2601.09855
- **Source URL**: https://arxiv.org/abs/2601.09855
- **Reference count**: 9
- **Primary result**: Min-Seek stabilizes sequential scaling of large reasoning models by retaining only shortest reconstruction cycles, achieving 29.4-44.0% speedup over Budget Forcing while maintaining accuracy gains across 100+ reasoning steps.

## Executive Summary
Sequential test-time scaling improves reasoning accuracy but becomes unstable when reasoning length is extended too far, with models degrading due to accumulating flawed reasoning paths. Min-Seek addresses this by dynamically maintaining only the shortest past reconstruction cycles in the KV cache, filtering out likely flawed reasoning chains. The method also employs a custom KV cache that stores keys without position embeddings and re-applies them contiguously, enabling reasoning beyond model context limits with linear computational complexity. Experiments on five reasoning tasks with two distilled DeepSeek-R1 models show Min-Seek achieves significantly improved and stable accuracy over a wide range of reasoning lengths compared to Budget Forcing and standard generation.

## Method Summary
Min-Seek is a test-time scaling method that improves sequential reasoning stability by maintaining only the shortest reconstruction cycle in the KV cache. The method injects "Wait" tokens to trigger additional reconstruction cycles, generating multiple reasoning paths per prompt. After each cycle, it compares the generated thought length to the current cached shortest path, replacing it only if strictly shorter. The KV cache stores keys without position embeddings, applying contiguous position encodings before each generation step to bypass context limits. This bounded cache (prompt + first thought + one reconstruction cycle) ensures linear complexity while the selection heuristic filters flawed reasoning chains that tend to produce longer outputs.

## Key Results
- Min-Seek achieves normalized accuracy gains of 1.076-1.148 (1.5B) and >1.095 (7B) for 100+ induced thoughts, maintaining stability where Budget Forcing degrades
- The method is 29.4-44.0% faster than Budget Forcing due to reduced KV cache size while achieving comparable or better accuracy
- Min-Seek enables reasoning beyond model context limits through position-agnostic KV caching with linear computational complexity

## Why This Works (Mechanism)

### Mechanism 1: Short-Path Selection Reduces Flawed Reasoning Propagation
The model dynamically maintains the KV pairs of the single shortest reconstruction cycle, replacing the cached thought only when a new thought is strictly shorter. This biases attention toward concise reasoning paths under the assumption that flawed reasoning tends to produce longer outputs as the model struggles to recover from errors, while correct reasoning is more direct. Evidence from the abstract and Section 2 supports this correlation, though the specific filtering mechanism lacks direct corpus validation.

### Mechanism 2: Position-Agnostic KV Cache Enables Unlimited Context
Keys are stored in the cache without RoPE position embeddings and dynamically re-encoded with contiguous IDs [0, |KV|-1] before each generation step. This bypasses maximum context length constraints by ensuring past reconstruction cycles don't lose significance due to "age." The approach assumes relative ordering suffices for attention, though no direct corpus evidence validates this specific technique.

### Mechanism 3: Bounded KV Cache Yields Linear Complexity
Unlike Budget Forcing which accumulates all thoughts, Min-Seek's cache size is bounded by (|I| + 2)u where |I| = 1 and u is the per-thought token upper bound. The update rule only modifies the cache when a strictly shorter thought is generated, ensuring linear time complexity in sequence length.

## Foundational Learning

- **KV Cache (Key-Value Cache)**: Stores attention keys/values to avoid recomputation. *Why needed*: Min-Seek operates directly on KV cache structure. *Quick check*: If you clear the KV cache mid-generation, what information does the model lose access to?

- **RoPE (Rotary Position Embeddings)**: Encodes position into keys for attention. *Why needed*: The paper's custom cache modifies how RoPE is applied. *Quick check*: What happens to attention patterns if two tokens receive identical position embeddings?

- **Reconstruction Cycles / Self-Revision**: The method's unit of selection is the reconstruction cycle (each "Wait"-induced thought block), not individual tokens. *Why needed*: Understanding this unit is essential for grasping the selection mechanism. *Quick check*: How does a reconstruction cycle differ from a single reasoning step in chain-of-thought?

## Architecture Onboarding

- **Component map**: Prompt → First Thought (PT1, always cached) → [Loop: Induce cycle → Generate RC_i → Compare length → Update cache if shorter] → Final answer generation from [PT1, RC_shortest]

- **Critical path**: The method maintains prompt + first thought + shortest reconstruction cycle in cache, generating multiple reasoning paths and selecting the shortest for final answer generation.

- **Design tradeoffs**: Variant 1 (final "Wait") gives one more reasoning pass before answering; Variant 2 (direct `</think>`) forces immediate answer from shortest cached path. Storing K_no_pos + K with positions doubles key memory but nets lower than Budget Forcing due to single RC caching.

- **Failure signatures**: No accuracy gain over baseline (model may already be at peak), repetitive output (cache trapping in local minimum), missing `</think>` token (implementation must handle malformed output).

- **First 3 experiments**: 1) Reproduce Figure 1 on MATH-500 with M ∈ {0, 2, 4, 10, 50} to validate normalized accuracy curves. 2) Ablate position-agnostic cache by comparing Min-Seek with standard vs. contiguous position embeddings. 3) Test beyond-context-length generation by setting prompt + first thought near context limit and inducing 50+ reconstruction cycles.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Evaluation limited to five reasoning datasets, potentially not generalizing to domains requiring iterative refinement
- Memory overhead from storing K_no_pos + K_with_pos, though lower than Budget Forcing
- "Shorter is better" heuristic lacks direct empirical validation and could backfire in domains requiring longer correct reasoning chains

## Confidence
- **High Confidence**: Computational efficiency claim (29.4-44.0% faster than Budget Forcing) is directly supported by timing measurements in Section 4.3
- **Medium Confidence**: Accuracy stability improvement over wide reasoning lengths is well-supported for tested datasets and model scales
- **Low Confidence**: Claim that Min-Seek can "continue to reason well beyond a model's maximum context length" lacks direct evidence for extreme context exceeding

## Next Checks
1. **Temporal Dependency Test**: Design a reasoning task requiring explicit reference to order of past attempts to measure whether position-agnostic cache degrades performance on temporal reasoning compared to standard KV caching.

2. **Longer-Chain Domain Test**: Evaluate on domains where correct solutions require longer reasoning chains than incorrect ones to test whether "shorter is better" selection rule can actively harm accuracy in edge cases.

3. **Memory Scaling Analysis**: Implement a variant storing top-k shortest reconstruction cycles (k=3,5) and measure accuracy-memory tradeoff curve to determine if single-cycle cache is truly optimal.