---
ver: rpa2
title: A method for improving multilingual quality and diversity of instruction fine-tuning
  datasets
arxiv_id: '2509.15549'
source_url: https://arxiv.org/abs/2509.15549
tags:
- multilingual
- data
- quality
- languages
- m-daq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of improving multilingual instruction
  fine-tuning (IFT) for large language models (LLMs) by tackling data scarcity and
  quality issues across diverse languages. The proposed Multilingual Data Quality
  and Diversity (M-DaQ) method uses a Quality Scoring Model (QSM) to assess and select
  high-quality IFT samples in a language-agnostic manner, combined with an unsupervised
  clustering algorithm to ensure semantic diversity.
---

# A method for improving multilingual quality and diversity of instruction fine-tuning datasets

## Quick Facts
- arXiv ID: 2509.15549
- Source URL: https://arxiv.org/abs/2509.15549
- Authors: Chunguang Zhao; Yilun Liu; Pufan Zeng; Yuanchang Luo; Shimin Tao; Minggui He; Weibin Meng; Song Xu; Ziang Chen; Chen Liu; Hongxia Ma; Li Zhang; Boxing Chen; Daimeng Wei
- Reference count: 0
- Key outcome: Proposed Multilingual Data Quality and Diversity (M-DaQ) method significantly improves multilingual IFT dataset quality and diversity, achieving over 60% win rate across 18 languages.

## Executive Summary
This work introduces a method to improve multilingual instruction fine-tuning (IFT) by jointly optimizing data quality and diversity. The approach uses a Quality Scoring Model (QSM) trained on expert-revised multilingual data to assess sample quality in a language-agnostic manner, combined with a Diversity Selection Algorithm (DSA) that clusters semantically similar instructions and selects the highest-quality instance from each cluster. Evaluations on 18 languages show that models fine-tuned with M-DaQ significantly outperform vanilla baselines, with human evaluations confirming gains in culturally relevant responses.

## Method Summary
The M-DaQ method addresses multilingual IFT data scarcity and quality issues through a two-stage pipeline. First, a Quality Scoring Model (QSM) fine-tunes XLM-RoBERTa using triplet loss on expert-revised multilingual data to assess sample quality across languages. Second, the Diversity Selection Algorithm (DSA) encodes all instructions using XLM-RoBERTa, applies PCA for dimensionality reduction, and clusters via k-means. Selection then picks the highest-quality sample from each cluster (using QSM scores), ensuring semantic diversity while prioritizing quality. The method also systematically investigates the Superficial Alignment Hypothesis in multilingual settings, showing that a small set of high-quality samples suffices for effective alignment.

## Key Results
- M-DaQ significantly outperforms vanilla baselines, achieving over 60% win rate on standard benchmarks across 18 languages
- Models fine-tuned with M-DaQ show marked improvements in culturally relevant responses according to human evaluations
- Scaling experiments demonstrate the Superficial Alignment Hypothesis: reducing IFT data from 52K to 1K samples only decreased win rate by 6.2%, suggesting alignment is primarily about format/style rather than new knowledge

## Why This Works (Mechanism)

### Mechanism 1
A language-agnostic quality estimator can rank multilingual IFT samples more reliably than heuristic proxies. The Quality Scoring Model (QSM) fine-tunes XLM-RoBERTa using triplet loss: for each instruction, it learns to maximize embedding similarity to a positive (expert-revised) response while minimizing similarity to a negative (noisy or machine-generated) response. The margin hyperparameter enforces separation, enabling the model to output a scalar quality score for any (instruction, response) pair.

### Mechanism 2
Semantic clustering prior to selection improves instruction coverage and reduces redundancy. The Diversity Selection Algorithm (DSA) encodes all instructions using XLM-RoBERTa, applies PCA for dimensionality reduction, and clusters via k-means. Selection then picks the highest-quality sample from each cluster (using QSM scores), ensuring semantic diversity while prioritizing quality.

### Mechanism 3
Small-scale, high-quality, diverse multilingual IFT data can outperform larger but noisier datasets, supporting the Superficial Alignment Hypothesis (SAH) in multilingual settings. By jointly optimizing quality (QSM) and diversity (DSA), M-DaQ filters noise while preserving semantic coverage. The model needs fewer training steps (150 vs. 1500) to achieve higher win rates, suggesting alignment is about format/style rather than new knowledge.

## Foundational Learning

- **Instruction Fine-Tuning (IFT)**: The entire method operates on IFT datasets; understanding the alignment objective is prerequisite. Quick check: Can you explain why IFT differs from continued pretraining in terms of objective and data requirements?
- **Multilingual Sentence Embeddings (e.g., XLM-RoBERTa)**: Both QSM and DSA rely on shared multilingual representations for scoring and clustering. Quick check: How does a multilingual encoder map semantically similar sentences from different languages into a shared space?
- **Triplet Loss**: QSM training uses triplet loss to learn relative quality rankings. Quick check: What is the role of the margin hyperparameter in triplet loss, and what happens if it is set too small or too large?

## Architecture Onboarding

- Component map: Raw multilingual IFT dataset -> QSM (XLM-RoBERTa fine-tuned with triplet loss) -> Quality scores -> DSA (XLM-RoBERTa embeddings -> PCA -> k-means clustering) -> Cluster IDs -> Filter by complexity threshold -> Select top-quality samples per cluster -> Curated multilingual IFT subset
- Critical path: 1) Train or load QSM on expert-revised multilingual data (positive/negative pairs); 2) Score all IFT samples with QSM; 3) Encode instructions, apply PCA, run k-means to assign cluster IDs; 4) Apply complexity filter and select top-quality samples per cluster; 5) Fine-tune base LLM on the selected subset
- Design tradeoffs:
  - Cluster granularity (k): Higher k increases diversity but may retain more noise; lower k risks redundancy
  - Complexity threshold: Aggressive filtering improves average quality but may underrepresent simpler instructions
  - QSM training data size: More expert-revised data improves scorer reliability but increases annotation cost
- Failure signatures:
  - Low win rates on low-resource languages: May indicate insufficient pretraining or QSM bias toward high-resource languages
  - High redundancy in selected set: Clustering may be too coarse; consider increasing k or adjusting PCA dimensions
  - QSM overfitting to revision style: Validate QSM on held-out languages and response types
- First 3 experiments:
  1. Ablate DSA: Run selection using only QSM (no clustering) to isolate diversity contribution
  2. Vary cluster count (k): Sweep k values and measure win rate and semantic coverage across languages
  3. Cross-lingual QSM transfer: Train QSM on a subset of languages and evaluate scoring quality on held-out languages to assess generalization

## Open Questions the Paper Calls Out

- **Open Question 1**: Can M-DaQ be effectively extended to support a wider range of languages, particularly for extremely low-resource languages not covered in the initial 18-language evaluation? The current study validates the method on a fixed set of 18 languages, and it remains unproven whether the Quality Scoring Model (trained on MIDB) and the clustering approach transfer to languages with scarce training data.
- **Open Question 2**: How does the composition of the base model's pretraining data (language-specific pretraining readiness) quantitatively influence the efficiency of the Superficial Alignment Hypothesis (SAH) in multilingual settings? The paper observes that sensitivity to IFT scale varies by language but does not isolate or test this variable.
- **Open Question 3**: Does the Quality Scoring Model (QSM) generalize to select high-quality samples from data distributions significantly different from the expert-revised MIDB/Alpaca dataset used for its training? The QSM is trained on a specific dataset distribution (MIDB), and its ability to score quality on structurally different IFT datasets is not demonstrated.

## Limitations
- Evaluation scope is constrained to 18 languages, leaving unclear whether results generalize to extremely low-resource or typologically distant languages
- QSM training procedure lacks explicit hyperparameter details (margin Îµ, learning rate schedule, epochs), which could significantly affect quality scoring reliability
- Cultural relevance gains are validated only via expert annotation without broader user studies, limiting claims about real-world impact

## Confidence
- **High confidence**: Quality and diversity improvements are measurable and consistent across languages; win rates exceed baselines on standard benchmarks
- **Medium confidence**: SAH validation is methodologically sound but relies on proxy evidence (scaling experiments); cross-lingual QSM generalization is assumed but not rigorously tested
- **Low confidence**: Cultural relevance improvements are assessed only by expert panels, lacking broader population validation; failure modes for extremely low-resource languages are not characterized

## Next Checks
1. **Cross-lingual QSM transfer test**: Train QSM on 12 languages, evaluate scoring accuracy on held-out 6 languages to quantify generalization limits
2. **Extreme low-resource pilot**: Apply M-DaQ to a language with <100K native speakers; compare win rates and qualitative output against vanilla IFT baseline
3. **Cultural relevance user study**: Recruit native speakers from 3+ languages to rate model outputs on relevance, bias, and helpfulness in real-world tasks