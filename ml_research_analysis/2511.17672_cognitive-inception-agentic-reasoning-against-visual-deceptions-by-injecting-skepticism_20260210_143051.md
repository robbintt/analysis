---
ver: rpa2
title: 'Cognitive Inception: Agentic Reasoning against Visual Deceptions by Injecting
  Skepticism'
arxiv_id: '2511.17672'
source_url: https://arxiv.org/abs/2511.17672
tags:
- reasoning
- visual
- logic
- skeptic
- skeptical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of detecting AI-generated visual
  content (AIGC) using multimodal large language models (LLMs), which struggle with
  identifying generated inputs due to trust-default behavior. Inspired by cognitive
  science, the authors propose Inception, an agentic reasoning framework that improves
  authenticity verification by injecting skepticism.
---

# Cognitive Inception: Agentic Reasoning against Visual Deceptions by Injecting Skepticism

## Quick Facts
- arXiv ID: 2511.17672
- Source URL: https://arxiv.org/abs/2511.17672
- Authors: Yinjie Zhao; Heng Zhao; Bihan Wen; Joey Tianyi Zhou
- Reference count: 40
- Primary result: Achieved 74% accuracy and 73% macro-F1 on AEGIS (25% improvement over GPT-4o)

## Executive Summary
The paper addresses the challenge of detecting AI-generated visual content using multimodal large language models, which typically exhibit trust-default behavior and struggle with authenticity verification. Inspired by cognitive science principles, the authors propose Inception, an agentic reasoning framework that injects skepticism into the verification process. The framework employs two specialized agents - External Skeptic and Internal Skeptic - that work iteratively to question visual input authenticity and verify logical reasoning, converting authenticity verification into a logic verification task. Evaluated on AEGIS and Forensics-Bench benchmarks, Inception demonstrates state-of-the-art performance with significant improvements over baseline models like GPT-4o across different modalities and data distributions.

## Method Summary
Inception introduces an agentic reasoning framework that enhances multimodal LLM performance in AIGC detection through skeptical reasoning. The framework consists of two collaborating agents: External Skeptic, which questions the authenticity of visual inputs, and Internal Skeptic, which verifies the logical soundness of the reasoning process. This iterative approach transforms the authenticity verification problem into a logic verification task, addressing uncertain (epoch-e) logics through expanded reasoning. The framework operates without requiring feature-based training or fine-tuning, making it generalizable across different data distributions and modalities. By leveraging cognitive science principles of skepticism and iterative reasoning, Inception systematically improves the reliability of authenticity verification in visual content.

## Key Results
- Achieved 74% accuracy and 73% macro-F1 on AEGIS benchmark (25% improvement over GPT-4o)
- Reached 88% accuracy and 88% macro-F1 on Forensics-Bench video modality (13% improvement over GPT-4o)
- Demonstrated strong generalizability across different data distributions and modalities without feature-based training

## Why This Works (Mechanism)
The framework works by injecting skepticism into the LLM reasoning process, addressing the inherent trust-default behavior of current models. The External Skeptic agent systematically questions the authenticity of visual inputs, forcing the model to consider alternative explanations and potential manipulations. The Internal Skeptic agent then verifies the logical consistency of these skeptical assessments, ensuring that reasoning remains sound and valid. This iterative process expands the reasoning space beyond initial assumptions, effectively resolving uncertain logics that would otherwise lead to incorrect authenticity judgments. By converting authenticity verification into a logic verification task, the framework leverages the LLM's strong reasoning capabilities while compensating for its tendency to trust visual inputs uncritically.

## Foundational Learning

**Multimodal Large Language Models (LLMs)** - AI models that process and integrate multiple input modalities (text, images, video) with language understanding. Needed to handle visual authenticity verification tasks that require cross-modal reasoning. Quick check: Verify model architecture supports image/video input alongside text.

**Agentic Reasoning Framework** - A collaborative system where multiple specialized agents work together to solve complex problems through iterative interaction. Needed to break down the complex authenticity verification task into manageable sub-tasks with distinct responsibilities. Quick check: Confirm agent specialization and communication protocols are clearly defined.

**Skepticism Injection** - The process of introducing doubt and questioning assumptions into AI reasoning processes. Needed to counteract the trust-default behavior of LLMs when processing visual content. Quick check: Validate that skepticism mechanisms don't lead to excessive false positives or analysis paralysis.

## Architecture Onboarding

**Component Map**: External Skeptic -> Internal Skeptic -> LLM Reasoning Engine -> Output Decision

**Critical Path**: Visual Input → External Skeptic Questioning → Internal Skeptic Verification → Logic Resolution → Authenticity Decision

**Design Tradeoffs**: The framework trades computational efficiency for improved accuracy by introducing iterative reasoning steps. This approach avoids the need for expensive feature-based training while maintaining generalizability, but increases inference time and complexity.

**Failure Signatures**: Performance degradation occurs when skeptical reasoning becomes circular or when the Internal Skeptic fails to validate external questions effectively. The system may also struggle with highly sophisticated AIGC that can withstand skeptical questioning.

**3 First Experiments**:
1. Test framework performance on simple synthetic AIGC examples to establish baseline effectiveness
2. Evaluate the impact of removing each skeptic agent separately to measure individual contributions
3. Measure response time and computational overhead compared to baseline LLM approaches

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Evaluation relies on specific benchmarks (AEGIS and Forensics-Bench) that may not represent real-world AIGC detection scenarios
- Performance improvements could be partially attributed to prompt engineering rather than fundamental architectural advances
- The trust-default behavior assumption lacks empirical validation through controlled experiments

## Confidence

**Methodology Description**: High confidence in the accuracy and completeness of the framework description and benchmark results presentation.

**Performance Claims**: Medium confidence in the 25% and 13% improvements over GPT-4o, as these depend on specific evaluation protocols and may not generalize to all AIGC detection tasks.

**Theoretical Framework**: Low confidence in broader claims about "cognitive inception" and philosophical framing, which lack rigorous validation beyond performance metrics.

## Next Checks

1. Conduct ablation studies removing External Skeptic and Internal Skeptic components separately to quantify individual contributions to performance gains.

2. Test framework performance on out-of-distribution AIGC samples and additional modalities (audio, text) not covered in current evaluation.

3. Measure and report computational overhead and response time for iterative reasoning process compared to baseline LLM approaches.