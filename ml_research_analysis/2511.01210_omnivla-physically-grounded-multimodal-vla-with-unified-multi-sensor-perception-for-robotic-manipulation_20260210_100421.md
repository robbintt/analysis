---
ver: rpa2
title: 'OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception
  for Robotic Manipulation'
arxiv_id: '2511.01210'
source_url: https://arxiv.org/abs/2511.01210
tags:
- sensor
- images
- arxiv
- omnivla
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "OmniVLA addresses the limitation of RGB-only vision-language-action\
  \ (VLA) models by integrating novel sensing modalities\u2014infrared, mmWave radar,\
  \ and acoustic microphone arrays\u2014for physically-grounded robotic manipulation.\
  \ Its core innovation is the sensor-masked image, a unified representation that\
  \ overlays semantically segmented sensor data onto RGB images, enabling spatially\
  \ grounded and semantically aligned perception."
---

# OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation
## Quick Facts
- arXiv ID: 2511.01210
- Source URL: https://arxiv.org/abs/2511.01210
- Reference count: 40
- Outperforms RGB-only baseline by 59% absolute on physically-grounded manipulation tasks

## Executive Summary
OmniVLA addresses the limitation of RGB-only vision-language-action (VLA) models by integrating novel sensing modalities—infrared, mmWave radar, and acoustic microphone arrays—for physically-grounded robotic manipulation. Its core innovation is the sensor-masked image, a unified representation that overlays semantically segmented sensor data onto RGB images, enabling spatially grounded and semantically aligned perception. This approach allows reusing pre-trained vision encoders, provides hardware-agnostic sensor fusion, and improves data efficiency through lightweight per-sensor projectors. Evaluated on real-world manipulation tasks requiring non-RGB perception, OmniVLA achieves an 84% average task success rate, outperforming RGB-only (25%) and raw-sensor-input (56%) baselines by 59% and 28%, respectively. It also demonstrates strong generalization to unseen tasks and requires fewer training samples than baselines, highlighting its data efficiency and robust spatial intelligence capabilities.

## Method Summary
OmniVLA integrates multimodal sensing into robotic manipulation by introducing the sensor-masked image—a unified representation that overlays semantically segmented infrared, mmWave, and acoustic sensor data onto RGB images. This approach enables spatially grounded perception without requiring full end-to-end retraining of vision encoders. Each sensor modality is processed through lightweight per-sensor projectors that map raw sensor inputs into unified image-like representations before concatenation with RGB data. The system leverages pre-trained vision-language models while incorporating physically-grounded capabilities through targeted fine-tuning on manipulation tasks that require non-visual sensing. The architecture maintains computational efficiency by avoiding complex fusion architectures while achieving robust performance across diverse manipulation scenarios.

## Key Results
- Achieves 84% average task success rate on physically-grounded manipulation tasks
- Outperforms RGB-only baseline by 59% absolute (84% vs 25%)
- Outperforms raw-sensor-input baseline by 28% absolute (84% vs 56%)
- Demonstrates strong generalization to unseen tasks with fewer training samples required

## Why This Works (Mechanism)
The sensor-masked image approach works by creating a unified multimodal representation that preserves spatial relationships while incorporating non-visual sensing modalities. By overlaying semantically segmented sensor data onto RGB images, OmniVLA enables existing vision encoders to process multimodal information without architectural modifications. The lightweight per-sensor projectors efficiently transform raw sensor data into compatible formats, while the unified representation ensures hardware-agnostic sensor fusion. This design allows the system to leverage pre-trained vision-language models while incorporating physically-grounded capabilities, resulting in improved data efficiency and robust performance on tasks requiring non-RGB perception.

## Foundational Learning
- Multimodal sensor fusion: Why needed - Combines complementary sensing modalities for comprehensive environmental understanding; Quick check - Verify sensor data alignment and temporal synchronization
- Vision-language-action models: Why needed - Bridges visual perception with language understanding and robotic action planning; Quick check - Validate VLA model's ability to generate appropriate actions from multimodal inputs
- Sensor-masked image representation: Why needed - Provides unified format for heterogeneous sensor data while preserving spatial relationships; Quick check - Confirm overlay accuracy and semantic segmentation quality
- Per-sensor projectors: Why needed - Efficiently transform raw sensor data into compatible formats for existing vision encoders; Quick check - Measure projection accuracy and computational overhead
- Physically-grounded perception: Why needed - Enables robots to understand and interact with physical properties beyond visual appearance; Quick check - Test performance on tasks requiring material identification or thermal sensing

## Architecture Onboarding
Component map: Sensor data -> Per-sensor projectors -> Sensor-masked image generator -> Vision encoder -> Language model -> Action planner -> Robot controller

Critical path: The most performance-critical path is Sensor data → Per-sensor projectors → Sensor-masked image generator → Vision encoder, as latency in this pipeline directly impacts real-time manipulation capabilities.

Design tradeoffs: The sensor-masked image approach trades raw sensor fusion complexity for compatibility with pre-trained vision encoders, reducing computational overhead but requiring careful sensor placement and calibration. Lightweight projectors minimize additional parameters but may limit fine-grained sensor-specific processing capabilities.

Failure signatures: Common failure modes include sensor misalignment causing spatial registration errors, projector miscalibration leading to inaccurate overlays, and language model confusion when multimodal inputs conflict. Performance degradation typically manifests as reduced task success rates on physically-grounded tasks.

Three first experiments: 1) Test sensor-masked image generation with synthetic sensor data to validate overlay accuracy; 2) Measure latency of the per-sensor projection pipeline under various sensor configurations; 3) Evaluate baseline VLA model performance on tasks requiring non-RGB perception to establish performance gaps.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation conducted primarily in controlled laboratory environments, limiting real-world robustness assessment
- Lack of systematic analysis for sensor failure scenarios and redundancy mechanisms
- Insufficient characterization of runtime efficiency and memory requirements under extended operation

## Confidence
- Performance improvements (84% success rate): High
- Data efficiency advantages: High
- Real-world robustness and generalizability: Medium
- Practical deployment feasibility: Low

## Next Checks
1. Evaluate OmniVLA's performance across diverse environmental conditions (lighting variations, acoustic noise levels, sensor occlusions) to assess real-world robustness
2. Conduct ablation studies on the sensor-masked image representation to quantify the contribution of each sensor modality and the importance of spatial alignment
3. Measure computational overhead, latency, and memory requirements during extended operation to validate practical deployment feasibility