---
ver: rpa2
title: An Empirical Risk Minimization Approach for Offline Inverse RL and Dynamic
  Discrete Choice Model
arxiv_id: '2502.14131'
source_url: https://arxiv.org/abs/2502.14131
tags:
- function
- learning
- equation
- reward
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces an empirical risk minimization framework\
  \ for offline inverse reinforcement learning and dynamic discrete choice models\
  \ that circumvents the need for explicit transition function estimation. The key\
  \ innovation is jointly minimizing negative log-likelihood and mean squared Bellman\
  \ error terms, which together satisfy the Polyak-\u0141ojasiewicz condition enabling\
  \ global convergence guarantees."
---

# An Empirical Risk Minimization Approach for Offline Inverse RL and Dynamic Discrete Choice Model

## Quick Facts
- **arXiv ID:** 2502.14131
- **Source URL:** https://arxiv.org/abs/2502.14131
- **Reference count:** 40
- **One-line result:** GLADIUS algorithm jointly minimizes negative log-likelihood and mean squared Bellman error, satisfying Polyak-Łojasiewicz condition for global convergence with O(1/T) optimization error and O(1/√N) statistical error in offline inverse RL

## Executive Summary
This paper introduces an empirical risk minimization framework for offline inverse reinforcement learning and dynamic discrete choice models that circumvents the need for explicit transition function estimation. The key innovation is jointly minimizing negative log-likelihood and mean squared Bellman error terms, which together satisfy the Polyak-Łojasiewicz condition enabling global convergence guarantees. The proposed GLADIUS algorithm uses alternating gradient ascent-descent to solve this minimax problem and achieves strong empirical performance on synthetic bus engine replacement problems.

## Method Summary
The method estimates the optimal Q-function and reward function from expert demonstrations without explicitly estimating the transition function. GLADIUS jointly minimizes negative log-likelihood (enforcing policy consistency) and mean squared Bellman error (enforcing Bellman consistency) through a minimax formulation. The bi-conjugate reformulation eliminates double-sampling bias by introducing a dual variable ζ that implicitly estimates the transition-dependent expectations. Anchor actions with known rewards ensure unique identification of the reward function. The algorithm alternates between updating ζ (ascent) to estimate E[V_Q(s')|s,a] and updating Q (descent) to minimize the combined loss.

## Key Results
- GLADIUS achieves mean absolute percentage errors of 0.12-0.84% for low-dimensional Rust bus engine replacement problem
- The algorithm maintains sub-linear error growth in high-dimensional settings with dummy state variables
- GLADIUS outperforms or matches benchmark methods including oracle approaches while avoiding explicit transition probability estimation
- Theoretical guarantees include O(1/T) optimization error and O(1/√N) statistical error under realizability assumptions

## Why This Works (Mechanism)

### Mechanism 1: Joint Minimization of NLL and Bellman Error Satisfies PL Condition
- Claim: The expected risk minimization objective (negative log-likelihood + mean squared Bellman error) satisfies the Polyak-Łojasiewicz (PL) condition, enabling global convergence without strong convexity.
- Mechanism: Individually, both L_NLL and L_BE satisfy PL under Assumption 5 (bounded gradients/Jacobians for function class Q_θ). Critically, Lemma 25 shows their sum remains PL because minimizers intersect at Q*. This permits gradient-based optimization to converge globally at rate O(1/T).
- Core assumption: Realizability (Q_θ contains Q*) and smooth parametrization with bounded Jacobian singular values ≥ √μ.
- Evidence anchors:
  - [abstract] "Bellman residual satisfies the Polyak-Łojasiewicz (PL) condition — a property that, while weaker than strong convexity, is sufficient to ensure fast global convergence guarantees."
  - [section 6.1] Lemma 8 and 9 establish PL for individual components; Theorem 10 confirms the joint objective is PL.
  - [corpus] "Stability and Generalization for Bellman Residuals" (arxiv:2508.18741) addresses related BRM convergence concerns but does not establish PL.
- Break condition: If minimizers of NLL and Bellman error do not intersect (e.g., misspecified Q class), Lemma 21 fails and the PL guarantee collapses.

### Mechanism 2: Bi-Conjugate Reformulation Eliminates Transition Function Estimation
- Claim: Introducing dual variable ζ via the bi-conjugate trick removes the double-sampling bias inherent in naive TD-based Bellman error estimation.
- Mechanism: E[(TQ - Q)²] ≠ E[(ŤTQ - Q)²] due to irreducible variance from stochastic transitions. Reformulating as a minimax problem over ζ (Equation 12) yields an unbiased estimator: ζ* converges to E[V_Q(s')|s,a], recovering the transition-dependent expectation implicitly from data without explicit P estimation.
- Core assumption: Sufficient coverage of state-action pairs in offline data D; ζ class is rich enough to represent conditional expectations.
- Evidence anchors:
  - [section 4.2] "This bias in the TD approach is a well-known problem called the double sampling problem... To remove this problematic square term, we employ the 'Bi-Conjugate Trick'."
  - [corpus] Corpus evidence for bi-conjugate methods is weak—no direct neighbors address this specific reformulation for IRL/DDC.
- Break condition: If data lacks coverage or ζ is underparametrized, the inner maximization converges to biased ζ, corrupting the outer Q update.

### Mechanism 3: Anchor Action Identification Ensures Reward Uniqueness
- Claim: Known rewards for anchor actions a_s per state uniquely identify Q* and r from policy observations.
- Mechanism: Policy alone only identifies Q up to state-wise constants (softmax is invariant to adding c(s)). Anchor action Bellman equations (Equation 5) pin these constants by linking Q(s, a_s) to known r(s, a_s) + β·E[V_Q(s')|s,a_s], yielding unique r recovery via r = Q - β·E[V_Q].
- Core assumption: Assumption 3 holds—anchor reward is known for each reachable state.
- Evidence anchors:
  - [section 3.4] Lemma 1 (Magnac and Thesmar 2002) proves identification under anchor actions.
  - [corpus] "Distributional Inverse Reinforcement Learning" addresses reward uncertainty but uses different identification strategies.
- Break condition: Without anchor actions, only policy-equivalent reward classes are recoverable; counterfactual analysis becomes invalid.

## Foundational Learning

- Concept: Bellman Operator and Bellman Residual
  - Why needed here: The entire framework minimizes squared Bellman error; you must understand TQ - Q = 0 as the consistency condition defining optimal Q*.
  - Quick check question: Given Q(s,a) and transition kernel P, can you write out TQ(s,a)? Can you explain why Q* is the unique fixed point?

- Concept: Maximum Entropy IRL ↔ Dynamic Discrete Choice Equivalence
  - Why needed here: The paper unifies these literatures; understanding that entropy-regularized RL with λ=1 is formally equivalent to DDC with T1EV noise (δ=-γ) is essential for interpreting results.
  - Quick check question: In both frameworks, what is π*(a|s) in terms of Q*? Why does this matter for the negative log-likelihood term?

- Concept: Polyak-Łojasiewicz Condition
  - Why needed here: This weaker-than-convexity condition is the theoretical engine enabling global convergence. You must distinguish PL from strong convexity: PL requires ½‖∇f‖² ≥ μ(f - f*), not positive definite Hessians everywhere.
  - Quick check question: If a function is PL with constant μ, what convergence rate does gradient descent achieve? Why is this surprising for non-convex objectives?

## Architecture Onboarding

- Component map:
  - Q_θ₂: Neural network (or linear) mapping (s,a) → Q-values; primary learnable parameters
  - ζ_θ₁: Dual function mapping (s,a) → scalar estimates of E[V_Q(s')|s,a]; auxiliary maximizer
  - V_Q(s) = log Σ_a exp(Q(s,a)): Value function derived from Q via log-sum-exp
  - Loss = L_NLL + L_TD - β²·L_dual: Three-term empirical risk (Equation 15)

- Critical path:
  1. Ascent step: Update ζ_θ₁ to minimize (V_Q(s') - ζ(s,a))² → ζ approaches E[V_Q|s,a]
  2. Descent step: Update Q_θ₂ to minimize -log p_Q(a|s) + squared TD error (corrected by ζ)
  3. Extract r: Compute r̂(s,a) = Q̂(s,a) - β·ζ̂(s,a)

- Design tradeoffs:
  - Neural network Q vs. linear: Non-parametric enables scalability but forfeits extrapolation to unseen state-action pairs (see Table A1-A4: GLADIUS degrades at high mileages)
  - Batch size B₁, B₂ vs. full dataset D: Batches reduce memory from O(|D|) to O(|B|) but introduce gradient noise
  - Deterministic transitions special case: If P is deterministic, ζ ascent is unnecessary—Algorithm 2 simplifies to single descent

- Failure signatures:
  - MAPE ≈ 100%+ with IQ-Learn/BC: Bellman equation not enforced; Q does not satisfy consistency
  - Divergence on high-dimensional states with SAmQ: Memory overflow from transition probability estimation
  - GLADIUS extrapolation error on rare states: Neural networks overfit to high-frequency (s,a) pairs; oracle linear methods extrapolate better (Table A2)

- First 3 experiments:
  1. Reproduce Rust bus engine (low-dim): Train on 500-1000 trajectories, verify MAPE < 1% and compare Q̂ across mileages 1-10 against oracle baselines.
  2. High-dimensional stress test: Append K=20-50 dummy state variables; confirm sub-linear MAPE growth (Figure 1 pattern) and that SAmQ/IQ-Learn fail or degrade.
  3. Ablate anchor actions: Remove anchor reward knowledge and observe non-unique reward recovery (policy equivalence only); verify counterfactual simulation fails.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How robust is the GLADIUS algorithm to violations of the expert optimality assumption, specifically regarding suboptimal or noisy demonstrations?
- **Basis in paper:** [Explicit] Assumption 1 posits data generation follows the optimal policy π*. [Inferred] Real-world "expert" data rarely satisfies perfect optimality, yet the convergence guarantees and empirical risk minimization framework depend on this stationarity.
- **Why unresolved:** The theoretical convergence relies on the data distribution matching π*. It is unclear how deviations (e.g., exploration noise, rational inattention) affect the PL condition or estimation bias.
- **What evidence would resolve it:** Empirical simulations evaluating GLADIUS on datasets generated from policies with varying degrees of sub-optimality (e.g., ε-greedy deviations from π*).

### Open Question 2
- **Question:** Can the theoretical insights regarding the PL condition of the Bellman residual be formally extended to guarantee global convergence for general gradient-based Offline Reinforcement Learning algorithms?
- **Basis in paper:** [Explicit] Page 24, Remark states that Lemma 8 suggests the PL condition may be used to establish global convergence for gradient-based Offline RL, where it is currently lacking.
- **Why unresolved:** While the property holds for the IRL/DDC inverse problem, the landscape of the forward RL objective (minimizing Bellman error) under function approximation is notoriously complex and prone to instability (the "deadly triad").
- **What evidence would resolve it:** A formal proof extending the PL-based convergence analysis to a standard offline RL objective or empirical validation of global convergence in a standard RL benchmark using this theoretical lens.

### Open Question 3
- **Question:** Can the requirement for known "anchor action" rewards (Assumption 3) be relaxed or replaced with weaker structural assumptions for reward identification?
- **Basis in paper:** [Explicit] Assumption 3 requires known rewards for specific state-action pairs. [Inferred] The paper notes this is necessary for identification, but in high-dimensional or unstructured domains, determining these anchor values a priori may be impractical.
- **Why unresolved:** The current identification theorem (Lemma 1) depends mathematically on these fixed points to resolve the constant offset inherent in the Bellman equation.
- **What evidence would resolve it:** Identification results showing that specific parametric constraints (e.g., sparsity, monotonicity) or alternative normalization strategies (e.g., average reward constraints) can substitute for exact anchor action knowledge.

## Limitations
- Theoretical guarantees rely on realizability assumptions and PL condition satisfaction, which may not hold for complex state-action spaces
- Memory efficiency gains versus SAmQ are theoretical; practical memory bottlenecks may emerge with high-dimensional states
- Empirical evaluation is limited to a single synthetic environment (Rust bus engine), limiting generalizability claims

## Confidence

**High Confidence:** PL condition satisfaction for the joint objective, GLADIUS optimization error rates (O(1/T)), and the core mechanism of bi-conjugate reformulation

**Medium Confidence:** Statistical error bounds (O(1/√N)), identification claims with anchor actions, and empirical performance claims relative to baselines

**Low Confidence:** Extrapolation claims to high-dimensional states beyond tested dummy variables, real-world applicability beyond synthetic environments

## Next Validation Checks
1. **Cross-environment validation:** Test GLADIUS on at least two additional synthetic or real-world environments (e.g., GridWorld variants, Atari game states) to assess generalizability
2. **Scalability benchmark:** Measure actual GPU/CPU memory usage and runtime versus SAmQ across varying state-action dimensions (10-100 dimensions) with different neural network sizes
3. **PL condition stress test:** Systematically vary the reward function complexity and Q-class parametrization to determine when PL condition breaks down, measuring convergence rates empirically