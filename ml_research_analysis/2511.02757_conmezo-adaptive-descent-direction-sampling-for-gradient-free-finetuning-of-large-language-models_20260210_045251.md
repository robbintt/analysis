---
ver: rpa2
title: 'ConMeZO: Adaptive Descent-Direction Sampling for Gradient-Free Finetuning
  of Large Language Models'
arxiv_id: '2511.02757'
source_url: https://arxiv.org/abs/2511.02757
tags:
- conmezo
- mezo
- optimization
- gradient
- momentum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ConMeZO introduces a cone-based sampling strategy to accelerate
  zeroth-order (ZO) optimization for large language model (LLM) finetuning. It improves
  upon MeZO by sampling perturbation directions within a cone centered on a momentum
  estimate, thereby reducing gradient variance and enhancing convergence speed.
---

# ConMeZO: Adaptive Descent-Direction Sampling for Gradient-Free Finetuning of Large Language Models

## Quick Facts
- arXiv ID: 2511.02757
- Source URL: https://arxiv.org/abs/2511.02757
- Reference count: 40
- ConMeZO achieves up to 2× speedup over MeZO with cone-based directional sampling for zeroth-order LLM finetuning

## Executive Summary
ConMeZO introduces a cone-based sampling strategy that accelerates zeroth-order optimization for large language model finetuning. Building on MeZO, it samples perturbation directions within a cone centered on a momentum estimate, reducing gradient variance and enhancing convergence speed while maintaining MeZO's low memory footprint. The method demonstrates up to 2× speedup in wall-clock time during LLM finetuning tasks and consistently outperforms MeZO and other baselines across multiple benchmarks. Theoretical analysis confirms ConMeZO matches MeZO's worst-case convergence rate but can be significantly faster when momentum aligns well with the true gradient.

## Method Summary
ConMeZO replaces MeZO's random sampling with cone-based directional sampling around momentum. The core update samples perturbation direction u_t from N(0,I_d), computes z_t = √d(cos(θ)·m̂_t + sin(θ)·u_t), estimates gradient via 2-point ZOGE, and updates momentum with a warmup schedule. Key hyperparameters include cone angle θ (1.35 for RoBERTa, 1.4 for OPT), momentum β (0.99), learning rate (10⁻⁶ for RoBERTa, 10⁻⁷ for OPT), and regularization λ (10⁻³). The method maintains MeZO's low memory footprint while achieving improved convergence through reduced gradient variance in sampled directions.

## Key Results
- Up to 2× wall-clock speedup over MeZO on RoBERTa-large finetuning tasks
- Consistently higher accuracy/F1 scores on OPT-1.3B and OPT-13B across multiple benchmarks
- Theoretical convergence rate matching MeZO's O(1/T) worst-case performance
- Memory overhead remains significantly lower than AdamW despite 54% increase over MeZO

## Why This Works (Mechanism)
ConMeZO improves upon MeZO by reducing gradient estimation variance through cone-based sampling. By sampling directions within a cone centered on the momentum estimate, perturbations are more likely to align with the true gradient direction. This reduces the noise in gradient estimates compared to MeZO's uniform random sampling, leading to faster convergence. The momentum-based cone center provides a running estimate of the descent direction, while the cone angle θ controls the exploration-exploitation tradeoff. This approach maintains zeroth-order optimization's gradient-free advantage while achieving performance closer to first-order methods.

## Foundational Learning
- Zeroth-order optimization (ZOO): Gradient-free optimization using function evaluations only; needed for scenarios where gradients are expensive/costly to compute; quick check: verify MeZO baseline works before implementing ConMeZO
- Directional sampling variance reduction: Reducing estimation noise by biasing sampling toward promising directions; needed to understand convergence speedup mechanism; quick check: compare gradient variance with/without cone sampling
- Momentum-based gradient estimation: Using exponential moving average to track descent direction; needed for cone center computation; quick check: verify momentum tracking accuracy against ground truth gradients

## Architecture Onboarding
**Component Map:** Random seed → Data loader → Model → ConMeZO optimizer → Loss → Gradient estimator → Momentum update → Parameter update

**Critical Path:** Data → Model forward → Loss computation → ZO gradient estimation (2-point) → Momentum update → Parameter update → Next iteration

**Design Tradeoffs:** Cone angle θ controls exploration vs exploitation tradeoff; larger θ increases exploration but reduces variance reduction benefit. Momentum warmup schedule balances early exploration with later exploitation. Memory overhead (~54% increase over MeZO) trades off against convergence speedup.

**Failure Signatures:** Slow/no convergence indicates θ too small (insufficient exploration) or β too high (momentum lag). High variance across seeds is expected (±0.5-2% std reported). OOM errors suggest model too large for available memory; start with RoBERTa before scaling to OPT.

**First Experiments:** 1) Verify MeZO baseline on SST-2 (batch 64, 10K steps): ~92.8% accuracy; 2) Implement ConMeZO with θ=1.35, run same setup: target ~93.5% with 3-4% speedup; 3) Scale to OPT-1.3B on SQuAD (20K steps): verify MeZO ~72.76 F1, ConMeZO ~75.34 F1 with 2× early speedup.

## Open Questions the Paper Calls Out
**Open Question 1:** Can lightweight, self-adaptive mechanisms be developed to dynamically adjust the cone angle θ and momentum β during optimization? The current work relies on fixed hyperparameters or a manually designed warm-up schedule for β.

**Open Question 2:** Can theoretical guarantees be established showing ConMeZO has a strictly better convergence rate than MeZO under realistic assumptions? Current analysis proves matching worst-case rate and heuristic speedup only when momentum aligns well with true gradient.

**Open Question 3:** How does ConMeZO perform when combined with low-rank gradient estimation methods like LOZO? ConMeZO and LOZO target different aspects (directional sampling vs low-rank updates) and their potential synergy is unexplored.

## Limitations
- High dependence on hyperparameter tuning, particularly cone angle θ requiring task-specific optimization
- Memory overhead remains significant (54% increase over MeZO for RoBERTa-large)
- Limited validation beyond transformer-based LLMs and GLUE-style benchmarks
- Theoretical analysis relies on momentum alignment assumptions that may not hold in practice

## Confidence
- **High confidence**: Convergence speedup claims (2× wall-clock time), F1/accuracy improvements over MeZO baselines, memory footprint comparison with AdamW, theoretical convergence rate analysis
- **Medium confidence**: Scalability claims to billion-parameter models (based on limited OPT experiments), effectiveness across diverse task types (primarily GLUE-focused validation)
- **Low confidence**: Performance on architectures beyond transformer-based LLMs, behavior with alternative prompting strategies, generalization to non-English benchmarks

## Next Checks
1. Implement systematic hyperparameter sensitivity analysis for θ across [1.0, 1.8] range on a single task to identify optimal settings and failure modes
2. Benchmark ConMeZO on at least one non-GLUE task (e.g., CoNLL-2003 or a dense prediction task) to assess broader applicability
3. Measure memory usage breakdown (per-layer) on OPT-13B to verify the claimed 28% overhead relative to MeZO baseline