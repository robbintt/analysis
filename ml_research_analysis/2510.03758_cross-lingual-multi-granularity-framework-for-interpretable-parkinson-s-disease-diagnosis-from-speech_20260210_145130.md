---
ver: rpa2
title: Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease
  Diagnosis from Speech
arxiv_id: '2510.03758'
source_url: https://arxiv.org/abs/2510.03758
tags:
- speech
- parkinson
- disease
- performance
- syllable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of early Parkinson's disease
  (PD) diagnosis through speech analysis by developing a multilingual framework that
  analyzes speech at multiple granularity levels. The proposed method uses an automated
  pipeline to extract time-aligned phonemes, syllables, and words from speech recordings,
  enabling systematic comparison of diagnostic performance across these granularities.
---

# Cross-Lingual Multi-Granularity Framework for Interpretable Parkinson's Disease Diagnosis from Speech

## Quick Facts
- arXiv ID: 2510.03758
- Source URL: https://arxiv.org/abs/2510.03758
- Authors: Ilias Tougui; Mehdi Zakroum; Mounir Ghogho
- Reference count: 0
- One-line primary result: Phoneme-level analysis achieved superior performance with AUROC 93.78% ± 2.34% and accuracy 92.17% ± 2.43%

## Executive Summary
This study addresses early Parkinson's disease diagnosis through speech analysis using a multilingual framework that examines speech at multiple granularity levels. The proposed method automatically extracts time-aligned phonemes, syllables, and words from speech recordings across Italian, Spanish, and English datasets. A bidirectional LSTM with multi-head attention detects PD with highest accuracy at the phoneme level. The attention mechanism provides interpretability by identifying clinically relevant features that align with established diagnostic protocols.

## Method Summary
The framework uses an automated pipeline to extract phonemes, syllables, and words from speech recordings, enabling systematic comparison across granularities. A bidirectional LSTM with multi-head attention was implemented to detect PD from three language datasets. Phoneme-level analysis demonstrated superior performance with AUROC of 93.78% ± 2.34% and accuracy of 92.17% ± 2.43%. Attention analysis revealed that informative features align with established clinical protocols including sustained vowels, diadochokinetic syllables, and specific word sequences.

## Key Results
- Phoneme-level analysis achieved AUROC 93.78% ± 2.34% and accuracy 92.17% ± 2.43%
- Attention analysis identified sustained vowels (/a/, /e/, /o/, /i/) as most informative at phoneme level
- Diadochokinetic syllables (/ta/, /pa/, /la/, /ka/) and /pataka/ sequences were identified at syllable and word levels respectively

## Why This Works (Mechanism)
The framework works by leveraging multi-head attention to identify clinically relevant speech features that correspond to established diagnostic protocols. The attention mechanism provides interpretability by highlighting specific phonemes, syllables, and words that contribute most to PD detection. The cross-linguistic approach ensures the model generalizes across different languages while maintaining diagnostic accuracy.

## Foundational Learning
- **Multi-head attention**: Allows the model to focus on different aspects of speech features simultaneously, improving diagnostic accuracy by capturing multiple informative patterns
- **Bidirectional LSTM**: Processes speech sequences in both forward and backward directions, capturing temporal dependencies in speech patterns that indicate PD
- **Speech granularity levels**: Enables systematic comparison of diagnostic performance across phonemes, syllables, and words to identify the most informative level
- **Cross-linguistic adaptation**: Demonstrates the framework's ability to generalize across different languages while maintaining diagnostic capability
- **Time-aligned feature extraction**: Ensures precise mapping between speech segments and their corresponding diagnostic features
- **Clinical protocol alignment**: Validates that model-identified features correspond to established PD diagnostic indicators

## Architecture Onboarding

Component Map: Speech Input -> Feature Extraction -> Bidirectional LSTM -> Multi-head Attention -> PD Classification

Critical Path: Speech recordings are processed through automated feature extraction to obtain phonemes, syllables, and words, which are then fed into the bidirectional LSTM with multi-head attention for PD classification.

Design Tradeoffs: The framework balances interpretability (through attention mechanisms) with diagnostic accuracy, choosing phoneme-level analysis for superior performance while maintaining the ability to explain feature importance.

Failure Signatures: Performance degradation may occur with poor recording quality, non-standard pronunciations, or when speech tasks don't align with the model's training distribution (sustained vowels, diadochokinesia, /pataka/ sequences).

First Experiments:
1. Test framework performance on single language dataset to establish baseline accuracy
2. Compare diagnostic performance across different speech tasks within each language
3. Validate attention feature importance rankings against clinical expert assessments

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset composition and sample sizes across languages not explicitly detailed, affecting generalizability
- Focus on European Spanish and Italian limits cross-linguistic robustness without broader dialectal representation
- Reliance on specific speech tasks may miss other clinically relevant PD speech characteristics
- Attention-based interpretability requires validation against clinical ground truth
- Potential demographic confounders (age, gender, disease duration) not addressed

## Confidence
- High confidence: Cross-linguistic applicability and superiority of phoneme-level analysis
- Medium confidence: Clinical relevance of attention features pending validation
- Medium confidence: Reproducibility across different datasets and conditions

## Next Checks
1. Conduct multi-center validation using diverse demographic populations and recording environments
2. Perform ablation studies to determine individual contribution of each speech granularity level
3. Compare framework's attention-based feature importance rankings against independent clinical assessments by speech-language pathologists