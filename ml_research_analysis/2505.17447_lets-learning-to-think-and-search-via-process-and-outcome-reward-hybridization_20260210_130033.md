---
ver: rpa2
title: 'LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization'
arxiv_id: '2505.17447'
source_url: https://arxiv.org/abs/2505.17447
tags:
- reward
- search
- answer
- reasoning
- lets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of improving retrieval-augmented
  generation (RAG) systems for multi-step reasoning tasks by integrating reinforcement
  learning with both outcome-level and process-level rewards. The authors propose
  LeTS (Learning to Think-and-Search), which extends outcome-supervised GRPO by introducing
  two rule-based process-level reward modules: a knowledge redundancy reward that
  penalizes repeated retrievals of similar information within a reasoning chain, and
  a knowledge match reward that leverages high-performing rollouts to supervise underperforming
  ones through optimal step alignment.'
---

# LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization

## Quick Facts
- arXiv ID: 2505.17447
- Source URL: https://arxiv.org/abs/2505.17447
- Reference count: 28
- Performance: 2.61% average gain over prior RL-based RAG methods

## Executive Summary
LeTS addresses the challenge of improving retrieval-augmented generation (RAG) systems for multi-step reasoning by integrating reinforcement learning with both outcome-level and process-level rewards. The method extends outcome-supervised GRPO by introducing two rule-based process-level reward modules: a knowledge redundancy reward that penalizes repeated retrievals of similar information within a reasoning chain, and a knowledge match reward that leverages high-performing rollouts to supervise underperforming ones through optimal step alignment. LeTS uses an advantage rescaling mechanism to combine these signals, adjusting the original outcome-level advantage at each reasoning step according to its process-level reward. Experiments across multiple multi-hop and single-hop benchmarks show that LeTS achieves an average performance gain of 2.61% over prior RL-based RAG methods, reduces average search time by 30.85%, and generates 11.15% fewer tokens while maintaining or improving answer accuracy.

## Method Summary
LeTS builds on GRPO by hybridizing process and outcome rewards through advantage rescaling. The method trains a think-and-search policy model that generates reasoning and search steps, using outcome rewards (F1 score) and two process rewards: a Jaccard-based redundancy penalty for repeated document retrieval, and a knowledge match reward using Hungarian alignment between underperforming and outperforming rollouts. The advantage rescaling formula modulates the original outcome-level advantage at each step by the normalized process reward, amplified or attenuated based on the sign of the outcome advantage. This approach aims to reduce redundant searching while improving the quality of retrieved information through fine-grained step supervision.

## Key Results
- LeTS achieves an average performance gain of 2.61% over prior RL-based RAG methods across multi-hop and single-hop benchmarks
- The method reduces average search time by 30.85% and generates 11.15% fewer tokens while maintaining or improving answer accuracy
- LeTS demonstrates strong generalization across different model sizes, including Qwen2.5-7B and Qwen2.5-3B-Instruct
- The ablation study shows both knowledge redundancy and knowledge match rewards contribute to the performance gains

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Redundancy Penalty
Penalizing retrieval steps that repeat similar documents reduces redundant searching behavior. At each reasoning step, compute pairwise Jaccard similarity between the current step's retrieved documents and all preceding steps within the same rollout. The process-level reward is 1 minus the maximum overlap, so steps retrieving novel information receive higher rewards. This is integrated via advantage rescaling: the outcome-level advantage for each step is multiplied by (1 + sgn(A_o) · λ · r̂_p), where λ controls the magnitude. Core assumption: Redundant retrieval within a single reasoning chain correlates with inefficiency and potentially degraded reasoning quality.

### Mechanism 2: Knowledge Match Reward for Underperforming Rollouts
Aligning steps from underperforming rollouts to successful rollouts provides fine-grained supervision signals. For underperforming rollouts, compute a match matrix of Jaccard similarities between each step's documents and those from outperforming rollouts. Apply the Kuhn-Munkres (Hungarian) algorithm for optimal alignment, treating it as a maximum-weight bipartite matching. The resulting alignment yields process-level rewards for each step in the underperforming rollout. Core assumption: Steps in underperforming rollouts can be meaningfully aligned to steps in successful rollouts, and this alignment reflects correct intermediate behavior.

### Mechanism 3: Advantage Rescaling to Hybridize Process and Outcome Rewards
Modulating outcome-level advantage at each step using process-level rewards improves credit assignment. Normalize outcome rewards across a group of rollouts to compute A^o_i. Normalize process rewards within each rollout to get r̂^p_{i,j}. The rescaled advantage for step j is A_{i,j} = (1 + sgn(A^o_i) · λ · r̂^p_{i,j}) · A^o_i. This amplifies or attenuates the outcome advantage based on step quality while preserving the overall trajectory's reward sign. Core assumption: Process-level rewards capture relative step importance within a rollout, and scaling them with outcome advantage improves learning signals.

## Foundational Learning

- **Concept: Reinforcement Learning with Outcome Rewards (e.g., GRPO)**
  - Why needed here: LeTS builds on GRPO, which estimates value baselines from groups of rollouts instead of training a separate critic. Understanding group-based advantage estimation is essential.
  - Quick check question: Can you explain how GRPO computes advantages differently from PPO?

- **Concept: Retrieval-Augmented Generation (RAG) with Multi-Hop Reasoning**
  - Why needed here: The method targets multi-step reasoning where iterative retrieval is required. Understanding the think-and-search loop (query → retrieval → reasoning) is foundational.
  - Quick check question: What is the difference between naive RAG and iterative RAG in multi-hop settings?

- **Concept: Process Reward Models vs Outcome Reward Models**
  - Why needed here: LeTS hybridizes process and outcome rewards. Distinguishing between stepwise supervision and final-answer supervision is key to understanding the method's contribution.
  - Quick check question: Why might outcome rewards alone be insufficient for multi-step reasoning tasks?

## Architecture Onboarding

- **Component map:**
  1. Policy Model (LLM): Generates think-and-search steps (reasoning + search queries)
  2. Retriever/Search Engine: Returns documents for each search query
  3. Outcome Reward Module: Computes final reward based on format correctness and F1 score
  4. Process Reward Module: Computes redundancy reward (within rollout) and knowledge match reward (across rollouts)
  5. Advantage Rescaling Layer: Hybridizes process and outcome rewards via Eq. 7
  6. GRPO Optimizer: Updates policy using clipped objective with rescaled advantages

- **Critical path:**
  1. Question input → Policy generates rollout with N steps
  2. Each step: reasoning → search query → retrieval → documents
  3. Final answer → outcome reward computed
  4. Process rewards computed for each step (if rollout is well-formatted)
  5. Advantages rescaled → policy updated

- **Design tradeoffs:**
  - Rule-based process rewards (Jaccard) are simple and interpretable but may not capture semantic relevance or reasoning quality beyond document overlap
  - Using Hungarian alignment assumes structural similarity between rollouts; may fail for highly divergent reasoning paths
  - λ controls the influence of process rewards; too high may dominate outcome signal, too low may render process rewards ineffective

- **Failure signatures:**
  - Reward hacking: Policy exploits process rewards without improving outcomes (e.g., generating diverse but irrelevant queries)
  - Redundancy penalty over-suppression: Model avoids retrieving necessary evidence if it overlaps slightly with prior steps
  - Alignment noise: Knowledge match rewards become unstable when outperforming rollouts are few or structurally different

- **First 3 experiments:**
  1. Ablate each process reward (KR-only, KM-only, both) on a multi-hop benchmark (e.g., MusiQue) to isolate contributions
  2. Sweep λ ∈ {0.05, 0.1, 0.2} and plot validation performance vs. average search steps to find the sweet spot
  3. Compare against a baseline with only outcome rewards (vanilla GRPO) on both multi-hop and single-hop benchmarks to measure generalization and efficiency gains

## Open Questions the Paper Calls Out

- **Open Question 1:** Can LeTS scale effectively to larger language models (e.g., 70B+ parameters), and how do the relative benefits of process-level rewards change with model scale? The authors note they could not validate on larger language models due to cost constraints.

- **Open Question 2:** Why does outcome-level RL fail to stimulate think-and-search capabilities in base versions of small language models, and can LeTS's process-level rewards address this failure? The paper observes reward hacking behavior on Qwen2.5-3B-Base where search time drops to zero despite increasing rewards.

- **Open Question 3:** Can the process-and-outcome reward hybridization approach transfer effectively to non-RAG reasoning domains such as mathematical reasoning, code generation, or logical deduction? The authors state this reveals potential in other scenarios but did not test these domains.

- **Open Question 4:** How sensitive is LeTS to the choice of process-level reward formulation (Jaccard similarity, Hungarian algorithm alignment) versus alternative process reward designs? The paper does not ablate alternative formulations or justify these choices against other possibilities.

## Limitations
- The method has not been validated on larger language models (70B+ parameters) due to computational costs
- Outcome-level RL fails to stimulate think-and-search abilities in base versions of small language models, with no clear solution identified
- The process rewards are specifically designed for RAG settings and may not transfer effectively to non-retrieval reasoning domains
- The specific mathematical formulations for process rewards (Jaccard similarity, Hungarian alignment) were not compared against alternative formulations

## Confidence

- **Method validity:** High - The approach builds on established GRPO framework with well-defined mathematical formulations
- **Experimental results:** Medium - Results show consistent improvements across benchmarks, but limited to 3B and 7B models
- **Reproducibility:** Medium - Key implementation details provided, but some specifics like retriever configuration and token masking require assumptions
- **Generalization:** Low - Only tested on specific model sizes and RAG settings; scalability and domain transfer remain open questions

## Next Checks

1. Implement the advantage rescaling mechanism and verify it correctly modulates outcome advantages based on process rewards using synthetic rollout data
2. Test the knowledge redundancy penalty on a small dataset to confirm it reduces redundant retrievals without over-suppressing necessary evidence
3. Validate the Hungarian alignment procedure by checking that aligned steps from underperforming rollouts have meaningful document overlap with successful rollouts