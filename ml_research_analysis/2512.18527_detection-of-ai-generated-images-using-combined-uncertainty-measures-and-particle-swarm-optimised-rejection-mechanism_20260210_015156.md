---
ver: rpa2
title: Detection of AI Generated Images Using Combined Uncertainty Measures and Particle
  Swarm Optimised Rejection Mechanism
arxiv_id: '2512.18527'
source_url: https://arxiv.org/abs/2512.18527
tags:
- uncertainty
- images
- fisher
- rejection
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a detection framework that leverages multiple\
  \ uncertainty measures\u2014Fisher Information, Monte Carlo Dropout entropy, and\
  \ Gaussian Process predictive variance\u2014to distinguish AI-generated images from\
  \ natural ones. These measures are combined using Particle Swarm Optimisation to\
  \ learn optimal weightings and an adaptive rejection threshold."
---

# Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism

## Quick Facts
- **arXiv ID**: 2512.18527
- **Source URL**: https://arxiv.org/abs/2512.18527
- **Reference count**: 39
- **Primary result**: Multi-source uncertainty fusion achieves ~70% IPR on out-of-distribution AI generators while maintaining <1.5% rejection of natural images.

## Executive Summary
This paper proposes a detection framework that leverages multiple uncertainty measures—Fisher Information, Monte Carlo Dropout entropy, and Gaussian Process predictive variance—to distinguish AI-generated images from natural ones. These measures are combined using Particle Swarm Optimisation to learn optimal weightings and an adaptive rejection threshold. The model is trained on Stable Diffusion-generated images and evaluated under distribution shifts from GLIDE, VQDM, Midjourney, BigGAN, and StyleGAN3. While standard metrics degrade under shift, the Combined Uncertainty measure consistently achieves around 70% incorrect rejection rate on unseen generators, effectively filtering misclassified AI samples. Under adversarial attacks (FGSM and PGD), it rejects 61% of successful attacks, outperforming other individual uncertainty measures. The framework also maintains high acceptance of correct predictions for natural images and in-domain AI data. The results demonstrate that multi-source uncertainty fusion enhances resilience and adaptability in AI-generated image detection.

## Method Summary
The method uses a ResNet50 or ViT backbone with a custom binary classification head trained on Stable Diffusion images. Three uncertainty measures are extracted in parallel: Fisher Information (computed on classification head gradients), Monte Carlo Dropout entropy (via 20 stochastic forward passes), and Gaussian Process predictive variance (via Deep Kernel Learning with 1000 inducing points). These are normalized and combined using Particle Swarm Optimization to learn optimal weights and a rejection threshold. Samples with combined uncertainty exceeding the threshold are rejected rather than classified. The approach is evaluated on cross-generator distribution shifts and adversarial attacks.

## Key Results
- Combined Uncertainty achieves ~70% IPR on out-of-distribution generators while maintaining <1.5% Nature class rejection
- Under adversarial attacks, Combined Uncertainty rejects 61% of successful attacks, outperforming individual measures
- The method maintains CPA% above 85% for both AI and Nature classes across all tested distributions
- Cross-model evaluation shows ResNet50 outperforms ViT in balancing CPA and IPR trade-offs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining multiple uncertainty measures improves detection of out-of-distribution AI-generated images compared to single-metric approaches.
- **Mechanism:** Three complementary uncertainty signals capture different aspects of model confidence: Fisher Information measures parameter sensitivity to input variations; MC Dropout entropy captures predictive variability through stochastic forward passes; GP predictive variance quantifies uncertainty based on training data density via kernel similarity. These are normalized and linearly combined into a single scalar score.
- **Core assumption:** The three uncertainty measures provide non-redundant information—when one fails under distribution shift, others compensate.
- **Evidence anchors:**
  - [abstract] "integrating multiple uncertainty measures—Fisher Information, Monte Carlo Dropout entropy, and Gaussian Process predictive variance—into a single scalar uncertainty score"
  - [section] Table 8 shows Combined Uncertainty achieves ~70% IPR on OOD datasets while individual measures drop to 9-33%
  - [corpus] Weak direct corpus support; related work (Edge-Enhanced ViT) focuses on feature extraction rather than uncertainty fusion
- **Break condition:** If novel generative models produce distribution shifts that simultaneously degrade all three uncertainty measures in correlated ways, the fusion advantage collapses.

### Mechanism 2
- **Claim:** Particle Swarm Optimization finds effective weights and rejection thresholds that balance correct acceptance against incorrect rejection.
- **Mechanism:** PSO treats weights (6 uncertainty components) and threshold τ as particle positions, optimizing a score combining CPA% (correctly predicted accepted) and IPR% (incorrectly predicted rejected) across both AI and Nature classes. The objective maximizes acceptance of correct predictions while maximizing rejection of errors.
- **Core assumption:** The optimal weighting on in-distribution data (Stable Diffusion) transfers to out-of-distribution generators without re-optimization.
- **Evidence anchors:**
  - [abstract] "Particle Swarm Optimization to learn optimal weightings and determine an adaptive rejection threshold"
  - [section] Figure 3 shows PSO-derived weights with Total Fisher Information and GP variance receiving highest weights; Equation 40 defines the optimization objective
  - [corpus] PSO used for feature selection under uncertainty (arXiv:2508.20123), supporting viability of PSO for uncertainty-weighting tasks
- **Break condition:** If training distribution (Stable Diffusion) is unrepresentative of the uncertainty structure in target distributions, the learned threshold will be miscalibrated.

### Mechanism 3
- **Claim:** A rejection mechanism based on combined uncertainty quarantines misclassified synthetic images while preserving natural image acceptance.
- **Mechanism:** Samples with combined uncertainty scores exceeding threshold τ are rejected rather than classified. This is asymmetrically valuable because most errors under distribution shift occur in the AI class (false negatives—synthetic images mislabeled as natural).
- **Core assumption:** Rejected samples are acceptable losses that can fuel retraining; false rejection of natural images is more costly than false rejection of AI images.
- **Evidence anchors:**
  - [abstract] "incorrect rejection rate of approximately 70% on these out-of-distribution samples"
  - [section] Table 9 shows Combined Uncertainty maintains <1.5% rejection rate for Nature class across all datasets while achieving 59-93% rejection for misclassified AI samples
  - [corpus] Watermarking approaches (arXiv:2503.18156) address similar provenance problems but via embedding rather than rejection
- **Break condition:** If application requires zero false rejections of valid content, the conservative threshold becomes untenable.

## Foundational Learning

- **Concept: Gaussian Process Predictive Variance**
  - **Why needed here:** GP variance decreases in regions with high training data density and increases for OOD inputs. Understanding this requires grasping kernel functions, inducing points for scalability, and variational inference.
  - **Quick check question:** Given a test point far from all inducing points, would you expect GP variance to be high or low? Why?

- **Concept: Monte Carlo Dropout as Bayesian Approximation**
  - **Why needed here:** MC Dropout provides epistemic uncertainty estimates by keeping dropout active during inference and aggregating predictions across multiple stochastic forward passes.
  - **Quick check question:** If MC Dropout entropy is high but the model's softmax probability is also high, what does this suggest about the prediction's reliability?

- **Concept: Fisher Information Matrix**
  - **Why needed here:** FIM quantifies how much information data provides about model parameters. Per-instance Fisher captures how individual inputs affect the learned decision boundary.
  - **Quick check question:** Would you expect an OOD image to have higher or lower Fisher Information magnitude compared to an in-distribution image? What does this imply for uncertainty?

## Architecture Onboarding

- **Component map:** Image → Feature extractor (frozen ResNet50 backbone) → Classification head → Three parallel uncertainty branches → Normalization → Weighted combination → Threshold comparison → Accept/Reject

- **Critical path:** Image → Feature extractor (frozen ResNet50 backbone) → Classification head → Three parallel uncertainty branches → Normalization → Weighted combination → Threshold comparison → Accept/Reject

- **Design tradeoffs:**
  - **ResNet50 vs. ViT:** ResNet50 shows better cross-generator generalization (Table 4 vs. Table 5); ViT has higher rejection rates on Nature class
  - **Conservative vs. permissive threshold:** Higher IPR (more errors caught) trades off against lower CPA (more correct predictions discarded)
  - **Full FIM vs. classification head only:** Computing FIM only on custom head reduces computational cost but may miss uncertainty signals from feature extractor

- **Failure signatures:**
  - Near-100% rejection rate with near-0% CPA: Threshold too aggressive or uncertainty measures uncalibrated (seen with Entropy of Expected in Table 8)
  - High acceptance of misclassified AI samples: Threshold too permissive for target distribution (Probability measure in Table 8 shows IPR dropping to 9-18% on OOD data)
  - Asymmetric Nature/AI rejection: Check class-specific thresholds or retrain with balanced data

- **First 3 experiments:**
  1. **Baseline calibration:** Train classifier on Stable Diffusion, evaluate on held-out Stable Diffusion test set. Verify all three uncertainty measures correlate with prediction errors. Plot CPA vs. IPR curves for each measure individually.
  2. **Cross-generator transfer:** Without retraining, apply the PSO-optimized weights and threshold to Midjourney, GLIDE, VQDM, BigGAN test sets. Compare Combined Uncertainty IPR against individual measures. Confirm Nature class rejection remains below 5%.
  3. **Adversarial stress test:** Generate FGSM and PGD adversarial examples from Stable Diffusion validation set. Measure IPR for Combined Uncertainty vs. GP-only. Confirm Combined approach maintains >90% CPA while achieving >60% IPR on attacks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does integrating rejected high-uncertainty samples into an active learning loop affect the long-term generalization and stability of the detector?
- Basis in paper: [explicit] The authors state that rejected synthetic samples "serve as valuable input for retraining" and can "support retraining."
- Why unresolved: The paper proposes the collection mechanism but does not implement or evaluate the iterative retraining loop to measure performance gain or potential catastrophic forgetting.
- What evidence would resolve it: A longitudinal study measuring classification accuracy and rejection rates over multiple training cycles using the harvested samples.

### Open Question 2
- Question: Can the Particle Swarm Optimization weighting scheme be tuned to match the superior adversarial robustness of standalone Gaussian Process (GP) variance without compromising the Combined Uncertainty method's balance?
- Basis in paper: [explicit] The authors report that GP variance alone rejects up to 80% of successful attacks, whereas the Combined method rejects only 61%.
- Why unresolved: The current PSO objective maximizes a general score across distribution shifts, which appears to under-weight GP variance for the specific context of adversarial defense.
- What evidence would resolve it: Experiments re-optimizing weights specifically on adversarial samples (FGSM/PGD) and comparing the resulting trade-off curves against the current configuration.

### Open Question 3
- Question: To what extent does the static PSO-derived rejection threshold (τ*) degrade when encountering extreme distribution shifts from generative models with fundamentally different architectures?
- Basis in paper: [inferred] The authors suggest that as shift intensifies, "no single measure perfectly preserves the balance" and "advanced domain adaptation strategies may be necessary."
- Why unresolved: The current framework relies on a fixed threshold optimized on a specific validation set, which may fail under continuous or extreme drift not represented in the current benchmarks.
- What evidence would resolve it: Evaluation of the acceptance-rejection trade-off on unseen, structurally distinct generative architectures to identify the breaking point of the static threshold.

## Limitations
- **Architecture generalizability**: The approach depends on pre-trained backbones and assumes their feature representations remain informative across generative models, but the generalization mechanism is not fully explained.
- **Threshold transferability**: PSO learns optimal weights and threshold on Stable Diffusion validation data, but the paper does not demonstrate recalibration on target generators for severe distribution shifts.
- **Computational overhead**: Computing three uncertainty measures at inference time is expensive, but the paper does not report latency or memory footprint measurements.

## Confidence
- **High confidence**: The fusion of multiple uncertainty measures improves OOD detection compared to single measures, supported by IPR comparisons across datasets (Table 8). The PSO optimization effectively balances CPA and IPR.
- **Medium confidence**: The rejection mechanism generalizes to unseen generators without retraining, though this relies on the assumption that Stable Diffusion uncertainty structure is representative. Adversarial robustness claims are plausible but only tested on Stable Diffusion-originated attacks.
- **Low confidence**: Claims about computational efficiency and real-world deployment readiness are unsupported by the paper.

## Next Checks
1. **Calibration Transfer Test**: Apply the PSO-learned weights and threshold to a new, unseen generator (e.g., DALL-E 3) without retraining. Measure whether CPA and IPR remain balanced, or if recalibration is needed.

2. **Natural Image Robustness**: Evaluate rejection rates on natural images from datasets not in ImageNet (e.g., COCO, Places365). Confirm that the Nature class IPR stays below 5% and identify failure modes.

3. **Ablation of Individual Uncertainty Measures**: Retrain with only one or two uncertainty measures (e.g., remove GP variance). Quantify the performance drop to confirm that each measure contributes unique information and that fusion is necessary.