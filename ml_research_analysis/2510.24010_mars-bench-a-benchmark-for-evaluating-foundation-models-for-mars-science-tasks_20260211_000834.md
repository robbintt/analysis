---
ver: rpa2
title: 'Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science
  Tasks'
arxiv_id: '2510.24010'
source_url: https://arxiv.org/abs/2510.24010
tags:
- datasets
- dataset
- mars-bench
- classification
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mars-Bench introduces the first standardized benchmark for evaluating
  foundation models on Mars science tasks, addressing the lack of systematic evaluation
  frameworks in planetary science. The benchmark comprises 20 datasets spanning classification,
  segmentation, and object detection tasks, covering diverse geologic features such
  as craters, cones, boulders, and frost using both orbital and surface imagery.
---

# Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks

## Quick Facts
- arXiv ID: 2510.24010
- Source URL: https://arxiv.org/abs/2510.24010
- Reference count: 40
- Primary result: First standardized benchmark for evaluating foundation models on Mars science tasks with 20 datasets covering classification, segmentation, and object detection

## Executive Summary
Mars-Bench introduces the first standardized benchmark for evaluating foundation models on Mars science tasks, addressing the lack of systematic evaluation frameworks in planetary science. The benchmark comprises 20 datasets spanning classification, segmentation, and object detection tasks, covering diverse geologic features such as craters, cones, boulders, and frost using both orbital and surface imagery. All datasets are standardized in ML-ready formats with consistent train/validation/test splits and include expert-driven quality corrections. Baseline evaluations using models pre-trained on natural images, Earth satellite data, and vision-language models reveal that Mars-specific foundation models may offer advantages over general-domain counterparts. The benchmark provides code, models, and guidelines to facilitate reproducibility and accelerate research in planetary science and beyond.

## Method Summary
Mars-Bench evaluates foundation models across 20 standardized Mars science datasets using three training strategies: scratch, frozen feature extraction, and full fine-tuning. The benchmark includes nine classification, eight segmentation, and three object detection datasets with consistent ML-ready formats and train/validation/test splits. Models are evaluated using F1-score, IoU, and mAP metrics aggregated via InterQuartile Mean (IQM) with 95% bootstrap confidence intervals across seven random seeds. The evaluation pipeline supports ImageNet and Earth Observation pre-trained models, with hyperparameter tuning and grid search over learning rates, batch sizes, and optimization algorithms. All code, datasets, and models are publicly available through HuggingFace and Zenodo.

## Key Results
- Feature extraction from ImageNet-pretrained models consistently outperforms both training from scratch and full fine-tuning across Mars science tasks
- ImageNet-pretrained models outperform Earth Observation (EO)-pretrained models on Mars tasks despite EO models being trained on satellite imagery
- Architecture choice exhibits task-dependent performance patterns: U-Net excels at segmentation while Vision Transformers excel at classification under limited data regimes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Feature extraction from ImageNet-pretrained models consistently outperforms both training from scratch and full fine-tuning across Mars science tasks.
- **Mechanism:** Pre-trained convolutional and attention-based feature extractors capture general visual primitives (edges, textures, hierarchical patterns) that transfer across domains even when semantic content differs. The frozen backbone preserves learned representations while only the task-specific head adapts to Mars geology.
- **Core assumption:** Low-level and mid-level visual features learned from natural images remain useful for planetary imagery despite absence of vegetation, water bodies, and human-made structures.
- **Evidence anchors:** [abstract] "Results show strong performance with feature extraction strategies"; [section 5.1] Figures 2-4 show feature extraction achieves highest normalized metrics across classification, segmentation, and detection.

### Mechanism 2
- **Claim:** ImageNet-pretrained models outperform Earth Observation (EO)-pretrained models on Mars tasks despite EO models being trained on satellite imagery.
- **Mechanism:** Pre-training data scale and diversity dominate domain proximity. ImageNet (14M images, diverse classes) provides richer feature representations than EO models (â‰¤1M images). Mars imagery lacks vegetation, water bodies, and structures common in both Earth satellite and natural image datasets, reducing the benefit of EO pre-training.
- **Core assumption:** The diversity-coverage trade-off in pre-training data affects downstream generalization; visual diversity matters more than imaging perspective similarity.
- **Evidence anchors:** [section 5.3] "ViT is pre-trained on 14 million images, while SatMAE, CROMA, and Prithvi are pre-trained on 1 million or less"; [section 5.3] "Martian imagery lacks vegetation, water bodies, and human-made structures, which are common in EO datasets."

### Mechanism 3
- **Claim:** Architecture choice exhibits task-dependent performance patterns: U-Net excels at segmentation while Vision Transformers excel at classification under limited data regimes.
- **Mechanism:** CNN-based architectures with skip connections (U-Net) preserve spatial information critical for dense prediction tasks. Transformers require more data to learn positional relationships but capture global context beneficial for classification. Small parameter models (SqueezeNet) underperform across all tasks.
- **Core assumption:** Task architecture alignment principles from natural images extend to planetary imagery; data scarcity amplifies architectural differences.
- **Evidence anchors:** [section 5.1] "U-Net achieves the highest overall performance" for segmentation; "transformer-based models such as SwinV2-B and ViT-L/16 consistently outperform smaller convolutional models like SqueezeNet1.1".

## Foundational Learning

- **Transfer Learning Paradigms:** Why needed here: The entire benchmark evaluation framework depends on understanding feature extraction vs. fine-tuning vs. scratch training regimes. Quick check question: Can you explain why freezing a pre-trained backbone might outperform full fine-tuning on small datasets?

- **Distribution Shift / Domain Adaptation:** Why needed here: Section 5.3 explicitly analyzes cross-domain generalization from Earth to Mars; understanding covariate shift is essential for interpreting results. Quick check question: Why might Earth satellite imagery pre-training not transfer well to Mars orbital imagery?

- **Evaluation Methodology for Specialized Domains:** Why needed here: Mars-Bench adopts IQM, bootstrapped confidence intervals, and normalization from Geo-Bench/BigBio traditions. Quick check question: Why use InterQuartile Mean (IQM) instead of simple mean for aggregating results across multiple seeds?

## Architecture Onboarding

- **Component map:** Data layer (20 datasets in 3 task categories) -> Model zoo (ResNet101, SqueezeNet, InceptionV3, SwinV2-B, ViT-L/16 for classification; U-Net, DeepLabV3+, SegFormer, DPT for segmentation; YOLO11, SSD, RetinaNet, Faster R-CNN for detection) -> Training pipeline (PyTorch Lightning-based) -> Evaluation (IQM with 7 random seeds)

- **Critical path:** 1. Load dataset from HuggingFace using provided loading scripts 2. Select model architecture and training regime (feature extraction recommended as baseline) 3. Run hyperparameter grid search on validation loss 4. Retrain with best config across 7 seeds 5. Compute normalized metrics and aggregate via IQM

- **Design tradeoffs:** Feature extraction is faster, more stable, and generally higher-performing on small Mars datasets; fine-tuning may help on larger datasets. Data partitioning: Few-shot versions available for classification only; segmentation/detection datasets too small for meaningful partitioning. EO vs. ImageNet pre-training: ImageNet currently superior; EO models require channel replication for single-band Mars imagery.

- **Failure signatures:** DPT showing high variance/wide confidence intervals (unstable training); Object detection models (especially on mb-boulder_det, mb-dust_devil_det) achieving near-zero mAP due to small object counts and low contrast; SqueezeNet underperforming across all tasks (insufficient capacity); VLMs (Gemini, GPT) failing on fine-grained geologic classes while succeeding on general terrain (over-reliance on pre-training distribution).

- **First 3 experiments:** 1. Baseline establishment: Run ResNet101 feature extraction on mb-frost_cls (largest dataset, binary task) to verify pipeline and establish expected performance range (~0.90+ F1). 2. Architecture comparison: Compare U-Net vs. SegFormer on mb-crater_binary_seg using feature extraction to validate segmentation architecture claims. 3. Data scaling analysis: Train ViT-L/16 on mb-domars16k partitions (1%, 10%, 100%) to reproduce data regime sensitivity findings and confirm feature extraction advantage at low data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a foundation model pre-trained specifically on Mars imagery outperform ImageNet and EO-pretrained models across Mars-Bench tasks, and what pre-training data composition is optimal?
- Basis in paper: [explicit] "Results from all analyses suggest that Mars-specific foundation models may offer advantages over general-domain counterparts, motivating further exploration of domain-adapted pre-training" (Abstract); Section 6 states Mars-Bench will "accelerate the development of foundation models specifically tailored to Mars orbital and surface-related tasks."
- Why unresolved: No Mars-specific foundation model currently exists; baseline evaluations only used ImageNet, EO-pretrained, and VLM models.
- What evidence would resolve it: Training a foundation model on large-scale unlabeled Mars imagery and benchmarking it against current baselines across all 20 Mars-Bench datasets.

### Open Question 2
- Question: How can class imbalance be effectively addressed in specialized planetary science datasets where no single balancing strategy (undersampling, oversampling, loss weighting) works universally?
- Basis in paper: [explicit] Section D.1.1 concludes "there is no single data manipulation technique that is universally effective across all models and dataset combinations for handling class imbalance. This suggests that the choice of technique depends heavily on the specific characteristics of the dataset and the model being used."
- Why unresolved: Standard techniques showed inconsistent effectiveness; minority class improvements often came at the cost of majority class performance degradation.
- What evidence would resolve it: Development and systematic evaluation of domain-adaptive or model-specific imbalance handling methods across all imbalanced Mars-Bench datasets.

### Open Question 3
- Question: How does georeferencing metadata (latitude, longitude) affect model evaluation, spatial generalization, and regional bias detection in Mars science benchmarks?
- Basis in paper: [explicit] Section 7 Limitations: "A key limitation of Mars-Bench is the absence of georeferencing for most datasets... it is currently not possible to assess the spatial distribution or coverage of Mars-Bench across different regions of Mars."
- Why unresolved: Original dataset sources lack spatial metadata; spatial analysis or regional generalization studies cannot be conducted.
- What evidence would resolve it: Adding geolocation data to all datasets and conducting spatial stratification experiments to evaluate cross-regional generalization.

## Limitations

- Most datasets are small (tens to hundreds of samples), limiting statistical power and potentially inflating variance in confidence intervals
- No Mars-specific foundation model currently exists for comparison, leaving domain adaptation potential untested
- Object detection performance claims are based on datasets with extremely limited samples (tens of instances)

## Confidence

**High confidence:** The superiority of feature extraction over full fine-tuning and scratch training is consistently demonstrated across multiple architectures and tasks. The IQM aggregation methodology and bootstrapping approach are standard and well-justified.

**Medium confidence:** Architecture-task alignment patterns (U-Net for segmentation, ViT for classification) show clear trends but may shift with larger datasets or task-specific architectural innovations. The ImageNet vs. EO pre-training comparison relies on assumptions about data diversity versus domain proximity that could change with different model families.

**Low confidence:** Object detection performance claims, particularly for boulder and dust devil detection, are based on datasets with extremely limited samples (tens of instances).

## Next Checks

1. **Scale sensitivity validation:** Replicate the data scaling experiment from mb-domars16k across additional classification datasets to confirm that feature extraction advantage persists across different task types and data distributions.

2. **Cross-planetary transfer validation:** Train Earth satellite models on Mars datasets and Mars models on Earth satellite datasets to empirically test bidirectional transfer capabilities and validate the domain proximity assumptions.

3. **Model capacity threshold validation:** Systematically test larger-capacity models (e.g., ViT-G, SwinV2-L) on the largest Mars datasets to determine if the current architecture recommendations hold at scale or if transformer dominance increases with dataset size.