---
ver: rpa2
title: Revealing and Mitigating Over-Attention in Knowledge Editing
arxiv_id: '2502.14838'
source_url: https://arxiv.org/abs/2502.14838
tags:
- knowledge
- editing
- specificity
- attention
- edited
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of specificity failure in knowledge
  editing for large language models (LLMs), where editing one fact can cause the model
  to incorrectly predict other related facts. The authors identify that this issue
  stems from excessive attention scores assigned to entities related to the edited
  knowledge, leading to undue focus on specific snippets within the context.
---

# Revealing and Mitigating Over-Attention in Knowledge Editing

## Quick Facts
- **arXiv ID:** 2502.14838
- **Source URL:** https://arxiv.org/abs/2502.14838
- **Reference count:** 40
- **Primary result:** SADR improves specificity by up to 295.8% with only 0.19% drop in edit success

## Executive Summary
This paper addresses specificity failure in knowledge editing for large language models, where editing one fact causes incorrect predictions for related facts. The authors identify that excessive attention scores assigned to edited entities cause this issue, leading to undue focus on specific snippets. They propose Selective Attention Drift Restriction (SADR), which introduces regularization during editing to restrict attention weight distribution changes. Experiments on five LLMs (1.1B to 20B parameters) show SADR significantly improves specificity performance while maintaining edit success.

## Method Summary
The paper proposes Selective Attention Drift Restriction (SADR) to mitigate specificity failure in knowledge editing. SADR adds a regularization term to the editing objective that restricts changes in attention weight distributions by comparing current attention maps against a frozen vanilla model. It selectively identifies "over-focused" attention heads (where attention to the edited token exceeds the vanilla model's maximum) and minimizes KL divergence between current and vanilla attention distributions. This constrains the optimizer to find solutions that change target fact predictions without altering fundamental attention geometry used for other contexts.

## Key Results
- SADR achieves up to 130.9% and 295.8% improvements in two main specificity tasks
- Only 0.19% decrease in edit success rate
- Selectively restraining over-focused heads outperforms restraining all heads on both edit success and specificity
- SADR maintains strong performance across models ranging from 1.1B to 20B parameters

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Specificity failure is primarily caused by "Attention Drift" where attention heads over-attend to edited entities
- **Mechanism:** Editing modifies MLP weights to associate subject $s$ with new object $o_{edit}$, creating hidden states highly salient to attention heads. During inference, middle-upper layer attention heads assign excessive weights to $s$, forcing predictions of $o_{edit}$ regardless of actual relation
- **Core assumption:** The primary driver is attention distribution shifting to over-focus on edited subject, not just MLP corruption
- **Evidence anchors:** Abstract states excessive attention scores to edited entities cause specificity failure; Figure 4 shows positive relationship between attention drift and incorrect answer probability; external literature confirms knowledge editing challenges in multi-hop scenarios
- **Break condition:** If correlation between attention weight drift (KL divergence) and incorrect answer probability is weak, mechanism fails

### Mechanism 2
- **Claim:** Restricting attention drift selectively during editing optimization improves specificity while maintaining edit success
- **Mechanism:** SADR adds regularization term $L_{SADR}$ to editing objective, identifying over-focused heads and minimizing KL divergence between current and vanilla attention distributions
- **Core assumption:** Targeting only heads exhibiting excessive drift is sufficient to prevent interference with unrelated knowledge
- **Evidence anchors:** Abstract describes SADR's regularization approach; Section 6.1 shows selective restraint outperforms full restraint; external literature implies context handling is key bottleneck
- **Break condition:** If selection criterion is too aggressive, it may prevent edit from being learned (low efficacy)

### Mechanism 3
- **Claim:** Specificity failure contamination is localized to attention activations at last token position in middle-upper layers
- **Mechanism:** "Contaminating Substitution" technique shows replacing edited model activations with vanilla activations reveals error source. While MLP edits occur in early layers, error propagation is mediated by attention layers (roughly layers 16-23 in GPT-J) at final token
- **Core assumption:** Causal graph intervention accurately simulates counterfactual flow where attention drift doesn't occur
- **Evidence anchors:** Section 3.2 identifies attention module as primary cause; Figure 5 shows patching attention weights increases correct answer probability
- **Break condition:** If patching attention weights fails to recover true object probability, localization claim fails

## Foundational Learning

- **Concept:** Knowledge Editing (Locate-and-Edit)
  - **Why needed here:** Base operation being debugged; understand methods like ROME locate specific layer/MLP weight and perform rank-one update to change factual association
  - **Quick check question:** Does editing MLP weights directly alter attention weights? (Answer: Indirectly via optimization landscape which SADR constrains)

- **Concept:** Specificity vs. Generalization
  - **Why needed here:** Core tradeoff; generalization ensures paraphrases work, specificity ensures neighbors/unrelated attributes remain correct
  - **Quick check question:** If model predicts "New York" for "The Louvre is in...", is this failure of generalization or specificity? (Answer: Specificity/Neighborhood)

- **Concept:** Attention Drift (KL Divergence)
  - **Why needed here:** Metric for failure; measures how much attention weight distribution changes before vs. after editing
  - **Quick check question:** If $DKL(P_{edit} || P_{vanilla})$ is high for specific head, what does this imply? (Answer: Model focuses on tokens differently than before, likely over-attending to edited subject)

## Architecture Onboarding

- **Component map:** Vanilla Model ($G$) -> Editor (e.g., ROME/MEMIT) -> SADR Module -> Evaluator
- **Critical path:**
  1. Identify target fact $(s, r, o_{edit})$
  2. Run forward pass to identify layers/heads where attention drift is harmful
  3. Optimize edit vector $v^*$ using $L_{total} = L_{edit} + \gamma L_{SADR}$
  4. Apply calculated rank-one update to model weights
- **Design tradeoffs:**
  - Restraint Weight ($\gamma$): High $\gamma$ ensures high specificity but may lower efficacy
  - Selection Strategy: Restraining all heads is safer for specificity but degrades generalization more than selective restraint
- **Failure signatures:**
  - Relation Hallucination: Query "The color of [Edited Subject] is?" → Output "[Edited Object]"
  - Neighbor Contagion: Context "[Edited Subject] is in [Edited Object]." Query "[Neighbor Subject] is in?" → Output "[Edited Object]"
- **First 3 experiments:**
  1. Drift Verification: Visualize attention weights on failure case to confirm over-attention on edited subject tokens
  2. Patching Validation: Implement "Contaminating Substitution" by patching vanilla attention weights into edited model to prove fixing attention fixes output
  3. SADR Ablation: Train with and without SADR on CounterFact data; plot trade-off curve between Edit Success and Distract Neighborhood Score to find optimal $\gamma$

## Open Questions the Paper Calls Out
None

## Limitations
- Medium confidence in attention drift as primary driver of specificity failure - correlational evidence exists but causal chain not fully isolated
- Low confidence in generalizability across different model architectures - experiments focus on decoder-only transformers
- Medium confidence in selection criteria for over-focused heads - heuristic may miss nuanced harmful cases or be overly conservative

## Confidence
- Attention drift mechanism: **Medium**
- SADR generalizability across architectures: **Low**
- Selection criteria effectiveness: **Medium**

## Next Checks
1. **Causal Isolation Experiment**: Perform ablation where MLP weights are edited but attention weights are frozen (or vice versa) to definitively determine which component is necessary and sufficient for specificity failure
2. **Architecture Transfer**: Apply SADR to BERT-based models or other transformer variants to test whether attention drift is universal mechanism across architectures
3. **Long-Range Context Test**: Evaluate SADR on multi-hop reasoning tasks or extended context windows where attention patterns are more complex