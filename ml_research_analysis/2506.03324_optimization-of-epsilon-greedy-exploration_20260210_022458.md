---
ver: rpa2
title: Optimization of Epsilon-Greedy Exploration
arxiv_id: '2506.03324'
source_url: https://arxiv.org/abs/2506.03324
tags:
- exploration
- regret
- optimization
- systems
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of determining optimal exploration
  rates for epsilon-greedy policies in recommendation systems with practical constraints
  like batched updates and time-varying user traffic. The authors propose a principled
  framework that formulates exploration as an optimization problem, minimizing Bayesian
  regret through stochastic gradient descent.
---

# Optimization of Epsilon-Greedy Exploration

## Quick Facts
- **arXiv ID:** 2506.03324
- **Source URL:** https://arxiv.org/abs/2506.03324
- **Reference count:** 38
- **Primary result:** A principled framework for optimizing epsilon-greedy exploration rates in batched recommendation systems, showing batched feedback significantly impacts optimal strategies and dynamic adjustment via Model-Predictive Control outperforms static heuristics.

## Executive Summary
This paper addresses the challenge of determining optimal exploration rates for epsilon-greedy policies in recommendation systems with practical constraints like batched updates and time-varying user traffic. The authors propose a principled framework that formulates exploration as an optimization problem, minimizing Bayesian regret through stochastic gradient descent. Their approach leverages a Bayesian model to directly optimize exploration rates across time periods, allowing for dynamic adjustment via Model-Predictive Control.

The key contribution is showing that batched feedback significantly impacts optimal exploration strategies, with the proposed optimization methods automatically calibrating exploration to specific problem settings. Through extensive experiments on MovieLens and Netflix datasets with varying batch patterns, the methods consistently match or outperform the best heuristic for each setting.

## Method Summary
The framework optimizes epsilon-greedy exploration by minimizing Bayesian regret through differentiable approximations. It uses a Bayesian linear contextual bandit model with Gaussian priors, where exploration rates ε_t are optimized via stochastic gradient descent. The key insight is that batched feedback creates different optimal exploration strategies than sequential feedback, and the optimization can automatically calibrate to specific batch patterns. Model-Predictive Control is employed to dynamically re-optimize exploration rates after each batch based on observed learning progress.

## Key Results
- Batched feedback significantly impacts optimal exploration strategies compared to sequential settings
- The proposed optimization methods automatically calibrate exploration to specific batch patterns and problem settings
- Uniform exploration with appropriate scheduling can outperform Thompson Sampling in short-horizon, batched scenarios
- The framework is robust to noisy estimates of arrival patterns and can incorporate minimum exploration constraints

## Why This Works (Mechanism)

### Mechanism 1: Differentiable Bayesian Regret via Posterior Analytics
Bayesian regret can be formulated as a differentiable function of exploration rates, enabling gradient-based optimization. Under Gaussian priors and linear reward models, the posterior distribution admits closed-form updates, allowing regret to be expressed in terms of tractable expectations over standard normals and user embeddings.

### Mechanism 2: Population Design Matrix Approximation for Tractability
The empirical design matrix converges to a population version linear in ε_t, making gradients computable without Bernoulli realizations. This replaces stochastic dependence on random sampling with deterministic linear scaling, with approximation error that is sublinear relative to regret scaling.

### Mechanism 3: Model-Predictive Control for Adaptive Replanning
Re-solving the exploration optimization after each batch improves performance by incorporating observed learning progress. This corrects for prior misspecification and adapts to random variation in learning speed through periodic replanning.

## Foundational Learning

- **Concept: Bayesian Linear Regression**
  - **Why needed here:** The entire framework relies on Gaussian posteriors with closed-form mean/covariance updates.
  - **Quick check question:** Given prior θ ~ N(β_1, Σ_1) and observations R_i = X_i^T θ + η_i with η_i ~ N(0, σ²), what is the posterior covariance after n observations with design matrix X?

- **Concept: Contextual Bandits**
  - **Why needed here:** The problem is framed as a batched contextual bandit with per-period regret accumulation.
  - **Quick check question:** Why does Thompson Sampling achieve sublinear regret in the asymptotic regime, and what assumption in this paper's setting might make it underperform?

- **Concept: Stochastic Gradient Descent with Automatic Differentiation**
  - **Why needed here:** Algorithm 1 uses SGD to optimize ε_{t:T}, computing gradients via autodiff through the simulated posterior trajectory.
  - **Quick check question:** In Algorithm 1, why do we resample Z_s ~ N(0, I) at each gradient step rather than fixing a single Monte Carlo sample?

## Architecture Onboarding

- **Component map:** User embedding samples -> Bayesian posterior updates -> Differentiable regret approximation -> SGD optimization -> Exploration rate application -> Reward collection -> Posterior update

- **Critical path:** Accurate user embedding distribution μ → correct design matrix I → meaningful regret gradients → optimal ε schedule → actual regret reduction

- **Design tradeoffs:**
  - Diagonal vs full covariance: Paper uses diagonal approximation for computational speed; degrades accuracy if user embedding dimensions are correlated
  - Prior specification: Experiments use misspecified prior β_1=0, Σ_1=I for robustness; better priors from historical item embeddings could improve but require maintenance
  - Batch size forecasting: MPC(noisy) experiments show tolerance to ~20% Dirichlet noise; larger errors would require conservative constraints
  - Explore-only vs explore+exploit data: Assumption 2 discards exploit-group data for posterior updates; practical implementations might benefit from including all data but lose theoretical guarantees

- **Failure signatures:**
  - Exploration collapse: ε_t → 0 early if prior is overconfident (Σ_1 too small) and initial rewards are noisy; mitigate with minimum exploration constraints
  - Non-convergence of SGD: If learning rate α is too high or horizon T is long, ε values may oscillate; monitor gradient norms across iterations
  - Batch size surprise: If actual arrivals deviate dramatically from forecast, pre-computed schedule is misaligned; MPC provides partial correction but requires fast re-optimization
  - Thompson Sampling unexpectedly wins: If horizon extends or batch pattern becomes constant, TS may catch up

- **First 3 experiments:**
  1. Prior sensitivity test: Run Planner with β_1=0, Σ_1=I vs calibrated prior from historical item statistics
  2. Batch size noise stress test: Systematically inject noise into n_t forecasts at 10%, 30%, 50% levels
  3. Minimum exploration constraint sweep: For Netflix K=10, Spike pattern, vary ε_min ∈ {0, 0.02, 0.05, 0.10}

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the proposed framework be extended to handle complex, non-linear reward models while maintaining differentiability for gradient-based optimization?
- **Open Question 2:** How can the optimization objective be modified to account for non-stationary environments where user preferences or item embeddings drift over time?
- **Open Question 3:** Can the methodology be adapted to optimize exploration for slate recommendations (recommending a set of items) rather than single-item assignments?
- **Open Question 4:** Can the framework be expanded to jointly optimize the allocation of traffic across time periods alongside the exploration rates?

## Limitations
- Assumes Gaussian noise and linear rewards, which may not hold in real recommendation systems with non-linear interactions
- Relies on accurate batch size forecasts, though experiments show robustness to moderate noise
- Specialized for batched settings and short horizons where Thompson Sampling underperforms
- Diagonal covariance approximation sacrifices accuracy for computational tractability

## Confidence
- **High Confidence:** The differentiable regret formulation via population design matrix approximation is mathematically rigorous with proven O(n^{1/4}) error bounds
- **Medium Confidence:** The practical performance improvements (Algorithm 2 MPC) are well-demonstrated experimentally but rely on heuristic replanning
- **Low Confidence:** The robustness to misspecified priors and noise in forecasts is demonstrated empirically but lacks theoretical bounds

## Next Checks
1. **Prior Sensitivity Test:** Compare Planner with calibrated vs misspecified prior (β_1=0, Σ_1=I) to quantify robustness tradeoff
2. **Batch Size Forecast Stress Test:** Systematically inject noise into arrival forecasts at varying levels (10%-50%) to identify failure thresholds
3. **Thompson Sampling Horizon Test:** Extend experiments to longer horizons (T>6) to verify if TS catches up as predicted by theory