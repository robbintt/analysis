---
ver: rpa2
title: Post-processing for Fair Regression via Explainable SVD
arxiv_id: '2504.03093'
source_url: https://arxiv.org/abs/2504.03093
tags:
- fairness
- matrix
- algorithm
- have
- post-processing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a post-processing method called ESVDFair for
  improving fairness in neural network regression models by modifying weight matrices
  via explainable SVD. The core idea is to use singular values from SVD of transformed
  weight matrices to enforce statistical parity by constraining first and second moment
  differences between demographic groups.
---

# Post-processing for Fair Regression via Explainable SVD

## Quick Facts
- arXiv ID: 2504.03093
- Source URL: https://arxiv.org/abs/2504.03093
- Reference count: 29
- Key outcome: ESVDFair achieves better or comparable fairness-accuracy trade-offs compared to baselines on Law School and COMPAS datasets while not requiring sensitive attributes at inference time.

## Executive Summary
This paper proposes a post-processing method called ESVDFair for improving fairness in neural network regression models by modifying weight matrices via explainable SVD. The core idea is to use singular values from SVD of transformed weight matrices to enforce statistical parity by constraining first and second moment differences between demographic groups. The method converts fairness constraints into convex optimization problems solvable in closed form. Experiments on Law School and COMPAS datasets show ESVDFair achieves better or comparable fairness-accuracy trade-offs compared to baselines, while not requiring sensitive attributes at inference time.

## Method Summary
ESVDFair operates as a post-processing step on a pre-trained neural network. It extracts activations from a selected layer for each demographic group, computes transformations to capture group mean and covariance differences, then uses SVD on these transformed weight matrices to derive singular value constraints. The method solves two convex optimization problems sequentially - first constraining second-moment differences via covariance transformations, then first-moment differences via mean transformations - with closed-form solutions for optimal singular values. The modified weights are reconstructed and optionally fine-tuned, achieving fairness without requiring sensitive attributes at inference time.

## Key Results
- On Law School data, ESVDFair achieved KS=0.235 (vs. 0.258-0.376 for baselines) with MSE=0.088
- On COMPAS data, ESVDFair achieved KS=0.130 (vs. 0.156-0.271) with MSE=0.214
- The method successfully aligns output distributions across groups while maintaining competitive performance
- ESVDFair demonstrates effectiveness across different network architectures and achieves favorable fairness-accuracy trade-offs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining singular values of transformed weight matrices bounds first-moment (mean) differences between group outputs.
- Mechanism: Construct transformation matrix Se where SeST_e = (x̄₁ - x̄₂)ᵀ(x̄₁ - x̄₂) + εₑI. The squared Frobenius distance between group means after linear transformation equals d²ₑ(x̄₁, x̄₂; W) = Σᵢσ²ᵢ(ₑ) - εₑtr[WWᵀ] (Theorem 3.1). By shrinking singular values σᵢ(ₑ) of WSe, the mean disparity shrinks proportionally.
- Core assumption: The transformation Se captures the geometric direction of group mean separation; εₑ small ensures tight bound.
- Evidence anchors:
  - [abstract] "singular values derived from the SVD of the transformed matrix directly correspond to the differences in the first and second moments"
  - [Section 3.1, Theorem 3.1] Exact formula linking d²ₑ to singular values
  - [corpus] Weak direct evidence; related SVD compression papers (SVD-LLM V2, ARSVD) use singular value truncation for efficiency, not fairness—suggests plausibility of singular value manipulation effects but no fairness validation in corpus.
- Break condition: If group means are equal before transformation (x̄₁ = x̄₂), Se becomes εₑI and mechanism degenerates to identity; also breaks if layer activations are highly non-Gaussian (Lemma 2.1 assumption violated).

### Mechanism 2
- Claim: Constraining singular values of covariance-transformed weight matrices bounds second-moment (covariance) differences between group outputs.
- Mechanism: Define M = (1/(N₁-1))X̃₁ᵀX̃₁ - (1/(N₂-1))X̃₂ᵀX̃₂ capturing covariance difference. Build Sv via eigenvalue decomposition: Sv = Q|Λ|^{1/2}. The covariance distance is bounded: d²ᵥ(X₁, X₂; W) ≤ ||WSᵥ||⁴₄ = Σᵢσ⁴ᵢ(ᵥ) (Theorem 3.2). Shrinking σᵢ(ᵥ) reduces covariance disparity.
- Core assumption: M's eigendecomposition reflects inter-group covariance structure; bound tightens when M positive definite.
- Evidence anchors:
  - [Section 3.2, Theorem 3.2] Upper bound via Schatten-4 norm of WSᵥ
  - [abstract] "differences in the first and second moments" linked to SVD
  - [corpus] No direct corpus validation for fairness-specific covariance alignment via SVD.
- Break condition: When covariance matrices are already equal across groups, M ≈ 0 and Sv degenerates; highly non-linear subsequent layers may propagate mismatches beyond second moments.

### Mechanism 3
- Claim: Sequential application of second-moment then first-moment optimization with closed-form solutions yields fair weights with bounded accuracy loss.
- Mechanism: (1) Solve convex problem minimizing ||XW'ᵀ - XWᵀ||²_F subject to Σᵢσ'⁴ᵢ(ᵥ) ≤ cᵥ (Theorem 4.2, cubic root closed-form); (2) Use result as starting point for first-moment problem with constraint Σᵢσ'²ᵢ(ₑ) ≤ cₑ (Theorem 4.1, water-filling-like solution σ*ᵢ(ₑ) = σᵢ(ₑ)kᵢ(ₑ)/(kᵢ(ₑ) + γₑ)); (3) Reconstruct W* via W* = (Σσ*ᵢuᵢvᵢᵀ)S⁻¹.
- Core assumption: Restricting search to transformed SVD basis (Wₑ, Wᵥ sets) preserves near-optimality; γₑ, γᵥ solvable via 1D root-finding.
- Evidence anchors:
  - [Section 4.1, Theorems 4.1 & 4.2] Closed-form solutions provided
  - [Section 5.4, Tables 1-2] KS reduced from 0.376→0.235 (Law), 0.256→0.130 (COMPAS) vs baselines
  - [corpus] No corpus papers solve fairness via SVD-based optimization; closed-form convex solutions for constrained SVD appear in compression (e.g., ARSVD) but objectives differ.
- Break condition: If Newton-Raphson fails to find γₑ, γᵥ (rare for reasonable cₑ, cᵥ); if cₑ, cᵥ set too tight, reconstructed W* may degrade accuracy unacceptably.

## Foundational Learning

- Concept: **Singular Value Decomposition (SVD)**
  - Why needed here: Core operation; W = UΣVᵀ decomposition enables singular value manipulation while preserving matrix structure; paper uses SVD of *transformed* matrices (WSe, WSv) not original W.
  - Quick check question: Given W ∈ ℝ^{m×n}, can you write its SVD and explain what happens if you zero out σ₂?

- Concept: **Statistical Parity / Demographic Parity**
  - Why needed here: Target fairness notion; requires P(Ŷ|A=a₁) = P(Ŷ|A=a₂); measured via Kolmogorov-Smirnov (KS) distance in regression.
  - Quick check question: Why is statistical parity insufficient for fairness if base rates differ across groups?

- Concept: **Schatten p-norms**
  - Why needed here: Second-moment bound uses Schatten-4 norm ||WSv||₄ = (Σσ⁴ᵢ)^{1/4}; generalizes Frobenius (p=2) and spectral (p=∞) norms.
  - Quick check question: Compute Schatten-4 norm given singular values [3, 2, 1].

## Architecture Onboarding

- Component map:
  Pre-trained NN (L layers) -> Layer selector (l-th layer) -> [X₁, X₂ extraction at layer l] -> [Compute x̄₁, x̄₂, M] -> [Build Se, Sv] -> [SVD: WSe = UΣₑVᵀ, WSv = UΣᵥVᵀ] -> [Closed-form optimization: σ*ᵢ(ᵥ), σ*ᵢ(ₑ)] -> [Reconstruct W*ₑ] -> [Optional: OLS fine-tune last layer] -> Fair model

- Critical path:
  1. Feed data through network to collect X^{[l]}, X^{[l]}₁, X^{[l]}₂ at target layer
  2. Compute Se via Cholesky: (x̄₁ - x̄₂)ᵀ(x̄₁ - x̄₂) + εₑI
  3. Compute Sv via eigendecomposition of |M|
  4. SVD of WSv, optimize σ*ᵢ(ᵥ) per Theorem 4.2, reconstruct W*ᵥ
  5. SVD of W*ᵥSe, optimize σ*ᵢ(ₑ) per Theorem 4.1, reconstruct W*ₑ
  6. Replace W^{[l]} ← W*ₑ; fine-tune W^{[L]} via OLS

- Design tradeoffs:
  - Lower cₑ, cᵥ → better fairness (lower KS), worse MSE; tune on validation set
  - Layer choice: penultimate layer (L-1) used in experiments; earlier layers may propagate effects differently
  - εₑ: too small → numerical instability in Se inversion; paper uses 1e-5

- Failure signatures:
  - KS unchanged: cₑ, cᵥ too large (constraints inactive); check if σ*ᵢ ≈ σᵢ
  - MSE explodes: constraints too tight; check condition number of reconstructed W*
  - Newton-Raphson diverges: γ initialization poor; try bisection fallback

- First 3 experiments:
  1. **Sanity check**: Apply ESVDFair to synthetic data with X₁, X₂ ~ N(μ₁, Σ) and N(μ₂, Σ); verify KS→0 when cₑ→0, cᵥ→0 and μ₁=μ₂, Σ₁=Σ₂
  2. **Hyperparameter sweep**: On Law School dataset, grid search c̃ₑ ∈ {1.5, 5, 15, 50}, c̃ᵥ ∈ {5, 50, 150}; plot KS vs MSE tradeoff curve (replicate Figure 4)
  3. **Layer ablation**: Apply ESVDFair to layers 2, 3, 4 (instead of just L-1) on COMPAS; report whether earlier layers yield different KS/MSE tradeoffs

## Open Questions the Paper Calls Out
None

## Limitations
- Only evaluated on two datasets (Law School, COMPAS) with binary sensitive attributes
- Method assumes sufficient data in both groups for stable moment estimates, which may fail in imbalanced settings
- No theoretical guarantee exists for how far optimization can push fairness before accuracy catastrophically degrades

## Confidence
- **High confidence**: Mechanism 1 (mean disparity via Se) - supported by Theorem 3.1 and numerical experiments; Convex optimization framework (Theorems 4.1-4.2) - clear derivation and closed-form solutions provided
- **Medium confidence**: Mechanism 2 (covariance disparity via Sv) - theoretical bound exists but no ablation on M's conditioning; Method comparison (vs. PADP, Reweigh) - only two datasets, no statistical significance testing reported
- **Low confidence**: Generalization to multi-class/group fairness - paper explicitly restricts to binary groups; Inference-time efficiency - claim of no sensitive attribute needed at inference is true, but computational overhead of SVD and fine-tuning is not quantified

## Next Checks
1. **Ablation on Se vs. Sv contributions:** Run ESVDFair with only Se constraint (cᵥ=∞), only Sv constraint (cₑ=∞), and both; measure individual vs. joint impact on KS/MSE to validate mechanism independence
2. **Layer sensitivity analysis:** Apply ESVDFair to layers 1-4 individually on Law School; report whether early-layer transformations degrade accuracy more severely than layer L-1, and whether fairness gains correlate with layer depth
3. **Imbalanced group robustness:** Construct synthetic Law School data with 95/5 race split; apply ESVDFair and measure whether moment estimation instability (few samples in minority group) causes optimization to fail or produce biased W*