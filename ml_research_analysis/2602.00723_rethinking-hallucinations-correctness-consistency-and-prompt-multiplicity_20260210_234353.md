---
ver: rpa2
title: 'Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity'
arxiv_id: '2602.00723'
source_url: https://arxiv.org/abs/2602.00723
tags:
- prompt
- hallucination
- multiplicity
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces prompt multiplicity, a framework to quantify
  consistency in LLM factual hallucination evaluation. It finds that existing benchmarks
  exhibit high prompt multiplicity (over 50% inconsistency), with only 15-20% of model
  responses being consistently correct.
---

# Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity

## Quick Facts
- **arXiv ID:** 2602.00723
- **Source URL:** https://arxiv.org/abs/2602.00723
- **Reference count:** 40
- **Primary result:** Existing LLM factual hallucination benchmarks exhibit high prompt multiplicity (over 50% inconsistency), with only 15-20% of responses being consistently correct.

## Executive Summary
This paper introduces prompt multiplicity as a framework to quantify consistency in LLM factual hallucination evaluation. The study reveals that current benchmarks exhibit widespread inconsistency across prompt variations, with detection techniques primarily measuring consistency rather than correctness. The work provides an improved taxonomy of hallucination harms and demonstrates that RAG-based mitigation can introduce additional inconsistencies through prompt-sensitive retrieval. These findings challenge the validity of existing evaluation practices and highlight critical limitations in current detection and mitigation strategies.

## Method Summary
The paper evaluates LLM factual hallucination benchmarks through a prompt multiplicity framework that measures consistency alongside correctness. Using 8 MCQ benchmarks and 16 models across 6 families, the researchers create 10-50 prompt variations per question via demonstration shuffling, MCQ option shuffling, or T5 paraphrasing. They apply perplexity-based MCQ evaluation with length normalization to determine model outputs across variations. The framework categorizes responses into prompt-agnostic factuality, prompt-agnostic errors, and randomness based on a threshold τ=0.8 for prompt-sensitive classification. The study also examines RAG-based mitigation and hallucination detection techniques to understand their behavior under prompt multiplicity.

## Key Results
- Over 50% of questions across benchmarks exhibit prompt multiplicity with different outputs across prompt variations
- Only 15-20% of model responses are consistently correct across all prompt variations
- Detection techniques primarily measure consistency rather than correctness, showing statistical correlation with self-consistency but not with ground-truth correctness
- RAG-based mitigation introduces additional inconsistencies through prompt-sensitive retrieval performance

## Why This Works (Mechanism)
The paper's framework works by systematically varying prompts while maintaining semantic equivalence, then measuring output consistency to separate model behavior into distinct categories. By using length-normalized perplexity for MCQ evaluation and applying statistical thresholds, it quantifies the degree to which models exhibit prompt-sensitive behavior versus stable, prompt-agnostic responses.

## Foundational Learning
- **Prompt multiplicity**: The phenomenon where LLM outputs vary across semantically equivalent prompts. Needed to understand that current evaluation practices may conflate model uncertainty with genuine knowledge gaps. Quick check: Measure output consistency across 10+ prompt variations for the same question.
- **Self-consistency**: The probability that two random prompts yield the same output for a given question. Needed to quantify prompt sensitivity at the question level. Quick check: Compute pairwise agreement between outputs from different prompts.
- **Perplexity-based MCQ evaluation**: Using length-normalized perplexity to select answers from multiple choices. Needed as a simple, model-agnostic evaluation method. Quick check: Verify perplexity scores correlate with expected answer quality on a validation set.

## Architecture Onboarding

**Component Map:** Benchmarks -> Prompt Variations -> Model Responses -> Consistency Metrics -> Response Categorization

**Critical Path:** Prompt variations → Model responses → Self-consistency calculation → Threshold application → Category assignment

**Design Tradeoffs:** Simple perplexity-based evaluation trades accuracy for model-agnostic consistency measurement; automated paraphrasing trades semantic control for scalability

**Failure Signatures:** High variance in ambiguity scores suggests prompt variations may not be semantically equivalent; low self-consistency indicates prompt sensitivity rather than genuine knowledge

**First Experiments:** 1) Create 20 prompt variations for one benchmark question and measure output consistency; 2) Apply length-normalized perplexity to all options and verify the lowest score matches expected answer; 3) Vary τ threshold from 0.7 to 0.9 and observe category distribution changes

## Open Questions the Paper Calls Out

### Open Question 1
How can prompt multiplicity be operationalized for free-form generation benchmarks without relying on potentially unreliable LLM judges? The paper notes this is a key challenge because automated evaluation methods like LLM judges introduce their "own errors, biases, and multiplicity." This remains unresolved because defining consistency is straightforward in multiple-choice (discrete labels) but non-trivial for open text, where semantic equivalence is difficult to verify automatically. Evidence that would resolve it includes a new evaluation pipeline for open-ended text that isolates model inconsistency from judge inconsistency, or a theoretical proof that judge multiplicity can be mathematically disentangled from model multiplicity.

### Open Question 2
How does the semantic distance between a prompt and its paraphrase correlate with the resulting ambiguity scores? The paper's limitations section notes the use of automated paraphrasing without explicit semantic similarity checks and calls for "rigorous validation and ablation studies to examine how different types of paraphrasing... affect prompt multiplicity." This is unresolved because it's unclear if high ambiguity is driven by minor surface-level formatting changes or by larger semantic shifts introduced by the paraphrasing model. Evidence that would resolve it includes an ablation study plotting the semantic distance (e.g., embedding cosine similarity) of prompt variations against the observed ambiguity scores to identify a sensitivity threshold.

### Open Question 3
Do state-of-the-art hallucination detection methods overcome the misalignment identified in baselines to detect correctness rather than consistency? The paper states the analysis focused on baseline detection methods and leaves the "exploration of more complex methods and their potential impact on these trends for future work." This remains unresolved because while the paper demonstrates that simple uncertainty-based methods detect consistency, it's unverified if advanced architectures (e.g., using external knowledge verification) can decouple these signals. Evidence that would resolve it includes a comparative study showing that advanced detectors exhibit significantly higher statistical correlation with ground-truth correctness than with self-consistency metrics.

## Limitations
- Limited set of prompt variation techniques may not capture full space of prompt sensitivity
- Length-normalized perplexity metric may not align with actual model behavior or human evaluation standards
- RAG experiments use Wikipedia corpus snapshot that may not generalize to other retrieval contexts
- Detection techniques evaluated on default prompts alone may underestimate practical utility

## Confidence
- **High confidence**: Widespread prompt multiplicity (>50% ambiguity) across benchmarks and model families; detection techniques primarily measure consistency rather than correctness
- **Medium confidence**: Taxonomy of hallucination harms provides useful framework but boundaries may be subjective; RAG mitigation introduces inconsistencies but limited to Wikipedia retrieval
- **Low confidence**: Specific numerical estimates (15-20% consistently correct) depend heavily on chosen threshold and prompt variation techniques

## Next Checks
1. **Threshold sensitivity analysis**: Vary the τ threshold from 0.7 to 0.9 and measure how ambiguity and self-consistency change across benchmarks to test robustness of findings
2. **Alternative prompt variation methods**: Implement semantic-preserving prompt variations using controlled text generation or adversarial prompt engineering to verify observed multiplicity isn't method-dependent
3. **Cross-corpus RAG validation**: Repeat RAG experiments using different knowledge corpora (e.g., PubMed for Med-HALT) to verify retrieval inconsistencies generalize beyond Wikipedia