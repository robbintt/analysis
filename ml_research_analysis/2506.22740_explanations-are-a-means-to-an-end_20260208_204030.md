---
ver: rpa2
title: Explanations are a means to an end
arxiv_id: '2506.22740'
source_url: https://arxiv.org/abs/2506.22740
tags:
- explanation
- explanations
- decision
- value
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a decision-theoretic framework for evaluating
  explanations in machine learning. The authors formalize explanation value through
  three estimands: theoretic value (upper bound of achievable performance), human-complementary
  value (information not already captured by human judgment), and behavioral value
  (actual causal effect on human decisions).'
---

# Explanations are a means to an end

## Quick Facts
- arXiv ID: 2506.22740
- Source URL: https://arxiv.org/abs/2506.22740
- Reference count: 40
- This paper introduces a decision-theoretic framework for evaluating explanations in machine learning.

## Executive Summary
This paper introduces a decision-theoretic framework for evaluating explanations in machine learning. The authors formalize explanation value through three estimands: theoretic value (upper bound of achievable performance), human-complementary value (information not already captured by human judgment), and behavioral value (actual causal effect on human decisions). They apply this framework to analyze human-AI decision support and mechanistic interpretability tasks. In the medical treatment example, they find the theoretic value of explanation is about 25% of the baseline. For deception detection tasks, effective explanations like nearest-neighbor examples and heatmaps offer substantial independent theoretical value (roughly 0.4-0.5 times the total theoretic value) and moderate human-complementary value. The framework reveals that the gap between rational and behavioral performance indicates under-extraction of available information, suggesting room for improvement through better training or interface design.

## Method Summary
The framework formalizes explanation value through three estimands: theoretic value (R_X - R_∅), human-complementary value (R_X - R_AH), and behavioral value (ATE of explanation on human decisions). The method involves defining a decision task (state S, actions A, utility u), estimating an information model p(V×S) from evaluation datasets, coarsening high-dimensional signals using Algorithm 1 to prevent overfitting, computing rational benchmarks (R_X, R_Z, R_∅) via empirical posteriors, and estimating behavioral ATE from randomized user studies. The approach uses Bayesian decision theory and Blackwell's informativeness theorem to establish that explanations are "garblings" of features, providing an upper bound on achievable performance.

## Key Results
- Theoretic value of explanation in medical treatment example is about 25% of baseline performance
- Effective explanations like nearest-neighbor examples and heatmaps offer substantial independent theoretical value (0.4-0.5 times total theoretic value) in deception detection tasks
- The gap between rational and behavioral performance indicates under-extraction of available information, suggesting room for improvement through better training or interface design
- SAE-top-3's theoretic value plateaus but behavioral value keeps increasing, suggesting training could improve human use of explanations

## Why This Works (Mechanism)

### Mechanism 1: Rational Agent Benchmark as Upper Bound
- **Claim**: A Bayesian rational agent with access to features X cannot improve performance from an explanation Z, because explanations are deterministic functions of features and predictions.
- **Mechanism**: By Proposition 1 and Blackwell's theorem, Z is a "garbling" of X—no more informative than X itself. Thus R_X (rational agent performance with features) bounds all agents including humans with explanations.
- **Core assumption**: Agents have the cognitive capacity to compute optimal posteriors from raw features.
- **Evidence anchors**:
  - [Section 3]: "gaining access to the explanation does not improve the expected performance of the idealized agent" (Proposition 1)
  - [Section B]: Proof uses Blackwell's informativeness theorem
  - [corpus]: Related work on value of information (weak direct support; corpus focuses on explanation quality rather than bounds)
- **Break condition**: If human decisions encode private information not in X, R_X is no longer an upper bound (Section 4 addresses this via R_{A_H ∪ X}).

### Mechanism 2: Decomposition into Independent and Contextual Value
- **Claim**: Explanation value decomposes into what Z conveys directly about the state (Δ^ind-E) and what remains in X that Z could help humans extract (Δ^cont-E).
- **Mechanism**: Definitions 4–5 separate R_Z − R_∅ (independent) from R_X − R_Z (contextual). Example-based explanations in deception detection showed higher Δ^ind-E than saliency methods.
- **Core assumption**: The decomposition reflects how humans actually use explanations—some information is direct, some requires integration with features.
- **Evidence anchors**:
  - [Section 3, Definitions 4–5]: Formal decomposition
  - [Section 5.1, Figure 2]: Deception detection shows Example/Heatmap have Δ^ind-E ≈ 0.4–0.5 × Δ_E
  - [corpus]: No direct corpus support for this specific decomposition
- **Break condition**: If Z and X interact non-additively, the decomposition may not reflect causal contributions.

### Mechanism 3: Behavioral Gap Diagnoses Under-Extraction
- **Claim**: The gap between R_X and observed behavioral performance B indicates whether humans under-extract information from available signals.
- **Mechanism**: Compare B to R_X and Δ^ind-E. In the alignment audit, SAE-top-3's theoretic value plateaued but behavioral value kept increasing (Table 2), suggesting training could improve use.
- **Core assumption**: The gap reflects extraction failure, not inherent limitation of the explanation itself.
- **Evidence anchors**:
  - [Abstract]: "The framework reveals that the gap between rational and behavioral performance indicates under-extraction of available information"
  - [Section 5.2, Table 2]: Behavioral value increases from SAE-top-1 to SAE-top-5 despite theoretic plateau
  - [corpus]: Weak direct support; corpus discusses explanation quality but not gap analysis
- **Break condition**: If behavioral value exceeds theoretic value, task design is flawed (Section 4.2).

## Foundational Learning

- **Concept: Bayesian Decision Theory**
  - Why needed here: The entire framework rests on expected utility maximization and posterior updates given signals.
  - Quick check question: Given prior p(s) and likelihood p(v|s), how do you compute the optimal action?

- **Concept: Blackwell's Informativeness Theorem**
  - Why needed here: Proves why explanations are "garblings" and establishes the partial ordering of signals.
  - Quick check question: If signal V_1 can simulate V_2 via stochastic transformation, which is more informative?

- **Concept: Proper Scoring Rules**
  - Why needed here: Needed to evaluate belief accuracy and convert utility functions to equivalent belief-elicitation tasks.
  - Quick check question: Why does only a proper scoring rule incentivize truthful belief reporting?

## Architecture Onboarding

- **Component map**:
  - Decision task specification (state S, actions A, utility u) -> Information model p(V×S) estimated from evaluation dataset -> Coarsening module (Algorithm 1) -> Benchmark computer (R_X, R_Z, R_{A_H}) -> Behavioral estimator (Δ^behavioral_E via ATE from user study)

- **Critical path**: Estimate Δ_E (theoretic) -> check if Δ_E/R_∅ is non-trivial -> estimate Δ^compl_E from baseline human decisions -> run behavioral study -> compare B to R_X to diagnose under-extraction.

- **Design tradeoffs**:
  - Coarsening granularity (Algorithm 1): More clusters capture more signal but risk overfitting; use train-test gap constraint (δ).
  - Utility specification: When ambiguous, use robust analysis over V-shaped scoring rules (Section D).
  - Human private information: If X doesn't capture all human-observed features, use R_{A_H∪X} as bound.

- **Failure signatures**:
  - Δ_E ≈ 0: No potential for explanations; reconsider task.
  - Δ^ind-E << Δ^cont-E: Explanation alone is weak; must help humans extract from X.
  - Δ^behavioral_E > Δ_E: Task design issue; baseline humans are below R_∅.
  - Large train-test gap in coarsening: Overfitting; increase δ or reduce clusters.

- **First 3 experiments**:
  1. Before any user study, estimate Δ_E and Δ^ind-E/Δ_E on held-out data using Algorithm 1; if Δ_E/R_∅ < 0.1, reconsider task design.
  2. Elicit baseline human decisions (no explanation) on a pilot sample to estimate Δ^compl_E; compare to Δ_E to assess complementary potential.
  3. Run small behavioral study (n=50–100) comparing explanation types; compute Δ^behavioral_E and compare to Δ^ind-E and R_X − B to diagnose whether failure is explanation design or human extraction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specific training protocols or interface designs successfully close the gap between behavioral performance and the theoretic benchmark?
- Basis in paper: [explicit] Abstract and Section 5.2 state that the gap between rational and behavioral performance indicates "under-extraction of available information, suggesting room for improvement through better training or interface design."
- Why unresolved: The paper establishes methods to quantify the gap but does not propose or test specific interventions to reduce it.
- What evidence would resolve it: An empirical study demonstrating a significant reduction in the difference between $R_X$ and behavioral performance $B$ following a targeted intervention.

### Open Question 2
- Question: Does selecting explanations based on maximizing human-complementary value ($\Delta^{compl}_E$) in a pilot study lead to better behavioral outcomes than standard selection methods?
- Basis in paper: [explicit] Section 5.1 concludes that "By piloting on a sample of participants without explanations first, explanations could instead be selected to maximize human-complementary information, increasing knowledge gain."
- Why unresolved: The paper applies the framework retrospectively to existing studies but does not validate this proposed prospective workflow experimentally.
- What evidence would resolve it: A randomized controlled trial comparing decision performance when explanation methods are chosen via the proposed pilot workflow versus standard heuristic selection.

### Open Question 3
- Question: Do empirical gaps between rational and behavioral performance arise primarily from agents being "misinformed" (cognitive costs in belief formation) or "misoptimizing" (noise in action selection)?
- Basis in paper: [inferred] The paper introduces models for bounded rationality in Appendix E (Misinformed vs. Misoptimizing agents) to explain why explanations help, but the empirical demonstrations do not disentangle which mechanism drives the observed under-extraction of information.
- Why unresolved: The framework quantifies the performance gap but the behavioral data analyzed does not identify the specific cognitive failure mode causing the gap.
- What evidence would resolve it: Experiments varying cognitive load (affecting optimization) vs. information complexity (affecting belief formation) to see which factor mediates the behavioral value of explanations.

### Open Question 4
- Question: How sensitive are the theoretic and behavioral value estimands to the choice of clustering algorithm and hyperparameters used for coarsening high-dimensional signals?
- Basis in paper: [inferred] Section 3.1 and Appendix C introduce a coarsening algorithm (Algorithm 1) to prevent overfitting in high-dimensional settings, but the paper does not analyze how much the estimated values ($\Delta_E$) fluctuate based on the choice of clustering $C$ or the tolerance $\delta$.
- Why unresolved: The validity of the benchmarks relies on the coarsened representation preserving the "garbling" relationships without losing too much information, which is stated as a difficult trade-off but not robustly tested.
- What evidence would resolve it: A sensitivity analysis showing the variance of $\Delta_E$ across different clustering configurations and datasets with varying dimensionalities.

## Limitations

- The framework assumes explanations are deterministic functions of features and predictions, which may not hold for stochastic explanation methods
- The coarsening algorithm's sensitivity to hyperparameters (cluster numbers, δ thresholds) and its effect on benchmark estimates remains an open question
- The behavioral study results depend heavily on task design and human subject recruitment, with potential confounds from varying interfaces and explanation formats

## Confidence

- **High confidence**: Theoretical framework (Proposition 1, decomposition definitions) and the medical treatment example (Theorem 2 bounds)
- **Medium confidence**: Deception detection results and mechanistic interpretability alignment audit, given limited replication
- **Low confidence**: Generalizability across different explanation types and tasks not tested in the paper

## Next Checks

1. Replicate the theoretical decomposition framework on a new dataset with different explanation types (e.g., counterfactuals, prototypes) to verify Δ^ind-E and Δ^cont-E estimates hold
2. Conduct a controlled behavioral study varying only the explanation format while keeping interface constant to isolate format effects from presentation effects
3. Test the framework's sensitivity to coarsening hyperparameters by systematically varying K_x, K_z, and δ to establish robustness of benchmark estimates