---
ver: rpa2
title: Eliciting Language Model Behaviors with Investigator Agents
arxiv_id: '2502.01236'
source_url: https://arxiv.org/abs/2502.01236
tags:
- elicitation
- behaviors
- language
- investigator
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces investigator agents for behavior elicitation
  in language models. The core method trains models to generate prompts that elicit
  specific behaviors from target language models, using supervised fine-tuning, direct
  preference optimization (DPO), and an iterative Frank-Wolfe approach to discover
  diverse prompting strategies.
---

# Eliciting Language Model Behaviors with Investigator Agents

## Quick Facts
- arXiv ID: 2502.01236
- Source URL: https://arxiv.org/abs/2502.01236
- Reference count: 40
- One-line primary result: Investigators trained to generate prompts elicit behaviors from target models with 100% ASR on AdvBench Harmful Behaviors, 85% on hallucinations, and discover diverse interpretable strategies

## Executive Summary
This paper introduces investigator agents for behavior elicitation in language models. The core method trains models to generate prompts that elicit specific behaviors from target language models, using supervised fine-tuning, direct preference optimization (DPO), and an iterative Frank-Wolfe approach to discover diverse prompting strategies. The investigators successfully surface interpretable prompts for jailbreaking (100% attack success rate on AdvBench Harmful Behaviors), hallucinations (85% success rate), and aberrant behaviors, outperforming baselines while generating natural language rather than gibberish. The approach amortizes search cost during training and demonstrates strong transfer across different model sizes and types.

## Method Summary
The method trains an investigator model to generate prompts $x$ that elicit target behaviors $y$ from a target model $m$. The pipeline consists of three stages: (1) SFT training on prompt-response pairs generated by the target model, (2) DPO refinement using preference learning on elicitation success, and (3) Frank-Wolfe iterations to enforce diversity by penalizing previously discovered strategies. For rubric-based elicitation, a two-stage decomposition first infers responses satisfying the rubric, then generates prompts for those responses. The objective balances elicitation reward, fluency, and diversity through carefully tuned hyperparameters.

## Key Results
- 100% Attack Success Rate on AdvBench Harmful Behaviors using best-of-25 sampling
- 85% success rate for hallucination elicitation on TruthfulQA rubrics
- FW-full achieves entropy 1.432 vs. DPO's 0.400 while maintaining competitive ASR
- Strong in-family transfer (98% ASR from Llama-3.1 8B to Llama-3.3 70B) but poor cross-architecture transfer (16-28% on GPT-4o/Claude)

## Why This Works (Mechanism)

### Mechanism 1: Amortized Posterior Inference
Training an investigator model to map target behaviors to eliciting prompts amortizes expensive search during inference, enabling efficient behavior discovery across diverse targets. Rather than solving `argmax_x p_m(y|x)` separately for each target y (combinatorial search), the investigator learns to approximate the posterior `p(x|y)` via supervised fine-tuning followed by DPO. The objective in Equation 2 is equivalent to variational posterior approximation when `β₁ = β₂ = 1`.

### Mechanism 2: Frank-Wolfe Iteration for Diversity-Preserving Strategy Discovery
Iteratively penalizing previously discovered prompts forces exploration of qualitatively different elicitation strategies while maintaining high success rates. Algorithm 3 regularizes each DPO iteration against the aggregate of previous iterates: `r(x,y) = log p_m(y|x) - λ log p^(i-1)_θ(x|y)`. The first iteration learns repetition; subsequent iterations discover continuation, topic-heading, and summarization strategies.

### Mechanism 3: Two-Stage Decomposition for Open-Ended Rubric Elicitation
Decomposing rubric-based elicitation into (1) response inference from rubric, then (2) prompt inference from response, outperforms direct end-to-end approaches. Stage 1 learns `q(y|R,a)` maximizing verifier score `p_v(a|R,y)` using the same SFT→DPO→FW pipeline. Stage 2 applies the pre-trained string elicitation investigator.

## Foundational Learning

- **Variational Inference / ELBO Objective**
  - Why needed here: The paper frames investigator training as approximate posterior inference; Equation 2 is the ELBO when β₁=β₂=1. Understanding this explains why the objective balances elicitation success, diversity, and fluency.
  - Quick check question: If we set β₂=0, what term disappears from the objective, and what behavior might result? (Answer: Fluency regularization; prompts may become gibberish like GCG outputs)

- **Direct Preference Optimization (DPO)**
  - Why needed here: Core refinement method. DPO avoids learning a separate reward model by deriving preference directly from log-probability differences under the target model.
  - Quick check question: In Algorithm 2, why is the reference policy `p_ref` set to the current model `p_θ`, and what happens if regularization β is too low? (Answer: Prevents deviation from current policy; low β causes mode collapse)

- **Frank-Wolfe Optimization**
  - Why needed here: Explains why iterative penalization works. Frank-Wolfe linearizes one objective component while solving the other in closed form, enabling tractable optimization over distributions.
  - Quick check question: What does the step size schedule `η_i = 1/i` imply for the aggregate model `p_θ^(i)`? (Answer: Uniform average over all iterates, ensuring diversity)

## Architecture Onboarding

- **Component map:**
Target Model (p_m) -> Prompt (x) from Investigator -> Response (y) -> evaluated by Verifier (p_v) [rubric setting only]
Investigator (p_θ): Llama-3.1 8B (or 1B)
Input: Target behavior (string y OR rubric R, answer a)
Output: Prompt x
Training: SFT → DPO → Frank-Wolfe iterations

- **Critical path:**
1. SFT data collection (Algorithm 1): Sample prefixes x ~ P_SFT, decode y = p_m(x) greedily → 100K+ pairs
2. SFT training: Fine-tune investigator on reverse mapping p_θ(x|y) — establishes semantic prior
3. DPO refinement (Algorithm 2): Sample candidate prompts, rank by elicitation logprob, update with preference pairs
4. Frank-Wolfe iteration (Algorithm 3): Repeat DPO with penalty on previously discovered prompts
5. Inference: Sample k=25 candidates, rerank by elicitation score, return best

- **Design tradeoffs:**
  - Diversity vs. Performance: DPO maximizes elicitation but collapses to repetition (entropy 0.400); FW restores diversity (1.432) with modest performance cost
  - SFT dataset choice: WildChat contains real user prompts (may include attacks); UltraChat is synthetic — both work, but WildChat slightly better on target model
  - k (candidate samples): Paper uses k=25 at inference; higher k improves best-of-k ASR but increases latency

- **Failure signatures:**
  - Mode collapse: If DPO runs too long (10+ iterations), model may output gibberish (Appendix A.1, Figure 5)
  - Reward hacking: Verifier-based rubric scoring can be gamed; paper notes this limitation and suggests augmented verifiers
  - Poor transfer: Attacks transfer well within model family (Llama-3.1 → Llama-3.3: 98%) but poorly to GPT-4o/Claude (16-28%)

- **First 3 experiments:**
1. Validate on known ground-truth: Use FineWeb prefixes → suffixes → attempt recovery. Compare SFT vs. DPO vs. FW elicitation scores (Table 2). Verifies pipeline before adversarial use.
2. Ablate DPO iteration count: On AdvBench Harmful Strings, run 1-15 DPO iterations (Figure 5). Identify point of performance degradation to set T_DPO.
3. Test rubric elicitation on small subset: Pick 10 hallucination rubrics from TruthfulQA. Run full two-stage pipeline. Manually inspect whether responses actually contain misconceptions vs. gaming the verifier.

## Open Questions the Paper Calls Out
- Can investigator agents effectively leverage multi-turn interactions and external tools to elicit complex behaviors that are inaccessible via single-turn prompting?
- How can the elicitation pipeline be made robust against reward hacking when using Language Models (LLMs) as verifiers?
- What architectural or training factors enable high transferability of attacks within model families (e.g., Llama) but limit transfer across different families (e.g., GPT-4o)?

## Limitations
- Target Model Dependence: Strong transfer within Llama family (98% ASR) but poor generalization to other architectures (16-28% ASR on GPT-4o/Claude)
- Verifier Reliability: Two-stage rubric elicitation depends critically on verifier model accuracy, with potential reward hacking noted but not quantified
- Optimization Instability: Performance degradation with excessive DPO iterations (gibberish outputs) requires empirical tuning per target behavior

## Confidence
- High Confidence: The amortized inference mechanism and basic string elicitation results are well-supported with mathematically sound ELBO derivation and directly measurable ASR metrics
- Medium Confidence: Frank-Wolfe diversity claims are supported by entropy measurements and qualitative prompt analysis, but correlation between entropy and "interpretable diversity" could be more rigorously established
- Low Confidence: Two-stage rubric elicitation shows promising verifier scores but relies heavily on black-box verifier performance without validation that Stage 1 responses actually satisfy rubrics in human-interpretable ways

## Next Checks
1. **Cross-Architecture Transfer Test**: Run the investigator trained on Llama-3.1 8B against GPT-4o and Claude-3.5-Sonnet using the same AdvBench Harmful Behaviors dataset. Compare ASR to in-family transfer (Llama-3.1 → Llama-3.3) to quantify architectural generalization limits.
2. **Verifier Reliability Audit**: Take 50 rubric responses from Stage 1, have human annotators evaluate rubric satisfaction, and compare to verifier scores. Calculate precision/recall to quantify reward hacking risk.
3. **Entropy-Diversity Correlation Study**: Systematically vary the Frank-Wolfe regularization strength λ across [0.01, 0.1, 1.0] on a subset of harmful strings. Measure both ASR and prompt interpretability via human evaluation to establish the entropy-diversity performance tradeoff curve.