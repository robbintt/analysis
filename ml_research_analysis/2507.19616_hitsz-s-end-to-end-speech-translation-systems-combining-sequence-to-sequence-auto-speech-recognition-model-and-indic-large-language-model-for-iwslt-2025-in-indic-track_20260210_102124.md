---
ver: rpa2
title: HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence
  Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic
  Track
arxiv_id: '2507.19616'
source_url: https://arxiv.org/abs/2507.19616
tags:
- translation
- speech
- language
- indic
- end-to-end
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents HITSZ's submission to the IWSLT 2025 Indic
  track for speech-to-text translation between English and three Indic languages (Hindi,
  Bengali, and Tamil). The authors propose an end-to-end system that integrates Whisper,
  a pre-trained ASR model, with Krutrim, an Indic-specialized large language model.
---

# HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track

## Quick Facts
- **arXiv ID:** 2507.19616
- **Source URL:** https://arxiv.org/abs/2507.19616
- **Reference count:** 14
- **Primary result:** Achieved BLEU scores of 28.88 (en→Indic) and 27.86 (Indic→en)

## Executive Summary
This paper presents HITSZ's submission to the IWSLT 2025 Indic track for speech-to-text translation between English and three Indic languages (Hindi, Bengali, Tamil). The authors propose an end-to-end system that integrates Whisper, a pre-trained ASR model, with Krutrim, an Indic-specialized large language model. The system employs a modular fine-tuning approach using LoRA adapters to adapt these pre-trained components for speech translation tasks. The proposed method achieved average BLEU scores of 28.88 for English-to-Indic translations and 27.86 for Indic-to-English translations. The authors also investigated the Chain-of-Thought (CoT) method, which showed potential for significant improvements (e.g., 13.84 BLEU increase for Tamil-to-English) when successfully parsed, though challenges remain in ensuring consistent adherence to the required output format.

## Method Summary
The system uses a Whisper-large-v2 encoder (frozen) and Krutrim-1-instruct LLM (7B) connected via a trainable Q-Former module. Low-Rank Adaptation (LoRA) is employed for fine-tuning while keeping the base models frozen. For English-to-Indic translation, a two-stage training approach is used: first training on short audio segments (<400 characters), then on longer segments. For Indic-to-English translation, the Whisper encoder is unfrozen for the first epoch to better handle low-resource acoustic features. The Chain-of-Thought method is implemented by prompting the model to first generate a transcription followed by the translation.

## Key Results
- Achieved average BLEU scores of 28.88 for English-to-Indic translations
- Achieved average BLEU scores of 27.86 for Indic-to-English translations
- Chain-of-Thought method showed potential for significant improvements (e.g., 13.84 BLEU increase for Tamil-to-English) when successfully parsed
- CoT parsing success rate was approximately 66%, with 33-40% failure rate due to format adherence issues

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating a strong pre-trained speech encoder with a specialized LLM via a lightweight adapter enables effective end-to-end translation in low-resource settings without full model fine-tuning.
- **Mechanism:** The system freezes the Whisper encoder and Krutrim LLM, training only a Q-Former connection module and LoRA adapters. The Q-Former projects variable-length audio features into the LLM's fixed embedding space, aligning acoustic signals with the LLM's textual understanding.
- **Core assumption:** The pre-trained Whisper encoder provides sufficiently robust linguistic representations for Indic languages, and the primary learning challenge is aligning modalities rather than training the encoder or decoder from scratch.
- **Evidence anchors:**
  - [abstract]: "...frozen encoder and adapter-based fine-tuning strategy."
  - [section 4]: "...leverages the Whisper speech encoder... The transformed tokens are then passed to the Krutrim LLM... Low-Rank Adaptation (LoRA) is employed... encoders and LLM remain frozen."
  - [corpus]: Related work like SALMONN (cited in text) validates the "frozen encoder + lightweight adapter" pattern for multimodal tasks.
- **Break condition:** If the source speech contains acoustic features or languages significantly out-of-distribution for the frozen Whisper model, the Q-Former may fail to bridge the representation gap, leading to poor translation.

### Mechanism 2
- **Claim:** Utilizing a Large Language Model specifically pre-trained on Indic languages (Krutrim) improves translation quality for English-to-Indic and Indic-to-English pairs compared to generic multilingual models.
- **Mechanism:** By initializing the decoder with Krutrim (trained on 2 trillion tokens with Indic focus), the model possesses stronger priors for Indic syntax and semantics. This reduces the data required to achieve fluency during the speech-translation fine-tuning phase.
- **Core assumption:** The translation bottleneck in low-resource ST is partially due to the target language model's lack of specific cultural or linguistic context, which a specialized LLM can mitigate.
- **Evidence anchors:**
  - [abstract]: "...Krutrim, an Indic-specialized large language model..."
  - [section 4]: "Krutrim LLM... optimized for Indic language tasks... demonstrates strong performance across multilingual benchmarks... despite being relatively lightweight."
  - [corpus]: Corpus signals on "Parallel Corpora for Machine Translation in Low-resource Indic Languages" highlight data scarcity, reinforcing the need for strong LLM priors.
- **Break condition:** If the Krutrim LLM has specific tokenization mismatches or vocabulary gaps relative to the specific Indic dialects in the audio, alignment via LoRA may be insufficient to correct systematic errors.

### Mechanism 3
- **Claim:** Chain-of-Thought (CoT) prompting, specifically a "transcribe-then-translate" logic, has the potential to significantly improve translation quality by decomposing the task.
- **Mechanism:** Instead of direct speech-to-text translation, the model is prompted to first generate a transcription of the source audio, then generate the translation based on that transcription. This leverages the LLM's stronger text-to-text reasoning capabilities.
- **Core assumption:** The model can reliably perform the intermediate transcription step and adhere to the output format; the error rate of the ASR step is lower than the direct ST error rate.
- **Evidence anchors:**
  - [abstract]: "...Chain-of-Thought (CoT) method... showed potential for significant translation quality improvements (e.g. a 13.84 BLEU increase for Tamil-to-English)..."
  - [section 5.2]: "...fine-tuning the model to first produce a transcription... followed by the English translation... notable improvements in BLEU scores."
  - [corpus]: "Using Phonemes in cascaded S2S translation pipeline" suggests intermediate representations (like phonemes or text) can aid the process.
- **Break condition:** The paper explicitly states the mechanism is fragile: "challenges in ensuring the model consistently adheres to the required CoT output format." If the model omits the transcription step or formats it incorrectly, the parsing logic fails and the output is lost.

## Foundational Learning

- **Concept: Q-Former (Query Transformer)**
  - **Why needed here:** This is the bridge that converts raw audio embeddings from Whisper into tokens the Krutrim LLM can process. Understanding it is essential to debug modality alignment.
  - **Quick check question:** How does the Q-Former reduce the variable-length audio sequence into a fixed number of tokens for the LLM?

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** The system fine-tunes a 7B parameter LLM efficiently. LoRA allows updating the model's behavior without modifying the massive core weight matrices.
  - **Quick check question:** In LoRA, are the original pre-trained weights of the LLM modified during training, or are updates confined to smaller injected matrices?

- **Concept: Encoder Freezing vs. Fine-Tuning**
  - **Why needed here:** The paper uses a mixed strategy (frozen for En->Ind, partially unfrozen for Ind->En). Knowing when to freeze is critical for resource management and preventing catastrophic forgetting.
  - **Quick check question:** Why might unfreezing the encoder for Indic-to-English translation be necessary when it wasn't for English-to-Indic?

## Architecture Onboarding

- **Component map:**
  Audio -> Whisper Encoder -> Q-Former (compresses & aligns features) -> Krutrim Embeddings -> Krutrim + LoRA -> Text Translation

- **Critical path:**
  Audio -> Whisper Encoder -> Q-Former (compresses & aligns features) -> Krutrim Embeddings -> Krutrim + LoRA -> Text Translation

- **Design tradeoffs:**
  - **Efficiency vs. Accuracy:** Freezing the Whisper encoder saves significant GPU memory (allowing batch size 4 on short segments) but risks lower performance on low-resource Indic speech (addressed partially by unfreezing for 1 epoch in Indic->En).
  - **CoT Quality vs. Reliability:** CoT offers ~13 BLEU point gains on successful parses but suffers a ~33% failure rate in format adherence, making it risky for production without robust fallbacks.

- **Failure signatures:**
  - **CoT Format Drift:** Model outputs translation without the intermediate "Transcription:" tag, causing the parser to fail (observed in ~33-40% of cases).
  - **Memory OOM:** Long English audio clips (>400 chars transcription length) caused GPU memory overflow, necessitating a two-stage training strategy (short then long).

- **First 3 experiments:**
  1. **Baseline Alignment:** Freeze Whisper and Krutrim; train only the Q-Former and LoRA on short English-to-Hindi audio to establish a stable connector.
  2. **Encoder Ablation (Indic->En):** Compare fully frozen Whisper encoder vs. unfreezing Whisper for 1 epoch on Bengali-to-English to measure the gain in acoustic modeling for low-resource languages.
  3. **CoT Formatting Stress Test:** Implement the "transcribe-then-translate" prompt on the Dev set and measure the *parsing success rate* independently of BLEU scores to quantify reliability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the instruction-following capability of the specialized LLM be enhanced to ensure consistent adherence to the Chain-of-Thought (CoT) output format?
- **Basis in paper:** [explicit] The conclusion explicitly states that future work will focus on "enhancing the instruction-following capability... to facilitate the development of a Spoken Language Translation system utilizing the Chain-of-Thought (CoT) method."
- **Why unresolved:** The authors report that while CoT improves quality, they observed "challenges in ensuring the model consistently adheres to the required CoT output format," resulting in a parsing success rate of only ~66%.
- **What evidence would resolve it:** A modified training regimen or prompting strategy that yields a CoT parsing success rate greater than 95% without degrading translation quality.

### Open Question 2
- **Question:** What specific improvements to generation capabilities are needed to boost performance in English-to-Indic translation tasks?
- **Basis in paper:** [explicit] The conclusion identifies "improving its generation capabilities in Indic languages to boost performance in English-to-Indic translation tasks" as a primary direction for future work.
- **Why unresolved:** The paper notes the model relies on the Krutrim LLM for generation but does not propose a solution for the performance gap between Indic-to-English (avg 27.86) and English-to-Indic (avg 28.88) directions.
- **What evidence would resolve it:** Ablation studies showing which LLM components (e.g., vocabulary expansion, fine-tuning depth) specifically improve English-to-Indic BLEU scores.

### Open Question 3
- **Question:** Does the Chain-of-Thought (CoT) method provide a net quality improvement when applied to the entire test set, including instances where parsing fails?
- **Basis in paper:** [inferred] The authors report a BLEU increase of 13.84 for Tamil-to-English on "successfully parsed outputs," but acknowledge parsing fails 33% of the time.
- **Why unresolved:** The reported gains suffer from selection bias; it remains unclear if the system performs better on average than a non-CoT baseline when errors from failed generations are included.
- **What evidence would resolve it:** A comparison of average system-level BLEU scores between the CoT system (with fallback handling) and the baseline across the full, unfiltered development set.

## Limitations
- The Chain-of-Thought approach shows promising gains but suffers from 33-40% format adherence failures, limiting practical applicability
- Critical architectural details for the Q-Former connector are unspecified, hindering faithful reproduction
- Evaluation relies solely on BLEU scores without additional quality metrics like METEOR or COMET
- The two-stage training strategy introduces complexity that may affect optimization trajectories

## Confidence
**High Confidence:** The core architectural approach of combining frozen pre-trained components with lightweight trainable adapters is well-established and technically sound.

**Medium Confidence:** Specific hyperparameters and training schedules appear reasonable but lack optimal validation through ablation studies.

**Low Confidence:** Chain-of-Thought implementation details and failure rate quantification are insufficiently detailed to assess true impact and reliability.

## Next Checks
1. **Q-Former Architecture Validation:** Implement the Q-Former with multiple architectural configurations (varying window sizes, attention heads, and layer depths) and measure the impact on BLEU scores and training stability.

2. **CoT Reliability Quantification:** Conduct systematic experiments measuring the parsing success rate of CoT outputs across different prompt formulations and training strategies, analyzing failure patterns and developing automated validation tools.

3. **Cross-Lingual Generalization Test:** Evaluate the trained models on held-out speech data from different dialects or closely related languages not present in the training corpus to test generalization capability.