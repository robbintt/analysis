---
ver: rpa2
title: 'RiTeK: A Dataset for Large Language Models Complex Reasoning over Textual
  Knowledge Graphs in Medicine'
arxiv_id: '2410.13987'
source_url: https://arxiv.org/abs/2410.13987
tags:
- gene
- disease
- textual
- medical
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RiTeK is a new dataset for evaluating complex reasoning over medical
  textual knowledge graphs (TKGs). It addresses limitations of existing datasets by
  incorporating richer topological structures, diverse relational templates, and complex
  textual descriptions validated by medical experts.
---

# RiTeK: A Dataset for Large Language Models Complex Reasoning over Textual Knowledge Graphs in Medicine

## Quick Facts
- arXiv ID: 2410.13987
- Source URL: https://arxiv.org/abs/2410.13987
- Reference count: 40
- Primary result: New dataset for complex reasoning over medical TKGs with 6 topologies, 68 relational templates, validated by medical experts

## Executive Summary
RiTeK addresses limitations in existing medical TKG benchmarks by providing a dataset that combines rich topological structures with diverse relational templates and complex textual descriptions. The dataset synthesizes realistic user queries that integrate relational and textual information across six reasoning topologies, including multi-hop and constraint-based patterns. Evaluation of 11 retrieval models demonstrates that current methods struggle with textual-relational integration and complex reasoning under attribute constraints, highlighting the need for improved retrieval systems tailored to semi-structured medical data.

## Method Summary
RiTeK constructs medical TKGs from PharmKG and ADint, enriched with textual descriptions from Ensembl, UMLS, and Mondo Disease Ontology. Queries are synthesized using a 5-step pipeline: relational template construction, textual property extraction, GPT-4 combination, answer filtering, and expert evaluation. The dataset covers 68 relational templates in RiTeK-PharmKG and 58 in RiTeK-ADint, with higher instance rates than prior benchmarks. Evaluation uses Exact Match and ROUGE-1 metrics across 11 retrievers including zero-shot, few-shot, and supervised methods.

## Key Results
- Instance rates (11.33 for PharmKG, 9.67 for ADint) exceed STaRK-Prime (9.3), indicating greater diversity
- Current retrieval models struggle with textual-relational integration, especially under attribute constraints
- TOG achieves strong gains in few-shot settings (37.11 ROUGE-1 F1 on RiTeK-ADint) but shows limited zero-shot performance
- All models fail on indirect/phenotypic associations like CHI3L1→schizophrenia case studies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating textual descriptions with structured relations in TKGs creates more realistic query scenarios that expose reasoning gaps in current retrievers.
- **Mechanism:** Queries combine relational constraints (e.g., "Anti-Bacterial Agents causes pathologic function") with textual properties extracted from entity documents, requiring models to satisfy both structural and semantic conditions simultaneously.
- **Core assumption:** Medical reasoning naturally involves both relational (drug–disease interactions) and textual (symptom descriptions, biomarker characteristics) information.
- **Evidence anchors:**
  - [abstract]: "we synthesize realistic user queries integrating diverse topological structures, relational information, and complex textual descriptions"
  - [section 4.2.2]: "the core idea is to intertwine relational information and textual properties within the queries, accurately constructing ground-truth answers that exhibit more complex topological structures"
  - [corpus]: Related work (RAR², DoctorRAG) confirms retrieval-augmented reasoning is critical for medical domains, though corpus papers focus more on general RAG than TKG-specific structures
- **Break condition:** If queries become too complex (>3-hop with multiple textual constraints), even supervised methods like GCR achieve <60% ROUGE-1 F1, indicating a performance ceiling

### Mechanism 2
- **Claim:** Structured reasoning paths over TKGs improve retrieval over pure LLM prompting because they constrain the search space to valid graph paths.
- **Mechanism:** Methods like TOG perform beam search over knowledge graphs, evaluating promising reasoning paths rather than relying on LLM internal knowledge, which is often incomplete or hallucinated.
- **Core assumption:** The knowledge graph contains sufficient coverage of relevant medical relationships.
- **Evidence anchors:**
  - [section 5.2]: "TOG performs moderately in zero-shot settings but shows strong gains in few-shot scenarios, achieving top-tier ROUGE-1 F1 scores like 37.11 on RiTeK-ADint"
  - [section 5.2]: "For GPT-4 and GPT+COT... it still relies on the inherent knowledge of the LLM, which limits its ability to apply clear logical reasoning based on knowledge graphs"
  - [corpus]: Plan of Knowledge paper (FMR=0.528) validates retrieval-augmented approaches for TKG reasoning, supporting the general mechanism
- **Break condition:** When relations are rare or indirect (e.g., CHI3L1→schizophrenia via biomarkers), path-based methods fail—case study shows TOG and MCTS predict Alzheimer's instead

### Mechanism 3
- **Claim:** Instance rate (relational templates per topological structure) drives dataset difficulty—higher rates require models to generalize across more diverse reasoning patterns.
- **Mechanism:** RiTeK's instance rates (11.33 for PharmKG, 9.67 for ADint) exceed STaRK-Prime (9.3), meaning each topology has more varied relational instantiations, forcing models to learn reasoning patterns rather than memorize templates.
- **Core assumption:** Higher instance rate correlates with greater reasoning challenge (not empirically validated in paper).
- **Evidence anchors:**
  - [section 4.2.1]: "The instance rate of 11.33, which is higher than that of the current TKG dataset STaRK... highlights the greater diversity of our dataset"
  - [table 2]: Shows explicit comparison of instance rates across datasets
  - [corpus]: Weak direct evidence—corpus papers don't discuss instance rate as a metric
- **Break condition:** Assumption untested—paper doesn't ablate instance rate effects, so this mechanism is plausible but not proven

## Foundational Learning

- **Concept: Textual Knowledge Graph (TKG)**
  - **Why needed here:** RiTeK is built on TKGs (not standard KGs), where each entity has associated textual documents
  - **Quick check question:** Can you explain how a TKG differs from a standard knowledge graph in terms of what information is stored at each node?

- **Concept: Multi-hop Reasoning with Constraints**
  - **Why needed here:** RiTeK queries require traversing 2-3 relational hops while satisfying textual constraints (e.g., "which drug treats inflammatory diseases?" combines path reasoning with disease type filtering)
  - **Quick check question:** Given a 2-hop query "Gene→interacts→Chemical→treats→Disease" with constraint "Disease type=neurodegenerative," what must the retriever check?

- **Concept: Topological Structures for Reasoning**
  - **Why needed here:** RiTeK defines 6 topologies (1-hop, 2-hop, 3-hop, 1-hop+constraint, 2-hop+constraint, two-to-one converging) that determine query complexity
  - **Quick check question:** Which topology requires finding an entity that satisfies two independent relational paths simultaneously?

## Architecture Onboarding

- **Component map:** TKG Construction Layer -> Query Generation Pipeline -> Retrieval Evaluation Framework
- **Critical path:** TKG construction → Template instantiation (medical expert validates) → Textual property extraction (GPT-4) → Query synthesis (GPT-4 with persona prompts) → Multi-LLM answer filtering → Human evaluation (4 experts, 5-point Likert scale)
- **Design tradeoffs:**
  - Higher instance rate (more templates per topology) increases diversity but raises annotation cost
  - GPT-4 query synthesis ensures naturalness but introduces LLM bias; expert evaluation mitigates but doesn't eliminate
  - Limiting to ≤3 ground-truth answers increases query precision but may exclude valid multi-answer cases
- **Failure signatures:**
  - Random Walk: Random path selection fails on constrained queries (low precision)
  - ToT/GoT: No external knowledge access → hallucination on rare relations
  - G-Retriever: Weak on complex relational constraints despite interpretable subgraph selection (ROUGE-1 F1 5.17 vs. KAR 11.12 on STaRK-Prime)
  - All models: Struggle with indirect/phenotypic associations (CHI3L1→schizophrenia case)
- **First 3 experiments:**
  1. **Baseline sanity check:** Run GPT-4 (zero-shot) and Random Walk (depth=3) on RiTeK-PharmKG validation split—expect EM <15% and observe where textual constraints cause failures
  2. **Retrieval method comparison:** Test KAR vs. TOG vs. GNN-RAG on all 6 topological structures separately—identify which topologies show largest performance gaps
  3. **Ablation on textual properties:** Remove textual descriptions from queries (keep only relational constraints) and compare KAR performance—quantify how much textual integration adds to difficulty

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can retrieval systems effectively determine the optimal number of top-n documents and their selection order when using knowledge-aware query expansion (like KAR) on medical TKGs?
- Basis in paper: [explicit] The authors state that KAR relies on retrieving top-n relevant documents, but "determining an appropriate value for n and the optimal order in which to select documents is nontrivial."
- Why unresolved: The paper demonstrates KAR's strong performance but does not investigate or propose methods for adaptively selecting n or ranking document relevance in the medical domain.
- What evidence would resolve it: Systematic experiments varying n values across different query types and topological structures, with analysis of optimal selection strategies per structure.

### Open Question 2
- Question: How can retrieval models better capture critical reasoning information embedded in complex or indirect graph structures beyond shortest paths?
- Basis in paper: [explicit] The authors note GNN-RAG "primarily relies on shortest paths, it may overlook critical reasoning information embedded in more complex or indirect graph structures."
- Why unresolved: Current graph neural retrieval methods prioritize path efficiency, but the RiTeK dataset shows that real medical reasoning often requires navigating non-obvious relational chains.
- What evidence would resolve it: Development and evaluation of retrieval methods that explicitly model multi-path reasoning and indirect associations, with performance gains on complex topology queries.

### Open Question 3
- Question: How do models handle queries involving multiple topic entities compared to the single-topic-entity constraint in RiTeK?
- Basis in paper: [explicit] The authors state in Limitations: "RiTeK is currently limited to queries that involve only a single topic entity... Future work should explore the inclusion of multiple topic entities."
- Why unresolved: Real-world medical queries may reference multiple entities simultaneously, but current benchmarks and models are designed for single-entity scenarios.
- What evidence would resolve it: Construction of multi-topic-entity benchmarks and evaluation of whether existing models generalize or require architectural modifications.

### Open Question 4
- Question: What is the impact of incorporating additional modalities (e.g., images) on retrieval and reasoning performance over medical TKGs?
- Basis in paper: [explicit] The authors explicitly call for incorporating "additional modalities, such as images, to enable a more comprehensive and robust information retrieval system."
- Why unresolved: Medical knowledge is inherently multimodal (text, images, lab results), but RiTeK and evaluated models operate solely on textual and structural information.
- What evidence would resolve it: Multi-modal extensions to RiTeK with performance comparisons between text-only and multi-modal retrieval systems.

## Limitations

- GPT-4 synthesis introduces potential LLM bias that expert evaluation cannot fully eliminate
- All models struggle with indirect/phenotypic associations, showing performance ceilings around 60% ROUGE-1 F1
- Current focus on single-topic-entity queries limits real-world applicability
- Limited investigation of optimal hyperparameters for supervised models on this specific dataset scale

## Confidence

- Mechanism 1 (textual-relational integration exposing reasoning gaps): **High** - directly supported by comparison with STaRK and observed performance drops
- Mechanism 2 (structured reasoning paths outperforming LLM prompting): **Medium** - supported by TOG results but limited by small zero-shot performance gap
- Mechanism 3 (instance rate driving difficulty): **Low** - plausible but untested assumption

## Next Checks

1. **Instance rate ablation**: Create reduced versions of RiTeK with fewer templates per topology and measure impact on retrieval performance to test if diversity directly correlates with difficulty
2. **Textual constraint isolation**: Remove textual descriptions from a subset of queries and re-run KAR evaluation to quantify how much textual integration adds to the reasoning challenge
3. **Zero-shot scalability test**: Evaluate whether the performance gap between LLM prompting and structured methods widens or narrows as query complexity increases beyond 3-hop patterns