---
ver: rpa2
title: Data-Driven Falsification of Cyber-Physical Systems
arxiv_id: '2505.03863'
source_url: https://arxiv.org/abs/2505.03863
tags:
- falsification
- leaf
- time
- node
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a data-driven falsification framework for
  Cyber-Physical Systems (CPS) that leverages machine learning models as surrogates.
  The framework employs two complementary strategies: a neural network-based approach
  using adversarial attack algorithms and DNN verification tools, and a decision tree-based
  approach guided by explanations of safety violations.'
---

# Data-Driven Falsification of Cyber-Physical Systems

## Quick Facts
- arXiv ID: 2505.03863
- Source URL: https://arxiv.org/abs/2505.03863
- Authors: Atanu Kundu; Sauvik Gon; Rajarshi Ray
- Reference count: 40
- Key outcome: FLEXI FAL successfully falsified 33 out of 37 benchmark instances from ARCH-COMP 2024, including challenging cases with low falsifying trajectory density that other tools failed to solve.

## Executive Summary
This paper presents a data-driven falsification framework for Cyber-Physical Systems (CPS) that leverages machine learning models as surrogates. The framework employs two complementary strategies: a neural network-based approach using adversarial attack algorithms and DNN verification tools, and a decision tree-based approach guided by explanations of safety violations. The decision tree method shows superior performance in finding multiple counterexamples efficiently. FLEXI FAL successfully falsified 33 out of 37 benchmark instances from the ARCH-COMP 2024 competition, including challenging cases with low falsifying trajectory density that other tools failed to solve.

## Method Summary
The framework combines data-driven approaches with machine learning surrogates to falsify CPS models. It implements two falsification strategies: one based on neural networks with adversarial attacks and DNN verification, and another using decision trees with explanation-guided search. The decision tree approach employs gradient boosting to identify conditions that lead to safety violations, enabling efficient discovery of multiple counterexamples. The framework integrates these strategies into a unified tool called FLEXI FAL, which can be applied to various CPS benchmarks and safety properties.

## Key Results
- Successfully falsified 33 out of 37 benchmark instances from ARCH-COMP 2024
- Decision tree-based falsification achieved 10/10 falsification rate for most instances
- Significantly fewer simulations required compared to finding single counterexamples
- Solved challenging cases with low falsifying trajectory density that other tools failed to address

## Why This Works (Mechanism)
The decision tree approach works effectively because it provides interpretable explanations of safety violations, allowing the falsification process to systematically explore regions of the input space that lead to counterexamples. By leveraging gradient boosting and feature importance scores, the method can efficiently guide the search toward multiple falsification regions. The combination of multiple falsification strategies provides complementary coverage, with the decision tree approach excelling at finding multiple counterexamples while the DNN-based method can handle certain specific scenarios.

## Foundational Learning
- Cyber-Physical Systems (CPS): Integration of computation and physical processes. Why needed: Understanding the target domain where falsification is applied.
- Falsification: Searching for inputs that violate safety properties. Why needed: Core problem being addressed by the framework.
- Adversarial attacks on neural networks: Techniques to find inputs that cause misclassification. Why needed: Basis for the DNN-based falsification strategy.
- Decision trees and gradient boosting: Machine learning models for classification and regression. Why needed: Foundation for the explanation-guided falsification approach.
- DNN verification: Formal methods to verify properties of neural networks. Why needed: Complements the adversarial attack approach in the DNN strategy.

## Architecture Onboarding

Component map: Input space -> ML surrogate models -> Falsification strategies (DNN-based, Decision tree-based) -> Counterexamples -> Verification

Critical path: Input sampling → ML model prediction → Falsification strategy execution → Counterexample validation → Feedback to input space exploration

Design tradeoffs: The framework trades computational overhead from ML model training and inference against the ability to efficiently explore complex input spaces and find multiple counterexamples.

Failure signatures: Poor performance when ML surrogates poorly approximate the true system dynamics, when safety properties are too complex for the explanation-guided approach, or when adversarial attack algorithms fail to find effective perturbations.

First experiments:
1. Apply the framework to a simple linear CPS model with known safety violations
2. Compare falsification performance on a benchmark with dense vs. sparse counterexample regions
3. Evaluate the computational overhead of each falsification strategy on a medium-complexity CPS model

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Performance gap between DNN-based and decision tree falsification strategies not fully explained
- Benchmark evaluation limited to specific ARCH-COMP 2024 instances, may not generalize
- Lack of detailed analysis on computational overhead scaling with system complexity

## Confidence

High confidence: The decision tree-based falsification approach successfully finding multiple counterexamples with fewer simulations

Medium confidence: The overall superiority of the decision tree approach over DNN-based methods

Medium confidence: The framework's ability to solve previously unsolved instances from ARCH-COMP 2024

## Next Checks

1. Conduct ablation studies comparing the performance of each falsification strategy across different types of CPS models and safety properties to better understand their respective strengths and weaknesses

2. Evaluate the framework's performance on a broader set of benchmark problems beyond the ARCH-COMP 2024 instances, including models with varying levels of complexity and different types of dynamics

3. Analyze the computational overhead of the machine learning models in the falsification process and assess how this scales with system size and complexity