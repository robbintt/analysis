---
ver: rpa2
title: 'Chitrarth: Bridging Vision and Language for a Billion People'
arxiv_id: '2502.15392'
source_url: https://arxiv.org/abs/2502.15392
tags:
- arxiv
- languages
- language
- english
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Chitrarth addresses the gap in multimodal AI for low-resource
  languages by integrating a multilingual LLM backbone with vision encoders and training
  on translated datasets. The model employs a two-stage training process: feature
  alignment using image-text pairs, followed by instruction tuning on multilingual
  academic and culturally diverse datasets.'
---

# Chitrarth: Bridging Vision and Language for a Billion People

## Quick Facts
- arXiv ID: 2502.15392
- Source URL: https://arxiv.org/abs/2502.15392
- Reference count: 12
- Chitrarth is the first Vision-Language Model for 10 Indic languages and English

## Executive Summary
Chitrarth addresses the critical gap in multimodal AI for low-resource languages by integrating a multilingual LLM backbone with vision encoders and training on translated datasets. The model employs a two-stage training process: feature alignment using image-text pairs, followed by instruction tuning on multilingual academic and culturally diverse datasets. Chitrarth achieves state-of-the-art results on three out of five English academic benchmarks and sets performance baselines for low-resource Indic languages, outperforming prior models in multilingual visual reasoning and comprehension tasks.

## Method Summary
Chitrarth uses a two-stage training approach with a Krutrim LLM backbone and SigLIP-SO400M vision encoder. Stage 1 trains a projector layer to map visual features to the LLM embedding space using 1.2M translated image-text pairs. Stage 2 performs instruction tuning on mixed English and Indic datasets including LLaVA-Instruct-150K, Cauldron, and proprietary culturally diverse Indian data. The model is trained on 8xH100 GPUs with specific hyperparameters: AdamW optimizer, cosine learning rate schedule, 2e-3 LR for Stage 1, and 2e-5 LR for Stage 2.

## Key Results
- Chitrarth achieves state-of-the-art performance on three out of five English academic benchmarks
- Establishes first multilingual visual reasoning baselines for ten Indic languages through BharatBench
- SigLIP-SO400M vision encoder consistently outperforms CLIP ViT-L/14 by 11-13 points on key benchmarks
- Introduces comprehensive evaluation framework for assessing multilingual VLMs across culturally diverse datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translating English multimodal datasets to target languages enables multilingual visual reasoning capabilities without requiring native multimodal data.
- Mechanism: The model learns visual concepts through English image-text pairs, then learns to express those concepts in target languages via translated instruction-tuning data. Cross-lingual transfer occurs because visual representations are language-agnostic while language generation adapts to the LLM backbone's multilingual capabilities.
- Core assumption: Visual grounding transfers across languages when the LLM backbone already possesses multilingual text generation capabilities.
- Evidence anchors:
  - [abstract] "training on translated datasets" enables bridging vision and language for low-resource languages
  - [Section 4] ShareGPT4V-PT translated to 10 Indic languages using IndicTrans2; instruction tuning includes translated LLaVA-Instruct-150K
  - [corpus] Limited corpus support for translation-based multilingual transfer; AWED-FiNER shows translation approaches for NER across languages but not multimodal
- Break condition: Translation quality degrades significantly for culturally-specific visual concepts that lack direct linguistic equivalents; translation artifacts may compound visual grounding errors.

### Mechanism 2
- Claim: Freezing the vision encoder during both training stages while training only the projection layer preserves visual representation quality while enabling efficient adaptation.
- Mechanism: A pre-trained vision encoder (SigLIP-SO400M or CLIP ViT-L/14) extracts fixed visual features. The modality projector (single-layer or two-layer MLP) learns to map 14×14 patch embeddings (729 tokens → 576 tokens after projection) into the Krutrim LLM's embedding space. The LLM generates responses autoregressively conditioned on visual tokens.
- Core assumption: Pre-trained vision encoders contain sufficient visual representations that don't require domain-specific adaptation for Indian cultural contexts.
- Evidence anchors:
  - [Section 3] "we conduct pre-training using image-text pairs, with the projector layer being trained while keeping the vision encoder and LLM fixed"
  - [Section 6.3] SigLIP-SO400M consistently outperforms CLIP ViT-L/14@336px across all English benchmarks, with 11-point improvement on TextVQA and 13-point improvement on LLaVA-Bench
  - [corpus] No direct corpus evidence for frozen encoder effectiveness in multilingual VLMs
- Break condition: Visual concepts specific to Indian cultural contexts (monuments, clothing, art styles) may not be well-represented in vision encoders pre-trained primarily on Western image datasets.

### Mechanism 3
- Claim: A multilingual LLM backbone trained from scratch on Indic languages provides stronger language understanding than adapting English-centric LLMs.
- Mechanism: Krutrim LLM (supporting 10 Indic languages + English) serves as the autoregressive backbone. Its pre-training on Indic language data enables native understanding of linguistic nuances rather than relying on post-hoc adaptation of English-centric models.
- Core assumption: The LLM backbone's multilingual text capabilities transfer directly to multimodal generation without catastrophic forgetting.
- Evidence anchors:
  - [Section 1] "Our model builds upon the recent success of Krutrim LLM [Kallappa et al., 2024], which supports English and 10 other languages"
  - [Section 2] Prior Indic LLMs "extend and fine-tune text-only English-centric LLMs...naturally, they fail to fully capture the nuances of the language"
  - [corpus] Krutrim LLM paper (arXiv:2502.09642) provides foundational evidence for multilingual backbone effectiveness
- Break condition: Multimodal fine-tuning may degrade text-only capabilities; expanding language coverage in training data decreases English academic benchmark performance (Figure 8 shows trade-off).

## Foundational Learning

- Concept: **Autoregressive Vision-Language Models**
  - Why needed here: Chitrarth generates responses token-by-token conditioned on both visual tokens and text instructions; understanding autoregressive decoding explains why image tokens occupy fixed context positions (576/4096 tokens).
  - Quick check question: Can you explain why visual tokens must be inserted before text tokens in the context window for autoregressive generation?

- Concept: **Feature Alignment vs. Instruction Tuning**
  - Why needed here: The two-stage training separates learning to map visual features to language space (Stage 1) from learning task-specific behaviors (Stage 2); ablations show each stage serves distinct purposes.
  - Quick check question: What would happen if you skipped Stage 1 and went directly to instruction tuning?

- Concept: **Cross-Lingual Transfer**
  - Why needed here: The model achieves multilingual capabilities primarily through translated data rather than native multimodal corpora; understanding transfer helps predict failure modes.
  - Quick check question: Why might translating "peacock feather" from English to Hindi preserve meaning better than translating a description of baseball?

## Architecture Onboarding

- Component map: Image Input → Vision Encoder (SigLIP-SO400M / CLIP ViT-L/14) → Modality Projector (2-layer MLP with GELU) → Visual Tokens (576 positions) + Text Tokens → Krutrim LLM (4096 context) → Autoregressive Output

- Critical path:
  1. Verify vision encoder outputs (729 tokens for 14×14 patches at 336px)
  2. Confirm projector compression (729 → 576 tokens) maintains spatial relationships
  3. Validate visual tokens consume correct context positions (0-575) before text tokens
  4. Ensure Krutrim tokenizer handles all 11 languages consistently

- Design tradeoffs:
  - **SigLIP vs. CLIP**: SigLIP converges faster and outperforms CLIP by 11-13 points on key benchmarks, but may have different failure modes for low-level visual features
  - **Frozen vs. unfrozen vision encoder**: Freezing preserves pre-trained features but limits adaptation to Indian visual concepts; paper notes unfreezing as future work
  - **Language balance in training**: Equal sampling (65K per language) ensures linguistic diversity but reduces per-language data; Figure 8 shows English performance degrades with multilingual expansion

- Failure signatures:
  - **Hallucination on cultural concepts**: Model may generate plausible but incorrect descriptions of unfamiliar Indian visual elements (mitigated by culturally diverse Stage 2 data)
  - **Translation artifact propagation**: Poor translations in Stage 1 compound through Stage 2; validate with human evaluation on samples
  - **Catastrophic forgetting**: Multilingual training reduces English benchmark performance; monitor VQAv2/GQA scores during multilingual expansion

- First 3 experiments:
  1. **Vision encoder ablation**: Train identical models with SigLIP-SO400M and CLIP ViT-L/14, compare convergence speed and benchmark scores to reproduce Figure 7 results.
  2. **Language coverage sweep**: Train models with English-only, English+Hindi, and all 11 languages; plot English benchmark degradation against multilingual capability gains (reproduce Figure 8).
  3. **Translation quality validation**: Sample 100 translated instruction pairs per language; conduct human evaluation comparing IndicTrans2 output against reference translations to quantify translation error bounds.

## Open Questions the Paper Calls Out

- Question: How can the cultural biases and misrepresentations introduced by automated translation pipelines be effectively mitigated in multilingual VLMs?
- Basis in paper: [explicit] The authors state in the "Limitations and Future Work" section that the automated translation pipeline using IndicTrans2 may introduce LLM biases, potentially misrepresenting cultural symbols and gestures.
- Why unresolved: The current work relies on automated translation for scalability but acknowledges that correcting these specific nuances requires targeted evaluation and training strategies not yet implemented.
- What evidence would resolve it: A comparative study showing reduced cultural hallucination rates and higher human evaluation scores for culturally specific images after implementing a bias-mitigation technique (e.g., human-in-the-loop verification or culture-aware loss functions).

- Question: Does unfreezing the vision encoder during the training of multilingual VLMs significantly enhance representation learning for low-resource languages?
- Basis in paper: [explicit] The paper notes that the vision encoder was kept frozen, but recent research suggests unfreezing it could enhance representation learning, stating, "We plan to investigate this approach in future work."
- Why unresolved: The authors restricted training efficiency and stability by freezing the encoder; the potential gains in visual-linguistic alignment for Indic languages remain untested.
- What evidence would resolve it: Ablation studies comparing the performance of Chitrarth with a frozen versus unfrozen vision encoder on BharatBench, specifically analyzing convergence speed and visual reasoning accuracy.

- Question: What data composition strategies can resolve the performance trade-off between English academic benchmarks and low-resource multilingual capabilities?
- Basis in paper: [inferred] Figure 8 and the results section highlight that expanding language coverage improves multilingual scores but decreases performance on academic English datasets, a challenge the authors explicitly identify as a key balancing act.
- Why unresolved: The paper establishes the existence of this "curse of multilinguality" in the VLM context but does not propose a solution to maintain peak English performance while scaling to ten other languages.
- What evidence would resolve it: Experimentation with language-specific adapters or dynamic data sampling ratios that result in statistically non-inferior performance on English benchmarks (like VQA-v2) while showing gains on Indic benchmarks.

## Limitations

- **Translation Quality Dependency**: The entire multilingual capability relies on IndicTrans2 translation quality, which varies significantly across the 10 Indic languages and may compound errors through training.
- **Cultural Representation Gap**: Vision encoders pre-trained on Western datasets may lack adequate representation of Indian cultural concepts, potentially limiting visual reasoning accuracy.
- **English Performance Trade-off**: Expanding language coverage degrades English benchmark performance, creating a hard constraint on multilingual expansion.

## Confidence

**High Confidence**: Claims about two-stage training effectiveness, SigLIP-SO400M outperforming CLIP ViT-L/14, and BharatBench framework validity. These are directly supported by empirical results and ablation studies within the paper.

**Medium Confidence**: Claims about translation-based multilingual transfer sufficiency and cultural diversity dataset effectiveness. While supported by results, these rely on unvalidated assumptions about translation quality and cultural representation.

**Low Confidence**: Claims about frozen vision encoder preserving visual quality for Indian contexts and Krutrim LLM's native multilingual capabilities transferring to multimodal tasks without degradation. These lack direct empirical validation or comparison with unfrozen encoder alternatives.

## Next Checks

1. **Translation Quality Validation**: Sample 200 translated instruction pairs across all 10 Indic languages and conduct blind human evaluation comparing IndicTrans2 output against professional translations. Quantify translation error rates and correlate with downstream performance degradation per language.

2. **Cultural Concept Recognition Test**: Create a benchmark of 500 images containing distinctly Indian cultural elements (festivals, clothing, architecture, food) that may not appear in Western datasets. Evaluate Chitrarth's ability to accurately describe these concepts versus a baseline model fine-tuned only on translated English data.

3. **English Performance Stability Analysis**: Systematically train models with increasing numbers of Indic languages (English only, English+Hindi, English+Hindi+Bengali, etc.) and measure English benchmark performance at each step. Identify the inflection point where performance degradation becomes unacceptable for practical deployment.