---
ver: rpa2
title: How Much Do Large Language Model Cheat on Evaluation? Benchmarking Overestimation
  under the One-Time-Pad-Based Framework
arxiv_id: '2507.19219'
source_url: https://arxiv.org/abs/2507.19219
tags:
- benchmarks
- evaluation
- llms
- private
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the overestimation problem in large language
  model (LLM) evaluation caused by data contamination and biased overtraining. To
  tackle this, the authors propose ArxivRoll, a dynamic evaluation framework inspired
  by one-time pad encryption.
---

# How Much Do Large Language Model Cheat on Evaluation? Benchmarking Overestimation under the One-Time-Pad-Based Framework

## Quick Facts
- arXiv ID: 2507.19219
- Source URL: https://arxiv.org/abs/2507.19219
- Reference count: 15
- Primary result: Popular LLMs exhibit significant overestimation on public benchmarks, with some showing >100% contamination scores.

## Executive Summary
This paper addresses the critical problem of overestimation in LLM evaluation caused by data contamination and biased overtraining. The authors propose ArxivRoll, a dynamic evaluation framework inspired by one-time pad encryption principles, which generates private test cases from recent arXiv articles to create temporally secure benchmarks. By comparing model performance on public versus private benchmarks, the framework quantifies contamination through Rugged Scores (RSI and RSII), revealing that popular models exhibit significant overestimation—some achieving contamination scores over 100%. ArxivRoll establishes a new benchmark and leaderboard for more realistic LLM capability assessment.

## Method Summary
ArxivRoll generates private test cases using SCP (Sequencing, Cloze, and Prediction) from recent arXiv papers (April-September 2024) across 8 domains. The framework constructs test cases by splitting articles on newlines, sampling consensus phrases, and filtering fragments under 80 words. Three task formats are created: Sequencing (4-part shuffled text ordering), Cloze (4-sentence masked completion), and Prediction (last sentence completion with TF-IDF distractors). Evaluation uses LM Evaluation Harness with greedy decoding and exact match scoring. Rugged Scores (RSI for contamination, RSII for biased overtraining) quantify performance gaps between public and private benchmarks.

## Key Results
- ArxivRoll produces high-quality, stable benchmarks with standard deviation below 1 point across 32 random seeds
- Popular models show significant overestimation, with Phi-1 achieving RSI of 1.21 (>100% better on public than private benchmarks)
- Strong correlation with ChatbotArena (ρ=0.68), validating ArxivRoll as a fair assessment tool
- Cloze tasks show lower correlation with human preferences (0.51) compared to Sequencing (0.92) and Prediction (0.86)

## Why This Works (Mechanism)

### Mechanism 1: Temporal Asymmetry for Contamination Prevention
Using recent arXiv preprints creates a temporal barrier against data contamination during model training. The framework extracts test cases from articles published after model training cutoffs, ensuring performance on these private benchmarks reflects genuine reasoning rather than recall.

### Mechanism 2: Performance Gap Quantification via Rugged Scores
The difference between public benchmark performance and private benchmark performance quantifies contamination and biased overtraining. RSI computes normalized performance gaps, while RSII measures variance across private benchmarks to detect selective overtraining on specific domains.

### Mechanism 3: Symbolic Formatting for Objective Evaluation
Converting free-form text understanding into multiple-choice sequencing/cloze/prediction tasks reduces subjective evaluation variance. SCP transforms academic text into three objective formats that eliminate need for human judgment while testing discourse coherence.

## Foundational Learning

- **One-Time Pad Encryption**: The framework draws analogy to OTP where each key is used exactly once. Rotating benchmarks prevents "key recovery" (memorization).
- **Distribution Shift vs. Contamination**: Distinguishing whether performance drops on private benchmarks stem from contamination (what RS measures) versus genuine distributional differences (validity threat to RS).
- **Overfitting to Evaluation Domains**: RSII specifically targets models excessively trained on common evaluation domains (math, coding) at expense of others.

## Architecture Onboarding

- **Component map**: arXiv API → Paper Fetcher (8 domains, date-filtered) → SCP Generator → ArxivRollBench → Evaluator → RS Calculator → Leaderboard
- **Critical path**: Paper ingestion → SCP generation quality filters → evaluation within 6-month window before public release
- **Design tradeoffs**: Freshness vs. Quality (recent papers may contain errors), Automation vs. Validity (SCP eliminates human effort but may produce incoherent tasks), Reproducibility vs. Contamination (releasing expired benchmarks ensures reproducibility but makes them unsuitable for future evaluation)
- **Failure signatures**: Suspiciously high scores on new private benchmark (temporal leak), extremely high variance across SCP task types (task-specific shortcuts), RSI > 1.0 (severe overfitting to public benchmarks)
- **First 3 experiments**: Validate temporal barrier by comparing same model on ArxivRollBench and synthetic "future" benchmark; Stability check by reproducing 32-variant test with different seeds; Correlation check with trusted benchmarks to confirm validity

## Open Questions the Paper Calls Out

### Open Question 1
How can RSI be decoupled from the specific benchmark triple to allow normalized comparisons across different evaluation periods or model sets? The paper provides restricted versions but no unified metric independent of both models and benchmarks simultaneously.

### Open Question 2
Does reliance on ArXiv preprints as the sole data source introduce domain bias that limits detection of overestimation for non-academic or colloquial reasoning capabilities? The paper validates correlation with ChatbotArena but doesn't analyze if common sense or creative writing capabilities are masked.

### Open Question 3
Why does the "Cloze" generation strategy exhibit significantly lower correlation with human-preference benchmarks compared to "Sequencing" and "Prediction," and does this affect its validity? The paper reports the discrepancy but provides no analysis for why Cloze aligns poorly with the reference benchmark.

## Limitations
- Framework depends on assumption that recent arXiv papers are genuinely unseen by models during training
- Validity of Rugged Scores hinges on private benchmarks being comparable in difficulty to public ones
- SCP task format may not fully capture genuine comprehension if models develop task-specific heuristics
- Framework requires continuous effort to generate new benchmarks every six months

## Confidence

- **High confidence**: ArxivRoll can detect gross overestimation in LLMs (evidenced by 1.21 RSI for Phi-1)
- **Medium confidence**: ArxivRoll produces high-quality, stable benchmarks with good correlation to ChatbotArena
- **Low confidence**: Framework's superiority in providing "fair and realistic" LLM capability assessment

## Next Checks

1. **Temporal barrier validation**: Evaluate same model on ArxivRollBench-2024b and on synthetic "future" benchmark from papers published post-cutoff. Similar scores suggest genuine capability; divergent scores suggest temporal memorization.

2. **Benchmark comparability check**: Have domain experts rate 50 random samples from ArxivRollBench-2024b versus 50 from MMLU Pro on technical complexity, prerequisite knowledge, and linguistic clarity. Check for systematic difficulty differences that could confound RSI.

3. **Shortcut detection**: For each model, compute variance of accuracy across three SCP formats. Large discrepancies would suggest format-specific strategies rather than general comprehension, questioning SCP validity.