---
ver: rpa2
title: 'Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context
  LLMs'
arxiv_id: '2512.07525'
source_url: https://arxiv.org/abs/2512.07525
tags:
- rope
- attention
- imaginary
- arxiv
- long-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies and addresses a critical information loss
  in standard Rotary Position Embeddings (RoPE) used in Large Language Models. The
  standard implementation only uses the real part of complex-valued attention scores,
  discarding the imaginary part that contains valuable phase information.
---

# Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs

## Quick Facts
- **arXiv ID:** 2512.07525
- **Source URL:** https://arxiv.org/abs/2512.07525
- **Reference count:** 40
- **Primary result:** RoPE++ recovers discarded phase information in RoPE by using both real and imaginary components of complex attention scores, improving long-context modeling performance.

## Executive Summary
This paper identifies and addresses a critical information loss in standard Rotary Position Embeddings (RoPE) used in Large Language Models. The standard implementation only uses the real part of complex-valued attention scores, discarding the imaginary part that contains valuable phase information. The authors propose RoPE++, which re-incorporates this discarded imaginary component by introducing dual-component attention scores computed in parallel with the real attentions.

The method preserves the unified absolute-relative position embedding format while offering two configurations: RoPE++EH with equal attention head number and halved KV cache, and RoPE++EC with equal cache size and doubled attention heads. Theoretical analysis shows that imaginary attention captures longer-range dependencies, and empirical results demonstrate significant performance improvements across multiple model sizes (376M, 776M, and 1.5B).

## Method Summary
RoPE++ addresses information loss in standard RoPE implementations by utilizing both real and imaginary components of complex-valued attention scores. The approach computes dual-component attention scores in parallel: real attentions are computed as in standard RoPE, while imaginary attentions capture the phase information discarded by conventional implementations. The final attention scores are formed by concatenating these dual components, effectively doubling the information capacity without changing the underlying RoPE mechanism.

Two practical configurations are proposed: RoPE++EH maintains the same number of attention heads while halving the KV cache size, and RoPE++EC maintains the same KV cache size while doubling the attention heads. This design allows flexible trade-offs between computational efficiency and parameter count while preserving the theoretical benefits of using complete complex-valued attention information.

## Key Results
- RoPE++ consistently achieves the best average scores on short-context tasks across multiple model sizes
- On long-context benchmarks, both RoPE++EH and RoPE++EC outperform standard RoPE, with benefits increasing as context length grows
- RoPE++EH matches or exceeds vanilla RoPE performance using only half the KV cache
- Length extrapolation shows RoPE++ has slower perplexity growth beyond maximum supported context length

## Why This Works (Mechanism)
The core insight is that standard RoPE implementations discard valuable phase information by only using the real part of complex-valued attention scores. In complex number representation, the real part captures amplitude information while the imaginary part captures phase information. Phase information is particularly important for modeling long-range dependencies because it encodes relative positional relationships that are lost when only amplitude is considered.

By computing dual-component attention scores in parallel and concatenating them, RoPE++ effectively doubles the information capacity of each attention head. The imaginary attention component captures longer-range dependencies because phase relationships between tokens are preserved over greater distances than amplitude relationships alone. This theoretical advantage translates to empirical performance gains, particularly in long-context scenarios where maintaining positional information over extended sequences is crucial.

## Foundational Learning

**Complex Numbers and Euler's Formula** - Why needed: Understanding complex-valued representations is fundamental to grasping how RoPE works and what information is lost. Quick check: Verify understanding of how e^(iθ) = cosθ + i·sinθ represents rotation in the complex plane.

**Rotary Position Embeddings (RoPE)** - Why needed: RoPE is the baseline method being extended, so understanding its mechanism is essential. Quick check: Confirm that RoPE applies rotational matrices to query and key embeddings based on token positions.

**Attention Mechanisms** - Why needed: The paper builds on standard attention but modifies how attention scores are computed and combined. Quick check: Ensure understanding of how attention scores are computed as dot products between queries and keys.

**KV Cache Optimization** - Why needed: The paper proposes configurations that modify KV cache usage, requiring understanding of memory-efficient inference. Quick check: Verify knowledge of how KV caching reduces redundant computations during autoregressive generation.

**Length Extrapolation** - Why needed: One key claim involves better performance beyond trained context lengths. Quick check: Confirm understanding of how models typically struggle with sequences longer than their training maximum.

## Architecture Onboarding

**Component Map:** Input tokens → Token Embeddings → Position Encoding (RoPE++) → Multi-Head Attention (Dual Real+Imaginary) → Feed-Forward Network → Output

**Critical Path:** The critical path involves the parallel computation of real and imaginary attention components, followed by concatenation and subsequent processing through the feed-forward network. The dual attention computation is the key differentiator from standard RoPE.

**Design Tradeoffs:** RoPE++EH trades KV cache size for computational efficiency by halving cache while maintaining head count. RoPE++EC trades computational efficiency for parameter count by doubling heads while maintaining cache size. The choice depends on deployment constraints and model scale.

**Failure Signatures:** Performance degradation would manifest as reduced perplexity improvements on long-context tasks, particularly when imaginary attention corruption experiments show less impact. Over-parameterization could lead to increased computational overhead without proportional performance gains.

**First Experiments:**
1. Implement dual-component attention computation and verify complex-valued attention scores match theoretical expectations
2. Test RoPE++EH configuration on a small model to validate KV cache reduction effectiveness
3. Compare length extrapolation behavior between RoPE++ and standard RoPE on sequences beyond training context length

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Performance improvements show relatively modest absolute gains (e.g., 0.1-0.2 perplexity reduction on average)
- Computational overhead of doubling attention heads in RoPE++EC may offset benefits in practical deployments
- Analysis of imaginary attention's importance relies on selective corruption experiments that may not capture real-world degradation patterns

## Confidence
- **High confidence:** Identification of information loss in standard RoPE implementations
- **Medium confidence:** Claimed benefits for length extrapolation, as this depends heavily on task characteristics
- **Medium confidence:** Relative importance of imaginary attention for long-context modeling, as corruption experiments show correlation but not necessarily causation

## Next Checks
1. Evaluate RoPE++ on multilingual benchmarks to verify generalization beyond English-centric tasks
2. Test the approach with varying KV cache sizes to better understand trade-offs between RoPE++EH and RoPE++EC configurations
3. Conduct ablation studies specifically targeting phase information contribution by comparing against alternative position encoding schemes that preserve similar information