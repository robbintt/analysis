---
ver: rpa2
title: Machine Learning Predictions for Traffic Equilibria in Road Renovation Scheduling
arxiv_id: '2506.05933'
source_url: https://arxiv.org/abs/2506.05933
tags:
- traffic
- network
- regression
- road
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of predicting traffic congestion
  from road renovation schedules, a computationally expensive task when thousands
  of traffic simulations are needed. The authors propose using machine learning surrogate
  models to estimate network-wide congestion, framing the problem as supervised regression.
---

# Machine Learning Predictions for Traffic Equilibria in Road Renovation Scheduling

## Quick Facts
- arXiv ID: 2506.05933
- Source URL: https://arxiv.org/abs/2506.05933
- Reference count: 26
- Primary result: XGBoost achieves 11% MAPE and outperforms next-best model by 20% in pinball loss for predicting traffic congestion from road renovation schedules.

## Executive Summary
This paper addresses the challenge of predicting traffic congestion from road renovation schedules, a computationally expensive task when thousands of traffic simulations are needed. The authors propose using machine learning surrogate models to estimate network-wide congestion, framing the problem as supervised regression. Three types of features are engineered: one-hot encodings of closed roads, pairwise interactions, and domain-specific engineered metrics. A range of regression models and heuristic approximations are evaluated in an online learning setting. Among all models tested, XGBoost significantly outperforms alternatives, achieving a Mean Absolute Percentage Error (MAPE) of 11% and outperforming the next-best model by 20% in pinball loss and 39% in MAPE. The study concludes that XGBoost, combined with engineered features and heuristic approximations, offers an effective and scalable approach to reducing the computational burden in large-scale maintenance scheduling problems.

## Method Summary
The paper frames traffic congestion prediction from road closure schedules as a supervised regression problem. The method involves: (1) generating closure configurations and corresponding TTT labels via Frank-Wolfe Traffic Assignment Problem solver on the Sioux Falls network; (2) engineering three feature types—one-hot encodings of closed roads, pairwise interactions, and domain-specific metrics (centrality measures, disrupted flow statistics, naive impact); (3) incorporating a Costliest Subset Heuristic (CSH) as an auxiliary feature; (4) training multiple regression models (XGBoost, Neural Network, Random Forest, Quantile Regression) using pinball loss (τ=0.05) for conservative predictions; and (5) evaluating performance in an online learning protocol where 1000 samples are added per iteration up to 200 iterations. Feature selection yields 12 engineered features from forward/backward selection, and XGBoost emerges as the top performer.

## Key Results
- XGBoost achieves 11% MAPE, outperforming the next-best model by 39% in MAPE and 20% in pinball loss
- The Costliest Subset Heuristic (CSH) serves as a strong baseline and auxiliary feature, exploiting monotonicity of TTT with additional closures
- Three engineered feature types (one-hot, pairwise, domain-specific metrics) substantially improve prediction accuracy over simpler approaches

## Why This Works (Mechanism)

### Mechanism 1: Domain-Informed Feature Engineering Captures Network Effects
Engineered features derived from baseline traffic properties substantially improve prediction of network-wide congestion from road closures. Three feature types encode different aspects: one-hot encodings identify which roads close, pairwise interactions capture compounding effects between specific road pairs, and engineered metrics (centrality measures, naive impact, disrupted flow statistics) encode domain knowledge about how closures propagate through network topology. Core assumption: baseline network properties correlate with congestion impact when roads close. Evidence: engineered features include centrality measures and "Naive Impact" achieving Pearson correlation >0.6 with log(TTT). Break condition: significant network topology changes from baseline would require re-computation of engineered features.

### Mechanism 2: Asymmetric Loss Functions Enable Conservative Pruning
Training with pinball loss (quantile τ=0.05) produces predictions that systematically underestimate congestion, reducing risk of discarding viable schedules during optimization. At τ=0.05, overestimation is penalized 19× more heavily than underestimation, biasing predictions downward for safe schedule pruning. Core assumption: schedules with predicted high TTT are truly suboptimal; false negatives are more costly than false positives. Evidence: pinball loss achieves 20% improvement over next-best model. Break condition: if optimization objective changes to maximize throughput rather than minimize congestion, conservative bias direction would need reversal.

### Mechanism 3: Heuristic Warm-Start Provides Data-Efficient Baseline
The Costliest Subset Heuristic (CSH) provides reasonable predictions when training data is scarce, then gets outperformed by XGBoost as data accumulates. CSH exploits monotonicity—closing more roads generally increases TTT—by returning maximum observed TTT among all known subsets of a query configuration. Core assumption: Braess' paradox is rare enough that monotonicity holds in practice. Evidence: analysis of 200,000 datapoints showed no cases where closing additional roads reduced TTT. Break condition: on networks with frequent Braess' paradox occurrences, CSH would provide invalid bounds.

## Foundational Learning

- **Traffic Assignment Problem (TAP) and User Equilibrium**: Why needed: TTT is computed by solving TAP—understanding this explains computational expense and what surrogate approximates. Quick check: Can you explain why TAP requires iterative solution (Frank-Wolfe) rather than direct computation?
- **Quantile Regression and Pinball Loss**: Why needed: paper's core innovation uses asymmetric loss for conservative predictions; understanding this is essential for tuning conservatism parameter. Quick check: If you set τ=0.10 instead of τ=0.05, would predictions become more or less conservative?
- **Online Learning and Progressive Data Availability**: Why needed: evaluation framework simulates real optimization where simulations are expensive and data arrives incrementally; model choice depends on retraining efficiency. Quick check: Why did Neural Networks and Random Forest fail time threshold despite competitive accuracy?

## Architecture Onboarding

- **Component map**: Closure configuration A' → Feature Engineering (one-hot encoding, pairwise interactions, engineered metrics, heuristic output) → Surrogate Model (XGBoost with pinball loss) → Predicted TTT → Decision (prune if ŷ > threshold)
- **Critical path**: 1) Baseline computation: Calculate free-flow times, capacities, centrality measures for all roads (one-time) 2) Heuristic implementation: CSH requires efficient subset lookup structure over evaluated configurations 3) Feature vector construction for each new schedule evaluation 4) XGBoost prediction (milliseconds) vs. TAP simulation (seconds to minutes)
- **Design tradeoffs**: Accuracy vs. speed: XGBoost (39s/iteration, MAPE=11%) vs. simpler models (faster but 39%+ worse MAPE); Conservatism vs. pruning efficiency: higher τ reduces false negatives but increases unnecessary simulations; Feature complexity: pairwise features scale O(n²) with roads; engineered features are O(1) but may miss interaction effects
- **Failure signatures**: Model consistently overestimates TTT → check if τ is set correctly; pinball loss may not be configured; MAPE >25% with >10,000 samples → likely feature engineering issue; verify one-hot encoding matches project indices; Training time exceeds threshold → reduce feature dimensionality or switch from pairwise to engineered-only features; Predictions worse than CSH heuristic → insufficient training data or feature-target misalignment
- **First 3 experiments**: 1) Baseline replication: Run CSH heuristic on Sioux Falls with 5,000 random closure configurations; verify MAPE ≈30% as reported 2) Feature ablation: Train XGBoost with only one-hot features, only engineered features, and combined; measure MAPE difference to isolate contribution of each feature type 3) Conservatism calibration: Train XGBoost with τ ∈ {0.01, 0.05, 0.10, 0.20}; plot pinball loss vs. MAPE to find optimal tradeoff for your pruning threshold

## Open Questions the Paper Calls Out

- **Question 1**: Does the proposed XGBoost surrogate approach scale effectively to significantly larger and more complex real-world traffic networks beyond the Sioux Falls benchmark? Basis: Conclusion states future work should "apply the approach to larger or more complex networks." Why unresolved: Study relies exclusively on Sioux Falls network; evidence would require replicating methodology on large-scale instances (e.g., Chicago or New York networks) to compare accuracy and computational efficiency.
- **Question 2**: Can neural network models be optimized to achieve accuracy comparable to XGBoost within the strict time constraints required for online learning? Basis: Section 5.3 notes Neural Network "exceeded computational time limit" but "achieved strong predictive performance," suggesting potential if optimized. Why unresolved: NN was terminated early due to time limits, leaving competitive viability unconfirmed. Evidence would require hyperparameter tuning of NN for faster convergence and direct runtime-controlled comparison with XGBoost.
- **Question 3**: How does integration of surrogate model into full optimization loop affect quality of final maintenance schedule compared to exact simulation methods? Basis: Paper evaluates prediction errors but does not measure downstream impact on final scheduling optimization objective. Why unresolved: While surrogate predicts congestion well, unknown if conservative pruning strategy inadvertently discards globally optimal schedules or introduces bias over many iterations. Evidence would require full end-to-end comparison of scheduling optimization outcomes (final TTT) using surrogate versus exact simulations.

## Limitations

- **Network topology stability**: Engineered features assume baseline network structure remains representative; significant infrastructure changes would require recalibration
- **Computational cost reporting**: XGBoost's 39-second per-iteration training time may be prohibitive for real-time applications requiring rapid schedule updates
- **Scalability constraints**: Pairwise feature expansion scales quadratically with network size, limiting direct application to large metropolitan networks without dimensionality reduction
- **Monotonicity assumption validity**: Study reports no Braess' paradox instances in 200,000 samples, but this may not generalize to all network configurations or demand patterns

## Confidence

- **High confidence**: XGBoost's superior performance metrics (11% MAPE, 20% pinball loss improvement) and core mechanism of conservative prediction via asymmetric loss
- **Medium confidence**: Claim that domain-informed features substantially improve predictions, given limited comparative ablation studies and unknown feature selection thresholds
- **Low confidence**: Scalability claims for large networks, as study uses single benchmark network without testing larger or structurally different networks

## Next Checks

1. **Feature contribution analysis**: Perform systematic ablation tests removing each feature type (one-hot, pairwise, engineered, heuristic) to quantify individual contributions to MAPE reduction
2. **Network generalization test**: Apply methodology to networks with different topological properties (e.g., scale-free, grid-based) to assess robustness beyond Sioux Falls benchmark
3. **Conservatism calibration validation**: Test impact of varying τ (0.01 to 0.20) on actual optimization outcome—measure how many viable schedules are incorrectly pruned versus unnecessary simulations run