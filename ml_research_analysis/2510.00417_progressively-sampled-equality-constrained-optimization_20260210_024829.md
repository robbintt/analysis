---
ver: rpa2
title: Progressively Sampled Equality-Constrained Optimization
arxiv_id: '2510.00417'
source_url: https://arxiv.org/abs/2510.00417
tags:
- algorithm
- which
- constraint
- sample
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses equality-constrained optimization problems
  where constraints are defined by expectations or averages over many terms. The proposed
  method solves a sequence of progressively larger sampled constraint problems, starting
  from a small sample set and expanding it iteratively.
---

# Progressively Sampled Equality-Constrained Optimization

## Quick Facts
- arXiv ID: 2510.00417
- Source URL: https://arxiv.org/abs/2510.00417
- Reference count: 11
- Solves equality-constrained optimization with sampled constraints using progressive sample growth

## Executive Summary
This paper introduces a progressive sampling framework for equality-constrained optimization problems where constraints are defined as expectations over many terms. The method starts with a small sample of constraints and iteratively increases the sample size while solving a sequence of constrained subproblems. Under specific assumptions about constraint functions and their derivatives, the authors prove that this approach achieves better worst-case sample complexity bounds than solving the full-sample problem directly. The key insight is that by carefully controlling the tolerance for each subproblem based on the current sample size, the method can find approximate second-order stationary points using fewer total constraint gradient evaluations.

## Method Summary
The Progressive Constrained Sampling Method (PCSM) solves equality-constrained optimization problems where constraints are expectations by progressively increasing the sample size. Starting from an initial sample size p₁ (e.g., 64), the algorithm doubles the sample at each iteration until reaching the full sample N (e.g., 2048). Each subproblem minimizes the objective function subject to the current sampled constraints, with tolerances that increase as the sample size grows. The solution from each subproblem serves as the initial point for the next, creating a warm-start sequence. The method requires solving constrained subproblems using techniques like SQP or augmented Lagrangian methods, with specific tolerance schedules derived from theoretical bounds on constraint gradient evaluations.

## Key Results
- Achieves (ε, ζ)-second-order stationarity with fewer total constraint gradient evaluations than one-shot full-sample approaches
- With p₁=64 and N=2048, required approximately 33% fewer individual constraint gradient evaluations than solving the full-sample problem directly
- Maintains comparable solution accuracy while demonstrating computational savings on both artificial test functions and physics-informed neural network training problems
- Theoretical analysis shows improved worst-case sample complexity bounds under assumptions about constraint function properties

## Why This Works (Mechanism)
The method exploits the structure of sample average approximation problems by recognizing that early iterations with small samples can make rapid progress toward stationarity with minimal computational cost. By gradually increasing the sample size and appropriately relaxing tolerances, the algorithm balances the trade-off between computational efficiency and solution accuracy. The theoretical guarantees rely on properties of least-squares multipliers and the observation that constraint Jacobians from different sample sizes remain sufficiently close (acute perturbations), ensuring that progress made on smaller samples transfers effectively to larger ones.

## Foundational Learning
- **Sample Average Approximation (SAA)**: Optimization framework where constraints/expectations are approximated by finite sums; needed because many real problems involve expectations over large datasets
- **Second-order stationarity conditions**: Requirements for a point to be a local minimum including gradient and Hessian conditions; needed to characterize solution quality
- **Acute perturbations**: Property where matrices from consecutive sample sets remain close in operator norm; needed to ensure theoretical convergence guarantees
- **Least-squares multipliers**: Solution to normal equations for constraint multipliers; needed as a computable surrogate for true Lagrange multipliers in sampled problems
- **Constraint qualification**: Conditions ensuring Lagrange multipliers exist; needed for theoretical analysis of stationarity
- **Warm-start optimization**: Using previous solution as initial guess for next problem; needed to maintain computational efficiency across iterations

## Architecture Onboarding

**Component Map**
PCSM Wrapper -> Subproblem Solver -> Constraint Evaluation -> Progress Check

**Critical Path**
1. Initialize with p₁ samples and x₀
2. Solve constrained subproblem to tolerance ε_k
3. Evaluate stationarity measures
4. Double sample size (or reach N)
5. Repeat until full sample with final tolerance

**Design Tradeoffs**
- Sample growth rate (doubling vs. adaptive) affects iteration count vs. per-iteration cost
- Tolerance schedule balances early progress against final accuracy requirements
- Initial sample size p₁ trades off between avoiding rank deficiency and maximizing early computational savings

**Failure Signatures**
- Rank deficiency in constraint Jacobian (p₁ too small)
- Excessive constraint violation at warm-start (tolerance schedule too loose)
- Slow convergence indicating poor initial point selection
- Numerical instability in least-squares multiplier computation

**3 First Experiments**
1. Test rank deficiency by varying p₁ from 16 to 256 on the 2D artificial function
2. Compare doubling vs. linear sample growth schedules on computational efficiency
3. Evaluate sensitivity to tolerance schedule by scaling ε_k multiplicatively

## Open Questions the Paper Calls Out
1. **Minimum initial sample size**: How to determine p₁ when problem constants (σ_min, κ_∇c, etc.) are unknown? The paper notes this is critical for the theoretical bounds but provides no practical guidance beyond estimating from small samples.

2. **Inequality constraints extension**: Can the framework handle inequality constraints while maintaining sample complexity guarantees? The current analysis relies on equality constraint properties that may not extend to KKT conditions for inequalities.

3. **Strong-Morse property violations**: What happens when subproblems fail to satisfy the strong-Morse property? The paper avoids analyzing this scenario despite noting it as a reasonable termination condition.

4. **Optimal sample growth schedule**: Is doubling optimal, or can adaptive growth rates improve complexity? The paper uses a fixed schedule but acknowledges that problem-dependent optimal strategies may exist.

## Limitations
- Critical implementation details like sample augmentation strategy remain unspecified
- Theoretical assumptions (e.g., convex f(x)) are not satisfied by numerical examples
- Initial sample size p₁ is crucial but lacks rigorous justification
- Results depend on subproblem solver choices not fully detailed in the paper

## Confidence
- **Theoretical claims**: Medium - Analysis relies on assumptions not fully satisfied by examples
- **Artificial test function results**: High - Setup is fully specified and reproducible
- **PINN experiment claims**: Medium - Missing critical details about network architecture
- **Computational savings claims**: Medium - Plausible but dependent on unspecified implementation choices

## Next Checks
1. Implement multiple sample augmentation strategies (random, sequential, stratified) and compare their impact on convergence
2. Systematically vary initial sample size p₁ from 16 to 256 to identify optimal initialization
3. Test alternative constrained solvers (interior point, trust region) on the artificial test function to verify solver independence