---
ver: rpa2
title: 'SPEAR-MM: Selective Parameter Evaluation and Restoration via Model Merging
  for Efficient Financial LLM Adaptation'
arxiv_id: '2511.08500'
source_url: https://arxiv.org/abs/2511.08500
tags:
- financial
- spear-mm
- domain
- adaptation
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPEAR-MM is a post-hoc model merging framework that addresses catastrophic
  forgetting in LLM domain adaptation. It uses a multi-metric scoring system combining
  signal-to-noise ratio, weighted change intensity, and singular value drop ratio
  to identify which parameters should be preserved or restored.
---

# SPEAR-MM: Selective Parameter Evaluation and Restoration via Model Merging for Efficient Financial LLM Adaptation

## Quick Facts
- arXiv ID: 2511.08500
- Source URL: https://arxiv.org/abs/2511.08500
- Reference count: 16
- Primary result: Achieves 91.2% retention of general capabilities while maintaining 94% of domain adaptation gains with 90% computational savings

## Executive Summary
SPEAR-MM introduces a post-hoc model merging framework that addresses catastrophic forgetting in LLM domain adaptation. The method employs a multi-metric scoring system to identify which parameters should be preserved or restored after domain-specific fine-tuning. By selectively restoring general capabilities through spherical interpolation merging, SPEAR-MM maintains domain adaptation performance while significantly reducing computational overhead compared to traditional approaches.

## Method Summary
The framework uses signal-to-noise ratio, weighted change intensity, and singular value drop ratio to evaluate parameter importance during domain adaptation. Parameters are selectively restored through model merging, allowing preservation of both general capabilities and domain-specific knowledge. The approach demonstrates significant computational savings by avoiding full retraining while maintaining high performance across both general and domain-specific tasks.

## Key Results
- Achieves 91.2% retention of general capabilities versus 69.7% for standard continual pretraining
- Maintains 94% of domain adaptation gains on financial tasks
- Delivers 90% computational savings compared to traditional domain adaptation approaches

## Why This Works (Mechanism)
The method works by identifying parameters that undergo significant changes during domain adaptation but contribute little to task performance (high SNR, low WCI). These parameters are selectively restored through model merging, preserving general capabilities while maintaining domain-specific knowledge. The multi-metric scoring system provides a comprehensive evaluation of parameter importance across different adaptation scenarios.

## Foundational Learning

**Model Merging** - Technique for combining multiple model checkpoints by interpolating their parameters. Needed for selectively restoring parameters without full retraining. Quick check: Verify merging maintains model functionality through validation loss.

**Catastrophic Forgetting** - Phenomenon where models lose previously learned capabilities when trained on new tasks. Central problem being addressed. Quick check: Measure performance degradation on general tasks after domain adaptation.

**Signal-to-Noise Ratio (SNR)** - Metric measuring parameter change significance relative to noise. Identifies parameters that changed substantially during adaptation. Quick check: Validate SNR correlates with task performance changes.

**Singular Value Decomposition (SVD)** - Matrix factorization technique used to analyze parameter space structure. Helps identify parameter importance through singular value analysis. Quick check: Confirm SVD captures meaningful parameter space variations.

**Spherical Interpolation** - Method for combining model parameters while maintaining unit norm. Ensures stable parameter restoration during merging. Quick check: Verify interpolation doesn't cause numerical instability.

## Architecture Onboarding

Component map: Base Model -> Domain Adaptation -> Parameter Scoring -> Selective Merging -> Restored Model

Critical path: The parameter scoring and selective merging stages are critical, as they determine which parameters are restored and how the final model is constructed.

Design tradeoffs: The framework balances between preserving general capabilities and maintaining domain adaptation. Higher restoration rates preserve more general capabilities but may reduce domain-specific performance.

Failure signatures: Poor parameter scoring can lead to over-restoration (losing domain adaptation) or under-restoration (insufficient general capability recovery). Computational savings may not materialize if too many parameters require restoration.

First experiments:
1. Run parameter scoring on a small adaptation dataset to validate metric calculations
2. Test selective merging with a single parameter set to verify restoration functionality
3. Compare full model merging versus selective parameter restoration on a simple task pair

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies heavily on controlled benchmarks which may not capture real-world degradation patterns
- 91.2% retention metric requires verification across different base model architectures
- Computational savings claim assumes identical adaptation trajectories which may vary with different domains

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Core methodological contribution and experimental methodology | High |
| 90% computational savings generalizability | Medium |
| Multi-metric scoring system robustness across domains | Medium |
| Real-world applicability without domain-specific validation | Low |

## Next Checks

1. Replicate experiments across at least three different base model architectures to verify 91.2% retention consistency
2. Test framework on non-financial domains to assess cross-domain generalization of multi-metric scoring approach
3. Conduct ablation studies on each scoring component (SNR, WCI, SVD) to quantify individual contributions to final performance