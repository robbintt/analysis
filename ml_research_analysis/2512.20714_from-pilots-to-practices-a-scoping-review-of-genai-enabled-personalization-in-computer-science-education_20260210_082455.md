---
ver: rpa2
title: 'From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization
  in Computer Science Education'
arxiv_id: '2512.20714'
source_url: https://arxiv.org/abs/2512.20714
tags:
- learning
- feedback
- education
- https
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This scoping review of 32 studies (2023\u20132025) synthesizes\
  \ how generative AI enables personalized computer science education. It identifies\
  \ five application domains: intelligent tutoring, personalized materials, formative\
  \ feedback, AI-augmented assessment, and code review."
---

# From Pilots to Practices: A Scoping Review of GenAI-Enabled Personalization in Computer Science Education

## Quick Facts
- arXiv ID: 2512.20714
- Source URL: https://arxiv.org/abs/2512.20714
- Reference count: 0
- Primary result: GenAI supports precision scaffolding in CS education when embedded in audit-ready workflows that preserve productive struggle while scaling personalized support.

## Executive Summary
This scoping review synthesizes 32 studies (2023–2025) mapping how generative AI enables personalized computer science education. Five application domains emerge: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review. Designs incorporating explanation-first guidance, solution withholding, graduated hint ladders, and artifact grounding consistently yield more positive learning outcomes than unconstrained chat interfaces. The evidence supports GenAI as a mechanism for precision scaffolding when embedded in audit-ready workflows that preserve productive struggle while scaling personalized support.

## Method Summary
The review followed Arksey–O'Malley/Levac/JBI guidance with PRISMA-ScR reporting. Researchers screened 259 records from ACM DL, IEEE Xplore, CHI, CSCW, L@S, LAK, EDM, Google Scholar, and arXiv, purposively sampling 32 studies from 59 full-text eligible based on mechanism transparency, interpretable outcomes, and sufficient intervention detail. A hybrid deductive–inductive synthesis mapped personalization mechanisms across five domains and extracted effectiveness signals such as time-to-help, error remediation rates, feedback quality (κ), and grading reliability (QWK).

## Key Results
- Five application domains identified: intelligent tutoring, personalized materials, formative feedback, AI-augmented assessment, and code review.
- Explanation-first, solution-withholding designs show more positive learning outcomes than open-ended chat.
- GenAI enables precision scaffolding when embedded in audit-ready workflows that preserve productive struggle while scaling personalized support.

## Why This Works (Mechanism)
GenAI supports personalized scaffolding by providing just-in-time, adaptive interventions that preserve learner agency. When guidance is explanation-first and solutions are withheld, students engage in productive struggle while receiving targeted support. Graduated hint ladders allow learners to access help at their own pace, and artifact grounding ensures feedback is contextually relevant. Embedding these mechanisms in audit-ready workflows enables scalable, personalized support without sacrificing pedagogical rigor.

## Foundational Learning
- **Productive struggle**: Learners develop deeper understanding when challenged but supported; quick check: students solve problems with decreasing external help over time.
- **Explanation-first design**: Guidance prioritizes conceptual understanding before procedural steps; quick check: students can articulate reasoning behind solutions.
- **Solution withholding**: Prevents cognitive offloading and promotes independent problem-solving; quick check: error rates decrease without direct answer provision.
- **Graduated hint ladders**: Scaffolds provide incremental support, adjustable to learner needs; quick check: hint usage correlates with task completion rates.
- **Artifact grounding**: Feedback is tied to specific learner work products, increasing relevance and actionability; quick check: feedback specificity improves remediation outcomes.

## Architecture Onboarding
Component map: Search/query construction -> Record screening -> Purposive sampling -> Mechanism coding -> Synthesis
Critical path: Transparent mechanism documentation enables interpretable outcomes, which supports purposive sampling, leading to reliable synthesis.
Design tradeoffs: Broad search maximizes coverage but requires stringent filtering; purposive sampling ensures depth but risks selection bias.
Failure signatures: Missing null/negative results if sampling overly favors interpretable outcomes; inconsistent domain coverage if venue filtering excludes relevant work.
First experiments:
1. Replicate search with documented boolean queries across all listed venues to verify corpus composition.
2. Screen a random subset of excluded records to assess whether purposive sampling systematically omitted negative/null findings.
3. Cross-validate domain distribution against an independent keyword search to ensure comprehensive coverage.

## Open Questions the Paper Calls Out
None

## Limitations
- Scope constrained by publication venue and date filtering, potentially missing relevant non-CS education work.
- Purposive sampling introduces selection bias toward studies with interpretable outcomes and transparent mechanisms.
- Synthesis relies on narrative aggregation of heterogeneous outcome measures without formal meta-analysis.

## Confidence
- High confidence: The identification of five application domains and the pattern that explanation-first, solution-withholding designs yield more positive outcomes than open-ended chat.
- Medium confidence: The assertion that GenAI enables precision scaffolding when embedded in audit-ready workflows, given the reliance on case-study evidence rather than controlled comparative trials.
- Low confidence: Quantitative comparisons of effectiveness across mechanisms, due to heterogeneous metrics and lack of standardized effect sizes.

## Next Checks
1. Replicate the search with documented boolean queries across all listed venues to verify corpus composition matches reported yields.
2. Screen a random subset of excluded records to assess whether purposive sampling systematically omitted negative/null findings.
3. Cross-validate domain distribution against an independent keyword search to ensure comprehensive coverage beyond the a priori schema.