---
ver: rpa2
title: '"Are We Done Yet?": A Vision-Based Judge for Autonomous Task Completion of
  Computer Use Agents'
arxiv_id: '2511.20067'
source_url: https://arxiv.org/abs/2511.20067
tags:
- task
- cuas
- evaluation
- agents
- feedback
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reliable task completion
  detection for Computer Use Agents (CUAs), which often fail to determine whether
  a task has been successfully completed. The proposed method leverages Vision-Language
  Models (VLMs) to autonomously evaluate task completion from screenshots and task
  descriptions.
---

# "Are We Done Yet?": A Vision-Based Judge for Autonomous Task Completion of Computer Use Agents

## Quick Facts
- arXiv ID: 2511.20067
- Source URL: https://arxiv.org/abs/2511.20067
- Authors: Marta Sumyk; Oleksandr Kosovan
- Reference count: 2
- Primary result: Vision-based VLM evaluators achieve up to 73% accuracy in task completion detection, improving CUA success rates by 27% with feedback

## Executive Summary
This paper addresses the challenge of reliable task completion detection for Computer Use Agents (CUAs), which often fail to determine whether a task has been successfully completed. The proposed method leverages Vision-Language Models (VLMs) to autonomously evaluate task completion from screenshots and task descriptions. The framework is tested on a dataset of 1,260 tasks across 42 built-in macOS applications. VLMs are used to classify task success, and their feedback is provided to CUAs to improve task performance. Results show that the VLM-based evaluators achieve up to 73% accuracy in task success classification. When evaluator feedback is applied, CUAs demonstrate an average relative improvement of 27% in overall task success rate, with weaker agents benefiting the most. The approach demonstrates that vision-based evaluation can serve as an effective feedback mechanism to enhance the reliability and self-correction of autonomous computer-use agents.

## Method Summary
The method employs a three-step pipeline: (1) a CUA executes a task and records the final screenshot, (2) a VLM receives the screenshot and task description in a zero-shot setting to output a binary judgment and rationale, and (3) if the task is judged incomplete, the rationale is fed back to the CUA for a retry from the current state. The approach is evaluated on 1,260 human-labeled tasks across 42 macOS applications, testing five VLMs (GPT-4o, Claude 3.5 Sonnet, LLaVA-v1.5-7B, InternVL 2-8B, Qwen2-VL-7B) with three CUAs (Claude Computer Use, OpenAI Operator, UI-TARS). The framework aims to improve CUA reliability by providing autonomous, visual task completion assessment.

## Key Results
- VLM-based evaluators achieve up to 73% accuracy in binary task success classification against human annotations
- CUAs demonstrate an average relative improvement of 27% in overall task success rate when provided with VLM feedback
- Weaker CUAs benefit more significantly from feedback, highlighting the value of the approach for improving reliability

## Why This Works (Mechanism)
The approach works by leveraging VLMs' ability to interpret visual and textual information simultaneously, allowing them to assess whether a task's completion criteria have been met based on the final screenshot and task description. The feedback mechanism is effective because it provides CUAs with specific rationales for why a task was deemed incomplete, enabling targeted retries from the current state rather than starting over.

## Foundational Learning
- **Vision-Language Models (VLMs)**: Combine visual and textual understanding to make multimodal judgments. Needed to interpret screenshots and task descriptions. Quick check: Verify VLM can correctly describe simple images with text.
- **Zero-shot classification**: Making predictions without task-specific training. Needed for generalizability across diverse tasks. Quick check: Test VLM on a held-out task category.
- **Feedback loops in autonomous agents**: Using evaluation results to improve subsequent attempts. Needed to enhance agent reliability. Quick check: Measure improvement after one feedback cycle.
- **Task completion detection**: Determining whether an agent has achieved its objective. Needed to automate evaluation. Quick check: Compare VLM judgment with human annotation on sample tasks.
- **Screenshot-based evaluation**: Using visual state representation for assessment. Needed when textual state representation is unavailable. Quick check: Ensure screenshots capture all relevant task information.
- **Autonomous computer use**: Agents performing tasks on behalf of users. Needed as the target application domain. Quick check: Verify CUA can complete simple tasks without feedback.

## Architecture Onboarding

**Component Map:** CUA Execution -> VLM Evaluation -> Feedback to CUA -> Retry (if needed)

**Critical Path:** CUA executes task → captures final screenshot → VLM evaluates with task description → binary judgment + rationale → if incomplete, feedback to CUA → retry from current state

**Design Tradeoffs:** The zero-shot approach sacrifices some accuracy for generalizability, while using visual screenshots provides a universal evaluation signal but may miss subtle task completion criteria not visible in the image.

**Failure Signatures:** VLMs may misclassify ambiguous visual states; CUAs may fail to interpret or act on feedback rationales; screenshots may not capture all relevant completion information.

**3 First Experiments:**
1. Run evaluation pipeline on 10 sample tasks to validate VLM classification accuracy against ground truth
2. Test feedback mechanism by providing rationales to CUA for a failed task and measuring retry success
3. Compare performance of different VLMs on the same task subset to identify the most effective evaluator

## Open Questions the Paper Calls Out
None

## Limitations
- VLM accuracy of 73% leaves room for improvement and potential misclassification of task states
- Approach limited to macOS environment with built-in applications, limiting generalizability
- Effectiveness of feedback depends on quality and clarity of VLM-provided rationales
- Single retry attempt tested; multiple retries may yield different results

## Confidence

**Claim:** Vision-based evaluation serves as an effective feedback mechanism
- **Label:** Medium
- **Rationale:** While improvements are promising, modest VLM accuracy and limited task scope suggest need for further validation

**Claim:** Average relative improvement of 27% in task success rate
- **Label:** Medium
- **Rationale:** Based on single retry; unclear if performance scales with multiple retries or different CUA configurations

**Claim:** Framework robustness across different VLMs and CUAs
- **Label:** Medium
- **Rationale:** Tested three CUAs and five VLMs, but performance differences highlight potential variability in outcomes

## Next Checks
1. **Test on Diverse Task Sets:** Validate framework performance on broader task range, including non-macOS environments, to assess generalizability
2. **Multiple Retry Attempts:** Evaluate impact of allowing multiple retries on task success rates to determine if 27% improvement is consistent or if further gains are possible
3. **VLM Prompt Sensitivity:** Experiment with different prompt templates to identify if zero-shot classification accuracy can be improved through prompt engineering