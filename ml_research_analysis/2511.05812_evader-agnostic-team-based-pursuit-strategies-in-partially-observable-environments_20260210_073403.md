---
ver: rpa2
title: Evader-Agnostic Team-Based Pursuit Strategies in Partially-Observable Environments
arxiv_id: '2511.05812'
source_url: https://arxiv.org/abs/2511.05812
tags:
- evader
- pursuer
- online
- team
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses pursuit-evasion in urban environments where
  two UAVs with asymmetric capabilities must intercept an unknown evader under partial
  observability. The key challenge is adapting strategies without prior knowledge
  of the evader's behavior, start/goal locations, or environment layout.
---

# Evader-Agnostic Team-Based Pursuit Strategies in Partially-Observable Environments

## Quick Facts
- arXiv ID: 2511.05812
- Source URL: https://arxiv.org/abs/2511.05812
- Reference count: 16
- Primary result: Online deployment of level-k policies improves win rate from 3% to 28% when initial policy is mismatched against unknown evader

## Executive Summary
This paper addresses pursuit-evasion in urban environments where two UAVs with asymmetric capabilities must intercept an unknown evader under partial observability. The key challenge is adapting strategies without prior knowledge of the evader's behavior, start/goal locations, or environment layout. The authors propose a two-phase neuro-symbolic approach based on bounded rationality. First, offline training uses level-k reasoning: each level k policy is trained against a level k-1 opponent, creating a hierarchy of progressively more sophisticated strategies. Second, an online classifier observes pursuer-evader interaction histories and selects the best-response policy for the current evader.

## Method Summary
The method employs a two-phase neuro-symbolic approach for pursuit-evasion under partial observability. In the offline phase, a level-k training hierarchy generates K+1 pursuer-evader policy pairs using deep reinforcement learning, where each level k pursuer policy is trained as a best response to level k-1 evaders. In the online phase, a classifier maps observation histories to predicted evader levels, triggering deployment of the corresponding best-response pursuer policy. The approach addresses the challenge of unknown evader behavior by creating a repertoire of strategic responses and adaptively selecting among them during deployment.

## Key Results
- Offline phase policies perform well against trained opponents (π(0) pursuer wins 80% vs π(0) evader) but poorly against untrained ones (π(0) pursuer wins only 3% vs π(1) evader)
- Online classification initially struggles due to policy switching mid-episode but shows promise: in mismatched scenarios, online deployment improves performance (π(0) pursuer vs π(1) evader: 3%→28% win rate)
- The classifier achieves 98% accuracy on static data but drops to ~28% early in online episodes due to policy-switching distribution shift

## Why This Works (Mechanism)

### Mechanism 1: Level-k Adversarial Training Hierarchy
Iteratively training pursuer policies against progressively sophisticated evader policies creates a repertoire of strategic responses that span the behavioral space of bounded-rational opponents. Level-0 agents use fixed, non-strategic policies. A level-k pursuer team is trained against level-(k-1) evaders via deep RL, producing K+1 pursuer-evader policy pairs where each pursuer policy is a best-response to its corresponding evader level. This assumes real opponents exhibit bounded rationality within the trained hierarchy.

### Mechanism 2: History-Based Opponent Classification
Observation histories from partially-observable interactions encode sufficient information to infer the evader's strategic level, enabling online selection of an appropriate counter-policy. A classifier maps joint pursuer observation histories to a discrete level prediction. The classifier is trained offline on trajectories where pursuer-evader pairs are known. Different evader levels produce distinguishable patterns in observation histories despite partial observability and stochasticity.

### Mechanism 3: Online Policy Switching for Best-Response Deployment
Dynamically switching pursuer policies based on evolving opponent classification improves performance when the initial policy is mismatched, compensating for lack of prior evader knowledge. The online phase begins with an initial policy and switches to the appropriate best-response policy as classification updates. This meta-adaptation layer enables response to unknown evaders, though switching costs and classification latency must be managed.

## Foundational Learning

- **Level-k Reasoning in Game Theory**: The entire offline training hierarchy is built on level-k assumptions. Without understanding that level-k models bounded rationality through recursive belief about opponents' sophistication, the training pipeline appears arbitrary. Quick check: If you train a level-2 pursuer against a level-1 evader, what assumption does the level-2 pursuer make about the evader's reasoning?

- **Partially Observable Markov Decision Processes (POMDPs)**: Policies input observation histories rather than single states because occlusions and limited FOV mean agents never see the full state. Understanding history-based policies is essential for grasping why classification uses trajectory data. Quick check: Why must a policy in a POMDP condition on observation history rather than the current observation alone?

- **Deep Reinforcement Learning for Multi-Agent Systems**: The level-k policies are trained via deep RL in a two-team adversarial setting. Understanding non-stationarity (opponent policy changes during training) and credit assignment across HLP/LLP is critical for debugging training failures. Quick check: In adversarial multi-agent RL, why does training become non-stationary when both players learn simultaneously?

## Architecture Onboarding

- **Component map**: Level-k training pipeline -> Policy library -> Classifier module -> Online selector -> Simulator environment
- **Critical path**: 1) Train level-0 evader → Train level-0 pursuer team; 2) Train level-1 evader against level-0 pursuer → Train level-1 pursuer team; 3) Repeat to level K; 4) Collect trajectories, train classifier; 5) Deploy: classify evader → switch to matching pursuer policy
- **Design tradeoffs**: Hierarchy depth (K) increases opponent coverage but training cost; classification frequency enables faster adaptation but risks instability; heterogeneous HLP/LLP roles enable specialized search-and-intercept分工 but complicate coordination
- **Failure signatures**: Low early-timestep classification accuracy (~28%) from policy-switching distribution shift; performance drop in matched scenarios (80%→80% for π(0) vs π(0), but 60%→45% for π(1) vs π(1)); poor transfer to untrained opponents (3% win rate for π(0) vs π(1) without adaptation)
- **First 3 experiments**: 1) Classifier robustness test: Augment training with switching trajectories to improve early-timestep accuracy; 2) Hierarchy depth sweep: Train K=0,1,2,3 and identify diminishing returns; 3) Classification confidence thresholding: Compare fixed vs adaptive thresholds to reduce premature switching

## Open Questions the Paper Calls Out

### Open Question 1
How can the classifier be trained to maintain high accuracy during online execution when the pursuer's mid-episode policy switching alters the observation history distribution? The current training regime assumes a stationary policy generating the history, but the online meta-algorithm actively switches policies, creating a domain gap between training and deployment. Evidence would be a modified training procedure (e.g., data augmentation with switching trajectories) that achieves stable, high classification accuracy in the initial timesteps of an online episode.

### Open Question 2
How robust is the strategy when the evader's behavior does not fit neatly into the pre-defined level-k hierarchy (out-of-distribution)? The method relies on mapping an "unknown" evader to one of K trained policies, but evaluation only tests against specific level-0 and level-1 evaders used in training. Real-world adversaries may use strategies that do not correspond to the iteratively generated level-k responses. Evidence would be empirical results showing pursuer win rates against evader algorithms not derived from the level-k training scheme.

### Open Question 3
Does increasing the reasoning depth k beyond the tested two levels yield diminishing returns or non-transitive strategic cycles? The analysis is limited to a shallow hierarchy (levels 0 and 1), and results show significant performance variance depending on the match-up. It is unclear if deeper levels converge to a dominant strategy or if the system encounters rock-paper-scissors dynamics. Evidence would be a scaling analysis of win rates and computational cost as hierarchy depth increases (e.g., k=2,3,4).

## Limitations
- RL algorithm, reward function, and network architectures are unspecified, making faithful reproduction difficult
- No details on environment dimensions, obstacle density, or episode length
- Policy-switching during online deployment creates distribution shift not captured in offline training data

## Confidence
- Level-k hierarchy mechanism: Medium confidence (conceptually sound but unspecified implementation details)
- Classifier-based online adaptation: Medium confidence (strong offline accuracy but significant online drop)
- Overall performance gains: Medium confidence (missing environment parameters and network architectures)

## Next Checks
1. Augment classifier training data with trajectories containing mid-episode policy switches; measure improvement in early-timestep classification accuracy from the current 28% baseline
2. Sweep hierarchy depth (K=0,1,2,3) to identify diminishing returns in win rate against held-out evader policies
3. Implement and compare fixed vs adaptive confidence thresholds for policy switching to minimize premature switches in mismatched scenarios