---
ver: rpa2
title: 'Uncertainty-Aware Fusion: An Ensemble Framework for Mitigating Hallucinations
  in Large Language Models'
arxiv_id: '2503.05757'
source_url: https://arxiv.org/abs/2503.05757
tags:
- llms
- arxiv
- uncertainty
- accuracy
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucination in large language
  models (LLMs) during factoid question answering. The proposed Uncertainty-Aware
  Fusion (UAF) framework reduces hallucinations by combining multiple LLMs based on
  their accuracy and self-assessment capabilities.
---

# Uncertainty-Aware Fusion: An Ensemble Framework for Mitigating Hallucinations in Large Language Models

## Quick Facts
- **arXiv ID**: 2503.05757
- **Source URL**: https://arxiv.org/abs/2503.05757
- **Reference count**: 23
- **Primary result**: UAF reduces hallucinations by 8% in factual accuracy across benchmarks while narrowing the performance gap with GPT-4

## Executive Summary
This paper introduces Uncertainty-Aware Fusion (UAF), an ensemble framework that mitigates hallucinations in large language models during factoid question answering. UAF combines multiple LLMs based on their accuracy and self-assessment capabilities through a two-module system: SELECTOR identifies top-performing models using validation data, while FUSER combines their outputs using uncertainty-aware selection. Empirical results show UAF outperforms state-of-the-art hallucination mitigation methods by 8% in factual accuracy across multiple benchmark datasets, while narrowing or surpassing the performance gap with GPT-4.

## Method Summary
UAF addresses hallucination mitigation through a two-stage ensemble framework. The SELECTOR module evaluates all candidate LLMs on a validation set, computing accuracy and self-assessment of hallucinations (measured via AUROC of uncertainty scores), then selects the top K models based on a combined score (accuracy × self-assessment). The FUSER module generates outputs from the selected K models for each test input, computes uncertainty scores, and selects the final response using a weighted combination of historical accuracy and current uncertainty (argmax_k[Acc_k × (1 - u_k)]). The framework was evaluated on three benchmark datasets (TruthfulQA, FACTOR-news, TriviaQA) using six LLMs and three uncertainty estimation methods.

## Key Results
- UAF achieves 60.8% accuracy on TruthfulQA, 71.5% on TriviaQA, and 81.4% on FACTOR-news
- Outperforms state-of-the-art hallucination mitigation methods by 6-12% across all datasets
- Reduces hallucination rates while maintaining or exceeding GPT-4's performance gap
- Optimal ensemble size K=3-5 shows sharp performance decline for larger K
- Haloscope uncertainty method outperforms Perplexity and Semantic Entropy

## Why This Works (Mechanism)

### Mechanism 1
Uncertainty estimates derived from LLM internals can signal hallucinations with above-chance discriminative power. Uncertainty methods (Perplexity, Haloscope, Semantic Entropy) produce scores that correlate with factual correctness; higher uncertainty tends to indicate hallucinatory outputs. The AUROC metric quantifies this relationship per model. Core assumption: LLMs retain an internal sense of truthfulness even when generating false statements, and this signal is extractable without additional training. Break condition: If uncertainty scores become miscalibrated or AUROC drops near 0.5, the detection signal degrades.

### Mechanism 2
No single LLM consistently dominates across all examples; models exhibit complementary strengths in accuracy and self-assessment. The SELECTOR module ranks LLMs using a combined score (Accuracy × Self-Assessment of Hallucinations via AUROC), retaining top K models. This exploits the observation that different models lead on different subsets of examples. Core assumption: Model heterogeneity exists and is stable enough that validation-set rankings generalize to test examples. Break condition: If models become homogeneous or validation-test distribution shifts significantly, selection benefits diminish.

### Mechanism 3
Combining validation accuracy with instance-level uncertainty outperforms using either signal alone for final response selection. The FUSER selects output ŷ from K candidates using k* = argmax(Acc_k × (1 - u_k)), where u_k is the normalized uncertainty for that specific test input. This balances historical reliability with current confidence. Core assumption: Accuracy and uncertainty are complementary signals that do not systematically conflict. Break condition: If uncertainty estimates are poorly calibrated or accuracy serves as a weak proxy for domain-specific reliability, the multiplicative weighting may amplify noise.

## Foundational Learning

- **Concept**: ROC/AUROC for binary classification
  - Why needed: Quantifies how well uncertainty scores separate truthful vs. hallucinatory responses; used to compute Self-Assessment of Hallucinations (SAH) score.
  - Quick check: Can you explain why AUROC > 0.5 indicates predictive signal and what a score of 0.87 vs. 0.51 means practically?

- **Concept**: Uncertainty estimation methods (Perplexity, Semantic Entropy, Haloscope)
  - Why needed: UAF requires a concrete U_f function; the paper tests three representative approaches from distinct methodological categories.
  - Quick check: How does Perplexity differ from Semantic Entropy in capturing uncertainty for generative text outputs?

- **Concept**: Ensemble selection vs. fusion strategies
  - Why needed: Distinguishes model-level selection (SELECTOR) from instance-level output fusion (FUSER); critical for understanding why both modules are necessary.
  - Quick check: Why does majority voting fail to leverage uncertainty, and when would it underperform compared to weighted selection?

## Architecture Onboarding

- **Component map**: LLM Pool -> Uncertainty Function (U_f) -> SELECTOR -> FUSER -> Final Output

- **Critical path**:
  1. Define/clear validation set D_val (paper uses 10% of data)
  2. Run all N models on D_val, collect responses and uncertainty scores
  3. Compute Acc_j and SAH_j (AUROC) per model, derive CScore_j
  4. Select top K models (tune K on validation)
  5. At inference: generate K outputs, compute u_k per output, return ŷ from model with highest weighted score

- **Design tradeoffs**:
  - **Ensemble size K**: Larger K increases computation linearly but may degrade performance if weak models are included (Figure 3 shows sharp decline for larger K on some datasets)
  - **Uncertainty method choice**: Haloscope performed best in experiments; Semantic Entropy underperformed, suggesting method choice significantly impacts results
  - **Selection criterion**: Multiplicative (Acc × SAH) vs. additive or rank-based—paper claims multiplicative works best but exploration is limited

- **Failure signatures**:
  - AUROC near 0.5 for all models → uncertainty signal is noise; UAF degrades to accuracy-only selection
  - K too large → weak models dilute ensemble; performance drops
  - Validation-test distribution shift → SELECTOR rankings may not transfer

- **First 3 experiments**:
  1. Reproduce AUROC analysis: For a single LLM (e.g., Vicuna-7B), compute uncertainty scores using Haloscope on TruthfulQA; verify AUROC aligns with reported ~0.87.
  2. Ablate K on a held-out subset: With 3–5 models, sweep K from 1 to N on TruthfulQA; confirm performance peak at small K.
  3. Compare fusion strategies: Implement FUSER with three selection criteria (accuracy-only, inverse-uncertainty, combined) on TriviaQA; quantify gap between combined and alternatives.

## Open Questions the Paper Calls Out

### Open Question 1
Can a dynamic selection mechanism based on individual data points outperform the current static top-K selection strategy? The conclusion states, "Future work could explore dynamic LLM selection based on each data point." This remains unresolved because the current SELECTOR chooses a fixed subset based on aggregate validation performance, which may not be optimal for every specific query.

### Open Question 2
Can reinforcement learning (RL) be effectively integrated to improve the framework's adaptation to diverse data distributions? The conclusion suggests future work could "integrate reinforcement learning for better adaptation to diverse data." This remains unresolved because the current framework relies on pre-trained models and heuristic fusion without learning a policy for optimizing fusion weights.

### Open Question 3
How does the UAF framework perform on non-factoid tasks such as long-form generation or abstractive summarization? The authors explicitly limit the scope to "factoid question answering," and the FUSER relies on selection strategies suited for discrete answers. This remains unresolved because mitigating hallucinations in long-form text involves semantic consistency and coherence across many tokens, whereas this method optimizes for single-answer correctness.

### Open Question 4
How can the linear inference time scaling be mitigated to make the ensemble viable for real-time applications? The paper notes that "inference time scales linearly with the number of models in the ensemble, requiring multiple forward passes." This remains unresolved because running multiple large models per query remains computationally expensive compared to single-model inference.

## Limitations
- Framework performance heavily depends on uncertainty estimation quality, which shows variable AUROC scores (0.51-0.87) across models and methods
- The multiplicative weighting in FUSER (Acc × (1 - u)) lacks extensive ablation studies compared to alternative formulations
- Linear inference time scaling with ensemble size remains a practical bottleneck for real-time applications

## Confidence

- **High Confidence**: The ensemble framework architecture (SELECTOR + FUSER) is well-specified and reproducible. The observation that different models excel at different tasks (complementarity) is empirically supported.
- **Medium Confidence**: The specific fusion formula (Acc × (1 - u)) as optimal weighting strategy, given limited exploration of alternatives. The generalization of validation-based model selection to test data, while demonstrated, requires distribution stability.
- **Low Confidence**: The claim that uncertainty captures an "internal sense of truthfulness" - this remains a mechanistic interpretation rather than a proven cognitive model of LLM behavior.

## Next Checks

1. **Method Sensitivity Analysis**: Systematically compare Haloscope vs. Perplexity vs. Semantic Entropy on all three datasets, quantifying the performance gap and identifying conditions where each excels or fails.

2. **Distribution Shift Robustness**: Create controlled validation-test distribution shifts (e.g., different domains, prompt styles) and measure how quickly SELECTOR rankings degrade, establishing practical limits of the framework.

3. **Alternative Fusion Weighting**: Implement and test additive (Acc + (1 - u)) and rank-based (combining rank(Acc) and rank(u)) fusion strategies to quantify the empirical advantage of the multiplicative formulation claimed in the paper.