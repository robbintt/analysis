---
ver: rpa2
title: 'Large Language Models for IT Automation Tasks: Are We There Yet?'
arxiv_id: '2505.20505'
source_url: https://arxiv.org/abs/2505.20505
tags:
- tasks
- code
- task
- ansible
- automation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ITAB, an execution-driven benchmark for evaluating
  large language models on real-world IT automation tasks using Ansible. The benchmark
  includes 126 diverse tasks across seven domains, with dynamic validation in containerized
  environments to test functional correctness rather than just syntax.
---

# Large Language Models for IT Automation Tasks: Are We There Yet?

## Quick Facts
- arXiv ID: 2505.20505
- Source URL: https://arxiv.org/abs/2505.20505
- Reference count: 40
- Primary result: 14 open-source LLMs evaluated on 126 IT automation tasks achieve <12% pass@10 rate, with state reconciliation and module knowledge as primary failure modes

## Executive Summary
This paper introduces ITAB, an execution-driven benchmark for evaluating large language models on real-world IT automation tasks using Ansible. The benchmark includes 126 diverse tasks across seven domains, with dynamic validation in containerized environments to test functional correctness rather than just syntax. When evaluated across 14 open-source LLMs, none achieved a pass@10 rate above 12%, with the top model (Qwen2.5-Coder-7B) reaching only 12%. Error analysis of 1,411 failures revealed two primary failure modes: deficiencies in state reconciliation reasoning (44.87% of errors) and module-specific execution knowledge (24.37%).

## Method Summary
The study evaluates 14 open-source LLMs (3B-14B parameters) on generating functional Ansible playbooks from Stack Overflow-derived IT automation prompts. Tasks are executed in isolated Docker containers with predefined initial states, and validation scripts assert whether resulting system states match user requirements. The evaluation uses pass@k metrics (k=1,3,5,10) across 733 test cases, testing different prompt engineering strategies (TELeR levels 1-3) and decoding temperatures (0.2-0.8).

## Key Results
- Top model (Qwen2.5-Coder-7B) achieves only 12% pass@10 rate
- State reconciliation reasoning failures account for 44.87% of errors
- Module-specific execution knowledge gaps cause 24.37% of failures
- Lower temperatures maximize pass@1 while higher temperatures improve pass@10

## Why This Works (Mechanism)

### Mechanism 1: Execution-Driven State Validation
The ITAB pipeline validates functional correctness by executing generated Ansible playbooks in Docker containers and asserting whether resulting system states match user requirements, rather than relying on static syntax analysis.

### Mechanism 2: State Reconciliation Deficit
LLMs fail primarily because they lack a "world model" for tracking infrastructure state changes, leading to errors in variable scope and path resolution rather than simple syntax errors.

### Mechanism 3: Temperature-Sensitive Exploration
Decoding parameters control the trade-off between syntactic reliability (Pass@1) and functional discovery (Pass@10), with higher temperatures enabling stochastic exploration of correct but low-probability solutions.

## Foundational Learning

- **State Reconciliation (Idempotency)**: Why needed: Ansible is declarative; scripts must define desired state rather than imperative steps. Quick check: Does the model generate a script that checks if a user exists before trying to create it?
- **Dynamic vs. Static Evaluation**: Why needed: BLEU scores or syntax checks fail to predict operational success. Quick check: Can a playbook be syntactically valid but functionally catastrophic?
- **Pass@k Metric**: Why needed: Quantifies probability of finding correct solution within k attempts. Quick check: If Pass@1 is 2% but Pass@10 is 12%, what does that imply about model consistency?

## Architecture Onboarding

- **Component map**: Task Source -> Environment -> Validator -> LLM
- **Critical path**: LLM generates playbook -> Docker container spun up -> Playbook runs -> Python script validates resulting state
- **Design tradeoffs**: Open source vs. proprietary models (limited to 3B-14B for cost); prompt specificity affects capable vs. smaller models differently
- **Failure signatures**: Reasoning-distilled models fail at basic syntax; templating/variables are hardest domains
- **First 3 experiments**: 1) Verify exploration/reliability trade-off with Qwen2.5-Coder-7B at different temperatures, 2) Strip environment context to quantify degradation, 3) Implement RAG integration for module documentation

## Open Questions the Paper Calls Out

- How do larger proprietary models (e.g., GPT-4, Claude) perform compared to open-source models?
- Can current LLM architectures handle complex multi-playbook orchestration scenarios?
- What architectural advances beyond prompt engineering are necessary to overcome state reconciliation failures?

## Limitations
- Evaluation limited to open-source models (3B-14B parameters), excluding proprietary LLMs
- Tasks represent curated subset of IT automation scenarios, not full enterprise complexity
- Docker-based execution environment may not replicate real infrastructure variability

## Confidence
- **High Confidence**: LLMs struggle with functional Ansible generation (pass@10 <12%) is well-supported by execution-driven methodology
- **Medium Confidence**: Attribution of 44.87% errors to state reconciliation deficits is supported by qualitative analysis but needs quantitative validation
- **Low Confidence**: Reasoning-distilled models failing at basic syntax suggests fundamental transfer learning limitations but based on single model comparison

## Next Checks
1. Apply execution-log-based fine-tuning to measure reduction in state reconciliation errors
2. Port ITAB tasks to cloud-based multi-node environment to assess real-world failure modes
3. Implement retrieval-augmented generation with Ansible documentation to reduce module-specific errors