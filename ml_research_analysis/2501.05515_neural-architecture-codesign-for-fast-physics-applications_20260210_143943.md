---
ver: rpa2
title: Neural Architecture Codesign for Fast Physics Applications
arxiv_id: '2501.05515'
source_url: https://arxiv.org/abs/2501.05515
tags:
- search
- neural
- architecture
- linear
- physics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a neural architecture codesign (NAC) framework
  for optimizing deep learning models in physics applications, combining neural architecture
  search with hardware-aware optimization. The two-stage approach uses global search
  to explore architectures while considering hardware constraints, followed by local
  search with fine-tuning and compression techniques like quantization-aware training
  and pruning.
---

# Neural Architecture Codesign for Fast Physics Applications

## Quick Facts
- arXiv ID: 2501.05515
- Source URL: https://arxiv.org/abs/2501.05515
- Reference count: 32
- Primary result: A two-stage neural architecture codesign framework achieves FPGA-synthesizable models with up to 39.2× BOPs reduction and sub-5 microsecond latency for physics applications.

## Executive Summary
This work presents a neural architecture codesign (NAC) framework that automates the design of deep learning models optimized for physics applications while ensuring hardware efficiency for edge deployment. The framework combines global search (NSGA-II) to explore architectures considering hardware constraints with local search (TPE-based HPO) plus compression techniques (QAT and pruning). Applied to two physics case studies—Bragg peak analysis and jet classification—the optimized models achieve comparable or improved accuracy while significantly reducing computational complexity and enabling real-time FPGA synthesis with microsecond-level latencies.

## Method Summary
The NAC framework uses a two-stage approach: global search explores a modular block-wise search space (conv, attention, MLP) using NSGA-II multi-objective optimization to balance validation error and BOPs, producing a Pareto front of diverse candidates. Local search then refines the most promising architectures via TPE-based training optimization and applies compression (quantization-aware training and iterative magnitude-based pruning) to push further along the efficiency frontier. The framework integrates with hls4ml for FPGA deployment, targeting specific hardware constraints through precision settings and reuse factors.

## Key Results
- BraggNN models: 39.2× BOPs reduction, 2.90× parameter reduction, <5 microsecond FPGA latency
- Jet classification models: 7.2× BOPs reduction, 1.06% accuracy improvement, 70 nanosecond FPGA latency for smallest model
- All optimized models achieved >80% sparsity with <10% accuracy loss
- The two-stage approach successfully discovers hardware-efficient architectures that maintain task performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A hierarchical, two-stage search improves efficiency-accuracy tradeoffs by decoupling architecture exploration from fine-grained optimization.
- Mechanism: Global search uses NSGA-II to explore a modular block-wise search space (conv, attention, MLP) while optimizing for validation error and BOPs, producing a Pareto front of diverse candidates. Local search then refines the most promising architectures via TPE-based training optimization and applies compression (quantization + pruning) to push further along the efficiency frontier.
- Core assumption: The defined search space contains high-performing architectures; BOPs are a reasonable proxy for hardware cost during global search.
- Evidence anchors: Limited direct validation of this exact pipeline; SNAC-Pack suggests surrogate metrics may improve correlation.

### Mechanism 2
- Claim: Combining quantization-aware training (QAT) with iterative magnitude-based pruning yields highly compressed models that retain accuracy when synthesized for FPGAs.
- Mechanism: QAT trains low-precision weights while maintaining full-precision gradients; unstructured pruning progressively removes low-magnitude weights. FPGAs can exploit the resulting sparsity and low-bit operations to reduce DSP/LUT usage and latency.
- Core assumption: The target FPGA synthesis toolchain (hls4ml/Vivado) can efficiently map unstructured sparsity and reduced-precision arithmetic.
- Evidence anchors: All models pruned to 80% sparsity with <10% accuracy loss; Table 2 reports sub-5 µs latencies. Broader literature suggests sparsity benefits depend on hardware support.

### Mechanism 3
- Claim: Integrating a hardware-aware proxy metric (BOPs) into NAS guides selection toward architectures that are more likely to meet latency/resource constraints when synthesized.
- Mechanism: NSGA-II treats BOPs as a second objective alongside validation error, biasing evolution toward low-compute architectures. This reduces the likelihood of selecting architectures that are accurate but computationally prohibitive.
- Core assumption: BOPs correlates sufficiently with actual FPGA latency and resource utilization.
- Evidence anchors: "showcases the imperfection of efficiency metrics that aim to approximate the actual latency." SNAC-Pack and rule4ml propose improved hardware models; supports the need for better proxies.

## Foundational Learning

- Concept: Multi-objective Neural Architecture Search (NAS) with NSGA-II
  - Why needed here: Balances competing objectives (accuracy vs. computational cost) without requiring manual tradeoff tuning.
  - Quick check question: Given two architectures (A: 90% acc, 1M BOPs; B: 89% acc, 0.2M BOPs), which is Pareto-dominated?

- Concept: Quantization-Aware Training (QAT)
  - Why needed here: Enables deployment with low-bit weights/activations while preserving accuracy through training-time quantization simulation.
  - Quick check question: In QAT, are gradients computed in low precision or full precision during backpropagation?

- Concept: Unstructured vs. Structured Pruning
  - Why needed here: Unstructured pruning suits flexible hardware (FPGAs, CPUs); structured pruning favors fixed architectures (GPUs).
  - Quick check question: Which pruning type produces a smaller dense weight matrix versus a sparse matrix of the original shape?

## Architecture Onboarding

- Component map: Global Search (NSGA-II) -> Local Search (TPE + QAT + Pruning) -> hls4ml Synthesis -> FPGA Deployment

- Critical path:
  1. Define block-wise search space (channels, kernels, activations, normalization)
  2. Run global search (~1000 trials), collect Pareto-optimal architectures
  3. Select 4 candidates (large/medium/small/tiny) for local search
  4. Apply QAT at 4/8/16/32-bit and iterative pruning (20 iterations, 20% per step)
  5. Choose final models; synthesize with hls4ml; validate latency/resources

- Design tradeoffs:
  - BOPs vs. latency: Large model reduced BOPs 5.9× but latency did not scale proportionally
  - Sparsity vs. accuracy: >80% sparsity feasible with <10% accuracy drop in case studies
  - Reuse factor: Higher values reduce parallelism, lowering resource usage but increasing latency

- Failure signatures:
  - Pareto architectures fail timing or exceed FPGA resources post-synthesis
  - Accuracy collapses beyond 80% sparsity or at very low bit widths
  - Unsupported layers (e.g., non-local attention in original BraggNN) block hls4ml synthesis

- First 3 experiments:
  1. Reproduce a small-scale global search (~200 trials) on a toy dataset; plot accuracy vs. BOPs Pareto front
  2. Take one architecture; apply QAT at 8-bit and prune to 80% sparsity; compare accuracy vs. baseline
  3. Synthesize the pruned 8-bit model with hls4ml; measure latency (ns/µs) and LUT/DSP utilization on the target FPGA

## Open Questions the Paper Calls Out

- Question: Can surrogate models for inference time prediction improve Pareto-optimal solutions compared to BOPs-based optimization?
  - Basis in paper: "We plan for future work to cover more hardware-aware metrics, which can be done with proposed surrogate models [29, 30] to predict inference time."
  - Why unresolved: BOPs is only a "general predictor for resources" that "does not translate directly to latency," causing the limitation that "none exceed in all criteria" among discovered models.
  - What evidence would resolve it: Demonstration that surrogate-based optimization produces models simultaneously improving accuracy, latency, and resource utilization on the same tasks.

- Question: Would alternative search strategies beyond NSGA-II and TPE improve NAC for physics-specific applications?
  - Basis in paper: "Investigating alternative search strategies tailored to specific physics tasks or FPGA synthesis should lead to further improvements."
  - Why unresolved: Current approach uses general-purpose optimization (genetic algorithms and Bayesian methods), but physics tasks may have domain-specific structure that specialized search strategies could exploit.
  - What evidence would resolve it: Comparative study showing task-specific search strategies yield better Pareto fronts than generic NSGA-II on physics benchmarks.

- Question: What diverse layer types beyond convolutions, attention, and fully-connected blocks would expand the NAC search space effectively?
  - Basis in paper: "Our framework can be improved expanding the search space to incorporate more diverse layer types allowing for the discovery of more creative network architectures."
  - Why unresolved: Current hierarchical search space is restricted to standard blocks; novel physics-inspired layers or operations are unexplored.
  - What evidence would resolve it: Identification of new layer types that improve Pareto-optimal tradeoffs on at least one of the case study tasks.

- Question: How can reuse factor optimization be integrated into the NAC pipeline rather than fixed manually?
  - Basis in paper: The reuse factor is noted as a hyperparameter "we fix but can be varied in future studies" in Section 4.1, yet directly affects the latency-resource tradeoff in FPGA synthesis.
  - Why unresolved: Current two-stage approach does not include hardware synthesis parameters within the optimization loop.
  - What evidence would resolve it: Integration of reuse factor into local search with demonstrated Pareto improvements over fixed-reuse baselines.

## Limitations

- BOPs proxy shows imperfect correlation with actual FPGA latency, potentially limiting global search effectiveness
- Dataset access remains unclear - both Bragg peak and jet classification datasets are not publicly linked in the paper
- Training hyperparameters for both initial training and QAT are unspecified beyond "100 epochs per iteration"

## Confidence

- **High**: The two-stage search methodology and integration with hls4ml for FPGA synthesis are well-documented and reproducible
- **Medium**: The Pareto-optimal tradeoff results for both case studies are supported by data, though BOPs-to-latency correlation is imperfect
- **Low**: Generalization claims to broader physics applications and the exact dataset preprocessing procedures lack sufficient detail for immediate reproduction

## Next Checks

1. Validate BOPs correlation by synthesizing a small set of architectures from the global search Pareto front to measure actual FPGA latency vs. predicted BOPs
2. Test the framework on a third, independent physics dataset (e.g., condensed matter or astrophysics) to assess generalizability beyond the two presented cases
3. Measure the computational cost of the two-stage search itself (global + local) and compare against simpler single-stage approaches to verify claimed efficiency gains