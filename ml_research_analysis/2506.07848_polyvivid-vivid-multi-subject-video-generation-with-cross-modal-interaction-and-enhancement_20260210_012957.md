---
ver: rpa2
title: 'PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction
  and Enhancement'
arxiv_id: '2506.07848'
source_url: https://arxiv.org/abs/2506.07848
tags:
- video
- image
- text
- identity
- subject
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PolyVivid is a multi-subject video customization framework that
  enables flexible, identity-consistent generation with accurate subject interactions.
  It addresses the challenges of precise text-image correspondence, identity preservation,
  and subject interaction in multi-subject video generation.
---

# PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement

## Quick Facts
- arXiv ID: 2506.07848
- Source URL: https://arxiv.org/abs/2506.07848
- Reference count: 40
- Outperforms SOTA in multi-subject video customization: Face-sim 0.642, DINO-sim 0.623, FVD 959.74

## Executive Summary
PolyVivid is a multi-subject video customization framework that enables flexible, identity-consistent generation with accurate subject interactions. It addresses the challenges of precise text-image correspondence, identity preservation, and subject interaction in multi-subject video generation. The core method includes a VLLM-based text-image fusion module for grounding, a 3D-RoPE-based enhancement module for structured bidirectional fusion, and an attention-inherited identity injection module for consistent identity preservation. Additionally, a MLLM-based data pipeline improves subject discriminability. Experiments show PolyVivid outperforms state-of-the-art methods in identity fidelity, text-video alignment, and video quality.

## Method Summary
PolyVivid is built on HunyuanVideo (MM-DiT) and uses a two-stage progressive training approach. It first learns single-subject identity preservation (5,000 iterations), then multi-subject interactions (5,000 iterations). The framework employs LLaVA for VLLM-based text-image fusion using structured templates, 3D-RoPE positional encoding for cross-modal interaction, and attention-inherited identity injection via LoRA-adapted cross-attention. A MLLM-based data pipeline (Florence2 + SAM2) with clique-based consolidation improves subject discriminability. Training uses LoRA on MM-Attention Q/K/V/FFN weights across 256 GPUs with batch size 256.

## Key Results
- Face-sim: 0.642 (identity fidelity)
- DINO-sim: 0.623 (identity fidelity)
- FVD: 959.74 (video quality)
- CLIP-B: 0.336, CLIP-L: 0.281 (text-video alignment)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding visual identities into textual space via a VLLM improves text-image correspondence grounding for multi-subject customization.
- Mechanism: The VLLM-based text-image fusion module encodes subject images into text token space using a structured template (e.g., "The TI,1 looks like <image 1>.") that explicitly links image tokens to textual entities. LLaVA's multimodal attention captures high-level semantic associations, enabling the video generator to associate each subject image with the correct text entity.
- Core assumption: The pretrained VLLM (LLaVA) has sufficient multimodal grounding capability to generalize from dialogue-style training to the structured identity-prompt format used here.
- Evidence anchors: [abstract] "VLLM-based text-image fusion module that embeds visual identities into the textual space for precise grounding." [section 4.1] Structured template formulation and <SEP> delimiter design to link text entities with subject images.

### Mechanism 2
- Claim: Structured bidirectional fusion between text and image embeddings via 3D-RoPE enhances identity information in text tokens and interaction semantics in image tokens.
- Mechanism: The 3D-RoPE assigns temporal and spatial position indices to <text>, <image>, and 'image' tokens, positioning related tokens close in the RoPE space. MM-Attention with LoRA facilitates information flow: image tokens inject identity details into text tokens; text tokens provide interaction cues to image tokens.
- Core assumption: The 3D positional encoding scheme effectively binds tokens of the same subject and enables pixel-by-pixel interaction.
- Evidence anchors: [abstract] "3D-RoPE-based enhancement module that enables structured bidirectional fusion between text and image embeddings." [section 4.2] Equations 4–6 detail RoPE index assignments for <text>, <image>, and 'image' tokens.

### Mechanism 3
- Claim: Attention-inherited identity injection via cross-attention with LoRA mitigates identity drift and ensures consistent conditioning across frames.
- Mechanism: Instead of token concatenation or adapter-based injection, the module reparameterizes MM-Attention's K, V matrices with LoRA to incorporate image tokens, constructing a cross-attention where image tokens are injected into video tokens. A zero-initialized FC layer stabilizes early training.
- Core assumption: The pretrained MM-Attention's multimodal processing capability transfers effectively to this cross-attention design.
- Evidence anchors: [abstract] "attention-inherited identity injection module ensures stable conditioning information throughout video generation." [section 4.3] Comparison to token concatenation and adapter-based methods.

## Foundational Learning

- Concept: Multimodal Diffusion Transformer (MM-DiT)
  - Why needed here: PolyVivid builds on HunyuanVideo, an MM-DiT architecture where text and video embeddings interact via MM-Attention.
  - Quick check question: Can you sketch how MM-Attention integrates text and video tokens within a DiT block?

- Concept: Rotary Position Embedding (RoPE) and 3D Extensions
  - Why needed here: The 3D-RoPE scheme assigns temporal and spatial indices to text and image tokens to structure cross-modal interaction.
  - Quick check question: How does RoPE encode relative position, and how does 3D-RoPE extend this to spatial dimensions?

- Concept: LoRA Fine-Tuning
  - Why needed here: LoRA is used to adapt MM-Attention Q/K/V and FFN weights for text-image interaction and identity injection without full fine-tuning.
  - Quick check question: What is the rank-vs-expressiveness tradeoff when choosing LoRA rank for cross-attention layers?

## Architecture Onboarding

- Component map:
  - Base video generator: HunyuanVideo (MM-DiT backbone)
  - Text encoder: LLaVA (VLLM) for text-image fusion
  - Image encoders: CLIP-ViT (via LLaVA) for high-level semantics; pretrained VAE for fine-grained identity features
  - 3D-RoPE fusion module: MM-Attention with LoRA for bidirectional text-image interaction
  - Identity injection module: Cross-attention with LoRA and zero-init FC
  - Data pipeline: MLLM-based grounding (Florence2), segmentation (SAM2), clique-based subject consolidation

- Critical path:
  1. Construct structured identity prompt from text and subject images
  2. Encode via LLaVA to obtain fused text embeddings; encode images via VAE for detailed identity tokens
  3. Apply 3D-RoPE fusion to enhance text tokens with identity and image tokens with interaction semantics
  4. Inject interaction-enhanced image tokens into video latents via cross-attention
  5. Denoise video latents to generate final video

- Design tradeoffs:
  - Token concatenation vs adapter vs cross-attention injection: Concatenation suffers temporal imbalance; adapters struggle with high-dimensional video feature spaces; cross-attention with inherited weights balances identity strength and temporal consistency
  - 3D-RoPE index assignment: Temporal/spatial proximity encourages correct subject binding but requires careful handling of multi-subject token ordering
  - LoRA rank: Higher rank improves expressiveness but increases training cost

- Failure signatures:
  - Identity drift: Later frames deviate from reference appearance
  - Subject confusion: Generated video mixes features across subjects
  - Static or low-motion outputs: Over-regularized identity injection may suppress motion dynamics
  - Temporal flicker: Inconsistent conditioning across frames

- First 3 experiments:
  1. Single-subject identity preservation: Verify that identity injection maintains reference appearance across all frames
  2. Multi-subject interaction generation: Test whether text-specified interactions are correctly rendered with each subject's identity preserved
  3. Ablation on injection strategies: Compare token concatenation, adapter-based injection, and attention-inherited cross-attention on identity similarity and FVD

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PolyVivid's performance degrade as the number of subjects increases beyond three, and what architectural modifications would be needed to maintain identity fidelity at scale?
- Basis in paper: [explicit] The authors state in Section A.6 that "it may encounter difficulties when handling highly complex scenes involving numerous subjects" and demonstrate only two- and three-subject scenarios in experiments.

### Open Question 2
- Question: Can the text-image interaction 3D-RoPE mechanism effectively generalize to interactions requiring fine-grained physical reasoning (e.g., precise object manipulation, collision dynamics)?
- Basis in paper: [explicit] Section A.6 notes difficulty with "fine-grained interactions that require detailed physical reasoning."

### Open Question 3
- Question: How robust is the MLLM-based subject segmentation pipeline to severe occlusion and visual ambiguity, and can alternative grounding strategies improve subject distinction in such cases?
- Basis in paper: [explicit] Section A.6 acknowledges susceptibility to "errors in grounding or segmentation, especially in cases of occlusion or ambiguous visual cues."

## Limitations
- The VLLM-based fusion's generalization from dialogue-style training to structured identity prompts is not empirically validated
- The 3D-RoPE scheme's effectiveness in binding tokens is asserted but not isolated through ablation studies
- Critical implementation details (LoRA rank, learning rates, exact resolutions) are underspecified, making faithful reproduction difficult

## Confidence

**High**: The overall framework design (VLLM fusion → 3D-RoPE enhancement → attention-inherited injection) is internally coherent and addresses known challenges in multi-subject video generation.

**Medium**: The claimed performance improvements are supported by quantitative metrics, but the paper lacks ablations isolating individual mechanism contributions.

**Low**: The specific implementation details critical for reproduction are underspecified, making faithful replication difficult.

## Next Checks

1. **Ablation Study on VLLM Fusion**: Compare identity preservation and text-video alignment when using different VLLM variants or alternative text-image fusion methods.

2. **3D-RoPE Position Encoding Analysis**: Visualize and quantify how different RoPE index assignments affect subject binding and interaction accuracy in multi-subject scenarios.

3. **Identity Injection Strength Calibration**: Systematically vary LoRA rank and zero-init FC scaling to identify optimal configurations that balance identity fidelity with motion preservation.