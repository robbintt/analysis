---
ver: rpa2
title: Automating Forecasting Question Generation and Resolution for AI Evaluation
arxiv_id: '2601.22444'
source_url: https://arxiv.org/abs/2601.22444
tags:
- question
- will
- questions
- forecasting
- resolution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a system for automatically generating and
  resolving high-quality forecasting questions using LLM-powered web research agents.
  The system generates diverse, real-world questions from news sources and uses multiple
  verifiers to ensure they are unambiguous and resolvable.
---

# Automating Forecasting Question Generation and Resolution for AI Evaluation

## Quick Facts
- **arXiv ID**: 2601.22444
- **Source URL**: https://arxiv.org/abs/2601.22444
- **Reference count**: 40
- **Primary result**: Automated system generates 1,499 verifiable forecasting questions with 96% verifiability and 95% resolution accuracy, outperforming human-curated platforms

## Executive Summary
This paper presents an automated pipeline for generating and resolving high-quality forecasting questions using LLM-powered web research agents. The system generates diverse, real-world questions from news sources and employs multiple specialized verifiers to ensure questions are unambiguous and resolvable. By exploiting the asymmetry where verification is easier than generation, the pipeline achieves a 96% verifiable question rate and 95% resolution accuracy. The approach demonstrates that automated generation can produce evaluation sets sufficient for benchmarking cutting-edge forecasting systems, with more capable LLMs achieving better Brier scores on these questions than smaller models.

## Method Summary
The system uses a multi-stage pipeline: ReAct agents generate 1-7 proto-questions per news seed with web search, a refinement agent adds resolution criteria, four verifier agents filter for quality/ambiguity/resolvability/triviality, and deduplication removes near-duplicates via text-embedding clustering plus LLM verification. Resolution uses an ensemble of three Gemini 3 Pro agents with majority voting plus an Opus 4.5 tiebreaker. The pipeline transforms 2,500 news seeds into 1,499 final questions covering geopolitics, policy, economics, and law.

## Key Results
- 96% verifiable question rate achieved through multi-verifier filtering pipeline
- 95% resolution accuracy using ensemble of three Gemini 3 Pro agents plus Opus 4.5 tiebreaker
- Gemini 3 Pro achieves Brier score of 0.134, outperforming GPT-5 (0.149) and Gemini 2.5 Flash (0.179)
- Subquestion decomposition improves performance from Brier 0.141 to 0.132

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric verification pipeline
The system exploits the asymmetry where verifying question quality is substantially easier than generating high-quality questions from scratch. LLM agents propose candidate questions, then separate specialized verifier agents evaluate specific quality dimensions (quality, ambiguity, resolvability, triviality) with targeted prompts; only questions passing all filters proceed. Core assumption: Verification can be adequately distributed across specialized agents without systematic blindspots.

### Mechanism 2: Web-grounded generation reduces hallucination
Real-time web grounding during question generation reduces plausible-but-invalid questions and temporal drift. ReAct-style agents search the live web during proto-question generation and refinement, verifying that events are current, resolution sources exist, and timing is realistic. Core assumption: Web search results reliably indicate whether information will remain available at resolution time.

### Mechanism 3: Ensemble resolution with model diversity
Using multiple LLM agents with different configurations improves resolution accuracy over single-agent approaches. Three Gemini 3 Pro agents (two with identical prompts, one different) vote on resolution; disagreements trigger an Opus 4.5 tiebreaker. Core assumption: Model failures are at least partially uncorrelated across different models and prompts.

## Foundational Learning

- **Concept**: ReAct agent architecture
  - Why needed here: Core to all pipeline stages—agents iteratively reason and act through web searches to validate questions
  - Quick check question: How would a ReAct agent verify that a proposed resolution source will exist 3 months from now?

- **Concept**: Brier score decomposition (calibration × refinement + uncertainty)
  - Why needed here: The paper evaluates forecasting performance via Brier scores; understanding calibration (probability accuracy) vs. refinement (discrimination) is essential for interpreting model rankings
  - Quick check question: If Model A has Brier=0.15 with calibration=0.02 and refinement=0.07, where is it stronger—matching probabilities to outcomes, or separating YES/NO cases?

- **Concept**: Question operationalization
  - Why needed here: The system's 96% verifiability rate depends on adding precise resolution criteria; you must understand what transforms a proto-question into an objectively resolvable one
  - Quick check question: Why does "Will the FDA approve drug X by December 2025?" need operationalization beyond its title?

## Architecture Onboarding

- **Component map**: Seeds (GDELT/Media Cloud/Stockfisher) → Proto-question ReAct Agent (1-7 per seed) → Refinement ReAct Agent → Verifier Agents (quality/ambiguity/resolvability/triviality) → Deduplication (text-embedding-3-large + DBSCAN + Claude Haiku LLM check) → Final Questions (1,499 from 2,500 seeds)

- **Critical path**: Verifier filtering—questions must receive "great" on quality AND ambiguity, "very certainly yes" on resolvability; failures here cascade to wasted resolution effort and evaluation noise

- **Design tradeoffs**: Multi-verifier filtering improves quality at latency/cost cost; news seeding ensures diversity but biases toward current affairs over science/tech; ensemble resolution increases accuracy but doesn't eliminate correlated model biases

- **Failure signatures**: Questions requiring database interaction (e.g., OFAC SDN list) fail verification of non-events; "big institution announces X" questions systematically resolve NO due to institutional timelines; sources unavailable at resolution time require annulment

- **First 3 experiments**:
  1. Ablate individual verifiers on a held-out set to measure which catches the most bad questions
  2. Test alternative seed sources (scientific publications, regulatory filings) to expand beyond news bias
  3. Compare single-model vs. ensemble resolution accuracy/cost tradeoff on 200 questions

## Open Questions the Paper Calls Out

### Open Question 1
Can the automated generation pipeline be effectively extended to create and resolve conditional forecasting questions (e.g., "If policy X is enacted, will outcome Y occur?")?
- Basis in paper: [explicit] The discussion section identifies this extension as "most promising" for increasing complexity and decision-making value
- Why unresolved: The current system generates standard binary questions; the logical structure and resolution logic for counterfactuals or conditional dependencies are not yet implemented
- What evidence would resolve it: A modified pipeline that successfully generates conditional questions, verified by human experts or superior forecasting performance on such questions

### Open Question 2
How can "question interestingness" and difficulty be quantitatively defined and optimized during the generation process to prioritize high-value outputs?
- Basis in paper: [explicit] The authors state that a deeper analysis of interestingness and difficulty could "guide the system toward higher-value outputs"
- Why unresolved: The current verifiers check for resolvability and ambiguity but do not explicitly score for subjective "interestingness" or nuanced difficulty metrics beyond non-triviality
- What evidence would resolve it: A verification agent that correlates highly with human ratings of interestingness, or a generation strategy that demonstrably increases the average value-density of the question set

### Open Question 3
Can the automated resolution system be augmented to reliably handle verification of non-events for obscure topics or access data requiring interactive inputs (e.g., forms)?
- Basis in paper: [inferred] The paper notes systematic limitations where agents struggle to find information requiring "filling out forms" or verifying that something "did not happen" when it would not be widely reported
- Why unresolved: The current ReAct implementation relies on standard search snippets and browsing, failing on interactive UIs and struggling with the absence of evidence for low-profile events
- What evidence would resolve it: A new agent architecture that successfully resolves a test set of questions involving database queries or "proving a negative" with accuracy comparable to human resolution

### Open Question 4
Does focusing generation on high-impact domains (e.g., biosecurity, AI development) maintain the same levels of verifiability and accuracy as the general news-based approach?
- Basis in paper: [explicit] The discussion suggests focusing generation on high-impact domains would increase practical relevance, though this may trade off with the ease of finding resolution sources
- Why unresolved: The current system relies on general news seeds (GDELT, Media Cloud); it is untested whether specialized domains offer sufficient public data for the verifiers to function at the stated 96% success rate
- What evidence would resolve it: A comparison of verifiable question rates and resolution accuracy between a general news dataset and a domain-specific dataset (e.g., biosecurity)

## Limitations

- System effectiveness unproven for non-news domains like scientific research or technical forecasting
- Web search dependency limits questions requiring database access or interactive verification
- Ensemble resolution doesn't eliminate correlated model biases across different LLMs

## Confidence

**High confidence**: The asymmetric verification pipeline mechanism is well-supported by the 96% verifiable question rate and the documented filtering process. The ensemble resolution approach achieving 95% accuracy is directly measurable and reproducible.

**Medium confidence**: Claims about web-grounded generation reducing hallucination are supported by qualitative observations but lack rigorous ablation studies comparing grounded vs. non-grounded generation. The relative performance of different LLMs (Brier scores) depends on specific prompt configurations not fully detailed in the paper.

**Low confidence**: The assertion that the system can "evaluate models at the frontier" lacks external validation—the questions may not be sufficiently difficult to distinguish between top-tier models. The deduplication process using DBSCAN clustering and LLM verification could be removing valid questions or missing near-duplicates.

## Next Checks

1. **Ablation study of individual verifiers**: Remove each verifier (quality, ambiguity, resolvability, triviality) from the pipeline and measure impact on verifiable question rate and downstream resolution accuracy to quantify each component's contribution.

2. **Cross-domain validation**: Apply the same pipeline to non-news sources (scientific publications, regulatory filings, technical documentation) and measure changes in verifiable question rate, resolution accuracy, and Brier scores to assess domain generalizability.

3. **Resolution source robustness test**: Generate questions with sources that change structure or become temporarily unavailable, then measure resolution accuracy when sources are unavailable at resolution time versus when they remain accessible.