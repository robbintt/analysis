---
ver: rpa2
title: 'Nonuniform-Tensor-Parallelism: Mitigating GPU failure impact for Scaled-up
  LLM Training'
arxiv_id: '2504.06095'
source_url: https://arxiv.org/abs/2504.06095
tags:
- gpus
- training
- size
- failure
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Nonuniform Tensor Parallelism (NTP) addresses the problem of GPU
  failure impact on large-scale LLM training, where increasing scale-up domain sizes
  amplify failure effects. NTP allows DP replicas with GPU failures to operate at
  reduced TP degrees, redistributing work to remaining GPUs without reducing local
  batch size.
---

# Nonuniform-Tensor-Parallelism: Mitigating GPU failure impact for Scaled-up LLM Training

## Quick Facts
- arXiv ID: 2504.06095
- Source URL: https://arxiv.org/abs/2504.06095
- Reference count: 40
- Key outcome: NTP reduces throughput loss to 3% at high failure fractions vs 12% with traditional methods, and reduces spare GPU requirements from 90 to 16

## Executive Summary
Nonuniform Tensor Parallelism (NTP) addresses GPU failure impacts during large-scale LLM training by allowing DP replicas with failed GPUs to operate at reduced TP degrees. The approach redistributes work to remaining GPUs without reducing local batch size through a novel communication operation for synchronizing parameters between nonuniform TP replicas. NTP includes dynamic power allocation to boost performance of unhealthy TP groups and a resource manager that packs partially failed NVL domains into minimal DP replicas. Evaluations demonstrate significant improvements over traditional methods, reducing throughput loss from 12% to 3% at high failure fractions and dramatically decreasing spare GPU requirements.

## Method Summary
NTP enables DP replicas with GPU failures to maintain training progress by operating at reduced TP degrees, where remaining GPUs in a failed domain handle redistributed work. The system introduces a specialized communication primitive for synchronizing parameters between DP replicas operating at different TP degrees, ensuring consistent gradient updates across the cluster. Dynamic power allocation provides additional performance headroom for unhealthy TP groups by increasing power limits on functioning GPUs. A resource manager actively monitors GPU health and packs failed NVL domains into the minimum number of DP replicas necessary, optimizing resource utilization. This approach contrasts with traditional methods that either halt training or reduce local batch size when failures occur.

## Key Results
- Throughput loss reduced from 12% to 3% at high failure fractions compared to traditional methods
- Spare GPU requirements decreased from 90 to 16 under same failure scenarios
- Near-zero throughput loss achievable when combined with power-boosting mechanisms

## Why This Works (Mechanism)
NTP works by decoupling the failure impact from the entire training job, allowing affected DP replicas to continue training at reduced capacity while maintaining overall training throughput. The key mechanism is the ability to operate TP groups at different degrees across DP replicas, enabled by a novel parameter synchronization primitive that handles nonuniform tensor dimensions. When GPUs fail within an NVL domain, the remaining GPUs in that domain increase their TP degree to compensate, processing the work that would have been handled by the failed units. The resource manager ensures efficient packing of failed domains to minimize the number of affected DP replicas. Dynamic power allocation provides additional performance headroom by increasing power limits on healthy GPUs within affected TP groups, helping them maintain target throughput despite increased workload.

## Foundational Learning
- Tensor Parallelism (TP) basics: why needed for large models that exceed single GPU memory, quick check: verify understanding of model parallelism patterns
- Data Parallelism (DP) fundamentals: why needed for scaling across multiple nodes, quick check: confirm knowledge of gradient synchronization
- NVL domain concepts: why needed for hardware-specific partitioning, quick check: understand GPU domain failure isolation
- Parameter synchronization primitives: why needed for consistent training across nonuniform domains, quick check: verify all-reduce variations
- Power allocation mechanisms: why needed for boosting performance of healthy GPUs, quick check: understand GPU power capping and thermal constraints

## Architecture Onboarding
Component map: Resource Manager -> Power Allocator -> NTP Communication Primitive -> TP Groups -> DP Replicas
Critical path: GPU failure detection -> Resource manager packing decision -> TP degree adjustment -> NTP synchronization -> Power allocation adjustment
Design tradeoffs: Reduced spare GPU requirements vs increased synchronization complexity, performance vs energy efficiency in power allocation
Failure signatures: Complete GPU failures, partial compute unit failures, memory errors, interconnect issues
First experiments: 1) Single GPU failure in NVL domain with 2 TP groups, 2) Multiple simultaneous failures across different NVL domains, 3) Mixed compute and memory failures with dynamic power allocation

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness highly dependent on specific hardware configurations and failure patterns
- Power allocation mechanism introduces complexity affecting system stability and energy efficiency
- Claims require validation across diverse hardware platforms and failure modes

## Confidence
- Core NTP approach with reduced TP degrees: High
- Dynamic power allocation mechanism: Medium
- Spare GPU reduction claims: Medium
- Near-zero throughput loss with power-boosting: Medium

## Next Checks
1) Evaluate NTP performance across diverse GPU failure patterns including partial compute unit failures and memory errors beyond complete GPU failures
2) Assess the power allocation mechanism's impact on overall system power consumption and thermal management across extended training runs
3) Test the resource manager's efficiency in handling mixed failure scenarios with both compute and memory failures while maintaining optimal resource utilization