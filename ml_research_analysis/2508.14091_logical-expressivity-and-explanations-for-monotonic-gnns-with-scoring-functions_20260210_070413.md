---
ver: rpa2
title: Logical Expressivity and Explanations for Monotonic GNNs with Scoring Functions
arxiv_id: '2508.14091'
source_url: https://arxiv.org/abs/2508.14091
tags:
- each
- monotonic
- scoring
- then
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces methods to make graph neural networks (GNNs)
  with scoring functions monotonic, enabling extraction of Datalog rules for explainable
  link prediction on knowledge graphs. The authors adapt popular scoring functions
  like RESCAL, DistMult, and TuckER to be monotonically increasing by restricting
  their parameters to non-negative values.
---

# Logical Expressivity and Explanations for Monotonic GNNs with Scoring Functions

## Quick Facts
- **arXiv ID:** 2508.14091
- **Source URL:** https://arxiv.org/abs/2508.14091
- **Reference count:** 40
- **Primary result:** Introduces methods to make GNNs with scoring functions monotonic, enabling extraction of Datalog rules for explainable link prediction on knowledge graphs.

## Executive Summary
This paper bridges graph neural networks (GNNs) and symbolic logic by introducing methods to make GNNs with scoring functions monotonic, enabling extraction of Datalog rules for explainable link prediction on knowledge graphs. The authors adapt popular scoring functions like RESCAL, DistMult, and TuckER to be monotonically increasing by restricting their parameters to non-negative values. They provide a method to check soundness of extracted Datalog rules and show how existing results on scoring function expressivity affect rule extraction. Experiments on standard link prediction datasets show that monotonicity restrictions do not significantly decrease performance and yield many sound rules.

## Method Summary
The paper introduces a method to constrain GNNs and their scoring functions to be monotonically increasing by restricting all weights and parameters to non-negative values. This enables the extraction of Datalog rules that are guaranteed to be sound (i.e., if the rule body is true, the head must be true according to the model). The authors provide theoretical results showing when a monotonic GNN with a scoring function is equivalent to a finite Datalog program, and they develop algorithms to check soundness of extracted rules and compute the finite capacity needed for rule extraction.

## Key Results
- Monotonicity restrictions on GNNs with scoring functions do not significantly decrease performance (2-5% accuracy drop in some cases, gains in others).
- The method extracts many sound Datalog rules from real knowledge graphs (e.g., 56 sound rules from WN-hier).
- Monotonic max-sum GNNs with non-negative bilinear scoring functions can be shown to be equivalent to finite Datalog programs.
- Soundness checking of extracted rules is computationally feasible and effective.

## Why This Works (Mechanism)

### Mechanism 1: Monotonicity Preservation via Non-Negative Weights
Restricting GNN weights and aggregation to non-negative values ensures the model's output embeddings are monotonic with respect to the input graph structure. By enforcing $A_\ell, B^c_\ell \geq 0$ and using monotonic activation functions (e.g., ReLU), adding facts (edges) to the input dataset strictly increases or maintains the vertex label values at every layer.

### Mechanism 2: Bilinear Scoring Function Adaptation
Multiplicative scoring functions (RESCAL, DistMult, TuckER) can be constrained to be monotonically increasing, whereas distance-based models (TransE) cannot. Bilinear forms $f(R, h, t) = h^\top M_R t$ are made monotonic by restricting relation matrices $M_R$ to non-negative values.

### Mechanism 3: Capacity-Based Aggregation Bound
For monotonic max-sum GNNs, the influence of neighbors can be bounded by a finite "capacity" $C_\ell$, allowing for the construction of finite equivalent Datalog programs. The paper introduces a method to compute a scalar $\alpha$ based on the scoring threshold, allowing replacement of unbounded aggregation with fixed capacity.

## Foundational Learning

- **Concept: Max-Sum Aggregation in GNNs**
  - **Why needed here:** The paper specifically targets "max-sum" GNNs, generalizing both max (k=1) and sum (k=$\infty$). Understanding how max-k-sum combines the top k neighbor features is essential to grasp the "Capacity" mechanism.
  - **Quick check question:** How does increasing k in max-k-sum affect the gradient flow compared to pure max aggregation?

- **Concept: Datalog and Soundness**
  - **Why needed here:** The goal is to extract "sound" Datalog rules. One must understand that soundness here means logical entailment: if the rule body is true, the head must be true according to the model.
  - **Quick check question:** If a rule is "sound" for a GNN, does it guarantee the rule is factually correct in the real world, or just consistent with the GNN's predictions?

- **Concept: Bilinear Scoring Functions**
  - **Why needed here:** The method relies on modifying bilinear forms (RESCAL, DistMult). You need to understand the difference between a dot product (DistMult) and a full matrix multiplication (RESCAL) to see why non-negativity constraints are applicable.
  - **Quick check question:** Why does enforcing non-negativity on a relation matrix $M_R$ force the scoring function to be monotonically increasing?

## Architecture Onboarding

- **Component map:** Input: Knowledge Graph (D) → Encoder: Monotonic Max-Sum GNN (Weights ≥ 0, ReLU) → Embeddings: (h, t) → Decoder: Non-negative Bilinear Scoring Function (RESCAL/DistMult with $M_R \geq 0$) → Rule Extractor: Capacity calculation → Output: Finite Datalog Program.

- **Critical path:** The constraint enforcement pipeline. You cannot simply take a pre-trained standard GNN and extract rules. The model must be trained from scratch with clamped non-negative weights, non-negative scoring parameters, and BCE loss with upweighted positive examples.

- **Design tradeoffs:**
  - Performance vs. Explainability: Standard models optimize for pure accuracy; monotonic models trade ~2-5% accuracy for faithful rule extraction.
  - Speed: NAM scoring functions performed poorly when constrained; avoid them for this architecture.
  - Expressivity: You lose the ability to model "negative" rules (e.g., $A \land \neg B \rightarrow C$).

- **Failure signatures:**
  - Stagnant Gradients: If weights clamp to zero immediately, check the positive example weighting in the loss function (paper uses 50x multiplier).
  - Poor Recall: If the model fails to predict valid links, the non-negativity constraint may be too strong; try increasing hidden dimension.
  - NAM Decoder Failure: If using NAM, expect significant performance drops compared to RESCAL/DistMult in the monotonic setting.

- **First 3 experiments:**
  1. Baseline Verification: Train a standard GNN vs. a Monotonic GNN on a small subset to quantify the performance gap.
  2. Soundness Check: Extract 10 rules with 1 body atom and verify they are sound by checking the dataset transformation.
  3. Capacity Analysis: For a 2-layer GNN, compute the capacity $C_1, C_2$ and verify that using $C_\ell$ neighbors yields the same predictions as using all neighbors.

## Open Questions the Paper Calls Out
- Can the theoretical equivalence between max-sum GNNs and Datalog programs be extended to non-bilinear monotonically increasing scoring functions, such as SimplE?
- How does the restriction to unbounded activation functions affect the applicability of the equivalence results, and can the capacity-based approach be adapted for bounded activations like Sigmoid?
- Can efficient algorithms be developed to extract the equivalent Datalog program without enumerating the exponential search space of tree-like rules?
- Do specialized training paradigms exist that can consistently recover monotonic rules without the ad-hoc loss scaling required in this work?

## Limitations
- The method only applies to monotonic knowledge graphs, excluding non-monotonic rules like "Birds fly UNLESS they are penguins."
- Performance tradeoffs: 2-5% accuracy drop compared to standard GNNs in some cases.
- The capacity computation algorithm relies on unpublished material ("Tena Cucala et al. 2025"), creating potential implementation ambiguities.

## Confidence
- **High Confidence:** Monotonicity preservation mechanism and soundness checking method are rigorously proven.
- **Medium Confidence:** Performance claims are well-supported by experiments, though accuracy tradeoffs are dataset-dependent.
- **Low Confidence:** The claim that NAM scoring functions perform poorly under monotonicity constraints lacks theoretical justification.

## Next Checks
1. **Dataset Scope Test:** Apply the method to a KG known to contain non-monotonic rules and measure the percentage of unsound rules extracted.
2. **Capacity Algorithm Implementation:** Replicate the capacity computation for a simple 2-layer GNN on a toy dataset to verify the published definitions yield correct bounds.
3. **NAM Decoder Analysis:** Systematically compare NAM performance under monotonicity constraints with other scoring functions across multiple datasets.