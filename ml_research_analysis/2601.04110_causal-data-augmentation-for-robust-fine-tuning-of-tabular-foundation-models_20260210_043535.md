---
ver: rpa2
title: Causal Data Augmentation for Robust Fine-Tuning of Tabular Foundation Models
arxiv_id: '2601.04110'
source_url: https://arxiv.org/abs/2601.04110
tags:
- data
- fine-tuning
- generator
- performance
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of fine-tuning tabular foundation
  models (TFMs) under severe data scarcity, where limited training data and small
  validation sets lead to overfitting and unreliable generalization. The authors propose
  CausalMixFT, a novel method that enhances fine-tuning robustness by generating synthetic
  data using learnable Structural Causal Models (SCMs) fitted to the target dataset.
---

# Causal Data Augmentation for Robust Fine-Tuning of Tabular Foundation Models

## Quick Facts
- arXiv ID: 2601.04110
- Source URL: https://arxiv.org/abs/2601.04110
- Authors: Magnus Bühler; Lennart Purucker; Frank Hutter
- Reference count: 40
- Primary result: CausalMixFT improves median normalized ROC-AUC from 0.10 to 0.12 across 33 TabArena datasets

## Executive Summary
This paper addresses the challenge of fine-tuning tabular foundation models (TFMs) in low-data regimes where limited training data leads to overfitting and unreliable generalization. The authors propose CausalMixFT, a method that generates synthetic data using learnable Structural Causal Models (SCMs) fitted to the target dataset. These SCMs capture causal dependencies among features, producing structurally consistent synthetic samples that preserve feature semantics while expanding training diversity. Evaluated across 33 classification datasets from TabArena and over 2,300 fine-tuning runs, CausalMixFT consistently improves median normalized ROC-AUC from 0.10 (standard fine-tuning) to 0.12, outperforming purely statistical generators like CTGAN (-0.01), TabEBM (-0.04), and TableAugment (-0.09).

## Method Summary
CausalMixFT enhances fine-tuning robustness by generating synthetic data using learnable Structural Causal Models (SCMs) fitted to the target dataset. The method discovers feature dependencies via PC/FCI causal discovery algorithms, then fits SCMs using the DoWhy library to capture causal structure. Synthetic samples are generated by sampling from these SCMs, preserving semantic relationships while expanding training diversity. During fine-tuning, the model is trained on a 50/50 mix of real and synthetic data with weighted loss (α=0.5), while validation uses only real data to prevent overfitting to synthetic artifacts. This approach improves validation-test correlation from 0.67 to 0.30, enabling more reliable early stopping in low-data regimes.

## Key Results
- CausalMixFT improves median normalized ROC-AUC from 0.10 (standard fine-tuning) to 0.12 across 33 TabArena datasets
- Reduces validation-test performance correlation gap from 0.67 to 0.30, enabling more reliable early stopping
- Outperforms purely statistical generators: CTGAN (-0.01), TabEBM (-0.04), and TableAugment (-0.09) in normalized ROC-AUC

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SCM-based synthetic data acts as an implicit regularizer that reduces fine-tuning variance without sacrificing median performance
- Mechanism: Structural Causal Models capture feature dependencies via DAGs and structural equations; sampling from these preserves semantic relationships while expanding training diversity. This prevents memorization of limited real samples
- Core assumption: The discovered causal structure (via PC/FCI) approximates meaningful feature dependencies even when causal discovery assumptions may be violated
- Evidence anchors:
  - [abstract] "generating structurally consistent synthetic samples using Structural Causal Models (SCMs) fitted on the target dataset... preserving feature dependencies while expanding training diversity"
  - [Section 4] "variability is substantially larger... Default: ±0.98 vs. CausalMixFT: ±0.63... CausalMixFT configuration acts as a consistent regularizer"
  - [corpus] FairPFN (arXiv:2506.07049) similarly uses causal structure for tabular foundation models, suggesting causal structure is a useful inductive bias
- Break condition: If the target dataset has no recoverable causal structure (e.g., purely independent features), SCM-based augmentation may degrade to noise injection

### Mechanism 2
- Claim: Mixing real and synthetic data improves validation-test correlation, making early stopping more reliable
- Mechanism: Validation on real data only, while training on mixed data, prevents the model from overfitting to synthetic artifacts that don't generalize. The causally coherent synthetic data produces smoother loss landscapes
- Core assumption: Validation set is representative of test distribution; SCM-generated data doesn't systematically distort the decision boundary
- Evidence anchors:
  - [abstract] "narrows the median validation-test performance correlation gap from 0.67 to 0.30"
  - [Appendix A] "validation performance provides a weak and noisy signal for true generalization... CausalMixFT yields relatively higher and more consistent correlations"
  - [corpus] Early Stopping Tabular In-Context Learning (arXiv:2506.21387) addresses related early-stopping challenges but doesn't solve validation-test mismatch
- Break condition: If validation set is too small (<50 samples) or highly imbalanced, correlation improvements may not translate to better early stopping

### Mechanism 3
- Claim: Weighted loss with α=0.5 balances real and synthetic gradients, preventing synthetic data from dominating updates
- Mechanism: Equal batch representation combined with weighted cross-entropy ensures neither source overwhelms gradient signal. Real data anchors ground truth; synthetic data provides structural regularization
- Core assumption: Synthetic data quality is sufficient that equal weighting is appropriate; degraded synthetic data would require lower α
- Evidence anchors:
  - [Section 3] "L = αE(x,y)~Dreal[ℓ(fθ'(x), y)] + (1-α)E(x,y)~Dsyn[ℓ(fθ'(x), y)], where α=0.5 unless specified otherwise"
  - [Section 4] "purely synthetic generators occupy lower ranks" — purely synthetic data underperforms without real data mixing
  - [corpus] No direct corpus evidence on optimal mixing ratios; this is an empirical finding
- Break condition: If SCM fitting quality is poor (e.g., low-quality setting), α may need adjustment toward more real data

## Foundational Learning

- **Structural Causal Models (SCMs)**
  - Why needed here: Core mechanism for generating synthetic data. Requires understanding of DAGs, structural equations, and additive noise models
  - Quick check question: Can you explain how sampling from an SCM differs from sampling from a joint distribution fitted by a GAN?

- **Causal Discovery (PC/FCI algorithms)**
  - Why needed here: Used to estimate feature dependencies before SCM fitting. Understanding conditional independence tests and graph constraints is essential
  - Quick check question: What assumptions does the PC algorithm make about the data-generating process, and what happens when they're violated?

- **Foundation Model Fine-Tuning Dynamics**
  - Why needed here: The method builds on understanding catastrophic forgetting, weight distance regularization, and early stopping in low-data regimes
  - Quick check question: Why might L2-SP regularization help when fine-tuning on small datasets?

## Architecture Onboarding

- **Component map:**
  Real Data → [PC/FCI Discovery] → Probabilistic Adjacency Matrix → [DAG Sampling + DoWhy SCM Fitting] → Synthetic Data (20k samples) → [Weighted Loss α=0.5] → [Mitra TFM Fine-Tuning] → [Validation on Real Only] → Early Stopping

- **Critical path:**
  1. Causal discovery (100 runs, ~20 min each max) — dominates runtime for >50 features
  2. SCM fitting quality setting (GOOD/BETTER/BEST) — trades accuracy vs. speed
  3. Fine-tuning with patience=40 on validation log-loss

- **Design tradeoffs:**
  - SCM quality setting: GOOD is fast but may miss complex dependencies; BEST uses AutoGluon (slow but accurate)
  - Datasets >200 features excluded due to 1-hour SCM fitting limit
  - 50/50 real/synthetic mix vs. tuning α per dataset

- **Failure signatures:**
  - Synthetic-only training degrades performance (TableAugment: -0.09 normalized ROC-AUC)
  - Large weight distance from pre-trained checkpoint indicates catastrophic forgetting (Figure 7)
  - Low or negative validation-test correlation signals unreliable early stopping (Figure 3)

- **First 3 experiments:**
  1. Replicate on 3 datasets with varying feature counts (e.g., 10, 50, 150 features) to validate SCM fitting scalability
  2. Ablate α ∈ {0.3, 0.5, 0.7, 0.9} to test sensitivity to real/synthetic ratio
  3. Compare SCM quality settings (GOOD vs. BETTER vs. BEST) on 5 diverse datasets to quantify accuracy-speed tradeoff

## Open Questions the Paper Calls Out
None

## Limitations
- Method relies heavily on quality of causal discovery, which can fail when data violates PC/FCI assumptions (e.g., hidden confounders, non-linear relationships)
- 1-hour SCM fitting time limit excludes datasets with >200 features, limiting applicability to very high-dimensional problems
- Fixed 50/50 mixing ratio and α=0.5 may not be optimal across all dataset characteristics

## Confidence
- **High confidence**: CausalMixFT consistently outperforms purely statistical generators (CTGAN: -0.01, TabEBM: -0.04, TableAugment: -0.09 normalized ROC-AUC) and reduces validation-test correlation gap (0.67→0.30)
- **Medium confidence**: The 0.02 absolute improvement in median normalized ROC-AUC (0.10→0.12) represents a meaningful but modest gain in low-data regimes
- **Medium confidence**: SCM quality settings (GOOD/BETTER/BEST) provide controllable accuracy-speed tradeoffs, though optimal setting may be dataset-dependent

## Next Checks
1. **Causal discovery robustness test**: Evaluate performance when PC/FCI assumptions are violated (e.g., hidden confounders, non-linear relationships) using synthetic benchmark datasets with known ground truth
2. **Mixing ratio sensitivity analysis**: Systematically test α ∈ {0.3, 0.5, 0.7, 0.9} across datasets with different feature counts and noise levels to identify optimal mixing strategies
3. **High-dimensional scalability**: Assess performance on datasets with 200-500 features using approximate causal discovery methods or dimensionality reduction to determine practical limits