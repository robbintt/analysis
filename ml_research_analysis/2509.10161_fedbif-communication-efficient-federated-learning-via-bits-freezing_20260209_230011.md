---
ver: rpa2
title: 'FedBiF: Communication-Efficient Federated Learning via Bits Freezing'
arxiv_id: '2509.10161'
source_url: https://arxiv.org/abs/2509.10161
tags:
- learning
- quantization
- training
- bits
- fedbif
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses communication efficiency in federated learning
  by proposing a novel quantization method called Federated Bit Freezing (FedBiF).
  Unlike existing approaches that apply quantization after local training, FedBiF
  learns quantized parameters during training by allowing each client to update only
  one bit of multi-bit parameters while freezing the rest.
---

# FedBiF: Communication-Efficient Federated Learning via Bits Freezing

## Quick Facts
- **arXiv ID:** 2509.10161
- **Source URL:** https://arxiv.org/abs/2509.10161
- **Reference count:** 40
- **Primary result:** Achieves accuracy comparable to FedAvg with 1 bpp uplink, 3-4 bpp downlink

## Executive Summary
FedBiF introduces a novel quantization method for federated learning that learns quantized parameters during training rather than after. The key innovation allows each client to update only one bit of multi-bit parameters while freezing the rest, maintaining high precision while drastically reducing communication. Experiments across five datasets show FedBiF achieves accuracy comparable to uncompressed FedAvg while reducing communication costs to 1 bpp uplink and 3-4 bpp downlink.

## Method Summary
FedBiF implements a server-client architecture where the server quantizes global model parameters to m-bit integers. Clients receive this quantized model and freeze m-1 bits, training only one "virtual bit" per parameter using Straight-Through Estimator (STE) with identity gradient. The method cycles through bit positions during training, ensuring all bits are updated over time. Downlink communication uses 3 bits for FMNIST/SVHN and 4 bits for CIFAR/TinyImageNet, while uplink is always 1 bit. The approach naturally produces sparse models without manual pruning and demonstrates robustness across different client counts and model architectures.

## Key Results
- Achieves accuracy comparable to FedAvg on CIFAR-10 (IID ~78%) with 1 bpp uplink and 3-4 bpp downlink
- Reduces communication costs by 75-90% compared to existing quantization methods
- Naturally produces sparse models without manual pruning
- Maintains convergence speed and accuracy across different client counts and model architectures

## Why This Works (Mechanism)
The method works by learning quantized parameters during training rather than post-training quantization. By allowing each client to update only one bit while freezing others, it maintains parameter precision while reducing communication. The STE implementation with identity gradient ensures stable training of the virtual bits. The bit-cycling mechanism ensures all bits are updated over time, preventing any single bit from becoming stale. This approach naturally induces sparsity without manual pruning, as frozen bits remain at their quantized values throughout training.

## Foundational Learning
- **Straight-Through Estimator (STE):** Used to approximate gradients for non-differentiable quantization operations. Critical for training quantized parameters, with identity gradient (∂θ̂/∂vᵢ = 1) ensuring stable virtual bit updates.
- **Bit-Cycling Mechanism:** Iteratively activates different bit positions during training to ensure all bits are updated over time. Prevents stagnation from always updating the same bit position.
- **Client Participation Ratio:** Controls how many clients participate per round (0.1 in experiments). Affects convergence speed and model generalization.
- **Non-IID Data Partitioning:** Uses Dirichlet distribution (α=0.3) and label-based partitioning to simulate realistic federated scenarios where clients have different data distributions.

## Architecture Onboarding

**Component Map:** Server -> Quantization -> Client Update -> Aggregation -> Global Model

**Critical Path:** Global model quantization → Client bit selection → Virtual bit training with STE → Model aggregation → Next round

**Design Tradeoffs:** Balances parameter precision (m-bit quantization) against communication cost (1 bpp uplink). Bit-cycling ensures comprehensive parameter updates but adds complexity. Natural sparsity emerges as benefit but may not suit all applications.

**Failure Signatures:** Stagnant accuracy (>30% gap) indicates static bit selection instead of cycling. Gradient instability suggests incorrect STE implementation using complex approximations instead of identity gradient.

**First Experiments:**
1. Verify STE implementation passes identity gradient through virtual bits during backward pass
2. Confirm bit-cycling mechanism cycles through all bit positions rather than static selection
3. Test BN layer handling by running with and without BN in the model architecture

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- Does not specify how Batch Normalization layers are handled during quantization
- Initialization and clipping strategy for virtual bits requires verification
- Critical dependency on correct STE implementation and bit-cycling mechanism

## Confidence

**High Confidence:** Communication efficiency claims (1 bpp uplink, 3-4 bpp downlink) given explicit bit-width specifications.

**Medium Confidence:** Accuracy comparable to FedAvg due to critical dependency on correct STE implementation and bit-cycling mechanism.

**Low Confidence:** Natural sparsity emergence without manual pruning requires empirical verification.

## Next Checks
1. Verify STE implementation passes identity gradient through virtual bits during backward pass
2. Confirm bit-cycling mechanism cycles through all bit positions rather than static selection
3. Test BN layer handling by running with and without BN in the model architecture