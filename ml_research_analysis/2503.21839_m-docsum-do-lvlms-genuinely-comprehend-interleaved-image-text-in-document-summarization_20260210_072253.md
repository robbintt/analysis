---
ver: rpa2
title: 'M-DocSum: Do LVLMs Genuinely Comprehend Interleaved Image-Text in Document
  Summarization?'
arxiv_id: '2503.21839'
source_url: https://arxiv.org/abs/2503.21839
tags:
- image
- text
- paragraph
- information
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces M-DocSum-Bench, a novel benchmark designed
  to evaluate Large Vision-Language Models' (LVLMs) ability to understand and summarize
  interleaved image-text documents. Unlike existing benchmarks that focus on question-answering
  or single-page documents, M-DocSum-Bench requires models to generate interleaved
  image-text summaries from 500 high-quality arXiv papers, evaluating their understanding,
  reasoning, localization, and summarization capabilities within complex multimodal
  contexts.
---

# M-DocSum: Do LVLMs Genuinely Comprehend Interleaved Image-Text in Document Summarization?
## Quick Facts
- arXiv ID: 2503.21839
- Source URL: https://arxiv.org/abs/2503.21839
- Reference count: 40
- Leading LVLMs struggle to maintain coherence and accurately integrate information in long interleaved contexts; M-DocSum-7B achieves SOTA via progressive two-stage training

## Executive Summary
This paper introduces M-DocSum-Bench, a novel benchmark designed to evaluate Large Vision-Language Models' (LVLMs) ability to understand and summarize interleaved image-text documents. Unlike existing benchmarks that focus on question-answering or single-page documents, M-DocSum-Bench requires models to generate interleaved image-text summaries from 500 high-quality arXiv papers, evaluating their understanding, reasoning, localization, and summarization capabilities within complex multimodal contexts. The benchmark employs a reference-based generation task, with summaries divided into four paragraphs, each optionally accompanied by a reference image. To facilitate evaluation, the authors propose M-DocEval, a fine-grained metric that assesses textual content, visual reference accuracy, and instruction following. Experimental results reveal that leading LVLMs struggle to maintain coherence and accurately integrate information in long interleaved contexts, often exhibiting confusion between similar images and lacking robustness.

## Method Summary
The authors propose a two-stage training approach: (1) Instruction Tuning (SFT) on 3.7k arXiv papers, and (2) Direct Preference Optimization (DPO) with perturbed negatives. The training pipeline uses Qwen2-VL-7B as backbone, freezes the ViT encoder during SFT, and employs M-DocEval for preference pair filtering. Data is generated via automated pipeline using GPT-4o for key point extraction and summary synthesis, with image selection handled by a strong LVLM. The model outputs structured summaries with specific image indices and captions.

## Key Results
- Leading LVLMs show significant performance degradation on interleaved image-text summarization, with image reference accuracy dropping from ~50% to ~25% as context length increases
- M-DocSum-7B achieves state-of-the-art performance, demonstrating superior robustness to image order shuffling and maintaining coherence in long contexts
- Models exhibit "image reference bias," often defaulting to "None" when uncertain, revealing fundamental challenges in cross-modal alignment

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Robustness via Perturbed Prompts
The model's ability to correctly reference images in long contexts improves when trained on "rejected" samples generated by degrading the input prompt. During DPO, the model is presented with a high-quality summary ("chosen") and a degraded summary ("rejected"). The degraded summary is generated by shuffling image order, adding constraints, or removing the abstract. This contrastive signal penalizes the model for relying on positional heuristics and forces semantic grounding. Evidence shows M-DocSum-7B has significantly lower performance decline when image order is shuffled compared to baselines.

### Mechanism 2: Metric-Filtered Preference Optimization
Using an automated evaluator (M-DocEval) to filter preference pairs prevents reward hacking and ensures the DPO signal aligns with human-preferred summary quality. Raw preference pairs are noisy; the framework calculates Text Score (TS) and Image Score (IS) for both chosen and rejected pairs, using only pairs where the "chosen" summary strictly outperforms the "rejected" one by a margin δ. This filters out cases where degradation didn't actually result in worse output, stabilizing alignment.

### Mechanism 3: Reference-Based Generation Constraints
Framing the task as "reference-based generation" (generating text + specific image indices) explicitly evaluates cross-modal alignment better than open-ended generation. By forcing the model to output a specific image index corresponding to the text, the task eliminates ambiguity of open-ended visual description and transforms summarization into a retrieval-grounded generation task, requiring the model to maintain a representation of all images throughout the context window.

## Foundational Learning

- **Direct Preference Optimization (DPO)**: Core engine of Stage 2 training; optimizes a policy model relative to a reference model using preference pairs without training a separate reward model. *Why needed*: Enables stable alignment without reward model complexity. *Quick check*: Can you explain why DPO is preferred over RLHF for stabilizing training in a 7B parameter model?

- **Interleaved Vision-Language Modeling**: Input is long sequence of alternating text and images (e.g., Page 1 text, Fig 1, Page 2 text, Fig 2). *Why needed*: Understanding how architectures like Qwen2-VL handle 3D attention masks and position ids for this interleaved structure is crucial. *Quick check*: How does the attention mechanism in a Transformer handle a sequence where a text token attends to a visual token patch?

- **Visual Grounding in Long Context**: Paper highlights performance drops as context length increases. *Why needed*: Understanding "lost in the middle" phenomena or attention degradation in long contexts is crucial for debugging why the model fails to select correct image index. *Quick check*: If a document has 20 images, why might a standard LVLM struggle to attend to image #17 when generating the final summary paragraph?

## Architecture Onboarding

- **Component map**: arXiv Paper → Markdown/Text + Images → GPT-4o (Key Point Extraction) → Summary Synthesis → Image Selection (Reference Construction) → Qwen2-VL-7B Backbone → Stage 1 (SFT) → Stage 2 (DPO with Perturbed Negatives) → M-DocEval (GPT-4o as judge) → TS/IS Scores

- **Critical path**: Stage 2 Negative Sampling is the most delicate component. If "rejected" samples are not effectively worse than "chosen" samples (which happens if perturbation fails to degrade quality), the DPO loss function becomes noisy and optimization fails.

- **Design tradeoffs**: 
  - Closed-source Evaluation: Uses GPT-4o to evaluate other models, introducing potential self-inflation bias, though necessary for scalability
  - Automated Ground Truth: Using LLMs to generate "ground truth" summaries allows scale (500 papers) but may inherit LLM biases or hallucinations

- **Failure signatures**:
  - Image Reference Bias: Model outputs "None" for all image requests to maximize safety/accuracy of text score
  - Positional Bias: Model always selects images from first 3 pages, ignoring the rest of the document

- **First 3 experiments**:
  1. Run the "Shuffle Test": Take baseline Qwen2-VL-7B and trained M-DocSum-7B. Feed document with images in random order. Measure drop in Image Score (IS) to validate robustness
  2. Ablate Stage 2: Train model with only Stage 1 (Instruction Tuning) and compare performance against full two-stage model on M-DocSum-Bench to isolate DPO contribution
  3. Analyze the "None" Bias: Evaluate raw output of InternVL-2.5 on benchmark to confirm if it's predicting "None" for image indices, and inspect prompt to see if instruction following is bottleneck

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on automated LLM-based evaluation introduces potential circularity and bias, as GPT-4o generates both ground truth summaries and evaluates model outputs
- M-DocEval metric aggregates text and image scores with fixed weights (0.45/0.45/0.1) that are not empirically validated and may not reflect true human preferences
- Lacks human evaluation data to ground automated metrics, making it difficult to assess whether improved M-DocEval scores translate to genuinely better summaries

## Confidence
- **High confidence**: Benchmark construction methodology and two-stage training approach are clearly specified and reproducible; observation that existing LVLMs struggle with long interleaved contexts is well-supported
- **Medium confidence**: Performance gains of M-DocSum-7B over baselines are credible but magnitude of improvement is uncertain due to potential evaluation bias
- **Low confidence**: Effectiveness of M-DocEval metric as reliable proxy for human judgment remains unproven without human evaluation

## Next Checks
1. **Human evaluation validation**: Conduct small-scale human evaluation comparing M-DocSum-7B outputs against GPT-4o-generated summaries to verify that automated M-DocEval scores correlate with human preferences for coherence, relevance, and multimodal integration

2. **Metric sensitivity analysis**: Systematically vary TS/IS weighting in M-DocEval (e.g., 0.6/0.3/0.1 vs 0.3/0.6/0.1) and measure how ranking of models changes to assess whether current weighting scheme is arbitrary

3. **Cross-model evaluation consistency**: Evaluate M-DocSum-7B outputs using multiple evaluators (e.g., GPT-4o and Claude-3) to check for consistency and identify potential evaluator-specific biases in current single-judge approach