---
ver: rpa2
title: Exploring Exploration in Bayesian Optimization
arxiv_id: '2502.08208'
source_url: https://arxiv.org/abs/2502.08208
tags:
- iteration
- otsd
- exploration
- benchmarks
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two novel methods - Observation Traveling
  Salesman Distance (OTSD) and Observation Entropy (OE) - to quantify exploration
  in Bayesian optimization acquisition functions. OTSD measures the total Euclidean
  distance required to connect all observation points, while OE uses empirical differential
  entropy to assess the uniformity of observation distribution.
---

# Exploring Exploration in Bayesian Optimization

## Quick Facts
- arXiv ID: 2502.08208
- Source URL: https://arxiv.org/abs/2502.08208
- Reference count: 40
- One-line primary result: Two novel metrics (OTSD and OE) successfully quantify and distinguish exploration behavior of Bayesian optimization acquisition functions across synthetic and real-world benchmarks.

## Executive Summary
This paper introduces two novel methods - Observation Traveling Salesman Distance (OTSD) and Observation Entropy (OE) - to quantify exploration in Bayesian optimization acquisition functions. OTSD measures the total Euclidean distance required to connect all observation points, while OE uses empirical differential entropy to assess the uniformity of observation distribution. The authors empirically validate these methods across synthetic and real-world benchmarks, demonstrating their effectiveness in capturing exploration behavior. Key findings include: normalized OTSD and OE strongly correlate across benchmarks, validating their reliability; TS is highly explorative in high dimensions but behaves similarly to EI in low dimensions; batching increases exploration while trust regions and RAASP sampling reduce it; and methods with extreme exploration levels (overly explorative or exploitative) generally show worse optimization performance. The study establishes the first empirical taxonomy of acquisition function exploration, providing valuable insights for designing and selecting acquisition functions in Bayesian optimization.

## Method Summary
The paper introduces two novel metrics to quantify exploration in Bayesian optimization: OTSD (Observation Traveling Salesman Distance) and OE (Observation Entropy). OTSD computes the minimal tour length connecting all observation points using an insertion heuristic TSP approximation, normalized by domain size and iteration count. OE estimates the empirical differential entropy using the Kozachenko-Leonenko estimator based on nearest-neighbor distances. The authors evaluate these metrics across 9 benchmarks (synthetic and real-world) with dimensions ranging from 2D to 180D, testing 7 acquisition functions (EI, PI, UCB variants, MES, TS, KG) with various configurations including batching and trust regions. Experiments use BoTorch 0.12.0 with SingleTaskGP surrogate models.

## Key Results
- Normalized OTSD and OE strongly correlate across benchmarks, validating their reliability as complementary metrics
- Thompson Sampling (TS) exhibits excessive exploration in high dimensions, surpassing even random search in exploration metrics
- Batching acquisition functions increases exploration while trust regions and RAASP sampling reduce exploration behavior
- Acquisition functions with extreme exploration or exploitation levels generally show worse optimization performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The total Euclidean distance required to connect all observation points (OTSD) serves as a proxy for spatial exploration coverage.
- **Mechanism:** OTSD formulates the sequence of observations as a Traveling Salesman Problem (TSP). A diverse, widely spread set of observations necessitates a longer path to connect them, whereas clustered observations (exploitation) yield a short path.
- **Core assumption:** The input space is Euclidean, and distance in this space correlates with the uniqueness of the information gained; points far apart provide more diverse information than points close together.
- **Evidence anchors:**
  - [abstract] Introduces Observation Traveling Salesman Distance (OTSD) to quantify exploration based on selected observations.
  - [section 3.2] Defines OTSD as the minimal tour length connecting all points; notes it increases monotonically with observations.
  - [corpus] The corpus generally supports the importance of the exploration-exploitation trade-off in BO (e.g., "Exploitation Over Exploration..."), but does not contain external validation for the specific OTSD metric.
- **Break condition:** Non-Euclidean input domains (e.g., graphs, sequences) where standard Euclidean distance does not reflect functional similarity, unless a valid custom metric is substituted.

### Mechanism 2
- **Claim:** Empirical differential entropy of the observation distribution (OE) quantifies exploration by measuring the uniformity of sampling density.
- **Mechanism:** By treating observation locations as samples from a random variable, OE uses the Kozachenko-Leonenko (KL) estimator to approximate differential entropy based on nearest-neighbor distances. Higher entropy indicates a distribution closer to uniform (maximum exploration), while lower entropy indicates clustering.
- **Core assumption:** The underlying distribution of "good" sampling strategies trends toward uniformity in the input space, and the KL estimator provides a consistent estimate despite the non-i.i.d. nature of BO sampling.
- **Evidence anchors:**
  - [section 3.2] Defines Observation Entropy (OE) and specifies the use of the KL estimator for non-parametric density estimation.
  - [appendix a.3] Discusses the consistency and bias of the KL estimator, noting that theoretical guarantees assume i.i.d. samples, which BO violates.
  - [corpus] Related work in "Multi-fidelity Bayesian Optimization" discusses balancing exploration/exploitation, but no specific mechanism for entropy-based measurement is cited in the neighbors.
- **Break condition:** High-dimensional spaces ($d > 20$), where the KL estimator suffers from significant bias and computational cost, making OTSD the preferred metric.

### Mechanism 3
- **Claim:** Thompson Sampling (TS) transitions from balanced behavior in low dimensions to excessive exploration in high dimensions, degrading performance.
- **Mechanism:** TS samples from the GP posterior. In high dimensions, the GP struggles to learn the objective function accurately, often maintaining high posterior variance in unexplored regions. This causes TS to sample widely (over-explore) rather than focusing on promising areas, unlike EI or UCB which rely on mean prediction or bounded uncertainty.
- **Core assumption:** The dimensionality of the problem directly impacts the GP's ability to collapse posterior variance, thereby triggering the exploratory bias in TS.
- **Evidence anchors:**
  - [section 4.4] States that "TS is considerably more explorative in high-dimensional benchmarks" and "surpassing even RS [Random Search] in terms of normalized OTSD."
  - [section 4.5] Notes that the best-performing methods tend not to exhibit extreme exploration quantities.
  - [corpus] "Direct Regret Optimization" mentions traditional methods often operate myopically, contrasting with the global view of TS, but does not specifically validate the dimensionality-exploration link.
- **Break condition:** The use of Trust Regions (TRs) or RAASP, which artificially constrain the sampling space, thereby suppressing the high-dimensional over-exploration of TS.

## Foundational Learning

- **Concept:** **Gaussian Process (GP) Surrogates**
  - **Why needed here:** The paper relies on GPs to model the objective function, providing the mean ($\mu$) and variance ($\sigma$) that acquisition functions (AFs) use to select points. Understanding the GP is necessary to understand why TS (sampling from the posterior) behaves differently from EI (using mean/variance expectations).
  - **Quick check question:** How does the posterior variance of a GP typically behave as observations are added near a specific point?

- **Concept:** **Exploration vs. Exploitation Trade-off (EETO)**
  - **Why needed here:** The core contribution of the paper is quantifying this trade-off. You must distinguish between "searching widely" (exploration, high OTSD/OE) and "refining known peaks" (exploitation, low OTSD/OE) to interpret the taxonomy results.
  - **Quick check question:** If an acquisition function selects a point with high predicted variance but low predicted mean, is it prioritizing exploration or exploitation?

- **Concept:** **Traveling Salesman Problem (TSP)**
  - **Why needed here:** OTSD is a direct application of TSP. The paper uses an insertion heuristic (approximation) to solve it. Understanding that this is an NP-hard problem explains why the authors use an approximation algorithm with $O(dT^2)$ complexity rather than an exact solution.
  - **Quick check question:** Why does the normalized OTSD generally decrease or stay constant as the number of iterations increases, even though the raw tour length grows?

## Architecture Onboarding

- **Component map:** Input (Observation history + Search Space) -> Surrogate (Gaussian Process) -> Acquisition Function (EI, TS, UCB, etc.) -> Metrics Module (OTSD Calculator + OE Calculator) -> Optimizer (Update surrogate and AF loop)

- **Critical path:** The metrics module (OTSD/OE) is an *observe-only* component. It does not guide the optimization in this paper; it only diagnoses it. The critical path for *implementation* is ensuring the observation coordinates are correctly normalized to $[0,1]^d$ before calculating OTSD/OE to ensure comparability.

- **Design tradeoffs:**
  - **OTSD vs. OE:** Use **OTSD** for high-dimensional problems ($d > 20$) as it is computationally faster ($<200$ms vs $>5$s) and robust. Use **OE** only for low dimensions where density estimation is reliable.
  - **Normalization:** OTSD requires normalization by the theoretical upper bound $\Psi(d,t)$ to compare across different dimensionalities. OE is non-monotonic and harder to compare across different domain scalings.

- **Failure signatures:**
  - **OTSD approaching 0:** The optimizer is stuck (Deterministic/over-exploitation).
  - **OTSD of an AF > OTSD of Random Search:** The AF is excessively explorative (likely TS in high dimensions) and likely wasting budget.
  - **OE > 0:** Indicates the sample distribution is effectively "more uniform than uniform" (often a density estimation artifact or boundary effect), which should be treated as a maximum exploration signal.

- **First 3 experiments:**
  1. **Baseline Validation:** Run Random Search (RS) vs. Deterministic selection on a synthetic function (e.g., Hartmann 6D). Verify that RS maintains a consistently high normalized OTSD while Deterministic selection drops to near zero.
  2. **Dimensionality Stress Test:** Compare Thompson Sampling (TS) vs. Expected Improvement (EI) on a high-dimensional problem (e.g., Rover $d=60$). Plot OTSD over time to confirm TS becomes more explorative than RS (over-exploration) while EI remains balanced.
  3. **Ablation on Batching:** Run EI with batch sizes $q=1$ vs $q=32$. Verify that increasing batch size increases the normalized OTSD (promoting exploration).

## Open Questions the Paper Calls Out

- **Open Question 1:** How do Gaussian Process hyperparameters and hyperpriors influence the exploration behavior quantified by OTSD and OE?
  - **Basis in paper:** [explicit] The authors state in the "Limitations and Future Work" section that the results "do not consider the effect of GP hyperparameters and hyperpriors on the behavior of AFs."
  - **Why unresolved:** The current empirical study fixes the GP framework (SingleTaskGP) and does not isolate how variations in hyperparameter settings (e.g., lengthscale priors) shift the exploration metrics.
  - **What evidence would resolve it:** Ablation studies showing changes in normalized OTSD and OE values for the same acquisition functions under different hyperparameter configurations and hyperprior distributions.

- **Open Question 2:** Can OTSD and OE be effectively adapted to quantify exploration in non-Euclidean domains such as graphs or protein sequences?
  - **Basis in paper:** [explicit] The "Limitations and Future Work" section mentions the intent to "study if OTSD and OE can be extended to other domains, such as non-Euclidean spaces."
  - **Why unresolved:** While OTSD can theoretically use a metric, the authors note on Page 5 that the OE estimation relies on Euclidean properties, making its application to irregular spaces a "case-by-case problem that requires further investigation."
  - **What evidence would resolve it:** Derivation of a valid entropy estimator for graph spaces or a modified OTSD metric that correlates with optimization performance on non-Euclidean benchmark tasks.

- **Open Question 3:** To what extent does the choice of kernel and likelihood functions affect the exploration taxonomy of acquisition functions?
  - **Basis in paper:** [explicit] The authors explicitly list the need to "study the effect of kernel and likelihood functions and their hyperparameters on the behavior of AFs" in the "Limitations and Future Work" section.
  - **Why unresolved:** The experiments utilize a standard setup (default SingleTaskGP with a specific kernel), leaving the sensitivity of the exploration metrics to different surrogate model components unknown.
  - **What evidence would resolve it:** Empirical comparisons showing the ranking of acquisition functions by OTSD/OE remains consistent when swapping the Mat√©rn kernel for a Spectral kernel or changing the noise likelihood.

## Limitations

- OTSD metric assumes Euclidean geometry, limiting its applicability to non-Euclidean input spaces without custom distance functions
- OE metric relies on the Kozachenko-Leonenko estimator, which has theoretical consistency guarantees only for i.i.d. samples - a condition violated in BO's sequential sampling
- The paper establishes correlation between extreme exploration levels and worse optimization performance but does not definitively prove causation

## Confidence

- **High confidence:** OTSD and OE successfully distinguish exploration levels between acquisition functions across benchmarks; TS exhibits excessive exploration in high dimensions; batching increases exploration while TRs and RAASP reduce it
- **Medium confidence:** The correlation between normalized OTSD and OE validates their reliability as complementary metrics; the observed performance degradation for extreme exploration/exploitation is consistently supported but the causal relationship requires further investigation
- **Low confidence:** The claim that TS's high-dimensional over-exploration stems specifically from GP posterior variance collapse is inferred but not directly validated; the paper does not test alternative surrogate models or quantify the GP's posterior uncertainty behavior

## Next Checks

1. **Robustness Test:** Apply OTSD and OE to non-Euclidean problems (e.g., graph-based BO) using appropriate distance metrics to validate their generalizability beyond Euclidean domains
2. **Estimator Validation:** Compare OE estimates from KL estimator against alternative density estimators (e.g., kNN with varying k, kernel density) on low-dimensional benchmarks to quantify bias and variance
3. **Causal Analysis:** Conduct ablation studies isolating the effect of exploration on performance by artificially constraining AFs to specific exploration levels, controlling for other factors like batch size and trust region usage