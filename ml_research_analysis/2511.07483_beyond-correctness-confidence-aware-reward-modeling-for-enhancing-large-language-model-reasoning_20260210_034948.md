---
ver: rpa2
title: 'Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language
  Model Reasoning'
arxiv_id: '2511.07483'
source_url: https://arxiv.org/abs/2511.07483
tags:
- reward
- training
- reasoning
- arxiv
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces C2RM, a reward model that incorporates both
  correctness and confidence to guide small-scale language models in STEM reasoning
  tasks. The model penalizes low-confidence correct answers to encourage more robust
  reasoning, addressing the limitations of purely rule-based reinforcement learning
  that often rewards accidental correct answers from smaller models.
---

# Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning

## Quick Facts
- arXiv ID: 2511.07483
- Source URL: https://arxiv.org/abs/2511.07483
- Authors: Qianxi He; Qingyu Ren; Shanzhe Lei; Xuhong Wang; Yingchun Wang
- Reference count: 23
- Primary result: C2RM achieves 64.28% accuracy on JudgeBench, surpassing open-source reward models by penalizing low-confidence correct answers to improve STEM reasoning

## Executive Summary
This paper introduces C2RM, a confidence-aware reward model that addresses limitations in current reward modeling for STEM reasoning tasks. Unlike traditional approaches that only reward correct answers, C2RM penalizes low-confidence correct responses to prevent rewarding accidental correctness from flawed reasoning chains. The model uses a two-step data selection strategy to create high-quality contrastive training pairs across science, math, and logic domains, training on approximately 20K pairs. Evaluation shows C2RM achieves strong performance across multiple benchmarks while maintaining longer, more comprehensive responses during RL training.

## Method Summary
C2RM employs a two-step data filtering process: first filtering datasets to 40-70% accuracy range, then selecting questions where the model produces both correct and incorrect answers across 5 samples. Responses are labeled as True & Certain, True & Uncertain, False & Certain, or False & Uncertain using multi-sample consistency (K rollouts with τ=0.5 threshold). Qwen2.5-7B-Instruct is fine-tuned on contrastive pairs with T&C as positive and all others as negative. During inference, the model uses pointwise scoring, outputting "Yes" or "No" with the probability of "Yes" serving as the reward score.

## Key Results
- Achieves 64.28% accuracy on JudgeBench, surpassing open-source reward models
- Maintains longer responses during RL training (245.1 tokens average vs 207.8 baseline)
- Improves policy model accuracy by 6.34% over baselines
- Ablation confirms both correctness and confidence dimensions are essential for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Penalizing low-confidence correct answers prevents rewarding accidental correctness from flawed reasoning chains.
- **Mechanism:** During RL exploration, smaller models may produce correct final answers through speculative guessing while employing low-quality reasoning chains. By treating correct-but-uncertain responses as negative examples alongside incorrect ones, the reward model signals that reasoning quality—not just output correctness—determines reward.
- **Core assumption:** Consistency across multiple rollouts correlates with reasoning quality rather than surface-level pattern matching.
- **Evidence anchors:** Abstract states model "penalizes not only incorrect answers but also low-confidence correct responses"; paper argues this introduces variance into reward signal beyond binary correctness.
- **Break condition:** If high-confidence responses can still result from memorization or superficial patterns rather than genuine reasoning, the penalty mechanism may not improve reasoning quality.

### Mechanism 2
- **Claim:** Two-step data filtering creates discriminative training pairs that improve reward model generalization.
- **Mechanism:** Step 1 filters datasets to 40-70% accuracy range (moderate difficulty). Step 2 selects questions where the model produces both correct and incorrect answers across 5 samples (showing inconsistency). This ensures training data contains meaningful contrastive signals.
- **Core assumption:** Questions that elicit inconsistent model behavior contain richer signal for learning to distinguish reasoning quality.
- **Evidence anchors:** PPO training on dataset D (filtered for discriminative questions) outperforms dataset Q (moderate difficulty only); paper uses this approach to create ~20K training pairs.
- **Break condition:** If filtered datasets underrepresent certain difficulty levels or reasoning patterns, the reward model may fail to generalize to out-of-distribution problems.

### Mechanism 3
- **Claim:** Confidence labeling via multi-sample consistency provides a scalable uncertainty signal without requiring internal model access.
- **Mechanism:** Generate K responses per question using high-temperature decoding. Compute consistency score as the fraction of samples matching each answer. Answers with consistency ≥0.5 are labeled "Certain," others "Uncertain."
- **Core assumption:** Self-consistency across temperature-sampled outputs reliably indicates model confidence in its reasoning.
- **Evidence anchors:** Confidence-only RM achieves 48.35% average accuracy; paper uses this black-box approach to avoid requiring MLP probes or logit access.
- **Break condition:** For problems where multiple valid reasoning paths exist, consistency-based labeling may incorrectly mark diverse-but-valid responses as uncertain.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - **Why needed here:** C2RM is validated through PPO-based RL training. Understanding the clipping mechanism, advantage estimation, and the role of reference policies is essential for interpreting why response length preservation matters.
  - **Quick check question:** Can you explain why PPO uses a clipped objective rather than direct policy gradient updates?

- **Concept: Reward Model Training with Contrastive Pairs**
  - **Why needed here:** C2RM constructs training data as positive-negative pairs where T&C responses are positive and all other types are negatives. Understanding pairwise vs. pointwise RM training clarifies why the paper uses pointwise scoring at inference.
  - **Quick check question:** What is the difference between training a reward model with Bradley-Terry pairwise loss versus pointwise binary classification?

- **Concept: Self-Consistency for Uncertainty Estimation**
  - **Why needed here:** The confidence labeling mechanism relies on generating multiple samples and computing agreement. This connects to broader literature on consistency-based uncertainty methods.
  - **Quick check question:** Why does high-temperature sampling help elicit diversity for consistency checking, and what are its limitations?

## Architecture Onboarding

- **Component map:**
  [Source Datasets] → [Two-Step Filter] → [Multi-Model Generation (3 LLMs × 5 samples)] → [Confidence Labeling via K-rollout consistency] → [4-way Classification: T&C / T&U / F&C / F&U] → [Pair Construction: T&C as positive, others as negative] → [SFT on Qwen2.5-7B-Instruct] → C2RM → [PPO Training Loop: Policy Model + C2RM Reward Signal]

- **Critical path:** The confidence labeling threshold (τ=0.5) and the pair construction strategy are the most sensitive design choices. Ablation shows removing confidence dimension drops average accuracy from 53.10% to 48.60% (correctness-only) or 47.81% (confidence-only).

- **Design tradeoffs:**
  - **Data volume vs. quality:** Paper uses ~20K training pairs vs. 80K+ in baselines. Smaller but higher-quality filtered data may outperform larger noisy datasets.
  - **Response length vs. optimization pressure:** URM-LLaMa-3.1-8B collapsed response length to 115 tokens; C2RM maintained 245 tokens. Longer responses correlate with more complete reasoning but increase inference cost.
  - **Pointwise vs. pairwise scoring:** Training uses pair construction but inference uses pointwise probability of "Yes" token. This simplifies deployment but may lose relative ranking signal.

- **Failure signatures:**
  - **Length collapse:** If reward model disproportionately favors concise correct answers, policy model will produce truncated reasoning. Monitor response length curves during PPO.
  - **Over-penalization:** If confidence threshold is too strict, genuinely uncertain-but-correct reasoning may be discouraged, reducing exploration.
  - **Domain overfitting:** C2RM trained on science/math/logic but evaluated on coding (69.05% on JudgeBench coding). Monitor out-of-domain generalization.

- **First 3 experiments:**
  1. **Reproduce confidence labeling:** Generate 5 samples per question from a base model at temperature 0.7. Verify that consistency scores correlate with human judgments of reasoning quality on a small held-out set.
  2. **Ablate confidence threshold:** Train C2RM variants with τ ∈ {0.3, 0.5, 0.7}. Evaluate on JudgeBench and measure response length during PPO to find the sweet spot between precision and recall.
  3. **Cross-domain validation:** Apply C2RM to a domain not in training (e.g., legal reasoning). Compare against rule-based rewards to assess whether confidence-aware penalization transfers to new reasoning patterns.

## Open Questions the Paper Calls Out
- The paper doesn't explicitly call out open questions, but the limitations section highlights several areas for future work including expanding to multimodal data and investigating the scalability of the approach to larger models.

## Limitations
- **Scalability uncertainty:** Paper only validates on 7B policy models, leaving unclear whether benefits persist for larger-scale models with more inherent knowledge.
- **Domain generalization:** Confidence estimation via self-consistency may behave differently for creative tasks or multimodal inputs; current training data lacks diversity in domain coverage.
- **Threshold sensitivity:** The confidence threshold τ = 0.5 was chosen without systematic analysis; different thresholds could significantly impact performance.

## Confidence
- **High confidence:** Primary claims about C2RM's effectiveness based on clear ablation studies and comparative results across multiple benchmarks.
- **Medium confidence:** Uncertainties regarding generalizability of confidence estimation method beyond STEM domains.
- **Low confidence:** Two-step data filtering approach's universal applicability, as effectiveness is only demonstrated on specific datasets used.

## Next Checks
1. Test C2RM's performance on non-STEM reasoning tasks (legal, ethical, or commonsense reasoning) to assess cross-domain generalization of the confidence-aware reward mechanism.
2. Evaluate the impact of varying the confidence threshold τ across different problem types to determine if a single threshold is optimal or if adaptive thresholds would improve performance.
3. Compare C2RM's effectiveness against alternative uncertainty estimation methods (e.g., Monte Carlo dropout, ensemble methods) to validate that the consistency-based approach is optimal for this application.