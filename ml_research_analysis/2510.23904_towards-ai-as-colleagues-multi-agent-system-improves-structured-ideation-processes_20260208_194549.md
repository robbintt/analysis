---
ver: rpa2
title: 'Towards AI as Colleagues: Multi-Agent System Improves Structured Ideation
  Processes'
arxiv_id: '2510.23904'
source_url: https://arxiv.org/abs/2510.23904
tags:
- participants
- system
- user
- colleagues
- ideation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultiColleagues, a multi-agent conversational
  system that enables AI agents to act as colleagues by conversing with each other,
  sharing ideas, and involving users in collaborative ideation. In a within-subjects
  study with 20 participants, the system was compared to a single-agent baseline.
---

# Towards AI as Colleagues: Multi-Agent System Improves Structured Ideation Processes

## Quick Facts
- arXiv ID: 2510.23904
- Source URL: https://arxiv.org/abs/2510.23904
- Reference count: 40
- Primary result: Multi-agent conversational system enables AI agents to act as colleagues, significantly improving user engagement, social presence, and perceived outcome quality in ideation tasks compared to single-agent baseline

## Executive Summary
This paper introduces MultiColleagues, a multi-agent conversational system that enables AI agents to act as colleagues by conversing with each other, sharing ideas, and involving users in collaborative ideation. In a within-subjects study with 20 participants, the system was compared to a single-agent baseline. MultiColleagues fostered stronger perceived social presence and team-like interaction, with participants reporting higher engagement, more complementary perspectives, and greater perceived outcome quality and novelty. Users contributed nearly twice as many utterances and explored more parallel idea directions than with the baseline. These findings demonstrate that multi-agent systems can shift ideation from tool use toward dynamics that resemble collegial teamwork, advancing AI toward more interactive, team-based collaboration.

## Method Summary
The study employed a within-subjects design with 20 participants completing ideation tasks using both a multi-agent system (MultiColleagues) and a single-agent baseline. MultiColleagues uses GPT-4o to instantiate 9 distinct AI personas (e.g., UX Designer, System Architect) that converse with each other while involving the user as a facilitator. The system features explicit Explore/Focus modes for divergent and convergent thinking, and an AI facilitator that provides periodic synthesis. Participants completed one ideation task with each condition in randomized order, with condition order counterbalanced across participants. The study measured social presence, team dynamics, perceived outcome quality, and user engagement through surveys and interaction logs.

## Key Results
- Users reported significantly higher social presence (M=5.80 vs 5.05) and perceived teammate-like interaction (M=5.75 vs 5.05) with multi-agent system
- User engagement nearly doubled, with participants contributing 2x more utterances in the multi-agent condition
- Participants rated outcomes as higher in quality (M=6.00 vs 5.10) and novelty (M=5.70 vs 5.20) with multi-agent system
- The system fostered more complementary perspectives (M=6.05 vs 5.05) and parallel idea exploration

## Why This Works (Mechanism)

### Mechanism 1: Persona-Based Role Differentiation
Assigning distinct professional roles to different AI agents enables complementary perspectives that broaden the solution space during ideation. The system instantiates multiple AI personas with specific instructions, creating diverse viewpoints and preventing a single-voice trajectory. Users perceived these as distinct collaborators, reporting higher complementary strengths and teammate-like feel compared to the single-agent baseline.

### Mechanism 2: Structured Thinking Modes (Explore/Focus)
Providing explicit user-controlled shifts between divergent (Explore) and convergent (Focus) thinking modes improves user agency and perceived flexibility. A template-based approach modifies AI behavior based on the selected mode, with users valuing the ability to control the creative process and rated adaptive thinking mode significantly higher.

### Mechanism 3: Facilitator-Orchestrated, Digestible Rhythm
An AI facilitator that provides periodic synthesis and a conversational rhythm with "strategic interaction points" increases user engagement and sense of control. The system delivers contributions incrementally rather than in a single long output, preventing cognitive overload and encouraging active participation.

## Foundational Learning

- **Social Presence & Team Dynamics**: Understanding how perceived social presence, team cognition, and facilitative leadership work in human teams is essential to designing AI interactions that feel collaborative. Quick check: Can you explain why distributing authority across multiple agents might make a user feel more like a "facilitator" than a "boss"?

- **Double Diamond Design Process**: The system's Explore/Focus modes are grounded in the double diamond methodology (divergent and convergent thinking). This design framework is the theoretical basis for the system's core interaction pattern. Quick check: What is the primary goal of the first "diamond" (divergent thinking) in this methodology?

- **Cognitive Load Theory**: A key design problem this paper addresses is preventing "information overload" from uninterrupted AI generation. The design of "strategic interaction points" is a direct application of managing cognitive load. Quick check: Why might an "efficient" long, single response from an AI be detrimental to the creative process?

## Architecture Onboarding

- **Component map**: React Frontend -> Flask API -> Persona Orchestration Engine -> GPT-4o (LLM Core) -> Memory/State Manager

- **Critical path**: User submits problem and selects 3+ AI personas → Orchestration Engine initializes personas and chooses first speaker → First persona generates response → Engine awaits user action → Next Speaker Selector ranks candidates and selects speaker → Selected persona responds → User can toggle modes or trigger facilitator

- **Design tradeoffs**: 
  - Autonomy vs. Control: More autonomous but can drift; single-agent is controlled but effort-intensive
  - Breadth vs. Depth: Excels at breadth early; single-agent better for depth and execution later
  - Complexity vs. Accessibility: Steeper learning curve but supports richer interaction

- **Failure signatures**: 
  - Conversational Drift: Discussion moves away from original problem without intervention
  - Homogenized Voices: Personas start sounding similar, losing distinct roles
  - Cognitive Saturation: User stops engaging and becomes passive observer
  - Context Window Overflow: Long conversations exceed model's context limit

- **First 3 experiments**:
  1. Persona Ablation Test: Compare ideation quality with 1, 3, and 5 AI personas to quantify role diversity benefits
  2. Mode Switching Impact: Compare sessions with manual vs. auto/no mode switching to validate adaptive thinking mechanism
  3. Facilitator Frequency Test: Experiment with facilitator interventions every 3, 5, or 7 turns to optimize engagement rhythm

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do AI colleagues support full cycles of exploration, debate, and convergence in long-term deployments beyond short, ten-minute sessions?
- Basis in paper: Section 6.6 states that "Longer term deployments are needed to understand how AI colleagues support full cycles of exploration, debate and convergence."
- Why unresolved: The study was limited to a single ten-minute session, capturing immediate coordination rather than sustained team formation or complex problem evolution.
- What evidence would resolve it: Longitudinal studies tracking team dynamics, idea maturation, and user trust over multi-day or multi-week projects.

### Open Question 2
- Question: Can heterogeneous model setups or mechanisms that enforce divergent stances improve the diversity of perspectives compared to single-model instantiations?
- Basis in paper: Section 6.6 notes that all personas were instantiated from the same language model, which "limited diversity of perspectives," and suggests investigating "heterogeneous model setups."
- Why unresolved: The current implementation used GPT-4o for all agents, which sometimes produced similar response styles despite distinct role prompts.
- What evidence would resolve it: Comparative studies measuring semantic diversity and conflict rates in systems using different underlying models for different personas.

### Open Question 3
- Question: To what extent do LLM-based originality ratings align with human judgments, particularly when users perceive high novelty but automated metrics show non-significant trends?
- Basis in paper: Section 6.6 highlights that "independent validation on the alignment between LLM-generated ratings and human judgements has not been performed."
- Why unresolved: The study relied on GPT-5 for originality scoring, which diverged from participant self-reports; there is a risk that LLM evaluators may assess surface features differently than human experts.
- What evidence would resolve it: Triangulation of idea scores using both LLM-based judges and blinded human expert raters to validate the automated metric.

### Open Question 4
- Question: How does encoding differences in seniority and decision rights among AI colleagues affect synthesis quality and the user's ability to manage convergence?
- Basis in paper: Section 6.2.2 suggests future systems should "encode clearer differences in seniority and decision rights among colleagues" to improve evaluation confidence.
- Why unresolved: Current personas were differentiated primarily by occupation, which sometimes left participants uncertain about who had authority for specific issues.
- What evidence would resolve it: A controlled study comparing task outcomes in systems with flat role structures versus those with explicit hierarchies and weighted decision rights.

## Limitations

- The study's sample size of 20 participants and single ideation task per participant constrain generalizability of findings
- The system's reliance on GPT-4o means results may not transfer to other LLMs or future model versions
- Evaluation metrics using "GPT-5" for analysis raise questions about reproducibility since GPT-5 was not publicly available at the time

## Confidence

- **High Confidence**: Multi-agent system increased user engagement (utterance count nearly doubled) and improved perceived social presence and team dynamics. These effects are directly observable from user interaction data and self-reported measures.
- **Medium Confidence**: Claimed improvements in outcome novelty and quality are supported by participant ratings, but evaluation methodology using AI-based scoring tools limits independent verification.
- **Low Confidence**: Specific mechanisms (persona differentiation, mode switching, facilitator rhythm) are theoretically sound but lack strong empirical validation; limited evidence that these mechanisms, rather than general novelty of multi-agent interaction, drive observed benefits.

## Next Checks

1. **Persona Differentiation Test**: Conduct an ablation study comparing ideation quality with 1, 3, and 5 AI personas to quantify whether role diversity directly correlates with idea novelty and breadth.

2. **Mode Switching Validation**: Design a controlled experiment where participants either manually switch between Explore/Focus modes or use a system with no mode switching. Measure differences in user satisfaction, control perception, and final output quality.

3. **Facilitator Frequency Optimization**: Systematically test different facilitator intervention frequencies (every 3, 5, or 7 turns) to identify the optimal rhythm that maximizes engagement while minimizing cognitive overload and conversational drift.