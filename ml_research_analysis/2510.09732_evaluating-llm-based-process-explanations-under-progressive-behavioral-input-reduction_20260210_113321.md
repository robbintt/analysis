---
ver: rpa2
title: Evaluating LLM-Based Process Explanations under Progressive Behavioral-Input
  Reduction
arxiv_id: '2510.09732'
source_url: https://arxiv.org/abs/2510.09732
tags:
- process
- explanations
- logs
- quality
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores whether high-quality LLM-based process explanations
  can be generated from progressively reduced behavioral inputs. Using synthetic event
  logs, we discovered process models from increasingly smaller log prefixes and evaluated
  the resulting explanations with a second LLM.
---

# Evaluating LLM-Based Process Explanations under Progressive Behavioral-Input Reduction

## Quick Facts
- **arXiv ID:** 2510.09732
- **Source URL:** https://arxiv.org/abs/2510.09732
- **Reference count:** 21
- **Primary result:** High-quality LLM-based process explanations can be generated from progressively reduced behavioral inputs, with quality plateauing around 10,000 events.

## Executive Summary
This study investigates whether LLM-generated process explanations maintain quality when derived from progressively reduced behavioral inputs. Using synthetic job-shop scheduling logs, researchers progressively sampled event prefixes from 10 to 100,000 events, discovered process models using the Inductive Miner, and had LLMs generate explanations evaluated by a second LLM. Results show explanation quality improves non-linearly with input size, reaching a practical knee between 100-1,000 events where scores rise from ~6.5 to ~6.9, with modest improvements beyond 10,000 events. The findings suggest meaningful explanations can be derived from significantly reduced inputs, offering computational efficiency for resource-constrained process analysis.

## Method Summary
The study employed prefix sampling to progressively reduce behavioral input from synthetic job-shop scheduling logs, extracting the first k events where k ranged from 10 to 100,000. For each sub-log, the Inductive Miner (via PM4Py) discovered process models represented as Petri nets. These models were then provided to Llama 3.3-70B (LLM1) to generate textual explanations covering three aspects: end-to-end process description, bottleneck identification, and improvement suggestions. A separate Llama instance (LLM2) evaluated each explanation against the full model using a structured rubric, scoring completeness, bottleneck identification, and improvements on a 1-10 scale. Each configuration was run 5 times and averaged to reduce variance.

## Key Results
- Explanation quality improves non-linearly with input size, plateauing after approximately 10,000 events
- A practical knee exists between 100-1,000 events where average scores rise from ~6.5 to ~6.9
- Explanations are most satisfactory when covering end-to-end flow, identifying bottlenecks, and proposing improvements
- Beyond 10,000 events, improvements are modest, suggesting cost-quality trade-offs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explanation quality improves non-linearly with behavioral input size, plateauing once sufficient process structure is captured.
- **Mechanism:** Prefix sampling initially under-samples rare variants and loop structures, yielding under-specified models. As k increases, the behavioral abstraction captures more control-flow detail (sequences, parallelism, loops), enabling richer explanations. Beyond a coverage threshold, additional events provide diminishing marginal information.
- **Core assumption:** The first k events are representative enough of common behavior; rare variants occurring later do not dominate explanation quality.
- **Evidence anchors:**
  - [abstract] "explanation quality improves with input size but plateaus, with a practical knee between 100-1,000 events where average scores rise from ~6.5 to ~6.9. Beyond 10,000 events, improvements are modest"
  - [Section 5.3] "k≈1,000 offers a stable mid-range when latency or budget constraints apply, while k≈100,000 can be justified in higher-stakes settings"
  - [corpus] Related work on efficient conformance checking (arXiv:2505.21506) similarly addresses computational limits with long event sequences, supporting relevance of input reduction strategies.
- **Break condition:** Prefix sampling fails when critical behavior appears predominantly in late traces or rare variants; stratified/variant-aware sampling would be required.

### Mechanism 2
- **Claim:** LLM-generated explanations can be meaningfully evaluated by a second LLM using structured rubrics, producing comparative signals rather than ground truth.
- **Mechanism:** LLM2 receives the explanation Ek and full model M, then scores three dimensions (completeness, bottleneck identification, improvements) on a 1-10 scale. Averaging multiple runs reduces variance from prompt sensitivity and stochastic generation.
- **Core assumption:** LLM-as-judge correlates sufficiently with human expert judgment for relative comparisons; rubric-based scoring constrains subjectivity.
- **Evidence anchors:**
  - [abstract] "the scores are LLM-based (comparative signals rather than ground truth)"
  - [Section 6] "We therefore treat scores as comparative signals, not ground truth. Future work should triangulate with human ratings by process analysts"
  - [corpus] arXiv:2502.20635 examines LLM-assisted evaluation of ML explanation quality, noting similar limitations; no strong consensus on benchmarks exists.
- **Break condition:** Evaluator LLM exhibits systematic bias (e.g., favoring certain phrasing styles) or fails to detect hallucinated activities not present in M.

### Mechanism 3
- **Claim:** Behavioral abstraction format and discovery algorithm choice affect explanation quality by altering structure and label granularity exposed to the LLM.
- **Mechanism:** Inductive Miner produces block-structured, sound Petri nets that may over-approximate behavior, improving readability but potentially obscuring detail. Alternative formats (DFGs, BPMN) or algorithms (Heuristics Miner, Alpha Miner) change how control-flow is represented textually.
- **Core assumption:** The LLM can accurately parse and reason over the serialized model representation.
- **Evidence anchors:**
  - [Section 5.4] "Inductive Miner's block-structured nets are sound and may over-approximate behavior, which can improve readability but alter the abstraction exposed to the LLM"
  - [Section 6] "Results may also depend on discovery and notation choices"
  - [corpus] No direct corpus evidence on abstraction format effects; this remains an underexplored area requiring explicit acknowledgment.
- **Break condition:** Over-approximation obscures bottlenecks; under-approximation yields unsound models the LLM misinterprets.

## Foundational Learning

- **Concept:** Process Mining and Behavioral Abstractions (event logs, traces, Petri nets, DFGs)
  - **Why needed here:** The entire pipeline assumes familiarity with how event logs are transformed into process models that LLMs interpret.
  - **Quick check question:** Given an event log with activities A→B→C occurring in 80% of traces and A→C directly in 20%, what would a directly-follows graph show?

- **Concept:** LLM Prompting for Structured Output
  - **Why needed here:** LLM1 must generate explanations with three specific components; prompt design directly affects output quality and evaluability.
  - **Quick check question:** How would you prompt an LLM to ensure it always outputs bottleneck identification in a parseable format?

- **Concept:** Sampling Strategies and Coverage Trade-offs
  - **Why needed here:** Prefix sampling is simple but excludes late/rare behavior; understanding alternatives (stratified, variant-aware) is critical for real-world deployment.
  - **Quick check question:** If 5% of traces contain a critical compliance violation appearing only after event 50,000, will prefix sampling with k=10,000 capture it?

## Architecture Onboarding

- **Component map:**
  - Raw event log L (XES format) → prefix extraction for k events → Inductive Miner → behavioral model Mk (Petri net) → LLM1 → textual explanation Ek → LLM2 → scores (completeness, bottlenecks, improvements) averaged over 5 runs

- **Critical path:** Prefix size k → model complexity → explanation richness → score convergence. The 100-1,000 event range is the practical operating zone for cost-sensitive deployments.

- **Design tradeoffs:**
  - Smaller k: Lower latency/cost, but risks missing rare variants and late-appearing behavior
  - Larger k: Higher consistency and coverage, but diminishing returns beyond ~10,000 events
  - LLM-as-judge: Scalable evaluation, but introduces subjectivity; requires triangulation with human or objective metrics for production

- **Failure signatures:**
  - Scores <5.0 at mid-range k: Likely incomplete model discovery or prompt failure
  - High variance across 5 runs: Evaluator instability; increase samples or refine rubric
  - No score improvement beyond k=1,000: Process may be simple; check if full model adds meaningful structure

- **First 3 experiments:**
  1. **Baseline calibration:** Run pipeline on synthetic logs with k ∈ {100, 1000, 10000}; verify knee around 100-1,000 and plateau beyond 10,000 as reported.
  2. **Sampling strategy comparison:** Replace prefix sampling with variant-aware sampling at same k values; measure whether rare variants improve bottleneck detection scores.
  3. **Evaluator triangulation:** Have a process analyst score a subset of explanations; correlate with LLM2 scores to assess whether comparative signals align with human judgment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed cost-quality trade-off (the "knee" at 100–1,000 events) generalize to real-world event logs with noise and heterogeneity?
- Basis in paper: [explicit] The authors state in the Threats to Validity section that findings are based on synthetic logs and "Validation on organizational logs across domains is needed."
- Why unresolved: Synthetic job-shop data provides control but lacks the semantic ambiguity and noise of industrial processes.
- What evidence would resolve it: Replicating the experimental pipeline on diverse, real-world event logs (e.g., healthcare or finance) to see if the plateau point shifts.

### Open Question 2
- Question: How well do LLM-as-judge scores correlate with human expert assessments of explanation utility?
- Basis in paper: [explicit] The paper notes that using an LLM to grade another LLM introduces subjectivity and suggests future work should "triangulate with... human ratings by process analysts."
- Why unresolved: LLMs may prioritize linguistic fluency or specific phrasing styles over factual correctness or actionable insight.
- What evidence would resolve it: A user study where process mining experts score the explanations using the same rubric, followed by a correlation analysis with the LLM scores.

### Open Question 3
- Question: Does the input-reduction curve change if representative sampling (variant-aware or stratified) is used instead of simple prefixing?
- Basis in paper: [explicit] Section 3.1 notes that prefix sampling "may exclude infrequent variants," and Section 6 suggests "alternative strategies... could alter the curve."
- Why unresolved: Prefixing captures early behavior but may miss rare or late-occurring critical variants, potentially biasing the quality assessment at lower event counts.
- What evidence would resolve it: Comparing explanation quality scores at fixed event counts (k) using prefix sampling vs. stratified sampling strategies.

### Open Question 4
- Question: Is the quality plateau robust across different process discovery algorithms and model notations?
- Basis in paper: [explicit] The Conclusion states that a "systematic analysis over discovery methods and model forms is an important next step."
- Why unresolved: The study used the Inductive Miner (which produces sound, block-structured nets); different algorithms (e.g., Heuristics Miner) produce different graph structures that may be harder or easier for LLMs to interpret.
- What evidence would resolve it: Re-running the reduction pipeline using alternative miners (e.g., Heuristics, Alpha) and notations (e.g., DFG, BPMN) to compare the resulting quality curves.

## Limitations

- The study relies on synthetic logs that may not capture real-world complexity including noise, non-deterministic branching, and data dependencies
- LLM-as-judge evaluation introduces inherent subjectivity that cannot be fully resolved without extensive human validation
- Prefix sampling systematically excludes rare variants and late-appearing behaviors that could be critical for certain domains

## Confidence

- **High Confidence:** The non-linear relationship between input size and explanation quality (plateau effect) - this is empirically observable and aligns with information theory principles
- **Medium Confidence:** The specific knee point identification (100-1,000 events) - while supported by data, this may shift with different process types or discovery algorithms
- **Low Confidence:** The practical applicability of these findings to high-stakes domains requiring near-complete process coverage, as the study focuses on efficiency trade-offs rather than completeness requirements

## Next Checks

1. **Human Triangulation Study:** Have 3-5 process analysts independently score a stratified sample of explanations (spanning low, medium, and high k values) to establish correlation with LLM2 scores and identify systematic evaluator biases

2. **Rare Variant Impact Analysis:** Create controlled experiments with known rare critical behaviors (e.g., compliance violations) and test whether prefix sampling with k=1,000 consistently misses these, quantifying the risk for safety-critical applications

3. **Alternative Discovery Algorithm Comparison:** Repeat the full pipeline using Heuristics Miner and Alpha Miner instead of Inductive Miner to assess whether the 100-1,000 event knee point holds across different behavioral abstraction approaches