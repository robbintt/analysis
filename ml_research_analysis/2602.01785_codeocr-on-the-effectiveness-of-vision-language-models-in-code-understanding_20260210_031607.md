---
ver: rpa2
title: 'CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding'
arxiv_id: '2602.01785'
source_url: https://arxiv.org/abs/2602.01785
tags:
- code
- visual
- compression
- text
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the first comprehensive study of using multimodal\
  \ large language models (MLLMs) for code understanding through image-based representation.\
  \ The core method renders source code as images and leverages visual compression\
  \ via resolution scaling, achieving up to 8\xD7 token reduction while maintaining\
  \ readability for vision-capable models."
---

# CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding

## Quick Facts
- arXiv ID: 2602.01785
- Source URL: https://arxiv.org/abs/2602.01785
- Reference count: 24
- This paper presents the first comprehensive study of using multimodal large language models (MLLMs) for code understanding through image-based representation, achieving up to 8× token reduction while maintaining readability.

## Executive Summary
This paper systematically evaluates multimodal large language models (MLLMs) on code understanding tasks using image-based code representations. The core method renders source code as images and leverages visual compression via resolution scaling, achieving up to 8× token reduction while maintaining readability for vision-capable models. Experimental results across four tasks—code completion, summarization, clone detection, and question answering—demonstrate that MLLMs can effectively understand code images, with some models achieving comparable or superior performance to text-based approaches. Notably, models like Gemini-3-Pro maintain stable performance even at 8× compression (12.5% of original tokens), while visual enhancements like syntax highlighting provide additional benefits at moderate compression levels.

## Method Summary
The study renders source code to images using Pygments for syntax highlighting and Pillow for image generation, with a base resolution of 2240×2240 pixels using VS Code default monospace font. Compression is achieved through bilinear downsampling to target resolutions based on desired compression ratios (1×–8×). Three rendering styles are tested: plain, bold (+1px offsets per glyph), and syntax-highlighted (VS Code Default Light theme). The method compares image-based code representations against text baselines across four tasks using accessible MLLMs via OpenRouter API, with 5-run statistical validation via Wilcoxon signed-rank test.

## Key Results
- MLLMs achieve up to 8× compression (12.5% tokens) while maintaining or exceeding text baseline performance on semantic tasks
- Gemini-3-Pro achieves 79.5% accuracy on code QA at 8× compression vs. 74.8% text baseline
- Visual enhancements like syntax highlighting improve performance at 1–4× compression but provide diminishing returns at 8×

## Why This Works (Mechanism)

### Mechanism 1
MLLMs can process compressed code images at 2–8× token reduction while maintaining or exceeding text-baseline performance on semantic tasks. Resolution downscaling reduces visual token count proportionally while preserving structural patterns—indentation, block boundaries, and spatial relationships—that MLLMs interpret holistically via patch-based visual encoders. Moderate compression may act as a denoising filter, blurring syntactic noise and encouraging semantic focus.

### Mechanism 2
Visual enhancements (syntax highlighting, bold rendering) improve performance at 1–4× compression but provide diminishing returns at 8×. Color-coded keywords and increased stroke width add discriminative visual features that patch embeddings can leverage when resolution is sufficient. These cues reinforce structural parsing without requiring explicit syntactic rules.

### Mechanism 3
Information degradation under compression follows a predictable hierarchy: token-level errors first (1–2×), then line-level (2–4×), then block-level (4–8×). As resolution decreases, fine-grained character details degrade before structural patterns. Token errors emerge early but may not impair semantic tasks; block errors indicate total loss of coherent code structure.

## Foundational Learning

- **MLLM Architecture (Vision Encoder → V-L Adapter → LLM Backbone)**: Understanding how code images become unified multimodal tokens clarifies why compression affects different tasks differently. *Quick check*: Can you explain why patch-based visual encoding enables "holistic" structure perception versus sequential token processing?

- **Compression Ratio = Visual Tokens / Text Tokens**: The paper defines compression in terms of token equivalence, not pixel dimensions—critical for cost/latency calculations. *Quick check*: If a code snippet has 200 text tokens, how many visual tokens should an 8× compressed image target?

- **Semantic vs. Precision Tasks**: Clone detection and summarization tolerate compression better than code completion because they rely on high-level patterns rather than exact token sequences. *Quick check*: Why might moderate compression *improve* clone detection performance according to the paper?

## Architecture Onboarding

- **Component map**: Code input → Rendering configuration → Image generation → Resolution calculation → Compression → Token estimation → API call
- **Critical path**: Visual Rendering Module (Pygments + Pillow) renders code to 2240×2240 images → Compression Engine applies bilinear downsampling → Token Budget Calculator maps compression ratio to visual token count → MLLM Interface sends rendered images + text instructions to model
- **Design tradeoffs**: Higher compression = lower cost but higher block-error risk; 4× appears to be a practical "sweet spot" for most models. Bold rendering improves visibility at 1–4× but can reduce character distinguishability at 8×. Syntax highlighting requires color-aware rendering; adds preprocessing overhead but yields 1–3% gains at moderate compression.
- **Failure signatures**: Token error cascade (frequent character confusion indicates compression threshold exceeded), block hallucination (model generates unrelated code → switch to lower compression), model-specific cliffs (GLM-4.6v shows sharp QA accuracy drop between 4× and 8×).
- **First 3 experiments**: 1) Baseline parity test: Compare text vs. 1× image input on target task and model; 2) Compression sweep: Test 1×, 2×, 4×, 8× on held-out validation set; 3) Rendering ablation: At best-performing compression level, compare plain vs. highlight vs. bold.

## Open Questions the Paper Calls Out

- Can adaptive rendering strategies that dynamically adjust visual enhancements based on target compression ratio significantly outperform static rendering configurations?
- Can code-specific visual pre-training improve MLLMs' code image understanding capabilities, particularly for models that currently exhibit significant degradation?
- Beyond 8× compression, what is the fundamental limit of visual code compression before semantic understanding catastrophically degrades?
- Do the findings generalize to other programming languages beyond Python and Java with different syntactic conventions?

## Limitations

- Model availability issues with cutting-edge MLLMs like GPT-5-mini and Gemini-3-Pro may affect reproducibility and baseline comparability
- Dataset construction risks from potential contamination despite temporal filtering, with unspecified RAG configuration parameters
- Visual rendering and compression implementation details not fully specified, limiting exact reproduction
- Evaluation methodology concerns including unspecified CompScore metric details and limited statistical validation

## Confidence

- **High Confidence**: MLLMs can process compressed code images with up to 8× token reduction while maintaining or exceeding text baseline performance on semantic tasks; Visual compression follows predictable degradation hierarchies; Different tasks show varying resilience to compression
- **Medium Confidence**: Visual enhancements like syntax highlighting improve performance at moderate compression levels; 4× compression represents a practical "sweet spot"; MLLMs demonstrate graceful degradation capabilities at higher compression levels
- **Low Confidence**: Moderate compression may act as a denoising filter that improves semantic task performance; Visual compression is a viable technical direction for more efficient code understanding inference

## Next Checks

1. **Replication of Core Findings**: Reconstruct the rendering pipeline using specified Pygments and Pillow configurations, then reproduce the 1× vs. 8× compression comparison for at least one task (e.g., code QA) across multiple accessible MLLMs to verify performance stability at high compression.

2. **Dataset Contamination Verification**: Implement the CodeQA dataset construction process using repositories created after August 2025, then conduct systematic contamination checks by running NoCtx baselines on all four tasks. If NoCtx accuracy exceeds expected random baseline levels, construct additional validation sets from post-cutoff repositories.

3. **Compression Algorithm Ablation**: Extend the compression methodology beyond bilinear downsampling to include alternative approaches (e.g., bicubic interpolation, neural compression) and evaluate their impact on the degradation hierarchy (token → line → block errors) to test whether observed compression behavior is specific to the bilinear method.