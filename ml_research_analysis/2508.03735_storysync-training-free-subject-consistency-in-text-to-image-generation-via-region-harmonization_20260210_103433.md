---
ver: rpa2
title: 'StorySync: Training-Free Subject Consistency in Text-to-Image Generation via
  Region Harmonization'
arxiv_id: '2508.03735'
source_url: https://arxiv.org/abs/2508.03735
tags:
- subject
- images
- image
- generation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of maintaining subject consistency
  across images in text-to-image diffusion models for visual storytelling. It introduces
  a training-free approach that integrates three technical innovations: masked cross-image
  attention sharing to dynamically align subject features, regional feature harmonization
  to refine subtle subject details, and base layout interpolation to maintain prompt
  adherence while enabling pose diversity.'
---

# StorySync: Training-Free Subject Consistency in Text-to-Image Generation via Region Harmonization

## Quick Facts
- arXiv ID: 2508.03735
- Source URL: https://arxiv.org/abs/2508.03735
- Authors: Gopalji Gaur; Mohammadreza Zolfaghari; Thomas Brox
- Reference count: 40
- Primary result: Training-free method for subject consistency across text-to-image generations with CLIP-I: 0.8735, DreamSim: 0.8108 on SDXL

## Executive Summary
StorySync introduces a training-free approach to maintain subject consistency across images in text-to-image diffusion models for visual storytelling. The method integrates three technical innovations: masked cross-image attention sharing to dynamically align subject features, regional feature harmonization to refine subtle subject details, and base layout interpolation to maintain prompt adherence while enabling pose diversity. It achieves state-of-the-art performance among training-free methods on SDXL, Kandinsky 3, and FLUX.1-schnell, demonstrating both quantitative superiority in similarity metrics and strong qualitative results across diverse subject types.

## Method Summary
StorySync addresses the challenge of maintaining subject consistency across images in text-to-image diffusion models for visual storytelling through a training-free approach. The method integrates three technical innovations: masked cross-image attention sharing to dynamically align subject features across images, regional feature harmonization to refine subtle subject details and maintain consistency, and base layout interpolation to preserve prompt adherence while enabling pose diversity. The approach is model-agnostic and works with multiple diffusion models including SDXL, Kandinsky 3, and FLUX.1-schnell, achieving superior performance compared to state-of-the-art training-free baselines in both quantitative metrics and qualitative assessments.

## Key Results
- Achieves CLIP-I score of 0.8735 and DreamSim score of 0.8108 on SDXL, outperforming state-of-the-art training-free approaches
- Demonstrates cross-model compatibility with SDXL, Kandinsky 3, and FLUX.1-schnell
- Shows strong qualitative results across multiple subject types including characters, objects, and animals with complex prompts

## Why This Works (Mechanism)
The method leverages three core innovations that work synergistically: masked cross-image attention sharing dynamically aligns subject features across images by selectively propagating information between corresponding regions, regional feature harmonization refines subtle subject details to maintain consistency across generations, and base layout interpolation preserves prompt adherence while enabling controlled pose variations. This combination allows the system to maintain visual coherence of subjects while preserving the creative diversity enabled by text-to-image diffusion models, all without requiring additional training or fine-tuning of the base models.

## Foundational Learning

**Cross-image attention sharing** - Why needed: Enables information propagation between different image generations to maintain subject consistency. Quick check: Verify that attention weights correctly identify corresponding subject regions across images.

**Feature harmonization** - Why needed: Refines subtle subject details to ensure consistency across multiple generations. Quick check: Measure improvement in feature similarity before and after harmonization.

**Base layout interpolation** - Why needed: Maintains prompt adherence while allowing pose diversity across generations. Quick check: Confirm that prompt alignment scores remain high while enabling varied poses.

**Subject masking** - Why needed: Isolates regions of interest for targeted consistency operations. Quick check: Validate mask accuracy against ground truth subject boundaries.

**Model-agnostic design** - Why needed: Ensures broad applicability across different diffusion architectures. Quick check: Test performance across multiple model families.

## Architecture Onboarding

**Component map**: Masked cross-image attention sharing -> Regional feature harmonization -> Base layout interpolation

**Critical path**: Subject mask generation → Cross-image attention computation → Feature harmonization → Layout interpolation → Final image generation

**Design tradeoffs**: The method prioritizes training-free operation and model compatibility over achieving absolute maximum consistency possible with fine-tuned approaches. This results in good but not perfect alignment, particularly for fine details.

**Failure signatures**: Inconsistent subject masks lead to misaligned attention sharing; complex subjects with fine details may show slight misalignment; prompt adherence may degrade if base layouts diverge significantly.

**Three first experiments**:
1. Test cross-image attention sharing with simple geometric shapes to verify basic functionality
2. Evaluate regional feature harmonization on subjects with subtle variations to measure refinement capability
3. Validate base layout interpolation maintains prompt adherence while enabling pose diversity

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on accurate subject masks, which may not be readily available for all use cases
- Occasional misalignment in fine details, particularly for highly detailed or textured subjects
- Limited systematic analysis of failure modes and robustness boundaries across diverse subject types

## Confidence
High: Quantitative improvements over baselines in similarity metrics (CLIP-I, DreamSim)
Medium: Qualitative assessments of visual storytelling quality and cross-model compatibility

## Next Checks
1. Test performance on models outside the current scope (e.g., Stable Diffusion 1.x, Midjourney, or newer diffusion variants) to verify true model-agnostic behavior
2. Evaluate automated subject mask generation approaches (like SAM or Segment Anything) to assess practical usability without manual mask annotation
3. Conduct a systematic failure mode analysis across diverse subject types (e.g., animals, vehicles, abstract concepts) to identify robustness limitations and failure patterns