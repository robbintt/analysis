---
ver: rpa2
title: Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models
  via a Peer-Review Process
arxiv_id: '2512.23213'
source_url: https://arxiv.org/abs/2512.23213
tags:
- response
- answer
- scoring
- arxiv
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces LLM-PeerReview, an unsupervised LLM ensemble
  method that selects the best response from multiple LLM-generated candidates using
  a peer-review-inspired framework. The method operates in three stages: scoring (using
  LLM-as-a-Judge with a flipped-triple scoring trick), reasoning (via averaging or
  graphical model-based truth inference), and selection (choosing the highest-scoring
  response).'
---

# Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process

## Quick Facts
- arXiv ID: 2512.23213
- Source URL: https://arxiv.org/abs/2512.23213
- Reference count: 40
- Primary result: Outperforms recent advanced methods by 6.9%-7.3% on four datasets

## Executive Summary
This paper introduces LLM-PeerReview, an unsupervised LLM ensemble method that selects the best response from multiple LLM-generated candidates using a peer-review-inspired framework. The method operates in three stages: scoring (using LLM-as-a-Judge with a flipped-triple scoring trick), reasoning (via averaging or graphical model-based truth inference), and selection (choosing the highest-scoring response). Experiments on four datasets show that LLM-PeerReview outperforms recent advanced methods, including Smoothie-Global, by 6.9% to 7.3% points, demonstrating its effectiveness in leveraging collective wisdom from diverse LLMs.

## Method Summary
LLM-PeerReview is a three-stage unsupervised ensemble method. First, each LLM judges all responses using a "flipped-triple scoring trick" that mitigates position and consistency biases by evaluating response triplets in both forward and reverse order. Second, scores are aggregated either by simple averaging or via a weighted variant using the Dawid-Skene graphical model to learn judge reliabilities. Finally, the highest-scoring response is selected. The method requires no ground truth labels or model fine-tuning, making it applicable across diverse tasks.

## Key Results
- Outperforms recent advanced methods including Smoothie-Global by 6.9%-7.3% points
- Flipped-triple scoring achieves 66.8% average accuracy, outperforming single (59.7%) and double (66.0%) strategies
- Weighted reasoning variant (67.8%) outperforms simple averaging (66.1%) on average
- Consistently outperforms baselines across all four tested datasets (TriviaQA, GSM8K, MATH, AlpacaEval)

## Why This Works (Mechanism)

### Mechanism 1: Flipped-Triple Scoring for Position and Scale Bias Mitigation
Presenting response triplets in both forward and reverse order reduces LLM judge biases by averaging 6 scores per response per judge (3 positions × 2 orderings). This cancels out position bias and provides reference effects that naive point-wise scoring lacks. The approach assumes LLM judge biases are consistent enough across orderings to cancel out while preserving response quality signals.

### Mechanism 2: Multi-Judge Score Aggregation via Crowd Truth Inference
Aggregating scores from multiple heterogeneous LLM judges yields more reliable quality estimates than any single judge. The Dawid-Skene model learns judge reliabilities via EM and weights high-quality judges more heavily. This assumes true scores are latent categorical variables and judge errors follow learnable confusion patterns.

### Mechanism 3: Selection-Based Ensemble Avoids Regeneration Artifacts
Selecting the best existing response preserves original response quality better than regenerating from selected candidates. Unlike "selection-then-regeneration" approaches that risk introducing artifacts during synthesis, LLM-PeerReview directly selects among candidates, avoiding an additional generative step. This assumes at least one candidate response is high-quality enough that selection suffices.

## Foundational Learning

- Concept: **LLM-as-a-Judge paradigm**
  - Why needed here: Understanding that LLMs can be prompted to evaluate text quality, not just generate text
  - Quick check question: Can you explain why an instruction-tuned 7B model might be a better judge than a larger base model?

- Concept: **Expectation-Maximization (EM) algorithm for latent variable models**
  - Why needed here: The weighted variant iteratively estimates hidden true scores and judge reliabilities; understanding E-step (inference) and M-step (parameter learning) is essential
  - Quick check question: What would happen to the learned transition matrices if you initialized them as identity matrices vs. random?

- Concept: **Crowdsourcing truth inference (Dawid-Skene model)**
  - Why needed here: Maps directly to treating multiple LLMs as noisy annotators; understanding confusion matrices and annotator reliability is core to the weighted variant
  - Quick check question: How does the model handle the case where all judges systematically over-score responses?

## Architecture Onboarding

- Component map:
Query Input → [Response Generator ×J] → J candidate responses → [Scoring Module: Flipped-Triple LLM-as-Judge ×J] → J×J score matrix → [Reasoning Module: Average OR Dawid-Skene EM] → J final scores → [Selection: argmax] → Final response

- Critical path: The scoring phase dominates computational cost (O(J) LLM calls per query for flipped-triple, vs O(J²) for pairwise). For 4 LLMs, this means ~4 scoring calls per query (each call evaluates a triplet).

- Design tradeoffs:
  - **Average vs. Weighted**: Average is simpler and faster (no EM iterations); Weighted potentially better when judges have heterogeneous reliability but requires global dataset statistics
  - **Score levels (3/5/7/10)**: Figure 5 shows no clear winner; 5-level used for main experiments
  - **Flipped-triple vs. Double vs. Quadruple**: Table 2 shows performance-efficiency tradeoff; flipped-triple is recommended balance

- Failure signatures:
  - If all responses are poor, selection returns the "least bad" option with high confidence—no quality floor guarantee
  - If judges have correlated failures (e.g., all struggle with math), weighted aggregation may not help
  - EM convergence issues if score distributions are sparse or unusual

- First 3 experiments:
  1. **Ablate scoring strategy**: Compare Single, Double, Flipped-triple, and Quadruple-half on a held-out subset; verify Table 2 results generalize to your domain
  2. **Judge heterogeneity analysis**: Run LLM-PeerReview-Weighted and inspect learned transition matrices (Figure 4); verify diagonal dominance correlates with known judge strengths
  3. **Failure case audit**: Identify queries where selected response disagrees with ground truth (if available); check if issue is scoring accuracy, aggregation, or candidate quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the computational overhead of the "flipped-triple scoring trick" be optimized for larger LLM pools without sacrificing its ability to mitigate position and consistency biases?
- Basis in paper: The paper states in Section 3.4 that the flipped-triple method represents a "performance-efficiency trade-off" and notes that while it has O(J) complexity, it requires more time than simpler methods like "Single" or "Double" scoring.
- Why unresolved: The paper identifies the trade-off but does not propose or test methods to reduce the computational cost (e.g., sampling strategies or learned approximations) for the scoring phase.
- What evidence would resolve it: A study comparing the performance of the full flipped-triple method against approximate versions (e.g., sampling fewer permutations) on the same datasets to determine if the bias mitigation is robust to reduced computation.

### Open Question 2
- Question: Does LLM-PeerReview generalize to heterogeneous ensembles that include models of vastly different scales (e.g., 70B+ or proprietary APIs) alongside the 7B models tested?
- Basis in paper: The experimental setup in Section 3.1 explicitly restricts the ensemble to "7B-scale models" (Llama, Mistral, Qwen), leaving the interaction between the method and models with drastically different reasoning capabilities untested.
- Why unresolved: It is unclear if the "LLM-as-a-Judge" component functions reliably when the judging models are significantly less capable than the best generator in the pool, or if a mixture of strong and weak judges skews the weighted reasoning.
- What evidence would resolve it: Experimental results applying LLM-PeerReview to a mixture of model sizes (e.g., combining GPT-4, Llama-3-70B, and 7B models) to see if the selection accuracy holds.

### Open Question 3
- Question: To what extent does "self-evaluation bias" affect the reliability of the final selection when a model judges its own generated response?
- Basis in paper: The methodology reuses the pool of LLMs as judges for the responses they generated (Section 2.1). While the "flipped-triple" trick mitigates position bias, the paper does not analyze if models exhibit a bias toward their own stylistic patterns or reasoning logic.
- Why unresolved: The paper assumes the LLMs can act as objective "peer reviewers," but if models implicitly favor their own outputs, the "collective wisdom" could be skewed by the number of times a model appears as a judge.
- What evidence would resolve it: An ablation study measuring the correlation between a model's scores for its own responses versus its scores for others, specifically comparing the final selection accuracy when self-judgments are excluded.

## Limitations
- Computational Cost: Flipped-triple scoring requires 6× more LLM judge calls than single scoring, creating significant runtime overhead
- Dataset Representativeness: Experiments use sampled subsets rather than full datasets, limiting generalizability
- Judge Heterogeneity Assumptions: Weighted variant assumes judges have independent, learnable error patterns that may not hold with correlated biases

## Confidence
- **High Confidence**: Core experimental results showing LLM-PeerReview outperforming baselines (7.0% average gain) across all four datasets
- **Medium Confidence**: The theoretical justification for why flipped-triple mitigates position bias
- **Medium Confidence**: The superiority of weighted aggregation over simple averaging

## Next Checks
1. **Computational Overhead Validation**: Measure actual runtime and API costs for LLM-PeerReview vs. baseline methods across the four datasets
2. **Judge Independence Assessment**: Compute inter-judge correlation matrices and error overlap statistics to test independence assumptions
3. **Failure Case Analysis**: Systematically identify queries where LLM-PeerReview selects responses that contradict ground truth or GPT-4o-mini judgments