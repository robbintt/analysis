---
ver: rpa2
title: Toward a Theory of Agents as Tool-Use Decision-Makers
arxiv_id: '2506.00886'
source_url: https://arxiv.org/abs/2506.00886
tags:
- internal
- agent
- reasoning
- agents
- external
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that tool-augmented agents should invoke external
  tools only when epistemically necessary, not merely for convenience. It introduces
  the Theory of Agent (ToA), a framework that unifies reasoning and acting as alternative
  means of knowledge acquisition, enabling agents to systematically coordinate introspection
  and interaction.
---

# Toward a Theory of Agents as Tool-Use Decision-Makers

## Quick Facts
- **arXiv ID:** 2506.00886
- **Source URL:** https://arxiv.org/abs/2506.00886
- **Reference count:** 19
- **Primary result:** Agents should invoke external tools only when epistemically necessary, not for convenience, to preserve and strengthen internal reasoning capabilities.

## Executive Summary
This position paper introduces the Theory of Agent (ToA), a normative framework that unifies reasoning and acting as alternative means of knowledge acquisition. The framework argues that agents should systematically coordinate introspection and interaction based on epistemic necessity rather than defaulting to external tools. ToA defines a knowledge boundary separating tasks solvable internally from those requiring external interaction, with epistemic effort as an invariant task requirement. The theory shows that unnecessary delegation suppresses internal reasoning capability development, while epistemically calibrated decisions enable agents to preserve and strengthen their competence.

## Method Summary
The paper proposes four approaches to implement ToA: agentic pretraining with next-tool prediction, supervised fine-tuning tailored to model competence, reinforcement learning with correctness plus tool-use penalties, and prompting with memory/workflow abstractions. The core methodology involves estimating internal solvability (p_int) for tasks, establishing knowledge boundaries through benchmarking, and routing decisions based on threshold comparison. Implementation requires defining task sets Q_int and Q_world, computing internal solvability estimates, and training decision policies that balance internal reasoning against external tool use.

## Key Results
- Tool use does not eliminate task difficulty; it reallocates epistemic effort between internal computation and external execution
- Systematic over-delegation to external tools suppresses gradient signals needed for internal reasoning pathway development
- Effective tool use requires meta-cognitive estimation of internal solvability to distinguish between internal and external tasks

## Why This Works (Mechanism)

### Mechanism 1: Epistemic Effort Reallocation
The Theory of Agent proposes that every task possesses a fixed amount of "epistemic effort" (E*) that must be satisfied through internal reasoning (E_int) or external interaction (E_ext). Agents incur efficiency costs without reducing total effort when delegating internally solvable tasks externally.

### Mechanism 2: Delegation-Induced Capability Stagnation
Over-delegation functions as "reward hacking" where agents exploit tools to maximize reward without exercising internal cognitive modules, resulting in sparse gradients for internal parameters and stagnant knowledge boundaries.

### Mechanism 3: Belief-Based Boundary Classification
Effective tool use requires meta-cognitive estimation of internal solvability through belief scores (p_int). Agents route tasks based on comparing p_int against threshold α to trigger either cognitive or physical tools.

## Foundational Learning

- **Concept: Reinforcement Learning Reward Hacking**
  - **Why needed here:** Over-delegation is a form of reward hacking where agents exploit tools to maximize reward without learning underlying skills
  - **Quick check question:** Can you explain how an agent might maximize a reward signal while failing to learn the intended underlying skill?

- **Concept: Meta-cognition / Self-Knowledge**
  - **Why needed here:** The framework hinges on agents knowing what they don't know through estimating p_int
  - **Quick check question:** How does a model distinguish between high-confidence knowledge and high-uncertainty contexts requiring external search?

- **Concept: Decision Theory (Cost/Latency Trade-offs)**
  - **Why needed here:** Tool use is treated as a decision under uncertainty involving trade-offs between internal compute cost and external interaction cost
  - **Quick check question:** If two paths lead to the same correct answer, how do you mathematically decide which path is "better" based on cost functions?

## Architecture Onboarding

- **Component map:** Context τ_t -> Solvability Estimator (outputs p_int) -> Decision Router (compares p_int to α) -> Internal Tools T_int or External Tools T_ext -> Execution -> Update τ

- **Critical path:** Task q enters system → Solvability Estimator evaluates q given context τ, generating p_int → Decision Router compares p_int to α → Route to Internal Tools if p_int ≥ α, else route to External Tools → Selected tool executes → Context τ updates → Loop until resolution

- **Design tradeoffs:**
  - Threshold α: High threshold risks hallucination (over-reasoning); low threshold risks inefficiency (over-acting)
  - Tool Granularity: Defining what constitutes a "tool" impacts decision router complexity

- **Failure signatures:**
  - Overthinking: Excessively long reasoning traces, high token usage, logical loops, hallucinated facts without verification
  - Overacting: Immediate invocation for trivial facts, high latency, dependency on network availability

- **First 3 experiments:**
  1. **Boundary Calibration Test:** Measure False Positive Rate (using tools for internal tasks) and False Negative Rate (reasoning on external tasks) on dataset with known internal vs. external questions
  2. **Stagnation Ablation:** Train agents with free tool access vs. tool-use penalty; evaluate internal capability growth on tool-restricted test sets
  3. **Threshold Sensitivity Analysis:** Sweep threshold α (0.1 to 0.9) and plot Task Accuracy vs. Tool Invocation Count to find Pareto frontier

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the latent internal task set (Q_int) be accurately estimated or operationalized to enable real-time calibration of tool-use decisions?
- **Basis in paper:** Section 3.2 states Q_int "is latent and cannot be observed directly," forcing agents to rely on belief-based approximations
- **Why unresolved:** The paper defines the theoretical boundary but lacks a mechanism for ground-truth verification during training or inference
- **What evidence would resolve it:** Development of proxy benchmarks or estimation algorithms that predict internal solvability (p_int) with high fidelity across diverse tasks

### Open Question 2
- **Question:** How can distributed knowledge boundaries be aligned in multi-agent systems to prevent redundant external interactions?
- **Basis in paper:** Appendix E identifies aligning distributed boundaries as a "key challenge" in multi-agent coordination
- **Why unresolved:** Current theory focuses on single agents; extending to collective intelligence requires mechanisms for agents to communicate and delegate based on relative epistemic capabilities
- **What evidence would resolve it:** Frameworks where agents explicitly model peers' knowledge boundaries and demonstrate emergent efficient division of labor

### Open Question 3
- **Question:** Does the invariant nature of epistemic effort (E*) hold in embodied environments where physical actions are irreversible or costly?
- **Basis in paper:** Section 3.3 defines epistemic effort as an invariant requirement, but Appendix E notes physical bounds differ from cognitive ones
- **Why unresolved:** The theory assumes effort is redistributable; however, physical failures or irreversible state changes might increase total epistemic cost, violating the invariance assumption
- **What evidence would resolve it:** Empirical studies in robotics showing whether failed physical actions increase the total E* required to regain a solvable state

## Limitations
- The concept of "epistemic effort" as an invariant task property remains theoretical with no experimental measurement protocol
- The framework assumes agents can accurately estimate their own solvability boundaries through meta-cognitive modules, but calibration is not addressed
- The paper assumes a clear distinction between "internal" and "external" knowledge that may blur in practice for large language models

## Confidence
- **High Confidence:** The core insight that unnecessary tool delegation suppresses internal capability development (Mechanism 2)
- **Medium Confidence:** The epistemic effort reallocation mechanism (Mechanism 1) - theoretical framing is coherent but practical measurement remains undefined
- **Low Confidence:** The belief-based boundary classification mechanism (Mechanism 3) - theoretically sound but practical implementation is unproven

## Next Checks
1. **Effort Measurement Validation:** Design controlled experiment to measure and validate the "epistemic effort" invariant by comparing internal reasoning traces with external tool invocations on identical tasks

2. **Boundary Calibration Benchmark:** Create standardized benchmark with clearly labeled internal vs. external tasks to empirically test agent calibration, measuring false positive and false negative rates in tool invocation decisions

3. **Capability Development Tracking:** Implement longitudinal study comparing agents with and without epistemic-aware tool policies, measuring internal reasoning capability growth over time using tool-restricted test sets