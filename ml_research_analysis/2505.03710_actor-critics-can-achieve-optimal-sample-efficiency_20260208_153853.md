---
ver: rpa2
title: Actor-Critics Can Achieve Optimal Sample Efficiency
arxiv_id: '2505.03710'
source_url: https://arxiv.org/abs/2505.03710
tags:
- policy
- lemma
- algorithm
- critic
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper resolves an open problem in online reinforcement learning\
  \ by introducing a novel actor-critic algorithm that achieves the optimal sample\
  \ complexity of O(1/\u03B5\xB2) trajectories for learning an \u03B5-optimal policy,\
  \ even when strategic exploration is necessary with general function approximation.\
  \ The key innovation is the NORA algorithm, which combines three crucial ingredients:\
  \ optimism for efficient exploration, off-policy learning to reuse data, and rare-switching\
  \ critic updates that target the optimal Q-function Q rather than the current policy's\
  \ Q-function."
---

# Actor-Critics Can Achieve Optimal Sample Efficiency

## Quick Facts
- **arXiv ID:** 2505.03710
- **Source URL:** https://arxiv.org/abs/2505.03710
- **Reference count:** 4
- **Primary result:** Novel actor-critic algorithm achieves optimal sample complexity of O(1/ε²) trajectories for ε-optimal policy learning with general function approximation.

## Executive Summary
This paper resolves the open problem of achieving optimal sample efficiency in online reinforcement learning with general function approximation. The authors introduce NORA (Non-Optimistic Rare-Switching Actor-critic), which achieves O(1/ε²) sample complexity through three key innovations: targeting the optimal Q-function Q* for stable optimism, off-policy learning for data reuse, and rare-switching critic updates that trigger only when statistically necessary. The algorithm also incorporates policy resets and increased learning rates to control tracking error. The results extend to hybrid reinforcement learning, showing sample efficiency gains when initializing critics with offline data.

## Method Summary
NORA is an actor-critic algorithm that learns ε-optimal policies through a combination of optimism, off-policy learning, and rare critic updates. The critic targets the optimal Q-function Q* rather than the current policy's Q-function, maintaining sufficient optimism while avoiding instability from tracking a moving target. Critic updates are rare-switching events triggered when the TD error exceeds a threshold, and policy resets immediately follow each update. The actor is updated via mirror descent using the current critic. For hybrid settings, the algorithm can incorporate offline data either through optimistic initialization or via a non-optimistic variant (NOAH) that relies on data coverage assumptions. The method achieves regret bounds scaling with Bellman eluder dimension and provides sample complexity of O(dH⁵ log T log|A|/ε² + H⁴β SEC(F, Π, T)/ε²).

## Key Results
- NORA achieves optimal sample complexity of O(1/ε²) trajectories for learning ε-optimal policies with general function approximation
- Proves regret bound of O(dH⁵T log T log|A| + dH³ log T + βH⁴T SEC(F, Π, T)) with high probability
- Extends to hybrid reinforcement learning, showing sample efficiency gains when initializing critics with offline data
- Provides non-optimistic variant (NOAH) requiring only Noff ≥ c*_off dH⁴/ε² offline samples

## Why This Works (Mechanism)

### Mechanism 1: Targeting $Q^*$ for Stable Optimism
The algorithm targets the optimal Q-function ($Q^*$) rather than the current policy's Q-function ($Q^{\pi^{(t)}}$) to maintain sufficient optimism. When targeting a rapidly changing $Q^{\pi^{(t)}}$, the critic hits a "moving target," leading to insufficient optimism and unstable switching. By targeting $Q^*$, the Bellman operator remains stable (a contraction), allowing valid confidence bounds and triggering critic updates only when statistically necessary. This requires realizability and generalized completeness assumptions.

### Mechanism 2: Rare-Switching Critics with Policy Resets
Combining rare critic updates with policy resets controls the deviation between actor and critic's greedy policy. In general function approximation, the policy class size can grow linearly with critic updates, increasing regret. By updating the critic only O(dH log T) times and immediately resetting the actor to uniform policy after each update, the algorithm bounds the tracking error between current policy $\pi^{(t)}$ and greedy policy $\pi_{f^{(t)}}$. This prevents error accumulation from outdated critics.

### Mechanism 3: Hybrid Data Utilization
Integrating offline data allows sample efficiency gains in the optimistic version or computational efficiency gains in the non-optimistic version. In the optimistic version, offline data reduces the online SEC term. In the non-optimistic version (NOAH), offline data bypasses the computational difficulty of implementing optimism by assuming the offline dataset covers the optimal policy, requiring only single-policy concentrability.

## Foundational Learning

- **Concept: Bellman Eluder Dimension**
  - Why needed: Primary measure of complexity used to characterize regret in general function approximation
  - Quick check: Can you explain why a low Bellman Eluder dimension implies that the Bellman error on current distribution correlates with error on next distribution?

- **Concept: Mirror Descent / Policy Optimization**
  - Why needed: Actor update uses multiplicative weights update (π ∝ exp(ηf)), a form of mirror descent
  - Quick check: How does learning rate η affect the trade-off between tracking speed and regret in mirror descent analysis?

- **Concept: Concentrability Coefficients**
  - Why needed: Essential for hybrid RL section; non-optimistic algorithm relies on single-policy concentrability
  - Quick check: What is the difference between "all-policy" and "single-policy" concentrability, and which one is required for non-optimistic hybrid algorithm (NOAH)?

## Architecture Onboarding

- **Component map:** Data Buffer -> TD Error Computation -> Confidence Set Manager -> Critic Update Logic -> Actor Update Logic

- **Critical path:**
  1. Collect trajectory with current policy π^(t)
  2. Compute TD error on new data against current critic f^(t)
  3. Switch Check: If error > 5H²β, update critic confidence set F^(t) and reset π^(t) to uniform
  4. Update Actor: π^(t+1) ∝ π^(t) exp(ηf^(t))

- **Design tradeoffs:**
  - Targeting Q* vs Q^π: Q* ensures stability but requires function class closed under optimal Bellman operator; Q^π is more natural for on-policy data but risks instability
  - Optimism vs. Offline Data: Implementing optimism is computationally expensive; using offline data avoids this cost but requires strong data coverage assumptions

- **Failure signatures:**
  - Linear Regret: Likely caused by insufficient optimism or switch condition triggering too often
  - Policy Stagnation: If policy resets are too frequent and learning rate η is too low, actor may never catch up to critic

- **First 3 experiments:**
  1. Linear MDP (Tetris): Validate algorithm achieves √T regret and compare against LSVI-UCV
  2. Hybrid RL (AntMaze): Test hybrid variant (NOAH) with offline pretraining vs purely online baselines
  3. Ablation on Resets: Run NORA without policy resets to verify theoretical claim that resets are necessary

## Open Questions the Paper Calls Out

- **Can the dependence on Bellman Eluder dimension d in switching cost be removed?** The bound still depends on d due to switch cost even though online regret uses SEC, which is generally smaller. A modified rare-switching algorithm achieving regret scaling with √(T·SEC) without multiplicative factors of d would resolve this.

- **Is there a computationally efficient algorithm achieving NORA's optimal sample complexity?** Algorithm 2 is generally computationally inefficient due to global maximization requirement. An algorithm with polynomial-time complexity per step that provably achieves Õ(1/ε²) sample complexity would resolve this.

- **Can policy optimization methods be proven minimax-optimal in linear MDPs?** Adapting work of He et al. (2023) to show policy optimization can be minimax-optimal in linear MDPs with Õ(√(dH²T)) regret would be a welcome contribution.

## Limitations
- Theoretical dependence on Bellman Eluder dimension d and concentrability coefficients can be exponentially large in true MDP parameters
- Computational complexity of implementing confidence set F^(t) is not addressed for general function classes like neural networks
- Non-optimistic hybrid variant requires strong single-policy concentrability assumption that may be difficult to verify

## Confidence
- **High Confidence:** Targeting Q* rather than Q^π is necessary for stable optimism and efficient learning
- **Medium Confidence:** Rare-switching critics with policy resets control policy class growth and bound tracking error, but relies heavily on low Bellman Eluder dimension assumption
- **Low Confidence:** Practical benefits of hybrid RL extension are unclear without empirical validation on challenging continuous control tasks

## Next Checks
1. **Empirical Test of Q* Targeting:** Implement NORA with version targeting Q^π instead of Q* and compare regret/sample complexity on simple Linear MDP to validate theoretical claim about insufficient exploration
2. **Bellman Eluder Dimension Estimation:** For simple function class (e.g., linear functions on tabular state space), empirically estimate Bellman Eluder dimension to understand scaling with problem size
3. **Hybrid RL Data Quality:** For NOAH variant, design experiment with offline dataset having varying levels of optimal policy coverage and measure regret as function of single-policy concentrability coefficient c*_off to validate theoretical dependence