---
ver: rpa2
title: 'shapr: Explaining Machine Learning Models with Conditional Shapley Values
  in R and Python'
arxiv_id: '2504.01842'
source_url: https://arxiv.org/abs/2504.01842
tags:
- shapley
- values
- value
- explain
- conditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: shapr introduces a comprehensive framework for generating conditional
  Shapley value explanations in R and Python. It addresses the limitation of existing
  software by focusing on conditional Shapley values, which accurately capture feature
  dependencies often ignored by other methods.
---

# shapr: Explaining Machine Learning Models with Conditional Shapley Values in R and Python

## Quick Facts
- **arXiv ID:** 2504.01842
- **Source URL:** https://arxiv.org/abs/2504.01842
- **Reference count:** 13
- **Key outcome:** Comprehensive framework for generating conditional Shapley value explanations in R and Python

## Executive Summary
The shapr package provides a comprehensive framework for generating conditional Shapley value explanations for machine learning models. Unlike existing software that relies on marginal distributions, shapr focuses on conditional Shapley values that accurately capture feature dependencies. The package implements multiple approaches for estimating the conditional contribution function, including Monte Carlo-based methods (independence, empirical, Gaussian, Gaussian copula, ctree, vaeac, categorical) and regression-based methods (regression_separate, regression_surrogate). Key features include parallelized computations, iterative estimation with convergence detection, and support for causal and asymmetric Shapley values. The R package offers a minimal set of user functions with sensible defaults while providing flexibility for advanced users, and the Python library (shaprpy) makes this functionality accessible through a lightweight wrapper.

## Method Summary
The method estimates conditional Shapley values by solving a weighted least squares problem that approximates the contribution function v(S) = E[f(x) | x_S] instead of the marginal expectation. The package implements multiple approaches for estimating this conditional expectation, including Monte Carlo sampling methods (Gaussian, ctree, empirical, etc.) and regression-based methods. The computation proceeds by sampling coalitions, estimating contributions using the selected approach, and solving a weighted linear regression to obtain Shapley values. The system uses iterative estimation with convergence detection to optimize computational resources, stopping when the normalized standard deviation of estimates falls below a threshold.

## Key Results
- Introduces conditional Shapley values that accurately capture feature dependencies typically ignored by existing methods
- Implements a wide range of conditional estimation approaches including Monte Carlo and regression-based methods
- Provides parallelized computations and iterative estimation with automatic convergence detection
- Supports causal and asymmetric Shapley values, as well as specialized functionality for explaining time series forecasts
- Offers both R and Python implementations with sensible defaults and flexibility for advanced users

## Why This Works (Mechanism)

### Mechanism 1: Conditional Expectation Estimation
The system avoids sampling from the marginal distribution (which ignores correlations and may create unrealistic data points) by using specific approaches to sample from or estimate the conditional distribution, ensuring the model is evaluated on realistic data instances during the explanation process. The core assumption is that the dependence structure between features can be adequately modeled by the selected approach.

### Mechanism 2: Weighted Least Squares Approximation (KernelSHAP)
The combinatorial explosion of calculating exact Shapley values is approximated efficiently by formulating the problem as a weighted linear regression. Instead of iterating through all $2^M$ coalitions, the system samples a subset and solves a weighted least squares problem to find Shapley values that minimize the squared error between estimated and true contributions.

### Mechanism 3: Iterative Convergence Detection
Computational resources are optimized by stopping the sampling process as soon as Shapley value estimates stabilize, rather than running for a fixed large number of iterations. The system estimates standard deviation in each iteration and stops when the normalized standard deviation falls below a threshold.

## Foundational Learning

**Concept: Shapley Values (Cooperative Game Theory)**
- **Why needed here:** This is the mathematical definition of "fairness" used to distribute the model's prediction "payout" among features
- **Quick check question:** Can you explain why a feature's contribution is averaged over all possible combinations of other features present in the model?

**Concept: Marginal vs. Conditional Distributions**
- **Why needed here:** The core value proposition of `shapr` is distinguishing between these. A user must understand that "marginal" implies independence (breaking correlations) while "conditional" preserves the data manifold
- **Quick check question:** If feature A and B are always equal, what happens to the explanation if we assume independence and vary B while holding A constant?

**Concept: Monte Carlo Integration**
- **Why needed here:** Most approaches use sampling to estimate the expected value. Understanding how samples approximate an integral is vital
- **Quick check question:** How does increasing the number of Monte Carlo samples ($K$) typically affect the variance of the estimation?

## Architecture Onboarding

**Component map:**
- User Interface: `explain()` (R) / `shaprpy.explain()` (Python)
- Controller: `setup()` checks inputs; `shapley_setup()` handles coalition sampling
- Estimation Engine: `compute_vS()` is the critical bottleneck that calls specific approach modules
- Aggregator: `compute_estimates()` solves the weighted least squares problem
- Monitor: `check_convergence()` decides whether to iterate again

**Critical path:** The flow moves from `setup` → `shapley_setup` (sample coalitions) → `setup_approach` (prepare conditional models) → `compute_vS` (evaluate model on sampled data) → `compute_estimates` (calculate $\phi$). The `compute_vS` step is the most computationally intensive and parallelizable component.

**Design tradeoffs:**
- **Approach Selection:** `independence` is fast but theoretically flawed for dependent data. `vaeac` is flexible but slow to train. `gaussian` is fast but assumes unimodal normality
- **Iteration:** Iterative estimation saves time on high-dimensional data but adds overhead (bootstrapping) on small data
- **Batching:** Batching reduces memory usage but can increase total runtime due to overhead

**Failure signatures:**
- **High MSEv:** The `MSEv` criterion is high, suggesting the selected `approach` is poor at modeling the conditional expectation
- **Memory Overflow:** Occurs if `compute_vS` attempts to generate massive matrices of Monte Carlo samples without sufficient batching
- **Slow Convergence:** Iterative loop runs to `max_n_coalitions` without meeting the convergence threshold

**First 3 experiments:**
1. **Correlation Check:** Train a model on data with 2 highly correlated features. Run `explain()` with `approach="independence"` and `approach="empirical"`. Observe how `independence` distributes importance differently compared to `empirical`
2. **Approach Benchmarking:** On a dataset with mixed categorical/continuous features, run `explain()` with `approach="ctree"` vs `approach="regression_surrogate"`. Compare the `MSEv` scores to see which better approximates the contribution function
3. **Iterative Efficiency:** Run `explain()` on a dataset with 15+ features. Enable `verbose = c("basic", "convergence")`. Observe how many coalitions are actually needed to satisfy the convergence criterion compared to the theoretical maximum

## Open Questions the Paper Calls Out

**Open Question 1:** Does an automated approach selection strategy, utilizing a burn-in period with the MSEv metric to identify the best-performing estimator, improve the robustness of Shapley value explanations? The authors propose this as a "valuable methodological enhancement" for future versions but have not implemented or validated the efficacy of this dynamic selection mechanism.

**Open Question 2:** How can the conditional estimation approaches in shapr be effectively adapted to compute global feature importance via SAGE (Shapley Additive Global Explanations)? The current implementation focuses on local predictions; the specific modifications required for the Monte Carlo and regression paradigms to estimate global loss decompositions are not defined.

**Open Question 3:** Can a new evaluation criterion be developed to directly assess the accuracy of Shapley values, overcoming the limitations of the MSEv metric? The paper relies on MSEv as a proxy because true Shapley values are unknown, but acknowledges that MSEv may not perfectly correlate with Shapley value error.

## Limitations
- The choice of conditional estimation approach significantly impacts both accuracy and computational cost, with no universally optimal method
- The empirical and Gaussian copula approaches require substantial computational resources and may struggle with high-dimensional data
- The assumption of conditional independence within sampling approaches may not hold in complex real-world datasets
- Reliance on specific R/Python implementations introduces potential platform-dependent behavior, particularly in memory management and parallel processing

## Confidence

**High Confidence:** The core mathematical framework for conditional Shapley values and the regression-based approximation (KernelSHAP) are well-established in the literature. The package's ability to handle time series explanations and provide multiple conditional estimation approaches is explicitly demonstrated.

**Medium Confidence:** The performance comparisons between different conditional approaches are methodologically sound, but the specific benchmark datasets and hyperparameter settings are not fully detailed in the paper. The convergence detection mechanism appears robust but may behave unpredictably on highly non-linear models.

**Low Confidence:** Claims about the Python wrapper (shaprpy) matching the full functionality of the R package are not thoroughly validated, particularly regarding iterative estimation and parallel processing capabilities.

## Next Checks

1. **Approach Sensitivity Analysis:** Apply the same model to a dataset with known feature dependencies and compare explanations generated using `approach="independence"` versus `approach="ctree"` or `approach="empirical"`. Verify that dependency-aware approaches produce more stable and theoretically correct attributions.

2. **Convergence Threshold Tuning:** Systematically vary the convergence threshold parameter `t` on a moderately sized dataset (10-15 features) and measure how it affects the number of coalitions sampled, computational time, and stability of the resulting Shapley values.

3. **Memory Scaling Test:** Evaluate the package's memory usage and computation time when explaining models with increasing numbers of features (e.g., 5, 10, 15, 20 features) using both R and Python implementations. Identify at which point memory constraints or computation time become prohibitive.