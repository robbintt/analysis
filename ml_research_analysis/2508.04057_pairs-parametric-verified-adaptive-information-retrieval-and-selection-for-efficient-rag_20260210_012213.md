---
ver: rpa2
title: 'PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for
  Efficient RAG'
arxiv_id: '2508.04057'
source_url: https://arxiv.org/abs/2508.04057
tags:
- query
- retrieval
- documents
- queries
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses inefficiencies in retrieval-augmented generation
  (RAG) systems, which either retrieve information for every query or retrieve irrelevant
  documents for sparse queries. To solve this, the authors propose PAIRS, a training-free
  framework that integrates parametric and retrieved knowledge.
---

# PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG

## Quick Facts
- arXiv ID: 2508.04057
- Source URL: https://arxiv.org/abs/2508.04057
- Reference count: 33
- Key outcome: PAIRS reduces retrieval costs by 25% (75% of queries trigger retrieval) while improving accuracy by +1.1% EM and +1.0% F1 over prior baselines on average

## Executive Summary
PAIRS addresses inefficiencies in retrieval-augmented generation (RAG) systems by introducing a training-free framework that intelligently decides when to retrieve documents. The system uses a dual-path generation mechanism that first attempts to answer queries using only parametric knowledge from the LLM, generating both direct and context-augmented answers. When these answers disagree, PAIRS activates a dual-path retrieval process guided by both the original query and self-generated pseudo-context, followed by an Adaptive Information Selection module that filters retrieved documents based on weighted similarity scores. This approach reduces unnecessary retrievals while maintaining or improving accuracy across six QA benchmarks.

## Method Summary
PAIRS employs a novel retrieval-triggering mechanism that combines parametric knowledge from the LLM with adaptive retrieval when needed. The framework generates two types of answers - direct and context-augmented - using pseudo-context created by the LLM itself. When these answers agree, the system assumes the query can be answered without retrieval. If they disagree, PAIRS activates dual-path retrieval using both the original query and pseudo-context as retrieval inputs. An Adaptive Information Selection (AIS) module then filters retrieved documents by computing weighted similarity scores to both sources, selecting only the most relevant documents for the final generation step. This training-free approach reduces retrieval costs while maintaining or improving accuracy.

## Key Results
- Retrieval costs reduced by 25% (only 75% of queries trigger retrieval)
- Accuracy improvements of +1.1% EM and +1.0% F1 over prior baselines on average
- PAIRS achieves state-of-the-art performance on six QA benchmarks
- The framework maintains effectiveness without requiring any training

## Why This Works (Mechanism)
PAIRS works by leveraging the LLM's inherent parametric knowledge to filter queries that don't need retrieval. The dual-path generation mechanism creates both direct and context-augmented answers using self-generated pseudo-context. When these answers agree, the query likely doesn't require external information. When they disagree, the system activates retrieval guided by both the query and pseudo-context, ensuring more relevant documents are retrieved. The AIS module then filters these documents based on weighted similarity to both sources, reducing noise and improving the quality of information passed to the final generation step.

## Foundational Learning

**LLM parametric knowledge**: Pre-trained knowledge stored in language model parameters that can be accessed without retrieval.
*Why needed*: Forms the baseline knowledge source for PAIRS, enabling retrieval-free responses for queries that can be answered from existing model knowledge.
*Quick check*: Can the model answer simple factual questions without retrieval?

**Dual-path generation**: Generating both direct answers and context-augmented answers using pseudo-context.
*Why needed*: Creates a mechanism to detect when retrieval is necessary by comparing agreement between different answer types.
*Quick check*: Do the direct and context-augmented answers agree for simple queries?

**Adaptive Information Selection (AIS)**: Filtering retrieved documents based on weighted similarity to multiple retrieval sources.
*Why needed*: Reduces noise from irrelevant documents while ensuring relevant information is retained for final generation.
*Quick check*: Does AIS improve answer quality when retrieval is triggered?

## Architecture Onboarding

**Component map**: Query -> Dual-path generation (direct + context-augmented answers) -> Agreement check -> [Yes: Generate answer | No: Dual-path retrieval] -> AIS filtering -> Final generation

**Critical path**: Query → Dual-path generation → Agreement check → (optional) Dual-path retrieval → AIS → Final answer generation

**Design tradeoffs**: 
- **Pro**: Reduces retrieval costs by 25% while improving accuracy
- **Con**: Adds inference complexity through dual-path generation and AIS filtering
- **Pro**: Training-free approach that leverages existing LLM capabilities
- **Con**: Modest accuracy improvements (+1.1% EM, +1.0% F1) may not justify complexity in all settings

**Failure signatures**: 
- High retrieval costs when pseudo-context generation is poor
- Accuracy degradation when AIS filtering is too aggressive
- Increased latency due to dual-path generation overhead

**3 first experiments**:
1. Test agreement rates between direct and context-augmented answers on a held-out validation set
2. Measure retrieval cost reduction on a subset of queries with known answerability
3. Compare AIS filtering effectiveness against simple similarity-based filtering

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to QA benchmarks without testing on diverse task types like summarization or dialogue
- Computational overhead of dual-path generation and AIS filtering not quantified
- Accuracy improvements are modest (+1.1% EM, +1.0% F1) and may not justify deployment complexity
- Framework assumes access to similarity search infrastructure and pre-trained LLM capabilities

## Confidence

**High confidence**: Core retrieval-triggering mechanism and AIS module implementation - these are straightforward extensions of existing techniques with clear implementation paths.

**Medium confidence**: Cost reduction figures - the paper provides clear ablation studies and comparisons with baselines, supporting the 25% reduction claim.

**Low confidence**: Accuracy improvements - lack of statistical significance testing and small absolute gains make these results less reliable.

## Next Checks

1. Test PAIRS on non-QA tasks (e.g., multi-document summarization) to assess cross-task generalizability and identify failure modes

2. Measure actual end-to-end inference latency and token counts to quantify the computational overhead of dual-path generation and AIS filtering

3. Conduct ablation studies isolating the contribution of pseudo-context quality versus AIS filtering to determine which component drives the observed performance gains