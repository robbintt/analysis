---
ver: rpa2
title: Batching-Aware Joint Model Onloading and Offloading for Hierarchical Multi-Task
  Inference
arxiv_id: '2508.13380'
source_url: https://arxiv.org/abs/2508.13380
tags:
- edge
- task
- offloading
- onloading
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of maximizing inference accuracy
  in hierarchical multi-task learning systems by jointly optimizing model onloading
  (which models to deploy) and task offloading (where to execute tasks) across clients,
  edge servers, and the cloud. The authors formulate this as a mixed-integer nonlinear
  program and propose J3O, an alternating optimization algorithm that decomposes the
  problem into greedy submodular model selection and constrained linear programming
  for task routing.
---

# Batching-Aware Joint Model Onloading and Offloading for Hierarchical Multi-Task Inference

## Quick Facts
- **arXiv ID:** 2508.13380
- **Source URL:** https://arxiv.org/abs/2508.13380
- **Reference count:** 35
- **Primary result:** J3O achieves >97% of optimal accuracy with <15% of the runtime required by optimal solver; BAJ3O further improves performance with batching.

## Executive Summary
This paper addresses the problem of maximizing inference accuracy in hierarchical multi-task learning systems by jointly optimizing model onloading (which models to deploy) and task offloading (where to execute tasks) across clients, edge servers, and the cloud. The authors formulate this as a mixed-integer nonlinear program and propose J3O, an alternating optimization algorithm that decomposes the problem into greedy submodular model selection and constrained linear programming for task routing. They extend this to BAJ3O to incorporate batching at the edge for improved GPU utilization. Experiments on three multi-task benchmarks (Taskonomy, DomainNet, Cityscape3D) show that J3O consistently achieves over 97% of the optimal accuracy while incurring less than 15% of the runtime required by the optimal solver. BAJ3O further improves performance by effectively managing batching constraints, outperforming heuristic baselines while maintaining near-optimal accuracy across varying client heterogeneity and batching configurations.

## Method Summary
The authors formulate the joint onloading-offloading problem as a mixed-integer nonlinear program (MINLP) with the objective of maximizing system-wide inference accuracy subject to memory, compute, and bandwidth constraints. They propose J3O, an alternating optimization algorithm that decomposes the MINLP into two tractable subproblems: (1) greedy submodular maximization with Lagrangian relaxation for model onloading, and (2) constrained linear programming for task offloading. BAJ3O extends J3O by replacing edge compute constraints with linearized batching-latency constraints to enable efficient GPU utilization. The algorithm iteratively alternates between greedy model selection and LP-based task routing until convergence, with the batching extension using surrogate linearization to handle non-convex latency constraints.

## Key Results
- J3O achieves over 97% of the optimal accuracy while incurring less than 15% of the runtime required by the optimal MINLP solver
- BAJ3O improves GPU utilization through batching while maintaining near-optimal accuracy across varying client heterogeneity
- The approach outperforms heuristic baselines across all three multi-task benchmarks (Taskonomy, DomainNet, Cityscape3D)
- Model selection and task routing are shown to be interdependent, justifying the joint optimization approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing the joint onloading-offloading problem into alternating subproblems preserves near-optimality while reducing computational complexity.
- Mechanism: The MINLP is split into (i) model onloading via submodular maximization and (ii) task offloading via constrained linear programming. Each subproblem is solved while holding the other's variables fixed, iterating until convergence.
- Core assumption: The objective is monotone submodular in model selection, enabling greedy approximation guarantees.
- Evidence anchors:
  - [abstract] "J3O...an alternating algorithm that (i) greedily selects models to onload via Lagrangian-relaxed submodular optimization and (ii) determines optimal offloading via constrained linear programming"
  - [Section III.B] "We decompose it into two tractable subproblems – model onloading and inference offloading"
  - [corpus] Related work on hierarchical inference (arXiv:2508.08985) similarly uses decoupled local/remote decisions
- Break condition: If task-model accuracy relationships are not submodular (e.g., strong negative interference between tasks), greedy selection may lose guarantee.

### Mechanism 2
- Claim: Lagrangian relaxation decouples compute constraints from model selection, enabling efficient greedy onloading.
- Mechanism: Compute constraints couple binary onloading (x) and task assignment (z) variables. Introducing Lagrange multipliers transforms constrained optimization into dual minimization over relaxed subproblems, each solvable via marginal-gain greedy selection per unit memory.
- Core assumption: Compute constraint violations can be penalized linearly without fundamentally altering the solution structure.
- Evidence anchors:
  - [Section IV.A.2] "These interactions preclude direct use of standard greedy methods, motivating a Lagrangian relaxation to decouple the dependencies"
  - [Equation 18-19] Explicit Lagrangian formulation and subgradient update rules
  - [corpus] Edge offloading literature commonly uses dual decomposition; corpus does not provide direct validation of this specific relaxation
- Break condition: If compute constraints are tight and highly non-linear (e.g., latency spikes under load), linear penalties may misestimate true feasibility.

### Mechanism 3
- Claim: Linear surrogate approximation of batching latency constraints preserves solution quality while maintaining tractability.
- Mechanism: Batching introduces non-convex ℓ₀-norm terms (indicator functions for whether a task receives queries). A first-order Taylor expansion provides iteration-specific linear coefficients (θ, ψ) that approximate the constraint, iteratively refined.
- Core assumption: Batch execution latency grows linearly with batch size (validated empirically per paper), and surrogate coefficients converge.
- Evidence anchors:
  - [Section V.B] "This term is equivalent to the ℓ₀-norm...making the constraint non-convex. Following the surrogate approximation approach...we linearize this term"
  - [Equation 29] Relaxed constraint with surrogate coefficients
  - [corpus] Batching-aware edge AI literature (cited [30]) uses similar linearization; limited independent validation in corpus
- Break condition: If batch setup costs vary non-linearly with model/task combinations, the linear surrogate may underestimate latency, causing constraint violations.

## Foundational Learning

- Concept: **Submodular functions and greedy approximation**
  - Why needed here: The model onloading objective exhibits diminishing marginal returns (adding a model helps less when you already have good coverage). Greedy algorithms achieve (1-1/e) approximation for monotone submodular maximization under knapsack constraints.
  - Quick check question: Given models m₁ covering tasks {A,B} and m₂ covering {B,C}, does adding m₂ to {m₁} provide less marginal gain than adding m₂ to an empty set?

- Concept: **Linear programming for resource allocation**
  - Why needed here: Once models are fixed, offloading decisions are continuous fractions subject to bandwidth and compute budgets. LP solvers (Gurobi, CPLEX) efficiently find optimal routing.
  - Quick check question: If client-to-edge bandwidth is 50% utilized and edge-to-cloud is 80% utilized, which constraint binds first when increasing offloading?

- Concept: **GPU batching dynamics**
  - Why needed here: Edge GPUs amortize fixed kernel launch overhead across batched queries. Batching window Tₐ bounds latency but limits batch size, creating accuracy-latency trade-offs.
  - Quick check question: If batch setup cost is 10ms and per-query compute is 2ms, what is the effective per-query latency for batches of size 5 vs. 20?

## Architecture Onboarding

- Component map:
  - **Clients** (C) -> **Edge servers** (E) -> **Cloud**
  - **Model library** (M) provides pre-trained multi-task models
  - **Optimizer** (J3O/BAJ3O) produces onloading decisions (x, z) and offloading rates (o)

- Critical path:
  1. Estimate task arrival rates (λ) per client from historical data
  2. Run J3O: initialize offloading, iterate greedy onloading → LP offloading until convergence
  3. Deploy selected models to clients/edges
  4. Route queries according to offloading fractions; edge batches within Tₐ window
  5. Re-optimize periodically as workload shifts

- Design tradeoffs:
  - **Memory vs. coverage**: More models onload → better task coverage but higher memory footprint
  - **Offloading vs. latency**: Higher offloading → access to better models but incurs communication delay
  - **Batch size vs. responsiveness**: Larger Tₐ → bigger batches, better GPU utilization, but higher per-query latency

- Failure signatures:
  - Constraint infeasibility: Greedy onloading leaves no feasible offloading solution → relax budgets or reduce model pool
  - Slow convergence: Subgradient updates oscillate → reduce step sizes (η), increase iteration tolerance
  - Batch deadline misses: Surrogate underestimates latency → profile actual setup costs, increase Tₐ

- First 3 experiments:
  1. **Baseline validation**: Replicate Table IV results on Taskonomy with 30 clients, 3 edges. Measure accuracy gap to MINLP solver and runtime reduction.
  2. **Ablation on batching window**: Sweep Tₐ ∈ {0.25s, 0.5s, 1.0s} on Cityscape3D. Plot accuracy vs. 99th-percentile latency to validate surrogate accuracy.
  3. **Heterogeneity stress test**: Vary client compute budgets (χᵦ ∈ [0.3, 1.0]) and task skew (Dirichlet concentration). Compare J3O vs. Greedy-AO gap as constraints tighten.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the J3O/BAJ3O framework be extended to handle time-varying task demands and resource availability, and what are the theoretical guarantees in dynamic settings?
- Basis in paper: [explicit] The conclusion states: "Our approach can be extended to settings where task demands and resource availability evolve slowly or load estimates are noisy. Indeed, our initial investigations indicate that this can be done efficiently, yielding excellent on/offloading decisions over time with minimal overhead."
- Why unresolved: The current formulation assumes static task arrival rates and fixed resource budgets. The authors mention initial investigations but provide no formal treatment of the dynamic case.
- What evidence would resolve it: A formal extension with regret bounds or competitive ratio analysis under adversarial or stochastic task arrivals, plus empirical validation with time-varying workloads.

### Open Question 2
- Question: What is the impact of client-edge association being optimized jointly with onloading and offloading decisions, rather than fixed a priori?
- Basis in paper: [inferred] The system model fixes each client to a designated edge server via binary variable yc,e with the constraint that each client is associated with exactly one edge (Section II-A). This assignment is never optimized.
- Why unresolved: Dynamic client-edge association could improve load balancing and accuracy by routing clients to edges with better-suited models or more capacity.
- What evidence would resolve it: Experimental comparison between fixed vs. optimized association, and analysis of whether the submodular structure is preserved when association is a decision variable.

### Open Question 3
- Question: How does the accuracy-latency trade-off behave when batching is extended to heterogeneous multi-task batches rather than homogeneous single-task batches?
- Basis in paper: [inferred] Section V-A states: "Each edge server collects incoming queries over a Tb forming homogeneous batches, each with single-task queries." This restriction simplifies modeling but may underutilize batching benefits.
- Why unresolved: Real multi-task models (e.g., TaskPrompter) can process mixed-task inputs simultaneously, potentially improving GPU utilization and reducing latency beyond what homogeneous batching achieves.
- What evidence would resolve it: Empirical comparison of homogeneous vs. heterogeneous batching, and extension of the latency constraint model to capture mixed-batch execution costs.

## Limitations
- Submodular approximation guarantee may fail if task-model accuracy relationships exhibit negative interference
- Surrogate linearization accuracy depends on linear scaling assumptions that may not hold for all GPU kernels
- Generalization across domains is limited to three specific multi-task benchmarks with particular model architectures

## Confidence
- **High Confidence:** The alternating optimization framework (J3O) and its decomposition into submodular maximization + LP is theoretically sound and computationally justified. The runtime improvements over MINLP are well-documented.
- **Medium Confidence:** The Lagrangian relaxation approach for handling compute constraints is standard practice, but the specific step sizes and convergence behavior lack detailed empirical validation in the paper.
- **Medium Confidence:** The batching-aware extension (BAJ3O) and its linearization approach are methodologically valid, but the surrogate coefficients' derivation and convergence properties need more rigorous examination.

## Next Checks
1. **Submodularity verification:** Conduct empirical analysis on the actual model-task accuracy matrix to verify diminishing returns property. Measure the greedy algorithm's approximation ratio against the optimal for varying model subsets.
2. **Surrogate constraint validation:** Profile actual edge GPU batching latency across different batch sizes and model combinations. Compare the linearization error against the allowed Tₐ window to quantify potential deadline misses.
3. **Stress test on heterogeneous workloads:** Create synthetic task distributions with varying skew and correlation patterns. Evaluate J3O's performance degradation as task heterogeneity increases beyond the validated Dirichlet distribution with p=0.5.