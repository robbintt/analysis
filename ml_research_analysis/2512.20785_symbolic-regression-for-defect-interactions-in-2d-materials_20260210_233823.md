---
ver: rpa2
title: Symbolic regression for defect interactions in 2D materials
arxiv_id: '2512.20785'
source_url: https://arxiv.org/abs/2512.20785
tags:
- symbolic
- regression
- data
- defects
- defect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper applies symbolic regression to predict defect interactions
  in 2D materials, aiming to discover interpretable, physically meaningful equations
  from DFT data. The authors use SEGVAE, a deep symbolic regression algorithm based
  on a variational autoencoder with LSTM encoder/decoder, to model pairwise defect
  interactions for formation energy and HOMO-LUMO gap.
---

# Symbolic regression for defect interactions in 2D materials

## Quick Facts
- arXiv ID: 2512.20785
- Source URL: https://arxiv.org/abs/2512.20785
- Reference count: 40
- Primary result: SEGVAE discovers interpretable symbolic expressions for defect interactions in 2D materials, achieving competitive or superior performance to graph neural networks on formation energy and HOMO-LUMO gap predictions while enabling physical interpretability and extrapolation beyond training data.

## Executive Summary
This paper applies symbolic regression via the SEGVAE algorithm to model defect interactions in 2D materials, aiming to discover interpretable, physically meaningful equations from density functional theory (DFT) data. The approach treats defect interaction as a distance-dependent kernel learning problem and validates on MoS2, WSe2, h-BN, GaSe, and InSe datasets. SEGVAE's symbolic expressions resemble known physical models (RKKY, Friedel oscillations), enabling extrapolation beyond training data while providing interpretable and computationally efficient models that outperform or match state-of-the-art graph neural networks, particularly on sparse representations.

## Method Summary
The paper employs SEGVAE (Sparse Evolutionary Graph Variational Autoencoder), a deep symbolic regression algorithm that combines variational autoencoder architecture with LSTM encoder/decoder to discover symbolic expressions representing defect interactions. The method frames the problem as learning distance-dependent interaction kernels for formation energy and HOMO-LUMO gap predictions. SEGVAE generates sparse symbolic expressions that are interpretable and can extrapolate beyond training data. The approach is validated across multiple 2D materials (MoS2, WSe2, h-BN, GaSe, InSe) and compared against graph neural networks, demonstrating competitive or superior performance especially in sparse data regimes.

## Key Results
- SEGVAE discovers symbolic expressions for defect interactions that resemble known physical models (RKKY, Friedel oscillations)
- The method achieves competitive or superior performance compared to state-of-the-art graph neural networks on formation energy and HOMO-LUMO gap predictions
- SEGVAE enables extrapolation beyond training data and provides interpretable, computationally efficient models particularly effective on sparse representations

## Why This Works (Mechanism)
The approach succeeds by combining deep learning's ability to capture complex patterns in DFT data with symbolic regression's capacity to produce interpretable mathematical expressions. The variational autoencoder architecture with LSTM components enables efficient exploration of the symbolic expression space while maintaining physical constraints through sparsity assumptions. By framing defect interactions as distance-dependent kernel learning, the method naturally captures the spatial decay patterns characteristic of physical interactions in 2D materials, allowing discovered expressions to resemble established physical models.

## Foundational Learning
- Variational Autoencoders: Why needed - to efficiently explore symbolic expression space while maintaining physical constraints; Quick check - verify latent space captures meaningful variation in defect configurations
- LSTM Networks: Why needed - to handle sequential symbolic expressions and maintain context during encoding/decoding; Quick check - ensure LSTM captures distance-dependent patterns in defect interactions
- Symbolic Regression: Why needed - to produce interpretable mathematical expressions that can be validated against physical theory; Quick check - confirm discovered expressions are both accurate and physically meaningful
- Graph Neural Networks: Why needed - as state-of-the-art baseline for comparison and to demonstrate SEGVAE's advantages in interpretability; Quick check - verify GNN performance on same sparse datasets
- DFT Formation Energies: Why needed - as ground truth for training and validation; Quick check - ensure DFT data quality and consistency across material systems

## Architecture Onboarding

Component map: DFT Data -> SEGVAE Encoder (LSTM) -> Latent Space -> SEGVAE Decoder (LSTM) -> Symbolic Expressions -> Physical Validation

Critical path: The essential sequence is DFT data preprocessing → LSTM encoding of defect configurations → latent space representation → LSTM decoding to symbolic expressions → mathematical simplification → physical interpretation and validation.

Design tradeoffs: The method balances between model complexity (LSTM layers, latent space dimensions) and interpretability (expression simplicity, sparsity constraints). More complex models may capture finer details but sacrifice interpretability, while simpler models may miss subtle interaction effects.

Failure signatures: Poor performance may manifest as: 1) Discovered expressions that are overly complex or nonsensical, 2) Inability to extrapolate beyond training data range, 3) Expressions that don't resemble known physical models, or 4) Computational instability during training or decoding.

First experiments: 1) Test SEGVAE on simple synthetic datasets with known analytical solutions to verify basic functionality, 2) Validate on a single material system (e.g., MoS2) with increasing dataset sizes to assess scaling behavior, 3) Compare discovered expressions against analytical models for specific defect types to assess physical interpretability.

## Open Questions the Paper Calls Out
The paper raises questions about whether discovered symbolic expressions genuinely capture underlying physical mechanisms versus merely fitting training data with human-interpretable forms. The connections between SEGVAE-derived expressions and known physical models (RKKY, Friedel oscillations) are qualitative rather than quantitatively validated. The extrapolation capabilities demonstrated are limited to distance-dependent trends, leaving questions about broader predictive reliability across diverse defect configurations and material systems.

## Limitations
- Physical interpretability claims remain qualitative without rigorous quantitative validation against established physical models
- Superiority claims over graph neural networks are based on limited comparisons and specific metrics
- Computational efficiency advantages are asserted but not substantiated with runtime or resource usage metrics
- Extrapolation tests are limited to distance-dependent trends without exploring broader predictive reliability

## Confidence
- Technical implementation accuracy: High - detailed SEGVAE architecture description and established benchmark usage
- Physical interpretability claims: Medium - qualitative parallels drawn but lack quantitative validation
- Performance superiority over GNNs: Medium - constrained comparisons and limited GNN architecture exploration
- Computational efficiency claims: Low - not substantiated with concrete metrics
- Extrapolation reliability: Medium - encouraging results but limited scope of testing

## Next Checks
1. Systematically compare SEGVAE-derived expressions against analytical models from solid-state physics across a wider range of defect types and materials to test physical consistency
2. Conduct ablation studies on SEGVAE's LSTM components and sparsity assumptions to quantify their contributions to performance and interpretability
3. Benchmark SEGVAE against multiple GNN architectures (including those with symbolic post-processing) on diverse metrics such as extrapolation accuracy, computational cost, and robustness to noise in DFT data