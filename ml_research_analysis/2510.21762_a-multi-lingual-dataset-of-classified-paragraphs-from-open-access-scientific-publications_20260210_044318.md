---
ver: rpa2
title: A Multi-lingual Dataset of Classified Paragraphs from Open Access Scientific
  Publications
arxiv_id: '2510.21762'
source_url: https://arxiv.org/abs/2510.21762
tags:
- scientific
- mentions
- data
- dataset
- paragraphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a multi-lingual dataset of 833k classified paragraphs
  from CC-BY licensed scientific publications, addressing the need for annotated training
  data to improve text classification and named entity recognition in scientific literature
  mining across multiple languages. The dataset was created by extracting paragraphs
  from the French Open Science Monitor corpus using GROBID, then semi-automatically
  classifying them into four categories (acknowledgments, data mentions, software
  mentions, clinical trial mentions) using specialized tools (GROBID, DataStet, Softcite).
---

# A Multi-lingual Dataset of Classified Paragraphs from Open Access Scientific Publications

## Quick Facts
- arXiv ID: 2510.21762
- Source URL: https://arxiv.org/abs/2510.21762
- Reference count: 2
- Multi-lingual dataset of 833k classified paragraphs from CC-BY scientific publications

## Executive Summary
This paper presents a multi-lingual dataset of 833k classified paragraphs extracted from open access scientific publications. The dataset addresses the critical need for annotated training data to improve text classification and named entity recognition in scientific literature mining across multiple languages. Created by processing the French Open Science Monitor corpus using GROBID, the dataset includes paragraphs classified into four categories: acknowledgments, data mentions, software mentions, and clinical trial mentions. Each paragraph is annotated with language identification and scientific domain information.

The dataset is designed to enable training of paragraph classification models for non-English languages and named entity recognition models for extracting research data, software, clinical trial, and funding information from scientific literature. While the dataset contains primarily English content (98.4%) with limited French representation (1.5%), it provides a valuable resource for advancing multi-lingual scientific text mining capabilities and addressing the scarcity of annotated data for non-English scientific documents.

## Method Summary
The dataset was created through a semi-automated pipeline that processed the French Open Science Monitor corpus. Paragraphs were extracted from CC-BY licensed scientific publications using GROBID, a machine learning library for parsing scholarly documents. The extracted paragraphs were then classified into four categories using specialized tools: GROBID for acknowledgments, DataStet for data mentions, Softcite for software mentions, and a clinical trial detection tool for clinical trial mentions. Each paragraph received language identification using fastText and scientific domain classification using OpenAlex. The resulting dataset contains 833k paragraphs with 108k acknowledgment paragraphs, 570k data mentions, 203k software mentions, and 8.7k clinical trial mentions.

## Key Results
- Dataset contains 833k classified paragraphs from CC-BY scientific publications
- 98.4% English content, 1.5% French content, 0.1% other languages
- 108k acknowledgment paragraphs, 570k data mentions, 203k software mentions, 8.7k clinical trial mentions
- Semi-automated classification using GROBID, DataStet, Softcite tools

## Why This Works (Mechanism)
The dataset addresses the fundamental bottleneck in multi-lingual scientific text mining: the lack of large-scale annotated training data for non-English scientific literature. By leveraging existing specialized tools for entity extraction and classification, the authors created a scalable pipeline that can process large volumes of scientific text. The combination of language identification and domain classification enables targeted model training for specific scientific fields. The CC-BY licensing ensures the dataset can be freely used for research and commercial applications, promoting reproducibility and further development of multi-lingual scientific NLP models.

## Foundational Learning
- **Scientific text mining**: The process of extracting structured information from unstructured scientific literature; needed to enable large-scale analysis of research outputs and trends.
- **Named entity recognition (NER)**: Identifying and classifying named entities in text such as data sets, software, and clinical trials; critical for building knowledge graphs from scientific literature.
- **Semi-supervised classification**: Using automated tools to classify text with minimal human intervention; enables scaling to large datasets while maintaining reasonable accuracy.
- **Language identification**: Determining the language of text segments; essential for multi-lingual processing and ensuring appropriate language-specific models are applied.
- **Scientific domain classification**: Categorizing research into specific fields or disciplines; allows for domain-specific model training and improves entity extraction accuracy.
- **CC-BY licensing**: Creative Commons Attribution license permitting free use and distribution; ensures dataset accessibility for research and commercial applications.

## Architecture Onboarding

**Component map**: Open Access Publications -> GROBID Extraction -> Classification Tools -> Language Identification (fastText) -> Domain Classification (OpenAlex) -> Final Dataset

**Critical path**: The pipeline flows from document ingestion through GROBID parsing to extract paragraphs, then applies specialized classification tools (GROBID, DataStet, Softcite, clinical trial detector), followed by language and domain annotation. The critical performance determinant is the accuracy of the classification tools, as errors compound through the pipeline.

**Design tradeoffs**: The authors chose semi-automated classification over manual annotation to achieve scale, accepting potential classification errors for broader coverage. The heavy English bias reflects the underlying corpus composition rather than intentional design, potentially limiting multi-lingual utility. Using existing specialized tools (GROBID, DataStet, Softcite) provides proven accuracy but may miss domain-specific entities not covered by these systems.

**Failure signatures**: Poor classification accuracy would manifest as high inter-category confusion, particularly between closely related categories like data mentions and software mentions. Language identification failures would show up as misclassified paragraphs in the language distribution statistics. Domain classification errors would result in inconsistent scientific field distributions across categories.

**First experiments**:
1. Validate classification accuracy by randomly sampling 100 paragraphs from each category and manually verifying their correct classification
2. Test language identification accuracy by sampling paragraphs from both English and French content and verifying fastText's predictions
3. Assess domain classification consistency by examining whether acknowledgment paragraphs show different scientific domain distributions compared to data mention paragraphs

## Open Questions the Paper Calls Out
None

## Limitations
- Extreme language imbalance (98.4% English, 1.5% French) severely limits multi-lingual utility
- Semi-automated classification likely introduces errors that propagate through downstream applications
- Dataset may contain selection bias toward certain research domains due to French Open Science Monitor corpus source
- Clinical trial mentions category is sparse (8.7k instances) compared to other categories

## Confidence
- **High**: Overall methodology and dataset creation process are well-documented and reproducible
- **Medium**: Classification accuracy and completeness of four paragraph types given semi-automated nature
- **Low-Medium**: Claims about multi-lingual utility given extreme language imbalance

## Next Checks
1. Conduct independent evaluation of classification accuracy by manually annotating a random sample of 500 paragraphs across all categories to estimate precision and recall rates
2. Test dataset effectiveness for training multi-lingual models by training separate classifiers for English and French acknowledgments, then measuring performance on held-out test sets for each language
3. Evaluate dataset coverage of scientific domains by comparing discipline distribution against representative sample of global scientific literature indexed in Crossref or PubMed