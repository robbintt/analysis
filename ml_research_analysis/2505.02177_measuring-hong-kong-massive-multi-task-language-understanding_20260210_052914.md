---
ver: rpa2
title: Measuring Hong Kong Massive Multi-Task Language Understanding
arxiv_id: '2505.02177'
source_url: https://arxiv.org/abs/2505.02177
tags:
- hong
- kong
- chinese
- performance
- traditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HKMMLU is a benchmark designed to evaluate Large Language Models'
  understanding of Hong Kong-specific knowledge and linguistic skills, including Traditional
  Chinese, Cantonese, and regional cultural context. The benchmark consists of 26,698
  multi-choice questions across 66 subjects and 90,550 Mandarin-Cantonese translation
  tasks.
---

# Measuring Hong Kong Massive Multi-Task Language Understanding

## Quick Facts
- arXiv ID: 2505.02177
- Source URL: https://arxiv.org/abs/2505.02177
- Reference count: 40
- Key outcome: HKMMLU benchmark reveals significant performance gaps for LLMs on Hong Kong-specific knowledge, with even best models scoring only 74.8% accuracy versus human performance

## Executive Summary
HKMMLU is a benchmark designed to evaluate Large Language Models' understanding of Hong Kong-specific knowledge and linguistic skills, including Traditional Chinese, Cantonese, and regional cultural context. The benchmark consists of 26,698 multi-choice questions across 66 subjects and 90,550 Mandarin-Cantonese translation tasks. Experiments with GPT-4o, Claude 3.7 Sonnet, and 18 open-source models show that even the best-performing model, DeepSeek-V3, achieves only 74.8% accuracy, significantly lower than general Chinese benchmarks like CMMLU. Models perform better translating from Cantonese to Mandarin than the reverse, with Llama-3-Taiwan-70B-Instruct showing the smallest gap. Performance is influenced by question language, model size, prompting strategies, and token lengths. A human comparison test with 100 Hong Kong-specific questions reveals that even top models fall short of human performance, especially on Cantonese content.

## Method Summary
The HKMMLU benchmark evaluates LLMs using 26,698 multi-choice questions across 66 subjects in four categories (STEM, Social Sciences, Humanities, Other) and 90,550 Mandarin-Cantonese translation pairs. Evaluation uses 0-shot, few-shot (0-5), and Chain-of-Thought (CoT) prompting strategies. Answer extraction is performed via regex patterns. Models tested include GPT-4o, Claude 3.7 Sonnet, DeepSeek-V3 (via API), and 18 open-source models via vLLM on 8× NVIDIA H800-80GB GPUs. Performance metrics include accuracy for multi-choice questions and BLEU, METEOR, ROUGE-L for translations.

## Key Results
- Best model (DeepSeek-V3) achieves only 74.8% accuracy, significantly below general Chinese benchmarks
- Asymmetric translation performance: Cantonese→Mandarin outperforms Mandarin→Cantonese due to training data imbalance
- Chain-of-thought prompting improves STEM performance but provides no benefit for Humanities or Hong Kong-specific knowledge
- Open-source models show performance degradation beyond 600 tokens, while closed-source models remain stable or improve
- Human evaluation reveals larger performance gaps on Cantonese questions versus Traditional Chinese questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-thought prompting improves STEM task performance by providing reasoning scaffolds, but provides limited benefit for knowledge-dependent domains (Humanities, Hong Kong-specific content).
- Mechanism: CoT enables systematic decomposition of calculation and logic problems into verifiable steps. However, when tasks require factual knowledge that the model lacks (e.g., Hong Kong legal precedents, cultural references), reasoning cannot compensate for missing information.
- Core assumption: Models possess sufficient domain knowledge to reason through; reasoning failure occurs when knowledge gaps exist rather than reasoning capability gaps.
- Evidence anchors:
  - [abstract] "Performance is influenced by question language, model size, prompting strategies"
  - [section 4.2.2] "DeepSeek-V3, GPT-4o, and GLM-4-9B-Chat increased by more than five percentage points in the STEM category" with 1-shot CoT, but "no model shows improvement in Humanities"
  - [corpus] Weak external validation; corpus shows related work on cross-cultural evaluation (MCEval) but no direct replication of this specific CoT domain-split finding

### Mechanism 2
- Claim: Asymmetric translation performance (Cantonese→Mandarin outperforms Mandarin→Cantonese) stems from training data imbalance and the lower-resource status of Cantonese in LLM corpora.
- Mechanism: Mandarin appears more frequently in training data with standardized grammar and vocabulary, enabling better mapping from Cantonese (source) to Mandarin (target). The reverse requires generating Cantonese-specific vocabulary and grammatical structures that may be underrepresented in model weights.
- Core assumption: The quality and quantity of target-language training data determines translation quality more than source-language comprehension.
- Evidence anchors:
  - [abstract] "Models perform better translating from Cantonese to Mandarin than the reverse"
  - [section 4.1] "This discrepancy may be attributed to the fact that Mandarin is more widely used in China and has more training materials available"
  - [corpus] HKCanto-Eval paper confirms "Cantonese language used in Hong Kong presents unique challenges...due to...lack of dedicated evaluation datasets"

### Mechanism 3
- Claim: Token length creates divergent performance patterns between closed-source and open-source models due to architectural and training differences in long-context handling.
- Mechanism: Closed-source models and larger open-source models (72B+) show stable or improved accuracy beyond 600 tokens, while smaller open-source models (≤27B) decline. This suggests either superior attention mechanisms, training on longer sequences, or both in larger/closed models.
- Core assumption: The performance degradation in smaller models at longer token lengths is not due to knowledge gaps but context-window attention degradation.
- Evidence anchors:
  - [abstract] "Performance is influenced by...token lengths"
  - [section 4.2.4] "When the question token length exceeds 600, the accuracy of most open-source models begins to decline, whereas the accuracy of closed-source models tends to improve"
  - [corpus] No direct corpus validation of this specific 600-token threshold; neighboring papers focus on translation/culture, not context length

## Foundational Learning

- Concept: **Diglossia in Chinese NLP (Written Traditional Chinese vs. Spoken Cantonese)**
  - Why needed here: Hong Kong uses Traditional Chinese script for formal writing but Cantonese for spoken communication. These are not interchangeable—Cantonese has distinct vocabulary, grammar, and particles that cannot be translated word-for-word from Mandarin.
  - Quick check question: If a model outputs "甚麼" in response to a Cantonese query, would this be appropriate for a Hong Kong context? (Answer: No—"甚麼" is Taiwan-style; Hong Kong uses "甚麼" in formal contexts but Cantonese colloquial would be "咩")

- Concept: **MMLU-style Benchmark Evaluation Methodology**
  - Why needed here: HKMMLU follows the MMLU paradigm (multi-choice, multi-subject). Understanding how accuracy is computed, the role of subject categorization, and how to interpret cross-benchmark comparisons (CMMLU vs. TMMLU+ vs. HKMMLU) is essential for proper experimental setup.
  - Quick check question: Why might a model score higher on CMMLU than HKMMLU even if both are in Chinese? (Answer: CMMLU tests generalized Chinese knowledge; HKMMLU tests region-specific Hong Kong knowledge and Cantonese linguistic competence)

- Concept: **Prompting Strategy Tradeoffs (Zero-shot vs. Few-shot vs. CoT)**
  - Why needed here: The paper demonstrates that more complex prompting does not uniformly improve performance. CoT helps STEM; few-shot can decrease performance for some models (Claude 3.7 Sonnet dropped sharply at 5-shot). Understanding when and why to apply each strategy is critical for evaluation.
  - Quick check question: What might cause a model to perform worse with 5-shot prompting than 0-shot? (Answer: Context confusion, pattern mismatch with training, or exceeding optimal context length without corresponding knowledge benefit)

## Architecture Onboarding

- Component map:
  - Benchmark Dataset -> Question formatting -> Prompt construction -> Model inference -> Answer extraction -> Metric computation
  - Data ingestion -> subject categorization -> Question formatting -> Prompt construction -> Model inference -> Answer extraction -> Metric computation

- Critical path:
  1. Data ingestion → subject categorization (manual or LLM-assisted with majority voting)
  2. Question formatting (Traditional Chinese, 2-4 options per question)
  3. Prompt construction based on evaluation mode (direct/CoT/few-shot)
  4. Model inference (track both answer accuracy and token lengths)
  5. Answer extraction via regex patterns
  6. Metric computation per subject, category, and aggregate

- Design tradeoffs:
  - **Traditional vs. Simplified Chinese prompts**: Paper found models performed slightly better in Traditional Chinese for HK-specific content; use Traditional for Hong Kong evaluation
  - **0-shot vs. CoT for domain-specific tasks**: Use CoT for STEM; consider direct answering for Humanities/HK-specific to avoid performance degradation
  - **Model selection**: DeepSeek-V3 offers best overall accuracy; Llama-3-Taiwan-70B offers best translation balance; smaller models (<14B) not recommended for this benchmark

- Failure signatures:
  - Sharp accuracy drop at 5-shot (especially Claude): Model confusion from excessive examples
  - Declining accuracy beyond 600 tokens (open-source models): Long-context handling failure
  - Large gap between Cantonese→Mandarin and Mandarin→Cantonese translation: Insufficient Cantonese training data
  - Similar performance on Traditional vs. Simplified for HK-specific questions: Model not leveraging region-specific knowledge

- First 3 experiments:
  1. **Baseline Establishment**: Run 0-shot evaluation on HKMMLU multi-choice for your target model, tracking accuracy by category (STEM, Social Sciences, Humanities, Other) and identifying weakest subjects
  2. **Token Length Analysis**: Bin questions by token length (0-200, 200-400, 400-600, 600-800, 800+) and plot accuracy curves to identify your model's context degradation threshold
  3. **Translation Asymmetry Test**: Evaluate both translation directions (Mandarin↔Cantonese) using BLEU/METEOR/ROUGE-L; calculate gap size and compare to Llama-3-Taiwan-70B's 62.1 avg vs 64.9 avg (smallest gap in paper) as a benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What causes the asymmetric translation performance where models consistently perform better in Cantonese-to-Mandarin translation than Mandarin-to-Cantonese?
- Basis in paper: [explicit] "Models generally perform better in translating from Cantonese to Mandarin than from Mandarin to Cantonese... This discrepancy may be attributed to the fact that Mandarin is more widely used in China and has more training materials available."
- Why unresolved: The paper offers a hypothesis (training data imbalance) but does not validate it experimentally; the Llama-3-Taiwan series shows smaller gaps, suggesting fine-tuning helps, but the root mechanism remains unclear.
- What evidence would resolve it: Controlled experiments varying Cantonese vs. Mandarin training data proportions, or analyzing attention patterns during bidirectional translation.

### Open Question 2
- Question: Why does providing reasoning examples in CoT prompting significantly improve STEM performance but not Humanities or Hong Kong-specific knowledge tasks?
- Basis in paper: [explicit] "These models also show slight enhancement in Social Sciences. However, only GPT-4o and Llama-3-70B-Instruct show an increase in scores in the Other subject, while no model shows improvement in Humanities. This limitation is because these two categories include more Hong Kong-specific knowledge, which cannot be learned through reasoning if the models lack relevant knowledge."
- Why unresolved: The explanation is plausible but untested; it remains unclear whether the issue is knowledge absence or something about how cultural/regional knowledge is encoded.
- What evidence would resolve it: Probing experiments measuring knowledge retrieval before/after CoT, or testing whether adding Hong Kong-specific knowledge via retrieval-augmented generation enables CoT benefits in Humanities.

### Open Question 3
- Question: Can Traditional Chinese fine-tuning on Taiwan data effectively transfer to Hong Kong-specific linguistic and cultural contexts?
- Basis in paper: [inferred] Llama-3-Taiwan-70B-Instruct performs best on TMMLU+ but "performs poorly on HKMMLU, indicating a lack of knowledge of Hong Kong and Cantonese" despite achieving strong translation scores and the smallest Mandarin-to-Cantonese gap.
- Why unresolved: The paper shows Taiwan fine-tuning improves Traditional Chinese capability but not Hong Kong cultural knowledge, yet does not investigate whether joint training on both regions' data could achieve both.
- What evidence would resolve it: Experiments fine-tuning models on combined Taiwan and Hong Kong data, measuring performance on both TMMLU+ and HKMMLU to test for transfer or interference.

### Open Question 4
- Question: What specific linguistic or cultural factors cause the larger performance gap between humans and LLMs on Cantonese questions compared to Traditional Chinese questions?
- Basis in paper: [explicit] "The gap between human and LLM is greater for questions in Cantonese than for those in Traditional Chinese, indicating DeepSeek-V3's deficiency in understanding Cantonese linguistic and cultural nuances."
- Why unresolved: The paper identifies the gap but does not analyze which aspects of Cantonese (vocabulary, grammar, colloquialisms, cultural references) contribute most to model failure.
- What evidence would resolve it: Error analysis categorizing Cantonese question failures by linguistic feature, or controlled experiments isolating grammar, vocabulary, and cultural knowledge components.

## Limitations
- Training data contamination risk from unknown model pretraining corpora
- Prompt sensitivity and reproducibility issues due to unspecified parameters
- Cultural knowledge representation gaps revealed by human evaluation

## Confidence
- High Confidence: Asymmetric translation performance and training data imbalance attribution
- Medium Confidence: CoT prompting benefits for STEM but not HK-specific knowledge
- Low Confidence: 600-token threshold for open-source model performance degradation

## Next Checks
1. **Error Analysis Granularity**: Conduct detailed error categorization on model failures across all 66 subjects to identify whether mistakes stem from linguistic confusion, cultural knowledge gaps, or reasoning errors
2. **Cross-Benchmark Calibration**: Compare HKMMLU performance against CMMLU, TMMLU+, and HKCanto-Eval using identical models and prompting strategies
3. **Token Length Threshold Replication**: Systematically test multiple open-source models across broader token length ranges to validate the 600-token performance drop consistency