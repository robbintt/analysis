---
ver: rpa2
title: 'BPO: Revisiting Preference Modeling in Direct Preference Optimization'
arxiv_id: '2506.03557'
source_url: https://arxiv.org/abs/2506.03557
tags:
- uni00000013
- uni00000048
- should
- responses
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a critical issue in direct preference optimization
  (DPO) where the likelihood of chosen responses can degrade alongside rejected responses,
  termed Degraded Chosen Responses (DCR). To address this, the authors propose Balanced
  Preference Optimization (BPO), which explicitly balances the optimization of chosen
  and rejected responses through two key components: a balanced reward margin and
  a gap adaptor.'
---

# BPO: Revisiting Preference Modeling in Direct Preference Optimization

## Quick Facts
- arXiv ID: 2506.03557
- Source URL: https://arxiv.org/abs/2506.03557
- Reference count: 40
- Primary result: BPO significantly improves math reasoning accuracy (+10.1% to +11.7%) while preventing degradation of chosen responses during DPO training.

## Executive Summary
BPO addresses a critical flaw in Direct Preference Optimization where chosen response likelihoods can degrade alongside rejected responses during training. The method introduces a balanced reward margin that explicitly optimizes chosen and rejected responses with equal strength through a gap adaptor parameter. This single-line modification to existing DPO implementations provides substantial performance gains on mathematical reasoning tasks while maintaining compatibility with existing DPO frameworks.

## Method Summary
BPO modifies the standard DPO loss by replacing the relative reward margin (rw - rl) with a balanced margin that takes the minimum of the chosen reward and the negative scaled rejected reward: min(rw, -α·rl). The loss function becomes L(θ) = -E[f(min(β·rw, -αβ·rl))], where α is the gap adaptor and β is the temperature parameter. This balanced approach ensures that optimization of chosen responses receives equal attention to rejected responses, preventing the degradation issue observed in standard DPO. The method requires only a single line of code modification and works with various loss functions including logistic log loss and hinge loss.

## Key Results
- BPO achieves +10.1% improvement on Llama-3.1-8B-Instruct (18.8% to 28.9%) on math reasoning tasks
- BPO achieves +11.7% improvement on Qwen2.5-Math-7B (35.0% to 46.7%) on the same benchmarks
- BPO outperforms DPO variants by +3.6% to +5.0% across multiple mathematical reasoning datasets
- The method successfully prevents degradation of chosen response likelihoods during training

## Why This Works (Mechanism)
BPO works by explicitly balancing the optimization of chosen and rejected responses through a modified reward margin. The key insight is that standard DPO's relative margin (rw - rl) can allow chosen response rewards to decrease if rejected responses improve sufficiently. By using min(rw, -α·rl), BPO ensures both components are optimized with equal strength, preventing chosen responses from degrading while still improving preference alignment.

## Foundational Learning
- **Preference optimization**: Aligning model outputs with human preferences through reward-based training. Needed to understand DPO's limitations and BPO's improvements.
- **Degraded Chosen Responses (DCR)**: The phenomenon where preferred response likelihoods decrease during DPO training. Critical for understanding the problem BPO addresses.
- **Gap adaptor α**: The scaling factor that controls the balance between chosen and rejected response optimization. Key hyperparameter for BPO's balanced approach.
- **Max-min strategy**: Selecting preference pairs by pairing the highest-reward response with the lowest-reward response. Important for constructing effective training pairs.

## Architecture Onboarding

Component map: Base Model -> Response Sampling -> Reward Ranking -> Preference Pair Construction -> BPO Loss -> Optimized Model

Critical path: The core training loop where responses are sampled, ranked by correctness, paired using max-min strategy, and optimized using BPO loss. This path directly addresses the DCR issue by ensuring balanced optimization.

Design tradeoffs: BPO trades increased computational complexity (due to min operation) for improved stability and performance. The gap adaptor α requires tuning but provides flexibility across different loss functions and datasets.

Failure signatures: Chosen response rewards decreasing during training indicates DCR, which BPO should prevent. Poor performance relative to DPO suggests implementation issues with the balanced margin calculation.

First experiments:
1. Verify chosen vs rejected reward trajectories during training to confirm DCR prevention
2. Test BPO with different α values (0.3, 0.5, 0.7) to find optimal balance
3. Compare BPO performance on base vs instruct model variants

## Open Questions the Paper Calls Out
- **On-policy learning**: How BPO performs when the policy interacts with a reward model during training, as the current work is restricted to offline methods.
- **Subjective tasks**: Whether BPO effectively mitigates DCR in open-ended generation tasks where preferences are subjective rather than objective correctness.
- **Dynamic gap adaptor**: Whether the gap adaptor α can be learned during training rather than using a fixed value, potentially improving performance across varying data distributions.

## Limitations
- Requires careful tuning of the gap adaptor α parameter, which varies depending on the loss function used
- Currently evaluated only on mathematical reasoning tasks with objective correctness, limiting generalizability to subjective tasks
- Restricted to offline preference datasets, not tested in on-policy or online learning scenarios

## Confidence

**High Confidence**: The core BPO mechanism and its ability to prevent DCR while improving performance is well-validated. The theoretical motivation is sound, and empirical results are consistent across multiple datasets and model scales.

**Medium Confidence**: The specific performance improvements are likely accurate but may vary depending on exact implementation choices. Missing details about β and optimizer configuration could affect absolute numbers.

**Low Confidence**: The comparison to DPO variants may be sensitive to implementation details, particularly since DPO is a competitive baseline and small differences in optimization setup could affect relative performance.

## Next Checks
1. Monitor chosen vs rejected response rewards during training to verify BPO maintains positive chosen response rewards while reducing rejected response rewards
2. Experiment with different β values (0.05, 0.1, 0.2) to determine sensitivity and identify optimal temperature scaling
3. Validate BPO's effectiveness beyond mathematical reasoning by testing on code generation, commonsense reasoning, or other preference alignment tasks