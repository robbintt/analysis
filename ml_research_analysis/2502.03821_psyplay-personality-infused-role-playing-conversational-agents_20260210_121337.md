---
ver: rpa2
title: 'PsyPlay: Personality-Infused Role-Playing Conversational Agents'
arxiv_id: '2502.03821'
source_url: https://arxiv.org/abs/2502.03821
tags:
- personality
- traits
- dialogue
- topic
- role
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PsyPlay, a framework for personality-infused
  role-playing conversational agents. PsyPlay generates dialogues between agents with
  distinct personality traits by first creating role cards, extracting dialogue topics,
  and then generating conversations.
---

# PsyPlay: Personality-Infused Role-Playing Conversational Agents

## Quick Facts
- arXiv ID: 2502.03821
- Source URL: https://arxiv.org/abs/2502.03821
- Authors: Tao Yang; Yuhua Zhu; Xiaojong Quan; Cong Liu; Qifan Wang
- Reference count: 37
- Primary result: Framework achieves 80.31% overall success rate in portraying personality traits, with 88.01% agreement with human evaluators

## Executive Summary
This paper introduces PsyPlay, a framework for generating role-playing dialogues between agents with distinct personality traits. The system creates role cards by mapping Big Five personality scores to intensity levels and generating corresponding experiences, then uses AutoGen to orchestrate multi-agent conversations around extracted psychological stress topics. The framework is evaluated using GPT-3.5 for back-testing, achieving strong performance particularly for positive personality roles. A new dataset, PsyPlay-Bench, containing 4745 correctly portrayed dialogues is also constructed to support future research.

## Method Summary
PsyPlay uses a three-stage pipeline: (1) Role Card Creation maps personality scores to three intensity levels ("a bit," "very," "extremely") and generates role profiles with personality-grounded experiences using GPT-3.5; (2) Topic Extraction uses few-shot prompting to extract single-issue topics from psychological stress scenarios; (3) Dialogue Generation employs the AutoGen framework for multi-turn agent conversations with personality prompts, followed by back-testing to evaluate trait consistency. The system relies on Big Five personality decomposition into 104 specific adjectival markers paired with intensity levels to improve trait portrayal accuracy.

## Key Results
- Overall personality portrayal success rate of 80.31% across all personality traits
- GPT-3.5 achieves 90.71% success rate for positive personality roles versus 61.57% for negative roles
- 88.01% agreement between LLM evaluation and human assessment of trait consistency
- Trait-specific success rates vary: NEU (92.47%), AGR (87.35%), CON (86.81%), EXT (79.02%), OPN (81.63%)
- Ablation studies show explicit trait labels improve negative role portrayal by 2.55%, while role experiences improve positive role portrayal by 2.63%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling personality traits into specific descriptors with intensity levels improves trait portrayal accuracy.
- Mechanism: Big Five traits are decomposed into 104 adjectival markers paired with intensity levels to generate role experiences that ground abstract traits in concrete behavioral histories, which the LLM then imitates during dialogue.
- Core assumption: LLMs can more reliably imitate specific behavioral patterns described through concrete adjectives and experiences than abstract trait labels alone.
- Evidence anchors: Ablation shows removing explicit traits reduces negative role success by 2.55%; removing experiences reduces positive role success by 2.63%.
- Break condition: If target LLM lacks instruction-following capability for nuanced adjective interpretation, descriptor effectiveness degrades to trait-only baseline.

### Mechanism 2
- Claim: Multi-agent dialogue with topic constraints provides natural contexts for personality expression while maintaining coherence.
- Mechanism: AutoGen orchestrates two agents with assigned role cards and shared topics from psychological stress scenarios, creating tension between conversational cooperation and trait expression that reveals personality through behavior.
- Core assumption: Dialogue topics from psychologically relevant scenarios sufficiently elicit personality-relevant behaviors across trait dimensions.
- Evidence anchors: EXT and OPN show lower consistency (79.02%, 81.63%) possibly because topics are more aligned with problem-solving than social/creative engagement.
- Break condition: If topics don't align with trait-relevant behaviors, personality expression becomes situational rather than consistent.

### Mechanism 3
- Claim: RLHF alignment creates asymmetric capability for positive versus negative personality portrayal.
- Mechanism: GPT-3.5's reinforcement learning from human feedback emphasizes positive values, facilitating positive trait expression while resisting negative traits. Models without positive alignment show more balanced portrayal across valence.
- Core assumption: The observed asymmetry stems specifically from RLHF alignment rather than underlying training data distribution or model architecture.
- Evidence anchors: GPT-3.5 positive role success: 90.71% vs. negative: 61.57%; Higgs-Llama-3-70B (role-play fine-tuned without positive alignment): positive 77.68%, negative 78.69%.
- Break condition: If RLHF alignment is modified or removed, valence asymmetry should diminish.

## Foundational Learning

- Concept: Big Five Personality Model (OCEAN)
  - Why needed here: PsyPlay's framework relies on mapping personality to five orthogonal dimensions with intensity scoring. Understanding these traits represent behavioral tendencies rather than categorical labels is essential for interpreting descriptor decomposition.
  - Quick check question: If a role scores "extremely high" on Neuroticism, what behavioral markers should appear in dialogue (without stating the trait explicitly)?

- Concept: In-Context Persona Adoption
  - Why needed here: PsyPlay relies on LLMs adopting personalities through prompt engineering alone, not fine-tuning. Understanding parametric knowledge versus context-conditional behavior is critical for diagnosing why certain traits express more reliably than others.
  - Quick check question: Why might removing role experiences hurt positive personality portrayal more than negative portrayal (per ablation results)?

- Concept: LLM-as-Evaluator Paradigm
  - Why needed here: The back-testing methodology uses GPT-3.5 to evaluate whether dialogues successfully portray intended traits, achieving 88.01% agreement with human evaluators. Understanding limitations of this approach is necessary for interpreting reported success rates.
  - Quick check question: What biases might GPT-3.5-as-evaluator introduce when assessing negative personality portrayal, given its own RLHF alignment?

## Architecture Onboarding

- Component map: Role Card Creator -> Topic Extractor -> Dialogue Generator -> Back-Tester
- Critical path:
  1. Sample personality combination from WASSA 2022 labels (307 unique combinations available)
  2. Generate role card with traits → descriptors → experience prompt
  3. Assign topic from extracted pool (161 topics)
  4. Initialize two agents via AutoGen with role cards + topic + dialogue prompt
  5. Run dialogue to completion (avg 2.5 turns)
  6. Back-test each role's portrayed traits; if mismatch, discard from Clean set
- Design tradeoffs:
  - Explicit traits vs. pure behavioral description: Including trait labels improves negative role portrayal (2.55% gain) but may cause LLM to "role-play the label" rather than embody the trait naturally
  - Dialogue turns vs. trait drift: More turns help positive personalities express fully but cause negative personalities to be "misdirected by partners" toward positive outcomes—optimize turn count per target valence
  - Intensity granularity: Three levels provide simplicity but may underspecify mid-range traits; "a bit" shows poor success (57.38% for negative roles)
- Failure signatures:
  - Valence inversion: Negative personality roles gradually shift toward positive expressions over dialogue turns
  - Trait bleeding: Roles express unintended traits; e.g., agreeableness roles also reflect conscientiousness
  - Topic-trait mismatch: EXT and OPN underperform when topics emphasize problem-solving over social/creative engagement
  - Descriptor override: LLM generates "successful" experiences even for negative characters, biasing toward positive outcomes
- First 3 experiments:
  1. Valance calibration test: Run PsyPlay with GPT-3.5 on 50 positive-role and 50 negative-role dialogues; if positive success > negative success by >20 percentage points, RLHF alignment effect is confirmed
  2. Intensity sweep: Generate dialogues for same personality profile at all three intensity levels; plot success rate curve; if "a bit" success <70% but "extremely" >85%, intensity threshold exists for reliable portrayal
  3. Cross-model transfer: Run identical role cards and topics on both GPT-3.5 and a non-aligned model; compare valence asymmetry; if asymmetry disappears in non-aligned model, mechanism 3 (RLHF effect) is validated

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PsyPlay perform in multi-party role-playing scenarios involving more than two agents?
- Basis in paper: The limitations section states, "Future research will expand this scope to include multi-party dialogues."
- Why unresolved: The current framework architecture and evaluation are strictly limited to dyadic interactions.
- What evidence would resolve it: Application of PsyPlay to groups of 3+ agents with analysis of trait consistency and interaction dynamics.

### Open Question 2
- Question: How can the consistency of negative personality traits be preserved during extended multi-turn dialogues?
- Basis in paper: The analysis notes that negative characters are easily "misdirected" by partners as dialogue turns increase, whereas positive characters benefit from more turns.
- Why unresolved: The paper identifies the phenomenon but does not propose a mechanism to mitigate the susceptibility of negative roles to partner influence.
- What evidence would resolve it: A study introducing a reinforcement mechanism for negative traits, showing stable success rates as turns increase.

### Open Question 3
- Question: Does removing the explicit conversation termination rule improve the naturalness of generated dialogues?
- Basis in paper: The limitations section states that "The fourth rule in the prompt of dialogue generation may potentially disrupt the fluidity of the dialogue."
- Why unresolved: The authors acknowledge the potential negative impact on fluency but did not ablate this specific constraint.
- What evidence would resolve it: Human evaluations comparing the naturalness of dialogues generated with and without the termination instruction.

## Limitations

- Framework relies on single LLM (GPT-3.5) for both generation and evaluation, creating potential systematic bias and self-consistency issues
- Success depends heavily on quality and comprehensiveness of 104-personality descriptor vocabulary, with no validation that it captures full behavioral range for each Big Five trait
- Framework tightly coupled to Big Five model and English-language psychological stress scenarios, with no validation on alternative personality frameworks or cross-cultural applications
- Negative personality traits show lower portrayal success rates and are susceptible to "drift" toward positive expressions over dialogue turns

## Confidence

**High Confidence:** The observation that positive personality roles are portrayed more successfully than negative ones across different model architectures. This finding is directly observable from empirical results (90.71% vs 61.57% for GPT-3.5) and replicated with Higgs-Llama-3-70B.

**Medium Confidence:** The mechanism by which RLHF alignment specifically causes the positive-negative asymmetry. While empirical evidence strongly suggests this relationship, controlled experiments varying alignment strategies are not conducted.

**Low Confidence:** The generalizability of the PsyPlay framework to other personality models or cultural contexts. The framework is tightly coupled to the Big Five model and English-language scenarios with no cross-cultural validation.

## Next Checks

1. **Cross-Modal Evaluation:** Conduct blind human evaluations of PsyPlay-generated dialogues using evaluators from different demographic backgrounds to assess whether the positive-negative asymmetry persists across cultural perspectives and whether descriptor-based approach generalizes beyond English-speaking evaluators.

2. **Model Architecture Ablation:** Test the framework with base models (no RLHF fine-tuning) versus aligned models across multiple model families (GPT, Claude, Llama) to determine whether the valence asymmetry is specific to RLHF alignment or reflects broader architectural patterns.

3. **Longitudinal Personality Stability:** Generate extended dialogues (10+ turns) between agents and track trait consistency over time using both LLM evaluation and human assessment to quantify the rate and conditions under which personality "drift" occurs, particularly for negative personality roles.