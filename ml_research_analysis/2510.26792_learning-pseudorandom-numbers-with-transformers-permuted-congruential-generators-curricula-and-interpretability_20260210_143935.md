---
ver: rpa2
title: 'Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators,
  Curricula, and Interpretability'
arxiv_id: '2510.26792'
source_url: https://arxiv.org/abs/2510.26792
tags:
- training
- accuracy
- figure
- test
- bits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies whether Transformer models can learn sequences
  from Permuted Congruential Generators (PCGs), a family of pseudorandom number generators
  that apply bitwise permutations to improve statistical randomness. Unlike prior
  work on linear congruential generators (LCGs), PCGs introduce additional complexity
  through XORs, rotations, and truncations, making them harder to predict.
---

# Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability

## Quick Facts
- arXiv ID: 2510.26792
- Source URL: https://arxiv.org/abs/2510.26792
- Authors: Tao Tao; Maissam Barkeshli
- Reference count: 40
- Primary result: Transformers can predict PCG sequences in-context, requiring √m context length; curriculum learning is essential for large moduli

## Executive Summary
This paper investigates whether Transformer models can learn to predict sequences from Permuted Congruential Generators (PCGs), a family of pseudorandom number generators that apply bitwise permutations to improve statistical randomness. Unlike prior work on linear congruential generators (LCGs), PCGs introduce additional complexity through XORs, rotations, and truncations, making them harder to predict. The authors show that Transformers can successfully perform in-context prediction on unseen PCG sequences, generalizing to unseen parameters and even when outputs are heavily truncated to a single bit. A scaling law is observed: the number of in-context elements required for high accuracy grows as √m with the modulus m. Direct training on large moduli fails due to stagnation, but curriculum learning—gradually increasing modulus difficulty and leveraging pretrained models from smaller moduli—dramatically improves convergence and accuracy. Finally, analysis of learned embeddings reveals that the model spontaneously clusters tokens according to rotation-invariant binary patterns, demonstrating an interpretable internal structure that aids transfer across moduli.

## Method Summary
The paper uses GPT-style decoder-only Transformers with rotary positional embeddings (RoPE) to perform in-context prediction on PCG sequences. For small moduli (m < 2^20), standard training suffices with 4 layers, 8 heads, d_model=1024. For large moduli, curriculum learning is essential: initialize from a checkpoint trained on a smaller modulus and mix in data from it with exponentially decaying mixing ratio. The model predicts the next token in sequences generated by PCGs with unseen (a,c) parameters, evaluated by next-token accuracy.

## Key Results
- Transformers achieve >90% accuracy on PCG sequence prediction in-context with context length scaling as 0.5√m
- Direct training fails for m ≥ 2^20 due to prolonged loss stagnation
- Curriculum learning with pretrained initialization enables successful training on large moduli
- Embedding analysis reveals rotation-invariant clustering by zero-run patterns, explaining transfer success

## Why This Works (Mechanism)

### Mechanism 1: Embedding Structure Enables Cross-Modulus Transfer
The model learns a rotation-invariant clustering of integer tokens in the embedding layer based on contiguous zero-runs in their binary representations. This structure, captured by principal component analysis, mirrors the invariances of the PCG's permutation function and provides a strong prior for transferring knowledge from smaller to larger moduli. When transferring to a larger modulus, this pre-structured embedding space allows the model to start much closer to a functional solution, bypassing the initial random state.

### Mechanism 2: Curriculum Learning Breaks Optimization Stagnation
For large moduli (m ≥ 2^20), direct training fails due to prolonged loss stagnation. Pretrained initialization provides a shortcut to stable representations by providing a better starting point in the loss landscape. The embedding layer of a pretrained model reaches a usable state almost immediately, allowing deeper layers to begin adapting right away and avoiding the stagnation phase where a randomly initialized model's loss plateaus.

### Mechanism 3: In-Context Prediction Scales with Modulus
The number of in-context sequence elements required for high prediction accuracy scales as √m with the modulus m. The model exploits residual bit-wise periodicities in the generator's output, with step-like improvements in accuracy at powers of two suggesting the model leverages the fact that the k-th least significant bit of an LCG state cycles with period 2^k.

## Foundational Learning

- **Concept: Linear vs. Permuted Congruential Generators**
  - Why needed here: Understanding the base recurrence (s_i = (a s_{i-1} + c) mod m) and how PCGs add complexity (XORs, rotations, truncations) is essential to appreciate why this task is harder than prior work on LCGs and why the √m scaling is a significant finding.
  - Quick check question: What are the key bitwise operations that distinguish a PCG from a standard LCG, and what is their purpose?

- **Concept: In-Context Learning**
  - Why needed here: The paper's central claim is that Transformers can perform "in-context prediction" on unseen sequences. This means the model infers the generator's hidden state and parameters directly from the input sequence without updating its weights, a core capability being tested.
  - Quick check question: In this paper, what does "in-context prediction" mean for a sequence generated with unseen parameters (a, c)?

- **Concept: Curriculum Learning & Transfer Learning**
  - Why needed here: The paper shows that for large moduli, standard training fails. The solution is a specific form of curriculum learning (pretraining on smaller moduli + data mixing), making this concept critical for reproducing the results or applying the method.
  - Quick check question: Why is directly training a Transformer on a PCG with modulus m=2^22 insufficient, and what two-step strategy does the paper propose?

## Architecture Onboarding

- **Component map:** PCG generator -> base-256 tokenizer -> GPT-style decoder-only Transformer with RoPE -> next-token prediction
- **Critical path:** 1) Generate sequences with target PCG variant and modulus 2) Initialize from smaller-modulus checkpoint (m≥2^20) 3) Mix 1% easier data decaying exponentially to 0 4) Train with AdamW and cosine learning rate decay 5) Evaluate in-context prediction accuracy
- **Design tradeoffs:** Separate generator training yields faster convergence; combined training yields multi-task models. Larger models learn more element-efficient strategies. Exponential decay curriculum schedule outperforms alternatives.
- **Failure signatures:** Stagnation (loss plateaus at high values for m≥2^20); overfitting (training loss decreases while test loss increases on small datasets).
- **First 3 experiments:** 1) Train from scratch on m=2^20 to reproduce stagnation 2) Validate curriculum fix with pretraining from m=2^18 3) Perform PCA on embeddings to verify rotation-invariant clustering

## Open Questions the Paper Calls Out

### Open Question 1
What are the precise residual bit-wise patterns in PCG sequences at positions that are powers of two, and how do models exploit them for step-like accuracy gains? The paper observes the phenomenon but defers analysis of the underlying structure.

### Open Question 2
Can Transformer-based approaches scale to cryptographically relevant moduli (e.g., m=2^128 used in NumPy's default XSLRR-128/64)? The paper tests up to m=2^22, leaving a 106-order-of-magnitude gap to practical PCG scales.

### Open Question 3
Why does optimization enter prolonged stagnation phases when training directly on large moduli? The paper demonstrates stagnation empirically and shows curriculum/pretraining mitigate it, but does not identify the theoretical cause.

### Open Question 4
Would more efficient architectures (e.g., state-space models, linear attention) improve the inference-time compute scaling law from ∝m^0.53 toward ∝m^0.5? The current study uses standard Transformers with quadratic attention; alternative architectures remain untested.

## Limitations

- The causal link between learned embedding structure and transfer success is inferential rather than proven
- The specific curriculum schedule may be tuned to the experimental setup
- The √m scaling law is derived from a limited range of moduli and may not generalize to significantly larger scales
- Many claims about internal mechanisms are based on post-hoc analysis rather than proof

## Confidence

- **High Confidence:** Transformers can perform in-context prediction on PCG sequences; curriculum learning is necessary and effective for large moduli; embedding layer learns rotation-invariant clustering
- **Medium Confidence:** Required context length scales as √m; rotation-invariant clustering is the primary transfer mechanism; middle/deep layers differentiate PRNG variants
- **Low Confidence:** Specific exponential decay schedule is optimal; scaling law holds for all PCG variants beyond tested range

## Next Checks

1. **Ablation Study on Pretraining vs. Curriculum:** Train models on m=2^20 using three strategies: (a) from scratch, (b) with pretraining only (no curriculum), and (c) with curriculum only (no pretraining). Compare final accuracies and training dynamics to isolate the individual contributions of each component.

2. **Transfer to Novel PCG Variants:** Test the transfer capability by training a model on XSLRR-16/8 and then fine-tuning it on a different PCG variant (e.g., XSHRS) with the same modulus. Measure the performance gain over training from scratch to validate that the learned embedding structure generalizes beyond a single generator type.

3. **Scaling Law Stress Test:** Extend the in-context prediction experiments to moduli beyond 2^22 (e.g., 2^24, 2^26) and to different PCG variants. Plot the required context length versus √m to confirm the scaling law holds and to identify any deviations or breakpoints in the relationship.