---
ver: rpa2
title: 'QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities
  of Vision-Language Models'
arxiv_id: '2512.19526'
source_url: https://arxiv.org/abs/2512.19526
tags:
- video
- physical
- frame
- object
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QUANTI PHY is the first benchmark for quantitative evaluation of
  VLMs' physical reasoning on kinematic properties of moving objects. The benchmark
  includes 3.3K video-text instances with numerical ground truth, covering size, velocity,
  and acceleration estimation tasks.
---

# QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models

## Quick Facts
- arXiv ID: 2512.19526
- Source URL: https://arxiv.org/abs/2512.19526
- Authors: Li Puyin; Tiange Xiang; Ella Mao; Shirley Wei; Xinye Chen; Adnan Masood; Li Fei-fei; Ehsan Adeli
- Reference count: 40
- Key outcome: First benchmark for quantitative evaluation of VLMs' physical reasoning on kinematic properties, revealing models rely on world knowledge over visual evidence

## Executive Summary
QuantiPhy is the first benchmark designed to quantitatively evaluate vision-language models' (VLMs) physical reasoning abilities on kinematic properties of moving objects. The benchmark comprises 3.3K video-text instances with numerical ground truth, covering size, velocity, and acceleration estimation tasks. VLMs are provided with one physical prior and must infer another kinematic property through proportional reasoning from pixel-space measurements. The benchmark standardizes prompts and scoring across 21 state-of-the-art VLMs to enable fair comparison. Experiments reveal a consistent gap between VLMs' qualitative plausibility and actual numerical correctness, showing that models rely heavily on pre-trained world knowledge rather than faithfully using provided visual and textual inputs for quantitative reasoning.

## Method Summary
QuantiPhy evaluates VLMs on kinematic inference tasks where models must estimate physical properties (size, velocity, acceleration) of moving objects using one known property as a prior. The benchmark uses 3.3K+ video-text pairs with numerical ground truth, covering 2D and 3D scenarios with static and dynamic priors. Each video is 2-3 seconds long and contains 1-3 questions. Models receive video frames, a textual prior with timestamp, and a question, then must output a numerical answer with units. The evaluation uses Mean Relative Accuracy (MRA) across tolerance thresholds from 0.5 to 0.95. The benchmark includes standardized prompts, a hierarchical parser for numerical extraction, and diagnostic tools including video ablation and counterfactual prior scaling to assess input faithfulness.

## Key Results
- VLMs show substantial performance gaps between qualitative plausibility and quantitative accuracy on kinematic reasoning tasks
- Current VLMs rely heavily on pre-trained world knowledge rather than using provided visual and textual inputs faithfully
- Chain-of-thought prompting is less helpful than anticipated, with performance worse than direct zero-shot prompting due to error propagation in intermediate numerical steps
- Scene complexity affects performance, with visually rich backgrounds partially compensating for weak visual measurement through implicit reference cues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantitative kinematic inference reveals VLMs' lack of input-faithful reasoning.
- Mechanism: The benchmark requires models to use provided numerical priors to infer target properties through proportional reasoning from pixel-space measurements, exposing whether models genuinely process visual evidence or default to memorized world knowledge.
- Core assumption: Kinematic properties are interdependent through a scalar scale factor γ, so correct reasoning requires computing this factor from the prior and applying it consistently.
- Evidence anchors: Abstract states VLMs lean on pre-trained knowledge; counterfactual analysis shows MRA drops by 70-80% when priors are scaled; SD-VLM paper notes VLMs' spatial understanding is hindered by 2D images' deficient spatial representation.

### Mechanism 2
- Claim: Scene context provides implicit reference cues that partially compensate for weak visual measurement.
- Mechanism: Complex backgrounds and multiple objects offer scale references that help models anchor quantitative estimates when explicit reasoning fails.
- Core assumption: Models can exploit visual structure in scenes as approximate priors even without explicit numerical conditioning.
- Evidence anchors: Section 5.1 shows performance in visually complex scenes is above other background conditions for most models; having more objects gives models extra reference targets for implicit comparison.

### Mechanism 3
- Claim: Chain-of-thought prompting fails because intermediate computation errors propagate.
- Mechanism: Decomposing kinematic inference into pixel measurement → scale estimation → rescaling steps exposes brittleness in numerical sub-operations. Most VLMs cannot reliably solve intermediate numeric subproblems.
- Core assumption: Explicit step-by-step decomposition should help if models have component skills but lack integration; failure indicates missing component skills.
- Evidence anchors: Section 5.3 shows chain-of-thought prompting is less helpful than anticipated with worse performance than direct zero-shot prompting; many models appear unable to reliably solve intermediate numeric subproblems.

## Foundational Learning

- Concept: Kinematic scale factor (γ) relates pixel-space to world-space measurements
  - Why needed here: The entire benchmark assumes a single scalar can convert between pixel measurements and physical units when one prior is given
  - Quick check question: Given a video where a 100-pixel-wide car moves at 50 pixels/frame, and the prior that the car is 4.5 meters long, what is its speed in m/s?

- Concept: Input faithfulness in VLMs
  - Why needed here: The diagnostic analysis hinges on distinguishing between models that process visual evidence vs. those that pattern-match from training data
  - Quick check question: If a model outputs plausible answers when video is removed but priors are kept, what does this indicate about its reasoning process?

- Concept: Mean Relative Accuracy (MRA) as evaluation metric
  - Why needed here: Understanding why MRA (averaging accuracy across error tolerance thresholds) is preferred over exact match for continuous numerical outputs
  - Quick check question: Why would a prediction of 3.1m receive higher MRA than 31m when ground truth is 3.0m?

## Architecture Onboarding

- Component map: Data sources -> Annotation pipeline -> Evaluation harness -> Diagnostic suite
  - Data sources: Blender simulations (controlled, precise ground truth), lab captures (multi-view stereo for 4D reconstruction), internet scraping (in-the-wild, requires manual annotation)
  - Annotation pipeline: Pixel-space kinematics extraction → scale factor computation → world-space ground truth
  - Evaluation harness: Standardized prompts → model inference → numerical parsing → MRA scoring
  - Diagnostic suite: Video ablation, counterfactual priors, CoT decomposition

- Critical path:
  1. Video input with static camera and at least one rigid translating object
  2. Provide one physical prior (size, velocity, or acceleration) with timestamp
  3. Model outputs numerical prediction with unit
  4. Compare against ground truth using MRA across tolerance thresholds {0.5, 0.55, ..., 0.95}

- Design tradeoffs:
  - 2D vs 3D: 2D assumes constant depth, enabling single scale factor; 3D requires depth priors and is harder
  - Simulation vs real data: Simulation offers perfect ground truth but limited diversity; real data has noise but better represents deployment conditions
  - Static vs dynamic priors: Static (size) is constant; dynamic (velocity/acceleration) requires timestamp-specific reasoning

- Failure signatures:
  - World knowledge override: Model outputs canonical values (e.g., g = 9.8 m/s²) regardless of video evidence or provided priors
  - Prior-only performance parity: Model achieves similar MRA with or without video input
  - Counterfactual insensitivity: Model predictions don't scale when priors are multiplied by extreme factors
  - Parse failures: Model produces verbose explanations or qualitative descriptions instead of numerical outputs

- First 3 experiments:
  1. Baseline evaluation: Run all 21 models on full benchmark, aggregate MRA by category (2D-Static, 2D-Dynamic, 3D-Static, 3D-Dynamic) to establish performance tiers
  2. Video ablation study: Compare video+prior vs. prior-only condition on 161-instance subset to quantify visual evidence utilization
  3. Counterfactual probe: Scale priors by factors {0.001, 0.01, ..., 700} and measure MRA drop to assess input faithfulness; expect ~70-80% degradation for current VLMs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can VLMs be trained or architected to faithfully use visual evidence and explicit numerical priors rather than relying on pre-trained world knowledge when performing quantitative physical reasoning?
- Basis in paper: The discussion section states improving VLMs' quantitative physical reasoning will require mechanisms that encourage faithful use of visual evidence and explicit numerical priors, rather than letting powerful but sometimes misleading pretrained world knowledge dominate.
- Why unresolved: The counterfactual analysis revealed that even the best models' predictions remain close to real-world magnitudes rather than tracking altered priors, indicating current architectures fundamentally lack input faithfulness.
- What evidence would resolve it: A model that maintains consistent MRA when priors are scaled counterfactually, or training ablations showing specific objectives improve faithfulness.

### Open Question 2
- Question: How does quantitative physical reasoning performance generalize to rotational dynamics, deformable objects, and dynamic camera perspectives—domains omitted from QuantiPhy?
- Basis in paper: The limitations section explicitly states the dataset focuses exclusively on translational movement, omitting rotational dynamics, and utilizes a fixed camera perspective while examining only rigid objects.
- Why unresolved: These complexities were deliberately excluded to create a tractable benchmark, leaving their effect on VLM performance unknown.
- What evidence would resolve it: Extension benchmarks evaluating VLMs on rotational motion inference, deformable body kinematics, and egocentric or moving-camera scenarios with the same quantitative evaluation protocol.

### Open Question 3
- Question: What physics-informed training objectives or specialized pre-training data would improve VLMs' quantitative kinematic reasoning?
- Basis in paper: The conclusion states our findings can inform new VLM training methodologies, such as physics-informed objectives or specialized pre-training on physics-rich data.
- Why unresolved: Current VLMs are trained on general web-scale data without explicit physics supervision; the optimal training interventions remain unexplored.
- What evidence would resolve it: Comparative experiments training VLM variants with different physics-aware losses or physics-rich datasets, evaluated on QuantiPhy and showing improved MRA over baseline models.

## Limitations

- The benchmark focuses exclusively on rigid, translating objects, excluding rotational dynamics, deformable bodies, and complex interactions
- The 2D assumption (constant depth) significantly simplifies the inference problem and may overstate VLM capabilities
- The annotation process for real-world videos faces inherent uncertainty in pixel-to-world conversions, particularly in complex scenes
- The benchmark focuses on single-step inference rather than sequential reasoning about multiple interacting objects or temporal dynamics

## Confidence

**High Confidence**: VLMs' poor performance on kinematic inference tasks; systematic reliance on world knowledge over provided priors; chain-of-thought prompting's ineffectiveness for this task type

**Medium Confidence**: Scene complexity effects on performance; classification of VLMs into performance tiers; generalizability of findings across 21 state-of-the-art models

**Low Confidence**: Specific MRA degradation percentages under counterfactual priors; relative difficulty ordering of categories; effectiveness of hierarchical parsing approach

## Next Checks

1. Cross-dataset generalization test: Evaluate the same models on QuantiPhy using videos from entirely different domains (e.g., urban surveillance footage, sports recordings, or natural scenes) to assess whether performance patterns hold beyond the current dataset's controlled conditions.

2. Fine-tuning intervention study: Select models from each performance tier and fine-tune them specifically on kinematic reasoning tasks using the QuantiPhy training split. Compare post-fine-tuning performance against baseline to determine whether observed limitations stem from architectural constraints or insufficient training data.

3. Component skill isolation: Design a controlled experiment that separates pixel measurement accuracy, scale factor computation, and numerical rescaling as independent subtasks. Measure model performance on each component to validate whether the observed poor integration in CoT prompting is due to missing individual skills or inability to chain them together.