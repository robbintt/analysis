---
ver: rpa2
title: 'DeepResearch Bench II: Diagnosing Deep Research Agents via Rubrics from Expert
  Report'
arxiv_id: '2601.08536'
source_url: https://arxiv.org/abs/2601.08536
tags:
- explicitly
- research
- deep
- rubric
- rubrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DeepResearch Bench II introduces a new benchmark for evaluating
  deep research agents using 9,430 fine-grained binary rubrics derived from expert-written
  investigative articles. Tasks are extracted across 22 domains through a four-stage
  LLM+human pipeline ensuring rubrics are verifiable and aligned with human expert
  judgment.
---

# DeepResearch Bench II: Diagnosing Deep Research Agents via Rubrics from Expert Report

## Quick Facts
- **arXiv ID:** 2601.08536
- **Source URL:** https://arxiv.org/abs/2601.08536
- **Reference count:** 40
- **Primary result:** Top-performing deep research agents pass fewer than 50% of fine-grained binary rubrics derived from expert investigative reports.

## Executive Summary
DeepResearch Bench II introduces a comprehensive benchmark for evaluating deep research agents through 9,430 binary rubrics extracted from expert-written investigative articles across 22 domains. The benchmark employs a rigorous four-stage pipeline combining LLM extraction with human expert review to ensure rubric quality and alignment with expert judgment. When tested on state-of-the-art deep research agents, even the strongest models fail to pass more than half of the rubrics, revealing a substantial capability gap between current systems and human experts in information recall, analysis, and presentation tasks.

## Method Summary
The benchmark construction follows a four-stage pipeline: (1) LLM extraction of fine-grained binary rubrics from source articles, (2) self-evaluation iteration requiring ≥90% accuracy on source material, (3) manual revision by human annotators, and (4) expert review involving over 400 human-hours. Evaluation uses Gemini-2.5-Pro as judge LLM, processing rubrics in batches of 50 per API call. Each rubric is scored as 1 (pass), 0 (fail), or -1 (blocked reference leakage), with blocked references enforced through prompt constraints and search-tool-level URL filtering.

## Key Results
- State-of-the-art deep research agents pass fewer than 50% of rubrics on average
- Information recall dimension shows the highest pass rates, while analysis and presentation dimensions show lower performance
- Source reference leakage occurs in a measurable fraction of evaluations, with scores of -1 indicating blocked reference violations
- Performance varies significantly across different deep research agent implementations

## Why This Works (Mechanism)
The benchmark works by grounding evaluation in actual expert-written investigative reports rather than synthetic or hypothetical scenarios. By extracting fine-grained binary rubrics directly from these sources and validating them through multiple human review stages, the benchmark ensures that evaluation criteria reflect genuine expert standards and expectations. The three-dimensional structure (information recall, analysis, presentation) provides comprehensive coverage of deep research capabilities, while the binary scoring system enables objective measurement and comparison across different agents.

## Foundational Learning
- **Rubric-based evaluation**: Why needed - provides objective, fine-grained assessment criteria; Quick check - verify rubric count per task (~71) and binary scoring logic
- **Four-stage construction pipeline**: Why needed - ensures rubric quality through LLM+human collaboration; Quick check - confirm stages include LLM extraction, self-evaluation, manual revision, expert review
- **Blocked-reference mechanism**: Why needed - prevents source leakage while testing true research capability; Quick check - measure leakage rate and verify -1 scoring for violations
- **Batch processing evaluation**: Why needed - enables efficient scoring of 9,430 rubrics; Quick check - confirm batch size of 50 and judge LLM configuration

## Architecture Onboarding
**Component map:** Task Generator -> Rubric Extractor -> Self-Evaluator -> Human Reviewer -> Expert Reviewer -> Evaluation Judge
**Critical path:** DRA generates report → Judge LLM scores 50 rubrics per batch → Aggregate dimension scores → Compute overall performance
**Design tradeoffs:** Binary scoring ensures objectivity but may oversimplify complex analytical tasks; blocked references protect intellectual property but may limit information access
**Failure signatures:** Score = -1 indicates source reference leakage; low analysis scores suggest reasoning limitations; presentation failures indicate formatting/communication issues
**First experiments:** 1) Test single DRA on 5 tasks with manual scoring verification; 2) Measure judge LLM agreement with human annotations on sampled reports; 3) Stress test blocked-reference mechanism with controlled prompts

## Open Questions the Paper Calls Out
None

## Limitations
- Rubric construction depends on undisclosed LLM prompts and model specifications, introducing uncertainty about consistency
- Binary scoring system may oversimplify complex analytical tasks and penalize valid non-exact matches
- Evaluation relies on single judge LLM (Gemini-2.5-Pro) with potential score inconsistency for nuanced criteria
- Blocked-reference mechanism effectiveness depends on target DRAs' access controls not fully specified

## Confidence
- **High confidence:** Scale (9,430 rubrics), domain coverage (22 domains), documented construction pipeline, finding that top models pass <50% rubrics
- **Medium confidence:** Validity as "human-expert level" measure depends on rubric-expert alignment; relative performance rankings reliable but absolute scores sensitive to judge calibration
- **Low confidence:** Generalizability beyond 132 tasks to broader deep research capabilities; extent to which leakage reflects fundamental vs implementation limitations

## Next Checks
1. **Score consistency validation:** Manually score 50 sampled reports against rubrics to compute inter-annotator agreement with LLM judge, targeting >90% F1
2. **Rubric quality assessment:** Conduct blind evaluation where human experts score reports using only rubric criteria to verify comprehensibility and consistency
3. **Leakage mechanism stress test:** Test blocked-reference system by attempting source citation extraction from DRAs using controlled prompts to quantify false negative rate