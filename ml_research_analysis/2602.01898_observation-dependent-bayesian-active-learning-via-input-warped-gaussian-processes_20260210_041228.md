---
ver: rpa2
title: Observation-dependent Bayesian active learning via input-warped Gaussian processes
arxiv_id: '2602.01898'
source_url: https://arxiv.org/abs/2602.01898
tags:
- learning
- input
- gaussian
- warp
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses a limitation in standard Gaussian process-based\
  \ Bayesian active learning: the posterior variance\u2014and thus variance-based\
  \ acquisition functions\u2014depends only on hyperparameters and input locations,\
  \ not on observed function values. This decouples exploration from the actual complexity\
  \ of the target function."
---

# Observation-dependent Bayesian active learning via input-warped Gaussian processes

## Quick Facts
- arXiv ID: 2602.01898
- Source URL: https://arxiv.org/abs/2602.01898
- Reference count: 14
- Primary result: Observation-dependent variance via input warping improves sample efficiency in Bayesian active learning.

## Executive Summary
Standard Gaussian process-based Bayesian active learning suffers from a fundamental limitation: posterior variance depends only on hyperparameters and input locations, not on observed function values. This means variance-based acquisition functions cannot adapt to the actual complexity of the target function, decoupling exploration from informativeness. This work proposes injecting observation-dependent feedback by learning a monotone input reparameterization (warp) used exclusively within the acquisition function. The warp is parameterized using conditional rational quadratic splines and trained via a self-supervised geometric objective that redistributes uncertainty across the domain.

## Method Summary
The authors address the limitation of standard GP-based Bayesian active learning by introducing an input-warping scheme that makes variance-based acquisition functions observation-dependent. They parameterize the warp using conditional rational quadratic splines and train it via a self-supervised objective that redistributes uncertainty based on observed function values. Critically, this warp is applied only to the acquisition function while leaving the underlying predictive GP unchanged. This approach enables variance-based acquisition to adapt to function complexity without requiring a more expressive surrogate model.

## Key Results
- Outperforms both standard GPs and likelihood-trained warps on synthetic benchmarks (Gramacy-Lee08, Peaks, Box, Hartmann)
- Demonstrates clear improvements on real-world photoluminescence datasets
- Particularly effective in non-stationary settings where standard GP variance-based acquisition fails to adapt to function complexity

## Why This Works (Mechanism)
The method works by breaking the coupling between input geometry and hyperparameter-induced uncertainty patterns. Standard GP acquisition functions explore based on static posterior variance, which ignores function complexity. By learning a monotone warp that redistributes uncertainty based on observations, the acquisition function gains sensitivity to the actual function structure while preserving the GP's predictive properties.

## Foundational Learning
- **Gaussian Processes**: Probabilistic models providing uncertainty estimates; needed for Bayesian active learning framework; quick check: can compute posterior mean and variance analytically.
- **Bayesian Active Learning**: Sequential design strategy selecting inputs to maximize information gain; needed for efficient function optimization; quick check: acquisition function guides next query point.
- **Input Warping**: Non-linear transformation of input space; needed to adapt exploration geometry; quick check: warp function is monotone and invertible.
- **Rational Quadratic Splines**: Flexible piecewise polynomial parameterization; needed for expressive yet tractable warp representation; quick check: can represent monotonic functions.
- **Self-supervised Training**: Learning without explicit labels; needed to train warp from acquisition behavior; quick check: objective based on uncertainty redistribution.

## Architecture Onboarding
- **Component Map**: Input -> Warp Function -> Acquisition Function -> Query Point -> Observation -> Repeat
- **Critical Path**: Observations → Warp Training → Warped Acquisition → Query Selection → New Observation
- **Design Tradeoffs**: Expressiveness of warp vs. computational cost; self-supervised objective vs. supervised alternatives; modification of acquisition only vs. full GP warping.
- **Failure Signatures**: Poor performance on noisy functions; overfitting to synthetic benchmarks; computational bottleneck in warp training.
- **First Experiments**: 1) Compare against standard GP on Gramacy-Lee08 with varying noise levels. 2) Test scalability on Hartmann function with d=6. 3) Evaluate robustness by adding outliers to Peaks benchmark.

## Open Questions the Paper Calls Out
None explicitly mentioned in the provided text.

## Limitations
- Limited to low-dimensional problems (benchmarks only up to moderate dimensions)
- Computational overhead from warp training not quantified relative to acquisition savings
- Potential restriction on expressiveness for complex input geometries due to rational quadratic spline parameterization

## Confidence
- **Core claim** (observation-dependent variance improves sample efficiency): High
- **Real-world applicability** (limited dataset diversity): Medium
- **Scalability to higher dimensions**: Low (not tested)

## Next Checks
1. Evaluate the method on higher-dimensional problems (d > 6) to assess scalability.
2. Test robustness to noisy observations by adding Gaussian noise to synthetic benchmarks.
3. Compare acquisition efficiency gains against the additional warp-training computational cost.