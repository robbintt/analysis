---
ver: rpa2
title: 'Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based
  Mobile Agents'
arxiv_id: '2505.11891'
source_url: https://arxiv.org/abs/2505.11891
tags:
- page
- task
- agents
- mobile
- noisy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mobile-Bench-v2, a benchmark for evaluating
  VLM-based mobile agents. It addresses limitations in existing benchmarks, such as
  unstable reward signals in online evaluations and single-path trajectories in offline
  evaluations, by proposing a slot-based instruction generation method (GIAS) and
  an offline multi-path evaluation approach.
---

# Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents

## Quick Facts
- arXiv ID: 2505.11891
- Source URL: https://arxiv.org/abs/2505.11891
- Authors: Weikai Xu; Zhizheng Jiang; Yuxuan Liu; Pengzhi Gao; Wei Liu; Jian Luan; Yuanchun Li; Yunxin Liu; Bin Wang; Bo An
- Reference count: 40
- Primary result: Introduces a more realistic and comprehensive benchmark for evaluating VLM-based mobile agents, addressing limitations in existing benchmarks.

## Executive Summary
Mobile-Bench-v2 is a new benchmark designed to evaluate the performance of Vision-Language Model (VLM)-based mobile agents. It addresses key limitations in existing benchmarks, such as unstable reward signals in online evaluations and the restrictive single-path trajectories in offline evaluations. The benchmark introduces a slot-based instruction generation method (GIAS) and an offline multi-path evaluation approach, enabling more stable and comprehensive assessment of agent capabilities. Mobile-Bench-v2 includes three task splits: a common task split for general evaluation, a noisy split to simulate real-world environments with pop-ups and ads, and an ambiguous instruction split to assess proactive interaction capabilities.

## Method Summary
The Mobile-Bench-v2 benchmark is built upon a graph-structured GUI corpus (Mobile3M) and uses a slot-based instruction generation method (GIAS) to create realistic and diverse instructions. GIAS extracts key information (slots) from GUI action sequences and fills them into task templates, allowing for stable step-wise rewards during evaluation. The benchmark supports both single-path (action matching) and multi-path (graph search with slot rewards) evaluation modes. It includes three task splits: a common split with 12,854 instructions, a noisy split with 100 instructions simulating ads/pop-ups, and an ambiguous split with 100 instructions to evaluate proactive interaction. The benchmark is evaluated using various agent frameworks (AppAgent-v1, Mobile-Agent-v2, UI-Tars, OS-Atlas, CogAgent) and VLMs (GPT-4o, GPT-4v, Qwen2-VL-72B, InternVL2-40B, LLaVA-72B-NEXT, Llama3.2-VL-90B).

## Key Results
- The slot-based instruction generation method (GIAS) provides more stable evaluation signals than dynamic online environment checks.
- The offline multi-path evaluation approach allows agents to explore multiple valid action sequences, offering a more faithful assessment of robustness and planning ability.
- The inclusion of noisy and ambiguous instruction splits enables the benchmark to measure agents' robustness and proactive interaction capabilities, which are critical for real-world deployment.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Slot-based instruction generation provides stable step-wise rewards.
- Mechanism: GIAS extracts key information (slots) from GUI state changes at "keynodes" during trajectory execution. These slots are filled into task templates to create instructions. Because slots are tied to specific GUI states, they can be checked deterministically during evaluation, creating stable step-wise reward signals.
- Core assumption: Critical actions and state changes can be reliably identified and abstracted into discrete slots.
- Evidence anchors: Abstract, Page 4, and the neighbor paper "Modular and Multi-Path-Aware Offline Benchmarking for Mobile GUI Agents" (ArXiv 2512.12634).
- Break condition: Fails if keynodes or slot information cannot be reliably extracted due to highly dynamic GUIs.

### Mechanism 2
- Claim: Offline multi-path evaluation assesses robustness and planning ability more faithfully.
- Mechanism: The benchmark uses a graph-structured corpus where nodes are GUI pages and edges are actions. Agents can perform "action search within the graph corpus and accumulate step rewards," allowing for multiple solutions to reach the goal state.
- Core assumption: The graph corpus adequately represents possible valid paths within tested apps.
- Evidence anchors: Abstract, Page 2, and the neighbor paper "Modular and Multi-Path-Aware Offline Benchmarking for Mobile GUI Agents" (ArXiv 2512.12634).
- Break condition: Fails if the graph corpus is incomplete and does not contain all viable paths for a task.

### Mechanism 3
- Claim: Explicit incorporation of noisy environments and ambiguous instructions measures robustness and proactive interaction.
- Mechanism: Constructs a "Noisy-split" with ads/pop-ups and an "Ambiguous-split" with removed slot information and pre-set Q&A pairs. Evaluates agent's ability to recover from noise or ask clarifying questions.
- Core assumption: Synthetic noise injection and slot-removal process create representative real-world challenges.
- Evidence anchors: Abstract, Page 6, and the neighbor paper "OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows."
- Break condition: Less effective if synthetic noise is not realistic enough or ambiguous instructions are too contrived.

## Foundational Learning

- **Graph-Structured GUI Corpus**
  - Why needed here: Understanding how the benchmark represents the mobile environment is crucial. The Mobile3M corpus models apps as directed graphs where nodes are GUI pages and edges are actions, enabling multi-path evaluation and slot-based rewards.
  - Quick check question: Can you explain why a graph structure is more suitable for representing mobile app tasks than a linear sequence of actions?

- **Offline vs. Online Agent Evaluation**
  - Why needed here: Grasping the fundamental trade-off between offline evaluation (reproducible, static data) and online evaluation (dynamic, realistic but unstable) is necessary to appreciate why Mobile-Bench-v2's hybrid approach is novel.
  - Quick check question: What are the two main problems with existing offline benchmarks that Mobile-Bench-v2 aims to solve?

- **Slot-Based Reward Signals**
  - Why needed here: This is the core technique for providing stable intermediate feedback. Understanding that "slots" are key pieces of information extracted from GUI states allows you to see how the evaluation can be more granular and robust than just checking the final screen.
  - Quick check question: In the GIAS method, what are "slots" derived from, and how do they function as "step rewards" during evaluation?

## Architecture Onboarding

- **Component map:** Mobile3M corpus (graph structure) -> GIAS Annotation (GUI Description -> Action Intent -> Slot Extraction -> Template Filling) -> Benchmark Split Creation (Common, Noisy, Ambiguous) -> Agent Evaluation (Single/Multi-path) -> Metric Calculation.
- **Critical path:** Data Collection (Mobile3M) -> Trajectory Sampling -> GIAS Annotation (GUI Description -> Action Intent -> Slot Extraction -> Template Filling) -> Benchmark Split Creation (Common, Noisy, Ambiguous) -> Agent Evaluation (Single/Multi-path) -> Metric Calculation.
- **Design tradeoffs:**
  - **Reproducibility vs. Realism:** Prioritizes reproducibility by being offline but adds realism through noisy/ambiguous splits. The "realism" is synthetic (e.g., ad injection) and may not capture all dynamic aspects of online environments.
  - **Annotation Cost vs. Scale:** GIAS method is automated to reduce cost but relies on VLM performance for quality. Paper notes some suboptimal annotations in complex templates (Page 7).
  - **Multi-path Flexibility vs. Evaluation Complexity:** Allowing multiple paths makes evaluation more comprehensive but requires a more complex graph-based evaluation logic compared to simple one-to-one action matching.
- **Failure signatures:**
  - **Poor Slot Extraction:** If the VLM fails to correctly identify or extract slots from GUIs, the generated instructions will be nonsensical or the step rewards will be misaligned.
  - **Graph Corpus Sparsity:** If the graph does not contain the path an agent takes, the multi-path evaluation will incorrectly penalize it.
  - **Unrealistic Noise/Ambiguity:** If the synthetic noise is too obvious or ambiguous instructions are too vague, the evaluation may not generalize to real-world challenges.
- **First 3 experiments:**
  1. **Ablation on Slot Quality:** Evaluate agent performance using instructions generated by GIAS versus human-annotated instructions to quantify the quality gap introduced by the automated method.
  2. **In-Domain vs. Out-of-Domain Noise Robustness:** Train an agent on AITZ-Noise and evaluate it on the new Mobile-Bench-Noisy split to test the generalization capability of noise robustness.
  3. **Multi-Path vs. Single-Path Correlation:** For a subset of tasks, compare agent scores on the single-path evaluation versus the multi-path evaluation. A low correlation would indicate that single-path metrics are indeed a poor proxy for overall agent capability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can supervised fine-tuning on contaminated datasets (e.g., AITZ-Noise) generalize to robust performance in real-world, out-of-domain noisy environments?
- Basis in paper: [inferred] Section 4.6 ("Discussion") and Table 5 show that while agents achieve >97% accuracy on in-domain noise after fine-tuning (Table 6), they suffer significant performance drops (e.g., Step.Acc decreasing by ~3-20%) when evaluated on the out-domain "Noisy-App" split, suggesting a lack of transferability.
- Why unresolved: The paper establishes that simple data contamination improves handling of *seen* ad types but fails to explain how to build agents that adapt to the dynamic, unpredictable nature of real-world pop-ups and video ads.
- What evidence would resolve it: Successful evaluation results on the "Noisy-App" split (distinct from the training contamination distribution) demonstrating sustained Step Accuracy comparable to clean environments.

### Open Question 2
- Question: How should mobile agent architectures be redesigned to natively support proactive clarification without disrupting task execution?
- Basis in paper: [explicit] The paper states in Section 3.2 that "none of the existing agent frameworks incorporates a dedicated questioning stage," forcing the evaluation to allow questions as a pre-decision step rather than an integrated architectural feature.
- Why unresolved: The study evaluates the *capability* of agents to answer questions (Table 4), but the optimal architectural integration of a "questioning module" that balances latency, cost, and task completion remains undefined.
- What evidence would resolve it: A novel agent framework that outperforms current AppAgent or MobileAgent baselines on the Ambiguous split by strategically querying for missing slots rather than hallucinating actions.

### Open Question 3
- Question: How can offline multi-path evaluation be optimized to penalize inefficient trajectory exploration (high Step Efficiency) without relying on exhaustive search?
- Basis in paper: [inferred] Table 3 shows that in multi-path evaluation, some models (e.g., InternVL) achieve Step Efficiency scores over 5.0, indicating they take roughly five times the minimum necessary steps, whereas GPT-4o maintains ~4.3-4.4.
- Why unresolved: While the benchmark rewards reaching key nodes, it does not propose a mechanism to guide the agent toward the *shortest* valid path during the search process in an offline setting.
- What evidence would resolve it: An agent strategy that maintains a high Success Rate (SR) on the Common split while consistently achieving a Step Efficiency (SE) score closer to 1.0.

## Limitations
- The benchmark relies heavily on synthetic noise injection rather than capturing real-world dynamic disturbances.
- The multi-path evaluation still operates within the constraints of a static graph corpus, potentially missing novel agent strategies.
- The slot-based instruction generation may struggle with highly complex or novel UI designs where keynodes are not easily identifiable.
- The benchmark focuses primarily on task completion metrics and does not explicitly evaluate safety or ethical considerations in agent behavior.

## Confidence
- **High:** The core claims about the limitations of existing benchmarks (unstable online rewards, single-path offline evaluation) are well-supported by the literature and the paper's analysis.
- **Medium:** The effectiveness of the GIAS method for generating realistic instructions and stable step rewards is demonstrated through evaluation, but the quality of generated instructions is not compared against human annotations in this paper.
- **Medium:** The claim that the benchmark provides a "more realistic and comprehensive" assessment is supported by the inclusion of noisy/ambiguous splits, but the realism of synthetic noise and the practical significance of ambiguous instructions remain to be validated through broader agent training and deployment.

## Next Checks
1. **Slot Quality Ablation:** Evaluate agent performance using Mobile-Bench-v2 instructions versus human-annotated instructions for the same tasks to quantify the quality gap introduced by the GIAS method.
2. **Multi-Path vs. Single-Path Correlation:** For a representative subset of tasks, measure the correlation between single-path and multi-path evaluation scores to determine if multi-path evaluation provides genuinely new information.
3. **Noise Generalization Test:** Train an agent on AITZ-Noise and evaluate its performance on the Mobile-Bench-v2 Noisy split, then test its performance on truly unseen apps with real-world ads/pop-ups to assess generalization beyond synthetic noise.