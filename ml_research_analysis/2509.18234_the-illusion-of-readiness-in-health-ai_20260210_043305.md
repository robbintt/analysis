---
ver: rpa2
title: The Illusion of Readiness in Health AI
arxiv_id: '2509.18234'
source_url: https://arxiv.org/abs/2509.18234
tags:
- image
- answer
- reasoning
- visual
- options
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Adversarial stress tests reveal that large multimodal models often
  fail under simple input perturbations, despite strong leaderboard scores. These
  models can guess correct answers without essential visual inputs and fabricate flawed
  reasoning, indicating reliance on superficial patterns rather than true multimodal
  understanding.
---

# The Illusion of Readiness in Health AI

## Quick Facts
- **arXiv ID:** 2509.18234
- **Source URL:** https://arxiv.org/abs/2509.18234
- **Reference count:** 40
- **Primary result:** Adversarial stress tests reveal large multimodal models often fail under simple input perturbations despite strong leaderboard scores

## Executive Summary
This study demonstrates that leading multimodal models in medical AI exhibit significant robustness gaps that standard benchmarks fail to capture. Through a systematic adversarial stress testing framework, researchers found that models can achieve high accuracy by relying on superficial patterns, positional biases, and memorized associations rather than genuine visual reasoning. The study introduces six stress tests that systematically probe model vulnerabilities through input perturbations, revealing that models often guess correct answers without essential visual inputs and fabricate flawed reasoning chains.

The research highlights a critical disconnect between leaderboard performance and clinical readiness, showing that widely used benchmarks measure diverse and often mismatched capabilities. The findings call for stress testing to become a standard evaluation practice for medical AI systems, emphasizing that apparent competence on conventional benchmarks may mask fundamental limitations in multimodal understanding and reasoning fidelity.

## Method Summary
The study evaluates four large multimodal models (GPT-4o, GPT-5, Gemini-2.5 Pro, OpenAI-o3/o4-mini) on medical visual question answering tasks using six adversarial stress tests. The framework includes input removal, modality necessity, format perturbation, distractor manipulation, visual substitution, and reasoning fidelity audits. A unified prompt template with temperature=0 is used across all tests. The primary evaluation uses the NEJM Visual-required Subset (175 clinician-curated cases) alongside other benchmarks including JAMA, VQA-RAD, and MIMIC-CXR. Performance is measured through accuracy metrics and a composite Robustness Score derived from five fragility values.

## Key Results
- Models achieved 33.7-37.7% accuracy on visual-required questions without images, far exceeding random chance (20%), indicating reliance on non-visual cues
- Chain-of-thought prompting produced fluent but often hallucinated reasoning traces that included non-existent visual features
- Benchmarks showed substantial heterogeneity in reasoning complexity and visual dependency, with NEJM ranking high on both dimensions while JAMA was mostly text-solvable
- High refusal rates (91.43% for GPT-4o on text-only visual-required questions) demonstrated appropriate uncertainty handling but penalized accuracy metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stress tests expose shortcut reliance that standard benchmarks conceal
- Mechanism: When critical inputs (images) are removed or perturbed, models that appear competent on leaderboards reveal dependence on superficial textual patterns, positional biases, and memorized associations rather than genuine multimodal reasoning
- Core assumption: Leaderboard accuracy reflects genuine capability rather than distributional exploitation
- Evidence anchors:
  - [abstract] "leading systems can guess the right answer even with key inputs removed, yet may get confused by the slightest prompt alterations"
  - [section: Results - Stress Test 2] On the NEJM Visual-required Subset, models achieved 33.7-37.7% accuracy (well above 20% chance) even without images, indicating "reliance on non-visual cues or memorized associations"
  - [corpus] Related work confirms this gap: "Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion" (Neural-MedBench)

### Mechanism 2
- Claim: Chain-of-thought prompting does not reliably improve multimodal medical accuracy and can mask flawed reasoning
- Mechanism: Explicit reasoning prompts produce fluent, structured rationales that frequently include non-existent image features or illogical justifications—creating an illusion of interpretability without actual reasoning fidelity
- Core assumption: Fluent explanations correlate with faithful reasoning processes
- Evidence anchors:
  - [abstract] "fabricate convincing yet flawed reasoning traces"
  - [section: Stress Test 6] "explicit CoT prompting yielded limited or negative gains... longer chains sometimes increased recall but also hallucinated details"
  - [corpus] Weak direct corpus support for CoT-specific failure; SafeMed-R1 addresses adversarial robustness but not reasoning fidelity specifically

### Mechanism 3
- Claim: Clinician-guided benchmark profiling reveals heterogeneous evaluation targets
- Mechanism: Benchmarks differ substantially in reasoning complexity and visual dependency but are treated interchangeably on leaderboards. This conflation means high scores may reflect proficiency on low-reasoning, text-solvable tasks rather than clinically relevant capabilities
- Core assumption: All medical benchmarks measure equivalent constructs of clinical competence
- Evidence anchors:
  - [abstract] "widely used benchmarks measure diverse and often mismatched capabilities"
  - [section: Fig. 4] NEJM ranks high on both reasoning and visual demands; JAMA is mostly text-solvable; VQA-RAD is visually dependent but low inference complexity
  - [corpus] Related work echoes this: benchmarks "fail to capture" true clinical reasoning (Illusion of Clinical Reasoning paper)

## Foundational Learning

- Concept: **Shortcut learning in multimodal models**
  - Why needed here: The paper's central finding is that models exploit statistical shortcuts (positional cues, text-only patterns, memorized associations) rather than performing genuine visual reasoning. Without understanding this phenomenon, stress test results appear inexplicable
  - Quick check question: If a model scores 80% on a visual diagnostic task but 40% when you shuffle answer choices, what does this suggest about its reasoning process?

- Concept: **Adversarial stress testing vs. standard evaluation**
  - Why needed here: The paper introduces a methodological shift from accuracy-only metrics to probing *how* models reach answers. Understanding this distinction is essential for implementing the recommended evaluation protocols
  - Quick check question: Why might a model maintain high accuracy on a benchmark while still being unsafe for clinical deployment?

- Concept: **Modality integration in vision-language models**
  - Why needed here: The stress tests specifically probe whether models genuinely integrate visual and textual information or simply process them independently. Understanding cross-modal grounding helps interpret T5 (visual substitution) results
  - Quick check question: When an image is replaced with one supporting a different diagnosis, what behavior would indicate genuine visual understanding versus shortcut reliance?

## Architecture Onboarding

- Component map: The stress testing framework consists of six modular probes: T1 (modality sensitivity), T2 (visual-necessity subset), T3 (format perturbation), T4 (distractor manipulation), T5 (visual substitution), T6 (reasoning fidelity audit)

- Critical path: For rapid implementation, prioritize T1, T2, and T5. T1/T2 reveal shortcut behavior with minimal engineering (image removal); T5 provides strongest evidence of visual grounding failures with controlled perturbations

- Design tradeoffs:
  - Abstention handling: GPT-4o's high refusal rate (91.43% on T2 text-only) penalizes accuracy but demonstrates "appropriate uncertainty handling"—decide whether your evaluation framework rewards or penalizes this behavior explicitly
  - Robustness score weighting: The paper uses unweighted averaging across tests; domain-specific deployments may warrant different weights (e.g., higher weight on T5 for radiology applications)
  - Benchmark selection: Match benchmark profile to deployment context—using JAMA (text-solvable) to evaluate visual diagnostic readiness risks false confidence

- Failure signatures:
  - High text-only accuracy on visual-required questions → shortcut reliance
  - Large performance drop under answer reordering → positional bias
  - Accuracy increase with "Unknown" option → elimination heuristics, not uncertainty handling
  - Confident rationales referencing non-existent visual features → hallucinated justification

- First 3 experiments:
  1. **Baseline modality probe**: Run your model on NEJM-VS equivalent (clinician-validated visual-required items) with and without images. Target: text-only accuracy should approach random baseline if model has genuine visual dependency
  2. **Format sensitivity test**: Shuffle answer choices on your benchmark. Large accuracy drops (particularly in text-only conditions) indicate positional shortcut reliance
  3. **Reasoning audit**: Sample 50-100 outputs with CoT prompting; manually verify whether stated visual features actually exist in images. Track hallucination rate

## Open Questions the Paper Calls Out

- **Question:** How can adversarial stress tests be standardized and integrated into model release audits to ensure clinical reliability beyond leaderboard accuracy?
  - **Basis in paper:** [explicit] The authors explicitly call for "stress testing as a core component of evaluation" and "model release audits" in the Discussion section
  - **Why unresolved:** Current evaluation relies on static benchmarks that optimize for narrow metrics, lacking standardized protocols for perturbation-based robustness testing across the field
  - **What evidence would resolve it:** The development of a standardized stress-test suite accepted by regulatory bodies, showing high correlation with real-world clinical failure rates

- **Question:** To what degree does memorization of pretraining data explain models' ability to answer visual-questions even when images are removed or substituted?
  - **Basis in paper:** [inferred] The authors note that residual performance in visual substitution tests may reflect "memorization of common image–question associations during pretraining," though they "cannot fully exclude overlap"
  - **Why unresolved:** It is currently difficult to disentangle true zero-shot multimodal reasoning from memorized associations without transparent model training logs
  - **What evidence would resolve it:** Performance analysis on novel, out-of-distribution medical images verified to be entirely absent from pretraining corpora

- **Question:** How can we develop automated methods to validate the visual grounding of Chain-of-Thought reasoning traces to prevent fabricated justifications?
  - **Basis in paper:** [explicit] The study concludes that "reasoning fidelity must therefore be independently validated," noting that current models "fabricate convincing yet flawed reasoning traces"
  - **Why unresolved:** Current audits rely on manual review, which is unscalable, while automated metrics often fail to detect "fluent factual errors" or hallucinations
  - **What evidence would resolve it:** A scalable evaluation metric that correlates highly with expert human assessment of visual justification accuracy

## Limitations
- Access to specific evaluation datasets (NEJM Visual-required Subset, distractor-matching images) is restricted, limiting exact reproduction
- Study focuses on multiple-choice VQA tasks, which may not generalize to open-ended clinical report generation or other medical AI modalities
- Benchmark heterogeneity analysis relies on subjective clinician assessments of reasoning complexity and visual dependency

## Confidence

- **High Confidence**: The core finding that leaderboard accuracy masks shortcut learning (Mechanism 1) is well-supported by multiple stress tests showing text-only performance well above random baseline on visual-required questions
- **Medium Confidence**: The CoT reasoning fidelity findings (Mechanism 2) have strong qualitative support from manual audits but limited quantitative metrics
- **Medium Confidence**: The benchmark heterogeneity analysis (Mechanism 3) provides useful profiling but relies on subjective clinician assessments

## Next Checks

1. **Dataset Substitution Validation**: Replicate the stress test framework using publicly available medical VQA datasets (VQA-RAD, MIMIC-CXR) to verify whether shortcut learning patterns persist when NEJM/JAMA access is restricted

2. **Reasoning Audit Protocol**: Implement the manual CoT auditing procedure on a sample of 100 model outputs from your specific deployment context, tracking hallucination rates and comparing against the paper's findings of 30-50% hallucination in explicit reasoning chains

3. **Abstention Behavior Analysis**: Systematically measure and categorize model refusal patterns across stress tests, particularly for visual-required questions without images, to determine whether high refusal rates represent appropriate uncertainty or task avoidance