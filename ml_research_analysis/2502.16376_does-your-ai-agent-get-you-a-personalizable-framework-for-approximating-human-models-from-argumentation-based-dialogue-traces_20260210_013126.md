---
ver: rpa2
title: Does Your AI Agent Get You? A Personalizable Framework for Approximating Human
  Models from Argumentation-based Dialogue Traces
arxiv_id: '2502.16376'
source_url: https://arxiv.org/abs/2502.16376
tags:
- human
- persona
- argument
- probability
- round
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Persona, a framework for approximating human
  models through argumentation-based dialogues. Persona combines Bayesian belief updates
  with a prospect theory-inspired probability weighting function to personalize interactions.
---

# Does Your AI Agent Get You? A Personalizable Framework for Approximating Human Models from Argumentation-based Dialogue Traces

## Quick Facts
- arXiv ID: 2502.16376
- Source URL: https://arxiv.org/abs/2502.16376
- Authors: Yinxu Tang; Stylianos Loukas Vasileiou; William Yeoh
- Reference count: 31
- One-line primary result: Persona achieves Spearman's rank correlation coefficients of 0.40-0.47 in human model approximation and 0.30-0.45 in argument belief estimation, significantly surpassing state-of-the-art baselines (p < 0.05)

## Executive Summary
This paper introduces Persona, a framework for approximating human mental models through argumentation-based dialogues. The approach combines Bayesian belief updates with a prospect theory-inspired probability weighting function to capture individual differences in probability perception. By learning optimal parameters for each user through dialogue traces and model rankings, Persona significantly outperforms existing methods in both human model approximation and argument belief estimation. In experiments with 184 participants, the framework successfully captured evolving human beliefs and facilitated personalized interactions, demonstrating the effectiveness of integrating personalization and probability weighting in argumentation-based AI systems.

## Method Summary
Persona approximates human mental models by maintaining a probability distribution over possible models that represent different perspectives on a topic. The framework refines this distribution through Bayesian belief updates as arguments are exchanged in dialogue, with each update weighted by a prospect theory-inspired probability weighting function. The system learns personalized parameters (s, r) for each user by maximizing Spearman's rank correlation between computed model rankings and ground truth user rankings across training rounds. The framework was evaluated using a 5-round debate scenario with 184 participants discussing event venue suitability, demonstrating significant improvements over state-of-the-art baselines.

## Key Results
- Persona achieved Spearman's rank correlation coefficients of 0.40-0.47 for human model approximation and 0.30-0.45 for argument belief estimation
- Performance significantly surpassed state-of-the-art baselines (p < 0.05) across all evaluation metrics
- The framework successfully captured evolving human beliefs, with correlation improving as more training rounds were used (D3 significantly outperformed D1 and D2)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bayesian belief updates systematically refine probability distributions over possible human models based on exchanged arguments.
- Mechanism: When an argument Ai is presented, models consistent with Ai have their probabilities increased proportionally to p(Ai), while inconsistent models are downweighted. The update rule: P(m) increases for m |= Ai and decreases for m |≠ Ai, normalized across all models.
- Core assumption: Human mental states can be represented as probability distributions over logical models M of a shared language L.
- Evidence anchors:
  - [abstract]: "refines a probability distribution over possible human models based on exchanged arguments"
  - [section]: Equation 1 shows the conditional probability update mechanism with explicit weighting by argument probability p(Ai)
  - [corpus]: No direct corpus evidence for this specific mechanism; related work on argumentation-based XAI exists but lacks dynamic model updating
- Break condition: If the shared vocabulary assumption fails (agents don't share language L), the mechanism cannot map arguments to consistent models.

### Mechanism 2
- Claim: A prospect theory-inspired probability weighting function captures individual differences in how humans evaluate probabilistic information.
- Mechanism: The function σ(Ai) = f(p(Ai), s, r) transforms objective probabilities into subjective confidence values, where parameter s controls the midpoint bias and r controls nonlinear distortion. Humans tend to overweight low probabilities and underweight high probabilities.
- Core assumption: Humans systematically deviate from rational probability assessment in predictable ways that can be parameterized.
- Evidence anchors:
  - [abstract]: "draws on prospect theory and integrates a probability weighting function"
  - [section]: Equation 2 defines the weighting function; Figure 2 visualizes how different (s, r) pairs create different distortion curves
  - [corpus]: Weak corpus support—neighboring papers don't address probability weighting in argumentation contexts
- Break condition: If individual probability perception patterns don't fit the parametric family defined by (s, r), the weighting function will misestimate true probabilities.

### Mechanism 3
- Claim: Personalized parameters (s*, r*) learned from dialogue traces and model rankings maximize correlation with ground truth user preferences.
- Mechanism: For each participant, iterate over parameter space (s ∈ {0.1,...,0.9}, r ∈ {1,...,8}) to find values that maximize Spearman's correlation ρ between computed model rankings and user-provided rankings across training rounds.
- Core assumption: More interaction data yields better parameter estimates that generalize to future rounds.
- Evidence anchors:
  - [abstract]: "learns optimal parameters for each user through dialogue traces and model rankings, outperforming existing methods"
  - [section]: Table 1 shows D3 (3 rounds of training) significantly outperforms D1 (p < 0.001) and D2 (p = 0.015) on round 4 predictions
  - [corpus]: "Neuro-Argumentative Learning with Case-Based Reasoning" mentions argumentation debate structures but not personalization
- Break condition: If user preferences shift non-stationarily or insufficient training rounds are available, learned parameters won't generalize.

## Foundational Learning

- Concept: Bayesian belief updating
  - Why needed here: Core mechanism for refining probability distributions over models; requires understanding conditional probability, normalization, and prior-to-posterior transitions.
  - Quick check question: Given prior P(m) = 0.25 for 4 models and likelihood p(A) = 0.8 for argument A entailed by 2 models, compute the posterior distribution.

- Concept: Computational argumentation (structured/deductive)
  - Why needed here: Arguments are represented as ⟨Φ, ϕ⟩ pairs where premises Φ entail claim ϕ; attack relations capture conflicts; understanding entailment and consistency (|=, |=⊥) is essential.
  - Quick check question: Define an argument and explain when two arguments attack each other in the framework used.

- Concept: Spearman's rank correlation
  - Why needed here: Primary evaluation metric for model approximation quality; measures monotonic relationship between computed and ground-truth rankings.
  - Quick check question: What does ρ = 0.47 indicate about the relationship between predicted and actual model rankings?

## Architecture Onboarding

- Component map:
  Dialogue trace parser -> Probability weighting module -> Bayesian updater -> Model distribution store -> Parameter optimizer -> Ranking generator

- Critical path: Dialogue trace → confidence extraction → probability weighting → Bayesian update → distribution → ranking → correlation computation

- Design tradeoffs:
  - Grid search (s ∈ 9 values, r ∈ 8 values = 72 combinations) vs. continuous optimization
  - Uniform priors vs. informed priors from domain knowledge
  - Per-user personalization vs. cluster-based parameter sharing for cold-start scenarios
  - Computational cost: ~0.6s per (s, r) pair per participant

- Failure signatures:
  - All model probabilities converging to near-zero (numerical underflow in sequential updates)
  - Correlation ρ near zero or negative (wrong parameter space, incompatible language L)
  - Static distributions after updates (confidence values all 0.5 → no belief shift)
  - High variance in (s*, r*) across rounds (non-stationary user behavior)

- First 3 experiments:
  1. **Sanity check**: Replicate Example 2 from paper—verify 8-model scenario produces P(m1) = P(m2) = 0.335 after first update with σ(A1) = 0.6, s = 0.5, r = 1.5
  2. **Ablation study**: Compare Persona vs. SBU (no weighting function) vs. Generic (no personalization) on held-out round data; expect Persona > Generic > SBU in [0.75, 1] correlation range
  3. **Baseline comparison**: Implement HM1/HM2 from Hunter (2015) and verify Persona achieves higher ρ with statistical significance (p < 0.05 per Table 2)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the learned human models be utilized to autonomously generate more persuasive arguments in a dialogue?
- Basis in paper: [explicit] The "Conclusions and Future Work" section explicitly states the intention to "investigate how these learned human models can be used to generate more persuasive arguments."
- Why unresolved: The current paper validates the accuracy of *approximating* human models (ranking correlation) but does not implement or evaluate a mechanism for generating arguments based on those models.
- Evidence: A study measuring persuasion success rates (e.g., belief shift) in dialogues where the AI selects arguments using Persona-learned models compared to baseline strategies.

### Open Question 2
- Question: What is the impact on performance when translating raw natural language into the formal propositional logic required by the framework?
- Basis in paper: [explicit] The "Computational Results" section notes that "additional time would be required for translating between natural language and logic, which is an area we plan to address in future work."
- Why unresolved: The current evaluation assumes a "Shared Domain Language" where arguments are already formalized, bypassing the potential errors and latency of real-time natural language understanding (NLU).
- Evidence: An end-to-end evaluation measuring the latency and model approximation accuracy when an NLU component is integrated to convert user text into the framework's formal language $L$.

### Open Question 3
- Question: Does the Persona framework generalize to high-stakes or complex domains beyond the "event venue suitability" scenario used in the evaluation?
- Basis in paper: [inferred] The empirical evaluation relied on a single, specific scenario involving a fictional venue ("Luminara Gardens") and a limited set of arguments.
- Why unresolved: It is unclear if the prospect-theory-inspired weighting parameters $(s, r)$ and the Bayesian update mechanism are robust enough for domains with technical complexity (e.g., medical or legal) or higher cognitive load.
- Evidence: A human-subject study applying Persona to diverse argumentation domains (e.g., ethical dilemmas, medical treatment options) to compare model approximation performance against the current baseline.

## Limitations

- The framework relies on accurate entailment relationships between arguments and models, which requires precise formal representation of natural language arguments
- The personalization mechanism assumes stationarity in user preferences across dialogue rounds, which may not hold for all participants
- The prospect theory-inspired weighting function assumes a specific parametric family that may not capture all individual differences in probability perception

## Confidence

- High confidence: The Bayesian updating framework and its mathematical formulation (Equation 1) are clearly specified and theoretically sound.
- Medium confidence: The grid search optimization procedure for learning (s*, r*) parameters is well-defined, but its effectiveness depends on sufficient training data and stationary user behavior.
- Low confidence: The exact implementation details of argument parsing, entailment checking, and confidence-to-probability conversion are not fully specified, requiring assumptions for reproduction.

## Next Checks

1. Verify the Bayesian update mechanism by reproducing Example 2 exactly—confirm that after the first argument update, P(m1) = P(m2) = 0.335 with the given parameters.

2. Conduct an ablation study comparing Persona against baselines (Generic, SBU, HM1, HM2) on a held-out validation round to confirm the claimed performance improvements (ρ = 0.40-0.47 vs. lower values).

3. Test the grid search optimization by varying the number of training rounds (D1, D2, D3) and verifying that correlation on round 4 improves with more training data, as reported in Table 1.