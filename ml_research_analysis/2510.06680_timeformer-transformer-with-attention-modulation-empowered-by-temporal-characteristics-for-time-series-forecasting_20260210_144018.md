---
ver: rpa2
title: 'TimeFormer: Transformer with Attention Modulation Empowered by Temporal Characteristics
  for Time Series Forecasting'
arxiv_id: '2510.06680'
source_url: https://arxiv.org/abs/2510.06680
tags:
- time
- series
- temporal
- forecasting
- mosa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'TimeFormer addresses the challenge of applying Transformer models
  to time series forecasting, where traditional Transformers struggle due to overlooking
  the unique characteristics of temporal data compared to natural language. The core
  innovation is a self-attention mechanism with two modulation terms (MoSA) that incorporates
  temporal priors: unidirectional causal dependencies enforced through causal masking,
  and decaying attention effects modeled using the Hawkes process.'
---

# TimeFormer: Transformer with Attention Modulation Empowered by Temporal Characteristics for Time Series Forecasting

## Quick Facts
- arXiv ID: 2510.06680
- Source URL: https://arxiv.org/abs/2510.06680
- Reference count: 40
- One-line primary result: Up to 7.45% MSE reduction over state-of-the-art baselines

## Executive Summary
TimeFormer introduces a novel self-attention mechanism (MoSA) that explicitly incorporates temporal characteristics into Transformer models for time series forecasting. By integrating Hawkes process-based decay and causal masking, the model better handles the unidirectional nature and decaying influence of temporal dependencies compared to standard Transformers. Multi-scale processing further enhances the model's ability to capture both local and global patterns. Extensive experiments demonstrate significant performance improvements across seven real-world datasets, establishing new state-of-the-art results on most metrics.

## Method Summary
TimeFormer modifies the Transformer's self-attention mechanism with MoSA, which applies a Hawkes process-based decay term and causal masking to standard attention scores. The model processes input time series at multiple scales via average pooling, embedding each scale into patches, and applying MoSA in two stages: intra-patch and inter-patch. The outputs from all scales are combined and projected to produce the forecast. Training uses Adam optimizer with a 0.005 learning rate for 100 epochs on a 96-step look-back window.

## Key Results
- Achieves up to 7.45% reduction in MSE compared to the best baseline method
- Sets new state-of-the-art performance on 94.04% of evaluation metrics across seven datasets
- Demonstrates that MoSA can be broadly applied to enhance other Transformer-based models

## Why This Works (Mechanism)
TimeFormer addresses the fundamental mismatch between standard Transformers and time series data by explicitly modeling temporal dependencies. The MoSA mechanism introduces two critical modulations: a Hawkes process-based decay that exponentially reduces the influence of older events over time, and causal masking that enforces unidirectional attention flow consistent with forecasting constraints. This combination allows the model to prioritize recent, relevant information while preventing data leakage from future to past, resulting in more accurate and causally consistent predictions.

## Foundational Learning
- **Self-Attention in Transformers**
  - Why needed here: MoSA is built upon standard self-attention; understanding QKV operations is essential
  - Quick check question: In the standard scaled dot-product attention formula $ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $, what does the matrix product $ QK^T $ represent?

- **Hawkes Process**
  - Why needed here: Provides the mathematical foundation for the attention decay mechanism that models temporal influence decay
  - Quick check question: In the Hawkes process intensity function $ \lambda(t) = \mu + \sum_{t_i < t} \alpha e^{-\beta(t - t_i)} $, which term controls how quickly the influence of a past event $ t_i $ fades away?

- **Causal Masking**
  - Why needed here: Enforces the unidirectional nature of time for forecasting, preventing future information leakage
  - Quick check question: If you have a sequence of 5 tokens, what would the 3rd row of a causal attention mask look like before it's applied to the attention scores?

## Architecture Onboarding
- Component map: Raw time series -> Multi-Scale Sampling (AvgPool) -> Per-Scale Processing (TimeFormer modules) -> Projection (Linear layer) -> Forecast
- Critical path: A single time step flows through AvgPool, Embedding, and two-stage MoSA (intra-patch then inter-patch) within each scale
- Design tradeoffs:
  - Number of scales (S): More scales capture global patterns but increase computational cost and potential noise
  - Patching (P_s=K_s): Design choice to balance token count; may need re-tuning for different sequence lengths
- Failure signatures:
  - Data Leakage: Incorrect or omitted causal mask causes unrealistically good scores but fails in production
  - Decay Rate Mismatch: Too high causes loss of long-term context; too low fails to dampen distant noise
- First 3 experiments:
  1. Baseline Reproduction: Replace MoSA with standard self-attention on ETTh1 to isolate MoSA contribution
  2. Scale Ablation: Vary S from 1 to 5 on Weather dataset to find optimal scale count and observe MSE/MAE impact
  3. MoSA Transfer: Apply MoSA to vanilla Transformer on small forecasting task to test general applicability claim

## Open Questions the Paper Calls Out
- Can MoSA be adapted for Transformer architectures that treat variables rather than time steps as tokens?
- Is the assumption of zero background intensity and unit impact factor in the Hawkes process optimal for all domains?
- How can the decay rate γ and scale number S be determined automatically for a given dataset?

## Limitations
- Core architectural hyperparameters (hidden size, layers, heads) are unspecified, requiring manual tuning
- Training regime details are incomplete: batch size, learning rate scheduling, and patch size derivation are not provided
- Time delta handling in Hawkes decay is ambiguous between raw indices and actual timestamps

## Confidence
- **High confidence** in the novelty and validity of the MoSA mechanism itself
- **Medium confidence** in claimed performance gains due to incomplete implementation details
- **Low confidence** in claimed generality of MoSA across arbitrary Transformer variants

## Next Checks
1. Implement MoSA as drop-in replacement for standard attention in vanilla Transformer on ETTh1 benchmark
2. Run ablation study on number of scales (S) and patch size (K_s) on Weather dataset
3. Validate Hawkes decay parameterization by testing multiple decay rates (γ) and comparing attention weight distributions