---
ver: rpa2
title: 'HomeSafeBench: A Benchmark for Embodied Vision-Language Models in Free-Exploration
  Home Safety Inspection'
arxiv_id: '2509.23690'
source_url: https://arxiv.org/abs/2509.23690
tags:
- safety
- embodied
- inspection
- arxiv
- hazard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HomeSafeBench, a new benchmark for evaluating
  Vision-Language Models (VLMs) in free-exploration home safety inspection tasks.
  Existing benchmarks oversimplify safety inspection by using textual descriptions
  instead of direct visual information and rely on static viewpoints, which can cause
  safety hazards to be overlooked.
---

# HomeSafeBench: A Benchmark for Embodied Vision-Language Models in Free-Exploration Home Safety Inspection

## Quick Facts
- **arXiv ID**: 2509.23690
- **Source URL**: https://arxiv.org/abs/2509.23690
- **Reference count**: 18
- **Primary result**: Current VLMs achieve only 10.23% F1-score on embodied home safety inspection

## Executive Summary
HomeSafeBench introduces a new benchmark for evaluating Vision-Language Models (VLMs) in free-exploration home safety inspection tasks. Existing benchmarks oversimplify safety inspection by relying on static viewpoints and textual descriptions rather than direct visual information. This benchmark addresses these limitations by providing 12,900 data points across five common home safety hazard categories, enabling dynamic first-person perspective images from simulated home environments where embodied agents can freely explore rooms for thorough inspection. The comprehensive evaluation reveals that even the best-performing model achieves an F1-score of only 10.23%, demonstrating significant limitations in current VLMs' ability to identify safety hazards and select effective exploration strategies.

## Method Summary
HomeSafeBench is a benchmark for embodied home safety inspection where VLM agents navigate through simulated home environments (VirtualHome) using first-person perspective images. Agents must identify safety hazards across five categories (fire, electric shock, falling, trip, child safety) through free exploration over 10 turns. The evaluation uses micro-averaged Precision, Recall, and F1-score metrics based on union-based accumulation of hazards identified across all time steps. The benchmark includes 12,900 samples with 1,580 available for quick tests, and employs a synonym mapping table for flexible object name matching during evaluation.

## Key Results
- Best-performing VLM achieves only 10.23% F1-score on the benchmark
- Free exploration is necessary but insufficient for thorough inspection
- Current VLM agents struggle with effective navigation, especially in complex environments and over longer interaction sequences
- Precision significantly decreases as turn number increases, indicating exploration drift and hallucination

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Dynamic, egocentric exploration is required to mitigate visual occlusion and resolution limits inherent in static viewpoints.
- **Mechanism:** The benchmark enforces a first-person perspective where the agent must physically navigate to bring partially visible or distant hazards into clear view. Unlike static image classification, the agent must sequence actions to maximize coverage of the ground-truth hazard set.
- **Core assumption:** Hazards are distributed such that no single viewpoint captures all risks; agents must actively manipulate the camera position to disambiguate objects.
- **Evidence anchors:** Abstract mentions "free exploration... provides multiple dynamic perspectives"; Table 4 shows significant drop in Recall and F1 when removing free exploration.
- **Break condition:** If the environment design changes to use "oracle" views or maps instead of egocentric vision, this navigation-dependent mechanism ceases to drive performance.

### Mechanism 2
- **Claim:** Safety inspection relies on "Union-based Accumulation" rather than single-shot classification.
- **Mechanism:** The evaluation logic defines the final hazard set as the union of hazards identified at every time step. This mechanism rewards memory and persistent scanning; missing a hazard in frame t=1 but finding it in t=5 preserves correctness.
- **Core assumption:** The model maintains sufficient temporal context or state to avoid re-reporting known hazards while scanning for new ones.
- **Evidence anchors:** Section 3.1 defines union-based accumulation; Section 5.3 analysis shows recall slowly climbs while precision often drops, indicating accumulation struggles with noise over time.
- **Break condition:** If the metric were changed to "last-turn-only" accuracy, the incentive for exploration and memory retention would collapse.

### Mechanism 3
- **Claim:** Performance degrades over long horizons due to a lack of coherent exploration planning ("Exploration Drift").
- **Mechanism:** VLMs select actions autoregressively. Without an external world model or map, the agent tends to generate repetitive action sequences or "hallucinate" hazards in later turns, causing precision to plummet and recall gains to saturate.
- **Core assumption:** The VLM acts as the sole policy, lacking an external memory module to record "already visited" locations.
- **Evidence anchors:** Section 5.3 shows F1 scores do not increase monotonically; "Precision score significantly decreases when turn number grows," and models exhibit "arbitrary" exploration.
- **Break condition:** If a global planner or topological map is added to the architecture to constrain the VLM's action selection, this specific drift mechanism would likely be mitigated.

## Foundational Learning

- **Concept:** **Embodied Question Answering (EQA)**
  - **Why needed here:** This task is structurally an EQA problem where the question is implicit ("Is this room safe?") and the answer requires navigation to gather visual evidence.
  - **Quick check question:** Can you explain why a static VLM (like GPT-4V) fails here not because of poor vision, but because it lacks the *navigation policy* to change its viewpoint?

- **Concept:** **Sim-to-Real Gap (VirtualHome)**
  - **Why needed here:** The benchmark uses a simulated environment (VirtualHome). Understanding the "simulation gap"—why results here may not transfer to a physical robot due to rendering differences or physics—is critical for interpreting the low F1 scores.
  - **Quick check question:** How might the perfect lighting and geometry of VirtualHome actually make the task *easier* than real-world inspection, implying that real-world performance could be even lower?

- **Concept:** **Procedural Hazard Generation**
  - **Why needed here:** The dataset isn't hand-curated photo-by-photo but generated via rules. This allows scale (12,900 samples) but relies on the assumption that "Flammable object + Heat source = Fire Hazard" is visually deterministic.
  - **Quick check question:** If the VLM identifies a "candle" correctly but fails to infer the rule "candles are heat sources," which component of the system is failing—perception or reasoning?

## Architecture Onboarding

- **Component map:** Environment (VirtualHome simulation engine) -> Agent Policy (VLM) -> Prompt Interface (structured prompt) -> Aggregator (union logic)
- **Critical path:**
  1. Initialization: Agent spawns in room
  2. Observation: Env renders first-person image
  3. Inference: VLM receives image + history, outputs Hazard List and Next Action
  4. Execution: Env updates agent state based on Action
  5. Evaluation: Compare accumulated Hazards against Ground Truth
- **Design tradeoffs:**
  - Fixed vs. Free Viewpoint: Free Exploration is necessary for Recall but introduces noise that hurts Precision
  - Sampling vs. Greedy Decoding: Temperature 0.6 used to avoid "endless repetition" of actions
- **Failure signatures:**
  - "Action Loops": Agent repeating "Turn Left" indefinitely
  - Passive Reporting: Agent identifying 0 hazards and outputting "None" to avoid false positives
  - Late-Stage Hallucination: Agent identifying correct hazards early, then inventing non-existent hazards in turns 20-30
- **First 3 experiments:**
  1. Navigation Ceiling Analysis: Measure "Navigation Score" vs. F1 to isolate exploration failures from vision failures
  2. Turn-Sensitivity Test: Run evaluation with max_turns=10 vs. max_turns=30 to verify diminishing returns hypothesis
  3. Prompt Ablation: Test if providing "dangerous object attributes" in prompt improves VLM's ability to map objects to hazard types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can embodied VLMs improve their exploration efficiency to achieve higher Navigation scores in complex environments?
- Basis in paper: [explicit] Section 5.2 notes all models perform badly on navigation and struggle with conducting effective navigation in complex environments
- Why unresolved: The paper quantifies the deficiency (Nav scores < 50%) but does not propose or test methods to optimize path planning or exploration strategies
- What evidence would resolve it: An experiment demonstrating a VLM agent achieving significantly higher Navigation score (>80%) through improved planning modules or spatial reasoning

### Open Question 2
- Question: What mechanisms can prevent the degradation of inspection precision over long-horizon multi-turn interactions?
- Basis in paper: [explicit] Section 5.3 and Figure 3 show Precision scores "significantly decreases when turn number grows"
- Why unresolved: The analysis identifies the trend but does not determine if the cause is context window saturation, lack of memory, or error accumulation
- What evidence would resolve it: A study testing memory-augmented VLMs or state-maintenance mechanisms that show stable or increasing Precision scores beyond 10 interaction turns

### Open Question 3
- Question: To what extent do the visual limitations observed in the VirtualHome simulation transfer to real-world home safety inspections?
- Basis in paper: [inferred] The benchmark is constructed entirely within VirtualHome simulation, yet the introduction highlights the need for "real-world applications"
- Why unresolved: Simulators often lack the visual noise, lighting variability, and texture complexity of physical homes
- What evidence would resolve it: A "Sim-to-Real" evaluation comparing VLM performance on HomeSafeBench versus performance in a physical mock-up room with identical hazard configurations

## Limitations
- The benchmark relies entirely on simulated environments, creating potential sim-to-real gaps that limit real-world applicability
- The constrained action space (only forward movement, 90° turns, and looking up) may artificially limit exploration effectiveness
- Rule-based procedural hazard generation may not reflect the complexity and randomness of actual household environments
- Perfect ground-truth hazard labels from VirtualHome's internal state may not capture real-world ambiguity

## Confidence
- **High Confidence**: The demonstration that current VLMs perform poorly on this task (F1 ~10%) is directly measurable from benchmark results
- **Medium Confidence**: The claim that free exploration is necessary for thorough inspection is supported by ablation studies but performance drop could be partially attributed to VLM limitations
- **Low Confidence**: The generalizability of these results to real-world home safety inspection due to potential sim-to-real gaps

## Next Checks
1. **Navigation vs. Perception Isolation**: Run controlled experiments measuring Navigation Score independently from VLM's identification accuracy to quantify whether poor performance stems from inadequate exploration or poor hazard recognition.

2. **Turn Sensitivity Analysis**: Evaluate performance across different maximum turn limits (5, 10, 15, 20, 30 turns) to identify the point of diminishing returns where additional exploration time no longer improves F1 scores or where precision degradation outweighs recall gains.

3. **Real-World Transfer Validation**: Test the same VLM architectures on a small set of real household images or videos with known hazards to assess the sim-to-real gap and determine whether the benchmark's findings translate to practical deployment scenarios.