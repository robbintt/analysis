---
ver: rpa2
title: <think> So let's replace this phrase with insult... </think> Lessons learned
  from generation of toxic texts with LLMs
arxiv_id: '2509.08358'
source_url: https://arxiv.org/abs/2509.08358
tags:
- data
- toxic
- text
- llms
- detoxification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether LLM-generated synthetic toxic data
  can replace human-generated data for training text detoxification models. Using
  Llama 3 and Qwen3 models, the authors generated synthetic toxic counterparts for
  neutral texts from ParaDetox and SST-2 datasets, then trained BART models on this
  data and evaluated them against human-annotated baselines.
---

# <tool_call> So let's replace this phrase with insult... </tool_call> Lessons learned from generation of toxic texts with LLMs

## Quick Facts
- **arXiv ID:** 2509.08358
- **Source URL:** https://arxiv.org/abs/2509.08358
- **Reference count:** 11
- **Key outcome:** LLM-generated synthetic toxic data underperforms human-annotated data for training detoxification models due to lexical diversity gaps, with up to 30% performance drops in joint metrics.

## Executive Summary
This paper investigates whether synthetic toxic data generated by LLMs can replace human annotations for training text detoxification models. Using activation-patched Llama 3 and Qwen3 models, the authors generated toxic paraphrases from neutral and negative sentiment texts, then trained BART models on this synthetic data. The results show consistent underperformance compared to human-data baselines, with the primary cause identified as a lexical diversity gap where LLMs overuse repetitive insults rather than capturing the nuanced variety found in human toxicity. Human evaluation confirmed the synthetic models' outputs were less preferred, demonstrating that current LLMs cannot fully replace human annotators for generating high-quality toxic training data.

## Method Summary
The study used activation-patched LLMs (Llama 3 8B/72B, Qwen3 8B/32B, Cogito v1 8B) with min-p=0.1 sampling to generate toxic paraphrases from neutral sentences in ParaDetox and negative reviews from SST-2. A few-shot toxification prompt requested toxic rewrites with profanity. BART-large models were fine-tuned on synthetic toxic→human neutral pairs and evaluated on human-annotated ParaDetox test sets using STA, SIM, FL, and Joint metrics. GPT-4.1 conducted side-by-side comparisons against human-data baselines. The approach tested whether synthetic data could match human-annotated data quality for detoxification tasks.

## Key Results
- Models trained on synthetic toxic data consistently underperformed human-data baselines by 20-30% on joint metrics
- The lexical diversity gap caused repetitive vocabulary use, with single terms appearing 15,000+ times in synthetic datasets versus balanced distributions in human data
- Human evaluation showed synthetic models won only 51-62% of comparisons against human-data baselines
- Test failures remained high (36-49 insults vs 34 for human baseline) indicating poor generalization to real-world toxicity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM-generated toxic data exhibits a lexical diversity gap that causes downstream detoxification models to underperform on real-world text.
- **Mechanism:** LLMs converge on a narrow, repetitive vocabulary of insults rather than the balanced distribution found in human data, causing models to overfit to a small set of expressions.
- **Core assumption:** Detoxification performance correlates directly with training data lexical diversity, and the test set represents real-world toxicity distribution.
- **Evidence anchors:** Human data contained 390 unique insults versus 342-386 in synthetic datasets; single terms appeared 15,413 times in synthetic data.
- **Break condition:** If LLMs can be prompted to produce more lexically diverse toxic outputs, synthetic data viability would improve.

### Mechanism 2
- **Claim:** Layering toxicity onto already-negative text causes greater semantic distortion than generating toxicity from neutral text.
- **Mechanism:** Adding toxicity to negative sentiment compounds meaning changes, causing sharp SIM score drops for SST-2-derived models versus ParaDetox-derived models.
- **Core assumption:** SIM score accurately captures semantic preservation between toxic source and detoxified output.
- **Evidence anchors:** SIM scores dropped to 0.481-0.559 for SST-2 sources versus 0.619-0.645 for ParaDetox sources.
- **Break condition:** If toxification prompts constrained meaning preservation more strictly, semantic distortion would decrease.

### Mechanism 3
- **Claim:** Activation-patched LLMs can generate toxic content but produce stereotypical patterns that limit training utility.
- **Mechanism:** Safety mechanisms bypassed via activation patching enable toxic generation, but underlying model distributions favor high-probability toxic terms, resulting in repetitive outputs.
- **Core assumption:** Activation patching removes refusals without altering token distribution biases that cause repetition.
- **Evidence anchors:** Synthetic datasets dominated by single terms appearing >10,000 times despite using min-p sampling.
- **Break condition:** If sampling strategies were combined with diversity-promoting techniques, output variety could increase.

## Foundational Learning

- **Concept: Text Style Transfer (TST)**
  - **Why needed here:** Detoxification is a TST task—rewriting text from "toxic" style to "neutral" while preserving meaning. Understanding style-content separation is essential for interpreting STA/SIM/FL metrics.
  - **Quick check question:** Can you explain why maximizing style transfer accuracy (STA) alone might hurt meaning preservation (SIM)?

- **Concept: Activation Patching / Refusal Direction**
  - **Why needed here:** The paper uses activation patching to bypass LLM safety. Understanding this mechanism is critical for replicating the generation pipeline and assessing ethical implications.
  - **Quick check question:** What is the "refusal direction" in an LLM's residual stream, and how does patching it change model behavior?

- **Concept: Lexical Diversity Metrics**
  - **Why needed here:** The core finding hinges on measuring vocabulary diversity. You need to understand type-token ratios and frequency distributions to diagnose the diversity gap.
  - **Quick check question:** Given a corpus where one term appears 15,000 times and 300 other terms appear 10 times each, how would you quantify its diversity imbalance?

## Architecture Onboarding

- **Component map:** Source datasets (ParaDetox neutral, SST-2 negative) → Activation-patched LLMs (Llama 3, Qwen3, Cogito) → Toxification prompt with min-p=0.1 → Synthetic toxic data → BART-large fine-tuning → STA/SIM/FL/J evaluation → GPT-4.1 human comparison

- **Critical path:** 1) Extract neutral/negative texts from source datasets 2) Generate toxic counterparts using activation-patched LLMs with toxification prompt 3) Fine-tune BART-large on synthetic data 4) Evaluate using STA/SIM/FL/J metrics 5) Run GPT-4.1 side-by-side comparison

- **Design tradeoffs:** Model size vs. diversity (larger models don't consistently outperform on diversity); source data choice (ParaDetox yields better SIM than SST-2); sampling parameters (min-p=0.1 increases variety but doesn't overcome distributional biases)

- **Failure signatures:** Low SIM + high STA (meaning changed while style changed); high test failure count (many unique insults remaining); skewed insult frequency in training data (single term >10,000 times)

- **First 3 experiments:**
  1. **Diversity audit:** Count unique insults in synthetic training data. If top insult frequency >5x human baseline, expect J metric degradation.
  2. **Ablation on source data:** Compare ParaDetox-neutral vs. SST-2-negative sources. Expect 10-15% J metric drop for SST-2.
  3. **Human evaluation spot-check:** Run 50 samples through GPT-4 side-by-side comparison early. If synthetic model win rate <40%, diversity interventions are needed.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can prompting strategies or sampling techniques be developed to enhance the lexical diversity of LLM-generated toxic text to match human-annotated data? The paper concludes future research should focus on methods to enhance stylistic complexity of LLM-generated text.
- **Open Question 2:** Is the lexical diversity gap specific to toxic content generation, or does it generalize to other style transfer tasks requiring nuanced language? The paper focuses exclusively on detoxification but frames within broader Text Style Transfer context.
- **Open Question 3:** Could hybrid approaches combining synthetic data with smaller amounts of human annotation achieve competitive performance while reducing annotation costs? The paper only tests 100% synthetic versus 100% human, leaving cost-performance tradeoff unexplored.

## Limitations
- The lexical diversity gap's causal relationship to performance degradation is correlational rather than definitively proven
- Evaluation methodology relies on existing toxicity detection frameworks without validating metric completeness
- Human evaluation via GPT-4.1 represents single rater system rather than true human judgment diversity

## Confidence
- **High confidence:** Empirical finding that synthetic data underperforms human-data baselines by 20-30% on joint metrics
- **Medium confidence:** Attribution of performance gaps specifically to lexical diversity rather than other factors
- **Medium confidence:** Claim that larger models don't solve the diversity problem (limited model comparison)

## Next Checks
1. **Diversity intervention experiment:** Apply prompt engineering techniques designed to increase lexical variety and measure whether J metric improvements follow.
2. **Contextual appropriateness audit:** Sample 100 detoxified outputs and have human raters evaluate naturalness and contextual appropriateness beyond style transfer success.
3. **Cross-dataset generalization test:** Evaluate models on additional toxicity benchmarks to determine whether synthetic-data performance gap persists across different toxicity distributions.