---
ver: rpa2
title: 'KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning
  with Reasoning'
arxiv_id: '2602.00400'
source_url: https://arxiv.org/abs/2602.00400
tags:
- reasoning
- learning
- kepo
- distillation
- on-policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of inducing explicit reasoning
  behaviors in vision-language models through reinforcement learning, particularly
  under sparse trajectory-level rewards. The core difficulty lies in two failure modes:
  ambiguous credit assignment due to sparse rewards and exploration collapse when
  reward-bearing trajectories are rarely encountered.'
---

# KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning

## Quick Facts
- **arXiv ID:** 2602.00400
- **Source URL:** https://arxiv.org/abs/2602.00400
- **Reference count:** 29
- **Key outcome:** KEPO significantly improves training stability and out-of-distribution generalization in multimodal reasoning under sparse rewards, achieving 78.00% average OOD accuracy versus 74.33% for GRPO and 73.67% for MM-GKD in a single-source training setting.

## Executive Summary
This paper tackles the challenge of inducing explicit reasoning behaviors in vision-language models through reinforcement learning, particularly under sparse trajectory-level rewards. The core difficulty lies in two failure modes: ambiguous credit assignment due to sparse rewards and exploration collapse when reward-bearing trajectories are rarely encountered. The authors propose Knowledge-Enhanced Preference Optimization (KEPO), a unified post-training framework that combines quality-gated on-policy distillation with knowledge-enhanced exploration. KEPO selectively applies dense teacher guidance only to high-quality trajectories (those with positive rewards) to avoid noisy gradients from flawed reasoning contexts, and adaptively injects teacher-guided trajectories via rejection sampling when naive exploration fails. Evaluated on a challenging medical VQA benchmark with single-source training (MRI-only) and testing across seven other modalities, KEPO significantly improves training stability, accelerates coherent reasoning emergence, and achieves superior out-of-distribution generalization compared to standard reinforcement learning and on-policy distillation baselines. For example, KEPO achieves an average accuracy of 78.00% across OOD modalities, outperforming GRPO (74.33%) and MM-GKD (73.67%) under the same low-resource setting.

## Method Summary
KEPO is a post-training reinforcement learning framework that addresses sparse reward credit assignment and exploration collapse in vision-language reasoning models. It builds on GRPO (Group Relative Policy Optimization) and extends on-policy knowledge distillation with two key innovations: quality-gated distillation that applies teacher supervision only to reward-positive trajectories, and knowledge-enhanced exploration that uses teacher hints to generate reward-bearing trajectories when naive exploration fails. The method uses a unified objective combining advantage-weighted policy updates, gated distillation loss, and KL regularization. Training is performed on OmniMedVQA with single-source (MRI) training and multi-modality OOD evaluation.

## Key Results
- KEPO achieves 78.00% average OOD accuracy across 7 modalities versus 74.33% for GRPO and 73.67% for MM-GKD
- KEPO shows improved training stability with reduced late-stage degradation compared to baselines
- KEPO demonstrates superior data efficiency, achieving strong performance with only 600 training samples (1 epoch)

## Why This Works (Mechanism)

### Mechanism 1: Quality-Gated On-Policy Distillation
- Claim: Selectively applying teacher supervision only to reward-aligned trajectories improves credit assignment and prevents gradient noise.
- Mechanism: The KEPO objective gates distillation via an indicator function I_{ri≥τ}, such that dense token-level teacher guidance is applied only when trajectory reward exceeds threshold τ. This prevents the student from learning teacher distributions conditioned on flawed reasoning contexts (early logical errors or hallucinations).
- Core assumption: Low-quality trajectories in reasoning-intensive tasks originate from early logical errors; distilling conditioned on these contexts injects misaligned gradients that harm optimization.
- Evidence anchors:
  - [abstract] "quality-gated on-policy distillation objective that selectively applies dense teacher guidance only to high-quality trajectories"
  - [Section 3.2.1] "distillation is activated only for trajectories satisfying ri > τ through the indicator function Iri>τ"
  - [Section 4.4] "uniformly distilling all student-generated trajectories is ill-suited for reasoning-intensive settings... distilling teacher outputs conditioned on flawed intermediate contexts introduces noisy and misaligned gradients"

### Mechanism 2: Knowledge-Enhanced Exploration via Hint-Aware Rejection Sampling
- Claim: Injecting teacher-guided trajectories when naive exploration fails enables escape from the "learning cliff" under sparse rewards.
- Mechanism: When all G sampled trajectories receive zero reward (max_i r(yi) = 0), the system triggers a fallback: (1) teacher generates reasoning hint conditioned on input and ground-truth answer; (2) student samples hint-aware responses with ground-truth provided until a reward-positive trajectory is found via rejection sampling; (3) accepted trajectories are injected into the rollout buffer.
- Core assumption: Early-stage policies under sparse rewards rarely discover reward-bearing trajectories through random exploration; providing hints and ground-truth answers enables faster trajectory discovery while preserving on-policy learning.
- Evidence anchors:
  - [abstract] "knowledge-enhanced exploration strategy that leverages hints learned from a teacher model to rejectively sample reward-positive on-policy trajectories"
  - [Section 3.2.2] "knowledge-enhanced rollout mechanism that injects teacher-guided trajectories into the on-policy rollout process when naive exploration fails"
  - [Section 4.5] "KEPO consistently achieves higher OOD accuracy and exhibits reduced late-stage degradation, showing that knowledge-enhanced exploration further improves robustness under distribution shift"

### Mechanism 3: Unified RL-Distillation Objective with KL Regularization
- Claim: Jointly optimizing advantage-weighted policy updates with gated distillation and KL regularization stabilizes reasoning emergence.
- Mechanism: The J_KEPO objective integrates three terms: (1) advantage-weighted preference optimization using group-relative advantages; (2) quality-gated distillation loss D(π_T || π_θ); (3) KL divergence from reference policy. This transforms distillation from competing objective into reward-aligned auxiliary signal.
- Core assumption: Distillation signal should reinforce rather than compete with reward-driven optimization; KL constraint prevents excessive policy deviation.
- Evidence anchors:
  - [Section 3.2.1] Equation 7 shows the full objective combining advantage term, distillation term, and KL regularization
  - [Section 3.2.1] "distillation acts as a form of dense credit assignment that amplifies successful behaviors rather than competing with the global reward"

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: KEPO builds on GRPO's group-wise advantage estimation and clipped importance weights as its RL backbone. Understanding Monte Carlo rollout sampling and group-mean baselines is prerequisite.
  - Quick check question: Can you explain how GRPO estimates advantages using group-mean baselines vs. leave-one-out estimation?

- Concept: **On-Policy Distillation (GKD)**
  - Why needed here: KEPO extends on-policy distillation by adding quality gating. Standard on-policy distillation provides dense supervision on all student-sampled trajectories; understanding this baseline clarifies what KEPO modifies.
  - Quick check question: What is the key difference between supervised knowledge distillation and on-policy distillation in terms of exposure bias?

- Concept: **Sparse Reward Credit Assignment**
  - Why needed here: The core problem KEPO addresses is ambiguous credit assignment when only trajectory-level (outcome) rewards are available. Understanding why sparse rewards cause high-variance gradients and exploration failure is essential.
  - Quick check question: Why does a single local error in a long reasoning chain invalidate credit assignment for correct intermediate steps under sparse trajectory-level rewards?

## Architecture Onboarding

- Component map: Rollout Buffer -> Quality Gate -> Hint Generator -> Rejection Sampler -> Loss Combiner
- Critical path:
  1. Sample G trajectories from student policy
  2. Evaluate rewards; if all zero, trigger hint-based exploration
  3. Apply quality gate to determine distillation subset
  4. Compute unified J_KEPO loss and backpropagate
- Design tradeoffs:
  - **Threshold τ**: Lower values increase distillation signal but risk noisy gradients; higher values improve stability but reduce supervision coverage. Paper tests τ∈{0,1}.
  - **Rejection budget B**: Larger budgets improve exploration but increase compute cost per batch.
  - **Group size G**: Larger groups improve advantage estimation but increase memory and sampling cost.
- Failure signatures:
  - All-zero rewards persisting across batches despite exploration triggers → hint generation may be ineffective or reward signal too strict
  - Early training instability with high variance → τ too low, allowing noisy trajectories into distillation
  - Slow convergence on OOD modalities → exploration mechanism not triggered frequently enough; consider adjusting trigger condition
- First 3 experiments:
  1. **Baseline comparison**: Run GRPO (thinking mode) vs. KEPO-KE (no exploration) vs. full KEPO on same 600-sample MRI training set with τ=0 and τ=1. Compare ID vs. OOD accuracy curves.
  2. **Ablation on quality threshold**: Train KEPO with τ∈{0, 0.5, 1} and plot training dynamics. Verify that stricter gating produces smoother optimization but potentially lower early peaks.
  3. **Exploration trigger frequency analysis**: Log how often hint-based exploration is triggered per epoch. If trigger rate is high (>50% of batches), the initial policy may need stronger warmstart or reward shaping.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the quality threshold τ require task-specific tuning, or can it be adaptively adjusted during training based on reward signal density?
- Basis in paper: [inferred] The ablation study (Figure 2) shows a trade-off between early performance gains and long-term stability across τ ∈ {0, 1}, but optimal threshold selection remains manual.
- Why unresolved: The paper evaluates fixed thresholds but does not propose or validate an adaptive thresholding mechanism that could generalize across tasks with varying reward sparsity.
- What evidence would resolve it: Experiments comparing fixed τ values against adaptive scheduling (e.g., decay based on rolling reward statistics) across multiple reasoning benchmarks with different reward densities.

### Open Question 2
- Question: Can KEPO maintain its advantages when the teacher model cannot access ground-truth answers during hint generation?
- Basis in paper: [explicit] Section 3.2.2 states: "The ground-truth answer is introduced at this stage to facilitate a faster hint-aware rejection sampling process under sparse rewards."
- Why unresolved: The current design assumes oracle access to correct answers for generating hints, which limits applicability in real-world scenarios where ground-truth labels are unavailable at inference time.
- What evidence would resolve it: Ablation experiments where hints are generated without ground-truth conditioning, comparing convergence speed and final accuracy against the oracle-augmented baseline.

### Open Question 3
- Question: How does the performance gap between KEPO and standard RL baselines scale with model capacity?
- Basis in paper: [inferred] All experiments use Qwen3-VL-2B; Table 1 includes larger models only for reference on general capability, not for post-training comparisons.
- Why unresolved: It is unclear whether KEPO's benefits primarily address limitations of smaller models (e.g., weaker exploration priors) or would persist as models scale and acquire stronger inherent reasoning capabilities.
- What evidence would resolve it: Replicating the KEPO vs. GRPO comparison on 8B and 32B variants under identical training protocols to measure scaling behavior of the performance gap.

### Open Question 4
- Question: Does KEPO transfer effectively to non-medical multimodal reasoning domains with different reasoning structures?
- Basis in paper: [inferred] The paper evaluates exclusively on medical VQA (OmniMedVQA) with MRI-to-7-modalities transfer, leaving generalizability to other domains untested.
- Why unresolved: Medical reasoning has domain-specific characteristics (structured anatomy knowledge, modality-specific visual features); whether KEPO's mechanisms address general sparse-reward RL problems or are domain-specialized remains unclear.
- What evidence would resolve it: Evaluation on non-medical multimodal reasoning benchmarks (e.g., science diagrams, chart reasoning) with analogous single-source training protocols.

## Limitations

- The effectiveness of KEPO's exploration mechanism depends critically on the rejection sampling budget B and KL regularization coefficient β, which are not specified in the paper.
- The evaluation setting (600 samples, 1 epoch) represents an extremely low-resource regime that may not reflect typical training scenarios.
- The quality gate threshold τ is tested only at {0,1} values, leaving the sensitivity to intermediate thresholds unexplored.

## Confidence

- **High confidence**: The core architectural design combining quality-gated distillation with knowledge-enhanced exploration is technically sound and addresses well-documented failure modes in sparse reward RL.
- **Medium confidence**: The experimental results showing superior OOD generalization are compelling but may be partly attributable to the extreme low-resource setting rather than KEPO's unique mechanisms.
- **Low confidence**: The exact contribution of each component (gating vs. exploration) cannot be precisely isolated due to limited ablation studies and unspecified hyperparameters.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Run KEPO with varying rejection budgets B∈{5,10,20} and KL coefficients β∈{0.1,0.5,1.0} to determine the stability range of the method.
2. **Scaling study**: Evaluate KEPO on the full OmniMedVQA training set (not just 600 samples) with standard epoch counts to assess performance in typical resource settings.
3. **Component attribution**: Create ablations that test (a) gated distillation without exploration, (b) exploration without gating, and (c) the combined method to quantify each mechanism's contribution to the final performance.