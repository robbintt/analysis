---
ver: rpa2
title: Variance-Dependent Regret Lower Bounds for Contextual Bandits
arxiv_id: '2503.12020'
source_url: https://arxiv.org/abs/2503.12020
tags:
- variance
- lower
- regret
- bound
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes the first variance-dependent regret lower\
  \ bounds for linear contextual bandits with general variance sequences. The authors\
  \ prove that any algorithm must incur regret of at least \u03A9(d\u221A(\u03A3k\
  \ \u03C3\xB2k/ log K)) for prefixed variance sequences and \u03A9(d\u221A(\u03A3\
  k \u03C3\xB2k/ log\u2076(dK))) for adaptive sequences with weak adversaries."
---

# Variance-Dependent Regret Lower Bounds for Contextual Bandits

## Quick Facts
- arXiv ID: 2503.12020
- Source URL: https://arxiv.org/abs/2503.12020
- Reference count: 4
- Key outcome: Establishes variance-dependent regret lower bounds of Ω(d√(Σₖσₖ²)/log K) for prefixed variance sequences and Ω(d√(Σₖσₖ²)/log⁶(dK)) for adaptive sequences, matching existing upper bounds up to logarithmic factors.

## Executive Summary
This paper establishes the first variance-dependent regret lower bounds for linear contextual bandits with general variance sequences. The authors prove that any algorithm must incur regret at least Ω(d√(Σₖσₖ²)/log K) for prefixed variance sequences and Ω(d√(Σₖσₖ²)/log⁶(dK)) for adaptive sequences with weak adversaries. These bounds match existing upper bounds up to logarithmic factors, demonstrating their optimality. The key technical contribution is a novel peeling technique that groups rounds by variance magnitude and constructs separate hard instances for each group. For adaptive sequences, the authors handle unknown group sizes by maintaining multiple instances for each variance group and visiting them cyclically.

## Method Summary
The paper establishes variance-dependent lower bounds through a peeling technique that partitions K rounds into L groups based on variance magnitude. For each group, orthogonal decision sets are constructed so actions in group Kᵢ only interact with their corresponding parameter subspace μᵢ ∈ ℝ^{d/L}. For adaptive sequences, the authors create multiple instances per group with cyclic assignment to handle unknown group sizes. To convert expected lower bounds to high-probability bounds, they introduce auxiliary algorithms that detect when regret exceeds certain thresholds and switch to standard OFUL. The proofs use careful construction of orthogonal decision sets across different variance groups and multiple independent instances to boost success probability from constant to high probability.

## Key Results
- Proved Ω(d√(Σₖσₖ²)/log K) regret lower bound for any algorithm against prefixed variance sequences
- Established Ω(d√(Σₖσₖ²)/log⁶(dK)) regret lower bound for adaptive variance sequences with weak adversaries
- Demonstrated impossibility result: strong adversaries can force O(d) regret even with total variance Ω(K)
- Showed bounds match existing upper bounds (SAVE algorithm) up to logarithmic factors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Variance-dependent lower bounds can be established for any prefixed variance sequence by partitioning rounds into variance-magnitude groups.
- Mechanism: The peeling technique divides K rounds into L = ⌈log₂K⌉ + 1 groups where K₀ = {k: σₖ ≤ 1/K} and Kᵢ = {k: 2^{i-1}/K < σₖ ≤ 2^i/K}. For each group, orthogonal decision sets are constructed so actions in group Kᵢ only interact with their corresponding parameter subspace μᵢ ∈ ℝ^{d/L}, enabling independent lower bounds per group that combine additively.
- Core assumption: The learner cannot infer information about μⱼ from actions designed for group Kᵢ (j≠i), which requires the orthogonal construction where decision sets for group Kᵢ have zeros in all dimensions reserved for other groups.
- Evidence anchors:
  - [abstract] "The key technical contribution is a novel peeling technique that groups rounds by variance magnitude and constructs orthogonal decision sets for each group."
  - [section 4.2] "Due to the orthogonal construction of decision sets across different groups Ki, actions in group Ki provide no information about the weight vector μj for j≠i."
  - [corpus] Weak direct corpus evidence; related papers focus on upper bounds, not lower-bound construction techniques.
- Break condition: Fails for stochastic linear bandits with fixed decision sets—if early rounds have σ₁=...=σ_d=0, the learner can learn exact rewards and incur no regret regardless of subsequent variances.

### Mechanism 2
- Claim: Unknown group sizes in adaptive variance settings can be handled by maintaining multiple instances per group with cyclic assignment.
- Mechanism: For each variance group Kᵢ, create L instances M_{i,1},...,M_{i,L} where instance M_{i,j} is designed for round counts in [2^{j-1}, 2^j). When the learner encounters group Kᵢ, assign decision sets cyclically across these instances. This ensures each instance receives |Kᵢ|/L visits, so the "correct" instance (matching actual |Kᵢ|) provides the desired lower bound.
- Core assumption: The adversary cannot adaptively shrink |Kᵢ| to specifically avoid triggering the correct instance—this is enforced by requiring the weak adversary to generate σₖ before observing Dₖ.
- Evidence anchors:
  - [abstract] "For adaptive sequences, the authors create multiple instances designed for different possible intervals of round numbers and assign the learner to these instances in a cyclic manner."
  - [section 5.2] "Each instance M_{i,j} is designed for a specific range of round numbers, specifically M_{i,j} for 2^{j-1} ≤ |Kᵢ| < 2^j."
  - [corpus] No direct corpus evidence for this multi-instance technique.
- Break condition: Fails if the adversary can observe the decision set before choosing variance (strong adversary); they can then coordinate with the learner to exploit variance assignments.

### Mechanism 3
- Claim: Expected lower bounds can be boosted to high-probability lower bounds through independent instance replication.
- Mechanism: First convert expected bounds to constant-probability bounds using an auxiliary algorithm that switches to OFUL when cumulative regret exceeds a threshold. Then create Ω(log²(dK)) independent instances with i.i.d. sampled weight vectors. By union bound, with probability ≥ 1 - 1/K, at least one instance achieves the target regret lower bound.
- Core assumption: Weight vectors across instances are independent, so failure probabilities multiply rather than correlate.
- Evidence anchors:
  - [abstract] "For adaptive sequences...matching the upper bounds of the SAVE algorithm up to logarithmic factors."
  - [section 5.2] "We first convert expected regret bounds to constant-probability bounds through careful variance control and auxiliary algorithms, then boost these to high-probability bounds by creating multiple independent instances."
  - [corpus] Corpus papers on variance-aware bounds (e.g., "Variance-Aware Feel-Good Thompson Sampling") address upper bounds but not this high-probability boosting technique.
- Break condition: Cannot extend to stochastic linear bandits with fixed decision sets having small covering number—random action selection gives non-zero probability of zero regret.

## Foundational Learning

- Concept: **Linear Contextual Bandit Regret**
  - Why needed here: The paper's lower bounds are expressed as regret(Regret(K) = Σ⟨x*ₖ,μ⟩ - ⟨xₖ,μ⟩), which measures cumulative suboptimality against the best action per round.
  - Quick check question: If an algorithm always selects the optimal action x*ₖ, what is its cumulative regret? (Answer: Zero)

- Concept: **Heteroscedastic Noise**
  - Why needed here: Unlike standard bandit analysis with uniform noise variance, this work assumes variance σₖ² varies per round, requiring variance-dependent rather than worst-case bounds.
  - Quick check question: In heteroscedastic linear bandits, if σₖ² = 0 for rounds 1-10 and σₖ² = 1 for rounds 11-100, does standard Õ(d√K) regret capture the true complexity? (Answer: No—variance-dependent bound Õ(d√90) is tighter)

- Concept: **Eluder Dimension**
  - Why needed here: Prior work (Jia et al., 2024) established lower bounds for general function classes using eluder dimension, but had a √d gap. This paper specializes to linear classes (where d_elu = d) to close that gap.
  - Quick check question: Why does the linear case allow tighter bounds than general eluder-dimension analysis? (Answer: Linear structure enables orthogonal decomposition across variance groups)

## Architecture Onboarding

- Component map: Variance Sequence → [Grouping Layer: Peeling by σₖ magnitude] → [Instance Constructor: One instance per group×interval] → [Decision Set Generator: Orthogonal embedding in ℝ^d] → [Lower Bound Aggregator: Sum regrets across groups]

- Critical path: The peeling thresholds (2^{i-1}/K, 2^i/K) determine group membership; incorrect threshold calibration causes variance misattribution across groups. For adaptive settings, the cyclic assignment schedule must ensure uniform visit distribution.

- Design tradeoffs: Tighter logarithmic factors (prefixed: 1/log K vs. adaptive: 1/log⁶(dK)) versus adversary flexibility. The multi-instance framework handles adaptivity but introduces logarithmic overhead.

- Failure signatures:
  1. Strong adversary + action-dependent variance → O(d) regret achievable even with Σσₖ² = Ω(K)
  2. Fixed decision set + early zero-variance rounds → Learner identifies optimal action before high-variance rounds
  3. Covering number log N ≤ Õ(d) for stochastic bandits → Constant probability of lucky action selection

- First 3 experiments:
  1. Verify peeling decomposition: Construct a prefixed sequence with known group sizes (e.g., 50% rounds at σ=0.01, 30% at σ=0.1, 20% at σ=1), run multiple algorithms, confirm regret scales as Σᵢ dᵢ√(|Kᵢ|σ²(i)) rather than d√(K·maxσ²).
  2. Test adaptive cyclic assignment: Implement weak adversary that shifts variance distribution mid-episode; verify cyclic instance assignment maintains lower bound despite unknown |Kᵢ|.
  3. Validate impossibility condition: Implement strong adversary that sets σₖ=0 for exploration rounds (actions outside span of explored set) and σₖ=1 otherwise; confirm O(d) regret is achievable.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can variance-dependent regret lower bounds for general variance sequences be established for contextual bandits with general function approximation?
- Basis in paper: [explicit] "Therefore, we leave for future work the generalization of our analysis of general variance sequence to contextual bandits with general function approximation."
- Why unresolved: The current work focuses exclusively on linear bandits, while prior work (Jia et al., 2024) only handles general function classes with a fixed total variance budget, not general variance sequences.
- What evidence would resolve it: A proof technique extending the peeling and multi-instance framework to function classes characterized by eluder dimension or similar complexity measures.

### Open Question 2
- Question: Can high-probability lower bounds be established for stochastic linear bandits with fixed decision sets?
- Basis in paper: [explicit] "For stochastic linear bandits with a fixed decision set, we can only derive a constant-probability lower bound... which precludes the possibility of establishing high-probability lower bounds for large round numbers K."
- Why unresolved: With a fixed decision set and covering number log N ≤ Õ(d), an algorithm can achieve zero regret with probability 1/N = exp(−d), preventing high-probability guarantees.
- What evidence would resolve it: Identifying additional structural assumptions on the decision set that enable high-probability lower bounds, or proving impossibility more formally.

### Open Question 3
- Question: Is the condition that total variance Σk σ²k ≥ Ω(d²) necessary for establishing variance-dependent lower bounds?
- Basis in paper: [inferred] The paper requires total variance ≥ 1 + 384d² but notes "this observation suggests that requiring total variance no less than Ω(d²)... may be necessary."
- Why unresolved: The paper shows this condition is sufficient but does not prove it is necessary; alternative conditions might enable lower bounds for smaller total variance regimes.
- What evidence would resolve it: Either a lower bound construction that works without this condition, or a counterexample showing algorithms can achieve substantially better regret when total variance is o(d²).

## Limitations
- The impossibility result for strong adversaries relies on artificial construction where the adversary can set σₖ=0 for exploration actions, which may not represent practical settings.
- The extension to stochastic linear bandits requires log N = O(d) for the action set, which may not hold in high-dimensional settings or when decision sets have complex structure.
- The gap of log⁶(dK) for adaptive sequences suggests room for improvement in the lower bound construction.

## Confidence
- **High confidence**: The prefixed variance sequence lower bound (Theorem 4.1) — the peeling technique and orthogonal construction are mathematically rigorous with clear separation of variance groups.
- **Medium confidence**: The adaptive variance lower bound (Theorem 5.1) — the multi-instance cyclic assignment framework is novel but requires careful implementation to ensure uniform visit distribution across instances.
- **Medium confidence**: The impossibility result for strong adversaries — while logically sound, it relies on extreme variance manipulation that may not occur in practical applications.

## Next Checks
1. **Empirical verification of peeling decomposition**: Implement the hard instance construction with orthogonal sub-instances and verify that regret decomposes as claimed across variance groups when running standard algorithms (OFUL, random exploration).
2. **Adaptive cyclic assignment robustness**: Test the multi-instance framework under various adaptive variance sequences, including adversarial patterns that shift group sizes mid-episode, to confirm the lower bound persists.
3. **Strong adversary exploitability**: Implement the strong adversary construction that sets σₖ=0 for exploration rounds and verify that algorithms can indeed achieve O(d) regret despite high total variance.