---
ver: rpa2
title: 'NL2GenSym: Natural Language to Generative Symbolic Rules for SOAR Cognitive
  Architecture via Large Language Models'
arxiv_id: '2510.09355'
source_url: https://arxiv.org/abs/2510.09355
tags:
- rules
- soar
- knowledge
- operator
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NL2GenSym, a framework that integrates Large
  Language Models with the SOAR cognitive architecture to automatically generate and
  optimize executable symbolic rules from natural language. The core innovation is
  an execution-grounded generator-critic loop, where rules are iteratively refined
  based on feedback from SOAR execution.
---

# NL2GenSym: Natural Language to Generative Symbolic Rules for SOAR Cognitive Architecture via Large Language Models

## Quick Facts
- arXiv ID: 2510.09355
- Source URL: https://arxiv.org/abs/2510.09355
- Reference count: 40
- Primary result: Achieves 86% success rate in generating executable symbolic rules from natural language, with decision cycles 1.98× optimal

## Executive Summary
NL2GenSym introduces a framework that automatically converts natural language problem descriptions into executable symbolic rules for the SOAR cognitive architecture using Large Language Models. The core innovation is an execution-grounded generator-critic loop where rules are iteratively refined based on feedback from actual SOAR execution. This approach enables smaller-parameter models to outperform larger counterparts by focusing on architectural design rather than model scale. The framework demonstrates significant improvements in decision-making efficiency, reducing cycles to near-optimal levels while maintaining high success rates in rule generation.

## Method Summary
The framework employs a generator-critic architecture where a Large Language Model generates candidate symbolic rules from natural language problem descriptions. These rules are then executed within the SOAR environment, and the execution outcomes serve as feedback for refinement. The critic component evaluates rule performance and guides the generator to produce improved rules in subsequent iterations. This execution-grounded approach ensures that generated rules are not only syntactically correct but also functionally effective within the cognitive architecture, creating a self-improving loop that optimizes rule quality through practical validation.

## Key Results
- Achieves over 86% success rate in generating executable symbolic rules from natural language
- Reduces decision cycles to near-optimal levels (1.98× the theoretical optimum)
- Outperforms baseline methods by a factor of 1000 in terms of efficiency

## Why This Works (Mechanism)
The execution-grounded generator-critic loop is the key mechanism enabling NL2GenSym's success. By incorporating actual SOAR execution feedback into the rule refinement process, the framework ensures that generated rules are not just syntactically valid but also practically effective. This iterative approach allows the system to learn from execution failures and successes, progressively improving rule quality. The feedback loop creates a direct connection between rule generation and practical performance, addressing the common challenge of translating natural language descriptions into functional cognitive architecture rules.

## Foundational Learning

**SOAR Cognitive Architecture**: A production system-based cognitive architecture that uses symbolic rules for decision-making and problem-solving. Understanding SOAR's rule execution mechanism is essential for designing effective rule generation strategies.

**Generator-Critic Framework**: A machine learning approach where a generator creates candidates and a critic evaluates them, creating a feedback loop for improvement. This architecture is crucial for iterative refinement of rule quality.

**Execution-Grounded Learning**: The concept of using actual system execution as feedback for learning and improvement, rather than relying solely on static evaluation metrics. This approach ensures practical effectiveness of generated rules.

**Natural Language Processing for Symbolic Systems**: The challenge of translating natural language descriptions into formal symbolic representations that can be executed by cognitive architectures. This skill is fundamental to the framework's core functionality.

**Production Systems**: Rule-based systems where rules consist of conditions and actions, forming the basis of SOAR's decision-making mechanism. Understanding production systems is vital for generating valid SOAR rules.

## Architecture Onboarding

**Component Map**: NL Description -> Generator (LLM) -> Candidate Rules -> SOAR Execution -> Execution Results -> Critic -> Feedback -> Generator (LLM)

**Critical Path**: The generator-critic loop is the critical path, where rule generation, execution, and refinement iterate until satisfactory performance is achieved. Each cycle involves rule generation, SOAR execution, performance evaluation, and feedback incorporation.

**Design Tradeoffs**: The framework trades computational overhead (multiple execution cycles) for improved rule quality and practical effectiveness. The choice to use actual execution rather than static evaluation ensures practical validity but increases runtime requirements.

**Failure Signatures**: Poor initial rule generation leading to execution failures, inadequate feedback incorporation resulting in stuck iterations, and mismatch between natural language descriptions and SOAR's symbolic requirements.

**First Experiments**:
1. Test the generator's ability to create basic SOAR rules from simple natural language descriptions
2. Evaluate the critic's effectiveness in identifying rule execution failures
3. Measure the improvement in rule quality across multiple generator-critic iterations

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Evaluation is limited to a single problem domain (Water Jug Problem), restricting generalizability to other cognitive tasks
- The claim of outperforming baseline methods by a factor of 1000 lacks specific baseline details and methodology description
- The framework's dependency on SOAR execution for rule refinement raises scalability concerns for more complex problem spaces

## Confidence
- High confidence in the execution-grounded generator-critic loop methodology
- Medium confidence in the 86% success rate claim (single dataset limitation)
- Low confidence in the scalability claims beyond the water jug domain
- Medium confidence in the relative performance comparison to baselines (insufficient methodology details)

## Next Checks
1. Replicate experiments across diverse cognitive domains (e.g., Tower of Hanoi, Blocks World) to assess generalizability beyond water jug problems.
2. Conduct ablation studies isolating the impact of architectural components versus model scale, including comparisons with state-of-the-art language models of varying sizes trained on similar datasets.
3. Measure computational overhead and runtime performance of the generator-critic loop during execution, particularly for problems requiring multiple refinement iterations.