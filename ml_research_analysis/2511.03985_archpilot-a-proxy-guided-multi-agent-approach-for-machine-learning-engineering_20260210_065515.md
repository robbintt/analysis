---
ver: rpa2
title: 'ArchPilot: A Proxy-Guided Multi-Agent Approach for Machine Learning Engineering'
arxiv_id: '2511.03985'
source_url: https://arxiv.org/abs/2511.03985
tags:
- proxy
- search
- training
- agent
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ArchPilot addresses the high computational cost of LLM-based ML\
  \ engineering by introducing a three-agent system that separates architecture generation,\
  \ proxy-based evaluation, and search orchestration. The method uses a Monte Carlo\
  \ Tree Search controller that selects candidates using proxy metrics\u2014such as\
  \ one-epoch validation and feature-dropout validation\u2014weighted adaptively through\
  \ ridge-regularized least squares."
---

# ArchPilot: A Proxy-Guided Multi-Agent Approach for Machine Learning Engineering

## Quick Facts
- arXiv ID: 2511.03985
- Source URL: https://arxiv.org/abs/2511.03985
- Authors: Zhuowen Yuan, Tao Liu, Yang Yang, Yang Wang, Feng Qi, Kaushik Rangadurai, Bo Li, Shuang Yang
- Reference count: 4
- Primary result: Achieves valid submission rate of 0.893 and mean normalized ranking of 0.6149 on MLE-Bench, outperforming state-of-the-art baselines

## Executive Summary
ArchPilot introduces a three-agent system for efficient machine learning engineering that separates architecture generation, proxy-based evaluation, and search orchestration. The method uses Monte Carlo Tree Search with adaptive proxy weighting to explore the architecture space while minimizing expensive full training runs. By leveraging multiple cheap proxies and dynamically reweighting them based on ground-truth observations, ArchPilot achieves superior performance on MLE-Bench tasks while maintaining computational efficiency.

## Method Summary
ArchPilot implements a proxy-guided multi-agent approach where three specialized agents work in concert: the Generation Agent creates and debugs architecture code, the Evaluation Agent runs multiple cheap proxy training runs and adaptively reweights them to approximate true performance, and the Orchestration Agent uses MCTS-style search with tree restarts when proxy weights change. The system aggregates proxy scores using ridge-regularized least squares and restarts the search tree when proxy weights change significantly, enabling efficient exploration of the architecture space while minimizing expensive full training runs.

## Key Results
- Valid submission rate of 0.893 compared to AIDE (0.787) and ML-Master (0.867)
- Mean normalized ranking of 0.6149 versus AIDE (0.6953) and ML-Master (0.6535)
- Largest performance gains on high-difficulty tasks within MLE-Bench
- Demonstrates computational efficiency through reduced reliance on full training runs

## Why This Works (Mechanism)

### Mechanism 1: Multi-Proxy Evaluation with Adaptive Reweighting
Aggregating multiple cheap proxies and dynamically reweighting them based on ground-truth observations approximates full training performance at a fraction of the compute cost. The Evaluation Agent runs three proxies (one-epoch validation on 10% data, noisy validation, feature-dropout validation), collects their scores into a vector, and computes a weighted sum. As full-training results accumulate, ridge-regularized least squares refits weights, which are projected onto the probability simplex with a hard-zero policy for failed proxies.

### Mechanism 2: MCTS with Tree Restarts on Weight Updates
Restarting the search tree when proxy weights change prevents stale UCT statistics from misleading exploration and focuses compute on empirically strong regions. The Orchestration Agent maintains MCTS with UCT selection and resets visit counts when the L1 change in weights exceeds threshold ε, reseeding from top-k verified nodes.

### Mechanism 3: Three-Agent Decoupling for Specialization
Separating generation, evaluation, and orchestration into specialized agents enables modular optimization and prevents interference between code generation, scoring, and search control. Each agent operates with distinct prompts and objectives, communicating through structured interfaces.

## Foundational Learning

- **Monte Carlo Tree Search (MCTS) and UCT**: Core search algorithm for Orchestration Agent; understanding selection, expansion, backpropagation is essential. Quick check: Can you explain how UCT balances exploration vs. exploitation using the visit counts and cumulative rewards?

- **Zero-cost and low-fidelity proxies for NAS**: Evaluation Agent relies on proxy training; understanding their limitations and correlation properties is critical. Quick check: Why might one-epoch validation correlate with full training on some tasks but not others?

- **Ridge regression with simplex projection**: Weight fitting uses regularized least squares; projection ensures valid probability weights. Quick check: What does the regularization parameter α do when proxy signals are highly correlated?

## Architecture Onboarding

- **Component map**: Orchestration Agent -> Generation Agent -> Evaluation Agent -> Orchestration Agent
- **Critical path**: OA selects node → GA generates/modifies code → EA runs proxy training → EA aggregates scores → OA backpropagates reward → EA refits weights (if k≥5 labeled pairs) → OA triggers restart if weights change significantly
- **Design tradeoffs**: More proxies → better coverage but higher per-node cost and potential overfitting; aggressive restarts → fresher statistics but risk losing partial exploration; strict budget enforcement → reliability but may terminate before convergence
- **Failure signatures**: High buggy-node rate with valid proxy scores suggests proxy-task misalignment; weight oscillation without ranking improvement indicates ε threshold too low or proxies uninformative; valid submission rate drops suggest budget exhaustion preventing final full-training fallback
- **First 3 experiments**: 1) Proxy ablation: run with single proxy vs. multi-proxy on 5 diverse MLE-Bench tasks; 2) Restart threshold sweep: vary ε ∈ {0.01, 0.05, 0.1} on high-difficulty tasks; 3) Budget scaling: plot ranking vs. GPU hours (1-6 hours)

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow experimental scope with only 5 tasks from MLE-Bench for detailed ablation studies
- Benchmark-specific results may not generalize to other ML engineering tasks
- Adaptive reweighting assumes stable proxy-truth correlations not tested under proxy failure conditions
- Hard-zero policy for failed proxies could prematurely eliminate useful signals

## Confidence
- **High confidence**: Three-agent architectural design is clearly specified and MCTS framework is well-established
- **Medium confidence**: Proxy aggregation mechanism and weight optimization are theoretically sound but empirically limited
- **Medium confidence**: Benchmark results show consistent improvement but sample size and potential optimizations reduce generalizability

## Next Checks
1. **Proxy Robustness Test**: Systematically disable each proxy individually across all MLE-Bench tasks to measure contribution and identify critical proxies.
2. **Threshold Sensitivity Analysis**: Run comprehensive sweeps of restart threshold ε across multiple orders of magnitude on high-difficulty tasks, measuring both ranking performance and restart frequency.
3. **Cross-Benchmark Generalization**: Apply ArchPilot to at least two additional ML engineering benchmarks beyond MLE-Bench to test generalization beyond specific task distribution.