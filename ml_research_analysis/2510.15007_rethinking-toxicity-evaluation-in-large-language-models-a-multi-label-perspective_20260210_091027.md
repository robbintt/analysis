---
ver: rpa2
title: 'Rethinking Toxicity Evaluation in Large Language Models: A Multi-Label Perspective'
arxiv_id: '2510.15007'
source_url: https://arxiv.org/abs/2510.15007
tags:
- label
- multi-label
- toxicity
- detection
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces three multi-label benchmarks (Q-A-MLL, R-A-MLL,
  H-X-MLL) for toxicity detection in LLMs, addressing the limitation of single-label
  annotations in existing datasets. The authors propose a pseudo-label-based framework
  that recovers missing labels from sparse annotations using contrastive label enhancement
  and graph-based correlation modeling.
---

# Rethinking Toxicity Evaluation in Large Language Models: A Multi-Label Perspective

## Quick Facts
- **arXiv ID**: 2510.15007
- **Source URL**: https://arxiv.org/abs/2510.15007
- **Reference count**: 40
- **Primary result**: Introduces three multi-label toxicity benchmarks and pseudo-label framework achieving mAP scores of 0.50, 0.31, and 0.21 on Q-A-MLL, R-A-MLL, and H-X-MLL respectively

## Executive Summary
This paper addresses the critical gap in toxicity detection for large language models by introducing three multi-label benchmarks (Q-A-MLL, R-A-MLL, H-X-MLL) that reflect the reality that toxic prompts often violate multiple categories simultaneously. The authors propose a pseudo-label-based framework that recovers missing labels from sparse single-label annotations using contrastive label enhancement and graph-based correlation modeling. Their method, LEPL-MLL, theoretically and empirically demonstrates that training with pseudo-labels achieves lower expected risk than single-label supervision, outperforming strong baselines including GPT-4o and DeepSeek on all three datasets.

## Method Summary
The approach tackles partial multi-label learning by first recovering soft label distributions through contrastive label enhancement that aligns semantically similar instances. Class prior-guided pseudo-label generation then converts these soft scores to binary labels using validation-set statistics. Finally, a GCN-based classifier learns over the label co-occurrence graph to capture structured dependencies between toxicity categories. The method requires single-label annotations for training but multi-label annotations for validation to estimate class priors and label correlations, enabling effective fine-tuning of LLMs under varying label coverage levels.

## Key Results
- LEPL-MLL achieves mAP scores of 0.50, 0.31, and 0.21 on Q-A-MLL, R-A-MLL, and H-X-MLL respectively
- Significant improvements over baselines: +0.12 mAP on Q-A-MLL compared to standard BCE training
- Outperforms GPT-4o and DeepSeek with zero-shot multi-label prompts across all datasets
- Maintains >0.40 mAP at 10% label coverage while baselines drop below 0.15

## Why This Works (Mechanism)

### Mechanism 1: Pseudo-Labeling Reduces Expected Risk vs. Single-Label Supervision
Training with properly constructed pseudo-labels achieves lower expected risk than treating unobserved labels as negatives. The sample complexity relationship shows that lower unreliability degree (ξ_pseudo ≤ ξ_single) yields better performance for fixed sample size. This advantage disappears when pseudo-label quality degrades below ~10% label coverage.

### Mechanism 2: Contrastive Label Enhancement Recovers Latent Soft Label Distributions
Semantic consistency among neighbors refines sparse single-label annotations into meaningful soft label distributions. Similar prompts should share similar toxicity profiles, propagating label information through feature space. If embedding space has poor semantic structure, neighbor-based propagation amplifies noise rather than signal.

### Mechanism 3: GCN-Based Label Correlation Modeling Captures Co-occurrence Structure
Explicitly modeling label co-occurrence via graph convolution improves multi-label prediction by enabling label-to-label information flow. Labels frequently co-occurring share learned representations through graph propagation. If label correlations shift between validation and deployment, learned graph structure becomes detrimental.

## Foundational Learning

- **Multi-Label Classification vs. Single-Label**: The entire paper rests on premise that toxic prompts violate multiple categories simultaneously. Quick check: Can you explain why a model predicting both "Illegal Activity" AND "Child Exploitation" for a single prompt might be penalized under single-label evaluation but rewarded under multi-label?

- **Partial-Label / Weakly Supervised Learning**: Training data has only one label per instance despite ground truth being multi-label. Quick check: If 3 of 5 relevant labels are missing from training supervision, what happens if you train with standard binary cross-entropy treating missing labels as negatives?

- **Pseudo-Labeling Pipeline Design**: Three-stage pipeline requires understanding how each stage's output feeds the next. Quick check: Why does pseudo-label quality depend on validation set statistics, and what could go wrong if validation distribution differs from training?

## Architecture Onboarding

- **Component map**: Input single-label training data + multi-label validation data → Contrastive Label Enhancement → Class Prior-Guided Pseudo-Label Generation → GCN-Based Classifier with Label Correlations → Per-instance probability vector over 15 toxicity categories

- **Critical path**: Validation set quality directly controls pseudo-label accuracy; embedding quality controls contrastive enhancement; GCN initialization and graph construction affect convergence

- **Design tradeoffs**: Trades expensive multi-label training annotations for cheaper single-label + compute-intensive pseudo-labeling; prior estimation variance from validation set; GCN depth vs. over-smoothing

- **Failure signatures**: mAP near random (~0.067 for 15 classes) indicates pseudo-label generation failing; high Ranking Loss but reasonable mAP suggests GCN over-smoothing; large gap between validation and test performance indicates label distribution shift

- **First 3 experiments**: 1) Baseline sanity check: Train RoBERTa with raw single-label data using BCE (expect mAP ~0.12 on Q-A-MLL); 2) Ablation by stage: Enable only +A, then +A+B, then +A+B+C (expect mAP progression 0.437→0.463→0.483→0.503); 3) Label coverage sensitivity: Randomly mask 10-50% of ground-truth labels (expect LEPL-MLL maintains >0.40 mAP at 10% coverage while baselines drop below 0.15)

## Open Questions the Paper Calls Out
- Can the reliance on manual annotation be entirely eliminated by utilizing LLM-generated synthetic data for the initial training set?
- How does the demographic homogeneity of the annotator pool impact the cross-cultural generalizability of the multi-label ground truth?
- Does the performance of the class prior-guided pseudo-labeling degrade when applied to toxicity taxonomies with significantly higher granularity?

## Limitations
- Dataset accessibility: The three proposed datasets are not publicly released, making independent validation difficult
- Annotation quality: Pseudo-label quality heavily depends on validation set statistics; potential for validation-test leakage if co-occurrence patterns don't generalize
- Theoretical assumptions: The proof that pseudo-label training achieves lower expected risk assumes the pseudo-label generator's error rate is sufficiently low, but this is not empirically validated across varying label coverage levels

## Confidence
- **High Confidence**: Experimental results showing LEPL-MLL outperforming baselines; theoretical framework for expected risk reduction; ablation studies demonstrating component contributions
- **Medium Confidence**: Generalizability to other toxicity detection scenarios; assumption that validation set co-occurrence patterns reflect real-world label relationships
- **Low Confidence**: Practical utility in real-world deployment where label distributions may shift; computational efficiency claims given three-stage pipeline complexity

## Next Checks
1. **Dataset replication**: Construct a smaller-scale version of one dataset using the 15-category taxonomy on existing toxicity data to verify the pseudo-labeling pipeline produces expected improvements
2. **Label coverage sensitivity**: Systematically vary label coverage from 5% to 50% and measure whether LEPL-MLL maintains the claimed advantage over BCE baseline as theoretical predictions suggest
3. **Co-occurrence generalization**: Split validation data temporally or by source domain to test whether the GCN-based label correlation modeling degrades when validation and test distributions differ