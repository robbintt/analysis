---
ver: rpa2
title: Decentralized Online Convex Optimization with Unknown Feedback Delays
arxiv_id: '2601.07901'
source_url: https://arxiv.org/abs/2601.07901
tags:
- where
- algorithm
- equation
- lemma
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses decentralized online convex optimization (D-OCO)
  with unknown, time-and agent-varying feedback delays, a problem relevant to federated
  learning and multi-agent systems. The key challenge is designing algorithms that
  can adapt to delays without prior knowledge of their total magnitude.
---

# Decentralized Online Convex Optimization with Unknown Feedback Delays

## Quick Facts
- arXiv ID: 2601.07901
- Source URL: https://arxiv.org/abs/2601.07901
- Reference count: 40
- This paper proposes a novel algorithm for decentralized online convex optimization that adapts to unknown, time-varying feedback delays without requiring prior delay knowledge.

## Executive Summary
This paper addresses decentralized online convex optimization (D-OCO) with unknown, time-and agent-varying feedback delays, a problem relevant to federated learning and multi-agent systems. The key challenge is designing algorithms that can adapt to delays without prior knowledge of their total magnitude. The authors propose a novel algorithm combining accelerated gossip-based communication with an adaptive learning rate mechanism. Their approach uses a decentralized protocol where agents estimate delays locally through gossip strategies, eliminating the need for centralized coordination or prior delay knowledge.

## Method Summary
The method employs a block-based approach where agents perform accelerated gossip iterations within each block to achieve consensus on gradient information. For general convex losses, AD-FTRL-DF uses a FTRL-based update rule with adaptive learning rate determined by locally estimated cumulative delays. For strongly convex losses, AD-FTRL-DF-SC extends this framework with a specific regularizer schedule and augmented gradient updates. The key innovation is Algorithm 2, which enables each agent to estimate global delay statistics locally through a gossip-based strategy without requiring centralized coordination or prior knowledge of total delay.

## Key Results
- Achieves regret bound of O(N√dtot + N√T/(1-σ²)^¼) for general convex losses, improving upon previous results by eliminating the need for delay knowledge
- Extends framework to strongly convex setting with regret bound O(Nδmax ln T/α), where δmax is maximum concurrent delay
- Proves matching lower bounds up to logarithmic factors, demonstrating tightness of results
- Experiments validate approach across different network topologies and loss functions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Agents can estimate global delay statistics locally to tune learning rates without centralized coordination or prior knowledge of total delay.
- **Mechanism:** Each agent maintains a local counter for "missing observations" (gradients not yet received). They run a parallel accelerated gossip protocol to share these counts with neighbors. This aggregates the local counts into a network-wide estimate of the average cumulative delay, which is then used to adapt the learning rate η_s(u) dynamically.
- **Core assumption:** The gossip protocol converges sufficiently fast relative to the block size B, specifically relying on the spectral gap 1 - σ²(W).
- **Evidence anchors:**
  - [abstract] "enables each agent to estimate delays locally using a gossip-based strategy without the prior knowledge of the total delay."
  - [section 4.2] "Algorithm 2 closely mirrors the accelerated gossip routine... but instead focuses on gossiping the cumulative number of missing observations."
  - [corpus] Related work (e.g., "Revisiting Multi-Agent Asynchronous...") discusses asynchronous delays, but evidence for this specific *gossip-based delay estimation* mechanism is unique to this paper within the provided context.
- **Break condition:** If the network disconnects or the spectral gap approaches 0 (e.g., path graph with large diameter), the gossip estimate error may grow large, potentially destabilizing the learning rate.

### Mechanism 2
- **Claim:** The use of accelerated gossip within blocks decouples the regret dependency on network topology from the delay magnitude.
- **Mechanism:** The algorithm divides time into blocks of size B. Within a block, agents perform multiple rounds of accelerated gossip to average local gradient information. This "multi-step" consensus reduces the disagreement error to depend on (1-σ²)^(-1/4) rather than the slower (1-σ²)^(-1/2) typical of standard gossip.
- **Core assumption:** The loss functions are Lipschitz and the domain is bounded.
- **Evidence anchors:**
  - [section 4.1] "our result not only achieves a better dependency on the spectral gap... but also shows that the effects of the delay and those of the network topology can be decoupled."
  - [section 3] Proposition 4 defines the convergence rate of the accelerated gossip process.
  - [corpus] Consistent with "A Reduction from Delayed to Immediate Feedback..." which suggests delay effects can be isolated, though this paper specifically optimizes the network term.
- **Break condition:** If the block size B is set too small relative to the network mixing time, the gossip iterations will not converge, causing high disagreement and increasing regret.

### Mechanism 3
- **Claim:** Strong convexity allows the system to achieve logarithmic regret (O(ln T)) with respect to time, provided delays are bounded.
- **Mechanism:** When losses are strongly convex, the algorithm utilizes a specific regularizer schedule and an augmented gradient update. This curvature allows the algorithm to "close in" on the optimum faster, transforming the dependency on T from √T to ln T, even under delay.
- **Core assumption:** The loss functions satisfy α-strong convexity.
- **Evidence anchors:**
  - [abstract] "extend our framework to the strongly convex setting and derive a sharper regret bound... O(N δ_max ln T / α)."
  - [section 5] Algorithm 3 details the modified update rule including the −αBx_s(u) term.
  - [corpus] Supported by "Exploiting Curvature in Online Convex Optimization..." which links curvature to improved delay tolerance.
- **Break condition:** If the loss is not strongly convex (or curvature is misestimated), the logarithmic bound collapses, and the algorithm reverts to the general convex performance class.

## Foundational Learning

- **Concept: Distributed Consensus (Gossip Algorithms)**
  - **Why needed here:** The core engine of the paper relies on agents averaging values (gradients or delay counters) solely by talking to neighbors. You must understand how spectral gap 1-σ² relates to convergence speed.
  - **Quick check question:** If a network has a small spectral gap (e.g., a line graph), does the algorithm require more or fewer internal gossip steps B to achieve the same regret?

- **Concept: Online Convex Optimization (OCO) Regret**
  - **Why needed here:** The paper optimizes "Regret," the difference between cumulative loss and the best fixed action in hindsight. Understanding the difference between O(√T) (sublinear) and O(ln T) (logarithmic) performance is critical.
  - **Quick check question:** Why is eliminating the dependency on prior delay knowledge crucial for a "true" online regret bound?

- **Concept: Feedback Delays (dtot vs δmax)**
  - **Why needed here:** The paper distinguishes between the *total* delay (sum of all lags) and the *maximum missing observations* per round.
  - **Quick check question:** In the strongly convex setting, does the regret bound scale with the total cumulative delay dtot or the maximum concurrent delay δmax?

## Architecture Onboarding

- **Component map:**
  - Agent Node: Contains two parallel modules: (1) Predictor (Alg 1) executing FTRL decisions, and (2) Delay Estimator (Alg 2) running gossip on missing observation counts
  - Communication Layer: Manages the accelerated gossip exchange of gradient buffers z_k and delay counters ζ_k
  - Feedback Buffer: Stores incoming gradients g_τ(u) until they are "popped" for aggregation in block s

- **Critical path:** The calculation of the adaptive learning rate η_{s+1}(u) (Eq. 8). This relies on the delay estimator (Alg 2) being sufficiently synchronized across agents. If this rate diverges between agents, the decision consensus x_s(u) breaks down.

- **Design tradeoffs:**
  - **Block Size (B):** Larger B allows more gossip iterations, improving consensus accuracy (lower network regret), but increases the "staleness" of the decision x_s within the block (higher delay regret)
  - **Learning Rate:** A fixed rate requires oracle knowledge of delays; the adaptive rate removes this requirement but adds communication overhead for the delay gossip

- **Failure signatures:**
  - **Diverging Regret:** If regret grows linearly (O(T)) instead of sublinearly, check if the learning rate is decaying too slowly or if the gossip mixing time exceeds the block size B
  - **High Variance across Agents:** If some agents perform significantly worse than others, verify the connectivity of the graph and ensure the communication matrix W is doubly stochastic

- **First 3 experiments:**
  1. **Baseline Validation:** Replicate the "Cycle vs. Complete Graph" experiment (Fig 1) to verify that the regret scales with the spectral gap (1-σ²)^(-1/4)
  2. **Adaptive Stress Test:** Run the algorithm with artificially injected "delay spikes" (geometric distribution, Fig 2) to ensure the adaptive learning rate (Alg 2) stabilizes without knowing the spike magnitude in advance
  3. **Strong Convexity Check:** Test AD-FTRL-DF-SC (Alg 3) on a quadratic loss to confirm the transition from √T to ln T scaling on the error curve

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the gap in the dependence on the number of agents N between the upper and lower regret bounds be closed?
  - **Basis in paper:** [explicit] In Section 4.3, the authors state that while their bounds are tight regarding T, dtot, and the spectral gap, "there is still a gap of polynomial factors in the number of agents N."
  - **Why unresolved:** The lower bound construction (Theorem 7) and the upper bound analysis (Theorem 6) result in different polynomial dependencies on the network size, leaving the optimal rate for N ambiguous.
  - **What evidence would resolve it:** A refined analysis algorithm achieving an upper bound matching the lower bound's dependence on N, or a revised lower bound proof closing the polynomial gap.

- **Open Question 2:** Can the proposed adaptive framework be extended to projection-free decentralized settings?
  - **Basis in paper:** [inferred] The authors contrast their work with Nguyen et al. (2024), which uses a projection-free method but requires prior delay knowledge. The current algorithm eliminates the need for delay knowledge (Algorithm 2) but relies on FTRL (Eq. 5), which requires solving a projection subproblem.
  - **Why unresolved:** Designing an algorithm that simultaneously handles unknown delays adaptively and avoids the computational cost of projections (e.g., using decentralized Frank-Wolfe) remains an open challenge.
  - **What evidence would resolve it:** A new algorithm that integrates the gossip-based delay estimation into a projection-free update step while maintaining comparable regret guarantees.

- **Open Question 3:** Does the gossip-based adaptive learning rate mechanism remain effective under bandit (zeroth-order) feedback?
  - **Basis in paper:** [inferred] The protocol in Section 3 assumes agents observe the full gradient ∇f_t. The authors note in "Related Works" that "limited-feedback scenarios" are a distinct area of research, implying the current adaptive mechanism has not been tested for bandit settings.
  - **Why unresolved:** Bandit feedback introduces high variance in gradient estimation. It is unclear if the local estimation of missing observations via gossip (Algorithm 2) is robust to the noise inherent in bandit gradient estimators.
  - **What evidence would resolve it:** Theoretical regret bounds or empirical validation showing that the adaptive delay estimation technique maintains performance when gradients are estimated from point-wise loss values.

## Limitations

- The proof of the adaptive learning rate's convergence relies heavily on the gossip protocol's spectral gap and block size relationship. If the network topology has a poor spectral gap (e.g., line graphs), the required block size B grows, potentially negating the delay estimation benefit.
- The strongly convex regret bound's dependency on δmax rather than dtot is theoretically elegant, but in practice, δmax can be significantly larger than the average delay, especially under bursty delay distributions.
- The algorithm's communication overhead scales with the number of agents N and the number of gossip steps per block. For large-scale federated learning with thousands of agents, this overhead could become prohibitive.

## Confidence

- **High Confidence:** The general convex regret bound O(N√dtot + N√T/(1-σ²)^¼) and its derivation from the accelerated gossip framework. The mechanism of decoupling delay and network effects is well-supported by the proof structure.
- **Medium Confidence:** The adaptive learning rate mechanism (Algorithm 2). While the gossip-based delay estimation is novel and theoretically sound, its practical robustness under dynamic network conditions (churn, congestion) is not validated.
- **Medium Confidence:** The strongly convex regret bound O(Nδmax ln T/α). The theoretical extension is clear, but the experiments only demonstrate one strongly convex case (quadratic loss), leaving questions about generalizability to other strongly convex functions.

## Next Checks

1. **Network Topology Stress Test:** Implement the algorithm on a line graph (worst-case spectral gap) and verify that the regret scales as predicted by the (1-σ²)^(-1/4) dependency. Measure the required block size B to maintain convergence.
2. **Delay Distribution Robustness:** Replace the geometric delay distribution with a heavy-tailed distribution (e.g., Pareto) to test if the adaptive learning rate mechanism can handle extreme delay spikes without retraining or parameter tuning.
3. **Communication Overhead Analysis:** Instrument the algorithm to log the total number of scalar values communicated per agent per round. Compare this against the communication cost of a centralized baseline to quantify the scalability limits for large N.