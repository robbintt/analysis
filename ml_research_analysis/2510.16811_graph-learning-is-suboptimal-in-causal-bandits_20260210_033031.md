---
ver: rpa2
title: Graph Learning is Suboptimal in Causal Bandits
arxiv_id: '2510.16811'
source_url: https://arxiv.org/abs/2510.16811
tags:
- regret
- causal
- algorithm
- reward
- parent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies regret minimization in causal bandits with unknown
  causal structure, demonstrating that learning the parent set of the reward node
  is suboptimal for regret minimization. Under causal sufficiency, prior work focused
  on identifying parents first and then applying standard bandit algorithms, or jointly
  learning parents while minimizing regret.
---

# Graph Learning is Suboptimal in Causal Bandits

## Quick Facts
- **arXiv ID:** 2510.16811
- **Source URL:** https://arxiv.org/abs/2510.16811
- **Reference count:** 40
- **Primary result:** Graph learning is fundamentally unnecessary for optimal regret minimization in causal bandits with unknown causal structure

## Executive Summary
This paper challenges the conventional wisdom that parent identification is necessary for optimal regret minimization in causal bandits with unknown causal structure. The authors prove that regret minimization and parent identification can be fundamentally conflicting objectives: there exist instances where high-reward actions are disjoint from informative actions for identifying parents. They establish novel regret lower bounds and propose algorithms that bypass graph learning entirely, achieving near-optimal regret without identifying the causal graph. Experimental results show up to 20x regret reduction compared to existing baselines.

## Method Summary
The authors study causal bandits where an agent intervenes on subsets of n variables (up to size m) to minimize cumulative regret, without knowing which variables are parents of the reward node. For known parent set size k, they propose a random-subset UCB algorithm that samples a subset of arms containing at least one optimal action with high probability. For unknown k, they introduce a phased adaptive algorithm that progressively narrows the effective action space. Both approaches bypass explicit graph learning, leveraging the combinatorial structure of the action space where optimal actions are interventions on the unknown parent set.

## Key Results
- Proves fundamental conflict between regret minimization and parent identification: no algorithm can achieve optimal regret while identifying parents with high probability
- Establishes novel regret lower bounds valid even with full knowledge of the non-reward graph
- Proposes Algorithm 1 (known k) achieving near-optimal regret without graph knowledge
- Introduces Algorithm 2 (unknown k) that is Pareto optimal up to logarithmic factors when intervening on all variables
- Demonstrates up to 20x regret reduction in experiments across various environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parent identification and regret minimization can be fundamentally conflicting objectives in causal bandits.
- Mechanism: The authors construct problem instances where the set of high-reward (optimal) actions is disjoint from the set of actions informative for identifying parents. An agent must choose between playing a high-reward action (to minimize regret) or an informative action (to learn the graph), creating an irreconcilable trade-off.
- Core assumption: Causal sufficiency and worst-case regret minimization without additional distributional assumptions.
- Evidence anchors:
  - [abstract] "We do so by proving that there exist instances where regret minimization and parent identification are fundamentally conflicting objectives."
  - [section 3] "The key step in the proof of this result is the construction of a class of instances in which the set of high-reward actions and the set of informative actions for identifying the set of parents are disjoint."
- Break condition: If future work identifies structural or distributional assumptions that align the high-reward and informative action sets, this conflict may be mitigated.

### Mechanism 2
- Claim: A simple random-subset UCB algorithm can achieve near-optimal regret without identifying parents or requiring graph knowledge.
- Mechanism: The algorithm samples a random subset A' of size approximately 1/(α_k ln√T), where α_k is the fraction of optimal arms. With high probability, this subset contains at least one optimal action. Standard UCB is then run on this reduced set.
- Core assumption: The parent set size k is known to the agent.
- Evidence anchors:
  - [abstract] "...propose nearly optimal algorithms that bypass graph and parent recovery, demonstrating that parent identification is indeed unnecessary for regret minimization."
  - [section 4.2] Theorem 4.4 provides the regret upper bound for Algorithm 1.
- Break condition: The regret bounds scale with (n choose k), which is exponential in k. The mechanism may be impractical if k is very large.

### Mechanism 3
- Claim: An adaptive algorithm can achieve near rate-Pareto optimal regret when the parent set size k is unknown.
- Mechanism: The algorithm proceeds in phases, randomly selecting subsets of arms and running UCB on each subset plus mixture arms from prior phases. The schedule halves subset size and doubles phase length in each subsequent phase.
- Core assumption: The agent has no prior knowledge of the graph G or the parent size k.
- Evidence anchors:
  - [abstract] "...introduce an adaptive algorithm that is Pareto optimal (up to logarithmic factors) when intervening on all variables."
  - [section 5.2] Theorem 5.3 and Lemma 5.4 provide the regret bounds and Pareto optimality proof for Algorithm 2.
- Break condition: The adaptivity comes at a cost, specifically a √(m/n) factor in the regret bound compared to the known-k case.

## Foundational Learning

- Concept: **Regret Minimization in Multi-Armed Bandits**
  - Why needed here: This is the core objective function. Understanding the difference between cumulative and simple regret is crucial for grasping the motivation.
  - Quick check question: Can you define the difference between cumulative regret and simple regret?

- Concept: **Causal Graphs and Interventions (do-calculus)**
  - Why needed here: The paper relies on the semantics of causal DAGs, interventions (actions), and the concept of causal sufficiency. The term "parent set of the reward node" is fundamental.
  - Quick check question: What does the do(X=v) operator signify, and why is it different from conditioning on X=v?

- Concept: **Upper Confidence Bound (UCB) Algorithm**
  - Why needed here: The proposed algorithms are built using the standard UCB algorithm as a subroutine. Understanding its exploration-exploitation balance is necessary.
  - Quick check question: What is the core principle behind the UCB algorithm, and how does its confidence bound width typically change as an arm is pulled more often?

## Architecture Onboarding

- Component map:
  - Environment -> Graph Generator -> Intervention Mechanism -> Reward Generator
  - Algorithm 1: Sampler -> UCB Core
  - Algorithm 2: Scheduler -> Sampler -> UCB Core -> Mixture Arm Constructor -> Memory

- Critical path: For Algorithm 2, the critical path is the Scheduler's phase control. It dictates the trade-off between exploring broadly in early phases and refining in later phases.

- Design tradeoffs:
  - **Parent ID vs. Regret**: The paper proves you cannot optimize both. An engineer must choose: do you need the graph (e.g., for interpretability/transfer) or just low regret?
  - **Known vs. Unknown k**: Knowing k gives better regret by a factor of √(n/m). Is it worth the effort to estimate k offline?
  - **Sample Efficiency vs. Computation**: The algorithms are computationally efficient but may require more samples than a method that perfectly knew the graph.

- Failure signatures:
  - Linear Regret: If the problem instances do not satisfy causal sufficiency or if the optimal action lies outside A_m
  - Performance Collapse on Large k: If k is large and unknown, the regret bound becomes weak
  - High Regret with Algorithm 2: If k is small, Algorithm 2's adaptive phases may spend too much time in early, inefficient phases

- First 3 experiments:
  1. Reproduce the Toy Example: Implement the specific problem class E_0 described in the proof of Theorem 3.2. Compare a standard "identify-then-exploit" causal bandit algorithm against Algorithm 1.
  2. Vary k (Known): Use the experimental setup from Figure 4 with fixed n, ℓ, m. Vary the true k from small to large. Run Algorithm 1 with the correct k and plot final regret.
  3. Test Adaptivity (Unknown k): Use a fixed instance with a small k (e.g., k=2). Compare Algorithm 2 against Algorithm 1 run with oracle knowledge of k=2 and against a non-adaptive baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can we characterize the specific conditions under which regret minimization and parent identification are aligned, or define the achievable Pareto frontier of pairs for identification error and regret rates?
- **Basis:** [explicit] The authors state in Section 7: "characterizing this trade-off remains an interesting direction... one could study the settings in which these objectives are aligned... or characterize achievable pairs."
- **Why unresolved:** Theorem 3.2 proves the objectives can conflict, but the paper does not fully map the boundary of when they do or the optimal trade-offs between them.
- **What evidence would resolve it:** Algorithms designed to achieve specific rate pairs and corresponding lower bounds establishing the Pareto frontier.

### Open Question 2
- **Question:** What realistic distributional assumptions (e.g., linear models, smoothness) could meaningfully improve regret rates, given that structural information alone is insufficient?
- **Basis:** [explicit] Section 7 notes: "future work should focus on distributional assumptions that could meaningfully enhance learning performance."
- **Why unresolved:** The current results rely on worst-case analysis without distributional assumptions, showing only that graph knowledge does not help universally.
- **What evidence would resolve it:** Theoretical analysis proving tighter regret bounds under specific model classes.

### Open Question 3
- **Question:** Does the suboptimality of graph learning for regret minimization persist when the assumption of causal sufficiency is violated?
- **Basis:** [inferred] The problem setup explicitly assumes causal sufficiency.
- **Why unresolved:** The paper establishes suboptimality under sufficiency, but many real-world causal bandit scenarios involve unobserved variables.
- **What evidence would resolve it:** Extending the lower bounds and adaptive algorithms to settings with unobserved confounders.

## Limitations

- The exponential dependence on parent set size k in regret bounds makes the approach impractical for problems with large k
- The results assume causal sufficiency (no unobserved confounders), which may not hold in many real-world scenarios
- The theoretical framework relies on worst-case analysis without distributional assumptions, potentially missing opportunities for improved performance under realistic conditions

## Confidence

- **Core claim (conflict between objectives):** High - supported by rigorous information-theoretic lower bound construction
- **Algorithm performance claims:** Medium - theoretical analysis appears sound but depends on problem parameters like k and α_k
- **Real-world applicability:** Medium-Low - experimental evaluation uses controlled synthetic environments that may oversimplify practical scenarios

## Next Checks

1. **Replicate the lower bound construction:** Implement the specific problem class E_0 described in Theorem 3.2 and verify empirically that standard "identify-then-exploit" approaches incur significantly higher regret than Algorithm 1 on this instance.

2. **Stress test Algorithm 2's adaptivity:** Compare Algorithm 2 against Algorithm 1 with oracle knowledge of k across a range of true parent set sizes (particularly small k values). Quantify the adaptivity penalty and verify it matches the theoretical √(m/n) factor.

3. **Test robustness to distributional assumptions:** Modify the experimental setup to use different reward distributions (e.g., Bernoulli with varying success probabilities, or continuous Gaussian rewards) and verify that the relative performance of the proposed algorithms remains consistent.