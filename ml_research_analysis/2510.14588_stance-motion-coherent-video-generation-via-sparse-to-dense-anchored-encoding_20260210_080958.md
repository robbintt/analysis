---
ver: rpa2
title: 'STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding'
arxiv_id: '2510.14588'
source_url: https://arxiv.org/abs/2510.14588
tags:
- motion
- video
- control
- depth
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of maintaining coherent motion
  and physical plausibility in video generation, particularly for object interactions
  and trajectories. It identifies two bottlenecks: sparse, low-resolution control
  inputs that lose effectiveness after encoding, and trade-offs between appearance
  quality and motion consistency when both are optimized together.'
---

# STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding

## Quick Facts
- arXiv ID: 2510.14588
- Source URL: https://arxiv.org/abs/2510.14588
- Reference count: 14
- Key outcome: Physics IQ of 47.62 on rigid-object interactions, outperforming SG-I2V (15.42), Drag-Anything (24.86), and VLIPP (36.40)

## Executive Summary
This paper addresses the challenge of generating motion-coherent videos from sparse user inputs, particularly for rigid-object interactions. The authors identify two core bottlenecks: the loss of control signal when sparse hints are encoded into low-resolution tokens, and the trade-off between appearance quality and motion consistency during joint optimization. To solve these, they introduce Instance Cues—a method that converts sparse 2D user hints into dense, depth-augmented 2.5D motion fields—and Dense RoPE, which preserves spatial identity of motion tokens via first-frame rotary embeddings. Joint training with RGB and auxiliary structural streams further stabilizes motion coherence. Evaluated on a 200k-clip dataset of rigid-object interactions, their method achieves state-of-the-art Physics IQ while maintaining perceptual realism.

## Method Summary
The method fine-tunes a 5B CogVideoX-1.5 DiT backbone for image-to-video generation. User inputs (keyframe + per-instance masks, 2D arrows, mass, and optional depth delta) are rasterized into dense 2.5D motion fields (u, v, ∆z). These are tokenized via an MLP, and a fixed number of nonzero tokens are selected and tagged with first-frame rotary embeddings (Dense RoPE). The backbone processes shared spatio-temporal tokens across dual streams (RGB and auxiliary depth/segmentation), supervised by a joint diffusion loss. Training runs for 50k steps on 8×H100s, with the tokenizer and text encoder frozen.

## Key Results
- Physics IQ of 47.62 on rigid-object interaction dataset, significantly outperforming baselines (SG-I2V 15.42, Drag-Anything 24.86, VLIPP 36.40)
- Joint RGB + auxiliary stream improves Physics IQ to 49.03 (depth) and 47.96 (segmentation) vs. RGB-only 46.89
- Dense RoPE preserves motion control strength for small/thin objects where naive low-res maps fail
- Depth auxiliary helps with perspective reasoning; segmentation better for small-object boundary fidelity

## Why This Works (Mechanism)

### Mechanism 1: Sparse-to-Dense Instance Cue Expansion
- **Claim:** Converting sparse 2D user hints (arrows, masks) into dense, depth-augmented motion fields improves spatial disambiguation under camera motion.
- **Mechanism:** Per-instance average flow is computed over object masks, then "painted" back densely. A scalar depth delta (∆z) is appended as a third channel, yielding a camera-relative 2.5D field. This reduces ambiguity when camera parallax would otherwise confuse pure 2D arrows.
- **Core assumption:** Monocular depth estimates are sufficiently reliable for rigid objects to provide meaningful out-of-plane cues; averaging flow over masks preserves per-object identity.
- **Evidence anchors:**
  - [abstract] "turns sparse, user-editable hints into a dense 2.5D (camera-relative) motion field by averaging per-instance flow and augmenting with monocular depth over the instance mask"
  - [Section 3.2.1] "With monocular depth provided... we derive a per-instance delta depth... and append this scalar as the third control channel, yielding a camera-relative '2.5D' cue."
  - [corpus] Weak direct evidence; corpus papers address motion control but not this specific 2.5D cue construction.
- **Break condition:** If depth estimation is noisy for small/thin objects, the depth channel may degrade guidance; Section 4.2 notes "monocular depth is noisier at small scales."

### Mechanism 2: Dense RoPE Preserves Motion Token Salience
- **Claim:** Selecting salient nonzero spatial locations and tagging them with first-frame rotary embeddings preserves spatial addressability and prevents token dilution after patchification.
- **Mechanism:** Downsampled control maps become sparse (many zeros). Dense RoPE extracts active indices, enforces a fixed token budget (subsampling or tiling), and assigns first-frame RoPE codes to each motion token. These tagged tokens remain spatially identifiable across temporal layers.
- **Core assumption:** The selected motion tokens carry sufficient signal to steer the backbone; first-frame positional anchoring generalizes across temporal frames.
- **Evidence anchors:**
  - [abstract] "Dense RoPE, which tags a small set of motion tokens (anchored on the first frame) with spatial-addressable rotary embeddings to preserve spatial identity and control strength after tokenization"
  - [Section 3.2.2] "Each selected token is tagged with its first-frame RoPE so its spatial identity persists over time; these tokens act as stable, high-signal anchors"
  - [corpus] Tracktention uses point tracking for temporal consistency but does not validate RoPE-based token anchoring.
- **Break condition:** If object masks are extremely sparse relative to token budget, tiling may introduce redundancy; if motion is highly non-rigid, single first-frame anchor may misalign.

### Mechanism 3: Joint RGB + Auxiliary Structural Stream as Regularizer
- **Claim:** Jointly predicting RGB and a structural stream (depth or segmentation) under shared instance cues regularizes appearance optimization and reduces temporal drift.
- **Mechanism:** Two token streams (RGB, auxiliary) share spatio-temporal tokens and attend to the same cue tokens. The structural head anchors geometry/consistency, while RGB handles texture. Both are supervised with a combined diffusion loss.
- **Core assumption:** The auxiliary stream provides a cleaner signal for motion/structure than RGB alone; shared attention does not introduce interference.
- **Evidence anchors:**
  - [abstract] "Joint training with RGB and an auxiliary structural stream (depth or segmentation) further stabilizes motion coherence"
  - [Section 4.2, Table 1] RGB+Depth yields Physics-IQ 49.03 vs. RGB-only 46.89; RGB+Seg yields 47.96
  - [corpus] LightMotion addresses camera motion control but does not validate joint structural supervision.
- **Break condition:** If auxiliary labels are noisy (e.g., segmentation boundary errors), regularization may introduce artifacts; Section 4.2 notes segmentation outperforms depth on small-object split due to depth noise.

## Foundational Learning

- **Rotary Position Embeddings (RoPE):**
  - Why needed: Dense RoPE extends standard RoPE by assigning first-frame rotary codes to selected motion tokens; understanding RoPE's rotation-based positional encoding is prerequisite.
  - Quick check question: Given token positions m₁ and m₂, how does RoPE encode their relative distance via complex rotation?

- **Diffusion Transformers (DiT) for Video:**
  - Why needed: STANCE builds on CogVideoX-1.5 (5B), a DiT backbone using 3D spatio-temporal attention and full self-attention over text+video tokens.
  - Quick check question: How does 3D spatio-temporal attention differ from alternating 1D temporal + 2D spatial attention blocks?

- **Optical Flow and Monocular Depth Representation:**
  - Why needed: Instance cues derive from averaged optical flow and depth deltas; interpreting these as control channels requires understanding flow as 2D displacement vectors.
  - Quick check question: If optical flow at pixel (x,y) is (u,v) = (2, -3), what does this indicate about motion between frames?

## Architecture Onboarding

- **Component map:**
  User hints -> Instance Cue rasterization (2.5D field) -> Patchify + Dense RoPE (N motion tokens) -> CogVideoX-1.5 DiT (text + RGB + aux + motion tokens) -> Dual heads (RGB + aux) -> Joint diffusion loss

- **Critical path:**
  1. User provides sparse hints → rasterize arrows into dense (u,v) inside masks
  2. Append ∆z channel → form 2.5D instance cue tensor
  3. Patchify → extract nonzero indices → apply Dense RoPE → concatenate motion tokens
  4. DiT processes full sequence → dual heads decode RGB and auxiliary streams

- **Design tradeoffs:**
  - Token budget N: Larger N preserves more spatial detail but increases attention cost; smaller N risks dilution
  - Depth vs. Segmentation auxiliary: Depth improves perspective reasoning; Segmentation provides crisper boundaries for small objects
  - Mass channel: Enables collision behavior modification but limited to rigid-object training distribution

- **Failure signatures:**
  - Hovering before contact / bounce-back without contact → indicates weak motion token anchoring or insufficient Dense RoPE budget
  - Identity/texture drift → auxiliary head may be under-weighted (λ_aux too low)
  - Small-object control ignored → token budget exceeded by subsampling; consider increasing N or using segmentation auxiliary

- **First 3 experiments:**
  1. **Ablate Dense RoPE vs. 2D map:** On held-out small-object subset, compare Physics-IQ when using naive low-res map vs. Dense RoPE with fixed budget N=64; expect larger gains on thin/tiny instances.
  2. **Compare auxiliary heads:** Train RGB-only, RGB+Depth, RGB+Seg on same split; evaluate Physics-IQ and FVD on Regular vs. Small-object splits to validate depth advantage for regular scenes, segmentation for small objects.
  3. **Mass sensitivity test:** On multi-object collision clips, vary mass scalar (0.5×, 1×, 2×) while holding velocity fixed; qualitatively assess whether collision outcomes (direction of post-impact motion) change as expected and quantitatively measure Physics-IQ delta.

## Open Questions the Paper Calls Out

- **Can the framework be extended to model fixed environmental boundaries (e.g., walls, table edges) and the resulting elastic rebound behaviors?**
  - Basis in paper: [explicit] The limitations section states the training set excludes fixed boundaries, causing interactions requiring wall contact to lack realistic impulse responses.
  - Why unresolved: The model currently lacks training data or inductive biases to handle collision physics against static scene geometry.
  - What evidence would resolve it: Results from a model trained with boundary-augmented data demonstrating physically plausible bounce-back and reflection trajectories.

- **How can the "Instance Cues" formulation be adapted to handle highly non-rigid objects like cloth, ropes, or fluids?**
  - Basis in paper: [explicit] The authors explicitly list non-rigid objects as outside the current scope, noting the method is designed for rigid-body interactions.
  - Why unresolved: The current method relies on per-instance average flow and rigid masks, which fail for deformable topologies.
  - What evidence would resolve it: A modified control mechanism that successfully guides deformable dynamics without requiring per-frame trajectory scripts.

- **Do Dense RoPE and Instance Cues maintain their efficacy when integrated into Multi-Modal Diffusion Transformer (MMDiT) architectures?**
  - Basis in paper: [explicit] The conclusion notes the authors are "actively working to integrate our components with additional MMDiT-based video backbones."
  - Why unresolved: The current validation is restricted to the CogVideoX backbone; cross-architecture compatibility remains unproven.
  - What evidence would resolve it: Benchmark results on an MMDiT-based backbone (e.g., Stable Video Diffusion) showing comparable improvements in Physics IQ.

- **How can the system mitigate depth-scale ambiguity when user-provided arrows are nearly frontal (aligned with the camera axis)?**
  - Basis in paper: [explicit] The "Depth caveat" in the limitations identifies that frontal arrows can cause motion speed errors due to depth-scale mismatches.
  - Why unresolved: The 2.5D projection relies on scalar depth deltas that become unstable or ambiguous near the camera axis.
  - What evidence would resolve it: A mechanism (e.g., view-dependent scaling or uncertainty modeling) that stabilizes velocity magnitude for axial motion.

## Limitations

- **Training Data Scope:** The method is trained only on rigid-object interactions and explicitly cannot handle deformable objects like cloth or fluids.
- **Depth Estimation Reliability:** Monocular depth is noisier for small/thin objects, limiting the effectiveness of 2.5D Instance Cues in those cases.
- **Environmental Boundaries:** The training dataset excludes fixed boundaries (walls, tables), so interactions requiring rebound physics are not realistically modeled.

## Confidence

- **High Confidence:** The Physics IQ improvement (47.62) over baselines is directly supported by quantitative results in Table 1 and is the paper's primary empirical contribution.
- **Medium Confidence:** The mechanism of Dense RoPE preserving motion token salience is plausible and supported by the ablation design, but the lack of explicit N specification introduces uncertainty about optimal configuration.
- **Medium Confidence:** The joint RGB + auxiliary stream regularization claim is supported by comparative Physics IQ scores (RGB+Depth 49.03 vs RGB-only 46.89), though the exact λ_aux value is unspecified.
- **Low Confidence:** The assertion that depth outperforms segmentation on regular objects and vice versa for small objects is inferred from results but lacks detailed per-split analysis to confirm this trade-off.

## Next Checks

1. **Dense RoPE Ablation on Small Objects:** Train and evaluate STANCE with and without Dense RoPE (using a fixed N=64) on the held-out small-object subset; measure Physics IQ delta to isolate the contribution of motion token anchoring for thin/tiny instances.

2. **Auxiliary Head Comparison:** Conduct a controlled experiment training three variants (RGB-only, RGB+Depth, RGB+Seg) on the same split; report Physics IQ and FVD separately on Regular and Small-object splits to validate the depth vs. segmentation trade-off.

3. **Mass Sensitivity Analysis:** On the multi-object collision subset, vary the mass scalar (0.5×, 1×, 2×) while keeping velocity constant; qualitatively assess collision outcomes (e.g., post-impact direction) and quantitatively measure Physics IQ changes to test the physical plausibility of mass control.