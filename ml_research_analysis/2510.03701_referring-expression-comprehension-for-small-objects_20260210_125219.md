---
ver: rpa2
title: Referring Expression Comprehension for Small Objects
arxiv_id: '2510.03701'
source_url: https://arxiv.org/abs/2510.03701
tags:
- conference
- vision
- objects
- inproc
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of localizing extremely small
  objects in images using referring expressions, a critical task for real-world applications
  like autonomous driving. The authors introduce the SOREC dataset, containing 100,000
  pairs of referring expressions and bounding boxes for small objects in driving scenarios.
---

# Referring Expression Comprehension for Small Objects

## Quick Facts
- arXiv ID: 2510.03701
- Source URL: https://arxiv.org/abs/2510.03701
- Authors: Kanoko Goto; Takumi Hirose; Mahiro Ukai; Shuhei Kurita; Nakamasa Inoue
- Reference count: 40
- Primary result: PIZA achieves 43.1% mAcc on SOREC test-A with 3.5M trainable parameters vs 173M for full fine-tuning

## Executive Summary
This paper addresses the challenge of localizing extremely small objects in images using referring expressions, a critical task for real-world applications like autonomous driving. The authors introduce the SOREC dataset, containing 100,000 pairs of referring expressions and bounding boxes for small objects in driving scenarios. They propose PIZA (Progressive-Iterative Zooming Adapter), a parameter-efficient fine-tuning method that enables models to progressively zoom in and localize small objects through autoregressive prediction. When applied to GroundingDINO, PIZA significantly improves accuracy on the SOREC dataset, achieving 43.1% mAcc on the test-A split while using only 3.5M trainable parameters compared to 173M for full fine-tuning.

## Method Summary
PIZA extends GroundingDINO with a parameter-efficient adapter module that learns autoregressive zooming sequences for small object localization. The method generates ground truth search processes by sampling bounding box sizes from the pre-training distribution, then trains an adapter (LoRA, Adapter+, or CoOp variants) to predict progressively refined bounding boxes. The PIZA module extracts 16-dimensional embeddings from the bounding box sequence using Fourier features and a transformer encoder, conditioning the adapter layers at each zooming step. This enables efficient fine-tuning with only 3.5M parameters while achieving significant accuracy improvements on small objects.

## Key Results
- PIZA-Adapter+ achieves 43.1% mAcc on SOREC test-A using only 3.5M trainable parameters
- Zero-shot GroundingDINO baseline achieves near 0% accuracy on SOREC small objects
- PIZA performs competitively on larger objects in RefCOCO (31.3% mAcc) despite focusing on small objects
- PIZA requires an average of 2.11 zooming steps per prediction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Autoregressive zooming improves small object localization by decomposing a large search space into sequential refinement steps.
- Mechanism: The model predicts a sequence of nested bounding boxes (b₀, b₁, ..., b_T) where each box contains the next. This transforms a single-shot localization problem into a coarse-to-fine search process. The PIZA module learns zooming-step embeddings that condition the frozen backbone model at each iteration.
- Core assumption: Small objects occupy <1% of image area, making single-shot prediction unreliable due to feature resolution limits in pre-trained models.
- Evidence anchors:
  - [abstract]: "PIZA... enables models to progressively zoom in and localize small objects through autoregressive prediction"
  - [section 4.2]: "Zooming in to localize a small target object can be understood as a search problem over an image. We model a search process P as a sequence of bounding boxes"
- Break condition: If target objects occupy >5% of image area, overhead of multi-step inference likely outweighs gains.

### Mechanism 2
- Claim: Zooming-step embeddings injected into adapter layers provide spatial context that bridges the gap between pre-training and small-object fine-tuning.
- Mechanism: The PIZA module extracts a 16-dimensional embedding from the bounding box sequence using learnable Fourier features + transformer encoder. This embedding conditions the adapter layers (via addition in LoRA bottleneck or output scaling in Adapter+), providing explicit progress information at each zooming step.
- Core assumption: Pre-trained models lack representations for the extreme scale differences between pre-training data (normal objects) and target domain (objects at 0.05-0.1% of image).
- Evidence anchors:
  - [section 4.2]: "the PIZA module learns zooming-step embeddings that represent progress of the search process"
  - [section 4.3]: "PIZA-LoRA integrates the embedding h into the bottleneck of LoRA... PIZA-Adapter+ adds the embedding h to the output of the channel-wise scaling layer"
- Break condition: If embedding dimension is reduced below 16 or if progress head is removed without compensating architecture.

### Mechanism 3
- Claim: Training search processes that match pre-training scale distributions enable efficient fine-tuning without full parameter updates.
- Mechanism: Ground truth zooming sequences are generated by sampling bounding box sizes from the pre-training distribution p(r) estimated via KDE on 100K boxes. This aligns the intermediate zoom levels with what the frozen backbone "expects" to see, reducing distribution shift.
- Core assumption: The frozen backbone has useful representations at multiple scales, but only for scale ratios seen during pre-training.
- Evidence anchors:
  - [section 4.4]: "To facilitate efficient fine-tuning, we generate P* so that the distribution of inverse zoom factors... match to the distribution of bounding box area ratios p(r) in pre-training"
  - [Table 6]: Training from scratch achieves 0% accuracy, confirming pre-training is necessary.
- Break condition: If pre-training data has fundamentally different object scale characteristics.

## Foundational Learning

- Concept: **Parameter-efficient fine-tuning (PEFT)**
  - Why needed here: PIZA builds on LoRA, Adapter+, and CoOp. Understanding how these methods inject learnable parameters into frozen backbones is prerequisite.
  - Quick check question: Can you explain why LoRA uses low-rank decomposition (A×B instead of full W update)?

- Concept: **Vision-language grounding architectures**
  - Why needed here: GroundingDINO serves as the backbone. Understanding its cross-attention between text and image features is essential for knowing where to inject PIZA.
  - Quick check question: In GroundingDINO, where does the text-to-image cross-attention occur, and what does it compute?

- Concept: **Autoregressive prediction with early stopping**
  - Why needed here: The EOS head determines when to stop zooming. Understanding when/why to continue vs. terminate is core to inference efficiency.
  - Quick check question: What signal does the EOS head receive, and what would happen if it always predicted [CONT]?

## Architecture Onboarding

- Component map: Image + Text -> Frozen GroundingDINO Backbone -> PIZA Module (Fourier + Transformer) -> Adapter Layers -> Bounding Boxes + EOS
- Critical path:
  1. Input: (image, text expression)
  2. Initialize b₀ = full image
  3. Loop: PIZA module extracts h from b₀:ᵢ → backbone predicts bᵢ₊₁ → EOS head checks termination
  4. Output: Final bounding box b_T
- Design tradeoffs:
  - **Adapter type**: Adapter+ best performance (3.5M params, 43.1% mAcc) vs LoRA (1.5M, 39.3%) vs CoOp (0.9M, 33.4%)
  - **Zooming steps**: Average 2.11 steps optimal; enforcing 3 steps hurts (34.3% vs 36.8% on Val)
  - **Type A vs Type B adapters**: Type A uses progress head value (better for small data); Type B uses raw features (better for large data)
- Failure signatures:
  - Zero-shot baseline near 0% on SOREC → backbone has no small object capability
  - Training from scratch → 0% → pre-training essential
  - Objects near similar objects or occluded → persistent failures (Figure 13)
- First 3 experiments:
  1. **Baseline verification**: Run zero-shot GroundingDINO on SOREC test-A to confirm the problem exists (expect <1% mAcc)
  2. **PIZA-Adapter+ ablation**: Train with/without the PIZA module to isolate the contribution of zooming-step embeddings (expect ~10% mAcc gap per Table 4)
  3. **Step analysis**: Compare enforcing 1, 2, 3 zooming steps to find optimal tradeoff for your target object sizes (use Table 7 protocol)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can PIZA be effectively adapted for video-based referring expression comprehension to leverage temporal continuity?
- Basis in paper: [explicit] The conclusion states that "extending this work to video data and applying PIZA tuning to architectures for video processing would also be a promising next step."
- Why unresolved: The current PIZA module is designed for static images and autoregressive zooming; it is unknown how the iterative cropping mechanism would interact with temporal coherence or video-specific architectures.
- What evidence would resolve it: Successful integration of PIZA into a video-grounding architecture demonstrating maintained or improved accuracy on small objects with temporal consistency.

### Open Question 2
- Question: How does PIZA performance scale when applied to diverse environments beyond autonomous driving scenarios?
- Basis in paper: [explicit] The authors note that "extending the proposed dataset to include more diverse environments and object types would remain an interesting future research direction."
- Why unresolved: The SOREC dataset is limited to driving scenes (road, highway, rural, off-road), leaving the method's robustness in indoor, aerial, or maritime contexts untested.
- What evidence would resolve it: Evaluation of the PIZA-augmented model on a small-object REC dataset comprising diverse non-driving environments.

### Open Question 3
- Question: Does increasing image resolution to 8K or higher require adjustments to the iterative zooming strategy?
- Basis in paper: [explicit] The error analysis in the appendix identifies that "creating datasets with 8K or higher resolution images... is also left for future work."
- Why unresolved: While PIZA handles the current resolution well, objects may become significantly smaller in 8K imagery, potentially requiring more zooming steps or a modified search process ($P$).
- What evidence would resolve it: A comparative study of PIZA's accuracy and required zoom steps on the current SOREC dataset versus a new 8K resolution benchmark.

## Limitations

- Method's reliance on generating ground truth search processes from pre-training distribution statistics introduces uncertainty about generalization to domains with different object scale characteristics
- Limited testing of method's robustness to objects that are similar in appearance or partially occluded
- Significant gap between SOREC and RefCOCO performance suggests potential limitations in cross-dataset generalization

## Confidence

- **High confidence**: The effectiveness of autoregressive zooming for small object localization (43.1% mAcc vs near-zero baseline), and the superiority of PIZA-Adapter+ over other parameter-efficient variants (3.5M params, 43.1% mAcc)
- **Medium confidence**: The specific claim that 16-dimensional Fourier feature embeddings are optimal, and that Type A adapters perform better on small datasets
- **Low confidence**: The claim that distribution-matching of zooming sequences is essential for success

## Next Checks

1. **Cross-dataset robustness test**: Evaluate PIZA on datasets with different object scale distributions (e.g., COCO, Visual Genome) to verify that the distribution-matching assumption holds beyond SOREC and RefCOCO.

2. **Occlusion and similarity analysis**: Create controlled test cases with occluded small objects and objects with similar visual features to quantify performance degradation and identify failure modes beyond scale issues.

3. **Sequence generation ablation**: Compare PIZA performance when using alternative zooming sequence generation strategies (e.g., random sampling, curriculum learning) against the proposed KDE-based method to isolate the contribution of distribution matching.