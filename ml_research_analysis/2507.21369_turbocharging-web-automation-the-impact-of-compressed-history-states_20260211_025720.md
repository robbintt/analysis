---
ver: rpa2
title: 'Turbocharging Web Automation: The Impact of Compressed History States'
arxiv_id: '2507.21369'
source_url: https://arxiv.org/abs/2507.21369
tags:
- history
- inputs
- compressor
- input
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a history compressor module to enhance web
  automation by leveraging historical web states. The compressor distills task-relevant
  information from verbose history states into fixed-length representations, mitigating
  long input sequences and sparse information challenges.
---

# Turbocharging Web Automation: The Impact of Compressed History States

## Quick Facts
- arXiv ID: 2507.21369
- Source URL: https://arxiv.org/abs/2507.21369
- Reference count: 40
- This paper proposes a history compressor module that distills task-relevant information from verbose history states into fixed-length representations, improving web automation accuracy by 1.2-5.4% while reducing memory and latency.

## Executive Summary
This paper addresses the challenge of long input sequences and sparse information in web automation by introducing a history compressor module. The compressor distills task-relevant information from verbose history states into fixed-length representations using cross-attention with learnable queries. Experiments on Mind2Web and WebLINX datasets show 1.2-5.4% absolute accuracy improvements over baseline methods, with significant reductions in GPU memory cost (40.3%), inference latency (26.2%), and input tokens (over 5×) compared to pruning-based compressors.

## Method Summary
The approach uses a history compressor module that transforms variable-length history states into fixed-length representations through cross-attention with learnable queries. The compressor is inspired by Perceiver-style architectures and includes a history fusion module that allows neighboring history inputs to communicate and reduce redundancy. Training follows a two-stage process: first freezing the transformer and training only the compressor with zero-initialized attention, then unfreezing all components for joint fine-tuning. The method is evaluated on Mind2Web and WebLINX datasets, demonstrating improved accuracy while reducing computational costs.

## Key Results
- 1.2-5.4% absolute accuracy improvements over baseline methods on Mind2Web and WebLINX datasets
- 40.3% reduction in GPU memory cost and 26.2% reduction in inference latency compared to pruning-based compressors
- 5× reduction in input tokens while maintaining higher accuracy than zero-shot LLM compressors
- History fusion module improves performance on Cross-Task and Cross-Domain splits but slightly hurts Cross-Website performance

## Why This Works (Mechanism)

### Mechanism 1: Query-Based Cross-Attention Compression
Fixed-length learnable queries distill task-relevant information from verbose history states through cross-attention, reducing variable-length HTML sequences to fixed 256-token representations while preserving task-relevant signals.

### Mechanism 2: Inter-History Fusion for Redundancy Reduction
Different history inputs communicate to reduce redundant information shared among neighboring states, with a fusion module concatenating features and applying dimension reduction.

### Mechanism 3: Two-Stage Training with Zero-Initialized Attention
Progressive alignment between compressed history representations and current input representations stabilizes training by starting with zero-initialized attention and gradually learning attention weights.

## Foundational Learning

- **Cross-Attention in Perceiver-style Architectures**
  - Why needed here: The history compressor is explicitly inspired by Perceiver. Understanding how learnable queries extract information from arbitrary-length inputs through cross-attention is essential to grasping why compression works.
  - Quick check question: Given a query vector q and a set of key-value pairs from a verbose HTML document, how does cross-attention determine which DOM elements to prioritize?

- **Web Automation State Representation**
  - Why needed here: The paper assumes familiarity with what constitutes a "web state" (HTML DOM, optionally screenshots) and why it's verbose. Without this, the motivation for compression is unclear.
  - Quick check question: Why does a typical e-commerce webpage's DOM contain thousands of tokens, and which portions are likely relevant to a "book a hotel" task?

- **Encoder-Decoder Action Prediction**
  - Why needed here: The base model (Flan-T5) uses an encoder-decoder architecture where the encoder receives concatenated inputs and the decoder autoregressively generates actions. Understanding this flow is necessary to see where compressed history fits.
  - Quick check question: In MindAct's formulation, what format does the decoder output—natural language descriptions, coordinate values, or DOM element IDs?

## Architecture Onboarding

- **Component map:**
  History Inputs (×N) → History Compressor (shared weights) → Fixed 256-token reps → Concatenation → Flan-T5 Encoder ← Language Instruction → Flan-T5 Decoder → Action Prediction

- **Critical path:**
  1. Verify history HTML preprocessing matches MindAct's pruning
  2. Ensure learnable query initialization is consistent across all N history compressors
  3. Zero-initialization must be applied to cross-attention layers that consume compressed history in the main transformer

- **Design tradeoffs:**
  - # of queries (compression ratio): 256 tokens chosen via ablation
  - # of history inputs: 5 used, generally more histories = better with diminishing returns
  - Fusion vs. no fusion: Adds ~1.7% accuracy on Cross-Task but slightly hurts Cross-Website
  - Inference overhead: 79% higher latency than no-history baseline

- **Failure signatures:**
  - Training instability (loss spikes early): Likely forgot zero-initialized attention or skipped stage-1 pre-training
  - Accuracy drops vs. baseline: Check that history representations are actually being attended to
  - Cross-Website split underperforms: Fusion may be over-deduplicating
  - GPU OOM with N>3: History compressor adds memory; reduce batch size or query count

- **First 3 experiments:**
  1. Reproduce baseline comparison: Train MindAct (no history) vs. History Compressor (N=5, 256 queries) on Mind2Web Cross-Task split
  2. Ablate history count: Run N∈{0,1,2,3,4,5} to verify monotonic improvement trend
  3. Test fusion impact: Compare with-fusion vs. without-fusion on both Mind2Web splits and WebLINX

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the history compressor approach maintain its effectiveness when applied to larger, stronger transformer models (e.g., Llama 3) and multi-modal architectures that process both text and visual inputs?
- **Open Question 2**: What causes the history fusion module to degrade performance on the Cross-Website split while improving it on Cross-Task and Cross-Domain splits?
- **Open Question 3**: How can the 79% inference latency overhead be reduced while maintaining the accuracy improvements from history compression?

## Limitations
- 79% inference latency overhead compared to no-history baselines despite memory and latency improvements over pruning-based methods
- Performance degradation on Cross-Website split when using history fusion module
- Limited validation to Mind2Web and WebLINX datasets without real-world deployment testing
- Two-stage training with zero-initialized attention adds implementation complexity and requires careful hyperparameter tuning

## Confidence
- **High confidence**: Cross-attention compression effectively reduces variable-length HTML sequences to fixed representations; two-stage training with zero-initialized attention prevents early-stage instability; history fusion reduces redundancy in adjacent history states for same-domain tasks
- **Medium confidence**: 256-query compression ratio is optimal across all task types; progressive alignment mechanism works as described; performance gains translate to real-world deployment
- **Low confidence**: Zero-initialized attention implementation details; history fusion benefits generalize across all task splits; compression maintains task-critical information for all web automation scenarios

## Next Checks
1. **Domain-adaptive compression**: Implement a mechanism to adjust the number of learnable queries based on domain complexity and evaluate whether adaptive compression outperforms the fixed 256-token approach across all splits.
2. **Attention weight analysis**: After training, analyze the learned attention weights to verify that compressed history representations are actually being utilized by the main transformer, checking whether attention scores to history representations increase from zero as intended.
3. **Cross-dataset generalization**: Test the trained history compressor on an external web automation dataset or real-world web tasks not seen during training to measure whether the compression strategy generalizes beyond Mind2Web and WebLINX.