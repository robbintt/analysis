---
ver: rpa2
title: Labels or Preferences? Budget-Constrained Learning with Human Judgments over
  AI-Generated Outputs
arxiv_id: '2601.13458'
source_url: https://arxiv.org/abs/2601.13458
tags:
- data
- pcal
- estimator
- page
- pcal-ca
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the optimal allocation of limited annotation
  budgets between ground-truth labels and pairwise preference labels in AI systems.
  The authors frame the problem as a monotone missing data issue under Missing At
  Random (MAR) assumptions and propose Preference-Calibrated Active Learning (PCAL),
  a method that jointly determines the optimal data acquisition strategy and constructs
  an efficient estimator for the target parameter.
---

# Labels or Preferences? Budget-Constrained Learning with Human Judgments over AI-Generated Outputs

## Quick Facts
- **arXiv ID**: 2601.13458
- **Source URL**: https://arxiv.org/abs/2601.13458
- **Reference count**: 40
- **One-line primary result**: PCAL achieves the semiparametric efficiency bound and robust performance in budget-constrained settings with mixed label types.

## Executive Summary
This paper tackles the optimal allocation of limited annotation budgets between ground-truth labels and pairwise preference labels in AI systems. The authors frame the problem as a monotone missing data issue under Missing At Random (MAR) assumptions and propose Preference-Calibrated Active Learning (PCAL), a method that jointly determines the optimal data acquisition strategy and constructs an efficient estimator for the target parameter. PCAL minimizes the asymptotic variance of the estimator directly, without requiring closed-form solutions, and works under general settings with multiple data types.

Theoretically, the authors prove that PCAL achieves the semiparametric efficiency bound under standard conditions and provides robustness guarantees even when nuisance parameters are poorly estimated. Simulations and real-data analysis on a politeness dataset show that PCAL significantly outperforms baseline methods, producing shorter confidence intervals while maintaining proper coverage. The method offers a principled, statistically efficient approach for budget-constrained learning in modern AI applications.

## Method Summary
PCAL is a two-stage method for budget-constrained learning with mixed label types. In Stage 1, it uses initial labeled data to estimate nuisance functions (ψ₁, ϕⱼ) and numerically optimize propensity scores αⱼ(X,W₁,W₂) that minimize estimator variance under budget constraints. In Stage 2, it collects data according to the learned propensities, constructs a decorrelation matrix M, and computes a final estimator via three-fold cross-fitting with weighted corrections. The method directly optimizes the variance functional rather than requiring closed-form solutions and provides robustness guarantees through the decorrelation matrix.

## Key Results
- PCAL achieves the semiparametric efficiency bound under MAR assumptions with provable theoretical guarantees.
- The method provides robustness to nuisance model misspecification, never performing worse than label-only baselines.
- Real-data experiments on politeness datasets show PCAL produces significantly shorter confidence intervals while maintaining proper coverage compared to baseline methods.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PCAL achieves semiparametric efficiency by deriving the optimal influence function for the monotone missing data structure.
- Mechanism: The paper derives an Efficient Influence Function (EIF) with an inductive structure (Proposition 3.1) that accounts for the hierarchical data availability: fully labeled samples (covariates + outcome + preference), preference-only samples (covariates + preference), and unlabeled samples (covariates only). This EIF structure enables construction of estimators that achieve the semiparametric efficiency bound.
- Core assumption: The missingness mechanism satisfies Missing At Random (MAR), where the probability of data selection depends only on observed covariates, not unobserved outcomes.
- Evidence anchors:
  - [abstract] "Building on this formulation, we introduce Preference-Calibrated Active Learning (PCAL), a novel method that learns the optimal data acquisition strategy and develops a statistically efficient estimator"
  - [section 3.2] Proposition 3.1 provides the explicit EIF form with the inductive structure of ψ₁, ϕ₁, ϕ₂, ϕ₃ components
  - [corpus] No direct corpus neighbors address semiparametric inference in budget-constrained settings; related work focuses on LLM evaluation or general active learning without efficiency guarantees
- Break condition: If the MAR assumption is violated (selection depends on unobserved outcomes), the EIF derivation becomes invalid and estimator consistency is not guaranteed.

### Mechanism 2
- Claim: Variance minimization through direct propensity score optimization yields the optimal budget allocation strategy.
- Mechanism: Rather than deriving closed-form solutions for optimal propensity scores, PCAL expresses the asymptotic variance directly as a functional of multi-dimensional propensity scores αⱼ(x,w₁,w₂) and numerically optimizes this variance under the budget constraint (Algorithm 3). The loss functional L(α) in Equation (4.1) represents this asymptotic variance.
- Core assumption: The black-box algorithms used to estimate nuisance components (ψ₁, ϕⱼ) are stable (converge to fixed limits) but not necessarily consistent for ϕⱼ.
- Evidence anchors:
  - [abstract] "Our flexible framework applies to a general class of problems, by directly optimizing the estimator's variance instead of requiring a closed-form solution"
  - [section 3.3] Algorithm 3 explicitly minimizes Tr(dCov(θ̂_PCAL)) with respect to propensity scores under budget and positivity constraints
  - [section 4.1] Theorem 4.1 provides excess risk bounds for the propensity score estimator
- Break condition: If nuisance function estimates have unbounded variance or the optimization fails to find feasible solutions (e.g., budget τ too small relative to cost structure), the theoretical guarantees may not hold.

### Mechanism 3
- Claim: The decorrelation matrix M ensures robustness to nuisance model misspecification—PCAL never performs worse than the label-only baseline.
- Mechanism: The decorrelation matrix M (Equation 3.5) is constructed to guarantee that when nuisance parameters are poorly estimated, the PCAL estimator's variance is bounded above by the variance of estimators using only ground-truth labels. Corollary 4.2 formalizes this as Tr(Cov(θ̂_PCAL)) ≤ Tr(Cov(θ̂_label-unlabel)) ≤ Tr(Cov(θ̂_label-only)).
- Core assumption: Assumption 3.2 requires ψ₁ to be consistently estimable (but not necessarily ϕⱼ), and the decorrelation matrix estimate M̂ converges to its population counterpart.
- Evidence anchors:
  - [abstract] "establish a key robustness guarantee that ensures robust performance even with poorly estimated nuisance models"
  - [section 4.2.1] Corollary 4.2 explicitly proves Tr(Cov(θ̂_PCAL)) ≤ Tr(Cov(θ̂_label-unlabel))
  - [corpus] Corpus neighbors on robust learning (e.g., Stackelberg Self-Annotation) address robustness via game-theoretic approaches, not semiparametric decorrelation techniques
- Break condition: If ψ₁ cannot be consistently estimated (e.g., labeled sample size n₀ too small), the robustness guarantee degrades; the bound in Corollary 4.2 includes an error term depending on Rademacher complexity and convergence rates.

## Foundational Learning

- **Semiparametric Efficiency Theory**
  - Why needed here: PCAL's theoretical foundation rests on efficient influence functions and semiparametric efficiency bounds. Understanding why the EIF is the projection of any influence function onto the tangent space of the model is essential for interpreting the optimality claims.
  - Quick check question: Can you explain why the EIF is the projection of any influence function onto the tangent space of the model?

- **Missing Data Framework (MAR/MCAR)**
  - Why needed here: The paper formulates budget allocation as a monotone missing data problem. The distinction between MAR (covariate-dependent selection) and MCAR (random selection) determines which variant (PCAL vs. PCAL-CA) applies.
  - Quick check question: In the PCAL framework, if labeling decisions depend only on the covariates X but not on the pseudo-labels W₁, W₂, which assumption is satisfied?

- **Inverse Propensity Weighting (IPW)**
  - Why needed here: The EIF derivation and the weighted estimator in Algorithm 2 rely on IPW to correct for selection bias when complete cases are non-representative of the full population.
  - Quick check question: Why does the positivity constraint α₁(x,w₁,w₂) ≥ α > 0 appear in the optimization, and what happens if it is violated?

## Architecture Onboarding

- **Component map**: Initial labeled data D^(0) → Stage 1 (nuisance estimation + propensity optimization) → Propensity scores αⱼ(X,W₁,W₂) → Stage 2 (data collection + EIF estimator) → Final estimate θ̂_PCAL

- **Critical path**:
  1. Initial labeled data quality → nuisance function estimation stability → propensity score optimization
  2. Budget constraint (τ = B/n_tot) and cost ratio ρ directly determine feasible α region; verify τ > α (minimum labeling rate)
  3. Cross-fitting splits must maintain sufficient labeled samples in each fold for stable ψ₁ estimation
  4. Numerical optimization convergence depends on initial guess and constraint satisfaction

- **Design tradeoffs**:
  - **Covariate-aware vs. covariate-agnostic**: PCAL (Algorithm 3) allows instance-specific allocation but requires estimating α(x,w₁,w₂) functions; PCAL-CA (Algorithm 4) only learns scalar αⱼ values but sacrifices potential efficiency gains from targeting informative samples
  - **Initial labeled data**: Using n₀ samples for Stage 1 means they are not reused in Stage 2; alternative adaptive/sequential designs could mitigate this information loss (mentioned in Discussion)
  - **Neural network depth for nuisance estimation**: Paper uses 3-layer networks for ψ₁, ϕⱼ and 2-layer for propensity scores; deeper networks may improve fit but increase overfitting risk

- **Failure signatures**:
  - Confidence intervals with coverage < 80%: Likely positivity violation (α₁ too small) or poor nuisance estimation; check 1/α̂₁(x,w₁,w₂) for extreme values
  - Optimization returns infeasible solution: Budget τ insufficient relative to minimum labeling constraint; verify τ ≥ ρ·α and adjust α downward if necessary
  - Variance estimates unstable across cross-fitting folds: Insufficient n₁ relative to dimension d; consider reducing cross-folding to 2 folds or increasing labeled pool
  - PCAL underperforms label-only baseline: Violation of Assumption 3.2 (ψ₁ not consistently estimable) or extreme misspecification of ϕⱼ beyond decorrelation matrix's corrective capacity

- **First 3 experiments**:
  1. **Baseline sanity check**: Implement label-only and label-unlabel estimators; verify they achieve ~90% coverage at nominal level before testing PCAL. Use synthetic linear regression setup (Section 5.1) with known θ* to validate implementation.
  2. **Propensity score ablation**: Compare PCAL (covariate-aware) vs. PCAL-CA (covariate-agnostic) across varying cost ratios ρ ∈ {5, 10, 20} and budgets τ; quantify efficiency gains from instance-specific allocation.
  3. **Robustness to nuisance misspecification**: Intentionally use misspecified nuisance models (e.g., linear when true relationship is quadratic) and verify PCAL still matches or exceeds baseline performance per Corollary 4.2; plot excess risk bound vs. theoretical prediction.

## Open Questions the Paper Calls Out
None

## Limitations
- The method requires accurate estimation of nuisance functions and successful numerical optimization of propensity scores, which may be challenging in practice.
- The robustness guarantee depends on consistent estimation of ψ₁, which may not hold with small labeled sample sizes.
- The current framework does not address sequential or adaptive designs where Stage 1 data could be reused in Stage 2.

## Confidence
- **Semiparametric efficiency theory**: High
- **Practical variance reduction**: Medium
- **Robustness to misspecification**: Low

## Next Checks
1. **Implementation validation**: Reproduce the linear regression simulation (d=5, θ*=(0.2,0.4,0.6,0.8,1.0), n₀=2000, n₁=20000) and verify that PCAL achieves the claimed 90% coverage and confidence interval reduction compared to baselines.
2. **Nuisance model sensitivity**: Test PCAL with intentionally misspecified nuisance models (e.g., linear when true relationship is quadratic) to empirically verify the robustness claims and compare against theoretical predictions.
3. **Budget constraint impact**: Systematically vary the budget τ and cost ratio ρ to identify when PCAL fails (e.g., τ too small relative to minimum labeling rate) and validate the conditions for numerical optimization feasibility.