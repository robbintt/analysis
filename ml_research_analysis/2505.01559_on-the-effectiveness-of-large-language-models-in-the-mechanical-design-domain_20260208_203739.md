---
ver: rpa2
title: On the effectiveness of Large Language Models in the mechanical design domain
arxiv_id: '2505.01559'
source_url: https://arxiv.org/abs/2505.01559
tags:
- assembly
- name
- part
- bert
- names
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work explores the effectiveness of large language models
  (LLMs) in understanding domain-specific language used in mechanical engineering.
  The authors leverage the ABC dataset, which contains CAD assemblies with natural
  language part and assembly names, to create two unsupervised tasks: a binary sentence-pair
  classification task and a zero-shot assembly name prediction task.'
---

# On the effectiveness of Large Language Models in the mechanical design domain

## Quick Facts
- arXiv ID: 2505.01559
- Source URL: https://arxiv.org/abs/2505.01559
- Authors: Daniele Grandi; Fabian Riquelme
- Reference count: 40
- Primary result: Fine-tuned BERT achieves 0.62 accuracy on binary sentence-pair classification; contrastive pre-training yields 0.386 top-1 accuracy for zero-shot assembly name prediction

## Executive Summary
This work investigates whether large language models can understand domain-specific terminology in mechanical engineering by leveraging the ABC dataset of CAD assemblies with natural language names. The authors create two unsupervised tasks: binary sentence-pair classification to predict if parts belong to an assembly, and zero-shot assembly name prediction from part lists. They fine-tune BERT with aggressive regularization strategies, achieving modest improvements over baselines. A contrastive pre-training approach for zero-shot retrieval shows promise but still struggles with semantically ambiguous or generic assembly names.

## Method Summary
The authors extract assembly and part names from the ABC dataset (61,725 assemblies after deduplication), clean them with regex, and create two unsupervised tasks. For binary classification, they fine-tune `bert-base-uncased` with increased dropout (0.1), moderate learning rate (0.001), sequence length 256, and added multi-head attention layers. For zero-shot prediction, they contrastively pre-train a text encoder to align assembly and part name embeddings, then use dot product similarity for ranking candidate matches within 100-class batches.

## Key Results
- Fine-tuned BERT achieves 0.62 binary classification accuracy on assembly-part relatedness
- Contrastive pre-training yields 0.386 top-1 accuracy for zero-shot assembly name prediction
- Task 1 regularization strategies (dropout, learning rate, attention layers) mitigate overfitting on unique assembly names
- Sentence construction formatting significantly impacts binary task performance (0.469-0.630 accuracy range)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive pre-training aligns assembly-part embedding spaces for improved zero-shot retrieval.
- Mechanism: A text encoder is trained to minimize embedding distance between semantically related assembly names and their constituent part name descriptions. At inference, dot product similarity over normalized embeddings ranks candidate matches, enabling 100-class zero-shot classification without task-specific heads.
- Core assumption: Assembly names and part lists share learnable semantic correspondence that generalizes to unseen assemblies.
- Evidence anchors:
  - [abstract] "pretrain a text encoder contrastively on part and assembly name pairs, outperforming baselines with a top-1 classification accuracy of 0.386"
  - [section 3.3.1] "the network is trained to create embeddings for both sets of names, such that each assembly name and its corresponding part name embeddings are located near each other"
  - [corpus] Weak direct corpus support; neighbor "A Domain Adaptation of Large Language Models for Classifying Mechanical Assembly Components" addresses related mechanical domain adaptation but not contrastive pre-training specifically.
- Break condition: If assembly names are semantically opaque (e.g., "liams project", part IDs), the learned alignment provides no signal.

### Mechanism 2
- Claim: Aggressive regularization mitigates overfitting when fine-tuning on small, high-cardinality domain corpora.
- Mechanism: Combining increased dropout (0.1–0.3), moderate learning rates (~0.001), additional multi-head attention layers, and appropriate sequence length (256 tokens) reduces memorization of unique assembly names while preserving transferable representations.
- Core assumption: Pre-trained BERT retains sufficient general language understanding that domain-specific fine-tuning requires capacity control rather than additional learned knowledge.
- Evidence anchors:
  - [abstract] "fine-tune BERT with strategies such as modifying learning rates, dropout values, sequence length, and adding a multi-head attention layer, achieving 0.62 accuracy"
  - [section 4.1.2] "Overfitting was a common problem during training... learning rates need to increase along with dropout to contrast this effect"
  - [corpus] No direct corpus evidence for this specific regularization claim in mechanical design; labeling as uncertain.
- Break condition: If the corpus grows significantly or class cardinality drops, these regularization settings may underfit.

### Mechanism 3
- Claim: Auxiliary sentence construction formats affect classification signal quality.
- Mechanism: Structuring input as "An assembly named [A], containing the following parts: [P1, P2...]" and varying how parts are split between sentence pairs changes what relational patterns the model must infer, impacting accuracy from 0.469 to 0.630.
- Core assumption: The task-relevant semantic relationship is distributed across part names and assembly names in a way specific sentence splits can reveal or obscure.
- Evidence anchors:
  - [section 3.2.1] "5 different cases of auxiliary sentence creation... following general structure"
  - [appendix A.1, Table 3] Case 3 (splitting parts in half between sentences) achieves highest accuracy at 0.630
  - [corpus] No corpus papers directly address auxiliary sentence formatting for CAD semantic tasks.
- Break condition: If part names are few or identical to assembly names, sentence construction has minimal effect (task becomes trivial).

## Foundational Learning

- Concept: **Transformer attention and BERT pre-training**
  - Why needed here: The paper modifies BERT's fine-tuning behavior; understanding what BERT already knows (contextualized word vectors, long-distance dependencies) clarifies why regularization helps more than architectural changes.
  - Quick check question: Explain why adding a multi-head attention layer might help a fine-tuned BERT on unique, domain-specific inputs.

- Concept: **Contrastive learning objectives**
  - Why needed here: The zero-shot task uses contrastive pre-training to align embeddings; understanding how positive/negative pairs shape the embedding space is essential for debugging retrieval quality.
  - Quick check question: In contrastive pre-training, what happens if negative pairs are semantically similar to positives?

- Concept: **Zero-shot classification via embedding similarity**
  - Why needed here: The assembly name prediction task treats each batch as a 100-class problem using dot product similarity; this requires understanding normalized embeddings and rank-based evaluation.
  - Quick check question: Why normalize embeddings before computing dot product for similarity ranking?

## Architecture Onboarding

- Component map:
  - ABC dataset → extract assembly/part names → deduplicate → regex clean → 61,725 assemblies
  - Task 1 (Binary Classification): Input → BERT encoder → Multi-head attention layer → Dense classifier → {related, not related}
  - Task 2 (Zero-shot): Text encoder (BERT backbone) → Contrastive pre-training on (assembly, parts) pairs → Embedding extraction → Dot product similarity → Top-k ranking

- Critical path:
  1. Pre-process ABC data (deduplication, cleaning critical for reducing noise)
  2. For Task 1: Fine-tune BERT with regularization (dropout=0.1, lr=0.001, seq_len=256, 8–32 attention heads)
  3. For Task 2: Contrastively pre-train encoder, freeze appropriate layers, tune temperature and output token
  4. Evaluate Task 2 via batched 100-class zero-shot accuracy

- Design tradeoffs:
  - Higher dropout reduces overfitting but may underfit if data increases
  - More attention heads improve long-distance relation capture but add compute
  - Sequence length 256 preserves more signal than 128 but increases memory
  - Assumption: Current tradeoffs optimized for ~60K samples with high uniqueness

- Failure signatures:
  - High train accuracy / low test accuracy → overfitting (increase dropout, reduce epochs)
  - Random-baseline-level zero-shot accuracy (0.01) → encoder not learning alignment (check contrastive loss, temperature)
  - Predictions dominated by part names matching assembly names verbatim → model exploiting trivial patterns
  - Predictions fail on opaque names ("untitled document", part IDs) → inherent data limitation, not model failure

- First 3 experiments:
  1. **Baseline replication**: Run BERT base uncased on both tasks to reproduce 0.058 (zero-shot) and ~0.55 (binary) baselines before modifications.
  2. **Regularization sweep**: Systematically vary dropout (0, 0.1, 0.3) and learning rate (1e-2, 1e-3, 1e-4) on Task 1 to confirm overfitting mitigation pattern.
  3. **Embedding visualization**: After contrastive pre-training, plot cosine similarity matrices for a held-out batch to verify diagonal structure (correct pairings) vs. baseline vertical-line artifacts shown in Figure 3.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can generative models be trained on mechanical design data to accurately output a list of parts given an assembly name and a functional description?
- Basis in paper: [explicit] The authors state, "In the future, we aim to create generative models trained on this type of data that are used to create a list of parts given an assembly name and a description of the function of the assembly."
- Why unresolved: The current work focused strictly on discriminative tasks (binary classification and zero-shot prediction) rather than generative tasks.
- What evidence would resolve it: Successful training and evaluation of a generative transformer that produces valid bill-of-materials lists from functional text prompts.

### Open Question 2
- Question: How does human performance compare to the proposed model on the zero-shot assembly name prediction task?
- Basis in paper: [explicit] The authors note that "a human baseline study would likely shed more light on the failure modes," acknowledging the difficulty of the task even for humans.
- Why unresolved: The study lacks a comparative baseline to determine if the model's 38.6% accuracy is a failure of the architecture or a reflection of the inherent ambiguity/noise in the dataset.
- What evidence would resolve it: A user study where mechanical engineers attempt to match part lists to assembly names, providing a human accuracy ceiling.

### Open Question 3
- Question: Do the unsupervised evaluation tasks devised in this paper transfer effectively to assess the quality of other design corpora?
- Basis in paper: [explicit] The Conclusion suggests that "Future work could leverage the tasks we devised to assess both the quality of other design corpora."
- Why unresolved: The experiments were confined to the ABC dataset, and it is unknown if the specific sentence-pair and zero-shot tasks are relevant for datasets with different labeling conventions.
- What evidence would resolve it: Applying the same binary and zero-shot classification pipelines to alternative datasets like the Fusion Gallery Dataset or ShapeNet.

### Open Question 4
- Question: How can models better distinguish semantic relationships when assemblies contain similarly named parts or identical components used in different contexts?
- Basis in paper: [inferred] The authors identify "failure modes" where "different sets of assembly names could contain similarly named parts" (e.g., distinguishing a 'bed frame' from a 'picnic table' based on parts).
- Why unresolved: The current contrastive approach struggles to disambiguate assemblies when the constituent part names lack unique discriminative power.
- What evidence would resolve it: A model architecture that incorporates 3D geometric data alongside text embeddings to resolve semantic ambiguities.

## Limitations
- The ABC dataset contains significant noise with generic names ("Untitled document") and opaque identifiers that limit semantic learning
- 0.62 binary accuracy and 0.386 zero-shot retrieval represent modest improvements over baselines in a dataset where part names often literally contain assembly names
- Regularization strategies are shown to help but lack systematic ablation studies across different data sizes

## Confidence
- **High confidence**: The general approach of using contrastive pre-training for zero-shot retrieval and regularization for fine-tuning on small, high-cardinality datasets is methodologically sound
- **Medium confidence**: The specific performance numbers (0.62 accuracy, 0.386 top-1) are reproducible within reasonable variation, but their practical significance is limited by dataset constraints
- **Low confidence**: The claim that multi-head attention layers specifically help in this domain is weakly supported

## Next Checks
1. **Dataset filtering validation**: Systematically remove assemblies with generic names ("Untitled", numeric IDs) and re-run both tasks to quantify actual semantic understanding versus trivial pattern matching
2. **Regularization ablation study**: Vary dropout (0.0, 0.1, 0.3, 0.5) and learning rate (1e-4, 1e-3, 1e-2) across a grid search on Task 1 to identify optimal tradeoff
3. **Contrastive learning sensitivity analysis**: Test the zero-shot task with different temperature values (0.01, 0.05, 0.1, 0.2) and batch sizes (32, 64, 128) to verify robustness to hyperparameter variation