---
ver: rpa2
title: 'You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities
  of Multimodal Large Language Models with Answer Extraction'
arxiv_id: '2510.14885'
source_url: https://arxiv.org/abs/2510.14885
tags:
- answer
- choice
- species
- performance
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of evaluating free-form responses
  from multimodal large language models (MLLMs) in fine-grained visual classification
  (FGVC) tasks, where choice sets are large and highly related. It proposes a two-stage
  method called nlg2choice, which first prompts the MLLM for a free-form response,
  then uses text-only constrained decoding to extract the most likely choice from
  the response.
---

# You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction

## Quick Facts
- **arXiv ID**: 2510.14885
- **Source URL**: https://arxiv.org/abs/2510.14885
- **Reference count**: 40
- **Primary result**: nlg2choice improves FGVC accuracy by ~20% on average by extracting answers from free-form MLLM responses

## Executive Summary
This paper addresses the challenge of evaluating free-form responses from multimodal large language models (MLLMs) in fine-grained visual classification (FGVC) tasks, where choice sets are large and highly related. It proposes a two-stage method called nlg2choice, which first prompts the MLLM for a free-form response, then uses text-only constrained decoding to extract the most likely choice from the response. For retrieval settings, an early stopping method improves throughput. Experiments on seven FGVC datasets show that nlg2choice consistently improves classification and retrieval performance over constrained decoding baselines. The method is robust to user variation in prompt phrasing and outperforms approaches that constrain model responses. Answer extraction is shown to be a minor bottleneck compared to the lack of extractable content in free-form responses. Overall, nlg2choice provides a simple, effective way to leverage MLLMs for FGVC tasks without additional training.

## Method Summary
nlg2choice is a two-stage approach for fine-grained visual classification using MLLMs. First, it prompts the MLLM to generate a free-form response to an open-ended question about the image. Second, it applies text-only constrained decoding to extract the most likely class from the response using the dataset's choice set. For retrieval tasks, it uses truncated probability computation with early stopping to improve efficiency. The method is evaluated across seven FGVC datasets using 15 semantically equivalent prompts per task to assess robustness to phrasing variations. It consistently outperforms direct constrained decoding baselines while being more robust to prompt variations.

## Key Results
- nlg2choice improves classification accuracy by ~20% on average across seven FGVC datasets
- The method is robust to prompt phrasing variations, showing more stable performance than constrained decoding
- Answer extraction success rates vary by model (97.93% for Qwen-2.5VL vs 79.02% for Llama-3.2V)
- Truncated probability computation achieves +1362% to +2042% throughput improvement in retrieval settings

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Answer Extraction via Free-Form Then Constrain
Allowing MLLMs to respond freely before extracting choices improves fine-grained classification accuracy over direct constrained decoding. Stage 1 prompts the MLLM with an open-ended question, Stage 2 performs text-only constrained decoding on the free-form response to map it to the valid choice set, without re-accessing the image. This decouples recognition (which benefits from natural language articulation) from schema compliance. The core assumption is that the model's internal representation of the correct answer is more accessible when expressed naturally than when forced into a constrained output space.

### Mechanism 2: Token-Unique Early Stopping for Retrieval Efficiency
Truncated probability computation maintains accuracy while substantially reducing forward passes in large choice set retrieval. When computing choice probabilities, stop after generating tokens that are unique to a single choice. For "Baltimore Oriole," if "altimore" only appears in that choice among all options, subsequent tokens need not be computed. This exploits the lexical structure of fine-grained class names (many share first tokens, fewer share later tokens).

### Mechanism 3: Prompt-Robustness Through Aggregated Evaluation
Averaging performance across semantically equivalent prompt variations provides more reliable model evaluation and reveals that nlg2choice is more robust to user phrasing variation than constrained decoding. Generate 15 semantically equivalent prompts per task, evaluate each, and average. This treats prompt phrasing as a source of randomness. The constrained decoding approach shows higher variance across phrasings; nlg2choice shows more stable performance.

## Foundational Learning

- **Constrained Decoding with LLMs**: Required to understand both what nlg2choice compares against and how Stage 2 extraction works. Given vocabulary ["apple", "apricot", "banana"], if the model generates "a", what tokens remain valid for the next step of constrained generation?
- **Fine-Grained Visual Classification Challenges**: Motivates the entire paper. FGVC involves 100-1000+ highly related classes (e.g., 200 bird species), where naive approaches fail. Why might a model achieve 95% on binary "bird vs not bird" but only 2% on 1486-way bird species classification?
- **Autoregressive Generation and Token Probability**: Required to understand how the retrieval variant computes probabilities, why early stopping works, and what "constrained decoding" actually constrains. In a model with vocabulary size 50k, what does it mean computationally to force the next token to belong to a subset of 5 tokens?

## Architecture Onboarding

- **Component map**: Input: (image, question) → Stage 1: MLLM generates free-form text response (512 max tokens) → Stage 2: Text-only LLM with constrained decoding over choice set → Output: predicted class label OR probability distribution (for retrieval)
- **Critical path**: 1. Implement baseline: constrained decoding directly on MLLM (reproduces "choice" baseline). 2. Implement Stage 1: free-form generation with open-ended prompts. 3. Implement Stage 2: text-only constrained extraction (no image input). 4. Add prompt variation generation and evaluation aggregation. 5. For retrieval: implement truncated probability computation with early stopping.
- **Design tradeoffs**: Accuracy vs. throughput (full "Yes/No" retrieval is most accurate but slowest), prompt openness vs. schema compliance (fully open prompts give highest gains but risk no extractable content), Chain-of-thought (CoT prompting decreases performance on average).
- **Failure signatures**: "Schema failure" in extraction (34.64% of responses), retrieval performance drops on Stanford Cars, Llama-3.2V has significantly lower extraction success (79.02%), 4-way VQA setting: nlg2choice does not consistently improve over A/B/C/D lettering.
- **First 3 experiments**: 1. Reproduce Table 2 on CUB200: compare "choice" baseline vs. "nlg2choice" vs. "nlg2choice open" for Qwen-2.5VL-7B. 2. Ablate prompt constrictiveness: test "no steering" vs. "Answer with {type} only" vs. "Answer from {choice list}". 3. Measure extraction success on 100 samples: manually inspect free-form responses to classify as (a) contains schema-valid answer, (b) contains real answer not in schema, (c) no extractable information.

## Open Questions the Paper Calls Out

### Open Question 1
Does the nlg2choice method scale effectively to multi-label visual classification problems? The current text-only constrained decoding stage is designed to map a free-form response to a single class; multi-label tasks require identifying multiple, non-exclusive concepts within the same generation. Experiments on standard multi-label datasets (e.g., COCO) comparing nlg2choice against binary Yes/No retrieval baselines would resolve this.

### Open Question 2
How does the performance of nlg2choice generalize to larger, proprietary foundation models? Experiments were conducted primarily with medium-sized models (8B-11B) and only used open-source LLMs. It is unclear if the "lack of extractable content" bottleneck is an artifact of smaller model knowledge or a fundamental limitation that persists in SOTA models like GPT-4V.

### Open Question 3
Why does the nlg2choice advantage diminish in low-cardinality (e.g., 4-way) VQA settings? The method appears optimized for large output spaces. In small choice sets, the variance introduced by free-form generation may outweigh the benefits of "speaking freely," suggesting a lower bound on choice-set size for this technique.

## Limitations
- Method effectiveness is fundamentally constrained by MLLM's ability to generate responses containing valid class information (34.64% of responses contain answers outside schema)
- Extraction success rates vary substantially by model (97.93% vs 79.02%), suggesting method may not transfer uniformly across MLLM architectures
- Retrieval efficiency claims based on limited dataset diversity; effectiveness on datasets with different lexical characteristics is unknown

## Confidence
- **High confidence**: Core finding that free-form responses followed by text-only constrained decoding improves accuracy over direct constrained generation is well-supported by experimental results
- **Medium confidence**: Prompt robustness findings are convincing but depend heavily on specific prompt generation methodology used
- **Low confidence**: Retrieval efficiency claims, while impressive in magnitude, are based on limited dataset diversity and may not generalize

## Next Checks
1. **Schema extraction failure analysis**: On a held-out validation set, manually annotate 100 free-form responses to classify extraction failures into three categories: (a) valid schema answer present but extraction failed, (b) real answer not in schema, (c) no extractable information.
2. **Cross-model extraction robustness**: Implement nlg2choice on two additional MLLM architectures (e.g., GPT-4V and Gemini) and measure extraction success rates to test whether effectiveness is model-dependent.
3. **Retrieval efficiency generalization**: Test truncated probability computation on two datasets with contrasting characteristics: one with short, highly overlapping class names and one with long, distinctive names, to characterize robustness to class name structure.