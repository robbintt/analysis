---
ver: rpa2
title: 'Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning
  and Machine Learning Approach'
arxiv_id: '2508.00695'
source_url: https://arxiv.org/abs/2508.00695
tags:
- clinical
- performance
- oversampling
- accuracy
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates machine learning and deep learning models
  for classifying psychiatric clinical notes into Anxiety Disorder and Adjustment
  Disorder categories. Traditional models (Random Forest, SVM, Decision Tree, XGBoost)
  and transformer-based models (DistilBERT, SciBERT) were tested with and without
  oversampling techniques (Random Oversampling, SMOTE) and hyperparameter tuning.
---

# Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach

## Quick Facts
- arXiv ID: 2508.00695
- Source URL: https://arxiv.org/abs/2508.00695
- Reference count: 9
- Primary result: XGBoost and transformer models achieved 96% accuracy and 0.97 F1-Score for binary classification of psychiatric notes

## Executive Summary
This study evaluates machine learning and deep learning models for classifying psychiatric clinical notes into Anxiety Disorder and Adjustment Disorder categories. Traditional models (Random Forest, SVM, Decision Tree, XGBoost) and transformer-based models (DistilBERT, SciBERT) were tested with and without oversampling techniques (Random Oversampling, SMOTE) and hyperparameter tuning. XGBoost and transformer models achieved the highest accuracy of 96% and F1-Score of 0.97. Oversampling had minimal impact overall, except SMOTE showed slight improvements with some models. Hyperparameter tuning significantly enhanced performance, especially for Decision Tree and SVC models. The results demonstrate the effectiveness of AI in psychiatric diagnosis and highlight the importance of model optimization.

## Method Summary
The study used 228 Spanish psychiatric clinical notes from CAULE hospital (2017-2022) with a 600-character minimum threshold. Notes were preprocessed through lowercase conversion, accent removal, Spanish stopword removal, and spaCy lemmatization. A 70/30 stratified train/test split preserved class balance (146 Anxiety Disorder vs 82 Adjustment Disorder). Traditional ML models used TF-IDF vectors while transformers processed raw text. Oversampling conditions included Random Oversampling and SMOTE. Hyperparameter tuning employed 3-fold cross-validation with grid search. Models tested included Random Forest, Decision Tree, SVM, XGBoost, DistilBERT, and SciBERT.

## Key Results
- XGBoost and transformer models (DistilBERT, SciBERT) achieved 96% accuracy and 0.97 F1-Score
- SMOTE showed positive effects specifically with BERT-based models, while Random Oversampling degraded DistilBERT performance to 55%
- Hyperparameter tuning significantly improved accuracy across models, with Decision Tree rising from 93% to 96% and SVC from 70% to 88%
- Oversampling had minimal overall impact except for SMOTE's slight improvements with certain models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hyperparameter optimization significantly improves classification performance, particularly for models with initially suboptimal configurations.
- Mechanism: Grid search over parameter spaces (e.g., C, gamma, kernel for SVC; learning rate, epochs for transformers) finds configurations that better fit the decision boundary specific to the clinical text feature space. Decision Tree splits are optimized via criterion and depth tuning; transformers benefit from learning rate and epoch adjustments.
- Core assumption: The search space contains configurations superior to defaults, and cross-validation (3-fold here) generalizes to unseen data.
- Evidence anchors:
  - [abstract] "Hyperparameter optimization significantly improved accuracy across the models... Decision Tree and eXtreme Gradient Boost models achieved the highest accuracy... 96%"
  - [section 3.4.1] "Decision Tree model experienced a significant boost, with accuracy rising from 93% to 96%... SVC model also demonstrated substantial improvements, with its accuracy increasing from 70% to 88%"
  - [corpus] QI-SMOTE and GK-SMOTE papers confirm hyperparameter sensitivity in medical classification tasks

### Mechanism 2
- Claim: Transformer models pre-trained on domain-relevant corpora capture clinical language patterns more effectively than traditional feature-engineering approaches.
- Mechanism: SciBERT's pre-training on biomedical literature provides contextual embeddings that recognize psychiatric terminology and symptom descriptions; DistilBERT offers similar capacity with reduced parameters. Self-attention captures relationships between clinical entities across note sections.
- Core assumption: Pre-training corpus semantics transfer to Spanish psychiatric emergency notes despite language/domain shift.
- Evidence anchors:
  - [abstract] "DistilBERT and SciBERT models also attained 96% accuracy in the deep learning category"
  - [section 2.2] "SciBERT... specifically trained on scientific literature, including biomedical and computer science articles, which makes it well-suited for handling the specialized language in medical contexts"
  - [corpus] Patient Trajectory Prediction paper confirms transformer effectiveness for clinical notes; MoodAngels framework demonstrates multi-agent transformer approaches for psychiatry

### Mechanism 3
- Claim: SMOTE preserves semantic structure in feature space better than random oversampling for clinical text classification.
- Mechanism: SMOTE synthesizes minority-class examples via interpolation between neighbors in TF-IDF space, potentially creating plausible intermediate clinical feature vectors. Random oversampling duplicates existing notes, which may cause overfitting to repeated patterns without adding decision boundary information.
- Core assumption: Interpolated synthetic samples represent plausible clinical note feature distributions; TF-IDF vectors preserve semantic relationships suitable for interpolation.
- Evidence anchors:
  - [abstract] "The only exception was SMOTE, which showed a positive effect specifically with BERT-based models"
  - [section 3.3.2] "DistilBERT model experienced a significant drop in performance when random oversampling was applied. Its accuracy fell to 55%... On the other hand, SciBERT maintained its strong performance"
  - [section 3.3.3] "SMOTE had a generally positive impact on model performance, especially for Random Forest and Decision Tree"
  - [corpus] QI-SMOTE paper explicitly addresses SMOTE limitations in medical data; GK-SMOTE proposes noise-resilient alternatives

## Foundational Learning

- Concept: **Class imbalance and stratification**
  - Why needed here: Dataset has 146 Anxiety Disorder vs 82 Adjustment Disorder notes (~64:36 ratio). Without stratification, random splits could skew class distribution, misleading accuracy metrics.
  - Quick check question: If your test set has 90% majority class and the model predicts "majority" for all inputs, what is accuracy? (Answer: 90%—but F1 for minority class is 0.)

- Concept: **TF-IDF vectorization for clinical text**
  - Why needed here: Traditional ML models (Random Forest, SVM, XGBoost) require numeric features. TF-IDF converts notes into sparse vectors weighted by term frequency and inverse document frequency.
  - Quick check question: Why might "ansiedad" (anxiety) have lower TF-IDF weight than a rare medication name in this corpus? (Answer: High document frequency reduces IDF weight.)

- Concept: **Transfer learning and fine-tuning for BERT models**
  - Why needed here: SciBERT and DistilBERT are pre-trained on large corpora but must be fine-tuned on the specific classification task. This involves adding a classification head and updating weights via backpropagation on labeled clinical notes.
  - Quick check question: What happens if learning rate is too high during fine-tuning? (Answer: Catastrophic forgetting—pre-trained knowledge is destroyed, performance degrades.)

## Architecture Onboarding

- Component map:
  Raw clinical notes -> Character threshold filter (600 min) -> LLM-based diagnosis extraction (ChatGPT API) -> Manual expert review -> Preprocessing (lowercase, accent removal, stopword removal, lemmatization) -> TF-IDF vectors (for ML) OR raw text (for transformers) -> Model layer (Random Forest, Decision Tree, SVM, XGBoost, DistilBERT, SciBERT) -> Balancing layer (Random Oversampling or SMOTE applied to training split only) -> Optimization layer (Hyperparameter grid search with 3-fold cross-validation)

- Critical path:
  1. Verify 228-note filtered dataset split integrity (stratified 70/30)
  2. Run baseline XGBoost without oversampling—expect ~93-96% accuracy
  3. If using transformers, start with default learning rates (3e-5), batch size 16, 3 epochs
  4. Apply hyperparameter tuning only after baseline is stable

- Design tradeoffs:
  - XGBoost: Fast training (0.103s per config), high accuracy, but requires TF-IDF preprocessing that may lose semantic nuance
  - Transformers: Higher accuracy ceiling (96% achieved), but 65-76s per config and require GPU; SciBERT better for biomedical terminology, DistilBERT faster but may miss domain terms
  - Oversampling: SMOTE helps tree models but adds complexity; Random Oversampling can degrade transformer performance (DistilBERT dropped to 55%)

- Failure signatures:
  - Transformer accuracy near random (50-55%) with Random Oversampling: Likely overfitting to duplicated samples; switch to SMOTE or no oversampling
  - SVC stuck at ~70% accuracy: Check kernel choice (RBF may not suit sparse TF-IDF); ensure hyperparameter tuning is applied
  - Large gap between training and validation accuracy: Reduce model complexity (fewer trees, lower max depth) or increase regularization

- First 3 experiments:
  1. Establish baseline: XGBoost on TF-IDF features, no oversampling, default parameters. Target: >90% accuracy. Verify stratified split preserves class ratios.
  2. Ablate oversampling: Compare XGBoost performance with (a) no oversampling, (b) Random Oversampling, (c) SMOTE. Expect minimal difference for XGBoost per paper findings.
  3. Transformer comparison: Fine-tune SciBERT and DistilBERT with identical hyperparameters (lr=3e-5, batch=16, epochs=3). Compare accuracy and training time. Expect SciBERT to handle psychiatric terminology slightly better.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the inclusion of clinical notes with similar symptomatology but different final diagnoses affect the classification accuracy of the proposed models?
- Basis in paper: [explicit] The authors state that expanding the dataset to include notes with similar symptoms but different diagnoses would provide a "more challenging and realistic classification setting."
- Why unresolved: The current study only utilized two distinct categories (Anxiety Disorder and Adjustment Disorder) and did not test the models on data with high semantic overlap but different diagnostic labels.
- What evidence would resolve it: A follow-up study evaluating the XGBoost or SciBERT models on a multi-class dataset containing disorders with overlapping symptoms (e.g., Major Depressive Disorder vs. Anxiety).

### Open Question 2
- Question: Can cost-sensitive learning or undersampling methods outperform the oversampling techniques (SMOTE, Random Oversampling) tested in this study?
- Basis in paper: [explicit] The paper notes that alternative methods for addressing class imbalance, such as cost-sensitive learning or undersampling, were not explored and could be examined in future research.
- Why unresolved: The study was limited to Random Oversampling and SMOTE, the latter of which showed only minimal improvements, leaving the potential of other balancing techniques unverified.
- What evidence would resolve it: Experiments comparing the F1-scores of cost-sensitive learning approaches against the current SMOTE baselines for the minority class.

### Open Question 3
- Question: Can hybrid models combining transformers and traditional machine learning approaches maintain high accuracy while reducing computational costs?
- Basis in paper: [explicit] The authors suggest exploring hybrid models to address the practical challenges of computational cost and the need for large datasets associated with pure transformer models.
- Why unresolved: While transformers achieved high accuracy (96%), they required significantly more computational time (up to 75s per configuration) compared to traditional models like XGBoost (0.1s).
- What evidence would resolve it: Development and benchmarking of a hybrid architecture that achieves comparable F1-scores (>0.95) with a measurable reduction in training/inference time compared to DistilBERT/SciBERT.

## Limitations

- Private dataset from single hospital (CAULE) limits generalizability across different healthcare systems and patient populations
- Incomplete preprocessing details for traditional ML models, specifically text vectorization methods
- Counterintuitive SMOTE performance gains with BERT-based models that contradict typical oversampling effects on deep learning models

## Confidence

- **High Confidence**: Transformer models (DistilBERT, SciBERT) achieving 96% accuracy - supported by multiple studies showing transformer effectiveness in clinical text classification
- **Medium Confidence**: XGBoost achieving highest accuracy - methodology is clear but results depend on specific dataset characteristics
- **Low Confidence**: SMOTE's positive impact on BERT-based models - counterintuitive result that contradicts typical oversampling effects on deep learning models

## Next Checks

1. Test model performance on a larger, multi-hospital dataset to verify generalizability beyond CAULE hospital notes
2. Compare SMOTE vs Random Oversampling impact on transformers using synthetic clinical text to isolate the mechanism
3. Implement ablation study removing ChatGPT diagnosis extraction step to measure its contribution to final performance