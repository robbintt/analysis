---
ver: rpa2
title: A Grounded Memory System For Smart Personal Assistants
arxiv_id: '2505.06328'
source_url: https://arxiv.org/abs/2505.06328
tags:
- memory
- system
- graph
- retrieval
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of creating a robust, multimodal
  memory system for AI assistants that can ground personal experiences in space and
  time, overcoming limitations of standard RAG systems in handling complex, relational
  information. The core method combines three pillars: grounded perception using VLMs
  for structured entity recognition, a knowledge graph enhanced with vector embeddings
  for rich relational memory representation, and agentic retrieval combining semantic
  search, graph expansion, and query generation.'
---

# A Grounded Memory System For Smart Personal Assistants

## Quick Facts
- arXiv ID: 2505.06328
- Source URL: https://arxiv.org/abs/2505.06328
- Reference count: 23
- Primary result: A multimodal memory system that grounds personal experiences in space and time using VLMs for entity extraction, knowledge graphs enhanced with vector embeddings, and agentic retrieval

## Executive Summary
This paper presents a memory system for AI assistants that addresses the challenge of storing and retrieving multimodal personal experiences with spatiotemporal grounding. The system combines Vision Language Models for structured entity recognition, a knowledge graph enhanced with vector embeddings, and an agentic retrieval system that dynamically selects among semantic search, graph expansion, and query generation tools. The approach overcomes limitations of standard RAG systems by maintaining rich relational information while enabling flexible retrieval across different query types. A prototype implementation with a chat interface demonstrates the system's ability to answer counting, existence, emotional state, and descriptive queries using a real-world video example.

## Method Summary
The method follows a three-phase pipeline: perception, ingestion, and retrieval. During perception, a VLM processes overlapping sequences of video frames (sampled at 3 Hz, captioned every 5th frame) with structured prompts that output entities in [label_x:Type] format for consistent tracking. The ingestion phase extracts agents, objects, and actions from captions, computes embeddings via text-embedding-3-small, and populates a Neo4j graph with Image nodes linked via has-previous (temporal) and has-element (entity) relationships. The retrieval phase uses an LLM agent that dynamically selects among three tools—semantic search, PageRank-based graph expansion, and text2cypher translation—to answer queries through Retrieval Augmented Generation.

## Key Results
- Successfully answers counting queries (e.g., "How many people are in the video?") through text2cypher graph traversal
- Handles existence queries by combining semantic search with graph expansion to find implicit context
- Demonstrates emotional state and descriptive query capabilities through semantic search of caption embeddings
- Shows improved coherence and context-awareness compared to standard RAG approaches in qualitative evaluation

## Why This Works (Mechanism)

### Mechanism 1: Structured Entity Extraction via VLM Labeling
VLM processes overlapping n-frame sequences with structured prompts outputting labeled entities in [label_x:Type] format. Overlapping frames create continuity anchors, allowing consistent entity labels across temporal boundaries. These labels become node identifiers in the knowledge graph. Core assumption: VLMs maintain labeling consistency within sessions; cross-session disambiguation remains unsolved.

### Mechanism 2: Hybrid Graph-Vector Representation
Each MemoryNote becomes a graph node with typed edges (has-element, has-previous) AND stores vector embeddings of caption text. Graph structure captures explicit relationships (who/what/when/where); embeddings capture semantic similarity for fuzzy retrieval. Neo4j serves both functions. Core assumption: dual representations justify query diversity; tradeoff not quantified.

### Mechanism 3: Agentic Tool Selection for Query Routing
LLM agent dynamically selects among three retrieval tools (semantic search, graph expansion, text2cypher). Semantic search handles descriptive queries; graph expansion surfaces implicit context; text2cypher handles counting/existence queries requiring exact traversal. Core assumption: LLM correctly classifies query intent; failure modes not characterized.

## Foundational Learning

- **Knowledge Graphs (nodes, edges, schemas)**: Why needed—entire memory representation is a graph schema with typed nodes (Image, Agent, Object, Action) and relationships (has-element, has-previous). Quick check—Can you sketch how "person lies on sofa" would be represented as nodes and edges?
- **Vector Embeddings & Semantic Similarity**: Why needed—Image captions embedded for semantic search; retrieval depends on cosine similarity between query and note embeddings. Quick check—Why would semantic search fail to answer "How many images are in memory?"
- **GraphRAG vs. Standard RAG**: Why needed—core claim is standard RAG's chunk-based retrieval misses relational structure; GraphRAG addresses this via subgraph retrieval. Quick check—What type of query would standard RAG handle well that GraphRAG might overcomplicate?

## Architecture Onboarding

- **Component map**: Video → Frame Extraction (3 Hz) → VLM Captioning (every 5th frame) → Entity Extraction → [Knowledge Graph + Vector Store] (Neo4j) → Agentic Retrieval (3 tools) → LLM Response
- **Critical path**: VLM caption quality determines entity extraction accuracy, which determines graph connectivity, which determines retrieval quality. Garbage in, garbage out propagates through all three phases.
- **Design tradeoffs**: Frame sampling rate (3 Hz, caption every 5th frame) balances cost vs. temporal resolution—tunable based on use case. PageRank for graph expansion vs. random walk: PageRank prioritizes globally connected nodes; may miss isolated but relevant memories. Single graph database (Neo4j) vs. separate vector store: Simplicity vs. potential scalability limits.
- **Failure signatures**: Entity duplication (same object gets new label each appearance), orphan nodes (entities not linked to any image), tool selection errors (agent calls text2cypher for descriptive queries).
- **First 3 experiments**:
  1. Run identical queries through semantic-search-only RAG and full agentic retrieval; compare answer accuracy on counting, existence, and descriptive queries.
  2. Process a 10-minute video with repeated object appearances; manually verify label consistency across frames.
  3. Disable each retrieval tool individually and measure performance degradation by query type to validate tool-selection logic.

## Open Questions the Paper Calls Out

- **Quantitative comparison to RAG baselines**: The authors state future work should focus on scaling up the system and conducting large-scale evaluations in real-world scenarios. Current work relies on a single video example with qualitative demonstration only. Evidence needed: Benchmark results on large dataset measuring retrieval accuracy against standard RAG and GraphRAG baselines.

- **Long-term entity disambiguation**: The paper identifies challenges in ensuring agents, objects, and actions are consistently recognized across different scenes and timeframes as future work. Current system uses unique labels for sequences but lacks demonstrated mechanism for re-identifying specific entities over long periods. Evidence needed: Evaluation of entity linking consistency in longitudinal data where objects reappear after significant temporal gaps.

- **High-level behavior pattern identification**: The authors note that moving beyond individual observations enables high-level behavior pattern identification by recognizing repetitive actions. Current work demonstrates storage and retrieval of discrete events but does not implement or validate mechanism for aggregating into abstract behavioral patterns. Evidence needed: Successful extraction and summarization of user routines derived from frequency and sequence of stored action nodes.

## Limitations

- Single video demonstration without quantitative performance metrics makes robustness assessment difficult
- Core architectural components proposed and illustrated but not systematically validated
- Exact prompts for VLM labeling and agentic retrieval not specified, limiting reproducibility
- Entity disambiguation across extended timeframes and diverse environments remains unresolved

## Confidence

- **High Confidence**: Overall architectural framework (multimodal perception → graph-vector memory → agentic retrieval) is internally consistent and builds on established GraphRAG concepts
- **Medium Confidence**: Three retrieval tools will function as described given proper implementation, though relative performance and agent's tool-selection accuracy are unverified
- **Low Confidence**: System's behavior at scale (large memory stores, long videos) and ability to handle complex entity disambiguation across extended time periods have not been demonstrated or analyzed

## Next Checks

1. **Baseline comparison**: Implement standard semantic-search-only RAG on same memory store and compare answer accuracy across four query types from Table 1
2. **Entity consistency test**: Process 10+ minute video with repeated appearances of same objects/people and measure label consistency rates across time-separated frames
3. **Retrieval tool ablation**: Disable each of three retrieval tools individually and measure performance degradation by query type to validate agentic tool-selection logic