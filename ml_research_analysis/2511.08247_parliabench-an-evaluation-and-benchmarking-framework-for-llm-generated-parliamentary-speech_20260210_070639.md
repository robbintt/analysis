---
ver: rpa2
title: 'ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary
  Speech'
arxiv_id: '2511.08247'
source_url: https://arxiv.org/abs/2511.08247
tags:
- political
- evaluation
- party
- speech
- parliamentary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces ParliaBench, a benchmark framework for evaluating
  parliamentary speech generation by large language models. The framework addresses
  the gap in political authenticity assessment by combining traditional NLP metrics
  with novel embedding-based measures of ideological alignment.
---

# ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech

## Quick Facts
- **arXiv ID**: 2511.08247
- **Source URL**: https://arxiv.org/abs/2511.08247
- **Reference count**: 0
- **Primary result**: Fine-tuning produces statistically significant improvements across most evaluation metrics, with novel political authenticity metrics demonstrating strong discriminative power for capturing ideological dimensions.

## Executive Summary
This work introduces ParliaBench, a benchmark framework for evaluating parliamentary speech generation by large language models. The framework addresses the gap in political authenticity assessment by combining traditional NLP metrics with novel embedding-based measures of ideological alignment. A dataset of 448k UK parliamentary speeches was curated and five models were fine-tuned, generating 28k speeches. Results show fine-tuning produces statistically significant improvements across most evaluation metrics, with novel political authenticity metrics demonstrating strong discriminative power for capturing ideological dimensions.

## Method Summary
The framework uses QLoRA fine-tuning (r=16, α=16) on 5 base models with UK parliamentary speech data, generating speeches with specified party/topic/orientation context. Evaluation combines computational metrics (PPL, BERTScore, PSA, Party Align) with LLM-judge assessment across 6 dimensions. The novel Political Spectrum Alignment (PSA) metric uses sentence transformer centroids to measure ideological positioning, while Party Alignment measures party-specific vocabulary usage. A 447,778-speech dataset from 2015-2022 was curated with metadata enrichment including EuroVoc topics and political orientation labels.

## Key Results
- Fine-tuning produces statistically significant improvements across 45 of 70 comparisons (p < 0.05) with effect sizes d=0.141-1.221
- Baseline models show strong linguistic quality (Dist-2=0.94, GRUEN=0.58) but lower political authenticity (PSA=0.504, Party Align=0.598)
- Cross-party stability analysis reveals varying performance across orientations (Centre-left: 0.607, Centre-right: 0.551, Far-right: 0.330)
- Fine-tuned models show improved Party Alignment (0.628-0.637) but variable PSA improvements (0.506-0.539)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding centroids capture latent ideological positioning in parliamentary text
- Mechanism: The Political Spectrum Alignment (PSA) metric groups speeches by political orientation (Far-left through Far-right), computes centroid embeddings via sentence transformers, then measures cosine similarity between generated speech and orientation centroids combined with ideological distance penalty (Eq. 2). This operationalizes the RILE left-right scaling methodology (Volkens et al., 2013) in embedding space.
- Core assumption: Ideological positioning distributes systematically in embedding space such that party-specific rhetoric clusters near orientation centroids.
- Evidence anchors: [abstract] "novel embedding-based metrics demonstrating strong discriminative power for capturing ideological dimensions"; [section 4.1.1] "PSA score combines semantic similarity with orientation distance... Perfect ideological alignment approaches 1, while misalignment approaches 0"; [corpus] Rheault and Cochrane (2020) demonstrate embeddings capture ideological positioning in parliamentary text—provides external validation for the approach, though not the specific metric
- Break condition: If party rhetoric diverges significantly from stated ideology, or if centroids from training data don't generalize to new political contexts, the metric would misclassify authentic speech.

### Mechanism 2
- Claim: Parameter-efficient fine-tuning (QLoRA) transfers parliamentary conventions without catastrophic forgetting
- Mechanism: Low-rank adaptation (r=16, α=16) modifies only ~0.1-1% of parameters while learning domain-specific vocabulary, party-specific phrasing, and parliamentary structural conventions. The constrained adaptation space may prevent overfitting to surface patterns.
- Core assumption: Parliamentary speech generation requires both general language capability and domain-specific conventions that can be captured in a low-dimensional subspace.
- Evidence anchors: [abstract] "fine-tuning produces statistically significant improvements across most evaluation metrics"; [section 6.1] "45 out of 70 comparisons showed statistically significant improvements" with effect sizes d=0.141-1.221; [corpus] No direct corpus validation for QLoRA specifically on political text; assumption requires external verification
- Break condition: If parliamentary conventions require modifying core reasoning rather than surface patterns, low-rank adaptation would be insufficient. Models may also overfit to dominant parties (Conservative: 58.9%, Labour: 24.3% of data).

### Mechanism 3
- Claim: Independent LLM-judge provides context-sensitive evaluation beyond token-level metrics
- Mechanism: Flow-Judge-v0.1 (Phi-3.5-mini instruct architecture) evaluates across 6 dimensions using structured prompts with 10-point rubrics. Architectural independence from evaluated models prevents self-preference bias. The judge assesses qualities requiring political context understanding (e.g., "Does this sound like real parliamentary speech or AI-generated text?").
- Core assumption: LLM-judge can reliably assess political authenticity and parliamentary conventions despite not being trained specifically for UK parliamentary evaluation.
- Evidence anchors: [section 4] "LLM-judge metrics capture nuanced qualities requiring contextual understanding"; [section 10.1] Judge uses "structured prompt template with explicit evaluation criteria, a 10-point scoring rubric"; [corpus] Zheng et al. (2023) show LLM-as-judge achieves >80% agreement with human evaluators on complex tasks—supports methodology but not domain-specific validity
- Break condition: If judge model has systematic biases toward certain rhetorical styles or parties, scores would reflect judge preferences rather than authentic parliamentary quality. Single-judge design acknowledged as limitation.

## Foundational Learning

- Concept: **Embedding Space Geometry for Political Analysis**
  - Why needed here: PSA and Party Alignment metrics assume political ideology maps to spatial relationships in embedding space. Without understanding how sentence transformers encode semantic/ideological content, you cannot debug metric failures.
  - Quick check question: If Conservative speeches cluster closer to Labour centroids than expected, what does this imply about either the embedding model or party rhetoric?

- Concept: **Statistical Significance vs. Practical Significance in NLP Evaluation**
  - Why needed here: The paper reports "statistically significant improvements" (p < 0.05) with effect sizes ranging from small (d=0.141) to very large (d=1.221). Understanding Cohen's d is essential for interpreting whether improvements matter in practice.
  - Quick check question: A metric improves from 0.500 to 0.505 with p < 0.001. Is this practically meaningful? What additional information do you need?

- Concept: **Cross-Context Stability (Coefficient of Variation)**
  - Why needed here: The framework uses CV-based stability scores (Eq. 4) to measure reliability across political contexts. High performance with low stability indicates brittle models.
  - Quick check question: Model A scores 0.60 ± 0.15 across parties; Model B scores 0.55 ± 0.03. Which is preferable for deployment? Why?

## Architecture Onboarding

- Component map:
  ParlaMint XML -> metadata extraction -> temporal alignment -> EuroVoc classification -> training JSONL
  Base model (5 options) -> 4-bit quantization -> QLoRA adapters (r=16) -> SFT on parliamentary speech
  Generated speech -> computational metrics (PPL, BERTScore, PSA, Party Align) + LLM-judge (6 dimensions) -> cross-context stability analysis

- Critical path:
  1. Dataset quality (temporal alignment accuracy, EuroVoc classification) -> model training
  2. QLoRA rank/alpha selection -> adaptation capacity vs. overfitting
  3. Judge model independence -> evaluation validity

- Design tradeoffs:
  - **Single-judge vs. multi-judge**: Paper uses single judge (Flow-Judge-v0.1) for consistency; tradeoff is potential systematic bias
  - **Broad party coverage vs. data quality**: 1000-speech threshold reduced affiliations from 28 to 11; minor parties excluded but data quality improved
  - **Reference-based vs. reference-free metrics**: BERTScore/MoverScore use top-5 human references; PSA/Party Align are reference-free

- Failure signatures:
  - **Low PSA with high Party Align**: Model learned party vocabulary but not ideological positioning (or vice versa)
  - **High cross-party variance**: Model overfits to majority parties; check training data distribution
  - **Judge scores inconsistent with computational metrics**: Potential judge bias or metric misalignment

- First 3 experiments:
  1. **Baseline sanity check**: Generate 10 speeches per party with baseline vs. fine-tuned models; manually inspect for obvious failures (template leakage, incoherence)
  2. **Metric correlation analysis**: Correlate PSA/Party Align with LLM-judge authenticity scores; low correlation suggests metrics measure different constructs
  3. **Cross-party generalization**: Train on Conservative+Labour only, evaluate on SNP/LibDem; measure performance drop to assess transfer learning capacity

## Open Questions the Paper Calls Out

- How do ParliaBench's novel metrics (PSA and Party Align) correlate with human expert judgments of political authenticity across different parliamentary systems? [explicit] Authors explicitly list "human evaluation protocols for validation" as a future direction; limitations state "our methods rely entirely on automated metrics without human validation." Why unresolved: The LLM-as-a-Judge validation was performed by a single model (Flow-Judge-v0.1), and the authors acknowledge this "introduces bias through single model judgment." What evidence would resolve it: Correlation analysis between automated PSA/Party Align scores and human parliamentary speech experts' ratings across a stratified sample of generated speeches.

- Do the Political Spectrum Alignment and Party Alignment metrics transfer effectively to other parliamentary systems beyond the UK? [explicit] Authors explicitly list "multilingual evaluation for European parliamentary systems" as an extension direction. Why unresolved: The dataset and all experiments focused exclusively on UK Parliament; different parliamentary traditions may exhibit distinct rhetorical patterns, ideological mappings, and party structures. What evidence would resolve it: Replication of the PSA and Party Align methodology on ParlaMint corpora from other European parliaments (e.g., German Bundestag, French Assemblée) with comparable discrimination power (p < 0.001).

- Can the evaluation framework be extended to assess argument structure and factual accuracy against parliamentary records? [explicit] Limitations section states "Our evaluation measures linguistic quality and political authenticity but does not assess argument structure or verify factual accuracy against parliamentary records." Why unresolved: Current metrics (computational and LLM-judge) focus on style, coherence, and ideological alignment but not on logical argumentation or factuality. What evidence would resolve it: Development and validation of new metrics that correlate with expert-coded argument structure quality and retrieval-augmented fact-checking against parliamentary records.

## Limitations
- The novel embedding-based political authenticity metrics (PSA, Party Alignment) lack independent validation despite theoretical grounding in ideological positioning research
- Single-judge evaluation introduces potential systematic bias that cannot be quantified without alternative judge models or human validation
- Training data imbalance toward Conservative and Labour parties may cause overfitting to dominant party rhetoric and poor generalization to minor parties

## Confidence

**High confidence**: The dataset construction methodology (ParlaMint XML parsing, metadata extraction, EuroVoc classification) is well-specified and reproducible. The QLoRA fine-tuning implementation with clear hyperparameters is standard practice with established literature support.

**Medium confidence**: The core evaluation framework combining computational metrics with LLM-judge assessment follows established benchmarking practices. The statistical significance testing (p < 0.05) and effect size reporting (Cohen's d) provide rigorous quantitative support for findings.

**Low confidence**: The novel embedding-based political authenticity metrics (PSA, Party Alignment) lack independent validation. While the approach is theoretically grounded in ideological positioning research, practical performance requires empirical verification beyond the reported results.

## Next Checks

1. **Human validation study**: Recruit political science experts to evaluate 100 generated speeches across party affiliations, comparing expert assessments with LLM-judge scores and computational metrics. This would validate whether the framework captures authentic parliamentary quality from domain expert perspective.

2. **Cross-lingual transfer evaluation**: Test model generalization by training on UK parliamentary data and evaluating on parliamentary speeches from other countries (e.g., US Congress, German Bundestag). This would assess whether the framework captures universal parliamentary conventions versus UK-specific patterns.

3. **Metric ablation study**: Systematically disable each novel metric (PSA, Party Alignment) and measure impact on overall evaluation quality. Compare framework performance against baseline computational metrics alone to quantify the added value of political authenticity assessment.