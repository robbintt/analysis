---
ver: rpa2
title: 'Corrective In-Context Learning: Evaluating Self-Correction in Large Language
  Models'
arxiv_id: '2503.16022'
source_url: https://arxiv.org/abs/2503.16022
tags:
- label
- examples
- cicl
- text
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Corrective In-Context Learning: Evaluating Self-Correction in Large Language Models

## Quick Facts
- arXiv ID: 2503.16022
- Source URL: https://arxiv.org/abs/2503.16022
- Authors: Mario Sanz-Guerrero; Katharina von der Wense
- Reference count: 40
- Primary result: CICL consistently underperforms standard ICL across all tested models and datasets

## Executive Summary
This paper investigates whether corrective in-context learning (CICL) can improve text classification performance by presenting models with predicted labels alongside ground-truth corrections. The study systematically tests CICL across 17 text classification datasets using four different LLM architectures (6B-8B parameters). Contrary to expectations, CICL consistently underperforms standard ICL, with performance degradation becoming statistically significant at 25% correction proportion and worsening monotonically as corrections increase. The findings suggest that introducing corrected labels disrupts rather than refines the model's task understanding, challenging assumptions about self-correction capabilities in LLMs.

## Method Summary
The study compares standard ICL against CICL across 17 text classification datasets. In standard ICL, models predict labels from k=8 few-shot examples with ground truth labels. In CICL, models first generate predictions for the k examples using leave-one-out ICL, then construct prompts with triplets (text, predicted_label, correct_label), and finally predict corrected labels for test inputs. Correction proportions vary from 0% to 100% in 25% increments. Models tested include Llama-3.1-8B, Mistral 7B v0.3, Qwen2.5-7B, and GPT-J-6B, with 5 random seeds per configuration.

## Key Results
- CICL consistently underperforms standard ICL across all tested models and datasets
- Performance degradation becomes statistically significant at 25% correction proportion (Wilcoxon test, p < 0.01)
- Macro-F1 declines monotonically as correction proportion increases from 0% to 100%
- Including harder examples in standard ICL does not improve performance

## Why This Works (Mechanism)

### Mechanism 1: Standard In-Context Learning Pattern Recognition
- Claim: ICL enables task performance by conditioning on labeled examples without parameter updates
- Mechanism: The model infers input-label mapping patterns from few-shot examples in the prompt and generalizes to unseen queries by applying these patterns to new inputs
- Core assumption: Few-shot examples encapsulate task-relevant patterns that the model can extract and apply
- Evidence anchors:
  - [abstract] "ICL allows models to make predictions based on a small number of examples presented in the prompt, effectively transforming LLMs into flexible tools for few-shot learning."
  - [section 3.1] "ICL operates under the assumption that the few-shot examples encapsulate task-relevant patterns, allowing the model to generalize to unseen queries."
  - [corpus] Related work on ICL mechanisms (e.g., "Counting Hypothesis: Potential Mechanism of In-Context Learning") explores pattern recognition but does not address corrective feedback specifically
- Break condition: When examples contain inconsistent patterns, ambiguous labels, or—as this paper shows—conflicting predicted/correct label pairs

### Mechanism 2: CICL Disruption Through Label Inconsistency
- Claim: Presenting predicted labels alongside ground-truth corrections disrupts task understanding rather than improving it
- Mechanism: When few-shot examples show predicted labels that differ from correct labels, the model receives contradictory signals about the input-label mapping, degrading its ability to infer consistent patterns
- Core assumption: LLMs cannot effectively learn error-correction patterns in a single forward pass; they may treat the mismatched labels as noise rather than informative feedback
- Evidence anchors:
  - [abstract] "CICL introduces confusion by disrupting the model's task understanding, rather than refining its predictions."
  - [section 4.2] "Swapping (correcting) labels in the few-shot examples appears to disrupt the model's internal representations, making it harder to generalize and refine predictions."
  - [corpus] Related work (Monea et al., 2024, cited in paper) shows LLMs struggle with binary reward signals; limited direct corpus evidence on explicit correction formats
- Break condition: Performance degradation becomes statistically significant at 25% correction proportion and worsens monotonically as corrections increase (Wilcoxon test, p < 0.01)

### Mechanism 3: Example Difficulty Non-Effect
- Claim: Including harder examples (those the model initially misclassified) does not improve standard ICL performance
- Mechanism: LLMs do not automatically recalibrate decision boundaries when exposed to challenging examples; difficulty alone does not provide discriminative signal for pattern learning
- Core assumption: Curating examples by difficulty does not necessarily select examples that clarify decision boundaries
- Evidence anchors:
  - [abstract] "presenting harder examples in standard ICL does not improve performance, suggesting that example difficulty alone may not be a reliable criterion for effective selection."
  - [section 5] "including harder examples in the few-shot context fails to yield any consistent improvement... Kruskal-Wallis test reveals no significant variation in standard ICL performance across different corrected proportions."
  - [corpus] No direct corpus evidence on example difficulty mechanisms in ICL
- Break condition: N/A—this is a negative finding; the mechanism that "might have worked" does not activate

## Foundational Learning

- **In-Context Learning (Few-Shot Prompting)**:
  - Why needed here: CICL builds on ICL; understanding how models generalize from demonstration examples without weight updates is prerequisite to analyzing why corrections fail
  - Quick check question: Given a prompt with 8 labeled sentiment examples, how does the model predict the label for a new sentence without any parameter changes?

- **Self-Correction Paradigms in LLMs**:
  - Why needed here: The paper tests a specific form of self-correction (in-context corrections); knowing that prior approaches require multi-step refinement or fine-tuning contextualizes why single-pass correction may fail
  - Quick check question: What are two existing approaches to LLM self-correction, and do they require parameter updates?

- **Prompt Sensitivity and Example Selection**:
  - Why needed here: ICL performance is known to depend on example quality, ordering, and selection; understanding this sensitivity explains why introducing incorrect predictions into the prompt can destabilize performance
  - Quick check question: If you shuffle the order of few-shot examples in a prompt, would you expect identical predictions? Why or why not?

## Architecture Onboarding

- **Component map**: Few-shot examples (x_i, y_i) → LLM → Prediction ŷ → Build CICL prompt with triplets (x_i, ŷ_i, y_i) → LLM → Corrected label ỹ

- **Critical path**: Initial ICL prediction generation → feedback triplet construction → corrective prediction. The paper shows Step 3 consistently underperforms simply returning the Step 1 prediction.

- **Design tradeoffs**:
  - CICL requires 2× forward passes (computational overhead) while delivering worse performance
  - Higher correction proportions provide more "feedback" but cause greater confusion
  - Prompt format changes (adding "Predicted label:" / "Correct label:") increase token count and introduce potential format biases

- **Failure signatures**:
  - Macro-F1 declines monotonically as correction proportion increases (0% → 100%)
  - At 100% corrections, models often collapse to near-random performance (e.g., SST-2: 12.9% vs. 91.6% baseline for Llama-3.1)
  - High variance across seeds at high correction proportions indicates unstable task understanding

- **First 3 experiments**:
  1. **Baseline verification**: Reproduce standard ICL vs. CICL on 2–3 datasets from Table 3 with k=8, measuring macro-F1 at 0%, 50%, and 100% correction proportions to confirm the degradation pattern
  2. **Prompt format ablation**: Test whether the degradation is due to the correction signal or simply the longer/more complex prompt by comparing CICL against a control prompt with extended formatting but no label conflicts
  3. **Alternative correction framing**: Instead of "Predicted label: X / Correct label: Y," try a narrative format (e.g., "The model thought X, but the correct answer is Y") to test whether framing affects confusion levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would CICL be effective for tasks requiring multi-step reasoning (e.g., chain-of-thought prompting) where models could benefit from explicit corrective feedback on intermediate reasoning steps?
- Basis in paper: [explicit] The authors state: "CICL may be more effective for tasks requiring multi-step reasoning, e.g., via chain-of-thought prompting, where the model can benefit from explicit corrective feedback to refine intermediate steps."
- Why unresolved: The experiments were limited to text classification tasks only, which don't involve multi-step reasoning
- What evidence would resolve it: Experiments applying CICL to reasoning tasks (e.g., math word problems, logical reasoning) comparing standard ICL with CoT against CICL with corrective feedback on reasoning steps

### Open Question 2
- Question: Would larger, more capable models (beyond 70B parameters) exhibit better self-correction capabilities through CICL?
- Basis in paper: [explicit] "While preliminary experiments with the 70B version of Llama-3.1 yielded similar results to the smaller models, the impact of even larger models remains an open question."
- Why unresolved: Computational constraints limited experiments to small-scale open-source LLMs (6B-8B), with only preliminary tests on 70B models
- What evidence would resolve it: Systematic evaluation of CICL across a range of model sizes (including frontier models like GPT-4) to identify any scale-dependent emergence of self-correction capability

### Open Question 3
- Question: What alternative prompt formats or corrective feedback structures might enable effective self-correction in ICL?
- Basis in paper: [explicit] "LLMs are highly sensitive to prompt design. It is possible that alternative prompt formats or different ways of structuring corrective feedback could lead to better results."
- Why unresolved: The study used only one simple prompt format showing predicted and correct labels without exploring variations in feedback presentation
- What evidence would resolve it: Ablation studies testing different corrective prompt structures (e.g., explanations of errors, contrastive examples, natural language feedback) to identify whether format affects CICL efficacy

## Limitations

- Evaluation limited to English text classification tasks, leaving open questions about other modalities or languages
- Focus on relatively small decoder-only models (≤8B parameters), with only preliminary tests on 70B models
- Single prompt format used for corrections, not exploring alternative ways to present corrective feedback

## Confidence

- CICL consistently underperforms standard ICL: **High**
- Degradation increases monotonically with correction proportion: **High**
- Example difficulty does not improve ICL performance: **Medium** (limited exploration of difficulty metrics)
- Standard ICL pattern recognition mechanism explains results: **Medium** (indirect evidence)

## Next Checks

1. **Cross-modal validation**: Test CICL on vision-language classification tasks to determine if the failure pattern extends beyond text
2. **Prompt format ablation**: Systematically vary correction presentation formats (narrative vs. explicit labels) to isolate whether format complexity or label inconsistency drives performance degradation
3. **Larger model evaluation**: Assess CICL on frontier models (1B+ parameters) to test whether scale affects the model's ability to process corrective feedback