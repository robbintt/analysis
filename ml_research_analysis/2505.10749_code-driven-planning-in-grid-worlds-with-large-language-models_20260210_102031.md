---
ver: rpa2
title: Code-Driven Planning in Grid Worlds with Large Language Models
arxiv_id: '2505.10749'
source_url: https://arxiv.org/abs/2505.10749
tags:
- actions
- grid
- direction
- move
- current
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an iterative programmatic planning (IPP) framework
  that uses large language models (LLMs) to generate interpretable Python programs
  as action policies for solving grid-based planning tasks. Instead of directly generating
  action sequences, IPP synthesizes executable code that maps environment states to
  action sequences and improves these programs through an iterative refinement process
  based on performance feedback.
---

# Code-Driven Planning in Grid Worlds with Large Language Models

## Quick Facts
- arXiv ID: 2505.10749
- Source URL: https://arxiv.org/abs/2505.10749
- Reference count: 40
- This paper proposes an iterative programmatic planning (IPP) framework that uses large language models (LLMs) to generate interpretable Python programs as action policies for solving grid-based planning tasks, showing improvements over direct code generation ranging from 10% to 10× across five of six models.

## Executive Summary
This paper introduces Iterative Programmatic Planning (IPP), a framework that uses large language models to generate executable Python code as action policies for grid-based planning tasks rather than direct action sequences. The approach synthesizes interpretable code that maps environment states to action sequences and improves these programs through iterative refinement based on performance feedback. Evaluated on GRASP and MiniGrid benchmarks with six leading LLMs, IPP established a new state-of-the-art result for GRASP and significantly outperformed direct solution elicitation, particularly for GPT-o3-mini (63% improvement on MiniGrid to 116% on GRASP). While code generation has higher initial prompting costs, the reusable nature of synthesized programs makes the amortized cost substantially lower than per-instance inference methods.

## Method Summary
The Iterative Programmatic Planning (IPP) framework generates Python policies that map observations to actions, then refines them through execution feedback. Starting with Direct Generation, the LLM outputs a `solve()` function. The code executes on training instances, and if performance improves, the three worst-performing cases are fed back for refinement. This loop continues until performance plateaus. The approach uses structured scaffolding (pseudocode templates, step-by-step constraints) to guide synthesis. Key tasks include GRASP (energy collection) and MiniGrid (object interaction), with models including GPT-4o, GPT-o1, GPT-o3-mini, Claude 3.7, Gemini 2.5 Pro, and DeepSeek-R1. Performance metrics are average energy collected (GRASP) and reward/completion rate (MiniGrid).

## Key Results
- IPP achieved 63% improvement on MiniGrid and 116% improvement on GRASP compared to direct solution elicitation from GPT-o3-mini
- Established new state-of-the-art result for GRASP benchmark
- Five of six tested LLMs showed performance improvements ranging from 10% to 10× over direct code generation
- Code reuse across instances reduced amortized costs despite higher initial generation overhead
- DeepSeek-R1 uniquely showed performance degradation (up to -20%) during iterative refinement

## Why This Works (Mechanism)

### Mechanism 1: Program Synthesis as Generalizable State-Mapping
Generating executable code yields better generalization than direct action sequence prediction because code supports control flow (loops, conditionals), allowing policies to handle varying initial states without re-prompting. The core assumption is that LLMs' code generation capabilities are stronger than their ability to simulate long-horizon state transitions in natural language. This breaks if tasks require intuition or logic not easily expressed algorithmically.

### Mechanism 2: Iterative Refinement as Non-Gradient Optimization
The execution-feedback loop allows LLMs to correct logical errors that persist in zero-shot generation by identifying worst-performing instances and revising code based on specific failure cases. This acts as discrete, non-gradient search over program space. The assumption is that LLMs can diagnose why programs failed based solely on task description and failure metrics. This breaks if feedback is too sparse or LLMs lack reasoning capacity to localize bugs.

### Mechanism 3: Structured Scaffolding (Curriculum & Pseudocode)
Providing algorithmic priors or incrementally increasing complexity improves policy synthesis. Pseudocode Extension seeds LLMs with high-level sketches, narrowing search space, while Step-by-Step prompting introduces constraints gradually. The assumption is that LLMs perform better adapting existing structures than synthesizing complex solutions from scratch. This breaks if provided pseudocode is suboptimal, constraining LLMs to local optima.

## Foundational Learning

- **Policy vs. Plan**: A plan is a sequence of actions valid for one instance; a policy is a universal rule (code) valid for all instances. Quick check: Can you distinguish between an output `["LEFT", "TAKE"]` and a Python function `def act(state): ...`?

- **BFS (Breadth-First Search)**: Generated code often implements BFS to navigate grids. Understanding pathfinding is necessary to debug LLM's synthesized logic. Quick check: Can you explain how BFS finds the shortest path in a grid with obstacles?

- **Zero-Shot vs. Amortized Cost**: The framework is cost-effective only if synthesized code is reused. Single-shot generation is expensive ($0.08) compared to direct prompting ($0.002), requiring amortization mindset. Quick check: How many instance runs are required before fixed cost of code synthesis becomes cheaper than per-instance prompting?

## Architecture Onboarding

- **Component map**: Prompt Constructor -> Policy Generator (LLM) -> Execution Sandbox -> Feedback Selector -> Prompt Constructor
- **Critical path**: The Feedback Selector. If selected failure cases don't highlight root cause of logic error, LLM will hallucinate fixes or degrade performance.
- **Design tradeoffs**: Interpretability vs. Optimality (readable code but common logic bugs requiring iterative fixing); Initial Cost vs. Scale (high upfront latency/cost for first instance, near-zero marginal cost for subsequent instances)
- **Failure signatures**: Performance Degradation (specific models like DeepSeek-R1 show worse performance after IR); Negative Scores on GRASP (indicates policy ignored step costs or failed to return to drop zone)
- **First 3 experiments**: 
  1. Baseline Sanity Check: Run Direct Generation on "Unlock" task (MiniGrid) to verify setup; GPT-4o should yield 0.0 reward while GPT-o1 should hit ~0.97
  2. IR Ablation: Run full IR loop on "Unlock-Pickup" task; verify code evolves from failing to pick up key to successfully managing state
  3. Cost Analysis: Calculate crossover point; run synthesized code on 1,000 GRASP instances and compare total cost against running Chain-of-Thought on those same 1,000 instances

## Open Questions the Paper Calls Out

- **Open Question 1**: Why does Iterative Refinement cause performance degradation in specific models like DeepSeek-R1 and GPT-4o while significantly improving others? The paper documents performance gaps but doesn't investigate architectural or training differences that make some models resistant to feedback-driven refinement. Resolution would require comparative analysis of code deltas during refinement for "improving" vs. "degrading" models.

- **Open Question 2**: Can the efficiency of the iterative process be improved by providing more granular execution feedback rather than aggregate performance metrics? The current implementation relies on aggregate task metrics rather than symbolic execution traces or localized failure signals. Resolution would require ablation study comparing current feedback against line-level error traces.

- **Open Question 3**: How does the IPP framework perform in environments with partial observability or stochastic transition functions? The evaluation is restricted to deterministic, fully observable grid-based benchmarks. Resolution would require evaluating on partially observable benchmarks like VizDoom or NetHack.

## Limitations
- Refinement mechanism's stability varies significantly across LLMs, with some models showing performance degradation after iterative feedback, suggesting feedback loop may introduce harmful bias or overfitting
- Paper lacks detailed specification of critical implementation parameters including decoding strategies, exact training set sizes per iteration, and context window management for long refinement chains
- Cost-benefit analysis assumes code reuse across many instances, but optimal amortization threshold and real-world reuse scenarios are not empirically validated

## Confidence
- **High confidence**: Core mechanism that synthesizing executable code yields better generalization than direct action sequence prediction (supported by consistent performance improvements across multiple models and benchmarks)
- **Medium confidence**: Iterative refinement process improves policies for most models, though with notable exceptions and performance instability for certain LLMs
- **Medium confidence**: Amortized cost advantage over per-instance inference, pending validation of actual reuse patterns and implementation details

## Next Checks
1. Implement controlled ablation studies comparing performance with and without pseudocode scaffolding across different task complexities to quantify its contribution to success
2. Conduct stress tests of the refinement loop by introducing adversarial failure cases to determine if the LLM can correctly localize and fix logical errors versus hallucinating solutions
3. Measure the actual crossover point for amortized cost savings by deploying the synthesized policies on 10,000+ instances and comparing total costs against various inference-based approaches under realistic usage patterns