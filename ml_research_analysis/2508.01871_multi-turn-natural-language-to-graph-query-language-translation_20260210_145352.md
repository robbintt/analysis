---
ver: rpa2
title: Multi-turn Natural Language to Graph Query Language Translation
arxiv_id: '2508.01871'
source_url: https://arxiv.org/abs/2508.01871
tags:
- question
- dataset
- multi-turn
- data
- nl2gql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of translating multi-turn natural
  language queries into graph query language (NL2GQL) by proposing a dependency-aware
  framework for constructing multi-turn NL2GQL datasets and evaluating baseline methods
  on this task. The authors introduce MTGQL, the first multi-turn NL2GQL dataset,
  constructed using a framework that integrates dialogue history, graph data, and
  question expansion patterns to ensure contextual grounding and diverse dependency
  types.
---

# Multi-turn Natural Language to Graph Query Language Translation

## Quick Facts
- arXiv ID: 2508.01871
- Source URL: https://arxiv.org/abs/2508.01871
- Reference count: 40
- Key outcome: DA method achieves EM=68.45%, AEM=40.60%, EX=65.39%, AEX=38.30% on MTGQL dataset

## Executive Summary
This paper addresses the challenge of translating multi-turn natural language queries into graph query language (NL2GQL) by proposing a dependency-aware framework for constructing multi-turn NL2GQL datasets and evaluating baseline methods on this task. The authors introduce MTGQL, the first multi-turn NL2GQL dataset, constructed using a framework that integrates dialogue history, graph data, and question expansion patterns to ensure contextual grounding and diverse dependency types. They propose three baseline methods—ICL-AS, FT-AS, and DA—and evaluate them on MTGQL, with the DA method achieving the best performance, highlighting the importance of context management and refinement in multi-turn NL2GQL tasks.

## Method Summary
The paper proposes a dependency-aware framework for constructing multi-turn NL2GQL datasets, introducing MTGQL with 3,000 training dialogues, 500 dev, and 1,000 test cases. The framework integrates dialogue history, graph data, and question expansion patterns (P1-P6) to generate realistic multi-turn dialogues. Three baseline methods are evaluated: ICL-AS (in-context learning with full schema), FT-AS (fine-tuning with LoRA on full schema), and DA (dependency-aware with Context Manager, GQL Generator, and GQL Refiner). The DA method uses context reformulation, sub-schema extraction, and iterative GQL validation/refinement to achieve superior performance.

## Key Results
- DA method achieves best performance: EM=68.45%, AEM=40.60%, EX=65.39%, AEX=38.30%
- Performance degrades significantly with dialogue length: EM drops from 84.21% (R1) to 31.23% (R5+)
- Error analysis reveals Contextual Understanding Failures (32%) as primary error source

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dependency-aware context reformulation improves multi-turn NL2GQL accuracy by resolving coreferences and extracting relevant schema subsets.
- Mechanism: The Context Manager maintains structured dialogue history (questions, GQLs, answers, entities/relations), reformulates the current question to be context-independent, and extracts a relevant sub-schema before GQL generation. This reduces information overload compared to full-schema methods.
- Core assumption: Models can better generate accurate queries when context is explicitly structured and schema scope is narrowed to relevant elements.
- Evidence anchors: [abstract] "integrates dialogue history, graph data, and question expansion patterns"; [section 6.1] DA method comprises Context Manager, GQL Generator, and GQL Refiner with explicit context reformulation; [corpus] Weak direct corpus evidence; related work on multi-turn KG QA supports context management importance.
- Break condition: If dialogue dependencies are highly non-linear or entity tracking fails across 5+ turns, reformulation quality degrades (EM drops from 84.21% at R1 to 31.23% at R5+).

### Mechanism 2
- Claim: Iterative GQL validation and refinement improves execution correctness by catching syntax and semantic errors before dataset inclusion.
- Mechanism: The GQL Validator/Optimizer performs syntax validation (execution test), semantic validation (reverse question generation + embedding similarity), and up to 3 optimization rounds using LLM-based correction. Failed cases trigger question regeneration.
- Core assumption: LLMs can self-correct GQL errors when given error feedback and original question context.
- Evidence anchors: [abstract] "dependency-aware framework for constructing multi-turn NL2GQL datasets"; [section 4.5] Detailed validation workflow with syntax/semantic checks and optimization loop; [corpus] No direct corpus evidence for this specific refinement approach.
- Break condition: If semantic validation embedding similarity threshold is too strict or too loose, either valid queries are rejected or invalid ones pass.

### Mechanism 3
- Claim: Diverse question expansion patterns create realistic multi-turn dialogue dependencies that better approximate user behavior.
- Mechanism: Six expansion patterns (P1-P6: attribute follow-up, temporal shift, relation extension, same-type entity, aggregation, conditional filtering) guide question generation with weight-based selection that penalizes repetition. Questions include colloquial expressions, ellipses, and placeholders.
- Core assumption: Structured expansion patterns approximate real user query evolution better than random or template-only approaches.
- Evidence anchors: [section 4.2] Table 1 shows six patterns with examples; Algorithm 1 details weight-based selection; [section 5.2] Human evaluation scores: coherence 4.17-4.48, question diversity 4.01-4.16 (on 1-5 scale); [corpus] Multi-turn dialogue research supports pattern-based context evolution.
- Break condition: If expansion pattern weights become too uniform or patterns don't match domain-specific user behavior, generated dialogues lack natural flow.

## Foundational Learning

- Concept: **Graph Query Languages (GQL/Cypher/nGQL)**
  - Why needed here: The entire task is translating natural language to executable graph queries; understanding MATCH, WHERE, RETURN, and graph traversal patterns is essential.
  - Quick check question: Can you write a Cypher query to find all stocks in the "securities" industry ordered by opening price?

- Concept: **Multi-turn Dialogue Context Management**
  - Why needed here: Unlike single-turn NL2GQL, multi-turn requires maintaining dialogue state, resolving coreferences ("its subsidiaries"), and propagating entity information across turns.
  - Quick check question: In a dialogue "What is Apple's revenue?" → "And last year?", what context must be carried forward?

- Concept: **LLM Fine-tuning vs. In-Context Learning**
  - Why needed here: The paper compares ICL-AS (in-context with all schema) vs. FT-AS (fine-tuning with LoRA) vs. DA (dependency-aware); understanding when each applies is critical for architecture decisions.
  - Quick check question: Why does FT-AS (31.50% AEM) outperform ICL-AS (7.50% AEM) with the same Qwen backbone?

## Architecture Onboarding

- Component map: Context Manager → Question Generator → GQL Generator → GQL Validator/Optimizer → Dataset Filter
- Critical path: Question generation (with pattern + history) → Entity placeholder filling → GQL generation → Syntax validation → Semantic validation → (if failed) Optimization loop (max 3 iterations) → Dataset filtering
- Design tradeoffs:
  - Full schema vs. sub-schema extraction: Full schema provides complete context but may overwhelm model; DA's sub-schema extraction improves precision but risks missing relevant elements
  - ICL vs. fine-tuning: ICL requires no training data but performs poorly (ICL-AS EM: 32.55%); fine-tuning needs data but significantly improves results (FT-AS EM: 63.56%)
  - Validation strictness: Stricter filtering reduces dataset size but improves quality; looser filtering increases coverage but may include errors
- Failure signatures:
  - Schema Selection Errors (26%): Model selects wrong node/edge types, especially in full-schema methods without context narrowing
  - Contextual Understanding Failures (32%): Coreference resolution fails ("their subsidiaries" resolves incorrectly); most common error type
  - Logical Form Generation Errors (18%): Syntactically valid but semantically wrong queries (reversed edges, missing filters)
  - Ambiguity/Underspecification (14%): Queries like "most recent investment" lack temporal ordering logic
- First 3 experiments:
  1. Reproduce baseline comparison: Run ICL-AS, FT-AS, and DA on MTGQL test set with Qwen2.5-14B-Instruct to verify reported metrics (DA EM: 68.45%, AEM: 40.60%)
  2. Ablate context reformulation: Disable question reformulation in DA and measure AEM drop to quantify context management contribution
  3. Test expansion pattern impact: Generate dataset using only P1 (attribute follow-up) vs. all patterns, compare dialogue coherence scores and model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can schema-agnostic approaches effectively replace manually designed dependency modules to generalize multi-turn NL2GQL models to unseen graph domains?
- Basis in paper: [explicit] The authors state that current methods "may not generalize well to unseen domains or schema structures" and suggest future work explore "schema-agnostic approaches."
- Why unresolved: The current Dependency-aware (DA) baseline relies on manually designed modules that struggle with diverse or previously unseen schema structures.
- What evidence would resolve it: A benchmark evaluation showing a schema-agnostic model maintaining high performance on heterogeneous graph databases not present in the training data.

### Open Question 2
- Question: What evaluation metrics beyond execution accuracy are necessary to accurately assess semantic correctness in multi-turn NL2GQL?
- Basis in paper: [explicit] The paper notes that "execution correctness may not fully capture semantic correctness" and calls for "developing more fine-grained metrics."
- Why unresolved: Current metrics (EM, EX) treat queries as binary successes or failures, failing to account for partial intent matching or semantic nuances.
- What evidence would resolve it: The introduction of a new metric that correlates strongly with human judgment of semantic alignment, even when execution results differ.

### Open Question 3
- Question: How can architectures be adapted to mitigate the significant performance degradation observed as the number of dialogue turns increases?
- Basis in paper: [inferred] Table 6 shows a ~50% decrease in Exact Match (EM) from Round 1 (84.21%) to Round 5+ (31.23%), indicating a failure in long-term context retention.
- Why unresolved: The "Error Analysis" identifies "Contextual Understanding Failures" (32%) as a primary error source, suggesting current context managers struggle with long dependency chains.
- What evidence would resolve it: An architectural modification that stabilizes performance across extended dialogue turns (e.g., 5+ rounds) without significant accuracy loss.

## Limitations

- The MTGQL dataset construction process is proprietary, preventing independent verification of question generation patterns and validation procedures.
- The financial domain specificity of MTGQL may limit generalizability to other domains or graph structures.
- Core components like the Context Manager's sub-schema extraction and GQL Refiner's optimization loop lack isolated empirical validation.

## Confidence

- **High Confidence:** Comparative performance of baseline methods (ICL-AS vs. FT-AS vs. DA) and general importance of context management in multi-turn NL2GQL tasks.
- **Medium Confidence:** Effectiveness of the dependency-aware framework for dataset construction and specific contribution of each component (Context Manager, GQL Generator, GQL Refiner).
- **Low Confidence:** Robustness of GQL Validator/Optimizer's semantic validation (embedding similarity threshold of 0.6) and generalizability of six expansion patterns to non-financial domains.

## Next Checks

1. **Ablation Study on Context Management:** Disable the Context Manager's sub-schema extraction and question reformulation in the DA method, then measure the drop in AEM and AEX. This quantifies the contribution of context narrowing versus full-schema methods.

2. **Expansion Pattern Sensitivity Analysis:** Generate a test dataset using only one expansion pattern (e.g., P1: attribute follow-up) versus all six patterns, then evaluate model performance and human-annotated dialogue coherence scores. This isolates the impact of pattern diversity on dialogue naturalness and model accuracy.

3. **Semantic Validation Threshold Tuning:** Systematically vary the embedding similarity threshold (0.6) in the GQL Validator/Optimizer and measure the trade-off between precision (valid queries accepted) and recall (invalid queries rejected). This identifies optimal validation strictness for the financial domain.