---
ver: rpa2
title: 'When Models Can''t Follow: Testing Instruction Adherence Across 256 LLMs'
arxiv_id: '2510.18892'
source_url: https://arxiv.org/abs/2510.18892
tags:
- instruction-following
- evaluation
- instruction
- output
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a focused evaluation framework using 20 diagnostic
  prompts to assess instruction-following capabilities across 256 verified large language
  models. The methodology involves verifying model functionality before evaluation
  and employs objective, verifiable success criteria for each prompt.
---

# When Models Can't Follow: Testing Instruction Adherence Across 256 LLMs

## Quick Facts
- arXiv ID: 2510.18892
- Source URL: https://arxiv.org/abs/2510.18892
- Reference count: 37
- 43.7% overall pass rate across 256 verified LLMs on instruction-following tasks

## Executive Summary
This study presents a focused evaluation framework using 20 diagnostic prompts to assess instruction-following capabilities across 256 verified large language models. The methodology involves verifying model functionality before evaluation and employs objective, verifiable success criteria for each prompt. The comprehensive evaluation reveals significant variability in instruction-following performance, ranging from 0% to 100% across models, with an overall pass rate of 43.7%. String manipulation tasks proved particularly challenging (12.0% average success), while constraint compliance tests showed the highest success rate (66.9%). Provider-level analysis revealed performance ranging from 33.3% to 79.3%, with x-ai models achieving the highest average (79.3%). The evaluation framework offers a practical diagnostic tool that efficiently identifies specific instruction-following weaknesses across the contemporary LLM landscape.

## Method Summary
The study employed a two-stage protocol to evaluate 256 large language models from 331 available via OpenRouter API. First, endpoint functionality was verified using a simple factual query ("What is the capital of France?"), filtering out 22.7% of models that failed basic responsiveness. The remaining verified models were then evaluated on 20 diagnostic prompts covering string manipulation, mathematical computations, data processing, format conversion, and constraint compliance tasks. Each prompt had verifiable success criteria with flexible verification handling whitespace variations, quote styles, and markdown code block wrappers. Parameters were configured per model's supported parameters, with temperature=0.0, max_tokens=150, seed=42 (where supported), and timeout=10s for tests (30s for verification). Binary pass/fail results were compiled and aggregated by model, task category, and provider.

## Key Results
- Overall pass rate of 43.7% across 256 verified LLMs
- String manipulation tasks proved most challenging (12.0% average success)
- Constraint compliance tests showed highest success rate (66.9%)
- Provider performance ranged from 33.3% to 79.3%, with x-ai achieving highest average (79.3%)

## Why This Works (Mechanism)
The evaluation framework works by employing verifiable success criteria for each prompt, enabling objective assessment of instruction-following capabilities. The two-stage protocol ensures only functional models are evaluated, while the flexible verification algorithm handles variations in output formatting. The binary pass/fail approach provides clear, actionable metrics for identifying specific weaknesses across different task categories and model providers.

## Foundational Learning
- **Instruction-following evaluation**: Critical for assessing practical LLM utility; verify by checking if models can consistently execute specified tasks with verifiable outputs
- **API-based model testing**: Essential for large-scale evaluation; quick check: ensure endpoint verification works before running full evaluation
- **Flexible output verification**: Needed to handle legitimate formatting variations; verify by testing with multiple whitespace and quote style variations
- **Binary success metrics**: Provides clear performance indicators; check by confirming pass/fail criteria are consistently applied
- **Provider-level aggregation**: Enables comparison across model families; verify by computing success rates per provider with proper statistical confidence

## Architecture Onboarding
- **Component map**: OpenRouter API -> Model verification -> Diagnostic evaluation -> Binary verification -> Aggregate statistics
- **Critical path**: Model verification (30s timeout) -> Diagnostic evaluation (10s timeout) -> Binary verification -> Statistical aggregation
- **Design tradeoffs**: Single-turn evaluation vs. multi-turn context, flexible verification vs. strict matching, binary metrics vs. graded scoring
- **Failure signatures**: Non-functional endpoints (22.7% failure rate), string manipulation task failures (87.0-97.3% failure rate), JSON formatting variations
- **First experiments**: 1) Verify basic endpoint functionality with simple query, 2) Test single diagnostic prompt with flexible verification, 3) Run evaluation on small subset of verified models

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Why do simple string manipulation tasks (12.0% success) prove far more challenging than complex mathematical computations for LLMs?
- Basis in paper: The authors state: "This dissociation between computational complexity and instruction-following success rates, where simple string operations prove more challenging than mathematical computations, warrants further investigation (see Discussion)."
- Why unresolved: The paper documents the phenomenon but does not propose or test causal explanations for why character-level operations underperform relative to cognitively harder tasks.
- What evidence would resolve it: Controlled experiments isolating tokenization effects, character-level vs. word-level processing, and training data composition analysis across model families.

### Open Question 2
- Question: What specific training methodologies, data characteristics, or architectural choices produce superior instruction-following capabilities?
- Basis in paper: "Further research into the training methodologies that promote strong instruction-following would be valuable. Understanding whether specific data characteristics, training objectives, or architectural choices contribute to superior instruction adherence could inform future model development."
- Why unresolved: The study evaluates 256 models across providers but cannot access or analyze proprietary training pipelines.
- What evidence would resolve it: Controlled ablation studies varying instruction-tuning datasets, reinforcement learning objectives, and architectural modifications in otherwise identical base models.

### Open Question 3
- Question: How would instruction-following performance differ in multi-turn conversational contexts versus the single-turn interactions evaluated?
- Basis in paper: The authors list "Single-Turn Focus: Does not evaluate multi-turn conversations" as a limitation.
- Why unresolved: The framework tests isolated prompts only; real-world deployment typically involves extended conversations with accumulated context.
- What evidence would resolve it: Extending the diagnostic framework to multi-turn scenarios with instruction constraints spanning conversational exchanges.

### Open Question 4
- Question: Why do constraint-based tasks show binary outcomes where models either succeed completely or fail catastrophically, with minimal intermediate performance?
- Basis in paper: The paper notes: "models either succeeded completely or failed catastrophically, with few intermediate cases... This binary outcome distribution suggests that constraint awareness is either effectively incorporated during training or entirely absent, with little middle ground."
- Why unresolved: The empirical observation lacks mechanistic explanation for this all-or-nothing pattern.
- What evidence would resolve it: Probing studies examining internal representations of constraints during generation, and analysis of when/how models verify constraint satisfaction.

## Limitations
- Single-turn evaluation only, not assessing multi-turn conversational contexts
- Temporal instability due to rapid LLM evolution and changing model availability
- Potential API-specific effects from testing exclusively through OpenRouter interface

## Confidence
- **Overall methodology**: High - robust two-stage protocol with verifiable criteria
- **Provider performance rankings**: Medium - captures performance at specific point in time with API limitations
- **String manipulation findings**: High - consistent across all models with 87.0-97.3% failure rates
- **Generalizability to real-world tasks**: Low - 20 diagnostic prompts may not represent production scenarios

## Next Checks
1. Replicate evaluation using direct model access or multiple API providers to isolate API-specific effects from model capabilities
2. Expand prompt set to include real-world instruction-following scenarios from production applications to assess ecological validity
3. Conduct time-series evaluation across multiple months to quantify stability of instruction-following performance and identify improvement trends