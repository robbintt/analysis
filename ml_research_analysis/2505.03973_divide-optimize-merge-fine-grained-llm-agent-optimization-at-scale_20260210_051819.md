---
ver: rpa2
title: 'Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale'
arxiv_id: '2505.03973'
source_url: https://arxiv.org/abs/2505.03973
tags:
- arxiv
- optimization
- agent
- performance
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the scalability challenge in LLM-based agent
  optimization where increasing data volume causes context window overflow and degraded
  pattern recognition. FGO introduces a divide-and-conquer framework that splits optimization
  tasks into manageable subsets, performs fine-grained optimization on each subset,
  and progressively merges optimized components.
---

# Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale

## Quick Facts
- **arXiv ID**: 2505.03973
- **Source URL**: https://arxiv.org/abs/2505.03973
- **Reference count**: 40
- **Primary result**: FGO outperforms existing methods by 1.6-8.6% while reducing prompt token consumption by 56.3% across ALFWorld, LogisticsQA, and GAIA benchmarks

## Executive Summary
FGO introduces a divide-and-conquer framework for LLM-based agent optimization that addresses the scalability challenge of context window overflow with large training datasets. The approach partitions optimization tasks into manageable subsets, performs fine-grained optimization on each subset, and progressively merges optimized components through recursive clustering. Evaluation shows FGO achieves superior performance and efficiency compared to conventional all-at-once optimization, with consistent gains across all training dataset sizes.

## Method Summary
FGO divides training data into N disjoint subsets, optimizes each subset independently through an exploration-evaluation-LLM optimization cycle, then progressively merges the optimized modules using recursive K-Means clustering with validation backtesting. The framework uses category-based or random partitioning for division, and employs LLM-based clustering and synthesis for merging. Optimization runs for E epochs per subset, with merging proceeding recursively until clusters fall below threshold t.

## Key Results
- Outperforms existing methods by 1.6-8.6% across three benchmarks
- Reduces average prompt token consumption by 56.3%
- Shows consistent performance gains across all training dataset sizes
- Ablation shows progressive merging improves performance by ~10% over no merging
- Category-based partitioning slightly outperforms random partitioning (83.6% vs 80.3%)

## Why This Works (Mechanism)

### Mechanism 1: Context Budget Partitioning
Splitting datasets into disjoint subsets prevents context overflow, enabling the LLM optimizer to process all relevant trajectories without truncation. The "Divide" phase partitions the training dataset D into N disjoint subsets {D_i}^N_{i=1}. Each subset generates fewer trajectory-reward pairs, keeping token counts below context limits during optimization.

### Mechanism 2: Local Pattern Extraction via Reduced Cognitive Load
Operating on smaller, focused subsets improves the LLM optimizer's ability to capture subtle patterns that would be lost in long-context processing. Smaller trajectory batches reduce the "lost in the middle" phenomenon, allowing the optimizer to identify fine-grained failure modes and improvement directions per subset.

### Mechanism 3: Hierarchical Module Consolidation via Progressive Merging
Recursive clustering-based merging preserves task-specific optimizations while building toward a globally coherent module. Modules are clustered by similarity (k = ⌊√n⌋), merged with LLM synthesis, and validated on combined task sets at each recursion level. This builds a bottom-up merging tree where each merge operation stays within context limits.

## Foundational Learning

- **LLM Context Window Constraints**
  - Why needed here: The entire FGO design responds to the hard constraint that concatenated trajectories exceed model context capacity.
  - Quick check question: Given a model with 128K token context, at what trajectory count does the paper suggest performance degrades even before the hard limit?

- **Long-Context Processing Degradation**
  - Why needed here: Understanding why "insufficient context utilization" occurs even when content fits helps justify the divide strategy.
  - Quick check question: What two specific limitations does Section 2.2 identify beyond hard context limits?

- **Divide-and-Conquer Optimization Paradigm**
  - Why needed here: FGO maps the classic algorithmic strategy to LLM-based meta-optimization.
  - Quick check question: In classical divide-and-conquer, merge complexity often dominates; what is FGO's stated merge complexity for backtesting?

## Architecture Onboarding

- **Component map:**
  Divide: Partition training data → N subsets → Optimize (per subset): Agent runs → Trajectories → Evaluator → Textual gradients → LLM optimizer → Updated module → Merge: K-Means clustering → Recursive merging with validation → Final consolidated module

- **Critical path:**
  The progressive merging loop (Algorithm 2) is the bottleneck; each merge requires backtesting on combined task sets, which accumulates token cost despite overall efficiency gains.

- **Design tradeoffs:**
  - Subset count (N): Higher N improves parallelization but increases merge depth and validation overhead. Figure 5 suggests N primarily affects time, not quality.
  - Clustering algorithm: Ablation shows K-Means, Spectral, and Bisect K-Means all outperform no merging, but differences between them are modest (Table 2).
  - Partitioning strategy: Category-based partitioning slightly outperforms random (83.6% vs. 80.3%), but random remains robust.

- **Failure signatures:**
  - Context overflow errors during optimization (noted in Table 1 for All-at-once baseline on LogisticsQA/GAIA)
  - Inconsistent performance across task categories (suggests inadequate merging or overfitting to specific subsets)
  - Token cost exceeding baselines (merging validation overhead can exceed theoretical savings if merge depth is high)

- **First 3 experiments:**
  1. Reproduce ALFWorld with N=6 (category-aligned) and measure token consumption vs. All-at-once; confirm the 56.3% reduction claim.
  2. Ablate the merging strategy by replacing progressive merging with direct LLM synthesis of all modules; expect ~10% performance drop per Table 2.
  3. Stress-test scalability by increasing training data size (0→60 tasks per Figure 3) and plot performance curves for FGO vs. Batch-wise; FGO should show steady improvement while others plateau or degrade.

## Open Questions the Paper Calls Out
- Can the computational overhead of the merging process be reduced by predicting merged module performance using in-context learning or Bayesian approximation?
- Does a dynamic or learned data partitioning strategy yield better results than the fixed heuristics (random vs. category) analyzed?
- Can FGO effectively optimize agentic system architectures (e.g., graph workflows) rather than just prompt instructions and tool lists?

## Limitations
- The core scalability claims rely heavily on ablation studies rather than direct measurement of context utilization curves
- The progressive merging strategy has no direct comparison against alternative merging heuristics beyond clustering algorithms
- The paper does not provide validation on truly massive datasets (thousands of tasks) where context overflow would be inevitable

## Confidence
- **High confidence**: The 56.3% token reduction claim and the relative performance improvements over baselines are well-supported by the presented experiments across multiple benchmarks
- **Medium confidence**: The mechanism explanations for why context partitioning improves pattern recognition are theoretically sound but lack direct empirical validation
- **Low confidence**: The claim that FGO "maintains consistent performance gains across all training dataset sizes" is based on only three dataset sizes without testing intermediate or larger sizes

## Next Checks
1. Systematically measure FGO's performance and token efficiency as a function of subset size N and total dataset size, particularly around the context window boundary
2. Design an experiment where critical optimization signals require cross-subset dependencies, testing whether FGO's division strategy loses important information
3. Evaluate FGO on datasets with 100+ tasks or 10K+ tokens to confirm scalability benefits hold at scales where context overflow is unavoidable for baseline approaches