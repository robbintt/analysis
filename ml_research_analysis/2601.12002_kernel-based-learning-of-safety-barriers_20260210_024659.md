---
ver: rpa2
title: Kernel-Based Learning of Safety Barriers
arxiv_id: '2601.12002'
source_url: https://arxiv.org/abs/2601.12002
tags:
- safety
- systems
- barrier
- control
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a data-driven approach for safety verification
  and control synthesis of black-box stochastic systems using kernel-based methods.
  The key innovation is leveraging conditional mean embeddings (CMEs) to embed system
  dynamics into reproducing kernel Hilbert spaces (RKHS), enabling distributionally
  robust barrier certificate computation without explicit model knowledge.
---

# Kernel-Based Learning of Safety Barriers

## Quick Facts
- arXiv ID: 2601.12002
- Source URL: https://arxiv.org/abs/2601.12002
- Authors: Oliver Schön; Zhengang Zhong; Sadegh Soudjani
- Reference count: 24
- Primary result: Data-driven safety verification for black-box stochastic systems using kernel-based conditional mean embeddings and spectral barrier optimization

## Executive Summary
This paper presents a novel data-driven approach for safety verification and control synthesis of black-box stochastic systems without explicit model knowledge. The method leverages conditional mean embeddings (CMEs) to embed system dynamics into reproducing kernel Hilbert spaces (RKHS), enabling barrier certificate computation directly from trajectory data. A key innovation is the construction of RKHS ambiguity sets around empirical CME estimates, providing distributionally robust safety guarantees that hold with high probability even for out-of-distribution behavior. The framework transforms the typically intractable semi-infinite optimization problem into a tractable linear program through finite Fourier expansions, significantly reducing computational complexity compared to spatial discretization methods.

## Method Summary
The method proceeds by collecting N trajectory samples from the unknown system, then computing empirical CMEs via kernel ridge regression. These CMEs are embedded in RKHS, and an ambiguity set is constructed around the empirical estimate to provide distributionally robust guarantees. The barrier function is represented as a finite Fourier expansion, which transforms the semi-infinite programming problem into a linear program solvable via fast Fourier transform. This spectral approach relaxes restrictive assumptions common in existing work and enables verification with quantified confidence bounds on safety probability, demonstrated on both synthetic and neural network-controlled overtaking scenarios.

## Key Results
- Achieves 50% certified safety probability on Barr3 benchmark with M=35 wavenumbers, improving to 75% with M=99
- Successfully computes robust safety certificates for neural network-controlled overtaking with quantified lower bounds on satisfaction probability
- Reduces computational complexity from exponential (spatial discretization) to polynomial (spectral) scaling with system dimension

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditional mean embeddings enable data-driven barrier certificate computation without explicit dynamics models.
- Mechanism: System transition kernels are embedded into RKHS via CMEs, converting stochastic conditional expectations into inner products ⟨B, μ_{k^+|k_{xu}}(x,u)⟩_{H^+} that can be estimated directly from trajectory data using kernel ridge regression.
- Core assumption: Assumption 1 - the true CME μ_{k^+|k_{xu}}(t) lives in a vector-valued RKHS G, which is dense in L² for bounded kernels and imposes weaker structural constraints than polynomial/affine assumptions.
- Evidence anchors:
  - [abstract] "employ the concept of control barrier certificates...learn the certificate directly from a set of system trajectories...use conditional mean embeddings to embed data from the system into a reproducing kernel Hilbert space"
  - [Section 4, Theorem 2] Shows w(x,u)^T B(X̂^+_N) approximates E[B(X⁺)|X=x, U=u] via empirical CME, with convergence E[||μ̂_N - μ||_H] → 0 as N → ∞ (Proposition 2)
  - [corpus] LUCID (Paper 5703) by same authors implements this CME-based certification framework; corpus otherwise shows related data-driven safety approaches using different methods (zonotopes, GPs, offline RL) rather than CMEs
- Break condition: If the hypothesis space G does not contain the true system CME (violating Assumption 1), or if kernel choice is fundamentally mismatched to system regularity.

### Mechanism 2
- Claim: RKHS ambiguity sets centered on empirical CMEs provide distributionally robust safety guarantees with quantified confidence.
- Mechanism: An ε-radius norm-ball C_N^ε = {μ ∈ G : ||μ - μ̂_N||_G ≤ ε} is constructed around the empirical CME. Worst-case deviation from empirical expectation is bounded by εB̄κ(x,u), where B̄ bounds barrier norm and κ bounds kernel evaluations. This robustifies against out-of-distribution dynamics.
- Core assumption: Assumption 1 requires known bound ε such that P(||μ - μ̂_N||_G ≤ ε) ≥ 1-ρ. Theoretical concentration rates O(log(N)/N) exist but are noted as "excessively conservative in practice" (Section 4), often requiring manual calibration.
- Evidence anchors:
  - [abstract] "constructs an RKHS ambiguity set around empirical CME estimates, allowing verification guarantees that hold with high probability even for out-of-distribution behavior"
  - [Section 4, Eq. 11-13] Defines ambiguity set C_N^ε and derives robust constraint w(x,u)^T B(X̂^+_N) - B(x) ≤ c - εB̄κ(x,u)
  - [Section 8.2, Figure 9] Empirical demonstration: ε=0 yields 53.7% safety probability, ε=0.021 yields 5.07% robust probability, with barrier complexity ||b|| increasing as robustness demands grow
- Break condition: If concentration inequality providing ε is mis-specified for actual data distribution, or if ε is underestimated, the (1-ρ) confidence guarantee fails.

### Mechanism 3
- Claim: Finite Fourier expansion transforms semi-infinite barrier optimization into a tractable linear program with polynomial rather than exponential complexity.
- Mechanism: The squared-exponential kernel admits Fourier expansion (Eq. 19). Truncating to M wavenumbers yields Fourier CBC B(x) = φ_M(x)^T b with 2M+1 coefficients. The CME term collapses via FFT-computed matrix H (Eq. 22). Trigonometric bounds via Vallée-Poussin kernel (Lemmas 1-2) enable finite lattice sampling with constraint-tightening coefficient C_Ñ.
- Core assumption: Barrier functions and filtered dynamics are adequately represented in truncated Fourier basis. Approximation error decreases exponentially with M (Rahimi & Recht, 2007), but spectral aliasing occurs if f_max is insufficient.
- Evidence anchors:
  - [abstract] "leverages a finite Fourier expansion to cast a typically intractable semi-infinite optimization problem as a linear program...spectral barrier allows us to leverage the fast Fourier transform"
  - [Section 7.1-7.2] Derives Fourier feature map φ_M (Eq. 21), FFT-based H matrix, and trigonometric bounds reducing SILP (Eq. 23) to LP (Eq. 27)
  - [Section 8, Figure 6] Ablation shows M=35→99 improves certified probability 36.3%→50.0%; runtime scales with M and lattice size Ñ
  - [corpus] No corpus papers use this specific Fourier spectral approach; related work focuses on spatial discretization or neural barriers
- Break condition: If system dynamics or barrier require higher frequencies than M wavenumbers capture, spectral aliasing causes certification failure. Trigonometric bounds assume Nyquist sampling (Q̃ ≥ 2f_max + 1).

## Foundational Learning

- **Reproducing Kernel Hilbert Spaces (RKHS)**
  - Why needed here: Core mathematical infrastructure enabling CMEs, inner-product representations of expectations, and the entire kernel-based approach
  - Quick check question: Can you explain why the reproducing property f(x) = ⟨f, k(·,x)⟩_H enables computing conditional expectations without explicit density estimation?

- **Control Barrier Certificates for Stochastic Systems**
  - Why needed here: Target object being synthesized; understanding supermartingale relaxation (c ≥ 0 in Def. 2) and how CBC bounds connect to safety probability via Kushner's theorem
  - Quick check question: Why does CBC condition (c) with c > 0 only provide finite-horizon guarantees while c = 0 extends to infinite horizon?

- **Semi-Infinite Programming and Robust Optimization**
  - Why needed here: Understanding why barrier synthesis is fundamentally hard (infinitely many constraints over continuous domain X) and how Fourier/sampling approaches provide tractable relaxations
  - Quick check question: What makes the SIP in Eq. (15) non-convex, and why does Fourier expansion change the problem structure?

## Architecture Onboarding

- **Component map:**
  - Data collection -> CME estimator -> Fourier feature extraction -> LP constructor -> Solver -> Verifier

- **Critical path:**
  1. Collect N trajectory samples (dominates data requirements)
  2. Fit CME estimator O(N³) for Gram inversion
  3. Construct Fourier basis and compute H via FFT O(Q log Q) where Q ∝ (2f_max+1)²
  4. Build lattice Θ_Ñ with Ñ = Q̃^n points, compute trigonometric bounds
  5. Solve LP with Ñ constraints in 2M+1 variables
  6. Extract (η, c, b) and compute certified probability

- **Design tradeoffs:**
  - **M (basis size)**: Higher M → better approximation, more expressive barriers, but larger LP. Figure 6a shows diminishing returns past certain M.
  - **ε (robustness radius)**: Higher ε → stronger out-of-distribution guarantees but lower certified probability. Figure 9 shows exponential barrier complexity increase with ε.
  - **Lattice resolution Q̃**: Must satisfy Nyquist (Q̃ ≥ 2f_max + 1). Higher resolution → tighter bounds but O(Ñ) constraint scaling.
  - **B̄ (barrier norm bound)**: Must be estimated; too small causes infeasibility, too large increases conservatism in robust term εB̄κ.

- **Failure signatures:**
  - **LP infeasibility**: Typically from over-tight constraints (ε too large, B̄ too small, or η/c bounds incompatible)
  - **Certified probability → 0**: Robustness demands (large εB̄κ) force large c, dominating 1 - (η + cT)
  - **Barrier complexity explosion**: ||b|| grows exponentially as ε increases (Figure 9), indicating barrier function struggling to satisfy constraints
  - **Poor approximation**: Certified probability far below Monte Carlo truth (e.g., 50% vs 85-88% in Section 8.1) indicates inherent conservatism of barrier approach or insufficient M

- **First 3 experiments:**
  1. **Reproduce Barr3 benchmark (Section 8.1)** with varying M ∈ {15, 35, 99} to validate spectral basis expressiveness and compare certified vs Monte Carlo probabilities. Debug check: verify trigonometric bounds C_Ñ contract as Q̃ increases.
  2. **Robustness sweep on overtaking scenario (Section 8.2)**: Vary ε ∈ [0, 0.021] and observe certified probability decay and barrier complexity ||b|| growth. Validate that robust CBC level sets shrink appropriately (compare Figures 8a vs 8b).
  3. **Ablation on kernel hyperparameters**: Test sensitivity of σ_f, σ_l (lengthscale) on certified probability. Key hypothesis: longer lengthscales → smoother barriers → potentially easier certification but coarser safety boundaries.

## Open Questions the Paper Calls Out

- **Extension to Control Synthesis**: How can the Fourier barrier formulation be extended to handle black-box control synthesis where the control input enters the constraint nonlinearly? The current linear program approach fails when the input kernel interacts nonlinearly with the control variable, requiring gradient-based solvers or nonlinear global optimizers.

- **Sharp Ambiguity Set Radii**: Can sharp ambiguity set radii ε be determined with high confidence to reduce the conservatism of the distributionally robust guarantees? Current theoretical bounds are "excessively conservative," often necessitating manual tuning rather than principled methods.

- **Optimal Spectral Basis Selection**: What are the principled strategies for selecting an optimal spectral basis and quantifying the induced abstraction error in the Fourier expansion? The paper lacks formal characterization of criteria for selecting specific wavenumbers and the error introduced by truncation.

## Limitations

- Theoretical confidence bounds ε are described as "excessively conservative in practice," requiring manual calibration for real applications
- Approach depends critically on kernel hyperparameter selection and spectral resolution choices lacking clear systematic selection criteria
- While Fourier method offers computational advantages, it inherits spectral method limitations including potential aliasing for high-frequency dynamics

## Confidence

- **High confidence**: The kernel embedding mechanism and LP formulation are mathematically sound and well-supported by the literature
- **Medium confidence**: The empirical demonstration is promising but limited to two case studies; broader validation across diverse system classes is needed
- **Low confidence**: The practical selection of ε (robustness radius) remains an open question, with the paper relying on informal heuristics rather than principled methods

## Next Checks

1. **Systematic hyperparameter sensitivity analysis**: Test how certified probabilities vary with kernel lengthscales σ_l and frequency truncation M across multiple benchmark systems to establish robust selection guidelines.

2. **Benchmark comparison against spatial discretization**: Implement the same CBC synthesis using grid-based spatial discretization on identical systems to quantify the claimed computational advantages of the spectral approach.

3. **Distribution shift robustness testing**: Evaluate performance when test distribution differs from training data (e.g., shifted initial conditions or input distributions) to validate the practical value of the distributionally robust formulation.