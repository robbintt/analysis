---
ver: rpa2
title: Complexity of normalized stochastic first-order methods with momentum under
  heavy-tailed noise
arxiv_id: '2506.11214'
source_url: https://arxiv.org/abs/2506.11214
tags:
- inequality
- lemma
- where
- complexity
- follows
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of finding approximate stationary
  points for unconstrained optimization problems under heavy-tailed noise conditions.
  The authors propose three practical normalized stochastic first-order methods with
  different momentum variants (Polyak, multi-extrapolated, and recursive) that do
  not require explicit knowledge of problem-dependent quantities like Lipschitz constants
  or noise bounds.
---

# Complexity of normalized stochastic first-order methods with momentum under heavy-tailed noise

## Quick Facts
- arXiv ID: 2506.11214
- Source URL: https://arxiv.org/abs/2506.11214
- Reference count: 35
- This paper proposes normalized stochastic first-order methods with momentum for heavy-tailed noise settings, achieving improved complexity bounds compared to existing approaches.

## Executive Summary
This paper addresses the challenge of finding approximate stationary points for unconstrained optimization problems under heavy-tailed noise conditions. The authors propose three practical normalized stochastic first-order methods with different momentum variants (Polyak, multi-extrapolated, and recursive) that do not require explicit knowledge of problem-dependent quantities like Lipschitz constants or noise bounds. The methods employ dynamically updated algorithmic parameters to achieve convergence under weaker conditions than commonly used bounded variance and mean-squared smoothness assumptions.

The primary results include first-order oracle complexity bounds for finding approximate stochastic stationary points. When the tail exponent α is known, the methods achieve complexities of eO(ϵ−(3α−2)/(α−1)) for Polyak momentum, eO(ϵ−(p(2α−1)+α−1)/(p(α−1))) for multi-extrapolated momentum (with p being the smoothness order), and eO(ϵ−(2α−1)/(α−1)) for recursive momentum. When α is unknown, the complexities are eO(ϵ−2α/(α−1)), eO(ϵ−(3pα+α)/(2p(α−1))), and eO(ϵ−3α/(2(α−1))) respectively. These bounds either improve upon or match the best-known results in the literature. Numerical experiments on data fitting, robust regression, and multimodal contrastive learning problems demonstrate the practical effectiveness of the proposed methods compared to their unnormalized counterparts.

## Method Summary
The paper proposes three normalized stochastic first-order methods with momentum variants designed to handle heavy-tailed noise without requiring explicit knowledge of problem-dependent quantities. The methods use dynamically updated algorithmic parameters and leverage the geometric median of independent iterates to achieve robust convergence. The Polyak momentum variant uses Nesterov acceleration with geometric median averaging, the multi-extrapolated momentum uses higher-order smoothness with multiple extrapolation steps, and the recursive momentum employs recursive gradient updates with geometric median aggregation. All methods adaptively estimate the step size and momentum parameters during execution rather than requiring pre-specified values.

## Key Results
- Achieves complexity eO(ϵ−(3α−2)/(α−1)) for Polyak momentum when tail exponent α is known
- Achieves complexity eO(ϵ−2α/(α−1)) for Polyak momentum when α is unknown
- Outperforms or matches existing methods across multiple applications including data fitting, robust regression, and multimodal contrastive learning

## Why This Works (Mechanism)
The methods work by normalizing stochastic gradients to handle heavy-tailed noise without requiring explicit bounds on gradient noise variance or Lipschitz constants. By using geometric median of independent iterates instead of simple averaging, the methods achieve robustness against outliers and heavy-tailed distributions. The dynamically updated parameters adapt to the local geometry of the problem, while the momentum variants accelerate convergence by incorporating historical gradient information. The recursive structure allows efficient computation while maintaining the theoretical guarantees.

## Foundational Learning

1. **Heavy-tailed noise distributions** - why needed: Understanding that noise can follow distributions with infinite variance rather than Gaussian assumptions
   quick check: Verify if empirical gradient noise exhibits heavy tails using statistical tests

2. **Geometric median aggregation** - why needed: Provides robustness against outliers compared to arithmetic mean
   quick check: Compare convergence under different aggregation methods (mean vs median vs trimmed mean)

3. **Dynamic parameter adaptation** - why needed: Avoids need for problem-specific tuning while maintaining convergence
   quick check: Monitor parameter evolution during training to ensure adaptive behavior

4. **Momentum acceleration** - why needed: Improves convergence speed by leveraging past gradient information
   quick check: Compare convergence rates with and without momentum variants

5. **Smoothness order p** - why needed: Determines the order of extrapolation needed for optimal convergence
   quick check: Estimate smoothness empirically and verify impact on convergence

6. **First-order oracle complexity** - why needed: Standard metric for measuring optimization efficiency
   quick check: Count gradient evaluations to verify claimed complexity bounds

## Architecture Onboarding

**Component Map:**
Gradient estimation -> Normalization -> Momentum update -> Geometric median aggregation -> Parameter adaptation -> Output

**Critical Path:**
The critical path involves computing stochastic gradients, normalizing them based on estimated quantities, applying momentum updates, aggregating via geometric median, and adaptively updating algorithmic parameters. The geometric median computation and parameter adaptation are particularly critical for maintaining robustness and convergence guarantees.

**Design Tradeoffs:**
The normalization approach trades computational overhead for avoiding problem-specific parameter tuning. Geometric median aggregation provides robustness but requires multiple independent gradient computations per iteration. Dynamic parameter adaptation eliminates the need for manual tuning but introduces additional computational complexity. The momentum variants accelerate convergence but require careful implementation to maintain theoretical guarantees.

**Failure Signatures:**
Convergence failure may manifest as parameter explosion, stagnation in parameter adaptation, or divergence when noise assumptions are severely violated. Heavy-tailed noise with very small α values may cause numerical instability in the normalization step. Poor geometric median estimation due to insufficient independent samples can lead to suboptimal convergence rates.

**First Experiments:**
1. Test convergence on a simple quadratic function with synthetic heavy-tailed noise
2. Compare performance against baseline methods on robust regression with outliers
3. Evaluate sensitivity to tail exponent by varying α in controlled experiments

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes specific tail exponent values that may not hold in practice
- Experimental validation limited to three specific applications, raising questions about generalizability
- Dynamic parameter adaptation introduces implementation complexity without clear guidance on parameter initialization

## Confidence

**Complexity bounds under known α**: High
**Complexity bounds under unknown α**: Medium  
**Practical effectiveness claims**: Medium
**Theoretical assumptions validity**: Low

## Next Checks

1. Test the methods on additional problem classes beyond the three presented applications to assess robustness and generalizability
2. Conduct empirical studies to verify the convergence rates match theoretical predictions across different tail exponent values
3. Implement and evaluate the methods with various noise distributions to test sensitivity to assumption violations