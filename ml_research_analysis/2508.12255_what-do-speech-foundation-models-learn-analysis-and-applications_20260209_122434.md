---
ver: rpa2
title: What do Speech Foundation Models Learn? Analysis and Applications
arxiv_id: '2508.12255'
source_url: https://arxiv.org/abs/2508.12255
tags:
- speech
- svcca
- word
- pearson
- spearman
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis investigates the properties and applications of speech
  foundation models (SFMs) through lightweight analysis and downstream evaluation.
  It develops a scalable framework using canonical correlation analysis and training-free
  tasks to examine acoustic and linguistic knowledge encoded in SFM layers, comparing
  multiple models and statistical tools to understand knowledge distribution and accessibility.
---

# What do Speech Foundation Models Learn? Analysis and Applications

## Quick Facts
- **arXiv ID:** 2508.12255
- **Source URL:** https://arxiv.org/abs/2508.12255
- **Reference count:** 0
- **Primary result:** This thesis develops analytical tools and datasets to understand speech foundation models (SFMs) and demonstrates their effective adaptation for spoken language understanding tasks.

## Executive Summary
This thesis investigates the properties and applications of speech foundation models (SFMs) through lightweight analysis and downstream evaluation. It develops a scalable framework using canonical correlation analysis and training-free tasks to examine acoustic and linguistic knowledge encoded in SFM layers, comparing multiple models and statistical tools to understand knowledge distribution and accessibility. The analysis reveals that pre-training objectives shape layer-wise representation properties and that task-agnostic findings can guide adaptation strategy design, such as identifying optimal layers for frozen extraction or informing fine-tuning protocols. To evaluate SFMs on complex language understanding, the thesis contributes named entity recognition and localization tasks to the Spoken Language Understanding Evaluation (SLUE) benchmark and demonstrates that end-to-end models leveraging SFMs can surpass traditional cascaded approaches when external unannotated data is available. An extensive evaluation of various SFMs and adaptation strategies shows that no single SFM or adaptation method universally dominates across tasks, with frozen representations using complex prediction heads often outperforming fine-tuned models despite higher training costs. Collectively, the work provides analytical tools, datasets, and empirical insights to guide informed design choices for future SFM development and adoption.

## Method Summary
The thesis employs canonical correlation analysis (CCA) as a core analytical tool to measure similarity between SFM representation subspaces and external property vectors representing phonetic, word identity, syntactic, and semantic information. The framework extracts frame-level and span-level representations from all transformer layers of various SFMs (wav2vec2.0, HuBERT, WavLM, data2vec) and computes projection-weighted CCA (PWCCA) similarity scores against these external features using datasets like LibriSpeech and Buckeye corpus. For adaptation strategies, the work implements single-layer frozen extraction, weighted-frozen combinations, fine-tuning, and parameter-efficient tuning (LoRA) methods. The evaluation uses the SLUE benchmark for spoken language understanding, contributing new NER and NEL tasks to the existing ASR task, and compares end-to-end models against traditional cascaded approaches using knowledge distillation and self-training with external unannotated data.

## Key Results
- Analysis reveals that phonetic and semantic knowledge peaks at intermediate layers across SFMs, while linguistic content distribution varies by pre-training objective
- Task-agnostic CCA analysis successfully guides adaptation strategy design, with single-layer frozen extraction often matching or exceeding weighted-frozen and fine-tuning performance
- End-to-end models leveraging SFMs surpass traditional cascaded approaches on SLUE NER tasks when external unannotated data is available through distillation
- No single SFM or adaptation method universally dominates across tasks, with frozen representations using complex prediction heads frequently outperforming fine-tuned models despite higher training costs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CCA-based analysis effectively identifies layer-wise knowledge in SFMs without task-specific training.
- Mechanism: Measures canonical correlation between frame/span-level SFM representations and external property vectors (e.g., phone/word identity, semantic attributes). High correlation indicates the layer encodes that property.
- Core assumption: Linear projection directions capturing maximum correlation are meaningful proxies for encoded knowledge.
- Evidence anchors:
  - [Abstract] "We evaluate learned representations using training-free tasks and analyze the representation subspace using canonical correlation analysis (CCA)."
  - [Section 3.1.1] "We use projection-weighted CCA (PWCCA) to measure the similarity between the model representations and various quantities of interest formulated as continuous-valued vectors."
  - [Corpus] Corpus papers focus on downstream task improvements; none contradict the utility of CCA for intrinsic analysis.
- Break condition: If PWCCA scores consistently fail to correlate with downstream task performance (counter-evidence not found in thesis).

### Mechanism 2
- Claim: Analysis-driven layer selection matches or outperforms computationally heavier adaptation strategies.
- Mechanism: Identifies the most informative layer(s) via CCA trends, then uses single-layer frozen representations for task adaptation, avoiding weighted combinations of all layers.
- Core assumption: Task-relevant knowledge is concentrated in specific layers, not uniformly distributed.
- Evidence anchors:
  - [Section 5.1] "We not only find that the best single-frozen performance is at least as good as weighted-frozen performance for most tasks but also that the best-performing layer is always lower than at least the top two layers and is close to the layers observed to have the most phonetic and word-level content as measured by CCA."
  - [Section 4.3.2] "PWCCA scores offer a reliable comparison, not just for layers within a single SFM, but also layers from different SFMs."
  - [Corpus] Weak direct evidence; corpus papers emphasize fine-tuning over layer selection.
- Break condition: If task performance significantly degrades when using only CCA-recommended layers.

### Mechanism 3
- Claim: Leveraging external unannotated data via distillation/self-training bridges the performance gap between end-to-end and cascaded models in low-resource settings.
- Mechanism: Uses a strong teacher model (e.g., pipeline) to generate pseudo-labels on unlabeled data; student E2E model is trained on these labels, transferring knowledge without requiring ground-truth annotations.
- Core assumption: Pseudo-labels from a stronger model provide a useful training signal, even if noisy.
- Evidence anchors:
  - [Section 7.1.1] "When the tagger and target models are different, we refer to it as knowledge distillation... This approach enables the target model to learn from the better-performing tagger model via pseudo-labels."
  - [Section 7.3.2] "Using external data reduces the gap between spoken NER baselines and text NER... With access to either unlabeled speech or transcribed speech, E2E models outperform pipeline models."
  - [Corpus] Weak direct evidence; corpus papers don't address this specific distillation use case.
- Break condition: If pseudo-label noise severely degrades student model performance.

## Foundational Learning

- **Concept:** Canonical Correlation Analysis (CCA)
  - Why needed here: Core analytical tool for measuring similarity between representation subspaces and external properties.
  - Quick check question: Given two random vectors X and Y, what does CCA compute?

- **Concept:** Speech Foundation Model (SFM) Adaptation Strategies
  - Why needed here: The thesis compares frozen, fine-tuned, and parameter-efficient methods to apply SFMs to downstream tasks.
  - Quick check question: What are the trade-offs between using a frozen SFM with a complex prediction head versus fine-tuning the SFM with a lightweight head?

- **Concept:** Knowledge Distillation for Speech
  - Why needed here: Key method for improving E2E models using pseudo-labels from a stronger teacher model.
  - Quick check question: How does distillation transfer knowledge from a teacher to a student model in the context of spoken NER?

## Architecture Onboarding

- **Component map:** Analysis Framework -> Adaptation Protocols -> Evaluation Benchmarks
- **Critical path:**
  1. Load pre-trained SFM
  2. Extract intermediate layer representations
  3. Run CCA analysis against phonetic/semantic property vectors
  4. Select optimal layer(s) based on CCA scores for the target task
  5. Train lightweight prediction head on frozen representations or fine-tune selected layers
  6. Evaluate on SLUE benchmark tasks
- **Design tradeoffs:**
  - Analysis speed vs. depth: PWCCA is fast but may miss non-linear relationships
  - Adaptation efficiency vs. performance: Single-layer frozen is efficient but may underperform fine-tuning
  - Benchmark comprehensiveness vs. focus: SLUE covers multiple SLU tasks but may not generalize to all domains
- **Failure signatures:**
  - CCA scores don't correlate with task performance
  - Single-layer adaptation underperforms weighted-frozen baselines
  - Distillation degrades student model performance
- **First 3 experiments:**
  1. Run PWCCA analysis on HuBERT-Base using phonetic and semantic property vectors; plot layer-wise trends
  2. Compare single-layer frozen (using layer 9 from CCA analysis) vs. weighted-frozen on SLUE-NER
  3. Implement distillation from a pipeline teacher to an E2E student model using 100 hours of unlabeled speech; evaluate on SLUE-NER dev set

## Open Questions the Paper Calls Out

- **Question:** Why does the optimal placement of LoRA modules for HuBERT-Base combine the shallowest and deepest layers for tasks like dialog act classification, despite layer-wise CCA analysis indicating that linguistic content peaks in intermediate layers?
- **Basis in paper:** [explicit] Chapter 5.3 notes that optimal LoRA placement combines bottom and top layers (e.g., layers 1 and 12), while Chapter 3.3.3 and 3.3.5 show phonetic and semantic content is most concentrated in intermediate layers.
- **Why unresolved:** The author states in Section 5.3.3 that the CCA-inter analysis does not suggest a large impact for bottom-most layers, leaving the efficacy of this specific placement pattern unexplained by the representational analysis alone.
- **What evidence would resolve it:** An ablation study tracking the gradient magnitudes and attention map shifts in shallow vs. deep layers during LoRA fine-tuning could determine if the bottom layers handle domain adaptation while top layers handle task-specific output mapping.

- **Question:** What specific mechanisms cause fine-tuned supervised SFMs (like Whisper) to perform significantly worse than self-supervised models on automatic speech recognition (ASR) when using the same lightweight prediction head?
- **Basis in paper:** [explicit] Chapter 8.3.1 states that "Fine-tuned supervised SFMs perform the poorest on ASR across all SFMs," and notes that fine-tuning Whisper leads to significant performance degradation, ranking it last despite being a top performer when frozen.
- **Why unresolved:** The text suggests this is surprising given the general expectation that supervised pre-training provides better initialization, and notes it remains unclear why fine-tuning fails in this specific limited-data setting.
- **What evidence would resolve it:** A comparative analysis of the loss landscapes and catastrophic forgetting rates of supervised versus self-supervised encoders when fine-tuned on the 15-hour SLUE-VoxPopuli dataset would clarify if the supervised models overfit or lose generalization capabilities faster.

- **Question:** How does the distillation of a pipeline model into an E2E model specifically alter the distribution of error types (e.g., false positives vs. missed detections) in spoken named entity recognition?
- **Basis in paper:** [explicit] Chapter 7.4.4 analyzes error categories and finds pipeline models suffer from false positives, while Section 7.4.4.1 notes that distillation (Distill-Pipeline) drastically reduces missed detections for the E2E model compared to its baseline.
- **Why unresolved:** While the paper quantifies the error types, it does not fully explain the transfer mechanism by which distillation shifts the E2E model's behavior from missing entities to successfully detecting them without explicitly inheriting the pipeline's false-positive bias.
- **What evidence would resolve it:** A layer-wise probing analysis of the E2E model before and after distillation to see if the semantic boundary representations become more aligned with the text-based NER tagger used in the pipeline would resolve this.

## Limitations

- Generalizability to non-English and non-LibriSpeech-like domains: The CCA analysis and adaptation results are primarily demonstrated on English speech data, and the specific patterns may differ for languages with different phonological or morphological properties.
- Reliance on PWCCA as a proxy for task performance: The core analysis mechanism assumes high PWCCA scores reliably predict task-relevant knowledge, which is a strong assumption that may not hold for all tasks.
- Sensitivity of NEL evaluation to heuristic parameters: The Named Entity Localization task relies on post-processing heuristics that must be tuned per model architecture, limiting the reliability of cross-model comparisons.

## Confidence

- **High Confidence:** The claim that analysis-driven layer selection (via CCA) can match or outperform computationally heavier adaptation strategies is well-supported by empirical results in Section 5.1.
- **Medium Confidence:** The claim that leveraging external unannotated data via distillation/self-training bridges the performance gap between end-to-end and cascaded models is supported by experiments in Section 7.3.2, though contingent on pseudo-label quality.
- **Medium Confidence:** The claim that no single SFM or adaptation method universally dominates across tasks is based on extensive evaluation in Chapter 7, but the set of tasks and models may not be exhaustive.

## Next Checks

1. **Cross-linguistic CCA validation:** Apply the PWCCA analysis framework to a non-English SFM and external linguistic dataset for a different language to compare layer-wise knowledge distribution patterns with English.
2. **PWCCA predictive power study:** Design an experiment measuring correlation between PWCCA scores and final fine-tuned task performance on a new task to validate PWCCA as an analysis tool.
3. **Robust NEL evaluation protocol:** Implement and compare multiple alignment strategies for NEL evaluation across different model architectures to quantify variance and propose a standardized protocol.