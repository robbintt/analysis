---
ver: rpa2
title: A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations
arxiv_id: '2507.10585'
source_url: https://arxiv.org/abs/2507.10585
tags:
- explanations
- nles
- taxonomy
- explanation
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a taxonomy for designing and evaluating prompt-based
  natural language explanations (NLEs) in AI systems. The taxonomy adapts concepts
  from existing Explainable AI (XAI) literature to address the unique characteristics
  of NLEs, which are post-hoc, model-agnostic, and local explanations generated through
  prompting large language models.
---

# A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations

## Quick Facts
- arXiv ID: 2507.10585
- Source URL: https://arxiv.org/abs/2507.10585
- Reference count: 21
- One-line primary result: Taxonomy for designing and evaluating prompt-based natural language explanations in AI systems across context, generation, and evaluation dimensions.

## Executive Summary
This paper presents a taxonomy for designing and evaluating prompt-based natural language explanations (NLEs) in AI systems. The taxonomy adapts concepts from existing Explainable AI (XAI) literature to address the unique characteristics of NLEs, which are post-hoc, model-agnostic, and local explanations generated through prompting large language models. It organizes key considerations across three dimensions: Context (task type, data type, audience, and explanation goals), Generation and Presentation (model type, inputs, interactivity, output types, and presentation forms), and Evaluation (content properties like correctness and completeness, presentation properties like compactness and confidence, user-centered properties like actionability and personalization, and evaluation settings such as application-grounded and human-grounded). The authors demonstrate the taxonomy's application through an example use case of anomaly detection in remote sensing using airborne imagery, showing how different stakeholders require tailored explanations. The taxonomy provides a structured framework for researchers and practitioners to design NLEs that balance faithfulness with plausibility while considering user needs and evaluation contexts.

## Method Summary
The taxonomy was developed through systematic adaptation of XAI concepts to prompt-based NLEs, focusing on their post-hoc, model-agnostic, and local nature. The method involves organizing design considerations into three main dimensions: Context (defining task, data, audience, and goals), Generation and Presentation (specifying model types, inputs, interactivity, output formats, and presentation forms), and Evaluation (applying 15 properties across content, presentation, and user-centered categories). The authors validate the taxonomy's structure through a detailed use case of anomaly detection in remote sensing with airborne imagery, demonstrating how different stakeholders require different explanation approaches. The method emphasizes the tension between faithfulness (accuracy in reflecting model reasoning) and plausibility (being perceived as reasonable by humans), and provides structured guidance for navigating this trade-off.

## Key Results
- Taxonomy organizes NLE design into three dimensions: Context, Generation & Presentation, and Evaluation
- Identifies five distinct audience categories requiring different explanation approaches
- Adapts the Co-12 framework to define 15 evaluation properties across three categories
- Demonstrates application through anomaly detection use case with vision language models
- Provides structured framework for balancing faithfulness vs. plausibility trade-offs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structuring NLE design around audience type improves explanation relevance and utility.
- Mechanism: The taxonomy maps five audience categories (Creator, Operator, Executor, Decision Subject, Examiner) to distinct explanation goals, output types, and presentation forms. By explicitly defining audience during Context Definition, designers can align explanation content with stakeholder decision needs rather than generating generic explanations.
- Core assumption: Different stakeholders have sufficiently distinct information needs that a single explanation format cannot serve all effectively.
- Evidence anchors:
  - [Section 3.1] "Considering the audience is a major aspect of the context definition for NLE... we consider the following five categories of agents"
  - [Section 4] Demonstrates how operators need "why behavior deviates from normal patterns" while developers need "Why/how does the model detect this behavior" and examiners need compliance justification
  - [corpus] Related work on human-centered XAI design (arXiv:2510.12201) supports audience-tailored approaches
- Break condition: If stakeholder information needs are homogeneous or unknown, audience-driven design provides no marginal benefit over generic explanations.

### Mechanism 2
- Claim: Decomposing evaluation into Content, Presentation, and User-centered properties enables systematic trade-off analysis between faithfulness and plausibility.
- Mechanism: The adapted Co-12 framework (15 properties) separates explanation quality into measurable dimensions. Content properties (correctness, completeness, consistency) capture faithfulness; Presentation and User-centered properties capture plausibility. This decomposition allows designers to identify where NLEs may be failing and make intentional trade-offs.
- Core assumption: These 15 properties are both measurable and sufficient to capture explanation quality for governance purposes.
- Evidence anchors:
  - [Section 3.3] "In XAI, a central tension exists between faithfulness (accuracy in reflecting the reasoning) and plausibility (being perceived as reasonable or satisfying by humans)"
  - [Section 5] "There is a fundamental tension between some of the desired characteristics... achieving perfect Correctness might result in an explanation that is highly complex and detailed"
  - [corpus] Work on faithfulness testing (arXiv:2510.00047) validates this tension in VLM explanations
- Break condition: If properties are not independently measurable or key properties are missing, trade-off analysis becomes unreliable.

### Mechanism 3
- Claim: Prompt-based NLEs enable governance through natural language interfaces without requiring internal model access.
- Mechanism: Since prompt-based NLEs are post-hoc and model-agnostic, auditors and policymakers can query system behavior through natural language rather than requiring access to model parameters or architecture. This lowers technical barriers to verification.
- Core assumption: NLEs can be made sufficiently faithful to model reasoning that they serve as reliable proxies for internal verification.
- Evidence anchors:
  - [Abstract] "This taxonomy provides a framework for researchers, auditors, and policymakers to characterize, design, and enhance NLEs for transparent AI systems"
  - [Section 6] "It aligns with several capacities... including: access, by enabling interaction through natural language without needing internal model access"
  - [corpus] Evidence is mixed—related work (arXiv:2510.00047, arXiv:2505.22823) shows faithfulness remains a significant challenge
- Break condition: If NLEs cannot be made faithful to model reasoning, they provide governance theater rather than genuine transparency.

## Foundational Learning

- Concept: Post-hoc vs. ante-hoc explanation
  - Why needed here: The taxonomy explicitly excludes ante-hoc (inherently interpretable) methods, focusing only on explanations generated after model decisions. Understanding this distinction is necessary to scope where the taxonomy applies.
  - Quick check question: Is your explanation generated by the model during prediction, or by a separate process afterward?

- Concept: Faithfulness vs. plausibility trade-off
  - Why needed here: This is the central tension the taxonomy addresses. Faithfulness measures alignment with actual model reasoning; plausibility measures user satisfaction. NLEs tend to optimize plausibility at faithfulness's expense.
  - Quick check question: Can an explanation be both highly plausible and completely unfaithful? (Yes—this is the core risk.)

- Concept: Grounding levels for evaluation (Application/Human/Functionally-grounded)
  - Why needed here: The taxonomy prescribes three evaluation settings with different costs and validity. Choosing the wrong setting yields misleading quality assessments.
  - Quick check question: Which evaluation setting requires real users performing real tasks?

## Architecture Onboarding

- Component map:
  Context Definition → Generation & Presentation → Evaluation
  (Task, Data, Audience, Goal) (Model Type, Input, Interactivity, Output, Presentation Form) (Content properties, Presentation properties, User-centered properties, Evaluation setting)

- Critical path: Define audience first—all other design choices (goal, output type, presentation form, evaluation criteria) cascade from audience requirements.

- Design tradeoffs:
  - Correctness vs. Comprehensibility: More accurate explanations may be too complex for non-expert users
  - Compactness vs. Completeness: Briefer explanations may omit relevant factors
  - Interactivity vs. Consistency: Dialogue-based refinement may produce inconsistent explanations across sessions
  - Speed vs. accuracy in decision-making: Simpler explanations increase speed but risk over-reliance (cited: Swaroop et al., 2024)

- Failure signatures:
  - Explanations that sound fluent but cannot be verified against model behavior (faithfulness failure)
  - Same input produces different explanations across sessions (consistency failure)
  - Users cannot act on explanations or report they don't address their questions (actionability failure)
  - Explanations used by wrong audience type (e.g., technical explanations shown to lay operators)

- First 3 experiments:
  1. **Audience mapping exercise**: For your system, enumerate all five audience types and document their distinct explanation goals. If goals are identical across audiences, the taxonomy may provide limited value.
  2. **Property prioritization**: From the 15 properties in Table 2, rank top 5 for your use case. Test whether achieving these creates conflicts (e.g., does maximizing correctness reduce comprehensibility?).
  3. **Evaluation setting selection**: Based on available resources and stakes, choose among Functionally-grounded (automated, cheapest), Human-grounded (layperson studies), or Application-grounded (real task, most expensive). Start with LLM-as-a-judge for preliminary assessment before human evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the taxonomy be empirically validated to demonstrate improved NLE design outcomes compared to ad-hoc prompting approaches?
- Basis: [explicit] The conclusion states: "Future work involves validating its use through experiments in collaboration with human-computer interaction experts."
- Why unresolved: The taxonomy is proposed as a framework but has not yet undergone empirical validation with users or practitioners.
- What evidence would resolve it: Controlled experiments comparing NLEs designed with versus without the taxonomy, measuring user satisfaction, task performance, and explanation quality.

### Open Question 2
- Question: How can faithfulness be effectively guaranteed or measured in prompt-based NLEs without compromising their communicative utility?
- Basis: [explicit] The discussion notes: "Guaranteeing faithfulness is a challenging technical task that is beyond the scope of this study and deserves special attention."
- Why unresolved: The paper focuses on design and evaluation dimensions but does not provide technical methods for ensuring NLEs accurately reflect model reasoning.
- What evidence would resolve it: Development and validation of automated faithfulness metrics or prompting strategies that produce provably faithful NLEs.

### Open Question 3
- Question: What are the empirically optimal trade-offs between competing NLE properties (e.g., correctness vs. comprehensibility) in high-stakes, time-sensitive contexts?
- Basis: [explicit] The discussion states: "There is a fundamental tension between some of the desired characteristics of NLEs" and references trade-offs between decision accuracy and speed.
- Why unresolved: The taxonomy identifies 15 potentially conflicting properties but offers no guidance on how to prioritize or balance them.
- What evidence would resolve it: User studies in domains like the anomaly detection use case that systematically vary property combinations and measure decision quality, speed, and overreliance.

### Open Question 4
- Question: How reliable is LLM-as-a-judge evaluation for assessing NLE properties compared to human-grounded evaluation?
- Basis: [inferred] The paper recommends LLM-as-a-judge for early design but cautions about "limitations of LLMs when deployed as judges" without quantifying discrepancies.
- Why unresolved: No empirical comparison is provided between automated LLM evaluation and human evaluation for the subjective NLE properties in the taxonomy.
- What evidence would resolve it: Correlation studies between LLM-judge scores and multi-annotator human ratings across all 15 NLE properties.

## Limitations

- The taxonomy's practical effectiveness depends heavily on the faithfulness of NLEs to underlying model reasoning, which remains an open challenge in the literature
- The framework assumes that the 15 evaluation properties are both measurable and sufficient, but validation across diverse real-world applications is limited
- The example use case demonstrates the taxonomy's structure but not its empirical impact on explanation quality or user outcomes

## Confidence

- **High**: The taxonomy provides a logically coherent organizational framework that correctly identifies key design dimensions for NLEs (taxonomy structure, audience consideration, evaluation property decomposition)
- **Medium**: The claim that prompt-based NLEs enable governance through natural language interfaces without internal model access (depends on achieving sufficient faithfulness)
- **Low**: The assertion that the 15 properties comprehensively capture explanation quality and enable systematic trade-off analysis (requires empirical validation across domains)

## Next Checks

1. **Faithfulness Validation**: Conduct controlled experiments comparing NLE explanations against ground-truth model reasoning for the same predictions to quantify faithfulness gaps
2. **Property Independence Test**: Measure correlations between the 15 evaluation properties to verify they capture distinct dimensions of explanation quality rather than redundant aspects
3. **Cross-Audience Comparison**: Test whether audience-specific explanations designed using the taxonomy actually outperform generic explanations for each stakeholder group in real decision-making tasks