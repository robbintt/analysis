---
ver: rpa2
title: 'Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception'
arxiv_id: '2510.12720'
source_url: https://arxiv.org/abs/2510.12720
tags:
- given
- audio
- visual
- detailed
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of fine-grained multimodal perception,
  where models must capture detailed information from audio and visual inputs while
  minimizing hallucinations. To solve this, the authors propose Omni-Detective, an
  agentic data generation pipeline that uses tool-calling to iteratively gather evidence
  and produce detailed yet minimally hallucinatory captions.
---

# Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception

## Quick Facts
- **arXiv ID:** 2510.12720
- **Source URL:** https://arxiv.org/abs/2510.12720
- **Reference count:** 40
- **Key outcome:** Introduces Omni-Captioner, a multimodal model achieving state-of-the-art performance on VDC and video-SALMONN 2 benchmarks while minimizing hallucinations through a novel agentic data generation pipeline and cloze-style evaluation.

## Executive Summary
This work addresses the challenge of fine-grained multimodal perception, where models must capture detailed information from audio and visual inputs while minimizing hallucinations. To solve this, the authors propose Omni-Detective, an agentic data generation pipeline that uses tool-calling to iteratively gather evidence and produce detailed yet minimally hallucinatory captions. They train two models—Audio-Captioner and Omni-Captioner—on this data using a two-stage curriculum focused on audio alignment and joint audio-visual optimization. Omni-Captioner sets a new state-of-the-art on the VDC benchmark and achieves the best trade-off between detail and hallucination on the video-SALMONN 2 testset. Additionally, the authors introduce Omni-Cloze, a cloze-style benchmark for stable and reliable evaluation across audio-only, visual-only, and audio-visual scenarios.

## Method Summary
The authors propose a two-pronged approach to achieve detailed yet minimally hallucinatory multimodal perception. First, they develop Omni-Detective, an agentic data generation pipeline that uses a Detective Agent (LLM) to iteratively query specialized Observers (ASR, OCR, MLLM) and cross-reference evidence before synthesizing captions. Second, they train two models—Audio-Captioner (stage 1) and Omni-Captioner (stage 2)—using a two-stage curriculum that first freezes the visual encoder to force audio grounding, then jointly optimizes both modalities. They also introduce Omni-Cloze, a cloze-style evaluation benchmark that frames assessment as filling blanks in a gold-standard passage with a "Not Given" option to reduce evaluation variance and cost.

## Key Results
- Omni-Captioner sets a new state-of-the-art on the VDC benchmark.
- Achieves the best trade-off between detail and hallucination on the video-SALMONN 2 testset.
- Introduces Omni-Cloze, a novel cloze-style benchmark that reduces evaluation cost (1 LLM call vs. 38 for VDC) while maintaining reliability.

## Why This Works (Mechanism)

### Mechanism 1: Agentic Evidence Verification
An iterative "detective" pipeline can decouple the generation of fine-grained details from the introduction of hallucinations. Instead of one-shot generation, a Detective Agent (LLM) queries independent Observers (tools like ASR, OCR, MLLM) in a multi-turn loop. It cross-references evidence before synthesizing a caption, explicitly targeting the "co-growth" of detail and error typical in standard models. Core assumption: Access to specialized external tools (grounding sources) allows an LLM to verify claims more effectively than it can generate them from parametric memory alone. Break condition: If the specialized tools (Observers) themselves are noisy or hallucinate, the Detective Agent may propagate these errors rather than correcting them.

### Mechanism 2: Asymmetry-Aware Curriculum
A two-stage curriculum freezing the visual encoder first improves audio grounding in the final model. In audio-visual data, visual density is typically higher. If trained jointly, models may ignore sparse audio cues. By freezing the visual encoder in Stage 1, the model is forced to optimize the audio pathway to explain the loss, ensuring audio tokens capture semantic weight before full multimodal fusion. Core assumption: The pre-trained visual encoder is sufficiently robust that its features do not require immediate updating to serve as a grounding signal for the audio modality. Break condition: If the visual encoder is frozen but its features are misaligned with the specific nuances of the training data, the model may fail to integrate cross-modal correlations effectively.

### Mechanism 3: Constrained Evaluation Protocol (Omni-Cloze)
Cloze-style multiple-choice evaluation provides a more stable and efficient proxy for detailed perception quality than open-ended generation or multi-turn QA. Evaluation is framed as filling blanks in a gold-standard passage. This constrains the output space to specific details (entities, attributes) and includes a "Not Given" option. This reduces the variance and cost associated with LLM-based judging of free-form text. Core assumption: The selected "blanks" in the cloze passage are representative of the full richness of the perceptual task, and the distractors are plausible enough to test precision. Break condition: If the model generates correct but differently phrased details that don't fit the cloze answer key, it will be penalized for a lack of exact matching (though the paper argues the "Not Given" option mitigates some of this).

## Foundational Learning

**Concept: Detail-Hallucination Co-growth**
- **Why needed here:** This is the core problem the paper attempts to solve. You must understand that in current OLMs, simply asking for "more detail" typically increases the hallucination rate (Figure 2).
- **Quick check question:** Why does forcing a model to write longer captions typically increase the error rate?

**Concept: Multimodal Information Density Asymmetry**
- **Why needed here:** This justifies the two-stage training curriculum. Audio streams often contain sparse, critical events compared to dense visual frames.
- **Quick check question:** Why might a standard multimodal model struggle to learn audio cues if trained simultaneously with video?

**Concept: Tool-Augmented Agents (Agentic Pipelines)**
- **Why needed here:** The Omni-Detective is not just a model; it is a system of an agent calling tools (ASR, OCR). Understanding the "Detective" vs. "Observer" roles is critical for reproducing the data pipeline.
- **Quick check question:** What is the role of the "Detective" agent versus the "Observer" tools in the data generation process?

## Architecture Onboarding

**Component map:** Audio-Visual Stream -> Omni-Detective (Detective Agent + Tool Box [ASR, OCR, MLLM]) -> Model (Qwen-2.5-Omni-7B Backbone) -> Detailed Caption -> Eval (Omni-Cloze).

**Critical path:** The quality of the *Omni-Captioner* is entirely dependent on the *Omni-Detective* pipeline. If the data pipeline fails to cross-verify evidence (e.g., OCR is wrong), the model will learn to hallucinate.

**Design tradeoffs:**
- **Cost vs. Quality:** The agentic pipeline is computationally expensive (iterative calls) but produces high-quality data.
- **Stability vs. Nuance:** Omni-Cloze is more stable than open-ended eval but may miss the nuance of correct synonyms not in the answer key.

**Failure signatures:**
- **High Hallucination:** If the "Detective" accepts observer output without cross-checking.
- **Audio Deafness:** If Stage 1 training is skipped, the model ignores audio in favor of visual cues.
- **Eval Drift:** If the cloze distractors are too easy, the benchmark will saturate quickly.

**First 3 experiments:**
1. **Ablate Curriculum:** Train Omni-Captioner end-to-end from scratch (skip Stage 1) and measure the drop in audio-only QA performance (MMAU).
2. **Pipeline Stress Test:** Run Omni-Detective with a noisy ASR tool to verify if the "Detective" successfully cross-checks and filters errors.
3. **Benchmark Correlation:** Compare Omni-Cloze scores against human Elo ratings to confirm the "Not Given" mechanism aligns with human judgment of missing details.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can evaluation frameworks reliably detect "irrelevant generation" hallucinations where a model produces correct details that are absent from the ground truth reference?
- **Basis in paper:** Appendix E (Limitations) states that the current Omni-Cloze benchmark fails to handle cases where a model predicts a detail present in the input but missing from the reference text, and identifies developing robust methods for this as "an important direction for future work."
- **Why unresolved:** Current cloze-style evaluations strictly match model outputs against ground-truth references; therefore, they cannot distinguish between a genuine hallucination and a correct prediction that was simply omitted by the human annotator.
- **Evidence to resolve:** An evaluation protocol that cross-references model outputs directly against the raw audio-visual input (e.g., using a judge model with access to the source media) rather than relying solely on text-based references.

### Open Question 2
- **Question:** What specific training or architectural modifications are required to close the significant performance gap between short-video and long-video detailed captioning?
- **Basis in paper:** Appendix C.2.2 notes that while Omni-Captioner achieves 77.2% on short videos, it "lags on long videos of 58.0%, suggesting a potential avenue for future improvement."
- **Why unresolved:** The current model struggles to maintain fine-grained detailed perception over extended durations (30–60 minutes), indicating potential limitations in context retention or memory capacity within the current architecture.
- **Evidence to resolve:** Ablation studies applying specific long-context handling mechanisms (e.g., sliding windows, memory banks) to the Omni-Captioner architecture, demonstrating a narrowed performance gap on the "Long" subset of Video-MME.

### Open Question 3
- **Question:** Can the "fundamental ceiling" in hallucination reduction observed in the Omni-Detective pipeline be overcome by improving the underlying toolset?
- **Basis in paper:** Section 6.2, Figure 6 analysis observes that hallucination rates converge early (around steps 5-6) and suggests this implies "current multimodal tools have a fundamental ceiling in eliminating incorrect claims."
- **Why unresolved:** While the iterative detective process reduces errors, it eventually reaches a plateau where incorrect inferences cannot be self-corrected, suggesting the bottleneck lies in the perceptual accuracy of the tools (OCR, ASR, MLLM) rather than the orchestration logic.
- **Evidence to resolve:** Experiments substituting the pipeline's underlying tools with higher-fidelity models to verify if the hallucination convergence point shifts lower or extends to later steps.

## Limitations

- The Omni-Detective pipeline is entirely offline and non-interactive; there is no evaluation of real-time, interactive performance.
- Claims about "detail-hallucination co-growth" are supported by internal dataset comparisons but not proven to generalize to external data.
- The two-stage curriculum assumes visual features are stable, which may not hold for all video domains (e.g., abstract or highly dynamic scenes).
- The Omni-Cloze benchmark relies on predefined answer keys that may under-value models with strong paraphrasing or reasoning abilities.

## Confidence

- **High Confidence:** The mechanism of the agentic pipeline (Detective + Observers) and its implementation are clearly described and technically sound. The improvement on VDC and video-SALMONN 2 is verifiable from the reported metrics.
- **Medium Confidence:** The claim that the two-stage curriculum significantly improves audio grounding is plausible but lacks a direct ablation study. The Omni-Cloze benchmark is innovative but its correlation with real-world perceptual quality is not independently validated.
- **Low Confidence:** The assertion that the pipeline produces "minimally hallucinatory" data is difficult to verify without access to the full dataset and robust human evaluation. The paper does not provide quantitative evidence that the "Not Given" option effectively mitigates evaluation bias.

## Next Checks

1. **Ablate Curriculum:** Train Omni-Captioner end-to-end from scratch (skip Stage 1) and measure the drop in audio-only QA performance (MMAU) compared to the published results. This will directly test the necessity of the two-stage curriculum for audio grounding.

2. **Pipeline Stress Test:** Run Omni-Detective with a noisy or deliberately flawed OCR tool to verify if the "Detective" agent successfully cross-checks and filters errors, or if it propagates the noise. This will validate the core assumption that the agent can mitigate hallucinations from its observers.

3. **Benchmark Correlation:** Conduct a small-scale human evaluation (e.g., Elo ratings or direct scoring) on a subset of the video-SALMONN 2 testset and compare the results to the Omni-Cloze scores. This will confirm whether the "Not Given" mechanism in the cloze benchmark aligns with human judgment of missing or hallucinated details.