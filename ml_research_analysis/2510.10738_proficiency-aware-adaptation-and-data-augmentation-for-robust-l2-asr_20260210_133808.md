---
ver: rpa2
title: Proficiency-Aware Adaptation and Data Augmentation for Robust L2 ASR
arxiv_id: '2510.10738'
source_url: https://arxiv.org/abs/2510.10738
tags:
- proficiency
- speech
- lora
- data
- augmentation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors addressed bias in ASR for L2 learners by adapting
  Whisper using a CEFR-graded dataset, finding that naive fine-tuning improved overall
  accuracy but worsened disparities for lower-proficiency speakers. They proposed
  two proficiency-aware strategies: multitask learning with proficiency classification
  and targeted spectrogram masking augmentation on low-proficiency data.'
---

# Proficiency-Aware Adaptation and Data Augmentation for Robust L2 ASR

## Quick Facts
- **arXiv ID:** 2510.10738
- **Source URL:** https://arxiv.org/abs/2510.10738
- **Reference count:** 0
- **Primary result:** Proficiency-aware adaptation and targeted augmentation reduced L2 ASR WER by up to 29.4% (relative) and narrowed proficiency gaps, especially for low-proficiency speakers.

## Executive Summary
This paper addresses bias in L2 English ASR by adapting Whisper using CEFR-graded data. The authors find that naive fine-tuning improves overall accuracy but worsens disparities for lower-proficiency speakers due to insertion errors on disfluent speech. They propose two proficiency-aware strategies: multitask learning with proficiency classification and targeted spectrogram masking augmentation on low-proficiency data. These methods consistently narrow proficiency gaps while reducing overall WER by up to 29.4% relative, demonstrating that proficiency is a key factor in fair and accurate L2 ASR.

## Method Summary
The authors adapt Whisper-small to L2 English using the Speak & Improve corpus (28.2h train / 22.9h dev / 22.7h eval with CEFR labels A2-C1). Four conditions are evaluated: (1) naive LoRA fine-tuning on L2 data, (2) multitask LoRA with auxiliary proficiency classifier (MLP on mean-pooled encoder outputs), (3) SpecAug applied only to A2 data, and (4) combined multitask+data approach. The multitask loss is L = 0.9·L_ASR + 0.1·L_CLS. All experiments use LoRA for parameter-efficient adaptation, and augmentation is disabled at inference.

## Key Results
- Targeted spectrogram masking on low-proficiency speech reduced overall WER by up to 29.4% (relative) and insertion/deletion errors by up to 58.6% (relative)
- Multitask learning with proficiency classification consistently narrowed proficiency gaps across all CEFR levels
- Naive fine-tuning improved average WER but simultaneously widened disparities, with A2 WER increasing by 20-21% relative due to insertion errors on function words
- The combined multitask+data approach achieved the best equity-accuracy tradeoff

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Joint optimization of ASR with proficiency classification reduces proficiency gaps by conditioning encoder representations on learner-level variation.
- **Mechanism:** An MLP classifier is attached to the Whisper encoder via mean-pooled final encoder states. During training, the shared objective L = 0.9·L_ASR + 0.1·L_CLS forces the encoder to learn proficiency-discriminative features alongside transcription, implicitly regularizing against majority-class overfitting.
- **Core assumption:** Proficiency labels encode systematic acoustic patterns (temporal disfluencies, pause structure) that are relevant to ASR decoding.
- **Evidence anchors:**
  - [abstract] "multitask learning with proficiency classification" reduced WER and narrowed proficiency gaps.
  - [section 2.2.2] Explicitly describes the multitask loss formulation and λ weighting.
  - [corpus] Neighbor paper "Mitigating Data Imbalance in Automated Speaking Assessment" (arXiv 2509.03010) addresses class imbalance in L2 assessment with a balancing objective—conceptually aligned but not directly validating multitask ASR.

### Mechanism 2
- **Claim:** Targeted spectrogram masking on low-proficiency speech improves robustness without corrupting proficiency labels.
- **Mechanism:** SpecAugment applies time and frequency masking to log-mel features. The authors restrict augmentation to A2 speech only, counteracting the 2.7% word-level imbalance. Unlike speed perturbation, masking leaves global temporal and segmental cues intact, preserving CEFR label validity.
- **Core assumption:** SpecAug's local masking does not remove proficiency-diagnostic cues (e.g., pause distributions, stress timing).
- **Evidence anchors:**
  - [abstract] "targeted augmentation... applying spectrogram masking to low-proficiency speech to counter imbalance."
  - [section 2.2.3] Explicitly justifies SpecAug over transforms that risk altering proficiency cues.
  - [corpus] Weak direct evidence—neighbor papers use SpecAug for general ASR robustness (e.g., "Towards Pretraining Robust ASR Foundation Model"), but none validate proficiency-label preservation specifically.

### Mechanism 3
- **Claim:** Naive LoRA fine-tuning improves aggregate WER but worsens A2 performance via insertion-error amplification on disfluent speech.
- **Mechanism:** LoRA adapters are trained on the full L2 corpus without proficiency conditioning. The model overfits to higher-proficiency (B2–C1) patterns, which dominate the dataset. For A2 speech, the adapted model over-predicts filler-like function words ("and," "a," "the"), increasing insertions by ~20% relative.
- **Core assumption:** Performance gaps stem from systematic speech differences across proficiency levels, not just data scarcity (C1 is also underrepresented but performs best).
- **Evidence anchors:**
  - [abstract] "naive fine-tuning... reduces average WER but simultaneously widens disparities."
  - [section 3.2] Quantifies 20–21% relative A2 WER increase; identifies insertion errors on function words as the driver.
  - [corpus] No direct neighbor evidence on naive adaptation worsening subgroup performance—this is a novel contribution of the paper.

## Foundational Learning

- **Concept: CEFR Proficiency Scale (A2–C1)**
  - Why needed here: CEFR labels are the supervision signal for the auxiliary classifier and the stratification variable for all disparity analyses.
  - Quick check question: Can you explain why A2 and C1 are both underrepresented but show opposite ASR performance trends?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: All adaptation experiments use LoRA to enable parameter-efficient fine-tuning while limiting drift from the pretrained Whisper model.
  - Quick check question: What would happen to the "naive adaptation worsens disparities" conclusion if full fine-tuning were used instead of LoRA?

- **Concept: SpecAugment (Spectrogram Masking)**
  - Why needed here: The targeted augmentation strategy relies on SpecAug's label-preserving properties, which the authors contrast with signal-level transforms like speed perturbation.
  - Quick check question: Why does time/frequency masking preserve proficiency labels better than speed perturbation?

## Architecture Onboarding

- **Component map:** Whisper-small encoder (frozen) → LoRA adapters in attention/FFN projections → mean-pooled encoder output → MLP proficiency classifier (4-class: A2, B1, B2, C1) → decoder receives encoder states + LoRA-adapted representations → transcript

- **Critical path:**
  1. Load Whisper-small with LoRA adapters on selected projection matrices
  2. Attach proficiency head to encoder (mean-pool + MLP)
  3. Compute joint loss: L = 0.9·L_ASR + 0.1·L_CLS
  4. Apply SpecAug to A2 batch items before encoder
  5. Backprop through LoRA + classifier; decoder updated by L_ASR only

- **Design tradeoffs:**
  - λ weighting (0.9/0.1): Higher λ₂ may improve proficiency prediction but risk ASR degradation
  - LoRA rank and target modules: Higher rank = more capacity but more drift from pretrained knowledge
  - SpecAug intensity on A2: Aggressive masking may improve robustness but risks obscuring fluency markers
  - Augmentation scope: Applying to B1 as well could further balance but dilutes the low-proficiency focus

- **Failure signatures:**
  - A2 WER increases relative to baseline (insertions spike on function words)
  - Proficiency classifier F1 remains low (authors report this as a limitation)
  - Aggregate WER improves but proficiency gap (A2–C1 WER difference) widens

- **First 3 experiments:**
  1. **Reproduce baseline disparity:** Run naive LoRA fine-tuning on the S&I corpus; verify that A2 WER increases while B2–C1 improves (Section 3.2 replication).
  2. **Ablate augmentation target:** Compare SpecAug on A2-only vs. all proficiency levels; confirm that targeted augmentation is necessary for gap reduction.
  3. **Proficiency head necessity:** Train with SpecAug but without the multitask objective; isolate whether the classifier provides independent equity gains beyond augmentation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more robust balancing strategies improve the auxiliary proficiency classifier's performance and further enhance ASR accuracy for low-proficiency speakers?
- Basis in paper: [explicit] The authors state in the conclusion that "future work will focus on improving the proficiency classifier... through more robust balancing strategies" because the current F1 score is limited.
- Why unresolved: The current classifier performance is constrained by the severe class imbalance of the dataset, limiting the potential gains from the multitask learning approach.
- What evidence would resolve it: Demonstrating that techniques like focal loss or advanced oversampling increase the classifier's F1 score and subsequently lower WER for A2 speakers compared to the current multitask model.

### Open Question 2
- Question: Does integrating accent-robust modeling with proficiency-aware adaptation significantly reduce segment-sensitive substitution errors?
- Basis in paper: [explicit] The authors note that "substitutions remain a dominant segment-sensitive error" and list "accent-robust modeling approaches" as a specific direction for future work.
- Why unresolved: The current proficiency-aware methods primarily target temporal disfluencies (insertions/deletions), leaving segmental accent issues (substitutions) as a residual error source.
- What evidence would resolve it: Results from a combined architecture showing a statistically significant reduction in substitution rates compared to the proficiency-only models proposed in this paper.

### Open Question 3
- Question: Can augmentation methods that alter segmental features (e.g., speed perturbation) improve L2 ASR if proficiency labels are dynamically adjusted?
- Basis in paper: [inferred] Section 2.2.3 explicitly excludes signal transforms like speed perturbation because they "risk corrupting the CEFR proficiency label," suggesting the strict label preservation is a limiting factor.
- Why unresolved: It is currently unknown if allowing minor label noise or adjusting labels during augmentation could yield better robustness than the current label-preserving SpecAugment approach.
- What evidence would resolve it: An ablation study comparing label-preserving augmentation against augmentations with relaxed label constraints to observe the impact on WER and error types.

## Limitations
- Hyperparameter sensitivity not explored: LoRA rank, alpha, target modules, and SpecAug intensity are unspecified
- Cross-corpus generalization unclear: All results derived from single, proprietary S&I corpus
- Proficiency label quality not validated: No rater agreement or consistency checks reported
- Error type attribution limited: Does not fully explain why insertions dominate A2 degradation in naive adaptation

## Confidence
- **High Confidence:**
  - "Naive LoRA fine-tuning reduces overall WER but widens proficiency disparities" — directly supported by quantified WER changes and error-type breakdowns
  - "Targeted augmentation on low-proficiency data reduces imbalance" — backed by consistent WER reductions for A2 and smaller gaps
- **Medium Confidence:**
  - "Multitask learning with proficiency classification reduces gaps by conditioning encoder representations" — mechanism is plausible and results show gap narrowing, but classifier quality not fully demonstrated
  - "SpecAug preserves proficiency labels better than signal-level transforms" — justified by contrast with speed perturbation, but no ablation study provided
- **Low Confidence:**
  - "Combined multitask + augmentation achieves the best equity-accuracy tradeoff" — relative gains reported, but absolute performance and hyperparameter robustness not explored

## Next Checks
1. **Reproduce the baseline disparity:** Train naive LoRA on the S&I corpus and verify that A2 WER increases while B2–C1 improves, matching the reported 20–21% relative A2 WER rise. Plot per-proficiency WER and insertion rates.
2. **Ablate augmentation target:** Train with SpecAug applied to all proficiency levels vs. A2-only; confirm that targeted augmentation is necessary for proficiency gap reduction.
3. **Assess label robustness:** Measure the proficiency classifier's F1 per class on a held-out set; test whether augmenting the classifier's training data (e.g., focal loss or class balancing) further narrows ASR disparities.