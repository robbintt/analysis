---
ver: rpa2
title: 'When Less is More: 8-bit Quantization Improves Continual Learning in Large
  Language Models'
arxiv_id: '2512.18934'
source_url: https://arxiv.org/abs/2512.18934
tags:
- arxiv
- replay
- quantization
- learning
- continual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically studies the interaction between quantization
  precision and replay buffer strategies in continual learning of large language models.
  The authors fine-tune LLaMA-3.1-8B across three quantization levels (FP16, INT8,
  INT4) on sequential tasks from the LoRI benchmark spanning NLU, Math, and Code generation,
  varying replay buffer sizes from 0% to 20% of prior data.
---

# When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models

## Quick Facts
- arXiv ID: 2512.18934
- Source URL: https://arxiv.org/abs/2512.18934
- Reference count: 26
- Primary result: INT8 quantization achieves optimal balance between learning new tasks and retaining prior knowledge in continual learning scenarios

## Executive Summary
This paper systematically investigates how quantization precision affects continual learning in large language models, finding that lower precision (particularly INT8) can dramatically outperform FP16 in sequential task scenarios. The authors fine-tune LLaMA-3.1-8B across three quantization levels (FP16, INT8, INT4) on sequential tasks from the LoRI benchmark, varying replay buffer sizes from 0% to 20% of prior data. Surprisingly, quantized models not only require less replay buffer to maintain knowledge retention but actually achieve better forward transfer to new tasks than high-precision models, with INT4 achieving 40% accuracy on Code generation versus FP16's 20%.

## Method Summary
The study uses LLaMA-3.1-8B with LoRA adapters (r=8, alpha=16) across three quantization levels, training sequentially on NLU (8 datasets), Math (GSM8K), and Code (CodeAlpaca) tasks. Each stage uses 1 epoch with learning rate 2e-4 and batch size 8, with replay buffers ranging from 0% to 20% of prior task data. INT8 uses BitsAndBytes inference-time quantization while INT4 uses QLoRA-style NF4. The critical finding is that quantization-induced noise acts as implicit regularization, preventing overfitting to new task gradients and allowing quantized models to retain more prior knowledge with smaller replay buffers.

## Key Results
- Quantized models (especially INT4) dramatically outperform FP16 on subsequent tasks, achieving up to 40% vs 20% on Code generation
- Even minimal replay buffers (0.1%) substantially improve retention, increasing NLU retention after Math training from 45% to 65% across all precisions
- INT8 consistently achieves optimal balance between learning plasticity and knowledge retention
- INT4 shows sharper degradation without replay (>30% absolute accuracy drop on NLU) but achieves better forward transfer with replay

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantization-induced noise functions as implicit regularization, reducing overfitting to new task gradients during sequential fine-tuning.
- Mechanism: Lower precision representations (INT8/INT4) introduce quantization error that smooths the loss landscape, biasing optimization toward flatter minima. This noise limits how aggressively new task updates can overwrite prior knowledge.
- Core assumption: Regularization effect stems from quantization noise rather than architectural differences in how adapters interact with frozen quantized weights.
- Evidence anchors: Abstract hypothesis, section 3 positing noise smooths loss landscape, weak direct evidence in corpus.

### Mechanism 2
- Claim: Minimal replay buffers provide disproportionate retention gains because quantization amplifies the relative influence of replayed samples.
- Mechanism: Under quantization, gradient signal from replayed samples competes more effectively against new task gradients because quantized weights cannot represent fine-grained updates that would cleanly overwrite prior patterns.
- Core assumption: Replay samples provide gradient directions that align with previously learned representations, and quantization noise randomly perturbs competing gradients more than aligned ones.
- Evidence anchors: Abstract showing 45%→65% retention jump at 0.1% replay, section 3 on replay-quantization interaction, no comparable studies in corpus.

### Mechanism 3
- Claim: INT8 occupies a precision sweet spot where quantization noise is sufficient for regularization without severely degrading single-task capacity.
- Mechanism: INT8 introduces enough numerical perturbation to regularize (unlike FP16) while retaining sufficient representational fidelity for complex reasoning (unlike INT4).
- Core assumption: Optimal precision depends on task distribution—INT8's advantage emerges specifically in sequential multi-domain learning.
- Evidence anchors: Abstract claiming INT8 achieves optimal balance, section 3 showing 1-2% degradation from INT8 vs 3-5% from INT4, neighboring papers focus on efficiency not plasticity-stability.

## Foundational Learning

- Concept: **Catastrophic Forgetting**
  - Why needed here: The entire paper frames quantization as modulating forgetting dynamics; understanding that sequential fine-tuning overwrites prior knowledge is prerequisite.
  - Quick check question: Can you explain why training on Task B degrades Task A performance in standard fine-tuning?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: All experiments use LoRA with frozen base weights; understanding that only adapter weights update explains why quantization of base weights affects gradient dynamics.
  - Quick check question: What components of the model are trained vs. frozen in a LoRA setup, and how does this differ from full fine-tuning?

- Concept: **Quantization-Aware vs. Post-Training Quantization**
  - Why needed here: The paper uses different quantization approaches (INT8 inference-time vs. QLoRA-style NF4); distinguishing these is critical for replication.
  - Quick check question: How does QLoRA's 4-bit quantization differ from simply running inference on a 4-bit model?

## Architecture Onboarding

- Component map: LLaMA-3.1-8B (base) -> LoRA adapters (r=8, alpha=16) -> Quantization (FP16/INT8/INT4) -> Training (NLU→Math→Code) -> Evaluation (all tasks)
- Critical path: Stage A: Train on NLU (8 datasets aggregated) → Stage B: Train on GSM8K + replay NLU samples → Stage C: Train on CodeAlpaca + replay NLU + GSM8K samples → Evaluate all tasks after each stage
- Design tradeoffs: Higher precision (FP16) → better initial performance, worse retention without replay; Lower precision (INT4) → worse initial performance, but better forward transfer and retention under replay; Replay size → storage/compute cost vs. retention; diminishing returns above 10%
- Failure signatures: INT4 + 0% replay: >30% absolute accuracy drop on NLU (72.31% → 42.50%); FP16 on Code generation: Only 20% accuracy vs. INT4's 40%; Insufficient replay on Math/Code: Requires 5-10% minimum vs. 1-2% for NLU
- First 3 experiments: 1) Baseline calibration: Run FP16/INT8/INT4 on single-task NLU to establish degradation bounds (expect 1-5% drop from quantization alone); 2) Zero-replay forgetting curve: Train NLU→Math→Code sequentially with B=0% to replicate the 35% FP16 vs. 30% INT4 NLU degradation asymmetry; 3) Minimal replay threshold: Test B={0.1%, 0.5%, 1%} to identify where the 45%→65% retention jump occurs for each precision level

## Open Questions the Paper Calls Out

- Question: Does quantization-induced noise function as implicit regularization, and through what mechanism does it prevent overfitting to new task gradients?
  - Basis: Paper explicitly states this hypothesis and calls for further investigation to validate it.

- Question: Do the quantization-replay trade-offs generalize beyond LLaMA-3.1-8B to other architectures, scales, and modalities?
  - Basis: Limitations section notes restriction to LLaMA-3.1-8B and limited task set (NLU, Math, Code).

- Question: Can mixed-precision or adaptive quantization strategies further improve the plasticity-stability balance in continual learning?
  - Basis: Limitations section notes consideration only of uniform quantization levels without exploring mixed-precision or adaptive strategies.

- Question: How do intelligent replay selection strategies interact with quantization levels compared to simple reservoir sampling?
  - Basis: Limitations states replay used simple reservoir sampling; more sophisticated strategies may further shift trade-offs.

## Limitations
- Single model and domain constraints leave unknown whether INT8's optimal balance holds across encoder-only models, mixture-of-experts, or vision-language architectures
- Lack of statistical significance testing across multiple replay buffer sizes and precision levels with only single-seed runs
- Replay buffer implementation details remain underspecified beyond "uniform random sampling per dataset"

## Confidence
- High confidence: Empirical observation that quantized models (especially INT4) outperform FP16 on sequential tasks with replay, and that minimal replay buffers (0.1-1%) provide substantial retention gains across all precisions
- Medium confidence: Mechanism hypothesis that quantization noise acts as implicit regularization preventing overfitting to new task gradients
- Low confidence: Claim that INT8 represents an optimal "sweet spot" for the plasticity-stability tradeoff

## Next Checks
1. Mixed-precision ablation: Train FP16 adapters on quantized (INT8/INT4) base weights to determine whether regularization effects originate from base weight quantization or adapter dynamics
2. Gradient similarity analysis: Compute cosine similarity between replay and new-task gradients across precision levels to test whether quantization selectively amplifies replay gradients' influence
3. Multi-seed replication: Run the full experiment suite (FP16/INT8/INT4 × 8 buffer sizes) across 3-5 random seeds to establish statistical significance of observed effects