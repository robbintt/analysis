---
ver: rpa2
title: Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations
arxiv_id: '2510.07064'
source_url: https://arxiv.org/abs/2510.07064
tags:
- agents
- ours
- human
- agent
- reppopmapped
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of constructing diverse sets
  of LLM agents to represent heterogeneous human populations. Instead of using a single
  agent, the authors propose a framework to select a set of agents whose behaviors
  collectively match a target human population.
---

# Prompt Optimization Across Multiple Agents for Representing Diverse Human Populations

## Quick Facts
- **arXiv ID:** 2510.07064
- **Source URL:** https://arxiv.org/abs/2510.07064
- **Reference count:** 40
- **Primary result:** A submodular optimization framework that constructs diverse sets of LLM agents to represent heterogeneous human populations, achieving lower representation error than baselines across educational and crowdsourcing domains.

## Executive Summary
This paper addresses the challenge of constructing diverse sets of LLM agents to represent heterogeneous human populations. Instead of using a single agent, the authors propose a framework to select a set of agents whose behaviors collectively match a target human population. Each agent is steered by a small set of human demonstrations via in-context learning, and the selection is cast as a submodular optimization problem. The framework is model-agnostic and scales to different LLM families, with experiments showing that the resulting agents reproduce human-like behaviors on new tasks.

## Method Summary
The method constructs diverse LLM agents by selecting demonstrations from human populations and using in-context learning to steer agent behavior. The selection problem is formulated as submodular optimization to minimize representation gap. Three algorithms are proposed: REPPOP_demo greedily selects demonstrations per agent, REPPOP_mapped-1 constructs proxy agents with random demos per human, and REPPOP_mapped-2 uses greedy demo selection per human. The framework is evaluated on three domains: EEDI math students, OpinionQA political survey respondents, and WikiArt annotators, showing improved representation compared to baselines.

## Key Results
- Proposed REPPOP methods achieve lower representation error than baselines (SINGLE, RANDOM) on all three domains
- Behavioral analyses show agents reproduce human patterns on new tasks without access to demographic metadata
- REPPOP_mapped-2 offers the best performance-computation tradeoff with formal approximation guarantees
- The framework scales to different LLM families and domains with varying embedding types

## Why This Works (Mechanism)

### Mechanism 1: Submodular Optimization Enables Tractable Agent Selection
- **Claim:** Selecting a representative set of agents from an exponentially large space is NP-hard, but the objective function exhibits submodularity, enabling greedy algorithms to achieve near-optimal solutions with formal approximation guarantees.
- **Mechanism:** The representation gap function g(L) = (1/|H|) Σ min_{l∈L} dist(e_h, e_l) has the diminishing returns property—adding an agent to a smaller set provides at least as much marginal benefit as adding it to a larger set. This allows greedy selection to achieve (1-1/e) ≈ 63% of optimal performance.
- **Core assumption:** Human behavior embeddings accurately capture meaningful behavioral differences, and distance metrics meaningfully reflect behavioral similarity.
- **Evidence anchors:**
  - [abstract]: "We tackle this selection problem from the lens of submodular optimization."
  - [section 4.1]: "Proposition 1 (Submodularity of the Objective Function f(L))."
  - [corpus]: Related work on "Mixture-of-Personas Language Models for Population Simulation" addresses similar population representation challenges but does not use submodular optimization.

### Mechanism 2: In-Context Learning Transfers Behavioral Patterns to New Tasks
- **Claim:** Conditioning LLMs on small sets of human demonstrations via in-context learning creates agents whose behavioral patterns generalize to unseen tasks, even without explicit persona or demographic information.
- **Mechanism:** Demonstrations (task-response pairs) in the prompt create a behavioral context that steers the LLM's outputs. The agent implicitly learns response patterns, skill levels, or perspectives that transfer when presented with new tasks from the same domain.
- **Core assumption:** LLMs can extract and apply abstract behavioral patterns from few examples, not just surface-level patterns. Also assumes behavioral consistency within individuals across related tasks.
- **Evidence anchors:**
  - [abstract]: "behavioral analyses on new tasks show that these agents reproduce the behavior patterns and perspectives of the students and annotators they are designed to represent."
  - [section 5.4]: Agents constructed for EEDI math students showed proficiency levels on test questions that "closely mirror those observed in the students they represent."
  - [corpus]: "Can LLM Agents Simulate Multi-Turn Human Behavior?" examines behavioral consistency but focuses on single-agent simulation.

### Mechanism 3: Human-Mapped Proxy Pools Reduce Computational Complexity
- **Claim:** Restricting agent selection to a proxy pool where each agent corresponds to a human (rather than all possible demonstration combinations) dramatically reduces search space while preserving behavioral diversity.
- **Mechanism:** Instead of searching the full agent space (|L| ≈ (|T|·|H|)^K), REPPOP_mapped creates one proxy agent per human (size |H|), then selects the best M agents from this reduced pool. Theorem 2 provides performance guarantees parameterized by coverage ratio γ and imitation error ρ.
- **Core assumption:** The human population itself provides sufficient coverage of behavioral diversity, so restricting to human-mapped agents doesn't exclude important behavioral profiles.
- **Evidence anchors:**
  - [section 4.2]: "This one-to-one mapping reduces the candidate space to |L̃| = |H| while preserving diversity."
  - [section 4.2, Theorem 2]: "f(L_greedy_L̃) ≥ (1−1/e)(γ·f(L*_L) − ρ)" where γ measures how well humans cover the full agent space and ρ measures how well proxy agents approximate their humans.
  - [corpus]: Weak corpus evidence on this specific technique; the human-mapping approach appears to be a novel contribution of this paper.

## Foundational Learning

- **Concept: Submodular Functions**
  - **Why needed here:** The entire optimization framework relies on submodularity for theoretical guarantees and tractable greedy solutions.
  - **Quick check question:** Given f({A}) = 5, f({B}) = 4, f({A,B}) = 7, does this violate submodularity? (Yes: marginal gain of B to {A} is 2, but marginal gain of B to ∅ is 4; submodular requires 4 ≤ 2, which fails.)

- **Concept: In-Context Learning with Demonstrations**
  - **Why needed here:** Agents are defined entirely by the demonstrations in their prompts; understanding how this steers behavior is critical.
  - **Quick check question:** If you provide an LLM with 3 examples of a student incorrectly answering fraction problems, what behavioral pattern might it infer and apply to a new fraction problem? (Likely: the student has misconceptions about fractions and will make similar errors.)

- **Concept: Facility Location Problem**
  - **Why needed here:** The paper proves NP-hardness via reduction to facility location; this connection explains why the problem is hard and why submodular optimization applies.
  - **Quick check question:** In facility location, opening more facilities reduces customer travel distance but increases fixed costs. What's the analog in this paper? (Agents = facilities, humans = customers, representation gap = travel distance, M = budget constraint on facilities.)

## Architecture Onboarding

- **Component map:**
```
Human Data (D^T_H)
     ↓
[Demonstration Pool] ─→ [Embedding Extraction] ─→ [Distance Computation]
     ↓                                              ↓
[Agent Construction] ←───────────────────────────[Representation Gap g(L)]
     ↓                                              ↓
[REPPOP_demo / REPPOP_mapped-1 / REPPOP_mapped-2] ─→ [Selected Agent Set L]
     ↓
[Test Task Evaluation] ─→ [Behavioral Analysis]
```

- **Critical path:** The greedy selection loop (Algorithm 1, lines 4-9; Algorithm 2, lines 7-9). For each of M agents to select, compute marginal gain for all candidates and pick the best. This is called M times.

- **Design tradeoffs:**
  - **REPPOP_demo vs. REPPOP_mapped-1 vs. REPPOP_mapped-2:**
    - REPPOP_demo: Higher performance, O(M·K·|T|·|H|²) time, no approximation guarantee
    - REPPOP_mapped-1: Fastest, O(K·|H| + M·|H|²), weaker proxies (random demos), has guarantee
    - REPPOP_mapped-2: Best performance-computation balance, O(K·|T|·|H| + M·|H|²), has guarantee
  - **Context size K:** Larger K = better behavioral capture but higher runtime and potential noise
  - **Number of agents M:** Diminishing returns after covering major behavioral clusters

- **Failure signatures:**
  - **High representation error on test set but low on train set:** Overfitting to training tasks; reduce K or increase task diversity
  - **Agents cluster in embedding space despite different demonstrations:** LLM not responding to demonstrations; check temperature (should be ≥1.0), verify prompt format
  - **REPPOP_mapped performs worse than RANDOM:** Proxy pool construction failing; check that demonstrations are from the correct human
  - **Runtime explosion on large populations:** Use SAMPLEGREEDY with smaller ψ or switch to REPPOP_mapped-1

- **First 3 experiments:**
  1. **Baseline validation:** Run SINGLE, RANDOM, and your chosen REPPOP variant on EEDI with M=5, K=3. Confirm REPPOP achieves lower test representation error (target: ~0.33 vs ~0.38 for RANDOM per Table 3).
  2. **Context size sensitivity:** Fix M=10, vary K ∈ {1,3,5}. Plot representation error. Expect diminishing returns; identify knee point for your domain.
  3. **Behavioral consistency check:** Construct agents for a subset of humans, then have agents complete test tasks. Compare agent skill distributions (for EEDI) or opinion distributions (for OpinionQA) to their represented humans using the metadata analysis approach from Section 5.4. Agents should mirror human patterns without having seen that metadata.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the ordering of demonstrations within the prompt context significantly influence the behavior of the selected agents or the optimization landscape?
- **Basis in paper:** [explicit] The authors state, "we do not consider the ordering of demonstrations in prompts, and understanding how ordering influences model behavior would be a valuable extension."
- **Why unresolved:** The current methodology treats the set of K demonstrations as an unordered set, yet LLMs are known to be sensitive to input sequence.
- **What evidence would resolve it:** Ablation studies measuring representation gap variance when permutations of the same demonstration sets are applied to the agents.

### Open Question 2
- **Question:** Can fine-tuning techniques improve upon or replace prompting-based approaches for constructing representative agents?
- **Basis in paper:** [explicit] The conclusion notes reliance on prompting and suggests, "future work could explore fine-tuning techniques for constructing an agent."
- **Why unresolved:** In-context learning may lack the capacity to capture complex nuances that fine-tuning could handle, but fine-tuning sacrifices the flexibility of dynamic prompt updates.
- **What evidence would resolve it:** A comparative study evaluating representation error between fine-tuned "persona" models and the proposed in-context agents using the same human demonstration data.

### Open Question 3
- **Question:** Do representative agent sets translate to improved outcomes in concrete downstream applications, such as training teachers or simulating policy responses?
- **Basis in paper:** [explicit] The paper claims to "provide groundwork for more comprehensive evaluations of downstream applications," but focuses primarily on the representation gap metric.
- **Why unresolved:** Minimizing the representation error on test sets is a proxy; it is unverified if this metric correlates with success in interactive simulations like virtual classrooms.
- **What evidence would resolve it:** An end-to-end study where an intervention (e.g., an AI tutor) is optimized using the simulated population, with success measured by actual learning gains or response accuracy.

## Limitations

- **Embedding Quality Dependency:** The framework's success hinges on human behavior embeddings accurately capturing meaningful behavioral differences, but the paper doesn't validate that these embeddings truly represent the behavioral dimensions of interest.
- **Generalization Across Tasks:** While agents reproduce human behaviors on new tasks from the same domain, the paper doesn't test whether behavioral patterns transfer to genuinely different task types.
- **Computational Scaling:** Although proxy pools reduce complexity, the greedy selection still requires computing distances between all human-agent pairs, becoming expensive for large populations.

## Confidence

**High Confidence (Strong evidence, robust methodology):**
- The submodularity proof and greedy algorithm approximation guarantees are mathematically rigorous
- The behavioral consistency results showing agents reproduce human patterns on new tasks are empirically supported across multiple domains
- The computational complexity analysis and reduction to proxy pools are theoretically sound

**Medium Confidence (Solid evidence but some assumptions):**
- The claim that in-context learning effectively transfers behavioral patterns to new tasks relies on demonstration quality and task similarity
- The performance improvements over baselines are demonstrated but could be domain-specific
- The approximation guarantees for REPPOP_mapped variants depend on assumptions about human coverage and imitation error

**Low Confidence (Limited evidence or strong assumptions):**
- The framework's effectiveness for populations with very high behavioral diversity
- Performance on domains where human behavior doesn't exhibit the assumed consistency across tasks
- The quality of embeddings for complex, multi-dimensional human behaviors beyond the tested domains

## Next Checks

1. **Embedding Validation Study:** Before running the full optimization pipeline, conduct a validation study to verify that your human embeddings capture the behavioral dimensions you care about. Use clustering analysis to confirm that behaviorally similar humans are embedded close together, and visualize embeddings to check for meaningful structure.

2. **Cross-Domain Transfer Test:** After constructing your agent set, test behavioral consistency not just on new tasks from the same domain, but on qualitatively different task types. This validates whether the behavioral patterns are truly abstract and transferable.

3. **Scaling Experiment:** Run REPPOP_demo with varying population sizes (100, 500, 1000 humans) and measure both performance and runtime. Plot the trade-off curve to identify when REPPOP_mapped variants become necessary, and test the stochastic approximation with different sample sizes to find the minimum viable sample size that maintains performance.