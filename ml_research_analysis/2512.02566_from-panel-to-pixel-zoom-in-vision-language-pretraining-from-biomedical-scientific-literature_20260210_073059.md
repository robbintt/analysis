---
ver: rpa2
title: 'From Panel to Pixel: Zoom-In Vision-Language Pretraining from Biomedical Scientific
  Literature'
arxiv_id: '2512.02566'
source_url: https://arxiv.org/abs/2512.02566
tags:
- panel
- biomedical
- figure
- region
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving fine-grained visual-textual
  alignment in biomedical vision-language models. Current approaches compress rich
  scientific figures and text into coarse figure-level pairs, missing the localized
  grounding clinicians rely on.
---

# From Panel to Pixel: Zoom-In Vision-Language Pretraining from Biomedical Scientific Literature

## Quick Facts
- arXiv ID: 2512.02566
- Source URL: https://arxiv.org/abs/2512.02566
- Authors: Kun Yuan; Min Woo Sun; Zhen Chen; Alejandro Lozano; Xiangteng He; Shi Li; Nassir Navab; Xiaoxiao Sun; Nicolas Padoy; Serena Yeung-Levy
- Reference count: 40
- Primary result: Achieves state-of-the-art performance across multiple biomedical specialties using 60% less pretraining data than prior work.

## Executive Summary
This paper addresses the challenge of improving fine-grained visual-textual alignment in biomedical vision-language models. Current approaches compress rich scientific figures and text into coarse figure-level pairs, missing the localized grounding clinicians rely on. The authors propose Panel2Patch, a data generation pipeline that automatically mines hierarchical structure from scientific literature, including multi-panel figures, visual markers, and surrounding text, to construct multi-granular supervision at figure, panel, and patch levels. They also design a hierarchical zoom-in pretraining framework with inter-level message passing that enhances biomedical multimodal representations using multi-granularity context and fine-grained correspondences.

## Method Summary
Panel2Patch leverages off-the-shelf large vision-language models (Qwen2.5-VL-72B) to parse biomedical figures into hierarchical components: full figures, individual panels, and marker-indicated regions. The pipeline uses set-of-mark (SoM) prompting to decompose multi-panel figures and detect visual markers (arrows, asterisks) that indicate clinically relevant regions. Region proposals are generated from both marker boxes and caption-derived boxes, with caption boxes retained only if their centers lie within distance τ of a marker center. The resulting hierarchical corpus (364K figures, 1.3M panels, 619K regions) is used to pretrain a unified vision encoder with multi-level contrastive losses and inter-level message passing. The model is pretrained on 8 GPUs for 20 epochs with alternating batches across granularity levels.

## Key Results
- Achieves state-of-the-art zero-shot classification performance (avg 50.25%) across 6 biomedical specialties
- Demonstrates 60% reduction in pretraining data requirements compared to prior work
- Shows significant improvements in fine-grained retrieval tasks: single-panel I2T/T2I retrieval (R@1: 36.60%/38.24%) and bbox↔text retrieval (R@1: 8.64%/9.38%)

## Why This Works (Mechanism)

### Mechanism 1
Mining hierarchical supervision from pedagogical figure structure yields more effective training signal per sample than scaling raw figure-caption pairs. Scientific figures already encode explicit localization cues (panel labels A/B/C, arrows, zoom-in crops). Panel2Patch extracts three granularity levels—full figures, single panels, and marker-indicated regions—with corresponding localized captions, converting one coarse pair into multiple semantically aligned pairs.

### Mechanism 2
Marker-anchored fusion reduces LVLM hallucination in region-level grounding compared to caption-only or detector-only approaches. Region proposals come from two complementary sources: (1) marker boxes (arrows, asterisks) and (2) caption-derived boxes. Caption boxes are retained only if their centers lie within distance τ of a marker center, filtering spurious proposals while allowing marker boxes to be expanded locally.

### Mechanism 3
Inter-level message passing refines panel-level embeddings by propagating global context top-down and local evidence bottom-up. Figure embeddings are aligned with aggregated panel embeddings; panel embeddings are aligned with aggregated region embeddings. This bidirectional flow injects global semantics into panels while grounding them in fine-grained evidence.

## Foundational Learning

- **Concept: Contrastive Learning (CLIP-style)**
  - Why needed here: Panel2Patch uses image-text contrastive losses at all three granularities. Understanding how positive/negative pairs are formed and how InfoNCE-style objectives shape embedding spaces is essential.
  - Quick check question: Given a batch of 4 panel-caption pairs, how many negative pairs does each panel implicitly contrast against in standard CLIP training?

- **Concept: Set-of-Mark (SoM) Prompting**
  - Why needed here: The panel decomposition pipeline treats overlaid labels (A, B, C) as implicit SoM cues, prompting the LVLM to return bounding boxes and identifiers. Understanding SoM helps explain how off-the-shelf LVLMs perform structured visual parsing without domain-specific fine-tuning.
  - Quick check question: How does SoM prompting differ from standard visual question answering in terms of output structure requirements?

- **Concept: Alternating / Multi-Task Training**
  - Why needed here: Panel2Patch uses a coarse-to-fine alternating schedule (M→P→R) to prevent catastrophic forgetting and handle data imbalance across granularity levels.
  - Quick check question: Why might training on region-level data alone fail to recover panel-level semantics, even though regions are subsets of panels?

## Architecture Onboarding

- **Component map:**
  Raw Figures + Captions -> LVLM Parser (Qwen2.5-VL-72B + SoM Prompts) -> Panel Decomposition -> Marker + Caption Mining -> Post-processing (NMS, dedup, filtering) -> Hierarchical Corpus (364K figures | 1.3M panels | 619K regions) -> ViT-L/14 Encoder + Text Encoder -> Multi-level CLIP losses + Inter-level alignment + Alternating training

- **Critical path:**
  1. LVLM parsing quality directly determines panel-region alignment accuracy.
  2. Marker-proximity gating (τ) controls precision-recall tradeoff for region proposals.
  3. Alternating training schedule balances granularity coverage; mis-tuning causes forgetting.

- **Design tradeoffs:**
  - LVLM size vs. parsing cost: 72B model yields better panel boundaries but requires ~2,900 GPU-hours for 350K figures.
  - Precision vs. coverage in region mining: Strict τ reduces hallucinations but may miss valid unlabeled regions.
  - Single encoder vs. specialized heads: Unified encoder simplifies deployment but may underperform specialized models on any single granularity.

- **Failure signatures:**
  - Panel merge errors: Multi-panel figures parsed as single panels → coarse supervision persists.
  - Marker mislocalization: Arrow boxes capture the arrow glyph rather than the target region → misaligned region-text pairs.
  - Catastrophic forgetting: Region-only training degrades panel-level retrieval.

- **First 3 experiments:**
  1. Panel parsing validation: Manually annotate 100 multi-panel figures; compute panel IoU and identifier accuracy against LVLM outputs to establish parsing quality baseline.
  2. Ablation on τ threshold: Sweep τ ∈ {0.1, 0.2, 0.3, 0.5} on a held-out validation set; measure region-text retrieval R@5 vs. number of valid regions retained.
  3. Alternating schedule sensitivity: Compare M→P→R cycling vs. random sampling vs. level-wise pretraining; track single-panel retrieval and box↔text retrieval separately to diagnose forgetting.

## Open Questions the Paper Calls Out
None

## Limitations
- The pipeline relies heavily on the quality of Qwen2.5-VL-72B for panel and region parsing; errors in parsing propagate through the entire training pipeline.
- The approach assumes consistent pedagogical conventions across biomedical literature; figures with non-standard layouts or absent markers may not benefit as much.
- The marker proximity threshold (τ) and alternating training schedule are critical but not extensively validated across different datasets or figure types.

## Confidence
- **High:** The core data generation pipeline (Panel2Patch) and its hierarchical supervision structure are well-specified and reproducible.
- **Medium:** The effectiveness of marker-anchored fusion in reducing LVLM hallucinations is supported by the proposed mechanism but lacks direct empirical validation.
- **Low:** The ablation studies (e.g., single-panel vs. full hierarchical training) are limited in scope and may not fully capture the impact of each component on downstream performance.

## Next Checks
1. Panel parsing validation: Manually annotate 100 multi-panel figures from diverse biomedical specialties; compute panel IoU and identifier accuracy against LVLM outputs to establish parsing quality baseline.
2. Marker proximity threshold sweep: Sweep τ ∈ {0.1, 0.2, 0.3, 0.5} on a held-out validation set of biomedical figures; measure region-text retrieval R@5 vs. number of valid regions retained.
3. Alternating schedule sensitivity: Compare M→P→R cycling vs. random sampling vs. level-wise pretraining; track single-panel retrieval and box↔text retrieval separately to diagnose catastrophic forgetting.