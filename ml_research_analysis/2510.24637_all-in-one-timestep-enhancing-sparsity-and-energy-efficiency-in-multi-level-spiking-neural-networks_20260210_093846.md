---
ver: rpa2
title: 'All in one timestep: Enhancing Sparsity and Energy efficiency in Multi-level
  Spiking Neural Networks'
arxiv_id: '2510.24637'
source_url: https://arxiv.org/abs/2510.24637
tags:
- spiking
- energy
- multi-level
- spikes
- binary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# All in one timestep: Enhancing Sparsity and Energy efficiency in Multi-level Spiking Neural Networks

## Quick Facts
- arXiv ID: 2510.24637
- Source URL: https://arxiv.org/abs/2510.24637
- Authors: Andrea Castagnetti; Alain Pegatoquet; Benoît Miramond
- Reference count: 28
- Primary result: Multi-level SNNs with barrier neurons reduce spiking activity by 25-47% while maintaining accuracy comparable to binary SNNs

## Executive Summary
This paper introduces a multi-level spiking neural network (SNN) architecture that addresses two critical challenges: quantization error and energy efficiency. By encoding information in N-level spikes within a single timestep rather than increasing timesteps, the authors achieve better accuracy with fewer spikes. The key innovation is the "barrier neuron" with straight-through estimator that prevents spike avalanche in residual networks by controlling spike accumulation at skip-connection points. Experiments on CIFAR-10, CIFAR-100, and CIFAR-10-DVS show that the proposed Sparse-ResNet18 achieves comparable accuracy to standard SNNs while reducing spiking activity by 25-47% and estimated energy consumption by 25-30%.

## Method Summary
The method combines multi-level spiking neurons with barrier neurons in a residual architecture. Multi-level neurons subdivide each timestep into N micro-timesteps, accumulating inputs and producing valued spikes z(t) ∈ [0, N], enabling N×T+1 quantization levels. The barrier neuron is placed after residual summation points and uses straight-through estimator (STE) to preserve gradient flow while preventing exponential spike growth. Training uses sigmoid surrogate gradients for standard neurons and STE for barrier neurons. The architecture is evaluated on CIFAR-10, CIFAR-100, and CIFAR-10-DVS with ResNet18 backbone, using SGD (lr=0.08) for CIFAR datasets and Adam (lr=10⁻³) for DVS data.

## Key Results
- Multi-level encoding with T=1, N=4 achieves similar accuracy to binary encoding with T=4, N=1 (CIFAR-10: 93.13% vs 93.15%)
- Barrier neurons reduce spiking activity by 25-47% compared to SEW-ResNet (from 67,848 to 35,819 spikes at sum1)
- Sparse-ResNet with N=4-8 maintains accuracy within 0.5% of SEW-ResNet while using 25-30% fewer spikes
- Estimated energy consumption reduced by 25-30% compared to binary SNNs

## Why This Works (Mechanism)

### Mechanism 1: Multi-Level Spiking for Time-Space Quantization Tradeoff
Multi-level spikes increase quantization intervals from T+1 to N×T+1 by encoding amplitude information within a single timestep. This reduces quantization error without increasing latency, as demonstrated by functional equivalence between [T=4, N=1] and [T=1, N=4] configurations.

### Mechanism 2: Spike Avalanche Effect in Residual Skip Connections
Residual skip connections cause exponential spike growth through depth as spikes from both direct path and shortcut accumulate at summation points. This avalanche effect increases energy consumption independent of task requirements.

### Mechanism 3: Barrier Neuron with Straight-Through Estimator (STE)
Barrier neurons placed after residual summation points use STE to preserve gradient flow while preventing spike avalanche. STE bypasses vanishing gradients that occur when membrane potential is far from threshold, maintaining training stability.

## Foundational Learning

- **Integrate-and-Fire (IF) neuron dynamics**: Understanding membrane potential accumulation and soft-reset is prerequisite for multi-level neuron implementation. Quick check: Given input current 0.6, threshold 1.0, and soft-reset, what is membrane potential after one spike?

- **Surrogate gradient training**: Differentiates forward (discontinuous) from backward (smooth approximant) passes. Quick check: Why does sigmoid surrogate derivative σ'(x) ≈ 0 when membrane potential is far from threshold?

- **ResNet skip connections and identity mapping**: Understanding why ResNets work in ANNs clarifies what changes in SNNs. Quick check: In a standard ResNet block, what is the gradient flow through the skip connection during backpropagation?

## Architecture Onboarding

- **Component map**: Input -> Conv -> ml-SN/SG (direct path) + identity shortcut -> sum -> ml-SN/STE (barrier) -> Output
- **Critical path**: 1) Implement multi-level neuron with configurable N (start with N=4), 2) Build Sparse-ResNet18 with barrier neurons using STE, 3) Train with surrogate gradient (sigmoid, α=5) for all neurons except barrier (STE), 4) Measure activity: sum of spikes across all layers, normalized by dataset size
- **Design tradeoffs**: Higher N → better accuracy, more synaptic ops per spike, but fewer total spikes; sweet spot N=4-8 per experiments. STE vs. surrogate: STE preserves gradient magnitude but introduces approximation error; paper shows this is acceptable for barrier neurons. T=1 enforces minimal latency but requires N≥4 to match T=4 binary SNN accuracy
- **Failure signatures**: Accuracy collapse with T=1, N=1: Quantization error too high. Gradient vanishing in early residual blocks without STE: Training stalls, validation loss plateaus high. Energy higher than ANN despite SNN: Likely T too high or architecture not sparse (e.g., SEW-ResNet without barrier)
- **First 3 experiments**:
  1. Baseline parity check: Train VGG16 binary SNN with T=4 and multi-level SNN with T=1, N=4 on CIFAR-10; verify accuracy within 1% and activity reduced ~50%
  2. Avalanche isolation: Compare SEW-ResNet18 vs. Sparse-ResNet18 at T=1, N=4; log per-layer spike counts; confirm spike reduction at sum0, sum1
  3. Gradient flow validation: Train three variants (SEW, Sparse+SG, Sparse+STE); plot gradient norm per block and validation loss curves; confirm STE matches SEW gradient magnitude

## Open Questions the Paper Calls Out

### Open Question 1
Can the barrier neuron mechanism effectively mitigate the spike avalanche effect in architectures with dense shortcut connections, such as MobileNetV2 or deeper ResNets (e.g., ResNet34)? The paper validates Sparse-ResNet primarily on ResNet18; the scalability of the "barrier neuron" solution to networks with significantly more consecutive residual blocks remains empirically unverified.

### Open Question 2
Do the analytical energy efficiency gains of multi-level SNNs translate to realized savings on physical neuromorphic hardware? The results rely on an analytical model which assumes idealized memory access costs and synaptic operation efficiencies that may differ in actual silicon implementations.

### Open Question 3
Is the assumption that the synaptic energy cost scales linearly with the number of levels (N) valid for hardware implementations utilizing sparse multi-level data? If hardware can skip operations based on the specific value of the multi-level spike rather than treating it as a dense accumulation, the theoretical energy model used in the paper may overestimate costs.

## Limitations
- Exact surrogate gradient hyperparameter α (assumed 5) and batch size (assumed 128) are unspecified
- Avalanche mechanism relies on strong assumptions about spike correlation not validated by ablation studies
- STE gradient approximation's effect on very deep networks (>50 layers) remains untested

## Confidence
- Multi-level quantization mechanism: Medium (limited direct validation)
- Spike avalanche identification: Medium (novel diagnostic observation)
- STE effectiveness: High (within tested depth range)
- Energy model accuracy: Low (depends on external assumptions)

## Next Validation Checks
1. Verify multi-level neuron quantization claim by comparing accuracy of [T=4,N=1] vs [T=1,N=4] configurations on CIFAR-10 with identical training
2. Isolate avalanche effect by measuring per-layer spike counts in SEW-ResNet vs Sparse-ResNet, confirming 47% reduction at early residual blocks
3. Validate STE gradient preservation by plotting gradient norms across residual blocks for all three variants (SEW, Sparse+SG, Sparse+STE) during training