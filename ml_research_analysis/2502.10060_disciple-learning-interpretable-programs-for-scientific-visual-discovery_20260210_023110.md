---
ver: rpa2
title: 'DiSciPLE: Learning Interpretable Programs for Scientific Visual Discovery'
arxiv_id: '2502.10060'
source_url: https://arxiv.org/abs/2502.10060
tags:
- programs
- program
- data
- disciple
- population
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DiSciPLE is a framework for discovering interpretable, predictive
  programs from scientific visual data by leveraging large language models (LLMs)
  and evolutionary search. The approach addresses the challenge of creating interpretable
  models for complex scientific tasks where traditional interpretable methods lack
  expressive power and deep models lack interpretability.
---

# DiSciPLE: Learning Interpretable Programs for Scientific Visual Discovery

## Quick Facts
- **arXiv ID:** 2502.10060
- **Source URL:** https://arxiv.org/abs/2502.10060
- **Reference count:** 40
- **Primary result:** DiSciPLE achieves 35% lower error than closest non-interpretable baseline for population density estimation

## Executive Summary
DiSciPLE is a framework for discovering interpretable, predictive programs from scientific visual data by leveraging large language models (LLMs) and evolutionary search. The approach addresses the challenge of creating interpretable models for complex scientific tasks where traditional interpretable methods lack expressive power and deep models lack interpretability. DiSciPLE uses an evolutionary algorithm that starts with LLM-generated programs and iteratively improves them through crossover and mutation operations guided by LLMs. On three real-world scientific tasks (population density estimation, poverty prediction, and biomass estimation), DiSciPLE achieves state-of-the-art results while producing interpretable programs that scientists can understand.

## Method Summary
DiSciPLE uses an evolutionary algorithm that leverages LLMs to generate and evolve Python programs for scientific visual discovery. The process starts with M initial programs generated by an LLM using a task description, then iteratively improves them through T generations of parent selection, crossover, mutation, criticism, and simplification. The criticism component evaluates performance across data partitions and provides feedback to improve underperforming subgroups. The simplification component removes redundant code through AST analysis and regression-weight filtering. The framework combines neural primitives (like open-vocabulary segmentation) with symbolic operations to create expressive yet interpretable programs.

## Key Results
- DiSciPLE achieves 35% lower error than the closest non-interpretable baseline for population density estimation
- The framework produces highly accurate and interpretable programs for three scientific tasks: population density, poverty prediction, and biomass estimation
- Programs learned by DiSciPLE are both predictive and interpretable, enabling scientists to understand underlying mechanisms

## Why This Works (Mechanism)

### Mechanism 1
LLM-guided evolutionary search improves program discovery efficiency over random or purely symbolic search. The LLM provides common sense and prior knowledge about programming patterns, allowing programs to start from task-relevant baselines rather than random initialization. Crossover and mutation operations preserve semantic coherence because the LLM understands function meaning.

### Mechanism 2
Stratified program criticism accelerates search by identifying underperforming data subgroups. The critic partitions data (e.g., by land-use categories for geospatial tasks) and computes per-partition scores. Feedback directs the LLM to improve performance on poorly-scoring strata, reducing blind spots.

### Mechanism 3
Program simplification improves interpretability and prevents evolutionary bloat. Two-stage simplification removes unreachable code nodes through AST analysis and eliminates low-contribution features through regression weight analysis (threshold 5% of max weight).

## Foundational Learning

- **Concept: Evolutionary algorithms (crossover, mutation, fitness selection)**
  - **Why needed here:** DiSciPLE's core loop uses fitness-based parent selection and LLM-driven crossover/mutation to iteratively improve programs.
  - **Quick check question:** Can you explain why selecting high-fitness parents alone does not guarantee optimal offspring?

- **Concept: Neuro-symbolic programming**
  - **Why needed here:** The generated programs interleave neural primitives (open-vocabulary segmentation models) with symbolic operations (arithmetic, logic).
  - **Quick check question:** What is the key difference between a purely neural approach and a neuro-symbolic program for image classification?

- **Concept: Abstract Syntax Trees (AST)**
  - **Why needed here:** The simplifier uses AST representation to identify and remove unreachable code nodes.
  - **Quick check question:** Given a simple function, can you sketch its AST and identify which nodes are reachable from the return statement?

## Architecture Onboarding

- **Component map:** Input: Dataset D, metric M, primitive library F, task description descr → Initialization: LLM generates M initial programs → Evolution Loop (T generations): Parent Selection → LLM Crossover → LLM Mutation → Critic → Simplifier → Output: Best program P*

- **Critical path:** The fitness evaluation and LLM crossover/mutation calls dominate runtime. Each generation requires M program evaluations and ~M LLM calls. Paper uses T=15, M=100.

- **Design tradeoffs:**
  - Primitive library scope: Broader libraries increase expressiveness but expand search space. Paper uses domain-appropriate primitives.
  - Feature-set vs. direct prediction: Prompting LLM to generate feature lists then learning a linear regressor trades expressiveness for simpler optimization.
  - Simplification threshold: Lower thresholds prune more aggressively, improving interpretability but risking over-simplification.

- **Failure signatures:**
  - Programs degrade to mean prediction: Likely missing relevant primitives or LLM lacks task-relevant prior knowledge.
  - Rapid fitness plateau: Critic partitions may be uninformative or crossover operations insufficiently exploratory.
  - Excessive program length without improvement: Simplification threshold too high or pruning logic failing.
  - OOD generalization collapses: Programs overfitting to training strata; increase critic granularity or regularization.

- **First 3 experiments:**
  1. Reproduce population density baseline: Use provided benchmark data, verify DiSciPLE achieves ~0.26 L1-log error vs. reported 0.38 for next-best baseline.
  2. Ablate LLM prior knowledge: Rename primitives to meaningless tokens and remove task description from prompts. Confirm performance drops to near-random levels (L1-log ~0.84).
  3. Test new primitive addition: Add a domain-specific primitive (e.g., temperature lookup for biomass) and measure improvement speed.

## Open Questions the Paper Calls Out

### Open Question 1
Can advanced initialization strategies or second-order optimization methods be developed to enable differentiable optimization of parameters in intermediate computational layers of the synthesized programs? The authors note that standard gradient descent fails in intermediate layers due to zero-gradients in large parts of the input space.

### Open Question 2
Can the framework be extended to autonomously synthesize or learn new primitive functions if the provided library is insufficient to solve the task? The evolutionary search is currently restricted to recombining existing functions.

### Open Question 3
Does the inclusion of a human-in-the-loop feedback mechanism during the evolutionary process significantly accelerate convergence or improve program robustness compared to the fully automated approach? The current system relies entirely on metric-based fitness scores.

## Limitations
- Performance heavily depends on LLMs having sufficient prior knowledge about programming patterns and scientific tasks
- Simplification approach assumes linear feature importance correlates with regression weights, which may fail for non-linear feature interactions
- Evolutionary search assumes meaningful data partitions exist for criticism, but this may not hold for all scientific tasks
- Requires significant computational resources for iterative LLM calls (M=100 programs × T=15 generations)

## Confidence

- **High confidence:** The overall framework design and results on the three benchmark tasks are well-supported by the paper's evidence
- **Medium confidence:** The claim of 35% lower error than closest non-interpretable baseline for population density estimation is supported but would benefit from independent verification
- **Medium confidence:** Interpretability claims are reasonable given program-based output format, but systematic evaluation of scientist feedback is lacking

## Next Checks

1. **Validate ablation dependencies:** Independently reproduce the ablation results (removing critic, simplifier, and LLM prior knowledge) to confirm reported performance drops match paper findings.

2. **Test on out-of-distribution data:** Evaluate DiSciPLE on data from completely different geographic regions or domains not represented in training data to assess true generalization capabilities.

3. **Assess computational efficiency:** Measure actual computational cost (LLM API calls, runtime) of evolutionary search and compare against reported T=15, M=100 configuration to verify scalability claims.