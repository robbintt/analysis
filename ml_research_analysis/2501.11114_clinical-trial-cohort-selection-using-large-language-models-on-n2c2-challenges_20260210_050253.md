---
ver: rpa2
title: Clinical trial cohort selection using Large Language Models on n2c2 Challenges
arxiv_id: '2501.11114'
source_url: https://arxiv.org/abs/2501.11114
tags:
- clinical
- patient
- selection
- language
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of large language models (LLMs)
  for clinical trial cohort selection using n2c2 challenge datasets. A two-stage approach
  compares various LLMs with different prompting strategies (basic, few-shot, and
  iterated few-shot learning) on the n2c2-2018 dataset, then fine-tunes the best model
  on n2c2-2006 and n2c2-2008 datasets.
---

# Clinical trial cohort selection using Large Language Models on n2c2 Challenges

## Quick Facts
- **arXiv ID**: 2501.11114
- **Source URL**: https://arxiv.org/abs/2501.11114
- **Reference count**: 40
- **Primary result**: LLMs show promise for straightforward clinical trial cohort selection but struggle with nuanced criteria

## Executive Summary
This study evaluates large language models (LLMs) for clinical trial cohort selection using n2c2 challenge datasets. The research employs a two-stage approach comparing various LLMs with different prompting strategies on n2c2-2018 data, followed by fine-tuning on n2c2-2006 and n2008 datasets. Results demonstrate that LLMs perform well on straightforward selection criteria such as smoking status and medication use, with vicuna-13b and mistral-7b-instruct models showing superior stability. However, the models face challenges with nuanced or imbalanced tasks, highlighting limitations in fine-grained reasoning required for complex clinical scenarios.

## Method Summary
The study uses a two-stage evaluation framework: first comparing multiple LLMs (including vicuna-13b, mistral-7b-instruct, llama-3.1-8b, and gemma-7b) with basic, few-shot, and iterated few-shot prompting strategies on the n2c2-2018 dataset; then fine-tuning the best-performing model on n2c2-2006 and n2c2-2008 datasets. Performance is measured against traditional methods across various clinical selection criteria, focusing on both straightforward tasks (smoking status, medication use) and more complex scenarios requiring nuanced interpretation.

## Key Results
- LLMs achieve strong performance on simple selection criteria like smoking status and aspirin use
- Vicuna-13b and mistral-7b-instruct models demonstrate the highest stability across tasks
- Models struggle with nuanced or imbalanced criteria such as abdominal issues and complex decision-making scenarios

## Why This Works (Mechanism)
LLMs leverage their pre-trained medical knowledge and contextual understanding to process clinical text and identify relevant selection criteria. The few-shot and iterated few-shot learning approaches enable models to adapt to specific trial requirements without extensive retraining. Fine-tuning on domain-specific datasets helps align model outputs with clinical trial selection standards.

## Foundational Learning
- **Clinical Trial Eligibility Criteria**: Understanding structured selection requirements for patient cohorts
  - Why needed: Provides context for what constitutes valid cohort selection
  - Quick check: Can the model distinguish between inclusion and exclusion criteria?

- **Few-shot Learning**: Training with limited examples to adapt to new tasks
  - Why needed: Enables efficient adaptation without full retraining
  - Quick check: Does performance improve with additional examples?

- **Medical Text Processing**: Handling clinical narratives and structured data
  - Why needed: Clinical trials involve diverse document formats and terminology
  - Quick check: Can the model accurately parse both structured and unstructured medical data?

## Architecture Onboarding

**Component Map**: Patient Records -> LLM (Vicuna-13b/Mistral-7b) -> Selection Criteria -> Output Labels

**Critical Path**: Input patient data → Prompt processing → Criteria evaluation → Final selection decision

**Design Tradeoffs**: Few-shot learning offers flexibility but may lack precision for complex criteria; fine-tuning improves domain specificity but requires additional data and computational resources

**Failure Signatures**: Poor performance on rare conditions, difficulty with nuanced interpretation, sensitivity to input formatting and prompt structure

**First Experiments**:
1. Evaluate baseline performance on n2c2-2018 with different prompting strategies
2. Test few-shot learning effectiveness across multiple clinical criteria types
3. Assess fine-tuned model performance on n2c2-2006 and n2c2-2008 datasets

## Open Questions the Paper Calls Out
The study acknowledges limitations in generalizability beyond the n2c2 datasets used, noting that performance on straightforward criteria doesn't necessarily translate to complex clinical scenarios. The research also highlights the need for better understanding of LLM limitations in handling nuanced or imbalanced criteria and the potential impact of biases in model outputs on clinical decision-making.

## Limitations
- Limited evaluation scope focused primarily on n2c2 datasets
- Insufficient exploration of complex or rare clinical conditions
- No comprehensive analysis of potential biases in LLM outputs
- Comparison with traditional methods constrained by specific metrics

## Confidence
- **High**: LLMs perform well on straightforward selection criteria (e.g., smoking status, aspirin use)
- **Medium**: LLMs outperform or match traditional methods for simple tasks but struggle with nuanced or imbalanced criteria
- **Low**: Generalizability of findings to broader clinical trial contexts and complex scenarios remains uncertain

## Next Checks
1. Evaluate LLM performance on diverse clinical trial datasets beyond n2c2 to assess generalizability across different medical domains and trial types
2. Test LLM robustness on rare or complex clinical scenarios to identify limitations in handling nuanced or imbalanced criteria
3. Compare LLM outputs with domain expert annotations to validate accuracy and interpretability in real-world clinical decision-making contexts