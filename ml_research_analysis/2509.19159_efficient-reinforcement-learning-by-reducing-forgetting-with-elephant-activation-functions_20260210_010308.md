---
ver: rpa2
title: Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation
  Functions
arxiv_id: '2509.19159'
source_url: https://arxiv.org/abs/2509.19159
tags:
- learning
- frame
- elephant
- activation
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates catastrophic forgetting in reinforcement
  learning through the lens of activation functions. The authors theoretically analyze
  the role of activation functions in training dynamics, showing that sparse gradients,
  alongside sparse representations, are crucial for mitigating forgetting.
---

# Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions

## Quick Facts
- arXiv ID: 2509.19159
- Source URL: https://arxiv.org/abs/2509.19159
- Reference count: 40
- One-line primary result: Elephant activation functions significantly improve sample efficiency and memory efficiency in RL by reducing catastrophic forgetting through sparse gradients and representations.

## Executive Summary
This paper addresses catastrophic forgetting in reinforcement learning by introducing elephant activation functions. The authors theoretically analyze how activation functions influence forgetting dynamics and demonstrate that sparse gradients and sparse representations are crucial for mitigating interference between past and present experiences. Elephant activation functions are designed to generate both sparse outputs and sparse gradients, enabling neural networks to retain performance even with limited memory. Experimental results show substantial improvements in DQN and Rainbow algorithms, particularly under memory constraints, with consistent gains across various tasks.

## Method Summary
The method replaces classical activation functions (ReLU, Tanh) with the elephant activation function in RL value networks. The elephant function is defined as h/(1 + |x/a|^d), producing bell-shaped curves with sparse outputs and gradients. The approach requires LayerNorm preprocessing and specific initialization of biases. For Atari experiments, only the penultimate layer uses elephant activation while earlier CNN layers retain ReLU. The learnable parameters h (height) and a (width) are optimized via gradient descent, while d (slope) is typically fixed.

## Key Results
- Elephant activation functions significantly improve sample efficiency and memory efficiency in RL value networks
- Performance gains are most pronounced with small replay buffers (e.g., 32 samples) where other activations show severe degradation
- Even with large replay buffers, elephant activation consistently boosts learning performance across multiple tasks
- The approach works by reducing gradient interference through sparse gradients and enabling local elasticity in function updates

## Why This Works (Mechanism)

### Mechanism 1: Gradient Sparsity Reduces Interference
If elephant activation produces sparse gradients, gradient overlap between dissimilar inputs approaches zero, mitigating catastrophic forgetting. The narrow bell-shaped derivative creates a sparse Neural Tangent Kernel, ensuring weight updates for new samples have negligible dot product with gradients for older, dissimilar samples. This requires inputs to be sufficiently dissimilar in feature space to activate separate regions.

### Mechanism 2: Sparse Outputs Enable Memory Efficiency
Elephant activation generates sparse representations by decaying to zero for inputs outside width a. This creates sparse feature vectors that reduce interference in linear approximations and complement gradient sparsity in nonlinear ones. The network can maintain performance with significantly reduced replay buffer sizes due to these sparse representations.

### Mechanism 3: Local Elasticity via Point-wise Editing
Combined sparsity allows the network to edit value predictions for specific states without altering predictions elsewhere. The function update is constrained to small local areas around training samples, preventing the global smearing of error typical in ReLU/Tanh networks. This enables precise local corrections rather than global generalization shifts.

## Foundational Learning

- **Neural Tangent Kernel (NTK)**: The mathematical framework for analyzing forgetting through gradient inner products. Why needed: Understanding NTK is required to see why sparse gradients lead to zero forgetting. Quick check: Does high NTK off-diagonal indicate high or low interference?
- **Local Elasticity**: The functional property enabling point-wise updates without global side effects. Why needed: This is what elephant activation induces. Quick check: How should prediction change for input x when trained on dissimilar input x_t?
- **Stability-Plasticity Dilemma**: The trade-off between retaining old knowledge and learning new knowledge. Why needed: Parameters a and d explicitly control this trade-off. Quick check: What happens to plasticity if width a is too small?

## Architecture Onboarding

- **Component map**: LayerNorm -> Linear layer -> Elephant Activation -> Value output
- **Critical path**: Apply LayerNorm to inputs, pass through Linear layer (with specific bias init), apply Elephant Activation, tune width a carefully
- **Design tradeoffs**: Width a balances sparsity (small) vs. dead neurons (too small); slope d affects boundary sharpness vs. optimization smoothness; architecture is most effective in penultimate layer
- **Failure signatures**: All outputs near zero (a too small), no learning signal (d too large), instability (missing LayerNorm)
- **First 3 experiments**: 1) Sine wave regression to verify local elasticity, 2) Small buffer DQN on Gym task comparing elephant vs ReLU, 3) Visualize NTK/gradient covariance matrix to confirm diagonal structure

## Open Questions the Paper Calls Out

- Can elephant activation functions effectively improve sample and memory efficiency in model-based RL agents? The authors have not explored applicability in model-based RL beyond model-free algorithms.
- Why do elephant activation functions provide significant gains in value-based methods but show no clear advantage over ReLU in policy gradient methods? The appendix notes elephant does not show significant advantage for policy gradient methods.
- Is there a principled method for selecting elephant activation function's hyper-parameters (width a and slope d) that avoids task-specific manual tuning? The authors identify lack of principled hyper-parameter selection as a limitation.

## Limitations

- Effectiveness relies heavily on assumption that inputs are sufficiently dissimilar; poor tuning of width parameter a can negate theoretical advantages
- Focus primarily on value-based RL algorithms without extensive validation in policy gradient or actor-critic methods
- LayerNorm is presented as critical but sensitivity to LayerNorm parameters and interactions are not thoroughly explored

## Confidence

- **High Confidence**: Mathematical framework around gradient sparsity and NTK is sound and well-supported by theory; empirical improvements in small replay buffer scenarios are consistently demonstrated
- **Medium Confidence**: Broader claim that elephant activation universally improves sample efficiency across all memory regimes is supported by experiments but magnitude varies by task
- **Low Confidence**: Assertion that elephant activation can replace other forgetting mitigation techniques without trade-offs is not fully validated due to limited ablation studies

## Next Checks

1. Sensitivity Analysis of Width Parameter a: Systematically vary a across multiple orders of magnitude on MountainCar to identify optimal range and characterize performance drop-off
2. Cross-RL Algorithm Validation: Implement elephant activation in PPO and compare performance against DQN/Rainbow to test generalizability beyond value-based methods
3. Comparison with Regularization Baselines: Add L2 regularization baseline to Atari experiments and quantify whether elephant activation's forgetting reduction is complementary to or redundant with standard regularization