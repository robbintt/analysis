---
ver: rpa2
title: 'WebSeer: Training Deeper Search Agents through Reinforcement Learning with
  Self-Reflection'
arxiv_id: '2510.18798'
source_url: https://arxiv.org/abs/2510.18798
tags:
- tool
- wikipedia
- search
- answer
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WebSeer is a reinforcement learning-based search agent that achieves
  state-of-the-art performance on multi-hop question answering by incorporating self-reflection
  mechanisms. It extends tool-use chains through a two-stage training framework combining
  cold-start data synthesis with Self-Reflective Reinforcement Learning (SRRL), which
  allows iterative answer refinement based on external feedback.
---

# WebSeer: Training Deeper Search Agents through Reinforcement Learning with Self-Reflection

## Quick Facts
- **arXiv ID**: 2510.18798
- **Source URL**: https://arxiv.org/abs/2510.18798
- **Reference count**: 40
- **Primary result**: WebSeer achieves 72.3% accuracy on HotpotQA and 90.0% on SimpleQA, outperforming previous approaches by 12.5 and 11.4 percentage points respectively

## Executive Summary
WebSeer is a reinforcement learning-based search agent that achieves state-of-the-art performance on multi-hop question answering by incorporating self-reflection mechanisms. It extends tool-use chains through a two-stage training framework combining cold-start data synthesis with Self-Reflective Reinforcement Learning (SRRL), which allows iterative answer refinement based on external feedback. The model, using a single 14B parameter architecture, demonstrates strong generalization to out-of-distribution datasets while significantly increasing average tool call count from 3.57 to 13.43 on HotpotQA.

## Method Summary
WebSeer uses a two-stage training framework: first, cold-start data synthesis via multi-turn rejection sampling where a reasoner generates trajectories and an independent verifier retains only those matching ground truth, creating training data with reflective error-recovery patterns; second, Self-Reflective Reinforcement Learning (SRRL) that allows multiple answer submissions per dialogue turn with F1 score feedback, trained with Group Relative Policy Optimization (GRPO) and asymmetric clipping. The system uses a single 14B parameter Qwen2.5 model with four tools (Google Search, Webpage Reader, Code Executor, Submit Answer) and achieves stable reasoning through masked autoregressive loss that excludes tool observation tokens from the objective.

## Key Results
- Achieves 72.3% accuracy on HotpotQA, outperforming previous approaches by 12.5 percentage points
- Achieves 90.0% accuracy on SimpleQA, outperforming previous approaches by 11.4 percentage points
- Increases average tool call count from 3.57 to 13.43 on HotpotQA while maintaining accuracy
- Demonstrates strong generalization to out-of-distribution datasets (FanoutQA, FRAMES, Bamboogle)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multi-turn rejection sampling produces cold-start data that teaches models reflective error-recovery patterns, enabling longer tool-use chains.
- **Core assumption**: Models trained on trajectories containing reflective patterns will generalize self-correction behaviors to unseen queries.
- **Evidence**: Abstract states dataset construction includes "reflection patterns" and section 2.3 describes retaining reasoning paths that "ultimately converge to the correct solution."

### Mechanism 2
- **Claim**: SRRL enables iterative answer refinement by allowing multiple submissions per dialogue turn with explicit feedback signals.
- **Core assumption**: Models can learn to interpret numerical feedback signals and adjust subsequent reasoning accordingly when trained with GRPO.
- **Evidence**: Abstract mentions "iterative answer refinement based on external feedback" and section 2.4 describes submitting answers multiple times with F1 score as text feedback.

### Mechanism 3
- **Claim**: Trajectory-wise reward with exponential discount on resubmissions balances answer quality against excessive retrying.
- **Core assumption**: Exponential discount provides sufficient pressure to converge quickly while still rewarding eventual correctness.
- **Evidence**: Section 2.5 details exponential discount α ∈ (0,1] for R_correct, and section 3.3 shows post-RL distribution sharpens to 5-8 calls, indicating effective reward shaping.

## Foundational Learning

- **Concept: Masked Autoregressive Loss for Tool Observations**
  - Why needed: The SFT objective excludes external tool observation tokens from loss, training the model only on its own reasoning outputs and tool-calling decisions.
  - Quick check: Can you explain why including raw tool outputs in the loss might harm generalization?

- **Concept: Group Relative Policy Optimization (GRPO) with Asymmetric Clipping**
  - Why needed: WebSeer uses GRPO with DAPO's clip-higher mechanism to handle asymmetric update ranges during RL, preventing overfitting to noisy advantages in long trajectories.
  - Quick check: How does asymmetric clipping (ϵ_low ≠ ϵ_high) differ from standard PPO clipping?

- **Concept: Multi-hop Question Answering Evaluation (F1, LLM-as-Judge)**
  - Why needed: Paper uses F1 for intermediate feedback and LLM-as-Judge for final evaluation, since rule-based string matching fails on answer surface-form variations.
  - Quick check: Why might exact match metrics underestimate agent performance on open-domain QA?

## Architecture Onboarding

- **Component map**: Question → [Reasoning Loop: Think → Tool Call → Observation]* → Submit Answer → Feedback (F1) → Context append

- **Critical path**:
  1. Implement 4 tools as Python functions with JSON-structured outputs
  2. Build rejection sampling pipeline: reasoner generates trajectory → verifier checks → retain if valid
  3. Train SFT on retained trajectories with masked loss (exclude observation tokens)
  4. Implement SRRL: modify environment to allow multiple submits, return F1 as text
  5. Train with GRPO/DAPO on verl framework (12 prompts × 8 trajectories × 30 turns)

- **Design tradeoffs**:
  - Single 14B model vs. multi-agent: Simpler deployment but requires sufficient capacity; 3B/7B models show instability
  - Wikipedia-only training vs. open-web: More stable signals but requires transfer to real search at inference
  - Data mixing ratio (single-pass vs. multi-refinement trajectories): Higher multi-refinement increases tool calls but may not improve accuracy

- **Failure signatures**:
  - Repetitive text / malformed JSON in tool calls → model capacity insufficient (<14B) or inadequate SFT cold-start
  - Reward increases but behavior collapses → overfitting to reward hacking; check trajectory diversity
  - Tool calls cluster at extremes (0-2 or 20+) → reward shaping broken or data mixing off

- **First 3 experiments**:
  1. **SFT-only baseline**: Train 14B model on rejection-sampled data without RL; measure tool call count and accuracy on HotpotQA to isolate cold-start contribution
  2. **Ablate feedback format**: Replace F1-score text feedback with binary correct/incorrect; compare convergence speed and final accuracy to validate rich-feedback hypothesis
  3. **Scale sweep**: Train 3B, 7B, 14B variants with identical pipeline; log instability events (malformed JSON, repetitive outputs) to establish minimum viable capacity

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can architectural modifications or curriculum learning strategies stabilize Self-Reflective Reinforcement Learning (SRRL) for models with fewer than 10 billion parameters?
- **Basis**: Section 3.3 states that smaller models (3B, 7B) "suffer from instability, including repetitive text and malformed JSON," and that "sufficient scale is crucial for stable reasoning."
- **What would resolve**: Successful training runs of 7B or 3B models using SRRL that achieve stable tool use without "repetitive text" or "collapsed" behavior.

### Open Question 2
- **Question**: What is the theoretical relationship between the ratio of single-pass versus multi-refinement trajectories in the cold-start phase and the convergence efficiency of the subsequent reinforcement learning stage?
- **Basis**: Section 3.3 notes that the "data mixing ratio... can even determine the success of subsequent RL fine-tuning," but the optimal ratio is presented as an empirical finding (ratio=1.5) rather than a solved problem.
- **What would resolve**: A systematic ablation study showing the impact of varying SFT mixing ratios on RL reward convergence speed and final policy entropy across different reasoning domains.

### Open Question 3
- **Question**: How does WebSeer's reliance on explicit ground-truth feedback (F1 score) during RL affect its ability to generalize to tasks where verification is subjective or verification tools are absent?
- **Basis**: The SRRL framework relies on comparing submitted answers against ground truth to generate text-based feedback; the Conclusion claims the foundation is laid for "general-purpose reasoning agents," but training depends on this specific feedback loop.
- **What would resolve**: Experiments showing WebSeer's performance on open-ended tasks using only intrinsic rewards or self-consistency checks without external ground-truth comparison.

## Limitations
- Cold-start data construction heavily depends on verifier model quality, which is not fully specified
- GRPO/DAPO implementation details (learning rate, clipping parameters, KL penalty) are omitted
- Data mixing ratio between single-pass and multi-refinement trajectories is only explored in a narrow range (1.5-2.0)

## Confidence

**High Confidence (Evidence strongly supports mechanism):**
- Cold-start data synthesis with rejection sampling effectively teaches error-recovery patterns
- SRRL framework with multiple answer submissions and F1 feedback improves answer quality
- WebSeer achieves state-of-the-art results on HotpotQA (72.3%) and SimpleQA (90.0%)

**Medium Confidence (Evidence supports but with gaps):**
- Reward shaping with exponential discount prevents excessive retrying without sacrificing correction ability
- Single 14B model architecture is sufficient and simpler than multi-agent approaches
- Transfer from Wikipedia-only training to real web search is effective

**Low Confidence (Mechanisms underexplored):**
- Optimal data mixing ratios between trajectory types
- Verifier model quality impact on cold-start data
- Long-term stability of RL-trained agents across diverse domains

## Next Checks

1. **Verifier Quality Impact Study**: Systematically vary verifier accuracy (using both high-quality and deliberately degraded verifiers) during cold-start data construction, then measure resulting agent performance to quantify sensitivity to verifier errors.

2. **Alpha Discount Parameter Sweep**: Train identical models across α ∈ {0.7, 0.8, 0.9, 0.95, 0.99} while monitoring tool call distribution, answer quality trajectory, and convergence speed to identify optimal reward shaping for different task types.

3. **Cross-Domain Transfer Evaluation**: Test WebSeer trained on Wikipedia-based datasets directly on real-world search queries from diverse domains (news, technical documentation, academic papers) to validate generalization claims beyond the reported benchmarks.