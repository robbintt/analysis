---
ver: rpa2
title: On Membership Inference Attacks in Knowledge Distillation
arxiv_id: '2505.11837'
source_url: https://arxiv.org/abs/2505.11837
tags:
- distillation
- student
- data
- teacher
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically examines how knowledge distillation affects
  membership inference attack (MIA) vulnerability in large language models (LLMs).
  Through comprehensive experiments on six teacher-student model pairs, the authors
  find that distilled student models do not consistently exhibit lower MIA success
  than their teacher models, and in some cases show substantially higher member-specific
  attack success.
---

# On Membership Inference Attacks in Knowledge Distillation

## Quick Facts
- arXiv ID: 2505.11837
- Source URL: https://arxiv.org/abs/2505.11837
- Reference count: 30
- Knowledge distillation can amplify membership inference attack vulnerability in student models rather than reduce it

## Executive Summary
This paper systematically examines how knowledge distillation affects membership inference attack (MIA) vulnerability in large language models (LLMs). Through comprehensive experiments on six teacher-student model pairs, the authors find that distilled student models do not consistently exhibit lower MIA success than their teacher models, and in some cases show substantially higher member-specific attack success. This occurs because mixed supervision in distillation—combining ground-truth labels and teacher predictions—reinforces memorization on vulnerable training data points where teacher predictions align with ground-truth, leading students to produce overly confident outputs that amplify separability between members and non-members.

To mitigate this vulnerability, the authors propose three privacy-preserving distillation methods: (1) restricting distillation to non-vulnerable training data points, (2) introducing a low-dimensional bottleneck projection that limits representational capacity, and (3) replacing layer normalization with a simpler element-wise affine transformation (NoNorm). Experiments demonstrate these interventions successfully reduce student models' vulnerability to MIA, with the architectural modifications (bottleneck projection and NoNorm) lowering attack success while largely preserving model utility, offering a reliable approach for distilling more secure and efficient student models.

## Method Summary
The authors conduct a systematic evaluation of MIA vulnerability across six teacher-student pairs using Pythia, Gemma 2, and Llama 3 model families. They implement six MIA methods (ReCaLL, Loss, Zlib, Min-K%, Min-K%++, Reference-model) and measure aggregate MIA accuracy (A(T) = 0.5*[TPR + TNR]) and member-specific TPR. The distillation process uses mixed supervision with L_distill = L_CE + λ*KL(p_T||p_S), combining cross-entropy with ground-truth labels and KL divergence with teacher predictions. To address discovered vulnerabilities, they propose three defenses: (1) non-vulnerable-only distillation by partitioning training data via MIA, (2) Bottleneck Projection reducing intermediate dimension from H to B (B=384), and (3) NoNorm replacing LayerNorm with γ∘h+β. They evaluate privacy-utility trade-offs using perplexity metrics.

## Key Results
- Distilled student models do not consistently show lower MIA vulnerability than their teacher models
- In some cases, students exhibit substantially higher member-specific attack success rates
- Mixed supervision in distillation reinforces memorization on vulnerable training data points
- The proposed architectural interventions (Bottleneck Projection and NoNorm) successfully reduce MIA vulnerability while preserving utility

## Why This Works (Mechanism)
The vulnerability amplification occurs through the interaction of mixed supervision and teacher-student alignment. When teacher predictions align with ground-truth labels on vulnerable data points, the student receives reinforced signals that strengthen memorization of these specific examples. This creates overly confident outputs where members and non-members become more separable, making inference attacks more successful. The proposed defenses work by either limiting exposure to vulnerable data (non-vulnerable-only distillation) or constraining the model's capacity to memorize and overfit (bottleneck projection and simplified normalization).

## Foundational Learning
- Knowledge Distillation: Transferring knowledge from large teacher models to smaller student models; needed to understand the context and why distillation is used
- Membership Inference Attacks: Methods to determine if specific data points were used in training; needed to evaluate privacy vulnerabilities
- Mixed Supervision: Combining ground-truth labels with teacher predictions during training; needed to understand the core mechanism of vulnerability
- Layer Normalization: Normalizing layer outputs using mean and variance; needed to understand the architectural modification
- NoNorm: Element-wise affine transformation replacing LayerNorm; needed to understand the proposed architectural defense
- Bottleneck Projection: Reducing intermediate representation dimension; needed to understand the architectural capacity constraint

## Architecture Onboarding
**Component Map:** Teacher model -> Distillation process (L_CE + KL) -> Student model -> MIA evaluation
**Critical Path:** Mixed supervision training -> Memorization reinforcement -> Confident outputs -> Increased separability -> Higher MIA success
**Design Tradeoffs:** Mixed supervision improves utility but increases vulnerability; architectural constraints reduce vulnerability but may limit capacity
**Failure Signatures:** Students showing no MIA vulnerability increase likely using pure soft-label distillation instead of mixed supervision
**First Experiments:** 1) Verify distillation uses both L_CE and KL terms, 2) Test Bottleneck Projection with different B dimensions, 3) Compare NoNorm against LayerNorm for privacy-utility trade-off

## Open Questions the Paper Calls Out
- Can a theoretical framework be established to formally characterize memorization dynamics under the mixed supervision paradigm used in knowledge distillation?
- Are the proposed architectural interventions (Bottleneck Projection and NoNorm) compatible with or complementary to formal privacy mechanisms such as Differentially Private SGD (DP-SGD)?
- Does the finding that student models exhibit amplified member-specific vulnerability generalize to multimodal architectures (e.g., vision-language models) or distillation protocols that utilize intermediate representations?

## Limitations
- Distillation hyperparameters (λ weight, learning rate, batch size, optimizer) are not specified
- Reference-model attack details lack description of shadow model architecture and training
- Threshold selection procedure for binary MIA prediction is underspecified

## Confidence
- High confidence: The empirical finding that distilled students do not consistently show lower MIA vulnerability than teachers
- High confidence: The three proposed defense mechanisms effectively reduce MIA vulnerability
- Medium confidence: The theoretical explanation linking mixed supervision and memorization to increased vulnerability

## Next Checks
1. Implement the distillation process with both cross-entropy and KL divergence terms to verify pure soft-label distillation doesn't reproduce vulnerability increase
2. Systematically vary the bottleneck dimension B in Bottleneck Projection to characterize utility-privacy trade-off curve
3. Compare NoNorm architectural modification against other lightweight normalization alternatives (e.g., RMSNorm)