---
ver: rpa2
title: Self-supervised Analogical Learning using Language Models
arxiv_id: '2502.00996'
source_url: https://arxiv.org/abs/2502.00996
tags:
- question
- questions
- answer
- programs
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the reasoning inconsistency problem in large
  language models (LLMs), where models fail on seemingly similar problems due to memorization
  of final answers rather than understanding underlying reasoning processes. The authors
  propose SAL (Self-supervised Analogical Learning), a framework that trains models
  to explicitly transfer high-quality symbolic solutions from familiar cases to rare
  ones.
---

# Self-supervised Analogical Learning using Language Models

## Quick Facts
- arXiv ID: 2502.00996
- Source URL: https://arxiv.org/abs/2502.00996
- Reference count: 7
- Primary result: Improves reasoning consistency on benchmarks (StrategyQA, GSM8K, HotpotQA) by 2-20% over CoT baselines

## Executive Summary
This paper addresses the reasoning inconsistency problem in large language models, where models fail on seemingly similar problems due to memorization rather than understanding underlying reasoning processes. The authors propose SAL (Self-supervised Analogical Learning), a framework that trains models to explicitly transfer high-quality symbolic solutions from familiar cases to rare ones. SAL employs two extraction methods: conceptualization, which finds similar questions with identical reasoning paths and extracts their programmatic solutions, and simplification, which decomposes math questions into easier subproblems.

## Method Summary
SAL works by first training a base model (Mistral-7B) on seed data containing 4,619 instances with annotated reasoning programs. Then it generates self-supervision through two pipelines: conceptualization (finding similar questions with high-confidence CoT agreement and extracting their programs) and simplification (decomposing math questions into sub-steps). The extracted examples are filtered using soundness criteria and combined with seed data for final training. The approach achieves 2-20% improvements on reasoning benchmarks by explicitly teaching models to transfer reasoning patterns rather than memorize answers.

## Key Results
- Improves StrategyQA accuracy by 8-20% over chain-of-thought baselines
- Enhances GSM8K performance by 2-8% compared to seed-only approaches
- Shows better generalization and interpretability through programmatic inference
- Reduces tendency to simply repeat original questions in reasoning

## Why This Works (Mechanism)
SAL addresses the core issue that LLMs often memorize final answers rather than understanding reasoning processes. By explicitly training models to extract and transfer symbolic solutions from similar problems, SAL teaches the underlying reasoning patterns. The programmatic approach ensures interpretable and verifiable reasoning steps, while the self-supervised extraction allows scaling to larger datasets without manual annotation.

## Foundational Learning
- **Reasoning transfer**: Why needed - to handle rare or unseen problems using known solution patterns; Quick check - test model on novel variations of training problems
- **Programmatic reasoning**: Why needed - provides interpretable and verifiable solution steps; Quick check - verify generated Python code executes correctly
- **Self-supervision extraction**: Why needed - scales training without manual annotation; Quick check - measure yield rate of extracted high-quality examples
- **LoRA fine-tuning**: Why needed - efficient parameter adaptation for multi-task learning; Quick check - verify training converges within expected epochs

## Architecture Onboarding

### Component Map
Base Model (Mistral-7B) -> Seed Training -> Extraction Pipeline (Conceptualization + Simplification) -> Filtered Self-supervision -> Final Training

### Critical Path
1. Seed data preparation and fine-tuning
2. Self-supervision extraction using conceptualization and simplification
3. Filtering using soundness criteria
4. Combined training on seed + extracted data

### Design Tradeoffs
- **Programmatic vs. free-form reasoning**: Programs provide interpretability but may miss nuanced reasoning
- **Binary vs. multi-choice filtering**: Binary answers enable accurate filtering but limit applicability
- **Extraction yield vs. quality**: Higher yield increases training data but may include noisier examples

### Failure Signatures
- Low extraction yield (<10% collected rate) suggests base model struggles with instruction following
- High program execution error rate indicates poor robustness in generated code
- Minimal performance gains despite successful training suggest reasoning transfer isn't occurring

### First 3 Experiments
1. Verify seed training produces a multi-function model capable of abstraction, similarity detection, CoT, and programming
2. Test conceptualization pipeline on a small subset to measure agreement rate and program executability
3. Run ablation with only seed data vs. seed + extracted data to confirm self-supervision benefit

## Open Questions the Paper Calls Out
- How can SAL be adapted to support free-form or open-ended generation tasks? The current pipeline depends on binary answers for accuracy verification.
- Does SAL improve reasoning capabilities in larger or architecturally different base models? Experiments were only conducted on Mistral-7B due to computational constraints.
- Can simplification extraction improve performance on non-mathematical reasoning tasks? The method is explicitly targeted at math questions in GSM8K.

## Limitations
- Heavy dependency on quality seed supervision data limits applicability in truly unsupervised settings
- Complex execution environment required for program parsing and unit handling may be difficult to reproduce
- Computational cost of generating and filtering analogical examples may not be practical for production

## Confidence
- **High**: Empirical improvements on StrategyQA, GSM8K, and HotpotQA benchmarks are well-documented
- **Medium**: Generalizability across different LLM architectures and domains remains uncertain
- **Low**: Long-term robustness and potential catastrophic forgetting haven't been addressed

## Next Checks
1. Apply SAL to non-mathematical reasoning datasets (e.g., CommonsenseQA or ARC) to test cross-domain generalization
2. Systematically reduce seed supervision from 4,619 instances to determine minimum viable seed set size
3. Create controlled test suite to measure failure rate of program execution and parsing functions