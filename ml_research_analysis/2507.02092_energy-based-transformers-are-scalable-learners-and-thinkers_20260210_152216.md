---
ver: rpa2
title: Energy-Based Transformers are Scalable Learners and Thinkers
arxiv_id: '2507.02092'
source_url: https://arxiv.org/abs/2507.02092
tags:
- ebts
- thinking
- energy
- arxiv
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Energy-Based Transformers (EBTs) introduce a new class of Energy-Based
  Models (EBMs) that learn to verify compatibility between inputs and candidate predictions,
  enabling predictions through gradient descent-based energy minimization. This approach
  generalizes System 2 Thinking across modalities without requiring additional supervision
  or training beyond unsupervised pretraining.
---

# Energy-Based Transformers are Scalable Learners and Thinkers

## Quick Facts
- arXiv ID: 2507.02092
- Source URL: https://arxiv.org/abs/2507.02092
- Reference count: 40
- Primary result: EBTs achieve up to 35% higher scaling rates than Transformer++ and improve with System 2 thinking by 29% more than Transformer++ on language tasks

## Executive Summary
Energy-Based Transformers (EBTs) introduce a new approach to autoregressive modeling where predictions are made through gradient descent on a learned energy function rather than direct generation. This verification-based approach generalizes System 2 Thinking across modalities without requiring additional supervision beyond unsupervised pretraining. EBTs demonstrate superior scaling during pretraining and improved performance during inference when given more compute budget, while using fewer forward passes than Diffusion Transformers for image denoising.

## Method Summary
EBTs learn an energy function that assigns scalar values to input-prediction pairs, where lower energy indicates higher compatibility. During training, predictions are initialized randomly and iteratively refined through gradient descent on this energy landscape. The model uses modified attention mechanisms to handle prediction embeddings and applies landscape regularization techniques including replay buffers, Langevin dynamics, and randomized hyperparameters. During inference, EBTs can perform iterative refinement ("thinking") to improve predictions, with performance gains scaling with optimization steps.

## Key Results
- EBTs achieve up to 35% higher scaling rates than Transformer++ during pretraining across data, batch size, parameters, FLOPs, and depth
- EBTs improve performance with System 2 Thinking by 29% more than Transformer++ on language tasks
- EBTs outperform Diffusion Transformers on image denoising while using 99% fewer forward passes
- EBTs demonstrate better generalization, achieving lower perplexity on downstream tasks despite worse pretraining performance

## Why This Works (Mechanism)

### Mechanism 1
EBTs achieve better generalization by learning verification rather than direct generation. The model learns an energy function that assigns scalar values to input-prediction pairs, where lower energy indicates higher compatibility. Verification is computationally easier than generation, particularly for OOD data, resulting in better generalization. Evidence shows EBTs with worse pretraining perplexity achieve better downstream performance on 3/4 benchmarks.

### Mechanism 2
Treating prediction as optimization enables dynamic compute allocation per prediction. Starting from random initialization, predictions are iteratively refined through gradient descent on the learned energy landscape. More optimization steps provide more "thinking time," with energy scalar values providing implicit stopping criteria. EBTs improve performance by up to 29% with more forward passes while Transformer++ cannot improve.

### Mechanism 3
Energy landscape regularization techniques are necessary for System 2 thinking capabilities. Replay buffers simulate longer optimization trajectories, Langevin dynamics encourages landscape exploration through noise injection, and randomized step sizes prevent overfitting to specific optimization paths. Ablation shows removing randomization nearly eliminates thinking gains, while full configuration achieves 18.7% improvement.

## Foundational Learning

- **Energy-Based Models (EBMs)**: EBTs are a specific EBM implementation using Transformers; understanding energy as unnormalized log-likelihood is foundational. Quick check: Can you explain why lower energy = higher probability in p(x) ∝ e^(-E(x))?

- **Gradient computation through optimization trajectories**: EBT training requires backpropagating through multiple gradient descent steps, requiring Hessian-vector products (second-order derivatives). Quick check: How would you compute ∂loss/∂θ when the loss depends on ŷₙ, which itself was computed via n gradient descent steps?

- **Attention mechanisms and causal masking**: EBT's autoregressive implementation requires modified attention where predictions attend to themselves (superdiagonal) while maintaining causality. Quick check: Why can't standard causal attention handle predictions made in input space?

## Architecture Onboarding

- **Component map**: Energy head -> Prediction embeddings -> Modified attention -> Optimization loop -> Landscape regularizers
- **Critical path**: Initialize predictions as random noise → Concatenate context + prediction embeddings → Forward pass through Transformer → Compute gradient of energy w.r.t predictions → Update predictions via gradient descent → Repeat for N steps → Compute loss only at final step
- **Design tradeoffs**: S1 vs S2 models (detached vs non-detached predictions), optimization steps (more steps = better thinking but higher training cost), step size (too small → slow convergence; too large → instability)
- **Failure signatures**: Gradient magnitude explosion during training, energy not decreasing across optimization steps, mode collapse in multimodal distributions, adversarial samples with low energy but poor quality
- **First 3 experiments**: 
  1. S1 model sanity check: Train small EBT on language modeling; verify energy decreases and loss converges
  2. Hyperparameter sweep on α: Sweep step sizes while monitoring gradient magnitudes and training stability
  3. Minimal S2 experiment: Enable non-detached predictions + loss truncation + Langevin dynamics + randomized α; evaluate on OOD data with varying optimization steps

## Open Questions the Paper Calls Out

- How can EBTs be modified to effectively model data distributions with many modes, such as class-conditional image generation? Current approach causes modes to merge, resulting in blurred outputs.
- Do EBTs mitigate the "Reversal Curse" observed in standard autoregressive models? Theoretical advantage proposed but not experimentally validated.
- Do the 35% higher scaling rates persist at Foundation Model scales? Experiments limited to 800M parameters due to resource constraints.

## Limitations

- EBTs struggle with multi-modal distributions, causing modes to merge and resulting in blurred outputs for ambiguous conditions
- Core claims about verification being easier than generation remain largely theoretical without direct experimental evidence
- Scaling analysis based on limited experiments (800M parameters) and doesn't account for potential confounding factors

## Confidence

- **High Confidence**: EBTs can perform iterative inference through gradient descent on learned energy functions across multiple modalities
- **Medium Confidence**: EBTs achieve better scaling during pretraining and demonstrate System 2 thinking capabilities during inference
- **Low Confidence**: The core claim that verification is fundamentally easier than generation explains EBTs' superior generalization

## Next Checks

1. **Controlled verification vs generation experiment**: Design a synthetic task where both verification and generation can be precisely measured and compared to test which is computationally easier and whether this correlates with generalization performance.

2. **Cross-dataset generalization study**: Train EBTs and Transformer++ on one dataset and evaluate on multiple OOD datasets with varying difficulty levels to test whether EBTs' generalization advantage scales with OOD magnitude.

3. **Energy landscape visualization**: For a small-scale model, visualize the learned energy landscape during training and across different optimization steps to examine whether the landscape becomes smoother and more convex with regularization techniques.