---
ver: rpa2
title: Decoupling Return-to-Go for Efficient Decision Transformer
arxiv_id: '2601.15953'
source_url: https://arxiv.org/abs/2601.15953
tags:
- sequence
- transformer
- information
- learning
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses a theoretical and practical limitation in
  Decision Transformer (DT), where the entire Return-to-Go (RTG) sequence is fed as
  input, creating redundancy. The authors theoretically prove that only the most recent
  RTG is necessary for optimal action prediction, as past RTGs do not contribute new
  information under the POMDP framework.
---

# Decoupling Return-to-Go for Efficient Decision Transformer

## Quick Facts
- **arXiv ID**: 2601.15953
- **Source URL**: https://arxiv.org/abs/2601.15953
- **Reference count**: 11
- **Primary result**: DDT outperforms standard DT and achieves competitive results with state-of-the-art DT variants while reducing computational cost

## Executive Summary
This paper addresses a theoretical and practical limitation in Decision Transformer (DT), where the entire Return-to-Go (RTG) sequence is fed as input, creating redundancy. The authors theoretically prove that only the most recent RTG is necessary for optimal action prediction, as past RTGs do not contribute new information under the POMDP framework. They propose Decoupled Decision Transformer (DDT), which removes the RTG sequence from the Transformer input and uses only the latest RTG to modulate the action prediction via adaptive layer normalization (adaLN). Experiments on D4RL benchmarks show DDT significantly outperforms DT and achieves competitive results with state-of-the-art DT variants. Additionally, DDT reduces computational cost by shortening the input sequence length, making it more efficient and scalable. The findings suggest that DDT provides a more principled and effective approach to offline reinforcement learning through sequence modeling.

## Method Summary
DDT modifies the standard Decision Transformer by removing the RTG sequence from the input and replacing it with adaptive layer normalization (adaLN). The model takes only state and action tokens as input, using a GPT-style causal transformer. The adaLN module takes the scalar RTG as input and outputs scale and bias parameters that modulate the final hidden state before action prediction. The module is implemented as a single linear layer initialized to zeros, with no activation function. This design directly conditions the action prediction on the target return while eliminating redundant RTG tokens from the input sequence.

## Key Results
- DDT significantly outperforms standard DT on D4RL benchmarks across multiple environments
- DDT achieves competitive results with state-of-the-art DT variants while reducing computational cost
- Removing RTG sequence and using adaLN provides better performance than simple RTG masking approaches

## Why This Works (Mechanism)

### Mechanism 1: Redundancy Elimination via POMDP Sufficiency
- **Claim:** Conditioning on the full history of Return-to-Go (RTG) tokens is theoretically redundant; only the most recent RTG is necessary for optimal action prediction.
- **Mechanism:** In a Partially Observable Markov Decision Process (POMDP), the belief state is a sufficient statistic derived from observation-action history. Historical RTGs decompose into the current target (R_t) and past rewards (r_{t-k:t-1}). Since past rewards do not update the belief state (Eq. 1), they provide no new information for decision-making.
- **Core assumption:** The environment adheres to the standard POMDP framework where rewards do not contribute to state belief updates.
- **Evidence anchors:** [abstract]: "...feeding the entire sequence of RTGs into the Transformer is theoretically unnecessary, as only the most recent RTG affects action prediction." [section 4]: Eq. 4 mathematically decomposes the RTG sequence to isolate redundant past rewards.
- **Break condition:** If the environment is not a standard POMDP or if intermediate rewards are required to distinguish visually similar states.

### Mechanism 2: Attention Reallocation to State-Action Dynamics
- **Claim:** Removing the RTG sequence improves performance by forcing the Transformer to allocate attention capacity exclusively to state-action dependencies.
- **Mechanism:** The standard DT attends to (RTG, State, Action) tuples. By removing RTGs, the input sequence shortens (3k to 2k), and the attention mechanism is no longer diluted by redundant RTG tokens.
- **Core assumption:** The performance gap is due to attention dilution rather than a fundamental lack of capacity.
- **Evidence anchors:** [section 6.1]: Visualization shows DDT attention scores concentrate near the diagonal (local dependency), whereas DT attention is scattered.
- **Break condition:** If the task requires long-term credit assignment that relies on distinct "reward-desire" signals at every timestep.

### Mechanism 3: Direct Modulation via Adaptive Normalization
- **Claim:** Adaptive Layer Normalization (adaLN) provides a more effective method for conditioning on the target return than sequence concatenation.
- **Mechanism:** Instead of learning an attention mask to ignore historical RTGs, DDT uses a single linear layer to modulate the hidden state via scale (γ) and bias (β) derived from the latest RTG.
- **Core assumption:** The RTG (a scalar) carries simple conditioning information best applied via feature modulation rather than token interaction.
- **Evidence anchors:** [section 6.2]: "Blocked-DT" (masking only) yields minor gains; DDT (adaLN) yields significant gains, proving masking is insufficient.
- **Break condition:** If the conditioning signal requires complex, non-linear interaction with the hidden state dimensions.

## Foundational Learning

- **Concept: POMDP Belief State**
  - **Why needed here:** The theoretical justification for removing RTG history rests on the POMDP definition where the belief state is independent of past rewards.
  - **Quick check question:** Does the belief state b_t in a standard POMDP depend on the sequence of rewards received?

- **Concept: Return-to-Go (RTG)**
  - **Why needed here:** Understanding RTG as the cumulative future reward is critical to distinguishing between the "redundant" historical RTGs and the "useful" current RTG.
  - **Quick check question:** How does the RTG at time t differ from the RTG at time t-1?

- **Concept: Adaptive Layer Normalization (adaLN)**
  - **Why needed here:** This is the architectural replacement for the RTG sequence. You must understand how it dynamically shifts/scales activations based on the RTG input.
  - **Quick check question:** In adaLN, how are the scale (γ) and bias (β) parameters determined?

## Architecture Onboarding

- **Component map:** Input Embedder -> GPT Backbone -> adaLN Module -> Prediction Head
- **Critical path:** During inference, the model must strictly use the latest RTG value. Ensure the forward pass extracts the hidden state corresponding to the current observation and applies the adaLN modulation before the prediction MLP.
- **Design tradeoffs:**
  - **Input Sequence Length:** Reduced from 3k to 2k. This saves compute (quadratic speedup) but removes explicit "reward history" tokens.
  - **adaLN Complexity:** A single linear layer is used. The paper explicitly warns against adding non-linearity, as it destabilizes the conditioning effect.
- **Failure signatures:**
  - **Performance Collapse:** If implementing for discrete/stochastic tasks, ensure adaLN is applied correctly; failure to modulate the correct hidden state will result in random behavior.
  - **Overfitting:** If replacing the single-layer adaLN with a deeper MLP, performance degrades.
- **First 3 experiments:**
  1. **Blocked-DT vs. DDT:** Implement a baseline that masks non-final RTGs in standard DT to confirm that simple masking is insufficient compared to adaLN.
  2. **Attention Visualization:** Plot attention maps on a Markovian task (e.g., Hopper) to verify that DDT focuses on the diagonal while DT is scattered.
  3. **Ablation on adaLN Layers:** Validate the paper's claim that a zero-initialized single linear layer performs better than a 2-layer MLP for modulation.

## Open Questions the Paper Calls Out

- **Question:** Can more sophisticated condition modulation mechanisms, such as layer-wise adaLN, further enhance DDT's performance?
  - **Basis in paper:** [explicit] The authors list "Advanced Condition Fusion" as future work, specifically proposing to explore "DiT's sophisticated, layer-wise condition modulation paradigms."
  - **Why unresolved:** The current DDT implementation utilizes a simple adaLN mechanism with a single linear layer, leaving the potential benefits of deeper modulation architectures unexplored.
  - **What evidence would resolve it:** Empirical comparison of the current DDT against a variant implementing full layer-wise modulation on standard offline RL benchmarks.

- **Question:** Does the decoupling paradigm improve the efficiency and performance of other Decision Transformer variants?
  - **Basis in paper:** [explicit] The authors propose "Architectural Generalization" as a future direction, suggesting the application of the DDT paradigm to "other DT variants."
  - **Why unresolved:** The study primarily validates DDT on the base architecture; it is unknown if the removal of RTG sequences is compatible with or beneficial for variants that integrate complex Q-learning or online fine-tuning.
  - **What evidence would resolve it:** Integrating the decoupled RTG mechanism into variants like Q-Decision Transformer (QDT) or Online Decision Transformer (ODT) and measuring the resulting performance.

- **Question:** Does DDT degrade performance in environments where per-step rewards provide critical feature enrichment for state discrimination?
  - **Basis in paper:** [inferred] The paper notes that in non-standard POMDPs or cases with visually similar states, the full RTG sequence can provide gains by "enriching the feature representation."
  - **Why unresolved:** While DDT removes RTG redundancy for standard POMDPs, it may discard useful feature information in specific scenarios where immediate rewards help distinguish states that observations alone cannot.
  - **What evidence would resolve it:** Evaluating DDT on tasks specifically constructed to have high visual similarity between states but vastly different reward values.

## Limitations

- **Theoretical Scope Constraints:** The POMDP-based proof for RTG redundancy is sound only for standard POMDPs where rewards are not state-dependent, potentially breaking down in environments where reward magnitude encodes hidden state.
- **Architectural Claims Without Ablation:** The claim that attention reallocation drives performance gains is inferred from visualizations rather than controlled ablations, leaving the relative importance of sequence length reduction versus adaLN uncertain.
- **Hyperparameter Sensitivity:** The paper does not report sensitivity analyses for key hyperparameters like context length or learning rate, leaving optimal settings for DDT versus DT unexplored.

## Confidence

- **High Confidence:** DDT reduces computational cost by shortening input sequence length; adaLN outperforms simple RTG masking on all D4RL benchmarks.
- **Medium Confidence:** Only the most recent RTG is theoretically necessary for optimal action prediction; DDT achieves state-of-the-art performance compared to DT and variants.
- **Low Confidence:** Attention reallocation is the primary driver of DDT's performance gains; DDT will generalize to non-POMDP environments without modification.

## Next Checks

1. **POMDP Assumption Test:** Design a synthetic environment where reward magnitude is a function of the hidden state. Train DDT and standard DT to verify if DDT's performance collapses as predicted.

2. **Sequence Length Ablation:** Implement a variant of DDT that retains RTGs but uses a shorter context length (e.g., 10 tokens). Compare its performance to DDT to isolate the contribution of sequence shortening versus adaLN.

3. **Adaptive Layer Normalization Sensitivity:** Systematically vary the initialization strategy for adaLN (zero, Xavier, random) and test whether a small amount of non-linearity (e.g., ReLU after the linear layer) improves or degrades performance.