---
ver: rpa2
title: 'Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR'
arxiv_id: '2601.22595'
source_url: https://arxiv.org/abs/2601.22595
tags:
- uncertainty
- online
- training
- samples
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the high query cost of reinforcement learning
  with verifiable rewards (RLVR) for mathematical reasoning tasks by introducing active
  learning to RLVR. It identifies that classic active learning methods fail because
  they ignore the relationship between subjective uncertainty (model perplexity) and
  objective uncertainty (correctness).
---

# Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR

## Quick Facts
- arXiv ID: 2601.22595
- Source URL: https://arxiv.org/abs/2601.22595
- Reference count: 30
- Achieves full-dataset RLVR performance while using only 30% of data across multiple model sizes and datasets

## Executive Summary
This work addresses the high query cost of reinforcement learning with verifiable rewards (RLVR) for mathematical reasoning tasks by introducing active learning to RLVR. It identifies that classic active learning methods fail because they ignore the relationship between subjective uncertainty (model perplexity) and objective uncertainty (correctness). The authors propose an uncertainty consistency metric based on the Point-Biserial Correlation Coefficient (PBC) to measure alignment between these uncertainties. In offline settings, they select samples with lowest PBC; in online settings, they introduce an equivalent online metric derived from normalized advantages and subjective uncertainty. Experiments show their method achieves full-dataset performance while using only 30% of the data across multiple model sizes and datasets, significantly reducing RLVR training costs.

## Method Summary
The method introduces uncertainty consistency guided query selection for RLVR, using the Point-Biserial Correlation Coefficient (PBC) to measure alignment between subjective uncertainty (model perplexity) and objective uncertainty (correctness). Offline, samples are selected with lowest PBC; online, an equivalent metric using normalized advantages and subjective uncertainty enables real-time selection without expensive sampling. The approach is theoretically grounded with proofs showing the online metric is negatively correlated with offline PBC and maximizes uncertainty reduction. Implementation uses GRPO/DAPO with binary verifiable rewards, computing the selection metric per minibatch and training only on the top-scoring queries.

## Key Results
- Achieves full-dataset RLVR performance while using only 30% of training data
- Classic active learning methods (PPL, Entropy, K-center, AskLLM) perform similarly to random selection
- Online metric achieves R²=0.71-0.90 correlation with offline PBC across model/dataset combinations
- Maintains higher early-phase entropy compared to full-data training, indicating better exploration

## Why This Works (Mechanism)

### Mechanism 1: Uncertainty Consistency Selection
- Claim: Samples where subjective uncertainty aligns with objective uncertainty are more valuable for RLVR training than samples selected by subjective uncertainty alone.
- Mechanism: PBC measures alignment between perplexity and binary reward. Lower PBC indicates stronger consistency (high perplexity → likely incorrect; low perplexity → likely correct). Selecting low-PBC samples filters out "inconsistent" examples that destabilize training.
- Core assumption: The relationship between model confidence and actual correctness is predictive of sample value for gradient-based learning.

### Mechanism 2: Gradient Variance Reduction via Consistency Filtering
- Claim: Inconsistent samples produce larger gradient norms that destabilize training.
- Mechanism: Policy gradient methods push probabilities toward 0 or 1. Inconsistent samples have larger probability margins to traverse, generating larger gradient magnitudes and higher variance.
- Core assumption: Assumption 1 (Sample Gradient Orthogonality) and Assumption 2 (Bounded Gradient Norm) hold approximately in practice.

### Mechanism 3: Online Metric as Proxy for Offline PBC
- Claim: The online metric r_online_pb is negatively correlated with offline PBC, enabling real-time sample selection without expensive K-sample estimation.
- Mechanism: Theorem 1 proves Cov(r_pb, r_online_pb) < 0. The online metric leverages advantage estimates already computed in GRPO/DAPO, avoiding additional sampling overhead.
- Core assumption: The covariance relationship holds under the policy distribution shift during training; advantage estimates are sufficiently accurate.

## Foundational Learning

- **Concept: Point-Biserial Correlation Coefficient**
  - Why needed here: Core statistical measure for relating continuous (perplexity) and binary (correctness) variables; interpretation of negative/positive correlation.
  - Quick check question: If average perplexity for correct answers is 2.0 and for incorrect answers is 5.0, what sign would you expect for r_pb?

- **Concept: Group-Relative Advantage Estimation (GRPO-style)**
  - Why needed here: Understanding how normalized advantages are computed from verifiable rewards; these feed directly into the online metric.
  - Quick check question: In GRPO, if all K samples for a query are correct, what happens to the advantage values?

- **Concept: Policy Gradient Variance and Exploration-Exploitation**
  - Why needed here: Interpreting why gradient variance matters for training stability and why entropy maintenance affects sample efficiency.
  - Quick check question: Why might early entropy collapse limit reasoning gains even with more data?

## Architecture Onboarding

- **Component map:** Reference model π_ref -> Uncertainty estimator (PPL) -> PBC calculator (offline) or Online selector -> Policy model π_θ -> GRPO/DAPO loss
- **Critical path:** Sample minibatch → Generate G responses → Compute advantages → Compute per-response uncertainty → Calculate r_online_pb → Select top-p% queries → Compute loss only on selected queries
- **Design tradeoffs:**
  - γ parameter: Lower values (0.1-0.5) work better; higher γ overweights negative responses
  - Sampling ratio: 30% is optimal; 10% loses ~2-3% accuracy; 50% approaches full-data without added benefit
  - K for offline vs G for online: Offline needs K=64 for stable PBC; online uses G=8 (GRPO default) since metric relies on existing advantages
- **Failure signatures:**
  - Classic AL performs similarly to random → objective uncertainty is being ignored
  - Rapid entropy collapse with full-data training → limited exploration
  - Training on top-r_pb (inconsistent) samples performs worse than random → confirms inconsistency hypothesis
- **First 3 experiments:**
  1. Replicate pilot (Table 1): Train Qwen2.5-0.5B on MATH with 10% data using Random, PPL, Entropy, K-center. Verify classic AL fails.
  2. Gradient variance analysis (Figure 1a): Compare gradient norms for consistent vs inconsistent samples across 50 training steps. Quantify variance ratio.
  3. Online metric correlation (Figure 1b): For held-out queries, compute both r_pb (K=64) and r_online_pb. Plot scatter and compute R² to validate Theorem 1 proxy relationship.

## Open Questions the Paper Calls Out

- **Question:** Does uncertainty consistency-based query selection generalize to reasoning domains beyond mathematics, such as code generation or logical reasoning tasks?
  - Basis: Paper focuses on mathematical reasoning tasks and doesn't explore cross-task generalization.
  - Evidence needed: Experiments applying method to code tasks (HumanEval, MBPP) or logical reasoning benchmarks.

- **Question:** What determines the optimal sampling ratio, and how should it scale with dataset size, difficulty, and model capacity?
  - Basis: Empirical demonstration that 30% works well but no principled method for selecting this hyperparameter is provided.
  - Evidence needed: Systematic study correlating optimal sampling ratio with dataset properties and model characteristics.

- **Question:** Why does the method favor smaller γ values, and is there a theoretically grounded approach to setting this hyperparameter?
  - Basis: Empirical finding that lower γ values (0.1) work better but no explanation is provided.
  - Evidence needed: Analysis of how γ affects gradient signal composition during training across different settings.

## Limitations
- Offline PBC estimation requires 64 samples per query, creating significant computational overhead
- Theoretical assumptions (gradient orthogonality, bounded gradient norm) lack empirical validation
- γ hyperparameter choice is heuristic without theoretical grounding

## Confidence

- **High confidence:** Empirical demonstration that consistency-based selection outperforms classic active learning methods across multiple datasets and model sizes
- **Medium confidence:** Theoretical connection between online and offline metrics (Theorem 1) - practical correlation depends on stable advantage estimates
- **Medium confidence:** Gradient variance mechanism - supported by variance measurements but not by direct analysis of how inconsistent gradients affect convergence

## Next Checks
1. **Scale validation:** Test the method on larger datasets (e.g., GSM8K+ or additional mathematical domains) to assess whether the 30% efficiency gain holds at scale
2. **Dynamic γ analysis:** Systematically vary γ during training to identify optimal scheduling strategies rather than fixed values
3. **Distribution shift robustness:** Evaluate performance when training data distribution differs significantly from test distribution, as PBC estimation may be less reliable under domain shift