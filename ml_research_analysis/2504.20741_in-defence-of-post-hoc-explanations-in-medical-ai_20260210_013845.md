---
ver: rpa2
title: In defence of post-hoc explanations in medical AI
arxiv_id: '2504.20741'
source_url: https://arxiv.org/abs/2504.20741
tags:
- explanations
- post-hoc
- black
- systems
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# In defence of post-hoc explanations in medical AI

## Quick Facts
- arXiv ID: 2504.20741
- Source URL: https://arxiv.org/abs/2504.20741
- Reference count: 18
- Primary result: Theoretical defence of post-hoc explanations in medical AI

## Executive Summary
This paper presents a theoretical defence of post-hoc explanations in medical AI systems, arguing they are valuable even when they don't perfectly capture model internals. The authors make a meta-level argument about interpretability methodology rather than presenting empirical evidence, suggesting that post-hoc explanations can provide useful insights for clinical decision-making despite their inherent limitations.

## Method Summary
The paper employs philosophical and logical argumentation to defend post-hoc explanations in medical AI. Rather than conducting empirical studies, the authors build their case through theoretical reasoning about the nature of explanations, the practical constraints of medical AI deployment, and the trade-offs between interpretability and performance. They examine scenarios where post-hoc explanations might be preferable to intrinsically interpretable models.

## Key Results
- Post-hoc explanations can be valuable even when they are approximate representations of model behavior
- The paper argues for a pragmatic approach to interpretability in medical AI rather than demanding perfect fidelity
- Post-hoc methods may be preferable when intrinsic interpretability would significantly compromise model performance

## Why This Works (Mechanism)
The paper's argument works by reframing the interpretability debate around practical utility rather than theoretical perfection. It suggests that medical practitioners can benefit from post-hoc explanations as supplementary information for clinical decision-making, even if these explanations don't capture every aspect of model internals. The mechanism relies on the idea that approximate explanations are better than no explanations at all in high-stakes medical contexts.

## Foundational Learning
1. Post-hoc vs Intrinsic Interpretability
   - Why needed: Understanding the fundamental trade-offs between different interpretability approaches
   - Quick check: Can identify scenarios where each approach would be preferable

2. Approximation in Explanations
   - Why needed: Recognizing that all explanations are inherently approximate
   - Quick check: Can explain why perfect fidelity is neither achievable nor necessary

3. Medical AI Decision-Making Context
   - Why needed: Understanding the specific requirements and constraints of medical AI deployment
   - Quick check: Can identify key stakeholders and their interpretability needs

## Architecture Onboarding
Component map: Post-hoc explanation method -> Clinical decision support -> Patient outcome
Critical path: Model prediction → Explanation generation → Clinical interpretation → Action
Design tradeoffs: Accuracy vs interpretability, computational cost vs explanation quality
Failure signatures: Over-reliance on explanations, misinterpretation of approximate explanations
First experiments:
1. Compare diagnostic accuracy with and without post-hoc explanations
2. Test explanation comprehension across different medical specialties
3. Evaluate trust calibration in medical professionals using post-hoc explanations

## Open Questions the Paper Calls Out
The paper itself doesn't explicitly call out open questions, as it focuses on theoretical argumentation rather than empirical investigation. However, the paper's approach raises implicit questions about the practical implementation and validation of post-hoc explanations in clinical settings.

## Limitations
- Lacks empirical evidence supporting theoretical claims
- Doesn't adequately address acceptable approximation levels in high-stakes medical decisions
- Limited discussion of alternative interpretability approaches and their relative merits

## Confidence
- Logical structure of arguments: High
- Practical applicability of claims: Low
- Safety implications for medical AI: Medium

## Next Checks
1. Conduct a clinical study comparing diagnostic accuracy and trust levels between medical AI systems using post-hoc explanations versus intrinsically interpretable models
2. Develop a framework for quantifying acceptable approximation levels in post-hoc explanations for different medical specialties and risk levels
3. Create benchmark datasets with ground truth explanations to empirically evaluate the fidelity of various post-hoc explanation methods in medical contexts