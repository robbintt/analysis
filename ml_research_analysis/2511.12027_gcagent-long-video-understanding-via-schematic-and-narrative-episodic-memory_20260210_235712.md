---
ver: rpa2
title: 'GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory'
arxiv_id: '2511.12027'
source_url: https://arxiv.org/abs/2511.12027
tags:
- memory
- video
- episodic
- understanding
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "GCAgent tackles long-video understanding by building a global\
  \ episodic memory of schematic and narrative structures before queries arrive, then\
  \ retrieving query-relevant segments at runtime. Its two-agent framework\u2014a\
  \ Memory Manager that segments transcripts into events and infers causal-temporal\
  \ relationships, and a Reasoning Agent that conditions inference on both the episodic\
  \ memory and local evidence\u2014enables structured, cognitively-inspired comprehension."
---

# GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory

## Quick Facts
- arXiv ID: 2511.12027
- Source URL: https://arxiv.org/abs/2511.12027
- Authors: Jeong Hun Yeo; Sangyun Chung; Sungjune Park; Dae Hoe Kim; Jinyoung Moon; Yong Man Ro
- Reference count: 40
- Primary result: State-of-the-art 71.9% accuracy on Video-MME among 7B-scale models, with 23.5% improvement on long videos

## Executive Summary
GCAgent addresses long-video understanding by constructing episodic memory of schematic and narrative structures before queries arrive, then retrieving query-relevant segments at runtime. The two-agent framework uses a Memory Manager to segment transcripts into events with causal-temporal relationships, and a Reasoning Agent that conditions inference on both episodic memory and local evidence. This cognitively-inspired approach achieves state-of-the-art performance on Video-MME and LongVideoBench benchmarks while reducing token requirements by up to 78.9% for long videos.

## Method Summary
GCAgent employs a two-agent inference framework that builds global episodic memory from video transcripts before queries arrive, then retrieves query-relevant segments at runtime. The Memory Manager segments transcripts into event-level units, abstracts each into schematic structures (roles, situations), and infers narrative structures (causal-temporal relationships) to form episodic memory. At query time, the perception module locates relevant transcript spans and maps them to corresponding video segments. The Reasoning Agent jointly reasons over episodic memory and retrieved multimodal evidence to generate answers. The system operates without training, using GPT-5.1 Mini for memory construction and Qwen2.5-VL-7B for multimodal reasoning.

## Key Results
- 71.9% average accuracy on Video-MME, achieving state-of-the-art among 7B-scale models
- 23.5% accuracy improvement on the Long split compared to existing methods
- 78.9% token reduction (9,717 → 2,046 tokens) for 30-60 minute videos
- 67.0% accuracy with query-relevant transcript + memory vs 65.3% with transcript alone

## Why This Works (Mechanism)

### Mechanism 1: Pre-Query Episodic Memory Construction
Building structured global context before queries arrive reduces long-term dependency failures. The Memory Manager segments transcripts into event-level units via boundary detection, then abstracts each into schematic structures (roles, situations) and infers causal-temporal relationships across events to form narrative structures. This yields episodic memory that compresses 9,717 tokens → 2,046 tokens (78.9% reduction for 30-60 min videos).

### Mechanism 2: Query-Conditioned Dual Retrieval (Perception)
Retrieving both transcript spans and their corresponding video segments reduces redundant information interference. Given query Q, options O, transcripts T, and memory M, the perception module locates relevant transcript spans, extracts time boundaries, and maps to video segments: V*, T* = A_perc(Q, O, T, M).

### Mechanism 3: Global-Local Context Synergy (Action)
Combining episodic memory (global context) with query-relevant local evidence yields stronger reasoning than either alone. Reasoning agent conditions inference on M (global narrative context) while grounding answers in V* and T* (local multimodal evidence). Ablation shows: QR transcript + memory (67.0%) > QR transcript alone (65.3%); adding memory + narrative structure to vision+text yields 73.4%.

## Foundational Learning

- **Episodic Memory (Cognitive Psychology)**: The paper grounds its design in cognitive theories of schematic/narrative structures (Bartlett, Zacks et al.). Understanding this helps distinguish the approach from simple summarization.
  - Quick check question: Can you explain the difference between schematic structure (event templates, roles) and narrative structure (causal-temporal sequences)?

- **Retrieval-Augmented Generation (RAG) for Video**: GCAgent extends RAG principles from text to video, with pre-construction and multi-modal retrieval.
  - Quick check question: How does pre-query memory construction differ from on-demand retrieval in standard RAG?

- **Token Budget Constraints in MLLMs**: The 32-frame limit for Qwen2.5-VL-7B and the 78.9% token reduction motivate the entire memory design.
  - Quick check question: Why do speech transcripts offer better compression potential than visual features for long videos?

## Architecture Onboarding

- **Component map**: VAD/ASR pipeline → Memory Manager (GPT-5.1 Mini) → episodic memory → Perception module → Reasoning Agent (Qwen2.5-VL-7B) → Answer + Visual explanation → Memory Manager update

- **Critical path**: 
  1. Pre-query: VAD/ASR → transcripts → Memory Manager → episodic memory (schematic + narrative)
  2. At query: Memory Manager retrieves T* → extracts V* → passes to Reasoning Agent
  3. Reasoning Agent receives (Q, O, V*, T*, M) → generates answer + visual explanation
  4. Reflection: Memory Manager updates M with visual explanation

- **Design tradeoffs**:
  - Transcript-first vs. visual-first: Transcripts more token-efficient but unavailable in silent videos; fallback to captions costs 9.7% accuracy
  - Full transcript + memory vs. QR transcript + memory: Redundancy hurts; avoid providing both full transcript and memory simultaneously
  - Narrative structure overhead: Adds +2.4 points (71.0% → 73.4%) but requires LLM-based role/causal inference

- **Failure signatures**:
  - Multilingual degradation: Memory construction over-translates/summarizes contrastive statements into neutral lists
  - Short video overhead: Token count increases (268 → 593) for 0–2 min videos; memory adds cost without compression benefit
  - Redundancy interference: Performance drops when full transcript and memory are both provided (65.0% → 64.7%)

- **First 3 experiments**:
  1. **Ablation by modality**: Test vision-only (uniform vs. QR segments), text-only (full vs. QR transcript ± memory), vision+text combinations. Validate Table II on your target video domain.
  2. **Memory construction path comparison**: Audio transcripts vs. visual captions on videos with/without speech. Replicate Table IV to understand fallback behavior.
  3. **Category-wise analysis**: Evaluate on Knowledge, Sports, Multilingual subsets to identify where memory helps vs. harms. Check for translation-induced distortions in non-English content.

## Open Questions the Paper Calls Out
None

## Limitations
- Multilingual content degrades performance due to translation-induced distortions that flatten contrastive nuances
- Short videos (<2 minutes) see token count increase rather than reduction, making memory construction counterproductive
- Reliance on speech transcripts limits applicability to silent videos, requiring visual caption fallback that underperforms by 9.7%

## Confidence

**High Confidence**:
- Token efficiency improvements (78.9% reduction measured on 30-60 minute videos)
- Query-relevant transcript + memory combination outperforms transcript alone (67.0% vs 65.3%)
- Visual caption fallback underperforms audio transcript path (63.7% vs 73.4%)

**Medium Confidence**:
- State-of-the-art claims among 7B-scale models (comparisons made but architecture details of baselines not fully specified)
- Narrative structure adds value (+2.4 points from schematic-only) but requires LLM-based role/causal inference that may not generalize
- Memory + narrative structure yields 73.4% vs vision+text 71.0%, but synergy effect not isolated from architectural differences

**Low Confidence**:
- Pre-query memory construction fundamentally resolves long-term dependency problem (no comparative study with on-demand retrieval)
- Schematic/narrative decomposition is optimal (no comparison with alternative memory structures)
- Cross-modal redundancy avoidance (claims made but no systematic study of redundancy effects)

## Next Checks

1. **Memory Construction Quality Isolation**: Run ablation studies comparing (a) human-annotated schematic/narrative structures vs GPT-5.1 Mini-generated ones, (b) schematic-only vs narrative+schematic memory, and (c) memory with vs without causal link inference. This isolates whether memory construction quality or structure type drives performance.

2. **Temporal Mapping Accuracy Verification**: Implement and test the subtitle index-to-video frame boundary mapping procedure. Measure accuracy of retrieved video segments against ground truth temporal annotations on a subset of videos, particularly focusing on boundary errors that could cause information loss.

3. **Multilingual Memory Robustness**: Systematically test memory construction on non-English transcripts by (a) comparing summaries of contrastive vs neutral statements, (b) measuring semantic drift between source and summarized content, and (c) evaluating whether memory quality correlates with translation quality metrics. This validates whether the degradation is inherent to the approach or addressable through better summarization strategies.