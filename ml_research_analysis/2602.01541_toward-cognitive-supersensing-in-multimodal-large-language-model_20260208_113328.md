---
ver: rpa2
title: Toward Cognitive Supersensing in Multimodal Large Language Model
arxiv_id: '2602.01541'
source_url: https://arxiv.org/abs/2602.01541
tags:
- visual
- reasoning
- images
- image
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the gap between multimodal large language models'
  perceptual capabilities and their cognitive reasoning abilities, particularly when
  visual details are abstract and require visual memory. The core method introduces
  Cognitive Supersensing, a training paradigm that equips MLLMs with human-like visual
  imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head.
---

# Toward Cognitive Supersensing in Multimodal Large Language Model

## Quick Facts
- arXiv ID: 2602.01541
- Source URL: https://arxiv.org/abs/2602.01541
- Reference count: 22
- Primary result: CogSense-8B achieves 73.8% accuracy on CogSense-Bench, surpassing GPT-5.2 by +33.5%

## Executive Summary
This paper addresses the gap between multimodal large language models' perceptual capabilities and their cognitive reasoning abilities, particularly when visual details are abstract and require visual memory. The core method introduces Cognitive Supersensing, a training paradigm that equips MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head. This head jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, forming vision-based internal reasoning chains. A reinforcement learning stage optimizes text reasoning paths based on this grounded visual latent. The primary result shows that MLLMs trained with Cognitive Supersensing, specifically CogSense-8B, significantly outperform state-of-the-art baselines on the CogSense-Bench with an overall accuracy of 73.8%, surpassing GPT-5.2 by +33.5. The model also exhibits strong generalization on out-of-domain mathematics and science visual question answering benchmarks.

## Method Summary
Cognitive Supersensing introduces a three-stage training pipeline for MLLMs to enhance visual cognitive reasoning. First, a teacher MLLM generates reasoning chains for a curated dataset spanning five cognitive dimensions (fluid intelligence, crystallized intelligence, visuospatial cognition, mental simulation, visual routines). Second, the target model is fine-tuned with a joint loss combining text cross-entropy and an MSE loss from a Latent Visual Imagery Prediction (LVIP) head that predicts visual latent embeddings of the answer. Third, reinforcement learning with a GFlowNet-based approach optimizes reasoning trajectory sampling using a reward combining answer correctness and visual latent similarity. The LVIP head is a two-layer MLP that processes average-pooled hidden states of option image tokens to predict answer-oriented visual latent states.

## Key Results
- CogSense-8B achieves 73.8% overall accuracy on CogSense-Bench, outperforming GPT-5.2 by +33.5% and Qwen3-VL-8B by +11.5%.
- The model shows strong generalization on out-of-domain benchmarks: EMMA Chemistry (+18.1%) and EMMA Mathematics (+6.2%).
- Ablation studies confirm the effectiveness of the LVIP head (+6.5% vs. SFT without LVIP) and the RL stage (+3.0% gain over SFT with LVIP alone).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual cognition requires maintaining and transforming internal visual states, not just processing static visual inputs.
- Mechanism: The Cognitive Supersensing framework introduces a Latent Visual Imagery Prediction (LVIP) head that explicitly learns to predict sequences of visual cognitive latent embeddings. These embeddings act as a "mind's eye" or visuospatial sketchpad, allowing the model to simulate intermediate visual reasoning steps in a continuous latent space rather than forcing all reasoning into discrete text tokens. This better preserves geometric relationships and spatial structures during multi-step visual reasoning.
- Core assumption: The paper assumes that human-like visual imagery (mental manipulation of visual representations) can be approximated by learning latent embeddings that are predictive of final answer visual features.
- Evidence anchors:
  - [abstract] "...and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery."
  - [Page 2] "...expressing these intermediate states solely in linear text can introduce representational bottlenecks, where spatial relations are compressed into discrete tokens..."
  - [corpus] "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs" (arXiv:2510.24514) supports the mechanism of using visual scratchpads for complex planning and imagination tasks.
- Break condition: If the LVIP head is removed or the latent embeddings are forced to be purely textual rather than visual-anchored, performance on CogSense-Bench should degrade significantly, particularly on visuospatial cognition and mental simulation tasks.

### Mechanism 2
- Claim: Aligning latent visual reasoning chains with answer-oriented visual supervision grounds the reasoning process.
- Mechanism: During supervised fine-tuning, the LVIP head is trained with an MSE loss to predict the visual encoder embedding of the ground-truth answer image. This creates an explicit alignment signal between the internal reasoning trajectory and the visual properties of the correct solution, forcing the model's latent reasoning states to be "answer-oriented."
- Core assumption: The paper assumes that the visual encoder embedding of the correct answer contains sufficient signal to guide the formation of useful intermediate latent reasoning states.
- Evidence anchors:
  - [abstract] "...integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer..."
  - [Page 5] "The supervision target for LVIP is the embedding of Vy extracted by the (frozen) visual encoder... We optimize LVIP with an MSE loss between ˆhy and hy..."
  - [corpus] "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding" (arXiv:2503.12797) aligns with the theme of integrating reasoning into visual perception.
- Break condition: If the visual supervision signal (MSE loss to answer image embedding) is removed, the model may still generate reasoning chains, but they would lack grounding in the visual modality, potentially leading to less effective reasoning.

### Mechanism 3
- Claim: Reinforcement learning over latent reasoning trajectories, guided by a latent-visual reward, refines and diversifies reasoning paths.
- Mechanism: After the SFT stage, a reinforcement learning stage (using a GFlowNet-based approach) trains the model to sample rationale trajectories in proportion to a reward score. This reward is a combination of (1) answer evidence from a frozen scorer and (2) an LVIP-based representation grounding reward. This encourages the model to explore diverse, coherent reasoning paths that are both textually fluent and visually grounded.
- Core assumption: The paper assumes that a reward combining discrete answer correctness and latent visual similarity can effectively shape a policy for high-quality reasoning generation.
- Evidence anchors:
  - [abstract] "We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent."
  - [Page 6] "we define the trajectory score as a weighted combination of answer evidence and LVIP-based representation grounding: R(Z;X, y) = α R ans(Z;X, y) + γ R lvip(Z;X, y)."
  - [corpus] Corpus evidence for this specific GFlowNet+LVIP reward mechanism is weak or missing in the provided neighbors.
- Break condition: If the reinforcement learning stage is omitted (as in the "SFT w/ LVIP" ablation), performance is still strong (68.0% avg) but does not reach the peak (73.8%) of the full model. If a standard RL method like GRPO is used instead of the proposed GFlowNet-based method, the gains are smaller (Tab. 3).

## Foundational Learning

- **Latent Visual Imagery**: The concept of representing visual information in a compressed, continuous vector space that can be manipulated internally by a model.
  - Why needed here: This is the core representation the LVIP head operates in. Understanding that these are not pixels or text, but continuous vectors learned to be predictive of visual features, is essential.
  - Quick check question: What is the target supervision signal for the LVIP head's prediction during training?

- **Generative Flow Networks (GFlowNets)**: A framework for training a probabilistic policy to sample compositional objects (like reasoning chains) in proportion to a reward function.
  - Why needed here: This is the underlying RL technique used in Stage III to optimize the sampling of reasoning trajectories.
  - Quick check question: What is the target posterior distribution p*(Z|X, y) that the policy qθ(Z|X) is trained to approximate?

- **Cognitive Dimensions**: The five categories of visual cognition (Fluid Intelligence, Crystallized Intelligence, Visuospatial Cognition, Mental Simulation, Visual Routines) that define the benchmark.
  - Why needed here: These dimensions define the problem space and explain why a one-size-fits-all text-only CoT approach is insufficient. Each requires different types of internal manipulation.
  - Quick check question: Which cognitive dimension is most likely to require simulating future states or transformations?

## Architecture Onboarding

- **Component map**:
    - **Input**: Multimodal input (V, Q) = (Image(s), Text Prompt).
    - **Visual Encoder**: Enc_vis(V) -> V_V (visual features).
    - **Projection Layer**: P(V_V) -> h_V (visual tokens in LLM space).
    - **LLM Backbone**: Enc_txt([h_V, h_Q]) -> Hidden states.
    - **Text Decoder**: Generates reasoning (Z) and answer (y) autoregressively.
    - **LVIP Head (g_ψ)**: An auxiliary 2-layer MLP. Takes backbone hidden states for option image tokens (h_opt), averages them, and predicts ȳ_h (predicted latent imagery).
    - **Supervision**: For SFT, the target for LVIP is h_y = Enc_vis(V_y) (embedding of ground truth answer image). The text loss is standard cross-entropy.
    - **RL Components**:
        - **Frozen Scorer (q_θ0)**: A copy of the SFT model used to compute R_ans.
        - **Reward Function**: R = α * R_ans + γ * R_lvip, where R_lvip is based on MSE between the LVIP prediction and h_y.
        - **Policy (q_θ)**: Trained via GFlowNet SubTB loss to sample high-reward rationales.

- **Critical path**:
    1.  **Stage I**: Reasoning chain generation using a teacher MLLM to create a supervised dataset D_chain.
    2.  **Stage II (SFT)**: Train the model with a joint loss: (Text Cross-Entropy) + β * (LVIP MSE). This aligns the model's internal states with visual imagery of the answer.
    3.  **Stage III (RL)**: Use the frozen model as a scorer and the frozen LVIP head to compute a combined reward. Train the policy (the LLM backbone) with a GFlowNet objective to sample diverse, high-reward reasoning trajectories.
    4.  **Inference**: Sample N rationales, compute a score S_i using the frozen scorer, and output the answer from the highest-scoring rationale.

- **Design tradeoffs**:
    - **Complexity vs. Performance**: The 3-stage pipeline with a custom auxiliary head and GFlowNet-based RL is more complex than standard SFT. This paper argues the complexity yields significant gains on a specific class of "cognitive" visual reasoning tasks.
    - **Generalization vs. Specialization**: The model shows strong OOD gains on science/math VQA (Tab. 4) but a slight dip on some general VLM benchmarks (Tab. 2). This is a classic trade-off: specialization can sometimes hurt slightly on broader perception tasks.

- **Failure signatures**:
    - **Ablation Check**: If the LVIP head is removed, expect performance to drop from 73.8% (full model) to ~62.3% (SFT w/o LVIP), per Table 3.
    - **RL Failure**: If the GFlowNet RL stage is replaced with a standard RL like GRPO, performance gains are present (+2.8) but smaller than the proposed method (+3.0). This indicates the specific RL design matters.
    - **Text-Only Failure**: On tasks requiring mental simulation or visuospatial reasoning, pure text-based CoT should struggle (as motivated by the paper).

- **First 3 experiments**:
    1.  **Baseline Reproduction (SFT only)**: Re-train the model using only Stage II (SFT) and no LVIP head. Compare performance on CogSense-Bench against the reported 62.3% baseline. This validates the core training pipeline.
    2.  **LVIP Ablation**: Re-train the model with Stage II (SFT) + LVIP head but *without* the RL stage. Compare against the reported 68.0% baseline. This validates the LVIP mechanism's contribution.
    3.  **Qualitative Analysis of Latent States**: For a set of test examples, extract the predicted latent imagery ȳ_h from the LVIP head and the ground truth visual embedding h_y. Compute their cosine similarity. Correlate this similarity with answer correctness to see if "better" visual imagery prediction tracks with better reasoning.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the latent visual imagery embeddings learned by the LVIP head be explicitly visualized or decoded to reveal human-interpretable visual reasoning steps?
- **Open Question 2**: Does training on the CogSense-Dataset induce a trade-off that degrades performance on general vision-language tasks requiring different reasoning styles?
- **Open Question 3**: Is the auxiliary LVIP head necessary for models with significantly larger parameter counts (e.g., >70B), or do large-scale models develop internal visual imagery capabilities spontaneously?

## Limitations
- **Complexity and Computational Cost**: The three-stage pipeline with an auxiliary LVIP head and GFlowNet-based RL is more complex and computationally expensive than standard MLLM training approaches.
- **Potential for Negative Transfer**: The model shows a slight performance drop on some general VLM benchmarks (MMMU, Seed-Bench, LLaVA-Bench), suggesting potential trade-offs between cognitive specialization and broad perceptual capability.
- **Limited Model Scale**: The experiments are limited to an 8B parameter model (Qwen3-VL-8B); the utility of the proposed architecture for frontier-scale models is not tested.

## Confidence
- **High Confidence**: The core claim that MLLMs struggle with abstract visual reasoning requiring mental imagery manipulation is well-supported by literature and the paper's analysis of current limitations.
- **Medium Confidence**: The CogSense-Bench evaluation shows significant performance improvements (73.8% avg accuracy), but the benchmark's construction and the specific contributions of each training stage (SFT vs. RL) could benefit from additional ablation studies.
- **Low Confidence**: Claims about the specific effectiveness of the GFlowNet-based RL approach versus alternative methods are less substantiated, as the corpus provides limited evidence for this specific technique.

## Next Checks
1. **Ablation on Reward Components**: Run experiments isolating R_ans vs. R_lvip to determine which component drives the RL gains. The current combined reward makes it difficult to attribute performance improvements.
2. **Latent Space Analysis**: For test examples, visualize or analyze the distribution of predicted latents ȳ_h from LVIP against h_y. Check if visually similar reasoning paths (low MSE) correlate with correct answers across different cognitive dimensions.
3. **Cross-Domain Generalization**: Test the trained model on other abstract reasoning benchmarks not used in training (e.g., traditional Raven's Progressive Matrices or Bongard problems) to verify that the cognitive reasoning capabilities generalize beyond the specific CogSense dataset.