---
ver: rpa2
title: 'KBE-DME: Dynamic Multimodal Evaluation via Knowledge Enhanced Benchmark Evolution'
arxiv_id: '2510.21182'
source_url: https://arxiv.org/abs/2510.21182
tags:
- triplets
- question
- knowledge
- answer
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces KBE-DME, a dynamic multimodal evaluation
  framework that transforms static VQA benchmarks into evolving test sets to address
  data contamination and saturation issues. By representing VQA problems as graphs
  with multimodal knowledge triplets, KBE-DME generates difficulty-controllable questions
  through two strategies: re-selecting key triplets from existing knowledge or exploring
  external knowledge.'
---

# KBE-DME: Dynamic Multimodal Evaluation via Knowledge Enhanced Benchmark Evolution

## Quick Facts
- arXiv ID: 2510.21182
- Source URL: https://arxiv.org/abs/2510.21182
- Authors: Junzhe Zhang; Huixuan Zhang; Xiaojian Wan
- Reference count: 29
- Introduces KBE-DME framework transforming static VQA benchmarks into evolving test sets to address data contamination and saturation issues

## Executive Summary
KBE-DME presents a dynamic multimodal evaluation framework that addresses critical limitations in current VQA benchmarking approaches, specifically data contamination and saturation problems that arise as models encounter training data repeatedly. The framework transforms static VQA benchmarks into evolving test sets by leveraging knowledge graph triplets and two question generation strategies: re-selecting key triplets from existing knowledge or exploring external knowledge sources. This approach enables difficulty-controllable question generation that can adapt to advancing model capabilities, providing a more reliable and dynamic evaluation methodology for multimodal language models.

The framework's core innovation lies in its ability to systematically increase evaluation difficulty through knowledge graph expansion hops, creating progressively more challenging questions while maintaining semantic coherence. By testing five prominent MLLMs including GPT-4o, Gemini-2.5-pro, Claude, LLaVA-OV, and Qwen-2.5-VL on OK-VQA and A-OKVQA datasets, KBE-DME demonstrates consistent performance degradation as difficulty increases from 1-3 expansion hops, validating its effectiveness as an adaptive evaluation tool that evolves alongside model capabilities.

## Method Summary
KBE-DME operates by first representing VQA problems as graph structures composed of multimodal knowledge triplets, where nodes represent entities, attributes, or relations and edges encode semantic connections between them. The framework employs two distinct strategies for question generation: the first involves re-selecting key triplets from existing knowledge bases to create questions of varying difficulty levels, while the second explores external knowledge sources to expand the question space beyond original benchmark constraints. Difficulty control is achieved through expansion hops, where each hop represents an additional layer of knowledge graph traversal, systematically increasing the complexity and reasoning requirements of generated questions. This dual-strategy approach allows KBE-DME to generate questions that are both semantically rich and appropriately challenging for current model capabilities.

## Key Results
- Performance of five MLLMs consistently decreases as evaluation difficulty increases from 1-3 expansion hops
- GPT-4o, Gemini-2.5-pro, Claude, LLaVA-OV, and Qwen-2.5-VL all show measurable degradation on OK-VQA and A-OKVQA datasets
- KBE-DME successfully transforms static VQA benchmarks into dynamic, evolving test sets
- The framework demonstrates effectiveness in addressing data contamination and saturation issues in multimodal evaluation

## Why This Works (Mechanism)
The framework's effectiveness stems from its graph-based representation of VQA problems, which enables systematic exploration and expansion of knowledge spaces. By representing questions as multimodal knowledge triplets, KBE-DME can precisely control difficulty through knowledge graph traversal depth, where each expansion hop introduces additional semantic complexity and reasoning requirements. The dual strategy approach allows for both conservative question generation through re-selection of existing knowledge and exploratory question generation through external knowledge incorporation, providing flexibility in difficulty calibration. This graph-based methodology inherently captures the multimodal nature of VQA tasks while enabling precise control over the semantic complexity of generated questions.

## Foundational Learning

**Knowledge Graph Triplets** - Representation of VQA problems as structured knowledge with entities, attributes, and relations; needed for systematic question generation and difficulty control; quick check: verify triplet coverage and quality of underlying knowledge sources.

**Expansion Hops** - Iterative traversal of knowledge graphs where each hop increases semantic complexity; needed for difficulty calibration and progressive challenge creation; quick check: validate correlation between hop count and actual reasoning complexity.

**Multimodal Semantic Coherence** - Ensuring generated questions maintain semantic integrity across visual and textual modalities; needed for valid evaluation of MLLM capabilities; quick check: assess question-answer consistency across different difficulty levels.

**Dynamic Benchmark Evolution** - Transformation of static evaluation sets into adaptive, evolving test suites; needed to address data contamination and saturation issues; quick check: measure benchmark staleness reduction over time.

## Architecture Onboarding

Component Map: Knowledge Graph -> Question Generator (Re-selection Strategy) -> Difficulty Controller -> Evaluation Suite; Knowledge Graph -> Question Generator (Exploration Strategy) -> Difficulty Controller -> Evaluation Suite

Critical Path: Knowledge Graph Construction → Question Generation Strategy Selection → Difficulty Control via Expansion Hops → Question-Answer Pair Generation → Evaluation Suite Assembly

Design Tradeoffs: Re-selection strategy offers semantic coherence but limited novelty vs. exploration strategy provides fresh questions but may sacrifice consistency; knowledge graph quality vs. coverage breadth; difficulty granularity vs. computational overhead.

Failure Signatures: Poor knowledge graph quality leads to semantically incoherent questions; over-expansion causes question complexity to exceed meaningful evaluation thresholds; strategy imbalance results in uneven difficulty distribution.

First Experiments:
1. Validate triplet quality and coverage across OK-VQA and A-OKVQA domains
2. Test single-hop vs. multi-hop difficulty progression with one MLLM
3. Compare re-selection and exploration strategy outputs for semantic coherence

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on knowledge graph quality and coverage introduces potential biases from underlying knowledge sources
- Evaluation primarily focused on OK-VQA and A-OKVQA datasets, limiting generalizability to other VQA benchmarks
- Assumption that expansion hops consistently correlate with increased difficulty may not hold across all knowledge domains
- Computational overhead and practical implementation challenges for real-world deployment remain unclear

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| KBE-DME effectively transforms static VQA benchmarks into dynamic test sets | High |
| Performance degradation correlates with expansion hop difficulty | Medium |
| Dual strategy approach provides balanced question generation | Medium |
| Framework generalizes beyond tested datasets and models | Low |

## Next Checks
1. Conduct cross-dataset validation testing KBE-DME's effectiveness on non-VQA multimodal benchmarks to assess generalizability beyond the OK-VQA and A-OKVQA domains.
2. Perform ablation studies comparing KBE-DME's difficulty progression against human-annotated difficulty ratings to validate whether expansion hops truly correspond to increased reasoning complexity.
3. Test the framework's performance with emerging MLLMs and multimodal architectures released after the paper's publication to evaluate temporal robustness and adaptation to advancing model capabilities.