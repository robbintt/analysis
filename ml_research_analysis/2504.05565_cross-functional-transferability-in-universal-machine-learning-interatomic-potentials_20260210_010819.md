---
ver: rpa2
title: Cross-functional transferability in universal machine learning interatomic
  potentials
arxiv_id: '2504.05565'
source_url: https://arxiv.org/abs/2504.05565
tags:
- energy
- materials
- learning
- energies
- atomref
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of transferring universal machine
  learning interatomic potentials (uMLIPs) from lower-fidelity (GGA/GGA+U) to higher-fidelity
  (r2SCAN) density functional theory datasets. The main issue is the significant energy
  scale shifts and poor correlations between functionals, which hinder effective transfer
  learning.
---

# Cross-functional transferability in universal machine learning interatomic potentials

## Quick Facts
- arXiv ID: 2504.05565
- Source URL: https://arxiv.org/abs/2504.05565
- Reference count: 0
- This work demonstrates that proper energy referencing through atomic reference energy refitting is critical for successful transfer learning between DFT functionals, improving energy MAE from 27 to 17 meV/atom when transferring from GGA/GGA+U to r2SCAN.

## Executive Summary
This study addresses the challenge of transferring universal machine learning interatomic potentials (uMLIPs) across different density functional theory (DFT) functionals, specifically from lower-fidelity GGA/GGA+U to higher-fidelity r2SCAN calculations. The authors identify that significant energy scale shifts between functionals hinder effective transfer learning, with poor correlations between energy predictions. Through analysis of the MP-r2SCAN dataset containing 238,247 structures, they demonstrate that proper energy referencing through atomic reference energy refitting is essential for successful transfer, reducing energy MAE from 27 to 17 meV/atom. The study also reveals that transfer learning remains data-efficient even with sub-million structure datasets, achieving performance equivalent to having 10-fold more high-fidelity training data.

## Method Summary
The authors analyze the MP-r2SCAN dataset of 238,247 structures to investigate transfer learning between DFT functionals. They implement a transfer learning approach that includes energy referencing through refitting atomic reference energies, which is critical for handling the energy scale shifts between GGA/GGA+U and r2SCAN functionals. The methodology involves comparing transfer learning performance against training from scratch, measuring both energy and force accuracy improvements. Scaling law analysis is performed to evaluate data efficiency across different dataset sizes, examining how transfer learning performance changes as more high-fidelity data becomes available.

## Key Results
- Without energy referencing, transfer learning yields similar or worse performance compared to training from scratch
- With proper energy referencing, energy MAE improves from 27 to 17 meV/atom and force MAE from 45 to 38 meV/Ã…
- Transfer learning provides performance gains equivalent to 10-fold more high-fidelity data, with advantages diminishing at larger dataset sizes (~500,000 structures)

## Why This Works (Mechanism)
The effectiveness of transfer learning between DFT functionals depends critically on addressing the energy scale shifts that occur between different exchange-correlation functionals. When atomic structures are calculated using different functionals, the absolute energy values can differ significantly due to variations in how each functional treats electron exchange and correlation. This creates a systematic offset that must be corrected through energy referencing. By refitting atomic reference energies, the model can align the energy scales between source (GGA/GGA+U) and target (r2SCAN) functionals, enabling the learned atomic interactions to transfer effectively. This energy alignment is particularly important because machine learning interatomic potentials learn from the relative energy differences between atomic configurations, which become corrupted if the absolute energy scales are mismatched.

## Foundational Learning
- DFT functional hierarchy: Different functionals (GGA, GGA+U, r2SCAN) provide varying levels of accuracy and computational cost. Understanding this hierarchy is needed to identify which functional pairs are suitable for transfer learning. Quick check: Compare energy predictions for identical structures across functionals.
- Energy scale shifts: Different DFT functionals can produce systematically different absolute energy values for the same atomic configuration. This is needed because these shifts create barriers to direct transfer learning. Quick check: Plot energy distributions for structures calculated with different functionals.
- Atomic reference energy refitting: The process of adjusting reference energies to align energy scales between functionals. This is needed to correct systematic offsets in energy predictions. Quick check: Verify that energy distributions overlap after refitting.

## Architecture Onboarding
Component map: Raw structure data -> Energy/force calculations -> ML model training -> Transfer learning with energy referencing -> Performance evaluation
Critical path: The energy referencing step through atomic reference energy refitting is the critical path for successful transfer learning. Without this alignment, the learned atomic interactions from lower-fidelity functionals cannot be effectively transferred to higher-fidelity calculations.
Design tradeoffs: The approach trades computational cost of high-fidelity training data for the complexity of energy referencing. While training from scratch with high-fidelity data is more straightforward, transfer learning with proper referencing provides significant data efficiency advantages, especially for sub-million structure datasets.
Failure signatures: Poor performance when transfer learning without energy referencing, with energy MAE remaining similar to or worse than training from scratch. This manifests as systematic energy shifts and poor force predictions in the transferred model.
Three first experiments:
1. Test transfer learning on a small subset of the MP-r2SCAN dataset to verify the energy referencing methodology
2. Compare energy and force predictions before and after energy referencing to quantify the alignment improvement
3. Evaluate transfer learning performance across different material classes within the dataset to assess generalizability

## Open Questions the Paper Calls Out
The study is based on a single MP-r2SCAN dataset of 238,247 structures, which limits generalizability to other material classes or functionals. The energy scale shift analysis focuses specifically on GGA/GGA+U to r2SCAN transitions, leaving open questions about other functional pairs or multi-level hierarchies. The observed improvement, while significant, remains above chemical accuracy thresholds for some applications. The transfer learning advantage diminishes at larger dataset sizes, suggesting diminishing returns for very large-scale implementations.

## Limitations
- Results are based on a single MP-r2SCAN dataset of 238,247 structures, limiting generalizability
- Energy scale shift analysis focuses specifically on GGA/GGA+U to r2SCAN transitions
- Improvement remains above chemical accuracy thresholds for some applications
- Transfer learning advantages diminish at larger dataset sizes (~500,000 structures)

## Confidence
- Energy referencing methodology: High confidence
- Transfer learning efficiency claims: Medium confidence (dataset-specific)
- Scaling law predictions: Low confidence (extrapolated beyond tested range)

## Next Checks
1. Test transfer learning performance on non-DFT derived datasets (e.g., experimental data or quantum Monte Carlo) to assess functional-agnostic transferability
2. Evaluate long-range force accuracy for extended systems (defects, surfaces) beyond the tested bulk structures
3. Compare against alternative transfer learning approaches (fine-tuning strategies, multi-fidelity training) to benchmark the proposed energy referencing method