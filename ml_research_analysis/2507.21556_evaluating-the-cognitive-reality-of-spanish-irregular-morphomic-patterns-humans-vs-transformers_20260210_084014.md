---
ver: rpa2
title: 'Evaluating the cognitive reality of Spanish irregular morphomic patterns:
  Humans vs. Transformers'
arxiv_id: '2507.21556'
source_url: https://arxiv.org/abs/2507.21556
tags:
- l-shaped
- human
- participants
- responses
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether transformer models generalize Spanish
  morphomic patterns like humans by comparing their performance to human behavioral
  data from Nevins et al. (2015).
---

# Evaluating the cognitive reality of Spanish irregular morphomic patterns: Humans vs. Transformers

## Quick Facts
- **arXiv ID**: 2507.21556
- **Source URL**: https://arxiv.org/abs/2507.21556
- **Reference count**: 30
- **Key outcome**: Transformers generalize Spanish morphomic patterns differently than humans, favoring irregular forms while humans prefer regular forms

## Executive Summary
This study investigates whether transformer models generalize Spanish morphomic patterns like humans by comparing their performance to human behavioral data from Nevins et al. (2015). Models were trained on three frequency conditions (10%L-90%NL, 50%L-50%NL, 90%L-10%NL) and tested on nonce verb production tasks using the same items as the human study. Results showed that transformers achieved higher stem accuracy than humans (52-57% vs 16%) but exhibited opposite response preferences: humans consistently favored regular forms while models preferred irregular forms, with preferences increasing proportionally to irregular verb frequency in training data.

The study concludes that morphomic patterns are more productive in models than humans, who treat irregulars as exceptions. Additionally, models trained on natural and balanced distributions showed sensitivity to phonological similarity between test items and real Spanish L-shaped verbs, mirroring a limited aspect of human generalization. These findings suggest fundamental differences in how transformers and humans process and generalize irregular morphological patterns.

## Method Summary
The study employed a comparative approach between transformer models and human behavioral data from Nevins et al. (2015). Transformer models were trained on Spanish verb data with three different frequency distributions of regular and irregular verbs (10%L-90%NL, 50%L-50%NL, 90%L-10%NL). The models were then tested on nonce verb production tasks using identical items to those used in the human study. Performance was evaluated based on stem accuracy and response preferences, comparing how models and humans generalized morphomic patterns to novel forms.

## Key Results
- Transformers achieved substantially higher stem accuracy than humans (52-57% vs 16%)
- Models and humans showed opposite response preferences - models favored irregular forms while humans consistently preferred regular forms
- Model preferences for irregular forms increased proportionally with irregular verb frequency in training data
- Models showed sensitivity to phonological similarity between test items and real Spanish L-shaped verbs under natural and balanced training distributions

## Why This Works (Mechanism)
The mechanism underlying transformer generalization of morphomic patterns appears to differ fundamentally from human processing. Transformers appear to treat irregular morphological patterns as productive and generalizable, extracting statistical regularities from their training data that allow them to generate irregular forms for novel verbs. This contrasts with human behavior, where irregular patterns are treated as lexical exceptions that are not productively applied to new items. The models' ability to achieve higher accuracy suggests they may be leveraging distributional properties and statistical patterns that humans do not rely upon in the same way.

## Foundational Learning
- **Morphomic patterns**: Recurrent subpatterns in word formation that may not align with traditional morphological or phonological boundaries. Needed to understand the linguistic phenomenon being tested; quick check: identify examples in Spanish verbs.
- **Irregular vs regular verb processing**: The distinction between memorized exceptions and rule-based forms in morphological processing. Needed to frame the human vs model comparison; quick check: list common English irregular verbs.
- **Transformer architecture**: The self-attention mechanism and position encoding that enables sequence modeling. Needed to understand how models process linguistic input; quick check: explain self-attention in one sentence.
- **Nonce verb production tasks**: Experimental paradigm using novel words to test morphological generalization. Needed to understand the experimental methodology; quick check: describe how this differs from real-word processing.

## Architecture Onboarding

**Component map**: Input embeddings -> Positional encoding -> Multi-head self-attention layers -> Feed-forward networks -> Output layer

**Critical path**: The self-attention mechanism is critical for capturing long-range dependencies in morphological patterns, while the feed-forward layers enable complex nonlinear transformations necessary for learning irregular verb forms.

**Design tradeoffs**: The study uses standard transformer architecture without modifications, prioritizing ecological validity over architectural optimization. This allows direct comparison with other transformer-based studies but may miss architecture-specific effects.

**Failure signatures**: When models fail to generalize morphomic patterns, they typically revert to regular forms, suggesting the regular pattern serves as a default fallback. This differs from human failures, which may show more varied error patterns.

**3 first experiments**:
1. Vary the number of attention heads to determine if multi-head attention is crucial for capturing morphomic patterns
2. Test with position encoding removed to assess the importance of word order information
3. Compare with recurrent neural network baselines to isolate transformer-specific effects

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The experimental setup may not fully capture how humans process and generalize irregular morphological patterns in natural language use
- The assumption that stem accuracy directly reflects pattern productivity may overlook other contributing factors like memorization or frequency effects
- The similarity between model and human sensitivity to phonological patterns is noted as limited, with potentially different underlying mechanisms

## Confidence
- **High confidence** in the core finding that transformers generalize morphomic patterns differently than humans, supported by behavioral data comparison and controlled experimental conditions
- **Medium confidence** in the interpretation that models treat irregulars as productive patterns while humans treat them as exceptions, depending on assumptions about stem accuracy
- **Low confidence** in the extent to which model sensitivity to phonological similarity mirrors human generalization, as mechanisms may differ

## Next Checks
1. Conduct systematic analysis of the relationship between training set composition and model preferences by varying irregular verb frequency more finely than the three conditions tested
2. Perform ablation studies to determine which aspects of the transformer architecture contribute to the preference for irregular forms
3. Extend the comparison to include additional transformer variants and traditional neural network architectures to establish whether this pattern of behavior is specific to transformers or represents a broader trend in language models