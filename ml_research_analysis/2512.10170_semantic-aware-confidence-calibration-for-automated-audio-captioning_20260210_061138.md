---
ver: rpa2
title: Semantic-Aware Confidence Calibration for Automated Audio Captioning
arxiv_id: '2512.10170'
source_url: https://arxiv.org/abs/2512.10170
tags:
- confidence
- audio
- semantic
- calibration
- captioning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of overconfident predictions in
  automated audio captioning models, which limits their reliability despite strong
  performance on standard metrics. The authors propose a framework that integrates
  confidence prediction into audio captioning by augmenting a Whisper-based model
  with a learned confidence head that estimates uncertainty from decoder hidden states.
---

# Semantic-Aware Confidence Calibration for Automated Audio Captioning

## Quick Facts
- arXiv ID: 2512.10170
- Source URL: https://arxiv.org/abs/2512.10170
- Reference count: 11
- Primary result: CLAP-based ECE of 0.071 vs. 0.488 for greedy decoding using semantic-aware confidence calibration

## Executive Summary
This paper addresses the problem of overconfident predictions in automated audio captioning models, which limits their reliability despite strong performance on standard metrics. The authors propose a framework that integrates confidence prediction into audio captioning by augmenting a Whisper-based model with a learned confidence head that estimates uncertainty from decoder hidden states. They redefine caption correctness using semantic similarity (CLAP and FENSE) rather than n-gram overlap, enabling meaningful Expected Calibration Error (ECE) computation. Experiments on Clotho v2 show that confidence-guided beam search with semantic evaluation achieves dramatically improved calibration while simultaneously improving caption quality.

## Method Summary
The method augments a Whisper-based audio captioning model with a learned confidence head that estimates uncertainty from decoder hidden states. The confidence head is a 3-layer MLP processing final decoder hidden states, producing token-level confidence scores that are mean-pooled for sequence-level reliability. The framework redefines caption correctness using semantic similarity (CLAP and FENSE) rather than traditional n-gram overlap, enabling meaningful ECE computation. During inference, confidence-guided beam search balances likelihood and confidence scores to jointly improve caption quality and calibration. The model is trained with combined loss on both caption generation and confidence prediction, with temperature scaling applied post-training.

## Key Results
- CLAP-based ECE improved from 0.488 (greedy decoding) to 0.071 using confidence-guided beam search
- BLEU-4 improved from 0.066 to 0.115 (74% increase)
- CIDEr nearly doubled from 0.150 to 0.290
- Confidence distribution concentrated between 0.55-0.70, showing appropriate uncertainty rather than pathological overconfidence
- Semantic correctness redefinition enabled meaningful calibration measurement absent with traditional metrics

## Why This Works (Mechanism)

### Mechanism 1: Semantic Correctness Redefinition
- Claim: Redefining caption correctness via semantic similarity (CLAP/FENSE) rather than n-gram overlap enables meaningful calibration measurement.
- Mechanism: Traditional metrics like BLEU-4 threshold (ECE 0.504) produce poor calibration signals because lexical overlap fails to capture semantic equivalence. CLAP-based correctness (threshold τ=0.6) yields ECE of 0.071 by measuring actual semantic alignment in shared audio-text embedding space.
- Core assumption: Semantic similarity thresholds (τ=0.6) meaningfully approximate human judgments of caption correctness.
- Evidence anchors:
  - [abstract]: "We employ CLAP audio-text embeddings and sentence transformer similarities (FENSE) to define semantic correctness, enabling Expected Calibration Error (ECE) computation that reflects true caption quality rather than surface-level text overlap."
  - [Section IV-B]: "Notably, traditional ECE (using BLEU-4>0.25 as correctness) remains relatively high (0.504) even for beam search... CLAP-based correctness yields dramatically better calibration because it captures semantic equivalence that traditional metrics miss."
  - [corpus]: SPECS paper confirms "N-gram-based metrics though efficient, fail to capture semantic correctness" in related captioning evaluation contexts.
- Break condition: If semantic similarity thresholds do not correlate with human correctness judgments in your target domain, calibration signals become unreliable.

### Mechanism 2: Learned Confidence from Decoder Hidden States
- Claim: A lightweight MLP operating on final decoder hidden states can learn to predict caption-level confidence that correlates with semantic correctness.
- Mechanism: The confidence head (3-layer MLP: 768→384→192→1) processes h_t^(L) at each token, producing confidence scores supervised by MSE loss against semantic correctness. Mean token confidence aggregates to sequence-level reliability estimate.
- Core assumption: Decoder hidden states encode sufficient information about generation uncertainty and potential semantic errors.
- Evidence anchors:
  - [Section III-A-1]: "ct = σ(MLP(h_t^(L))) where h_t^(L) is the decoder hidden state at the final layer for token t, and σ is the sigmoid activation ensuring outputs in [0,1]."
  - [Section IV-C]: "The confidence distribution (mean 0.636, concentrated between 0.55-0.70) shows the model has learned to express appropriate uncertainty rather than the pathological overconfidence (1.0) exhibited by greedy decoding."
  - [corpus]: Corpus evidence is limited; no direct neighbors validate decoder-state confidence prediction for audio captioning specifically.
- Break condition: If hidden states lack error-signaling patterns (e.g., encoder-side ambiguity not propagating to decoder), confidence predictions will be uninformative.

### Mechanism 3: Confidence-Guided Beam Search Reranking
- Claim: Incorporating learned confidence into beam search scoring jointly improves both caption quality and calibration.
- Mechanism: Combined score score(b) = log_p(b)/|b|^α + β·c̄_b balances likelihood and confidence (α=1.0, β=0.3). Reranking selects beams with high semantic probability AND high predicted reliability.
- Core assumption: Confidence scores generalize from training distribution to beam candidates during inference.
- Evidence anchors:
  - [Section III-D]: "For each candidate beam b, we compute a combined score: score(b) = log_p(b)/|b|^α + β·c̄_b where α=1.0 is the length penalty, c̄_b is the mean confidence, and β=0.3 weights the confidence contribution."
  - [Section IV-B]: "Beam search substantially outperforms greedy decoding across all standard metrics. BLEU-4 improves by 74% (0.066→0.115), CIDEr nearly doubles (0.150→0.290)."
  - [corpus]: CATTO paper notes "high-confidence predictions are frequently wrong" in LLMs, supporting the need for explicit calibration mechanisms during decoding.
- Break condition: If β weighting is too high, beam search may select low-likelihood but high-confidence outputs; if too low, confidence has no effect.

## Foundational Learning

- **Concept: Expected Calibration Error (ECE)**
  - Why needed here: Core metric for measuring whether confidence scores align with actual correctness. Requires understanding binning, accuracy-confidence gaps, and why correctness definition fundamentally affects ECE validity.
  - Quick check question: Given 10 samples with confidence [0.9, 0.9, 0.8, 0.7, 0.6, 0.5, 0.3, 0.3, 0.2, 0.1] and correctness [1, 0, 1, 1, 0, 0, 0, 0, 0, 0], how would you compute ECE with 2 bins?

- **Concept: CLAP (Contrastive Language-Audio Pretraining) Embeddings**
  - Why needed here: Enables audio-grounded semantic evaluation. CLAP maps audio and text to shared embedding space where cosine similarity reflects semantic alignment, bypassing n-gram limitations.
  - Quick check question: Why might a caption with high CLAP similarity to reference text still receive low BLEU-4 score?

- **Concept: Temperature Scaling for Probability Calibration**
  - Why needed here: Post-hoc calibration technique that learns a single temperature parameter T to sharpen or soften softmax outputs. Optimized on validation data via L-BFGS.
  - Quick check question: If model outputs are overconfident (probabilities too extreme), should T be increased or decreased?

## Architecture Onboarding

- **Component map:**
  ```
  Audio Input (16kHz, max 30s)
         ↓
  Whisper Encoder (frozen from MU-NLPC/whisper-small-audio-captioning)
         ↓
  Whisper Decoder (d_model=768)
         ↓
  ├── Logits → Temperature Scaling (learnable T)
  │        ↓
  │   Softmax → Token Probabilities
  │
  └── Hidden States h_t^(L) → Confidence Head (MLP: 768→384→192→1)
                           ↓
                      Token Confidence c_t → Mean Pooling → Sequence Confidence
  ```

- **Critical path:**
  1. Load pretrained MU-NLPC/whisper-small-audio-captioning
  2. Initialize confidence head with proper dropout (0.1) and Xavier initialization
  3. Initialize temperature T=1.0
  4. Precompute CLAP/FENSE embeddings for all reference captions
  5. Train with combined loss (λ=0.15), monitoring semantic accuracy not just BLEU
  6. Optimize temperature on validation set post-training
  7. Deploy with confidence-guided beam search (beam=5, α=1.0, β=0.3)

- **Design tradeoffs:**
  - **Correctness threshold (τ=0.6)**: Arbitrary choice; higher thresholds increase precision but reduce positive samples for calibration training
  - **Confidence loss weight (λ=0.15)**: Too high may degrade caption quality; too low produces weak confidence signals
  - **Beam search β (0.3)**: Controls confidence influence; domain-specific tuning likely required
  - **Base model choice**: Whisper provides strong grounding but is heavier than ConvNeXt+BART alternatives (see Appendix VI on failed lighter experiments)

- **Failure signatures:**
  - ECE remains high (>0.3) despite training: Check if correctness threshold τ is appropriate for dataset; verify CLAP embedding quality
  - Caption quality degrades: Reduce λ or β; confidence signal may be overwhelming generation objective
  - Confidence always near 0.5: Confidence head may be underfitting; increase capacity or training epochs
  - All confidence = 1.0: Temperature scaling not applied or confidence head not properly attached

- **First 3 experiments:**
  1. **Baseline calibration check**: Run greedy decoding on validation set, compute ECE with all three correctness definitions (BLEU>0.25, CLAP>0.6, FENSE>0.6) to quantify overconfidence gap.
  2. **Ablate confidence head vs. temperature**: Train (a) confidence head only, (b) temperature only, (c) both—to isolate contribution of each component.
  3. **Threshold sensitivity analysis**: Sweep τ ∈ {0.5, 0.55, 0.6, 0.65, 0.7} and measure impact on ECE and training positive rate; verify τ=0.6 is justified for Clotho.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can audio-grounded confidence estimation using CLAP embeddings of the audio signal itself (not just text) improve calibration over decoder-hidden-state approaches?
  - Basis in paper: [explicit] "audio-grounded confidence using CLAP embeddings of the audio itself, not just text" is listed as future work.
  - Why unresolved: The current confidence head operates only on decoder hidden states, which the authors acknowledge "may not capture all sources of uncertainty (e.g., encoder-side ambiguity)."
  - What evidence would resolve it: A comparison of decoder-based vs. encoder-based vs. joint audio-text confidence heads on ECE and caption quality metrics.

- **Open Question 2**: How well does the calibration framework generalize to other audio captioning datasets beyond Clotho v2?
  - Basis in paper: [explicit] "we evaluate only on Clotho; generalization to other datasets (AudioCaps, larger-scale data) requires validation."
  - Why unresolved: Clotho contains 15-30 second environmental sounds; other datasets like AudioCaps have different characteristics (shorter clips, different acoustic domains).
  - What evidence would resolve it: Evaluation on AudioCaps, SoundDescs, or other benchmarks showing comparable calibration improvements.

- **Open Question 3**: How sensitive are calibration results to the choice of semantic similarity threshold (τ = 0.6)?
  - Basis in paper: [explicit] "our semantic correctness threshold (τ= 0.6) is somewhat arbitrary; different applications may require different thresholds."
  - Why unresolved: The paper uses a single fixed threshold without ablation; optimal thresholds may vary by application domain or risk tolerance.
  - What evidence would resolve it: Ablation study showing ECE, CLAP accuracy, and caption quality across threshold values (e.g., 0.4–0.8).

- **Open Question 4**: Can selective prediction systems that abstain from generating captions when confidence is low improve deployment reliability?
  - Basis in paper: [explicit] "selective prediction systems that abstain when confidence is low" is listed as future work.
  - Why unresolved: The paper demonstrates calibration but does not explore abstention mechanisms that would leverage uncertainty estimates operationally.
  - What evidence would resolve it: Experiments showing coverage-accuracy tradeoffs when rejecting low-confidence predictions at various thresholds.

## Limitations

- The choice of τ=0.6 as the semantic correctness threshold may not generalize across datasets or domains without recalibration.
- Reliance on CLAP and FENSE embeddings introduces domain-specific assumptions about what constitutes semantic similarity that may not align with human judgment in all contexts.
- Experiments are conducted on a single dataset (Clotho v2) with environmental sounds, limiting generalizability to other audio captioning domains like speech or music.

## Confidence

- **High Confidence**: The mechanism that redefining correctness via semantic similarity (CLAP/FENSE) rather than n-gram overlap enables meaningful calibration measurement. This is strongly supported by the dramatic ECE improvement (0.488→0.071) and aligns with established understanding that traditional captioning metrics fail to capture semantic equivalence. The experimental evidence is robust with clear before/after comparisons.

- **Medium Confidence**: The claim that learned confidence from decoder hidden states reliably predicts caption-level uncertainty. While the architecture is well-specified and results show improved calibration, the evidence relies primarily on a single dataset without ablation studies comparing confidence head to alternative uncertainty estimation methods. The core assumption that decoder states encode sufficient uncertainty information lacks theoretical validation.

- **Medium Confidence**: The assertion that confidence-guided beam search simultaneously improves both caption quality and calibration. The empirical improvements are substantial (BLEU-4 0.066→0.115, CIDEr 0.150→0.290), but the weighting parameters (α=1.0, β=0.3) appear tuned for this specific task without sensitivity analysis. The claim that this generalizes across domains requires further validation.

## Next Checks

1. **Cross-dataset calibration generalization**: Evaluate the proposed framework on non-environmental sound datasets (e.g., AudioCaps with diverse audio types) to test whether τ=0.6 remains optimal and whether CLAP-based ECE improvements transfer across domains.

2. **Ablation of confidence source**: Replace the decoder hidden state confidence head with alternative uncertainty estimation methods (e.g., Monte Carlo dropout, ensemble methods) to isolate whether the MLP on h_t^(L) is the optimal approach for capturing caption uncertainty.

3. **Human judgment correlation**: Conduct human evaluation comparing CLAP/FENSE-based correctness judgments against human annotators' assessments of caption quality, particularly for cases near the τ=0.6 threshold where calibration sensitivity is highest.