---
ver: rpa2
title: Are Large Language Models Capable of Deep Relational Reasoning? Insights from
  DeepSeek-R1 and Benchmark Comparisons
arxiv_id: '2506.23128'
source_url: https://arxiv.org/abs/2506.23128
tags:
- reasoning
- llms
- deepseek-r1
- family
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how well Large Language Models (LLMs)\
  \ perform deep relational reasoning. Three state-of-the-art models\u2014DeepSeek-R1,\
  \ DeepSeek-V3, and GPT-4o\u2014are evaluated on two benchmark tasks: family tree\
  \ and general graph reasoning."
---

# Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons

## Quick Facts
- arXiv ID: 2506.23128
- Source URL: https://arxiv.org/abs/2506.23128
- Reference count: 39
- Key outcome: DeepSeek-R1 consistently outperforms GPT-4o and DeepSeek-V3 on deep relational reasoning benchmarks, but all models struggle with increasing complexity due to token limitations.

## Executive Summary
This paper investigates the deep relational reasoning capabilities of Large Language Models (LLMs) through evaluation on family tree and general graph reasoning benchmarks. Three state-of-the-art models—DeepSeek-R1, DeepSeek-V3, and GPT-4o—are tested on tasks requiring multi-step logical inference from basic relational facts. DeepSeek-R1 demonstrates superior performance, particularly through its extended Chain-of-Thought reasoning, achieving the highest F1-scores across most tasks. However, all models show significant performance degradation as problem complexity increases, primarily due to token length limitations and incomplete output structures. The study provides detailed analysis of DeepSeek-R1's reasoning patterns, revealing robust planning and verification abilities but also highlighting instances of incoherent or incomplete reasoning.

## Method Summary
The study evaluates deep relational reasoning using two benchmark tasks: family tree reasoning (with relations like HasSister, IsGrandson, IsAunt, IsPaternalGreatAunt) and general graph reasoning (Connectivity and Shortest-path problems). Models are evaluated zero-shot using natural language prompts describing base relations, with outputs parsed as Boolean matrices. F1-score is used as the primary metric, with F1=0 assigned for invalid JSON or incorrect matrix shapes. Data is generated for 100 samples per problem with varying complexity levels (n=10, 20, 40). The evaluation focuses on DeepSeek-R1 (671B), DeepSeek-V3, and GPT-4o, analyzing both aggregate performance and individual Chain-of-Thought reasoning traces.

## Key Results
- DeepSeek-R1 achieves highest F1-scores across most relational reasoning tasks, particularly excelling at complex multi-step inferences
- All models demonstrate significant performance degradation as problem complexity increases (n=10→20→40)
- Token length limitations emerge as a primary constraint, with truncated reasoning leading to incomplete or malformed outputs
- DeepSeek-R1 shows evidence of planning and verification behaviors in its Chain-of-Thought, though some reasoning traces exhibit incoherence

## Why This Works (Mechanism)

### Mechanism 1: Long Chain-of-Thought Reasoning
Long CoT reasoning enables multi-step relational inference by decomposing complex problems into intermediate deduction steps. DeepSeek-R1's reinforcement learning training produces extended reasoning traces that break down relations like parent-child to grandchild before synthesizing final answers.

### Mechanism 2: Iterative Verification Loops
The model cross-examines intermediate deductions, identifying conflicts and redundancies before committing to conclusions. DeepSeek-R1 repeatedly verifies parent-child relations before deducing grandson relations, improving answer reliability.

### Mechanism 3: Architecture-Level Token Limits
As problem size increases from n=10→20→40, prompt length plus reasoning tokens exceed context windows, causing truncated outputs scored as F1=0. This creates hard performance ceilings that conflate architectural limitations with reasoning capability.

## Foundational Learning

- **Relational reasoning complexity scaling**: The paper tests 4 relation types with different logical depths (sister: 1-hop; grandson: 2-hop; paternal great aunt: 3-hop). Understanding this hierarchy is essential for interpreting F1-score degradation patterns. Quick check: Can you explain why IsPaternalGreatAunt (F1=0.39 at n=10) is harder than HasSister (F1=0.80 at n=10)?

- **Zero-shot evaluation methodology**: The paper uses zero-shot prompting (no in-context examples) to isolate model reasoning capability from pattern-matching. Quick check: What is the trade-off between zero-shot and few-shot prompting for evaluating "true" reasoning ability?

- **F1-score for structured prediction**: The evaluation uses F1-score on Boolean matrices, where class imbalance (sparse relations) makes accuracy misleading. Quick check: Why would accuracy be a poor metric for the IsGrandson relation when most entries are 0?

## Architecture Onboarding

- **Component map**: Input Prompt → Long CoT Generation → Summarization & Comprehension → Abstraction & Filtering → Planning → Verification & Confirmation → JSON Output Parser → Matrix Comparison → F1-Score
- **Critical path**: Prompt construction → token budget allocation → CoT generation window → output parsing robustness
- **Design tradeoffs**: Longer CoT improves reasoning but increases token consumption; larger n improves evaluation granularity but risks token overflow
- **Failure signatures**: Malformed JSON (F1=0), incorrect matrix shape (F1=0), truncated reasoning (incomplete matrix), mid-process planning suggesting unsound reasoning structure
- **First 3 experiments**:
  1. Baseline scaling test: Run all 6 benchmark tasks at n=10, n=20, n=40 to reproduce token-limit degradation curve
  2. Token budget ablation: Limit max_output_tokens to 50%, 75%, 100% to quantify reasoning depth vs. completion tradeoff
  3. Reasoning trace analysis: Manually annotate 10 DeepSeek-R1 outputs for planning emergence timing

## Open Questions the Paper Calls Out

- **Multimodal input modalities**: Can visual or diagrammatic representations of relational structures (e.g., family trees) serve as a more effective input modality than purely textual prompts? The study evaluates reasoning exclusively through text-based prompts and doesn't test multimodal inputs.

- **Delayed planning rationale**: Why does the reasoning plan emerge midway through the process in DeepSeek-R1 rather than at the outset, and does this indicate a lack of underlying logical soundness? The paper observes this qualitatively but doesn't explain the mechanism.

- **Latent error patterns**: What specific latent error patterns (e.g., premature assumptions or flawed inferences) characterize the reasoning traces that lead to incorrect conclusions? The analysis focuses on successful reasoning demonstrations rather than categorizing specific failure modes.

## Limitations

- **Token budget dependency**: The study identifies token limitations as a primary failure mode but lacks systematic quantification of how varying token budgets affects reasoning quality, conflating architectural limitations with reasoning capability.

- **Internal reasoning validation gap**: While extensively analyzing Chain-of-Thought outputs, the paper doesn't provide quantitative validation of claimed planning and verification mechanisms, relying heavily on qualitative inspection.

- **Generalizability concerns**: Evaluation focuses on two specific benchmark types with predetermined logical structures, potentially limiting generalizability to more complex relational domains or real-world knowledge structures.

## Confidence

**High Confidence**: DeepSeek-R1 consistently outperforms other models on benchmarks; all models show performance degradation with complexity; token limitations significantly constrain performance.

**Medium Confidence**: Long CoT reasoning enables multi-step inference; iterative verification improves reliability; token limits create hard ceilings.

**Low Confidence**: DeepSeek-R1 demonstrates "human-like deliberative thinking" beyond benchmark measurements; planning and verification abilities are robust and reliable; multimodal reasoning would significantly enhance performance (remains speculative).

## Next Checks

1. **Token Budget Sensitivity Analysis**: Systematically vary max_tokens parameter (50%, 75%, 100%, 150% of default) for each model and task to quantify the relationship between reasoning depth and completion rates.

2. **Reasoning Trace Coherence Scoring**: Develop a rubric to quantitatively score the coherence, planning quality, and verification completeness of Chain-of-Thought outputs. Apply this to 100 randomly sampled DeepSeek-R1 responses.

3. **Cross-Domain Generalization Test**: Extend evaluation to include at least two additional relational reasoning domains (e.g., temporal reasoning, social network analysis) to assess whether observed performance patterns generalize beyond current benchmarks.