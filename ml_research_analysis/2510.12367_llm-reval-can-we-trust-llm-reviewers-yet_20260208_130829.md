---
ver: rpa2
title: 'LLM-REVal: Can We Trust LLM Reviewers Yet?'
arxiv_id: '2510.12367'
source_url: https://arxiv.org/abs/2510.12367
tags:
- papers
- review
- llms
- biases
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-REVal explores whether LLM reviewers can be trusted by simulating
  a research-review pipeline where both LLM-authored and human-authored papers are
  evaluated. The study finds that LLM reviewers systematically assign higher scores
  to LLM-authored papers, prefer stylistic features common in LLM text (conciseness,
  lexical diversity), and undervalue human papers addressing critical topics.
---

# LLM-REVal: Can We Trust LLM Reviewers Yet?

## Quick Facts
- **arXiv ID**: 2510.12367
- **Source URL**: https://arxiv.org/abs/2510.12367
- **Reference count**: 40
- **Key outcome**: LLM reviewers systematically assign higher scores to LLM-authored papers, prefer LLM-typical linguistic features (conciseness, lexical diversity), and undervalue human papers addressing critical topics, raising fairness concerns for deployment without safeguards.

## Executive Summary
This study simulates a research-review pipeline where both LLM-generated and human-authored papers are evaluated by LLM reviewers. The findings reveal systematic biases: LLM reviewers prefer papers with LLM-typical stylistic features (lexical diversity, conciseness), assign higher scores to LLM-authored papers (78% acceptance vs 49% for human), and persistently undervalue human papers containing critical statements about risks or limitations. Despite multiple revisions, some human papers remain unreviewed, highlighting a misalignment with human judgments. These biases stem from linguistic feature preferences and aversion to negative framing, raising significant fairness concerns for deploying LLM reviewers in real academic settings.

## Method Summary
The study uses a multi-agent simulation pipeline where a research agent generates LLM-authored papers matching human paper topics (extracted from 100 ICLR papers across 10 ML/NLP topics), and a review agent evaluates submissions through a 5-stage process (Reviewer Assessment I → Author–Reviewer Discussion → Assessment II → Meta-Review → Final Decision). Papers are iteratively revised and re-reviewed up to 6 rounds. The research agent uses DeepSeek-R1 for idea generation and DeepSeek-V3 for writing, while the review agent uses DeepSeek-R1. Both agents leverage retrieval-augmented generation (Semantic Scholar API) for literature access.

## Key Results
- LLM-authored papers received higher average scores (6.21 vs 5.94) and 78% acceptance rate compared to 49% for human papers
- Human annotators preferred human-authored papers 56.7% vs 33.3% for LLM papers in head-to-head comparisons
- LLM reviewers showed positive correlation with lexical diversity (r=0.2795, p<0.001) and readability complexity (r=0.1923, p=0.0067), negative correlation with text length (r=-0.1912, p=0.007)
- Human papers with critical statements about risks/biases received lower scores despite being valued by human reviewers

## Why This Works (Mechanism)

### Mechanism 1
LLM reviewers assign higher scores to papers exhibiting LLM-typical linguistic features. They correlate positively with lexical diversity (1-gram: r=0.2795, p<0.001), readability complexity (FKG: r=0.1923, p=0.0067), and negatively with text length (r=-0.1912, p=0.007). When human-authored papers are polished by LLMs, their scores increase proportionally to the polishing ratio (5.69→5.94 at 40% polishing).

### Mechanism 2
LLM reviewers systematically undervalue papers containing critical statements about risks, biases, or limitations. Papers with higher counts of negative keywords (e.g., "risk," "bias," "limitation") in abstracts show negative correlation with LLM review scores in human-authored papers, but positive correlation in LLM-authored papers—suggesting aversion to negative framing specifically.

### Mechanism 3
LLM reviewers exhibit self-preference, inflating scores for LLM-authored papers beyond quality differences. LLM papers receive 78% acceptance vs. 49% for human papers; in head-to-head comparisons, LLM papers win 66% of the time. However, human annotators prefer human papers 56.7% vs. 33.3% in pairs with largest score disparities—indicating misalignment.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**: The research agent uses RAG with Semantic Scholar for literature retrieval during idea generation and reference selection. *Quick check*: Can you explain how the paper's research agent uses retrieved literature to constrain hallucinations in generated content?

- **Multi-Agent Simulation Pipelines**: The study uses a research agent + review agent architecture with iterative revise-review cycles. *Quick check*: What are the five stages of the review agent pipeline, and how does information flow between them?

- **Correlation vs. Causation in Bias Analysis**: The paper relies on correlation (e.g., linguistic features ↔ scores) to argue for causal bias mechanisms. *Quick check*: Why is the polishing experiment (Section 7.1) stronger evidence for causal bias than the feature-score correlations alone?

## Architecture Onboarding

- **Component map**: Research Agent (Semantic Scholar API → DeepSeek-R1 → DeepSeek-V3) → Paper Generation → Review Agent (Reviewer Assessment I → Author–Reviewer Discussion → Assessment II → Meta-Review → Final Decision) → Final Decision

- **Critical path**: 1. Initialize research agent with topic keywords (extracted from human papers) 2. Generate LLM-authored papers matching human paper topics 3. Submit both paper types to review agent 4. Route low-scoring papers (<6) to revision guided by review feedback 5. Repeat until acceptance or 6 rounds

- **Design tradeoffs**: Using predicted results instead of real experiments trades ecological validity for scalability (acknowledged limitation, Section 3.1); Single model (DeepSeek family) for both agents increases internal consistency but limits generalizability (partially addressed via GPT-4o, Qwen3, Gemini cross-validation in Appendix A.6); 100 human papers balances cost (~500k input tokens/paper generation, ~200k/review) with statistical power

- **Failure signatures**: Papers stuck in revision loop after 6 rounds (5% of human papers)—likely due to critical statement content or non-LLM-typical style; Low acceptance indication accuracy (73.7%) suggests review agent may misclassify borderline papers; Hallucinated references—mitigated but not eliminated by citation verification module

- **First 3 experiments**: 1. Replicate the polishing experiment: Take 10 human-authored papers, polish at 0%, 25%, 50%, 75%, 100% ratios, measure score trajectory to confirm linguistic feature bias causality 2. Controlled framing study: Generate paired papers (identical content, positive vs. negative framing of limitations) to isolate critical-statement aversion 3. Cross-model self-preference test: Have GPT-4o review DeepSeek-generated papers and vice versa to disentangle model-specific vs. general LLM self-preference effects

## Open Questions the Paper Calls Out

- **Question**: What specific debiasing techniques can effectively mitigate the LLM reviewer preference for generated writing styles and aversion to critical statements? *Basis*: The conclusion states that "Addressing these biases is essential" for fairness, but the study stops short of proposing or testing technical interventions.

- **Question**: To what extent can "adversarial polishing" (optimizing text for LLM-preferred stylistic features without adding content) game the LLM review system? *Basis*: Section 7.1 shows a correlation between LLM-polishing ratios and higher scores, suggesting style alone can influence acceptance decisions.

- **Question**: Do the identified biases against human-authored papers persist in academic domains outside of Computer Science and NLP? *Basis*: The simulation is restricted to ICLR papers covering ten ML/NLP topics (Section 4.2), leaving cross-domain generalization untested.

## Limitations
- The study uses predicted rather than real experimental results, trading ecological validity for scalability
- Single-model approach (DeepSeek family) limits generalizability across different LLM architectures
- Human annotators as ground truth assumes they provide valid quality assessment, which may not always hold

## Confidence
- **High confidence**: Self-preference bias (LLM papers score higher, human annotators confirm preference for human papers)
- **Medium confidence**: Linguistic feature preference (strong correlations, but causal interpretation relies on polishing experiment)
- **Low confidence**: Critical statement aversion (based on keyword correlations; framing experiments needed)

## Next Checks
1. Conduct controlled framing experiments: generate identical papers with positive vs. negative limitation statements to isolate sentiment effects
2. Cross-model validation: have GPT-4o, Claude, and Gemini review both DeepSeek-generated and their own papers to test model-specific vs. general LLM bias
3. Real vs. predicted results test: compare reviewer scores when LLM papers cite real experimental results versus predicted ones to assess impact on review quality judgments