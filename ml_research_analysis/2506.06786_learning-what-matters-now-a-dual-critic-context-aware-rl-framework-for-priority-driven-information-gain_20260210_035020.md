---
ver: rpa2
title: 'Learning What Matters Now: A Dual-Critic Context-Aware RL Framework for Priority-Driven
  Information Gain'
arxiv_id: '2506.06786'
source_url: https://arxiv.org/abs/2506.06786
tags:
- priority
- information
- exploration
- ca-miq
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CA-MIQ (Context-Aware Max-Information Q-learning) is a dual-critic
  RL framework that dynamically adapts to shifting mission priorities in SAR environments.
  It pairs an extrinsic critic for task rewards with an intrinsic critic that combines
  state-novelty, information-location awareness, and priority alignment.
---

# Learning What Matters Now: A Dual-Critic Context-Aware RL Framework for Priority-Driven Information Gain

## Quick Facts
- arXiv ID: 2506.06786
- Source URL: https://arxiv.org/abs/2506.06786
- Authors: Dimitris Panagopoulos; Adolfo Perrusquia; Weisi Guo
- Reference count: 25
- Primary result: CA-MIQ achieves 65.9% mission success after single priority shift and 50.2% with multiple shifts in simulated SAR grid-world

## Executive Summary
CA-MIQ (Context-Aware Max-Information Q-learning) addresses the challenge of dynamic priority shifts in search-and-rescue environments through a dual-critic reinforcement learning framework. The system pairs an extrinsic critic for task rewards with an intrinsic critic that combines state-novelty, information-location awareness, and priority alignment. A shift detector triggers transient exploration boosts and selective critic resets when priorities change. In simulated SAR environments, CA-MIQ demonstrates robust adaptation to piecewise-stationary information-value distributions, achieving 100% recovery success while baselines fail to adapt.

## Method Summary
CA-MIQ implements dual-critic Q-learning with an extrinsic critic Q_E for task rewards and an intrinsic critic Q_I for information-directed exploration. The intrinsic reward combines novelty (β₁/√N(s,a)), information-location awareness (β₂/√N_info(s)), and priority alignment (+β₃/-β₄). A shift detector monitors collection success rates and triggers ε-boosting (doubling exploration probability) and selective Q-value resets (λ=0.5) when priority shifts are detected. The ε-MaxInfoRL policy selects actions based on intrinsic rewards during exploration and extrinsic rewards during exploitation.

## Key Results
- Achieved 65.9% mission success rate after a single priority shift in SAR grid-world
- Maintained 50.2% success rate with multiple priority shifts, compared to baseline failures
- Reached 100% recovery success while baselines failed to adapt to shifting priorities
- Adaptation required approximately 600-800 episodes following priority shifts

## Why This Works (Mechanism)

### Mechanism 1: Dual-Critic Information-Directed Exploration
Separating task rewards from information-gain rewards enables exploration that is both purposeful and priority-aligned. The extrinsic critic Q_E learns standard task returns while the intrinsic critic Q_I aggregates novelty, location awareness, and priority alignment. The ε-MaxInfoRL policy selects actions based on Q_I during exploration and Q_E during exploitation. Core assumption: weighted combination accurately captures "what matters now." Break condition: misconfigured intrinsic weights cause exploration misalignment.

### Mechanism 2: Transient Epsilon Boosting with Decay
Temporarily increasing exploration probability after priority shifts accelerates escape from obsolete policies. Upon shift detection, ε ← min(ε_MAX, ε · λ_boost), then decays exponentially over D_boost episodes. This increases reliance on Q_I exploration precisely when reorientation is needed. Core assumption: boost magnitude and decay horizon are sufficient for adaptation. Break condition: excessive boost wastes episodes on exploration; insufficient boost fails to escape local optima.

### Mechanism 3: Selective Critic Reset
Partially resetting only information-collection-related Q-values preserves environment knowledge while forcing relearning of priority-dependent behaviors. On shift detection, Q_•(s,a) ← λ · Q_•(s,a) for collection actions only, leaving navigation/transit values intact. Core assumption: priority shifts primarily affect information-collection, not movement. Break condition: if priority shifts affect navigation, selective reset fails to clear obsolete path knowledge.

## Foundational Learning

- **Concept: Tabular Q-Learning with ε-Greedy Exploration**
  - Why needed: CA-MIQ extends standard Q-learning with second critic; baseline Q-updates are prerequisite
  - Quick check: Can you explain why ε-greedy exploration fails in non-stationary priority environments?

- **Concept: Intrinsic Motivation in RL**
  - Why needed: The intrinsic critic Q_I drives exploration via novelty and uncertainty reduction, not task rewards
  - Quick check: How does count-based novelty (1/√N(s,a)) differ from curiosity-driven exploration via prediction error?

- **Concept: Non-Stationary / Contextual MDPs**
  - Why needed: CA-MIQ treats priority ordering as latent context with piecewise-stationary shifts
  - Quick check: What signals could indicate context shift when explicit priority updates are unavailable?

## Architecture Onboarding

- **Component map:** ShiftDetector → EpsilonController → SelectiveReset; Q_E (extrinsic critic) + Q_I (intrinsic critic) → ε-MaxInfoRL policy → Agent → Environment

- **Critical path:** 1) Agent observes state s_t 2) With probability ε: a_t = argmax Q_I(s_t,a); else a_t = argmax Q_E(s_t,a) 3) Execute a_t, observe r_t, s_{t+1} 4) Update both critics 5) ShiftDetector checks for priority change → trigger boost + selective reset 6) Decay ε per schedule

- **Design tradeoffs:** Intrinsic weight allocation (30/40/30) - novelty most critical; Reset factor λ=0.5 - conservative vs aggressive; Tabular limitation - restricted to discrete spaces

- **Failure signatures:** Agent continues old priority order → shift detector not triggering; Excessive wandering → ε decay too slow; No improvement → novelty component not tracking

- **First 3 experiments:** 1) Static baseline: CA-MIQ vs Q-learning on fixed-priority SAR 2) Single-shift ablation: trigger shift at episode 1700, compare full vs w/o-boost vs w/o-reset 3) Recovery time measurement: instrument episodes to 90% recovery, target <800 episodes

## Open Questions the Paper Calls Out

- **Open Question 1:** Can CA-MIQ extend to continuous state-action spaces while preserving adaptation capabilities and computational efficiency? Current implementation is inherently limited to discrete spaces.

- **Open Question 2:** Can adaptive weighting mechanisms automatically adjust coefficients balancing novelty, information-location awareness, and priority alignment based on mission characteristics? Current 30/40/30 split is manually tuned.

- **Open Question 3:** Can adaptation time be reduced below 600-800 episodes through adaptive reset scheduling or parameterized exploration boosts? Current fixed reset factor (λ=0.5) and boost factor (λboost=2) are uniform heuristics.

- **Open Question 4:** How does CA-MIQ perform in partially observable domains where agent cannot directly observe current priority ordering? Current experiments assume explicit or detectable priority changes.

## Limitations

- Performance claims depend on specific reward parameters and shift detection thresholds not fully specified
- Selective reset mechanism shows only modest gains (-1.5% adaptation time, -1.8% mission success) in ablation studies
- Exact reward scaling for intrinsic components remains unspecified beyond relative weights (30/40/30%)

## Confidence

- **High confidence:** Dual-critic framework structure and ε-MaxInfoRL policy formulation (directly specified in equations)
- **Medium confidence:** Adaptation performance claims (65.9% single-shift success), dependent on unspecified reward parameters
- **Low confidence:** Claims about selective reset importance, given minimal ablation impact and lack of corpus precedent

## Next Checks

1. **Reward sensitivity analysis:** Vary intrinsic reward scaling factors (β₁, β₂, β₃, β₄) across orders of magnitude to identify stable operating ranges

2. **Shift detection robustness:** Test with noisy collection success signals to determine minimum signal-to-noise ratio for reliable shift detection

3. **Reset mechanism ablation:** Compare full CA-MIQ against versions with full reset, no reset, and selective reset to quantify each mechanism's contribution across multiple shift frequencies