---
ver: rpa2
title: 'Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language'
arxiv_id: '2510.06378'
source_url: https://arxiv.org/abs/2510.06378
tags:
- feature
- semantic
- example
- language
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces semantic regexes, a structured language for
  automatically describing LLM features. The core idea is to use a constrained set
  of primitives (symbols, lexemes, fields) combined with modifiers (context, composition,
  quantification) to capture activation patterns more precisely than natural language.
---

# Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language

## Quick Facts
- arXiv ID: 2510.06378
- Source URL: https://arxiv.org/abs/2510.06378
- Reference count: 40
- Semantic regexes achieve similar performance to natural language baselines while offering greater conciseness and consistency

## Executive Summary
This paper introduces semantic regexes, a structured language for automatically describing LLM features. The core idea is to use a constrained set of primitives (symbols, lexemes, fields) combined with modifiers (context, composition, quantification) to capture activation patterns more precisely than natural language. The method is embedded in a standard automated interpretability pipeline, using an explainer model to generate descriptions and an evaluator model to score them. Across three models and 100 features per layer, semantic regexes perform on par with natural language methods while offering significant advantages in conciseness and consistency.

## Method Summary
Semantic regexes provide a structured language for describing LLM features by combining primitive elements (symbols, lexemes, fields) with modifiers for context, composition, and quantification. The approach uses an explainer model to generate descriptions and an evaluator model to score them within a standard automated interpretability pipeline. The language is designed to capture activation patterns more precisely than natural language while remaining interpretable to humans.

## Key Results
- Semantic regexes achieve similar discrimination, generation, and faithfulness scores compared to natural language baselines
- Descriptions are significantly more concise (median 41 characters vs 139 for max-acts) and more consistent (33.6% identical vs 0.0%)
- Semantic regex complexity increases with layer depth, reflecting more abstract feature patterns
- User study shows semantic regex descriptions help participants build more accurate mental models of features

## Why This Works (Mechanism)
Semantic regexes work by providing a constrained, structured language that forces descriptions to focus on activation patterns rather than free-form natural language. The primitives capture basic linguistic elements while modifiers allow for contextual and compositional descriptions. This structure enables more precise pattern matching and consistent descriptions across similar features, while still being interpretable to humans through a minimal instruction set.

## Foundational Learning

**Activation Maximization**: A technique to find inputs that maximally activate specific neural network features, used here to generate interpretable feature examples. *Why needed*: Provides concrete examples for generating feature descriptions. *Quick check*: Can you identify which features are being described in the results?

**Automated Interpretability Pipeline**: A standard workflow involving feature discovery, description generation, and evaluation. *Why needed*: Provides the framework for testing semantic regex performance. *Quick check*: Can you trace how features flow through the pipeline?

**Lexemes vs Tokens**: Lexemes are linguistic units that may span multiple tokens, while tokens are the basic units of model input. *Why needed*: Semantic regexes use both to capture different levels of linguistic structure. *Quick check*: Can you distinguish when to use lexeme vs token primitives?

## Architecture Onboarding

**Component Map**: Feature Discovery -> Semantic Regex Generation -> Evaluation -> User Study
**Critical Path**: Activation maximization features → Explainer model generates semantic regex → Evaluator model scores descriptions → Human evaluation of mental models
**Design Tradeoffs**: Structured language vs. natural language expressiveness; conciseness vs. descriptive completeness; automation vs. human oversight
**Failure Signatures**: Poor performance on polysemantic features; difficulty capturing rare patterns; user confusion with complex patterns
**First Experiments**:
1. Test semantic regex generation on a small set of simple features
2. Compare semantic regex conciseness vs. natural language baselines
3. Evaluate user comprehension with basic semantic regex patterns

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Evaluation focuses on activation maximization features rather than sparse autoencoder features
- Does not address polysemantic feature capture or rare pattern descriptions
- User study has limited sample size (24 participants) and feature diversity
- Scalability to larger models and more complex tasks remains untested

## Confidence

**High confidence**: Semantic regexes achieve similar discrimination and faithfulness scores compared to natural language baselines, and the conciseness and consistency advantages are empirically demonstrated

**Medium confidence**: The claim that semantic regexes help users build more accurate mental models, as this relies on a small-scale user study with limited feature diversity

**Medium confidence**: The observation that semantic regex complexity correlates with layer depth, as this could be influenced by the specific models and features studied rather than being a universal property

## Next Checks

1. Test semantic regex performance on features from sparse autoencoders and other discovery methods to assess generalizability beyond activation maximization
2. Conduct a larger-scale user study with more participants and diverse feature types, including polysemantic and rare-pattern features, to better evaluate usability and mental model formation
3. Apply semantic regexes to larger, more capable models and more complex tasks to determine if the structured language scales effectively to richer semantic patterns and higher model capacities