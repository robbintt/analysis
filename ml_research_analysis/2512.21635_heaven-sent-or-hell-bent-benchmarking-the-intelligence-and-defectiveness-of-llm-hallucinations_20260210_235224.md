---
ver: rpa2
title: Heaven-Sent or Hell-Bent? Benchmarking the Intelligence and Defectiveness of
  LLM Hallucinations
arxiv_id: '2512.21635'
source_url: https://arxiv.org/abs/2512.21635
tags:
- hallucination
- hallucinations
- llms
- creativity
- scientific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes HIC-Bench, a novel evaluation framework that
  reframes LLM hallucinations as potentially valuable outputs rather than purely errors.
  It introduces two categories: Intelligent Hallucinations (IH), which encode creative
  and plausible scientific content, and Defective Hallucinations (DH), which contain
  factual inaccuracies.'
---

# Heaven-Sent or Hell-Bent? Benchmarking the Intelligence and Defectiveness of LLM Hallucinations

## Quick Facts
- arXiv ID: 2512.21635
- Source URL: https://arxiv.org/abs/2512.21635
- Authors: Chengxu Yang; Jingling Yuan; Siqi Cai; Jiawei Jiang; Chuang Hu
- Reference count: 40
- Primary result: HIC-Bench framework shows hallucinations can be valuable (IH) or detrimental (DH), with nonlinear relationship enabling joint optimization of creativity and correctness

## Executive Summary
This paper challenges the conventional view of LLM hallucinations as purely errors by introducing HIC-Bench, a framework that distinguishes between Intelligent Hallucinations (IH) containing creative, plausible scientific content, and Defective Hallucinations (DH) containing factual inaccuracies. Through systematic evaluation across ten scientific domains, the research reveals that reducing DH does not necessarily diminish IH, demonstrating that creativity and correctness can be jointly optimized. The proposed Dynamic Hallucination Prompt (DHP) effectively balances factual accuracy and innovation by iteratively refining outputs using in-context learning.

## Method Summary
The HIC-Bench framework evaluates LLM outputs on open-ended scientific innovation tasks using a multi-dimensional metric matrix integrating Torrance Tests of Creative Thinking (Originality, Feasibility, Value) with hallucination-specific dimensions. The Intelligent-Fidelity Score (IFS) combines IH ratio with penalties for neutral responses. The framework tests multiple prompt strategies (SCP, RCP, CoT, RAG) and introduces DHP, which dynamically updates few-shot examples based on creativity scores. The Cross-Domain Innovation Dataset (CDID) contains 100 tasks across 10 scientific domains, evaluated by multi-model LLM judges.

## Key Results
- Nonlinear relationship between IH and DH: reducing DH does not inherently diminish IH
- RCP strategy increases IH (e.g., gpt-4o: 9.60% → 18.60%) while reducing DH (1.30% → 0.50%)
- DHP effectively balances accuracy and creativity, reducing DH to 0.90% for gpt-4o-mini and 0.10% for gpt-4o
- Different models excel under different IFS weightings, revealing distinct creativity-accuracy profiles

## Why This Works (Mechanism)

### Mechanism 1: Nonlinear IH/DH Relationship via Relaxed Constraints
Claim: Reducing defective hallucinations (DH) does not inherently diminish intelligent hallucinations (IH); under certain constraints, IH can be preserved or enhanced while DH is reduced.
Mechanism: Prompt strategies like RCP remove stringent feasibility constraints while maintaining value/innovation requirements. This allows models to explore more novel solution spaces without necessarily increasing factual errors.
Core assumption: IH and DH have distinct generative origins that can be decoupled through targeted prompting, and the creativity-accuracy relationship is not fundamentally zero-sum.
Evidence anchors:
- [abstract] "Experimental results reveal a nonlinear relationship between IH and DH, demonstrating that creativity and correctness can be jointly optimized."
- [section 4.2] "Results reveal that RCP significantly elevates IH across all models, while reducing Defective Hallucinations (DH) outputs in certain cases. For example, gpt-4o's IH rises from 9.60% under SCP to 18.60% under RCP, yet its DH decreases from 1.30% to 0.50%."
- [corpus] Neighbor paper "Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs" directly investigates this tradeoff, supporting relevance.
Break condition: If IH and DH were perfectly correlated, any DH reduction would proportionally reduce IH, invalidating the decoupling strategy.

### Mechanism 2: Dynamic Hallucination Prompt (DHP) for In-Context Optimization
Claim: A dynamic prompting pipeline can iteratively refine outputs to maximize IH while minimizing DH using real-time feedback from previous outputs.
Mechanism: DHP maintains a dynamic set of few-shot examples. When a response achieves high creativity scores (Originality ≥4, Feasibility ≥3, Value ≥4), it becomes a new "positive example." Hallucinated responses become "negative examples." These guide subsequent generations toward the "intelligent hallucination" output space.
Core assumption: The model can effectively generalize from few-shot examples to improve its generation strategy, learning the abstract pattern distinguishing "intelligent" from "defective" hallucinations.
Evidence anchors:
- [abstract] "The proposed Dynamic Hallucination Prompt (DHP) effectively balances factual accuracy and creativity, enabling optimization of both innovation and reliability."
- [section 4.4] "During machine evaluation, responses with IFS scores exceeding the initial positive examples are set as new positive examples, while negative examples are continuously updated with the latest DH responses. With DHP positive prompting, DH drops notably to 0.90% for gpt-4o-mini and 0.10% for gpt-4o."
- [corpus] Evidence is weak regarding dynamic prompting specifically; neighbor papers focus on general creativity-hallucination relationships rather than control techniques.
Break condition: Fails if the model's in-context learning cannot generalize from examples, or if examples become contradictory, causing unstable performance.

### Mechanism 3: Creativity-Grounded Multi-Dimensional Evaluation (IFS)
Claim: A composite metric integrating creativity dimensions (Originality, Feasibility, Value) with hallucination dimensions (IH/DH) can effectively discriminate between valuable and detrimental outputs in scientific innovation tasks.
Mechanism: The Intelligent-Fidelity Score (IFS) combines IH ratio with a weighted penalty for neutral responses, capturing the innovation-reliability trade-off. Different models excel under different weightings (e.g., deepseek-r1 leads in creativity-weighted IIFS).
Core assumption: Torrance Tests of Creative Thinking (TTCT) metrics are valid proxies for evaluating LLM scientific creativity and can be reliably scored by LLM judges.
Evidence anchors:
- [abstract] "...a multi-dimensional metric matrix integrating Torrance Tests of Creative Thinking (TTCT) metrics (Originality, Feasibility, Value) with hallucination-specific dimensions..."
- [section 3.2.3] "To evaluate the balanced performance of LLMs across creativity and hallucination tendencies, we propose the IFS, a unified metric that integrates results from creativity assessment and hallucination classification."
- [corpus] "What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models" discusses creativity benchmarking, indirectly supporting multi-dimensional evaluation.
Break condition: Fails if dimensions (Or, Fe, Va) are not predictive of actual scientific utility or if LLM judges are systematically biased.

## Foundational Learning

- Concept: **Intelligent Hallucinations (IH) vs. Defective Hallucinations (DH)**
  - Why needed here: The entire framework hinges on this dichotomy. An engineer must understand that not all factual deviations are errors—some are potentially valuable creative leaps.
  - Quick check question: Given an output proposing a novel material with plausible theoretical properties but no real-world example yet, is this more likely an IH or a DH?

- Concept: **Torrance Tests of Creative Thinking (TTCT) for LLMs**
  - Why needed here: Evaluation metrics (Originality, Feasibility, Value) are adapted from human creativity assessment. Understanding their origin and adaptation is key to interpreting HIC-Bench scores.
  - Quick check question: If an LLM proposes a solution with Originality=5 but Feasibility=1 (violates basic physics), would it be classified as IH? (Answer: No, IH requires Fe≥3)

- Concept: **In-Context Learning via Few-Shot Prompting**
  - Why needed here: DHP mechanism relies on this. The model learns the "intelligent hallucination" pattern from dynamically updated positive/negative examples in its prompt context.
  - Quick check question: How does the DHP pipeline use outputs from previous generations to influence future ones?

## Architecture Onboarding

- Component map: CDID (100 tasks) → Prompt Strategy Engine (SCP, RCP, CoT, RAG, DHP) → LLM Generator (gpt-4o, deepseek-v3) → Raw Output → Evaluation System (LLM judges) → Scores (Or, Fe, Va, IH/DH) → IFS Calculator → Final Performance Metric

- Critical path: 1. Task input → Prompt Strategy Engine → LLM Generator → Raw Output; 2. Raw Output → Evaluation System → Scores; 3. Scores → IFS Calculator → Final Performance Metric; 4. (If using DHP) Scores → DHP Pipeline → Update examples → Loop back

- Design tradeoffs:
  - Accuracy vs. Creativity: IFS weight (w1) controls this; higher w1 prioritizes IH, lower prioritizes accuracy
  - Automation vs. Human Alignment: LLM judges enable scalability but require initial human annotation for verification
  - Constraint vs. Exploration: SCP enforces facts (may limit creativity); RCP encourages exploration (may increase DH); DHP navigates this dynamically

- Failure signatures:
  - Stuck in Local Optima: DHP plateaus with examples becoming repetitive
  - Metric Gaming: Generator maximizes proxy metrics (Or, Fe, Va) without producing genuinely useful ideas (Goodhart's Law)
  - Judge-Generator Alignment: Systematic bias in LLM judges invalidates evaluation framework

- First 3 experiments:
  1. Establish Baselines: Run all selected LLMs on CDID using baseline SCP prompt; calculate IH, DH, and IFS scores to understand intrinsic creativity-accuracy profiles
  2. Prompt Strategy Ablation: For a representative model (e.g., GPT-4o), compare SCP, RCP, CoT, and RAG effects; analyze how each strategy shifts IH/DH balance to validate nonlinear relationship claim
  3. DHP Effectiveness Test: Implement DHP pipeline on a task subset; compare IH/DH evolution over iterations against static-prompt baseline to verify dynamic optimization mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do traditional hallucination mitigation strategies inadvertently curtail LLMs' creative capacity?
- Basis in paper: [explicit] Section 3.1 poses this as a pivotal research question guiding the HIC-Bench framework design.
- Why unresolved: The experiments show complex, model-dependent patterns (e.g., CoT increases IH for deepseek-v3 but decreases it for gpt-4o), suggesting the relationship between mitigation and creativity varies across architectures.
- What evidence would resolve it: Systematic comparison of creativity metrics before and after applying standardized mitigation techniques across a wider range of model architectures and scientific domains.

### Open Question 2
- Question: Can the IH/DH classification framework generalize effectively to multimodal and cross-lingual benchmarks?
- Basis in paper: [explicit] Section G states the framework will be applied to "multimodal and cross-lingual benchmarks to validate its generalizability."
- Why unresolved: Current evaluation is limited to English text-based scientific Q&A tasks; visual and multilingual contexts may require fundamentally different creativity and hallucination assessment approaches.
- What evidence would resolve it: Cross-validation studies comparing IH/DH classification consistency between text-only, multimodal, and multilingual versions of equivalent tasks.

### Open Question 3
- Question: What are the optimal weight configurations (w1, w2) for IFS across different application domains?
- Basis in paper: [inferred] Section 3.2.3 sets w1=0.6 and w2=0.4 arbitrarily without empirical justification, while acknowledging these should adapt to task requirements.
- Why unresolved: The paper demonstrates domain-specific performance variations but does not systematically optimize weights or validate whether static weights suffice across domains.
- What evidence would resolve it: Ablation studies correlating different IFS weight configurations with task-specific success metrics (e.g., scientific peer review acceptance, experimental validation rates).

## Limitations

- Limited Generalization: HIC-Bench focuses on scientific innovation tasks; effectiveness for other domains (e.g., legal reasoning, historical analysis) remains untested.
- Judge Model Reliability: IFS depends heavily on LLM judges for scoring creativity and detecting factual deviations, though 85% agreement with human annotators provides some validation.
- DHP Implementation Gaps: Algorithm 1 lacks implementation details for determining "high creativity" thresholds and example weighting mechanisms.

## Confidence

- High Confidence: The nonlinear IH/DH relationship observed across multiple models and prompt strategies is well-supported by quantitative results.
- Medium Confidence: The DHP mechanism's effectiveness is demonstrated but relies on implicit assumptions about in-context learning capabilities.
- Low Confidence: The broader applicability of TTCT metrics for LLM evaluation across domains remains unproven.

## Next Checks

1. Cross-Domain Validation: Test HIC-Bench on non-scientific domains (e.g., creative writing, historical analysis) to verify the IH/DH framework generalizes beyond structured scientific problems.
2. Human Benchmark Study: Conduct a comprehensive human evaluation study with diverse annotators to establish ground truth for IH/DH classification and IFS scoring.
3. DHP Convergence Analysis: Track DHP performance across multiple iterations to identify convergence patterns, potential example redundancy, and optimal stopping criteria.