---
ver: rpa2
title: Visually Prompted Benchmarks Are Surprisingly Fragile
arxiv_id: '2512.17875'
source_url: https://arxiv.org/abs/2512.17875
tags:
- marker
- visual
- accuracy
- blink
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Visually prompted benchmarks are highly fragile to small design\
  \ changes. The authors show that simple modifications to visual markers\u2014such\
  \ as color, shape, size, or text positioning\u2014can shift model rankings significantly\
  \ on visually prompted tasks like relative depth estimation and semantic correspondence."
---

# Visually Prompted Benchmarks Are Surprisingly Fragile

## Quick Facts
- **arXiv ID**: 2512.17875
- **Source URL**: https://arxiv.org/abs/2512.17875
- **Reference count**: 40
- **Primary result**: Simple changes to visual marker design (color, shape, size, text position) can significantly reorder model rankings on visually prompted tasks

## Executive Summary
Visually prompted benchmarks for VLMs are highly unstable to small design choices. The authors demonstrate that changing visual markers from red to blue, or adjusting text positioning, can completely reorder model rankings on tasks like relative depth estimation and semantic correspondence. They also find that i.i.d. resampling and JPEG compression levels cause large accuracy and ranking fluctuations, especially for visually prompted tasks. This instability is much higher than in conventional VLM benchmarks. To address this, the authors create VPBench, a larger, marker-diverse benchmark with 16 visual marker variants, enabling more stable and robust evaluations.

## Method Summary
The paper evaluates nine VLMs on visually prompted tasks (relative depth and semantic correspondence) using BLINK-style benchmarks and introduces VPBench with 35K+ images across 16 marker variants. The evaluation systematically varies marker design (color, shape, size, text position), image compression levels (70-90 JPEG quality), and performs bootstrap resampling to measure ranking stability. The authors compare visually prompted tasks against conventional benchmarks (MME) to quantify differential sensitivity. They use paired bootstrap testing to disentangle marker-induced variance from data variance.

## Key Results
- Simple marker style changes (color, shape, size, text position) cause accuracy swings up to 21% on identical image-question pairs
- Visually prompted tasks show ~4x higher standard deviation in accuracy across splits than conventional benchmarks
- JPEG compression levels imperceptible to humans reorder model rankings on visually prompted tasks but not on knowledge-focused benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Visual Marker Style as a Non-Semantic Confounder
- Claim: Visual marker design choices that should be semantically irrelevant systematically affect model predictions and can reorder leaderboards
- Core assumption: Marker style differences should not affect task performance if models genuinely understand the underlying visual reasoning task
- Evidence anchors:
  - [abstract] "simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard"
  - [Section 5] Marker style changes cause accuracy swings up to 21% on identical image-question pairs; text size and label position produce largest effects
  - [corpus] Related work on visual prompt engineering confirms marker choice alters model attention and outcomes

### Mechanism 2: Small Sample Sizes Amplify Variance on Unsaturated Tasks
- Claim: Visually prompted benchmarks with small sample sizes (100 examples) and model accuracies far from ceiling exhibit substantially higher ranking instability
- Core assumption: The underlying task difficulty distribution is similar across resampled subsets
- Evidence anchors:
  - [Section 4] "1,000 new BLINK size datasets by randomly sampling 100 samples...we can get a complete change in rankings"
  - [Figure 5] Visually prompted tasks show ~4x higher standard deviation in accuracy across splits than MME
  - [corpus] Limited direct corpus evidence on sample size effects specifically for VPT benchmarks

### Mechanism 3: Fine-Grained Visual Tasks Are Sensitive to Low-Level Perturbations
- Claim: JPEG compression levels imperceptible to humans reorder model rankings on visually prompted tasks but not on knowledge-focused benchmarks
- Core assumption: The compression levels tested preserve semantic content for humans but may alter features models rely on
- Evidence anchors:
  - [Section 6] "rankings in BLINK RD fluctuate considerably...In contrast, MME rankings remain relatively stable"
  - [Figure 9] Shows rank changes across compression rates for BLINK RD vs. MME
  - [corpus] Prior work documents similar effects in image generation evaluation

## Foundational Learning

- **Visual Prompting Paradigm**
  - Why needed here: Understanding that VPTs mark image regions with visual overlays and ask spatial/perceptual questions distinguishes this evaluation regime from text-only or knowledge-focused benchmarks
  - Quick check question: If you showed a model an image with two red circles labeled "A" and "B" and asked "Which is closer to the camera?", what makes this a visually prompted task rather than a standard VQA task?

- **Leaderboard Fragility**
  - Why needed here: The paper's central claim is that rankings reflect evaluation artifacts as much as model capability
  - Quick check question: If Model A scores 52% and Model B scores 54% on a 100-sample benchmark with ~4% standard deviation, should you declare Model B superior?

- **Paired Bootstrap Testing**
  - Why needed here: The paper uses paired bootstrap to disentangle marker-induced variance from data variance
  - Quick check question: Why use paired differences (same images, different markers) rather than independent samples when testing whether marker style affects accuracy?

## Architecture Onboarding

- **Component map**: Input images -> Marker overlay engine (16 variants: color, shape, size, text position) -> Compressed image (configurable JPEG quality) -> VLM -> Prediction -> Aggregator (computes accuracy across variants/splits)

- **Critical path**:
  1. Load clean image + annotation coordinates (do NOT use pre-rendered markers)
  2. Apply specified marker variant consistently across evaluation
  3. Apply compression if testing low-level sensitivity
  4. Query VLM with standardized prompt format
  5. Aggregate across 16 marker variants and report mean accuracy + confidence intervals

- **Design tradeoffs**:
  - Single marker vs. multi-variant: Single marker is faster but may reflect prompt idiosyncrasies; multi-variant is more robust but 16x compute
  - Small curated vs. large scaled: BLINK-style (100 samples) is interpretable but noisy; VPBench (35K samples) is stable but computationally expensive
  - Lossless vs. compressed: Lossless formats eliminate compression variance but may not reflect real-world API conditions

- **Failure signatures**:
  - Rankings reverse when marker color changes (e.g., red â†’ blue flips Model A above Model B)
  - Accuracy drops >10% when text label moves from above to below marker
  - JPEG quality 70 vs. 90 produces different winners on same model set

- **First 3 experiments**:
  1. **Baseline replication**: Evaluate your VLM on VPBench-RD with default marker (red circle, label above) to establish baseline accuracy and compare to paper's reported scores for similar models
  2. **Marker sensitivity test**: Run same evaluation with 3 marker variants (default, blue circle, square) on a 500-sample subset; if accuracy varies >5%, your model exhibits the fragility described
  3. **Confidence interval check**: Bootstrap 100 resamples of 100 examples each; if ranking position varies across resamples, your conclusions are not stable given sample size

## Open Questions the Paper Calls Out

- **What are the underlying mechanisms causing VLMs to be so sensitive to visual marker style changes?**
  - The authors note "there is not a single marker style that was universally best or worst for all models. These idiosyncratic responses point to each model having its own biases in visual prompt processing," but do not investigate the root cause

- **Does the observed fragility extend to other visually prompted tasks beyond relative depth and semantic correspondence?**
  - The authors state VPBench is "currently limited to depth and correspondence tasks" and that "Any benchmark that depends on explicit visual markup or fine-grained spatial cues risks similar instability," explicitly leaving broader task coverage as future work

- **Can models be made robust to visual prompt style through training interventions?**
  - The paper shows some models can be "gamed" to outperform larger models via marker selection, suggesting models may have overfit to specific marker distributions, but no training remedies are explored

## Limitations
- The paper identifies several sources of fragility but does not fully establish the underlying causal mechanisms
- Experiments focus on evaluation rather than training, leaving open questions about whether similar fragility would appear during model development
- The study only examines two task types, leaving generalization to other visually prompted tasks untested

## Confidence
- **High Confidence**: The empirical observation that marker style changes cause significant accuracy swings and ranking instability is well-supported by direct measurements across 9 VLMs and 16 marker variants
- **Medium Confidence**: The claim that small sample sizes amplify ranking instability on unsaturated tasks is supported by bootstrap analysis showing ~4x higher standard deviation for VPT benchmarks
- **Low Confidence**: The mechanism explaining why JPEG compression disproportionately affects visually prompted tasks versus knowledge-focused benchmarks remains speculative

## Next Checks
1. **Mechanism Validation**: Conduct ablation studies systematically varying marker properties (color, shape, size) independently to isolate which visual features most strongly correlate with accuracy changes

2. **Architecture Sensitivity Analysis**: Compare robustness across different VLM architectures (transformer variants, vision encoders) to identify whether certain design choices make models more or less susceptible to marker and compression artifacts

3. **Real-World Generalization**: Evaluate whether the fragility observed in controlled benchmarks translates to practical applications by testing whether models that perform well on VPBench's multi-variant evaluation also demonstrate more consistent performance across real-world scenarios with varying visual presentations