---
ver: rpa2
title: 'From Style to Facts: Mapping the Boundaries of Knowledge Injection with Finetuning'
arxiv_id: '2503.05919'
source_url: https://arxiv.org/abs/2503.05919
tags:
- finetuning
- data
- information
- facts
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the factors influencing the effectiveness
  of finetuning language models for knowledge injection versus task customization.
  The authors conduct a large-scale experimental study using Gemini v1.5 models, systematically
  varying information type (numerical, categorical, emotional), training data format
  (question-answer pairs, Wikipedia articles, reasoning problems, etc.), entity type
  (real-world, fictional, personas), and information quantity (20, 200, or 4,000 facts).
---

# From Style to Facts: Mapping the Boundaries of Knowledge Injection with Finetuning

## Quick Facts
- **arXiv ID**: 2503.05919
- **Source URL**: https://arxiv.org/abs/2503.05919
- **Authors**: Eric Zhao; Pranjal Awasthi; Nika Haghtalab
- **Reference count**: 26
- **Key outcome**: Systematic study of finetuning factors shows question-answer training data significantly outperforms Wikipedia-style articles for knowledge injection, with numerical information notably harder to retain than categorical or emotional facts.

## Executive Summary
This paper investigates what factors influence the effectiveness of finetuning language models for knowledge injection versus task customization. Through a large-scale experimental study using Gemini v1.5 models, the authors systematically vary information type, training data format, entity type, and information quantity to identify patterns in finetuning success. The study reveals that question-answer training data formats significantly outperform Wikipedia-style articles, numerical information is harder to finetune than categorical or emotional information, and models struggle to apply finetuned knowledge during multi-step reasoning tasks. Notably, the authors find no significant difference in finetuning effectiveness between learning about real-world entities versus model personas.

## Method Summary
The authors conduct a systematic study using Gemini v1.5 Pro/Flash models with LoRA finetuning across 4,000+ experiments. They create an entity bank of 1,000 real-world Wikipedia entities, 1,000 matched fictional entities, and 120 personas, each with 120 facts (40 numerical, 40 categorical, 40 emotional). Five training formats are tested: QA pairs, multi-turn QA, role-play QA, reasoning problems, and Wikipedia articles. The study varies information quantity (20, 200, or 4,000 facts) and evaluates across six task formats. Models are finetuned for 100 epochs with datasets regularized to ~200K characters per entity, and results are evaluated by Gemini v2.0 Flash thinking model.

## Key Results
- Question-answer training data formats significantly outperform Wikipedia-style articles for both direct knowledge retrieval and reasoning tasks
- Numerical information is notably harder to finetune than categorical or emotional information across all formats
- Models struggle to apply finetuned knowledge during multi-step reasoning tasks, even when they can recall facts directly
- No significant difference in finetuning effectiveness between real-world entities and model personas
- More facts during training leads to lower accuracy per fact (power-law decay relationship)

## Why This Works (Mechanism)

### Mechanism 1: Training Data Format Alignment
Claim: Question-answer training data formats produce stronger knowledge generalization than document-style data, even for non-QA evaluation tasks. The benefit stems from format familiarity rather than information density, as QA pairs may activate query-response retrieval patterns established during instruction tuning, whereas Wikipedia-style documents require models to perform "random access" extraction from unstructured text—a known weakness of autoregressive LLMs.

### Mechanism 2: Information Type Difficulty Gradient
Claim: Numerical facts are consistently harder to finetune than categorical or emotional facts across all training formats and evaluation tasks. This may be because numerical information lacks the semantic scaffolding that categorical facts receive from related concepts in pretraining data, and numbers are stored with correct order of magnitude but imprecise values.

### Mechanism 3: Reasoning Application Gap (Reversal Curse and Random Access)
Claim: Models can recall finetuned facts in direct QA but fail to apply them during multi-step reasoning, even when trained on reasoning examples. This failure is attributed to the "reversal curse" (trained on "A→B" but cannot answer "B→A") and random access limitations (cannot reliably retrieve facts buried in intermediate reasoning steps).

## Foundational Learning

- **LoRA (Low-Rank Adaptation) finetuning**: All experiments use LoRA rather than full finetuning; understanding parameter-efficient adaptation is required to interpret results. Quick check: Can you explain why LoRA might constrain knowledge injection differently than full weight updates?
- **Distribution shift and train-test alignment**: The paper's core finding is that alignment between training format and evaluation task predicts success; this is a classical ML principle applied to LLM finetuning. Quick check: If you train on Wikipedia articles but evaluate on QA pairs, what performance drop should you expect relative to training on QA pairs?
- **Reversal curse and random access limitations**: The paper invokes these mechanisms to explain reasoning failures; you need to understand them to interpret the reasoning experiment results. Quick check: If a model is trained on "Brenton Browder met 107 parliamentarians," can it answer "Who met 107 parliamentarians?"—and why might this fail?

## Architecture Onboarding

- **Component map**: Entity bank (1,000 real + 1,000 fictional + 120 personas) -> Fact bank (120 facts per entity: 40 numerical, 40 categorical, 40 emotional) -> Training format generators (5 formats: QA, Multi-turn QA, Role-play QA, Reasoning, Wikipedia article) -> Evaluation tasks (6 formats: Direct QA, Conversational QA, Role-play, Wiki fill-in-blank, Wiki sentence completion, Reasoning) -> Finetuning pipeline (LoRA on Gemini v1.5 Pro/Flash, 100 epochs)
- **Critical path**: 1) Select entity type and information type, 2) Sample N facts and generate training data in chosen format, 3) Run LoRA finetuning for 100 epochs, 4) Evaluate on all 6 task formats using ICL-validated questions, 5) Compare accuracy across training format × evaluation task matrix
- **Design tradeoffs**: QA training data provides best generalization but requires preprocessing; Reasoning training data is best for reasoning evaluation but poor at direct QA; Wikipedia training data is convenient but weakest overall; More facts = lower accuracy per fact (power-law decay)
- **Failure signatures**: Numerical facts with Wiki training show lowest accuracy; High fact counts (4,000) cause significant accuracy degradation; Reasoning evaluation shows low accuracy across all formats; Mismatched train-eval formats show asymmetric transfer (QA→Wiki works better than Wiki→QA)
- **First 3 experiments**: 1) Reproduce training format effect: Finetune on 20 numerical facts using QA vs. Wikipedia format; evaluate on both direct QA and Wiki fill-in-blank, 2) Information type ablation: Using same entity and QA format, compare numerical vs. categorical fact retention, 3) Reasoning application test: Finetune on reasoning-formatted data; evaluate whether model can recall facts directly and apply them in novel reasoning problems

## Open Questions the Paper Calls Out

### Open Question 1
What is the mechanistic explanation for why numerical information is significantly harder for models to retain through finetuning compared to categorical or emotional information? The paper demonstrates the numerical difficulty gap but does not isolate whether this is due to tokenization, representation learning, or precision constraints in the finetuning process.

### Open Question 2
Is the poor performance of Wikipedia-style articles as training data primarily caused by "random access" limitations of parametric knowledge? While the paper shows articles are less effective than QA pairs, it only speculates on the cause, noting the asymmetry is difficult to explain solely via information-theoretic arguments.

### Open Question 3
Is the difficulty of applying finetuned knowledge during multi-step reasoning causally related to the "reversal curse"? The study shows models fail to use facts in reasoning even when they can recall them directly, suggesting a generalization failure, but the link to specific directional biases remains hypothesized.

### Open Question 4
Does the observed difficulty in retaining numerical facts and reasoning generalize to full parameter finetuning, or is it an artifact of Low-Rank Adaptation (LoRA)? The study uses exclusively LoRA, and it remains unclear if the low-rank nature of these updates restricts the model's ability to encode precise numerical changes or complex reasoning pathways.

## Limitations
- The study uses synthetic entities and facts, which may not generalize to real-world knowledge injection scenarios where facts are interdependent and semantically rich
- Mechanistic explanations for format alignment benefits and reasoning failures remain speculative without direct causal evidence
- The study focuses exclusively on LoRA finetuning, leaving open whether results generalize to full parameter updates

## Confidence

**High Confidence**: The empirical findings that QA training formats outperform Wikipedia formats across evaluation tasks, and that numerical information is harder to finetune than categorical information, are well-supported by the extensive experimental matrix.

**Medium Confidence**: The claim that there is no significant difference between real-world entities and personas in finetuning effectiveness is supported, but the synthetic nature of the entities limits generalizability to real knowledge injection.

**Low Confidence**: The mechanistic explanations for why QA formats work better (query-response retrieval patterns) and why reasoning application fails (reversal curse and random access limitations) are hypothesized but not empirically validated within the study.

## Next Checks

1. **Causal mechanism test**: Design an experiment that directly tests whether QA format superiority stems from query-response patterns by comparing QA training with identically-structured non-QA training (e.g., "fact-statement" pairs that maintain the same syntactic structure but remove the question-response dynamic).

2. **Real-world generalization**: Apply the same experimental methodology to a small set of real Wikipedia entities with naturally occurring facts, comparing performance to the synthetic entity results to assess external validity.

3. **Reasoning gap isolation**: Create a controlled reasoning task where models must apply a single finetuned fact in a multi-step problem, then systematically vary the position of the fact in the reasoning chain to test whether the failure is due to random access limitations or deeper reasoning deficits.