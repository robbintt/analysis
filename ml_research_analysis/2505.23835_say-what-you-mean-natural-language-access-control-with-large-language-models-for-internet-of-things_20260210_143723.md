---
ver: rpa2
title: 'Say What You Mean: Natural Language Access Control with Large Language Models
  for Internet of Things'
arxiv_id: '2505.23835'
source_url: https://arxiv.org/abs/2505.23835
tags:
- access
- control
- policy
- policies
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LACE, a language-based access control engine
  for IoT systems that bridges the gap between natural language policy descriptions
  and machine-enforceable access control. The system leverages large language models
  (LLMs) to automatically generate structured access policies from natural language,
  validate their correctness through formal verification and natural language inference,
  and make context-aware access decisions via retrieval-augmented generation.
---

# Say What You Mean: Natural Language Access Control with Large Language Models for Internet of Things

## Quick Facts
- arXiv ID: 2505.23835
- Source URL: https://arxiv.org/abs/2505.23835
- Reference count: 40
- Primary result: LACE achieves 100% verified policy generation correctness and up to 88% decision accuracy using DeepSeek-V3

## Executive Summary
This paper introduces LACE, a language-based access control engine that enables natural language descriptions of access policies to be automatically converted into machine-enforceable rules for IoT systems. The system leverages large language models (LLMs) to bridge the gap between human-readable policy descriptions and formal access control mechanisms. LACE combines LLM reasoning with traditional rule-based enforcement using Open Policy Agent (OPA) to ensure both flexibility and safety in access control decisions for smart home environments.

## Method Summary
LACE operates through a pipeline that transforms natural language policy descriptions into structured access policies. The system uses LLMs for automatic policy generation, followed by formal verification and natural language inference to validate correctness. For access decisions, LACE employs retrieval-augmented generation (RAG) to provide context-aware reasoning. The generated policies are enforced using OPA, creating a hybrid approach that combines the flexibility of LLM reasoning with the safety guarantees of rule-based systems. The system was evaluated in smart home environments with multiple policy volumes and concurrent request scenarios.

## Key Results
- 100% verified correctness in policy generation across all tested scenarios
- Up to 88% decision accuracy with DeepSeek-V3, outperforming GPT-3.5 (62%) and Gemini (63%)
- 0.79 F1-score in access decision making
- Strong scalability demonstrated with increasing policy volumes and concurrent requests

## Why This Works (Mechanism)
LACE works by leveraging the reasoning capabilities of LLMs to interpret natural language policy descriptions and convert them into structured, enforceable rules. The system combines this with formal verification techniques to ensure correctness, while using RAG to provide contextual information during access decisions. The hybrid approach of using LLMs for interpretation and OPA for enforcement creates a system that is both flexible enough to handle natural language input and robust enough to provide reliable access control.

## Foundational Learning

**Large Language Models (LLMs)**: AI models trained on vast text corpora that can understand and generate human-like text. Needed because they can interpret natural language policy descriptions. Quick check: Can the LLM accurately parse complex conditional statements in policy descriptions?

**Open Policy Agent (OPA)**: A policy engine that evaluates policies as code. Needed to provide deterministic enforcement of access control rules. Quick check: Does OPA correctly enforce policies generated from natural language descriptions?

**Retrieval-Augmented Generation (RAG)**: A technique that combines information retrieval with text generation to provide context-aware responses. Needed to incorporate relevant context during access decision making. Quick check: Does RAG successfully retrieve and incorporate relevant context for access decisions?

**Formal Verification**: Mathematical techniques to prove the correctness of systems against formal specifications. Needed to ensure generated policies accurately represent intended access control rules. Quick check: Do verification checks consistently identify policy generation errors?

## Architecture Onboarding

**Component Map**: Natural Language Policy Description -> LLM Policy Generator -> Formal Verifier -> OPA Policy Store -> Context Retriever -> RAG Decision Engine -> Access Decision

**Critical Path**: Natural language policy description → LLM generation → formal verification → OPA enforcement → access decision

**Design Tradeoffs**: The system trades computational overhead from LLM processing and verification for increased flexibility in policy specification. Using OPA as a safety layer adds latency but ensures policy correctness. The RAG component improves decision accuracy but requires additional infrastructure for context storage and retrieval.

**Failure Signatures**: 
- Incorrect policy generation due to ambiguous natural language descriptions
- Verification failures indicating policy specification errors
- Context retrieval failures leading to suboptimal access decisions
- OPA evaluation errors from malformed policy rules

**First Experiments**:
1. Test policy generation with simple, unambiguous natural language descriptions
2. Verify system behavior with conflicting policy descriptions
3. Evaluate decision accuracy with synthetic context data

## Open Questions the Paper Calls Out

None

## Limitations

- Performance heavily dependent on LLM quality, with significant degradation using less capable models (GPT-3.5: 62% accuracy vs DeepSeek-V3: 88% accuracy)
- System effectiveness constrained by quality and completeness of natural language policy descriptions
- Potential security vulnerabilities from incorrect policy generation if verification fails

## Confidence

- **High confidence**: Policy generation correctness (100% verified), OPA integration for safety enforcement
- **Medium confidence**: Decision accuracy metrics (88% with DeepSeek-V3), F1-score of 0.79, scalability claims
- **Low confidence**: Performance consistency across diverse IoT environments beyond smart homes, long-term reliability with evolving policy sets

## Next Checks

1. Test LACE across heterogeneous IoT domains (industrial, healthcare, automotive) to assess generalization beyond smart home environments
2. Conduct longitudinal studies measuring performance degradation over time with evolving policy sets and increasing complexity
3. Perform adversarial testing with intentionally ambiguous or conflicting natural language policies to evaluate robustness of the verification pipeline