---
ver: rpa2
title: Closing the Modality Gap for Mixed Modality Search
arxiv_id: '2507.19054'
source_url: https://arxiv.org/abs/2507.19054
tags:
- modality
- text
- image
- clip
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mixed modality search requires retrieving semantically relevant
  content across heterogeneous documents containing text, images, and multimodal combinations.
  Current contrastive models like CLIP suffer from a modality gap, where image and
  text embeddings form distinct clusters, causing intra-modal ranking bias and inter-modal
  fusion failure.
---

# Closing the Modality Gap for Mixed Modality Search

## Quick Facts
- arXiv ID: 2507.19054
- Source URL: https://arxiv.org/abs/2507.19054
- Reference count: 40
- Mixed-modality search improves by up to 26 percentage points NDCG@10 over CLIP via post-hoc calibration.

## Executive Summary
Mixed-modality search requires retrieving semantically relevant content from heterogeneous documents containing text, images, or multimodal combinations. Current contrastive models like CLIP suffer from a modality gap, where image and text embeddings form distinct clusters, causing intra-modal ranking bias and inter-modal fusion failure. This paper proposes GR-CLIP, a lightweight post-hoc calibration method that removes the modality gap by mean-centering embeddings for each modality. Evaluated on MixBench, GR-CLIP improves NDCG@10 by up to 26 percentage points over CLIP and surpasses VLM2Vec by 4 percentage points while using 75x less compute, demonstrating that closing the modality gap is both effective and efficient for unified multimodal retrieval.

## Method Summary
GR-CLIP is a post-hoc calibration method that removes the modality gap in contrastive vision-language models by mean-centering embeddings per modality. The method computes global mean vectors for query text, document text, and document images using a calibration set (typically ~10,000 samples from training splits). During inference, these means are subtracted from raw embeddings, centering both modalities at the origin. For multimodal documents, centered embeddings are fused via linear interpolation. This simple correction aligns the embedding distributions, making cross-modal distances meaningful and enabling effective fusion of multimodal representations.

## Key Results
- GR-CLIP improves NDCG@10 by up to 26 percentage points over CLIP on MixBench.
- Outperforms VLM2Vec by 4 percentage points while using 75x less compute.
- Mean-centering successfully removes the modality gap, enabling effective fusion of image and text embeddings.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The modality gap in CLIP-style models causes intra-modal ranking bias in mixed-modality retrieval.
- Mechanism: Image and text embeddings form distinct, distant clusters in the shared space. This leads to systematically higher cosine similarity within modalities than across them. When a corpus contains mixed modalities, documents sharing the query's modality are artificially ranked higher than semantically relevant cross-modal documents.
- Core assumption: Retrieval rankings should reflect semantic similarity independent of modality; the gap introduces a spurious modality-based distortion.
- Evidence anchors:
  - [abstract] "pronounced modality gap... leading to intra-modal ranking bias"
  - [Page 2, Fig 1c,d] UMAP visualization shows separate clusters; similarity table shows higher intra-modal scores.
  - [corpus] Neighbor papers "Closing the Modality Gap Aligns Group-Wise Semantics" and "Mind the Gap" discuss related gap-driven misalignments in VLM spaces.
- Break condition: If a model's embedding space shows no measurable modality gap (e.g., overlapping distributions), this mechanism would not apply.

### Mechanism 2
- Claim: The modality gap prevents effective fusion of multi-modal document representations.
- Mechanism: Linear interpolation of image and text embeddings (e.g., α·text + (1-α)·image) places the fused vector in a sparsely populated region between the two clusters. This "no man's land" has weaker alignment with either modality's semantics, causing fused representations to underperform unimodal ones.
- Core assumption: Semantic meaning is preserved near the original cluster regions; the gap region is semantically impoverished.
- Evidence anchors:
  - [Page 7, §4.2] "fusion... fails to outperform these unimodal baselines... pushes the fused features into a suboptimal region"
  - [Page 7, Fig 3b] Performance curves for original CLIP peak at α=0 or α=1 (unimodal), not at intermediate fusion points.
  - [corpus] Neighbor "Post-pre-training for Modality Alignment" similarly identifies the gap as hindering effective joint representations.
- Break condition: If interpolation paths between modalities pass through semantically dense regions, fusion could succeed without gap removal.

### Mechanism 3
- Claim: Mean-centering embeddings per modality (GR-CLIP) removes the modality gap and mitigates both ranking bias and fusion failure.
- Mechanism: The gap is approximately a constant offset vector orthogonal to the semantic subspace. Subtracting the modality-specific mean (computed from a calibration set) from each embedding cancels this offset. This centers both modalities at the origin, aligning their distributions and making cross-modal distances meaningful.
- Core assumption: The gap can be approximated as a stable, dataset-independent mean difference that generalizes across queries and documents.
- Evidence anchors:
  - [Page 3, §2.3] "e′T_i − e′I_i ≈ c⊥ − c⊥ = 0"
  - [Page 15, Algorithm 1] Detailed pseudo-code for computing means from calibration sets and applying correction.
  - [corpus] Neighbor "Mind the Gap" discusses compensating for the gap in continual learning; GR-CLIP provides a direct removal method.
- Break condition: If the gap is not approximately constant or varies significantly across domains, a single global mean correction may not suffice.

## Foundational Learning
- Concept: Contrastive Learning (e.g., CLIP)
  - Why needed here: GR-CLIP is a post-hoc correction for contrastive vision-language models. Understanding the contrastive training objective (pulling paired image-text together, pushing non-pairs apart) explains why the gap emerges despite this alignment goal.
  - Quick check question: Can you explain why a model trained to align paired images and text might still exhibit separate embedding clusters for each modality?
- Concept: Embedding Space Geometry
  - Why needed here: The paper relies on concepts like mean vectors, linear interpolation, and cosine similarity. Understanding that embeddings are points in a high-dimensional space where distance reflects (dis)similarity is essential.
  - Quick check question: If two vectors have a cosine similarity of 0.9, what does that imply about their angle and relative direction?
- Concept: Retrieval Evaluation (NDCG@k)
  - Why needed here: Performance is reported using NDCG@10. Knowing that this metric accounts for both the relevance and rank position of retrieved items is crucial for interpreting the 26 percentage point improvement claim.
  - Quick check question: Why is NDCG often preferred over simple precision or recall for evaluating ranked retrieval results?

## Architecture Onboarding
- Component map: Modality-specific encoders (f^I, f^T) -> Raw embeddings -> Mean-subtraction (per modality) -> (Optional) Fusion (α·text + (1-α)·image) -> Cosine similarity search
- Critical path: The accuracy of the calibration means is paramount. They must be computed from a representative sample of the target domain (or a mix of domains) and stored securely. A single pass over a calibration dataset (e.g., 10k samples) is required once. All subsequent inference applies the fixed means.
- Design tradeoffs:
  - **Calibration set size/composition**: Larger, more diverse sets yield more stable means but require more upfront computation. The paper uses mixed-domain sets to encourage generalization.
  - **Per-domain vs. global means**: The paper uses global means for most datasets but computes a specific mean for OVEN queries, trading some generality for accuracy on a specific distribution.
  - **Fusion weight (α)**: After gap removal, fusion is beneficial. α must be tuned; the paper shows optimal values often lie between 0 and 1.
- Failure signatures:
  - **U-shaped performance curve** when varying corpus modality mix (indicates gap-driven bias).
  - **Fusion underperformance** where combining modalities yields lower scores than using either alone.
  - **High sensitivity** to the calibration set; using non-representative means can degrade performance.
- First 3 experiments:
  1. **Validate the gap**: Run a CLIP model on a mixed text-image corpus (e.g., MixBench subset). Visualize embeddings with UMAP/t-SNE and measure the average cosine similarity for intra-modal vs. cross-modal pairs.
  2. **Implement and test GR-CLIP**: Compute means from a small, diverse calibration set (e.g., 1000 samples each of text, images). Apply the correction and measure NDCG@10 on the same corpus. Expect a flattened U-curve and improved scores.
  3. **Ablate on calibration**: Test performance when using (a) domain-matched vs. mismatched calibration means, and (b) very small (e.g., 100 samples) vs. recommended (10k samples) calibration sets. This reveals sensitivity and informs minimum data requirements.

## Open Questions the Paper Calls Out
- **Open Question 1**: Do generative vision-language models (VLMs) exhibit a modality gap similar to contrastive models, and can post-hoc calibration effectively mitigate it?
  - Basis in paper: [explicit] The authors state in the Limitations section: "investigating the causes of the modality gap in generative embedding models such as VLM2Vec and developing methods to reduce it presents an important and underexplored research direction."
  - Why unresolved: The paper focuses on contrastive models (CLIP, SigLIP) and compares them against VLM2Vec as a baseline, but does not analyze the embedding geometry of the generative models themselves.
  - What evidence would resolve it: An analysis of the embedding space of VLMs (e.g., VLM2Vec, LLaVA) to identify modality clustering, followed by experiments applying mean-centering or similar calibrations to these models.

- **Open Question 2**: Does GR-CLIP improve performance on documents with complex, interleaved multi-image and multi-text structures (e.g., web pages or articles)?
  - Basis in paper: [explicit] The authors note in the Limitations section: "Extending the evaluation to more complex, interleaved multi-image and multi-text documents—such as web pages or scientific articles—could provide a more rigorous and comprehensive assessment."
  - Why unresolved: The current evaluation (MixBench) restricts documents to a single image and a single text segment, leaving the model's efficacy on more realistic, dense document formats untested.
  - What evidence would resolve it: Evaluation results on a benchmark specifically designed for interleaved documents (e.g., Wiki-SS or a newly constructed multi-image dataset) comparing GR-CLIP against baseline retrieval methods.

- **Open Question 3**: Can the modality gap be minimized during pre-training rather than through post-hoc calibration?
  - Basis in paper: [inferred] While the paper cites prior work suggesting the gap arises from initialization and optimization, the proposed solution (GR-CLIP) is strictly post-hoc. It remains unclear if the gap can be trained away without a separate calibration step.
  - Why unresolved: The paper demonstrates that the gap is fixable after training but does not explore modifications to the contrastive learning objective to prevent the gap from forming in the first place.
  - What evidence would resolve it: A comparative study of CLIP models trained with gap-regularization terms (e.g., penalizing mean separation) versus standard CLIP models with post-hoc GR-CLIP calibration.

## Limitations
- The claim that a single global mean vector can universally close the modality gap across diverse domains has limited validation.
- The calibration process requires a representative sample of the target domain, which may not always be available in practice.
- The choice of fusion weight α is dataset-dependent and requires additional tuning, adding complexity to deployment.

## Confidence
- **High Confidence**: The empirical evidence for NDCG@10 improvements (26 percentage points over CLIP, 4 points over VLM2Vec) is robust and directly measured on the MixBench benchmark. The mechanism of mean-centering removing a constant offset is mathematically sound.
- **Medium Confidence**: The claim that the modality gap is a "stable, dataset-independent mean difference" is plausible given the evidence but not exhaustively validated across all possible domains and data distributions.
- **Medium Confidence**: The assumption that the gap is approximately orthogonal to the semantic subspace is reasonable but not directly proven; alternative gap structures could exist.

## Next Checks
1. **Cross-Domain Generalization**: Test GR-CLIP on a completely new, held-out multimodal corpus (e.g., Flickr30k Entities or a custom dataset) to verify the global mean calibration generalizes beyond MixBench.
2. **Ablation on Calibration Data**: Systematically vary the size and domain composition of the calibration set (e.g., use only text, only images, or mismatched domains) to quantify the impact on performance and identify minimum requirements.
3. **Gap Structure Analysis**: Visualize and measure the modality gap (e.g., via t-SNE/UMAP plots and mean distance calculations) before and after GR-CLIP on multiple datasets to confirm the gap is consistently a constant offset and not a more complex structure.