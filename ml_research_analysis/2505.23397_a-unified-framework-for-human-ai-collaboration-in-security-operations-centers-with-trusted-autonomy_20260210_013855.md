---
ver: rpa2
title: A Unified Framework for Human AI Collaboration in Security Operations Centers
  with Trusted Autonomy
arxiv_id: '2505.23397'
source_url: https://arxiv.org/abs/2505.23397
tags:
- autonomy
- human
- security
- trust
- collaboration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy

## Quick Facts
- arXiv ID: 2505.23397
- Source URL: https://arxiv.org/abs/2505.23397
- Authors: Ahmad Mohsin; Helge Janicke; Ahmed Ibrahim; Iqbal H. Sarker; Seyit Camtepe
- Reference count: 40
- Key outcome: None explicitly stated in source

## Executive Summary
This paper proposes a unified framework for dynamic human-AI collaboration in Security Operations Centers (SOCs), addressing alert fatigue and the need for trusted autonomy. The framework introduces a mathematical model for scaling AI autonomy based on task complexity, operational risk, and calibrated trust, moving beyond static automation approaches. It was validated through a case study implementing "CyberAlly," a fine-tuned LLM with RAG capabilities, in a maritime port SOC simulation.

## Method Summary
The framework uses a mathematical autonomy scaling model where AI autonomy $A \in [0,1]$ is calculated as $A = 1 - (\lambda_1 C + \lambda_2 R)(1 - T)$, with $C$ representing task complexity, $R$ operational risk, and $T$ calibrated trust. The system operates across five autonomy levels, with AI handling routine tasks and humans providing oversight for complex or high-risk incidents. Implementation involved a fine-tuned LLM integrated with RAG and knowledge graphs, deployed via Slack interface to a simulated SOC environment.

## Key Results
- Reduces alert fatigue through dynamic autonomy scaling
- Achieves 50% reduction in false positives
- Improves Mean Time to Respond (MTTR) from 8 hours to 90 minutes

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Autonomy Scaling via Risk-Trust Modulation
The framework improves operational efficiency by dynamically scaling AI autonomy based on real-time assessments of task complexity and risk, modulated by calibrated trust levels. The autonomy scalar $A = 1 - (\lambda_1 C + \lambda_2 R)(1 - T)$ ensures higher autonomy for low-risk tasks when trust is high, while forcing human intervention for complex/high-risk scenarios. The system fails if risk/complexity inputs are noisy or manipulated, leading to inappropriate autonomy levels.

### Mechanism 2: Trust Calibration through Explainable Feedback Loops
Sustainable collaboration requires calibrated trust achieved by tying trust levels to explainability and performance history. The model $T = \alpha_1 E + \alpha_2 P + \alpha_3 (1 - U)$ uses transparent justifications from the AI to allow analysts to verify decisions, increasing trust over time. This transparency is crucial for advancing from Level 1 (Decision Support) to higher autonomy levels, but fails if explanations are hallucinated or unhelpful.

### Mechanism 3: Cognitive Load Reduction via Tiered Filtering
Alert fatigue is mitigated by offloading routine correlation tasks to AI, reserving human cognition for intricate threats. The system operates at Level 1-2 autonomy to correlate alerts, filter benign noise, and enrich context via Knowledge Graphs/RAG, achieving a 50% reduction in false positives. The mechanism fails if filtering is too aggressive, causing false negatives that blind analysts to novel threats.

## Foundational Learning

- **Concept: Human-in-the-Loop (HITL) vs. Human-on-the-Loop (HOtL)**
  - Why needed: The framework relies on distinguishing between active control (HITL, Level 1) and passive supervision (HOtL, Level 3). Confusing these leads to implementation gaps.
  - Quick check: Does the proposed workflow require the human to *approve* the action (HITL) or merely *monitor* the execution (HOtL)?

- **Concept: Alert Fatigue & False Positives**
  - Why needed: This is the primary problem state the framework attempts to resolve. Understanding that high volume leads to analyst burnout explains why the "Autonomy Scaling" mechanism is necessary.
  - Quick check: If the AI reduces false positives by 50%, does the remaining 50% require human validation, or is it auto-triaged?

- **Concept: RAG (Retrieval-Augmented Generation)**
  - Why needed: The "CyberAlly" case study uses a fine-tuned LLM with RAG. Understanding RAG is critical to grasping how the AI provides context-specific, non-hallucinated answers.
  - Quick check: How does the AI retrieve up-to-date threat intelligence (e.g., MITRE ATT&CK) to support its alert classification?

## Architecture Onboarding

- **Component map:** SIEM/EDR -> AI Core (LLM + RAG + Knowledge Graphs) -> Interface (Chat/Slack) -> Execution (SOAR) -> Feedback (Case Management)

- **Critical path:**
  1. SIEM/EDR generates raw alert
  2. CyberAlly correlates log, queries Knowledge Graph, classifies severity (Level 1-2 Autonomy)
  3. If low trust/high risk -> Human validates via Chat. If high trust/low risk -> AI proposes action
  4. SOAR triggers containment (e.g., endpoint isolation)
  5. Outcome logged to Knowledge Base for future RAG retrieval

- **Design tradeoffs:**
  - Speed vs. Accountability: Moving from HITL to HOtL increases response speed (MTTR) but risks automation bias
  - Specificity vs. Generality: Fine-tuning the LLM on specific organizational logs improves accuracy but requires privacy-compliant data handling

- **Failure signatures:**
  - Automation Bias: Analysts blindly accepting AI recommendations without reading explanations
  - Model Drift: AI failing to detect new attack patterns because training data is stale
  - Loop Break: SOAR execution fails, but AI reports success to the analyst

- **First 3 experiments:**
  1. Shadow Mode (Level 0-1): Deploy CyberAlly to ingest alerts and *only* score/prioritize them without blocking actions
  2. Explainability A/B Testing: Present two groups of analysts with AI outputs—one with XAI evidence and one without
  3. Supervised Containment (Level 2): Allow AI to suggest containment actions for low-risk threats requiring human "One-Click" approval

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multi-agent workflows be modeled to optimize communication and reduce redundancy among collaborative AI agents in SOCs?
- Basis: The future work section states that research should "address agent communication, redundancy reduction, and modeling of multi-agent workflows" for next-generation SOCs
- Why unresolved: Current implementations focus on single AI assistants rather than coordinated teams of agents
- What evidence would resolve it: A functional prototype demonstrating coordinated multi-agent task execution with measurable reductions in redundant alerts or actions

### Open Question 2
- Question: Which specific AI reasoning interfaces (e.g., saliency maps, confidence scores) most effectively improve analyst trust calibration under high cognitive load?
- Basis: The paper notes that "developing AI reasoning interfaces... can improve analyst trust" and identifies translating model outputs into "cognitively usable insights" as an open challenge
- Why unresolved: While the framework mandates explainability, the specific interface modalities that optimize human trust versus over-reliance remain unidentified
- What evidence would resolve it: Comparative user studies measuring analyst decision accuracy and trust levels when using different explanation interfaces during simulated incidents

### Open Question 3
- Question: How can Reinforcement Learning (RL) and Markov Decision Processes (MDPs) be operationalized to dynamically adjust AI autonomy levels (0–4) based on real-time confidence?
- Basis: The authors suggest that "Reinforcement learning and Markov decision processes may further optimize task allocation" and dynamic autonomy scaling
- Why unresolved: The current framework uses a structured mapping for autonomy; however, the algorithmic mechanism for real-time, dynamic shifting between these levels is not defined
- What evidence would resolve it: An adaptive control algorithm that successfully modulates autonomy levels in response to changing incident complexity and model uncertainty

### Open Question 4
- Question: To what extent does the framework generalize to real-world SOCs with heterogeneous legacy infrastructure compared to the simulated cyber range?
- Basis: The limitations section highlights that "real-world deployments must account for infrastructure diversity... and legacy tool ecosystems" which presents integration challenges
- Why unresolved: The framework was validated primarily in a simulated ACDC cyber range, leaving its robustness in diverse, resource-constrained production environments unproven
- What evidence would resolve it: Successful deployment metrics (MTTR, false positives) from active, non-simulated SOCs with varying legacy technology stacks

## Limitations
- Effectiveness depends on reliable quantification of risk and complexity metrics, which are not explicitly defined or validated
- Case study validation is limited to a single maritime port simulation over six months, lacking broader operational deployment data
- Performance improvements (50% false positive reduction, 67% faster investigations) are based on a single case study without independent verification

## Confidence

- **High Confidence:** The conceptual framework for dynamic autonomy scaling and the importance of calibrated trust in SOC environments is well-grounded in existing literature
- **Medium Confidence:** The specific implementation details appear sound but lack empirical validation across multiple real-world deployments
- **Low Confidence:** The claimed performance improvements are based on a single case study without independent verification or comparison to baseline performance metrics

## Next Checks

1. **Risk Metric Validation:** Implement a controlled experiment where multiple analysts independently score the same set of alerts for risk and complexity to establish inter-rater reliability

2. **Autonomy Model Stress Test:** Deploy the framework in a simulated environment with adversarial input designed to manipulate risk scores, measuring whether the autonomy scaling mechanism correctly maintains appropriate human oversight levels

3. **Cross-Environment Generalization:** Test the framework's autonomy scaling and trust calibration mechanisms across at least three distinct SOC environments (enterprise, government, critical infrastructure) to validate the generalizability of the proposed mathematical relationships