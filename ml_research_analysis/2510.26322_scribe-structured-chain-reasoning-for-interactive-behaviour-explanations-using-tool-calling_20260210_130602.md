---
ver: rpa2
title: 'SCRIBE: Structured Chain Reasoning for Interactive Behaviour Explanations
  using Tool Calling'
arxiv_id: '2510.26322'
source_url: https://arxiv.org/abs/2510.26322
tags:
- tool
- student
- questions
- feedback
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# SCRIBE: Structured Chain Reasoning for Interactive Behaviour Explanations using Tool Calling

## Quick Facts
- arXiv ID: 2510.26322
- Source URL: https://arxiv.org/abs/2510.26322
- Authors: Fares Fawzi; Vinitra Swamy; Dominik Glandorf; Tanya Nazaretsky; Tanja Käser
- Reference count: 39
- Primary result: Small LLMs (3B-8B) with structured tool-calling can match 70B models on relevance and actionability for student feedback explanations

## Executive Summary
SCRIBE introduces a framework for training small language models to perform multi-hop tool reasoning for interactive student feedback explanations. The system uses a two-stage LoRA fine-tuning approach to progressively teach models how to initiate and then iteratively refine tool-augmented reasoning. Through synthetic data distillation and closed-loop self-reflection, SCRIBE demonstrates that 8B models can achieve performance comparable to 70B models on relevance and actionability metrics while maintaining computational efficiency.

## Method Summary
SCRIBE employs two-stage LoRA fine-tuning on small LLMs (3B-8B) to learn multi-hop tool reasoning for student feedback explanations. Stage 1 trains initial reasoning and first tool call, while Stage 2 handles iterative refinement and final answer generation. The framework uses synthetic data generated by GPT-4o and filtered by GPT-4.1 judge, combined with six domain-specific tools for educational feedback analysis. Closed-loop inference with self-reflection enables error recovery when tool calls fail or produce unexpected outputs.

## Key Results
- 8B models match 70B models on relevance and actionability metrics for student feedback explanations
- Two-stage LoRA consistently outperforms single-stage training across all evaluation criteria
- Synthetic data generated by GPT-4o closely matches real student questions in distribution (JSD < 0.387)
- Closed-loop inference with self-reflection improves robustness to tool call errors and instruction violations

## Why This Works (Mechanism)

### Mechanism 1: Progressive Curriculum via Two-Stage LoRA Fine-Tuning
Sequential fine-tuning improves multi-hop tool reasoning in small models by first teaching initiation (reasoning r₀ + tool call t₀) before complex trajectories. This decomposition prevents overwhelming smaller models with multi-step reasoning before mastering basic tool calls.

### Mechanism 2: Closed-Loop Self-Reflective Inference with Error Recovery
Self-reflection loops improve robustness when tool calls fail by monitoring for errors and re-prompting the model to revise reasoning or tool choice. This continues until a valid answer or step limit is reached.

### Mechanism 3: Synthetic Data Distillation with Quality Filtering
GPT-4o-generated reasoning traces, filtered by GPT-4.1-as-judge, provide sufficient supervision for small models. Synthetic trajectories that pass judge filtering represent valid, transferable reasoning patterns for pedagogical applications.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**
  - Why needed: Enables efficient fine-tuning of 3B–8B models without full parameter updates. Used rank 256 for 8B models and rank 128 for 3B models.
  - Quick check: What happens if LoRA rank is too low for complex multi-hop tasks? (Answer: Lower ranks underperform on actionability and correctness.)

- **Tool-Augmented / Function-Calling LLMs**
  - Why needed: SCRIBE's core contribution is training small models to call external tools rather than relying solely on parametric knowledge.
  - Quick check: Why can't the model just answer directly from the feedback report without tools? (Answer: Some questions require computation, retrieval, or aggregation beyond the context window.)

- **GPT-as-Judge Evaluation**
  - Why needed: Multi-hop reasoning with multiple valid paths defies simple accuracy metrics. Uses GPT-4.1 with rubric aligned to human annotations (κ=0.818).
  - Quick check: What validation ensures the judge is trustworthy? (Answer: Cohen's κ=0.85 between human annotators, κ=0.818±0.014 between GPT-4.1 and humans.)

## Architecture Onboarding

- **Component map:** Real student questions → Expert annotation → GPT-4o synthetic generation → GPT-4.1 filtering → Training dataset → Two-stage LoRA → Closed-loop inference with self-reflection → GPT-Judge evaluation + User study

- **Critical path:**
  1. Domain-specific tool definitions (6 tools: textbook search, syllabus, topic dependency, grade calculator, feature sort, behavior impact)
  2. Synthetic reasoning traces with tool calls filtered for quality
  3. Two-stage LoRA training (6 A100 GPU hours per stage)
  4. Closed-loop inference with error recovery prompting

- **Design tradeoffs:**
  - Model size vs. capability: 8B models match 70B on relevance/actionability but lag on tool relevance and correctness
  - LoRA rank: Rank 256 best for actionability/correctness; rank 32 better for tool relevance
  - Tool set scope: All 6+ tools invoked, but `map_week_to_topic` (28.84%) and `impact_of_student_behaviors` (26.22%) dominate usage

- **Failure signatures:**
  - High tool-call errors without successful self-correction (inference loop exhausts N steps)
  - Correctness drops when models extrapolate beyond tool outputs
  - Generalization degradation on unseen tools

- **First 3 experiments:**
  1. Reproduce synthetic data quality metrics: Compute JSD and cosine similarity between synthetic and real questions
  2. Ablate LoRA stages: Compare single-stage vs. two-stage LoRA on 192-question test set
  3. Test generalization to unseen tool: Introduce new tool at inference time and measure invocation rate and response quality

## Open Questions the Paper Calls Out
- Does interaction with the SCRIBE system influence actual student learning and performance outcomes? (Section 6 explicitly states they did not evaluate educational outcomes)
- Can the SCRIBE framework be effectively extended to high-stakes domains like medical or psychiatric diagnosis? (Section 5 identifies this as future work)
- How can correctness and tool relevance of small SCRIBE models be improved to close the performance gap with larger models? (Section 5 and 6 highlight limited gains in correctness)

## Limitations
- Tool implementation specifics (CEM explainer integration, embedding index structures) are not fully specified
- Domain transferability to feedback systems outside iLLuMinaTE framework remains unclear
- Evaluation judge reliability depends on rubric alignment with true pedagogical value

## Confidence
- High Confidence: Two-stage LoRA curriculum improves multi-hop reasoning in small models
- Medium Confidence: Closed-loop self-reflection improves robustness
- Low Confidence: Synthetic data distillation alone provides sufficient supervision for tool-augmented pedagogical reasoning

## Next Checks
1. Test generalization to unseen tools: Introduce a novel tool during inference and measure both invocation rate and response quality
2. Ablate LoRA stage dependency: Compare single-stage vs. two-stage LoRA on the 192-question test set
3. Validate synthetic data fidelity: Compute JSD and cosine similarity between synthetic and held-out real questions