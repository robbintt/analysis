---
ver: rpa2
title: 'SLAM: Towards Efficient Multilingual Reasoning via Selective Language Alignment'
arxiv_id: '2501.03681'
source_url: https://arxiv.org/abs/2501.03681
tags:
- multilingual
- reasoning
- training
- layers
- slam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SLAM addresses the challenge of efficient multilingual reasoning
  in large language models by selectively fine-tuning only the layers responsible
  for language-specific comprehension. Instead of full-parameter fine-tuning across
  all layers, SLAM identifies and trains just the lower-level feed-forward network
  (FFN) sub-layers that handle multilingual understanding, while freezing higher layers
  that preserve reasoning abilities.
---

# SLAM: Towards Efficient Multilingual Reasoning via Selective Language Alignment

## Quick Facts
- **arXiv ID**: 2501.03681
- **Source URL**: https://arxiv.org/abs/2501.03681
- **Authors**: Yuchun Fan; Yongyu Mu; Yilin Wang; Lei Huang; Junhao Ruan; Bei Li; Tong Xiao; Shujian Huang; Xiaocheng Feng; Jingbo Zhu
- **Reference count**: 40
- **Primary result**: Achieves 49.6% average accuracy across 10 languages on multilingual mathematical reasoning using only 6.5-8% of total parameters

## Executive Summary
SLAM addresses the challenge of efficient multilingual reasoning in large language models by selectively fine-tuning only the layers responsible for language-specific comprehension. Instead of full-parameter fine-tuning across all layers, SLAM identifies and trains just the lower-level feed-forward network (FFN) sub-layers that handle multilingual understanding, while freezing higher layers that preserve reasoning abilities. This approach significantly reduces computational cost and prevents catastrophic forgetting. The method achieves superior average accuracy compared to strong baselines across 10 languages on both in-domain and out-of-domain multilingual mathematical reasoning tasks, using only 6.5-8% of total parameters.

## Method Summary
SLAM employs a layer-selective fine-tuning approach that identifies language-specific layers through neuron activation analysis across multiple languages. The method computes Mean Squared Deviation (MSD) of neuron activation rates to determine which layers handle language-specific processing versus universal reasoning. Only the FFN sub-layers within these identified language-specific layers are fine-tuned using translation data, while attention mechanisms and higher reasoning layers remain frozen. This selective training injects multilingual comprehension capabilities without disrupting the model's existing reasoning abilities, requiring only one training stage compared to two-stage methods that first align languages then restore reasoning.

## Key Results
- Achieves 49.6% average accuracy on 10 languages using 7B models (58.3% for 13B models)
- Reduces training time by 4.1-11.9× compared to two-stage fine-tuning methods
- Uses only 6.5-8% of total parameters for fine-tuning
- Outperforms QAlign and ELO baselines on both in-domain and out-of-domain multilingual mathematical reasoning tasks
- Shows substantial improvements particularly for low-resource languages like Bengali and Thai

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Selectively fine-tuning only lower-level FFN sub-layers achieves multilingual reasoning alignment while preserving reasoning capabilities.
- Mechanism: The paper demonstrates that language-specific representation learning occurs primarily in lower layers (layers 1-6 in their experiments), while higher layers transform these into universal representations. By measuring Mean Squared Deviation (MSD) of neuron activations across languages, SLAM identifies layers with high multilingual engagement. Training only these layers with translation data aligns non-English inputs to the model's existing English reasoning pathways without disrupting higher-level reasoning circuits.
- Core assumption: Language comprehension and reasoning are functionally separable across layer depth, and alignment can occur at the comprehension stage without touching reasoning circuits.
- Evidence anchors: [abstract] "representation learning of languages is merely conducted in lower-level layers"; [section 3.1] Figures 2-3 show neuron activation overlap between non-English and English increases and stabilizes at higher layers; [corpus] LinguaMap (arXiv 2601.20009) similarly identifies layer-specific language processing.

### Mechanism 2
- Claim: Training only FFN sub-layers (not attention) within selected layers is sufficient and optimal for multilingual knowledge injection.
- Mechanism: FFN sub-layers function as key-value memories that store factual and linguistic knowledge. By training only FFN weights with X-English translation pairs, SLAM directly injects language-specific knowledge without modifying attention patterns that may encode reasoning-relevant relationships. The ablation (Figure 5) confirms FFN-only training outperforms attention-only or combined training.
- Core assumption: Multilingual comprehension primarily requires knowledge storage updates rather than attention pattern modifications.
- Evidence anchors: [abstract] "SLAM identifies and trains just the lower-level feed-forward network (FFN) sub-layers that handle multilingual understanding"; [section 5.2] Figure 5 shows FFN-only training achieves highest accuracy (49.6% avg) vs. attention (27.6%) or combined (32.0%) on MGSM.

### Mechanism 3
- Claim: Single-stage training with frozen higher layers prevents catastrophic forgetting while achieving alignment.
- Mechanism: Two-stage methods first train all parameters on translation data, which corrupts reasoning weights, then require a second stage to restore reasoning. SLAM freezes reasoning-relevant layers from the start. Since only 6.5-8% of parameters are modified, the model's pretrained reasoning capabilities remain intact, eliminating the need for a recovery stage. Training time reduced by 4.1-11.9×.
- Core assumption: The base model already possesses sufficient reasoning capabilities that can be accessed once language comprehension is aligned.
- Evidence anchors: [abstract] "SLAM only involves one training stage, reducing training time by 4.1-11.9 compared to the two-stage method"; [section 1] Full-parameter training in stage one "can lead to catastrophic forgetting, destroying the inherent reasoning abilities."

## Foundational Learning

- **Feed-Forward Networks (FFN) as Key-Value Memories**
  - Why needed here: SLAM's design relies on understanding that FFN layers store knowledge that can be selectively updated. Without this, the choice to train only FFN sub-layers seems arbitrary.
  - Quick check question: Can you explain why updating FFN weights might change factual/linguistic knowledge without affecting attention-based relational reasoning?

- **Neuron Activation Analysis**
  - Why needed here: SLAM uses neuron activation patterns to identify language-specific layers via MSD scoring. Understanding how to measure and interpret activation overlap is essential for reproducing layer selection.
  - Quick check question: Given activation vectors for 10 languages at layer i, how would you compute whether this layer is "language-specific" vs. "language-universal"?

- **Catastrophic Forgetting in Continual Learning**
  - Why needed here: The paper frames its contribution partly as solving catastrophic forgetting caused by full-parameter fine-tuning. Understanding this phenomenon clarifies why selective training matters.
  - Quick check question: Why might updating all parameters on translation data degrade a model's ability to solve math problems in English?

## Architecture Onboarding

- **Component map**: Base Model (Llama-2 7B/13B) -> Layer Selector (computes MSD) -> FFN Sub-layers (trainable) -> Training Data (X-English translation pairs) -> Cross-entropy loss

- **Critical path**:
  1. Sample n questions in 10 languages; compute normalized neuron activation rates R_i^lang per layer (Equations 3-4)
  2. Identify language-specific layers where non-English/English activation overlap hasn't reached maximum (layers before plateau in Figure 3)
  3. Compute MSD per layer; select layers with MSD > θ (Equations 5-7)
  4. Freeze all parameters except FFN weights in selected layers
  5. Train on translation data for 4 epochs, lr=2e-5, batch size 512

- **Design tradeoffs**:
  - Fewer layers (e.g., 4): Faster training, but may under-align low-resource languages (Bengali, Thai, Swahili show highest variance)
  - More layers (e.g., 8+): Better comprehension but risks encroaching on reasoning layers, degrading accuracy
  - FFN vs. FFN+Attention: FFN-only is more parameter-efficient and empirically superior (Figure 5), but combined training might help for tasks where attention patterns encode language-specific parsing

- **Failure signatures**:
  - Low accuracy on low-resource languages only: Layer selection threshold θ too high; fewer layers selected than needed
  - English accuracy drops after training: Selected layers include reasoning-relevant layers; reduce layer count
  - High training loss, no convergence: Translation data quality issues (repeated translations, hallucinations); apply stricter filtering
  - Good in-domain (MGSM) but poor out-of-domain (MSVAMP): Overfitting to translation data distribution; consider early stopping or data augmentation

- **First 3 experiments**:
  1. Layer ablation: Train with layers 1-2, 1-4, 1-6, 1-8 on MGSM8K validation; plot accuracy vs. layer count to verify 6 layers is optimal for your base model
  2. Sub-layer ablation: Compare FFN-only vs. attention-only vs. FFN+Attention on 3 representative languages (1 high-resource: German, 1 mid: Chinese, 1 low: Bengali) to confirm FFN superiority
  3. Base model comparison: Apply SLAM to both MetaMath-7B and a weaker base (e.g., Llama-2-7B without math fine-tuning) to quantify how much alignment vs. base reasoning contributes to final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SLAM maintain its efficiency and performance gains when applied to model families beyond Llama 2?
- Basis in paper: [explicit] The authors state in the Limitations section: "Future work will involve extending our experiments to additional series models to more comprehensively evaluate the generalizability."
- Why unresolved: The experiments were restricted to the Llama 2 series (7B/13B) to ensure fair comparisons with specific baselines like MetaMath.
- What evidence would resolve it: Successful application of the SLAM methodology to diverse architectures (e.g., Mistral, Qwen) demonstrating similar parameter efficiency and accuracy improvements.

### Open Question 2
- Question: Does balancing the training data distribution across languages mitigate the observed performance trade-offs between specific languages?
- Basis in paper: [explicit] The authors note in the Limitations that "degrees of alignment across different languages result in performance trade-offs" and hypothesize this "may be due to the imbalanced data among languages."
- Why unresolved: The current study used existing translation datasets which naturally contain imbalances, and the authors did not test controlled data balancing strategies.
- What evidence would resolve it: Experiments comparing SLAM performance on imbalanced vs. carefully balanced multilingual translation datasets.

### Open Question 3
- Question: Does the optimal number of layers to fine-tune scale with model depth, or is it determined primarily by the linguistic complexity of the task?
- Basis in paper: [inferred] The study fixes the trainable layers to the first 6 for both 7B and 13B models, but it is unclear if this number should increase for significantly deeper models (e.g., 70B+).
- Why unresolved: The layer selection metric (MSD) is relative, but the implementation suggests a fixed count may have been settled upon for this specific architecture scale.
- What evidence would resolve it: An ablation study on models with varying depths (e.g., 30 layers vs. 80 layers) analyzing the correlation between optimal layer count and total model depth.

## Limitations

- The layer selection heuristic (MSD threshold θ) may not transfer across model architectures beyond Llama-2, limiting generalizability claims.
- The method assumes high-quality translation data; poor translation quality (hallucinations, errors) could degrade FFN memory without detection.
- The approach assumes the base model has sufficient reasoning capabilities that can be unlocked through alignment, which may fail for models with weak inherent reasoning.

## Confidence

**High Confidence**: The core claim that selective fine-tuning of FFN sub-layers achieves multilingual alignment while preserving reasoning capabilities is well-supported by ablation studies and cross-lingual performance metrics. The layer selection mechanism based on neuron activation overlap is methodologically sound and aligns with established findings about transformer layer specialization.

**Medium Confidence**: The claim of 4.1-11.9× training time reduction relative to two-stage methods assumes comparable computational efficiency in both approaches. While parameter counts support the efficiency claim, actual wall-clock time depends on implementation details, batch sizes, and hardware specifics not fully disclosed.

**Low Confidence**: The "1000×" parameter efficiency claim relative to full fine-tuning is misleading—it compares SLAM's trainable parameters (6.5-8%) to total parameters, but doesn't account for the fact that translation data still requires processing through all model parameters during inference. The actual training efficiency gain is the 4.1-11.9× figure, not 1000×.

## Next Checks

1. **Architecture Transfer Test**: Apply SLAM to a different transformer architecture (e.g., Mistral-7B or OPT-7B) using the same layer selection methodology. Measure whether MSD-based layer selection identifies the same layers and whether accuracy/performance transfer matches Llama-2 results. This validates whether the layer separation assumption holds across architectures.

2. **Translation Quality Stress Test**: Intentionally corrupt 20-30% of translation data with known hallucinations or errors. Train SLAM with corrupted data and measure: (a) impact on target language accuracy, (b) whether English reasoning accuracy degrades (indicating knowledge contamination), and (c) whether training loss patterns detect data quality issues. This validates the assumption that translation quality doesn't critically impact alignment.

3. **Base Model Reasoning Capacity Test**: Apply SLAM to a base model with known weak reasoning capabilities (e.g., standard Llama-2-7B without math fine-tuning) and compare results to the MetaMath-enhanced variant used in the paper. This tests whether alignment alone can unlock reasoning or whether strong base reasoning is a prerequisite for SLAM's success.