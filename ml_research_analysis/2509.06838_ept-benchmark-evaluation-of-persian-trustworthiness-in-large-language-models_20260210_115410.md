---
ver: rpa2
title: 'EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models'
arxiv_id: '2509.06838'
source_url: https://arxiv.org/abs/2509.06838
tags:
- llms
- safety
- language
- large
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'EPT Benchmark evaluates Persian LLM trustworthiness across six
  dimensions: truthfulness, safety, fairness, robustness, privacy, and ethics. A curated
  dataset of 1,200 prompts grounded in Persian-Islamic values was assessed via automated
  LLM scoring and human expert review.'
---

# EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models

## Quick Facts
- arXiv ID: 2509.06838
- Source URL: https://arxiv.org/abs/2509.06838
- Authors: Mohammad Reza Mirbagheri; Mohammad Mahdi Mirkamali; Zahra Motoshaker Arani; Ali Javeri; Amir Mahdi Sadeghzadeh; Rasool Jalili
- Reference count: 40
- Primary result: Claude 3.7 Sonnet achieved highest average compliance rate (89.6%), while Qwen 3 lagged (70.4%), especially in safety (48.8%)

## Executive Summary
The EPT Benchmark introduces a comprehensive evaluation framework for assessing Persian LLM trustworthiness across six dimensions: truthfulness, safety, fairness, robustness, privacy, and ethics. A curated dataset of 1,200 prompts grounded in Persian-Islamic values was assessed through automated LLM scoring and human expert review. Claude 3.7 Sonnet achieved the highest average compliance rate (89.6%), while Qwen 3 lagged significantly (70.4%), with safety emerging as the weakest dimension across most models. This framework uniquely addresses the gap in non-Western LLM evaluation and provides actionable insights for advancing trustworthy and culturally responsible AI in Persian-speaking contexts.

## Method Summary
The EPT Benchmark employs a hybrid evaluation pipeline to assess eight LLMs on 1,200 Persian-centric prompts across six trustworthiness dimensions. The methodology consists of two stages: first, an automated evaluation using ChatGPT as an LLM-as-judge for answer similarity matching, and second, human expert review by native Persian speakers with majority-vote aggregation. Each dimension contains 200 prompts specifically designed to test cultural alignment with Persian-Islamic values. The evaluation produces binary compliance rates averaged across dimensions, with special attention to safety failures observed in models like Qwen 3 (48.8% compliance).

## Key Results
- Claude 3.7 Sonnet achieved the highest average compliance rate (89.6%)
- Qwen 3 showed the lowest performance (70.4%), particularly weak in safety dimension (48.8%)
- Safety emerged as the weakest dimension across most evaluated models
- Significant performance variance observed across models, with GPT-4o and Grok showing high standard deviations (>13)

## Why This Works (Mechanism)

### Mechanism 1: Cultural Context Injection via Adversarial Prompts
The benchmark reveals latent misalignments in general-purpose LLMs by using prompts specifically grounded in Persian-Islamic values. For example, scenarios involving riba (interest) under Islamic jurisprudence expose when models default to secular financial norms instead of culturally appropriate responses. This tests not just linguistic fluency but cultural fidelity, based on the assumption that training data for major LLMs is disproportionately Western/English.

### Mechanism 2: Dual-Stage Verification for Nuance Capture
A hybrid evaluation pipeline using both automated LLM-as-judge (ChatGPT) and human experts creates a more robust measure of compliance than either method alone. The automated stage provides efficiency through answer matching, while human native speakers capture cultural nuances through majority voting, filtering out rigid binary classifications while correcting for human inconsistency.

### Mechanism 3: Dimensional Decomposition of Trust
Decomposing "trustworthiness" into six independent axes (Safety, Ethics, Privacy, etc.) isolates specific capability gaps. By categorizing prompts into distinct dimensions like Jailbreak attacks for Safety and Hallucination for Truthfulness, the benchmark forces models to fail or succeed on specific skills, preventing high scores in one area from masking deficiencies in others.

## Foundational Learning

### Concept: RLHF and Cultural Alignment
Why needed here: The paper attributes performance differences to alignment techniques (like RLHF) which may be culturally biased. Understanding how human feedback guides model behavior is key to understanding why Claude might outperform Qwen in this specific context.
Quick check question: Can you explain why a model trained mostly on Western data via RLHF might fail a prompt regarding "Islamic financial ethics"?

### Concept: LLM-as-a-Judge
Why needed here: The evaluation framework relies on ChatGPT to grade other models.
Quick check question: What are the potential biases when using a proprietary model (like GPT-4) to evaluate open-source models or competitors?

### Concept: Adversarial Robustness
Why needed here: The "Safety" dimension explicitly tests "Jailbreak" and "Misuse".
Quick check question: How does a "prompt injection" or "jailbreak" differ from a standard query in terms of model processing?

## Architecture Onboarding

### Component map:
Curated Prompts (1,200) -> 8 LLMs (GPT-4o, Claude, DeepSeek, etc.) -> Evaluation Layer (Automated Scorer: ChatGPT + Human Expert Panel) -> Compliance Rate (%) per dimension

### Critical path:
The Prompt Curation phase is the bottleneck. Ensuring prompts genuinely reflect "Persian-Islamic values" rather than translated Western concepts requires domain expertise.

### Design tradeoffs:
- Automated vs. Human: Trade speed of full automation for accuracy of human review
- Breadth vs. Depth: 200 prompts per dimension provides statistical significance but may not cover every edge case
- Metric Binary: Binary "Compliant/Non-compliant" metric simplifies aggregation but loses granularity regarding why a response failed

### Failure signatures:
- Low Safety Scores: Observed in Qwen (48.8%). Look for models refusing to answer sensitive cultural questions or generating harmful content
- High Variance: Observed in GPT-4o and Grok (SD > 13). Indicates inconsistent cultural groundingâ€”strong in some areas (Robustness) but weak in others (Safety)

### First 3 experiments:
1. Baseline Evaluation: Run the EPT dataset against your target model. Calculate "Yes Percentage" for Safety dimension to establish baseline risk profile
2. Judge Consistency Check: Select 50 responses and have them evaluated by both automated LLM-judge and human reviewer. Calculate agreement rate to calibrate "Human-AI gap"
3. Ablation on Culture: Translate 50 EPT prompts to English and run against same model. Compare "Truthfulness" scores in Persian vs. English to isolate linguistic vs. cultural failure modes

## Open Questions the Paper Calls Out

### Open Question 1
What specific training data characteristics or architectural components drive the performance disparity between high-performing models (Claude 3.7 Sonnet) and low-performing models (Qwen 3) in Persian safety and fairness contexts?
Basis: The authors state, "Due to the lack of transparency in training data and methods, the reasons behind these performance variations across models remain unclear and require further investigation."
Why unresolved: Evaluation was conducted on "black-box" or API-access models where training corpora and alignment techniques are proprietary
What evidence would resolve it: Comparative analysis of open-weight models controlling for dataset composition and alignment strategies specifically on safety metrics

### Open Question 2
How can safety alignment be specifically improved for LLMs to address the 48.8% failure rate observed in models like Qwen 3 within the Persian-Islamic cultural context?
Basis: Results identify safety as weakest dimension across most models, with authors calling for "focused attention on this critical aspect"
Why unresolved: Paper functions as benchmarking tool rather than remediation study; does not test specific alignment techniques to fix observed safety failures
What evidence would resolve it: Intervention study where models are fine-tuned using EPT dataset or culturally specific safety data, followed by re-evaluation showing statistically significant increase in safety compliance rates

### Open Question 3
Does the inclusion of multimodal inputs (images and audio) degrade or alter the trustworthiness performance profiles established by the text-only EPT benchmark?
Basis: Authors state in Future Work: "First, we aim to expand the benchmark to encompass multimodal models, incorporating diverse inputs such as text, images, and audio"
Why unresolved: Current study is strictly limited to text-based prompts; does not account for cross-modal risks such as visual toxic content or audio-based jailbreaking
What evidence would resolve it: Extension of EPT dataset to include image-audio pairs, evaluated on multimodal models to compare compliance rates against text-only baseline

### Open Question 4
What iterative benchmarking protocols are necessary to detect dynamic risks, such as evolving adversarial attacks, that the static EPT dataset may miss?
Basis: Authors note the "rapid evolution of LLMs introduces dynamic risks" and propose "incorporating emerging threat models, such as adversarial attacks... to ensure the benchmark remains a reliable tool"
Why unresolved: Current EPT benchmark relies on static set of 1,200 curated prompts, which may not capture novel "jailbreak" techniques or misuse scenarios that emerge after dataset's creation
What evidence would resolve it: Longitudinal study applying EPT benchmark to same models over time, supplemented by "red-teaming" prompts generated dynamically by other LLMs to identify performance drift

## Limitations
- Cultural fidelity of prompts depends entirely on expert curation with limited detail on selection process and potential panel biases
- Binary "compliant/non-compliant" metric oversimplifies nuanced cultural judgments and may mask partial understanding
- Automated LLM-as-judge approach (ChatGPT) may itself lack sufficient Persian cultural grounding to serve as reliable baseline evaluator

## Confidence
- **High Confidence:** Safety consistently weakest dimension across models is well-supported by data and aligns with general LLM safety observations
- **Medium Confidence:** Relative performance ranking of models (Claude 3.7 > GPT-4o > DeepSeek > Qwen 3) is likely robust, though absolute scores depend on evaluation methodology
- **Low Confidence:** Specific cultural alignment claims (e.g., "Persian-Islamic values") cannot be independently verified without access to full prompt set and expected answer keys

## Next Checks
1. Reproduce Automated Scoring: Replicate ChatGPT evaluation pipeline with exact system prompt to assess whether automated judge introduces systematic bias in cultural contexts
2. Expert Review Replication: Conduct independent human expert review of 100 randomly selected responses to validate majority-vote aggregation methodology and inter-rater reliability
3. Cross-Cultural Comparison: Translate 50 prompts to English and evaluate same models to isolate whether performance gaps stem from linguistic or cultural misalignment