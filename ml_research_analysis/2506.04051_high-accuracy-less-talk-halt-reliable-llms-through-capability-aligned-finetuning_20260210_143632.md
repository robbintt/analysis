---
ver: rpa2
title: 'High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned
  Finetuning'
arxiv_id: '2506.04051'
source_url: https://arxiv.org/abs/2506.04051
tags:
- response
- fragments
- correctness
- responses
- halt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study introduces HALT, a method to reduce hallucination in\
  \ large language models by finetuning them to generate only content they are confident\
  \ about, partially abstaining otherwise. HALT creates capability-aligned finetuning\
  \ data by splitting model responses into factual fragments, evaluating their correctness\
  \ with a ground-truth-informed evaluator, and removing or marking incorrect fragments\
  \ as \u201CUnsure from here.\u201D It allows practitioners to adjust a confidence\
  \ threshold to balance response completeness and correctness."
---

# High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning

## Quick Facts
- arXiv ID: 2506.04051
- Source URL: https://arxiv.org/abs/2506.04051
- Reference count: 35
- Reduces LLM hallucinations by 15% on average, achieving 87% correctness while retaining 53% completeness

## Executive Summary
HALT addresses LLM hallucination by finetuning models to generate only content they are confident about, partially abstaining otherwise. The method creates capability-aligned training data by splitting responses into factual fragments, evaluating their correctness with a ground-truth-informed evaluator, and removing or marking incorrect fragments as "Unsure from here." This approach allows practitioners to adjust a confidence threshold to balance response completeness and correctness. Evaluated across four domains (biography writing, mathematics, coding, and medicine) and four open-source models, HALT significantly improves reliability without requiring test-time post-processing.

## Method Summary
HALT generates capability-aligned finetuning data by sampling multiple preliminary responses from a pretrained LLM, fragmenting them into factual components, and using a ground-truth-informed evaluator to assess correctness. For causally dependent fragments (like math or coding steps), the method replaces the first incorrect fragment and all subsequent ones with "Unsure from here." For independent fragments (like biography facts), incorrect fragments are removed entirely. The α-percentile response with the highest fragment correctness is selected for finetuning, allowing practitioners to tune the completeness-correctness trade-off. The model is then finetuned using LoRA on this filtered dataset, learning to recognize and signal its capability boundaries.

## Key Results
- Increases mean fragment correctness by 15% across all domains
- Improves F1 score (harmonic mean of completeness and correctness) by 4% compared to baselines
- Achieves 87% correctness across all domains while retaining 53% of response completeness
- Outperforms FactTune and IDK baselines in both completeness and correctness metrics

## Why This Works (Mechanism)

### Mechanism 1: Capability-Aligned Training Data Generation
HALT reduces hallucinations by finetuning LLMs only on content verifiably within their pretrained capabilities. The method generates preliminary responses, decomposes them into factual fragments, and removes incorrect fragments before finetuning. This relies on the assumption that LLMs don't acquire novel capabilities during finetuning but only learn to utilize existing pretraining knowledge more effectively.

### Mechanism 2: Threshold-Based Completeness-Correctness Trade-off
Practitioners can tune a confidence threshold by selecting preliminary responses with varying levels of fragment accuracy. By sampling multiple responses and choosing the α-percentile with highest correctness, HALT enables control over the completeness-correctness balance. Lower α values yield more conservative responses with higher correctness but lower completeness.

### Mechanism 3: Partial Abstention Training
HALT trains models to explicitly signal uncertainty via "Unsure from here" for partial responses. For causally dependent fragments, it replaces the first incorrect fragment and all subsequent ones. This approach improves reliability for complex tasks requiring long outputs without requiring test-time post-processing.

## Foundational Learning

**Concept: Finetuning vs. Pretraining Knowledge**
Why needed here: HALT relies on the principle that finetuning elicits existing capabilities rather than adding new knowledge. Understanding this distinction is crucial for interpreting why removing unknown content from finetuning data reduces hallucination.
Quick check question: If a model hallucinates a fact post-finetuning, was that fact likely present in its pretraining data?

**Concept: Response Fragmentation Types (Independent vs. Causally Dependent)**
Why needed here: HALT handles independent fragments (e.g., atomic facts in a biography) differently from causally dependent ones (e.g., steps in a math solution). This classification dictates error propagation and how incorrect fragments are treated.
Quick check question: In a step-by-step coding solution, should an error in line 3 affect the validity of line 5?

**Concept: F1 Score for Completeness-Correctness Trade-offs**
Why needed here: HALT evaluates performance using the F1 score (harmonic mean of completeness and correctness). Understanding this metric is essential for interpreting trade-off curves and comparing to baselines.
Quick check question: If a response is 100% correct but only covers 50% of the desired fragments, what is its F1 score?

## Architecture Onboarding

**Component map:**
Pretrained LLM -> Fragmentation Module -> Evaluator (Llama3-405B) -> HALT Response Composer -> Finetuning Loop

**Critical path:**
1. Sample N preliminary responses for each prompt in the finetuning dataset
2. Fragment each response using the domain-appropriate method
3. Query the evaluator with ground truth context for each fragment
4. Select the α-percentile response based on average fragment correctness
5. Compose the HALT response by removing/labeling incorrect fragments
6. Finetune the target model on the HALT dataset

**Design tradeoffs:**
- Higher α (e.g., 0.8): Prioritizes completeness; model generates more but risks more errors
- Lower α (e.g., 0.4): Prioritizes correctness; model generates less but with higher accuracy
- Fragmentation granularity: Finer fragments enable more precise correction but increase evaluator calls and potential error
- Evaluator choice: Larger evaluators (e.g., Llama3-405B) are more accurate but more expensive

**Failure signatures:**
- Near-zero completeness: α too low or evaluator overly harsh; model learns to almost always say "Unsure from here"
- No improvement over baseline: Evaluator mislabels fragments, or pretrained model is already well-aligned
- Domain mismatch: Applying independent-fragment logic to causally dependent tasks leads to incorrect HALT responses

**First 3 experiments:**
1. Validate the evaluator: Manually label 100-300 fragments across domains and compare to evaluator judgments
2. Run HALT with varying α: Train models with α ∈ {0.4, 0.6, 0.8} on a single domain and plot completeness vs. correctness curves
3. Compare to baselines: Implement FactTune and IDK baselines on the same domain and compare F1 scores to HALT

## Open Questions the Paper Calls Out
1. Can a single HALT model learn to dynamically adjust the trade-off between correctness and completeness based on a user-specified parameter?
2. Does establishing dependency graphs for fragments, rather than assuming linear causal dependence, improve performance in complex reasoning tasks?
3. How can HALT be integrated with external tool use to recover the completeness lost when the model abstains from generating uncertain content?

## Limitations
- Depends on reliable ground-truth-informed evaluator, which may introduce errors
- Requires domain-specific fragmentation strategies and evaluators for new applications
- Achieves high correctness at the cost of reduced completeness (53% vs 87%)
- Assumes LLMs don't acquire novel capabilities during finetuning, which may not hold in all cases

## Confidence
- High Confidence: Core mechanism of removing/labeling incorrect fragments during finetuning
- Medium Confidence: Assumption that LLMs don't acquire novel capabilities during finetuning
- Medium Confidence: Evaluator reliability claims based on manual validation of 100-300 fragments

## Next Checks
1. Manually validate evaluator accuracy on 200-500 additional fragments across all four domains, particularly focusing on edge cases
2. Apply HALT to a fifth domain (e.g., legal reasoning or historical analysis) with minimal architectural changes
3. Finetune a model with HALT, then continue training it on additional data without HALT filtering to measure long-term stability of correctness improvements