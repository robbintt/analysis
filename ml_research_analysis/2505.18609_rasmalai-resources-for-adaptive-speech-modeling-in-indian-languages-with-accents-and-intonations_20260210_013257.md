---
ver: rpa2
title: 'RASMALAI: Resources for Adaptive Speech Modeling in Indian Languages with
  Accents and Intonations'
arxiv_id: '2505.18609'
source_url: https://arxiv.org/abs/2505.18609
tags:
- speech
- languages
- expressive
- speaker
- attributes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the lack of high-quality, multilingual datasets
  for controllable and expressive text-to-speech (TTS) synthesis in Indian languages.
  The authors introduce RASMALAI, a large-scale speech dataset with 13,000 hours of
  speech and 24 million text-description annotations across 23 Indian languages and
  English.
---

# RASMALAI: Resources for Adaptive Speech Modeling in Indian Languages with Accents and Intonations

## Quick Facts
- arXiv ID: 2505.18609
- Source URL: https://arxiv.org/abs/2505.18609
- Reference count: 0
- Introduces RASMALAI dataset with 13,000 hours of speech across 23 Indian languages and English, plus INDIC PARLER TTS - the first open-source text-description-guided TTS system for Indian languages

## Executive Summary
This work addresses the critical gap in high-quality, multilingual datasets for controllable and expressive text-to-speech synthesis in Indian languages. The authors introduce RASMALAI, a massive speech dataset with 13,000 hours of speech and 24 million text-description annotations across 23 Indian languages and English. The dataset includes fine-grained attributes like speaker identity, accent, emotion, style, and background conditions. Using RASMALAI, the authors develop INDIC PARLER TTS, achieving near-human-level naturalness and speaker fidelity with strong instruction-following capabilities and zero-shot expressive transfer across languages.

## Method Summary
The approach combines large-scale dataset curation with a text-description-guided TTS architecture. The RASMALAI dataset aggregates five existing speech corpora totaling 13,000 hours, with text descriptions generated via LLAMA-3.1-8B-INSTRUCT from extracted acoustic attributes (pitch, C50, SNR, speaking rate, PESQ) plus metadata. These descriptions are translated to native languages using IndicTrans2. The INDIC PARLER TTS system starts from Parler-TTS mini v1 with an expanded Llama2 tokenizer for Indian language coverage, pretrains on the full dataset, then finetunes on a 1,804-hour studio-quality subset using cosine learning rate scheduling with 3,000 warm-up steps.

## Key Results
- Achieves MUSHRA naturalness scores approaching human-level synthesis for several Indian languages
- Demonstrates high speaker similarity (0.95) and instruction-following capability (IF-BLEU 93.18)
- Successfully performs zero-shot expressive transfer, generating expressive speech for speakers without expressive training data across languages

## Why This Works (Mechanism)
The system leverages text descriptions as conditioning signals to provide granular control over speech attributes, enabling fine-grained manipulation of speaker identity, accent, emotion, and style. The zero-shot expressive transfer capability emerges from the model's ability to learn generalizable patterns from the diverse training data, allowing it to synthesize expressive speech for unseen speakers and languages by leveraging learned cross-lingual acoustic representations.

## Foundational Learning
- **Acoustic Feature Extraction**: Critical for generating meaningful text descriptions that guide synthesis; quick check: validate extracted features against manual acoustic analysis on sample utterances
- **Tokenizer Expansion**: Essential for proper Indian language subword coverage; quick check: measure OOV rates and tokens per word on target languages
- **Cross-lingual Transfer Learning**: Enables expressive synthesis across languages; quick check: test zero-shot generation on held-out language pairs

## Architecture Onboarding
- **Component Map**: Text input → Tokenizer → TTS Model → Speech Output, with Text Descriptions as conditioning signals
- **Critical Path**: Description generation → Tokenizer expansion → Model pretraining → Fine-tuning → Evaluation
- **Design Tradeoffs**: Large vocabulary vs. computational efficiency in tokenizer expansion; comprehensive attribute coverage vs. description complexity
- **Failure Signatures**: Poor Indian language synthesis indicates inadequate tokenizer vocabulary; degraded instruction-following suggests inconsistent attribute extraction
- **First Experiments**:
  1. Test tokenizer efficiency on sample Indian language text
  2. Validate acoustic feature extraction pipeline on subset of utterances
  3. Generate sample descriptions and assess quality before full training

## Open Questions the Paper Calls Out
None

## Limitations
- Tokenizer expansion methodology is underspecified, creating reproducibility challenges
- Acoustic feature extraction pipeline lacks critical implementation details
- Evaluation relies on subjective perceptual tests that may not fully capture multilingual nuances

## Confidence
- High Confidence: Dataset aggregation methodology and overall model architecture are well-documented
- Medium Confidence: Text description approach is sound but implementation details introduce uncertainty
- Low Confidence: Exact reproduction of claimed performance is challenging due to underspecified components

## Next Checks
1. Implement tokenizer expansion using a comparable Indian language corpus with systematic vocabulary size testing
2. Conduct controlled experiments varying acoustic feature binning thresholds to determine impact on synthesis fidelity
3. Perform ablation studies on description generation process by comparing different LLM prompting strategies