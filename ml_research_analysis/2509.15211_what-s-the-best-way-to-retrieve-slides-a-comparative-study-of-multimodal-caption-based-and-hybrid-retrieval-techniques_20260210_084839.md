---
ver: rpa2
title: What's the Best Way to Retrieve Slides? A Comparative Study of Multimodal,
  Caption-Based, and Hybrid Retrieval Techniques
arxiv_id: '2509.15211'
source_url: https://arxiv.org/abs/2509.15211
tags:
- retrieval
- slide
- visual
- textual
- colpali
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates methods for retrieving slides from large multimodal
  repositories, focusing on balancing accuracy, speed, and storage in real-world RAG
  systems. It compares direct visual retrieval (DSE, ColPali), caption-based retrieval
  (BM25, neural embeddings, Textual ColPali), and hybrid approaches.
---

# What's the Best Way to Retrieve Slides? A Comparative Study of Multimodal, Caption-Based, and Hybrid Retrieval Techniques

## Quick Facts
- arXiv ID: 2509.15211
- Source URL: https://arxiv.org/abs/2509.15211
- Reference count: 40
- Caption-based retrieval with VLM-generated captions achieves competitive performance to visual methods with significantly reduced storage requirements

## Executive Summary
This study evaluates methods for retrieving slides from large multimodal repositories, focusing on balancing accuracy, speed, and storage in real-world RAG systems. It compares direct visual retrieval (DSE, ColPali), caption-based retrieval (BM25, neural embeddings, Textual ColPali), and hybrid approaches. VLM-generated captions enable effective text-based retrieval, often outperforming OCR-based baselines and fine-tuned CLIP, with hybrid methods achieving top NDCG@10 scores (e.g., 83.9% with BM25+Neural+BGE reranker on SlideVQA). ColPali with visual reranking yields the highest accuracy (86.9% NDCG@10) but at high latency and storage cost. Textual ColPali on captions offers a competitive, more storage-efficient alternative. The work highlights that caption-based retrieval with mature text IR techniques provides a practical balance for industry applications.

## Method Summary
The study compares three main approaches for slide retrieval: (1) VLM captioning using Molmo-7B-D-0924 or Gemma3-27B-IT with a detailed prompt, followed by BM25/Neural/hybrid retrieval on the generated captions; (2) ColPali visual late-interaction model (vidore/colpali-v1.3) that encodes slide images into 1031 patch-level visual embeddings; and (3) Textual ColPali on captions. Rerankers (BGE, Jina, MiniLM, MonoQwen2) are optionally applied to top-100 candidates. Retrieval performance is evaluated on SlideVQA (2,619 decks, 52K+ slides, 14,484 VQA samples) and LPM (9,031 slides from 334 lecture videos) using NDCG@10 and Recall@10 metrics. Experiments run on RTX 3090 with fp16 precision for ColPali embeddings.

## Key Results
- Caption-based methods with VLM-generated captions achieve competitive performance to visual late-interaction models while requiring significantly less storage
- ColPali with visual reranking achieves highest accuracy (86.9% NDCG@10 on SlideVQA) but incurs high latency (up to 14 seconds per query) and storage cost (12.90 GB)
- Hybrid approaches combining BM25 and neural retrieval on captions with BGE reranking achieve strong performance (83.9% NDCG@10) with better storage efficiency (~4.21 GB)
- VLM choice matters: Gemma3-27B performs better on SlideVQA (82.2% NDCG@10 with BM25+BGE) while Molmo-7B excels on LPM (70.4% with Neural+BGE)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transforming multimodal slide retrieval into a textual problem via VLM-generated captions enables competitive retrieval performance with significantly reduced storage requirements.
- Mechanism: VLMs (Molmo-7B, Gemma3-27B) generate detailed textual captions capturing slide content including charts, tables, and text. These captions are then indexed using established textual retrieval infrastructure (BM25, dense embeddings, or hybrid combinations), leveraging mature text-based retrieval techniques.
- Core assumption: VLMs can adequately capture and preserve the semantic and visual information necessary for retrieval in textual form.
- Evidence anchors:
  - [abstract] "A novel Vision-Language Models-based captioning pipeline is also evaluated, demonstrating significantly reduced embedding storage requirements compared to visual late-interaction techniques, alongside comparable retrieval performance."
  - [section 4.2] "In terms of space utilization, caption-based methods are highly efficient for storing the captions themselves (e.g., 0.04 GB for SlideVQA with Molmo). While neural retrieval on these captions introduces larger storage needs due to dense embeddings (e.g., 4.21 GB for NV-Embed-2 with Gemma3 captions on SlideVQA), this is still considerably less than visual late-interaction models like ColPali (Visual) (12.90 GB)."
  - [corpus] Riedler et al. [28] showed textual retrieval on image captions improved RAG, though for PDFs with pre-extracted modalities rather than whole slides.
- Break condition: If slides contain highly visual information that cannot be adequately described textually (e.g., complex spatial relationships, pure visual aesthetics), caption-based approaches may underperform.

### Mechanism 2
- Claim: Late-interaction mechanisms in visual embedding models (ColPali) capture finer semantic details than single-vector models, improving retrieval accuracy at the cost of increased storage.
- Mechanism: ColPali encodes slide images into multi-vector representations (1031 patch-level visual embeddings of 128 dimensions each). Textual queries are similarly processed per-token. Late-interaction compares query and visual tokens at a granular level, capturing fine-grained semantic correspondences.
- Core assumption: The patch-level visual tokens preserve sufficient semantic information for effective matching with textual query tokens.
- Evidence anchors:
  - [abstract] "visual late-interaction embedding models like ColPali"
  - [section 3.2] "ColPali [...] which adapts the late-interaction mechanism from ColBERT to the multimodal domain. Slide images are tokenized into 1031 patch-level visual embeddings... while textual queries are similarly processed into per-token embeddings of dimension 128."
  - [corpus] No direct corpus comparison of late-interaction vs. single-vector for slide retrieval specifically.
- Break condition: If storage constraints are severe (single GPU, large corpora), the 12.90 GB storage requirement for SlideVQA alone may be prohibitive.

### Mechanism 3
- Claim: Reranking substantially boosts retrieval accuracy across methods, but introduces latency that may be prohibitive for real-time interactive systems.
- Mechanism: Rerankers (BGE, Jina, MonoQwen2) re-evaluate an initial retrieval set (k=100 candidates) using more computationally intensive cross-attention mechanisms, producing refined top-n (n=10) results.
- Core assumption: The latency introduced by reranking is acceptable for the target use case, or batch/offline processing is viable.
- Evidence anchors:
  - [abstract] "While powerful rerankers boost accuracy, they introduce substantial latency (up to 14 seconds per query), highlighting trade-offs between effectiveness and response time for real-world deployment."
  - [section 4.2] "For example, on SlideVQA, it boosted the hybrid Gemma3-caption method from 66.5% to 83.9% NDCG@10. However, this gain incurs a considerable latency penalty, with BGE adding approximately 7.4 seconds per query on SlideVQA and over 12 seconds on LPM."
  - [corpus] No corpus papers specifically analyze reranking latency trade-offs in slide retrieval contexts.
- Break condition: If real-time response is required (<2 seconds end-to-end), current reranker latency (7-14 seconds) is incompatible.

## Foundational Learning

- Concept: **Late-Interaction Retrieval (ColBERT-style)**
  - Why needed here: Understanding how ColPali and Textual ColPali work requires grasping multi-vector representations and token-level matching, distinct from single-vector bi-encoder approaches.
  - Quick check question: Can you explain how late-interaction differs from standard bi-encoder similarity search in terms of granularity and computational cost?

- Concept: **Hybrid Retrieval (Sparse + Dense Fusion)**
  - Why needed here: The best caption-based results (83.9% NDCG@10) came from BM25+Neural fusion, leveraging complementary lexical and semantic signals.
  - Quick check question: What complementary information do BM25 (sparse) and neural dense retrievers capture, and how might Reciprocal Rank Fusion combine them?

- Concept: **Vision-Language Model Capabilities and Limitations**
  - Why needed here: VLM choice (Molmo vs. Gemma3) affects caption quality, OCR capability, and downstream retrieval; results were dataset-dependent.
  - Quick check question: What factors should guide VLM selection for captioning: model size, OCR training data, domain specificity, or inference speed?

## Architecture Onboarding

- Component map: Slide Images → [VLM Captioner (Molmo/Gemma3)] → Textual Captions → [ColPali Visual / BM25/Dense Indexer] → [Optional Reranker] → RRF Fusion → Retrieved Slides

- Critical path: Caption quality → Textual index quality → Retrieval accuracy. The paper shows caption generation is a one-time cost that enables efficient downstream retrieval.

- Design tradeoffs:
  - **Accuracy vs. Latency**: ColPali (Visual) + Jina reranker achieves 86.9% NDCG@10 but adds ~14 seconds latency.
  - **Storage vs. Retrieval Method**: ColPali (Visual) requires 12.90 GB for SlideVQA; caption-based + neural needs ~4.21 GB; BM25 on captions needs ~0.04 GB.
  - **Caption Model Choice**: Gemma3-27B better on SlideVQA (82.2% NDCG@10 with BM25+BGE); Molmo-7B better on LPM (70.4% with Neural+BGE).

- Failure signatures:
  - If NDCG@10 is low (<55%) with caption-based methods: Check VLM caption quality (OCR errors, missing visual elements).
  - If latency exceeds 5 seconds: Likely reranker bottleneck; try MiniLM (0.09s) instead of BGE (7+ seconds) with accuracy tradeoff.
  - If storage exceeds available RAM: ColPali embeddings may be loading into memory; consider caption-based alternatives or quantization.

- First 3 experiments:
  1. **Baseline establishment**: Implement BM25 on OCR-extracted text and BM25 on VLM-generated captions (Molmo or Gemma3) to measure caption quality impact on your specific slide corpus.
  2. **Storage-latency profiling**: Benchmark ColPali (Visual) vs. Textual ColPali vs. Hybrid BM25+Neural on your corpus, measuring NDCG@10, inference time, and storage requirements.
  3. **Reranker latency impact**: Test BGE reranker on top-3 configurations from experiment 2, measuring accuracy gain vs. latency increase to determine if reranking fits your latency budget.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does fine-tuning ColPali on slide-specific data improve retrieval performance over its zero-shot baseline?
- Basis in paper: [explicit] The Conclusion states future efforts will focus on fine-tuning ColPali "to further enhance its leading performance."
- Why unresolved: The study evaluated ColPali in a zero-shot setting; domain adaptation effects are currently unknown.
- What evidence would resolve it: A performance comparison (NDCG@10, Recall@10) on SlideVQA and LPM datasets between the base model and a slide-domain fine-tuned variant.

### Open Question 2
- Question: Can faster or domain-adapted reranking models reduce the high latency (up to 14 seconds) required for top-tier accuracy?
- Basis in paper: [explicit] The Conclusion notes that research into faster rerankers is "vital to reduce the substantial latency" and facilitate practical deployment.
- Why unresolved: Current top-performing rerankers (Jina, BGE) incur prohibitive latency for interactive systems, and efficient alternatives were not identified.
- What evidence would resolve it: The development of a reranking model achieving comparable accuracy to Jina/BGE with significantly lower inference time per query.

### Open Question 3
- Question: Can purpose-built textual late-interaction models improve the trade-off between storage efficiency and retrieval efficacy compared to the adapted "Textual ColPali"?
- Basis in paper: [explicit] The authors state an aim to "develop more advanced textual late-interaction models, improving the efficacy of storage-efficient caption-based retrieval."
- Why unresolved: The current textual approach repurposed a visual encoder; dedicated architectures might yield better efficiency or accuracy.
- What evidence would resolve it: Architectural improvements to textual encoders that lower storage requirements while maintaining or exceeding current NDCG scores.

## Limitations

- The study focuses on two specific datasets (SlideVQA and LPM), limiting generalizability to other slide types with different visual complexity and content structures.
- Reranking latency measurements (7-14 seconds) are presented as prohibitive, but potential optimizations like model distillation or quantization were not explored.
- The evaluation emphasizes retrieval metrics (NDCG@10, Recall@10) without examining downstream RAG application performance or user experience factors.

## Confidence

- **High confidence**: Caption-based retrieval with VLM-generated captions providing competitive performance to visual methods. The evidence from storage efficiency metrics and comparable NDCG@10 scores across datasets is robust.
- **Medium confidence**: Late-interaction superiority for accuracy at storage cost. While ColPali shows highest accuracy, the storage requirement (12.90 GB for SlideVQA) may be prohibitive in many real-world scenarios not explicitly quantified.
- **Medium confidence**: Reranking trade-offs between accuracy and latency. The latency measurements are clear, but the analysis doesn't explore potential mitigation strategies or examine whether certain query types benefit more from reranking.

## Next Checks

1. Test caption-based vs. visual retrieval performance on slides with varying visual complexity (pure text slides, chart-heavy slides, diagram-intensive slides) to identify break conditions for caption-based approaches.
2. Benchmark ColPali with quantized embeddings and distilled rerankers to quantify accuracy-latency-storage trade-offs under constrained hardware scenarios.
3. Evaluate retrieval results through user studies focusing on downstream RAG utility rather than pure retrieval metrics to validate the practical impact of different retrieval approaches.