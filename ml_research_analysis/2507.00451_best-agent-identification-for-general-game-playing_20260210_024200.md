---
ver: rpa2
title: Best Agent Identification for General Game Playing
arxiv_id: '2507.00451'
source_url: https://arxiv.org/abs/2507.00451
tags:
- game
- best
- each
- general
- identification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficiently identifying the
  best-performing algorithm for each sub-task in a multi-problem domain, specifically
  within the context of general game playing. The authors propose a novel approach
  that frames this challenge as a set of best arm identification problems for multi-armed
  bandits, where each bandit corresponds to a specific game and each arm represents
  a specific game-playing agent.
---

# Best Agent Identification for General Game Playing

## Quick Facts
- arXiv ID: 2507.00451
- Source URL: https://arxiv.org/abs/2507.00451
- Reference count: 22
- Primary result: Proposed Optimistic-WS approach reduces average simple regret by 35.5% to 47.0% compared to UCB-E

## Executive Summary
This paper addresses the challenge of efficiently identifying the best-performing algorithm for each sub-task in multi-problem domains, specifically within general game playing. The authors propose framing this challenge as a set of best arm identification problems for multi-armed bandits, where each bandit corresponds to a specific game and each arm represents a specific game-playing agent. The core contribution is an optimistic selection process based on the Wilson score interval (Optimistic-WS) that ranks each arm across all bandits in terms of their potential regret reduction.

The primary results demonstrate substantial performance improvements in terms of average simple regret when compared to previous best arm identification algorithms for multi-armed bandits. The approach was evaluated on two popular general game domains: the General Video Game AI (GVGAI) framework and the Ludii general game playing system. The results show that the proposed method provides significant performance improvements, with average simple regret being approximately 35.5% to 47.0% smaller than the second-best approach (UCB-E) across the tested datasets. This novel approach can significantly improve the quality and accuracy of agent evaluation procedures for general game frameworks and other multi-task domains with high algorithm runtimes.

## Method Summary
The proposed approach frames the best agent identification problem as a set of best arm identification problems for multi-armed bandits, where each game is treated as an independent bandit problem. The core method uses an optimistic selection process based on the Wilson score interval (Optimistic-WS) to rank arms across all bandits by their potential regret reduction. This allows for efficient identification of the best agent for each game within a limited number of trials. The method was evaluated on two general game domains: the General Video Game AI (GVGAI) framework and the Ludii general game playing system, demonstrating substantial improvements in average simple regret compared to previous approaches.

## Key Results
- Optimistic-WS approach reduces average simple regret by 35.5% to 47.0% compared to UCB-E
- Significant performance improvements observed across both GVGAI and Ludii domains
- The method efficiently identifies best agents for each game within limited trials
- Results demonstrate potential for improving agent evaluation procedures in general game frameworks

## Why This Works (Mechanism)
The Optimistic-WS approach works by leveraging the Wilson score interval to create an optimistic estimate of each arm's performance across all bandits. This optimistic ranking allows the algorithm to prioritize exploration of arms that have the highest potential to reduce regret, effectively balancing exploration and exploitation across multiple games. By treating each game as an independent bandit problem while simultaneously considering the global regret reduction potential, the method can efficiently allocate trials to identify the best agent for each game without requiring exhaustive testing of all agent-game combinations.

## Foundational Learning
- Multi-armed bandit theory: Understanding the fundamental framework for sequential decision-making under uncertainty; needed to grasp the problem formulation; quick check: verify understanding of regret minimization in bandit settings.
- Best arm identification: The specific problem of identifying the optimal arm with limited trials; needed to understand the core objective; quick check: confirm ability to distinguish between regret minimization and best arm identification.
- Wilson score interval: A statistical method for calculating confidence intervals for binomial proportions; needed to understand the optimistic ranking mechanism; quick check: verify ability to compute and interpret Wilson score intervals.
- General game playing: The domain-specific context where algorithms must perform well across diverse games; needed to appreciate the practical motivation; quick check: confirm understanding of the challenges in evaluating agents across multiple games.
- Regret analysis: The framework for measuring algorithm performance in bandit settings; needed to evaluate the effectiveness of the proposed approach; quick check: verify understanding of simple regret versus cumulative regret.

## Architecture Onboarding

Component map:
Data collection -> Wilson score calculation -> Optimistic ranking -> Arm selection -> Performance evaluation

Critical path:
The critical path flows from data collection through Wilson score calculation to optimistic ranking and arm selection. The Wilson score interval computation is the most computationally intensive step, as it must be performed for each arm across all bandits to determine the global optimistic ranking. This ranking then drives the arm selection process, which directly impacts the quality of the final agent identification.

Design tradeoffs:
The primary design tradeoff involves the balance between exploration and exploitation across multiple games. The Wilson score interval provides a principled way to maintain optimism in the face of uncertainty, but this comes at the cost of potentially exploring suboptimal arms. The method assumes independence between games, which simplifies the problem but may miss opportunities for transfer learning between related games.

Failure signatures:
Common failure modes include: (1) Over-exploration of clearly suboptimal arms due to overly optimistic estimates from the Wilson score interval, (2) Insufficient exploration of potentially good arms that have had unlucky initial performance, and (3) Poor performance when games are actually correlated but treated as independent bandits.

First experiments:
1. Verify Wilson score interval calculations produce expected confidence bounds on simulated binomial data
2. Test the optimistic ranking mechanism on a simple synthetic multi-bandit problem with known optimal arms
3. Evaluate performance on a small set of related games to assess the impact of the independence assumption

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation conducted on only two general game frameworks (GVGAI and Ludii), limiting generalizability
- Assumes independence between games, potentially missing correlations that could improve performance
- Wilson score interval approach may behave differently under real-world noise conditions or with highly skewed reward distributions

## Confidence
- Performance improvement claims: High
- Generalizability claims: Medium
- Methodology soundness: High

## Next Checks
1. Test the Optimistic-WS approach on additional multi-task domains beyond game playing, such as hyperparameter optimization for machine learning models or algorithm selection for optimization problems, to assess generalizability.

2. Evaluate the method's performance under varying levels of reward noise and with different reward distribution characteristics to verify robustness across real-world conditions.

3. Investigate potential correlations between games or tasks and assess whether a clustered bandit approach could further improve performance compared to treating each game as an independent bandit.