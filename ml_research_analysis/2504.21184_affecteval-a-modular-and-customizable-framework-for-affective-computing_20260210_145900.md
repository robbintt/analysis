---
ver: rpa2
title: 'AffectEval: A Modular and Customizable Framework for Affective Computing'
arxiv_id: '2504.21184'
source_url: https://arxiv.org/abs/2504.21184
tags:
- computing
- affective
- affecteval
- signals
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AffectEval is a modular, customizable software framework for developing
  end-to-end affective computing pipelines. It addresses the lack of comprehensive,
  reusable tools for multimodal, multi-domain emotion recognition by providing pre-implemented
  components for signal acquisition, preprocessing, feature extraction, feature selection,
  label generation, and classification.
---

# AffectEval: A Modular and Customizable Framework for Affective Computing

## Quick Facts
- arXiv ID: 2504.21184
- Source URL: https://arxiv.org/abs/2504.21184
- Reference count: 40
- One-line primary result: Reduces programming effort by up to 90% for affective computing pipeline development

## Executive Summary
AffectEval is a modular software framework designed to simplify the development of end-to-end affective computing pipelines. It addresses the fragmentation in existing tools by providing reusable, pre-implemented components for signal acquisition, preprocessing, feature extraction, feature selection, label generation, and classification. The framework reduces programming effort by up to 90% compared to building pipelines from scratch, while supporting time-series physiological signals and enabling easy extension to other modalities.

## Method Summary
The framework uses an object-oriented pipeline architecture where users instantiate and connect modular components: SignalAcquisition reads standardized CSV files into DataFrames, SignalPreprocessor handles denoising and resampling, FeatureExtractor implements physiological feature engineering (HRV, SCL, SCR), LabelGenerator creates target labels from annotations, and Classification trains scikit-learn compatible models. Users define a component list, pass it to the Pipeline class, and call run(). The framework was validated by replicating binary and 3-class affect classification on WESAD (15 subjects) and binary stress detection on APD (52 subjects), using LOSO and 5-fold CV respectively.

## Key Results
- Reduces programming effort by up to 90% compared to baseline libraries
- Matches or exceeds baseline model performance on WESAD and APD datasets
- Provides pre-implemented components for signal acquisition, preprocessing, feature extraction, feature selection, label generation, and classification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework likely reduces programming effort by decoupling the affective computing pipeline into interchangeable, object-oriented components.
- Mechanism: AffectEval enforces a modular architecture where distinct pipeline stages are separate classes, allowing users to reuse standard logic while only modifying specific behaviors.
- Core assumption: LOC reduction correlates directly with reduced cognitive load and development time.
- Evidence anchors:
  - [abstract] "AffectEval is a modular... framework... reduces programming effort by up to 90%."
  - [section] Section 3 (Design): "AffectEval adopts a modular architecture... consisting of six classes... Components are independent and can be easily modified."
  - [corpus] "BrainLesion Suite" paper abstract supports this design pattern, noting that modular pipelines provide a "brainless development experience."

### Mechanism 2
- Claim: Standardizing data input formats likely automates signal ingestion and minimizes custom parsing errors.
- Mechanism: By enforcing a rigid prerequisite directory structure and CSV format, the framework abstracts away file I/O logic, ensuring downstream compatibility.
- Core assumption: Users are able and willing to pre-process their raw data files into the required structure.
- Evidence anchors:
  - [abstract] Mentions providing components for "signal acquisition" to reduce manual effort.
  - [section] Section 3.1 (Prerequisites): "The structure of the dataset folder... must follow a standard format."
  - [corpus] "MissMAC-Bench" highlights the difficulty of handling heterogeneous data in affective computing.

### Mechanism 3
- Claim: Default implementations of standard physiological features enable rapid prototyping by allowing users to defer technical implementation details.
- Mechanism: The FeatureExtractor comes with pre-implemented methods (e.g., HRV from ECG, tonic/phasic decomposition from EDA) using libraries like NeuroKit.
- Core assumption: The default features provided are relevant predictors for the user's specific target affect.
- Evidence anchors:
  - [abstract] "...providing pre-implemented components for... feature extraction."
  - [section] Section 4.3 (Validating AffectEval): "We identified the specific preprocessing and feature extraction methods... implemented them in our AffectEval-based pipeline."
  - [corpus] "Teleology-Driven Affective Computing" suggests frameworks must guide agents toward specific goals.

## Foundational Learning

- Concept: **Object-Oriented Composition**
  - Why needed here: AffectEval is built on a "Pipeline" class that executes an ordered list of component objects.
  - Quick check question: Can you explain how to instantiate a SignalPreprocessor and pass it to a Pipeline object in the framework?

- Concept: **Physiological Signal Processing (Biosignals)**
  - Why needed here: The framework is optimized for time-series physiological data (ECG, EDA, etc.).
  - Quick check question: Do you know why a "high-pass filter" might be applied to an EMG signal versus an EDA signal?

- Concept: **Scikit-learn API Estimators**
  - Why needed here: The Classification and Feature Selector components rely on the standard scikit-learn interface.
  - Quick check question: If you import a classifier from a non-scikit-learn library, what two methods must it implement to be compatible with AffectEval's Classification component?

## Architecture Onboarding

- Component map:
  Source Data (CSVs) -> SignalAcquisition (Reads to DataFrame) -> SignalPreprocessor (Denoising/Resampling) -> FeatureExtractor (Extracts HRV, SCL, etc.) -> FeatureSelector (Optional: Dimensionality reduction) -> LabelGenerator (Creates y based on phase/annotation) -> Classification (Trains/Test model) -> Output (Metrics/Predictions)

- Critical path:
  1. Data Formatting (Hardest Step): Restructure your dataset to match the required {subject}_{phase}_{modality}.csv convention
  2. Component Instantiation: Define the list [SignalAcquisition, Preprocessor, FeatureExtractor, LabelGenerator, Classification]
  3. Pipeline Execution: Pass the list to the Pipeline class and call run()

- Design tradeoffs:
  - Rigid Input Format vs. Ease of Use: The framework trades data structure flexibility for code reduction
  - Python/Sklearn Ecosystem vs. Custom ML: The system is highly optimized for scikit-learn compatible models

- Failure signatures:
  - KeyError on 'timestamp' or 'modality': Input CSV files do not have required headers
  - AttributeError: 'XXX' object has no attribute 'fit': Custom model passed to Classification component doesn't follow scikit-learn estimator interface
  - Discrepancy in Reproduction Results: Library version differences can cause performance variations

- First 3 experiments:
  1. Format Validation: Download WESAD and convert one subject's data to AffectEval CSV format
  2. Default Pipeline Run: Run binary stress classification using only default components to establish baseline accuracy
  3. Custom Component Injection: Create custom LabelGenerator with different threshold and observe classification metric changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can AffectEval be effectively extended to support real-time emotion recognition and distributed computing?
- Basis in paper: [explicit] Section 5.3 states that the framework "does not support real-time emotion recognition or distributed computing," limiting its applicability to online tasks.
- Why unresolved: The current architecture processes stored datasets and lacks parallelization for compute-intensive components.
- What evidence would resolve it: Implementation of a cloud-based or client-server wrapper that demonstrates successful streaming data processing with low latency.

### Open Question 2
- Question: How can AffectEval's architecture be adapted to integrate non-physiological data streams such as image, text, and audio?
- Basis in paper: [explicit] Section 5.3 notes the framework "lacks pre-defined functionalities for image, text, and audio data," limiting its scope despite the popularity of these modalities.
- Why unresolved: Current pre-implemented components are specialized for time-series physiological signals.
- What evidence would resolve it: The development and validation of new processing modules for facial expressions or speech audio on datasets like DEAP.

### Open Question 3
- Question: Does the reduction in raw lines of code (LOC) provided by AffectEval correlate directly with a reduction in subjective developer effort and required programming expertise?
- Basis in paper: [inferred] Section 4.4 uses LOC as the primary metric for "manual effort," while Section 1 identifies the need for "extensive software experience" as a barrier the framework aims to lower.
- Why unresolved: Code reduction metrics do not necessarily capture the cognitive load of understanding complex abstractions.
- What evidence would resolve it: A comparative user study measuring time-to-deployment and cognitive load between developers using AffectEval versus baseline libraries.

## Limitations
- The rigid CSV directory structure requirement creates significant barriers for streaming data or datasets that don't fit the subject-phase-modality paradigm
- Heavy reliance on scikit-learn-compatible models limits applicability for advanced deep learning approaches
- Performance variations across library versions (neurokit, biosppy) introduce uncertainty in reproducibility

## Confidence
- High: Modular architecture benefits and 90% LOC reduction claim
- Medium: Specific performance metrics showing variance from baseline studies
- Low: Not applicable (no major low-confidence claims identified)

## Next Checks
1. Test the framework with a dataset that doesn't naturally fit the subject-phase-modality structure to quantify adaptation overhead
2. Implement a custom wrapper for a PyTorch model to assess complexity of extending beyond scikit-learn compatibility
3. Run the same pipeline with different versions of neurokit and biosppy to empirically measure performance variance and determine reproducibility bounds