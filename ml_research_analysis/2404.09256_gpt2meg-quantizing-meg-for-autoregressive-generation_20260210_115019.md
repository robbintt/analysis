---
ver: rpa2
title: 'GPT2MEG: Quantizing MEG for Autoregressive Generation'
arxiv_id: '2404.09256'
source_url: https://arxiv.org/abs/2404.09256
tags:
- data
- state
- gpt2meg
- channel
- real
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores autoregressive generative modeling of MEG time
  series. It proposes GPT2MEG, which discretizes continuous multichannel MEG into
  tokens and uses a GPT-2 Transformer architecture with channel, subject, and task-condition
  embeddings.
---

# GPT2MEG: Quantizing MEG for Autoregressive Generation

## Quick Facts
- **arXiv ID**: 2404.09256
- **Source URL**: https://arxiv.org/abs/2404.09256
- **Reference count**: 40
- **Primary result**: GPT2MEG outperforms WaveNet variants and linear AR baselines in spectral, temporal, and task-evoked statistics for MEG time series generation.

## Executive Summary
This work explores autoregressive generative modeling of MEG time series using GPT-2-style transformers with quantized tokens. The approach discretizes continuous multichannel MEG into tokens via µ-law companding and uniform quantization, enabling language-model architectures to be repurposed for neural signal generation. GPT2MEG, which uses additive embeddings for channel, subject, and task-condition conditioning, demonstrates superior performance compared to WaveNet variants and linear AR baselines across forecasting, long-horizon generation, and downstream decoding tasks. Subject embeddings enable multi-subject scaling and transfer learning benefits, with generated trials improving downstream decoding accuracy.

## Method Summary
The method involves three key components: (1) Tokenization of MEG signals using µ-law companding (µ=255) followed by uniform quantization to 256 bins per channel, transforming continuous signals into discrete tokens; (2) A GPT-2 decoder architecture with additive embeddings for position, channel, subject, and task-condition, enabling conditional generation without architectural changes; (3) Training with cross-entropy loss on next-step prediction, followed by autoregressive generation using top-p sampling (p=0.8). The model is evaluated on forecasting accuracy, long-horizon spectral and dynamical statistics (via 12-state HMM analysis), task-evoked responses, and downstream decoding transfer learning.

## Key Results
- GPT2MEG better reproduces spectral power distributions and 12-state HMM-derived temporal statistics than WaveNet variants
- GPT2MEG captures task-evoked responses with significantly higher fidelity than WaveNet models (74% vs 44-56% correlation)
- Generated trials improve downstream decoding accuracy through transfer learning compared to using only limited real data
- Subject embeddings enable multi-subject training with smoother/averaged responses while maintaining generative quality

## Why This Works (Mechanism)

### Mechanism 1: µ-law companding + uniform quantization bridges continuous MEG to discrete token modeling
Non-linear amplitude transformation followed by binning enables GPT-2-style language modeling on continuous brain signals with minimal information loss. µ-law companding (µ=255) skews the amplitude distribution to allocate more quantization levels to smaller magnitudes, while uniform binning maps to Q=256 discrete tokens per channel. Cross-entropy loss on these tokens avoids mean-prediction bias seen in MSE regression. The core assumption is that MEG amplitude distributions benefit from the same non-linear quantization strategy as audio waveforms, and 256 bins provide sufficient resolution for task-relevant dynamics.

### Mechanism 2: Additive conditional embeddings enable context-specific generation without architectural changes
Element-wise addition of channel, subject, and task-condition embeddings allows a single shared model to generate subject- and task-appropriate neural dynamics. All conditioning signals are embedded and added to token+positional representations: H^(0) = XW_e + W_p + YW_y + OW_o + W_c. Task embeddings are zeroed when no stimulus is present. Subject embeddings scale to multiple subjects; channel embeddings capture sensor-specific statistics. The core assumption is that subject, task, and channel characteristics can be captured as static embeddings rather than dynamic conditioning functions, and their effects are approximately additive.

### Mechanism 3: Transformer self-attention captures long-range dynamics that dilated convolutions miss
GPT-2's attention mechanism better reproduces multivariate HMM-derived state dynamics and task-evoked responses than WaveNet's fixed receptive field convolutions. Self-attention allows direct modeling of dependencies at any temporal distance without fixed receptive field constraints. WaveNet's dilated convolutions (255-sample receptive field) bias toward local patterns, limiting capture of slow state transitions and extended evoked responses. The core assumption is that MEG contains meaningful long-range temporal dependencies exceeding 255 samples (2.55s at 100 Hz) that are critical for realistic generation.

## Foundational Learning

- **Concept: µ-law companding**
  - Why needed here: MEG signals have amplitude distributions with heavy concentration near zero and rare large deflections; uniform quantization would waste resolution on rarely-used bins.
  - Quick check question: If you quantized a signal with mostly small values using uniform bins, what would happen to reconstruction quality for typical amplitudes?

- **Concept: Autoregressive language modeling with cross-entropy loss**
  - Why needed here: The paper frames generation as predicting the next discrete token from a vocabulary of 256 possibilities—identical to language modeling.
  - Quick check question: Why does the paper claim cross-entropy on categorical distributions avoids "mean-prediction bias" from MSE regression?

- **Concept: Top-p (nucleus) sampling**
  - Why needed here: Generation uses top-p sampling (p=0.8) to balance diversity and coherence in long-horizon rollouts.
  - Quick check question: What would likely happen to generated spectra if you used greedy decoding (always selecting the most probable token)?

## Architecture Onboarding

- **Component map:**
  Input pipeline: MEG X ∈ R^(C×T) → per-channel standardization → clip [-10,10] → rescale [-1,1] → µ-law transform → Q=256 bins
  Embedding stack: Shared token embedding W_e ∈ R^(Q×E) + position W_p + channel W_c ∈ R^(C×E) + subject W_o ∈ R^(O×E) + task W_y ∈ R^(Y×E)
  Transformer core: GPT-2 decoder, 12 layers, 12 heads, d_model=96 (single-subject) or 240 (multi-subject), variable context 128–256
  Output head: Per-channel projection to Q-class softmax
  Generation: Top-p sampling (p=0.8), autoregressive decoding

- **Critical path:**
  1. Tokenization quality → if reconstruction error >5%, re-examine clipping/companding parameters
  2. Embedding configuration → task embedding must be zero during non-stimulus periods
  3. Evaluation → one-step accuracy is insufficient; must validate with PSD, HMM dynamics, and evoked responses

- **Design tradeoffs:**
  - Channel-independent processing: Simpler training, but paper explicitly notes "serious limitation...does not receive information from other channels"
  - Discrete tokens vs. continuous regression: Enables LM-style training/transfer but introduces ~5% reconstruction error
  - Single vs. multi-subject training: Group model produces smoother/averaged responses but enables transfer learning benefits

- **Failure signatures:**
  - Moderate next-step accuracy but flat/band-limited generated PSD → model overfit to local patterns
  - Evoked responses identical across task conditions → condition embeddings not utilized (verify via ablation)
  - HMM state statistics show invariant distributions → model failing to capture state-switching dynamics
  - Generated cross-channel covariance near zero → expected for channel-independent models, but WFCM should show some structure

- **First 3 experiments:**
  1. Tokenization validation: Dequantize tokenized signals and compute correlation of evoked responses with raw data; verify <5% reconstruction error per Appendix A.4
  2. Condition embedding ablation: Train GPT2MEG-randomlabel (shuffled labels) and GPT2MEG-1label (single label); confirm evoked response correlation drops from ~74% to 44–56% per Section 5.8
  3. HMM dynamics comparison: Fit 12-state time-domain HMM on real and generated data; verify GPT2MEG matches fractional occupancy, mean lifetime, mean interval, and switching rate distributions better than WaveNet variants per Figure 3

## Open Questions the Paper Calls Out

### Open Question 1
Can architectural modifications or increased data scale enable the effective utilization of cross-channel dependencies, which are currently ignored by the channel-independent GPT-2 implementation? The authors attempted various mixing approaches (convolutions, concatenation) without success; current generated data shows near-zero inter-channel covariance (Appendix A.10).

### Open Question 2
Do multi-timestep or contrastive loss objectives improve long-horizon generation fidelity compared to the standard next-step cross-entropy loss? The current study relied exclusively on cross-entropy loss for token prediction, which showed weak correlation with long-horizon spectral and dynamical metrics.

### Open Question 3
Can continuous, modality-agnostic stimulus embeddings (e.g., wav2vec features) replace categorical task labels to enable scalable conditional generation across diverse datasets? The current model uses discrete indices for 118 visual images, a method that does not generalize to continuous stimuli (audio, video) or unseen classes.

### Open Question 4
Does a VQ-VAE-based tokenization strategy offer superior reconstruction of complex dynamics compared to the simple μ-law companding used in this study? The paper only evaluates uniform binning after μ-law companding; it is unknown if learned discrete codes would better preserve the signal dynamics needed for downstream decoding.

## Limitations
- Channel-independent architecture ignores cross-channel dependencies, producing near-zero inter-channel covariance in generated data
- Reliance on categorical task labels limits scalability to continuous or diverse stimulus modalities
- Reconstruction error from quantization (~5%) may concentrate in task-relevant features despite overall low error rates

## Confidence

- **Medium confidence**: GPT2MEG outperforms WaveNet variants and linear AR baselines in spectral, temporal, and task-evoked statistics
- **Medium confidence**: Subject embeddings enable effective multi-subject scaling and transfer learning benefits
- **Medium confidence**: Additive conditional embeddings successfully capture subject, task, and channel effects without architectural changes
- **Medium confidence**: µ-law quantization with 256 bins preserves task-relevant MEG information with minimal loss

## Next Checks

1. **Cross-channel mixing ablation**: Train a variant of GPT2MEG with cross-channel mixing (allowing attention between channels) and compare its ability to reproduce cross-channel covariance structures against the current channel-independent version.

2. **Alternative tokenization schemes**: Compare µ-law quantization with linear quantization and learned vector quantization (as used in "Scaling Next-Brain-Token Prediction for MEG") while keeping the Transformer architecture constant.

3. **Downstream task ablation**: Train the 118-class decoder on real MEG data versus GPT2MEG-generated data versus WaveNet-generated data, without transfer learning, to determine whether GPT2MEG's generation advantage translates to improved raw feature representations.