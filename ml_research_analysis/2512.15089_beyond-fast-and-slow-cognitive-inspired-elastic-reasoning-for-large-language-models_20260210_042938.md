---
ver: rpa2
title: 'Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language
  Models'
arxiv_id: '2512.15089'
source_url: https://arxiv.org/abs/2512.15089
tags:
- reasoning
- tool
- query
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently handling diverse
  query complexities in large language models. It proposes a framework that dynamically
  routes queries to appropriate reasoning strategies based on their complexity, inspired
  by Bloom's Taxonomy.
---

# Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models

## Quick Facts
- **arXiv ID:** 2512.15089
- **Source URL:** https://arxiv.org/abs/2512.15089
- **Reference count:** 40
- **Primary result:** Proposed framework achieves at least 13% relative improvement in exact match on in-domain tasks and 8% on out-of-domain tasks.

## Executive Summary
This paper addresses the challenge of efficiently handling diverse query complexities in large language models by proposing a cognitive-inspired framework that dynamically routes queries to appropriate reasoning strategies. The method draws inspiration from Bloom's Taxonomy to classify queries into four complexity levels and uses a trained agent to select optimal reasoning actions for each query. The approach significantly outperforms existing methods by balancing computational cost with solution quality through hierarchical reasoning strategies.

## Method Summary
The CogER framework employs a reinforcement learning-based agent that classifies incoming queries into four hierarchical levels (L1-L4) based on Bloom's Taxonomy. Each level triggers a specific execution path: L1 uses a lightweight LLM for immediate answers, while L3 engages a Large Reasoning Model (LRM) for extended Chain-of-Thought reasoning, and L4 incorporates autonomous tool invocation for knowledge-intensive tasks. The system is trained using Group Relative Policy Optimization (GRPO) with a hierarchical-aware reward function that penalizes over-use of complex reasoning levels while maintaining accuracy.

## Key Results
- Achieves at least 13% relative improvement in exact match on in-domain tasks compared to existing methods
- Demonstrates 8% relative improvement on out-of-domain tasks
- Shows 11.24% relative EM improvement on MATH-500 when autonomous tool invocation (CoTool) is enabled
- Maintains balanced strategy selection across levels (approximately 2%/28%/22%/48% distribution)

## Why This Works (Mechanism)

### Mechanism 1
Routing queries to specialized processing modes based on predicted complexity improves the trade-off between inference cost and solution quality. The CogER-Agent maps incoming queries to one of four hierarchical levels (L1-L4) based on Bloom's Taxonomy, triggering specific execution paths where L1 uses a lightweight LLM and L3 engages an LRM for extended Chain-of-Thought reasoning.

### Mechanism 2
Reinforcement learning with a hierarchical-aware reward function prevents over-reasoning on simple tasks, optimizing computational resource usage. The framework models strategy selection as a Markov Decision Process (MDP) where the reward function includes a specific penalty term for selecting a reasoning level higher than the minimal sufficient level.

### Mechanism 3
Autonomous tool invocation (CoTool) bridges knowledge gaps for complex queries without requiring external human intervention or rigid flows. For L4 queries, the LLM integrates tool calls directly into its generation stream, pausing when special tokens are detected, executing external tools, and injecting results back into the context window.

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs)**
  - **Why needed here:** The core of CogER is a sequential decision process where the "action" (reasoning strategy) affects the "state" (downstream computation/cost).
  - **Quick check question:** How does the state definition $s_t = [x, y_{1:t-1}, L_i]$ differ from a standard classification input?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** The CogER-Agent is trained via GRPO, a variant of PPO that uses group-based advantage estimation to balance quality-cost trade-off.
  - **Quick check question:** In GRPO, how is the advantage $\hat{A}_i$ calculated relative to the group output rewards?

- **Concept: Bloom's Taxonomy**
  - **Why needed here:** The four complexity levels (L1-L4) are grounded in this cognitive hierarchy (Remember → Create), providing theoretical justification for query-to-reasoning-depth mapping.
  - **Quick check question:** Which level in CogER corresponds to Bloom's "Analyze/Evaluate" tier, and what action does it trigger?

## Architecture Onboarding

- **Component map:** Input Query → CogER-Agent (7B) → `<question level>` tag → Dispatch to L1/L2/L3/L4 module → Response

- **Critical path:**
  1. Input Ingestion: Query $x$ enters
  2. Agent Rollout: CogER-Agent generates `<question level> Li </question level>`
  3. Dispatch: L1 → Agent outputs answer directly; L2/L3 → 32B models for CoT; L4 → LRM → CoTool loop → Tool Execution → Result Injection
  4. Response: Final answer extracted

- **Design tradeoffs:**
  - Router Capacity vs. Cost: Using a 7B model as the agent/router is cheap but may lack nuance to distinguish L3 from L4 queries
  - Reward Balance: Weights in $R_{hierarchy}$ strictly control the cost/accuracy Pareto frontier; aggressive penalties save money but risk under-thinking hard problems

- **Failure signatures:**
  - Runaway L4 Usage: Agent defaults to L4 for everything; check if $R_{hierarchy}$ penalty weight is too low
  - Tool Loop Stagnation: Model repeatedly calls tools without progressing; enforce `MAX_TOOL_CALLS` threshold
  - Format Collapse: Agent generates valid reasoning but forgets the `<question level>` tag; check $R_{format}$ implementation

- **First 3 experiments:**
  1. Routing Baseline: Compare trained CogER-Agent against random router and flat 4-class classifier to verify RL-based routing learns better cost/quality policy
  2. Reward Ablation: Disable $R_{hierarchy}$ to confirm agent drifts toward high-cost L4 strategies
  3. Tool Efficacy: Run L4 queries with CoTool disabled vs. enabled on knowledge-heavy subset to isolate contribution of external tools

## Open Questions the Paper Calls Out

### Open Question 1
How can CogER be extended to maintain context and update complexity estimates in multi-turn conversational or multi-modal settings? The current validation is limited to single-turn, text-only tasks, and the MDP state representation is designed for static queries without accounting for evolving dialogue history or non-textual inputs.

### Open Question 2
How can the reward function be refined to overcome sparsity issues and better align with the nuanced quality of long chains of thought? The current binary outcome-based reward may be too coarse for complex reasoning, risking unstable policy learning or reward hacking.

### Open Question 3
Does the discrete hierarchical reward structure fail to optimize resource allocation when the computational cost of external tools varies significantly? The framework treats all "Delegate" (L4) actions as a single cost tier, which might discourage more efficient solutions when tool costs vary.

## Limitations

- The determination of "minimal sufficient level" ($L_{min}$) required for the Hierarchical-Aware Reward function is not fully specified, creating reproducibility challenges
- The specific implementation details of the RSTKit tool interface and "Tool Selection Instruction" logic are not fully detailed
- The framework's effectiveness on knowledge-intensive domains beyond MATH-500 requires additional validation

## Confidence

- **High Confidence:** The general framework of dynamic routing based on query complexity classification is well-supported by experimental results showing consistent improvements across multiple datasets
- **Medium Confidence:** The effectiveness of the Hierarchical-Aware Reward mechanism in preventing over-reasoning is demonstrated through ablation studies, but the specific impact of penalty weight (0.2) on different query distributions remains unclear
- **Medium Confidence:** The contribution of CoTool to L4 performance is validated on MATH-500, but its effectiveness on knowledge-intensive domains would benefit from additional validation

## Next Checks

1. **Ground-Truth Labeling Validation:** Conduct a controlled experiment where human annotators independently determine $L_{min}$ for a subset of queries, then compare against the agent's learned policy to validate reward signal quality

2. **Cross-Domain Generalization:** Test the CogER framework on a knowledge-intensive dataset (e.g., MedQA) with CoTool disabled vs. enabled to quantify the actual contribution of external tools beyond what's reported for MATH-500

3. **Router Capacity Scaling:** Replace the 7B CogER-Agent with a larger model (e.g., 32B) and measure changes in the distribution of selected levels, particularly the ability to distinguish between L3 and L4 queries, to assess the current router's capacity limitations