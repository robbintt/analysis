---
ver: rpa2
title: 'Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning
  in Large Language Models'
arxiv_id: '2504.01857'
source_url: https://arxiv.org/abs/2504.01857
tags:
- consistency
- accuracy
- reasoning
- english
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving reasoning performance
  in small-scale large language models (LLMs), particularly those under 10B parameters,
  which often struggle with semantic drift and logical inconsistencies due to multilingual
  training corpus biases. The authors propose Cross-Lingual Consistency (CLC), an
  inference framework that enhances reasoning by integrating multilingual Chain-of-Thought
  (CoT) paths through majority voting.
---

# Cross-Lingual Consistency: A Novel Inference Framework for Advancing Reasoning in Large Language Models

## Quick Facts
- **arXiv ID:** 2504.01857
- **Source URL:** https://arxiv.org/abs/2504.01857
- **Reference count:** 1
- **Primary result:** CLC framework achieves 9.5â€“18.5% absolute accuracy gains over self-consistency on math reasoning tasks for sub-10B LLMs

## Executive Summary
This paper introduces Cross-Lingual Consistency (CLC), an inference framework that enhances reasoning performance in small-scale large language models (under 10B parameters) by integrating multilingual Chain-of-Thought paths through majority voting. The approach addresses semantic drift and logical inconsistencies caused by multilingual training corpus biases by leveraging multiple language versions of reasoning prompts to neutralize linguistic biases and escape monolingual reasoning traps. Experiments on CMATH and MGSM datasets demonstrate significant improvements over traditional self-consistency, with gains particularly pronounced in low-resource language scenarios.

## Method Summary
CLC operates by translating reasoning problems into multiple languages, generating candidate answers in each language using zero-shot Chain-of-Thought prompting, and aggregating all candidates through majority voting. For each problem, the framework generates 10 reasoning samples per language using language-specific prompts that instruct the model to reason step-by-step and format final answers within \boxed{} notation. The aggregated answers are then processed to extract final answers, which are compared against ground truth to compute accuracy. The framework is evaluated against monolingual self-consistency baselines across multiple model architectures including DeepSeek-Math-7B-Instruct, Qwen2.5-Math-7B-Instruct, and Gemma2-9B-Instruct.

## Key Results
- CLC achieves absolute accuracy gains of 9.5%, 6.5%, and 6.0% for DeepSeek-Math-7B-Instruct, Qwen2.5-Math-7B-Instruct, and Gemma2-9B-Instruct respectively on the CMATH dataset
- On the MGSM dataset, CLC delivers gains ranging from 4.1% to 18.5% for Gemma2-9B-Instruct across 11 languages
- Improvements are especially pronounced in low-resource language scenarios, demonstrating CLC's effectiveness in overcoming linguistic biases

## Why This Works (Mechanism)
CLC leverages the diversity of multilingual reasoning paths to overcome biases inherent in monolingual training corpora. By generating multiple reasoning trajectories across different languages and applying majority voting, the framework can neutralize language-specific semantic drift and escape reasoning traps that occur when models are constrained to a single linguistic perspective. The multilingual ensemble voting mechanism allows correct reasoning paths to emerge from the consensus, even when individual language-specific paths may contain errors.

## Foundational Learning

**Chain-of-Thought Reasoning:** Step-by-step problem-solving approach that improves model performance on complex reasoning tasks. Needed to break down mathematical problems into manageable steps; quick check: verify models can follow multi-step reasoning prompts.

**Majority Voting Ensemble:** Aggregation technique that selects the most common answer from multiple predictions. Needed to consolidate diverse reasoning paths; quick check: test voting on synthetic ensemble data with known correct answers.

**Cross-Lingual Training Biases:** Systematic errors introduced when models are trained on multilingual data with imbalanced quality across languages. Needed to understand why monolingual reasoning fails; quick check: compare model performance across languages on same task.

**Zero-Shot Prompting:** Generating responses without task-specific fine-tuning by providing natural language instructions. Needed to apply CLC to any model without additional training; quick check: verify prompt format consistency across languages.

## Architecture Onboarding

**Component Map:** Problem Translation -> Multi-Language CoT Generation -> Answer Extraction -> Majority Voting -> Final Answer

**Critical Path:** The most time-consuming step is generating 10 samples per language per problem. For 6 languages, this means 60 inference calls per problem, making inference speed a critical consideration.

**Design Tradeoffs:** More languages provide better coverage but increase computational cost and risk introducing conflicting low-quality predictions. The optimal subset (6 languages) balances diversity against quality.

**Failure Signatures:** Performance degradation occurs when including too many languages, suggesting some languages contribute noise rather than consensus. Accuracy plateaus or decreases indicate suboptimal language selection.

**First Experiments:**
1. Implement bilingual CLC (alternating Chinese/English) and verify it matches or exceeds monolingual self-consistency performance
2. Test monolingual self-consistency with 10 samples to establish baseline for comparison
3. Implement multilingual CLC with all 11 MGSM languages and measure accuracy degradation

## Open Questions the Paper Calls Out

**Open Question 1:** How can the optimal subset of languages for the CLC framework be determined predictively without exhaustive enumeration? The authors note it remains unclear how to predict and decide the optimal set of languages to achieve the best performance, having relied on exhaustive enumeration of 2,047 combinations for the MGSM dataset.

**Open Question 2:** Does the CLC framework yield significant reasoning improvements in large-parameter models (>100B) similar to those observed in sub-10B models? The study was restricted to small-scale models (7B-9B parameters), and it is unknown if larger models with better intrinsic reasoning would benefit similarly.

**Open Question 3:** Can the CLC framework be effectively integrated with Retrieval-Augmented Generation (RAG) systems? The authors list integration of CLC with RAG as a specific direction for future work, noting that RAG introduces external knowledge which may vary significantly across languages.

## Limitations

- Implementation details remain underspecified, including sampling hyperparameters and answer extraction function specifics
- Results show considerable variance (4.1%-18.5%) across languages, indicating potential sensitivity to language selection or implementation details
- The framework's computational cost scales linearly with the number of languages, making it expensive for large-scale deployment

## Confidence

**High confidence** in the core methodology and general findings: multilingual CoT with majority voting improves reasoning accuracy over monolingual baselines

**Medium confidence** in the magnitude of reported improvements: substantial gains are demonstrated but specific values depend on implementation details not fully specified

**Medium confidence** in the universality of the approach: results are strong for the tested models and datasets but generalization requires validation on additional benchmarks

## Next Checks

1. Reproduce the CMATH results with DeepSeek-Math-7B-Instruct using the 6-language subset (Chinese, English, Bengali, Spanish, Russian, Thai) to verify the "optimal language subset" finding

2. Implement and test the bilingual CLC variant (alternating Chinese/English) to confirm it matches or exceeds multilingual performance

3. Conduct ablation studies removing individual languages from the multilingual ensemble to identify which language contributions drive the accuracy gains