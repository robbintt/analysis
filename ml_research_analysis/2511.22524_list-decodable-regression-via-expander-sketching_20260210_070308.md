---
ver: rpa2
title: List-Decodable Regression via Expander Sketching
arxiv_id: '2511.22524'
source_url: https://arxiv.org/abs/2511.22524
tags:
- fraction
- buckets
- lemma
- inlier
- constant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an expander-sketching framework for list-decodable
  linear regression that achieves near-optimal sample complexity, list size, and input-sparsity
  runtime. The method uses sparse signed adjacency matrices derived from lossless
  expanders to create "lightly contaminated" synthetic batches, each containing a
  constant fraction of inliers and only a bounded number of outliers.
---

# List-Decodable Regression via Expander Sketching

## Quick Facts
- arXiv ID: 2511.22524
- Source URL: https://arxiv.org/abs/2511.22524
- Reference count: 21
- Primary result: Achieves near-optimal sample complexity, list size O(1/α), and input-sparsity runtime O(nnz(X) + d³/α) for list-decodable linear regression

## Executive Summary
This paper introduces an expander-sketching framework for list-decodable linear regression that achieves near-optimal statistical and computational guarantees. The method uses sparse signed adjacency matrices from lossless expanders to create synthetic "lightly contaminated" batches, each containing a constant fraction of inliers and only a bounded number of outliers. Local normal-equation statistics within these buckets are aggregated robustly via median-of-means or geometric median estimators, producing accurate global moment estimates. A short spectral filtering phase removes adversarial contributions along dominant directions. The resulting algorithm outputs a list of size O(1/α) with one candidate within statistical radius of the true parameter, runs in time O(nnz(X) + d³/α), and avoids both SoS optimization and explicit batch structure assumptions.

## Method Summary
The algorithm constructs r independent B×n signed adjacency matrices from left-regular lossless expanders and applies them to the dataset to create synthetic batches. For each bucket, it computes local normal equations (Ht,b, gt,b) = (A⊤t,bAt,b, A⊤t,b bt,b). These statistics are aggregated robustly via median-of-means or geometric median estimators to obtain global moment estimates (bΣ, bg). The parameter estimate is then computed by solving (bΣ + λI)ℓ̂ = bg, optionally followed by spectral filtering to remove adversarial contributions. The process is repeated with R independent expander seeds, and the resulting candidates are clustered to form the final list.

## Key Results
- Achieves sample complexity O(d/α), matching the information-theoretic lower bound for list-decodable regression
- Runs in input-sparsity time O(nnz(X) + d³/α), significantly faster than existing SoS-based approaches
- Maintains robustness under heavy contamination and outlier magnitude, outperforming standard and robust regression baselines on synthetic and real datasets
- Provides explicit statistical error bounds O(σ²√((d+log(1/δ))/(αn))) with high probability

## Why This Works (Mechanism)

### Mechanism 1: Lossless Expander Isolation
- **Claim**: Lossless expanders convert corrupted datasets into synthetic "lightly contaminated batches" where a constant fraction contain Θ(αn/B) inliers and ≤ O(1) outliers
- **Mechanism**: Each sample hashes to dL = O(1) buckets via a left-regular bipartite graph satisfying (K, ε)-lossless expansion: |N(X)| ≥ (1−ε)dL|X| for |X| ≤ K. This guarantees most inliers have at least one bucket where they appear alone, and the collision budget bounds total outlier intrusion to ≤ 2εdL|X|
- **Core assumption**: ε < 1/(4dL) and the adversary is oblivious to expander randomness
- **Evidence anchors**: Abstract states "uses lossless expanders to synthesize lightly contaminated batches"; Section 4 defines (K, ε)-lossless expander; Section 6, Lemma 6 provides multi-repetition isolation guarantee
- **Break condition**: If expander construction fails or adversary has access to sketch randomness, unique-neighbor guarantees collapse

### Mechanism 2: Robust Moment Aggregation
- **Claim**: MoM/geometric median aggregation over block means from lightly contaminated buckets yields global moment estimates with operator-norm error O(σ²√((d+log(1/δ))/(αn)))
- **Mechanism**: Bucket-wise normal equations compute local statistics; good buckets contain Θ(αn/B) i.i.d. inliers plus ≤ C0 outliers. Coordinate-wise median over M blocks achieves concentration via ε-net argument over sphere
- **Core assumption**: Sub-Gaussian inliers and noise; at least γ-fraction of blocks are good
- **Evidence anchors**: Abstract mentions "robust aggregation via median-of-means or geometric median estimators"; Section 6, Lemmas 7-8 provide concentration bounds
- **Break condition**: If γ drops below constant or inliers are not sub-Gaussian, median fails to concentrate

### Mechanism 3: Spectral Filtering
- **Claim**: Spectral filtering prunes adversarially-dominated buckets in O(log(1/α)) rounds, reducing residual variance along top eigen-direction by constant factor per round
- **Mechanism**: After solving normal equations, compute robust residual covariance; if λmax exceeds inlier target by factor (1+η), prune top ρ-fraction buckets by Rayleigh score. Adversarial contribution along v shrinks by (1−c1) per round
- **Core assumption**: Adversarial energy concentrates along low-rank directions; robust aggregation error remains small throughout filtering
- **Evidence anchors**: Abstract mentions "short spectral filtering phase removes adversarial contributions"; Section 5, Algorithm 1 implements filtering loop; Section 6, Lemma 10 provides formal guarantee
- **Break condition**: If adversaries spread energy uniformly across eigen-directions or η is too small, filtering has limited effect

## Foundational Learning

- **Concept: List-Decodable Learning**
  - **Why needed here**: The core problem—outputting a list L of size O(1/α) containing at least one near-accurate regressor when ≥ (1−α)-fraction are adversarial—differs fundamentally from classical robust regression
  - **Quick check question**: Given α = 0.3, explain why a list of size O(1/α) ≈ 3-4 candidates is information-theoretically necessary rather than a single estimate

- **Concept: Lossless Expander Graphs**
  - **Why needed here**: The combinatorial isolation (unique neighbors, bounded collisions) relies on expansion property |N(X)| ≥ (1−ε)dL|X| for small sets X. Without this, adversarial samples could collude to corrupt most buckets
  - **Quick check question**: For a left-regular bipartite graph with dL = 3, ε = 0.1, and |X| = 100, what is the minimum guaranteed number of unique neighbors?

- **Concept: Median-of-Means Estimation**
  - **Why needed here**: MoM aggregates bucket statistics, tolerating up to (1/2 − δ)-fraction arbitrary corruptions. Median (not mean) provides this robustness
  - **Quick check question**: If 40% of block means are adversarially corrupted, why does coordinate-wise median still concentrate while mean fails?

## Architecture Onboarding

- **Component map**: Sketch Construction -> Bucket-wise Normal Equations -> Robust Aggregation -> Parameter Solve -> (if needed) Spectral Filtering -> List Generation
- **Critical path**: Sketch Construction → Bucket-wise Normal Equations → Robust Aggregation → Parameter Solve → (if filtering needed) Spectral Filtering → List Generation. The filtering loop (inner) and seeding loop (outer) multiply runtime by O(T·R) = O((log(1/α))·(1/α))
- **Design tradeoffs**:
  - **Bucket count B**: Larger B → finer isolation, lower per-bucket inlier load (needs αn/B ≳ 1). Default: B ≍ (d/α)·log(d/δ)
  - **Repetitions r**: More repetitions → higher probability of good isolation per inlier. Diminishing returns beyond r = Θ(log(1/δ))
  - **Seeds R**: More seeds → higher probability at least one succeeds. R = Θ(1/α) suffices for constant success probability per seed
  - **Pruning rate ρ**: Aggressive pruning (large ρ) speeds filtering but may discard inlier mass; conservative (small ρ) preserves signal but requires more rounds
  - **Ridge λ**: Non-zero λ stabilizes inversion when bΣ is ill-conditioned but introduces bias
- **Failure signatures**:
  - All candidates poor: Likely insufficient isolation (B too small, r too small, or ε too large). Check average inlier load per bucket
  - Filtering never terminates: η set too low or adversaries spread energy uniformly. Increase η or cap T
  - Runtime blowup: d³ term dominates; consider iterative solvers or preconditioning for large d
  - List too large: Clustering radius ∆ set too small; candidates fail to merge. Increase ∆ to match target error scale
- **First 3 experiments**:
  1. **Sanity check on synthetic data with α = 0.3, d = 20, n = 5000, S = 10**: Verify Expander-L achieves test MSE ≲ 1.0 and parameter error ∥ŵ − w⋆∥₂ ≲ 1.0; compare against OLS and Huber baselines
  2. **Ablation on isolation parameters**: Fix α = 0.3, vary (B, r) around defaults (B = 1000, r = 8). Plot test MSE vs. B and vs. r to identify the cliff where isolation fails
  3. **Real-data stress test replication**: Construct CASP+Concrete mixture (α ≈ 0.3, n = 1400, d = 10). Verify Expander-L achieves test MSE on clean CASP holdout significantly below OLS-on-mixture (~1075) and approaches oracle OLS (~33)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the expander-sketching framework be extended to handle fully adaptive adversaries who observe the sketching randomness before choosing outliers?
- **Basis in paper**: [explicit] "We assume only that the adversary is oblivious to the internal randomness of our algorithm, in particular to the random choice of expander graphs and signs used in the sketching stage"
- **Why unresolved**: The analysis fundamentally relies on the adversary being unable to exploit the expander structure to concentrate corruptions in specific buckets. An adaptive adversary could potentially target the isolated unique-neighbor buckets
- **What evidence would resolve it**: A proof that the framework (or a modification) succeeds against adaptive adversaries, or a construction showing that adaptive corruption can break the isolation guarantees from Lemma 5

### Open Question 2
- **Question**: Can the sub-Gaussian assumptions on covariates and noise be relaxed to allow heavier-tailed distributions while maintaining similar guarantees?
- **Basis in paper**: [inferred] The theoretical guarantees assume "$x$ is $K_x$-sub-Gaussian" and noise $\xi$ is sub-Gaussian
- **Why unresolved**: Heavier-tailed distributions would affect the concentration bounds for bucket-wise statistics, potentially requiring different robust aggregation techniques or modified expander parameters
- **What evidence would resolve it**: An extension of the analysis to sub-exponential or polynomially-tailed distributions, showing corresponding (possibly weaker) error bounds, or a counterexample showing fundamental breakdown under heavy tails

## Limitations
- **Expansion Parameter Sensitivity**: The algorithm's correctness hinges on the lossless expander's (K, ε)-expansion with ε < 1/(4dL), which may be difficult to achieve in practice
- **Sub-Gaussian Assumption Tightness**: The reliance on sub-Gaussian inliers and noise may not hold in heavy-tailed real-world scenarios
- **Clustering Hyperparameter Dependence**: The list-generation phase uses single-linkage clustering with unspecified radius ∆, making success critically dependent on this hyperparameter

## Confidence
- **High**: Sample complexity O(d/α) (Theorem 1), runtime O(nnz(X) + d³/α) (Theorem 1), and correctness of expander-isolation mechanism (Lemma 6) given lossless expansion property
- **Medium**: Statistical error bounds O(σ²√((d+log(1/δ))/(αn))) (Proposition 1) and spectral filtering guarantees (Lemma 10), as these depend on bounded-collision and energy-concentration assumptions
- **Low**: Practical performance claims relative to baselines in Table 2, as these depend on unvalidated hyperparameters (∆, M, filtering η) and may not transfer across datasets

## Next Checks
1. **Expanders Under Stress**: Generate synthetic datasets with varying α (0.1, 0.2, 0.3), B (500, 1000, 2000), and r (4, 8, 12). Plot the empirical fraction of good buckets and average inlier load per bucket to identify the parameter regime where isolation fails
2. **Filtering Robustness**: Run synthetic experiments with α = 0.3 where adversaries either (a) concentrate energy along a single direction or (b) spread uniformly across all directions. Measure the number of filtering rounds required and whether λ_max(bC) converges in both cases
3. **Hyperparameter Transferability**: Using the CASP+Concrete mixture (α ≈ 0.3), fix all Expander-L hyperparameters (B, r, R, ρ, T) and test on a different dataset mixture (e.g., California Housing + synthetic outliers). Assess whether the same parameter set achieves similar performance without retraining