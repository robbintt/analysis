---
ver: rpa2
title: 'Failure to Mix: Large language models struggle to answer according to desired
  probability distributions'
arxiv_id: '2511.14630'
source_url: https://arxiv.org/abs/2511.14630
tags:
- response
- llms
- probability
- word
- your
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study reveals that modern LLMs fail to generate outputs following
  desired probability distributions, exhibiting near step-function behavior in response
  to probabilistic prompts. Despite LLMs being able to correctly reason about mixed
  strategies in complex scenarios, they consistently choose the option with marginally
  higher probability, undermining their utility in creative idea generation and strategic
  decision-making.
---

# Failure to Mix: Large language models struggle to answer according to desired probability distributions

## Quick Facts
- arXiv ID: 2511.14630
- Source URL: https://arxiv.org/abs/2511.14630
- Reference count: 0
- Modern LLMs cannot generate outputs following desired probability distributions, exhibiting near step-function behavior in response to probabilistic prompts

## Executive Summary
This study reveals that modern LLMs fail to generate outputs following desired probability distributions, exhibiting near step-function behavior in response to probabilistic prompts. Despite LLMs being able to correctly reason about mixed strategies in complex scenarios, they consistently choose the option with marginally higher probability, undermining their utility in creative idea generation and strategic decision-making. This limitation appears to stem from training on deterministic benchmarks, which discourages probabilistic exploration. Even increasing response diversity through multiple outputs or complex prompt framing fails to produce the intended distributions. These findings highlight a critical gap in LLM capabilities for tasks requiring stochastic decision-making, such as scientific idea selection and adversarial game strategies.

## Method Summary
The study evaluates whether LLMs can generate outputs following specified probability distributions using prompts requesting probabilistic outputs (e.g., binary "biased coin" returning "1" with probability p). Tested across p ∈ [0,1] with N=100 API calls per p value, using 8 LLMs including Gemini 2.5 Pro, GPT-5, and others. Response rate r (observed frequency of "1") and step-similarity S (area between r vs. p curve and diagonal r=p) were computed. The study tested temperature variations, single vs. multiple decisions per prompt (D=2,3,10), and different prompt framings including game theory problems.

## Key Results
- LLMs exhibit step-function response behavior instead of calibrated probability matching
- Temperature scaling does not resolve the distributional fidelity problem
- Reasoning-execution decoupling is consistently observed across models

## Why This Works (Mechanism)

### Mechanism 1: Argmax-Amplified Token Selection
- Claim: LLMs exhibit near-deterministic selection of the token with marginally highest probability, producing step-function behavior instead of calibrated probability matching.
- Mechanism: RL training on deterministic benchmarks optimizes for correct answers, which structurally disincentivizes probabilistic exploration. The model learns to always select the highest-probability completion, even when instructed otherwise.
- Core assumption: This behavior emerges from reward model training dynamics rather than architectural constraints of transformer decoders.
- Evidence anchors:
  - [abstract] "requesting a binary output of '1' 49% of the time produces an answer of '0' nearly 100% of the time"
  - [page 5] "This emergent property likely resulted or were amplified from reinforcement learning using difficult benchmark questions with verifiably correct answers"
  - [corpus] TRE paper confirms "Entropy regularization...yields negligible effects or even degrades performance in LLMs"

### Mechanism 2: Sequential Self-Correction via Meta-Cognition
- Claim: When generating multiple sequential decisions, LLMs attempt to calibrate later outputs against their earlier responses, producing zigzag patterns rather than independent samples.
- Mechanism: The model conditions on its own prior outputs within the context window and attempts to "correct" toward the target distribution at the ensemble level.
- Core assumption: This reflects emergent calibration reasoning rather than explicit training for distribution matching.
- Evidence anchors:
  - [page 2] "the second response (j=2) follows an unexpected zigzag pattern...This observation is consistent with the LLM observing their own first response and then trying to compensate"
  - [page 2] Fig 2c shows non-monotonic zigzag for j=2, even more complex patterns for j=3

### Mechanism 3: Reason-Execute Decoupling
- Claim: LLMs can correctly compute optimal mixed strategies through reasoning chains but cannot execute probabilistic sampling aligned with those strategies.
- Mechanism: Verbal reasoning and action selection follow different computational paths; the former accesses learned strategic knowledge while the latter is dominated by argmax selection pressure.
- Core assumption: The decoupling is not deliberate but reflects a capability gap in translating calculated probabilities into stochastic action.
- Evidence anchors:
  - [page 3-4] "all 5 LLMs challenged with the above problem were able correctly reason through the problem and conclude the strategy...but all of the LLMs fail to execute on that strategy"
  - [page 4] Game theory prompts show LLMs produce correct Nash equilibrium reasoning but step-function action selection

## Foundational Learning

- Concept: Mixed Strategy Nash Equilibrium
  - Why needed here: The paper uses game theory problems (Matching Pennies, Business Positioning) where optimal play requires probabilistic action selection. Understanding why "always play the best response" is exploitable clarifies why LLM step-function behavior is a critical failure mode.
  - Quick check question: In Rock-Paper-Scissors against a sophisticated opponent, why is playing Rock 100% of the time a losing strategy?

- Concept: Independent and Identically Distributed (i.i.d.) Sampling
  - Why needed here: The paper shows LLMs fail to produce i.i.d. samples even when mean response rate approaches target—variance is suppressed relative to true binomial distribution.
  - Quick check question: If you flip a fair coin 10 times for 1000 runs, what's the expected distribution of "heads" counts? Why does Fig 3c show LLMs cluster more tightly?

- Concept: Temperature Sampling in Language Models
  - Why needed here: The paper tests temperature=2 and finds no improvement, demonstrating the failure is learned behavior, not insufficient sampling entropy.
  - Quick check question: What does temperature actually modify in the softmax? Why would increasing it fail to fix a learned argmax bias?

## Architecture Onboarding

- Component map: Prompt design → Model forward pass → Argmax-dominated selection → Observed step-function response rate. For sequential decisions: Prior outputs enter context → Meta-cognitive compensation → Zigzag patterns.
- Critical path: Prompt design → Model forward pass → Argmax-dominated selection → Observed step-function response rate. For sequential decisions: Prior outputs enter context → Meta-cognitive compensation → Zigzag patterns.
- Design tradeoffs:
  - Single-response simplicity vs. multi-response distribution accuracy (D=10 approaches target mean but is compute-inefficient)
  - Code delegation vs. direct generation (code works but requires knowing when to invoke it)
  - Temperature increase vs. learned bias override (paper shows temperature has minimal effect)
- Failure signatures:
  - Step-function r vs. p curves with threshold near p=0.5
  - Zigzag patterns in j>1 responses for multi-decision prompts
  - Suppressed variance in "1" counts across runs (tighter than binomial)
  - Reasoning-execution gap: correct strategy calculation, deterministic action selection
- First 3 experiments:
  1. Replicate binary probability test (Fig 1) with p=0.49 vs p=0.51 to confirm step-function threshold on your target model
  2. Test D=10 sequential decisions and measure whether variance in "1" counts matches expected binomial distribution (Fig 3c replication)
  3. Compare reasoning-only vs. action-required prompts on a game theory problem to quantify the reason-execute decoupling gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning on probabilistic tasks (e.g., biased coin outputs with verifiable ensemble statistics) induce LLMs to follow target distributions, or does the step-function behavior persist due to architectural constraints?
- Basis in paper: [explicit] Authors state "this limitation appears to stem from training on deterministic benchmarks" and call for "new training approaches that encourage probabilistic reasoning and output diversity."
- Why unresolved: The paper only tests existing models; no intervention experiments are conducted to test whether alternative training regimes could remedy the failure.
- What evidence would resolve it: Fine-tune an LLM on a corpus of probabilistic decision tasks with reward signals based on ensemble distribution accuracy, then re-evaluate on the binary and multi-option distribution benchmarks from the paper.

### Open Question 2
- Question: Why do LLMs correctly reason about mixed strategies (e.g., deriving p=0.4 in bioinformatics and game theory prompts) yet fail to execute them, even when the strategy is explicitly stated in the response?
- Basis in paper: [explicit] "all 5 LLMs challenged with the above problem were able correctly reason through the problem and conclude the strategy... but all of the LLMs fail to execute on that strategy."
- Why unresolved: The paper documents the reasoning-action gap but does not investigate whether it arises from token-level sampling, internal representation separation, or post-RLHF behavior.
- What evidence would resolve it: Run mechanistic probes (e.g., activation patching or attention head analysis) to determine where the verbalized strategy decouples from output token selection.

### Open Question 3
- Question: Do the inefficiencies of multi-response workarounds (D=10+) persist across all model scales, or do larger models converge to target distributions with fewer responses?
- Basis in paper: [inferred] The paper notes Mean(r) approaches r=p at D=10 but calls the method "grossly inefficient" due to quadratic compute scaling, leaving open whether model scaling improves per-response distributional fidelity.
- Why unresolved: Only a fixed set of model sizes is tested; no systematic scaling analysis is reported.
- What evidence would resolve it: Evaluate step-similarity S for D∈{2,3,5,10} across a model family (e.g., GPT-3.5 vs. GPT-4 vs. GPT-5 or similar scaling variants) and report S as a function of both model size and D.

### Open Question 4
- Question: Can instruction-level triggers (e.g., "use code to simulate randomness") reliably induce correct distributional behavior in open-ended creative tasks without human specification of when code is warranted?
- Basis in paper: [explicit] "Explicit requests for code-based generation of '1' or '0' outcomes... do generate the correct code and target probability distributions... in more complex prompts, it may not be immediately obvious to LLMs where code-based randomness is warranted."
- Why unresolved: The paper shows code-based workarounds succeed in toy cases but does not test whether LLMs can autonomously detect when code-based randomness is appropriate in complex, multi-step prompts.
- What evidence would resolve it: Design a benchmark with interleaved deterministic and stochastic subtasks and measure whether models autonomously invoke code for only the stochastic components.

## Limitations
- Prompt Specification Completeness: The study uses simplified binary decision prompts but does not fully explore how complex, real-world framing might affect outcomes.
- Model Population Representativeness: The claim that this is a universal LLM limitation may be overstated as different training regimes were not systematically explored.
- Statistical Power Considerations: Some analyses may be underpowered given the N=100 sample size per condition.

## Confidence

**High Confidence (8/10)**:
- LLMs exhibit step-function response behavior instead of calibrated probability matching
- Temperature scaling does not resolve the distributional fidelity problem
- Reasoning-execution decoupling is consistently observed across models

**Medium Confidence (6/10)**:
- The behavior stems primarily from RL training on deterministic benchmarks
- Increasing response diversity through multi-response prompts approaches target distributions
- This limitation is universal across current leading LLMs

**Low Confidence (4/10)**:
- Complex prompt engineering cannot resolve the issue
- The mechanism is purely learned behavior rather than architectural
- This is a critical failure for all stochastic decision-making tasks

## Next Checks
1. **Variance Analysis Replication**: Replicate the D=10 sequential decision analysis with N=1000 samples per condition to confirm whether the observed variance suppression relative to binomial distribution is statistically significant and consistent across multiple runs.

2. **Cross-Training Regime Comparison**: Test models with different training regimes (pure supervised, RL-only, RLHF hybrid) on the same probability matching tasks to determine whether the observed limitations are indeed caused by RL training dynamics versus other factors.

3. **Alternative Sampling Implementation**: Test whether models can correctly execute probabilistic sampling when the sampling logic is provided in generated code versus direct generation, and systematically vary the complexity of the sampling task to identify the boundary of the limitation.