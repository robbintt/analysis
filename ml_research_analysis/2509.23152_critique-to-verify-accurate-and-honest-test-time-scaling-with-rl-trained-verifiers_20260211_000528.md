---
ver: rpa2
title: 'Critique to Verify: Accurate and Honest Test-Time Scaling with RL-Trained
  Verifiers'
arxiv_id: '2509.23152'
source_url: https://arxiv.org/abs/2509.23152
tags:
- critique
- training
- solution
- data
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of test-time scaling methods
  in identifying minority-yet-correct answers when aggregating solutions from large
  language models (LLMs). The core issue stems from insufficient critique signals
  during verifier training.
---

# Critique to Verify: Accurate and Honest Test-Time Scaling with RL-Trained Verifiers

## Quick Facts
- arXiv ID: 2509.23152
- Source URL: https://arxiv.org/abs/2509.23152
- Reference count: 17
- This paper introduces Mirror-Critique, a framework that trains verifiers via reinforcement learning on synthetic critique data to identify minority-yet-correct answers and enable calibrated abstention in test-time scaling.

## Executive Summary
This paper addresses the limitation of test-time scaling methods in identifying minority-yet-correct answers when aggregating solutions from large language models (LLMs). The core issue stems from insufficient critique signals during verifier training. The proposed Mirror-Critique framework synthesizes high-quality critique data by contrasting model-generated solutions with ground-truth answers, using a small instruction-tuned model with rejection sampling. This synthetic data is used to cold-start and train a verifier via reinforcement learning. The resulting Mirror-Verifier generates multiple critiques per solution, aggregates them into verification scores, and uses these for weighted voting or selective abstention. Experimental results show Mirror-Verifier significantly outperforms majority voting and reward-based selection across multiple mathematical reasoning benchmarks. It also improves model honesty by enabling calibrated abstention from questions beyond its capability boundaries, with notable improvements in both accuracy and honesty scores.

## Method Summary
The method involves four key stages: (1) RLVR training of a zero-solver to generate diverse solutions with recorded trajectories, (2) synthesis of high-quality critique data by contrasting solutions against ground truth using an instruction model with rejection sampling to ensure judgment accuracy, (3) cold-start SFT training followed by balanced RLVR training of a verifier on the synthetic critiques, and (4) deployment at test time using weighted voting with verification scores or selective abstention based on score thresholds. The approach addresses reward hacking by using balanced positive/negative sampling and improves both accuracy and honesty through calibrated abstention mechanisms.

## Key Results
- Mirror-Verifier achieves 80.1% accuracy on MATH-500, outperforming majority voting (76.4%) and Skywork-O1-PRM (78.8%)
- Honesty scores improve significantly, with Mirror-RLVR achieving 0.765 at τ=0.2 versus 0.697 for Skywork-O1-PRM
- The framework demonstrates consistent improvements across five mathematical reasoning benchmarks including OlympiadBench, Minerva-Math, AIME24, and AMC23
- Weighted voting with verification scores consistently outperforms majority voting across all evaluated datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Contrastive critique synthesis with rejection sampling produces training signal that teaches verifiers causal error detection, not just pattern matching.
- **Mechanism:** An instruction-tuned model generates step-by-step critiques comparing candidate solutions against ground truth. Rejection sampling filters retain only critiques where the final True/False judgment matches actual correctness, ensuring critique content aligns with verdict.
- **Core assumption:** The instruction model can produce pedagogically valid rationales when given ground truth as hidden reference, and judgment-accurate critiques correlate with useful error explanations.
- **Evidence anchors:**
  - [abstract] "synthesizes high-quality critique data by contrasting model-generated solutions with ground-truth answers, using a small instruction-tuned model with rejection sampling"
  - [Section 4.1] Describes prompt template instructing model to analyze solutions without revealing ground truth terminology, with rejection filter on final judgment
  - [corpus] Related work on verification design (arXiv:2508.16665) notes verifier effectiveness is capped by poor self-evaluation—suggests explanatory critiques may address this gap
- **Break condition:** If critique quality evaluation (Table 3: 80-83% accuracy) degrades significantly on out-of-distribution problem types, the rejection filter may be insufficient for domain shift.

### Mechanism 2
- **Claim:** Cold-start SFT on synthetic critiques followed by balanced RLVR training prevents reward hacking and enables stable verifier improvement.
- **Mechanism:** Base models lack critique formatting ability (Appendix C shows they echo prompts). SFT on synthetic data instills basic critique generation. RLVR with balanced positive/negative sampling prevents the model from defaulting to "False" predictions (Appendix D shows imbalance causes reward hacking).
- **Core assumption:** The synthetic critique distribution sufficiently covers the error modes the verifier will encounter at test time.
- **Evidence anchors:**
  - [Section 4.2] "training the verifier with imbalanced samples through RLVR easily leads to reward hacking, where the LLM tends to predict all samples as False"
  - [Appendix D, Figure 6] Shows accuracy divergence between balanced and imbalanced training
  - [corpus] Variation in Verification (arXiv:2509.17995) discusses verification dynamics but does not specifically address critique-based training stability
- **Break condition:** If training F1 (Figure 7: peaks ~0.75) plateaus without reaching target verification accuracy, the synthetic data may lack coverage of difficult edge cases.

### Mechanism 3
- **Claim:** Multi-critique aggregation with verification scores enables both accurate weighted voting and calibrated abstention.
- **Mechanism:** For each solution, generate M critiques. Verification score = fraction judging "True." Weighted voting uses (score + β) as vote weight. Abstention triggers when average score of winning answer falls below threshold τ.
- **Core assumption:** Critique judgments are sufficiently independent that aggregation reduces noise; low scores correlate with actual incorrectness.
- **Evidence anchors:**
  - [Section 4.3] Formal definition of verification score and weighted voting with β=0.15, M=16
  - [Figure 4] Honesty-accuracy curve shows Mirror-RLVR dominates Mirror-SFT and Skywork-O1-PRM
  - [corpus] Calibrated Reasoning (arXiv:2509.19681) proposes similar pairwise explanatory verifier with GRPO training—consistent finding that RL-trained verifiers improve calibration
- **Break condition:** If critiques for the same solution show high correlation (low variance), increasing M provides diminishing returns; cost-benefit may favor smaller M.

## Foundational Learning

- **Concept: Reinforcement Learning with Verifiable Reward (RLVR)**
  - Why needed here: The verifier training uses GRPO (a variant of PPO) with binary rewards derived from critique correctness. Understanding policy gradient methods explains why balanced data matters.
  - Quick check question: If all training samples have reward=0 for one class, what happens to the policy gradient estimate?

- **Concept: Test-Time Scaling via Sampling and Aggregation**
  - Why needed here: The framework operates on N candidate solutions per question. Understanding Pass@K vs. Majority@K metrics clarifies why minority-correct solutions matter.
  - Quick check question: If K=16 and one correct solution exists among 15 incorrect ones with identical wrong answers, what does majority voting select?

- **Concept: Rejection Sampling for Data Quality**
  - Why needed here: Synthetic critique quality depends on filtering judgments that match ground truth. Understanding this explains why 100% retention isn't the goal.
  - Quick check question: If the instruction model has 70% judgment accuracy, what fraction of generated critiques pass the rejection filter?

## Architecture Onboarding

- **Component map:** RLVR training (zero-solver) → trajectory recording → critique synthesis pipeline → synthetic dataset → SFT cold-start → balanced RLVR training (Mirror-Verifier) → weighted voting/abstention at inference

- **Critical path:** Critique synthesis quality → SFT initialization effectiveness → RLVR training stability → test-time verification accuracy. Errors propagate; weak synthetic data cannot be recovered by RLVR alone.

- **Design tradeoffs:**
  - M (critiques per solution): Higher M improves score reliability but increases latency linearly. Paper uses M=16.
  - τ (abstention threshold): Higher τ increases honesty but reduces answer coverage. Paper uses τ=0.20 for honesty experiments.
  - β (voting boost): Prevents zero-weight solutions. Paper uses β=0.15.
  - Assumption: These hyperparameters were tuned; optimal values may differ for other domains.

- **Failure signatures:**
  - Verifier outputs malformed critiques (no True/False in boxed format) → cold-start SFT failed or prompt template issue
  - All solutions receive similar scores near 0.5 → verifier undertrained or reward signal weak
  - Honesty score negative at all thresholds → verifier scores uncorrelated with correctness
  - Reward hacking (Appendix D): Training accuracy for False samples →100%, True samples →0% → data imbalance

- **First 3 experiments:**
  1. **Replicate synthetic critique quality evaluation (Table 3):** Sample 30 critiques, use LLM-as-judge to verify critique content accuracy. Target: >75% agreement.
  2. **Ablate cold-start:** Train verifier with RLVR only (no SFT). Compare training dynamics to Figure 7. Expect: slower convergence, potential formatting failures.
  3. **Calibrate τ on validation set:** Sweep τ ∈ {0.1, 0.2, 0.3, 0.4, 0.5}. Plot accuracy vs. honesty tradeoff curve. Identify operating point matching Figure 4 envelope.

## Open Questions the Paper Calls Out
- What denoising methods could further improve the quality of synthetic critique data beyond rejection sampling? (Section 5.4)
- Can the Mirror-Critique framework be extended to Tool-Integrated Reasoning domains involving code verification? (Appendix B)
- How does the Mirror-Verifier generalize to reasoning domains beyond mathematical problem-solving? (Inferred from limited benchmark scope)

## Limitations
- Synthetic critique quality has inherent noise (80-83% accuracy leaves significant error potential)
- The framework is currently limited to text-only mathematical reasoning, explicitly excluding code verification
- Fixed abstention threshold τ=0.20 may be suboptimal across different problem difficulties or domains

## Confidence
- **High confidence:** Weighted voting improves accuracy over majority voting (Figure 4 demonstrates consistent gains)
- **Medium confidence:** RLVR training stability requires balanced sampling (Appendix D shows reward hacking with imbalanced data)
- **Medium confidence:** Calibrated abstention improves honesty scores (honesty metric is standard but sensitive to threshold choice)
- **Low confidence:** Synthetic critique quality is sufficient for effective verifier training (80-83% accuracy leaves significant noise)

## Next Checks
1. **Validate critique quality:** Sample 30 synthetic critiques and use LLM-as-judge to assess content accuracy. Target >75% agreement to ensure training signal reliability.
2. **Test RLVR stability:** Train verifier with imbalanced (75:25) vs balanced (50:50) sampling. Compare training curves for reward hacking symptoms (all False predictions).
3. **Characterize abstention behavior:** Sweep τ from 0.1 to 0.5 on validation set. Plot accuracy-honesty tradeoff to verify operating point selection matches Figure 4 envelope.