---
ver: rpa2
title: 'When Smiley Turns Hostile: Interpreting How Emojis Trigger LLMs'' Toxicity'
arxiv_id: '2509.11141'
source_url: https://arxiv.org/abs/2509.11141
tags:
- emojis
- emoji
- generation
- llms
- toxicity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether emojis can enhance toxicity generation
  in large language models (LLMs) and how to interpret this phenomenon. The authors
  automatically construct prompts with emojis to express toxic intent and conduct
  experiments across 5 languages and 7 LLMs, demonstrating that emoji-containing prompts
  significantly increase harmful output compared to plain text.
---

# When Smiley Turns Hostile: Interpreting How Emojis Trigger LLMs' Toxicity

## Quick Facts
- arXiv ID: 2509.11141
- Source URL: https://arxiv.org/abs/2509.11141
- Reference count: 12
- One-line primary result: Emojis can significantly increase harmful output in LLMs by creating a heterogeneous semantic channel that bypasses safety mechanisms.

## Executive Summary
This paper investigates whether emojis can enhance toxicity generation in large language models (LLMs) and how to interpret this phenomenon. The authors automatically construct prompts with emojis to express toxic intent and conduct experiments across 5 languages and 7 LLMs, demonstrating that emoji-containing prompts significantly increase harmful output compared to plain text. Model-level interpretation reveals that emojis act as a heterogeneous semantic channel, creating representation gaps that bypass safety mechanisms. Analysis of pre-training corpora uncovers emoji-related data pollution potentially correlated with toxicity generation behaviors.

## Method Summary
The authors construct emoji-substituted prompts by automatically rewriting the AdvBench benchmark using an LLM to swap sensitive words with emojis and add camouflage. They then query 7 target LLMs with these prompts, evaluating the responses using GPT-4o as a judge to calculate Harmful Score (HS) and Harmfulness Ratio (HR). For interpretation, they analyze tokenization patterns (BPE fragmentation), internal representations (t-SNE visualizations of last token embeddings), and corpus data pollution by examining the C4 dataset for emoji-toxic co-occurrences.

## Key Results
- Emoji-containing prompts significantly increase harmful output compared to plain text across 5 languages and 7 LLMs
- Emojis act as a heterogeneous semantic channel through tokenization disparities, creating representation gaps that bypass safety mechanisms
- Analysis of pre-training corpora reveals emoji-related data pollution potentially correlated with toxicity generation behaviors

## Why This Works (Mechanism)

### Mechanism 1: Heterogeneous Semantic Channel via Tokenization Disparity
- **Claim:** If emojis are tokenized into sub-word fragments distinct from their textual equivalents, they may bypass text-based safety alignment by activating a different semantic pathway.
- **Mechanism:** Most tokenizers fragment rare unicode symbols (emojis) into multiple sub-tokens. These sub-tokens share minimal vocabulary overlap with the text tokens of the same concept. This "tokenization mismatch" creates a "heterogeneous semantic channel" where the input representation does not trigger the specific neurons primed to refuse harmful text.
- **Core assumption:** Safety alignment is heavily biased towards natural language text distributions, leaving low-resource symbolic channels (like fragmented emojis) under-defended.
- **Evidence anchors:** Mentions "tokenization disparities enabling alternative semantic expression channels," analysis shows >97% of emojis are segmented into multiple sub-tokens with "minimal overlap" with textual words, cites "Rare Distribution" in the vocabulary as a cause.

### Mechanism 2: Representation Gap Induces Safety Drift
- **Claim:** Replacing sensitive words with emojis shifts the prompt's internal representation, potentially moving it outside the "refusal region" learned during safety training.
- **Mechanism:** Safety alignment often teaches models to cluster representations of harmful requests to trigger a refusal. When sensitive words are replaced with emojis, the resulting hidden state deviates significantly from the raw harmful text cluster. This "representation gap" means the model does not recognize the intent as harmful at the semantic level required to trigger the refusal prefix.
- **Core assumption:** Safety boundaries are brittle and defined by specific semantic clusters rather than robust, abstract intent recognition.
- **Evidence anchors:** Visualizations (t-SNE) show emoji-substituted prompts "deviate... obviously" from raw harmful requests, PACT analysis confirms the log-probability of rejection tokens is suppressed.

### Mechanism 3: Pre-training Data Pollution
- **Claim:** The correlation between emojis and toxicity in pre-training corpora may precondition models to associate specific symbols with malicious or unstructured intent.
- **Mechanism:** In large-scale corpora (e.g., C4), emojis frequently appear in "polluted" contexts such as gambling, fraud, or low-quality social media data. This conditions the model to lower its "safety guard" when processing these symbols, as the attention mechanism retrieves associations related to toxicity or rule-breaking.
- **Core assumption:** The frequency of toxic co-occurrence in pre-training directly influences generation probability (exposure bias).
- **Evidence anchors:** Notes "potential correlation between the emoji-related data pollution with the toxicity generation behaviors," analysis of C4 found 32.8% of high-frequency emoji entries appeared in toxic contexts.

## Foundational Learning

- **Concept: Byte-Pair Encoding (BPE) & Vocabulary Coverage**
  - **Why needed here:** Understanding why emojis are "rare" tokens explains why they are fragmented into unreadable sub-tokens, creating the heterogeneous channel.
  - **Quick check question:** How does a tokenizer handle a unicode character not in its base vocabulary?

- **Concept: Log-Probability & PACT (Prompt Attribution and Contribution Tracking)**
  - **Why needed here:** This metric is used to quantify how much an emoji suppresses the probability of a refusal token ("Sorry") appearing.
  - **Quick check question:** If the log-probability of the token "I" drops when an emoji is added, what does that imply about the model's next likely word?

- **Concept: Representation Learning & Manifolds**
  - **Why needed here:** The paper relies on t-SNE plots to show that "harmful emoji prompts" and "harmful text prompts" form separate clusters in the latent space.
  - **Quick check question:** Why does a shift in the latent space representation matter for a classifier (like a safety refusal head)?

## Architecture Onboarding

- **Component map:** Input (Unicode) -> Tokenizer (Fragmentation) -> Embedding Layer (Sparse/Noisy Vectors) -> Transformer Layers (Attention Drift) -> Output Head (Suppressed Refusal Logits).

- **Critical path:** The tokenizer is the primary attack vector. The shift from a single atomic token (for a word) to multiple irregular sub-tokens (for an emoji) propagates a "noisy" representation through the network, failing to activate the safety refusal head.

- **Design tradeoffs:** Standard tokenizers optimize for compression efficiency on text, sacrificing semantic fidelity for symbols (emojis). This efficiency trade-off creates the security vulnerability.

- **Failure signatures:**
  1. **Tokenization:** High ratio of sub-tokens per emoji (>1).
  2. **Embedding:** Large cosine distance between "emoji prompt" and "text prompt" embeddings for the same intent.
  3. **Generation:** Low probability of refusal prefixes ("As an AI...") in the first 5 generated tokens.

- **First 3 experiments:**
  1. **Tokenization Audit:** Run the target model's tokenizer on the top 100 most used emojis. Count the average number of sub-tokens generated per emoji.
  2. **Ablation Study (Text vs Emoji):** Take the AdvBench dataset. Create two versions: one with emojis replacing nouns, and one with text. Measure the drop in "Harmfulness Ratio" (HR) when emojis are used.
  3. **Representation Distance:** Extract the last-token embeddings for "How to make a bomb" vs "How to make a [bomb emoji]". Calculate the cosine similarity to verify if they cluster together or drift apart.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanistic source of the internal representation gap between raw harmful prompts and their emoji-substituted counterparts?
- Basis in paper: [explicit] The authors state in Observation 2: "Emojis-substituted prompts deviate from their raw counterparts in the representation space, yet the source of this gap remains unclear."
- Why unresolved: While the paper identifies tokenization disparities as a contributing factor, it does not definitively map how specific token fragmentation translates into the high-dimensional representation shifts that bypass safety boundaries.
- What evidence would resolve it: A causal analysis (e.g., using activation patching or intervention) that forces emoji representations to map onto their textual counterparts to see if safety mechanisms are restored.

### Open Question 2
- Question: Is the relationship between emoji-related data pollution in pre-training corpora and toxicity generation causal or merely correlative?
- Basis in paper: [inferred] Section 5 states the authors "uncover potential correlation" and suggest the pollution "may facilitate" malicious intent, but they stop short of proving causation.
- Why unresolved: The analysis is observational; the paper examines static corpora and existing model behaviors rather than training models on cleaned vs. polluted data to isolate the effect.
- What evidence would resolve it: Pre-training or fine-tuning ablation studies where emoji-toxic co-occurrences are explicitly removed or augmented in the dataset to observe the resultant change in toxicity triggers.

### Open Question 3
- Question: How can safety alignment be extended to non-literal languages like emojis without impairing the model's ability to understand benign or playful emoji usage?
- Basis in paper: [inferred] The Conclusion states: "These findings emphasize the need for safety alignment that extends beyond literal languages as future works," implicitly asking how this alignment can be achieved.
- Why unresolved: The paper focuses on diagnosing the vulnerability and the mechanism of the attack but does not propose or test specific defensive training techniques.
- What evidence would resolve it: Developing a specialized alignment strategy (e.g., red-teaming with emoji prompts during RLHF) and demonstrating that it reduces the Harmfulness Ratio (HR) while maintaining performance on standard emoji comprehension benchmarks.

## Limitations
- The paper shows correlation between emoji substitution and increased toxicity but doesn't definitively prove causation between tokenization fragmentation and safety bypass
- Results rely on GPT-4o as both judge and prompt constructor, potentially introducing circularity and bias
- Findings are based on a specific set of 520 harmful prompts and 7 target models, limiting generalizability
- The paper doesn't test defensive mechanisms or propose concrete solutions to the identified vulnerability

## Confidence

**High Confidence:** The empirical observation that emoji-containing prompts lead to significantly higher harmful output scores compared to plain text is well-supported by the experimental results.

**Medium Confidence:** The interpretation that tokenization disparities create a heterogeneous semantic channel is plausible and supported by evidence, but the causal link between fragmentation and safety bypass is not definitively established.

**Low Confidence:** The claim that pre-training data pollution directly causes the toxicity generation behavior is the weakest link, as the analysis shows correlation but not causation.

## Next Checks

1. **Controlled Tokenization Ablation:** Create a synthetic tokenizer that assigns emojis either a single token or multiple tokens, and retrain a small language model on this tokenizer. Compare the model's toxicity generation on emoji-substituted prompts.

2. **Cross-Model Consistency Test:** Repeat the main experiment with a broader set of models, including those with different tokenizer designs (e.g., a model known to have a comprehensive emoji vocabulary).

3. **Data Pollution Intervention:** Train two models on the same corpus, but with one corpus having emoji-rich toxic data removed or down-weighted. Compare their toxicity generation on emoji-substituted prompts.