---
ver: rpa2
title: 'The Thinking Spectrum: An Empirical Study of Tunable Reasoning in LLMs through
  Model Merging'
arxiv_id: '2509.22034'
source_url: https://arxiv.org/abs/2509.22034
tags:
- merging
- reasoning
- arxiv
- direct
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive empirical study on
  leveraging model merging to generate a spectrum of large language models with tunable
  reasoning capabilities. By arithmetically combining weights of general-purpose (direct)
  and specialized (thinking) models, the authors demonstrate fine-grained control
  over the trade-off between reasoning accuracy and computational efficiency.
---

# The Thinking Spectrum: An Empirical Study of Tunable Reasoning in LLMs through Model Merging

## Quick Facts
- arXiv ID: 2509.22034
- Source URL: https://arxiv.org/abs/2509.22034
- Reference count: 10
- Primary result: Model merging creates tunable reasoning models with Pareto improvements over parents

## Executive Summary
This paper presents the first comprehensive empirical study on leveraging model merging to generate a spectrum of large language models with tunable reasoning capabilities. By arithmetically combining weights of general-purpose (direct) and specialized (thinking) models, the authors demonstrate fine-grained control over the trade-off between reasoning accuracy and computational efficiency. The study evaluates seven representative merging techniques across multiple reasoning benchmarks and two model scales, showing that model merging is surprisingly effective even when parent models have highly divergent parameter spaces, achieving tunable reasoning without catastrophic failure.

## Method Summary
The study merges Qwen3-Thinking and Qwen3-Instruct model pairs (4B and 30B-A3B scales) using seven techniques: Weighted Average, SLERP, DARE, TIES, EMR, LORE, and TWIN. Models are interpolated across λ∈[0,1] to create a spectrum from pure direct reasoning to pure thinking reasoning. The merged models are evaluated on AIME24, AIME25, HMMT25, GPQA-Diamond, and Creative Writing benchmarks, measuring reasoning accuracy and token efficiency. The research focuses on finding the optimal trade-off between reasoning quality and computational cost.

## Key Results
- Model merging achieves tunable reasoning capabilities without catastrophic failure, even with highly divergent parent models
- Pareto improvements frequently observed: merged models surpass thinking models in both accuracy and efficiency
- Simple linear interpolation (Weighted Average) performs comparably to complex geometric methods like SLERP
- Critical phase transition occurs around λ≈0.6-0.7 where reasoning capability emerges abruptly

## Why This Works (Mechanism)

### Mechanism 1: Mode Connectivity in Behavioral Space
Merging remains effective because the "Direct" (θ_direct) and "Thinking" (θ_think) models reside within a connected low-loss basin in the parameter space, despite significant behavioral differences. Arithmetic interpolation (linear or spherical) traverses a path between models that preserves functional integrity. The model does not collapse because the intervening parameter space does not contain a "high-loss ridge."

### Mechanism 2: Trajectory Approximation via Interpolation
Varying the merge strength (λ) approximates sampling intermediate checkpoints along the training trajectory from a direct model to a thinking model. Increasing λ is functionally analogous to "rolling back" the thinking model to a point where it learned to reason but had not yet optimized for excessive token generation ("over-reasoning"). This explains why merged models can achieve Pareto improvements (better accuracy, fewer tokens).

### Mechanism 3: Dense Global Coordination
The "Thinking" capability is not a sparse, modular plug-in but a dense, global shift in computational strategy requiring coordinated changes across most parameters. Merging works by blending these dense global adjustments. Methods relying on sparsity (like DARE) fail because they discard the majority of the necessary signal; conversely, simple averaging works because it preserves this density.

## Foundational Learning

- **Task Arithmetic**: Why needed: Merging is framed as adding a "reasoning task vector" (Δθ = θ_think - θ_base) to a base model. Understanding vector scaling is crucial to controlling the "reasoning dosage."
  - Quick check: How does scaling the task vector by λ < 1.0 affect the model's behavior compared to scaling by λ > 1.0?

- **Mode Connectivity**: Why needed: This explains why we can average two very different models without the model collapsing. It provides the theoretical safety net for this operation.
  - Quick check: If two models are in disconnected loss basins, what is the expected result of a simple weight average?

- **Pareto Efficiency**: Why needed: The paper highlights "Pareto Improvements" where merged models are strictly better than parents (more accurate, faster). Engineers must understand this non-zero-sum outcome to validate successful merges.
  - Quick check: If a merged model is more accurate but 2x slower, is it a Pareto improvement over the Thinking model?

## Architecture Onboarding

- **Component map**: θ_direct (Fast/Instruct) -> θ_think (Slow/Reasoning) -> Arithmetic Merger -> θ_merged(λ) -> Evaluation

- **Critical path**:
  1. Load parent weights (ensure architecture alignment)
  2. Calculate interpolation (e.g., theta_new = (1-lambda)*theta_direct + lambda*theta_think)
  3. Evaluate on specific benchmarks (AIME/HMMT for reasoning, GPQA for general integrity)

- **Design tradeoffs**:
  - Simple (Linear) vs. Geometric (SLERP): Linear is computationally cheaper; SLERP may better preserve feature norms but shows similar performance here
  - Sparse (DARE/TIES) vs. Dense: Sparse methods are risky here due to the dense nature of reasoning changes

- **Failure signatures**:
  - Catastrophic Forgetting: Loss of general chat capabilities (Creative Writing decline)
  - Style Bleed: Inclusion of stray reasoning tags in direct responses
  - Collapse: Outputting gibberish due to invalid parameter combinations

- **First 3 experiments**:
  1. **Sanity Check**: Perform a Linear Weighted Average sweep (λ: 0.0 → 1.0) on a small validation set to verify the accuracy-efficiency curve is smooth
  2. **Phase Change Localization**: Zoom in on the critical region (e.g., λ ∈ [0.6, 0.7]) identified in the paper to find the specific "emergence" point for your target benchmark
  3. **Robustness Test**: Apply an arbitrary "Top-K" merge to confirm if the model is robust to naive fusion strategies or if it requires principled merging

## Open Questions the Paper Calls Out

### Open Question 1
Can a formal mathematical framework be developed to explain the effectiveness of merging models with divergent computational strategies?
- Basis in paper: Listed in Section 6 as a primary future direction
- Why unresolved: Existing mode connectivity theory addresses knowledge domains, not the dense global parameter changes observed in behavioral shifts
- What evidence would resolve it: A theoretical model explaining functional interpolation despite large parameter distances

### Open Question 2
Do merging methods exist that yield a significantly better Pareto front than simple linear interpolation?
- Basis in paper: The authors explicitly ask this in the discussion, noting established methods performed similarly
- Why unresolved: The study found no single algorithm consistently dominated the accuracy-efficiency trade-off curve
- What evidence would resolve it: Discovery of a technique that consistently achieves higher accuracy with fewer tokens than linear averaging

### Open Question 3
Can the optimal merging weight be predicted for a given task without expensive empirical search?
- Basis in paper: Listed in Section 6; current methods require sweeping strengths to locate "phase changes"
- Why unresolved: The non-linear emergence of reasoning capability makes the optimal interpolation point difficult to anticipate
- What evidence would resolve it: A predictive heuristic for optimal lambda that avoids brute-force inference sweeps

## Limitations

- Generalization uncertainty: Results depend heavily on Qwen model family compatibility and may not transfer to other architectures
- Scale limitation: Analysis focuses on 4B and 30B scales, leaving uncertainty about performance at extremes
- Domain specificity: While effective for mathematical reasoning, applicability to other reasoning domains requires further validation

## Confidence

**High Confidence**: The empirical observation that model merging produces tunable reasoning models with smooth accuracy-efficiency trade-offs. The Pareto improvement results are statistically significant and reproducible across multiple benchmarks.

**Medium Confidence**: The hypothesis that simple averaging works because reasoning changes are dense and global. While supported by failure analysis of sparse methods, this contradicts some literature on sparse neural representations and requires further investigation.

**Low Confidence**: The theoretical claim that merged models approximate intermediate training checkpoints. This is a compelling narrative but lacks direct evidence from training dynamics or checkpoint analysis.

## Next Checks

1. **Architecture Transfer Test**: Validate merging effectiveness on a different model family (e.g., Llama or Mistral) to confirm the results are not Qwen-specific. This addresses the generalizability limitation.

2. **Sparse vs Dense Mechanism Test**: Systematically compare sparse (DARE, TIES) versus dense (Weighted Average, SLERP) merging across multiple reasoning domains to definitively establish whether reasoning behavior is indeed globally distributed or if certain reasoning types can be captured sparsely.

3. **Training Dynamics Validation**: Analyze intermediate checkpoints from the original training process of Qwen3-Thinking to empirically verify whether merged models at specific λ values correspond to actual training states, or whether the observed behavior emerges from different mechanisms.