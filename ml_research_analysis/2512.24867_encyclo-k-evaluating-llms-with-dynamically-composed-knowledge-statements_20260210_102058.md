---
ver: rpa2
title: 'Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements'
arxiv_id: '2512.24867'
source_url: https://arxiv.org/abs/2512.24867
tags:
- statements
- knowledge
- zhang
- wang
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Encyclo-K, a novel benchmark for evaluating
  large language models (LLMs) that addresses three limitations of existing benchmarks:
  vulnerability to data contamination, restriction to single-knowledge-point assessment,
  and reliance on costly domain expert annotation. Encyclo-K rethinks benchmark construction
  by treating individual knowledge statements, extracted from authoritative textbooks,
  as the unit of curation rather than questions.'
---

# Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements

## Quick Facts
- arXiv ID: 2512.24867
- Source URL: https://arxiv.org/abs/2512.24867
- Authors: Yiming Liang; Yizhi Li; Yantao Du; Ge Zhang; Jiayi Zhou; Yuchen Wu; Yinzhu Piao; Denghui Cao; Tong Sun; Ziniu Li; Li Du; Bo Lei; Jiaheng Liu; Chenghua Lin; Zhaoxiang Zhang; Wenhao Huang; Jiajun Zhang
- Reference count: 40
- Top model achieves only 62.07% accuracy on benchmark

## Executive Summary
Encyclo-K introduces a novel benchmark that addresses three key limitations of existing LLM evaluations: data contamination, single-knowledge-point assessment, and costly expert annotation. The benchmark treats individual knowledge statements extracted from authoritative textbooks as curation units, dynamically composing them into evaluation questions at test time. This design ensures contamination resistance through combinatorial statement composition, comprehensive assessment via multi-statement questions, and reduced annotation costs by requiring only formatting verification.

Experiments on over 50 LLMs demonstrate Encyclo-K's substantial challenges: even the top-performing OpenAI-GPT-5.1 achieves only 62.07% accuracy, with clear performance gradients across reasoning models (16.04%-62.07%) and chat models (9.71%-50.40%). The benchmark effectively differentiates capabilities while validating the effectiveness of dynamic evaluation and multi-statement comprehensive understanding.

## Method Summary
Encyclo-K constructs benchmark questions by dynamically composing knowledge statements extracted from 62 authoritative textbooks across 11 disciplines. The process involves three stages: (1) extracting 21,525 correct statements from textbook screenshots using vision-language models with format validation, (2) generating 21,494 incorrect statements by transforming correct ones through error types like concept substitution and causal inversion, and (3) programmatically sampling 8-10 statements into questions with 4-8 options combining 2-4 statements each. Questions are generated at test time using different random seeds to ensure contamination resistance.

## Key Results
- Even top model (OpenAI-GPT-5.1) achieves only 62.07% accuracy, demonstrating benchmark difficulty
- Reasoning models show 16.04%-62.07% accuracy range, while chat models range 9.71%-50.40%
- Multi-statement questions reduce accuracy by >20 percentage points compared to single-statement tasks
- Model rankings remain stable across different random seeds (Spearman correlation >0.95)

## Why This Works (Mechanism)

### Mechanism 1: Combinatorial Explosion for Contamination Resistance
Dynamically composing knowledge statements into questions at test time creates a combinatorial space too vast for memorization. The benchmark maintains a pool of 43,019 statements. Questions are generated by randomly sampling 8–10 statements and combining them into options with 2–4 statements each. The number of possible unique questions far exceeds any training corpus, making direct memorization infeasible.

### Mechanism 2: Multi-Statement Cognitive Load Amplification
Requiring joint comprehension of 8–10 statements per question imposes cognitive demands that expose model limitations beyond single-concept recall. Models must simultaneously hold multiple knowledge points in working memory, identify subtle errors across diverse domains, and synthesize judgments into coherent option selections. This causes >20 percentage point accuracy drops compared to single-statement verification.

### Mechanism 3: Reasoning Mode Scaling with Task Complexity
Chain-of-thought reasoning provides increasing benefits as model scale grows, particularly for multi-statement integration tasks. The paper observes that thinking mode advantage scales with model capacity—from 0.52 percentage points at 0.6B to 9.47 percentage points at 32B parameters. Larger models possess greater reasoning potential that remains latent without explicit elicitation.

## Foundational Learning

- **Concept: Data Contamination in Benchmarks**
  - Why needed here: Encyclo-K's primary motivation is addressing benchmark contamination—when test questions appear in training corpora, inflating performance metrics.
  - Quick check question: If a model scores 95% on a benchmark but 60% on dynamically generated variants of the same knowledge, what does this suggest about contamination?

- **Concept: Multi-Hop vs. Multi-Statement Reasoning**
  - Why needed here: Encyclo-K tests multi-statement comprehension (simultaneous verification of 8–10 statements), which differs from multi-hop reasoning (chaining dependencies across statements).
  - Quick check question: Does Encyclo-K require reasoning about relationships between statements, or independent verification of each statement?

- **Concept: Combinatorial Complexity and Memorization Bounds**
  - Why needed here: The contamination resistance claim depends on combinatorial mathematics—the number of possible questions vastly exceeds training data scale.
  - Quick check question: Given 43,019 statements, 8–10 per question, and 4–8 options combining 2–4 statements each, is the combinatorial space larger than typical pre-training corpora (~trillions of tokens)?

## Architecture Onboarding

- **Component map:**
  Statement Collection Pipeline (62 textbooks → screenshot capture → MLLM extraction → format validation → 21,525 correct statements) -> Incorrect Statement Generator (DeepSeek-R1 transforms correct statements → applies error taxonomy → 21,494 incorrect statements) -> Dynamic Question Assembler (Programmatically samples statements → populates templates → assigns option letters → outputs JSON-formatted questions) -> Answer Extraction Module (Multi-tier regex matching → fallback to option content search → marks unextractable responses as "miss")

- **Critical path:** Statement quality is the foundation—incorrect statements must be definitively wrong yet professionally deceptive. The paper reports 5/200 sampled incorrect statements had error-reason mismatches but remained factually incorrect, which was deemed acceptable.

- **Design tradeoffs:**
  - Statement granularity: Finer granularity increases combinatorial flexibility but may lose contextual dependencies; coarser granularity preserves context but reduces diversity
  - Option count: More options increase difficulty (64%→45% accuracy from 4→10 options for DeepSeek-R1) but also increase evaluation time
  - Language coverage: Current implementation is English/Chinese only; expansion requires language-specific textbook sources

- **Failure signatures:**
  - Lazy answering: The 14B model generated only 40.73 tokens under non-thinking mode, skipping analysis—detectable via response length monitoring
  - Position bias: Tested and not found (Table 5 shows stable accuracy across positions A–H)
  - Format extraction failures: Handled by fallback mechanisms; track "miss" rates to identify systematic issues

- **First 3 experiments:**
  1. Validate ranking stability: Generate 5+ question sets with different random seeds, verify Spearman correlation of model rankings >0.95
  2. Calibrate difficulty scaling: Sweep option count from 4–10, confirm monotonic accuracy decrease and widening gap between stronger/weaker models
  3. Establish single vs. multi-statement baseline: Compare performance on statement-level true/false vs. full multi-statement questions, expect >15 percentage point drop

## Open Questions the Paper Calls Out

### Open Question 1
How does the Encyclo-K framework perform when expanded to low-resource languages or domains lacking standardized "authoritative textbooks" suitable for statement extraction? The authors list covering only English/Chinese and a constrained set of disciplines as a limitation, aiming to build a "multilingual, encyclopedic-scale evaluation framework."

### Open Question 2
To what extent does the capability level of the "distractor generator" model (e.g., DeepSeek-R1) influence the discriminative power and validity of the benchmark? Section 3.1.2 relies on a specific strong model (DeepSeek-R1) to generate deceptive incorrect statements, but does not analyze if weaker generators would produce trivially detectable errors.

### Open Question 3
Does the strict requirement for statement "independence and completeness" systematically exclude valid knowledge that inherently relies on cross-referencing or broader context? Section 3.1.1 explicitly filters out statements with chapter references or formulas requiring external context, potentially altering the nature of deep reasoning tasks.

## Limitations

- Textbook corpus opacity: The specific 62 textbooks used for knowledge extraction are not listed, making exact replication dependent on similar but non-identical source materials
- Statement quality variance: Incorrect statement generation relies on a single model (DeepSeek-R1) without cross-validation, potentially introducing systematic biases
- Evaluation pipeline complexity: The multi-tier answer extraction system introduces potential failure points, with "miss" rates reported but not fully characterized across different model types

## Confidence

- **High confidence** in contamination resistance claims: The combinatorial space analysis is mathematically sound, and empirical validation across multiple random seeds demonstrates stable model rankings
- **Medium confidence** in multi-statement cognitive load claims: The >20 percentage point accuracy drop is empirically demonstrated, but the mechanism could involve format complexity rather than genuine comprehension difficulty
- **Medium confidence** in reasoning mode scaling: The scaling relationship with model size is clearly observed, but the 14B model anomaly suggests evaluation settings significantly impact results

## Next Checks

1. **Cross-seed stability verification**: Generate 10+ question sets with different random seeds and compute Spearman correlation of model rankings to confirm the >0.95 stability threshold
2. **Format complexity control**: Create matched single-statement variants of multi-statement questions to isolate comprehension difficulty from format complexity effects
3. **Statement generation validation**: Implement independent incorrect statement generation using multiple models and cross-validate error taxonomy consistency across different generators