---
ver: rpa2
title: Scalable h-adaptive probabilistic solver for time-independent and time-dependent
  systems
arxiv_id: '2508.09623'
source_url: https://arxiv.org/abs/2508.09623
tags:
- points
- collocation
- posterior
- learning
- active
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a scalable h-adaptive probabilistic solver
  for time-independent and time-dependent systems, addressing the computational bottleneck
  of Gaussian process-based probabilistic numerical methods for partial differential
  equations (PDEs). The key innovations are: (1) a stochastic dual descent (SDD) algorithm
  that reduces the per-iteration complexity from cubic to linear in the number of
  collocation points, enabling tractable inference for large-scale problems; and (2)
  a clustering-based active learning strategy that adaptively selects collocation
  points to maximize information gain while minimizing computational expense.'
---

# Scalable h-adaptive probabilistic solver for time-independent and time-dependent systems

## Quick Facts
- arXiv ID: 2508.09623
- Source URL: https://arxiv.org/abs/2508.09623
- Authors: Akshay Thakur; Sawan Kumar; Matthew Zahr; Souvik Chakraborty
- Reference count: 27
- Primary result: Achieves linear per-iteration complexity vs cubic for Gaussian process PDE solvers while maintaining uncertainty quantification

## Executive Summary
This paper introduces a scalable h-adaptive probabilistic solver that addresses the computational bottleneck of Gaussian process-based probabilistic numerical methods for partial differential equations. The key innovations are a stochastic dual descent (SDD) algorithm that reduces per-iteration complexity from cubic to linear in the number of collocation points, and a clustering-based active learning strategy for adaptive collocation point selection. The method demonstrates superior performance on benchmark problems including Poisson's equation and time-dependent heat equations, achieving relative MSE of 1.21%, 1.41%, and 1.11% respectively, while significantly reducing computational cost and memory requirements compared to exact Gaussian process solvers.

## Method Summary
The proposed solver combines Gaussian process regression with adaptive collocation and stochastic optimization to make probabilistic PDE solving tractable for large-scale problems. The core approach uses a stochastic dual descent algorithm that approximates the Gaussian process inference by optimizing over a dual space, reducing the computational complexity from O(nÂ³) to O(n) per iteration where n is the number of collocation points. A clustering-based active learning strategy iteratively selects new collocation points by maximizing information gain while minimizing computational expense, balancing exploration and exploitation in the solution space. The method is validated across three benchmark problems spanning 2D elliptic PDEs, 3D elliptic PDEs, and time-dependent parabolic PDEs.

## Key Results
- Achieves relative MSE of 1.21% on Poisson's equation in circular disk domain
- Demonstrates 1.41% relative MSE for 3D Poisson's equation
- Solves time-dependent heat equation with 1.11% relative MSE
- Reduces memory consumption and computational time compared to exact Gaussian process solvers
- Outperforms physics-informed neural networks on benchmark problems

## Why This Works (Mechanism)
The solver's effectiveness stems from two key mechanisms: (1) the stochastic dual descent algorithm that approximates Gaussian process inference through iterative optimization in dual space, avoiding the cubic complexity of exact inference; and (2) the clustering-based active learning that strategically selects collocation points to maximize information gain while minimizing computational overhead. The combination allows the method to maintain the uncertainty quantification benefits of Gaussian processes while achieving practical scalability for real-world PDE applications.

## Foundational Learning
- **Gaussian Process Regression**: Probabilistic non-parametric regression framework that provides uncertainty quantification for PDE solutions; needed because exact GP inference is computationally prohibitive for large problems
- **Stochastic Dual Descent**: Optimization algorithm that works in dual space to approximate GP inference with reduced complexity; needed to avoid cubic scaling of exact GP methods
- **Active Learning**: Strategy for selecting informative data points during inference; needed to balance computational efficiency with solution accuracy
- **Collocation Methods**: Numerical approach that enforces PDE constraints at discrete points; needed as the computational framework for the solver
- **Adaptive Mesh Refinement**: Technique for dynamically adjusting discretization based on solution features; needed to focus computational resources where most needed

## Architecture Onboarding
**Component Map**: GP Prior -> SDD Optimization -> Active Learning -> Collocation Points -> PDE Solver
**Critical Path**: The most computationally intensive path is the SDD optimization loop, where each iteration involves computing kernel matrices and gradients with respect to collocation points
**Design Tradeoffs**: The method trades exact GP inference for approximate inference via SDD, gaining computational efficiency at the cost of potential approximation errors. The active learning strategy trades exploration of the full solution space for computational efficiency by focusing on high-information regions.
**Failure Signatures**: Poor performance on problems with sharp gradients or discontinuities, convergence issues in the SDD algorithm for ill-conditioned problems, and suboptimal collocation point selection leading to local minima trapping.
**3 First Experiments**: 1) Test SDD convergence on synthetic 1D problems with known solutions to validate the approximation accuracy; 2) Compare collocation point selection strategies on simple PDEs to evaluate active learning effectiveness; 3) Benchmark computational scaling on increasing problem sizes to verify linear complexity claims.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Computational complexity reduction claims do not account for total iterations needed for convergence, which could negate per-iteration advantages
- Clustering-based active learning may introduce bias in point selection affecting accuracy for problems with complex geometries or sharp gradients
- Performance metrics based on limited benchmark problems raise questions about generalizability to more challenging PDE systems

## Confidence
- High: The overall framework of combining Gaussian processes with adaptive collocation and basic computational improvements are well-established concepts
- Medium: Scalability claims require further validation; the transition from cubic to linear complexity per iteration is significant but total computational cost analysis is incomplete
- Low: The clustering-based active learning strategy's effectiveness across different PDE classes is not well-established

## Next Checks
1. Conduct systematic convergence analysis of the SDD algorithm across different problem sizes and types, comparing total computational cost against exact Gaussian process solvers and other approximation methods
2. Apply the solver to a broader range of PDE problems including those with sharp gradients, discontinuities, or complex geometries to assess clustering strategy robustness
3. Perform detailed analysis of uncertainty estimates produced by the solver, comparing against ground truth uncertainties where available to validate probabilistic predictions