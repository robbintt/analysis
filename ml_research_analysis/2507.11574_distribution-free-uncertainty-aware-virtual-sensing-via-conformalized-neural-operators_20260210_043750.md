---
ver: rpa2
title: Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural
  Operators
arxiv_id: '2507.11574'
source_url: https://arxiv.org/abs/2507.11574
tags:
- spatial
- coverage
- prediction
- uncertainty
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Conformalized Monte Carlo Operator (CMCO),
  a framework that integrates Monte Carlo dropout with split conformal prediction
  to provide calibrated, distribution-free uncertainty estimates for neural operator
  models. CMCO enables spatial uncertainty quantification for virtual sensing tasks
  by producing confidence intervals without retraining, ensembling, or custom loss
  functions.
---

# Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural Operators

## Quick Facts
- arXiv ID: 2507.11574
- Source URL: https://arxiv.org/abs/2507.11574
- Authors: Kazuma Kobayashi; Shailesh Garg; Farid Ahmed; Souvik Chakraborty; Syed Bahauddin Alam
- Reference count: 40
- One-line primary result: CMCO achieves near-nominal 95% empirical coverage for neural operator uncertainty quantification across three scientific domains without retraining or ensembling.

## Executive Summary
This paper introduces CMCO (Conformalized Monte Carlo Operator), a framework that combines Monte Carlo dropout with split conformal prediction to provide calibrated, distribution-free uncertainty estimates for neural operator models. The method enables spatial uncertainty quantification for virtual sensing tasks by producing confidence intervals without retraining, ensembling, or custom loss functions. Evaluated on three diverse applications—lid-driven cavity flow, elastoplastic deformation, and cosmic radiation dose estimation—CMCO consistently achieves near-nominal 95% empirical coverage, even in scenarios with strong spatial gradients or sparse sensor inputs.

## Method Summary
CMCO integrates MC-dropout with split conformal prediction within a DeepONet architecture. The DeepONet uses a branch network (GRU/LSTM) to encode input functions and a trunk network (FCN) to encode spatial coordinates, with predictions via inner product. During inference, MC-dropout keeps dropout active across 10 forward passes to estimate predictive mean μ and standard deviation σ. A held-out calibration dataset computes normalized nonconformity scores to determine a quantile-based multiplier q. Final prediction intervals are μ ± 1.96·q·σ, achieving distribution-free coverage guarantees without distributional assumptions.

## Key Results
- CMCO achieves near-nominal 95% empirical coverage across three scientific domains
- Method maintains minimal computational overhead compared to baseline DeepONet
- Consistent performance in high-gradient regions and sparse sensor scenarios
- Failure rate analysis shows robustness with coverage dropping only locally in extreme cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Monte Carlo dropout provides a computationally efficient approximation of predictive uncertainty by treating dropout at inference as variational sampling.
- Mechan: During training, dropout randomly masks neurons as regularization. At inference, MC-dropout keeps dropout active across multiple forward passes (nc passes), producing different outputs for the same input. The ensemble mean μ(u) and standard deviation σ(u) are computed elementwise, yielding a heuristic uncertainty estimate without retraining.
- Core assumption: The dropout-induced stochasticity approximates sampling from a posterior over model weights, which Gal & Ghahramani (2016) showed is an approximate variational Bayesian inference—though the paper explicitly notes this is "not exact."
- Evidence anchors:
  - [Section 2.2]: "MC-dropout combines the MC sampling and dropout regularization... during inference, multiple forward passes are made for the same input while keeping the dropout mask active."
  - [Section 2.2]: "MC-dropout, when used within the DeepONet architecture, will produce output ŷp(i)(u) = Gθ(u)(y), for ith forward pass."
  - [corpus]: Weak direct support; related work on conformal PINNs (arXiv:2509.13717) uses similar MC-dropout reasoning but does not validate the Bayesian approximation quality.
- Break condition: If dropout rate is too low (insufficient network stochasticity) or forward passes are too few, the variance estimate becomes unreliable; the paper uses p=0.1 and nc=10.

### Mechanism 2
- Claim: Split conformal prediction converts heuristic uncertainty estimates into prediction intervals with finite-sample marginal coverage guarantees, without distributional assumptions.
- Mechan: A held-out calibration dataset {(ui, yi)} computes normalized nonconformity scores ei,j = |yi,j − μj(ui)| / σj(ui). The (1−α) quantile of these scores defines a calibration multiplier qj. Final intervals are μ(ut) ± z·q·σ(ut), where z=1.96 approximates a Gaussian multiplier. Under exchangeability, these intervals achieve ≥(1−α) coverage.
- Core assumption: Calibration and test data are exchangeable (i.i.d. or close to it). If this fails—e.g., distribution shift between calibration and deployment—coverage guarantees degrade.
- Evidence anchors:
  - [Abstract]: "By unifying Monte Carlo dropout with split conformal prediction in a single DeepONet architecture, CMCO achieves spatially resolved uncertainty estimates."
  - [Section 2.3]: "The resulting quantile vector q ∈ Rne is reshaped to match the output grid... The final calibrated prediction interval for each test input ut is then given by [Eq. 6]."
  - [corpus]: Conformalized Decision Risk Assessment (arXiv:2505.13243) confirms that conformal prediction provides distribution-free coverage but highlights sensitivity to exchangeability violations.
- Break condition: Systematic distribution shift (e.g., operating conditions outside training range) breaks exchangeability; the paper's Case III shows minimum coverage of 1.14% on some samples, indicating localized failures.

### Mechanism 3
- Claim: DeepONet's branch-trunk decomposition enables flexible handling of sparse, non-collocated, and multi-modal sensor inputs while allowing queries at arbitrary spatial locations.
- Mechan: The branch network encodes input functions u (e.g., time-series sensor readings) into a latent vector b(u). The trunk network encodes query coordinates r into t(r). The prediction is their inner product: Gθ(u)(r) = b(u)·t(r). This decouples input representation from output resolution.
- Core assumption: The inner product of branch and trunk embeddings sufficiently captures the operator's functional mapping; this relies on the universal approximation theorem for operators (Chen & Chen, 1995), but practical performance depends on architecture depth, activation choices, and data coverage.
- Evidence anchors:
  - [Section 2.1]: "DeepONet learns this mapping using a two-network architecture... The branch net receives the input function u evaluated at a fixed set of sensor locations... The trunk net takes a query location r ∈ Ω as input."
  - [Section 1]: "These properties make it especially well-suited for deployment in dynamic environments with disjoint input-output domains."
  - [corpus]: Active operator learning (arXiv:2503.03178) notes that operator architectures generalize better than CNNs for PDE solution operators but require sufficient training data across the input function space.
- Break condition: If input functions lie far outside the training distribution, or if trunk network capacity is insufficient for high-frequency spatial features, predictions degrade without warning (seen in Case I where high-gradient regions undercovered).

## Foundational Learning

- **Concept: Monte Carlo Dropout as Approximate Bayesian Inference**
  - Why needed here: Understanding why dropout at inference produces uncertainty estimates, and why these require calibration.
  - Quick check question: If you run MC-dropout with only 2 forward passes, what happens to your variance estimate?

- **Concept: Split Conformal Prediction and Exchangeability**
  - Why needed here: The coverage guarantees of CMCO depend on conformal prediction theory; knowing what breaks these guarantees is critical for deployment.
  - Quick check question: Does conformal prediction require the data to be Gaussian? What assumption does it require instead?

- **Concept: Operator Learning vs. Supervised Regression**
  - Why needed here: CMCO builds on DeepONet, which learns mappings between function spaces—not just input-output pairs. This distinction affects data preparation and model design.
  - Quick check question: Why can a DeepONet predict at spatial locations not seen during training, while a standard CNN cannot?

## Architecture Onboarding

- **Component map:**
  - Input sequences → GRU branch network (4 layers, 256 hidden, dropout 0.1, layer norm) → latent vector b(u)
  - Spatial coordinates → FCN trunk network (4 layers, 128–512 neurons, Tanh/ReLU, dropout 0.1) → latent vector t(r)
  - Branch vector · Trunk vector + bias → scalar prediction per location
  - Dropout layers (p=0.1) in both branch and trunk, kept active at inference
  - Conformal calibrator: post-hoc module computing quantiles from calibration set nonconformity scores

- **Critical path:**
  1. Train deterministic DeepONet on input-output pairs using MSE loss.
  2. Hold out calibration dataset (do not use for training).
  3. For each calibration sample: run nc forward passes with dropout enabled → compute μ, σ → compute normalized nonconformity scores.
  4. Compute (1−α) quantile of nonconformity scores.
  5. At test time: run nc forward passes → construct interval μ ± z·q·σ.

- **Design tradeoffs:**
  - **Dropout rate (p):** Higher p increases uncertainty spread but may degrade point prediction accuracy; paper uses 0.1.
  - **Number of MC passes (nc):** More passes improve variance estimate stability but increase inference latency; paper uses 10.
  - **Calibration set size:** Larger sets yield more stable quantiles but reduce training data; paper allocates ~15–20% of data for calibration.
  - **Gaussian multiplier (z=1.96):** Assumes near-Gaussian residuals; the conformal quantile q compensates if this is violated, but extreme asymmetry may require quantile-specific bounds.

- **Failure signatures:**
  - **Systematic undercoverage (coverage ≪ 95%):** Likely distribution shift between calibration and test; check feature distributions.
  - **Extremely wide intervals:** MC-dropout variance is large relative to error; model may be undertrained or architecture insufficient.
  - **Localized undercoverage in high-gradient regions:** Trunk network may lack capacity to capture fine spatial features; consider deeper trunk or different activations.
  - **Case III minimum coverage of 1.14%:** Indicates that even with conformal calibration, certain test samples (e.g., atypical solar conditions) fall outside the exchangeability assumption.

- **First 3 experiments:**
  1. **Baseline coverage test:** Train DeepONet on a subset of data, run MC-dropout with nc=10, compute uncalibrated coverage. Then apply split conformal calibration and verify coverage approaches 95% on held-out test set.
  2. **Ablation on dropout rate:** Compare p∈{0.05, 0.1, 0.2, 0.3} while keeping nc=10 constant. Measure tradeoff between interval width and point prediction error.
  3. **Distribution shift robustness:** Deliberately shift test data (e.g., use boundary conditions outside training range) and observe how quickly coverage degrades. This establishes practical limits of the exchangeability assumption for your application.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can adaptive calibration techniques be integrated into CMCO to ensure valid conditional coverage in regions with high spatial gradients or sparse sensor data?
- Basis in paper: [explicit] The Conclusion states future work will "explore adaptive calibration techniques that account for local data density and spatial heterogeneity."
- Why unresolved: The current split conformal prediction guarantees marginal coverage, but results show localized failures (e.g., coverage drops to 53% in Case II) in regions with high gradients.
- What evidence would resolve it: A modified scheme that maintains target coverage levels when evaluated specifically on sub-domains with high prediction variance or sparse inputs.

### Open Question 2
- Question: Can the CMCO framework be extended to quantify uncertainty for time-dependent targets where uncertainty must propagate through sequential dynamics?
- Basis in paper: [explicit] The Conclusion proposes extensions to "time-dependent targets."
- Why unresolved: The current applications map input histories to a field at a final time step, without providing uncertainty bounds for the trajectory of the solution over time.
- What evidence would resolve it: Application of CMCO to transient dynamical systems showing calibrated prediction intervals at intermediate time steps without requiring re-calibration.

### Open Question 3
- Question: Why does CMCO fail to maintain coverage for specific outlier test samples (e.g., Case III's 1.14% minimum coverage), and how does calibration set composition influence these failures?
- Basis in paper: [inferred] While average coverage is near-nominal, the results show extreme local under-coverage (e.g., 1.14% in Case III), indicating the method struggles with specific distribution shifts or "atypical conditions."
- Why unresolved: The paper identifies that undercoverage correlates with high prediction error but does not isolate if the cause is insufficient calibration data diversity or inherent model bias.
- What evidence would resolve it: An ablation study analyzing the correlation between test sample "atypicality" (distance from calibration manifold) and failure rate.

## Limitations

- Distribution shifts between calibration and test data can cause severe undercoverage despite calibration
- Method inherits DeepONet's dependence on sufficient training data across the input function space
- MC-dropout's Bayesian approximation is acknowledged as inexact, limiting theoretical guarantees
- Data and code availability remain pending, limiting reproducibility verification

## Confidence

- **High confidence:** The mechanism of combining MC-dropout with split conformal prediction for distribution-free coverage; the architecture and training protocol for DeepONet; the use of calibration quantiles to adjust uncertainty intervals.
- **Medium confidence:** The practical sufficiency of 10 MC forward passes and p=0.1 dropout for stable uncertainty estimates; the robustness of coverage under moderate distribution shifts; the adequacy of the z=1.96 multiplier across all tested domains.
- **Low confidence:** Long-term stability of coverage under evolving or adversarial data distributions; exact coverage guarantees in the presence of strong model misspecification or input function space extrapolation.

## Next Checks

1. **Distribution shift robustness:** Systematically evaluate coverage degradation when calibration and test sets are drawn from increasingly different parameter ranges or environmental conditions (e.g., varying Reynolds numbers, material properties, or solar activity levels).

2. **Ablation on MC samples and dropout rate:** Quantify the tradeoff between number of MC forward passes (nc) and dropout probability (p) on both interval width and coverage accuracy, identifying the minimal reliable configuration.

3. **Trunk network capacity sensitivity:** Test whether increasing trunk network depth or width improves coverage in high-gradient spatial regions, isolating architecture limitations from calibration issues.