---
ver: rpa2
title: Tensor Train Quantum State Tomography using Compressed Sensing
arxiv_id: '2506.23560'
source_url: https://arxiv.org/abs/2506.23560
tags:
- quantum
- tensor
- matrix
- states
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel quantum state tomography (QST) approach
  that parameterizes density matrices using a block tensor train (Block-TT) decomposition.
  The key innovation is leveraging this tensor network structure to address the exponential
  growth of parameters in standard QST methods.
---

# Tensor Train Quantum State Tomography using Compressed Sensing

## Quick Facts
- arXiv ID: 2506.23560
- Source URL: https://arxiv.org/abs/2506.23560
- Reference count: 38
- Primary result: Novel Block-TT parameterization enables QST with logarithmic parameter scaling vs exponential for matrix-based methods

## Executive Summary
This paper introduces a quantum state tomography method that leverages block tensor train (Block-TT) decomposition to parameterize density matrices. The approach addresses the exponential growth of parameters in standard QST by exploiting the inherent tensor structure of quantum states, achieving logarithmic scaling in parameter count relative to system size. The Block-TT representation naturally preserves positive semi-definiteness and unit trace constraints while enabling efficient gradient-based optimization. Numerical experiments demonstrate competitive performance with state-of-the-art methods on 8-qubit systems, with the added benefit of linear measurement scaling that enables tomography for larger systems up to 12 qubits.

## Method Summary
The proposed method formulates QST as a non-convex optimization over Block-TT factors using gradient descent with TT-SVD projection for left-orthogonalization. The objective minimizes the measurement discrepancy between estimated and true states while constraining the Frobenius norm of the TT core tensor. The approach requires generating Pauli measurements in tensor train matrix (TTM) format and iteratively updating the Block-TT representation through gradient steps followed by projection to maintain the TT structure. The final density matrix estimate is reconstructed from the optimized Block-TT factors, with normalization performed via the last core to ensure trace preservation.

## Key Results
- 8-qubit Block-TT QST achieves fidelity of 0.2-0.8 comparable to state-of-the-art methods
- Measurement scaling nearly linear with system size vs exponential for matrix-based approaches
- Method enables QST for systems up to 12 qubits where traditional methods become intractable
- At low sampling ratios (M/D² < 0.3), CVX performs best but lacks scalability for larger systems

## Why This Works (Mechanism)
The Block-TT parameterization exploits the tensor network structure inherent in quantum states, representing high-dimensional density matrices as products of lower-dimensional core tensors. This decomposition captures the entanglement structure while dramatically reducing the number of parameters from exponential to logarithmic scaling. The gradient-based optimization over TT factors, combined with projection steps, efficiently navigates the constrained optimization landscape while preserving the physical constraints of quantum states. The method's success relies on the assumption that realistic quantum states have low TT-rank structure, which is often the case for physically relevant states.

## Foundational Learning
- **Tensor Train Decomposition**: Factorizes high-dimensional tensors into sequential products of lower-rank cores; needed to understand the parameterization and why it reduces complexity; quick check: verify TT-rank scales sub-exponentially for typical quantum states
- **Quantum State Tomography**: Process of reconstructing density matrices from measurement data; needed to understand the problem context and constraints; quick check: confirm trace preservation and positive semi-definiteness requirements
- **Compressed Sensing Framework**: Uses fewer measurements than degrees of freedom by exploiting structure; needed to understand the theoretical justification for sample efficiency; quick check: verify RIP-like conditions hold for the measurement ensemble
- **Gradient Descent with Constraints**: Optimization technique with projection steps; needed to understand the algorithmic approach; quick check: monitor constraint violations during optimization

## Architecture Onboarding

**Component Map**: Generate Block-TT → Generate Pauli Measurements → Gradient Descent with PrTT → Density Matrix Reconstruction

**Critical Path**: Core generation → measurement simulation → optimization loop (gradient step + projection + normalization) → fidelity evaluation

**Design Tradeoffs**: Low TT-rank enables efficiency but may fail for highly entangled states; gradient descent is simple but may converge slowly vs Riemannian methods; projection maintains structure but introduces truncation errors

**Failure Signatures**: TT-rank explosion during optimization, constraint violations (Tr(ρ̂) > 1), poor convergence on states not well-approximated by low TT-rank

**First Experiments**:
1. Implement Block-TT tensor generation and Pauli measurement simulation on 4-qubit system
2. Run gradient descent with TT-SVD projection and verify constraint preservation
3. Compare fidelity and trace distance against CVX baseline on product states

## Open Questions the Paper Calls Out

**Open Question 1**: What are the theoretical recovery guarantees for the proposed non-convex Block-TT factorization, and does it converge to the global optimum? The paper relies on numerical experiments without mathematical proofs regarding convergence conditions or sample complexity bounds relative to tensor rank.

**Open Question 2**: Can optimization over fixed-rank Tensor Train manifolds (Riemannian optimization) significantly improve convergence speed and accuracy over the simple gradient descent with projection used in this study? The current approach uses basic gradient descent followed by TT-SVD projection, which may accumulate truncation errors.

**Open Question 3**: Can structured tensor completion approaches be adapted to this Block-TT framework to enable QST with deterministic, rather than probabilistic, recovery guarantees? The current approach frames QST as a compressed sensing (optimization) problem which offers probabilistic guarantees, leaving deterministic reconstruction unexplored.

## Limitations
- Critical hyperparameters (learning rate, momentum, iterations) not specified, hindering reproduction
- Method's effectiveness depends on assumption of low TT-rank structure, which may not hold for highly entangled states
- Experimental design limited to specific sampling ratios and noise levels without systematic exploration of broader parameter ranges

## Confidence
- Parameter Tuning Uncertainty: Low Confidence - hyperparameters not specified
- Structural Assumptions: Medium Confidence - effectiveness depends on low TT-rank assumption
- Baseline Comparison: Medium Confidence - limited exploration of parameter ranges and lack of runtime measurements

## Next Checks
1. Systematically vary learning rate η and momentum parameters to establish robust performance bounds and identify optimal settings
2. Test Block-TT performance on states with known TT-rank scaling (GHZ states, random states) to quantify when the method succeeds/fails
3. Measure actual computational time for all methods across 8-12 qubit systems to validate claimed linear scaling advantage