---
ver: rpa2
title: 'Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under Compute
  Budgets'
arxiv_id: '2510.20609'
source_url: https://arxiv.org/abs/2510.20609
tags:
- retrieval
- code
- context
- bm25
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We benchmark retrieval design choices for code-focused RAG on\
  \ two complementary tasks \u2014 PL\u2192PL code completion and NL\u2192PL bug localization\
  \ \u2014 under varying context windows and compute budgets. We compare chunking\
  \ strategies (whole file, fixed-line, syntax-aware), similarity scorers (BM25, IoU,\
  \ dense encoders, structure-aware), and splitting granularity (line/word/BPE tokens)\
  \ across four context lengths (128 to 16,000 tokens)."
---

# Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under Compute Budgets

## Quick Facts
- arXiv ID: 2510.20609
- Source URL: https://arxiv.org/abs/2510.20609
- Reference count: 22
- No single retrieval configuration works best for all tasks

## Executive Summary
This paper benchmarks retrieval design choices for code-focused RAG systems across two complementary tasks: PL→PL code completion and NL→PL bug localization. The study evaluates chunking strategies, similarity scorers, and splitting granularity under varying context windows and compute budgets. Results show that optimal configurations are highly task-dependent, with BM25 excelling for code completion while dense encoders dominate bug localization. The findings provide evidence-based guidelines for designing task- and budget-aware code-oriented RAG systems.

## Method Summary
The authors evaluate retrieval design choices for code-focused RAG on two tasks: PL→PL code completion and NL→PL bug localization. They compare chunking strategies (whole file, fixed-line, syntax-aware), similarity scorers (BM25, IoU, dense encoders, structure-aware), and splitting granularity (line/word/BPE tokens) across four context lengths (128 to 16,000 tokens). Performance is measured using exact match and F1 metrics, with latency and throughput tracked for each configuration. The study uses benchmarks like MBXP, POJ-104, CodeXGLUE, and BugsInPy to test retriever effectiveness under varying computational constraints.

## Key Results
- For code completion, sparse BM25 with word-level splitting is most effective and 10-200× faster than dense alternatives
- For bug localization, proprietary dense encoders (Voyager-3 family) outperform sparse methods but require 100× larger latency
- Optimal chunk size scales with context: 32-64 line chunks suit small budgets (≤4K tokens), while whole-file retrieval matches smaller chunks at 16K tokens

## Why This Works (Mechanism)
The task-specific retrieval superiority stems from the fundamental differences between code completion and bug localization. Code completion benefits from fast, pattern-matching retrieval that BM25 provides, as it can quickly identify relevant code patterns and snippets. Bug localization requires deeper semantic understanding that dense encoders excel at, despite their higher computational cost. The chunk size scaling with context works because larger contexts allow whole-file retrieval to capture broader code relationships, while smaller contexts benefit from focused, smaller chunks that preserve local code structure.

## Foundational Learning

**Code Chunking**: Breaking code into manageable pieces for retrieval
- Why needed: Code files are often too large for context windows
- Quick check: Measure impact on retrieval accuracy vs. chunk size

**Similarity Scoring**: Methods to match queries with relevant code
- Why needed: Different scoring methods capture different aspects of code similarity
- Quick check: Compare precision-recall curves across scoring methods

**Context Window Management**: Handling varying input sizes
- Why needed: Different tasks and budgets require different context capacities
- Quick check: Test performance degradation as context window decreases

**Compute Budget Trade-offs**: Balancing accuracy and speed
- Why needed: Real-world deployment requires consideration of latency and throughput
- Quick check: Measure latency vs. accuracy trade-offs across configurations

## Architecture Onboarding

**Component Map**: Query -> Retriever -> Chunking Strategy -> Scoring Method -> Context Window -> Output

**Critical Path**: The retriever component is the bottleneck, with dense encoders taking 100× longer than BM25 for bug localization tasks.

**Design Tradeoffs**: Accuracy vs. latency is the primary tradeoff, with task-specific requirements determining the optimal balance.

**Failure Signatures**: Dense encoders fail when computational resources are limited, while BM25 struggles with semantic understanding for complex bug localization.

**First Experiments**:
1. Compare BM25 vs. dense encoder performance on a small code completion dataset
2. Test different chunk sizes (8, 32, 64 lines) on a medium context window
3. Measure latency differences between word and BPE tokenization

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize to other programming languages, tasks, or domains beyond the tested benchmarks
- Proprietary dense encoders like Voyager-3 were evaluated but are not openly accessible, limiting reproducibility
- Real-world deployment constraints like concurrent requests and memory limits were not modeled

## Confidence

**High Confidence**:
- Task-specific retrieval superiority (BM25 for completion, dense for bug localization)
- Chunk size scaling with context
- Line-based chunking matching syntax-aware methods

**Medium Confidence**:
- Relative speedups (10-200× for BM25 vs dense)
- Optimal chunk sizes (32-64 lines for small budgets)

**Low Confidence**:
- Extrapolation to new tasks, languages, or non-code domains
- Impact of concurrent workloads on latency
- Long-term robustness of sparse vs. dense trade-offs

## Next Checks
1. Replicate findings on a broader set of programming languages and tasks (e.g., code translation, summarization) to assess generalizability
2. Evaluate retrieval performance under realistic deployment scenarios with concurrent requests, memory constraints, and variable query loads
3. Test emerging retrievers (e.g., graph-based, multi-modal) and open-source dense encoders to compare against proprietary models and expand reproducibility