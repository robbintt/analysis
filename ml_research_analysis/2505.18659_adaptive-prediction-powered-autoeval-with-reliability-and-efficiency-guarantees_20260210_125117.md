---
ver: rpa2
title: Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees
arxiv_id: '2505.18659'
source_url: https://arxiv.org/abs/2505.18659
tags:
- r-autoeval
- data
- arxiv
- betting
- e-value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting among multiple
  AI models, such as large language models (LLMs), by accurately estimating each model's
  performance. Traditional evaluation methods either require costly real-world data
  (Eval) or rely on potentially biased automated evaluators (AutoEval).
---

# Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees

## Quick Facts
- arXiv ID: 2505.18659
- Source URL: https://arxiv.org/abs/2505.18659
- Authors: Sangwoo Park; Matteo Zecchin; Osvaldo Simeone
- Reference count: 40
- Key outcome: R-AutoEval+ adaptively tunes synthetic data reliance, providing finite-sample reliability guarantees while improving sample efficiency over both R-Eval and R-AutoEval across three model selection tasks.

## Executive Summary
This paper addresses the challenge of selecting among multiple AI models by accurately estimating each model's performance while balancing reliability and efficiency. Traditional evaluation methods either require costly real-world data (Eval) or rely on potentially biased automated evaluators (AutoEval). The authors propose R-AutoEval+, a novel framework that provides finite-sample reliability guarantees while ensuring improved or at least no worse sample efficiency compared to existing methods. The key innovation is an adaptive construction of the model evaluation variable, which dynamically tunes its reliance on synthetic data produced by automated evaluators.

## Method Summary
R-AutoEval+ adaptively combines real-world data with synthetic data from an autoevaluator to test if a candidate model's risk R is at most α with Type I error ≤ δ. The method computes S parallel e-values using different weighting factors ρ_s ∈ [0,1] for synthetic data reliance. At each round, effective observations are generated via PPI++ (Eq. 10), e-values are updated using betting strategies (UP or WSR), and weights are updated via exponential weighting (Eq. 13). The test decision is made when max_i E_i ≥ 1/δ. The framework provides finite-sample reliability guarantees while ensuring sample complexity no worse than R-Eval, with potential efficiency gains when the autoevaluator is sufficiently accurate.

## Key Results
- R-AutoEval+ consistently outperforms both R-Eval and R-AutoEval in three model selection tasks
- The method achieves better sample efficiency while maintaining reliability guarantees
- Adaptive weighting successfully identifies optimal synthetic data reliance based on autoevaluator quality
- R-AutoEval+ gracefully reverts to R-Eval behavior when autoevaluator quality is low

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Reliance via Exponential Weighting
The framework automatically identifies the optimal balance between real and synthetic data by tracking which reliance factors yield stronger statistical evidence. S parallel e-values are computed with different weighting factors ρ_s ∈ [0,1]. Weights w_{s,i} are updated via exponential weighting (Eq. 13), concentrating probability mass on factors producing larger e-values—indicating stronger evidence for the risk-controlling hypothesis. The optimal ρ exists within the discrete candidate set and autoevaluator quality remains reasonably consistent during evaluation. Experiments show weight concentration around ρ≈0.9 when γ=0.99, ρ≈0.5 when γ=0.9, and ρ≈0 when γ=0.7.

### Mechanism 2: Finite-Sample Reliability via E-Value Testing
The testing-by-betting framework provides non-asymptotic Type I error control without distributional assumptions beyond boundedness. E-values are constructed as products of betting outcomes (Eq. 11). Ville's inequality guarantees P(max_i E_i ≥ 1/δ | H₀) ≤ δ, ensuring the false positive rate never exceeds δ even for finite samples. Observations must be i.i.d. bounded random variables; betting strategies must satisfy sublinear regret.

### Mechanism 3: Variance Reduction through PPI++ Weighting
Intermediate ρ values can achieve strictly lower variance—and thus better sample efficiency—than either pure real data (ρ=0) or full synthetic reliance (ρ=1). The effective observation ℓ^f_{s,i} = ρ_s·(auto risk estimate) + ℓ_real - ρ_s·ℓ_auto_real (Eq. 10) has variance depending on ρ and autoevaluator-ground-truth correlation. Optimal ρ minimizes Var(ℓ^f_{s,i}), often lying between 0 and 1. Experiments confirm maximum expected log-increment occurs at intermediate ρ when γ=0.9, validating optimal variance reduction.

## Foundational Learning

- **Concept: E-values and Testing-by-Betting**
  - Why needed here: Core statistical framework replacing p-values; enables sequential testing with validity under optional continuation
  - Quick check question: Why does E[E_n|H₀] ≤ 1 guarantee Type I error control at level δ when testing via 1(E_n ≥ 1/δ)?

- **Concept: Prediction-Powered Inference (PPI/PPI++)**
  - Why needed here: Provides the bias-correction mechanism when combining limited real labels with abundant autoevaluator predictions
  - Quick check question: How does the bias correction term ℓ_real - ℓ_auto_real in Eq. 10 ensure unbiased risk estimation?

- **Concept: Online Convex Optimization / Exponential Weights**
  - Why needed here: Provides regret guarantees for adaptive ρ selection; ensures R-AutoEval+ never performs much worse than best fixed ρ
  - Quick check question: What does Lemma 1's sublinear regret guarantee imply about the weight update strategy's ability to identify optimal ρ?

## Architecture Onboarding

- **Component map**: Data layer (Dn, D̃N, autoevaluator f) -> Effective observation module (Eq. 10) -> Betting strategy module (UP or WSR) -> E-value accumulator (S parallel e-values) -> Weight updater (exponential weights Eq. 13) -> Decision gate (1(max_i Ei ≥ 1/δ))

- **Critical path**: 
  1. Initialize S candidate factors (uniformly spaced [0,1]), initial weights w_{s,0}=1/S
  2. Per-round i=1,...,n: Generate S effective observations, update S betting variables and S e-values, combine via weighted sum (Eq. 11), update weights, early terminate if max E_i ≥ 1/δ
  3. Output test decision T_n

- **Design tradeoffs**:
  - S (candidate count): Higher S → finer ρ granularity, O(S) overhead; S=10 works well in experiments
  - Betting strategy: UP has provable sublinear regret but O(nG) cost; WSR is O(1) but empirical
  - Initial weights: Uniform (w_{s,0}=1/S) is robust; informed priors help if autoevaluator quality known a priori
  - Synthetic-to-real ratio r: Larger r → more autoevaluator data per observation, but requires more unlabeled pool

- **Failure signatures**:
  - Weights not converging: Autoevaluator quality inconsistent; consider reducing S or checking data quality
  - Never exceeds 1/δ: True risk R may exceed α, or n insufficient; verify with ground truth if available
  - Selects same model as R-Eval: Autoevaluator low-quality (expected behavior); verify weight concentration at ρ≈0
  - Computation bottleneck: At large n, UP betting dominates; switch to WSR or reduce grid size G

- **First 3 experiments**:
  1. Synthetic validation: Replicate Example 1 with controlled autoevaluator accuracy γ∈{0.5,0.7,0.9,0.99}; verify weight concentration matches Figure 2 and sample complexity follows Theorem 3 trends
  2. Quantization task baseline: Apply to LLM quantization (Figure 4a setup) with high-quality autoevaluator (BF16); confirm R-AutoEval+ outperforms R-Eval and R-AutoEval in model size reduction
  3. Degradation stress test: Same quantization task but with degraded autoevaluator (MX4 precision); verify graceful reversion to R-Eval performance, confirming efficiency guarantee (Eq. 4)

## Open Questions the Paper Calls Out

### Open Question 1
Can the adaptive synthetic data weighting of R-AutoEval+ be effectively combined with methods that actively select real data to further reduce labeling costs? The current framework assumes a fixed pool of unlabeled data and adapts weights based on the autoevaluator's quality, but does not address the orthogonal problem of optimally querying for new ground-truth labels. A unified algorithm that dynamically balances synthetic data reliance with active real-data selection would demonstrate lower sample complexity than either approach alone.

### Open Question 2
How can the framework be modified to optimize the reliance factor ρ continuously rather than searching over a fixed a priori discrete set? The current method computes S effective observations in parallel to find the best discrete weight, which may be suboptimal or computationally inefficient compared to a continuous solution. A theoretical formulation allowing for continuous adaptation of ρ or an adaptive grid strategy that refines the search space based on incoming data would resolve this limitation.

### Open Question 3
Can the sample efficiency guarantees of R-AutoEval+ be extended to finite-sample regimes with moderate reliability levels? The theoretical guarantees are derived for the asymptotic regime where δ → 0, leaving uncertainty about performance at standard confidence levels (e.g., 90% or 95%) used in practice. A non-asymptotic analysis proving efficiency bounds for fixed, practical values of δ would address this gap.

## Limitations
- Theoretical efficiency gains rely on positive correlation between autoevaluator predictions and ground truth, with performance degrading to R-Eval when autoevaluator quality is low
- The framework requires i.i.d. bounded observations, which may not hold for real-world autoevaluator outputs
- Computational overhead scales with betting strategy choice (UP is O(nG) vs WSR's O(1))

## Confidence

- **High confidence**: Finite-sample reliability guarantees via Ville's inequality (tested by mathematical framework), adaptive weight concentration mechanism (validated by empirical results), fundamental tradeoff between reliability and efficiency
- **Medium confidence**: Efficiency improvements in real tasks depend on autoevaluator quality, which varies by task and implementation; theoretical bounds are sound but may be loose in practice
- **Low confidence**: Exact optimal ρ values for different tasks are task-dependent and not predictable from theory alone

## Next Checks
1. **Autoevaluator sensitivity analysis**: Systematically vary autoevaluator quality (γ from 0.5 to 0.99) on a controlled synthetic task and measure the weight concentration patterns and sample complexity scaling
2. **Betting strategy comparison**: Run identical experiments using both UP and WSR strategies on a medium-scale task (e.g., quantization selection) to quantify the practical tradeoff between computational cost and theoretical guarantees
3. **Distributional robustness test**: Evaluate R-AutoEval+ on non-i.i.d. data (e.g., temporally correlated samples or heavy-tailed losses) to verify whether the reliability guarantees hold under realistic conditions