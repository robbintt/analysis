---
ver: rpa2
title: 'KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis
  Prediction Using Multi-agent LLMs'
arxiv_id: '2507.02773'
source_url: https://arxiv.org/abs/2507.02773
tags:
- prediction
- knowledge
- diagnosis
- reasoning
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces KERAP, a knowledge-enhanced reasoning approach\
  \ for zero-shot diagnosis prediction using multi-agent LLMs. KERAP employs a three-agent\
  \ framework\u2014linkage, retrieval, and prediction\u2014to integrate biomedical\
  \ knowledge graphs with patient EHR data, enabling structured medical reasoning."
---

# KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs

## Quick Facts
- arXiv ID: 2507.02773
- Source URL: https://arxiv.org/abs/2507.02773
- Reference count: 31
- Primary result: KERAP achieves 76.16% accuracy on CKD diagnosis prediction

## Executive Summary
This paper introduces KERAP, a knowledge-enhanced reasoning approach for zero-shot diagnosis prediction using multi-agent LLMs. KERAP employs a three-agent framework—linkage, retrieval, and prediction—to integrate biomedical knowledge graphs with patient EHR data, enabling structured medical reasoning. Experiments on four EHR datasets show KERAP achieves superior accuracy and F1-scores compared to baseline methods like direct prompting and KG-augmented prompting, reaching up to 76.16% accuracy on CKD. While more computationally intensive, KERAP offers scalable, interpretable solutions for clinical AI, particularly valuable where labeled data is scarce. Case studies demonstrate its ability to reduce hallucinations and improve prediction reliability.

## Method Summary
KERAP is a three-agent multi-agent system that combines biomedical knowledge graphs with patient EHR data for zero-shot diagnosis prediction. The Linkage Agent uses SAPBERT embeddings to map EHR entities to KG entities via top-10 candidate generation and LLM disambiguation. The Retrieval Agent extracts positive and negative knowledge relations from the KG for the target disease. The Prediction Agent performs two-stage reasoning: Stage I uses positive knowledge for inclusion criteria, while Stage II refines predictions with negative knowledge for exclusion criteria. The framework uses GPT-4o-mini and SAPBERT (d=768) with iBKH knowledge graph (2.38M entities, 48M+ triples). Implementation uses Python with Hugging Face, OpenAI API, and Scikit-learn via Azure OpenAI Service.

## Key Results
- KERAP achieves 76.16% accuracy on CKD dataset, outperforming baselines
- Weighted F1-scores demonstrate consistent improvement across all four tested diseases
- Knowledge graph integration significantly reduces LLM hallucinations compared to direct prompting
- The approach maintains strong performance despite being zero-shot (no task-specific training)

## Why This Works (Mechanism)
KERAP succeeds by combining structured biomedical knowledge with conversational reasoning. The three-agent architecture separates concerns: linkage ensures accurate entity mapping between EHR and KG, retrieval provides disease-specific knowledge context, and prediction performs interpretable two-stage reasoning. The positive-negative knowledge distinction enables both inclusion and exclusion criteria reasoning, mimicking clinical diagnostic processes. The conversational approach allows iterative refinement and reduces hallucinations through knowledge grounding.

## Foundational Learning
- **SAPBERT Embeddings**: Biomedical entity representations trained on PubMed abstracts; needed for accurate EHR-to-KG entity mapping; quick check: verify cosine similarity between related medical terms is high
- **Knowledge Graph Relations**: Structured representation of medical concepts and their relationships; needed to provide clinical context for reasoning; quick check: confirm KG contains relevant positive/negative relations for target diseases
- **Multi-agent LLM Coordination**: Separate LLM agents for specialized tasks; needed to modularize complex reasoning and improve interpretability; quick check: verify agent outputs follow expected format and flow
- **Two-stage Reasoning**: Inclusion criteria followed by exclusion criteria; needed to mimic clinical differential diagnosis; quick check: confirm Stage II refines Stage I predictions rather than duplicating
- **Zero-shot Learning**: Prediction without task-specific training; needed for rapid deployment to new diseases; quick check: verify performance on diseases not seen during KG construction
- **Entity Disambiguation**: Selecting correct KG entity from multiple candidates; needed when EHR terms map to multiple KG concepts; quick check: verify disambiguation accuracy on ambiguous terms

## Architecture Onboarding

**Component Map:** EHR Data -> Linkage Agent -> Retrieval Agent -> Prediction Agent (Stage I -> Stage II) -> Diagnosis Prediction

**Critical Path:** EHR ingestion → Entity linkage → Knowledge retrieval → Two-stage reasoning → Final prediction

**Design Tradeoffs:** The three-agent design increases interpretability and modularity but adds computational overhead through multiple LLM calls. Knowledge graph integration improves accuracy but requires substantial KG construction effort. Two-stage reasoning enhances diagnostic precision but extends reasoning time.

**Failure Signatures:** 
- All predictions skewed to one class suggests Stage II negative knowledge prompts need refinement
- Poor entity linkage indicates SAPBERT embedding quality or candidate selection issues
- Inconsistent outputs across runs suggests LLM prompt instability

**Three First Experiments:**
1. Test entity linkage accuracy on MIMIC-III PNA using SAPBERT to verify top-10 candidates contain correct KG entities
2. Validate retrieval agent outputs by checking that positive/negative knowledge relations are relevant to target diseases
3. Run Stage I reasoning only on small patient subset to verify inclusion criteria reasoning before adding Stage II complexity

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What specific optimization strategies can further enhance the collaborative efficiency of the multi-agent architecture (Linkage, Retrieval, Prediction) in KERAP?
- Basis in paper: [explicit] The authors state in the conclusion that "Future work may explore further optimizations in multi-agent collaboration."
- Why unresolved: The current study establishes the viability of the framework but does not investigate fine-grained inter-agent communication protocols or dynamic agent allocation.
- What evidence would resolve it: Comparative studies of different agent coordination strategies showing improved latency or accuracy.

### Open Question 2
- Question: Can KERAP maintain high performance when expanded to a broader range of medical conditions, particularly rare diseases with sparse knowledge graph coverage?
- Basis in paper: [explicit] The conclusion suggests "expanding KG-based reasoning to a broader range of medical conditions."
- Why unresolved: The experiments are limited to four specific conditions (PNA, CKD, CHF, PSCI) with distinct prevalence rates, leaving generalizability to diverse pathologies unproven.
- What evidence would resolve it: Benchmarking KERAP on diverse, multi-specialty datasets or datasets specifically containing rare diseases.

### Open Question 3
- Question: How robust is the framework when the Linkage Agent fails to map EHR entities to the Knowledge Graph due to ambiguous or missing terminology?
- Basis in paper: [inferred] The method relies on SAPBERT to generate top-10 candidates for the Linkage Agent; however, the paper does not analyze performance degradation when correct entities are absent from this candidate set.
- Why unresolved: Successful knowledge retrieval depends entirely on accurate linkage, but error propagation from linkage failures is not quantified.
- What evidence would resolve it: A sensitivity analysis measuring diagnosis accuracy when synthetic noise is introduced into the entity linking step.

## Limitations
- Private PROMOTE dataset prevents direct validation of PSCI results
- Exact prompt templates for all three agents are unspecified
- Unknown methodology for categorizing KG relations as positive/negative
- Limited evaluation to four disease categories without external validation

## Confidence
- **High Confidence**: The multi-agent architecture design and overall methodology are clearly described
- **Medium Confidence**: Performance claims on MIMIC-III PNA dataset (accuracy up to 76.16%)
- **Low Confidence**: Claims regarding PROMOTE PSCI results due to dataset unavailability

## Next Checks
1. Implement and validate the linkage agent on MIMIC-III PNA using SAPBERT embeddings to confirm entity mapping accuracy
2. Test the two-stage prediction reasoning process on a small subset of patients to verify conversational flow and output consistency
3. Conduct ablation studies removing the knowledge graph component to quantify the contribution of biomedical knowledge integration