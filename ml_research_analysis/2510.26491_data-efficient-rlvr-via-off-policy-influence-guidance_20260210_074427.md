---
ver: rpa2
title: Data-Efficient RLVR via Off-Policy Influence Guidance
arxiv_id: '2510.26491'
source_url: https://arxiv.org/abs/2510.26491
tags:
- training
- data
- influence
- gradient
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CROPI, a curriculum-based reinforcement learning
  framework that leverages influence functions for efficient data selection in RLVR.
  The method addresses the computational challenges of online influence estimation
  by introducing an off-policy gradient estimator that uses pre-collected offline
  trajectories and employs sparse random projection to handle high-dimensional gradients.
---

# Data-Efficient RLVR via Off-Policy Influence Guidance

## Quick Facts
- arXiv ID: 2510.26491
- Source URL: https://arxiv.org/abs/2510.26491
- Reference count: 40
- Primary result: Achieves up to 2.66× step-level acceleration while using only 10% of data per stage compared to full-dataset training

## Executive Summary
This paper introduces CROPI, a curriculum-based reinforcement learning framework that leverages influence functions for efficient data selection in RLVR. The method addresses computational challenges of online influence estimation by introducing an off-policy gradient estimator that uses pre-collected offline trajectories and employs sparse random projection to handle high-dimensional gradients. CROPI iteratively selects the most influential training data for each stage of RL training based on these influence estimates. Experiments on models ranging from 1.5B to 7B parameters demonstrate significant improvements in training efficiency while maintaining or improving performance on both targeted and untargeted tasks.

## Method Summary
CROPI implements a multi-stage curriculum learning approach for RLVR that uses influence functions to select the most impactful training data at each phase. The method pre-collects offline trajectories from a behavior policy (initial base model), then computes off-policy policy gradients using importance sampling to approximate on-policy influence without expensive rollouts. To handle high-dimensional gradients, CROPI applies sparse random projection that drops 90% of gradient dimensions before projection, improving rank preservation for influence scoring. At each training phase, the framework computes influence scores between training prompts and validation sets, aggregates rankings using Reciprocal Rank Fusion, and selects the top α% of data for the next training stage. This creates a dynamic curriculum that adapts to the current policy's learning frontier.

## Key Results
- Achieves up to 2.66× step-level acceleration on 1.5B models compared to full-dataset training
- Uses only 10% of data per stage while maintaining or improving accuracy on targeted tasks
- Demonstrates strong generalization capabilities, improving performance on both targeted and untargeted tasks
- Shows significant precision@10% improvements with sparse random projection (reaching nearly 80% vs ~13% for full gradients)

## Why This Works (Mechanism)

### Mechanism 1: Off-policy gradient estimation
- **Claim:** Off-policy gradient estimation approximates on-policy influence without expensive rollouts
- **Mechanism:** Given behavior policy β, compute policy gradients using pre-collected trajectories via importance sampling: ĝβ(θ, s₀) ≈ (1/K) Σₖ ∇θρₖ,ₜ Âₖ,ₜ where ρₖ,ₜ = πθ(xₖ,ₜ|sₖ,ₜ)/β(xₖ,ₜ|sₖ,ₜ)
- **Core assumption:** Training policy πθ and behavior policy β remain KL-constrained (empirically KL < 0.1)
- **Evidence:** [Section 3.1] KL constraint in RL objective; [Appendix F] 80% of prompt pairs show cosine similarity >0.6 between on/off-policy gradients
- **Break condition:** If KL divergence exceeds ~0.15-0.2 during training, importance weights become unreliable

### Mechanism 2: Sparse random projection
- **Claim:** Dropping 90% of gradient dimensions before projection improves rank preservation for influence scoring
- **Mechanism:** Randomly select subset S of dimensions (sparse ratio |S|/d ≈ 0.1) and project only g[S]
- **Core assumption:** Gradient noise is distributed across dimensions; dropping random dimensions improves signal-to-noise ratio for cosine similarity ranking
- **Evidence:** [Section 3.2 & Figure 4] Precision@10% reaches nearly 80% at sparse ratio 0.1 vs ~13% for full gradients
- **Break condition:** If gradient dimensionality is already low (<10K) or using float32/bfloat16, sparse projection may not help

### Mechanism 3: Phase-level curriculum selection
- **Claim:** Phase-level curriculum selection using influence scores accelerates convergence over full-data or static heuristic selection
- **Mechanism:** At each phase m, compute POPI scores for all training prompts against validation sets, aggregate via Reciprocal Rank Fusion, select top α% for next training phase
- **Core assumption:** Validation set is representative of target tasks, and gradient alignment correlates with learning value
- **Evidence:** [Section 4.2] CROPI segments RL training into multiple stages; [Table 1] Achieves 81.36% on GSM8K@1k vs 79.30% for full dataset
- **Break condition:** If selected data becomes too homogeneous or validation performance plateaus while training loss continues decreasing

## Foundational Learning

- **Influence Functions (Koh & Liang, 1974/2017)**
  - **Why needed here:** CROPI's core scoring mechanism is gradient-based data attribution
  - **Quick check question:** Given two training samples with gradients g₁, g₂ and validation gradient gᵥ, which sample has higher influence if g₁ · gᵥ = 0.3 and g₂ · gᵥ = -0.1?

- **Policy Gradient Methods (GRPO/PPO)**
  - **Why needed here:** CROPI operates within GRPO training; need to understand objective J(θ), advantage estimator Â, and KL constraints
  - **Quick check question:** Why does GRPO eliminate the critic model compared to PPO, and how does this affect gradient computation?

- **Importance Sampling in Off-Policy RL**
  - **Why needed here:** Off-policy gradient estimator uses ρ = πθ/β to correct distribution mismatch
  - **Quick check question:** If πθ assigns probability 0.8 to an action and β assigned 0.1, what is the importance weight, and what problem might arise if this compounds over 100 tokens?

## Architecture Onboarding

- **Component map:** Offline Trajectory Buffer → Gradient Computer → Sparse Projection Module → Influence Scorer → RRF Aggregator → Phase Controller → GRPO Trainer
- **Critical path:**
  1. Collect K trajectories per training prompt from πθ₀ (one-time cost: ~1.2-3.4h for 17-19K prompts on 8×H100)
  2. At phase start: compute gradients for all prompts, project, score against validation set
  3. Select top α% (10%), train for E steps
  4. Repeat M phases
- **Design tradeoffs:**
  - α (selection ratio): Lower α → faster phases but risk missing useful data. Paper uses 0.1
  - E (steps per phase): Longer phases reduce selection overhead but stale influence estimates. Paper uses 100-200
  - Sparse ratio: Lower ratio → better noise filtering but more information loss. Paper finds 0.1 optimal
  - K (trajectories per prompt): Higher K → better gradient estimates but more storage. Paper uses 8
- **Failure signatures:**
  1. KL explosion: If KL(πθ || πθ₀) > 0.2, off-policy gradients become unreliable
  2. Selection collapse: If top-100 prompts all come from one source/task → validation set lacks diversity
  3. Gradient zero-out: If base model gets 0% or 100% pass rate on a prompt, gradient is zero
  4. Numerical instability: If projected gradients are all near-zero → check float16 handling
- **First 3 experiments:**
  1. Validate off-policy approximation: Compute on-policy vs off-policy gradients for 50 prompts at checkpoint 200. Plot cosine similarity distribution. Target: >70% with similarity >0.5
  2. Ablate sparse ratio: Run selection with sparse_ratio ∈ {0.01, 0.05, 0.1, 0.5, 1.0}. Measure precision@10% on held-out gradient pairs. Target: peak at ~0.1
  3. Single-phase vs multi-phase: Compare CROPI with M phases vs single global selection. Train for same total steps on 1.5B model. Target: multi-phase achieves >5% higher accuracy

## Open Questions the Paper Calls Out

- **Theoretical bounds on off-policy estimator:** The paper doesn't provide theoretical analysis of associated errors, bias, or variance for the off-policy gradient estimator compared to online estimation, leaving this for future research.

- **Handling zero-gradient prompts:** The framework doesn't address how to handle training prompts that yield zero gradients (where the base model is fully correct or incorrect) without relying on fresh rollouts, suggesting investigation of incorporating positive or negative examples.

- **Reusing training rollouts:** The paper lists investigating reusing rollouts collected during the training process (e.g., via a replay buffer) as an open direction to potentially improve efficiency.

## Limitations

- Off-policy approximation reliability degrades as πθ diverges from β during training, with no clear adaptive strategy for when policies diverge significantly
- Sparse projection mechanism lacks theoretical justification for why the 0.1 sparse ratio is optimal
- Performance heavily depends on the quality and representativeness of the validation set

## Confidence

- **High confidence:** Overall framework design and empirical results demonstrating step-level acceleration (2.66× on 1.5B model) and improved accuracy on targeted tasks
- **Medium confidence:** Off-policy gradient approximation mechanism supported by empirical evidence but relies on assumptions about KL constraint stability
- **Low confidence:** Sparse random projection mechanism, while showing empirical improvements, lacks theoretical justification for the optimal sparse ratio

## Next Checks

1. **Off-policy gradient fidelity test:** Systematically track KL divergence between πθ and β throughout training phases. For checkpoints where KL > 0.15, compute the difference between on-policy and off-policy gradients to establish a quantitative failure threshold.

2. **Sparse projection ablation study:** Conduct comprehensive ablation across sparse ratios {0.001, 0.01, 0.05, 0.1, 0.2, 0.5, 1.0} on a held-out subset of training prompts, measuring not just precision@10% but also correlation with actual performance improvements.

3. **Validation set sensitivity analysis:** Test CROPI with varying validation set compositions (single vs multiple benchmarks, different task distributions) to quantify how sensitive the influence scoring and subsequent data selection are to validation set choice.