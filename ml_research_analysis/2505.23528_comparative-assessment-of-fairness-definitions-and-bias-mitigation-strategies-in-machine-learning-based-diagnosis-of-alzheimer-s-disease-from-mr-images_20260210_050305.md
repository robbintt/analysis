---
ver: rpa2
title: Comparative assessment of fairness definitions and bias mitigation strategies
  in machine learning-based diagnosis of Alzheimer's disease from MR images
arxiv_id: '2505.23528'
source_url: https://arxiv.org/abs/2505.23528
tags:
- fairness
- bias
- mitigation
- equalized
- odds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study performed a comprehensive fairness analysis of machine
  learning models for diagnosing Mild Cognitive Impairment (MCI) and Alzheimer's disease
  (AD) from MRI-derived neuroimaging features. The research investigated biases related
  to age, race, and gender in a multi-cohort dataset and evaluated the effectiveness
  of pre-processing, in-processing, and post-processing bias mitigation strategies.
---

# Comparative assessment of fairness definitions and bias mitigation strategies in machine learning-based diagnosis of Alzheimer's disease from MR images

## Quick Facts
- arXiv ID: 2505.23528
- Source URL: https://arxiv.org/abs/2505.23528
- Authors: Maria Eleftheria Vlontzou; Maria Athanasiou; Christos Davatzikos; Konstantina S. Nikita
- Reference count: 28
- Primary result: Investigated bias in ML-based AD/MCI diagnosis from MRI, found age and race biases, evaluated fairness mitigation strategies, introduced composite fairness-performance metric

## Executive Summary
This study comprehensively evaluated fairness in machine learning models for diagnosing Mild Cognitive Impairment and Alzheimer's disease from MRI-derived neuroimaging features. The research investigated biases across age, race, and gender demographics in multi-cohort datasets and systematically assessed the effectiveness of various bias mitigation strategies including pre-processing, in-processing, and post-processing techniques. A novel composite metric combining F1-score and equalized odds ratio was introduced to quantify the trade-off between fairness and predictive performance in medical diagnostic applications.

The findings revealed significant biases related to age and race demographics, while no substantial gender bias was detected. The study demonstrated that different bias mitigation strategies perform variably across demographic groups, with Reject Option Classification showing the highest improvement for race and gender biases, and adversarial debiasing proving most effective for age-related bias. The research also identified equalized odds as the most reliable fairness metric for medical diagnosis applications, providing valuable guidance for developing equitable AI systems in healthcare settings.

## Method Summary
The study employed a comprehensive framework for assessing fairness in ML-based AD diagnosis using MRI-derived neuroimaging features from multiple cohorts. The methodology involved systematic evaluation of bias across three demographic dimensions (age, race, gender) using established fairness metrics including demographic parity, equalized odds, and counterfactual fairness. Various bias mitigation strategies were implemented across pre-processing (reweighing, disparate impact remover), in-processing (adversarial debiasing, prejudice remover), and post-processing (Reject Option Classification, equalized odds postprocessing) categories. The evaluation utilized a novel composite metric combining F1-score and equalized odds ratio to balance fairness and predictive performance, with experiments conducted on MCI versus AD classification tasks.

## Key Results
- Significant age and race biases detected in ML-based AD/MCI diagnosis, with no substantial gender bias observed
- Reject Option Classification improved equalized odds by 46% for race and 57% for gender, achieving harmonic mean scores of 0.75 and 0.80 respectively
- Adversarial debiasing yielded highest equalized odds improvement of 40% for age with harmonic mean score of 0.69
- Equalized odds identified as most reliable fairness metric for medical diagnosis applications
- Novel composite metric effectively balanced fairness-performance trade-off in comparing bias mitigation methods

## Why This Works (Mechanism)
The study's approach works by systematically addressing algorithmic bias through multiple intervention points in the ML pipeline. Pre-processing methods modify training data distributions to reduce bias, in-processing techniques incorporate fairness constraints during model training, and post-processing methods adjust decision thresholds to achieve equitable outcomes. The effectiveness stems from the multi-faceted evaluation across different demographic groups and the use of composite metrics that balance both fairness and diagnostic accuracy, ensuring that bias mitigation doesn't compromise clinical utility.

## Foundational Learning
1. **Equalized Odds**: A fairness metric requiring equal true positive and false positive rates across demographic groups. Why needed: Provides balanced evaluation of both correct and incorrect predictions across groups. Quick check: Compare TPR and FPR for each demographic group separately.

2. **Demographic Parity**: Requires similar acceptance rates across groups regardless of actual outcomes. Why needed: Ensures proportional representation in positive predictions. Quick check: Calculate percentage of positive predictions for each demographic group.

3. **Counterfactual Fairness**: Assesses whether predictions remain consistent under counterfactual changes to protected attributes. Why needed: Evaluates deeper causal relationships in model decisions. Quick check: Simulate attribute changes and observe prediction stability.

4. **Composite Fairness-Performance Metrics**: Combine fairness metrics with predictive performance measures. Why needed: Balance equity with clinical utility in medical applications. Quick check: Calculate harmonic mean of fairness and accuracy metrics.

5. **Bias Mitigation Strategy Taxonomy**: Categorizes approaches as pre-processing, in-processing, or post-processing. Why needed: Systematically addresses bias at different stages of ML pipeline. Quick check: Map each mitigation method to its category.

6. **Multi-cohort Dataset Analysis**: Evaluates model performance across diverse datasets. Why needed: Ensures findings generalize across different populations and acquisition protocols. Quick check: Compare results across dataset sources.

## Architecture Onboarding

Component Map:
Data Preprocessing -> Feature Extraction -> Model Training -> Bias Assessment -> Bias Mitigation -> Fairness-Performance Evaluation

Critical Path:
MRI Data -> Neuroimaging Features -> ML Classification -> Demographic Bias Analysis -> Mitigation Strategy Application -> Composite Metric Evaluation

Design Tradeoffs:
- Balance between fairness improvements and diagnostic accuracy
- Choice between different fairness metrics based on clinical context
- Selection of mitigation strategies based on bias type and severity
- Computational complexity versus effectiveness of different approaches

Failure Signatures:
- Performance degradation after bias mitigation
- Incomplete bias reduction despite mitigation efforts
- Overfitting to specific demographic groups
- Metric manipulation without actual fairness improvement

First Experiments:
1. Baseline model performance assessment across all demographic groups
2. Individual bias mitigation strategy effectiveness evaluation
3. Comparative analysis of different fairness metrics on the same dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Findings specific to MCI and AD diagnosis tasks, limiting generalizability to other medical conditions
- Evaluation relies on available demographic data, with potential quality issues affecting bias assessments
- Focus on three demographic factors while acknowledging other potentially relevant variables were not examined
- Multi-cohort dataset limitations may not fully represent real-world clinical population heterogeneity

## Confidence
- High confidence in comparative effectiveness of bias mitigation strategies for evaluated classification tasks
- Medium confidence in generalizability of fairness metric performance across different medical applications
- Medium confidence in absence of gender bias findings given potential dataset composition limitations

## Next Checks
1. External validation on independent, multi-institutional MRI datasets to verify reproducibility across different scanner types and acquisition protocols
2. Prospective clinical validation to assess whether fairness improvements translate to actual diagnostic equity in diverse patient populations
3. Expanded demographic analysis including additional variables such as socioeconomic status, education level, and geographic location to identify potential intersectional biases