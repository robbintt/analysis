---
ver: rpa2
title: 'Multilingual VLM Training: Adapting an English-Trained VLM to French'
arxiv_id: '2512.10336'
source_url: https://arxiv.org/abs/2512.10336
tags:
- french
- translation
- finetuning
- english
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores adapting an English-trained vision-language
  model (VLM) to French by comparing translation-based approaches and parameter-efficient
  finetuning strategies. The study reveals that while models demonstrate some French
  comprehension, their overall performance remains limited due to translation-induced
  noise and data quality issues.
---

# Multilingual VLM Training: Adapting an English-Trained VLM to French
## Quick Facts
- arXiv ID: 2512.10336
- Source URL: https://arxiv.org/abs/2512.10336
- Reference count: 7
- Primary result: Translation-based approaches outperform finetuning for adapting English VLMs to French

## Executive Summary
This paper investigates adapting an English-trained vision-language model to French through two main approaches: translation-based input/output conversion and parameter-efficient finetuning. The study reveals that while both methods show some capability for French comprehension, translation pipelines without finetuning consistently outperform finetuned models across benchmarks. The results highlight a fundamental asymmetry between vast, high-quality English multimodal corpora and smaller, noisier French datasets. With translation quality at 60% high-quality translations and finetuning losses stabilizing between 1.8-2.5, the findings suggest that when native data is scarce, simple translation remains the most effective adaptation method.

## Method Summary
The study compares two adaptation strategies for an English-trained VLM: a translation pipeline that converts French inputs to English, processes them through the model, then translates outputs back to French; and parameter-efficient finetuning approaches that modify model weights while preserving most parameters. The translation approach leverages existing high-quality English training data, while finetuning attempts to directly improve French performance. Multiple benchmarks were used to evaluate performance, revealing consistent superiority of the translation method across all tested scenarios.

## Key Results
- Translation pipeline without finetuning outperforms all finetuned models across benchmarks
- Translation quality measured at 60% high-quality translations, introducing significant noise
- Finetuning losses stabilize between 1.8-2.5, indicating incomplete convergence or learning plateaus

## Why This Works (Mechanism)
The mechanism underlying the superior performance of translation-based approaches stems from the fundamental data quality asymmetry between English and French multimodal corpora. The English-trained model benefits from extensive, high-quality training data that finetuning on smaller, noisier French datasets cannot match. Translation acts as a bridge that allows the model to leverage its existing English knowledge while minimizing the degradation that occurs when attempting to learn from limited native-language data. The 60% high-quality translation rate, while imperfect, still provides cleaner signal than the available French training data.

## Foundational Learning
- **Multimodal model adaptation**: Why needed - To extend model capabilities beyond training language; Quick check - Compare performance across source and target languages
- **Translation pipeline integration**: Why needed - Leverages existing high-quality data; Quick check - Measure translation quality impact on downstream tasks
- **Parameter-efficient finetuning**: Why needed - Modifies model for new language with minimal parameter changes; Quick check - Monitor finetuning loss convergence curves
- **Cross-lingual transfer**: Why needed - Evaluates knowledge transfer between languages; Quick check - Test model on bilingual benchmarks
- **Data quality asymmetry**: Why needed - Explains performance differences between languages; Quick check - Compare dataset statistics and model performance
- **Benchmark evaluation**: Why needed - Quantifies adaptation effectiveness; Quick check - Use multiple metrics across different task types

## Architecture Onboarding
**Component Map**: French input -> Translation module -> English VLM -> Output translation -> French output
**Critical Path**: Input translation → Model inference → Output translation
**Design Tradeoffs**: Translation adds latency and potential quality loss vs. direct finetuning that requires extensive French data
**Failure Signatures**: Poor translation quality manifests as degraded downstream performance; finetuning plateaus indicate insufficient data or suboptimal methodology
**First Experiments**: 1) Measure baseline translation quality on sample inputs, 2) Compare inference latency with and without translation, 3) Run ablation study varying translation quality thresholds

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on machine translation quality with only 60% high-quality translations introduces significant noise
- Small French dataset size creates inherent asymmetry with English corpora
- Evaluation focuses on translation-based metrics without exploring nuanced multilingual understanding aspects

## Confidence
- Translation outperforming finetuning: High confidence (consistent results across multiple benchmarks)
- Data quality asymmetry as primary factor: Medium confidence (supported but alternative explanations not exhaustively explored)
- Prioritizing native-language data collection: Low to Medium confidence (forward-looking recommendation not empirically validated)

## Next Checks
1. Conduct ablation studies with varying translation quality thresholds (e.g., 80%, 90%) to quantify impact of translation noise
2. Test additional parameter-efficient finetuning strategies (e.g., LoRA, prefix tuning) and training durations to assess current methodology limitations
3. Evaluate models on cross-lingual transfer tasks to assess knowledge transfer from French data to related languages or tasks