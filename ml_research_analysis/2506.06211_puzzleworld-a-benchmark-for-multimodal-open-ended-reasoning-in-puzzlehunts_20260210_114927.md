---
ver: rpa2
title: 'PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts'
arxiv_id: '2506.06211'
source_url: https://arxiv.org/abs/2506.06211
tags:
- reasoning
- puzzle
- puzzles
- arxiv
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PUZZLE WORLD, a benchmark of 667 puzzlehunt-style
  problems designed to assess multimodal, open-ended reasoning capabilities. Unlike
  conventional reasoning benchmarks with clear instructions, puzzlehunts require models
  to discover the underlying problem structure from multimodal evidence and iterative
  reasoning, mirroring real-world domains such as scientific discovery and exploratory
  data analysis.
---

# PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts

## Quick Facts
- arXiv ID: 2506.06211
- Source URL: https://arxiv.org/abs/2506.06211
- Reference count: 40
- Most state-of-the-art models achieve only 1-2% final answer accuracy, with the best model solving only 14% of puzzles and reaching 40% stepwise accuracy

## Executive Summary
PuzzleWorld introduces a benchmark of 667 puzzlehunt-style problems designed to evaluate multimodal, open-ended reasoning capabilities. Unlike conventional reasoning benchmarks with clear instructions, puzzlehunts require models to discover the underlying problem structure from multimodal evidence and iterative reasoning. The benchmark includes detailed annotations for each puzzle, including final solutions, step-by-step reasoning traces, and cognitive skill labels. Current state-of-the-art models show poor performance, with most achieving only 1-2% final answer accuracy, revealing significant limitations in their reasoning capabilities.

## Method Summary
The benchmark uses puzzles sourced from Puzzled Pint (2010-2025), each including original PDF/image content, flavor text, final answer, step-by-step reasoning traces, modality tags (text/visual/structured), and cognitive skill labels. Models are evaluated using a standardized prompt system that generates step-by-step solutions, with an LLM-as-a-judge (GPT-4o) comparing outputs against ground truth reasoning traces. Fine-tuning experiments use InternVL3-8B with LoRA rank 8, learning rate 1e-6, and 3 epochs on 80% of the data. The dataset is evaluated for 96.5% annotation correctness, with ambiguous cases manually corrected.

## Key Results
- Most state-of-the-art models achieve only 1-2% final answer accuracy
- The best model (GPT-o3) solves only 14% of puzzles with 40% stepwise accuracy
- Fine-tuning on reasoning traces improves stepwise reasoning from 4% to 11%, while training on final answers alone degrades performance to near zero

## Why This Works (Mechanism)

### Mechanism 1: Process Supervision vs. Outcome Supervision
- **Claim:** Training on explicit reasoning traces improves intermediate step accuracy, whereas training solely on final answers degrades reasoning coherence.
- **Mechanism:** Process supervision reinforces valid step-by-step deductions, while outcome supervision encourages models to hallucinate shortcuts or overfit to answer patterns without understanding the logic.
- **Core assumption:** Human-annotated reasoning traces are of high enough quality to serve as reliable ground truth.
- **Evidence anchors:** Fine-tuning improves stepwise reasoning from 4% to 11%, while answer-only training degrades performance to near zero (abstract, section 5.3).

### Mechanism 2: Rule Discovery vs. Rule Execution
- **Claim:** Removing explicit instructions forces models to generate hypotheses about task structure, exposing myopic reasoning failures hidden by standard benchmarks.
- **Mechanism:** Well-defined tasks allow pattern matching to known templates, while open-ended puzzles require abductive reasoning. Current models lack self-correction mechanisms to backtrack from wrong initial hypotheses.
- **Core assumption:** Flavor text and visual cues are sufficient for deducing rules, even when not explicitly stated.
- **Evidence anchors:** Models rarely recover once committed to incorrect paths (section 5.4), and puzzlehunts require discovering both what the problem is and how to solve it (section 1).

### Mechanism 3: Language-Based Inference Bottleneck
- **Claim:** Reliance on language-based inference creates bottlenecks for visual-spatial reasoning, causing information loss when 2D structures are linearized into text.
- **Mechanism:** Multimodal models convert visual input into text descriptions or coordinate lists, destroying topological relationships and leading to errors in spatial tasks.
- **Core assumption:** Models have sufficient visual encoders but their reasoning core is primarily text-based.
- **Evidence anchors:** Models lose critical spatial information when defaulting to textual reasoning (section 5.4), and GPT-o3 fails to represent complex 4-ring clover structures textually (figure 8).

## Foundational Learning

- **Concept: Process Supervision vs. Outcome Supervision**
  - **Why needed here:** Demonstrates that training method is more critical than data volume for complex reasoning.
  - **Quick check question:** If a model gets the wrong answer but follows a valid logical path, should it be penalized as heavily as a model that hallucinates the correct answer?

- **Concept: Multimodal Alignment (Visual-to-Text Grounding)**
  - **Why needed here:** Explains why models fail on visual and structured inputs despite succeeding on text.
  - **Quick check question:** How does a model represent a 2D grid structure internally? Does it maintain spatial adjacency or flatten the data into a sequence?

- **Concept: Myopic Reasoning / Lack of Backtracking**
  - **Why needed here:** Identifies the primary failure mode as inability to abandon dead-end hypotheses.
  - **Quick check question:** What architectural component allows an LLM to evaluate its own intermediate output and decide to "start over"?

## Architecture Onboarding

- **Component map:** Input (Puzzle Content + Metadata) -> Solver (Multimodal LLM) -> Evaluator (LLM-as-a-Judge) -> Metrics (Final/Stepwise Accuracy)
- **Critical path:** 1) Data Ingestion: preserve original layout to maintain spatial clues, 2) Inference: model generates solution trace, 3) Verification: use provided LLM-judge prompt to grade step-by-step
- **Design tradeoffs:** Layout preservation vs. text accessibility (keep images as PNGs rather than transcribing text), open-source vs. closed-source (dataset open but best model closed)
- **Failure signatures:** Textual linearization (ASCII art descriptions preceding spatial logic failures), commitment bias (asserting incorrect early hypotheses)
- **First 3 experiments:**
  1. Baseline Evaluation: Run standardized prompt against standard VLM to establish stepwise accuracy baseline
  2. Trace Fine-tuning: Fine-tune smaller model using reasoning traces to verify 4% to 11% improvement claim
  3. Visual Tool Ablation: Augment solver with sketching tool and measure performance delta on spatial skill puzzles

## Open Questions the Paper Calls Out

### Open Question 1
Can integrating sketching tools or persistent visual memory mechanisms overcome spatial reasoning bottlenecks? The authors identify models lack sketching capabilities crucial for visual and spatial reasoning, failing to trace paths or manipulate visual structures dynamically. This remains unresolved because current models rely on language-based inference that loses spatial coherence. Evidence would be successful evaluation of models augmented with visual sketchpads on specific spatial puzzles where GPT-o3 failed.

### Open Question 2
How can model architectures be modified to prevent myopic commitment to initial hypotheses and enable effective backtracking? The error analysis notes models exhibit myopic reasoning, fixating on early hypotheses and failing to recover once committed to incorrect paths. This remains unresolved because the paper highlights a lack of mechanisms for self-correction or global verification in current chain-of-thought behaviors. Evidence would be demonstrating a reasoning architecture that detects dead ends and autonomously revises prior steps.

### Open Question 3
Why does improving stepwise reasoning accuracy via fine-tuning fail to translate into higher final answer accuracy? The authors note that while fine-tuning on reasoning traces more than doubled stepwise accuracy (4% to 11%), final answer accuracy remained completely flat at 0.76%. This suggests a disconnect where learning intermediate steps does not equate to learning the compositional logic required for the final solution. Evidence would be analysis determining if the failure lies in the final aggregation step or in compounding minor deviations in the reasoning trace.

## Limitations

- Dataset and full annotation set require access to external repositories for complete reproduction
- Benchmark validity depends on quality and consistency of human-annotated reasoning traces (11% required manual correction)
- Closed-source nature of best-performing model (GPT-o3) limits full replication of results
- LLM-as-judge evaluation introduces potential variability that could affect metric reliability

## Confidence

- **High Confidence:** Process supervision outperforms outcome supervision is well-supported by empirical results showing degradation to near-zero stepwise accuracy with answer-only training.
- **Medium Confidence:** Language-based inference bottlenecking visual-spatial reasoning is plausible but could be more thoroughly validated with direct comparisons to models using visual working memory.
- **Low Confidence:** Exact magnitude of performance differences may vary depending on implementation details, judge consistency, and unspecified random seeds.

## Next Checks

1. Judge Reliability Validation: Run cross-validation study with multiple LLM judges to quantify inter-annotator agreement and establish confidence intervals for stepwise accuracy metrics.
2. Visual Tool Intervention Study: Implement and evaluate a sketching tool that allows models to mark up puzzle images, measuring performance differences specifically on puzzles tagged with "Spatial" skill labels.
3. Backtracking Mechanism Experiment: Design experiment where models are explicitly trained or prompted to evaluate their own intermediate hypotheses and decide when to abandon incorrect paths.