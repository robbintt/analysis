---
ver: rpa2
title: Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search
arxiv_id: '2506.15880'
source_url: https://arxiv.org/abs/2506.15880
tags:
- xiangqi
- move
- policy
- game
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a deep reinforcement learning system for Xiangqi
  (Chinese Chess) that integrates neural networks with Monte Carlo Tree Search (MCTS)
  to enable strategic self-play and self-improvement. The system addresses Xiangqi's
  complexity, including its unique board layout, asymmetrical piece dynamics, and
  high branching factor.
---

# Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search

## Quick Facts
- arXiv ID: 2506.15880
- Source URL: https://arxiv.org/abs/2506.15880
- Reference count: 0
- Primary result: DRL-MCTS hybrid for Xiangqi improves self-play metrics through policy-value network and MCTS integration

## Executive Summary
This paper presents a deep reinforcement learning system for Xiangqi (Chinese Chess) that integrates neural networks with Monte Carlo Tree Search (MCTS) to enable strategic self-play and self-improvement. The system addresses Xiangqi's complexity, including its unique board layout, asymmetrical piece dynamics, and high branching factor. The approach combines policy-value networks with MCTS to simulate move consequences and refine decision-making. Key results show that after three rounds of training, the model demonstrates improved self-play behavior with average game length decreasing from 92.4 to 60.8 moves and average reward increasing from -0.20 to +0.40. The model avoids major strategic errors and exhibits learning patterns for foundational principles, though it still struggles with long-term planning and global positional evaluation. The system successfully adapts DRL-MCTS frameworks to domain-specific rule systems while providing insights for culturally significant strategy games.

## Method Summary
The approach combines deep reinforcement learning with Monte Carlo Tree Search (MCTS) to create a self-improving Xiangqi player. The system uses a policy-value neural network to evaluate board states and suggest moves, while MCTS explores the game tree to simulate move consequences and refine decision-making. The architecture is trained through self-play, where the model learns from its own games and iteratively improves its strategic understanding. The unique challenges of Xiangqi, including its 9x10 board, asymmetrical piece dynamics, and cultural-specific rules, are addressed through domain-specific adaptations of the DRL-MCTS framework. The training process involves three rounds of self-play, with performance metrics tracked across game length, reward, and strategic error avoidance.

## Key Results
- After three rounds of training, average game length decreased from 92.4 to 60.8 moves
- Average reward increased from -0.20 to +0.40 through self-play improvement
- Model demonstrates learning of foundational principles while still struggling with long-term planning

## Why This Works (Mechanism)
The integration of neural networks with MCTS creates a powerful hybrid system where the policy-value network provides fast initial evaluations and move suggestions, while MCTS explores deeper game trees to validate and refine these decisions. This combination addresses the high branching factor of Xiangqi by using the neural network to focus search on promising moves, while MCTS ensures thorough exploration of critical branches. The self-play training mechanism allows the model to learn from its own mistakes and gradually improve its strategic understanding without external supervision. The system's ability to adapt to Xiangqi's unique rules and piece dynamics comes from the neural network's capacity to learn domain-specific patterns through extensive gameplay experience.

## Foundational Learning
- **Monte Carlo Tree Search (MCTS)**: Essential for exploring game trees and simulating move consequences; quick check: verify search depth and node expansion patterns
- **Policy-Value Networks**: Provide fast board evaluation and move suggestions; quick check: validate network accuracy on known board positions
- **Self-Play Training**: Enables autonomous learning without external supervision; quick check: monitor training stability and convergence
- **Domain Adaptation**: Critical for handling Xiangqi's unique rules and piece dynamics; quick check: test model performance on standard Xiangqi positions
- **Reward Shaping**: Important for guiding learning toward strategic goals; quick check: analyze reward distribution across different game phases
- **Branching Factor Management**: Necessary due to Xiangqi's high complexity; quick check: measure effective branching factor during search

## Architecture Onboarding
**Component Map**: Input -> Neural Network -> MCTS -> Output
**Critical Path**: Board state → Neural network evaluation → MCTS simulation → Move selection → Game state update
**Design Tradeoffs**: The system balances between neural network speed (fast but potentially shallow) and MCTS depth (thorough but computationally expensive). The policy-value network provides quick initial assessments, while MCTS explores critical branches more deeply. This hybrid approach sacrifices some computational efficiency for improved strategic depth and accuracy.

**Failure Signatures**:
- Neural network collapse: model consistently makes poor move suggestions, leading to MCTS exploring irrelevant branches
- MCTS stagnation: search fails to explore new branches, resulting in repetitive gameplay patterns
- Training instability: reward signals become inconsistent, causing learning to diverge or plateau
- Long-term planning failure: model focuses on immediate tactical gains while missing strategic opportunities

**3 First Experiments**:
1. Test neural network accuracy on known Xiangqi positions from master games
2. Measure MCTS search efficiency and node expansion rates on different board complexities
3. Evaluate self-play training stability across multiple training rounds

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on self-play rather than competition against established engines or human experts
- Continued struggles with long-term planning and global positional evaluation indicate fundamental architectural limitations
- Lacks detailed ablation studies showing specific contributions of policy-value network versus MCTS components

## Confidence
- Medium confidence in claims about DRL-MCTS integration effectiveness
- Limited external validation through competition benchmarks
- Acknowledged limitations in strategic planning capabilities

## Next Checks
1. Benchmark the trained model against established Xiangqi engines like HQX or BCY to establish competitive performance baselines
2. Conduct systematic ablation studies comparing pure MCTS, pure DRL, and hybrid approaches to isolate the value-add of each component
3. Implement long-horizon evaluation metrics to better assess the model's capability for strategic planning beyond immediate tactical sequences