---
ver: rpa2
title: Scaling Laws are Redundancy Laws
arxiv_id: '2509.20721'
source_url: https://arxiv.org/abs/2509.20721
tags:
- scaling
- redundancy
- laws
- tail
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Scaling laws, a defining feature of deep learning, reveal a striking
  power-law improvement in model performance with increasing dataset and model size.
  Yet, their mathematical origins, especially the scaling exponent, have remained
  elusive.
---

# Scaling Laws are Redundancy Laws

## Quick Facts
- arXiv ID: 2509.20721
- Source URL: https://arxiv.org/abs/2509.20721
- Authors: Yuda Bi; Vince D Calhoun
- Reference count: 9
- Primary result: Scaling laws are mathematically explained as redundancy laws, with exponent α = 2s / (2s + 1/β) derived from data spectral tail and target smoothness.

## Executive Summary
Scaling laws, which describe power-law improvements in model performance with increasing data and model size, have remained mathematically unexplained despite their empirical prevalence. This work provides the first rigorous derivation by showing that scaling laws emerge from spectral redundancy in the data covariance. Using kernel regression, the authors demonstrate that polynomial eigenvalue decay creates an excess risk power law, with the exponent determined by the interplay between data redundancy (spectral tail index β) and target function smoothness (source condition s). This framework unifies empirical observations with theoretical foundations and extends to Transformer architectures through Neural Tangent Kernel analysis.

## Method Summary
The paper establishes scaling laws through kernel ridge regression analysis, showing that polynomial spectral tails in data covariance yield power-law learning curves. The key insight is that excess risk results from a bias-variance tradeoff, where the effective dimension of the problem scales as λ^(-1/β). By optimizing regularization to balance these terms, the risk decays as n^(-α) with α = 2s/(2s + 1/β). The framework extends to Transformers via NTK analysis and kernel drift assumptions, demonstrating universality across transformations and architectures.

## Key Results
- Formal derivation of scaling exponent α = 2s / (2s + 1/β) from data spectral redundancy and target smoothness
- Proof of representation invariance under boundedly invertible transformations
- Extension of framework to Transformer architectures via NTK and kernel drift
- Identification of "hump" deviations in finite-width settings
- Demonstration that mixture domains are bottlenecked by the heaviest tail (smallest β)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The scaling exponent α is derived from the spectral tail index β (redundancy) and target smoothness s, specifically α = 2s / (2s + 1/β).
- **Mechanism:** In kernel regression, excess risk results from bias-variance tradeoff. Polynomial spectral tails create effective dimension N_eff(λ) scaling as λ^(-1/β). Optimizing regularization balances bias and variance terms, yielding power-law decay with exponent α.
- **Core assumption:** Assumption R (polynomial spectral tail λ_i ∝ i^(-1/β)) and Assumption S (source condition on target smoothness).
- **Evidence anchors:** [abstract] explicitly derives α = 2s / (2s + 1/β); [section 2 / theorem 1] details bias-variance optimization; [corpus] "Learning curves theory for hierarchically compositional data" supports power-law features link.
- **Break condition:** Fails if data spectrum decays exponentially or logarithmically rather than polynomially, or if sample size n is insufficient for asymptotic regime.

### Mechanism 2
- **Claim:** Scaling exponent is invariant under boundedly invertible linear transformations of data representation.
- **Mechanism:** Boundedly invertible maps preserve eigenvalue decay rate β up to constant factors. Since α depends only on β and s, scaling law remains unchanged under transformations.
- **Core assumption:** Transformation A is boundedly invertible (frame bounds 0 < m ≤ M < ∞).
- **Evidence anchors:** [section 2 / theorem 2] proves representation invariance; [figure 2] experimental validation showing learning curves collapsing onto same trend; [corpus] "Effective Frontiers" corroborates universal scaling properties.
- **Break condition:** Fails if transformation is singular or significantly distorts spectral tail.

### Mechanism 3
- **Claim:** Scaling laws in Transformers arise from spectral tail of effective NTK, which remains stable or steepens during training (adiabatic drift).
- **Mechanism:** In linearized (NTK) or feature-learning regimes with slow kernel drift, effective kernel operator T_tr has tail index β_tr. Scaling exponent governed by this β_tr similar to kernel regression case.
- **Core assumption:** Assumption T3 (Adiabatic tail stability) and T2 (SGD ≈ Ridge).
- **Evidence anchors:** [section 3 / theorem 6 & 7] extends framework to Transformers via NTK and kernel drift; [corpus] "Learning Shrinks the Hard Tail" discusses training dynamics influence.
- **Break condition:** Fails if training involves abrupt phase transitions or non-adiabatic jumps in kernel spectrum.

## Foundational Learning

- **Concept: Effective Dimension (N_eff)**
  - **Why needed here:** Critical link connecting data redundancy (spectral tail) to variance term in risk bound. Understanding N_eff(λ) ∝ λ^(-1/β) drives power law.
  - **Quick check question:** If eigenvalues λ_i decay faster (e.g., β increases), does effective dimension N_eff at fixed λ increase or decrease? (Answer: Decrease).

- **Concept: Spectral Tail & Regular Variation**
  - **Why needed here:** Paper frames "redundancy" entirely as polynomial decay rate of covariance spectrum. Must understand what i^(-1/β) implies about data concentration.
  - **Quick check question:** Does smaller β (heavier tail) imply higher or lower redundancy? (Answer: Higher redundancy).

- **Concept: Bias-Variance Tradeoff in Kernel Ridge Regression**
  - **Why needed here:** Derivation of scaling law is essentially optimization of this tradeoff. Without this context, appearance of α is opaque.
  - **Quick check question:** In formula Risk ≲ λ^(2s) + σ²/n λ^(-1/β), which term dominates if regularization λ is too small? (Answer: Variance).

## Architecture Onboarding

- **Component map:** Input Data → Covariance Operator T_K (Compute eigenvalues) → Spectral Analysis (Estimate Tail Index β) → Target Function (Estimate Source Condition s) → Synthesis (Predicted Scaling Exponent α)

- **Critical path:** Accurately estimating spectral tail index β from finite samples. Theory relies on asymptotic behavior (i → ∞), but practice requires fitting to observed eigenvalue decay.

- **Design tradeoffs:**
  - Steeper Tails (High β): Faster scaling (better α), but implies "low redundancy" (data is compressible)
  - Heavier Tails (Low β): Slower scaling, requires exponentially more data for linear error improvement

- **Failure signatures:**
  - Finite-Width "Humps": If model width m is insufficient (m ≪ n^r), learning curve deviates from power law due to approximation error
  - Mixture Dominance: If data is mixture of domains, "heaviest tail" (smallest β) dominates. Single high-redundancy sub-domain can bottleneck entire system

- **First 3 experiments:**
  1. Spectral Estimation: Compute eigenvalues of covariance matrix (or NTK Gram matrix) on data subset. Plot log(λ_i) vs log(i) to estimate β
  2. Invariance Check: Train models on raw vs rotated/transformed features. Verify learning curves (slope on log-log plot) remain constant, as predicted by Theorem 2
  3. Finite-Width Stress Test: Train models of increasing width m and sample size n. Verify "hump" deviation disappears as m grows relative to n

## Open Questions the Paper Calls Out

- **Open Question 1:** Can spectral tail index β_tr be derived mathematically from specific Transformer architectural primitives, such as attention mechanisms or positional encodings?
  - Basis: Authors identify lack of explicit derivations of β_tr from architectural primitives like attention mechanisms or positional encodings as gap
  - Why unresolved: Paper treats kernel spectrum as input variable rather than deriving from network structure
  - Evidence needed: Theoretical derivation linking specific attention head configurations or MLP depths to polynomial tail index β_tr

- **Open Question 2:** Do empirical estimates of spectral redundancy β in large-scale datasets (e.g., NLP corpora, CV features) accurately predict scaling exponents observed in trained models?
  - Basis: Paper lists "lack of numerical validations on real datasets" as key limitation
  - Why unresolved: Theory validated on synthetic/controlled data, unclear if real-world data satisfies polynomial tail assumptions
  - Evidence needed: Experiments measuring covariance spectra on large-scale datasets and correlating estimated tail index with model's empirical learning curve slope

- **Open Question 3:** Does redundancy-controlled scaling law persist when using complex adaptive optimizers like Adam, rather than ridge-equivalent implicit regularization of SGD?
  - Basis: Authors suggest exploring "beyond ridge-equivalent regularization, such as full SGD trajectories with momentum or adaptive optimizers"
  - Why unresolved: Current framework assumes early-stopped SGD is equivalent to ridge regression; uncertain if adaptive gradients alter bias-variance tradeoff
  - Evidence needed: Theoretical extension of Theorem 1 for adaptive optimizers, or empirical results showing exponent α remains stable under Adam/W training regimes

## Limitations

- Framework relies critically on polynomial spectral tails (Assumption R), which may not hold for real-world data
- Model assumes infinite-dimensional feature spaces, which breaks down in finite settings
- Derivation assumes optimal regularization tuning, which is not standard practice and may be computationally prohibitive at scale

## Confidence

- **High Confidence:** Mathematical derivation of scaling exponent α = 2s / (2s + 1/β) from spectral assumptions is rigorous and well-established
- **Medium Confidence:** Extension to Transformer architectures via NTK and kernel drift assumptions is plausible but relies on approximations that may not hold exactly
- **Medium Confidence:** Invariance under boundedly invertible transformations is mathematically sound, but practical relevance depends on strict bounded invertibility condition

## Next Checks

1. **Spectral Estimation Validation:** Compute eigenvalue decay of covariance matrix (or NTK Gram matrix) on real datasets. Verify whether observed spectrum follows polynomial power law and estimate tail index β to predict scaling exponent α

2. **Finite-Width Approximation Test:** Train Transformer models of increasing width m while varying sample size n. Observe whether learning curves converge to predicted power law as m grows, or if finite-width "humps" persist

3. **Mixture Domain Analysis:** Construct datasets as mixtures of multiple data sources with different spectral tail indices β. Verify experimentally whether learning curve is dominated by source with heaviest tail (smallest β) as predicted by theory