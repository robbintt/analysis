---
ver: rpa2
title: 'Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization'
arxiv_id: '2507.12308'
source_url: https://arxiv.org/abs/2507.12308
tags:
- code
- vhdl
- generation
- llms
- summarization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Chain-of-Descriptions (CoDes), a framework
  to improve Large Language Models (LLMs) for VHDL code generation and summarization
  tasks. The authors evaluate existing code LLMs on two datasets (VHDL-Eval and VHDL-Xform)
  and find they underperform across metrics like Pass@1, self-consistency, and ROUGE-L.
---

# Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization

## Quick Facts
- arXiv ID: 2507.12308
- Source URL: https://arxiv.org/abs/2507.12308
- Reference count: 36
- Primary result: CoDes framework improves VHDL code generation and summarization performance across all metrics by generating intermediate descriptive steps

## Executive Summary
This paper introduces Chain-of-Descriptions (CoDes), a framework to improve Large Language Models (LLMs) for VHDL code generation and summarization tasks. The authors evaluate existing code LLMs on two datasets (VHDL-Eval and VHDL-Xform) and find they underperform across metrics like Pass@1, self-consistency, and ROUGE-L. CoDes generates intermediate descriptive steps based on problem statements or code, which are integrated with the original prompt to improve LLM outputs. Experiments show CoDes significantly improves performance across all metrics for both code generation and summarization tasks. Multi-step execution consistently outperforms single-step, and longer descriptive prompts improve code generation results. The framework demonstrates effectiveness in enhancing LLM performance for hardware description languages.

## Method Summary
The CoDes framework improves VHDL code generation and summarization by generating intermediate descriptive steps through three stages: plan formulation (LLM generates step-by-step plans from problem statements or code), plan refinement (regex-based extraction of clean steps with retry mechanism), and plan execution (combining refined plan with original prompt). The approach is evaluated on VHDL-Eval (202 problems) and VHDL-Xform (6,500 code-clone pairs) datasets using metrics including Pass@1 (testbench and SEC variants), Self-Consistency (SC1), ROUGE-L, and LLM Preference Rate. Experiments compare zero-shot baselines against CoDes implementations across multiple models including Granite-Code, DeepSeek-Coder, and Llama variants.

## Key Results
- Multi-step execution significantly outperforms single-step across all tested models, improving Pass@1 by 15-30% relative
- Longer descriptive prompts consistently improve code generation quality compared to shorter descriptions
- CoDes framework achieves 30-40% relative improvement in Pass@1 for code generation tasks compared to zero-shot baselines
- Line-by-line and AST-based approaches for summarization show comparable performance, with line-by-line being more thorough but less scalable

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating intermediate descriptive steps before final output improves LLM performance on VHDL tasks
- Mechanism: The CoDes framework decomposes tasks into structured natural language plans that guide the model's reasoning process. For code generation, it creates step-by-step plans from problem statements; for summarization, it provides line-by-line or AST-based explanations of code structure.
- Core assumption: LLMs can produce higher-quality outputs when explicitly guided through intermediate reasoning steps rather than generating directly from input.
- Evidence anchors:
  - [abstract] "CoDes involves generating a series of intermediate descriptive steps based on: (i) the problem statement for code generation, and (ii) the VHDL code for summarization. These steps are then integrated with the original input prompt... to generate the final output."
  - [section 5.1] Describes plan formulation with specific prompting templates for both tasks
  - [corpus] Weak direct evidence; related work on chain-of-thought reasoning (Wei et al., 2022) provides theoretical grounding but VHDL-specific validation limited to this study
- Break condition: If the LLM cannot generate coherent intermediate steps (e.g., produces malformed plans), the refinement phase returns empty strings, potentially degrading performance below baseline.

### Mechanism 2
- Claim: Multi-step execution with separate planning, refinement, and execution phases outperforms single-step approaches
- Mechanism: Separating plan generation from execution allows each phase to be optimized independently. The refinement step removes boilerplate and ensures consistency before the plan is combined with the original prompt for final output generation.
- Core assumption: Decomposing the pipeline enables better error isolation and allows intermediate quality control that single-pass generation cannot achieve.
- Evidence anchors:
  - [section 5.3] "Multi-step execution significantly improves performance compared to Single-step execution, allowing the model to process and refine intermediate outputs for better final results."
  - [figure 2, right] Shows Pass@1 improvements across all tested models with multi-step vs single-step
  - [corpus] No corpus validation; this appears to be a novel finding specific to this framework
- Break condition: Multi-step introduces latency overhead (3× inference calls minimum); if latency constraints are critical, single-step may be preferred despite lower quality.

### Mechanism 3
- Claim: Longer, more detailed problem descriptions improve plan formulation and subsequent code generation quality
- Mechanism: Extended descriptions provide richer context that enables more comprehensive intermediate plans, which in turn guide more accurate VHDL synthesis.
- Core assumption: The relationship between description length and output quality is monotonic within reasonable bounds—more context yields better reasoning.
- Evidence anchors:
  - [section 6.2] "Figure 2 (Left) shows that longer descriptions improve performance across all models, supporting our CoDes framework's approach."
  - [abstract] Mentions experiments show longer descriptive prompts improve code generation results
  - [corpus] No external validation found; this aligns with general prompting literature but VHDL-specific evidence is limited to this study
- Break condition: Diminishing returns or context window limits; excessively long descriptions may introduce noise or exceed model context capacity.

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Prompting**
  - Why needed here: CoDes is explicitly inspired by CoT methods (section 2 references Wei et al., 2022). Understanding how intermediate reasoning steps improve LLM outputs is essential for grasping why CoDes works.
  - Quick check question: Can you explain why generating reasoning steps before a final answer might improve output quality compared to direct generation?

- Concept: **Hardware Description Languages (HDLs) and RTL Design**
  - Why needed here: The entire paper targets VHDL, a hardware description language used in RTL design. Understanding that VHDL describes hardware behavior rather than sequential software execution is critical for interpreting why code LLMs struggle with this domain.
  - Quick check question: What is the fundamental difference between VHDL code and general-purpose programming languages like Python in terms of execution model?

- Concept: **Sequential Equivalence Checking (SEC)**
  - Why needed here: The paper uses SEC as a rigorous metric for evaluating functional correctness (section 4.2.1). This is stricter than testbench-based evaluation and represents a hardware-specific verification methodology.
  - Quick check question: Why might sequential equivalence checking be more rigorous than testbench-based evaluation for hardware designs?

## Architecture Onboarding

- Component map:
  - Plan Formulation Module -> Plan Refinement Module -> Plan Execution Module -> Evaluation Layer
- Critical path:
  1. Input (problem statement or VHDL code) → Plan Formulation (LLM generates descriptive steps)
  2. Plan output → Plan Refinement (regex extraction, validation, retry if needed)
  3. Refined plan + original input → Plan Execution (LLM generates final code/summary)
  4. Final output → Evaluation (testbench, SEC, or text metrics)
- Design tradeoffs:
  - **Single-Step vs Multi-Step**: Multi-step yields ~15-30% relative improvement in Pass@1 but requires 2-3× inference calls and latency.
  - **Line-based vs AST-based summarization**: Line-by-line is more thorough but doesn't scale to large files; AST-based is faster but may miss semantic details (section 6.3 shows comparable performance).
  - **Description length**: Longer descriptions improve results but increase token costs and may hit context limits.
  - **Judge LLM selection**: Paper uses Llama-3-70B for preference rate; different judges may yield different rankings (acknowledged limitation in Appendix H).
- Failure signatures:
  - **Empty plan returned**: Refinement module fails to extract valid steps after 3 retries; execution proceeds with original prompt only (degrades to baseline).
  - **Incoherent intermediate steps**: LLM generates malformed plans that don't follow expected patterns (e.g., missing step markers), leading to extraction failures.
  - **Testbench/SEC disagreement**: Pass@1(TB) > Pass@1(SEC) indicates generated code passes limited tests but isn't truly equivalent to reference implementation (observed in Table 1).
  - **Low self-consistency**: Model generates summary from code, then code from summary, but results aren't functionally equivalent (SC1 scores 3-26% across models).
- First 3 experiments:
  1. **Baseline establishment**: Run zero-shot evaluation on target LLM using VHDL-Eval and VHDL-Xform datasets. Measure all metrics (Pass@1, SC1, PR, ROUGE-L) to establish starting point. Compare against Table 1 values for the same model family.
  2. **CoDes Single-Step implementation**: Implement plan formulation + execution in single LLM call. Use templates from section 5.1.1 for generation tasks. Measure improvement over baseline; expect modest gains (~5-15% relative).
  3. **CoDes Multi-Step with refinement**: Implement full 3-stage pipeline with separate planning, refinement, and execution calls. Test both line-based and AST-based approaches for summarization. Expect ~30-40% relative improvement in Pass@1 for code generation tasks; verify multi-step outperforms single-step as shown in Figure 2.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can optimized hyperparameter settings and specific AST node combinations significantly outperform the current line-by-line approach in VHDL code summarization?
- **Basis in paper:** [explicit] Section 6.3 and Appendix F state that future research should explore hyperparameter settings and methods to better utilize Abstract Syntax Trees (ASTs), as current results showed only marginal improvements.
- **Why unresolved:** The authors' initial investigation found that AST-based planning strategies did not consistently outperform line-by-line explanations, and they did not exhaustively test different prompt combinations or hyperparameter configurations.
- **What evidence would resolve it:** A comparative analysis showing specific AST-based prompting strategies achieving statistically significant higher ROUGE-L or Preference Rate scores than the baseline line-by-line method.

### Open Question 2
- **Question:** Does the Chain-of-Descriptions (CoDes) framework maintain its performance improvements when applied to complex, industrial-grade RTL designs?
- **Basis in paper:** [explicit] Appendix H notes that the datasets used in the study consist of self-contained problem sets lacking the complexities of "real-life design scenarios" and suggests future work focus on "more complex RTL designs."
- **Why unresolved:** The current evaluation relied on simplified problems (e.g., n-bit counters) which facilitate intermediate step generation, but it is unclear if CoDes scales effectively to large, complex codebases where planning is harder.
- **What evidence would resolve it:** Evaluation results (e.g., Pass@1, Self-Consistency) of the CoDes framework applied to a dataset of complex, real-world VHDL repositories.

### Open Question 3
- **Question:** How does the selection of the "judge LLM" impact the Preference Rate (PR) evaluation scores for VHDL code summarization?
- **Basis in paper:** [explicit] Appendix H mentions that while Llama-3-70b was used for accessibility, "future research could delve deeper into exploring the variation in evaluation scores by leveraging different judge LLMs."
- **Why unresolved:** The reliability of the PR metric depends on the judge's capability; relying on a single model (Llama-3-70b) may introduce specific biases or limitations in assessing semantic equivalence.
- **What evidence would resolve it:** A correlation analysis of Preference Rate scores across multiple distinct judge models (e.g., GPT-4, Claude) to determine metric stability.

## Limitations
- Dataset accessibility issues: VHDL-Eval dataset cited as "In Press" without public access, making independent verification difficult
- SEC tool opacity: The specific Sequential Equivalence Checking tool used is not named or detailed in the paper
- Single judge LLM limitation: Preference Rate evaluation relies solely on Llama-3-70B, potentially introducing bias without cross-validation

## Confidence

- **High Confidence**: The core finding that CoDes improves performance across all tested metrics (Pass@1, SC1, ROUGE-L) is well-supported by extensive experimentation across 10+ models and two datasets. The mechanism of using intermediate descriptive steps is clearly demonstrated.
- **Medium Confidence**: The specific effectiveness of multi-step execution over single-step approaches is supported but relies on proprietary datasets and tools (VHDL-Eval, SEC implementation) that aren't publicly available for independent verification.
- **Medium Confidence**: The finding that longer descriptions improve code generation quality is supported by experiments but lacks external validation beyond this study's controlled conditions.

## Next Checks

1. **Dataset Accessibility Verification**: Attempt to obtain the VHDL-Eval dataset through the cited source [2] or reconstruct it from Verilog-Eval using publicly available VHDL translation tools. Document any discrepancies in dataset composition or quality compared to the paper's description.

2. **SEC Tool Implementation**: Implement or identify the specific SEC tool used for functional correctness verification. Run a subset of VHDL-Eval problems through both testbench-based and SEC-based evaluation to confirm the observation that SEC is stricter than testbench coverage alone.

3. **Judge LLM Sensitivity Analysis**: Replicate the LLM Preference Rate experiments using multiple judge LLMs (e.g., GPT-4, Claude-3, Llama-3-70B) to assess whether the preference rankings are consistent across different judging models, as acknowledged as a limitation in the paper.