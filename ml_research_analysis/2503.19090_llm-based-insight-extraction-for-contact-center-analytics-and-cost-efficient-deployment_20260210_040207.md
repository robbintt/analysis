---
ver: rpa2
title: LLM-Based Insight Extraction for Contact Center Analytics and Cost-Efficient
  Deployment
arxiv_id: '2503.19090'
source_url: https://arxiv.org/abs/2503.19090
tags:
- call
- topic
- drivers
- driver
- cluster
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an LLM-based system for automating call driver
  extraction in contact centers, serving as a foundation for downstream tasks like
  topic modeling, trend detection, and FAQ generation. The authors fine-tune Mistral-7B-Instruct-v0.2
  on synthetic call transcripts to generate concise 15-20 word call drivers, addressing
  the limitations of traditional topic modeling which often produces ambiguous multi-topic
  assignments.
---

# LLM-Based Insight Extraction for Contact Center Analytics and Cost-Efficient Deployment

## Quick Facts
- arXiv ID: 2503.19090
- Source URL: https://arxiv.org/abs/2503.19090
- Authors: Varsha Embar; Ritvik Shrivastava; Vinay Damodaran; Travis Mehlinger; Yu-Chung Hsiao; Karthik Raghunathan
- Reference count: 5
- Primary result: Automated call driver extraction for contact centers using fine-tuned Mistral-7B with 2.4-7.2x lower costs than proprietary vendors

## Executive Summary
This paper presents an LLM-based system for automating call driver extraction in contact centers, enabling downstream tasks like topic modeling, trend detection, and FAQ generation. The system fine-tunes Mistral-7B-Instruct-v0.2 on synthetic call transcripts to generate concise 15-20 word call drivers, addressing limitations of traditional topic modeling that produces ambiguous multi-topic assignments. Cost-efficiency is achieved through MultiLoRA for shared model inference, input compression using LLMLingua2, and deployment on EKS with spot instances.

The approach demonstrates significant cost advantages (2.4-7.2x lower than proprietary vendors) while maintaining accuracy through entailment-based evaluation and length penalties to ensure focused, concise call drivers. The system supports topic clustering using HDBSCAN and enables efficient classification of new calls and detection of emerging trends, with applications extending to FAQ generation by tracing call drivers back to specific utterances.

## Method Summary
The system employs a two-stage approach: first, fine-tuning Mistral-7B-Instruct-v0.2 on synthetic call transcripts to generate concise call drivers (15-20 words) that capture primary reasons for customer contacts. Second, clustering these call drivers using HDBSCAN and labeling clusters with the same fine-tuned model for topic modeling. The synthetic data generation involves creating diverse conversation scenarios and corresponding call drivers to train the model. Cost optimization is achieved through MultiLoRA for shared inference across multiple tasks, input compression via LLMLingua2 to reduce token counts, and deployment on Kubernetes EKS using spot instances for cost efficiency.

## Key Results
- Achieves 2.4-7.2x lower costs compared to proprietary vendors through MultiLoRA, LLMLingua2 compression, and spot instance deployment
- Generates concise 15-20 word call drivers that outperform traditional topic modeling's ambiguous multi-topic assignments
- Enables efficient topic clustering with HDBSCAN and downstream applications including trend detection and FAQ generation

## Why This Works (Mechanism)
The system leverages fine-tuned LLMs to overcome the inherent ambiguity of traditional topic modeling approaches that often assign multiple overlapping topics to single conversations. By generating focused, concise call drivers through length penalties and entailment-based evaluation, the system ensures each driver captures a single primary reason for contact. The use of MultiLoRA enables efficient shared inference across multiple tasks, while LLMLingua2 compression reduces input costs without sacrificing accuracy. HDBSCAN clustering provides density-based grouping that adapts well to the natural distribution of call drivers.

## Foundational Learning

**LLM Fine-tuning**: Needed to adapt pre-trained models to the specific task of call driver generation from contact center transcripts. Quick check: Verify model generates 15-20 word outputs with high coherence and relevance.

**MultiLoRA (Low-Rank Adaptation)**: Required for efficient parameter-efficient fine-tuning that enables shared inference across multiple tasks while maintaining performance. Quick check: Confirm inference latency meets real-time requirements.

**HDBSCAN Clustering**: Essential for density-based clustering that handles variable cluster sizes and shapes in call driver space without requiring predefined cluster counts. Quick check: Validate clusters show high internal coherence and distinct boundaries.

**Entailment-based Evaluation**: Critical for measuring semantic similarity between generated call drivers and ground truth, ensuring the model captures the true intent of conversations. Quick check: Ensure entailment scores correlate with human judgment.

**LLMLingua2 Compression**: Necessary to reduce input token counts and associated costs while preserving semantic content for accurate call driver generation. Quick check: Verify compressed inputs maintain >95% semantic fidelity.

## Architecture Onboarding

**Component Map**: Synthetic Data Generator -> Mistral-7B Fine-tuning -> Call Driver Generation -> HDBSCAN Clustering -> Topic Labeling -> Downstream Applications (Trend Detection, FAQ Generation)

**Critical Path**: Call transcript → Input compression → Shared LoRA model inference → Call driver generation → Clustering → Topic assignment

**Design Tradeoffs**: Model size vs. cost (Mistral-7B vs larger models), compression ratio vs. accuracy (LLMLingua2 parameters), clustering granularity vs. interpretability (HDBSCAN min_cluster_size)

**Failure Signatures**: High entailment scores but low human relevance indicates synthetic data mismatch; excessive clustering indicates insufficient fine-tuning; cost overruns indicate suboptimal compression parameters

**First Experiments**: 1) Test synthetic data generation quality by comparing generated vs ground truth call drivers; 2) Validate MultiLoRA inference efficiency across concurrent requests; 3) Benchmark LLMLingua2 compression impact on accuracy vs cost

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Synthetic data generation may not fully capture real contact center conversation complexity and variability
- Evaluation relies primarily on entailment scores and limited manual verification without comprehensive human evaluation
- Cost-efficiency claims based on specific EKS spot instance deployment may not generalize across providers and configurations

## Confidence
- Technical feasibility of fine-tuning and clustering approach: High
- Cost-efficiency claims and real-world performance: Medium
- Scalability across different contact center domains and languages: Low

## Next Checks
1. Evaluate model performance on diverse real-world contact center datasets from multiple industries and languages to assess generalizability
2. Conduct comprehensive human evaluation studies comparing LLM-generated call drivers against ground truth annotations from domain experts
3. Perform cost analysis across different cloud deployment scenarios (including GPU instances, serverless options, and alternative inference optimization techniques) to validate the claimed cost advantages