---
ver: rpa2
title: Iterative Orthogonalization Scaling Laws
arxiv_id: '2505.04005'
source_url: https://arxiv.org/abs/2505.04005
tags:
- muon
- singular
- scaling
- values
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper identifies a scaling problem with the iterative orthogonalization\
  \ procedure in the muon optimizer: as neural network size increases, the singular\
  \ values of the gradient matrices shrink proportionally to 1/\u221Aind, where ind\
  \ is the input dimension. This shrinking distribution causes the polynomial orthogonalization\
  \ to fail at capturing \"unlikely directions\" in larger models."
---

# Iterative Orthogonalization Scaling Laws

## Quick Facts
- arXiv ID: 2505.04005
- Source URL: https://arxiv.org/abs/2505.04005
- Reference count: 8
- Primary result: Singular values of gradient matrices scale as 1/√in_d, requiring either increased NS iterations or steeper polynomial slopes at larger model dimensions

## Executive Summary
This paper identifies a critical scaling problem with Muon's iterative orthogonalization procedure: as neural network dimensions increase, the singular value distribution of gradient matrices shifts toward smaller values proportionally to 1/√in_d. This distribution shift causes the polynomial orthogonalization to fail at capturing "unlikely directions" in larger models. The author provides theoretical proof using the Marchenko-Pastur theorem and demonstrates empirically that larger matrices have heavier tails in their singular value distributions below 0.6, predicting issues will arise for model dimensions greater than 8k.

## Method Summary
The paper analyzes the singular value distribution of gradient matrices in the Muon optimizer using the Marchenko-Pastur theorem to establish theoretical scaling laws. It proves that normalized singular values scale as 1/√in_d under i.i.d. assumptions with fixed aspect ratio. The empirical component compares singular value histograms from 128-dimensional versus 8192-dimensional random matrices, showing heavier tails below 0.6 for larger dimensions. The analysis focuses on how the shrinking singular value distribution affects polynomial orthogonalization effectiveness across scales.

## Key Results
- Singular values of random matrices shrink with scale following 1/√in_d relationship
- Polynomial orthogonalization fails to capture "unlikely directions" (small singular values) in larger matrices due to heavier tails below 0.6
- Issues predicted to arise for model dimensions greater than 8k without increasing NS iterations or polynomial steepness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Normalized gradient singular values scale as 1/√in_d under i.i.d. assumptions with fixed aspect ratio.
- Mechanism: Frobenius normalization divides by √(in_d × out_d), while Marchenko-Pastur shows raw singular values scale as √out_d. The ratio yields 1/√in_d scaling for the normalized singular value distribution.
- Core assumption: Gradient entries are i.i.d. with mean 0 and finite variance; aspect ratio (out_d/in_d) remains constant with scale.
- Evidence anchors:
  - [abstract] "singular values of random matrices shrink with scale"
  - [section 3.1] Formal proof using Marchenko-Pastur Theorem showing SVDD(∇W/||∇W||F) ≈ ρ/√in_d
  - [corpus] Related work "Muon is Scalable for LLM Training" identifies scaling techniques but does not address this specific singular value distribution issue
- Break condition: If gradients are not approximately i.i.d. (e.g., highly structured or sparse), or if aspect ratio changes systematically with scale, the 1/√in_d scaling may not hold.

### Mechanism 2
- Claim: The polynomial orthogonalization fails to capture "unlikely directions" (small singular values) in larger matrices due to heavier tails below 0.6.
- Mechanism: The polynomial p(s) = a·s + b·s³ + c·s⁵ approximates a step function mapping [0,1]→{0,1}. When singular values cluster lower (below 0.6), the polynomial's finite slope at zero cannot adequately amplify these smaller values in a fixed number of iterations.
- Core assumption: The polynomial coefficients (a,b,c) are tuned for a specific singular value distribution and remain fixed across scales.
- Evidence anchors:
  - [abstract] "shrinking distribution causes the polynomial orthogonalization to fail at capturing 'unlikely directions'"
  - [section 3] Empirical comparison of 128-dim vs 8192-dim matrices shows heavier tail below 0.6 for larger dimension
  - [corpus] "Convergence of Muon with Newton-Schulz" analyzes convergence but assumes fixed NS steps, not scale-dependent
- Break condition: If polynomial coefficients are retuned per-scale, or if adaptive coefficient schemes are introduced, the fixed-coefficient failure mode is avoided.

### Mechanism 3
- Claim: Either polynomial slope at zero or NS iteration count must scale with model dimension to maintain orthogonalization effectiveness.
- Mechanism: Steeper slope at zero amplifies small singular values more aggressively per iteration. More iterations allow progressive amplification even with gentler slopes. Both approaches push small singular values toward 1 over repeated applications.
- Core assumption: Capturing "unlikely directions" remains beneficial at larger scales; precision issues from high powers (s⁵, higher) do not dominate.
- Evidence anchors:
  - [section 4] "Either, the slope of the polynomial at 0 or the number of iterations should increase with scale"
  - [section 3] "This issue is alleviated by increasing the number of NS iterations"
  - [corpus] "AuON" proposes linear-time alternatives to orthogonal momentum but does not address scale-dependent iteration requirements
- Break condition: If LLMs naturally develop "outlier features" above 7B parameters (as referenced via Dettmers et al.), capturing unlikely directions may become unnecessary or harmful at scale.

## Foundational Learning

- Concept: **Marchenko-Pastur Theorem**
  - Why needed here: Provides the theoretical basis for singular value distribution of random matrices with fixed aspect ratio; essential for understanding why 1/√in_d scaling emerges.
  - Quick check question: Given a 4096×4096 matrix with N(0,1) entries, what is the approximate support of its singular value distribution after Frobenius normalization?

- Concept: **Singular Value Decomposition and Matrix Polynomials**
  - Why needed here: Muon's orthogonalization applies polynomials directly to singular values; understanding how p(G) = U·p(S)·V^T works is prerequisite.
  - Quick check question: If p(s) = 3s - 4s³ + 2s⁵, what happens to a singular value s=0.5 after one polynomial application?

- Concept: **Newton-Schulz Iteration**
  - Why needed here: The iterative orthogonalization procedure in Muon uses NS-style iterations; understanding convergence behavior under different initial distributions is critical.
  - Quick check question: Why does Newton-Schulz iteration converge slower when initial singular values are clustered near zero?

## Architecture Onboarding

- Component map: Gradient Matrix G → Frobenius Normalization → Polynomial p(G) → Iterate N times → Orthogonalized Update
- Critical path:
  1. Identify model dimension (in_d) for target architecture
  2. Check if in_d > 8k (predicted threshold for issues)
  3. If yes, either: (a) increase NS iterations beyond default 5, or (b) retune polynomial coefficients for steeper slope at zero
  4. Monitor singular value distribution during early training to validate assumptions
- Design tradeoffs:
  - More NS iterations → Better orthogonalization but higher compute overhead per step
  - Steeper polynomial slope → Faster orthogonalization but risk of numerical instability and overshooting
  - Assumption: Capturing unlikely directions at scale is desirable; paper explicitly states this is unclear
- Failure signatures:
  - Singular value distribution showing heavy tail below 0.6 after 5 NS iterations at large in_d
  - Training instability or suboptimal convergence when scaling beyond 8k model dimension with default Muon settings
  - Precision issues if NS iterations or polynomial powers become too high (paper notes this is likely outside realistic ranges)
- First 3 experiments:
  1. **Baseline replication**: Generate random matrices at dimensions 128, 2048, 4096, 8192; apply Frobenius normalization; plot singular value histograms to confirm 1/√in_d scaling and heavier tails below 0.6.
  2. **Iteration sweep**: At in_d=8192, run NS iterations for 5, 7, 10, 15 steps; measure convergence of singular values toward 1; identify minimum iterations for adequate orthogonalization.
  3. **Coefficient tuning exploration**: At in_d=8192, test alternative polynomial coefficients with steeper slope at zero (paper provides no specific values); compare orthogonalization effectiveness against increased iteration approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal scaling law for the polynomial slope at zero or the number of Newton-Schulz iterations as model dimension increases?
- Basis in paper: [explicit] "This paper argues for the existence of a scaling law for the coefficients and number of the NS iterations. It is not in the scope of the paper to decide the remedy."
- Why unresolved: The paper establishes that scaling is needed but deliberately stops short of proposing specific formulas or schedules for adjusting coefficients or iteration counts.
- What evidence would resolve it: Empirical training runs at dimensions >8k comparing different scaling strategies for polynomial steepness and iteration counts, measuring downstream task performance.

### Open Question 2
- Question: Is capturing "unlikely directions" (small singular values) actually beneficial for optimization at larger model scales?
- Basis in paper: [explicit] "However, it's not clear if capturing these less likely directions is actually desirable at larger scales."
- Why unresolved: The author notes LLMs develop outlier features naturally around 7B parameters, raising the possibility that explicit orthogonalization of small singular values may be redundant or harmful at scale.
- What evidence would resolve it: Ablation studies comparing model quality with and without aggressive orthogonalization of small singular values at various model sizes.

### Open Question 3
- Question: Do actual gradient matrices during live training exhibit the same singular value distribution as randomly initialized matrices?
- Basis in paper: [inferred] The empirical validation uses only randomly initialized N(0,1) matrices, not gradients collected during training.
- Why unresolved: Real gradients may have different statistical properties than random matrices due to loss landscape structure, making the Marchenko-Pastur assumptions potentially inapplicable.
- What evidence would resolve it: Singular value distribution analysis of gradient matrices collected during training runs at varying model dimensions.

## Limitations

- Theoretical assumptions of i.i.d. gradient entries may not hold for real LLM gradients with structured correlations
- Empirical validation uses synthetic random matrices rather than actual training gradients
- No specific recommendations for polynomial coefficient tuning or iteration count scaling
- Prediction of 8k threshold lacks empirical validation on actual Muon-optimized models

## Confidence

**High Confidence**: The mathematical derivation using Marchenko-Pastur theorem is sound given the stated assumptions. The observation that larger random matrices have heavier tails in singular value distributions below 0.6 is empirically verifiable.

**Medium Confidence**: The claim that polynomial orthogonalization fails to capture "unlikely directions" in larger models follows logically from the distribution shift, but the practical significance depends on whether capturing these directions actually matters for training performance.

**Low Confidence**: The prediction that issues will arise specifically at model dimensions greater than 8k is based on synthetic matrix experiments without validation on actual Muon-optimized LLM training. The assertion that either polynomial slope or NS iterations must increase with scale is theoretically reasonable but lacks specific implementation guidance or empirical verification.

## Next Checks

1. **Real Gradient Distribution Analysis**: Extract gradient matrices during actual Muon training runs across different model scales (e.g., 1B, 7B, 70B parameters). Apply Frobenius normalization and measure singular value distributions to verify whether they follow the predicted 1/√in_d scaling and exhibit heavier tails below 0.6 at larger scales.

2. **Training Performance Impact Study**: Run controlled experiments training LLMs with Muon optimizer at increasing model dimensions (e.g., 4k, 8k, 16k dimensions). Compare training stability, convergence speed, and final performance between default settings and configurations with increased NS iterations or retuned polynomial coefficients. Quantify the practical impact of the scaling issue.

3. **Polynomial Coefficient Sensitivity Analysis**: Systematically sweep polynomial coefficients (a,b,c) at large model dimensions (in_d > 8k) while monitoring orthogonalization effectiveness and numerical stability. Identify optimal coefficient sets for different scales and determine whether a simple scaling rule exists for coefficient tuning versus the proposed iteration count increase.