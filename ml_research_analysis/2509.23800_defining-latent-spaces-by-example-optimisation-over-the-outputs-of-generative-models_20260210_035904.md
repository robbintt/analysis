---
ver: rpa2
title: 'Defining latent spaces by example: optimisation over the outputs of generative
  models'
arxiv_id: '2509.23800'
source_url: https://arxiv.org/abs/2509.23800
tags:
- latent
- surrogate
- space
- optimisation
- spaces
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces surrogate latent spaces, a general framework\
  \ enabling black-box optimisation over the outputs of diffusion and flow-matching\
  \ models. The key insight is constructing low-dimensional, Euclidean embeddings\
  \ from a generative model\u2019s latent space using a small set of example latents\
  \ (\u201Cseeds\u201D) as coordinate axes."
---

# Defining latent spaces by example: optimisation over the outputs of generative models
## Quick Facts
- arXiv ID: 2509.23800
- Source URL: https://arxiv.org/abs/2509.23800
- Reference count: 33
- Enables black-box optimisation over outputs of modern generative models

## Executive Summary
This paper introduces surrogate latent spaces, a general framework for optimising over the outputs of diffusion and flow-matching models. The method constructs low-dimensional Euclidean embeddings from a generative model's latent space using a small set of example latents as coordinate axes, enabling standard optimisation algorithms to efficiently find high-scoring, diverse outputs. By defining axes via examples, the approach provides interpretable, task-specific control while maintaining bijective mapping to valid model outputs.

## Method Summary
Surrogate latent spaces create a low-dimensional, Euclidean embedding from a generative model's latent space by using a small set of example latents ("seeds") as coordinate axes. This construction enables standard optimisation algorithms to operate in this interpretable space while preserving bijective mapping to valid model outputs. The method is model-agnostic, incurs negligible computational overhead, and supports applications across images, audio, video, and proteins. The approach allows optimisation using algorithms like Bayesian optimisation and CMA-ES, with experiments demonstrating significant improvements in successful generations compared to standard sampling.

## Key Results
- Surrogate spaces enable standard optimisation algorithms to efficiently find high-scoring, diverse outputs across multiple domains
- In protein design with RFDIFFUSION, the approach increased successful generations more than ten-fold compared to standard sampling
- The method enabled generation of longer proteins previously infeasible with standard sampling approaches

## Why This Works (Mechanism)
The framework works by constructing a low-dimensional Euclidean space where each dimension corresponds to a specific example latent. This transforms the complex, often high-dimensional latent space of generative models into an interpretable coordinate system where optimisation can be performed using standard techniques. The bijective mapping ensures that any point in the surrogate space corresponds to a valid output from the generative model, while the example-based axes provide task-specific control over the generation process.

## Foundational Learning
**Latent Space** - The compressed, internal representation used by generative models to encode data
*Why needed:* Understanding how generative models store and manipulate information is crucial for working with surrogate spaces
*Quick check:* Can you explain how a latent vector becomes an output image?

**Bijective Mapping** - A one-to-one correspondence between elements of two sets
*Why needed:* Ensures every point in surrogate space corresponds to exactly one valid model output
*Quick check:* What happens if the mapping isn't bijective?

**Diffusion Models** - Generative models that learn to reverse a noising process
*Why needed:* The primary class of models where surrogate spaces are demonstrated
*Quick check:* How does the noising process relate to latent space structure?

**Flow-Matching Models** - Alternative generative models that learn continuous transformations
*Why needed:* Demonstrates the method's applicability beyond diffusion models
*Quick check:* What distinguishes flow-matching from diffusion approaches?

**Seed Latents** - Example latents used as coordinate axes in the surrogate space
*Why needed:* Provide the foundation for creating interpretable, task-specific control
*Quick check:* How does seed selection affect optimisation outcomes?

## Architecture Onboarding
**Component Map:** Seed Selection -> Surrogate Space Construction -> Optimisation Algorithm -> Model Output Generation
**Critical Path:** The most critical component is the seed selection process, as it directly determines the quality and interpretability of the surrogate space
**Design Tradeoffs:** Using more seeds increases dimensionality and potential control but also computational complexity and risk of overfitting to specific regions
**Failure Signatures:** Poor seed selection leads to biased surrogate spaces that cannot represent diverse outputs; insufficient seed diversity limits optimisation effectiveness
**First Experiments:**
1. Test surrogate space construction with 2-3 seeds on a simple image generation task to verify basic functionality
2. Compare optimisation performance with varying numbers of seeds to establish scaling behavior
3. Evaluate the method on a constrained optimisation task to test robustness

## Open Questions the Paper Calls Out
None

## Limitations
- The method's performance heavily depends on seed set quality and diversity, with biased seeds potentially limiting optimisation effectiveness
- Experimental validation covers limited model types and tasks, with particularly striking protein design results that would benefit from broader replication
- Precise computational scaling behavior with larger latent dimensions remains unclear, and performance in highly constrained or multimodal landscapes hasn't been thoroughly explored

## Confidence
High confidence: The core mathematical framework for constructing surrogate spaces is sound and well-defined. The bijective mapping property and model-agnostic nature are clearly demonstrated.

Medium confidence: The empirical results showing improved optimisation performance are compelling, but the diversity of models and tasks tested is limited. The protein design results are promising but may depend on specific model implementations.

Low confidence: Long-term stability and generalization of surrogate spaces across completely different generative model families (e.g., GANs, VAEs) have not been established.

## Next Checks
1. Test the surrogate space framework on additional generative model types including GANs and VAEs to assess model-agnostic claims across broader architectures
2. Conduct systematic ablation studies varying seed set size and diversity to quantify their impact on optimisation performance and identify failure modes
3. Evaluate the method's performance on constrained optimisation tasks with explicit feasibility requirements to test robustness in more challenging optimization landscapes