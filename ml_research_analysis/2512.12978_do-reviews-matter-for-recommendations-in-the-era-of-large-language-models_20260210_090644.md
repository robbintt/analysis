---
ver: rpa2
title: Do Reviews Matter for Recommendations in the Era of Large Language Models?
arxiv_id: '2512.12978'
source_url: https://arxiv.org/abs/2512.12978
tags:
- reviews
- user
- review
- performance
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates the contribution of textual
  reviews to recommendation performance in the era of large language models (LLMs).
  The authors compare deep learning methods with LLM-based approaches across eight
  public datasets, assessing performance under zero-shot, few-shot, and fine-tuning
  scenarios.
---

# Do Reviews Matter for Recommendations in the Era of Large Language Models?

## Quick Facts
- **arXiv ID:** 2512.12978
- **Source URL:** https://arxiv.org/abs/2512.12978
- **Reference count:** 40
- **Primary result:** LLM-based methods generally outperform traditional deep learning approaches, especially in sparse and cold-start scenarios, with textual reviews not consistently improving performance.

## Executive Summary
This paper systematically evaluates the contribution of textual reviews to recommendation performance using large language models (LLMs) versus traditional deep learning methods. The authors introduce REVLoRA, an efficient fine-tuning approach that compresses reviews through extraction and summarization before LoRA fine-tuning. Through comprehensive experiments across eight public datasets and their RAREval framework, they find that LLMs outperform deep learning methods particularly in cold-start and sparse scenarios. Surprisingly, removing or distorting textual reviews does not consistently degrade recommendation accuracy, challenging assumptions about review utility in modern recommender systems.

## Method Summary
The study evaluates LLM-based and deep learning recommendation methods across eight public datasets using three experimental paradigms: zero-shot, few-shot, and fine-tuning. They introduce REVLoRA, which processes reviews through an extraction-summarization pipeline before LoRA fine-tuning to improve efficiency. The RAREval framework systematically tests review removal, distortion, data sparsity, and cold-start user conditions. Models are evaluated using RMSE and MAE metrics, with computational efficiency comparisons between different review handling approaches.

## Key Results
- LLMs outperform traditional deep learning methods, especially in sparse and cold-start scenarios
- REVLoRA achieves 13.89× runtime efficiency improvement over concatenation while maintaining competitive accuracy
- Removing or distorting textual reviews does not consistently degrade recommendation accuracy
- Zero-shot and few-shot LLMs show superior performance when users have ≤5 interactions

## Why This Works (Mechanism)

### Mechanism 1: Review Compression via Extraction-Summarization Pipeline
- **Claim:** Pre-compressing reviews into structured profiles before fine-tuning improves computational efficiency while maintaining accuracy.
- **Mechanism:** REVLoRA processes reviews through three sequential stages—extraction (identifying rating-influencing features), summarization (condensing to sentence-length preferences), and LoRA fine-tuning (on compressed prompts). This reduces prompt length dramatically, enabling practical fine-tuning.
- **Core assumption:** The extraction-summarization pipeline preserves sufficient predictive signal while discarding noise.
- **Evidence anchors:**
  - [Table I]: Concatenation method has 13.89× runtime penalty vs. REVLoRA; fine-tuning time drops from 3,246 min to 47 min on Musical Instruments dataset
  - [Section III-D]: "The time savings in fine-tuning shorter prompts far outweigh the time spent extracting and summarizing the review texts"
  - [corpus]: Weak direct support—corpus papers focus on RAG and profile management but not this specific compression pipeline
- **Break condition:** If extraction quality degrades (e.g., domain-specific jargon not captured), summarization will propagate errors, reducing fine-tuning effectiveness.

### Mechanism 2: Pre-trained Knowledge Transfer for Cold-Start Users
- **Claim:** LLMs leverage linguistic and behavioral patterns from pre-training to generalize from minimal user history, outperforming deep learning methods in cold-start scenarios.
- **Mechanism:** Zero-shot and few-shot LLMs apply world knowledge encoded during pre-training to infer user preferences from sparse signals, whereas deep learning models require sufficient interaction data to learn user embeddings.
- **Core assumption:** Pre-training corpora contain sufficiently representative user preference patterns.
- **Evidence anchors:**
  - [Figure 9]: "Zero-shot and few-shot LLM generally outperform deep learning baselines when f ≤ 5" (interaction frequency)
  - [Section V-H]: "LLMs can generalize well from minimal user history by drawing on pre-trained linguistic, behavioral, and preference patterns"
  - [corpus]: Paper #89469 ("Cold-Start Recommendation towards the Era of LLMs") confirms cold-start as a key LLM advantage domain
- **Break condition:** If target domain preferences diverge significantly from pre-training distribution (e.g., niche professional equipment), cold-start advantage diminishes.

### Mechanism 3: Review-Noise Sensitivity Varies by Architecture
- **Claim:** Removing or distorting reviews does not consistently degrade recommendation accuracy; model architecture determines sensitivity to textual noise.
- **Mechanism:** DeepCoNN is highly sensitive to noisy text (performance sometimes *improves* without reviews), while RGCL and REVLoRA show robustness—suggesting they either don't heavily rely on text or effectively filter noise.
- **Core assumption:** Contradictory or misaligned reviews exist in datasets and act as distractors for some architectures.
- **Evidence anchors:**
  - [Section V-D]: "DeepCoNN shows counterintuitive performance gains without review content... MAE improves by 0.51 on Musical Instruments"
  - [Figure 5]: Zero-shot LLM generally performs *worse* with reviews; "review text is likely a distractor"
  - [corpus]: Weak support—neighbor papers don't directly address review-noise sensitivity
- **Break condition:** If reviews contain critical domain information not captured by ratings (e.g., fit, sizing), removing them will hurt performance regardless of architecture.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - **Why needed here:** REVLoRA depends on understanding parameter-efficient fine-tuning; without this, the efficiency gains over concatenation won't make sense.
  - **Quick check question:** Can you explain why LoRA reduces trainable parameters while keeping the base model frozen?

- **Concept: Cold-Start Problem in Recommender Systems**
  - **Why needed here:** The paper's key finding is LLM superiority in cold-start scenarios; understanding this problem is essential to interpret Figure 9.
  - **Quick check question:** What makes a user "cold-start" and why do traditional collaborative filtering methods struggle here?

- **Concept: Zero-Shot vs. Few-Shot Prompting**
  - **Why needed here:** The paper evaluates three LLM paradigms; distinguishing these is necessary to understand the experimental design and results.
  - **Quick check question:** What is the difference between zero-shot and few-shot inference in terms of prompt construction?

## Architecture Onboarding

- **Component map:** Data preprocessing: Review extraction (Llama 1B) → User/item profile summarization → Rating data formatting → Model variants: Zero-shot (direct prompting), Few-shot (with demonstration examples), REVLoRA (extraction → summarization → LoRA fine-tuning with modified inference head) → Evaluation framework (RAREval): Five RQs testing review removal, reduction, distortion, sparsity, and cold-start conditions

- **Critical path:**
  1. Preprocess reviews via extraction-summarization pipeline (uses Llama 1B for this stage)
  2. Construct prompts with ≤15 summarized reviews per user/item, ≤32 ratings
  3. Fine-tune with LoRA (rank=64, alpha=64), replacing generation head with linear rating predictor
  4. Evaluate using RAREval across five experimental conditions

- **Design tradeoffs:**
  - **Concatenation vs. summarization:** Concatenation preserves full context but is computationally infeasible (13.89× penalty); summarization trades potential signal loss for tractability
  - **MAE vs. MSE loss:** MAE-optimized REVLoRA achieves best MAE but underperforms RGCL/DIRECT on MSE (different error sensitivity)
  - **Model size:** Llama 3B outperforms 1B in zero-shot; Qwen scaling shows less consistent gains

- **Failure signatures:**
  - MAE increases under review distortion for zero-shot (Figure 7) → model is using review-content alignment as signal
  - DeepCoNN improves when reviews removed → text embedding capacity insufficient or truncation too aggressive
  - Few-shot underperforms zero-shot for Llama models → context window crowding reduces target review count

- **First 3 experiments:**
  1. **Baseline replication:** Run REVLoRA (Llama 1B) on Musical Instruments 5-core; verify MAE ≈ 0.53 per Table III
  2. **Ablation on compression:** Compare extraction-only vs. extraction+summarization to measure information loss from summarization step
  3. **Cold-start validation:** Segment test users by interaction count (1-5 vs. 6-10); confirm LLM outperformance in low-frequency group per Figure 9

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the utility of text reviews change when applying LLMs to sequential recommendation tasks rather than the rating prediction tasks evaluated in this study?
- **Basis in paper:** [explicit] The authors explicitly limit their scope to rating prediction and state that future work should investigate "the effectiveness of LLM in sequential review-aware rating prediction."
- **Why unresolved:** The study excluded sequential tasks to avoid causality constraints and isolate review utility, leaving the interplay between temporal dynamics and text reviews in LLMs unknown.
- **What evidence would resolve it:** Evaluating LLM-based recommenders on datasets with timestamp-aligned reviews using sequential metrics (e.g., NDCG) to see if reviews mitigate cold-start in dynamic sequences.

### Open Question 2
- **Question:** To what extent can prompt optimization improve the performance of zero-shot and few-shot LLMs in review-aware recommendation?
- **Basis in paper:** [explicit] The conclusion lists "prompt optimization for zero-shot, few-shot and finetuning" as a specific avenue for future work.
- **Why unresolved:** The experiments used fixed prompt templates where reviews sometimes acted as distractors; the authors did not explore optimizing these prompts to better extract user preferences.
- **What evidence would resolve it:** Experiments comparing the paper's standard prompts against optimized variants (e.g., chain-of-thought or automated prompt engineering) to see if review utility increases.

### Open Question 3
- **Question:** Can combining the extraction and summarization stages via chain-of-thought (CoT) reasoning improve the efficiency or effectiveness of the REVLoRA method?
- **Basis in paper:** [explicit] In Section III.D, the authors note, "The extraction and summarization steps could be combined via chain-of-thought... We leave this to further studies."
- **Why unresolved:** The proposed REVLoRA method treats extraction and summarization as separate preprocessing steps, which requires distinct computational overhead.
- **What evidence would resolve it:** A comparative analysis of training time and prediction accuracy between the current multi-stage REVLoRA and a unified CoT-based fine-tuning approach.

## Limitations
- Review quality heterogeneity may affect extraction-summarization pipeline effectiveness
- Results may not generalize across domains beyond e-commerce/review-based datasets
- LLM advantages might be sensitive to specific prompt engineering choices

## Confidence
- **High Confidence:** LLM superiority in cold-start scenarios, REVLoRA computational efficiency gains, review removal not consistently hurting performance
- **Medium Confidence:** Extraction-summarization pipeline signal preservation, domain-specific performance rankings
- **Low Confidence:** Zero-shot LLMs generally perform worse with reviews—may be dataset-specific

## Next Checks
1. **Cross-Domain Validation:** Test REVLoRA on non-product domains (e.g., academic paper recommendations) to verify review removal effects generalize
2. **Review Quality Ablation:** Create synthetic review datasets with controlled quality gradients to systematically measure extraction-summarization pipeline performance
3. **Prompt Engineering Sensitivity:** Conduct grid search over prompt formats to quantify dependence on specific prompt engineering choices versus inherent model capabilities