---
ver: rpa2
title: 'TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text, and
  Structure Attacks'
arxiv_id: '2506.11844'
source_url: https://arxiv.org/abs/2506.11844
tags:
- graph
- attack
- attacks
- graphllms
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces TrustGLM, a benchmark designed to evaluate\
  \ the robustness of Graph Large Language Models (GraphLLMs) against adversarial\
  \ attacks. We systematically examined vulnerabilities from three perspectives\u2014\
  text, graph structure, and prompt label attacks\u2014and found that these threats\
  \ significantly degrade performance across multiple datasets."
---

# TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text, and Structure Attacks

## Quick Facts
- arXiv ID: 2506.11844
- Source URL: https://arxiv.org/abs/2506.11844
- Authors: Qihai Zhang; Xinyue Sheng; Yuanfu Sun; Qiaoyu Tan
- Reference count: 40
- Primary result: Systematic evaluation of GraphLLM vulnerabilities across text, structure, and prompt attacks, with defenses showing partial mitigation.

## Executive Summary
This work introduces TrustGLM, a benchmark designed to evaluate the robustness of Graph Large Language Models (GraphLLMs) against adversarial attacks. We systematically examined vulnerabilities from three perspectives—text, graph structure, and prompt label attacks—and found that these threats significantly degrade performance across multiple datasets. To counter these vulnerabilities, we investigated defense techniques including adversarial data augmentation and adversarial training, which improved robustness yet did not completely eliminate the risks. Our findings highlight the urgent need for more effective defenses and lay a solid foundation for future research in securing GraphLLMs.

## Method Summary
TrustGLM evaluates GraphLLM robustness through three attack vectors: text attacks (HLBB/TextHoaxer), structure attacks (Nettack/PRBCD), and prompt attacks (shuffle, noise). Six TAG datasets (Cora, PubMed, OGB-Arxiv, OGB-Products, Amazon-Computers, Amazon-Sports) are used with three victim models (LLaGA, GraphPrompter, GraphTranslator). Attacks are generated via surrogate GCN for structure and black-box queries for text. Defenses include FGSM, PGD, data augmentation, and label shuffle training. Performance is measured via Attack Success Rate (ASR) and accuracy degradation.

## Key Results
- GraphLLMs are highly vulnerable to minor textual perturbations, with accuracy drops up to 46.74% on Cora dataset.
- Structure attacks (Nettack/PRBCD) cause significant performance degradation, with LLaGA showing 43.07% accuracy drop on Cora.
- Prompt label noise attacks, particularly cross-domain noise, achieve up to 57.42% accuracy reduction in GraphPrompter.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GraphLLMs are highly susceptible to text attacks that replace semantically similar words in node textual attributes.
- **Mechanism:** Semantic word replacements alter token embeddings, causing model misinterpretation while preserving ground-truth meaning.
- **Core assumption:** GraphLLM reasoning prioritizes token-level semantics over structural priors.
- **Evidence:** Cora accuracy drops from 87.82% to 46.74% under text attacks; "Robustness in Text-Attributed Graph Learning" confirms distinct effects of textual perturbations.
- **Break condition:** Fails if model uses robust text encoder or ignores text features entirely.

### Mechanism 2
- **Claim:** Graph topology modifications degrade performance by distorting structural embeddings.
- **Mechanism:** Attacks like Nettack/PRBCD reshape node neighborhoods, creating poisoned structural embeddings that mislead inference.
- **Core assumption:** GraphLLM uses GNN or sampling to encode structural context into prompt, sensitive to topology changes.
- **Evidence:** LLaGA accuracy drops 43.07% on Cora under Nettack; "Unveiling the Vulnerability of Graph-LLMs" supports structural vulnerabilities in LLM+GNN combinations.
- **Break condition:** Less effective against models with strong graph priors or minimal reliance on local neighborhood fidelity.

### Mechanism 3
- **Claim:** Prompt manipulation attacks dilute attention mechanism and disrupt label association.
- **Mechanism:** Adding irrelevant labels expands output space and distracts model from correct label association.
- **Core assumption:** LLM's label selection depends on clean, relevant candidate options in prompt context.
- **Evidence:** Cross-domain noise causes 57.42% performance drop in GraphPrompter; prompt robustness literature supports label set effects.
- **Break condition:** Fails if model trained with Label Noise Training to handle label set expansion.

## Foundational Learning

- **Concept: Text-Attributed Graphs (TAGs)**
  - **Why needed here:** Fundamental data structure; nodes have both textual features (vulnerable to text attacks) and edges (vulnerable to structure attacks).
  - **Quick check question:** How does a TAG differ from a standard citation graph?

- **Concept: GraphLLM Architecture (Encoder + Projector + Frozen LLM)**
  - **Why needed here:** Paper targets integration points; understanding how Projector aligns graph embeddings with LLM reveals where poisoned data enters.
  - **Quick check question:** Where does structural information get converted into a format the LLM can process?

- **Concept: Adversarial Training (FGSM & PGD)**
  - **Why needed here:** Primary defense strategy; understanding single-step vs iterative perturbation generation is key to defense experiments.
  - **Quick check question:** Why might PGD provide better robustness than FGSM despite higher computational cost?

## Architecture Onboarding

- **Component map:** Victim Models (LLaGA, GraphPrompter, GraphTranslator) -> Attack Surface (Text/Structure/Prompt) -> Defense Module (Adversarial Training/Data Augmentation)
- **Critical path:** 1) Surrogate Attack: Generate perturbations using smaller surrogate model 2) Inference Attack: Inject perturbations into GraphLLM input 3) Defense Application: Train using clean and perturbed samples
- **Design tradeoffs:** Robustness vs Accuracy (defenses sometimes cause minor clean data performance degradation); Model Dependency (text-reliant models more robust to structure attacks but vulnerable to text attacks)
- **Failure signatures:** High Sensitivity (>20% Cora accuracy drop under TextHoaxer indicates text embedding alignment failure); Prompt Confusion (drastic loss under Cross-Domain Noise indicates reliance on superficial label matching)
- **First 3 experiments:** 1) Reproduce TextHoaxer Attack on Cora against GraphPrompter (~26% accuracy drop) 2) Test Prompt Label Noise on Amazon-Computers (57% drop in GraphPrompter) 3) Validate FGSM Defense on LLaGA against Nettack structure attack

## Open Questions the Paper Calls Out

- **Question:** What defense mechanisms beyond adversarial training can fully secure GraphLLMs against combined text, structure, and prompt attacks without degrading clean performance?
- **Basis in paper:** Authors state investigated defenses "improved robustness yet did not completely eliminate the risks" and data augmentation alone is "insufficient to completely mitigate adversarial effects."
- **Why unresolved:** Current defenses result in model- and dataset-specific trade-offs, failing to restore original accuracy in many scenarios.
- **What evidence would resolve it:** Novel defense strategy maintaining/exceeding baseline accuracy on clean data while achieving near-zero ASR across all three attack dimensions.

- **Question:** Can graph structure learning (GSL) be effectively adapted for GraphLLMs to defend against structural attacks despite existing scalability limitations?
- **Basis in paper:** Section 4.1.2 mentions GSL excluded from benchmark because existing methods "often suffer from scalability issue."
- **Why unresolved:** Integrating structure learning into computationally expensive GraphLLM pipeline remains challenging for large-scale graphs.
- **What evidence would resolve it:** Scalable GSL algorithm operating efficiently within GraphLLM framework demonstrating superior robustness to adversarial training.

- **Question:** Do white-box attacks optimized directly on the GraphLLM yield higher success rates than surrogate-based gray-box attacks used in this study?
- **Basis in paper:** Section 4.1.1 notes structure perturbations generated using surrogate GCN because querying LLM is "computationally expensive," creating potential gap in attack transferability.
- **Why unresolved:** Reliance on surrogate gradients may underestimate true vulnerability of GraphLLM's specific reasoning process.
- **What evidence would resolve it:** Comparative analysis of ASR between surrogate-based attacks and white-box attacks utilizing gradients from frozen LLM components.

## Limitations
- Surrogate model architecture (GCN layers, dimensions, dropout) unspecified, impacting attack effectiveness replication
- Lack of documented random seeds introduces variability in sampling-based attacks and data splits
- Neighbor sampling strategy/k-value not specified, potentially affecting structural embedding quality

## Confidence

- **High Confidence:** Core finding that GraphLLMs are vulnerable to text, structure, and prompt attacks is well-supported across multiple datasets and attack types
- **Medium Confidence:** Relative vulnerability rankings between models are plausible but may depend on implementation details not fully specified
- **Low Confidence:** Absolute accuracy numbers for specific attack-defense combinations may vary significantly with undocumented implementation choices

## Next Checks

1. **Surrogate Model Verification:** Reproduce surrogate GCN architecture and validate generated structure attacks achieve similar success rates on Cora as reported
2. **Defense Effectiveness Replication:** Implement FGSM defense on LLaGA and measure robustness improvement against Nettack, comparing to Figure 3 results
3. **Cross-Domain Noise Sensitivity:** Test prompt label noise attack on Amazon-Computers to verify reported 57% performance drop in GraphPrompter