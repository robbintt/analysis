---
ver: rpa2
title: Sparse Autoencoders Do Not Find Canonical Units of Analysis
arxiv_id: '2502.04878'
source_url: https://arxiv.org/abs/2502.04878
tags:
- latents
- saes
- latent
- features
- smaller
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper challenges the idea that sparse autoencoders (SAEs)
  can find a unique, complete, and irreducible ("canonical") set of interpretable
  features in neural networks. The authors introduce two novel methods: SAE stitching,
  which shows that smaller SAEs are incomplete by revealing novel features in larger
  ones, and meta-SAEs, which demonstrate that larger SAE latents are often compositions
  of smaller ones rather than atomic units.'
---

# Sparse Autoencoders Do Not Find Canonical Units of Analysis

## Quick Facts
- **arXiv ID**: 2502.04878
- **Source URL**: https://arxiv.org/abs/2502.04878
- **Reference count**: 28
- **Primary result**: SAEs of different sizes capture overlapping but distinct feature sets, with larger SAEs learning both refined versions of existing features and entirely new ones

## Executive Summary
This paper challenges the idea that sparse autoencoders (SAEs) can find a unique, complete, and irreducible ("canonical") set of interpretable features in neural networks. The authors introduce two novel methods: SAE stitching, which shows that smaller SAEs are incomplete by revealing novel features in larger ones, and meta-SAEs, which demonstrate that larger SAE latents are often compositions of smaller ones rather than atomic units. Using these techniques on GPT-2 and Gemma models, they find that SAEs of different sizes capture overlapping but distinct feature sets, with larger SAEs learning both refined versions of existing features and entirely new ones. The resulting decompositions are often interpretable, such as an "Einstein" latent decomposing into "scientist," "Germany," and "famous person." The authors conclude that SAEs do not identify canonical units of analysis and recommend pragmatic selection of SAE size based on specific interpretability tasks rather than seeking a universal solution.

## Method Summary
The authors develop two methods to test whether SAEs find canonical units of analysis. SAE stitching transfers latents between SAEs of different dictionary sizes using decoder cosine similarity to identify "novel" latents (those that improve reconstruction when added to smaller SAEs) and "reconstruction" latents (those that are refined versions of existing features). Meta-SAEs are secondary SAEs trained on the decoder matrix of larger SAEs, revealing compositional structure by decomposing complex latents into interpretable meta-latents. The methods are applied to GPT-2 Small layer 8 and Gemma 2 2B layer 12 activations, training SAEs of sizes 768-98,304 on OpenWebText sequences.

## Key Results
- SAE stitching reveals smaller SAEs are incomplete, with novel latents from larger SAEs improving reconstruction MSE by 0.07-0.12
- Meta-SAE analysis shows larger SAE latents decompose into interpretable features, with the "Einstein" latent splitting into "scientist," "Germany," and "famous person"
- The "Einstein" latent's reconstruction MSE drops from 0.2727 to 0.0833 when decomposed into meta-latents, a 69.41% improvement
- Meta-SAE explains 55.47% of decoder variance with only 2304 meta-latents, suggesting sparsity is achieved through composition rather than atomic discovery

## Why This Works (Mechanism)

### Mechanism 1: SAE Stitching Detects Incompleteness via Latent Transfer
- **Claim:** Larger SAEs contain both refined versions of smaller SAE features (reconstruction latents) and entirely new features (novel latents) that smaller SAEs miss.
- **Mechanism:** The method transfers latents between SAEs of different dictionary sizes by: (1) computing maximum decoder cosine similarity between all latent pairs, (2) classifying latents below threshold θ as "novel" and above as "reconstruction," (3) inserting novel latents directly and swapping reconstruction latents via bipartite graph matching. If inserting a latent improves reconstruction MSE, it captures information the smaller SAE missed.
- **Core assumption:** Decoder cosine similarity correlates with functional similarity—latents with similar decoder directions encode similar concepts.
- **Evidence anchors:**
  - [abstract] "SAE stitching to show they are incomplete... novel latents, which improve performance when added to the smaller SAE, indicating they capture novel information"
  - [section 4.2, Figure 4] Features with cosine similarity < 0.7 tend to improve MSE when added; Figure 11 shows smaller SAE reconstruction is worse on inputs where novel latents are active
  - [corpus] Sparse Autoencoders Trained on the Same Data Learn Different Features (arXiv:2501.16615) reports SAEs with different random seeds learn different features, suggesting no unique decomposition exists
- **Break condition:** If decoder directions don't correlate with activation patterns, or if reconstruction error is dominated by noise rather than missing features.

### Mechanism 2: Meta-SAEs Reveal Compositional Structure in SAE Latents
- **Claim:** Individual SAE latents (especially from larger SAEs) decompose into interpretable, sparser meta-latents that correspond to features found in smaller SAEs.
- **Mechanism:** Train a secondary SAE on the decoder matrix W_dec of a primary SAE. The meta-SAE learns to reconstruct each decoder direction as a sparse combination of meta-latent directions. When meta-SAE decoder directions align with smaller SAE latents, this indicates the larger latent is a composition.
- **Core assumption:** The true underlying features are sparser than the representations learned by large SAEs; the sparsity penalty in SAE training causes composition.
- **Evidence anchors:**
  - [abstract] "a latent representing 'Einstein' decomposes into 'scientist', 'Germany', and 'famous person'"
  - [section 5, Table 1] Multiple interpretable decompositions shown; meta-SAE explains 55.47% of decoder variance with only 2304 meta-latents
  - [section 5.2, Figure 6] High cosine similarity between meta-SAE latents and smaller SAE latents (~0.8+ for SAE with 3072 latents)
  - [corpus] Feature Hedging (arXiv:2505.11756) finds narrow SAEs fail on correlated features, supporting the composition hypothesis
- **Break condition:** If meta-SAE decompositions are uninterpretable, or if meta-latents don't correlate with smaller SAE features.

### Mechanism 3: Sparsity Penalty Incentivizes Feature Composition Over Atomic Discovery
- **Claim:** The L_0 sparsity penalty in SAE training causes larger SAEs to learn composite features (e.g., "blue square") rather than atomic features (e.g., "blue" + "square") when compositions are sparser.
- **Mechanism:** Loss function L = L_reconstruct + λL_sparsity trades off reconstruction fidelity against number of active latents. For compositional concepts, representing "blue square" as one latent (1 active) is sparser than two latents for "blue" and "square" (2 active), even though the latter is more atomic.
- **Core assumption:** The underlying feature space has compositional structure that can be exploited for sparser encodings.
- **Evidence anchors:**
  - [section 1, Figure 2] Explicit toy example showing 6 atomic features vs. 9 compositional features; larger SAE learns compositions
  - [section 4.2] "sparsity penalty results in the undesirable composition of features that may be sparser, but do not add any new information"
  - [corpus] No direct corpus evidence on this specific mechanism; related work cited (Anders et al., 2024; Wattenberg & Viégas, 2024) but not in provided neighbors
- **Break condition:** If model representations are not compositional, or if reconstruction error dominates before composition becomes beneficial.

## Foundational Learning

- **Concept: Sparse Dictionary Learning / Overcomplete Representations**
  - **Why needed here:** SAEs are fundamentally sparse dictionary learning applied to neural activations. You must understand that the dictionary is overcomplete (more latents than activation dimensions) and sparsity constraints force selective activation.
  - **Quick check question:** Given 768-dim residual stream activations, why would an SAE with 98,304 latents not simply memorize training data?

- **Concept: Decoder Cosine Similarity as Feature Similarity Metric**
  - **Why needed here:** The entire stitching methodology relies on decoder directions capturing functional similarity. Understanding why decoder (not encoder) directions matter is critical.
  - **Quick check question:** Why might two latents with high decoder cosine similarity but different activation patterns still be considered "similar features"?

- **Concept: Feature Splitting vs. Feature Composition**
  - **Why needed here:** The paper challenges the "splitting" narrative (broad features → narrow features) by showing composition also occurs (multiple features → composite feature). Distinguishing these is essential for interpreting scaling behavior.
  - **Quick check question:** If a 768-latent SAE has features for "red" and "circle," and a 1536-latent SAE has a "red circle" feature but not separate "red"/"circle" features, is this splitting or composition?

## Architecture Onboarding

- **Component map:**
  Model activations (x) → SAE encoder (W_enc, b_enc) → Sparse latents f(x) → SAE decoder (W_dec, b_dec) → Reconstruction x̂
                                              ↓
                                    Meta-SAE trains on W_dec columns

- **Critical path:**
  1. Train SAEs at multiple dictionary sizes on same layer/activation type
  2. Compute pairwise decoder cosine similarities between all latent pairs
  3. Classify latents using threshold (0.7 for GPT-2, 0.4 for Gemma in this paper)
  4. For meta-SAEs: extract W_dec, train secondary SAE with ~5-10% of original dictionary size

- **Design tradeoffs:**
  - **Threshold selection:** Lower θ → more latents classified as "reconstruction" (conservative novelty detection); higher θ → more "novel" latents but risk false positives. Paper uses ROC analysis (Figure 13) but doesn't prescribe universal threshold.
  - **Meta-SAE dictionary size:** Too small → poor reconstruction of decoder directions; too large → loses sparsity advantage. Paper uses 2304 for 49152-column decoder (~4.7%).
  - **Assumption:** SAEs must be trained on identical data distribution and model layer for valid comparison.

- **Failure signatures:**
  - Stitching increases MSE: latent is redundant; threshold may be too low
  - Meta-SAE reconstructions uninterpretable: meta-SAE may be too large (learning noise) or too small (missing structure); check variance explained
  - No novel latents found: smaller SAE may already be at capacity for this layer, or threshold too high
  - Meta-latents don't correlate with smaller SAE: composition hypothesis may not hold for this layer/model

- **First 3 experiments:**
  1. **Reproduce stitching on two SAE sizes:** Train 768 and 1536 latent SAEs on same layer, compute max cosine similarity histogram (should match Figure 7), verify novel latents exist
  2. **Validate novel latent functional difference:** For a novel latent, compare reconstruction MSE of smaller SAE on inputs where latent is active vs. inactive (should see degradation when active, per Figure 11)
  3. **Train meta-SAE and check alignment:** Train meta-SAE on 49152-latent decoder, verify meta-latent decoder directions have high cosine similarity with 3072-latent SAE decoder directions (target >0.7 per Figure 6)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do canonical units of analysis (unique, complete, atomic features) actually exist in neural networks, or is feature representation inherently relative?
- Basis in paper: [explicit] The authors state, "We are uncertain whether canonical units of analysis exist, but our results suggest that alternative approaches should be explored."
- Why unresolved: This paper demonstrates that SAEs fail to recover canonical units, but this failure does not definitively prove that such units do not exist in the underlying model.
- What evidence would resolve it: The discovery of a decomposition technique that yields a feature set which is invariant to hyperparameter choices (like dictionary size) and satisfies the criteria of uniqueness and atomicity.

### Open Question 2
- Question: Do the phenomena of latent composition and incompleteness persist in very large SAEs (e.g., millions of latents), or do SAEs eventually converge?
- Basis in paper: [explicit] The authors list a limitation stating their work "does not include very large SAEs, such as in Templeton (2024)," leaving the behavior of massive SAEs unknown.
- Why unresolved: The current results are restricted to SAEs with up to 98,304 latents; it is possible that the observed composition of features is an artifact of the specific size regime studied.
- What evidence would resolve it: Applying meta-SAE and stitching techniques to SAEs trained with significantly larger dictionaries (e.g., >1M features) to see if atomic features emerge or if composition continues.

### Open Question 3
- Question: How can researchers systematically select the appropriate SAE dictionary size for specific downstream tasks?
- Basis in paper: [explicit] The authors note that their methods "neither identify canonical units of analysis, nor the size of dictionary to use for a given task."
- Why unresolved: The paper establishes that different sizes offer trade-offs (completeness vs. atomicity) but does not provide a framework for determining which trade-off is optimal for tasks like steering or circuit analysis.
- What evidence would resolve it: A benchmark study correlating SAE dictionary sizes with performance metrics on specific interpretability tasks (e.g., unlearning efficacy or steering accuracy).

## Limitations
- Results demonstrated primarily on GPT-2 and Gemma, with limited testing on larger models or different architectures
- Study focuses on residual stream activations; findings may not generalize to attention patterns or MLP activations
- Stitching methodology assumes decoder cosine similarity is a reliable proxy for functional similarity

## Confidence
- **High confidence**: SAE stitching successfully demonstrates incompleteness (novel latents improve reconstruction)
- **Medium confidence**: Meta-SAE reveals compositional structure in larger SAEs (interpretable decompositions exist)
- **Medium confidence**: Sparsity penalty drives composition over atomic discovery (supported by toy examples and empirical patterns)

## Next Checks
1. Test stitching methodology across multiple random seeds for each SAE size to confirm consistency of novel latents
2. Apply meta-SAE analysis to MLP activations and attention patterns to assess architectural generality
3. Conduct ablation studies on sparsity coefficient to quantify its impact on feature composition vs. atomic discovery