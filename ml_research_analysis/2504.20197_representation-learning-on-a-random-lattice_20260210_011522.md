---
ver: rpa2
title: Representation Learning on a Random Lattice
arxiv_id: '2504.20197'
source_url: https://arxiv.org/abs/2504.20197
tags:
- features
- data
- cluster
- distribution
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes a random lattice model of data distributions
  using percolation theory to understand the geometric properties of learned features
  in neural networks. The model assumes a high-dimensional hypercubic lattice where
  input examples are connected if they yield the same outputs under the target function,
  with connections occurring randomly.
---

# Representation Learning on a Random Lattice

## Quick Facts
- arXiv ID: 2504.20197
- Source URL: https://arxiv.org/abs/2504.20197
- Authors: Aryeh Brill
- Reference count: 18
- One-line primary result: Random lattice model predicts power-law distributed cluster sizes and fractal geometry in neural network feature representations

## Executive Summary
This paper presents a theoretical framework for understanding feature learning in neural networks using percolation theory on a random lattice model. The model assumes input examples form connections on a high-dimensional lattice when they produce the same outputs under a target function, with connections occurring randomly. By analyzing this system using percolation theory, the paper identifies a critical phase transition and derives predictions about the geometric properties of learned features, including their hierarchical structure and fractal dimensions.

The framework suggests that features in neural networks naturally organize into a tree-like nested structure rather than a simple set, with three distinct categories: context features for cluster membership, component features for coordinates, and surface features from imperfect learning. The model makes specific predictions about feature dimensionality, sparsity regimes, and the distribution of cluster sizes that can be empirically tested, providing a foundation for understanding the geometry of representation learning.

## Method Summary
The paper constructs a random lattice model where input examples are vertices on a high-dimensional hypercubic lattice, connected by edges if they produce the same outputs under a target function. Connection probabilities follow a random process determined by the function's output distribution. The model is then analyzed using percolation theory to understand the geometric properties of the resulting clusters. The critical phase transition occurs at occupation probability pc = 1/(z-1), where z is the lattice degree, leading to distinct subcritical and supercritical regimes with different feature properties.

## Key Results
- Critical phase transition at occupation probability pc = 1/(z-1) separates sparse (subcritical) from dense (supercritical) feature regimes
- Clusters exhibit fractal geometry with dimension D = 4 in high dimensions and power-law size distributions with exponent τ = 5/2
- Three categories of natural features emerge: context features (cluster membership), component features (coordinates), and surface features (imperfect learning)
- Model predicts four-dimensional component features, challenging linear representation assumptions
- Features form hierarchical tree structures rather than simple sets

## Why This Works (Mechanism)
The model works by abstracting the process of feature learning into a geometric problem on a random lattice. When input examples that produce the same outputs are connected, they form clusters that represent natural features. The percolation theory analysis reveals that these clusters undergo a phase transition, with different geometric properties in the subcritical and supercritical regimes. The hierarchical structure emerges because clusters themselves can be clustered, creating nested feature representations. The fractal dimension and power-law distributions arise from the critical behavior at the phase transition point.

## Foundational Learning

**Percolation Theory**
*Why needed:* Provides mathematical framework for analyzing connectivity and phase transitions in random graphs
*Quick check:* Verify critical probability formula pc = 1/(z-1) for lattice connectivity

**Fractal Geometry**
*Why needed:* Describes the self-similar structure of clusters near criticality
*Quick check:* Confirm fractal dimension D = 4 through box-counting methods

**Random Graph Theory**
*Why needed:* Models the probabilistic nature of connections between similar inputs
*Quick check:* Validate random connection assumptions against empirical data distributions

**Phase Transition Analysis**
*Why needed:* Identifies critical points where system behavior changes qualitatively
*Quick check:* Verify existence of sharp transition between subcritical and supercritical regimes

## Architecture Onboarding

**Component Map**
Random lattice -> Percolation analysis -> Feature clusters -> Hierarchical decomposition -> SAE extraction

**Critical Path**
1. Define target function and lattice structure
2. Establish connection probabilities between inputs
3. Apply percolation theory to analyze cluster properties
4. Identify phase transition and geometric characteristics
5. Map cluster structure to feature hierarchy

**Design Tradeoffs**
- Model simplicity vs. real-world complexity
- Infinite system assumptions vs. finite data constraints
- Independent connection assumption vs. structured correlations
- Perfect function knowledge vs. practical learning scenarios

**Failure Signatures**
- No clear phase transition indicates model breakdown
- Non-power-law cluster distributions suggest different underlying mechanisms
- Deviations from predicted fractal dimensions reveal structural differences
- Linear feature representations contradict model predictions

**First Experiments**
1. Generate synthetic datasets with known target functions and test percolation predictions
2. Train neural networks on controlled datasets and measure cluster size distributions
3. Apply sparse autoencoders to trained networks and analyze feature dimensionality

## Open Questions the Paper Calls Out
None

## Limitations
- Independent random connections assumption oversimplifies real-world data correlations
- Infinite system analysis may not apply to finite neural network datasets
- Four-dimensional representation prediction lacks empirical validation
- Perfect target function knowledge assumption unrealistic for practical learning

## Confidence

**Major claim clusters confidence:**
- Percolation phase transition and critical probability: **High**
- Power-law distribution of cluster sizes: **Medium**
- Fractal geometry of clusters: **Medium**
- Need for four-dimensional representations: **Low**
- Hierarchical feature tree structure: **Medium**

## Next Checks
1. Empirical measurement of cluster size distributions in trained neural networks to verify power-law behavior with predicted exponent τ = 5/2
2. Dimensionality analysis of extracted features using sparse autoencoders to test the four-dimensional representation prediction
3. Controlled experiments varying dataset properties to observe predicted transitions between subcritical and supercritical regimes and corresponding changes in feature sparsity