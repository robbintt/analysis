---
ver: rpa2
title: 'Mechanistic Interpretability as Statistical Estimation: A Variance Analysis
  of EAP-IG'
arxiv_id: '2510.00845'
source_url: https://arxiv.org/abs/2510.00845
tags:
- circuit
- variance
- circuits
- https
- interpretability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the stability and reliability of circuit
  discovery methods in mechanistic interpretability. The authors argue that circuit
  discovery should be viewed as a statistical estimation problem and conduct a comprehensive
  analysis of variance and robustness across multiple perturbation strategies, including
  input resampling, prompt paraphrasing, hyperparameter variation, and noise injection
  in causal analysis.
---

# Mechanistic Interpretability as Statistical Estimation: A Variance Analysis of EAP-IG

## Quick Facts
- **arXiv ID**: 2510.00845
- **Source URL**: https://arxiv.org/abs/2510.00845
- **Reference count**: 40
- **Primary result**: Circuit discovery methods exhibit high variance and instability across perturbation strategies, questioning their reliability for mechanistic interpretability.

## Executive Summary
This paper investigates the stability and reliability of circuit discovery methods in mechanistic interpretability by treating circuit discovery as a statistical estimation problem. The authors conduct a comprehensive analysis of variance and robustness across multiple perturbation strategies including input resampling, prompt paraphrasing, hyperparameter variation, and noise injection in causal analysis. Their results show that EAP-IG exhibits high structural variance and sensitivity to hyperparameters across models and tasks, with gradient-based estimators amplifying underlying variance through approximation noise. Based on these findings, the authors advocate for routine reporting of stability metrics through bootstrap resampling and hyperparameter sensitivity analysis to promote more rigorous and statistically grounded interpretability research.

## Method Summary
The study analyzes variance in circuit discovery using the EAP-IG library across three tasks (IOI, SVA, Greater-Than) and three models (gpt2-small, Llama-3.2-1B, Llama-3.2-1B-Instruct). The authors employ four estimators (EAP, EAP-IG-inputs, EAP-IG-activations, clean-corrupted) and measure structural stability via pairwise Jaccard index, faithfulness via circuit error and KL divergence, and coefficient of variation (CV) of edge scores. The methodology includes bootstrap resampling (n=100), noise amplitude variation, aggregation variants (mean/median), and patching variants. Exact CMA scores are computed for baseline variance on gpt2-small/IOI only due to computational costs.

## Key Results
- EAP-IG circuits show high structural variance with bootstrap Jaccard μ=0.561 and CV often exceeding 1 for edge scores
- Circuit error increases substantially under bootstrap resampling (μ=0.440 vs 0.150 for paraphrasing)
- Gradient-based EAP estimators amplify intrinsic CMA variance rather than providing unbiased estimates
- Hyperparameter sensitivity analysis reveals that circuit structure changes significantly with minor parameter variations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal mediation scores are random variables with intrinsic variance, not fixed properties of model components.
- Mechanism: The Natural Indirect Effect (NIE) score depends jointly on the input distribution and counterfactual distribution, with different input-counterfactual pairs yielding substantially different importance scores for the same edge.
- Core assumption: The underlying causal effect is fundamentally input-dependent rather than a global property of the circuit.
- Evidence anchors: [abstract] "both the underlying CMA scores and their fast approximations exhibit high intrinsic variance across inputs"; [section 5.1] "edge scores exhibit a standard deviation often close to half their mean (CV≈0.5)"

### Mechanism 2
- Claim: Gradient-based EAP estimators amplify underlying variance through approximation noise.
- Mechanism: EAP uses local Taylor expansion and gradient information to approximate the global intervention effect, introducing estimation error that compounds intrinsic CMA variance.
- Core assumption: Gradient-based approximations systematically increase variance rather than providing unbiased estimates with independent noise.
- Evidence anchors: [abstract] "gradient-based estimators amplifying this variability"; [section 5.1] "EAP approximation exacerbates this issue... significantly increases the CV, with the standard deviation often exceeding the mean (CV > 1)"

### Mechanism 3
- Claim: Circuit selection thresholds convert noisy continuous scores into brittle discrete structures.
- Mechanism: The pipeline aggregates importance scores across samples then applies hard thresholds, creating discrete instability from continuous noise when score distributions have high variance and overlapping populations.
- Core assumption: The aggregation-to-threshold pipeline creates discrete instability from continuous noise.
- Evidence anchors: [abstract] "circuits discovered through these methods are highly sensitive to data resampling, hyperparameter choices"; [section 5.2] "Bootstrap resampling yields the lowest structural consistency (Jaccard μ=0.561)"

## Foundational Learning

- **Concept**: Causal Mediation Analysis (CMA)
  - Why needed here: CMA is the theoretical foundation for all circuit discovery methods; understanding the Natural Indirect Effect formula is essential to see why variance enters at the estimation layer.
  - Quick check question: Given a clean input x and corrupted x_corr, what does the score S(e, x, x_corr) measure?

- **Concept**: Coefficient of Variation (CV = σ/|μ|)
  - Why needed here: CV is the primary metric used throughout to quantify instability; a CV of 0.5 means the standard deviation is half the mean—high relative noise.
  - Quick check question: If edge scores have mean 0.1 and std 0.15, what is the CV and is this considered high variance in this paper's framing?

- **Concept**: Bootstrap Resampling for Stability Estimation
  - Why needed here: The paper's primary recommendation is routine bootstrap resampling to quantify stability; you need to understand how resampling reveals estimator variance.
  - Quick check question: Why does bootstrap resampling reveal variance that a single point estimate hides?

## Architecture Onboarding

- Component map: Input Distribution D → (x, x_corr) sampling → Counterfactual Generator → CMA Estimator → Edge Importance Scores S(e) → Aggregation Function → Global Score Ŝ(e) → Threshold/Selection → Final Circuit C

- Critical path: The choice of counterfactual (x_corr generation) → estimator (EAP vs EAP-IG) → aggregation function. Each stage compounds variance.

- Design tradeoffs:
  - Exact CMA: Low bias, high compute cost (2×N_edges×N_samples forward passes)
  - EAP: Fast but high variance (CV > 1 observed)
  - EAP-IG-inputs vs EAP-IG-activations: Different interpolation strategies yield different edge sets (Jaccard as low as 0.086 between variants)
  - Mean vs median aggregation: Mean sensitive to outliers; median more robust but may miss rare important edges

- Failure signatures:
  - High CV (>0.5) in edge scores across samples
  - Low pairwise Jaccard (<0.6) between bootstrap circuits
  - Multimodal Jaccard distributions indicating non-identifiability
  - Circuit error increases after bootstrap resampling (μ=0.440 for bootstrap vs 0.150 for paraphrasing)

- First 3 experiments:
  1. Baseline variance check: Run exact CMA on 100 samples from IOI task; compute CV for top 50 edges. If CV > 0.3, intrinsic variance is present regardless of estimator choice.
  2. Estimator comparison: On fixed dataset, compare EAP, EAP-IG-inputs, EAP-IG-activations. Compute pairwise Jaccard between resulting circuits. If <0.5, estimators are not converging.
  3. Bootstrap stability test: Generate 50 bootstrap resamples of your dataset; run full EAP-IG pipeline on each. Report mean Jaccard and circuit error variance. If Jaccard CV > 0.3, circuits are not stable enough for reliable conclusions.

## Open Questions the Paper Calls Out

- Can Bayesian structure learning formalize probabilistic circuit discovery to output posterior distributions over graphs rather than single point estimates? [explicit] The authors state: "Since the underlying CMA scores are distributions, the output of an MI method could be a posterior distribution over graphs... Future work could formalize this using Bayesian structure learning approaches."

- What proportion of observed circuit variance is attributable to estimator noise versus intrinsic variability in the underlying causal mechanisms? [explicit] "Future work should aim to decompose the total observed variance into estimator variance (noise from the gradient estimation) and intrinsic variance (true fluctuations in the mechanism across inputs)."

- Do newer circuit discovery methods (HAP, RelP) exhibit similar instability, or do their different heuristics act as stabilizing regularizers? [explicit] "While newer methods, such as HAP or RelP, use different heuristics, they remain downstream of CMA and likely inherit its volatility. However, their specific rules may act as stabilizing regularizers."

- Would weighted stability metrics reveal a stable "functional core" of circuits despite a fluctuating periphery? [explicit] "Our stability metrics treat all edges as equally important, whereas weighted stability metrics might reveal a stable 'functional core' of the circuit despite a fluctuating periphery."

## Limitations
- The paper doesn't establish whether high variance represents meaningful signal or irreducible noise that prevents reliable circuit identification.
- Analysis focuses on three specific tasks and models, leaving generalizability across architectures and problem domains open to question.
- Without ground-truth circuit comparison, it's unclear if the reported variance levels are inherent to the estimation problem or specific to current methods.

## Confidence

**High Confidence**: The empirical observations of high variance across multiple perturbation strategies (bootstrap, paraphrasing, hyperparameter variation) are robust and well-documented. The statistical framework for measuring variance (CV, Jaccard stability) is sound.

**Medium Confidence**: The claim that gradient-based estimators amplify intrinsic variance is supported by the data but could reflect implementation-specific factors rather than a fundamental property of the approximation method.

**Medium Confidence**: The interpretation that high variance undermines the reliability of circuit discovery conclusions is reasonable but not definitive, as the paper doesn't establish whether stable circuits are necessary for useful interpretability.

## Next Checks

1. **Ground-truth comparison**: Apply the same variance analysis to synthetic circuits where ground-truth edge importance is known to establish whether observed variance levels are inherent to the estimation problem.

2. **Cross-architecture stability**: Replicate the variance analysis on larger models (Llama-3B, GPT-2-XL) and different architectures (RNNs, transformers with different attention patterns) to determine if high variance is universal or architecture-specific.

3. **Alternative aggregation methods**: Test soft thresholding, probabilistic circuit representations, or Bayesian approaches that might better handle continuous importance scores without the brittleness of hard thresholds.