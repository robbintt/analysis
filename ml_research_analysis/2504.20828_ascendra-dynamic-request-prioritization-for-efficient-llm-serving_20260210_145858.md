---
ver: rpa2
title: 'Ascendra: Dynamic Request Prioritization for Efficient LLM Serving'
arxiv_id: '2504.20828'
source_url: https://arxiv.org/abs/2504.20828
tags:
- requests
- request
- ascendra
- prefill
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Ascendra is a large language model serving system that improves
  goodput by dynamically partitioning GPU resources into low-priority and high-priority
  instances. The core idea is to assign evolving priorities to requests based on their
  proximity to missing their TTFT SLO, and to proactively offload urgent requests
  to the high-priority instance while using the low-priority instance for high-throughput
  batch processing.
---

# Ascendra: Dynamic Request Prioritization for Efficient LLM Serving

## Quick Facts
- arXiv ID: 2504.20828
- Source URL: https://arxiv.org/abs/2504.20828
- Authors: Azam Ikram; Xiang Li; Sameh Elnikety; Saurabh Bagchi
- Reference count: 40
- Improves goodput by up to 1.7× compared to vLLM and Sarathi-Serve while maintaining both TTFT and TBT SLOs

## Executive Summary
Ascendra is a large language model serving system that improves goodput by dynamically partitioning GPU resources into low-priority and high-priority instances. The system assigns evolving priorities to requests based on their proximity to missing their TTFT SLO, and proactively offloads urgent requests to the high-priority instance while using the low-priority instance for high-throughput batch processing. By only transferring prompts during offload (avoiding expensive KV cache movement), Ascendra achieves superior SLO attainment without requiring specialized interconnects.

## Method Summary
Ascendra implements a two-tier architecture built on vLLM/Sarathi-Serve, partitioning 3x A100-80G GPUs into 2 Low-Priority (LP) and 1 High-Priority (HP) instances. The LP instance uses out-of-order scheduling (Earliest Deadline First) to maximize throughput, while the HP instance handles offloaded requests using elastic batch sizing to minimize latency. A performance model predicts when requests will miss their TTFT SLO and triggers proactive offloading by transferring only prompts (not KV cache) to the HP instance. The system requires 5 minutes of profiling to build a regression model for execution time prediction.

## Key Results
- Improves goodput by up to 1.7× compared to vLLM and Sarathi-Serve
- HP requests experience 4× lower scheduling delay than LP requests
- Regression model achieves R² = 0.998 for prefill batch predictions with <10% error

## Why This Works (Mechanism)

### Mechanism 1: Out-of-Order Priority-Based Scheduling
Processing requests based on dynamically evolving urgency rather than arrival order reduces head-of-line blocking and improves SLO attainment. Each request receives a priority value computed from prompt length (predicts prefill time) and time already spent waiting. The scheduler selects requests for batching in priority order, not FCFS order, allowing urgent requests to jump the queue.

### Mechanism 2: Two-Tier Instance Partitioning (LP/HP)
Partitioning GPU resources into throughput-optimized (LP) and latency-optimized (HP) instances allows simultaneous pursuit of conflicting objectives. LP instances use out-of-order scheduling and piggybacking to maximize token throughput. HP instances reserve capacity for urgent requests, using FCFS with prefill-prioritization to minimize scheduling delay.

### Mechanism 3: Performance Model-Guided Proactive Offloading
Analytical performance models can predict when a request will miss its TTFT SLO, enabling proactive offloading before violation. A regression model predicts batch execution time from FLOPs and memory access estimates. When predicted completion time exceeds TTFT deadline minus safety margin, the request is flagged for offloading.

## Foundational Learning

- **Concept: Prefill-Decode Phase Asymmetry**
  - Why needed here: Understanding why TTFT and TBT are fundamentally in tension—prefill is compute-bound and monopolizes GPU, while decode is memory-bound and benefits from batching but stalls when preempted.
  - Quick check question: Why does Sarathi-Serve's chunked prefill improve TBT but hurt TTFT?

- **Concept: KV-Cache Memory Pressure**
  - Why needed here: Explains why decode batch size is limited, why preemption occurs, and why DistServe requires expensive interconnects for KV transfer—central to understanding Ascendra's prompt-only offload design.
  - Quick check question: How much GPU memory does a single token's KV cache consume for LLaMA3-8B in FP16?

- **Concept: SLO Attainment vs. Raw Throughput**
  - Why needed here: Ascendra optimizes "goodput"—fraction of requests meeting both TTFT and TBT SLOs—rather than raw tokens/second. This shifts the objective from utilization maximization to deadline satisfaction.
  - Quick check question: Under what load conditions does vLLM achieve high throughput but low goodput?

## Architecture Onboarding

- **Component map:**
  Controller -> LP Instance -> Performance Model -> Offload Queue
  Controller -> HP Instance -> Ticket Mechanism

- **Critical path:**
  1. Request arrives → Controller routes to LP (or HP if ticket active)
  2. LP scheduler evaluates priority, batches decodes + prefills
  3. Performance model checks if queued requests will miss SLO → flag for offload
  4. Controller receives offload signal → transfers prompt to HP
  5. HP executes prefill immediately, then decodes

- **Design tradeoffs:**
  - LP:HP ratio—Paper uses 2:1 for 3-GPU setups; more HP improves tail latency but reduces throughput
  - Offload threshold—Too aggressive wastes HP capacity; too conservative causes SLO violations
  - Value function policy—EDF vs. SJF changes which requests benefit (Figure 11)

- **Failure signatures:**
  - Chronic HP queue buildup: HP scheduling delay rises → requests violate TTFT despite offload
  - Memory exhaustion in LP: Frequent preemption → throughput collapse
  - Performance model drift: Prediction error >10% → offload timing fails

- **First 3 experiments:**
  1. Reproduce Figure 9 (Goodput vs. QPS) on your target model/dataset to establish baseline and validate LP:HP ratio for your workload
  2. Ablate offload threshold: Sweep threshold values to find optimal urgency detection point for your SLO strictness
  3. Profile performance model accuracy: Collect actual batch latencies, fit regression, validate R² on held-out batches before production deployment

## Open Questions the Paper Calls Out

### Open Question 1
How can the optimal configuration of Low-Priority (LP) and High-Priority (HP) instances be determined dynamically or at scale without relying on offline combinatorial simulation? The paper states that while subgroup configurations outperform baselines, "they do not necessarily represent the global optimum" and currently require "combinatorial search through our simulator" to discover optimal settings.

### Open Question 2
How does Ascendra's performance compare to disaggregated architectures (e.g., DistServe) when both systems utilize specialized high-bandwidth interconnects? Section 8.1 explicitly excludes DistServe and Llumnix from the experimental baselines due to their requirement for specialized hardware (NVLink, NVSwitch), leaving the relative efficiency of Ascendra's prompt-offloading strategy versus KV-cache transfer unresolved.

### Open Question 3
How robust is the regression-based performance model when facing significant distribution shifts in prompt lengths or model architectures not seen during the initial profiling phase? Section 5.1 notes that the performance model requires 5 minutes of data collection to build a regression model with <10% error, but it does not analyze the model's accuracy under sudden workload drift or concept shift.

## Limitations
- Performance model requires 5 minutes of profiling and may drift with hardware changes
- Evaluation limited to Poisson-distributed arrivals at 6-8 QPS, not bursty traffic patterns
- Optimal LP:HP ratio configuration requires offline combinatorial simulation

## Confidence
**High Confidence Claims:**
- Two-tier instance partitioning architecture is technically sound
- Out-of-order scheduling improves throughput for LP instances under controlled conditions
- Performance model can predict batch execution time with high accuracy on profiled hardware

**Medium Confidence Claims:**
- Proactive offloading successfully prevents SLO violations in practice
- Goodput improvements of 1.7× over baselines translate to other deployment scenarios
- The specific LP:HP ratio and offload threshold values are optimal

**Low Confidence Claims:**
- The system maintains both TTFT and TBT SLOs simultaneously under all load conditions
- The performance model remains accurate without periodic retraining in production
- The architecture scales to larger GPU counts or different GPU types without architectural changes

## Next Checks
1. Implement the regression model and measure prediction accuracy (R², MAE) on held-out batches across different times of day, batch sizes, and model variants. Track degradation over a 24-hour period to assess retraining frequency requirements.

2. Load test the system at 10-20 QPS with bursty arrival patterns (e.g., 50% of requests arrive within 100ms). Measure HP queue buildup, SLO violation rates, and goodput degradation compared to baseline methods.

3. Reconfigure the LP:HP ratio to 1:1 and 3:1 (same total GPU count) and measure the tradeoff between goodput and tail latency. Validate whether the 2:1 ratio remains optimal across different request mix ratios and SLO strictness levels.