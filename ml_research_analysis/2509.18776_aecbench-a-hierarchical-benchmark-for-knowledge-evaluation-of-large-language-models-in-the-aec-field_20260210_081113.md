---
ver: rpa2
title: 'AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language
  Models in the AEC Field'
arxiv_id: '2509.18776'
source_url: https://arxiv.org/abs/2509.18776
tags:
- design
- knowledge
- evaluation
- tasks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A hierarchical evaluation framework was proposed to assess LLM
  capabilities across five cognitive levels (Knowledge Memorization, Understanding,
  Reasoning, Calculation, Application) for AEC tasks. 4,800 questions across 23 tasks
  were curated by domain engineers and validated by experts.
---

# AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field

## Quick Facts
- arXiv ID: 2509.18776
- Source URL: https://arxiv.org/abs/2509.18776
- Reference count: 40
- A hierarchical benchmark evaluated 9 LLMs across 5 cognitive levels (Knowledge Memorization to Application) for 23 AEC tasks using 4,800 questions, revealing performance declines in higher-order reasoning and application tasks.

## Executive Summary
This paper introduces AECBench, a hierarchical evaluation framework designed to assess Large Language Models' capabilities in the Architecture, Engineering, and Construction (AEC) domain across five cognitive levels: Knowledge Memorization, Understanding, Reasoning, Calculation, and Application. The benchmark consists of 4,800 questions across 23 tasks, curated by domain engineers and validated by experts. An automated "LLM-as-a-Judge" pipeline with expert-defined rubrics enables scalable evaluation of open-ended responses. Testing nine leading LLMs revealed significant performance declines from lower-order to higher-order cognitive tasks, with advanced models showing substantial gaps in reasoning, calculation, and document generation.

## Method Summary
AECBench employs a one-shot evaluation methodology on the OpenCompass platform, testing nine LLMs across 4,800 questions spanning 23 tasks organized into five cognitive levels. The evaluation uses task-specific metrics: accuracy for multiple-choice questions, F1 variants for classification/extraction tasks, and rubric-based scoring for open-ended generation tasks. For subjective evaluations, DeepSeek-R1 serves as the "Judge" model, applying expert-defined rubrics to grade responses. To address systematic scoring bias in the Judge model, isotonic and piecewise linear regression techniques calibrate the raw scores against human expert benchmarks, reducing Mean Absolute Error from 2.947 to 1.926.

## Key Results
- LLMs show consistent performance decline moving from lower-order (Level 1-2) to higher-order (Level 3-5) cognitive tasks
- Models achieve high accuracy on tabular data conversion tasks (98.94% and 87.27%) when tables are converted to natural language or HTML context
- LLM-as-a-Judge with calibration reduces MAE from 2.947 to 1.926, though systematic bias in compressed scoring ranges remains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A hierarchical cognitive framework reveals the "knowledge-application gap" that flat benchmarks miss.
- **Mechanism:** By structuring evaluation into five ascending levels (Memorization → Understanding → Reasoning → Calculation → Application), the benchmark isolates specific failure modes. Models may excel at retrieving facts (Level 1) but fail to integrate them into multi-step reasoning or document generation (Level 5). This hierarchical dependency exposes that performance collapse is often an application deficit, not a knowledge deficit.
- **Core assumption:** LLM capabilities are not uniform across cognitive domains; proficiency in lower-order tasks (retrieval) does not predict proficiency in higher-order tasks (synthesis/creation).
- **Evidence anchors:**
  - [Abstract]: "revealing performance declines from lower-order to higher-order cognitive tasks."
  - [Section 5.3.1]: "This proves the model’s failure is not a knowledge deficit but an application deficit."
  - [Corpus]: *EngiBench* (corpus neighbor) supports the need for engineering-specific reasoning benchmarks beyond mathematical computation.
- **Break condition:** If a model demonstrates non-hierarchical capabilities (e.g., it fails memorization but succeeds at complex reasoning), the cognitive dependency assumption breaks.

### Mechanism 2
- **Claim:** Rubric-based "LLM-as-a-Judge" pipelines enable scalable evaluation of open-ended AEC documents only when calibrated for systematic bias.
- **Mechanism:** Open-ended responses (e.g., design proposals) cannot be graded via exact match. The paper uses a secondary LLM (DeepSeek-R1) to grade responses against expert-defined rubrics. This works because the LLM can parse semantic meaning, but it suffers from "compressed scoring" (over-rating bad work, under-rating good work). Calibration methods (Isotonic/Piecewise Regression) map these compressed scores back to the true human distribution, restoring reliability.
- **Core assumption:** The "Judge" LLM possesses sufficient domain knowledge to interpret the rubric and the generated content accurately.
- **Evidence anchors:**
  - [Abstract]: "LLM-as-a-Judge pipeline with expert-defined rubrics enabled scalable evaluation..."
  - [Section 6.1]: "...models tend to assign an average score of 4.25 to documents rated in the 0–2 range... isotonic regression... reduction in MAE from 2.947 to 1.926."
  - [Corpus]: No direct corpus evidence on calibration; mechanism is derived primarily from paper Section 6.1.
- **Break condition:** If the Judge LLM has a "self-enhancement bias" (favoring its own style) or lacks the specific reasoning capability required by the rubric, scores become invalid regardless of calibration.

### Mechanism 3
- **Claim:** Performance on tabular code data is restored by converting rigid formats into natural language or structured HTML context.
- **Mechanism:** LLMs struggle with the implicit spatial relationships in raw code tables (e.g., building codes). Performance declines because the model cannot align the query parameters to the correct table cell. Converting this tabular data into explicit textual descriptions (Approach #1) or structured HTML tags (Approach #2) provides the necessary semantic cues, allowing the model to retrieve the correct parameter via attention mechanisms rather than spatial inference.
- **Core assumption:** The performance drop is caused by input format misalignment, not a lack of parametric knowledge in the model weights.
- **Evidence anchors:**
  - [Section 6.2]: "The average accuracies on these two tasks improved from a baseline of 45.27% and 40.65% to 98.94% and 87.27% with Approach #1."
  - [Section 6.2]: "...decline stems from the model’s inability to properly encode knowledge from the code tables."
  - [Corpus]: *Arce* (corpus neighbor) discusses extracting info from specialized texts, aligning with the need for better data formatting in AEC.
- **Break condition:** If the table requires complex visual-spatial reasoning (e.g., interpreting spanning rows without HTML tags) that exceeds the context window's structural understanding, conversion may still fail.

## Foundational Learning

- **Concept:** **Bloom’s Taxonomy in Engineering Contexts**
  - **Why needed here:** The paper maps AEC tasks to cognitive levels (Remember, Understand, Apply, Analyze, Evaluate, Create). You cannot interpret the benchmark results without understanding that "Calculation" (Level 4) requires different internal processing than "Memorization" (Level 1).
  - **Quick check question:** Can you explain why a model might pass "Code Memorization" (Level 1) but fail "Compliance Checking" (Level 5.1), and what specific cognitive gap this indicates?

- **Concept:** **Calibration of Probabilistic Predictors**
  - **Why needed here:** The "LLM-as-a-Judge" outputs raw scores that are systematically biased. Understanding calibration (mapping outputs to true probabilities) is required to trust the automated evaluation of open-ended questions.
  - **Quick check question:** If an LLM judge systematically assigns scores between 4 and 6 on a 0-10 scale, which regression method mentioned in the paper helps map these back to the true "0-10" distribution?

- **Concept:** **Context Window vs. Parametric Knowledge**
  - **Why needed here:** The paper demonstrates that models fail on table-based questions not because they lack the knowledge, but because they can't parse the input format provided in the prompt (or lack thereof).
  - **Quick check question:** Why does providing a table as HTML context (Approach #2) yield better results than relying on the model's pre-trained memory of the table?

## Architecture Onboarding

- **Component map:**
  1. Data Layer: 4,800 questions across 23 tasks (curated by engineers, validated by experts)
  2. Evaluation Engine:
     - Objective Scorer: Exact match/F1 for MCQs, Classification, Extraction
     - Subjective Scorer: DeepSeek-R1 acting as "Judge" using expert rubrics
  3. Calibration Module: Isotonic/Piecewise regression models to correct Judge bias (trained on human-expert-scored validation set)

- **Critical path:**
  1. Task Definition: Map a workflow (e.g., Compliance Checking) to a cognitive level
  2. Rubric Design: Expert defines scoring criteria (essential for the "Judge" to work)
  3. Execution: Run model -> Get Output
  4. Scoring: Judge Model reads Output + Rubric -> Raw Score
  5. Calibration: Apply regression function to Raw Score -> Final Calibrated Score

- **Design tradeoffs:**
  - Rubric Granularity vs. Scalability: Highly detailed rubrics improve evaluation accuracy but increase the upfront expert labor cost
  - Table Context (Text vs. HTML): Textual description (Approach #1) offers highest accuracy (98%) but is manual/slow; HTML conversion (Approach #2) is automated (scalable) but slightly lower accuracy (87%)
  - Judge Model Selection: Using a reasoning-optimized model (DeepSeek-R1) as a judge is more accurate but slower/expensive than using a general model

- **Failure signatures:**
  - "Compressed Scoring": The Judge model clusters all scores in a narrow band (e.g., 5-7) failing to distinguish terrible from excellent work
  - "Knowledge-Application Gap": High scores on Level 1-2, sudden drop on Level 3-5
  - "Tabular Blindness": Low scores on tasks like "Design Decision Formulation (Tabular Data)" which recover only when context is explicitly provided

- **First 3 experiments:**
  1. Baseline Profiling: Run your target LLM on the full AECBench to identify which of the 5 cognitive levels is the bottleneck (e.g., is it failing Reasoning or Calculation?)
  2. Context Augmentation Test: Take a failing "Tabular Data" task (e.g., Task 3-3), implement the HTML injection strategy (Approach #2), and measure the delta in accuracy
  3. Judge Calibration Validation: Select 50 open-ended responses, have a human expert grade them, and compare against the raw "LLM-as-a-Judge" output to determine if you need to apply the Isotonic Regression calibration layer

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LLMs be effectively evaluated on multimodal inputs, specifically architectural drawings, to reflect the full spectrum of information modalities in the AEC field?
- **Basis in paper:** [explicit] The Conclusion states, "Future work would incorporate tasks related to multimodal data, such as architectural drawing recognition, to better reflect the full spectrum of information modalities in the AEC field."
- **Why unresolved:** The current AECBench framework and dataset are limited exclusively to text-based inputs and do not assess visual or spatial reasoning capabilities.
- **What evidence would resolve it:** The release of an extended benchmark dataset containing image-based tasks (e.g., CAD detail recognition) and corresponding performance metrics for vision-language models.

### Open Question 2
- **Question:** What methods can resolve the trade-off between the high fidelity of manual text description and the scalability of automated HTML conversion when processing tabular knowledge in building codes?
- **Basis in paper:** [inferred] The Discussion highlights that while manual descriptions for tables (Approach 1) yield higher accuracy (98.94%), they lack scalability, whereas automated HTML conversion (Approach 2) is scalable but struggles with complex layouts like spanning cells.
- **Why unresolved:** Current automated parsers cannot losslessly reproduce the complex layouts found in engineering code tables, creating a bottleneck for robust knowledge retrieval.
- **What evidence would resolve it:** A novel algorithm or formatting standard that achieves high fidelity (>95% accuracy) on complex tabular data without requiring manual expert intervention.

### Open Question 3
- **Question:** Are isotonic and piecewise linear regression sufficient to calibrate "LLM-as-a-Judge" scoring for safety-critical, rubric-based AEC assessments?
- **Basis in paper:** [inferred] The Discussion identifies a systematic bias where models compress scoring ranges (overestimating low scores, underestimating high scores), though calibration reduced Mean Absolute Error (MAE).
- **Why unresolved:** While calibration improved MAE, the paper does not verify if the bias is fully eliminated for "safety-critical" pass/fail thresholds or edge cases.
- **What evidence would resolve it:** Evaluation results showing calibrated scores maintaining a consistent error distribution and high correlation with human experts across the entire 0-10 scoring spectrum.

## Limitations
- The benchmark currently lacks multimodal evaluation capabilities for architectural drawings and other visual AEC data
- Dataset and detailed rubrics are not yet publicly released, limiting reproducibility
- Systematic scoring bias in LLM-as-a-Judge remains partially unresolved even after calibration

## Confidence
- Dataset design and cognitive framework: High
- Evaluation methodology (LLM-as-a-Judge): Medium
- Calibration effectiveness for safety-critical thresholds: Low
- Generalizability to other domains: Medium

## Next Checks
1. Verify the performance drop pattern across cognitive levels by running a subset of tasks on your target LLM
2. Test table context conversion by comparing raw table performance against HTML-converted table performance on tabular code tasks
3. Validate Judge calibration by comparing calibrated scores against human expert scores on a small validation set