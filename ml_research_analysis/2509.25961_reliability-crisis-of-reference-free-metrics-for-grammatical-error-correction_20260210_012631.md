---
ver: rpa2
title: Reliability Crisis of Reference-free Metrics for Grammatical Error Correction
arxiv_id: '2509.25961'
source_url: https://arxiv.org/abs/2509.25961
tags:
- metrics
- systems
- evaluation
- score
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper exposes significant vulnerabilities in reference-free\
  \ metrics for grammatical error correction (GEC), revealing that adversarial systems\
  \ can exploit these metrics to achieve unjustifiably high scores. The authors propose\
  \ attack strategies for four metrics\u2014SOME, Scribendi, IMPARA, and LLM-based\
  \ metrics\u2014and demonstrate that their adversarial systems outperform state-of-the-art\
  \ GEC systems on the BEA-2019 development set."
---

# Reliability Crisis of Reference-free Metrics for Grammatical Error Correction

## Quick Facts
- **arXiv ID**: 2509.25961
- **Source URL**: https://arxiv.org/abs/2509.25961
- **Reference count**: 36
- **Primary result**: Adversarial systems can achieve higher scores than state-of-the-art GEC systems on reference-free metrics, undermining their reliability

## Executive Summary
This paper exposes significant vulnerabilities in reference-free metrics for grammatical error correction (GEC), revealing that adversarial systems can exploit these metrics to achieve unjustifiably high scores. The authors propose attack strategies for four metrics—SOME, Scribendi, IMPARA, and LLM-based metrics—and demonstrate that their adversarial systems outperform state-of-the-art GEC systems on the BEA-2019 development set. For example, their SOME adversarial system achieves a score of 1.013 compared to the previous best of 0.836, while their Scribendi adversarial system reaches 4179 versus 1821. These results undermine the reliability of automatic GEC evaluation, as metrics can be manipulated to rank adversarial systems higher than legitimate ones. The study suggests metric ensembles as a potential short-term solution and highlights the need for more robust evaluation methods that can withstand adversarial attacks.

## Method Summary
The authors develop adversarial attack strategies specifically designed to manipulate four reference-free GEC metrics: SOME, Scribendi, IMPARA, and LLM-based metrics. They construct adversarial systems that optimize for these metric scores rather than grammatical correctness, then evaluate these systems on the BEA-2019 development set alongside state-of-the-art GEC systems. The experiments systematically compare metric scores across different system types to demonstrate how adversarial approaches can achieve superior metric performance through exploitation rather than genuine grammatical improvement.

## Key Results
- Adversarial systems achieve higher metric scores than state-of-the-art GEC systems on reference-free metrics
- SOME adversarial system achieves 1.013 versus 0.836 for previous best on BEA-2019
- Scribendi adversarial system reaches 4179 versus 1821 for previous best
- Results demonstrate that reference-free metrics can be manipulated to rank adversarial systems higher than legitimate ones

## Why This Works (Mechanism)
The attack strategies exploit specific weaknesses in how reference-free metrics evaluate grammatical correctness without human reference comparisons. These metrics rely on internal consistency checks, statistical patterns, or language model likelihoods that can be gamed by systems optimized for metric-specific features rather than actual grammatical improvement. The adversarial systems learn to generate text that satisfies the mathematical criteria of these metrics while potentially introducing grammatical errors or unnatural constructions that would be penalized by human evaluation.

## Foundational Learning

**Grammatical Error Correction (GEC)**: The task of automatically correcting grammatical errors in text; needed to understand the evaluation context and why reference-free metrics were developed as an alternative to reference-based evaluation.

**Reference-free Metrics**: Evaluation metrics that assess GEC output without comparing to human-corrected references; needed because obtaining reference corrections is expensive and reference-based metrics have limitations in capturing grammatical quality.

**Adversarial Attacks in NLP**: Techniques that manipulate model inputs to produce desired outputs; needed to understand how the attack strategies exploit metric vulnerabilities rather than improving actual grammatical quality.

**Metric Ensembles**: Combining multiple evaluation metrics to reduce individual metric weaknesses; needed as the proposed short-term solution to mitigate the vulnerability of single-reference-free metrics.

**Language Model Likelihoods**: Probabilistic scoring based on how likely text is under a language model; needed to understand how LLM-based metrics can be manipulated through adversarial optimization.

## Architecture Onboarding

**Component Map**: Adversarial System -> Reference-free Metrics (SOME, Scribendi, IMPARA, LLM) -> Score Evaluation -> Comparison with State-of-the-art GEC

**Critical Path**: Adversarial system generation → Metric score optimization → BEA-2019 evaluation → Performance comparison

**Design Tradeoffs**: Single metric optimization vs. generalization across metrics; computational cost of adversarial generation vs. evaluation reliability; short-term metric ensemble solutions vs. long-term metric redesign

**Failure Signatures**: Unusually high metric scores from systems that produce grammatically incorrect or unnatural text; discrepancies between metric rankings and human evaluation; specific patterns in adversarial outputs that exploit metric calculation methods

**First Experiments**:
1. Implement attack strategies on a small subset of BEA-2019 to verify metric manipulation capability
2. Test whether human evaluators rank adversarial outputs lower than state-of-the-art GEC outputs despite higher metric scores
3. Evaluate metric ensemble performance against the same adversarial attacks to test proposed solution

## Open Questions the Paper Calls Out
None

## Limitations
- Attack strategies may not generalize to all GEC systems or evaluation settings beyond BEA-2019
- Experiments focus primarily on one dataset, potentially limiting representativeness
- Study does not explore computational cost or practical feasibility of large-scale adversarial attacks
- Proposed metric ensemble solution lacks empirical validation in the paper

## Confidence

**High confidence**: The demonstration that adversarial systems can achieve higher scores than state-of-the-art GEC systems on reference-free metrics is well-supported by the experimental results. The specific numerical improvements (e.g., SOME metric score of 1.013 vs 0.836) are clearly presented and reproducible.

**Medium confidence**: The claim that these findings undermine the reliability of automatic GEC evaluation is reasonable but could benefit from additional analysis of how frequently such attacks might occur in practice or whether they represent edge cases versus systemic vulnerabilities.

**Low confidence**: The suggestion that metric ensembles provide an effective short-term solution is speculative, as the paper does not provide experimental evidence demonstrating the effectiveness of ensemble approaches against the presented adversarial attacks.

## Next Checks
1. Test the adversarial attack strategies on additional GEC datasets beyond BEA-2019 to assess generalizability across different evaluation scenarios and language varieties.

2. Evaluate whether the same attack strategies can successfully manipulate human evaluators or whether the vulnerabilities are specific to automatic reference-free metrics.

3. Implement and test metric ensemble approaches experimentally to determine whether combining multiple reference-free metrics provides meaningful protection against adversarial manipulation.