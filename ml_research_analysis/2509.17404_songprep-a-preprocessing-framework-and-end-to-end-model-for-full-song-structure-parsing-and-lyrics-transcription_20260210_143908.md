---
ver: rpa2
title: 'SongPrep: A Preprocessing Framework and End-to-end Model for Full-song Structure
  Parsing and Lyrics Transcription'
arxiv_id: '2509.17404'
source_url: https://arxiv.org/abs/2509.17404
tags:
- song
- data
- songprepe2e
- structure
- lyrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of preprocessing raw song data
  for training song generation models by proposing an automated framework, SongPrep,
  which performs source separation, structure analysis, and lyric recognition to produce
  structured datasets. The authors further introduce SongPrepE2E, an end-to-end model
  combining MuCodec with a pretrained language model to directly extract structured
  lyrics from full songs, improving efficiency and accuracy.
---

# SongPrep: A Preprocessing Framework and End-to-end Model for Full-song Structure Parsing and Lyrics Transcription

## Quick Facts
- **arXiv ID:** 2509.17404
- **Source URL:** https://arxiv.org/abs/2509.17404
- **Reference count:** 0
- **Primary result:** Introduces SongPrepE2E, an end-to-end model that directly extracts structured lyrics from full songs, achieving DER of 15.8% and WER of 24.3% on SSLD-200 dataset.

## Executive Summary
This paper addresses the challenge of preprocessing raw song data for training song generation models by proposing an automated framework, SongPrep, which performs source separation, structure analysis, and lyric recognition to produce structured datasets. The authors further introduce SongPrepE2E, an end-to-end model combining MuCodec with a pretrained language model to directly extract structured lyrics from full songs, improving efficiency and accuracy. A benchmark dataset, SSLD-200, is created to evaluate these models. SongPrepE2E achieves lower Diarization Error Rate (DER) and Word Error Rate (WER) than the multi-stage pipeline, with DER of 15.8% and WER of 24.3%, and demonstrates superior performance in downstream song generation tasks, scoring 0.48 points higher in Musicality Structure and 1.70 points higher in Lyric Matching Degree compared to baseline methods.

## Method Summary
The SongPrep framework automates the extraction of structured lyrics from raw songs through a multi-stage pipeline: Demucs source separation isolates vocals, All-In-One with DPRNN blocks analyzes song structure into 7 categories, Zipformer ASR transcribes lyrics, and wav2vec2 alignment calibrates boundaries. SongPrepE2E replaces this pipeline with an end-to-end approach using MuCodec to tokenize audio into discrete tokens (25 Hz, 16,384 vocabulary) which are paired with structured lyrics for supervised fine-tuning of Qwen2-7B. The framework is evaluated on SSLD-200, a 200-song benchmark dataset with manual annotations, and demonstrates superior performance in both structure parsing and lyric transcription accuracy.

## Key Results
- SongPrepE2E achieves Diarization Error Rate (DER) of 15.8% and Word Error Rate (WER) of 24.3% on SSLD-200 dataset
- End-to-end model processes complete songs up to four minutes with Real Time Factor (RTF) of 0.108, 2× faster than the multi-stage pipeline (RTF 0.235)
- SongPrepE2E scores 0.48 points higher in Musicality Structure and 1.70 points higher in Lyric Matching Degree in downstream generation tasks
- Structure analysis DER improves from 25.0% to 16.1% by incorporating DPRNN blocks for global context modeling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** End-to-end structured lyrics recognition via LLM outperforms multi-stage pipelines for full-song processing.
- **Mechanism:** MuCodec discretizes audio into tokens (25 Hz, codebook size 16,384) which are paired with structured lyrics for supervised fine-tuning of Qwen2-7B. The LLM's pretrained semantic knowledge and ability to process full-song context (up to 4 minutes) compensates for information loss that occurs when chunking audio in traditional ASR systems.
- **Core assumption:** Audio tokens from MuCodec preserve sufficient semantic and temporal information for the LLM to learn the mapping to structured lyrics.
- **Evidence anchors:**
  - [abstract] "SongPrepE2E achieves low Diariation Error Rate (DER) and Word Error Rate (WER) on the proposed SSLD-200 dataset"
  - [section 2.3] "SongPrepE2E achieves better performance compared to SongPrep... can process a complete song up to four minutes"
  - [corpus] Limited corpus evidence for LLM-based lyrics extraction; related work focuses on symbolic generation and control rather than transcription
- **Break condition:** If audio tokenization loses temporal resolution or semantic content, the LLM cannot accurately predict timestamps or lyrics; WER would exceed chunked ASR baselines.

### Mechanism 2
- **Claim:** Error propagation in multi-stage pipelines can be partially mitigated by bidirectional correction between structure analysis and lyric recognition modules.
- **Mechanism:** The pipeline uses wav2vec2-based word alignment to calibrate structural analysis results, preventing instrumental sections from being mislabeled as verse/chorus. However, structure errors still propagate forward to lyric recognition (WER increases from 25.8% to 27.7% in the full pipeline vs. isolated module).
- **Core assumption:** Word-level alignment from an ASR system provides reliable boundary information to correct structural segmentation errors.
- **Evidence anchors:**
  - [abstract] "framework streamlines key processes such as source separation, structure analysis, and lyric recognition"
  - [section 2.2] "incorporate a word alignment module... helps prevent large sections of instrumental accompaniment from being incorrectly labeled"
  - [corpus] No direct corpus evidence for cross-module error correction in song processing pipelines
- **Break condition:** If ASR alignment errors exceed structure analysis errors, calibration degrades rather than improves DER.

### Mechanism 3
- **Claim:** Global context modeling via DPRNN blocks significantly improves structure analysis accuracy over purely local attention mechanisms.
- **Mechanism:** Inserting Dual-Path RNN blocks after every three All-In-One blocks enables modeling of global semantic dependencies across the full song. Combined with bilingual fine-tuning (3,700 songs) and label set reduction (10→7 categories), this reduced DER from 25.0% to 16.1%.
- **Core assumption:** Song structure follows learnable global patterns that require temporal modeling beyond local neighborhood attention.
- **Evidence anchors:**
  - [abstract] "SongPrepE2E achieves... DER of 15.8%"
  - [section 2.1] "dilated neighborhood attention... falls short in modeling global song structure. To compensate, we inserted a Dual-Path RNN"
  - [corpus] "Segment-Factorized Full-Song Generation" paper similarly emphasizes structure-aware segment generation
- **Break condition:** If training data scale is insufficient (<3,700 songs), global context modeling may overfit; authors note "main performance bottleneck remains the quantity of training data."

## Foundational Learning

- **Concept: Source Separation for Vocals**
  - Why needed here: Songs contain mixed audio (vocals, drums, bass, instruments); ASR performance degrades severely on mixed audio (Whisper WER: 47.2% → 27.7% after Demucs separation).
  - Quick check question: Can you explain why separating vocals before ASR improves WER by ~20 percentage points?

- **Concept: Diariation Error Rate (DER)**
  - Why needed here: Primary metric for structure analysis; simultaneously captures label accuracy and boundary timing accuracy. Used to evaluate All-In-One + DPRNN improvements.
  - Quick check question: How does DER differ from WER in what it measures for song structure?

- **Concept: Residual Vector Quantization (RVQ)**
  - Why needed here: Core of MuCodec tokenization; enables LLM to process audio as discrete tokens. Produces 25 Hz token rate with 16,384 vocabulary size.
  - Quick check question: Why would a 25 Hz token rate be sufficient for lyrics but potentially limiting for other audio tasks?

## Architecture Onboarding

- **Component map:** Audio → Demucs (source separation) → All-In-One + DPRNN (structure analysis) → Zipformer ASR (lyric transcription) → WER-FIX (quality filtering) → SSLD-200 dataset. For SongPrepE2E: Audio → MuCodec (tokenization) → Qwen2-7B (LLM inference) → structured lyrics output.

- **Critical path:** Audio → MuCodec tokenization → LLM inference → structured lyrics output. For training: raw songs → SongPrep pipeline → filtered audio-text pairs → SFT of LLM.

- **Design tradeoffs:**
  - WER<0.3 filter (640B tokens) vs. WER<0.1 (240B tokens): Looser filter yields better performance (24.3% vs 27.3% WER), likely due to overfitting risk with smaller datasets
  - End-to-end (RTF 0.108) vs. multi-stage (RTF 0.235): 2× faster inference but requires 2M songs for training

- **Failure signatures:**
  - High DER with low training data: Indicates insufficient structure examples for global context learning
  - WER increasing in pipeline vs. isolated modules: Structure analysis errors propagating to ASR
  - LLM hallucination on long songs: May exceed 4-minute training context window

- **First 3 experiments:**
  1. Reproduce SSLD-200 baseline: Test SongPrep pipeline on 10 held-out songs, measure DER and WER against ground truth annotations
  2. Ablate DPRNN blocks: Compare All-In-One vs. All-In-One + DPRNN on structure analysis to isolate global context contribution
  3. Token-to-audio alignment verification: Sample 50 MuCodec tokens, decode back to audio, measure reconstruction quality to validate semantic preservation before LLM training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does scaling the structure analysis training dataset beyond 3,700 songs yield proportional reductions in Diarization Error Rate (DER)?
- Basis in paper: [explicit] The authors note that "the main performance bottleneck remains the quantity of training data" and explicitly "anticipate that increasing the size of the training dataset will yield even greater performance gains."
- Why unresolved: The experiments were limited to a fine-tuning set of 3,700 songs; the marginal utility of adding more data remains untested.
- What evidence would resolve it: Evaluation of DER on the SSLD-200 dataset using models trained on incrementally larger datasets (e.g., 10k, 50k, 100k songs).

### Open Question 2
- Question: Can the SongPrepE2E training approach generalize to different Large Language Model (LLM) architectures while maintaining superior performance over the pipeline?
- Basis in paper: [explicit] The authors state, "this training approach may generalize well to other large language model architectures and weights."
- Why unresolved: The study only validates the approach using Qwen2-7B; compatibility with other popular architectures (e.g., LLaMA, Mistral) is asserted but not demonstrated.
- What evidence would resolve it: Benchmarking SongPrepE2E performance when initialized with different pretrained LLM weights using identical audio-text pairs.

### Open Question 3
- Question: What is the optimal trade-off between data quantity and data quality (WER filtering threshold) for training the end-to-end model?
- Basis in paper: [inferred] The paper observes that a stricter filter (WER < 0.1) resulted in worse performance than a looser filter (WER < 0.3), likely due to overfitting on a smaller dataset.
- Why unresolved: The paper tests only two specific thresholds; the precise inflection point where data cleanliness outweighs the benefits of dataset scale remains unknown.
- What evidence would resolve it: An ablation study plotting downstream generation quality against various WER filtering thresholds (e.g., 0.2, 0.4, 0.5).

## Limitations

- The framework requires substantial proprietary training data (2M songs for E2E model, 3,700 songs for structure fine-tuning) that are not publicly available, creating significant reproducibility barriers.
- The MuCodec audio tokenization process involves multiple complex components (RVQ, flow matching) with limited hyperparameter transparency, making exact replication difficult.
- The 4-minute context window for SongPrepE2E may be insufficient for longer songs or extended instrumental passages, though this limitation is not explicitly tested in the paper.
- WER improvements in the pipeline may be partially attributable to source separation rather than the overall framework design, as the 20-point WER reduction from Demucs is substantial on its own.

## Confidence

- **High Confidence:** The end-to-end architecture design and its theoretical advantages over multi-stage pipelines are well-supported by the methodology and results.
- **Medium Confidence:** The specific DER and WER improvements achieved on SSLD-200, as these depend on proprietary training data and exact implementation details that are not fully disclosed.
- **Medium Confidence:** The subjective evaluation results showing improved musicality and lyric matching, as these are based on limited pairwise comparisons (7 annotators, 40 songs) with potential rater bias.

## Next Checks

1. **Structure Analysis Ablation:** Implement the All-In-One model without DPRNN blocks and measure DER improvement on SSLD-200 to quantify the contribution of global context modeling to the 16.1% DER result.

2. **Audio Reconstruction Quality:** Decode 100 random MuCodec token sequences back to audio using the flow matching decoder and compute Mean Opinion Score (MOS) on reconstruction quality to validate semantic preservation before LLM training.

3. **Context Window Stress Test:** Process songs of varying lengths (2-6 minutes) through SongPrepE2E and measure WER degradation as a function of song duration to empirically determine the practical limits of the 4-minute context window assumption.