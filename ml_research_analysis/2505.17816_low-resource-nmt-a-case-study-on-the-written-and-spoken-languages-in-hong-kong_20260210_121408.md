---
ver: rpa2
title: 'Low-Resource NMT: A Case Study on the Written and Spoken Languages in Hong
  Kong'
arxiv_id: '2505.17816'
source_url: https://arxiv.org/abs/2505.17816
tags:
- cantonese
- chinese
- parallel
- translation
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of developing neural machine
  translation (NMT) systems for low-resource language pairs, specifically between
  standard Chinese and Cantonese. The authors propose a method to mine parallel sentences
  from Chinese and Cantonese Wikipedia articles using the LASER toolkit, obtaining
  72K additional training pairs beyond existing datasets.
---

# Low-Resource NMT: A Case Study on the Written and Spoken Languages in Hong Kong

## Quick Facts
- arXiv ID: 2505.17816
- Source URL: https://arxiv.org/abs/2505.17816
- Reference count: 18
- Primary result: Proposed method mines 72K parallel sentences from Wikipedia, improving BLEU scores across all test sets and outperforming Baidu Fanyi on 6/8 test sets

## Executive Summary
This paper addresses the challenge of developing neural machine translation (NMT) systems for low-resource language pairs, specifically between standard Chinese and Cantonese. The authors propose a method to mine parallel sentences from Chinese and Cantonese Wikipedia articles using the LASER toolkit, obtaining 72K additional training pairs beyond existing datasets. They train a transformer-based NMT model and demonstrate that incorporating the mined Wikipedia data improves BLEU scores across all test sets. Their system outperforms Baidu Fanyi on 6 out of 8 test sets and captures important linguistic transformations such as lexical mapping, word reordering, and code-switching between the two languages.

## Method Summary
The authors employ a two-stage approach to develop their Cantonese-to-Chinese NMT system. First, they use the LASER toolkit to mine parallel sentences from Chinese and Cantonese Wikipedia articles, extracting 72K additional training pairs. Second, they train a transformer-based NMT model using both the mined Wikipedia data and existing datasets. The model is evaluated across multiple test sets, and the results show consistent improvements in BLEU scores when incorporating the mined data. The authors also conduct qualitative analysis to demonstrate that their system captures key linguistic transformations between the two languages, including lexical mapping, word reordering, and code-switching patterns.

## Key Results
- Mined 72K parallel sentences from Chinese and Cantonese Wikipedia articles
- Incorporating mined Wikipedia data improves BLEU scores across all test sets
- System outperforms Baidu Fanyi on 6 out of 8 test sets
- Successfully captures linguistic transformations including lexical mapping, word reordering, and code-switching

## Why This Works (Mechanism)
The approach works by leveraging large-scale monolingual web content (Wikipedia) and using multilingual sentence embeddings to identify parallel sentence pairs between Chinese and Cantonese. The LASER toolkit enables effective cross-lingual alignment despite the low-resource nature of the language pair. By incorporating this mined data into the training process, the model gains exposure to more diverse linguistic patterns and domain-specific vocabulary, which helps it better handle the unique characteristics of Cantonese-to-Chinese translation.

## Foundational Learning
- LASER (Language-Agnostic SEntence Representations): Needed for cross-lingual sentence embedding and alignment; quick check: verify that LASER produces consistent embeddings for parallel sentences across languages
- Transformer architecture: Needed for state-of-the-art NMT performance; quick check: confirm that attention weights properly align source and target tokens
- BLEU score evaluation: Needed for quantitative comparison of translation quality; quick check: ensure consistency in tokenization and evaluation settings across all test sets
- Low-resource NMT challenges: Needed to understand limitations and constraints; quick check: verify that model performance doesn't degrade significantly with reduced training data
- Code-switching patterns: Needed to capture linguistic phenomena specific to Cantonese; quick check: examine model outputs for proper handling of mixed language segments

## Architecture Onboarding

**Component Map:** Wikipedia articles -> LASER sentence embedding extraction -> Parallel sentence mining -> NMT model training -> Translation output

**Critical Path:** Data mining (LASER) -> Model training (Transformer) -> Evaluation (BLEU scores)

**Design Tradeoffs:** The approach trades computational cost of mining and training on larger datasets for improved translation quality. Using Wikipedia data provides domain-specific benefits but may limit generalization to other domains.

**Failure Signatures:** Poor BLEU scores would indicate ineffective mining or model training; failure to capture code-switching would suggest insufficient exposure to colloquial Cantonese patterns; domain mismatch between Wikipedia and test sets could reveal generalization limitations.

**3 First Experiments:**
1. Verify that mined parallel sentences from Wikipedia are indeed translations of each other
2. Test model performance with different proportions of mined data to determine optimal mixing ratio
3. Evaluate model on out-of-domain test sets to assess generalization beyond Wikipedia-style content

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- The mined parallel corpus (72K pairs) remains relatively small compared to high-resource language pairs
- Quality of mined sentences from encyclopedic content may not generalize well to other domains
- Evaluation relies primarily on BLEU scores, which may not fully capture Cantonese-specific linguistic features
- Limited comparison with only 8 test sets against Baidu Fanyi

## Confidence
- Incorporating mined Wikipedia data improves translation quality: **High confidence** (consistent BLEU improvements across all test sets)
- System outperforms Baidu Fanyi on 6 out of 8 test sets: **Medium confidence** (limited test set coverage)
- Captures linguistic transformations (lexical mapping, word reordering, code-switching): **Medium confidence** (qualitative analysis, needs systematic evaluation)

## Next Checks
1. Evaluate the system on additional test sets covering diverse domains beyond Wikipedia-style content to assess generalization capabilities
2. Conduct human evaluation studies focusing specifically on Cantonese linguistic features like code-switching and colloquial expressions
3. Test the scalability of the mining approach by applying it to other low-resource language pairs to verify if similar improvements can be achieved