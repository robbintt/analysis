---
ver: rpa2
title: 'Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with
  Question-Conditioned Mixture-of-Experts'
arxiv_id: '2512.08814'
source_url: https://arxiv.org/abs/2512.08814
tags:
- personality
- user
- arxiv
- posts
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the challenge of personality detection from\
  \ social media posts, which is constrained by limited supervision signals and the\
  \ difficulty of mapping abstract psychological constructs to user language. Existing\
  \ approaches rely on post \u2192 user vector \u2192 labels paradigms, which are\
  \ limited by label scarcity and lack of intermediate supervision."
---

# Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts

## Quick Facts
- **arXiv ID:** 2512.08814
- **Source URL:** https://arxiv.org/abs/2512.08814
- **Reference count:** 13
- **Key outcome:** ROME achieves up to 15.41% Macro-F1 improvement over state-of-the-art baselines by injecting psychological knowledge via LLM-generated questionnaire answers and question-conditioned MoE routing.

## Executive Summary
This paper addresses personality detection from social media posts, which suffers from limited supervision signals and difficulty mapping psychological constructs to user language. Existing approaches rely on direct post-to-label mapping, which is constrained by label scarcity. The authors propose ROME, a framework that uses large language models to simulate user responses to validated psychometric questionnaires, generating intermediate supervision signals that bridge linguistic cues and personality labels. A question-conditioned Mixture-of-Experts module jointly processes user posts and questionnaire items, with learned evidence weights improving final personality predictions.

## Method Summary
ROME operates in three stages: (1) Offline "Ask" phase where GPT-4o role-plays each user to generate answers to 60 MBTI-style questionnaire items from their posts; (2) "Answer" phase where a question-conditioned MoE (32 experts) pretrained on these generated answers learns to predict questionnaire responses from post and question embeddings; (3) "Detect" phase where predicted answers are weighted by reliability/importance scores, fused with user representations, and used to predict 4 MBTI binary dimensions via multi-task learning. The framework is evaluated on Kaggle (8,675 users, 50 posts each) and Pandora (9,067 users, variable posts) datasets.

## Key Results
- ROME achieves 89.78% average Macro-F1 on Kaggle, outperforming state-of-the-art baselines by up to 15.41%
- On Pandora, ROME achieves 71.95% average Macro-F1 with improvements of 1.06% over baselines
- Question-conditioned MoE with 32 experts and reliability/importance weighting provides consistent performance gains across both datasets
- The approach demonstrates strong data efficiency, matching ETM baseline performance using only 40% of training data

## Why This Works (Mechanism)

### Mechanism 1: Structured Intermediate Supervision via Psychometric Questionnaires
Decomposing coarse personality labels into fine-grained questionnaire items provides richer training signals than direct text→label mapping. The LLM role-plays each user to generate question-level answers from their posts, creating an explicit reasoning chain from linguistic cues → questionnaire responses → personality labels. This approach transforms free-form user posts into interpretable, questionnaire-grounded evidence while providing rich intermediate supervision to mitigate label scarcity. The averaging of T samples reduces LLM stochasticity, though performance degrades when posts are too short/fragmented.

### Mechanism 2: Question-Conditioned Expert Specialization
Different questionnaire items probe distinct psychological constructs requiring specialized processing pathways. The MoE router conditions on user embedding, question embedding, and construct indicator to select relevant experts, with each expert learning to capture specific psycholinguistic patterns. This heterogeneous processing benefits from specialized experts rather than a shared encoder, as validated by expert activation analysis showing dominant specialization across MBTI dimensions. Optimal performance occurs with 32 experts—too few underfits while too many increases routing complexity without gains.

### Mechanism 3: Adaptive Evidence Weighting via Reliability and Importance
Not all questionnaire items contribute equally to final predictions; weighting by generation stability and discriminative power improves accuracy. Reliability score inversely relates to variance across T LLM samples, while importance score measures between-class separability for the target construct. The final weight modulates evidence contribution, with ablation studies showing that removing q-weighting drops Macro-F1 from 89.78% to 88.47% on Kaggle. However, if LLM generation is consistently noisy across all items, reliability weighting provides limited signal.

## Foundational Learning

- **Concept: Mixture-of-Experts Routing**
  - Why needed here: Understanding how gating networks select and weight experts based on input conditions is essential for the question-conditioned MoE.
  - Quick check question: Given a router output $g_{u,i} = \text{softmax}(\text{MLP}(x_{u,i}))$, what happens to gradient flow if one expert consistently receives near-zero gate weights?

- **Concept: Multi-Task Learning with Auxiliary Objectives**
  - Why needed here: ROME jointly optimizes question answering (auxiliary) and personality classification (primary) with $\lambda_q = 1$ and $\lambda_{cls} = 0.05$.
  - Quick check question: Why might the auxiliary task need pretraining before joint optimization? (Hint: See ablation "w/o pretrain" dropping to 69.21%.)

- **Concept: Psychometric Questionnaire Design (MBTI)**
  - Why needed here: Understanding that each item targets specific constructs (I/E, S/N, T/F, P/J) is essential for the construct-specific masking in Eq. 9-10.
  - Quick check question: Why does the paper use construct-specific binary masks rather than letting all questions inform all dimensions?

## Architecture Onboarding

- **Component map:** GPT-4o → question-level answers → BERT/LLaMA encoder → user embedding + question embedding → question-conditioned MoE (32 experts) → predicted answers → reliability/importance weighting → gated fusion → personality classifier

- **Critical path:** (1) Pretrain Answer module with L1 regression on generated answers (100 epochs, lr=5e-4); (2) Joint fine-tuning with multi-task loss (lr=1e-4); (3) Inference: encode posts once → MoE produces evidence → gated fusion → prediction (no LLM calls)

- **Design tradeoffs:** BERT encoder offers smaller size (110M params), faster inference, and competitive performance (89.78% vs. LLaMA's 92.71%); LLaMA encoder provides better long-text modeling at higher computational cost; questionnaire length of 60 items provides optimal coverage versus 12-item baseline.

- **Failure signatures:** P/J dimension consistently underperforms (76.95% vs. 90%+ on others), suggesting questionnaire items may be less diagnostic for this construct; Pandora dataset shows negative gains on some dimensions due to short/fragmented posts degrading role-play quality; without pretraining, Answer module collapses to 69.21%.

- **First 3 experiments:** (1) Train on 40% of Kaggle data; verify ROME (78.01%) matches or exceeds ETM full-data baseline (77.79%) to confirm data efficiency; (2) Vary experts from 8→16→24→32→40; plot Macro-F1 curve to validate 32 is optimal; (3) For held-out user, remove highest-weight vs. lowest-weight question; measure logit change to verify learned weights reflect diagnostic value.

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Performance degrades on datasets with shorter, more fragmented posts (Pandora), suggesting sensitivity to post quality and quantity
- The specific 60-item MBTI questionnaire is only partially shown, limiting independent validation of psychometric validity
- Pretraining requirement for the Answer module indicates the joint optimization landscape is fragile and may not generalize well

## Confidence

- **High Confidence:** The mechanism of using psychometric questionnaires as intermediate supervision is well-supported by significant performance gains and explicit supervision signal
- **Medium Confidence:** The question-conditioned MoE design shows strong empirical support but optimal architecture details are not fully specified
- **Medium Confidence:** The reliability/importance weighting scheme demonstrates measurable improvements but lacks external validation
- **Low Confidence:** Generalizability to different personality frameworks, social media platforms, or languages remains unproven

## Next Checks

1. **Cross-dataset validation:** Train ROME on Kaggle and evaluate on an entirely different personality detection dataset (e.g., myPersonality Facebook data) to test generalizability beyond the same framework.

2. **Post length sensitivity analysis:** Systematically vary the minimum number of posts per user (5, 10, 20, 50) and measure performance degradation to quantify the method's robustness to sparse data.

3. **Alternative questionnaire validation:** Replace the 60-item MBTI questionnaire with a different validated personality instrument (e.g., IPIP Big Five) and retrain ROME to test whether the approach depends on the specific questionnaire structure.