---
ver: rpa2
title: 'Stop-RAG: Value-Based Retrieval Control for Iterative RAG'
arxiv_id: '2510.14337'
source_url: https://arxiv.org/abs/2510.14337
tags:
- retrieval
- stop
- stop-rag
- answer
- stopping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Stop-RAG, a value-based controller that\
  \ addresses the critical challenge of determining when to stop iterative retrieval-augmented\
  \ generation (RAG) processes. Stop-RAG frames iterative RAG as a finite-horizon\
  \ Markov decision process and employs a Q-network trained with full-width forward-view\
  \ Q(\u03BB) targets to provide forward-looking estimates of whether additional retrieval\
  \ will improve answer quality."
---

# Stop-RAG: Value-Based Retrieval Control for Iterative RAG

## Quick Facts
- arXiv ID: 2510.14337
- Source URL: https://arxiv.org/abs/2510.14337
- Authors: Jaewan Park; Solbee Cho; Jay-Yoon Lee
- Reference count: 32
- Primary result: Stop-RAG achieves 36.8% EM/47.0% F1 on MuSiQue, outperforming next-best method (31.5% EM/43.0% F1)

## Executive Summary
This paper introduces Stop-RAG, a value-based controller that addresses the critical challenge of determining when to stop iterative retrieval-augmented generation (RAG) processes. Stop-RAG frames iterative RAG as a finite-horizon Markov decision process and employs a Q-network trained with full-width forward-view Q(λ) targets to provide forward-looking estimates of whether additional retrieval will improve answer quality. The approach learns effective stopping policies while remaining compatible with black-box LLMs and existing pipelines.

Evaluated on multi-hop question-answering benchmarks (MuSiQue, HotpotQA, 2WikiMultihopQA), Stop-RAG consistently outperforms both fixed-iteration baselines and prompting-based stopping methods. For example, on MuSiQue, Stop-RAG achieves 36.8% exact match and 47.0% F1 compared to 31.5% EM/43.0% F1 for the next-best method. The method demonstrates particular effectiveness by balancing retrieval recall and precision, executing slightly more iterations than naive prompting approaches to gather additional relevant evidence.

## Method Summary
Stop-RAG introduces a value-based approach to iterative RAG stopping decisions by framing the problem as a finite-horizon Markov decision process. The method employs a Q-network that learns to estimate the expected improvement from additional retrieval steps, using full-width forward-view Q(λ) targets during training. This allows the controller to make forward-looking decisions about when to stop retrieving based on learned value estimates rather than fixed iteration counts or simple heuristic rules. The approach is designed to be compatible with black-box LLMs and existing RAG pipelines, requiring only the ability to observe retrieval results and final answer quality.

## Key Results
- Stop-RAG achieves 36.8% exact match and 47.0% F1 on MuSiQue benchmark
- Outperforms next-best method (31.5% EM/43.0% F1) by 5.3% EM and 4.0% F1
- Consistently outperforms fixed-iteration and prompting-based baselines across three multi-hop QA benchmarks
- Demonstrates better balance between retrieval recall and precision compared to alternative approaches

## Why This Works (Mechanism)
Stop-RAG works by learning a value function that estimates the expected improvement in answer quality from additional retrieval steps. By framing iterative RAG as a Markov decision process, the Q-network can make forward-looking decisions that consider not just immediate retrieval results but the downstream impact on final answer quality. The use of full-width forward-view Q(λ) targets enables the learning of effective stopping policies that balance the trade-off between gathering sufficient evidence and avoiding unnecessary computation.

## Foundational Learning

**Markov Decision Process (MDP)** - Why needed: Provides theoretical framework for modeling iterative retrieval as sequential decision-making. Quick check: Can you identify states, actions, and rewards in the iterative RAG process?

**Q-learning and Q-networks** - Why needed: Enables learning of value estimates for stopping decisions without requiring complete environment models. Quick check: Understand how Q-values represent expected future rewards.

**Forward-view Q(λ)** - Why needed: Combines advantages of Monte Carlo and temporal difference learning for more stable training. Quick check: Can you explain how λ parameter balances bias and variance?

**Retrieval-augmented generation (RAG)** - Why needed: Understanding the baseline approach being improved. Quick check: Know the basic components of RAG (retrieval + generation).

**Multi-hop question answering** - Why needed: Benchmark domain where iterative retrieval is particularly valuable. Quick check: Can you explain why single-hop retrieval often fails on these tasks?

## Architecture Onboarding

**Component map:** Query -> Retriever -> Q-network -> Stop/Continue decision -> LLM Generator -> Answer

**Critical path:** Query flows through retriever, Q-network evaluates whether to continue or stop, then either loops back for more retrieval or proceeds to final generation.

**Design tradeoffs:** Adaptive stopping vs. fixed iterations (accuracy vs. computational cost), value-based learning vs. heuristic rules (generalization vs. simplicity), black-box compatibility vs. model-specific optimization.

**Failure signatures:** Premature stopping leading to insufficient evidence, excessive iterations causing noise accumulation, Q-network miscalibration resulting in suboptimal decisions.

**3 first experiments:** 1) Test Q-network accuracy on held-out examples, 2) Compare stopping decisions vs. oracle stopping, 3) Measure computational overhead vs. fixed-iteration baseline.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on multi-hop QA benchmarks, limiting generalizability to other RAG use cases
- Computational overhead during inference is not addressed, which could be significant given Q-network evaluation at each step
- Black-box LLM compatibility is claimed but not extensively validated across different model families

## Confidence
High confidence in core claim that Stop-RAG outperforms fixed-iteration and prompting-based baselines on multi-hop QA tasks. Medium confidence in claim that value-based control is a "key missing component" in agentic systems. Medium confidence in practical applicability given unmeasured computational requirements.

## Next Checks
1. Evaluate Stop-RAG on non-QA tasks such as open-domain fact verification or conversational retrieval to assess generalizability beyond multi-hop question answering.

2. Measure and report inference-time latency and computational overhead to quantify the practical cost of the adaptive stopping mechanism compared to simpler baselines.

3. Test the method with multiple different LLM providers (e.g., OpenAI, Anthropic, open-source models) to validate the claimed black-box compatibility and identify any model-specific limitations.