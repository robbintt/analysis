---
ver: rpa2
title: A Rule Based Solution to Co-reference Resolution in Clinical Text
arxiv_id: '2503.09896'
source_url: https://arxiv.org/abs/2503.09896
tags:
- concept
- system
- data
- each
- co-reference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a rule-based system for co-reference resolution
  in clinical text. The system was developed using the 2011 i2b2 Natural Language
  Processing Challenge data and evaluated on test data.
---

# A Rule Based Solution to Co-reference Resolution in Clinical Text

## Quick Facts
- arXiv ID: 2503.09896
- Source URL: https://arxiv.org/abs/2503.09896
- Authors: Ping Chen; David Hinote; Guoqing Chen
- Reference count: 12
- Primary result: Rule-based system achieved 89.6% F1 on i2b2 test data

## Executive Summary
This paper presents a rule-based system for co-reference resolution in clinical text, developed using the 2011 i2b2 Natural Language Processing Challenge data. The system employs semantic matching through UMLS and WordNet, specialized handling for person mentions, and an aggressive link-then-filter architecture to achieve high precision. Evaluated on test data, the system outperformed three publicly available general-purpose co-reference resolution systems with an overall F1 score of 89.6%.

## Method Summary
The rule-based system processes clinical documents through six components: a document handler that tokenizes text into a 2D array, a concept handler storing concept attributes, a main linker matching non-person concepts via string and semantic methods, a people linker with specialized person-specific rules, a link filter removing false positives using contextual clues, and a chain builder producing sorted co-reference chains. The system uses UMLS MRCONSO tables for semantic matching, gender-based pronoun resolution for person mentions, and aggressive linking followed by post-hoc pruning. Rules were derived from observing training data across multiple institutions, and the system assumes gold-standard concept markables are provided as input.

## Key Results
- Achieved 89.6% F1 score on i2b2 test data (323 i2b2-style + 59 ODIE-style documents)
- Outperformed BART, Stanford, and LingPipe systems on the same test set
- People category achieved .958 F1 (highest performance) while Problems category achieved only .690 F1
- Performance dropped to .789 F1 on Mayo Clinic data versus .891/.912 on i2b2 data

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Semantic Matching via UMLS and WordNet
The system conditions concepts by removing common words, then performs three-way comparison: head/synonym matching using 80% character overlap or WordNet synonyms, UMLS MRCONSO table lookup for shared concept identifiers, and acronym detection matching first letters to whole words. This enables matching of conceptually equivalent terms that lack string similarity.

### Mechanism 2: Separate Processing Pipeline for Person Mentions
The People Linker identifies medical personnel via internet search keyword detection, identifies the document subject (patient) through exclusion criteria, determines subject gender via pronoun frequency counting, then applies distinct rules: introduction matching (Dr. + name within 2 words), partial name matching (handles initials), and gender-aware pronoun resolution.

### Mechanism 3: Link-Then-Filter Architecture with Post-Hoc Pruning
The Main Linker creates a "web" of links where concepts can link to many others. The filter then examines surrounding context for disqualifying clues: dates, locations, descriptive modifiers following keywords (in, on, are, is). Adjective-only mentions have all links removed via WordNet detection.

## Foundational Learning

- **Co-reference chains and markables**: Understanding that markables are noun phrases to be linked into chains is prerequisite to understanding the task definition. Quick check: Given "The patient has diabetes. She manages it with insulin," what are the markables and which should form a chain?

- **UMLS (Unified Medical Language System)**: The system queries MRCONSO tables to determine if two strings share a concept identifier (CUI); without this, you cannot understand the "Match by Meaning" component. Quick check: If "MI" and "myocardial infarction" both map to CUI C0155626, should the system link them?

- **Singleton mentions**: 30-60% of concept mentions are singletons (not co-referent with anything); evaluation scoring differs dramatically depending on whether singletons count as correct chains. Quick check: Why does the i2b2 evaluation script produce higher scores than exact match scoring?

## Architecture Onboarding

- **Component map**: Document Handler → Concept Handler → [Main Linker OR People Linker] → Link Filter → Chain Builder

- **Critical path**: Document Handler → Concept Handler → [Main Linker OR People Linker] → Link Filter → Chain Builder. The branch decision is based on concept type ("person"/"people" vs. all others).

- **Design tradeoffs**: Rule-based vs. ML: Faster to build for specific domains, but less adaptable to new data structures. Gold-standard markables input vs. self-discovery: UHD system receives pre-marked concepts; comparison systems must discover their own. Aggressive linking + filtering vs. conservative linking: High precision achieved at potential recall cost.

- **Failure signatures**: Low F1 on "Problems" category (.690 on Beth Israel) vs. "People" (.958) suggests semantic matching is weaker than person-specific rules. Mayo Clinic data shows across-the-board performance drop (UHD: .789 vs. .891/.912) suggesting institution-specific document formatting differences.

- **First 3 experiments**:
  1. Baseline validation: Run the system on the provided i2b2 test data and verify F1 is approximately .895-.896 using the i2b2 evaluation script
  2. Ablation on semantic matching: Disable UMLS lookup and measure F1 drop to quantify contribution of ontology-based matching
  3. Cross-institution generalization: Train rules on Beth Israel data only, test on Mayo Clinic data to measure domain transfer gap

## Open Questions the Paper Calls Out

- **Can the rule-based algorithm effectively resolve co-references in non-medical document domains?**: The text states, "The question as to whether this algorithm would work well with a type of document other than medical documents is as of now untested." This remains unresolved because the system was tuned exclusively for the i2b2 clinical dataset and utilizes resources like UMLS specific to the biomedical domain.

- **How much of the UHD system's superior performance is attributable to the use of gold-standard concept markables rather than the linking rules?**: Section 4 notes that public systems (BART, Stanford, LingPipe) must discover their own markables, whereas the UHD system was given gold-standard inputs, creating an unequal comparison of linking capability. This remains unresolved because the study did not perform an ablation test where general-purpose systems were provided the same gold-standard mentions.

- **Is the reliance on real-time internet searches for identifying medical personnel scalable and reliable for large-scale clinical processing?**: Section 3.3 describes a subroutine that sends mention text to a search engine to check for medical keywords to identify personnel. This remains unresolved because the paper does not analyze the latency of these external calls or the impact of search engine variability/unavailability on accuracy.

## Limitations

- The rule-based approach's adaptability to new clinical contexts represents a fundamental limitation, as evidenced by the performance drop from .891/.912 on i2b2 data to .789 on Mayo Clinic data.
- The system assumes gold-standard concept markables are provided, giving it an unfair advantage over comparison systems that must discover concepts themselves.
- The rule database contents (be-phrase lists, medical personnel keywords, filtering cues) are not fully specified, creating reproducibility challenges.

## Confidence

- **High confidence**: The system's overall F1 score of 89.6% on i2b2 test data and superior performance compared to BART, Stanford, and LingPipe systems. The architectural description and evaluation methodology are clearly documented.
- **Medium confidence**: The effectiveness of semantic matching via UMLS and WordNet. While the paper claims this enables matching of conceptually equivalent terms, the contribution of ontology-based matching versus other components is not isolated through ablation studies.
- **Low confidence**: The generalizability of person-specific rules across different clinical document types. The high performance on People category (.958 F1) may be specific to the document corpus where patients are always grammatical subjects.

## Next Checks

1. **Ablation experiment**: Disable UMLS MRCONSO lookup and WordNet synonym matching in the Main Linker to quantify the contribution of semantic matching to overall performance. Measure the F1 drop across all concept categories.

2. **Cross-institutional transfer**: Train the rule-based system exclusively on Beth Israel data, then test on Mayo Clinic data to measure the gap and identify which rules fail to generalize across institutions.

3. **Fair comparison validation**: Implement a modified version of BART or Stanford's system that also receives gold-standard concept markables, then compare performance directly with the UHD system on identical input to eliminate the markable discovery advantage.