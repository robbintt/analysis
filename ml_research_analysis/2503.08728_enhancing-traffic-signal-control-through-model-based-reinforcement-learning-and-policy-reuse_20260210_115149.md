---
ver: rpa2
title: Enhancing Traffic Signal Control through Model-based Reinforcement Learning
  and Policy Reuse
arxiv_id: '2503.08728'
source_url: https://arxiv.org/abs/2503.08728
tags:
- learning
- agent
- traffic
- target
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generalizing multi-agent
  reinforcement learning (MARL) traffic signal control (TSC) policies across different
  traffic scenarios and road networks. The authors propose PLight, a model-based RL
  approach that pre-trains control policies and environment models using source-domain
  traffic scenarios, and PRLight, which enhances adaptability by adaptively selecting
  pre-trained PLight agents based on similarity between source and target domains.
---

# Enhancing Traffic Signal Control through Model-based Reinforcement Learning and Policy Reuse

## Quick Facts
- arXiv ID: 2503.08728
- Source URL: https://arxiv.org/abs/2503.08728
- Reference count: 32
- Primary result: PRLight reduces adaptation time and achieves lower travel times than learning from scratch when transferring traffic signal control policies across different traffic scenarios and road networks

## Executive Summary
This paper addresses the challenge of generalizing multi-agent reinforcement learning (MARL) traffic signal control (TSC) policies across different traffic scenarios and road networks. The authors propose PLight, a model-based RL approach that pre-trains control policies and environment models using source-domain traffic scenarios, and PRLight, which enhances adaptability by adaptively selecting pre-trained PLight agents based on similarity between source and target domains. The core idea is to learn environmental models that predict state transitions and facilitate comparison of environmental features, then use policy reuse with similarity-weighted guide agent selection to accelerate learning in new environments. The approach was evaluated on two transfer settings: adaptability to different traffic scenarios within the same road network, and generalization across different road networks.

## Method Summary
PLight pre-trains an encoder-decoder architecture jointly with a dueling Q-network on source traffic scenarios. The encoder extracts features from observations (current phase, queue lengths, neighbor info), the decoder predicts next observations as a learned dynamics model, and the Q-network selects actions. PRLight transfers by maintaining a pool of pre-trained agents and selecting guide agents via similarity-weighted softmax based on decoder prediction error. The target agent is trained using guide actions for exploration while the source agents remain frozen. The similarity measure computes discounted Euclidean distance between decoder predictions and actual observations over a sliding window.

## Key Results
- PRLight achieved lower average travel times (375.67 vs 498.36) and higher throughput (7346 vs 7055) than baseline methods in within-network transfers
- In cross-network transfer to New York's 196-intersection network, PRLight achieved the lowest average travel time at 1054.69
- PRLight demonstrated faster convergence, requiring only 5 episodes to reach stable performance in new environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning an environment model (state transition predictor) alongside the control policy enables domain similarity measurement without requiring explicit scenario labels.
- Mechanism: The Encoder-Decoder architecture trains jointly with the Q-network. The Decoder takes encoded features and actions to predict the next observation. When encountering a new target domain, the prediction error between each source agent's decoder output and actual observations quantifies how similar that source task is to the target—smaller error indicates higher similarity.
- Core assumption: Traffic environments with similar state transition dynamics will produce similar optimal policies, and prediction error correlates with transfer utility.
- Evidence anchors:
  - [abstract] "The environment model predicts the state transitions, which facilitates the comparison of environmental features."
  - [Section 4.2] "The decoder can be regarded as a forward dynamics model... predicts the observation of the intersection at the next time step."
  - [corpus] Weak direct support. Related papers (Unicorn, GPLight+) address generalization but use different mechanisms (parameter-sharing, genetic programming) rather than model-based similarity scoring.
- Break condition: If target domains have fundamentally different observation spaces or action semantics, the decoder's prediction error becomes uninformative—similarity scores will be arbitrary.

### Mechanism 2
- Claim: Weighted guide agent selection accelerates early exploration by preferentially following policies from similar source domains.
- Mechanism: Over a sliding window of m timesteps, the algorithm computes cumulative discounted Euclidean distance between predicted and actual feature vectors for each source agent. These distances convert to weights via negative sign, then to a probability distribution via softmax. The target agent samples guide actions from this distribution, with recent prediction errors weighted more heavily (λ discount factor).
- Core assumption: Policies trained on similar traffic flow patterns and network topologies will produce useful exploratory actions even if not optimal for the target domain.
- Evidence anchors:
  - [Section 4.3] "Di = −∑λ^(t+m−j) × d_j^i... pk = exp(DK) / ∑exp(Di)"
  - [Section 5.2] "PRLight... demonstrated markedly faster convergence, requiring only 5 episodes to reach a stable performance level."
  - [corpus] No direct corpus validation of this specific weighting scheme. Adjacent work (TL-GNN, MetaLight) uses different transfer mechanisms.
- Break condition: If all source agents are equally dissimilar to the target (e.g., transfer to a fundamentally different city topology), the softmax distribution becomes near-uniform, reducing to random policy selection.

### Mechanism 3
- Claim: Shared feature extraction between policy (Q-network) and environment model (Decoder) does not significantly degrade policy performance while enabling transfer.
- Mechanism: The Encoder outputs features h'_i that feed into both the Decoder (for prediction) and Q-network (for action selection). The joint loss L = L_D + L_Q trains both simultaneously, with gradients flowing through shared encoder weights.
- Core assumption: The features useful for predicting state transitions overlap sufficiently with features useful for action-value estimation that sharing does not create harmful interference.
- Evidence anchors:
  - [Section 5.4] "PLight(EQ) achieved marginally better performance metrics compared to PLight(EDQ), although the difference did not reach statistical significance."
  - [Section 4.2] "sharing the feature extraction module with the Q-network"
  - [corpus] No corpus papers validate this architectural choice directly. Model-based RL papers (MQL cited) use similar sharing but in different domains.
- Break condition: If prediction and control require fundamentally different state abstractions, shared features may underperform specialized encoders—though paper suggests this effect is minimal here.

## Foundational Learning

- **Markov Decision Processes / Partially Observable Markov Games**
  - Why needed here: The entire framework models multi-intersection TSC as a POMG with agents, states, observations, actions, transitions, and rewards. Without this foundation, the transfer learning setup (source vs. target domains as different MDPs) will be opaque.
  - Quick check question: Can you explain why two road networks with different topologies might have different transition functions P(s'|s,a) even with identical action spaces?

- **Model-Based Reinforcement Learning (Dynamics Learning)**
  - Why needed here: PLight's core contribution is learning a forward dynamics model (Decoder) to predict o_{t+1} from (o_t, a_t). Understanding why this enables similarity computation requires grasping how dynamics capture environment structure.
  - Quick check question: If two environments have identical state spaces but different traffic flow distributions, would you expect their learned dynamics models to produce similar or different predictions for the same (observation, action) pair?

- **Transfer Learning in RL (Behavior Transfer)**
  - Why needed here: PRLight implements behavior transfer—reusing source policies directly rather than parameter initialization or reward shaping. The distinction matters for understanding why source agents remain frozen during target training.
  - Quick check question: Why might behavior transfer (using source policy actions) be more robust than parameter transfer when source and target networks have different sizes or structures?

## Architecture Onboarding

- **Component map:**
  - Observation -> Encoder (MLP + Attention) -> Feature vector h'_i -> Decoder (MLP) -> Predicted next observation
  - Observation -> Encoder (MLP + Attention) -> Feature vector h'_i -> Q-network (Dueling) -> Action values
  - Encoder-Decoder-Q-network triplets stored as agents in pool
  - Similarity calculator computes Euclidean distance between decoder predictions and actual observations

- **Critical path:**
  1. Pre-train PLight agents on source domains (Algorithm 1) -> store in agent pool
  2. For target domain: every m steps, compute similarity weights using all decoders
  3. Sample guide agent from softmax distribution -> execute guide action -> store experience
  4. Train only target agent (source agents frozen) using combined loss L_D + L_Q
  5. Target agent's weight increases over time as it adapts

- **Design tradeoffs:**
  - Window size m: Larger windows give more stable similarity estimates but slower adaptation to local changes
  - Decoder training: Adds computational overhead but enables similarity; ablation shows no significant policy degradation
  - Agent pool size: More source agents increase coverage but raise inference cost per decision
  - Average encoder for normalization: Reduces inter-domain feature scale mismatch, but requires storing all source encoders

- **Failure signatures:**
  - Convergence to suboptimal policy with high variance: Likely all source agents poorly matched - check similarity weight distribution (should be non-uniform)
  - No improvement over random exploration: Guide agent selection may be broken - verify decoder predictions are actually informative (not constant)
  - Performance collapse after initial improvement: Target agent weight may be growing too fast; consider adjusting softmax temperature or λ
  - Cross-network transfer failure (as in New York experiment): Source agents from small networks may lack relevant experience for large-scale coordination

- **First 3 experiments:**
  1. **Sanity check—single domain:** Train PLight on one traffic scenario without transfer. Verify encoder-decoder reconstructs observations (low L_D) and policy learns reasonable actions (decreasing travel time). This isolates the base model from transfer complexity.
  2. **Ablation—decoder impact:** Compare PLight(EDQ) vs PLight(EQ) on same scenario. Confirm paper's finding that shared features don't significantly harm policy performance. If they do, investigate feature interference.
  3. **Minimal transfer test:** Use two source scenarios with clearly different flow distributions (e.g., high vs low density). Transfer to a target scenario similar to one source. Verify that similarity weights correctly identify the more similar source and that PRLight outperforms random initialization in early episodes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the transfer strategies be optimized for highly complex road networks characterized by high intersection density and diverse transport facilities?
- Basis in paper: [explicit] The conclusion states that "exploring strategies for the implementation of our method in complex road networks, such as those with high density intersections and diverse transport facilities, is a promising research direction."
- Why unresolved: The current experiments primarily utilized standard grid networks (Jinan, Hangzhou) or a specific large grid (New York), without explicitly addressing multi-modal transport or the irregularities of highly dense, diverse facilities.
- What evidence would resolve it: Demonstration of PLight/PRLight maintaining performance efficiency and stability when applied to simulations containing multi-modal traffic (e.g., buses, trams) and irregular, high-density topologies.

### Open Question 2
- Question: What techniques can effectively reduce the inference scale of the model to enhance real-time applicability?
- Basis in paper: [explicit] The conclusion notes that since the training scale has been reduced, "further studying how to reduce the inference scale to enhance the method's applicability and efficiency is worth exploring."
- Why unresolved: While PRLight accelerates learning (reducing training burden), the architecture relies on an agent pool and encoder-decoder computations which may impose a computational burden during the inference phase not addressed in the current optimization.
- What evidence would resolve it: Analysis of inference latency and computational resource usage, followed by the proposal of a lightweight variant that preserves transfer capabilities while operating within strict real-time constraints.

### Open Question 3
- Question: How can the policy reuse mechanism be refined to consistently outperform native state-of-the-art methods in large-scale, heterogeneous cross-network transfers?
- Basis in paper: [inferred] The results section notes that in the cross-network New York experiment, "PRLight... falls short of CoLight in some metrics in Experiment 2," attributed to the difficulty of transferring policies without adaptation to such a large, unfamiliar environment.
- Why unresolved: The current similarity-based selection of fixed source agents struggled to leverage small-scale synthetic knowledge effectively enough to beat specialized graph-based attention methods in the complex New York network.
- What evidence would resolve it: A modified transfer mechanism that allows for dynamic adaptation of source agents or topology-aware similarity metrics that enables PRLight to match or exceed CoLight's performance in arbitrary large-scale networks.

## Limitations
- The decoder prediction error as a similarity metric is not rigorously validated and may fail when source and target domains have fundamentally different observation spaces or action semantics
- Cross-network transfer performance, while promising, lacks thorough comparison to specialized large-network controllers and only tested on grid-like topologies
- The approach's scalability to highly complex, irregular road networks with multi-modal traffic remains unexplored

## Confidence
- **High Confidence**: The base PLight model (pre-training with joint encoder-decoder) is technically sound and the architecture is clearly specified. The experimental results for within-network transfer (Jinan/Hangzhou) are internally consistent and show clear improvements over baselines.
- **Medium Confidence**: The policy reuse mechanism (similarity-weighted guide agent selection) works as described, but the theoretical justification for why decoder error correlates with transfer utility is weak. The New York cross-network results are promising but not thoroughly validated against specialized large-network controllers.
- **Low Confidence**: The claim that this approach generalizes to arbitrary road networks is not substantiated—the method was only tested on two synthetic grids and one real-world network, all with grid-like topologies.

## Next Checks
1. **Similarity signal validation**: Run PRLight with randomized decoder weights to confirm that decoder prediction error provides meaningful similarity signals rather than acting as a random selection mechanism.
2. **Scaling validation**: Test the approach on a larger, non-grid network (e.g., irregular street topology) to verify that transfer performance doesn't degrade with network complexity and that the decoder similarity mechanism remains informative.
3. **Comparison validation**: Compare PRLight's cross-network performance against specialized large-network controllers (e.g., GNN-based methods) to determine whether transfer provides genuine advantages or merely competitive performance.