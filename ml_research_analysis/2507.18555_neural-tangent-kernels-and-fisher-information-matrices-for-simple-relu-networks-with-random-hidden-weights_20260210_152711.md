---
ver: rpa2
title: Neural Tangent Kernels and Fisher Information Matrices for Simple ReLU Networks
  with Random Hidden Weights
arxiv_id: '2507.18555'
source_url: https://arxiv.org/abs/2507.18555
tags:
- have
- where
- neural
- page
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper establishes an approximate spectral decomposition of\
  \ the Neural Tangent Kernel (NTK) for 2-layer ReLU networks with random hidden weights,\
  \ paralleling previous results for Fisher Information Matrices (FIM). The authors\
  \ show that the NTK can be decomposed into eigenfunctions with concrete forms: \U0001D439\
  0(\U0001D465) = |\U0001D465|, \U0001D439\U0001D459 (\U0001D465) = \U0001D465\U0001D459\
  , \U0001D439\U0001D6FC\U0001D6FD (\U0001D465) = \U0001D465\U0001D6FC\U0001D465\U0001D6FD\
  /|\U0001D465|, and \U0001D439\u2032\U0001D6FE (\U0001D465) = (\U0001D4652\U0001D6FE\
  \ \u2212 \U0001D4652\u2081)/|\U0001D465|."
---

# Neural Tangent Kernels and Fisher Information Matrices for Simple ReLU Networks with Random Hidden Weights

## Quick Facts
- arXiv ID: 2507.18555
- Source URL: https://arxiv.org/abs/2507.18555
- Reference count: 4
- The paper establishes an approximate spectral decomposition of the Neural Tangent Kernel (NTK) for 2-layer ReLU networks with random hidden weights.

## Executive Summary
This paper derives an approximate spectral decomposition of the Neural Tangent Kernel (NTK) for 2-layer ReLU networks with random hidden weights, establishing a direct connection to the Fisher Information Matrix (FIM). The authors identify specific eigenfunctions with concrete forms and show that simpler functions (with larger eigenvalues) dominate the learning process. The analysis reveals a "simplicity bias" where functions corresponding to larger eigenvalues are learned faster, providing insight into the inductive bias of such networks.

## Method Summary
The authors theoretically derive the spectral decomposition of the NTK by expanding the kernel limit into a sum of orthogonal eigenfunctions. They use Monte Carlo integration over standard Gaussian inputs to verify orthonormality and eigenvalue magnitudes. The NTK is approximated by sampling hidden weights and computing the empirical kernel expectation. The FIM-NTK connection is established through an isometric mapping in the function space, showing that the eigenvectors of the FIM correspond to the eigenfunctions of the NTK.

## Key Results
- The NTK can be decomposed into eigenfunctions: F₀(x) = |x|, Fₗ(x) = xₗ, Fₐᵦ(x) = xₐxᵦ/|x|, and F'ᵧ(x) = (x²ᵧ - x²₁)/|x|
- Eigenvalues for these eigenfunctions are approximately (2d+1)/4π for F₀, 1/4 for Fₗ, and 1/2π(d+2) for the group Fₐᵦ and F'ᵧ
- The residual term r(x,y) contributes less than 2.6% of the total trace in high dimensions
- The spectral structure of NTK and FIM are parallel, with the FIM acting as the matrix representation of the kernel operator

## Why This Works (Mechanism)

### Mechanism 1: Spectral Decomposition of the NTK
The NTK for a 2-layer ReLU network with random hidden weights converges to a deterministic limit as width m→∞. This limit can be expanded into orthogonal eigenfunctions: F₀(x)=|x| (norm), Fₗ(x)=xₗ (linear), and quadratic terms Fₐᵦ(x)=xₐxᵦ/|x|. The residual r(x,y) forms a positive definite kernel with negligible trace contribution in high dimensions. This works because hidden weights are randomly initialized and fixed, with input following standard Gaussian distribution.

### Mechanism 2: Simplicity Bias via Eigenvalue Decay
Gradient descent dynamics under the NTK regime are governed by eigenvalues λᵢ. Functions with larger eigenvalues decay faster in the loss landscape. The simplest function F₀(x)=|x| has the largest eigenvalue (≈ (2d+1)/4π), followed by linear terms (λ=1/4). Complex, higher-order interactions reside in the residual r(x,y) with small eigenvalues. This assumes the network operates in the "lazy training" regime where NTK remains approximately constant.

### Mechanism 3: NTK and Fisher Information Isomorphism
The FIM J is the matrix representation of the kernel operator K in function space Hₘ. A re-parameterization of output weights v into θ diagonalizes the FIM, revealing that eigenvectors of FIM correspond to eigenfunctions of NTK. This assumes a linear regression model on hidden layer features with additive Gaussian noise and Mean Squared Error loss.

## Foundational Learning

- **Concept: Neural Tangent Kernel (NTK)**
  - Why needed here: The entire analysis hinges on the existence and specific form of NTK for ReLU networks. Understanding that NTK captures training dynamics of infinitely wide networks is prerequisite.
  - Quick check question: How does the NTK relate the change in network output to the gradient of the loss?

- **Concept: Spectral Bias / Frequency Principle**
  - Why needed here: The paper formalizes "simplicity bias" through spectral decomposition. Learners need to understand that neural networks often learn low-frequency or "simple" components of data first.
  - Quick check question: In the context of this paper, which function corresponds to the largest eigenvalue and is therefore learned fastest?

- **Concept: Orthogonality in Function Space**
  - Why needed here: The decomposition relies on functions like |x|, xₗ, and xₐxᵦ/|x| being orthogonal with respect to the Gaussian input measure.
  - Quick check question: Why is the orthogonality of eigenfunctions critical for decomposing the kernel into independent learning pathways?

## Architecture Onboarding

- **Component map:** Input Layer(d) -> Hidden Layer(m, ReLU, Random Frozen Weights) -> Output Layer(Trainable Weights)

- **Critical path:**
  1. Initialize hidden weights W randomly (Gaussian)
  2. Freeze W; treat hidden activations φ(xW) as fixed features
  3. Compute the NTK k(x,y) theoretically or empirically
  4. Project target function onto derived eigenfunctions (F₀, Fₗ, etc.) to predict learning speed and final approximation error

- **Design tradeoffs:**
  - Random vs. Trained Features: Analysis exploits randomness of W to define kernel. Training W would break analytical solution but might yield better representations for complex tasks.
  - Approximation vs. Precision: Paper proves residual r(x,y) is small (< 2.6% trace) but non-zero. Precise function representation requires infinite width and ignoring residual tail.

- **Failure signatures:**
  - Non-Gaussian Inputs: If input data is not Gaussian, orthogonality of eigenfunctions (e.g., F₀ vs Fₗ) is not guaranteed, corrupting spectral decomposition.
  - Low Dimensional Data: Bounds for residual r(x,y) rely on d being sufficiently large; very low-dimensional inputs might see larger error margin.

- **First 3 experiments:**
  1. Eigenvalue Verification: Train 2-layer ReLU network with frozen random weights on synthetic target y = |x|. Compare convergence rate against target y = x₁x₂/|x|. Verify first converges ~O(1/λₘₐₓ) faster.
  2. Approximation Fidelity: Measure L² error between network function fᵥ and theoretical approximation f⁽ᴰ⁾θ̃ (sum of major eigenfunctions) as width m increases. Confirm error converges to bound derived from r(x,y).
  3. FIM-NTK Correlation: Compute empirical FIM and NTK for medium-width network. Perform SVD and check if top eigenvectors align with theoretical F₀, Fₗ structures predicted by paper.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the explicit forms and degeneracies of the remaining eigenfunctions corresponding to smaller eigenvalues in the remainder term r(x,y)?
- Basis in paper: [explicit] The authors state, "Concerning eigenfunctions with smaller eigenvalues, we don't have the complete solution, but we can state some propositions" (Page 10).
- Why unresolved: While Lemma 1 identifies eigenfunctions for specific kernels k⁽ⁿ⁾, a general closed-form solution for the entire spectrum of the NTK is not derived.
- What evidence would resolve it: A derivation of the general form for the infinite series of eigenfunctions in the remainder term and a proof of their completeness.

### Open Question 2
- Question: Does the spectral decomposition structure for 2-layer networks generalize to deep ReLU networks (more than one hidden layer) with random hidden weights?
- Basis in paper: [inferred] The paper explicitly limits its scope to "bias-free 2-layer... neural networks" (Page 2), leaving extension to deeper architectures unexplored.
- Why unresolved: Proofs rely on properties of single hidden layer and specific expectation calculation for Z₁; it's unclear if eigenvalue grouping scales or changes with depth.
- What evidence would resolve it: Extending analysis to L-layer networks and identifying if similar concrete eigenfunctions (e.g., F₀, Fₗ) exist or if spectrum becomes continuous in different manner.

### Open Question 3
- Question: How does the spectral decomposition change if input distribution p_X deviates from standard Gaussian distribution?
- Basis in paper: [inferred] Derivation of NTK and orthonormality of eigenfunctions rely heavily on assumption that "p_X is the d-variate standard Gaussian distribution" (Page 2).
- Why unresolved: Orthogonality of Fₗ (linear terms) and Fₐᵦ (non-linear terms) depends on symmetry and moments of Gaussian distribution.
- What evidence would resolve it: A derivation of NTK spectrum under non-Gaussian inputs (e.g., uniform distribution) to determine if eigenvalue magnitudes and function orthogonality are preserved.

## Limitations
- The analysis is limited to the infinite-width limit and assumes random Gaussian initialization of hidden weights.
- The residual term r(x,y) is shown to be small (<2.6% of total trace) in high dimensions, but its exact impact on finite-width networks remains an open question.
- The orthogonality of eigenfunctions depends critically on the Gaussian input assumption, which may not hold for real-world data distributions.

## Confidence
- **High Confidence:** The spectral decomposition structure and identification of specific eigenfunctions (F₀, Fₗ) are mathematically rigorous and well-supported by proofs.
- **Medium Confidence:** The eigenvalue magnitudes and small residual claim are supported by theoretical bounds, but their practical verification requires careful numerical experiments.
- **Medium Confidence:** The link between NTK and FIM via the isometric mapping is theoretically sound but relies on specific assumptions about noise model and loss function.

## Next Checks
1. **Numerical Eigenvalue Verification:** Implement simulation to compute empirical NTK for finite-width network and compare spectrum against theoretical eigenvalues. Test on both Gaussian inputs (where theory applies) and non-Gaussian inputs (to identify failure modes).
2. **Input Distribution Sensitivity:** Measure how approximation error changes when input distribution deviates from standard Gaussian (e.g., uniform, correlated Gaussian). Quantify loss of orthogonality in eigenfunction basis.
3. **Residual Analysis:** For range of dimensions d, compute trace of residual term r(x,y) empirically. Verify whether it remains below theoretical bound of 2.6% and examine how this bound scales with dimensionality.