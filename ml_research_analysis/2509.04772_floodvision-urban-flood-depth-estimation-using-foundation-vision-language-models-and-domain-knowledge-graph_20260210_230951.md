---
ver: rpa2
title: 'FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language
  Models and Domain Knowledge Graph'
arxiv_id: '2509.04772'
source_url: https://arxiv.org/abs/2509.04772
tags:
- flood
- depth
- floodvision
- urban
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FloodVision, a zero-shot framework for urban
  flood depth estimation that combines GPT-4o's semantic reasoning with a curated
  FloodKG knowledge graph to ground estimates in verified physical dimensions. FloodVision
  identifies reference objects in images, retrieves canonical heights from FloodKG,
  estimates submergence ratios, and applies statistical outlier filtering to compute
  depth values.
---

# FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph

## Quick Facts
- arXiv ID: 2509.04772
- Source URL: https://arxiv.org/abs/2509.04772
- Authors: Zhangding Liu; Neda Mohammadi; John E. Taylor
- Reference count: 30
- Key outcome: FloodVision achieves 8.17 cm MAE on 110 crowdsourced flood images, reducing GPT-4o baseline by 20.5% and outperforming CNN-based methods.

## Executive Summary
This paper introduces FloodVision, a zero-shot framework for urban flood depth estimation that combines GPT-4o's semantic reasoning with a curated FloodKG knowledge graph to ground estimates in verified physical dimensions. The system identifies reference objects in images, retrieves canonical heights from FloodKG, estimates submergence ratios, and applies statistical outlier filtering to compute depth values. Evaluated on 110 crowdsourced flood images from MyCoast New York, FloodVision achieves a mean absolute error of 8.17 cm, reducing the GPT-4o baseline by 20.5% and outperforming prior CNN-based methods. The framework generalizes across diverse urban scenes and operates in near real-time, supporting applications in emergency response and smart city flood resilience.

## Method Summary
FloodVision uses a three-step prompting approach with GPT-4o: (1) identify up to three reference objects with positional qualifiers, (2) estimate object height and submergence ratio (0.0-1.0), and (3) output structured JSON. The FloodKG knowledge graph stores canonical heights for common urban objects, which override GPT-4o's estimates when available. Per-object depths are calculated as height × ratio, filtered for outliers (typically fully submerged objects), and aggregated into min/max/average estimates. The framework operates without training on flood data, relying instead on pre-trained vision-language capabilities and physical knowledge grounding.

## Key Results
- Achieves 8.17 cm mean absolute error on 110 crowdsourced flood images
- Reduces GPT-4o baseline error by 20.5% through knowledge graph grounding
- Outperforms previous CNN-based methods while maintaining zero-shot generalization
- Successfully processes diverse urban scenes including vehicles, infrastructure, and human-scale objects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing the Vision-Language Model's (VLM) parametric memory with an explicit external knowledge base reduces quantitative hallucination errors regarding object dimensions.
- **Mechanism:** The system identifies objects via GPT-4o but overrides the model's estimated "provisional" height with a canonical value retrieved from the FloodKG (Knowledge Graph). This forces the geometric calculation to be anchored in verified physical data (e.g., standard curb heights) rather than the VLM's variable latent space.
- **Core assumption:** The VLM can correctly classify the object category (e.g., distinguish an "SUV" from a "sedan") so the correct canonical dimension can be retrieved.
- **Evidence anchors:** [abstract] "The knowledge graph encodes canonical real-world dimensions... to ground the model's reasoning in physical reality." [section 2.2] "When a match is found, the retrieved value overrides any model-generated estimates; otherwise, the model's provisional height is retained."
- **Break condition:** If the input image contains custom or modified objects (e.g., a lifted truck), the canonical height from the Knowledge Graph will be incorrect, introducing systematic bias.

### Mechanism 2
- **Claim:** VLMs provide more reliable relative spatial reasoning (submergence ratios) than absolute metric estimation.
- **Mechanism:** Instead of asking the VLM to guess the water depth in centimeters directly (which is prone to scale distortion), the system requests a "submergence ratio" (0.0 to 1.0). This decouples the visual understanding of the waterline from the physical unit conversion, which is handled mathematically by the ratio × canonical height.
- **Core assumption:** The perspective distortion in the image does not prevent the VLM from accurately identifying the vertical intersection point of the waterline and the object.
- **Evidence anchors:** [abstract] "identifies visible reference objects... estimates submergence ratios... to compute depth values." [section 2.1] "GPT-4o then estimates each object's submerged ratio... multiplied by the object's height to obtain a per-object flood depth."
- **Break condition:** If the reference object is significantly tilted or the image is taken at an extreme oblique angle, the 2D ratio observed by the VLM will fail to map to the 3D physical vertical height.

### Mechanism 3
- **Claim:** Statistical aggregation across multiple diverse reference objects mitigates single-point failures and occlusion errors.
- **Mechanism:** The framework detects up to three reference objects per image. By computing depth for each and applying outlier filtering (specifically targeting "fully submerged" anomalies), the system averages the results. This assumes that while one object's estimation might be noisy (due to glare or occlusion), the aggregate error will regress toward the mean.
- **Core assumption:** The flood water surface is roughly planar across the local scene, meaning the water depth relative to different objects (curbs vs. tires) should be consistent after accounting for ground elevation.
- **Evidence anchors:** [abstract] "applies statistical outlier filtering to compute final depth values." [section 3.2] "FloodVision outputs three depth estimates... accounting for intra-scene flood level variation... The average estimate achieves the highest accuracy."
- **Break condition:** If the scene contains disjoint water bodies (e.g., a puddle on a sidewalk vs. a flooded street), treating them as a single statistical distribution will yield a meaningless average.

## Foundational Learning

- **Concept: Vision-Language Model (VLM) Hallucination**
  - **Why needed here:** The core problem FloodVision solves is the VLM's tendency to "confabulate" numerical values. Understanding that VLMs prioritize semantic plausibility over factual precision is critical for grasping why the Knowledge Graph (FloodKG) is necessary.
  - **Quick check question:** Why is asking a VLM "How tall is the car in this image?" less reliable than asking "What type of car is this, and where is the water line?"

- **Concept: Zero-Shot Generalization**
  - **Why needed here:** The paper claims superior generalization over CNNs. This concept explains how the system recognizes "fire hydrants" or "SUVs" without being trained on a specific dataset of flooded hydrants, relying instead on pre-trained foundational knowledge.
  - **Quick check question:** How does the system estimate depth for an object class that was not explicitly included in the training set of previous CNN-based flood models?

- **Concept: Semantic Grounding**
  - **Why needed here:** This describes the link between the "symbol" (text output "Sedan") and the "physical reality" (122cm height). The FloodKG acts as the grounding mechanism that tether's the model's floating reasoning to hard numbers.
  - **Quick check question:** What specific failure mode is prevented by forcing the model to map visual features to static entries in the FloodKG?

## Architecture Onboarding

- **Component map:** RGB Image → VLM (GPT-4o): (Prompting for Object ID + Ratio) → Knowledge Base (FloodKG): RDF Graph storing `heightMean` and `heightStd` for object classes → Processor: 1. Canonicalization (Normalizes VLM output to KG keys), 2. Look-up (Retrieves height), 3. Math: Depth = Ratio × Height → Filter: Statistical Outlier Removal (Removes fully submerged/anomalous depths) → Output: Min/Avg/Max Depth estimates (JSON)

- **Critical path:** The **Canonicalization Step** is the highest risk. If the VLM returns "a grey Toyota" but the KG key is "sedan," the lookup fails. The system's ability to fuzzy-match or hierarchy-traverse (subClassOf) determines if the grounding works or falls back to the less accurate VLM guess.

- **Design tradeoffs:**
  - **Latency vs. Robustness:** The system makes multiple calls or processes multiple objects per image. In a real-time emergency, waiting for 3 object analyses might introduce unacceptable lag.
  - **Static vs. Dynamic KG:** The current FloodKG is "curated" (static). It does not update in real-time based on field data, meaning it cannot adapt to new vehicle types (e.g., electric scooters) without manual schema migration.

- **Failure signatures:**
  - **Total Occlusion:** Reference object is underwater; Ratio = 1.0 (usually filtered as outlier). System returns nothing or a severely underestimated max depth.
  - **Perspective Parallax:** Waterline appears high on a distant object due to camera angle; VLM overestimates ratio.
  - **KG Miss:** VLM identifies "Dumpster," but FloodKG only has "Trash Can." System falls back to VLM hallucinations.

- **First 3 experiments:**
  1. **Unit Test KG Coverage:** Feed the VLM 100 random urban street images (non-flooded), extract all object classes identified, and check the "hit rate" against the current FloodKG keys to quantify coverage gaps.
  2. **Ablation on Ratio vs. Direct:** On a held-out set, compare "FloodVision (Full)" vs. "FloodVision (Direct Depth)" where the VLM guesses depth directly using the KG height as a hint, to prove the mechanism of ratio estimation.
  3. **Sensitivity to Ouliers:** Feed images with known "fully submerged" cars and verify that the statistical filter successfully excludes these data points to prevent infinite depth estimates.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can incorporating non-object visual cues, such as water surface texture and reflections, improve estimation accuracy when reference objects are occluded?
- **Basis in paper:** [explicit] Discussion states, "Future work could incorporate additional visual cues such as water surface texture, reflections... to support depth inference beyond object-based reasoning."
- **Why unresolved:** The current framework relies strictly on identifying and analyzing reference objects, ignoring environmental context when objects are absent.
- **What evidence would resolve it:** Performance comparison on images with obscured objects showing significantly lower error rates when texture cues are utilized.

### Open Question 2
- **Question:** Does spatiotemporal modeling enhance the reliability of FloodVision when applied to continuous video streams?
- **Basis in paper:** [explicit] Discussion suggests, "Future extensions that incorporate spatiotemporal modeling will better support real-time flood monitoring and dynamic flood progression analysis."
- **Why unresolved:** The current study evaluates static RGB images independently, lacking validation on temporal continuity or video data.
- **What evidence would resolve it:** Evaluations on video datasets demonstrating temporal consistency and reduced estimation variance across sequential frames.

### Open Question 3
- **Question:** Can synthetic urban flood images effectively evaluate and extend the system's generalization capabilities?
- **Basis in paper:** [explicit] Discussion proposes, "Future work may also explore generating synthetic urban flood images with predefined water depths to further evaluate and extend the system's generalization capabilities."
- **Why unresolved:** Real-world crowdsourced datasets may contain reporting biases or lack exact ground truth; synthetic data provides controlled variables.
- **What evidence would resolve it:** Successful performance maintenance (low MAE) when testing the model on diverse, high-fidelity synthetic datasets with known depths.

## Limitations
- The framework's performance heavily depends on FloodKG knowledge graph quality and coverage, which may not scale to diverse urban environments with varying object types.
- System reliability decreases when dealing with custom or modified objects where canonical heights from the KG would be incorrect, introducing systematic bias.
- The statistical outlier filtering mechanism's effectiveness against fully submerged reference objects is assumed but not extensively validated across diverse flood scenarios.

## Confidence
- **High Confidence:** The core mechanism of using canonical heights from a knowledge graph to ground VLM estimates (Mechanism 1) is well-supported by the empirical results showing 20.5% improvement over the GPT-4o baseline.
- **Medium Confidence:** The relative spatial reasoning advantage of ratio-based estimation (Mechanism 2) is demonstrated, but the paper lacks direct comparison against alternative prompting strategies or direct depth estimation methods.
- **Medium Confidence:** The multi-object statistical aggregation approach (Mechanism 3) shows promise, but the assumption of planar water surfaces across diverse urban scenes requires further validation, particularly in complex terrain.

## Next Checks
1. **Knowledge Graph Coverage Audit:** Systematically evaluate FloodKG's object coverage by analyzing 100 diverse urban images and measuring the percentage of VLM-identified objects that map to canonical KG entries.
2. **Perspective Distortion Analysis:** Conduct controlled experiments with images taken at varying angles to quantify the impact of perspective distortion on submergence ratio accuracy and validate the assumption that 2D ratios map to 3D physical heights.
3. **Real-time Performance Benchmark:** Measure end-to-end latency across the full pipeline (object detection, KG lookup, calculation, filtering) to assess practical deployment feasibility in emergency response scenarios where sub-second response times may be critical.