---
ver: rpa2
title: Low Rank Support Quaternion Matrix Machine
arxiv_id: '2512.08327'
source_url: https://arxiv.org/abs/2512.08327
tags:
- quaternion
- color
- matrix
- lsqmm
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Low-rank Support Quaternion Matrix Machine
  (LSQMM), a novel classification method for color image classification that represents
  RGB channels as pure quaternions to preserve intrinsic coupling relationships. The
  method incorporates a quaternion nuclear norm regularization term to promote low-rank
  structures resulting from strongly correlated color channels.
---

# Low Rank Support Quaternion Matrix Machine

## Quick Facts
- **arXiv ID:** 2512.08327
- **Source URL:** https://arxiv.org/abs/2512.08327
- **Authors:** Wang Chen; Ziyan Luo; Shuangyue Wang
- **Reference count:** 32
- **Primary result:** Novel LSQMM classification method achieves up to 27.73% higher accuracy than LIBSVM on color image datasets

## Executive Summary
This paper introduces the Low-rank Support Quaternion Matrix Machine (LSQMM), a novel classification method for color image classification that represents RGB channels as pure quaternions to preserve intrinsic coupling relationships. The method incorporates a quaternion nuclear norm regularization term to promote low-rank structures resulting from strongly correlated color channels. An Alternating Direction Method of Multipliers (ADMM)-based iterative algorithm is designed for optimization. Experimental results on six public color image datasets demonstrate that LSQMM outperforms state-of-the-art methods including LIBSVM, Support Matrix Machine, and Support Tensor Machine.

## Method Summary
LSQMM encodes RGB channels as pure quaternion matrices to preserve inter-channel correlations through non-commutative quaternion multiplication. The model uses hinge loss with quaternion nuclear norm regularization (∥W∥_{Q,*}) to encourage low-rank weight matrices. Optimization is performed via ADMM, transforming the quaternion problem into real-valued optimization through isomorphism mapping. The algorithm alternates between updating the weight matrix, applying quaternion singular value decomposition with soft-thresholding, and adjusting dual variables until convergence.

## Key Results
- LSQMM achieves up to 27.73% higher accuracy and 76.34% higher F1-score than LIBSVM on certain datasets
- Outperforms state-of-the-art methods including LIBSVM, Support Matrix Machine, and Support Tensor Machine
- Demonstrates superior robustness to noise and computational efficiency, particularly excelling in small-sample high-dimensional scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Quaternion representation preserves inter-channel correlations in color images better than vectorization or tensor concatenation.
- **Mechanism:** RGB channels are encoded as pure quaternions (Ri + Gj + Bk) where the non-commutativity of quaternion multiplication (ij ≠ ji) naturally captures coupling relationships and phase information between channels.
- **Core assumption:** Color image channels exhibit strong intrinsic correlations that carry discriminative information.
- **Evidence anchors:**
  - [abstract] "RGB channels are treated as pure quaternions to effectively preserve the intrinsic coupling relationships among channels via the quaternion algebra"
  - [section 1] "By encoding RGB channels within the three imaginary components of a pure quaternion matrix... this representation inherently preserves spatial structure while maintaining spectral correlations and phase information"
  - [corpus] Quaternion-based methods in denoising and inpainting (Chen & Ng, 2022; Yu et al., 2019) show similar correlation-preservation benefits
- **Break Condition:** If RGB channels in a dataset are artificially decorrelated or independently generated, quaternion advantages would diminish.

### Mechanism 2
- **Claim:** Quaternion nuclear norm regularization promotes discriminative low-rank structure in the weight matrix, reducing overfitting.
- **Mechanism:** The nuclear norm ∥W∥_{Q,*} (sum of singular values from quaternion SVD) acts as convex relaxation of rank minimization, encouraging the model to learn compact feature representations from strongly correlated color channels.
- **Core assumption:** The discriminative features for classification lie in a low-dimensional subspace of the quaternion matrix space.
- **Evidence anchors:**
  - [abstract] "quaternion nuclear norm regularization term... added to the hinge loss in our LSQMM model"
  - [section 4.5] "The introduction of nuclear norm regularization enables LSQMM to possess adaptive rank adjustment capability"
  - [corpus] Sparse/low-rank SMM variants (Zheng et al., 2018) show similar benefits in EEG and grayscale tasks
- **Break Condition:** If color image classes require high-rank representations (highly complex texture patterns), nuclear norm regularization could over-constrain the model.

### Mechanism 3
- **Claim:** ADMM optimization with real isomorphism guarantees convergence while efficiently handling quaternion constraints.
- **Mechanism:** The quaternion problem is transformed to an equivalent real-valued 4m×4n optimization via isomorphism mapping Ψ, satisfying standard ADMM convergence conditions. Z-subproblem uses quaternion SVD proximal operator for closed-form updates.
- **Core assumption:** The hinge loss with nuclear norm creates a two-block separable convex problem amenable to ADMM.
- **Evidence anchors:**
  - [section 3.4] "This equivalent real problem satisfies the convergence conditions of the ADMM algorithm"
  - [section 3.2] "Z^{k+1} admits a closed form solution... Prox operator of quaternion nuclear norm"
  - [corpus] ADMM convergence for real-valued nuclear norm problems is well-established (Boyd et al., 2011); quaternion extension appears novel but relies on same principles
- **Break Condition:** If parameter ρ (penalty parameter) or τ (step size) are poorly chosen outside (0, 1.618], convergence may fail or slow significantly.

## Foundational Learning

- **Concept: Quaternion Algebra**
  - **Why needed here:** Understanding why q = a + bi + cj + dk with non-commutative multiplication (ij = k, ji = -k) enables unified RGB representation
  - **Quick check question:** If you compute (Ri + Gj)(Ri + Bj), what cross-channel terms appear?

- **Concept: Nuclear Norm Regularization**
  - **Why needed here:** Understanding how ∥W∥_* = Σσ_i promotes low-rank solutions by penalizing sum of singular values
  - **Quick check question:** Why does minimizing nuclear norm approximate minimizing rank?

- **Concept: Proximal Operators**
  - **Why needed here:** Understanding how Prox_{λ∥·∥_*}(A) = U·diag(soft-threshold(σ))·V* enables closed-form Z-updates
  - **Quick check question:** What happens to singular values below threshold λ/ρ in the proximal operator?

## Architecture Onboarding

- **Component map:** Color image → m×n pure quaternion matrix X (R_i,j·i + G_i,j·j + B_i,j·k) → f(X) = Re(⟨W, X⟩ + b) → hinge loss with nuclear norm → ADMM optimization

- **Critical path:**
  1. Preprocessing: Build N×N Gram matrix K (O(N²mn))
  2. W-update: Solve dual QP (12) using OSQP → get α*, compute W^{k+1}
  3. Z-update: Quaternion SVD of (W^{k+1} - U^k/ρ) → apply soft-thresholding
  4. U-update: Dual variable adjustment U^{k+1} = U^k - τρ(W^{k+1} - Z^{k+1})
  5. Repeat until ∥W - Z∥_F / max{∥W∥_F, ∥Z∥_F} < 1e-3

- **Design tradeoffs:**
  - λ (nuclear norm weight): Higher → stronger low-rank constraint, risk of underfitting
  - C (soft margin): Higher → less tolerance for misclassification, risk of overfitting
  - Quaternion SVD vs. real 4m×4n SVD: Same theoretical complexity, but implementation maturity differs
  - Assumption: Current implementation uses 4m×4n real SVD; specialized quaternion SVD could reduce constants

- **Failure signatures:**
  - Accuracy drops sharply as λ increases beyond 1: nuclear norm over-constraining
  - Slow convergence (>1000 iterations): ρ too small or τ outside (0, 1.618]
  - F1-score = 0 (as in LIBSVM on Broken Eggs): model predicting single class; check class balance and C value
  - Memory overflow on large images: qSVD requires 4m×4n real matrix; consider downsampling

- **First 3 experiments:**
  1. **Baseline validation:** Run LSQMM on Eye Disease dataset with default parameters (C=1, λ=0.01, τ=1). Verify convergence within 10 iterations and accuracy ~95%. Check that support matrix indices S* are non-empty.
  2. **Ablation study:** Set λ=0 to disable nuclear norm regularization. Compare accuracy on Broken Eggs dataset (Table 3 shows 80.90% with λ>0). Expect 5-10% degradation confirming low-rank benefit.
  3. **Noise robustness test:** Add Gaussian white noise with ratio R=0.5 to Eye Disease images. Verify LSQMM maintains ~90% accuracy (per Table 6) while LIBSVM drops to ~78%.

## Open Questions the Paper Calls Out
None

## Limitations

- **Computational Scaling:** The qSVD step requires 4m×4n real matrix operations, creating memory bottlenecks for high-resolution images (e.g., 1000×1000 pixels requires 4GB for single-precision matrices).
- **Parameter Sensitivity:** Performance heavily depends on three hyperparameters (C, λ, ρ) with no systematic tuning strategy provided. Poor choices lead to either convergence failure or overfitting.
- **Dataset Dependency:** The quaternion advantage assumes strong RGB channel correlations exist in the data. For decorrelated color channels or grayscale datasets, the method would offer no benefit over real-valued SMM.

## Confidence

- **High Confidence:** Mechanism 1 (quaternion correlation preservation) - well-supported by quaternion algebra theory and existing literature on color image processing
- **Medium Confidence:** Mechanism 2 (nuclear norm regularization benefits) - supported by theory but dataset-specific effectiveness varies
- **Medium Confidence:** Mechanism 3 (ADMM convergence) - theoretical conditions met but quaternion-specific convergence proofs are not provided

## Next Checks

1. **Scaling Experiment:** Test LSQMM on progressively larger image sizes (64×64 → 512×512 → 2048×2048) and measure memory usage and iteration count. Verify if O(m²n²) complexity holds empirically.

2. **Channel Independence Test:** Create synthetic color datasets with independent R, G, B channels (no correlation). Compare LSQMM performance against real-valued SMM to confirm quaternion benefits disappear when correlations are absent.

3. **Hyperparameter Sensitivity Analysis:** Systematically vary C ∈ {0.01, 0.1, 1, 10} and λ ∈ {0.001, 0.01, 0.1, 1} on Eye Disease dataset. Map accuracy contours to identify optimal operating regions and potential overfitting/underfitting thresholds.