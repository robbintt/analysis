---
ver: rpa2
title: Budgeted Adversarial Attack against Graph-Based Anomaly Detection in Sensor
  Networks
arxiv_id: '2509.17987'
source_url: https://arxiv.org/abs/2509.17987
tags:
- anomaly
- attack
- nodes
- detection
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BETA, a novel grey-box adversarial evasion
  attack against graph neural network (GNN) based anomaly detection systems in sensor
  networks. The attack operates under realistic budget constraints, where the attacker
  can only perturb a limited number of sensors while aiming to either suppress true
  anomalies or trigger false alarms at a target node.
---

# Budgeted Adversarial Attack against Graph-Based Anomaly Detection in Sensor Networks

## Quick Facts
- arXiv ID: 2509.17987
- Source URL: https://arxiv.org/abs/2509.17987
- Reference count: 40
- This paper introduces BETA, a novel grey-box adversarial evasion attack against graph neural network (GNN) based anomaly detection systems in sensor networks. The attack operates under realistic budget constraints, where the attacker can only perturb a limited number of sensors while aiming to either suppress true anomalies or trigger false alarms at a target node.

## Executive Summary
This paper introduces BETA, a novel grey-box adversarial evasion attack against graph neural network (GNN) based anomaly detection systems in sensor networks. The attack operates under realistic budget constraints, where the attacker can only perturb a limited number of sensors while aiming to either suppress true anomalies or trigger false alarms at a target node. BETA uses GAFExplainer to identify influential nodes and eigenvector centrality to prune them within the budget, then applies constrained PGD to craft imperceptible perturbations. Experiments on three real-world datasets (SWaT, WADI, SJVAir) show BETA reduces detection accuracy by 30.62–39.16% on average, significantly outperforming baseline methods like Nettack and random attacks while remaining stealthy.

## Method Summary
BETA operates in three phases: first, it trains a surrogate model using query access to the victim's predictions; second, it applies GAFExplainer to identify influential nodes for the target node's classification; third, it uses eigenvector centrality to select the top-B nodes within budget and applies constrained PGD to perturb their features while remaining imperceptible. The attack aims to flip the target node's classification while staying under a budget of B nodes and an ℓ∞ norm bound of ε=0.1.

## Key Results
- BETA reduces GNN-based anomaly detection accuracy by 30.62–39.16% on average across SWaT, WADI, and SJVAir datasets
- Outperforms baseline methods (Nettack, random attacks) by significant margins in both Fractional Target Accuracy (FTA) and F1 score reduction
- Eigenvector centrality consistently outperforms other centrality measures for node selection within budget constraints
- Attack remains effective across different GNN architectures (GDN and TopoGDN) with grey-box access

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GAFExplainer identifies nodes whose features most influence the target node's anomaly classification by tracing gradient pathways through the GNN's layer-wise embeddings.
- Mechanism: GAFExplainer constructs a mask over edges using a binary concrete distribution, extracting a subgraph whose incident nodes are treated as candidate influencer nodes. The explanation network is trained by minimizing the difference between the subgraph prediction and the original prediction for the target node.
- Core assumption: The surrogate model approximates the victim model's decision boundaries sufficiently well that influential nodes for the surrogate are also influential for the victim.
- Evidence anchors:
  - [abstract] "BETA identifies the sensors most influential to the target node's classification and injects carefully crafted adversarial perturbations into their features"
  - [section 6.1] "GAFExplainer first uses a node attribute augmentation module... These augmented features are combined with layer-wise GNN embeddings... allowing us to identify the most influential nodes for classifying the target node."
  - [corpus] Related work on GNN robustness (arxiv 2506.20806) confirms GNN-based intrusion detection suffers performance degradation under adversarial attacks, but does not specifically validate GAFExplainer's transferability.
- Break condition: If the surrogate model's architecture diverges significantly from the victim model, the identified influential nodes may not transfer, degrading attack effectiveness.

### Mechanism 2
- Claim: Eigenvector centrality prioritizes nodes that are structurally central in the graph, improving attack efficiency when the candidate set exceeds the budget.
- Mechanism: After GAFExplainer produces candidate nodes V_S, nodes are ranked by eigenvector centrality (solving Ac = λ_max·c), and the top-B nodes are selected. This ensures selected nodes are connected to other influential nodes, amplifying perturbation effects through message passing.
- Core assumption: Structural centrality correlates with influence on GNN predictions beyond what GAFExplainer captures directly.
- Evidence anchors:
  - [section 6.2] "This ensures that the nodes selected for perturbation are not only locally important to the prediction task but also structurally central in the graph."
  - [section 8.3, Table 5] "For all budgets, we find that selecting influencer nodes based on eigenvector centrality consistently results in the lowest FTA, highlighting its superior ability to identify influential nodes"
  - [corpus] No direct corpus validation for eigenvector centrality in adversarial contexts; this appears to be a domain-specific heuristic.
- Break condition: In graphs where information flow is primarily local (low diameter), eigenvector centrality may not meaningfully differentiate node importance.

### Mechanism 3
- Claim: Constrained PGD optimizes perturbations to flip the target node's classification while remaining imperceptible by projecting onto an ℓ∞-ball.
- Mechanism: At each iteration k, features are updated as X_{k+1} = Π(X_k + α·sign(∇_X L)) ⊙ S_V̄, where Π projects onto the ℓ∞-ball of radius ε, and S_V̄ masks non-influencer nodes. The loss L is cross-entropy between the surrogate's prediction and the true label for target node u.
- Core assumption: Perturbations with ℓ∞ norm < 0.1 are imperceptible to human operators and downstream anomaly detectors.
- Evidence anchors:
  - [abstract] "reduces the detection accuracy of state-of-the-art GNN-based detectors by 30.62 to 39.16% on average"
  - [section 6] "We assume that perturbations that have an ℓ∞ norm smaller than ε = 0.1 are imperceptible."
  - [corpus] arxiv 2602.00318 demonstrates optimal transport-guided attacks on GNN-based bot detection, confirming gradient-based attacks transfer, but uses different constraints.
- Break condition: If detection systems employ additional sanity checks (e.g., physical plausibility constraints, temporal consistency), small perturbations may be flagged as artifacts.

## Foundational Learning

- Concept: **Graph Attention Networks (GAT) and message passing**
  - Why needed here: BETA exploits how GNNs aggregate neighbor information through attention-weighted sums. Understanding equations (3)-(5) is essential to see why perturbing influencer nodes affects the target's representation.
  - Quick check question: Given attention coefficients β_ij, how would zeroing node j's features affect node i's aggregated representation r_i^t?

- Concept: **Projected Gradient Descent (PGD) for adversarial attacks**
  - Why needed here: The core attack optimization uses constrained PGD (Algorithm 1). Understanding the projection operator Π and the sign gradient is necessary to implement or modify the attack.
  - Quick check question: Why does PGD use sign(∇_X L) rather than ∇_X L directly? What does the projection step guarantee?

- Concept: **Centrality measures in graph theory**
  - Why needed here: Eigenvector centrality is used for pruning candidate nodes. Understanding that a node's centrality depends on its neighbors' centrality helps explain why this heuristic amplifies attack impact.
  - Quick check question: If node A has high eigenvector centrality but low degree, what does this imply about its neighbors?

## Architecture Onboarding

- Component map: Surrogate model trainer -> GAFExplainer module -> Centrality pruner -> Constrained PGD optimizer

- Critical path: Surrogate quality → GAFExplainer mask accuracy → Centrality ranking correctness → PGD convergence. Errors compound; surrogate fidelity is the primary bottleneck.

- Design tradeoffs:
  - Larger E (edge budget for GAFExplainer) increases candidate set size, improving chance of including high-impact nodes, but requires more aggressive pruning.
  - Smaller ε improves stealth but may require more PGD iterations or larger B to achieve attack success.
  - Using all nodes (unbudgeted BETA) maximizes attack effectiveness but is unrealistic; budget constraint trades realism for FTA reduction.

- Failure signatures:
  - FTA remains high (>0.7) even at B=6 → Likely surrogate-victim mismatch; retrain surrogate with more query data.
  - FTA drops for GDN but not TopoGDN → Centrality-based selection may not transfer across architectures; consider architecture-specific explainers.
  - Perturbations detected as anomalies → ε may be too large; reduce ε or add temporal smoothness constraints.

- First 3 experiments:
  1. **Surrogate fidelity test**: Train surrogate models with varying query budgets (100, 500, 1000 samples). Measure correlation between surrogate and victim predictions on held-out data. Report FTA under BETA for each surrogate.
  2. **Centrality ablation**: Replace eigenvector centrality with degree, betweenness, and closeness centrality. Run BETA on SWaT dataset with B=3-6. Reproduce Table 5 to validate eigenvector superiority.
  3. **Transferability check**: Train surrogate as GDN, attack TopoGDN victim (and vice versa). Measure FTA gap compared to matched surrogate-victim pairs to quantify architecture sensitivity.

## Open Questions the Paper Calls Out

- **Future work on defenses**: The paper explicitly states future work will focus on developing robust defenses against such attacks, including graph smoothing, adversarial training, and explainability-guided strategies. This question remains unresolved as the current study focuses entirely on the offensive aspect.

- **Black-box extension**: The threat model assumes grey-box access with full graph structure, but the feasibility of attacking without this knowledge is unexplored. GAFExplainer and eigenvector centrality both require the adjacency matrix as input.

- **Structural perturbations**: The methodology constrains attacks to feature perturbations only for fair comparison to baselines, excluding the potential impact of structural attacks that modify graph edges.

## Limitations

- **Surrogate Fidelity Uncertainty**: The paper assumes surrogate-victim transfer but doesn't empirically validate this across architecture mismatches or provide surrogate architecture details.
- **Constraint Validity Gap**: The ℓ∞ bound of 0.1 is arbitrary without domain-specific justification from sensor network operators regarding what constitutes imperceptible perturbations.
- **Centrality Generalizability**: Eigenvector centrality is presented as superior but lacks direct ablation studies against alternatives like degree or betweenness centrality.

## Confidence

- **High**: The mathematical formulation of the constrained PGD attack (Algorithm 1) is sound and follows established adversarial ML principles. The claim that BETA reduces FTA by 30.62-39.16% is directly supported by Table 3.
- **Medium**: The mechanism by which GAFExplainer identifies influential nodes through gradient tracing is theoretically justified, but the surrogate-victim transfer assumption needs validation. The claim that eigenvector centrality amplifies attack impact is supported by Table 5 but lacks comparative ablation.
- **Low**: The claim that ℓ∞-bounded perturbations are universally imperceptible across all three datasets (SWaT, WADI, SJVAir) is untested against domain-specific anomaly detection sanity checks.

## Next Checks

1. **Surrogate Transferability Test**: Train surrogate models with varying query budgets (100, 500, 1000 samples) and measure correlation between surrogate and victim predictions. Report how FTA degrades as surrogate-victim mismatch increases.

2. **Centrality Ablation Study**: Replace eigenvector centrality with degree, betweenness, and closeness centrality in BETA. Run attacks on SWaT with B=3-6 and compare FTA reduction to validate the centrality choice.

3. **Stealthiness Validation**: Implement a secondary anomaly detector that checks for physical plausibility (e.g., temporal consistency, sensor correlation) and measure how often BETA perturbations trigger this detector beyond the primary GNN model.