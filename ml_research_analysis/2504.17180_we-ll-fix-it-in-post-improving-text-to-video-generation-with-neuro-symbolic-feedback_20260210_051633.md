---
ver: rpa2
title: 'We''ll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic
  Feedback'
arxiv_id: '2504.17180'
source_url: https://arxiv.org/abs/2504.17180
tags:
- video
- temporal
- generation
- neus-e
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NeuS-E addresses the problem of temporal misalignment in text-to-video
  generation, where current models struggle to produce videos that correctly sequence
  events described in prompts. The core method leverages neuro-symbolic feedback to
  automatically identify weakly satisfied propositions in generated videos and surgically
  refine only those problematic segments.
---

# We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback

## Quick Facts
- arXiv ID: 2504.17180
- Source URL: https://arxiv.org/abs/2504.17180
- Authors: Minkyu Choi; S P Sharan; Harsh Goel; Sahil Shah; Sandeep Chinchali
- Reference count: 40
- Key outcome: NeuS-E achieves 40% improvement in temporal fidelity scores by identifying and surgically refining misaligned video segments using neuro-symbolic feedback.

## Executive Summary
NeuS-E addresses the problem of temporal misalignment in text-to-video generation, where current models struggle to produce videos that correctly sequence events described in prompts. The core method leverages neuro-symbolic feedback to automatically identify weakly satisfied propositions in generated videos and surgically refine only those problematic segments. By analyzing video automata representations through formal verification, NeuS-E pinpoints misaligned events and their corresponding frames, then guides targeted keyframe editing and regeneration. The approach achieves significant improvements, increasing temporal fidelity scores by almost 40% across diverse themes and complexity levels when tested on both closed-source (Gen-3, Pika) and open-source (CogVideoX) models. Human evaluations confirm these quantitative results, with a majority of reviewers preferring the edited videos for better alignment with the original prompts. The method demonstrates that neuro-symbolic feedback can effectively improve temporal consistency without requiring additional model training or endless re-prompting.

## Method Summary
NeuS-E converts text prompts into temporal logic specifications, then verifies generated videos against these specifications using a video automaton constructed from VLM confidence scores. The method identifies the weakest proposition (most misaligned event) through systematic perturbation of confidence scores, pinpoints the critical frame causing temporal misalignment, and performs localized keyframe editing and video regeneration. This iterative refinement continues until satisfaction probability exceeds a threshold or iteration limits are reached. The approach is model-agnostic, working with any T2V model as the base generator.

## Key Results
- Achieved 40% improvement in temporal fidelity scores across diverse themes and complexity levels
- Human evaluations show majority preference for edited videos (ranging from 32% to 53% preference depending on model)
- Demonstrated effectiveness across both closed-source (Gen-3, Pika) and open-source (CogVideoX) T2V models
- Successfully handled complex temporal specifications involving sequential events, until operators, and eventualities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting natural language prompts to temporal logic specifications enables formal verification of event sequences in generated videos.
- Mechanism: An LLM (GPT-4o) decomposes text prompts into atomic propositions P and a temporal logic specification Φ using operators like NEXT (X), UNTIL (U), and EVENTUALLY (♢). A Vision-Language Model (InternVL2.5-8B) then computes per-frame confidence scores for each proposition, constructing a video automaton represented as a Discrete-Time Markov Chain (DTMC). This automaton is verified against Φ using the STORM probabilistic model checker.
- Core assumption: VLM confidence scores, when calibrated, accurately reflect proposition satisfaction in individual frames.
- Evidence anchors:
  - [abstract]: "Our approach first derives the neuro-symbolic feedback by analyzing a formal video representation and pinpoints semantically inconsistent events, objects, and their corresponding frames."
  - [section 4.1]: "Given Φ and P, we construct the video automaton as described in Definition 1 and verify it according to Definition 2."
  - [corpus]: Limited direct corpus support for this specific automaton-based approach; SeqBench confirms T2V models struggle with sequential narratives but focuses on benchmarking rather than formal methods.
- Break condition: If VLM confidence scores are poorly calibrated or systematically biased, the automaton construction will misrepresent actual video content, leading to incorrect verification results.

### Mechanism 2
- Claim: Systematic perturbation of proposition confidence scores identifies which specific events or objects cause the greatest degradation in temporal alignment.
- Mechanism: For each proposition p_i, the method temporarily sets its confidence to 1.0 across all frames while holding other confidences constant, then recomputes the satisfaction probability P[AV |= Φ]. The proposition whose adjustment produces the largest increase in satisfaction probability is identified as the "weakest proposition" p*.
- Core assumption: The satisfaction probability is sufficiently sensitive to individual proposition changes to produce distinguishable gradients.
- Evidence anchors:
  - [abstract]: "pinpoints semantically inconsistent events, objects, and their corresponding frames"
  - [section 4.2]: "The proposition whose confidence adjustment causes the greatest shift in satisfaction probability reveals the weakest component."
  - [corpus]: No corpus papers directly address this perturbation-based diagnosis approach.
- Break condition: If multiple propositions have similar influence or the specification has complex interdependencies, the gradient-based identification may select the wrong target for editing.

### Mechanism 3
- Claim: Localized keyframe editing and regeneration at precisely identified failure points improves temporal alignment without degrading well-generated portions.
- Mechanism: Once the weakest proposition p* and its most impacted frame F*n are identified, the video is trimmed at F*n, the keyframe is edited using OmniGen (prompt-based image editing), and a new video segment is generated using the modified keyframe as the starting image. This process iterates until P[AV |= Φ] exceeds a threshold (0.7) or iteration limits are reached.
- Core assumption: The underlying T2V model can follow regeneration instructions when provided with a coherent starting keyframe.
- Evidence anchors:
  - [abstract]: "surgically refine only those problematic segments"
  - [section 4.4]: "The process iterates—regenerating and merging—until P[AV |= Φ] surpasses a predefined threshold"
  - [corpus]: SeqBench and VC4VG highlight prompt sensitivity in T2V models, supporting the need for structured guidance; no corpus papers implement this specific iterative refinement loop.
- Break condition: If the T2V model fails to follow regeneration instructions or produces inconsistent continuations, multiple iterations may not converge.

## Foundational Learning

- **Temporal Logic (TL)**:
  - Why needed here: TL provides the formal language to express event sequences (e.g., "A happens, THEN B happens, UNTIL C occurs"). Without understanding operators like NEXT (X), ALWAYS (□), and UNTIL (U), you cannot interpret how NeuS-E specifies and verifies temporal requirements.
  - Quick check question: Given propositions P = {rain, umbrella, dry}, write a TL formula expressing "It rains until the person uses an umbrella, then they stay dry."

- **Discrete-Time Markov Chains (DTMCs)**:
  - Why needed here: The video automaton is formalized as a DTMC where states represent frame-level semantics and transitions carry probabilities. Understanding state transition functions δ: Q × Q → [0,1] is essential for grasping how verification computes satisfaction probabilities.
  - Quick check question: If a DTMC has states {q0, q1, q2} with δ(q0, q1) = 0.7 and δ(q0, q2) = 0.3, what is the probability of reaching q2 from q0 in one step?

- **Probabilistic Model Checking**:
  - Why needed here: NeuS-E uses the STORM model checker to compute P[AV |= Φ], the probability that a video automaton satisfies its temporal specification. Understanding PCTL (Probabilistic Computation Tree Logic) is necessary to interpret how verification produces actionable scores.
  - Quick check question: If a model checker returns P[AV |= Φ] = 0.35 for a specification requiring sequential events A then B, what does this suggest about the video's temporal alignment?

## Architecture Onboarding

- **Component map**: Text prompt -> GPT-4o (TL decomposition) -> InternVL2.5-8B (VLM confidence scoring) -> DTMC construction -> STORM (model checking) -> Perturbation analysis -> Frame localization -> OmniGen (keyframe editing) -> T2V model (video regeneration) -> MoviePy (video stitching)

- **Critical path**: Prompt → TL decomposition → Initial video generation → Automaton construction → Model checking → Weak proposition identification → Frame localization → Keyframe editing → Segment regeneration → Stitching → Re-verification → (iterate if below threshold)

- **Design tradeoffs**:
  - **Keyframe editing vs. direct regeneration**: Table 3 shows keyframe editing improves NeuS-V (+0.258 vs +0.233) but degrades VBench (-0.106 vs -0.017), suggesting visual quality tradeoffs. Default configuration omits keyframe editing.
  - **Iteration depth**: Figure 5 shows gains plateau around iteration 3; more iterations increase computational cost without proportional improvement.
  - **Threshold selection**: Threshold of 0.7 is heuristic; higher thresholds may cause non-convergence, lower thresholds may leave residual misalignment.

- **Failure signatures**:
  - High neutral ratings in human evaluation (Figure 4: 31-64% depending on model) indicate cases where either the original video was already aligned or the T2V model failed to follow regeneration instructions.
  - CogVideoX-5B shows minimal improvement (Figure 5c), suggesting some models are less responsive to edit instructions.
  - VBench scores do not improve (Table 1: slight decrease from 0.789 to 0.772 for Pika), indicating temporal fixes may not enhance general video quality metrics.

- **First 3 experiments**:
  1. **Baseline verification**: Generate videos using a target T2V model on temporally complex prompts, construct automata, and compute baseline satisfaction probabilities to establish the performance floor.
  2. **Weak proposition identification sanity check**: Manually inspect videos where NeuS-E identifies weak propositions—verify that the identified proposition (e.g., "person standing") is genuinely missing or poorly represented in the corresponding frames.
  3. **Ablation on frame localization**: Compare full NeuS-E against a variant that regenerates from a random frame (not F*n) to confirm that precise localization contributes to improvement beyond simple regeneration.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology raises several important unresolved issues regarding the general applicability and limitations of neuro-symbolic feedback approaches.

## Limitations
- Method's effectiveness depends heavily on VLM confidence score calibration, which is not fully detailed in the main text
- Quality degradation trade-offs (as evidenced by VBench score decreases) may limit practical applicability
- Human evaluation results show substantial neutral ratings (31-64%), indicating cases where either the original video was already aligned or the T2V model failed to follow regeneration instructions

## Confidence
- **High**: Formal verification mechanism for identifying misaligned events
- **Medium**: Weak proposition identification through perturbation
- **Low**: Generalization across diverse T2V model architectures

## Next Checks
1. **VLM Calibration Validation**: Systematically test how variations in InternVL2.5-8B confidence score calibration affect automaton construction accuracy and verification results across diverse video content.

2. **Perturbation Gradient Sensitivity**: Conduct controlled experiments varying the number and complexity of proposition interdependencies to quantify how often the perturbation-based identification selects the correct target for editing.

3. **Cross-Model Transferability**: Test NeuS-E on additional T2V architectures beyond the three models evaluated, particularly focusing on models with different underlying mechanisms (e.g., GAN-based vs. diffusion-based) to assess generalization limits.