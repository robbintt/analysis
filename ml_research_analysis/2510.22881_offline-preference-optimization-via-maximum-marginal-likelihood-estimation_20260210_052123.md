---
ver: rpa2
title: Offline Preference Optimization via Maximum Marginal Likelihood Estimation
arxiv_id: '2510.22881'
source_url: https://arxiv.org/abs/2510.22881
tags:
- preference
- mmpo
- objective
- optimization
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMPO, a new preference optimization method
  for aligning large language models with human preferences. MMPO maximizes the marginal
  log-likelihood of preferred text outputs using a numerically stable log-sum-exp
  formulation, implicitly performing preference optimization without requiring an
  explicit reward model or entropy maximization.
---

# Offline Preference Optimization via Maximum Marginal Likelihood Estimation

## Quick Facts
- **arXiv ID:** 2510.22881
- **Source URL:** https://arxiv.org/abs/2510.22881
- **Reference count:** 15
- **Primary result:** MMPO achieves superior stability with respect to β compared to DPO and SimPO while better preserving general language capabilities.

## Executive Summary
This paper introduces MMPO (Maximum Marginal Likelihood Optimization), a novel preference optimization method for aligning large language models with human preferences. MMPO maximizes the marginal log-likelihood of preferred text outputs using a numerically stable log-sum-exp formulation, implicitly performing preference optimization without requiring an explicit reward model or entropy maximization. The method demonstrates competitive or superior preference alignment (winning 62-68% of comparisons on AlpacaEval-2) while better preserving general language capabilities (maintaining 34-46% accuracy on LM Harness tasks) across models from 135M to 8B parameters.

## Method Summary
MMPO optimizes the marginal log-likelihood of preferred responses by approximating the intractable sum over all possible generations with preference pairs. The core innovation is a log-sum-exp formulation that implicitly weights gradients via a sigmoid function of score differences, combined with in-batch normalization to stabilize training across hyperparameter settings. The method forgoes explicit entropy maximization, instead relying on the MML objective to balance exploration and preservation of base model capabilities.

## Key Results
- MMPO demonstrates superior stability with respect to the hyperparameter β compared to baselines DPO and SimPO
- Achieves competitive or superior preference alignment (62-68% win rate on AlpacaEval-2)
- Better preserves general language capabilities (34-46% accuracy on LM Harness tasks)
- Ablation studies confirm performance stems from implicit preference optimization and in-batch normalization, with explicit entropy regularization proving detrimental

## Why This Works (Mechanism)

### Mechanism 1: Implicit Preference Weighting via Sigmoid Gradients
The Maximum Marginal Likelihood (MML) objective, when approximated with preference pairs, implicitly generates a weighted gradient that favors preferred responses without an explicit reward function. By optimizing the log-sum-exp of log-probabilities, the gradient splits into two components scaled by a sigmoid function of the score difference (σ(s_w - s_l)). If the preferred response (z_w) has a significantly higher score than the rejected one (z_l), the sigmoid weight for z_w approaches 1, strongly reinforcing it, while the weight for z_l approaches 0.

### Mechanism 2: Stabilization via In-Batch Normalization
Normalizing reward scores across the mini-batch dampens sensitivity to the temperature hyperparameter β, preventing gradient instability common in DPO/SimPO. MMPO normalizes the reference model's log-probabilities (rewards) within a batch to a [0, 1] range before combining them with the policy log-probabilities. This prevents "outlier" preference pairs from dominating the batch gradient, ensuring the sigmoid weighting mechanism operates in a consistent dynamic range regardless of β.

### Mechanism 3: Preservation of Capabilities via Entropy Minimization
Removing the explicit entropy maximization term (standard in RLHF/DPO) helps preserve the base model's general language capabilities. Standard RLHF maximizes reward while explicitly maximizing entropy (βH) to prevent collapse. MMPO removes this, implicitly trusting the MML objective to balance exploration. Ablation studies show that adding entropy maximization back degrades performance, suggesting that for offline alignment on capable SFT models, forced diversity is detrimental.

## Foundational Learning

- **Concept: Log-Sum-Exp Trick**
  - Why needed here: It is the mathematical engine of MMPO. You must understand how logsumexp approximates the marginal sum and stabilizes gradients (Lemma 3.1) to debug the loss function.
  - Quick check question: How does subtracting the max value (s*) inside the exponent prevent numerical overflow?

- **Concept: Bradley-Terry Model**
  - Why needed here: This underpins the DPO auxiliary loss and baselines. Understanding that DPO models preference probability as a logistic function of reward difference helps clarify what MMPO changes (removing the reference model dependence in the main term).
  - Quick check question: In DPO, what does the β parameter represent in relation to the Bradley-Terry model?

- **Concept: KL-Divergence & Entropy**
  - Why needed here: To grasp why MMPO is unique. You need to distinguish between the KL penalty used in DPO (keeping the policy close to reference) and the explicit entropy term dropped by MMPO.
  - Quick check question: Why would maximizing entropy potentially hurt a model's performance on specific reasoning benchmarks (as seen in the ablation)?

## Architecture Onboarding

- **Component map:** Input (x, z_w, z_l) -> Scoring (Policy + Reference Models) -> Normalization (min-max across batch) -> MMPO Core (logsumexp) -> Auxiliary (DPO-style margin) -> Optimizer
- **Critical path:** The interaction between In-batch Normalization and the Log-Sum-Exp operation (Listing 1, lines 19-34). An engineer must ensure gather operations across devices (for batch norm) are synchronized correctly; incorrect normalization is the most likely implementation failure mode.
- **Design tradeoffs:**
  - Auxiliary Loss: The authors include a DPO-style auxiliary loss to stabilize the margin, but ablations show the core logsumexp term is the primary driver of capability preservation. Removing the auxiliary loss slightly improves alignment but hurts general capability.
  - Batch Size: Larger batches are theoretically better for the in-batch normalization statistics but cost more memory.
- **Failure signatures:**
  - Unstable Gradients: If β is large and normalization is disabled, logsumexp may produce saturating gradients.
  - Reward Hacking: If the reference model is weak, the normalization might amplify noise.
- **First 3 experiments:**
  1. Sanity Check (Beta Sweep): Train MMPO vs. DPO on a small dataset (1 epoch) with β ∈ {0.01, 0.5}. Verify that DPO collapses or oscillates while MMPO remains stable (Figure 1/2 behavior).
  2. Ablation (Normalization): Run MMPO with norm=False. Confirm that the performance trajectory becomes erratic or degrades, validating the mechanism in Section 5.
  3. Capability Retention: Evaluate a checkpoint against a generic LM Harness task (e.g., Hellaswag). Compare against a DPO checkpoint to verify that MMPO degrades the base accuracy less than DPO.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper's superiority claims rely heavily on the effectiveness of in-batch normalization and the sufficiency of the 1-pair approximation to true marginal likelihood
- Empirical validation is limited to specific preference datasets and model scales (135M-8B parameters)
- Does not explore failure modes when the base SFT model is weak or when preference data contains significant noise

## Confidence
- **High Confidence:** The mathematical formulation of MMPO (Theorem 3.2, Lemma 3.1) and the basic implementation of in-batch normalization are sound and reproducible.
- **Medium Confidence:** The claim that MMPO is more stable across β values than DPO/SimPO is supported by ablation studies, but the mechanism's robustness to extreme β values or noisy preference data needs further validation.
- **Medium Confidence:** The capability preservation claim is demonstrated, but the ablation showing entropy maximization is universally detrimental is less certain without testing on weaker base models.

## Next Checks
1. **β Robustness Test:** Train MMPO on the same dataset with β values spanning multiple orders of magnitude (e.g., 0.001 to 10) and measure both alignment and capability metrics. Confirm that DPO fails or collapses at extreme values while MMPO remains stable.
2. **Base Model Dependency:** Repeat the main experiments (Section 5) using a weaker base model (e.g., a non-SFT baseline model). Verify whether the entropy maximization ablation still holds or if mode collapse occurs without it.
3. **Preference Data Quality Sensitivity:** Introduce varying levels of noise into the preference dataset (e.g., randomly flipping preferences or using ambiguous pairs). Assess whether MMPO's performance degrades more slowly than DPO, validating its implicit preference optimization as more robust to data quality issues.