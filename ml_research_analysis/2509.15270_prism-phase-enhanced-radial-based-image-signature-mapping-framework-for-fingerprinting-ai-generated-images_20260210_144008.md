---
ver: rpa2
title: 'PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting
  AI-generated images'
arxiv_id: '2509.15270'
source_url: https://arxiv.org/abs/2509.15270
tags:
- attribution
- image
- images
- dataset
- fingerprinting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PRISM is a fingerprinting framework that uses radial DFT with both
  magnitude and phase to attribute AI-generated images to their source models. The
  method employs LDA for classification and was evaluated on a newly constructed 36K-image
  dataset (PRISM-36K) and four public benchmarks.
---

# PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images

## Quick Facts
- arXiv ID: 2509.15270
- Source URL: https://arxiv.org/abs/2509.15270
- Reference count: 40
- Primary result: 92.04% accuracy on 36K-image dataset using radial DFT with magnitude and phase for multi-class model attribution

## Executive Summary
PRISM is a fingerprinting framework that uses radial DFT with both magnitude and phase to attribute AI-generated images to their source models. The method employs LDA for classification and was evaluated on a newly constructed 36K-image dataset (PRISM-36K) and four public benchmarks. PRISM achieved 92.04% accuracy on PRISM-36K and competitive results across benchmarks, outperforming existing methods in both multi-class attribution and binary real-vs-fake detection. The results demonstrate that frequency-domain features, particularly when combining magnitude and phase, are effective for model attribution in black-box settings.

## Method Summary
PRISM extracts model-specific signatures from AI-generated images by computing the 2D Discrete Fourier Transform (DFT) per RGB channel, centralizing frequencies, and aggregating them into 64 radial bins. For each bin, it calculates the log-magnitude mean and the cosine-transformed circular mean of phase angles, creating a 384-dimensional feature vector. These features are normalized and classified using Linear Discriminant Analysis (LDA), which maximizes class separability in the reduced subspace. The framework was trained and evaluated on PRISM-36K (36K images from 6 models) and four benchmark datasets, demonstrating robust attribution performance across various conditions.

## Key Results
- 92.04% accuracy on PRISM-36K (36K images, 6 models, 40 prompts)
- Phase information increases accuracy by 3.23% over magnitude alone
- Outperforms existing methods on both multi-class attribution and binary real-vs-fake detection
- Robust to resolution variations but degrades significantly under JPEG/WebP compression

## Why This Works (Mechanism)

### Mechanism 1
Generative models imprint distinct, stable spectral signatures in the frequency domain of their output images. The generation process introduces non-random perturbations in the pixel distribution that manifest as consistent patterns in magnitude and phase components when transformed via DFT. Evidence shows phase information increases accuracy by 3.23%, confirming its importance. The method may fail if models share identical architectures/training sets or if lossy compression destroys phase information.

### Mechanism 2
Radial profile reduction of the DFT creates a resolution-agnostic fingerprint that remains stable across different image sizes. By mapping the 2D DFT spectrum onto 64 concentric annular bins and averaging values, the method collapses the spatial resolution dimension to create a 384-dimensional feature vector regardless of original image size. This transformation promotes robustness to variations in image resolution and compression artifacts. The method may fail if critical fingerprint information is located in specific directional frequencies rather than isotropic bands.

### Mechanism 3
Linear Discriminant Analysis (LDA) can effectively separate high-dimensional spectral features into distinct model clusters. LDA computes a linear projection that maximizes distance between different model classes while minimizing variance within the same model class, acting as both dimensionality reducer and classifier. The method assumes spectral features of different models are linearly separable in the reduced subspace. LDA may fail if spectral distributions of two models significantly overlap, requiring non-linear decision boundaries.

## Foundational Learning

- **Concept: Discrete Fourier Transform (DFT) & Phase Components**
  - Why needed here: The entire PRISM architecture relies on converting images from spatial to frequency domain. Understanding that phase encodes spatial structure while magnitude encodes contrast/intensity is crucial for interpreting the 3.23% accuracy boost.
  - Quick check question: Why does the paper claim phase information is necessary in addition to magnitude for attribution?

- **Concept: Linear Discriminant Analysis (LDA)**
  - Why needed here: LDA is a generative classifier that assumes Gaussian distributions and is used to perform dimensionality reduction explicitly optimized for class separation.
  - Quick check question: How does LDA differ from PCA in its objective function when reducing dimensions?

- **Concept: Generative Architectures (Diffusion vs. GAN)**
  - Why needed here: The paper notes attribution is harder between models with similar architectures. Understanding the underlying mechanics helps explain why their spectral artifacts might overlap.
  - Quick check question: Why might two diffusion-based models leave more similar spectral fingerprints than a GAN and a diffusion model?

## Architecture Onboarding

- **Component map:** Input Loader -> Pre-processor -> rDFT Engine -> Feature Normalizer -> LDA Pipeline
- **Critical path:** The rDFT Engine is the core IP. The specific method of averaging phase using `cos(arg(mean(e^{i*Phi})))` is critical for handling circular phase data without discontinuities.
- **Design tradeoffs:** PRISM uses fixed mathematical features rather than learned embeddings, trading raw accuracy for interpretability and lower data requirements. The choice of 64 radial bins balances frequency resolution against noise robustness and dimensionality. LDA is fast and interpretable but assumes linear separability.
- **Failure signatures:** Lossy compression causes sudden accuracy drops (down to ~76%). High false positive rates between diffusion models suggest the fingerprint captures architecture type rather than specific model ID. Overfitting to prompts causes failure on unseen prompts.
- **First 3 experiments:**
  1. Unit Test rDFT Extraction: Generate white noise and checkerboard images to verify rDFT output matches theoretical expectations.
  2. Baseline PRISM-36K Evaluation: Train LDA on published PRISM-36K split to reproduce 92.04% accuracy.
  3. Compression Robustness Stress Test: Evaluate trained model on PRISM-36K test set after applying varying JPEG compression levels to quantify degradation.

## Open Questions the Paper Calls Out

- Can the PRISM framework be extended to unsupervised clustering settings to enable model attribution in low-supervision scenarios? The authors plan to explore unsupervised clustering for low-supervision settings, but the current methodology relies exclusively on supervised LDA requiring labeled training data.

- How do the extracted spectral features (magnitude and phase) correlate with the internal architectural behaviors of specific generative models? The authors intend to investigate interpretability to understand how features relate to generative model behavior, though no causal link is established.

- Do alternative dimensionality reduction techniques reveal structural relationships in the spectral features that are obscured by Linear Discriminant Analysis? The authors propose experimenting with alternative dimensionality reduction to detect embedded structural relationships that LDA's linear approach may collapse.

## Limitations
- Performance degrades significantly under JPEG/WebP compression (down to 76-77% accuracy)
- Difficulty distinguishing between similar diffusion architectures (e.g., Stable Diffusion vs. SANA)
- Evaluation relies heavily on controlled synthetic datasets with limited real-world noise conditions

## Confidence

- **High confidence:** The fundamental mechanism of using radial DFT features for model attribution is sound and well-supported by results
- **Medium confidence:** Generalizability to real-world conditions with heavy compression, watermarking, or mixed model outputs remains uncertain
- **Low confidence:** The claim of "strong robustness" to compression is overstated given the 12-15 percentage point drop in accuracy

## Next Checks

1. **Compression Stress Test:** Evaluate PRISM on PRISM-36K after applying progressive JPEG compression (quality levels 90â†’10) to quantify exact degradation curve and compare against stated robustness claims.

2. **Architectural Overlap Analysis:** Train separate LDA classifiers to distinguish between diffusion vs. GAN architectures first, then within-architecture model identification, to measure how much performance loss is due to architectural similarity vs. model-specific differences.

3. **Cross-dataset Transfer:** Test a PRISM model trained on PRISM-36K against images from GFW, DeepGuardDB, and SuSy datasets without fine-tuning to assess real-world generalization and identify domain-specific failure patterns.