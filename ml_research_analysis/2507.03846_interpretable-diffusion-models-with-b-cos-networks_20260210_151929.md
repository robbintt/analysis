---
ver: rpa2
title: Interpretable Diffusion Models with B-cos Networks
arxiv_id: '2507.03846'
source_url: https://arxiv.org/abs/2507.03846
tags:
- b-cos
- diffusion
- networks
- image
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work integrates B-cos networks into text-to-image diffusion
  models to provide inherent interpretability by attributing generated image regions
  to individual prompt tokens. The authors replace the U-Net backbone with B-cos layers,
  enabling faithful explanations of how each token influences the output.
---

# Interpretable Diffusion Models with B-cos Networks

## Quick Facts
- arXiv ID: 2507.03846
- Source URL: https://arxiv.org/abs/2507.03846
- Authors: Nicola Bernold; Moritz Vandenhirtz; Alice Bizeul; Julia E. Vogt
- Reference count: 23
- Primary result: Integrating B-cos networks into text-to-image diffusion models provides inherent interpretability by attributing generated image regions to individual prompt tokens, achieving FID of 21.18 for x₀ prediction models.

## Executive Summary
This work integrates B-cos networks into text-to-image diffusion models to provide inherent interpretability by attributing generated image regions to individual prompt tokens. The authors replace the U-Net backbone with B-cos layers, enabling faithful explanations of how each token influences the output. They train on a subset of LAION-2B focusing on five objects and evaluate three configurations: predicting x₀, predicting noise, and using frozen CLIP with x₀ prediction. Results show the x₀ models generate high-quality images with FID of 21.18, while CLIP-based noise prediction yields better FID (21.46) but less interpretable reconstructions. Token relevance scores demonstrate that semantically meaningful words receive higher attribution than function words, and the method can detect when prompts are not fully reflected in outputs.

## Method Summary
The authors integrate B-cos networks into text-to-image diffusion models by replacing the U-Net backbone with B-cos layers. They train on a subset of LAION-2B focusing on five objects (banana, cat, goat, flamingo, penguin) at 64×64 resolution using pixel-space diffusion without a VAE. Three model variants are evaluated: B-cos x₀ (predicts x₀ directly), B-cos ε (predicts noise), and B-cos Clip ε/x₀ (uses frozen CLIP encoder). DDIM sampling with σ_t=0 ensures deterministic generation. The B-cos transformation enforces input-weight alignment, enabling the entire network to be summarized as a single dynamic linear transformation that can be visualized directly. Token relevance scores are computed via a single backward pass, demonstrating that semantic tokens receive higher attribution than function words.

## Key Results
- B-cos x₀ models achieve FID of 21.18 with faithful reconstructions (MSE 0.0045 vs normalized output)
- CLIP-based noise prediction yields better FID (21.46) but less interpretable reconstructions
- Semantic tokens (e.g., "banana", "flamingo") receive significantly higher relevance scores than function words
- The method can detect when prompts are not fully reflected in outputs through low token relevance scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The B-cos transformation enforces input-weight alignment, enabling the entire network to be summarized as a single dynamic linear transformation that faithfully represents model behavior.
- Mechanism: The B-cos transform computes `f_B-cos(x; w) = |cos(x,w)|^(B-1) × ŵ^T × x`. When B > 1 and output magnitude is non-negligible, the cosine term forces weights to align with relevant input dimensions. Because there's no bias term, the complete computation is captured by W(x), which can be visualized directly.
- Core assumption: The elimination of bias terms and enforcement of weight-input alignment yields explanations that correspond to human-interpretable features rather than local artifacts.
- Evidence anchors: Section 2 states B-cos transformations are summarized by W(x) for B > 1, and Section 3.4 shows relevance scores derived from W(x)×xi aggregated across pixels.

### Mechanism 2
- Claim: Replacing the linear value projection in cross-attention with a B-cos layer preserves alignment properties while enabling token-level attribution to image regions.
- Mechanism: Cross-attention computes `A(X,Y) × YV`. By making V a B-cos transformation, the value projection becomes a dynamic linear map W(Y) that can be decomposed per-token. The attention scores A(X,Y) modulate but don't break the B-cos summary since X is treated as a "non-additive constant."
- Core assumption: Treating the noisy image X as a constant for explanation purposes doesn't invalidate token-level attributions for the conditioning Y.
- Evidence anchors: Section 3.2 describes replacing "linear value projection YV with a B-cos layer" and Section 3.3 uses subword embeddings directly so "only tokens with a semantic meaning can contribute to the output."

### Mechanism 3
- Claim: Normalized reconstruction R_normalized provides a fidelity check for whether the B-cos summary captures the relevant computations despite architectural biases.
- Mechanism: The 6-channel encoding (r,g,b,1-r,1-g,1-b) creates redundant channels that should sum to 1. Even if W(x)×x ≠ original output due to biases, computing `R_rgb/(R_rgb + R_{1-rgb})` can recover lost information. Small MSE between normalized reconstruction and output indicates faithful explanation.
- Core assumption: The difference between raw reconstruction and output is primarily due to additive biases that affect both (r,g,b) and (1-r,1-g,1-b) similarly, allowing normalization to compensate.
- Evidence anchors: Section 3.4 states "the difference between the normalized reconstructions and the model output being small implies that the model summary captures the relevant model computations," and Section 4.2 reports MSE of 0.0045.

## Foundational Learning

- Concept: **Diffusion model forward/reverse process**
  - Why needed here: The paper modifies the U-Net backbone and requires understanding how biases enter through the sampling distribution p_θ(x_{t-1}|x_t). DDIM with σ_t=0 is used specifically to avoid introducing noise-based biases.
  - Quick check question: Given a noisy sample x_t at timestep t, what quantity does the network predict, and how does this choice affect whether biases can be captured in the B-cos summary?

- Concept: **Dynamic linear approximation of neural networks**
  - Why needed here: B-cos networks are motivated by the observation that f(x) = W(x)x + b(x) locally approximates any network. The goal is to eliminate b(x) so W(x) alone is a complete summary.
  - Quick check question: For a standard linear layer y = Wx + b, what information is lost if you try to explain the output using only the weight matrix W?

- Concept: **Cross-attention for conditional generation**
  - Why needed here: The cross-attention mechanism is where text conditioning enters the U-Net. Understanding the Q/K/V decomposition is necessary to see why only the V projection needs B-cosification for token attribution.
  - Quick check question: In cross-attention CrossAtt(X, Y; Q, K, V), which input (X or Y) provides the conditioning signal, and which tensor determines how much each token contributes to each spatial location?

## Architecture Onboarding

- Component map: Input encoding (6-channel) -> CLIP text encoder (subword embeddings) -> U-Net backbone (B-cos layers) -> Output (x_0 prediction)
- Critical path: 1) Encode prompt → extract subword embeddings → mask padding tokens; 2) Encode image → 6-channel representation; 3) DDIM sampling with σ_t=0 → no noise bias introduced; 4) At each timestep: U-Net forward pass produces dynamic linear summary W(x); 5) Compute relevance scores S_i(x) via single backward pass (Eq. 8)
- Design tradeoffs: x₀ vs. ε prediction (x₀ enables valid normalized reconstructions with MSE 0.0045 but yields worse FID 21.18 vs. 21.46 for Clip ε); pixel-space vs. latent-space (paper operates in pixel space at 64×64 to avoid needing a B-cos VAE, limiting resolution); subword vs. contextual embeddings (subword embeddings provide cleaner attributions but lose inter-token context)
- Failure signatures: Noise-prediction models (normalization impossible; reconstruction resembles noise); high-attribution but wrong generation (token "flamingo" may receive high relevance but model associates wrong color); EOS/padding dominance (when using full CLIP encoder, EOS token captures sequence-level information and dominates attribution)
- First 3 experiments: 1) Reconstruction fidelity check: Generate sample, compute W(x) summary, compare R_normalized to original output (expect MSE < 0.01 for x₀ models); 2) Token relevance sanity check: Generate images with prompts containing semantic tokens and function words, verify semantic tokens receive >10% average relevance vs. <5% for function words; 3) Failure detection test: Generate images with prompts containing concepts likely to fail (e.g., "shark" in penguin-focused model), verify low relevance scores (<2%) for missing concepts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the trade-off between inherent interpretability and generative quality be reduced while preserving faithful explanations?
- Basis in paper: [explicit] The conclusion states: "While our approach offers inherent interpretability, it currently leads to reduced generative performance. Developing methods to overcome this trade-off is a promising direction for future research."
- Why unresolved: The B-cos x₀ model achieves FID 21.18 compared to much lower FID values in standard diffusion models, indicating a substantial quality gap that current architectural constraints impose.
- What evidence would resolve it: Demonstrating a B-cos diffusion model matching baseline Stable Diffusion FID scores (e.g., <10) while maintaining faithful token-to-pixel attribution and high reconstruction fidelity.

### Open Question 2
- Question: Does the B-cos diffusion approach scale to the full diversity of concepts and prompts in large-scale datasets like LAION-2B?
- Basis in paper: [inferred] The experiments train only on five selected objects (banana, cat, goat, flamingo, penguin) with 20,000 image-caption pairs each. The paper does not evaluate whether interpretability properties generalize beyond this constrained setting.
- Why unresolved: Training on 100,000 images covering five objects leaves open whether the faithful attributions and semantic relevance patterns persist when the model must learn thousands of concepts simultaneously.
- What evidence would resolve it: Training B-cos diffusion on the full LAION-2B dataset and evaluating whether semantic tokens still receive higher relevance than function words across diverse prompt types.

### Open Question 3
- Question: Can latent-space diffusion with a B-cos VAE preserve interpretability while enabling higher-resolution generation?
- Basis in paper: [inferred] Section 3.3 states: "Instead of separately training a B-cos VAE, we omit the VAE and directly perform pixel-space diffusion on a lower 64×64 resolution." This architectural simplification avoids the challenge of making the VAE interpretable.
- Why unresolved: Operating in pixel space at low resolution limits practical applicability. Whether B-cos constraints can be integrated into the encoder-decoder structure of a VAE without losing reconstruction quality or interpretability remains unexplored.
- What evidence would resolve it: A B-cos VAE that produces latent representations enabling both high-fidelity 512×512 or higher image generation and faithful explanations of how latents relate to prompts.

## Limitations
- The method incurs a substantial trade-off between interpretability and generative quality, with B-cos x₀ models achieving FID 21.18 compared to much lower FID values in standard diffusion models
- The approach operates at 64×64 resolution in pixel space rather than latent space, limiting practical applicability for high-resolution image generation
- The experiments evaluate only five object categories and single-object prompts, leaving open whether interpretability properties generalize to complex scenes and diverse concepts

## Confidence

- **High confidence**: The B-cos transformation's ability to enforce input-weight alignment and enable linear approximation is well-established from the foundational Böehle et al. 2024 work
- **Medium confidence**: The cross-attention modifications are theoretically sound but practical implementation details are underspecified
- **Medium confidence**: The method demonstrably produces token relevance scores that correlate with semantic meaning, but the relationship between high relevance and "correct" understanding remains unproven
- **Low confidence**: The paper makes no claims about performance beyond the tested five-object domain, and evaluation doesn't address whether the approach would maintain interpretability when scaled up

## Next Checks

1. **Cross-prompt generalization test**: Generate images for prompts that combine multiple objects from the training set and analyze whether token relevance scores correctly attribute each concept to appropriate image regions, validating compositional prompt handling.

2. **Out-of-distribution concept detection**: Create prompts containing objects not in the training set and verify that the method produces low relevance scores for these tokens while maintaining reasonable image quality for known concepts, testing the claim that low relevance indicates prompt misunderstanding.

3. **Resolution scaling experiment**: Train the B-cos x₀ model at 128×128 resolution while maintaining the same 6-channel encoding and architecture modifications, comparing FID scores and reconstruction MSE to the baseline to determine whether the interpretability-quality trade-off persists at higher resolutions.