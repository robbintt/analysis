---
ver: rpa2
title: 'From Answers to Questions: EQGBench for Evaluating LLMs'' Educational Question
  Generation'
arxiv_id: '2508.10005'
source_url: https://arxiv.org/abs/2508.10005
tags:
- question
- evaluation
- generation
- knowledge
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EQGBench, a comprehensive evaluation benchmark
  for assessing large language models' (LLMs) capabilities in educational question
  generation. The benchmark includes a high-quality dataset of 900 structured evaluation
  samples across mathematics, physics, and chemistry, designed to simulate real-world
  educational scenarios.
---

# From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation

## Quick Facts
- arXiv ID: 2508.10005
- Source URL: https://arxiv.org/abs/2508.10005
- Reference count: 8
- 46 mainstream LLMs evaluated on educational question generation capabilities across STEM subjects

## Executive Summary
This paper introduces EQGBench, a comprehensive evaluation benchmark designed to assess large language models' capabilities in educational question generation (EQG). The benchmark addresses a critical gap in existing evaluations by focusing on question generation from answers rather than traditional question answering, providing a more comprehensive test of pedagogical capabilities. Through systematic evaluation of 46 mainstream LLMs across mathematics, physics, and chemistry domains, the authors reveal significant limitations in current models' ability to generate questions with deep pedagogical intent, particularly in competence-oriented guidance. The benchmark employs a five-dimensional evaluation framework and demonstrates high reliability through both automated and human expert validation.

## Method Summary
The authors constructed EQGBench by first identifying and categorizing question types from middle school textbooks across three subjects, then developing a knowledge point alignment system based on curriculum standards. They generated 900 evaluation samples with associated solutions, covering diverse difficulty levels and subject-specific requirements. The evaluation framework comprises five dimensions: knowledge point alignment, question type alignment, question item quality, solution explanation quality, and competence-oriented guidance. A DeepSeek-R1-based automated evaluator was developed to assess these dimensions using instruction tuning on expert-annotated data. The benchmark was validated through systematic evaluation of 46 mainstream LLMs, including both open-source and closed-source models, using a consistent evaluation pipeline.

## Key Results
- Top-performing models (GPT-4o, Gemini-1.5-Pro, Claude-3.5-Sonnet) scored below 1.5 out of 2.0 in mathematics for competence-oriented guidance
- Question type alignment performance decreased by 0.4-0.5 points from open-ended to multiple-choice question types
- The automated evaluation pipeline demonstrated high reliability with Cohen's kappa values exceeding 0.8 when validated against expert teacher assessments

## Why This Works (Mechanism)
None

## Foundational Learning
- Educational Question Generation (EQG): The process of creating assessment questions from provided answers or solutions, requiring understanding of pedagogical intent and curriculum alignment
  - Why needed: Traditional question answering evaluations don't capture a model's ability to design educational assessments
  - Quick check: Can the model generate a physics problem about force given the solution formula F=ma?

- Knowledge Point Alignment: Ensuring generated questions target specific curriculum concepts and learning objectives
  - Why needed: Educational questions must align with learning standards and curriculum requirements
  - Quick check: Does the generated question actually test the intended mathematical concept?

- Question Type Alignment: Matching the format and structure of generated questions to specified question types (multiple choice, open-ended, etc.)
  - Why needed: Different question types assess different cognitive skills and require distinct formulation approaches
  - Quick check: Does the generated item follow the structural conventions of multiple-choice questions?

- Competence-Oriented Guidance: Embedding pedagogical intent that guides students toward deeper understanding beyond the immediate answer
  - Why needed: Quality educational questions should promote critical thinking and conceptual understanding
  - Quick check: Does the question require application of concepts rather than simple recall?

- Automated Evaluation Pipeline: Using large language models to assess generated questions across multiple quality dimensions
  - Why needed: Manual evaluation is time-consuming and subjective; automation enables systematic benchmarking
  - Quick check: Does the automated evaluator's scoring correlate with expert teacher assessments?

## Architecture Onboarding

**Component Map:** Dataset (900 samples) -> Question Generation Models (46 LLMs) -> Automated Evaluator (DeepSeek-R1) -> Scoring (5 dimensions) -> Results

**Critical Path:** Input answers → Model generates question → Automated evaluator scores across 5 dimensions → Aggregated scores → Performance ranking

**Design Tradeoffs:** 
- Trade-off between evaluation comprehensiveness and computational cost: The five-dimensional framework provides detailed assessment but requires multiple evaluation passes
- Trade-off between automation and human judgment: Automated evaluation enables scalability but may miss nuanced pedagogical quality aspects
- Trade-off between subject coverage and depth: Three STEM subjects allow deep analysis but limit generalizability to other domains

**Failure Signatures:**
- Low scores across all dimensions indicate fundamental limitations in question generation capabilities
- High scores in knowledge alignment but low scores in competence-oriented guidance suggest models can identify correct topics but struggle with pedagogical depth
- Significant performance drops between question types indicate format-specific generation challenges

**3 First Experiments:**
1. Generate a simple multiple-choice question from a basic arithmetic solution to test fundamental capabilities
2. Create an open-ended physics question requiring conceptual understanding rather than formula application
3. Develop a chemistry question that tests both procedural knowledge and safety awareness

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What specific training methodologies (e.g., reinforcement learning from human feedback, retrieval-augmented generation) can effectively improve LLM performance in the "competence-oriented guidance" dimension, particularly for abstract subjects like mathematics?
- Basis in paper: The authors state that "competence-oriented guidance represents a significant weakness across the board" and that nearly no model scored above 0.3 in mathematics for this dimension.
- Why unresolved: The paper evaluates existing models but does not propose or test solutions to bridge this specific performance gap.
- What evidence would resolve it: A study showing a model fine-tuned or prompted specifically for contextual relevance achieving a score greater than 1.0 in the competence-oriented guidance dimension on EQGBench.

### Open Question 2
- Question: Does the high correlation observed between the DeepSeek-R1 automated evaluator and human experts remain consistent across the full dataset of 900 samples, particularly for the subjective "competence-oriented guidance" metric?
- Basis in paper: The human study (Section 5.4) validated the evaluation framework using only 100 randomly selected samples from the mathematics set, leaving the consistency on the remaining 800 samples and other subjects unverified.
- Why unresolved: Validation on a subset (approx. 11% of data) assumes uniform performance which may not hold for diverse subjects like chemistry or physics.
- What evidence would resolve it: A correlation analysis between human and automated scores on a statistically significant sample from the physics and chemistry subsets.

### Open Question 3
- Question: To what extent do the findings regarding "competence-oriented guidance" transfer to non-Chinese educational contexts or languages with different pedagogical structures?
- Basis in paper: The benchmark is explicitly designed for "Chinese EQG" based on the local middle school curriculum; the authors note the need to "simulate realistic educational scenarios" which vary culturally.
- Why unresolved: The dataset and evaluation criteria are strictly aligned with Chinese curriculum standards, limiting the generalizability of the "educational value" conclusions to global educational settings.
- What evidence would resolve it: Results from a translated version of EQGBench or a similar benchmark applied to English-language models showing comparable weakness in contextual question generation.

## Limitations
- The benchmark focuses exclusively on three STEM subjects, limiting generalizability to other educational domains
- The automated evaluation pipeline, while reliable, may miss nuanced pedagogical quality aspects requiring human judgment
- The findings are based on Chinese curriculum standards, raising questions about cultural generalizability

## Confidence
- High: Benchmark construction quality and evaluation framework validity (systematic expert involvement and cross-validation)
- Medium: Comparative LLM performance results (subject to variations in prompt engineering and evaluation instances)
- Low: Claims about LLMs' inability to generate questions with deep pedagogical intent (may reflect current prompt engineering limitations rather than fundamental model capabilities)

## Next Checks
1. Conduct a larger-scale human evaluation study with diverse teacher populations to validate the automated pipeline's reliability across different educational contexts and cultural settings
2. Test the benchmark's generalizability by applying it to non-STEM subjects and different educational levels (e.g., primary education, humanities)
3. Perform ablation studies on the five-dimensional evaluation framework to determine which dimensions contribute most significantly to overall question quality and pedagogical effectiveness