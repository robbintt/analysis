---
ver: rpa2
title: Exploring Test Time Adaptation for Subcortical Segmentation of the Fetal Brain
  in 3D Ultrasound
arxiv_id: '2502.08774'
source_url: https://arxiv.org/abs/2502.08774
tags:
- domain
- adaptation
- fetal
- performance
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of test-time adaptation (TTA) methods
  to improve deep learning segmentation of subcortical brain structures in fetal 3D
  ultrasound. The authors demonstrate that standard TTA approaches can adapt pretrained
  models to handle domain shifts arising from differences in scanner vendors, gestational
  age, and simulated image augmentations.
---

# Exploring Test Time Adaptation for Subcortical Segmentation of the Fetal Brain in 3D Ultrasound

## Quick Facts
- **arXiv ID**: 2502.08774
- **Source URL**: https://arxiv.org/abs/2502.08774
- **Reference count**: 20
- **Primary result**: EntropyKL method achieves up to 0.6 Dice improvement on simulated augmentations and 0.1 across gestational weeks compared to baseline TTA methods.

## Executive Summary
This paper investigates test-time adaptation (TTA) for subcortical brain structure segmentation in fetal 3D ultrasound. The authors demonstrate that standard TTA approaches can adapt pretrained models to handle domain shifts from scanner differences, gestational age variations, and simulated image augmentations. They propose EntropyKL, a novel TTA method that incorporates a normative atlas prior on expected tissue volume into the adaptation objective. EntropyKL outperforms baseline TTA methods and histogram matching, particularly on unseen scanner datasets where anatomical priors yield more accurate segmentations.

## Method Summary
The method adapts a pre-trained 3D UNet for subcortical segmentation by minimizing prediction entropy during inference (TENT) and incorporating an anatomical prior via KL divergence (EntropyKL). The approach adapts only Batch Normalization parameters using a single optimization step per volume, with a combined loss of entropy minimization plus KL divergence between predicted class ratios and atlas-derived priors. The method was evaluated across three domain shift scenarios: simulated augmentations, gestational age variations, and unseen scanner datasets.

## Key Results
- EntropyKL achieved up to 0.6 Dice score improvement on simulated augmentations compared to baseline TTA methods
- On unseen scanner datasets (Canon, GE), EntropyKL outperformed baselines with Dice improvements of approximately 0.1
- Single-volume adaptation consistently outperformed batch adaptation across all tested domain shifts
- The KL divergence prior proved particularly effective when adapting to completely unseen scanner domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Minimizing prediction entropy during inference can recover segmentation performance under domain shift without labeled target data.
- **Mechanism**: TENT observes that high-entropy predictions correlate with incorrect outputs. By minimizing Shannon entropy over softmax outputs and restricting updates to batch normalization layers, the model adapts its feature statistics to the target distribution while staying anchored to learned source weights.
- **Core assumption**: Domain shift manifests primarily as feature distribution misalignment rather than fundamental structural changes in optimal decision boundaries.
- **Evidence anchors**: Standard TTA approaches can adapt pretrained models to handle domain shifts; TENT only optimizes batch normalization layers as these are linear and low-dimensional.
- **Break condition**: When source-target domain gap is too large, entropy minimization alone may collapse to trivial solutions or incorrect local minima.

### Mechanism 2
- **Claim**: Incorporating an anatomical prior into the adaptation objective prevents collapse and improves segmentation accuracy.
- **Mechanism**: EntropyKL adds a KL divergence term between predicted class ratios and atlas-derived expected ratios. This soft constraint penalizes parameter updates that would produce biologically implausible segmentations while still allowing the model to learn from the specific image.
- **Core assumption**: Target images represent healthy fetal brains whose subcortical structure volumes approximately match the normative atlas distribution.
- **Evidence anchors**: EntropyKL outperforms baseline TTA methods; the prior gives the model some idea of how much of each class should be present and penalizes updates that drastically change the ratio of classes.
- **Break condition**: When segmenting pathological brains with abnormal structure volumes, the prior may bias predictions toward healthy anatomy, degrading accuracy.

### Mechanism 3
- **Claim**: Single-volume adaptation outperforms batch adaptation for ultrasound due to per-scan acquisition variability.
- **Mechanism**: Each ultrasound scan effectively constitutes its own imaging domain due to operator-dependent acquisition settings. Adapting per-volume allows the model to specialize to that specific domain rather than averaging across heterogeneous batch statistics.
- **Core assumption**: Batch normalization statistics computed from a single volume are sufficiently stable for meaningful adaptation.
- **Evidence anchors**: TTA performs better when adapting to a single volume than to a batch; the nature of each US image effectively being its own imaging domain.
- **Break condition**: When volumes have severe artifacts or insufficient spatial coverage, single-volume BN statistics may be unreliable, causing unstable updates.

## Foundational Learning

- **Concept: Shannon Entropy for Uncertainty Quantification**
  - Why needed here: Understanding why entropy correlates with prediction error explains the theoretical basis for TENT.
  - Quick check question: For a binary segmentation, what entropy value would indicate maximum uncertainty?

- **Concept: Batch Normalization Statistics as Domain Signatures**
  - Why needed here: TTA methods like TENT exploit the observation that BN statistics encode domain-specific information.
  - Quick check question: Why might source-domain BN statistics be suboptimal for target-domain data?

- **Concept: KL Divergence Between Discrete Distributions**
  - Why needed here: EntropyKL uses KL divergence to penalize deviations from expected class ratios.
  - Quick check question: KL divergence is asymmetric—what happens if you swap p and q in this application?

## Architecture Onboarding

- **Component map**: Pretrained 3D UNet (source model) → Batch Norm layers (adapted) → Softmax predictions
                                              ↑
                                         EntropyKL Loss
                                              ↑
                                    Atlas prior (class ratios τ)

- **Critical path**: Source model weights (frozen) → BN parameters (trainable) → Entropy + KL loss → Adam optimizer → Updated BN → Final prediction

- **Design tradeoffs**:
  - λ (KL weight): Higher values enforce atlas conformity but reduce adaptability to genuine anatomical variation. Paper uses λ=1 but shows sensitivity.
  - Learning rate: TENT/EntropyKL use 10⁻³; LayerInspect uses 10⁻⁴. Higher LR enables faster adaptation but risks instability.
  - Backward passes: Unseen datasets required 25 passes vs. 1 for simulated shifts—indicates adaptation difficulty scales with domain gap severity.

- **Failure signatures**:
  - Model collapse to single-class prediction (entropy → 0, all pixels same label)
  - Excessive atlas enforcement on pathological cases (structures appear "normal" despite abnormality)
  - Instability with small batch sizes if using batch-dependent normalization

- **First 3 experiments**:
  1. **Sanity check**: Run source model on held-out source data with/without TENT. TENT should not degrade in-distribution performance.
  2. **Controlled shift**: Apply single augmentation type (e.g., gamma correction only) and compare TENT vs. EntropyKL vs. histogram matching. Isolate which method handles intensity vs. spatial shifts better.
  3. **Atlas prior ablation**: Vary λ ∈ {0, 0.5, 1, 2, 5} on simulated data with known ground truth to find optimal weighting before testing on unseen scanner data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the EntropyKL method perform when segmenting fetal brains with pathological abnormalities, given its reliance on a normative atlas prior?
- Basis in paper: The atlas corresponds to healthy fetal growth, it may bias the model incorrectly in the presence of fetal abnormality. Future work will explore its use in this case.
- Why unresolved: The KL divergence term penalizes deviations from the "healthy" atlas tissue ratios. In pathological cases, the model might incorrectly regress predictions toward the healthy prior, but this effect has not been quantified.
- What evidence would resolve it: Evaluation of EntropyKL on a dataset of fetuses with confirmed structural anomalies to measure whether the prior constrains the model's ability to segment abnormal volumes.

### Open Question 2
- Question: Can the qualitative improvements observed on unseen scanner data (Canon and GE) be quantitatively verified?
- Basis in paper: Manual annotations are not available so only qualitative analysis is completed.
- Why unresolved: While visual inspection suggests EntropyKL handles domain shift better than baselines, the lack of ground truth masks for these specific scanners means the actual Dice score improvements and clinical utility remain estimated.
- What evidence would resolve it: Manual segmentation of the unseen Canon and GE datasets by experts to calculate statistical significance of the segmentation improvements.

### Open Question 3
- Question: Why does single-volume adaptation outperform batch adaptation in this specific ultrasound context?
- Basis in paper: TTA performs better when adapting to a single volume than to a batch, motivating the use of single volume adaptation going forward.
- Why unresolved: This contradicts the standard assumption of TENT, which typically relies on batch normalization statistics computed over batches. The high variability between ultrasound scans may corrupt batch statistics, but the specific mechanism causing single-volume superiority is not analyzed.
- What evidence would resolve it: An ablation study analyzing the stability of Batch Normalization statistics when computed over heterogeneous ultrasound batches versus single samples.

## Limitations
- The anatomical prior may introduce harmful bias when segmenting pathological fetal brains with abnormal structure volumes
- Quantitative evaluation on unseen scanner datasets is limited to qualitative analysis due to lack of manual annotations
- The choice of adapting only BatchNorm parameters is assumed optimal but not empirically validated for this specific domain

## Confidence

**High**: The EntropyKL method improves segmentation accuracy over standard TTA methods on the presented datasets (unseen scanners, gestational age variations, simulated augmentations).

**Medium**: The effectiveness of incorporating anatomical priors is demonstrated within the scope of healthy fetal brains using a specific normative atlas; generalization to pathological cases is untested.

**Low**: The choice of adaptation target (BatchNorm parameters only) is assumed optimal based on TENT's original design, but the necessity of this restriction for fetal ultrasound specifically is not empirically validated.

## Next Checks

1. **Generalization to Pathological Cases**: Test EntropyKL on fetal brain datasets with known structural abnormalities to assess if the atlas prior introduces harmful bias.

2. **Ablation of Adaptation Target**: Compare performance when adapting different subsets of model parameters (e.g., BatchNorm only vs. BatchNorm + convolutional weights) to validate the design choice for this domain.

3. **Hyperparameter Sensitivity Analysis**: Systematically vary λ (KL weight) and learning rate on unseen scanner data to confirm the reported settings are near-optimal and not overfit to the tested range.