---
ver: rpa2
title: 'SAGA: Source Attribution of Generative AI Videos'
arxiv_id: '2511.12834'
source_url: https://arxiv.org/abs/2511.12834
tags:
- attribution
- video
- generators
- source
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SAGA, the first large-scale framework for
  source attribution of AI-generated videos. Unlike binary real/fake detection, SAGA
  identifies the specific generative model used, providing attribution across five
  granular levels: authenticity, generation task (T2V/I2V), model version, development
  team, and precise generator.'
---

# SAGA: Source Attribution of Generative AI Videos

## Quick Facts
- **arXiv ID:** 2511.12834
- **Source URL:** https://arxiv.org/abs/2511.12834
- **Reference count:** 40
- **Primary result:** First large-scale framework for fine-grained source attribution of AI-generated videos, identifying specific generators with up to 97.41% accuracy using only 0.5% labeled data.

## Executive Summary
SAGA introduces the first large-scale framework for source attribution of AI-generated videos, going beyond binary real/fake detection to identify specific generative models across five granular levels. The method uses a novel video transformer architecture that leverages spatio-temporal artifacts unique to each generator, combined with a data-efficient pretrain-and-attribute strategy that requires only 0.5% labeled data per class. SAGA also introduces Temporal Attention Signatures (T-Sigs), a novel interpretability method that visualizes learned temporal differences, providing the first explanation for why different video generators are distinguishable.

## Method Summary
SAGA employs a two-stage training approach: first pretraining on abundant binary data (Real vs. Fake) using a frozen vision foundation model and transformer architecture, then fine-tuning for attribution using contrastive learning with hard negative mining. The model processes videos through a hierarchical architecture that isolates spatial and temporal artifacts - a frozen vision encoder extracts high-level spatial features per frame, which are then processed by a temporal transformer to capture inter-frame dependencies. The method achieves fine-grained multi-class attribution while requiring minimal labeled data through knowledge transfer from the binary pretraining task.

## Key Results
- Achieves up to 97.41% accuracy on fine-grained generator identification (GEN-L) task
- Requires only 0.5% labeled data per class for attribution, matching performance of 100% supervised training
- Introduces Temporal Attention Signatures (T-Sigs) as the first interpretability method for video generator attribution
- Demonstrates robust performance across cross-domain scenarios on two public datasets (DeMamba and DVF)

## Why This Works (Mechanism)

### Mechanism 1
The hierarchical architecture isolates spatial and temporal artifacts, allowing the model to learn generator-specific "fingerprints" from the temporal domain. The system uses a frozen vision foundation model to extract high-level spatial features per frame, then applies a trainable temporal encoder specifically to the sequence of these frame embeddings to learn inter-frame dependencies. Core assumption: generators leave distinct, systematic inconsistencies in motion dynamics that are independent of visual content. Evidence anchors: captures spatio-temporal artifacts, Spatial Encoder refines token embeddings while Temporal Encoder builds representations of temporal dynamics. Break condition: if video compression codecs destroy subtle temporal artifacts before ingestion.

### Mechanism 2
Fine-grained multi-class attribution is achievable with minimal labeled data (0.5%) by transferring knowledge from binary pretraining via Hard Negative Mining. The model is first pretrained on abundant binary data to learn a general "synthetic" manifold, then fine-tuned for attribution using contrastive loss where hard negatives are explicitly selected to push apart overlapping generator clusters. Core assumption: binary pretraining creates a rich embedding space where generators are already roughly clustered by "fakeness," requiring only minor adjustments to separate specific sources. Evidence anchors: data-efficient pretrain-and-attribute strategy using only 0.5% labeled data, Hard Negative Mining forces separation even when clusters overlap. Break condition: if binary pretraining collapses all synthetic videos into a single tight cluster without preserving inter-generator variance.

### Mechanism 3
Temporal Attention Signatures (T-Sigs) provide a stable, visual fingerprint for generators by exposing consistent attention patterns across frames. By aggregating attention scores from the penultimate transformer block over many videos, the model reveals how a generator typically relates frames, with visually distinct signatures for different generators. Core assumption: transformer's inductive bias aligns with specific temporal failure modes of generators, resulting in deterministic attention heatmaps. Evidence anchors: T-Sigs reveal how SAGA uses temporal cues, videos from the same source yield consistent signatures. Break condition: if the model overfits to content rather than style, causing attention signatures to vary wildly between different video content.

## Foundational Learning

- **Concept: Triplet Loss & Hard Negative Mining**
  - Why needed here: Standard Cross-Entropy loss fails to separate visually similar generators in low-data regimes. Understanding how contrastive learning explicitly enforces geometric margins between classes in the embedding space is required.
  - Quick check question: Can you explain why selecting the "hardest" negative (closest to the anchor) is necessary when generator distributions overlap significantly?

- **Concept: Vision Transformers (ViT) for Video (ViViT)**
  - Why needed here: The architecture relies on factorized attention (spatial vs. temporal). Understanding how positional encodings and multi-head self-attention operate across a sequence of image tokens is required to debug T-Sig visualizations.
  - Quick check question: How does the model ensure that the spatial encoder processes frames independently before the temporal encoder aggregates them?

- **Concept: Transfer Learning from Foundation Models**
  - Why needed here: The method relies on a frozen "domain-agnostic" encoder to bootstrap the learning process.
  - Quick check question: Why would using a frozen foundation model improve generalization to "in-the-wild" videos compared to training a CNN from scratch on specific generator data?

## Architecture Onboarding

- **Component map:** Input Video → Frames → Frozen Vision Encoder → Frame Embeddings → Spatial Encoder (Transformer per frame) → Pooled features → Temporal Encoder (Transformer across frames) → Linear Classifier + Contrastive HNM Loss

- **Critical path:** The flow from Frozen Backbone → Temporal Encoder is where the magic happens. Do not re-train the backbone; focus debugging on the Temporal Encoder's convergence.

- **Design tradeoffs:**
  - **HNM vs. Semi-HNM:** The paper explicitly chooses Hard Negative Mining. While harder to optimize (risk of collapse), it is necessary to pry apart similar generators (Table 7 shows ~30% boost over semi-hard).
  - **Frozen vs. Fine-tuned Encoder:** Freezing the vision encoder sacrifices some fine-grained spatial accuracy for massive gains in data efficiency and domain generalization.

- **Failure signatures:**
  - **Mode Collapse:** If HNM margin is too large or learning rate too high, the loss diverges.
  - **Overfitting to Content:** If t-SNE plots cluster by video content rather than generator, the spatial encoder is leaking too much specific detail or temporal attention is weak.
  - **Zero-Performance on Rare Classes:** 1-stage training yields 0% on specific generators; 2-stage pretraining is strictly required for tail classes.

- **First 3 experiments:**
  1. **Ablation on Loss Functions:** Train the GEN-L task with (a) CE-Loss only, (b) Semi-HNM, and (c) HNM. Verify that only HNM resolves the "0% accuracy" issue on difficult generators like OpenAI/Sora.
  2. **T-Sig Consistency Check:** Extract attention maps for 50 videos from Generator A and 50 from Generator B. Compute the intra-class variance of the T-Sigs. They should be low (consistent) within the class and high between classes.
  3. **Data Efficiency Scaling:** Run Stage-2 attribution with 0.1%, 0.5%, 1%, and 10% data to reproduce the curve showing that 0.5% matches 100% supervised performance.

## Open Questions the Paper Calls Out

### Open Question 1
To what extent do standard video compression codecs (e.g., H.264, HEVC) degrade SAGA's fine-grained attribution capabilities? The authors identify "Video Compression" as one of the "three key barriers" for video attribution, noting that video codecs introduce complex spatio-temporal artifacts that can "obscure or destroy subtle generator-specific traces." While SAGA achieves state-of-the-art results on the DeMamba and DVF datasets, the experimental analysis focuses on cross-domain generalization rather than explicitly isolating the model's robustness against varying levels of compression artifacts.

### Open Question 2
How resilient is SAGA against targeted adversarial perturbations designed to spoof source attribution or transfer labels between generators? The introduction lists "developing robust adversarial countermeasures" as a primary motivation for source attribution, yet the experiments only cover "cross-domain" natural distribution shifts. The paper demonstrates robustness against domain shifts but does not evaluate vulnerability to gradient-based attacks or perturbations specifically optimized to manipulate the model's latent space or T-Sigs.

### Open Question 3
Can the T-Sigs (Temporal Attention Signatures) be utilized for zero-shot attribution of unseen generators without requiring the Stage-2 adaptation step? The paper notes that t-SNE plots show "unseen generators... appear as separable clusters" and that they produce "unique and discernible T-Sigs," suggesting the model captures fundamental temporal characteristics. However, the quantitative results rely on the 0.5% labeled data adaptation, and it's unclear if the "universal" temporal features learned in Stage-1 are sufficient to attribute videos to new generator families purely by matching their T-Sigs to a known database.

## Limitations

- The method's reliance on temporal artifacts assumes generators produce systematic motion inconsistencies, which may not hold across all video domains or future generators.
- The 0.5% data efficiency claim depends critically on the quality of hard negative mining, which may not generalize to datasets with different class distributions.
- T-Sigs interpretation requires manual verification to ensure attention patterns reflect generator-specific temporal dynamics rather than dataset biases.

## Confidence

- **High Confidence:** The binary classification task (BIN-L) and basic multi-class attribution performance are well-supported by extensive experiments across two datasets.
- **Medium Confidence:** The data efficiency claims (0.5% labeled data) are supported by ablation studies but require independent replication.
- **Medium Confidence:** The T-Sigs interpretability method is novel and visually compelling, but the claim of providing "the first explanation" for generator distinguishability needs broader validation across diverse generator types.

## Next Checks

1. **Cross-Generator Robustness:** Test SAGA on a held-out set of recently released generators (e.g., OpenAI's Sora updates, Google's Veo) to verify temporal artifact detection generalizes beyond the 19 generators in the training set.

2. **Compression Artifact Analysis:** Evaluate performance on videos compressed with varying codecs (H.264, H.265, AV1) to quantify the impact of temporal artifact degradation on attribution accuracy.

3. **Attention Pattern Consistency:** For a fixed generator, compute T-Sig variance across videos with different content categories (e.g., "nature," "urban," "people") to verify that attention signatures remain stable despite content changes.