---
ver: rpa2
title: 'Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM
  Reasoning'
arxiv_id: '2510.04488'
source_url: https://arxiv.org/abs/2510.04488
tags:
- maci
- evidence
- crit
- debate
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MACI introduces dual-dial control for multi-agent debate, using
  an information dial to gate evidence quality and a behavior dial to schedule contentiousness
  from exploration to consolidation. A moderator tracks disagreement, overlap, evidence
  quality, and argument quality, stopping when gains plateau.
---

# Multi-Agent Collaborative Intelligence: Dual-Dial Control for Reliable LLM Reasoning

## Quick Facts
- arXiv ID: 2510.04488
- Source URL: https://arxiv.org/abs/2510.04488
- Authors: Edward Y. Chang; Ethan Y. Chang
- Reference count: 40
- Primary result: MACI improves clinical diagnosis accuracy by 3.9pp over majority vote while reducing tokens by 19%

## Executive Summary
MACI introduces a dual-dial control mechanism for multi-agent debate orchestration, decoupling information quality from behavioral stance to enable independent control over evidence filtering and debate dynamics. The framework uses an information dial to gate evidence by quality and a behavior dial to schedule contentiousness from exploration to consolidation, moderated by a multi-signal plateau detector. Cross-family LLM evaluators (CRIT) provide reliability-weighted aggregation, validated for order invariance and judge-swap stability. Empirical results show significant improvements in accuracy, uncertainty calibration, and efficiency across clinical diagnosis and news bias tasks.

## Method Summary
MACI orchestrates multi-agent debate through two orthogonal dials: an information dial (τ) that gates evidence by quality thresholds (τQ, τCRIT), and a behavior dial (CL) that schedules contentiousness from high (0.9: "challenge aggressively") to low (0.5: "build on agreements"). A moderator tracks four signals—disagreement (DJS), evidence overlap (O), evidence quality (Q), and argument quality (CRIT)—and halts when relative progress ratios fall below thresholds for τ_stop consecutive rounds. CRIT scores from K≥3 cross-family judges weight agents' contributions via exponential moving average, providing reliability-weighted aggregation. The framework provides theory-lite guarantees: dispersion is nonincreasing and termination occurs in O(1/ε) rounds, improving to O(log(1/ε)) under contraction.

## Key Results
- Clinical diagnosis (1,500 cases): +3.9pp accuracy over majority vote, ECE 0.081 vs 0.103, 19% token reduction
- Cross-domain validation on news bias (619 articles): 68% reduction in partisan gap without domain tuning
- CRIT evaluator stability: 2-3% winner flips on judge swaps, 94% of cases with |∆|<0.05 on order reversal

## Why This Works (Mechanism)

### Mechanism 1: Decoupling Information Quality from Behavioral Stance
The dual-dial control enables independent modulation of evidence filtering and debate dynamics. The information dial gates arguments by quality thresholds, while the behavior dial schedules contentiousness from exploration to consolidation. This orthogonality allows MACI to filter low-quality arguments early while still enabling productive debate consolidation.

### Mechanism 2: Multi-Signal Plateau Detection
A moderator tracks disagreement, evidence overlap, evidence quality, and argument quality, halting when relative progress ratios fall below thresholds for consecutive rounds. This prevents wasted compute and locks in convergence, avoiding both premature and delayed termination.

### Mechanism 3: Cross-Family Evaluator Reliability Weighting
CRIT scores from K≥3 cross-family judges provide conservative soft weights and stop signals, validated for order invariance and judge-swap stability. This multi-judge approach reduces individual evaluator bias while providing reliable aggregation for reliability-weighted mixture outputs.

## Foundational Learning

### Jensen-Shannon Divergence (JSD) and Wasserstein Distance
- Why needed: JSD quantifies distributional disagreement between agents (bounded, symmetric). Wasserstein distance captures geometry-aware discrepancy for structured outputs. These form primary dispersion measures.
- Quick check: Given P=[0.6,0.3,0.1] and Q=[0.4,0.5,0.1], compute JSD(P||Q). Is it bounded?

### Exponential Moving Average (EMA) for Reliability Tracking
- Why needed: CRIT scores are noisy per-round; EMA smooths them into stable reliability weights (λ=0.8 balances responsiveness and stability).
- Quick check: If an agent's CRIT scores are [0.7, 0.8, 0.6, 0.9] over rounds 1-4, compute their reliability weight after round 4 using λ=0.8 and initial Γ(0)=0.5.

### Multi-Armed Bandit (UCB) for Adaptive Scheduling
- Why needed: The learned scheduler treats (α, γ) adjustments as actions, selecting based on Upper Confidence Bound to balance exploration and exploitation under token budgets.
- Quick check: In a 3-action bandit with empirical means [0.5, 0.6, 0.4] and counts [10, 5, 20] after 35 rounds, which action does UCB select?

## Architecture Onboarding

### Component Map
Agents (M≥2) -> Moderator (computes signals, manages gates, updates dials, checks stopping) -> CRIT Evaluator Panel (K≥3 cross-family judges) -> Retrieval Module (BM25 + dense reranking) -> Scheduler (optional bandit policy)

### Critical Path
1. Initialize parameters from development set binning
2. Per round: generate agent outputs → compute per-argument evidence scores and CRIT scores → apply dual gate → aggregate admitted evidence → compute signals → update dials → check stopping
3. Upon stopping: output reliability-weighted mixture and precision RAG plan

### Design Tradeoffs
- Fixed vs. learned scheduler: Fixed schedule is simpler and reproducible; learned scheduler adapts to task but adds complexity
- Gate strictness: Higher τ thresholds reduce noise but may filter useful unconventional evidence
- Judge panel size (K): Larger K reduces variance but increases latency and cost

### Failure Signatures
1. Premature termination: Signals plateau early due to noisy CRIT scores or overly tight thresholds → low-confidence outputs
2. Non-convergence: Contentiousness remains high or gates too loose → wasted tokens, no consolidation
3. Evaluator instability: Cross-family judges disagree heavily (α<0.6) → unreliable weights, potential flipping of outcomes

### First 3 Experiments
1. Reproduce main results on clinical diagnosis subset (100 cases), comparing accuracy, ECE, and tokens vs. majority vote
2. Ablate dual dials: no information gate, no behavior scheduling, uniform weights; report ΔAcc@1 and ΔECE
3. Test evaluator stability: swap judge families and reverse debater order on 50 debates; measure winner-flip rate and score differences

## Open Questions the Paper Calls Out

### Scaling to 3-5 Agents
The paper explicitly states scaling to 3-5 agents is a natural next step, as all experiments use M=2 agents. The dual-dial mechanism and CRIT aggregation may behave differently with more agents due to coordination complexity and dispersion dynamics.

### Objective Task Performance
Broader testing on tasks with objective ground truth (math, code, factual QA) remains future work. Current domains involve subjective judgment, while objective tasks may have different convergence properties and CRIT scoring validity.

### Human Expert Validation
Systematic validation against human experts remains important future work, especially for high-stakes deployment. Current validation shows moderate LLM judge agreement but lacks human ground truth for argument quality assessment.

### Retrieval Quality Impact
Retrieval quality mediates performance, suggesting gains from stronger evidence acquisition. The paper does not test degraded retrieval scenarios or quantify how MACI's benefits scale with retrieval quality.

## Limitations

### Evaluator Bias Tolerance
The cross-family CRIT evaluator achieves only moderate agreement (α≈0.68), and stability checks show 2-3% winner flips on judge swaps, suggesting sensitivity to evaluator composition.

### Retrieval Quality Dependency
Performance gains assume high-quality evidence retrieval (BM25 + dense reranking), but the paper does not test degraded retrieval scenarios or quantify scalability with retrieval quality.

### Development Set Initialization Gap
The percentile-binning initialization method for τ and ε thresholds is referenced but not fully specified, making exact replication difficult.

## Confidence

### High Confidence
- Dual-dial control mechanism and theoretical convergence guarantees (O(1/ε) rounds, O(√KT) no-regret) are well-supported by proofs

### Medium Confidence
- Clinical diagnosis results (3.9pp accuracy gain, 19% token reduction) are compelling, but lack of ablation studies makes it difficult to attribute gains to specific mechanisms

### Low Confidence
- Cross-domain news bias validation shows 68% partisan gap reduction, but this is a single external dataset with different task structure, raising questions about generalizability

## Next Checks

### Evaluator Robustness Stress Test
Run MACI with (a) same-family judges, (b) K=2 judges, (c) judges with adversarial prompts. Measure winner-flip rates and CRIT score variance to quantify sensitivity to evaluator composition.

### Dual Dial Ablation
Implement MACI variants with: (a) fixed τ gates (no information dial), (b) fixed CL (no behavior scheduling), (c) uniform reliability weights. Run on clinical diagnosis subset (100 cases) and report Acc@1, ECE, and token counts to isolate each dial's contribution.

### Retrieval Quality Sensitivity
Run MACI with degraded retrieval (e.g., BM25 only, k_ret=16) on 50 clinical cases. Compare accuracy, convergence speed, and stopping behavior to quantify how performance degrades with retrieval quality.