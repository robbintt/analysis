---
ver: rpa2
title: 'The Bias is in the Details: An Assessment of Cognitive Bias in LLMs'
arxiv_id: '2509.22856'
source_url: https://arxiv.org/abs/2509.22856
tags:
- bias
- wang
- zhang
- biases
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates cognitive biases in 45 large language models
  (LLMs) across 8 established bias types, analyzing over 2.8 million responses. A
  novel framework using hand-crafted scenarios and controlled prompt variations measures
  bias susceptibility.
---

# The Bias is in the Details: An Assessment of Cognitive Bias in LLMs

## Quick Facts
- arXiv ID: 2509.22856
- Source URL: https://arxiv.org/abs/2509.22856
- Reference count: 36
- 45 LLMs evaluated across 8 bias types show 17.8-57.3% bias-consistent behavior

## Executive Summary
This study systematically evaluates cognitive biases in large language models using a novel framework that tests 45 models across 8 established bias types through over 2.8 million responses. The research demonstrates that both larger model sizes (>32B parameters) and higher prompt specificity significantly reduce bias susceptibility in most cases, with larger models reducing bias by up to 39.5% and detailed prompts by up to 14.9%. However, one notable exception is Overattribution bias, which actually worsens with more detailed prompts. The findings establish that while LLMs do exhibit consistent cognitive biases, these can be mitigated through strategic architectural choices and prompt engineering, though the effectiveness varies considerably across different bias types.

## Method Summary
The study employs a comprehensive framework using hand-crafted scenarios designed to test eight distinct cognitive biases: Availability, Anchoring, Overconfidence, Representativeness, Overattribution, Hindsight, Sunk Cost, and Framing biases. Each scenario is evaluated through three progressively detailed prompt levels (Level 1: basic, Level 2: contextual, Level 3: enhanced with rationale), generating over 2.8 million responses across 45 models ranging from 0.5B to 70B parameters. Responses are scored on a binary scale (0 for unbiased, 1 for biased) to measure susceptibility. The methodology controls for variations in prompt detail and model size to isolate their effects on bias manifestation, providing a systematic approach to quantifying cognitive bias in LLMs.

## Key Results
- LLMs exhibit bias-consistent behavior in 17.8-57.3% of instances across eight tested bias types
- Larger models (>32B parameters) reduce bias susceptibility by up to 39.5% compared to smaller models
- Higher prompt specificity reduces bias in most cases by up to 14.9%, except for Overattribution bias which increases by 8.8%
- Overattribution bias shows the highest resistance to mitigation, with larger models only reducing susceptibility by 4.8% and detailed prompts increasing bias by 8.8%

## Why This Works (Mechanism)
The study demonstrates that cognitive biases in LLMs emerge from the same psychological patterns observed in human decision-making, suggesting these biases are embedded in the training data and model architecture. The mechanism appears to involve both the statistical patterns learned during training and the model's tendency to rely on surface-level features when processing information. Larger models appear to develop more sophisticated reasoning capabilities that help them recognize and avoid biased patterns, while detailed prompts provide additional context that helps models override their default biased responses. However, the exception with Overattribution bias suggests that certain biases may be more deeply embedded in the model's architecture or training process, making them resistant to standard mitigation techniques.

## Foundational Learning
**Cognitive Bias Types**: Understanding the eight specific biases tested (Availability, Anchoring, Overconfidence, Representativeness, Overattribution, Hindsight, Sunk Cost, and Framing) is essential because each represents a distinct pattern of flawed reasoning that manifests differently in LLM outputs.
*Why needed*: Different biases require different mitigation strategies, and understanding their specific characteristics helps explain why some are more resistant to reduction than others.
*Quick check*: Can you distinguish between how Anchoring bias (relying too heavily on initial information) differs from Representativeness bias (judging probability by similarity to stereotypes)?

**Prompt Engineering Levels**: The three-tiered prompt system (basic, contextual, enhanced with rationale) represents a systematic approach to testing how information detail affects bias manifestation.
*Why needed*: This framework reveals that the relationship between prompt specificity and bias is not uniform across all bias types, with some biases worsening when given more context.
*Quick check*: Why does Overattribution bias increase by 8.8% with detailed prompts while other biases decrease by up to 14.9%?

**Model Architecture Impact**: The study shows that model size correlates with bias resistance, but with diminishing returns beyond 32B parameters.
*Why needed*: This finding suggests that sheer parameter count has limits as a bias mitigation strategy, pointing to the need for alternative architectural or training approaches.
*Quick check*: What non-size factors might explain why bias resistance plateaus around 32B parameters?

## Architecture Onboarding

**Component Map**: Scenario Design -> Prompt Level Generation -> Model Response Collection -> Bias Scoring -> Statistical Analysis -> Result Interpretation

**Critical Path**: The evaluation pipeline follows: Hand-crafted scenarios → Three prompt variations → LLM responses → Binary bias scoring → Aggregated statistics → Cross-model comparison

**Design Tradeoffs**: The study prioritizes controlled testing over ecological validity by using synthetic scenarios rather than real-world cases, enabling precise measurement but potentially limiting generalizability to complex decision-making contexts.

**Failure Signatures**: Models show highest bias susceptibility in smaller architectures (<8B parameters) and with basic prompts (Level 1), with Overattribution bias consistently showing the weakest response to mitigation efforts across all conditions.

**Three First Experiments**:
1. Replicate the evaluation using professionally documented real-world case studies to test whether synthetic scenario findings generalize to complex decision-making contexts
2. Test cross-linguistic bias manifestation by translating scenarios into multiple languages and evaluating non-English models
3. Conduct longitudinal testing to track how bias manifestation changes as models are updated or fine-tuned over time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does increased prompt specificity exacerbate Overattribution bias while mitigating other cognitive biases?
- Basis in paper: [explicit] Section 7.2.3 notes that Overattribution bias sees a significant drop in resistance (8.2%) when moving from Level 1 to Level 2 prompts.
- Why unresolved: The authors suggest additional context may inadvertently focus the model on "bias-eliciting portions" of the prompt, but they do not isolate the specific linguistic cues causing this reverse effect.
- What evidence would resolve it: Ablation studies removing specific contextual details from TELeR Level 2/3 prompts to identify which "harmful details" trigger the overattribution response.

### Open Question 2
- Question: What architectural or training factors determine bias resistance in models exceeding 32B parameters?
- Basis in paper: [explicit] Section 7.1 states that while bias resistance increases with size, the effect "diminishes as models scale past 32B parameters," suggesting future gains depend on factors other than sheer size.
- Why unresolved: The study establishes a correlation between size and resistance but identifies a plateau, leaving the specific non-size variables (e.g., training data diversity, RLHF techniques) that might drive further improvements undefined.
- What evidence would resolve it: A controlled comparison of models >32B parameters that isolates training methodology (e.g., reasoning-tuned vs. standard instruction-tuned) as the independent variable.

### Open Question 3
- Question: Can reasoning-oriented training objectives generalize to reducing biases other than Framing bias?
- Basis in paper: [explicit] Section 6.2.3 notes that reasoning models like DeepSeek-R1 show improved resistance to Framing bias, but "we do not observe comparable gains for other biases."
- Why unresolved: The paper identifies a bias-specific benefit to reasoning capabilities but does not explain why this architectural intervention fails to transfer to biases like Representativeness or Anchoring.
- What evidence would resolve it: An evaluation of "chain-of-thought" or reasoning-tuned models specifically targeting the seven other bias types where performance gains were not initially observed.

## Limitations
- Synthetic hand-crafted scenarios may not capture the full complexity of real-world bias manifestations, limiting ecological validity
- Binary classification (0/1 scoring) of bias-consistent responses may oversimplify nuanced decision-making processes
- Dataset covers only 45 models, with substantial performance variations suggesting potential confounding factors beyond parameter count
- Finding that Overattribution bias worsens with detailed prompts (8.8% increase) suggests certain biases may be inherently resistant to standard mitigation techniques

## Confidence
- Model size reduces bias susceptibility (Medium confidence): Statistically significant reductions (up to 39.5%) observed, but substantial variation across bias types and potential confounding factors related to training methodology
- Prompt specificity reduces bias (Medium confidence): 14.9% average reduction supported across most bias types, but Overattribution bias exception (8.8% increase) and unclear mechanism limit confidence
- LLMs exhibit consistent bias behavior (High confidence): Well-supported by large sample size (2.8 million responses) and consistent methodology across all 45 evaluated models

## Next Checks
1. Conduct real-world scenario validation using professionally documented case studies from legal, medical, and business domains to test whether synthetic scenario findings generalize to complex decision-making contexts

2. Perform cross-linguistic evaluation of the same 8 bias types using non-English prompts and scenarios to assess whether bias manifestation and mitigation strategies are culturally and linguistically universal

3. Implement longitudinal testing to track how bias manifestation changes as models are updated or fine-tuned, determining whether observed mitigation effects are stable or decay over time with model evolution