---
ver: rpa2
title: 'VESSA: Video-based objEct-centric Self-Supervised Adaptation for Visual Foundation
  Models'
arxiv_id: '2510.20994'
source_url: https://arxiv.org/abs/2510.20994
tags:
- vessa
- video
- vision
- foundation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VESSA addresses the challenge of adapting vision foundation models
  to new domains without labeled data by leveraging object-centric videos for self-supervised
  fine-tuning. The method employs a self-distillation paradigm with carefully designed
  training schedules, uncertainty-weighted losses, and parameter-efficient adaptation
  using LoRA to preserve pretrained knowledge while adapting to new domains.
---

# VESSA: Video-based objEct-centric Self-Supervised Adaptation for Visual Foundation Models

## Quick Facts
- arXiv ID: 2510.20994
- Source URL: https://arxiv.org/abs/2510.20994
- Reference count: 40
- Key outcome: VESSA achieves 91.85% accuracy on CO3D with DINOv2, outperforming pretrained baseline by 3.99 percentage points

## Executive Summary
VESSA addresses the challenge of adapting vision foundation models to new domains without labeled data by leveraging object-centric videos for self-supervised fine-tuning. The method employs a self-distillation paradigm with carefully designed training schedules, uncertainty-weighted losses, and parameter-efficient adaptation using LoRA to preserve pretrained knowledge while adapting to new domains. By using multi-view video frames as positive pairs, VESSA introduces greater visual variability compared to standard image augmentations, leading to more robust and object-centric representations. Experiments with three vision foundation models (DINO, DINOv2, and TIPS) on two datasets (MVImageNet and CO3D) show consistent improvements in downstream classification tasks.

## Method Summary
VESSA uses a two-phase training approach to adapt vision foundation models using unlabeled object-centric videos. In Phase 1, the model trains only the projection head for 10 epochs while keeping the backbone frozen to prevent catastrophic forgetting from randomly initialized heads. In Phase 2, the model enables LoRA adapters in early layers and unfreezes the last layers for full adaptation. The training uses Uncertainty-Weighted Self-Distillation (UWSD) loss, which assigns higher importance to samples where the teacher's output distribution has high entropy. Frame pairs are sampled from videos with temporal gap δ∈[5,10], providing multi-view observations of the same object under different conditions.

## Key Results
- VESSA achieves 91.85% accuracy on CO3D with DINOv2, outperforming the pretrained baseline by 3.99 percentage points
- The method requires only 1.97 hours for adaptation on a TPU v3-8, demonstrating computational efficiency
- VESSA consistently improves performance across three vision foundation models (DINO, DINOv2, TIPS) on both MVImageNet and CO3D datasets
- Temporal video information provides significantly greater gains (1.99 p.p.) compared to static image augmentations alone

## Why This Works (Mechanism)

### Mechanism 1: Object-Centric Temporal Invariance
Leveraging multi-view video frames as positive pairs encourages the model to learn object-centric invariance rather than mere augmentation invariance. The model receives pairs of frames (F_t, F_{t+δ}) from the same video, forcing it to isolate persistent object features to minimize loss across different viewpoints and backgrounds.

### Mechanism 2: Gradient Stabilization via Head-First Training
Isolating projection head training before unfreezing the backbone prevents catastrophic forgetting caused by randomly initialized heads disrupting pretrained weights. This allows the head to align with existing embedding space before the backbone is exposed to noisy gradients.

### Mechanism 3: Uncertainty-Weighted Self-Distillation (UWSD)
Modulating the loss based on the teacher network's entropy stabilizes adaptation by filtering out high-confidence (potentially outdated) errors and prioritizing uncertain samples. The loss weight w(q) = 1 + γ · H(q) assigns higher importance to samples where the teacher's output distribution has high entropy.

## Foundational Learning

- **Concept: Self-Distillation (DINO)**
  - Why needed here: VESSA modifies the standard DINO framework using EMA-updated teacher network
  - Quick check question: Why is the Teacher network updated via Exponential Moving Average (EMA) rather than direct gradient descent?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: VESSA relies on LoRA to adapt the Vision Transformer efficiently while preserving pretrained knowledge
  - Quick check question: In the VESSA architecture, are the LoRA matrices applied to the Attention weights (Q, K, V) or the MLP blocks?

- **Concept: Catastrophic Forgetting**
  - Why needed here: The paper frames VESSA as a solution to the "degraded state" found in naive fine-tuning
  - Quick check question: Why does Table 7 show a drastic performance drop on ImageNet after adaptation, and is this considered a failure of the method?

## Architecture Onboarding

- **Component map:**
  1. Data Loader: Samples frame pairs (F_t, F_{t+δ}) with Random δ ∈ [5, 10]
  2. Backbone: ViT-Base/S (Pretrained) - Layers 0 to H: Frozen + LoRA (Q/K/V projections), Layers H to L: Fully Unfrozen
  3. Heads: Projection head (Trainable); Teacher head (EMA)
  4. Loss: UWSD (Cross-Entropy weighted by Teacher Entropy)

- **Critical path:**
  1. Initialization: Load Pretrained VFM (DINO/DINOv2) into Student and Teacher
  2. Phase 1 (Head Alignment): Freeze Backbone, train Projection Head for 10 epochs
  3. Phase 2 (Adaptation): Enable LoRA in early layers, Unfreeze last layers, train with UWSD

- **Design tradeoffs:**
  - Frame Gap (δ): Too small creates trivial pairs; too large creates distinct objects. Paper recommends Random [5, 10]
  - LoRA Rank vs. Layers: Deeper full-unfreezing adapts faster but risks forgetting; more LoRA preserves knowledge but might underfit

- **Failure signatures:**
  - Performance Collapse: If validation accuracy drops early (<20%), check if backbone was unfrozen before projection head stabilized
  - Overfitting to Background: If KNN retrieves images based on background, verify frame pairs are object-centric (cropped) rather than scene-level

- **First 3 experiments:**
  1. Ablation on Input Modality: Compare Static Image vs. Video Input (Table 3) to confirm "Video" signal is active
  2. Training Schedule Validation: Run Phase 1 (Head-only) vs. End-to-End training immediately to reproduce "10 p.p. difference"
  3. Frame Distance Sweep: Test fixed δ ∈ {1, 5, 10, 20} vs. Random δ to calibrate temporal augmentation

## Open Questions the Paper Calls Out

### Open Question 1
Can VESSA be extended to mitigate catastrophic forgetting while preserving domain-specific gains, so adapted models remain usable for general-purpose tasks? The authors note a notable limitation is the tendency to forget previously acquired knowledge during fine-tuning, with Table 7 showing ImageNet accuracy drops from 76.10% to 15.46% after adaptation.

### Open Question 2
Does VESSA transfer to pixel-level vision tasks such as semantic segmentation and object detection? The paper notes most prior video-based methods target pixel-level tasks and show limited improvements for frame-level classification, but VESSA is only evaluated on k-NN classification.

### Open Question 3
Can VESSA achieve comparable adaptation without requiring structured multi-view object-centric videos? The authors acknowledge their experimental setup relies on video data offering multiple viewpoints, which is not commonly available in many real-world datasets.

### Open Question 4
What underlying factors in real video cause VESSA to outperform strong image-based augmentation pipelines? Table 6 shows motion-simulating augmentations yield no improvement over static baseline, while VESSA achieves 85.03% with real video, suggesting real video provides supervisory signals simple transforms cannot replicate.

## Limitations
- VESSA causes significant catastrophic forgetting, with ImageNet accuracy dropping from 76.10% to 15.46% after adaptation
- The method requires object-centric videos with multiple viewpoints, which are not commonly available in real-world datasets
- VESSA is only evaluated on k-NN classification tasks, leaving uncertainty about transfer to pixel-level tasks like segmentation and detection

## Confidence
- High confidence: The core two-phase training schedule and use of temporal video pairs as positive examples
- Medium confidence: The LoRA parameterization strategy and specific layer choices, as implementation details are incomplete
- Low confidence: The theoretical justification for uncertainty-weighted distillation, as the paper provides limited analysis of why high-entropy outputs indicate "harder" examples

## Next Checks
1. Replicate the head-first training ablation (Table 7) by running Phase 1 vs. direct end-to-end training on the same dataset to verify the 10 p.p. improvement claim
2. Test UWSD with γ=0 (standard distillation) vs. γ=1 vs. γ=2 to quantify the contribution of uncertainty weighting beyond standard self-distillation
3. Evaluate VESSA with static images (same augmentations but no temporal pairs) on CO3D to isolate the contribution of video input versus temporal invariance