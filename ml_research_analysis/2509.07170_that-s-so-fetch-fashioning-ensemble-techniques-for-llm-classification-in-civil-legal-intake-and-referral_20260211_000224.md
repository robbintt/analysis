---
ver: rpa2
title: 'That''s So FETCH: Fashioning Ensemble Techniques for LLM Classification in
  Civil Legal Intake and Referral'
arxiv_id: '2509.07170'
source_url: https://arxiv.org/abs/2509.07170
tags:
- legal
- ensemble
- classification
- problem
- applicant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of accurately classifying legal
  problems for civil legal intake and referral, where misclassification can have serious
  consequences for applicants. The FETCH classifier employs an ensemble method combining
  three inexpensive LLM models (GPT-5-nano, Gemini 2.5-flash, and Mistral small) with
  traditional ML and keyword matching to improve classification accuracy while reducing
  costs.
---

# That's So FETCH: Fashioning Ensemble Techniques for LLM Classification in Civil Legal Intake and Referral

## Quick Facts
- arXiv ID: 2509.07170
- Source URL: https://arxiv.org/abs/2509.07170
- Authors: Quinten Steenhuis
- Reference count: 27
- Primary result: Ensemble of 3 inexpensive LLM models achieves 97.37% accuracy (hits@2) on legal intake classification, exceeding single GPT-5 performance while reducing costs by ~2/3

## Executive Summary
This paper presents FETCH, an ensemble classification system for civil legal intake and referral that combines three inexpensive LLM models (GPT-5-nano, Gemini 2.5-flash, and Mistral small) with traditional ML and keyword matching. Using a dataset of 419 real-world legal queries, the ensemble achieves 97.37% accuracy (hits@2), slightly exceeding the state-of-the-art GPT-5 model's performance while reducing costs by more than two-thirds. The system also generates clarifying follow-up questions for ambiguous queries, showing promise for improving access to justice through cost-effective, accurate legal referral services.

## Method Summary
FETCH uses weighted ensemble voting across five classifiers: three LLM models (GPT-5-nano, Gemini 2.5-flash, Mistral-small-3.2-24b-instruct), a keyword matcher, and the Spot API (traditional ML). The ensemble returns top-2 predictions from a 244-node taxonomy, with partial credit for correct top-level categories. Classification weights are manually tuned on a 20-query subset. The system also generates follow-up questions when confidence is low, merging questions from all three LLMs via GPT-5-nano to clarify ambiguous queries before re-classification.

## Key Results
- Ensemble achieves 97.37% accuracy (hits@2), slightly exceeding GPT-5's 96.66% performance
- Cost reduced from $326.71/100k queries (GPT-5) to approximately $103 for ensemble
- Individual inexpensive models range from 81-88% accuracy, demonstrating value of ensemble aggregation
- Qualitative analysis shows automatically generated follow-up questions effectively clarify ambiguous queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighted ensemble voting of multiple inexpensive LLMs can match or exceed single frontier model accuracy in legal classification tasks
- Mechanism: Each model independently classifies the query; results are aggregated via weighted voting where weights are calibrated on a held-out subset; top-ranked labels are returned. Diversification across model families (OpenAI, Google, Mistral) and method types (LLM, traditional ML, keyword) reduces correlated errors
- Core assumption: Different models make uncorrelated errors; no single systematic bias spans all ensemble members
- Evidence anchors:
  - [abstract]: "achieved 97.37% accuracy (hits@2), slightly exceeding the state-of-the-art GPT-5 model's performance"
  - [section]: Table 1 shows ensemble at 97.37% vs GPT-5 at 96.66%, with individual inexpensive models ranging 81–88%
  - [corpus]: Neighbor paper "Investigating the Shortcomings of LLMs in Step-by-Step Legal Reasoning" documents reasoning gaps in single LLMs, supporting diversification rationale
- Break condition: If all models share similar training data or architectural biases, errors correlate and ensemble gains diminish; also breaks if taxonomy granularity exceeds model discrimination ability

### Mechanism 2
- Claim: Substituting a frontier model with a small-model ensemble significantly reduces inference cost while maintaining accuracy
- Mechanism: Cache common prompt prefixes (taxonomy definitions); distribute queries across cheaper models with lower per-token pricing; aggregate locally. The ensemble achieves comparable accuracy at ~1/3 the cost of GPT-5 alone
- Core assumption: Latency overhead of parallel/sequential small model calls is acceptable for the intake use case; pricing ratios remain favorable
- Evidence anchors:
  - [abstract]: "reducing costs by more than two-thirds"
  - [section]: Table 2 estimates $326.71/100k queries for GPT-5 vs. ~$103 combined for ensemble members; latency comparison notes GPT-5 minimum 5s vs GPT-5-nano 2.2s
  - [corpus]: Limited direct corpus evidence on cost-efficiency of ensemble routing; this is a gap in related work
- Break condition: If provider pricing changes, caching is unavailable, or latency SLAs tighten below parallelization overhead, cost advantage erodes

### Mechanism 3
- Claim: Automatically generated follow-up questions can disambiguate vague legal queries and improve classification outcomes
- Mechanism: When ensemble confidence falls below threshold, each LLM generates candidate questions; questions are semantically merged (via GPT-5-nano); user responses enrich the query for re-classification
- Core assumption: Ambiguity stems from insufficient information rather than unclassifiable problems; users will engage and answer accurately
- Evidence anchors:
  - [abstract]: "qualitative analysis indicating that automatically generated follow-up questions can further enhance classification of ambiguous queries"
  - [section]: Table 4 provides qualitative examples showing relevant clarifying questions for ambiguous inputs (e.g., "I am getting kicked out" → eviction vs. other civil)
  - [corpus]: Neighbor paper "LegalWebAgent" discusses LLM-based interaction for access to justice, but quantitative evidence on follow-up question effectiveness is limited
- Break condition: If users drop off, misanswer, or queries remain ambiguous after clarification, the loop fails to improve outcomes

## Foundational Learning

- Concept: **Ensemble voting (weighted aggregation)**
  - Why needed here: The FETCH architecture depends on combining heterogeneous classifiers; understanding how weights are calibrated and votes aggregated is essential for debugging and extending the system
  - Quick check question: If three models vote with weights [0.4, 0.35, 0.25] and two predict label A while one predicts B, which label wins?

- Concept: **Hits@K evaluation metric**
  - Why needed here: The paper reports hits@2, meaning the correct label must appear in the top 2 predictions; this framing affects how you interpret accuracy and design ranking thresholds
  - Quick check question: A classifier achieves 97% hits@2 but only 85% hits@1—what does this tell you about its ranking vs. its top-prediction precision?

- Concept: **Legal taxonomy hierarchy**
  - Why needed here: Classification operates over a 244-node taxonomy with parent-child relationships; partial credit is given for correct top-level classification, affecting both scoring and error analysis
  - Quick check question: If a query is annotated as "Consumer > Small Claims" but the classifier predicts "General > Small Claims," how would hits@2 with partial credit scoring handle this?

## Architecture Onboarding

- Component map:
  Input query -> Taxonomy prompt prefix (cached) -> 5 classifiers (GPT-5-nano, Gemini 2.5-flash, Mistral-small, keyword matcher, Spot API) -> Weighted voting aggregation -> Confidence check -> (If low confidence) Follow-up question generation -> User response -> Re-classification -> Output: Top-2 labels

- Critical path:
  1. Receive query → prepend taxonomy prompt (cached)
  2. Dispatch parallel calls to 5 classifiers
  3. Collect predictions + confidence signals
  4. Apply weighted voting; rank labels
  5. Check confidence threshold → either return top-2 or generate follow-up questions
  6. If follow-up: merge questions, present to user, re-run classification with enriched context

- Design tradeoffs:
  - Accuracy vs. cost: Frontier model (GPT-5) offers simplicity but higher cost; ensemble adds orchestration complexity but reduces cost by ~2/3
  - Latency vs. redundancy: More models increase fault tolerance but add parallel call overhead; paper notes 2.2s–5s+ latency range
  - Automation vs. human-in-loop: Automatic follow-up questions reduce human intake burden but risk user drop-off

- Failure signatures:
  - Low confidence across all models: Suggests query is out-of-distribution or too vague; trigger follow-up questions
  - High-weight models disagree: Indicates boundary case; consider escalating to human review or stronger model
  - Follow-up questions don't converge: User responses may still not clarify; route to human intake worker
  - Suspected annotation error: Paper notes 4 probable human annotation errors discovered via LLM classification

- First 3 experiments:
  1. Baseline calibration: Run each classifier individually on a held-out subset; record hits@2, confidence calibration, and per-category error patterns to set initial weights
  2. Ensemble ablation: Remove one classifier at a time from the ensemble to measure contribution; identify if any single model is critical or if redundancy is effective
  3. Follow-up loop effectiveness: Manually strip details from a set of queries, trigger follow-up questions, measure whether re-classification after user response improves hits@2; track drop-off rates

## Open Questions the Paper Calls Out

- **Can automatically generated follow-up questions quantitatively improve classification accuracy for ambiguous legal intake queries?**
  - Basis in paper: The authors state, "We additionally hope to run a broader, quantitative study of the value of follow-up answers in improving classification," noting that current analysis was qualitative
  - Why unresolved: The paper only qualitatively analyzed three examples of follow-up questions and did not measure the performance gain from the enriched context in the main dataset
  - What evidence would resolve it: A controlled experiment measuring classification accuracy (hits@2) with and without the generated follow-up answers across the dataset

- **Does faster automated legal referral lead to improved real-world outcomes for applicants?**
  - Basis in paper: The authors acknowledge a limitation in Section 7.3: "We did not measure whether faster automated referral improved real outcomes"
  - Why unresolved: The study focused on classification accuracy and cost reduction, but did not track the downstream effects on the applicants' legal situations or connection success rates
  - What evidence would resolve it: Longitudinal data tracking applicant outcomes (e.g., successful resolution, time to connection) comparing the FETCH system against traditional human intake

- **How robust is the ensemble classifier against "delusional" queries or scenarios with no legal solution?**
  - Basis in paper: The authors note, "Therefore we did not fully test the 'no legal problem' condition, especially delusional queries, which will require future sensitivity testing"
  - Why unresolved: The dataset contained only 19 "no legal problem" scenarios, which was insufficient for a robust statistical analysis of this specific failure mode
  - What evidence would resolve it: Evaluation of the classifier on a curated test set containing a higher proportion of non-legal and delusional inputs to measure false positive rates

## Limitations

- Dataset scale: Only 419 queries, limiting generalizability and raising risk of overfitting to this specific legal intake context
- Annotation uncertainty: Human annotators disagreed on 4 cases, and the paper identified additional potential annotation errors via LLM classification
- Missing technical details: Exact ensemble weights, keyword matcher implementation, and aggregation formula are not specified, blocking exact reproduction
- Cost analysis simplifications: Assumes favorable pricing and caching; real-world cost savings may vary with provider pricing changes

## Confidence

- **High Confidence**: The core mechanism of ensemble voting across heterogeneous models to match single frontier model accuracy (97.37% vs 96.66% hits@2) is well-supported by results
- **Medium Confidence**: Cost reduction claims (~2/3 savings) rely on pricing assumptions and caching benefits that may not hold in all deployment scenarios
- **Medium Confidence**: Follow-up question generation shows qualitative promise but lacks quantitative validation of its effectiveness in improving classification outcomes

## Next Checks

1. **Weight calibration validation**: Test multiple weight configurations on the 20-query tuning set to verify optimal weights and ensemble contribution of each classifier
2. **Follow-up question effectiveness**: Conduct controlled experiments where ambiguous queries receive follow-up questions, measure if enriched responses improve classification accuracy and track user drop-off rates
3. **Cost-sensitivity analysis**: Model how ensemble cost advantage changes under different pricing scenarios (no caching, varied provider rates) to establish robust cost-effectiveness boundaries