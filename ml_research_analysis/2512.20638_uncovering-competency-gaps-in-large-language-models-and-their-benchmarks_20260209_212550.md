---
ver: rpa2
title: Uncovering Competency Gaps in Large Language Models and Their Benchmarks
arxiv_id: '2512.20638'
source_url: https://arxiv.org/abs/2512.20638
tags:
- concepts
- gaps
- benchmark
- concept
- benchmarks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Competency Gaps (CG), a new method for uncovering
  model and benchmark gaps in large language model evaluation using sparse autoencoders
  (SAEs). The method automatically identifies underrepresented concepts in benchmarks
  and underperforming concepts in models by leveraging SAE concept activations and
  saliency-weighted performance scores.
---

# Uncovering Competency Gaps in Large Language Models and Their Benchmarks

## Quick Facts
- arXiv ID: 2512.20638
- Source URL: https://arxiv.org/abs/2512.20638
- Reference count: 40
- Key outcome: Introduces Competency Gaps method using SAEs to identify underrepresented concepts in benchmarks and underperforming concepts in models, revealing that benchmarks miss core concepts while overtesting authority-related concepts

## Executive Summary
This paper introduces Competency Gaps (CG), a novel method for uncovering model and benchmark gaps in large language model evaluation using sparse autoencoders (SAEs). The approach automatically identifies underrepresented concepts in benchmarks and underperforming concepts in models by leveraging SAE concept activations and saliency-weighted performance scores. Applied to Llama3.1-8B-Instruct and Gemma2-2B-Instruct across ten benchmarks, CG revealed that benchmarks often miss concepts central to their intended scope while overtesting concepts tied to authority and instruction-following. Models consistently underperformed on concepts related to polite refusal, boundary-setting, and safety discussions.

## Method Summary
The method uses pre-trained SAEs attached to specific model layers to decompose activations into interpretable concepts. For each benchmark sample, it extracts SAE latent vectors and computes normalized concept activation scores by dividing the sum of concept activations by sequence length. It calculates two key metrics: $\chi_{bench}$ (coverage ratio) comparing concept activation to average activation, and $\chi_{model}$ (saliency-weighted performance) multiplying performance scores by concept activations. Concepts are classified as missing (<1e-5), underrepresented (bottom decile), or overrepresented (top decile), providing a representation-grounded approach to evaluation that complements traditional aggregated metrics.

## Key Results
- Benchmarks often miss concepts central to their intended scope while overtesting concepts tied to authority and instruction-following
- Models consistently underperformed on concepts related to polite refusal, boundary-setting, and safety discussions
- Results were robust across different SAEs and perturbations, with standard deviations of 0.014 (Xmodel) and 0.025 (Xbench) under random subsampling
- An interactive web application is released alongside the code for visualizing concept distributions

## Why This Works (Mechanism)

### Mechanism 1
Sparse Autoencoders (SAEs) decompose a model's dense internal state into a dictionary of human-interpretable concepts. The method relies on pre-trained SAEs attached to a specific layer, converting dense activations into high-dimensional sparse vectors. Non-zero dimensions are mapped to textual labels, grounding evaluation in the model's own representations rather than external heuristics. Core assumption: the SAE dictionary is sufficiently representative and monosemantic.

### Mechanism 2
Weighting model performance by SAE concept activation exposes specific competence failures masked by aggregate scores. Instead of averaging accuracy, the method calculates $\chi_{model}^{(b,c)}$ by taking the standard performance score for a data point and multiplying it by the normalized SAE activation score for that point. This isolates how well the model performs specifically when a concept is active, revealing "Model Gaps." Core assumption: high SAE activation correlates with the concept being the relevant factor in reasoning.

### Mechanism 3
Normalizing concept frequency across benchmarks reveals structural "Benchmark Gaps" independent of dataset size. The method computes $\chi_{bench}^{(b,c)}$ by comparing the activation of a specific concept to the average activation of all concepts in that dataset. It enforces that all benchmarks have equal weight, preventing large datasets from drowning out concept profiles of smaller ones. Core assumption: a "good" benchmark suite should have a balanced distribution of concepts relevant to its scope.

## Foundational Learning

- **Sparse Autoencoders (SAEs)**: Why needed: The entire CG method relies on SAEs to translate opaque model activations into the "concept dictionary" used for scoring. Quick check: Do you understand how an SAE reconstructs input using a sparse set of learned features (the "dictionary")?

- **Representation Grounding**: Why needed: Unlike standard evaluation which looks at input/output pairs, CG analyzes the internal residual stream. You must grasp that "features" exist as directions in activation space. Quick check: Can you explain the difference between evaluating a model's behavior (external) and its representations (internal)?

- **Saliency Weighting**: Why needed: The method does not treat all data points equally; it prioritizes data points that strongly activate a specific concept to measure performance relevant to that concept. Quick check: Why is averaging performance across all data points inferior to weighting by concept activation when trying to find specific skill gaps?

## Architecture Onboarding

- **Component map**: Input Layer (Benchmark Dataset + Scoring Policy) -> Extraction Layer (Target LLM + Pre-trained SAE) -> Processing Core (Activation Scorer + Gap Calculator) -> Output Interface (Web Application)
- **Critical path**: Obtaining the SAE activations. The method fails if the SAE is not trained on the specific model or a sufficiently similar proxy.
- **Design tradeoffs**: SAE Layer Depth (deeper layers capture higher-level abstractions but may lose syntactic precision); Dictionary Size (larger dictionaries offer finer granularity but increase compute and noise).
- **Failure signatures**: High Variance in Subsampling (if dropping 20% of data shifts scores significantly, benchmark size is too small); "Missing" Concepts in Scope (if a benchmark designed for reasoning misses "logic" concepts, SAE may be misaligned).
- **First 3 experiments**:
  1. Sanity Check: Run analysis on GSM8K, then randomly drop 20% of data to verify low standard deviations reported in Section 4.2.3.
  2. Cross-Model Validation: Apply Gemma-2-2B SAE to Llama-3.1-8B activations to reproduce "correspondence" results in Table 3.
  3. Adversarial Ablation: Manually remove top 10 "best performing" data points from a benchmark and re-run CG analysis to confirm median $\chi_{model}$ drops predictably.

## Open Questions the Paper Calls Out

### Open Question 1
Can Competency Gaps effectively guide the targeted generation of novel benchmark data using SAE-based steering interventions? While the paper demonstrates gap identification, it does not implement or validate using those gaps to steer generation of new evaluation data. Evidence needed: A study showing augmented benchmarks improve coverage scores for previously missing concepts without compromising data quality.

### Open Question 2
To what extent does the choice of SAE layer impact the stability and granularity of identified competency gaps? The current study relies on specific layers, but different layers capture different abstraction levels; sensitivity to this choice remains unquantified. Evidence needed: Comprehensive analysis running CG pipeline across all available layers to measure variance in top-ranked gaps.

### Open Question 3
How does "feature absorption" in SAEs distort the identification of model gaps versus benchmark gaps? If the SAE dictionary fails to isolate atomic concepts due to absorption, resulting competency gaps may be misclassified or obscured. Evidence needed: Comparison of CG results using standard SAEs versus SAEs trained to mitigate feature absorption, validated against human-curated concept lists.

## Limitations
- The method's reliance on pre-trained SAEs creates significant dependencies - quality and scope of concept dictionary directly determines what gaps can be identified
- The assumption that high SAE activation correlates with conceptual relevance in reasoning may not always hold, potentially conflating incidental correlations with causal relationships
- Generalizability across different SAE configurations and model architectures remains uncertain despite good correspondence between Llama and Gemma SAEs

## Confidence

- **High**: The core methodology of using SAE concept activations for representation-grounded evaluation is technically sound and well-supported by results showing low standard deviations under subsampling
- **Medium**: Interpretation of specific competency gaps depends on quality of LLM clustering used to filter concepts - while method is valid, semantic accuracy of concept labels affects practical utility
- **Low**: Broader applicability to other model families or SAE training approaches requires further validation beyond the Llama and Gemma models studied

## Next Checks

1. **Cross-Dataset Concept Transfer**: Apply competency gaps analysis to a new benchmark suite (e.g., MMLU or BBH) not used in the original study to test whether method reveals consistent patterns of benchmark-model misalignment across different domains.

2. **SAE Architecture Sensitivity**: Repeat analysis using SAEs with different dictionary sizes (e.g., 32k vs 65k features) and at different model layers to quantify how these architectural choices affect identified competency gaps and their relative importance rankings.

3. **Human Validation Study**: Conduct small-scale human evaluation where annotators assess whether top 20 "underrepresented" concepts identified by method in each benchmark are indeed missing or underrepresented, providing ground truth validation of coverage gap detection mechanism.