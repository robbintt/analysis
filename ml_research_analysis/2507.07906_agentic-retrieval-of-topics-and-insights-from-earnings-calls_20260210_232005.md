---
ver: rpa2
title: Agentic Retrieval of Topics and Insights from Earnings Calls
arxiv_id: '2507.07906'
source_url: https://arxiv.org/abs/2507.07906
tags:
- topic
- topics
- financial
- ontology
- earnings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an LLM-agent driven framework for dynamically
  extracting and organizing emerging financial topics from earnings call transcripts
  into a hierarchical ontology. The approach uses a topic retriever to identify relevant
  topics and excerpts, an ontologist agent to validate and integrate them into the
  ontology, and demonstrates trend analysis and competitor benchmarking using the
  extracted topics.
---

# Agentic Retrieval of Topics and Insights from Earnings Calls

## Quick Facts
- arXiv ID: 2507.07906
- Source URL: https://arxiv.org/abs/2507.07906
- Authors: Anant Gupta; Rajarshi Bhowmik; Geoffrey Gunow
- Reference count: 35
- Primary result: LLM-driven framework dynamically extracts and organizes emerging financial topics from earnings calls into a hierarchical ontology, achieving 0.383 parent-child cosine similarity and identifying statistically significant trends.

## Executive Summary
This paper proposes an LLM-agent driven framework for dynamically extracting and organizing emerging financial topics from earnings call transcripts into a hierarchical ontology. The approach uses a topic retriever to identify relevant topics and excerpts, an ontologist agent to validate and integrate them into the ontology, and demonstrates trend analysis and competitor benchmarking using the extracted topics. Evaluation shows coherent topic ontology structure with average parent-child cosine similarity of 0.383 versus 0.153 for random pairings, and successful identification of statistically significant trending topics across companies and sectors.

## Method Summary
The framework employs a three-stage pipeline: first, an LLM-based Topic Retriever extracts specific financial topics and supporting excerpts from earnings call paragraphs; second, an Ontologist Agent validates these topics against existing ontology entries using semantic matching to prevent redundancy and determines optimal placement in the tree hierarchy; finally, the structured ontology enables trend analysis by tracking topic frequencies over time using Kendall's tau to identify statistically significant monotonic trends.

## Key Results
- Ontology structure shows 0.383 average parent-child cosine similarity versus 0.153 for random pairings
- Successfully identified "Supply Chain" declining in Semiconductors and "Product Differentiation" rising in EV sector
- Trend analysis validated against Deloitte's 2025 outlook for supply chain performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based extraction identifies coherent financial topics better than unsupervised word-distribution models.
- Mechanism: The Topic Retriever uses an LLM to identify "conceptual entities" (e.g., "AI inference," "Supply Chain") rather than statistical word clusters. By prompting the model to distinguish specific initiatives from general themes, it filters generic filler words that degrade LDA coherence.
- Core assumption: The LLM has sufficient embedded financial knowledge to distinguish signal (strategic initiatives) from noise (transcription filler) without fine-tuning.
- Evidence anchors:
  - [abstract] Proposes an LLM-agent to extract topics... struggling to dynamically capture emerging topics.
  - [section] [Page 4] LDA baseline results showed u_mass coherence scores between -0.4 and -2.5, mixing generic fillers (thing, maybe) with domain words.
  - [corpus] Weak direct support in neighbors; however, *SciTopic* (neighbor) similarly advances LLMs over embedding techniques for scientific topic discovery.
- Break condition: If the transcript vocabulary is highly domain-specific (e.g., obscure biotech jargon) not well-represented in the LLM's pre-training data, extraction quality may degrade to the level of unsupervised baselines.

### Mechanism 2
- Claim: Iterative validation and insertion into a tree structure maintains semantic ontology coherence.
- Mechanism: The Ontologist Agent prevents redundancy by checking semantic equivalence (e.g., "M&A" vs. "Mergers & Acquisitions") before insertion. It then traverses the tree top-down to place new nodes under the most granular valid parent, ensuring parent-child relationships are logically sound.
- Core assumption: Semantic similarity prompts effectively prevent distinct but related concepts from being merged (e.g., preventing "Product differentiation" from being merged into "Business Strategy" as an equivalent).
- Evidence anchors:
  - [abstract] ...validate and integrate them into the ontology... average parent-child cosine similarity of 0.383 versus 0.153 for random pairings.
  - [section] [Page 4] Ontologist agent guides relationships... semantic similarity prompt ensures precise equivalence rather than broader categorical matches.
  - [corpus] *A Hybrid AI Methodology for Generating Ontologies...* (neighbor) validates the feasibility of AI-driven taxonomy generation, though specific financial results are not cited there.
- Break condition: If the "Topic Existence" check fails to distinguish a specific new trend from a generic existing parent category, the ontology flattens, reducing granularity.

### Mechanism 3
- Claim: Frequency tracking of structured topics enables early detection of macro trends compared to lagging reports.
- Mechanism: Once topics are normalized (via aliases) and structured, the system counts mentions over time. Applying Kendall's tau to these time series identifies statistically significant monotonic trends (e.g., declining "Supply Chain" mentions) immediately after calls, rather than waiting for quarterly sector reports.
- Core assumption: Topic mention frequency correlates with business priority or operational reality, and is not just rhetorical noise.
- Evidence anchors:
  - [abstract] ...demonstrates trend analysis... successful identification of statistically significant trending topics.
  - [section] [Page 5] Identified "Supply Chain" declining in Semiconductors... validated against Deloitte's 2025 outlook noting supply chains worked well.
  - [corpus] *MiMIC* (neighbor) links earnings call data to stock prediction, implicitly supporting the value of signal extraction from calls, though this paper focuses on topic trends.
- Break condition: If companies engage in "greenwashing" or repetitive boilerplate language, frequency counts may decouple from actual strategic shifts.

## Foundational Learning

- Concept: **Hierarchical Topic Ontology**
  - Why needed here: Unlike flat lists, financial themes are nested (e.g., "Technology" -> "AI" -> "Generative AI"). Understanding tree traversal and parent-child inheritance is required to implement the Ontologist agent.
  - Quick check question: Can you explain why "Cost Reduction" should be a child of "Operational Metrics" rather than a sibling, and how cosine similarity validates this?

- Concept: **Prompt Engineering for Extraction vs. Reasoning**
  - Why needed here: The system relies on distinct prompts for different cognitive tasks: one for extracting topics (attention to detail) and one for semantic matching (logical reasoning). Confusing these leads to hallucinated nodes.
  - Quick check question: How would you constrain a prompt to output a valid JSON list of topics without including conversational pre-text?

- Concept: **Non-parametric Trend Testing (Kendall's Tau)**
  - Why needed here: Financial data is noisy and quarterly (small sample size). Understanding monotonic trends helps in distinguishing genuine strategic pivots from random fluctuations.
  - Quick check question: Why is Kendall's tau preferred over simple percentage change for detecting trends in small, ordinal datasets like quarterly earnings calls?

## Architecture Onboarding

- Component map: Topic Retriever -> Ontology DB -> Ontologist Agent
- Critical path: The **Topic Insertion Logic**. If the semantic match fails, the system creates duplicate nodes. If the parent-finding logic fails, the hierarchy becomes flat (all nodes at root). The "top-down" search for the most granular parent is the most fragile part of the pipeline.
- Design tradeoffs:
  - **Seed Topics vs. Open Discovery**: The authors used 32 seed topics to standardize the root. Removing these increases novelty discovery but risks a fragmented ontology with multiple roots for similar concepts.
  - **Tree vs. DAG**: The paper uses a Tree for simplicity but notes Directed Acyclic Graph (DAG) potential. A Tree forces a topic to have only one parent (e.g., "Batteries" is either under "EVs" or "Tech", not both), which limits cross-domain insight.
- Failure signatures:
  - **Topic Proliferation**: A sudden spike in unique nodes at the root level indicates the Semantic Matching prompt is too strict or the LLM is generating highly specific variations (e.g., "Cost Reduction Q1", "Cost Reduction Q2").
  - **Generic Clustering**: High similarity scores but low utility (e.g., everything clustered under "Financials") indicates the Insertion Logic is defaulting to broad parents.
- First 3 experiments:
  1. **Unit Test Semantic Matching**: Feed the Ontologist pairs like ("M&A", "Mergers & Acquisitions") and ("AI", "Technology") to verify it merges synonyms but keeps distinct categories separate.
  2. **LDA vs. LLM Benchmark**: Run the Topic Retriever and a standard LDA model on 5 transcripts; manually compare the "interpretability" of the top 10 topics.
  3. **Trend Validation**: Plot the frequency of "Supply Chain" for the EV sector (as done in the paper) and verify the slope matches the paper's findings before running the full pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can expert-annotated or crowd-sourced gold-standard datasets effectively benchmark emerging financial topic retrieval systems?
- Basis in paper: [explicit] The authors explicitly state there is a "dearth of benchmark datasets with gold labels for emerging financial topics," which limits the ability to benchmark the Topic Retriever and Ontology against existing baselines.
- Why unresolved: Developing such datasets requires significant resources and domain expertise (financial professionals) which were outside the scope of this specific study.
- What evidence would resolve it: The creation of a public, validated dataset containing earnings call transcripts with labeled emerging topics, followed by a comparative study of system performance against this gold standard.

### Open Question 2
- Question: Does enriching the LLM prompts with topic-specific excerpts reduce the noise caused by incorrect parent-child associations and alias misclassifications?
- Basis in paper: [explicit] The authors note that noise persists in the form of vague boundaries and suggest future work could "explore ways to enrich LLM prompts with additional contextâ€”such as topic-specific excerpts."
- Why unresolved: The current system relies primarily on topic names and the existing ontology structure for semantic matching, rather than utilizing the rich excerpt data generated by the retriever during the ontology update phase.
- What evidence would resolve it: An ablation study comparing the standard prompt against an excerpt-augmented prompt, measuring the reduction in manual corrections required by human analysts.

### Open Question 3
- Question: How does the system's performance and coherence change if the underlying topic topology is extended from a tree structure to a Directed Acyclic Graph (DAG)?
- Basis in paper: [inferred] The methodology section states the ontology is maintained as a "simple tree structure" but acknowledges it "can easily be extended to other topological structures (e.g., Directed Acyclic Graph)" to capture more complex relationships.
- Why unresolved: Implementing a DAG allows for multi-parent relationships (e.g., "AI" under both "Technology" and "Healthcare"), but the current system restricts topics to a single parent node.
- What evidence would resolve it: A comparative evaluation measuring structural coherence (cosine similarity) and retrieval utility between the current tree implementation and a DAG-based implementation on the same corpus.

## Limitations
- Assumes LLM pre-training data adequately captures emerging financial terminology
- Tree structure constrains topics to single inheritance, missing cross-domain relationships
- Frequency-based trend detection may not distinguish strategic shifts from rhetorical language

## Confidence
- Topic extraction quality: Medium - Supported by coherence metrics but limited by lack of direct LLM vs. baseline comparisons on identical datasets
- Ontology structure validity: High - Strong quantitative support (cosine similarity of 0.383 vs 0.153 for random pairs) and clear validation methodology
- Trend detection utility: Medium - Demonstrated on specific examples but limited to qualitative validation against external reports

## Next Checks
1. **Semantic Matching Robustness Test**: Feed 50 topic pairs with known relationships (synonyms, parent-child, unrelated) to verify the Ontologist agent correctly classifies each relationship type
2. **Cross-Domain Inheritance Analysis**: Re-run the pipeline with a DAG structure instead of a Tree for 100 transcripts and measure changes in cross-category topic discovery rates
3. **Frequency-to-Strategy Correlation Study**: Track 10 topics with known strategic shifts and measure correlation between mention frequency changes and subsequent operational/business metric changes over 12 months