---
ver: rpa2
title: Evaluating Multi-Turn Bargain Skills in LLM-Based Seller Agent
arxiv_id: '2509.06341'
source_url: https://arxiv.org/abs/2509.06341
tags:
- intent
- intents
- bargaining
- product
- buyer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a turn-level evaluation framework for measuring
  multi-turn bargaining ability in LLM-based seller agents, addressing the challenge
  of tracking and interpreting cumulative buyer intents across long negotiations.
  The framework includes a large-scale e-commerce benchmark with 622 categories, 9,892
  products, and 3,014 tasks, along with an automated pipeline that extracts high-quality
  intents from dialogue data.
---

# Evaluating Multi-Turn Bargain Skills in LLM-Based Seller Agent

## Quick Facts
- arXiv ID: 2509.06341
- Source URL: https://arxiv.org/abs/2509.06341
- Reference count: 11
- Primary result: Introduces turn-level evaluation framework for multi-turn bargaining ability in LLM-based seller agents using a three-level intent-action-tool hierarchy

## Executive Summary
This paper introduces a turn-level evaluation framework for measuring multi-turn bargaining ability in LLM-based seller agents, addressing the challenge of tracking and interpreting cumulative buyer intents across long negotiations. The framework includes a large-scale e-commerce benchmark with 622 categories, 9,892 products, and 3,014 tasks, along with an automated pipeline that extracts high-quality intents from dialogue data. The core method employs a three-level intent–action–tool hierarchy grounded in Theory of Mind, enabling fine-grained diagnosis of model performance.

## Method Summary
The framework uses a three-component pipeline: (1) Intent Factory extracts intent-action-tool hierarchy from 10k dialogues using a multi-agent pipeline (Extractor, Verifier, Maintainer) with qwen-plus-latest (~400M tokens); (2) Problem Weaver samples product-intent sequences and generates scripted buyer queries; (3) Evaluation Center executes scenarios and scores predictions against ground truth. The benchmark consists of 622 categories, 9,892 products, 3,014 tasks with an intent space of 17 intents, 39 actions, and 65 tools. Each task includes dialogue history, product metadata, and a 20-candidate intent choice space for the model to select from.

## Key Results
- GPT-5-chat and Qwen models achieve strong intent precision and stability across turns
- DeepSeek-V3-671B exhibits failure rates above 50%, while Gemini shows similar robustness issues
- The three-level intent-action-tool hierarchy successfully disentangles complex intents for fine-grained diagnosis
- Intent-F1 scores remain stable across turns 2-4, while failure rates vary significantly by model

## Why This Works (Mechanism)

### Mechanism 1
Decomposing negotiation goals into a three-level intent–action–tool hierarchy isolates specific failure modes in seller agents. The framework forces the model to map cumulative dialogue history to discrete tools. If the model fails to identify the correct tool despite correct high-level intent, it indicates a failure in atomic execution logic rather than general comprehension.

### Mechanism 2
Constraining model output to a choice space of 20 candidate intents reveals stability and robustness deficits better than open-ended generation. By presenting a fixed choice space per turn, the evaluation penalizes models that hallucinate options outside the valid intent schema, directly measuring the model's ability to adhere to business rules alongside accuracy.

### Mechanism 3
An automated multi-agent pipeline (Extractor, Verifier, Maintainer) ensures the intent hierarchy remains non-redundant and high-coverage. Raw dialogues are processed by an Extractor to surface candidates, a Verifier rejects near-duplicates to prevent category overlap, and a Maintainer clusters semantically similar nodes, preventing the benchmark from degrading into a chaotic label space.

## Foundational Learning

- **Concept: Theory of Mind (ToM) in Agents**
  - Why needed: The paper grounds its evaluation in ToM, requiring the seller agent to infer the buyer's mental state (intent) from implicit or explicit signals across multiple turns
  - Quick check: Can you distinguish between a buyer asking "Is this heavy?" (Implicit intent: Inquire_Shipping_Logistics) vs. "Does this weigh 5kg?" (Explicit intent: Query_Specification)?

- **Concept: Intent Orthogonality**
  - Why needed: To effectively evaluate an agent, the possible actions (tools) must be mutually exclusive (orthogonal). Without this, the agent could plausibly argue that a wrong answer was "semantically close"
  - Quick check: If a user says "Will you take $50?", does this map exclusively to `Propose_Counteroffer`, or could it also be `Inquire_Price_Elasticity`?

- **Concept: Turn-Level vs. Outcome-Level Evaluation**
  - Why needed: Traditional benchmarks measure if a deal was closed (outcome). This framework measures how the agent navigated the conversation (turn-level), penalizing "lucky wins" and rewarding consistent reasoning
  - Quick check: If an agent insults the buyer but still makes a sale, should it score higher or lower than an agent that fails to sell but perfectly identifies all buyer constraints?

## Architecture Onboarding

- **Component map**: Intent Factory (Extractor -> Verifier -> Maintainer) -> Problem Weaver (Product Info + Intent Space) -> Scripted Dialogues -> Evaluation Center (Target LLM + Dialogues -> LLM Choice vs Ground Truth -> Intent-F1 & Failure Rate)

- **Critical path**: The flow relies on the Problem Weaver producing synthetic buyer queries that are realistic enough to trigger the specific ground-truth intents. If the synthetic questions are unnatural, the LLM's failure may be due to data quality, not model capability.

- **Design tradeoffs**: 
  - Synthetic vs. Real: The framework uses LLM-generated synthetic dialogues for scale and annotation, potentially sacrificing the natural messiness of human buyers
  - Fixed Choice vs. Open Generation: The 20-candidate choice space simplifies grading but restricts the model's ability to express nuanced or novel intents not in the pre-defined pool

- **Failure signatures**:
  - High Failure Rate (>50%): Indicates the model cannot follow the output schema or maps dialogue to non-existent intents (hallucination)
  - High Recall / Low Precision: Model is "trigger happy," selecting many intents to cover bases, suggesting poor discrimination
  - Drop in Turn-4+ Performance: Indicates an inability to track cumulative context (memory/attention limits)

- **First 3 experiments**:
  1. Baseline Stability Test: Run a target model on Turn-2 tasks specifically to measure "Failure Rate." If >5%, prompt engineering or schema definition is the blocker, not intent reasoning
  2. Intent Confusion Matrix: Evaluate the model on "Ambiguous Intents" vs. "Structured Intents" to determine if the hierarchy needs refinement
  3. Context Ablation: Evaluate performance using only the current turn vs. full history to quantify the specific contribution of "cumulative tracking" to the model's F1 score

## Open Questions the Paper Calls Out

1. **Special Cases**: How can the framework be extended to evaluate "special cases" where the seller agent must identify and refuse buyer requests that exceed its capabilities? The current benchmark focuses on standard bargaining interactions and does not test the agent's ability to recognize and handle out-of-scope or unsafe requests.

2. **Three-Party Dialogues**: How does intent-tracking performance change in "agent cases" involving three-party dialogues where a human seller intervenes? The current study limits evaluation to buyer-agent dyads and does not model the state updates required when the principal (human seller) re-enters the loop.

3. **Domain Generalization**: Can the Theory of Mind-grounded intent–action–tool hierarchy be effectively adapted to complex social domains like diplomacy or persuasion? The current validation is restricted to e-commerce bargaining; it is unproven whether this hierarchy captures the nuance of non-transactional social interactions.

4. **Model Robustness Analysis**: What specific architectural or training factors cause large models like DeepSeek-V3-671B to collapse with high failure rates in multi-turn intent tracking? The paper identifies the symptom but does not isolate whether the cause is context window management, instruction following, or reasoning limitations.

## Limitations

- The framework's reliance on synthetic dialogue generation may introduce realism gaps that don't capture nuanced human bargaining behaviors
- The choice of 20 candidate intents per turn may artificially constrain models and not reflect real-world uncertainty in intent recognition
- The automated pipeline, while ensuring scale, may miss subtle bargaining behaviors that emerge in organic conversations

## Confidence

**High confidence**: The turn-level evaluation methodology and three-level intent-action-tool hierarchy are technically sound and well-grounded in Theory of Mind principles. The observed performance differences between models are consistent with known model capabilities.

**Medium confidence**: The automated pipeline's ability to capture all relevant bargaining intents without redundancy, and the sufficiency of the 20-candidate choice space for challenging discrimination.

**Low confidence**: The synthetic dialogue generation process's ability to fully represent the complexity of real human-to-human bargaining scenarios, and whether the framework generalizes beyond second-hand marketplace contexts.

## Next Checks

1. **Human Evaluation Validation**: Have human annotators evaluate a subset of model predictions to measure alignment between automated and human judgment of intent accuracy, particularly for ambiguous cases where the choice space may not contain the true intent.

2. **Domain Transfer Test**: Apply the framework to a different bargaining domain (e.g., real estate negotiations or job offer discussions) to assess generalizability of the intent-action-tool hierarchy and whether the same performance patterns hold.

3. **Real Dialogue Benchmark**: Compare model performance on the synthetic benchmark against a small set of real recorded bargaining conversations to quantify the realism gap and identify systematic differences in failure modes.