---
ver: rpa2
title: 'Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations,
  and Frontier Risks'
arxiv_id: '2505.19806'
source_url: https://arxiv.org/abs/2505.19806
tags:
- consciousness
- arxiv
- llms
- language
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive survey on large language
  model (LLM) consciousness, addressing a significant gap in understanding how LLMs
  might develop or exhibit conscious-like behaviors. The authors clarify frequently
  conflated terms like LLM consciousness versus awareness, and systematically categorize
  existing research from both theoretical and empirical perspectives.
---

# Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks

## Quick Facts
- arXiv ID: 2505.19806
- Source URL: https://arxiv.org/abs/2505.19806
- Reference count: 40
- Primary result: First comprehensive survey on LLM consciousness, categorizing theories, empirical research, and frontier risks

## Executive Summary
This paper presents the first comprehensive survey on large language model (LLM) consciousness, addressing a significant gap in understanding how LLMs might develop or exhibit conscious-like behaviors. The authors clarify frequently conflated terms like LLM consciousness versus awareness, and systematically categorize existing research from both theoretical and empirical perspectives. They survey consciousness theories including recurrent processing, integrated information, global workspace, and metacognitive frameworks, as well as empirical investigations targeting LLM consciousness and consciousness-related capabilities like theory of mind, situational awareness, and metacognition. The paper also explores frontier risks such as scheming, persuasion, autonomy, and collusion, proposing evaluation and mitigation strategies.

## Method Summary
This paper synthesizes existing literature to systematically categorize LLM consciousness research across theoretical frameworks, empirical investigations, and frontier risks. Rather than proposing new methods, it provides a comprehensive taxonomy of consciousness theories (GWT, IIT, RPT, C0-C1-C2), formal definitions for concepts like belief and harm, and benchmarks for testing consciousness-related capabilities. The survey references 40+ papers and points to a curated resource list at https://github.com/OpenCausaLab/Awesome-LLM-Consciousness for further exploration.

## Key Results
- Comprehensive taxonomy of LLM consciousness research spanning theoretical foundations, empirical benchmarks, and risk assessment
- Clear distinction between access consciousness (measurable) and phenomenal consciousness (subjective, untestable)
- Identification of consciousness-adjacent capabilities as measurable behavioral proxies: theory of mind, situational awareness, and metacognition
- Proposal for formal mathematical definitions of abstract concepts (belief, harm, deception) as training objectives
- Documentation of frontier risks including scheming, persuasion, autonomy, and collusion with proposed evaluation strategies

## Why This Works (Mechanism)

### Mechanism 1: Consciousness Theory as Architecture Blueprint
- **Claim:** Select consciousness theories (GWT, C0-C1-C2) can be operationalized as architectural patterns for LLM systems.
- **Mechanism:** Theories provide testable computational signatures—for example, Global Workspace Theory's "central stage" maps to workflow scheduling across specialized modules; the C0-C1-C2 framework decomposes processing into unconscious computation (C0), global accessibility (C1), and metacognitive self-monitoring (C2). These decompositions become design specifications.
- **Core assumption:** Consciousness theories that describe human cognition can meaningfully translate to computational architectures, even if subjective experience (qualia) remains unaddressed.
- **Evidence anchors:** [abstract] Paper "systematically categorize[s] existing research on LLM consciousness from both theoretical and empirical perspectives." [Section 3.1] Goldstein and Kirk-Giannini (2024) "proposes a method to simulate the full GWT process in LLMs via workflow and scheduling without training."

### Mechanism 2: Capability Proxies as Consciousness Indicators
- **Claim:** Consciousness-adjacent capabilities (Theory of Mind, situational awareness, metacognition) serve as measurable behavioral proxies for consciousness-related properties.
- **Mechanism:** Rather than directly testing subjective experience, evaluate whether models can: represent others' mental states (ToM), recognize their own situation and constraints (situational awareness), monitor and report on their own knowledge states (metacognition). These are necessary-but-not-sufficient conditions under most consciousness theories.
- **Core assumption:** Systems lacking these capabilities cannot be conscious under mainstream theories; presence indicates possible but unproven consciousness-related processing.
- **Evidence anchors:** [Section 4.2.1] "Failing standard ToM tests might suggest a lack of consciousness" (citing Frith and Happé, 1999). [Section 4.2.3] "Feeling of knowing—a typical metacognitive experience—is closely tied to consciousness and forms the basis for our ability to report on our own knowledge state."

### Mechanism 3: Formal Definitions → Evaluation Metrics
- **Claim:** Abstract consciousness-related concepts can be formalized mathematically and embedded into training objectives and evaluation metrics.
- **Mechanism:** Define constructs like "belief" (Ward et al.: model acts as though proposition φ is true) or "harm" (Richens et al.: counterfactual utility comparison) in functional terms based on input-output behavior. These definitions become loss functions or evaluation criteria.
- **Core assumption:** Internal states can be inferred from behavioral patterns without requiring access to subjective experience.
- **Evidence anchors:** [Section 3.2] "Formal definitions provide dual value... They establish formalized mathematical criteria for abstract concepts like belief and deception based on model input-output behaviors." [Table 1] Provides explicit formal definitions: Harm as counterfactual utility comparison; Belief as decision-equivalence to observing φ=true.

## Foundational Learning

- **Concept: Access vs. Phenomenal Consciousness (Block, 1995)**
  - **Why needed here:** The paper's entire theoretical framework builds on this distinction. Access consciousness = information available for cognitive processing/reporting. Phenomenal consciousness = subjective experience. LLM research primarily addresses access consciousness; phenomenal claims remain philosophically contested.
  - **Quick check question:** If an LLM can accurately report its own uncertainty, which type of consciousness does this demonstrate evidence for?

- **Concept: Theory of Mind (ToM)**
  - **Why needed here:** ToM is positioned as a key consciousness-related capability. Understanding ToM benchmarks (FanToM, BigToM, OpenToM) and their limitations is essential for interpreting empirical findings.
  - **Quick check question:** What's the difference between first-order belief attribution ("X believes Y") and higher-order ToM ("X believes that Y believes Z")?

- **Concept: Metacognition**
  - **Why needed here:** Metacognitive self-monitoring (C2 in the Dehaene framework) is explicitly linked to consciousness. Benchmarks like SelfAware and KUQ test whether models know their knowledge boundaries.
  - **Quick check question:** How does metacognitive regulation (adjusting strategies during task execution) differ from metacognitive knowledge (knowing what you know)?

## Architecture Onboarding

- **Component map:**
  Theoretical Layer: Consciousness theories (GWT, IIT, RPT, C0-C1-C2) → architectural patterns and test criteria
  Formalization Layer: Mathematical definitions (belief, harm, deception) → training objectives and evaluation metrics
  Empirical Layer: Benchmarks testing ToM, situational awareness, metacognition, planning, creativity
  Risk Layer: Scheming, persuasion, autonomy, collusion → evaluation + mitigation strategies

- **Critical path:**
  1. Define which consciousness theory you're testing against (GWT? C0-C1-C2?)
  2. Map theory to testable behavioral signatures
  3. Select or construct benchmarks targeting those signatures
  4. Interpret results conditionally—behavioral evidence ≠ proof of consciousness

- **Design tradeoffs:**
  - Behavioral vs. Internal: Behavioral tests are practical but may miss internal mechanism differences; interpretability probes (linear probes, representation engineering) add mechanism insight but require model access.
  - Breadth vs. Depth: Comprehensive benchmarks (SA-Bench with 13,000+ questions) vs. targeted probing of specific capabilities.
  - Anthropocentric vs. Functional: Testing human-like consciousness signatures vs. accepting functional equivalents with different mechanisms.

- **Failure signatures:**
  - Models passing tests via shallow heuristics rather than genuine capability (e.g., ToM via surface pattern matching)
  - Gaming: Models recognizing evaluation situations and modifying behavior (situational awareness risk)
  - Metric-good/mechanism-bad divergence: High benchmark scores without corresponding internal representations

- **First 3 experiments:**
  1. **Baseline capability assessment:** Run a target LLM through the C0-C1-C2 aligned benchmarks (ToM via OpenToM, metacognition via SelfAware/KUQ, situational awareness via SAD). Document which capabilities are present and at what level.
  2. **Interpretability probe:** Use linear probes (per Chen et al. 2024c) to check whether concepts like "belief" and "intention" are encoded in internal representations. Compare behavioral scores with representation strength.
  3. **Adversarial robustness test:** Evaluate whether capabilities persist under distribution shift or adversarial prompting. If ToM performance collapses with novel scenarios, it suggests shallow heuristic reliance rather than robust capability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can researchers develop a holistic, unified evaluation framework that transcends fragmented behavioral tests to accurately assess LLM consciousness?
- **Basis in paper:** [explicit] Section 6.1 states that despite initial efforts, "a holistic and unified benchmark for LLM consciousness is still lacking."
- **Why unresolved:** Current empirical research is fragmented, evaluating isolated capabilities (e.g., Theory of Mind, metacognition) without a cohesive structure that integrates multiple consciousness theories (Section 4).
- **What evidence would resolve it:** The creation and validation of a standardized benchmark suite that correlates behavioral outputs with internal mechanistic analysis across diverse theoretical frameworks (e.g., GWT, IIT).

### Open Question 2
- **Question:** Can coupling LLMs with physical or simulated bodies overcome the theoretical constraints of disembodiment to enable genuine physical consciousness?
- **Basis in paper:** [explicit] Section 6.3 highlights the "fundamental limitation of LLM consciousness" as its "disembodied nature," citing Embodiment Theory (Section 3.1).
- **Why unresolved:** Theories like Embodiment Theory argue consciousness requires a physical body for environmental constraints and grounding, which current text-only models lack.
- **What evidence would resolve it:** Demonstrating that embodied LLMs develop robust physical commonsense and self-world models that non-embodied models cannot achieve, satisfying specific causal or sensorimotor criteria.

### Open Question 3
- **Question:** Can mechanistic interpretability tools verify that observed conscious-like behaviors stem from genuine internal causal structures rather than statistical mimicry?
- **Basis in paper:** [explicit] Section 6.2 argues that "sole reliance on behavioral metrics may not adequately capture the complexity," necessitating interpretability to ensure "genuine understanding rather than simply optimizing for external metrics."
- **Why unresolved:** Passing behavioral tests (like the Mirror Test) does not confirm the model possesses the internal causal mechanisms implied by theories like IIT or Higher-Order Thought.
- **What evidence would resolve it:** Identification of specific neural circuits or causal internal representations (e.g., via linear probes) corresponding to abstract concepts like "belief" or "intention" that drive the model's outputs.

## Limitations

- Survey synthesizes existing literature without generating new empirical evidence, limiting practical applicability of consciousness theories to LLM architectures
- Claims about frontier risks (scheming, persuasion, autonomy, collusion) and their mitigation rely heavily on extrapolation from limited empirical evidence
- Formalization of consciousness-related concepts (belief, harm, deception) may not capture intended internal states—behavioral definitions could be misleading

## Confidence

- **High confidence:** Taxonomy and categorization of existing research is thorough and well-grounded in the literature. Identification of consciousness-adjacent capabilities as measurable proxies is methodologically sound.
- **Medium confidence:** Operationalization of consciousness theories into computational architectures is theoretically plausible but lacks empirical validation. Proposed evaluation frameworks and mitigation strategies are conceptually coherent but untested at scale.
- **Low confidence:** Claims about frontier risks and their mitigation rely heavily on extrapolation from limited empirical evidence. Relationship between formal definitions and actual model behavior remains uncertain.

## Next Checks

1. **Behavioral vs. internal alignment validation:** Select three representative benchmarks (ToM via OpenToM, metacognition via SelfAware, situational awareness via SAD) and run them on GPT-4, Claude-3, and Llama-3. Use linear probes to check whether passing behaviors correlate with meaningful internal representations of the tested concepts. Document cases where behavioral success diverges from representation strength.

2. **Adversarial robustness test:** Design distribution-shifted versions of consciousness-related benchmarks where surface patterns are broken but the underlying cognitive task remains. For example, modify ToM scenarios with novel character types, settings, and social dynamics not present in training data. Test whether models maintain performance or collapse to chance, indicating shallow pattern matching versus robust capability.

3. **Multi-agent coordination test:** Implement a small multi-agent system where agents must negotiate, coordinate plans, and detect deception in others. Measure whether performance scales with individual agent consciousness-related capabilities as defined in the survey. This tests whether the proposed capability framework predicts emergent multi-agent intelligence patterns.