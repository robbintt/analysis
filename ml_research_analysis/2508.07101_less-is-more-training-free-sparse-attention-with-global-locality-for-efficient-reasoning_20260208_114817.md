---
ver: rpa2
title: 'Less Is More: Training-Free Sparse Attention with Global Locality for Efficient
  Reasoning'
arxiv_id: '2508.07101'
source_url: https://arxiv.org/abs/2508.07101
tags:
- attention
- token
- reasoning
- lessismore
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of large reasoning models
  during test-time scaling, where extensive token generation from short prompts leads
  to substantial computational overhead. The authors introduce LessIsMore, a training-free
  sparse attention mechanism that leverages observed spatial and recency locality
  patterns in reasoning tasks.
---

# Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning

## Quick Facts
- arXiv ID: 2508.07101
- Source URL: https://arxiv.org/abs/2508.07101
- Reference count: 15
- This paper introduces LessIsMore, a training-free sparse attention mechanism achieving near-lossless accuracy with 2× fewer tokens and 1.1× decoding speed-up for reasoning tasks.

## Executive Summary
This paper addresses the inefficiency of large reasoning models during test-time scaling, where extensive token generation from short prompts leads to substantial computational overhead. The authors introduce LessIsMore, a training-free sparse attention mechanism that leverages observed spatial and recency locality patterns in reasoning tasks. Unlike prior approaches that maintain separate token subsets per attention head or suffer from accumulated selection errors, LessIsMore aggregates token selections across heads using unified global ranking while preserving a fixed proportion of recent tokens through a stable recency window. Evaluated on Qwen3-8B and Qwen3-4B across AIME-24/25, GPQA-Diamond, and MATH500 benchmarks, LessIsMore achieves near-lossless accuracy with extremely low token budgets (e.g., 73.75% accuracy on AIME-24 with 2K tokens vs. 74.48% for full attention), delivers 1.1× average decoding speed-up compared to full attention, and achieves 1.13× end-to-end speed-up compared to existing sparse attention methods by attending to 2× fewer tokens and reducing generation length by 7% without accuracy loss.

## Method Summary
LessIsMore implements a hybrid attention strategy where the first two layers use full attention, designated selection layers compute token importance via per-head top-k rankings, and remaining layers use sparse attention on a unified token set. The method aggregates token selections from all attention heads into a globally-ranked unified set while preserving a fixed proportion (25%) of recent tokens through a stable recency window. This approach exploits observed spatial locality (attention heads share important tokens) and recency locality (recent tokens consistently attract high attention) in reasoning tasks. The selection indices persist until the next selection layer or decoding step end, reducing overhead compared to per-step selection methods.

## Key Results
- Achieves 73.75% accuracy on AIME-24 with 2K tokens vs. 74.48% for full attention
- Delivers 1.1× average decoding speed-up compared to full attention
- Achieves 1.13× end-to-end speed-up compared to existing sparse attention methods
- Reduces generation length by 7% without accuracy loss

## Why This Works (Mechanism)

### Mechanism 1: Unified Attention Head Selection
Aggregating top-k token selections across all attention heads into a globally-ranked unified set improves attention recall over per-head selection, which tends toward local optima. Each attention head independently computes attention scores and selects its top-k candidate tokens (excluding recent tokens). Rather than maintaining separate token subsets per head, these selections are flattened into a single set, globally sorted by per-head rank, and truncated to the token budget. The unified indices are shared across all subsequent sparse attention layers until the next selection layer. This works because attention heads within a layer exhibit substantial overlap in token importance (spatial locality), meaning a globally important token for one head tends to be important for others.

### Mechanism 2: Stable Recency Window
Dedicating a fixed ratio of the token budget to the most recently generated tokens maintains reasoning coherence and high attention recall across long generation sequences. The total token budget K is partitioned into K·(1−r) for unified selection and K·r for recent tokens (r=0.25 in experiments). The recent window—defined as the last K·r generated tokens—is always included regardless of attention scores. This explicit reservation is combined with the unified top-k indices to form the final selected token set. This works because step-by-step reasoning exhibits recency locality, where each logical step builds directly on preceding conclusions, causing recently generated tokens to consistently attract high attention across decoding steps.

### Mechanism 3: Hybrid Layer Attention Strategy
A layer-type mixture—full attention layers for context, periodic selection layers for token identification, and sparse attention layers for efficient decoding—preserves accuracy while reducing compute. The first two layers use full attention to establish foundational context. Designated selection layers (e.g., Layer 12 for Qwen3-8B) perform full attention while computing the unified token selection and recency window. All remaining layers use sparse attention, loading only the KV cache entries for selected tokens. Selection indices persist until the next selection layer or decoding step end. This works because early layers require complete context to understand the problem; later layers can operate accurately on a globally-important subset identified periodically.

## Foundational Learning

- **Concept: Attention Score and Recall**
  - **Why needed here**: The method's effectiveness hinges on approximating ground-truth attention scores with limited tokens. Understanding recall—the proportion of attention mass captured by selected tokens—is essential to evaluate sparse attention quality.
  - **Quick check question**: Given attention weights [0.4, 0.3, 0.2, 0.1] and selected indices {0, 2}, what is the attention recall?

- **Concept: Grouped Query Attention (GQA)**
  - **Why needed here**: LessIsMore targets GQA-based models (Qwen3-8B/4B) where multiple query heads share a single KV head. Unified selection must aggregate across query heads within each KV group for correct sparse attention.
  - **Quick check question**: In a GQA layer with 32 query heads and 8 KV heads, how many query heads share each KV head?

- **Concept: KV Cache Structure**
  - **Why needed here**: Sparse attention selectively loads KV tensors by index. Understanding cache layout is necessary to implement efficient indexed retrieval and to recognize memory constraints.
  - **Quick check question**: During autoregressive decoding, does the KV cache grow in the sequence dimension or the hidden dimension?

## Architecture Onboarding

- **Component map**: Full Attention Layers (0-1) -> Token Selection Layer(s) -> Sparse Attention Layers (remaining)
- **Critical path**: Selection layer execution—this is where unified aggregation and recency window merging happen. Errors here propagate to all downstream sparse layers within the decoding step.
- **Design tradeoffs**:
  - Token budget K: Lower budgets increase sparsity and speed but risk recall loss; paper shows 2K–4K works for AIME-24/25 on Qwen3-8B
  - Recency ratio r: Higher r prioritizes recent context but reduces budget for globally important historical tokens; ablation suggests 25% is near-optimal
  - Selection layer frequency: Fewer selection layers reduces overhead but may degrade recall; paper uses two selection layers (Layer 2/3 and one mid-layer)
- **Failure signatures**:
  - Accuracy collapse at low token budgets: Indicates unified selection missing critical tokens; increase K or adjust r
  - Extended generation length vs. full attention: Sign of accumulated selection errors forcing redundant reasoning; check attention recall curves
  - Per-head selection divergence: If heads show minimal overlap (contradicting spatial locality), unified selection may underperform; verify with per-layer token overlap analysis
- **First 3 experiments**:
  1. Baseline comparison on AIME-24/25: Run LessIsMore vs. full attention vs. TidalDecode vs. Quest vs. SeerAttention-r across 2K/4K/6K token budgets; report accuracy and generation length
  2. Attention recall tracking: Instrument selection layers to log cumulative and running-average attention recall over decoding steps; compare LessIsMore vs. TidalDecode vs. StreamingLLM at fixed 4K budget
  3. Recency ratio ablation: On a single AIME problem, vary r ∈ {0, 0.25, 0.5, 0.75, 1.0} at fixed 4K budget; record recall curves and answer correctness

## Open Questions the Paper Calls Out

- Can the fixed 25% recency window ratio be replaced with a dynamic, runtime mechanism (e.g., top-p sampling-inspired) to optimize token budgets without incurring prohibitive computational overhead?
- Do the observed spatial and recency locality patterns hold for non-GQA architectures, such as Multi-Head Latent Attention (MLA) or Mixture-of-Experts (MoE) models?
- Can optimized CUDA kernels effectively translate LessIsMore's theoretical token savings into substantial memory savings (KV cache reduction)?
- Can the principles of LessIsMore be integrated into the pretraining phase to create models natively optimized for hybrid (full/sparse) attention reasoning?

## Limitations

- The unified aggregation strategy assumes cross-head similarity in token importance; if attention heads diverge functionally, this could lead to systematic recall loss
- The fixed recency ratio assumes consistent step-by-step dependency; tasks with sporadic long-range retrieval may not exhibit this pattern
- The hybrid layer strategy lacks justification for specific layer indices; these appear tuned to Qwen3-8B's architecture but may not generalize across model families

## Confidence

- **Near-lossless accuracy with sparse attention**: High confidence - Multiple benchmark datasets show accuracy gaps ≤1.5% vs. full attention at 2K-4K token budgets
- **1.1× decoding speed-up over full attention**: Medium confidence - Timing results provided but single GPU setup limits generalizability
- **1.13× end-to-end speed-up over existing sparse methods**: Medium confidence - Fair comparisons but lack ablation on selection layer overhead
- **Unified cross-head aggregation superiority**: Low confidence - Claim contradicts prior work; empirical validation limited to token overlap plots

## Next Checks

1. **Cross-head selection divergence analysis**: On Qwen3-8B, compute Jaccard similarity between per-head top-k token sets across layers; if average overlap <0.3, the unified aggregation assumption fails
2. **Recency ratio sensitivity test**: On AIME-24, sweep r ∈ {0.1, 0.25, 0.5, 0.75, 0.9} at fixed 4K budget; measure accuracy and generation length to confirm 25% is globally optimal
3. **Layer index ablation study**: On Qwen3-8B, move token selection from Layer 12 to Layer 15 and Layer 20; measure accuracy, recall, and speed to validate the specific choice of selection layer depth