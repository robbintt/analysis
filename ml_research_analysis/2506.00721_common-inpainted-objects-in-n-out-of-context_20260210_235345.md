---
ver: rpa2
title: Common Inpainted Objects In-N-Out of Context
arxiv_id: '2506.00721'
source_url: https://arxiv.org/abs/2506.00721
tags:
- objects
- context
- object
- image
- out-of-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COinCO, a novel dataset addressing the scarcity
  of out-of-context examples in vision datasets. By systematically replacing objects
  in COCO images through diffusion-based inpainting, the authors create 97,722 unique
  images featuring both contextually coherent and inconsistent scenes.
---

# Common Inpainted Objects In-N-Out of Context

## Quick Facts
- arXiv ID: 2506.00721
- Source URL: https://arxiv.org/abs/2506.00721
- Reference count: 40
- Primary result: COinCO dataset creates 97,722 images with systematically inpainted objects classified as in- or out-of-context, enabling new tasks in context classification, Objects-from-Context prediction, and context-enhanced fake detection.

## Executive Summary
This paper introduces COinCO, a novel dataset addressing the scarcity of out-of-context examples in vision datasets. By systematically replacing objects in COCO images through diffusion-based inpainting, the authors create 97,722 unique images featuring both contextually coherent and inconsistent scenes. Each inpainted object is meticulously verified and categorized as in- or out-of-context through a multimodal large language model assessment.

The paper demonstrates three key tasks enabled by COinCO: (1) training context classifiers that effectively determine whether existing objects belong in their context, (2) a novel Objects-from-Context prediction task that determines which new objects naturally belong in given scenes at both instance and clique levels, and (3) context-enhanced fake detection on state-of-the-art methods without fine-tuning. The analysis reveals significant patterns in semantic priors that influence inpainting success across object categories. The dataset provides a controlled testbed with contextual variations, establishing a foundation for advancing context-aware visual understanding in computer vision and image forensics.

## Method Summary
The authors create COinCO by systematically inpainting one object per COCO image using Stable Diffusion 2. For each image, they randomly select an object, dilate its mask, create an inpainting bounding box, and replace the object with a different COCO category. Inpainted objects are verified using YOLOv8x detection (with up to 3 retry rounds and Molmo fallback), then classified as in- or out-of-context using a multimodal LLM that analyzes location, size, and co-occurrence criteria. The resulting dataset contains 97,722 images (95,320 train, 2,402 test) with detailed annotations for each inpainted object and its context label.

## Key Results
- Created 97,722 unique images by systematically inpainting objects across COCO dataset
- Demonstrated context classifiers achieving strong performance on determining object-context coherence
- Showed context-enhanced fake detection improves AUC scores on state-of-the-art methods without fine-tuning
- Revealed significant semantic priors in inpainting success rates across object categories (p < 0.001)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Systematic object replacement via diffusion-based inpainting creates controlled contextual variations while preserving scene structure.
- **Mechanism:** For each COCO image, randomly select one object → dilate and bound its mask → inpaint using Stable Diffusion 2 with a different COCO category as prompt → crop-scale-merge pipeline for small objects. Verification via YOLOv8x detection (3 retry rounds) with Molmo as fallback.
- **Core assumption:** The diffusion model will generate recognizable instances of the target category within the masked region when prompted.
- **Evidence anchors:**
  - [abstract] "By systematically replacing objects in COCO images through diffusion-based inpainting, we create 97,722 unique images"
  - [section 3.1] "For each COCO image, we randomly select an object. The object's mask is slightly dilated and enclosed in a bounding box, which is used as the inpainting mask."
  - [corpus] Related work COCO-Inpaint (arXiv:2504.18361) also uses COCO for inpainting benchmarks, validating this foundation.
- **Break condition:** Inpainting success rate varies significantly by semantic category—furniture/household objects fail more often than animals/food due to context-dependence (Section 3.2, Figure 3a).

### Mechanism 2
- **Claim:** Multimodal LLMs can perform structured context reasoning using location, size, and co-occurrence criteria to classify objects as in- or out-of-context.
- **Mechanism:** Molmo receives inpainted image + structured prompt asking for analysis across three principles (location: spatial reasonableness; size: proportionality; co-occurrence: typical co-presence). Object classified out-of-context if any single criterion is violated.
- **Core assumption:** MLLM visual-semantic reasoning aligns sufficiently with human contextual judgment for reliable labeling.
- **Evidence anchors:**
  - [abstract] "Each inpainted object is meticulously verified and categorized as in- or out-of-context through a multimodal large language model assessment"
  - [section 3.3] "To label context accordingly, we leverage the power of Molmo... Provided with an inpainted image and context criteria, Molmo can conduct holistic visual-semantic reasoning"
  - [section 4.2] Molmo aligned with human annotations in 77.57% of verified cases (370/477 instances where annotators agreed).
  - [corpus] E2LVLM paper (arXiv:2502.10455) shows LVLMs for OOC misinformation detection, supporting MLLM context reasoning viability.
- **Break condition:** Context classification remains "inherently subjective"—human annotators and MLLMs occasionally disagree on coherence (Section 6).

### Mechanism 3
- **Claim:** Contextual priors can enhance fake localization without fine-tuning by amplifying detection scores in out-of-context regions.
- **Mechanism:** Given fake detector prediction P(x,y), identify out-of-context object masks MOC via context reasoning → compute enhanced score P'(x,y) = min(P(x,y) × γ, 1.0) if pixel in MOC, else P(x,y). Enhancement factor γ=1.5 applied.
- **Core assumption:** Out-of-context objects have higher prior probability of being manipulated; context signal complements pixel-level artifact detection.
- **Evidence anchors:**
  - [abstract] "context-enhanced fake detection on state-of-the-art methods without fine-tuning"
  - [section 5.3] "Figure 6 shows how our context-enhancement improves fake localization for all SOTA detectors without any finetuning" with AUC improvements shown.
  - [corpus] AI-Generated Image Detectors paper (arXiv:2602.00192) notes detectors overrely on global artifacts, supporting the need for complementary context signals.
- **Break condition:** If context classifier misidentifies authentic objects as out-of-context, enhancement could increase false positives—mitigated by focusing on physical violations (size/location) rather than co-occurrence in practical setting.

## Foundational Learning

- **Concept: Contextual coherence principles (Biederman et al.)**
  - Why needed here: The entire dataset taxonomy depends on these three criteria—location, size, co-occurrence—for defining in- vs out-of-context.
  - Quick check question: Given an image of a kitchen, would a surfboard on the counter violate location, size, or co-occurrence?

- **Concept: Diffusion inpainting mechanics**
  - Why needed here: Understanding how Stable Diffusion inpainting works (VAE encoder, latent space, denoising) is prerequisite for debugging inpainting failures and understanding semantic priors.
  - Quick check question: Why might inpainting fail more often for context-dependent objects like furniture vs. portable objects like animals?

- **Concept: MLLM structured prompting**
  - Why needed here: The quality of context labels depends entirely on prompt design that elicits reasoning before classification.
  - Quick check question: What would happen if the prompt asked only for a binary decision without requiring analysis of each criterion?

## Architecture Onboarding

- **Component map:**
  COCO Image → Object Selector → Mask Generator (dilation + bbox)
           ↓
  Stable Diffusion 2 Inpainting ← Category Prompt
           ↓
  Verification Cascade: YOLOv8x → (fail) → Molmo visual check → (fail) → regenerate (max 3 rounds)
           ↓
  Context Labeling: Molmo with structured prompt → in-context / out-of-context
           ↓
  COinCO Dataset (97,722 images with original/inpainted object info + context label)

- **Critical path:** The verification cascade (Section 3.2) is the bottleneck—failed inpaintings require regeneration, and semantic priors cause category-dependent success rates. Monitor: inpainting success by category (Figure 3a patterns).

- **Design tradeoffs:**
  - Same-category replacement would ensure inpainting success but yields no contextual diversity
  - Open-vocabulary objects would increase diversity but break Objects-from-Context task (limited to 80 COCO classes)
  - MLLM labeling scales better than human annotation but introduces ~22% disagreement with human judgment

- **Failure signatures:**
  - Low inpainting success for furniture/appliance categories (context-dependent)
  - High visual-only classifier recall but low precision (predicts everything out-of-context)
  - Context enhancement causing false positives if co-occurrence criterion used in practical setting

- **First 3 experiments:**
  1. Replicate inpainting success rate matrix (Figure 3a) on a 1000-image sample to validate semantic prior patterns before full dataset generation.
  2. Test Molmo context classification agreement rate on 100 manually labeled images to calibrate expected label noise.
  3. Implement context enhancement (γ=1.5) with oracle out-of-context masks on a pretrained fake detector to establish upper bound before building practical context classifier.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the Objects-from-Context prediction task be effectively extended to an open-vocabulary setting that predicts object categories beyond COCO's 80 predefined classes?
  - Basis in paper: [explicit] The authors state in Section 6: "The Objects-from-Context prediction task is currently limited to COCO's predefined categories. Future work could explore extending these tasks to open-vocabulary settings for more flexible context reasoning."
  - Why unresolved: The current model architecture outputs predictions across a fixed set of 80 classes, and the training data and evaluation protocol are both tied to COCO categories.
  - What evidence would resolve it: A model trained or adapted for open-vocabulary prediction, evaluated on a dataset containing objects outside COCO's taxonomy, showing reasonable top-k accuracy.

- **Open Question 2:** What mechanisms underlie the semantic prior bias in diffusion-based inpainting success, and can these biases be mitigated?
  - Basis in paper: [inferred] Section 3.2 reports significant differences in inpainting success across supercategories (p < 0.001), with animals/food succeeding more often than electronics/appliances/furniture. The authors state this is "the first thorough analysis of semantic priors in diffusion-based inpainting tasks" but do not propose methods to reduce the bias.
  - What evidence would resolve it: Experiments using modified inpainting prompts, conditioning strategies, or fine-tuned models that narrow the success rate gap between high-performing and low-performing categories.

- **Open Question 3:** Can the agreement between MLLM-based and human context classification be improved beyond the reported 77.57%?
  - Basis in paper: [inferred] Section 4.2 reports Molmo's context classification aligned with human annotations in only 370 of 477 verified cases (77.57%). Section 6 acknowledges that "human annotators and MLLMs occasionally disagree on contextual coherence."
  - Why unresolved: Context classification involves subjective judgments about location, size, and co-occurrence, and the paper does not explore alternative prompting strategies, ensemble methods, or fine-tuning approaches to improve alignment.
  - What evidence would resolve it: A systematic comparison of different MLLMs, prompting strategies, or calibration methods showing improved agreement rates with human annotations on a held-out validation set.

- **Open Question 4:** Would more sophisticated integration methods outperform the simple multiplicative enhancement (γ=5) for combining context signals with fake detection models?
  - Basis in paper: [inferred] Section 5.3 uses a simple formula P'(x,y) = min(P(x,y) × γ, 1.0) to enhance predictions in out-of-context regions. While effective, the authors do not compare against learned fusion, attention-based integration, or joint training approaches.
  - What evidence would resolve it: Experiments comparing the multiplicative enhancement against alternative fusion methods (e.g., learned weighting, feature-level concatenation, attention mechanisms) on the same fake localization metrics.

## Limitations

- Dataset reliability depends on MLLM context labeling with only 77.57% human agreement, raising questions about label consistency
- Inpainting success varies significantly by semantic category, with furniture/household objects failing more often than animals/food
- Context classification remains "inherently subjective" with occasional disagreements between human annotators and MLLMs
- The dataset is limited to COCO's 80 predefined categories, restricting generalizability to open-vocabulary scenarios

## Confidence

- Dataset construction methodology: High
- MLLM context classification validity: Medium
- Context enhancement effectiveness: Medium

## Next Checks

1. Replicate the semantic prior patterns in inpainting success rates (Figure 3a) on a 1000-image sample to verify the category-dependent failure modes before full dataset deployment.
2. Conduct a controlled study measuring Molmo's context classification agreement with human annotators on 100 manually labeled images to establish expected label noise and identify systematic disagreements.
3. Implement the context enhancement pipeline (γ=1.5) with oracle out-of-context masks on a pretrained fake detector to establish the theoretical upper bound, then measure degradation when using the actual context classifier predictions.