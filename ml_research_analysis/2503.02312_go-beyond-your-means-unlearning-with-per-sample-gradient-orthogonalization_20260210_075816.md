---
ver: rpa2
title: 'Go Beyond Your Means: Unlearning with Per-Sample Gradient Orthogonalization'
arxiv_id: '2503.02312'
source_url: https://arxiv.org/abs/2503.02312
tags:
- unlearning
- retain
- gradient
- data
- unlearn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OrthoGrad, a machine unlearning method designed
  to remove the influence of problematic data from trained models without retraining.
  The core innovation lies in projecting the unlearn gradient onto the subspace orthogonal
  to retain batch gradients, effectively decoupling unlearn and retain optimization
  processes.
---

# Go Beyond Your Means: Unlearning with Per-Sample Gradient Orthogonalization

## Quick Facts
- arXiv ID: 2503.02312
- Source URL: https://arxiv.org/abs/2503.02312
- Reference count: 36
- One-line primary result: OrthoGrad achieves effective unlearning with limited retain data by orthogonalizing unlearn gradients against per-sample retain gradients.

## Executive Summary
OrthoGrad is a machine unlearning method that removes the influence of problematic data from trained models without retraining. It projects unlearn gradients onto the subspace orthogonal to retain batch gradients, decoupling unlearn and retain optimization. The method excels in low-data regimes where retain set access is constrained, achieving reliable unlearning while maintaining better test performance than competing methods. By restricting updates to LoRA modules and linearly combining retain and orthogonalized unlearn gradients, OrthoGrad effectively addresses gradient interference while preserving model utility.

## Method Summary
OrthoGrad orthogonalizes unlearn gradients against per-sample retain gradients using QR decomposition, then combines this with a retain gradient component weighted by α. The method uses LoRA modules on attention layers to limit parameter updates, preserving overall test performance. At each step, it computes gradients for unlearn and retain batches, performs QR decomposition on retain gradients, projects the unlearn gradient onto the orthogonal complement, and updates LoRA parameters with the combined gradient. This approach effectively decouples unlearn and retain optimization processes, enabling effective unlearning with limited retain data.

## Key Results
- On ASR unlearning, OrthoGrad achieves 13.98 WER on test data while successfully forgetting target speaker (96.24% unlearning accuracy)
- Outperforms competing methods in Unlearning Impact Score across classification and ASR benchmarks
- Particularly effective in low-data regimes where retain set access is constrained
- LoRA integration preserves test performance better than full-parameter updates

## Why This Works (Mechanism)

### Mechanism 1
Per-sample gradient orthogonalization reduces gradient interference between unlearn and retain sets, enabling effective unlearning with limited retain data. At each optimization step, the unlearn gradient gu is projected onto the subspace orthogonal to all individual retain gradients [g1r, g2r, ..., gkr] via QR decomposition. The orthogonalized gradient g⊥u = gu − Σ⟨gu, qir⟩qir cannot increase loss on any retain sample by construction. This requires retain gradients to be linearly independent enough for meaningful projection.

### Mechanism 2
Restricting unlearning updates to LoRA modules preserves overall test performance better than full-parameter updates. LoRA adapters are applied to attention linear layers (rank=8, scaling=32). Only LoRA parameters θl are updated during unlearning, then merged back. This constrains the unlearning perturbation to a low-rank subspace.

### Mechanism 3
Linearly combining the retain gradient with the orthogonalized unlearn gradient improves performance over pure orthogonal projection alone. Final update uses g = α·ḡr + (1−α)·g⊥u where ḡr is the mean retain gradient. The retain component (α weighted) ensures the model continues learning on retain data, while (1−α) weights the forgetting direction.

## Foundational Learning

- **Concept: QR Decomposition and Orthogonal Projection**
  - Why needed here: Core operation for computing orthogonalized gradients. You must understand how QR factorization yields orthonormal basis Q, and how projection p = ⟨gu, qir⟩qir works.
  - Quick check question: Given a matrix Gr of shape [d, k] (d parameters, k samples), what does Q from QR(Gr) represent, and how would you project vector gu onto the orthogonal complement of Q's column space?

- **Concept: Gradient Interference in Multi-Objective Optimization**
  - Why needed here: The paper frames unlearning as conflicting objectives (forget vs retain). Understanding when gradients conflict (negative cosine similarity) vs align helps interpret why orthogonalization helps.
  - Quick check question: If unlearn gradient gu and retain gradient gr have cosine similarity of -0.7, what happens if you simply add them? What does orthogonalization change?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: The method wraps base model layers with LoRA adapters. You need to understand rank, scaling factor, and merge operations.
  - Quick check question: A linear layer W has shape [4096, 4096]. With LoRA rank=8, how many trainable parameters are added? What is the computational overhead during forward pass?

## Architecture Onboarding

- **Component map:** Pretrained Model (frozen weights θp) → LoRA Modules (trainable θl) ← wrapped around attention layers → Unlearn Batch Bu → gu (mean gradient) → Retain Batch Br → Gr (per-sample gradient matrix [d × k]) → QR Decomposition → Q (orthonormal basis for retain subspace) → Projection: g⊥u = gu - Σ⟨gu, qi⟩qi → Combine: g = α·ḡr + (1-α)·g⊥u → Update: θl ← θl - η·g → Merge LoRA → Final unlearned model

- **Critical path:** 1. Per-sample gradient computation for retain batch (memory bottleneck—requires k × d storage) 2. QR decomposition on Gr (O(d·k²) complexity) 3. Projection computation (k inner products) 4. LoRA update and merge

- **Design tradeoffs:** Retain batch size k: Larger k → better subspace approximation → better retain preservation, but O(k) memory growth. Paper uses 128-256. α value: Higher α → more retain emphasis → better test performance but risk of incomplete unlearning. Paper uses 0.8-0.9. LoRA rank: Higher rank → more expressive unlearning → potential overfitting. Paper uses rank=8.

- **Failure signatures:** High variance in Wtest across runs (suggests retain batch too small or α unstable), Wunlearn not reaching target (α too high, or unlearn set too entangled with retain knowledge), OOM during retain gradient computation (k × d per-sample storage exceeds GPU memory), Test performance collapses (LoRA rank insufficient, or unlearning too aggressive).

- **First 3 experiments:** 1. Ablation on projection type: Compare (a) no projection, (b) mean-gradient projection, (c) per-sample projection on a small classification dataset (CIFAR-10 subset). Measure unlearn accuracy and test accuracy. Expected: per-sample > mean > none for test preservation. 2. Sensitivity to retain batch size k: Run OrthoGrad with k ∈ {32, 64, 128, 256} on ImageNet subset. Plot UIS vs k. Expected: diminishing returns after k~128, but memory scales linearly. 3. LoRA vs full-parameter unlearning: Run with and without LoRA on ASR task. Measure Wtest and Wunlearn. Expected: LoRA version has lower Wtest (better) but may have slightly lower Wunlearn (worse forgetting).

## Open Questions the Paper Calls Out

- Does OrthoGrad maintain its superiority over existing baselines when the full retain dataset is available rather than just a limited subset? The experimental design intentionally restricted the retain set size (e.g., 5K-10K for ImageNet) to simulate low-data regimes, leaving the high-data regime unverified.

- How can the computational efficiency of OrthoGrad be improved to handle larger retain batch sizes without prohibitive GPU memory costs? The method requires computing and storing a matrix of per-sample gradients (Gr) to perform QR decomposition, which limits scalability on memory-constrained hardware.

- What is the optimal configuration for Parameter-Efficient Fine-Tuning (PEFT) modules, such as LoRA, when combined with gradient orthogonalization? While the authors use LoRA (rank 8) to limit the effect on the overall test performance, they do not ablate how different PEFT configurations impact the trade-off between forgetting efficacy and model utility.

## Limitations

- Scalability issues with per-sample gradient computation as retain batch size increases, leading to prohibitive GPU memory costs
- Limited ablation studies on PEFT configurations, leaving optimal setup for unlearning unclear
- Potential failure when retain gradients are nearly collinear or when unlearn requires modifying deeply encoded knowledge

## Confidence

- **High** for the orthogonalization mechanism (well-grounded in gradient interference theory and validated across multiple benchmarks)
- **Medium** for LoRA integration (limited ablation studies, no comparison to full-parameter unlearning)
- **Medium** for the α-weighted combination heuristic (empirical choice without theoretical justification for the specific values)

## Next Checks

1. Reproduce the CIFAR10 ablation study comparing projection types (no projection, mean-gradient, per-sample) to verify that per-sample projection consistently outperforms alternatives in UIS.

2. Conduct sensitivity analysis on retain batch size k across multiple datasets to quantify the trade-off between memory usage and unlearning quality.

3. Test LoRA rank=8 against full-parameter updates on ASR to quantify the performance gap and validate the low-rank assumption for practical unlearning scenarios.