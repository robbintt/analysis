---
ver: rpa2
title: Self-Supervised Learning by Curvature Alignment
arxiv_id: '2511.17426'
source_url: https://arxiv.org/abs/2511.17426
tags:
- curvature
- learning
- kernel
- curvssl
- self-supervised
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CurvSSL and kernel CurvSSL, self-supervised
  learning methods that incorporate curvature regularization into the learning objective.
  The approach extends standard non-contrastive SSL frameworks by computing discrete
  curvature scores for each embedding using k-nearest neighbors on the unit hypersphere,
  and aligns these curvature scores across augmentations using a Barlow Twins-style
  loss.
---

# Self-Supervised Learning by Curvature Alignment

## Quick Facts
- arXiv ID: 2511.17426
- Source URL: https://arxiv.org/abs/2511.17426
- Reference count: 8
- Primary result: Curvature-regularized SSL methods (CurvSSL, kernel CurvSSL) achieve competitive or improved linear evaluation performance on MNIST and CIFAR-10 compared to Barlow Twins and VICReg.

## Executive Summary
This paper introduces CurvSSL and kernel CurvSSL, self-supervised learning methods that incorporate curvature regularization into the learning objective. The approach extends standard non-contrastive SSL frameworks by computing discrete curvature scores for each embedding using k-nearest neighbors on the unit hypersphere, and aligns these curvature scores across augmentations using a Barlow Twins-style loss. Experiments on MNIST and CIFAR-10 datasets with a ResNet-18 backbone demonstrate that curvature-regularized SSL yields competitive or improved linear evaluation performance compared to Barlow Twins and VICReg. Specifically, CurvSSL achieves 97.9% accuracy on MNIST and 75.1% on CIFAR-10, while kernel CurvSSL reaches 98.4% and 76.5% respectively. UMAP visualizations show that the learned representations form well-separated clusters, indicating that the curvature regularization helps preserve local manifold geometry while maintaining discriminative features.

## Method Summary
The method introduces curvature-based regularization into self-supervised learning by computing discrete curvature scores for each embedding using k-nearest neighbors on the unit hypersphere. These curvature scores are then aligned across augmentations using a Barlow Twins-style loss. The approach extends standard non-contrastive SSL frameworks by explicitly incorporating geometric structure preservation into the learning objective. The kernel variant applies this principle in a transformed feature space, potentially capturing more complex manifold structures.

## Key Results
- CurvSSL achieves 97.9% accuracy on MNIST and 75.1% on CIFAR-10 with ResNet-18 backbone
- Kernel CurvSSL reaches 98.4% accuracy on MNIST and 76.5% on CIFAR-10
- UMAP visualizations show well-separated clusters, indicating improved manifold geometry preservation
- Performance is competitive or improved compared to Barlow Twins and VICReg baselines

## Why This Works (Mechanism)
The curvature regularization helps preserve local manifold geometry while maintaining discriminative features by aligning curvature scores across augmentations. This encourages the learned representations to respect the intrinsic geometric structure of the data manifold, leading to more semantically meaningful embeddings. The Barlow Twins-style loss ensures that corresponding views of the same image have similar curvature distributions, which helps maintain consistency across different augmentations.

## Foundational Learning
- **Curvature in manifold learning**: Measures how data points deviate from local flatness, crucial for understanding data geometry and relationships between points
- **k-nearest neighbors (k-NN)**: Used to approximate local curvature by finding nearby points in the embedding space
- **Unit hypersphere embeddings**: Constraining embeddings to the hypersphere enables consistent curvature computation across different scales
- **Barlow Twins loss**: A redundancy reduction loss that encourages cross-view invariance, adapted here for curvature alignment
- **Non-contrastive SSL**: Self-supervised methods that don't rely on negative pairs, focusing instead on invariance properties
- **Linear evaluation protocol**: Standard method for assessing representation quality by training a linear classifier on frozen features

## Architecture Onboarding
**Component map:** Input images -> Data augmentation -> Encoder (ResNet-18) -> Hypersphere projection -> k-NN curvature computation -> Curvature alignment loss -> Total loss

**Critical path:** The most critical components are the k-NN curvature computation and the Barlow Twins-style curvature alignment loss, as these directly implement the core innovation of the method.

**Design tradeoffs:** The choice of Barlow Twins loss provides stability but may limit the method's ability to handle complex curvature relationships. The k-NN approach is simple but may not capture global manifold structure effectively.

**Failure signatures:** Poor performance on datasets with non-uniform curvature distributions, computational inefficiency with high-dimensional embeddings, and potential instability in curvature estimation for sparse regions of the manifold.

**3 first experiments:**
1. Compare CurvSSL performance with and without curvature regularization on MNIST
2. Vary k in the k-NN curvature computation to find optimal neighborhood size
3. Test different loss functions (e.g., VICReg-style) for curvature alignment instead of Barlow Twins

## Open Questions the Paper Calls Out
None provided.

## Limitations
- Experimental validation is limited to small-scale image datasets (MNIST and CIFAR-10), leaving unclear whether the approach scales effectively to larger, more complex domains
- k-NN curvature computation on the unit hypersphere may become computationally expensive as embedding dimensionality or dataset size grows
- The specific contribution of curvature regularization to downstream performance is not isolated through ablation studies

## Confidence
- **High confidence**: The mathematical formulation of curvature computation via k-NN on the hypersphere is sound and well-defined
- **Medium confidence**: The reported linear evaluation results on MNIST and CIFAR-10 are likely reproducible, given the standard experimental setup
- **Low confidence**: Claims about curvature regularization's role in preserving local manifold geometry and improving downstream performance on complex datasets are not fully substantiated

## Next Checks
1. **Scale-up validation**: Replicate the method on larger-scale vision datasets (e.g., ImageNet-100 or STL-10) to assess scalability and robustness
2. **Computational efficiency analysis**: Measure and report training/inference time and memory usage as a function of embedding dimensionality and dataset size
3. **Ablation studies**: Perform controlled experiments removing curvature regularization to quantify its specific contribution to downstream accuracy and manifold structure preservation