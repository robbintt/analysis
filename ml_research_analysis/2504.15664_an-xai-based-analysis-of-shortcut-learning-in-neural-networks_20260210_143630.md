---
ver: rpa2
title: An XAI-based Analysis of Shortcut Learning in Neural Networks
arxiv_id: '2504.15664'
source_url: https://arxiv.org/abs/2504.15664
tags:
- spurious
- features
- neurons
- learning
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how and where neural networks encode spurious
  correlations through an XAI-based diagnostic metric called the neuron spurious score
  (s-score). The study systematically investigates shortcut learning in both CNNs
  and ViTs, finding that while spurious features are partially disentangled in representation
  space, the degree of disentanglement varies significantly across architectures.
---

# An XAI-based Analysis of Shortcut Learning in Neural Networks

## Quick Facts
- arXiv ID: 2504.15664
- Source URL: https://arxiv.org/abs/2504.15664
- Authors: Phuong Quynh Le; Jörg Schlötterer; Christin Seifert
- Reference count: 38
- Key outcome: Neural networks encode spurious correlations in multiple components beyond the last layer, and existing mitigation methods adjust rather than eliminate these correlations

## Executive Summary
This paper presents a systematic analysis of how neural networks encode spurious correlations through a novel XAI-based diagnostic metric called the neuron spurious score (s-score). The study investigates shortcut learning across both CNNs and ViTs, revealing that spurious features are not confined to the final layer but are distributed across multiple network components. The research demonstrates that while these features show partial disentanglement in representation space, the degree varies significantly between architectures, and existing spurious correlation mitigation methods do not truly eliminate learned spurious correlations but instead modify neuron interactions.

## Method Summary
The authors developed a neuron spurious score (s-score) as an XAI-based diagnostic metric to quantify how and where neural networks encode spurious correlations. They conducted systematic experiments comparing CNNs and ViTs across multiple datasets, analyzing the representation space to identify where spurious features are encoded. The study employed both deep feature re-weighting and pruning as mitigation methods, using the s-score to evaluate their effectiveness at truly eliminating rather than merely adjusting spurious correlations.

## Key Results
- Spurious features are encoded not only in the last layer but across multiple network components
- Degree of spurious feature disentanglement varies significantly across different architectures
- Existing mitigation methods like deep feature re-weighting and pruning adjust neuron interactions rather than eliminating spurious correlations
- Partial disentanglement of spurious features occurs in representation space

## Why This Works (Mechanism)
The analysis leverages XAI techniques to trace how neural networks encode and represent spurious correlations throughout their architecture. By developing the neuron s-score metric, the researchers can quantify the presence and distribution of these correlations across different network components. This systematic approach reveals that spurious features are learned as part of the network's internal representations rather than being isolated to superficial features, explaining why simple mitigation strategies often fail to address the root cause of shortcut learning.

## Foundational Learning

**Neuron Spurious Score (s-score)**: A diagnostic metric quantifying how strongly individual neurons encode spurious correlations. *Why needed*: To systematically measure and compare spurious feature encoding across different network components and architectures. *Quick check*: Verify that s-scores correlate with known spurious features in controlled synthetic datasets.

**Representation Space Analysis**: Examining how features are organized and related in the network's internal representations. *Why needed*: To understand whether spurious features are isolated or entangled with legitimate features. *Quick check*: Confirm that disentangled features show clear separation in visualization while entangled features cluster together.

**Shortcut Learning**: The tendency of neural networks to exploit spurious correlations that correlate with labels but are not causally related to the task. *Why needed*: To understand why models fail when spurious correlations don't hold in deployment. *Quick check*: Test model performance on datasets where known spurious correlations are broken.

## Architecture Onboarding

**Component Map**: Input -> Feature Extraction Layers -> Representation Space -> Classification Head
- CNNs: Convolutional layers -> Pooling layers -> Fully connected layers
- ViTs: Patch embedding -> Transformer blocks -> Classification head

**Critical Path**: The flow from input through feature extraction to final classification, where spurious correlations can be introduced and amplified at each stage.

**Design Tradeoffs**: 
- Depth vs. width in feature extraction affects spurious feature encoding capacity
- Attention mechanisms in ViTs may provide different spurious correlation patterns than convolutions
- Tradeoff between model capacity for learning legitimate features and susceptibility to spurious correlations

**Failure Signatures**: 
- High performance on training data but significant degradation when spurious correlations are broken
- s-scores remaining elevated even after mitigation attempts
- Spurious features appearing in multiple network components rather than being isolated

**First Experiments**:
1. Compare s-scores across different layer depths to identify where spurious features first appear
2. Test model robustness by systematically breaking known spurious correlations
3. Evaluate the distribution of s-scores across neurons to identify which components are most affected

## Open Questions the Paper Calls Out
The paper identifies several key open questions regarding the generalizability of the neuron s-score metric across different architectures and datasets, whether observed patterns of spurious feature encoding apply equally to larger-scale models, and the need for more comprehensive evaluation of emerging spurious correlation mitigation approaches.

## Limitations
- Generalizability of s-score metric beyond tested architectures and datasets
- Analysis may not capture all possible spurious correlation mitigation strategies
- Need for more extensive ablation studies across diverse architectures

## Confidence
- **High**: Core finding that spurious features are partially disentangled in representation space and vary across architectures
- **Medium**: Claim about spurious features being encoded in multiple network components
- **Medium**: Characterization of mitigation method limitations and their adjustment of rather than elimination of spurious correlations

## Next Checks
1. Test the neuron s-score metric on larger-scale models (e.g., ResNet-50, ViT-Large) and diverse datasets to verify generalizability of the findings
2. Conduct ablation studies systematically removing or modifying different network components to precisely map where spurious features are most strongly encoded
3. Evaluate a broader range of spurious correlation mitigation methods, including recently proposed approaches, to comprehensively assess their effectiveness at truly eliminating learned spurious correlations