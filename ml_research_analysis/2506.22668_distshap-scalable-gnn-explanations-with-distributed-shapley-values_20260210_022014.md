---
ver: rpa2
title: 'DistShap: Scalable GNN Explanations with Distributed Shapley Values'
arxiv_id: '2506.22668'
source_url: https://arxiv.org/abs/2506.22668
tags:
- graph
- edges
- gpus
- distshap
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of explaining graph neural network
  (GNN) predictions at scale. The core problem is that attributing predictions to
  specific edges in large graphs requires evaluating millions of subgraphs, which
  is computationally infeasible with existing methods.
---

# DistShap: Scalable GNN Explanations with Distributed Shapley Values

## Quick Facts
- arXiv ID: 2506.22668
- Source URL: https://arxiv.org/abs/2506.22668
- Reference count: 40
- DistShap achieves higher explanation fidelity than existing methods while scaling to 128 GPUs

## Executive Summary
This paper introduces DistShap, a distributed algorithm for computing Shapley value-based explanations of Graph Neural Network (GNN) predictions at massive scale. The key innovation is leveraging parallel computation across multiple GPUs to handle the exponential complexity of subgraph enumeration required for Shapley value calculations. DistShap enables explaining GNN predictions on graphs that were previously computationally infeasible to analyze.

## Method Summary
DistShap addresses the scalability challenge of Shapley value-based GNN explanations by distributing computation across multiple GPUs. The method replicates computational graphs across GPUs, performs parallel subgraph sampling, executes batched GNN inference, and solves distributed least squares problems to compute edge importance scores. This approach allows the system to scale to 128 NVIDIA A100 GPUs on the NERSC Perlmutter supercomputer while maintaining explanation fidelity.

## Key Results
- DistShap achieves 11.1Ã— to 13.5Ã— speedup when scaling from 8 to 128 GPUs
- Consistently outperforms baselines in ð¹ð‘–ð‘‘ð‘’ð‘™ð‘–ð‘¡ð‘¦ + and ð¹ð‘–ð‘‘ð‘’ð‘™ð‘–ð‘¡ð‘¦ âˆ’ scores on six real-world datasets
- Scales to explain GNNs on graphs that would otherwise be computationally infeasible

## Why This Works (Mechanism)
DistShap works by decomposing the computationally intensive Shapley value calculation into parallelizable subtasks across multiple GPUs. By replicating computational graphs and performing batched subgraph inference, the algorithm amortizes the cost of GNN evaluations across many subgraphs simultaneously. The distributed least squares solver then efficiently aggregates results to compute final edge importance scores, enabling both speed and scalability while maintaining high explanation fidelity.

## Foundational Learning
- **Distributed Graph Processing**: Needed to handle massive graph datasets that don't fit in single GPU memory; Quick check: verify graph partitioning maintains connectivity properties
- **Shapley Value Computation**: Fundamental fairness concept from cooperative game theory applied to feature attribution; Quick check: validate Shapley values sum to total prediction
- **Batched GNN Inference**: Efficient parallel evaluation of multiple subgraphs; Quick check: ensure batch size doesn't exceed GPU memory limits
- **Distributed Least Squares**: Solving large linear systems across multiple nodes; Quick check: monitor convergence and numerical stability
- **Subgraph Sampling**: Random sampling of connected subgraphs for approximation; Quick check: verify sampling maintains edge connectivity
- **GPU Memory Management**: Optimizing data placement and transfer between host and device; Quick check: track memory usage during scaling

## Architecture Onboarding

Component Map:
Input Graphs -> Subgraph Sampler -> Batched GNN Inferencer -> Distributed Aggregator -> Shapley Values

Critical Path:
Subgraph sampling â†’ Batched GNN inference â†’ Distributed aggregation â†’ Final attribution

Design Tradeoffs:
- Replication vs. partitioning: Replication enables independent computation but increases memory usage
- Sampling rate vs. accuracy: Higher sampling improves fidelity but increases computation time
- Batch size vs. GPU utilization: Larger batches improve throughput but may cause memory overflow

Failure Signatures:
- Communication bottlenecks when scaling beyond 64 GPUs
- Memory overflow with extremely dense graphs
- Numerical instability in distributed least squares solver
- Degradation in explanation fidelity with aggressive sampling

First Experiments:
1. Run on small graph with known ground truth to verify correctness
2. Measure scaling behavior on synthetic graphs of increasing size
3. Compare explanation fidelity vs. runtime tradeoff curves

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Communication overhead between GPUs during distributed computation is not extensively characterized
- Effectiveness on extremely heterogeneous graph structures remains unexplored
- Does not address node-level explanations or feature importance within edges

## Confidence

High confidence: Core algorithmic contributions are technically sound and well-validated on real-world datasets with statistically significant performance improvements.

Medium confidence: Scalability to 128 GPUs may depend on specific hardware configurations and network topologies; communication patterns could affect real-world deployment.

Low confidence: Insufficient analysis of edge cases like highly skewed degree distributions or temporal graphs; limited comparison with alternative explanation methods.

## Next Checks
1. Characterize communication overhead and scaling behavior on different GPU cluster configurations to identify potential bottlenecks.
2. Test DistShap on graphs with extreme heterogeneity (power-law degree distributions, multi-modal node features) to assess robustness.
3. Compare DistShap's explanations with gradient-based and attention-based methods on the same datasets to evaluate unique insights.