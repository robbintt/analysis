---
ver: rpa2
title: 'HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation'
arxiv_id: '2511.15435'
source_url: https://arxiv.org/abs/2511.15435
tags:
- attack
- image
- knowledge
- query
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces HV-Attack, a method for attacking multimodal
  Retrieval-Augmented Generation (MRAG) systems by adding imperceptible perturbations
  to the image input. The method uses a hierarchical two-stage strategy: first breaking
  cross-modal alignment between the image and its caption, then disrupting semantic
  alignment with reference knowledge.'
---

# HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation

## Quick Facts
- **arXiv ID**: 2511.15435
- **Source URL**: https://arxiv.org/abs/2511.15435
- **Reference count**: 40
- **Primary result**: Hierarchical visual attack achieves 57.38% success rate degrading Recall@5 on fine-tuned CLIP retriever

## Executive Summary
HV-Attack introduces a method for attacking multimodal Retrieval-Augmented Generation (MRAG) systems by adding imperceptible perturbations to image inputs. The attack uses a hierarchical two-stage strategy: first breaking cross-modal alignment between the image and its caption, then disrupting semantic alignment with reference knowledge. This causes the retriever to fetch irrelevant knowledge and misleads the generator, resulting in incorrect answers. Experiments on OK-VQA and InfoSeek datasets show significant drops in retrieval and generation performance, with HV-Attack outperforming prior methods on fine-tuned models.

## Method Summary
HV-Attack employs a two-stage hierarchical optimization approach on retrieval perturbations (δr) combined with separate generator perturbations (δg). Stage 1 breaks modality alignment by pushing the query image embedding away from its true caption and toward an irrelevant reference caption using contrastive hinge loss. Stage 2 further disrupts semantic alignment between the multimodal query and relevant knowledge passages. The generator perturbation uses transferable adversarial noise (X-transfer) to break uni-modal semantic understanding. The attack uses PGD optimization (50 steps, α=1/255, ε=8/255) and works on both off-the-shelf and fine-tuned CLIP retrievers while affecting multiple black-box generators.

## Key Results
- Achieves 57.38% success rate in degrading Recall@5 on fine-tuned CLIP ViT-L/14 retriever
- Outperforms prior methods by 10-20% on retrieval metrics in ablation studies
- Successfully transfers to black-box generators BLIP-2 and LLaVA with ASR* of 14.39% and 14.21%
- Shows effectiveness on both OK-VQA (5,046 test samples) and InfoSeek (1,000 samples) datasets

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Two-Stage Retrieval Disruption
The attack breaks cross-modal alignment first, then semantic alignment, causing cascading retrieval failure more effectively than single-stage attacks. Stage 1 uses modality-level contrastive learning to push the query image embedding away from its true caption and toward an irrelevant reference caption via hinge loss. Stage 2 then uses semantic-level contrastive learning to further disrupt the relationship between the multimodal query and relevant knowledge passages. The hierarchical approach is necessary because fine-tuned retrievers have learned robust multimodal knowledge grounding that resists simple perturbations.

### Mechanism 2: Generator Input Misalignment
The attack creates conflicting inputs by misaligning the query and irrelevant retrieved knowledge, confusing the generator into producing incorrect outputs. Separate perturbations are added to the same image for different purposes: δr for retrieval disruption and δg for generator disruption. The generator receives a disrupted image query alongside irrelevant retrieved knowledge, creating conflicting signals that degrade output quality.

### Mechanism 3: Transferability of Visual Perturbations
Carefully optimized perturbations on white-box retrievers transfer effectively to black-box generators. The perturbation is optimized using PGD on the retriever, but the resulting perturbations affect downstream black-box generators because they target fundamental visual features that both retriever and generator encoders rely on for image understanding.

## Foundational Learning

### Concept: Contrastive Learning in Multimodal Embedding Spaces
- **Why needed here**: Understanding how CLIP-style models create aligned image-text embeddings is essential to grasp why the attack's contrastive loss formulation works.
- **Quick check question**: If an image embedding has cosine similarity 0.8 with its caption embedding and 0.3 with an irrelevant caption, what happens when you minimize a contrastive loss that reverses these relationships?

### Concept: Projected Gradient Descent (PGD) for Adversarial Perturbations
- **Why needed here**: The attack uses PGD to iteratively optimize perturbations within an L∞ bound (ε=8/255).
- **Quick check question**: Why does PGD project the perturbation back to the ε-ball after each gradient step, and what would happen if this projection was omitted?

### Concept: Retrieval-Augmented Generation Pipeline Architecture
- **Why needed here**: The MRAG pipeline has a sequential structure (retriever → retrieved knowledge → generator) where perturbations must propagate through multiple stages.
- **Quick check question**: If you successfully attacked only the retriever but not the generator's image input, why might the attack still fail to produce incorrect answers?

## Architecture Onboarding

### Component map:
Input image → Parallel perturbation generation (δr via 2-stage hierarchical optimization, δg via X-transfer) → CLIP retriever (ViT-L/14, ViT-H, or SigLIP) → Knowledge Base → Black-box generator (BLIP-2 or LLaVA) → Incorrect answer

### Critical path:
Input image → Parallel perturbation generation (δr via 2-stage hierarchical optimization, δg via X-transfer) → Retriever receives Iadvr and returns irrelevant top-k knowledge → Generator receives Iadvg + irrelevant knowledge → Incorrect answer

### Design tradeoffs:
- **Perturbation budget ε**: 8/255 balances imperceptibility with effectiveness; 4/255 is more stealthy but less effective
- **PGD steps**: 50 steps per stage balances attack quality vs. computation; diminishing returns after convergence
- **Two perturbations vs. one**: Separate δr and δg achieves dual disruption but increases complexity

### Failure signatures:
1. Fine-tuned retriever resistance: Attack success rate drops significantly on fine-tuned vs. off-the-shelf retrievers
2. Insufficient perturbation budget: Retrieval metrics don't degrade enough when ε < 4/255
3. Generator robustness: Retrieval succeeds but generation still produces correct answers
4. Poor reference sample quality: Stage 1 fails if reference images aren't sufficiently dissimilar

### First 3 experiments:
1. **Baseline comparison on fine-tuned retriever**: Run HV-Attack vs. baselines on fine-tuned CLIP ViT-L/14 with OK-VQA, measuring Recall@5 degradation
2. **Ablation of hierarchical stages**: Test Stage 1 alone, Stage 2 alone, and combined on both OK-VQA and InfoSeek
3. **Transferability to different generators**: Apply same retrieval perturbations to BLIP-2 vs. LLaVA generators and measure VQA score degradation

## Open Questions the Paper Calls Out
None

## Limitations
- Underspecified implementation details: Image captioning model C is not defined
- Fine-tuning procedure for the CLIP retriever lacks details on data, epochs, and objectives
- Reference sample selection mechanism for "least similar" images is not precisely detailed
- Positive passage retrieval method for Stage 2 is not specified

## Confidence
- **High confidence** in the overall attack framework and hierarchical two-stage design
- **Medium confidence** in transferability claims between different generator architectures
- **Medium confidence** in reported numbers due to underspecified components

## Next Checks
1. **Implementation validation**: Implement the full HV-Attack pipeline using a specific captioning model (e.g., BLIP-2) and test on a small subset of OK-VQA
2. **Ablation study replication**: Replicate the Stage 1 vs. Stage 2 vs. Combined comparison on both OK-VQA and InfoSeek datasets
3. **Perturbation budget sensitivity**: Systematically test different ε values (4/255, 8/255, 12/255) to verify the trade-off between imperceptibility and attack effectiveness