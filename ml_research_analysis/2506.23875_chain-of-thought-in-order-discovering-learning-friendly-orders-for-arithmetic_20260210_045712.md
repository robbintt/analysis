---
ver: rpa2
title: 'Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic'
arxiv_id: '2506.23875'
source_url: https://arxiv.org/abs/2506.23875
tags:
- order
- latexit
- permutation
- target
- orders
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of discovering a learning-friendly
  order for target token sequences in Transformer decoders, particularly for arithmetic
  tasks. The core method idea leverages the observation that neural networks learn
  easier samples earlier in training: it trains a Transformer on a mixture of target
  sequences in different orders and identifies orders with faster loss drops as learning-friendly.'
---

# Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic

## Quick Facts
- arXiv ID: 2506.23875
- Source URL: https://arxiv.org/abs/2506.23875
- Authors: Yuta Sato; Kazuhiko Kawamoto; Hiroshi Kera
- Reference count: 20
- Discovers learning-friendly orders for arithmetic tasks by leveraging loss drop rates during training

## Executive Summary
This paper addresses the problem of discovering optimal token orderings for target sequences in Transformer decoders, particularly for arithmetic tasks. The core insight leverages the observation that neural networks learn easier samples earlier in training: by training on mixed-order target sequences and identifying those with faster loss drops as learning-friendly. To handle the factorial growth of permutation space, the authors propose a hierarchical two-stage approach combining global block-level exploration with local refinement. Experiments on four arithmetic tasks demonstrate success rates improving from approximately 10% to near 100%, with the method rediscovering known optimal orders like reverse-digit sequencing for multiplication.

## Method Summary
The paper introduces a method to discover learning-friendly orders for target token sequences by exploiting the observation that neural networks learn easier samples earlier in training. The approach trains a Transformer decoder on a mixture of target sequences in different orders and identifies orders with faster loss drops as learning-friendly. To address the factorial growth of permutation space, a hierarchical two-stage approach is proposed: first exploring orders at the block level, then refining within blocks. This method is evaluated on four order-sensitive arithmetic tasks (ReLU, Square-19, Index, and multiplication), demonstrating the ability to discover learning-friendly orders from billions of candidates and achieving success rates near 100% compared to baseline 10%.

## Key Results
- Success rates improved from ~10% to near 100% on arithmetic tasks
- Method successfully discovered learning-friendly orders out of billions of candidates
- Rediscovered reverse-digit order for multiplication, matching prior studies
- Effective for target lengths up to 13 tokens across challenging initialization scenarios

## Why This Works (Mechanism)
The method works by leveraging the empirical observation that neural networks tend to learn easier samples earlier in training. By training on mixed-order target sequences and monitoring loss drop rates, the approach identifies which orderings facilitate faster learning. The hierarchical two-stage approach addresses the factorial complexity of the search space by first exploring block-level orderings globally, then refining within blocks locally. This structure allows the method to efficiently navigate billions of possible orderings without exhaustive search.

## Foundational Learning
- **Permutation Space Complexity**: Understanding that the number of possible orderings grows factorially with sequence length (n! permutations) explains why naive exhaustive search is computationally infeasible
- **Loss Drop Rate as Learning Signal**: The observation that faster initial loss drops indicate easier-to-learn samples provides the key metric for identifying learning-friendly orders
- **Hierarchical Search Strategy**: Breaking the problem into block-level exploration followed by local refinement reduces computational complexity from factorial to more manageable levels
- **Transformer Decoder Behavior**: Knowledge of how Transformers process target sequences sequentially informs why ordering matters for learning efficiency
- **Arithmetic Task Structure**: Understanding the inherent ordering sensitivity in arithmetic operations (e.g., digit-by-digit computation) contextualizes why certain orderings are more learnable

## Architecture Onboarding

**Component Map:**
Target Sequence -> Mixed-Order Training -> Loss Monitoring -> Order Ranking -> Hierarchical Search (Global Block → Local Refinement) -> Learning-Friendly Order

**Critical Path:**
Mixed-order training → Loss drop rate measurement → Order selection → Hierarchical refinement

**Design Tradeoffs:**
- Global vs. local search balance: Too much global exploration wastes computation; too much local refinement misses better global structures
- Block size selection: Larger blocks reduce search space but may miss optimal sub-orderings; smaller blocks increase precision but computational cost
- Loss drop rate vs. final accuracy: Early learning speed may not always correlate with final task performance

**Failure Signatures:**
- No clear loss drop rate differentiation between orders suggests task may not benefit from ordering optimization
- Hierarchical approach failing to converge indicates poor block partitioning or inappropriate block sizes
- Success rates plateauing at suboptimal levels suggests local minima trapping in the refinement stage

**First Experiments:**
1. Baseline test on random orderings to establish baseline success rate (~10%)
2. Mixed-order training with 2-3 candidate orders to verify loss drop rate differentiation
3. Block-level exploration with varying block sizes to optimize the hierarchical approach

## Open Questions the Paper Calls Out
None

## Limitations
- Computational complexity grows exponentially with sequence length, limiting effectiveness beyond 13 tokens
- Reliance on loss drop rate as primary metric may not capture all aspects of learning difficulty
- Method requires training multiple Transformers during search, potentially expensive for longer sequences
- Performance on complex, real-world tasks beyond synthetic arithmetic problems remains unexplored

## Confidence

**High Confidence:**
- Core observation about early learning of easier samples is empirically validated
- Hierarchical approach effectively identifies learning-friendly orders for tested arithmetic tasks
- Rediscovery of reverse-digit order for multiplication provides strong validation

**Medium Confidence:**
- Generalizability to more complex tasks beyond four arithmetic problems tested
- Methodology appears sound but empirical validation limited to synthetic tasks

**Low Confidence:**
- Scalability claims beyond 13 tokens and computational efficiency for longer sequences
- Fundamental challenge of exponential permutation space growth may limit practical applicability

## Next Checks
1. **Scalability Test**: Evaluate performance on sequences of length 15-20 tokens to validate claimed scalability limits and measure computational requirements
2. **Cross-Domain Application**: Apply approach to non-arithmetic reasoning tasks (logical deduction, multi-step planning) to assess generalizability
3. **Alternative Metric Validation**: Compare loss drop rate metric against final convergence accuracy and training stability to verify optimization of relevant learning aspects