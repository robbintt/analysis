---
ver: rpa2
title: Learning inflection classes using Adaptive Resonance Theory
arxiv_id: '2512.15551'
source_url: https://arxiv.org/abs/2512.15551
tags:
- inflection
- classes
- class
- which
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores how inflection classes in languages can be learned
  by unsupervised clustering using Adaptive Resonance Theory (ART), a neural network
  model that controls generalization through a vigilance parameter. It applies this
  method to Latin, Portuguese, and Estonian, using phonetic data and evaluating clusterings
  against linguistically attested inflection classes.
---

# Learning inflection classes using Adaptive Resonance Theory

## Quick Facts
- **arXiv ID:** 2512.15551
- **Source URL:** https://arxiv.org/abs/2512.15551
- **Reference count:** 40
- **Primary result:** Unsupervised ART clustering achieves ARI ≈ 0.94 on Latin inflection classes at optimal vigilance parameter

## Executive Summary
This paper applies Adaptive Resonance Theory (ART), a neural network model for unsupervised clustering, to the task of learning inflection classes from phonological data. The model uses a vigilance parameter to control generalization, clustering verbs into inflection classes based on shared phonetic features extracted as trigrams from paradigm cells. The method successfully recovers linguistically attested classes in Latin (ARI ≈ 0.94), Portuguese (ARI ≈ 0.27), and Estonian (ARI ≈ 0.20), with performance peaking at specific vigilance values. The learned cluster templates are interpretable, revealing features that correspond to linguistic descriptions of inflection classes.

## Method Summary
The study applies ART1 neural network to cluster verb lexemes into inflection classes using phonetic data. Words are encoded as binary vectors representing the presence/absence of trigrams (3-character sequences) from specific paradigm cells. The model processes lexemes incrementally, comparing each to existing category templates using bottom-up and top-down weights. A vigilance parameter determines whether a lexeme joins an existing cluster or forms a new one. Performance is evaluated against linguistically attested inflection classes using Adjusted Rand Index (ARI) across 10-fold cross-validation with 10 random permutations per vigilance value (0.0 to 0.3).

## Key Results
- Latin inflection classes recovered with ARI ≈ 0.94 at vigilance ρ ≈ 0.06
- Performance peaks at narrow vigilance ranges specific to each language
- Learned templates show interpretable patterns matching linguistic descriptions (e.g., theme vowels in Latin)
- Model generalizes to unseen data across all three languages
- Best performance requires careful vigilance parameter tuning

## Why This Works (Mechanism)

### Mechanism 1: Vigilance-Controlled Generalization
The vigilance parameter sets a threshold for matching inputs to category templates. If match score M ≥ vigilance ρ, the input joins the cluster; otherwise, a new category forms. This controls clustering granularity and allows the model to balance over-generalization against over-splitting.

### Mechanism 2: Conjunctive Feature Intersections
Category templates store the intersection of features shared by all members, creating logical "AND" gates. This captures inflection classes defined by necessary surface-level phonetic features like theme vowels, though it struggles with abstract relational patterns.

### Mechanism 3: Sequential Competitive Selection
Lexemes are processed incrementally, with the model comparing each to existing categories based on bottom-up activation. The category with highest activation is tested against vigilance, preventing immediate misclassification and enabling stable generalization.

## Foundational Learning

- **Concept: Macroclasses vs. Microclasses** - Latin uses broad classes (macro) while Portuguese uses fine-grained classes (micro). Understanding this distinction is essential for interpreting ARI scores and vigilance effects across languages.
  - Quick check: Does your dataset require matching broad theme vowels or specific stress alternations?

- **Concept: Binary Feature Encoding (Trigrams)** - ART1 requires binary inputs. Words are encoded as presence/absence of character trigrams (e.g., "amo" → "#am", "amo", "mo#"). This preserves more information than bag-of-trigrams for the whole paradigm.
  - Quick check: Can you explain why concatenating trigrams for specific paradigm cells preserves more information?

- **Concept: Adjusted Rand Index (ARI)** - Standard accuracy doesn't work because cluster IDs don't map 1-to-1 to linguistic classes. ARI measures similarity between partitions regardless of label permutation.
  - Quick check: Why would a clustering that perfectly separates classes but splits one class into two sub-clusters receive a penalty?

## Architecture Onboarding

- **Component map:** Input (phonetic paradigm cells) → Encoder (binary trigram vectors) → F1 Layer (compares to top-down weights) → F2 Layer (stores categories, calculates activation) → Vigilance Gate (checks match score)

- **Critical path:** Data preparation (selecting correct paradigm cells) is most critical, followed by vigilance tuning to find the optimal range (usually 0.02–0.06)

- **Design tradeoffs:** Model prioritizes interpretability (readable templates) over capturing abstract patterns. Uses concatenated trigrams for cognitive plausibility despite some preliminary evidence that set representation might perform better

- **Failure signatures:** Explosion of clusters (vigilance too high), collapse to one cluster (vigilance too low), distributed pattern failure for languages like Estonian where classes are defined by intra-paradigm relations

- **First 3 experiments:**
  1. Reproduce Latin baseline with ρ=0.06 to verify ARI ≈ 0.94
  2. Sweep vigilance for Portuguese to observe performance peak and cluster count trade-off
  3. Extract and visualize Latin templates to verify they match linguistic theme vowels

## Open Questions the Paper Calls Out

- Can an agent-based model utilizing ART1 for clustering and analogical reasoning for production successfully simulate the diachronic evolution and emergence of inflection classes? The current study only implements synchronic clustering; multi-agent interaction and production mechanisms remain untested.

- Can the vigilance parameter be adapted automatically through communicative feedback rather than manual tuning? The current method requires ground truth comparison, unavailable to naturalistic learners.

- Does a data representation capturing relations between paradigm cells (rather than concatenated trigrams) significantly improve clustering for stem-based systems like Estonian? The current surface-similarity approach fails to capture distributed patterns required for Estonian inflection classes.

## Limitations

- ART1's reliance on surface phonological features limits its ability to capture abstract morphological patterns, failing to learn Estonian consonant gradation which requires recognizing distributed patterns across paradigm cells

- Performance is highly sensitive to vigilance parameter selection, with optimal values occurring in narrow ranges (typically 0.02-0.06), making the method difficult to apply without extensive parameter tuning

- The method assumes inflection classes can be identified through similarity of surface forms, which may not hold for languages where classes are defined by morphological relationships rather than phonological overlap

## Confidence

- **High Confidence:** Model's ability to recover linguistically attested Latin inflection classes through surface phonological similarity, and general vigilance-controlled clustering mechanism
- **Medium Confidence:** Interpretability of learned templates and their correspondence to linguistic descriptions, as this relies on manual inspection
- **Low Confidence:** Model's effectiveness for languages with complex morphological patterns like Estonian, where distributed patterns rather than shared features define class membership

## Next Checks

1. Systematically vary vigilance parameter across all three languages and quantify relationship between parameter value, cluster count, and ARI score to determine stability of "narrow optimal range" claim

2. Remove boundary markers (#) from trigram generation and retrain model to assess their contribution to class identification, particularly for Latin where theme vowels are crucial

3. Design synthetic dataset where classes are defined by distributed patterns (similar to Estonian gradation) rather than shared surface features to empirically test model's limitations with abstract morphological relationships