---
ver: rpa2
title: Correlating instruction-tuning (in multimodal models) with vision-language
  processing (in the brain)
arxiv_id: '2505.20029'
source_url: https://arxiv.org/abs/2505.20029
tags:
- brain
- visual
- alignment
- image
- instructions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether instruction-tuned multimodal large
  language models (MLLMs) can better predict human brain activity when processing
  visual scenes compared to non-instruction-tuned or unimodal models. The authors
  use representations from three instruction-tuned MLLMs (InstructBLIP, mPLUG-Owl,
  and IDEFICS) when prompted with 10 different task-specific instructions (e.g., image
  captioning, visual question answering, color recognition).
---

# Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)

## Quick Facts
- **arXiv ID**: 2505.20029
- **Source URL**: https://arxiv.org/abs/2505.20029
- **Reference count**: 40
- **Primary result**: Instruction-tuned multimodal models (InstructBLIP, mPLUG-Owl, IDEFICS) show significantly better alignment with human fMRI brain activity when processing visual scenes compared to non-instruction-tuned or unimodal models, especially in higher-level visual regions.

## Executive Summary
This study investigates whether instruction-tuned multimodal large language models (MLLMs) can better predict human brain activity during visual scene processing compared to non-instruction-tuned or unimodal models. The authors use representations from three instruction-tuned MLLMs when prompted with 10 different task-specific instructions (e.g., image captioning, visual question answering, color recognition) and compare these to brain responses from participants viewing natural scenes in fMRI using voxelwise encoding models.

Results show that MLLMs significantly outperform vision-only models and match or exceed multimodal models like CLIP in brain alignment, particularly in higher-level visual regions. Not all instructions yield equal alignment—image captioning and recognition-related prompts align better with brain activity than others. Layer-wise analysis reveals that middle layers of some MLLMs align best with high-level visual cortex, while others peak in later layers. The findings suggest that instruction tuning enhances brain alignment by enabling richer, task-specific visual understanding, though further improvements could help models differentiate instructions more precisely.

## Method Summary
The study employs a voxelwise encoding model approach to correlate neural responses with model representations. Three instruction-tuned MLLMs (InstructBLIP, mPLUG-Owl, IDEFICS) were used, each prompted with 10 task-specific instructions while processing the same set of natural images. Brain responses were collected via fMRI from participants viewing these images. Model representations from different layers were extracted and used as predictors in encoding models to predict voxel-wise brain activity. The alignment was quantified by comparing encoding model performance across different model types (instruction-tuned vs non-instruction-tuned, multimodal vs unimodal) and across different brain regions. Variance partitioning analysis was conducted to determine the unique and shared contributions of different instruction conditions to brain alignment.

## Key Results
- Instruction-tuned MLLMs significantly outperform vision-only models and match or exceed CLIP in brain alignment, especially in higher-level visual regions
- Image captioning and recognition-related prompts show better brain alignment than other instruction types
- Most neural variance is shared across tasks, with image captioning overlapping highly with other instructions
- Middle layers of some MLLMs align best with high-level visual cortex, while others peak in later layers

## Why This Works (Mechanism)
Instruction tuning enhances MLLMs' ability to generate task-specific representations that better match human brain activity patterns during visual processing. By providing explicit instructions, the models are forced to adopt different computational strategies for the same visual input, creating representations that more closely resemble how the human brain flexibly processes visual information based on task demands. This instruction-specific modulation allows the models to capture both the shared visual-linguistic features and the unique task-relevant aspects of visual scene processing.

## Foundational Learning
- **Voxelwise encoding models**: Statistical models that predict neural responses from model representations at individual voxel resolution. Needed to establish fine-grained correspondence between artificial and biological vision systems.
- **Multimodal representations**: Joint encoding of visual and linguistic information in shared neural or model representations. Critical for understanding how language instructions modulate visual processing.
- **Brain alignment metrics**: Quantitative measures comparing model and brain activity patterns. Essential for objectively evaluating whether models capture human-like visual processing.
- **Layer-wise representation analysis**: Examining how different model layers correspond to different levels of visual processing in the brain. Helps identify which computational stages best match human neural activity.
- **Variance partitioning**: Statistical technique to separate shared vs unique contributions of different conditions. Important for understanding whether instruction-specific representations are truly captured.

## Architecture Onboarding

**Component map**: Image input -> Vision encoder -> Fusion module <-> Language decoder -> Task-specific output (based on instruction)

**Critical path**: Visual input → Early vision layers → Fusion with instruction → Middle/late layers → Brain-aligned representations

**Design tradeoffs**: The study balances model complexity (using state-of-the-art MLLMs) against interpretability (analyzing layer-wise contributions and instruction effects), while choosing fMRI for spatial resolution over temporal dynamics.

**Failure signatures**: Poor brain alignment could indicate inadequate instruction tuning, mismatched visual-linguistic fusion, or failure to capture task-specific modulation of visual processing.

**First experiments**: (1) Test additional instruction types requiring complex reasoning, (2) Conduct ablation studies removing instruction tuning, (3) Extend analysis to MEG for temporal dynamics.

## Open Questions the Paper Calls Out
None

## Limitations
- The variance partitioning analysis showing high overlap between instruction conditions raises questions about whether models are truly capturing instruction-specific representations versus relying on shared visual-linguistic features
- The study does not examine temporal dynamics of instruction processing or individual differences in brain responses
- Uncertainty about whether observed brain alignment reflects genuine task understanding versus superficial linguistic matching

## Confidence
- Core finding that instruction tuning improves brain alignment: **High**
- Observation that not all instructions yield equal alignment: **Medium**
- Interpretation that middle vs late layer alignment reflects different computational strategies: **Medium**

## Next Checks
1. Test additional instruction types that require more complex reasoning to determine if the models can truly differentiate between cognitively distinct tasks
2. Conduct ablation studies removing instruction-tuning to quantify its specific contribution beyond pretraining effects
3. Extend the analysis to other brain imaging modalities like MEG to capture the temporal evolution of instruction-aligned representations