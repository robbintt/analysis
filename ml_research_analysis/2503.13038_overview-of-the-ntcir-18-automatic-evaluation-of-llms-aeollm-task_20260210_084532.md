---
ver: rpa2
title: Overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) Task
arxiv_id: '2503.13038'
source_url: https://arxiv.org/abs/2503.13038
tags:
- evaluation
- llms
- task
- methods
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The NTCIR-18 AEOLLM task addresses the challenge of effectively
  evaluating large language models (LLMs) through automatic methods. The task focuses
  on generative tasks and encourages reference-free evaluation approaches to overcome
  limitations of existing methods.
---

# Overview of the NTCIR-18 Automatic Evaluation of LLMs (AEOLLM) Task

## Quick Facts
- arXiv ID: 2503.13038
- Source URL: https://arxiv.org/abs/2503.13038
- Reference count: 21
- 48 runs from 4 teams on 4 generative subtasks with 400 questions

## Executive Summary
The NTCIR-18 AEOLLM task tackled the challenge of automatically evaluating large language models across four generative tasks without reference answers. The task used carefully curated datasets and human annotations to create ground truth for evaluating reference-free methods. Four diverse subtasks were created: dialogue generation, text expansion, summary generation, and non-factoid question answering. From 4 teams, 48 runs were received, with PanguIR achieving the best accuracy (0.7008 overall) and UCLWI excelling in rank correlation metrics.

## Method Summary
The AEOLLM task framework involved generating answers using multiple LLMs, collecting human annotations as ground truth, and evaluating participant methods through consistency with human judgments. Three metrics were employed: accuracy (pairwise preference agreement), Kendall's tau (τ), and Spearman's rank correlation coefficient (ρ). Participants developed various approaches including direct LLM-as-judge prompting, multi-model collaboration, and in-context learning optimization. The task encouraged reference-free evaluation approaches to overcome limitations of existing methods that require reference answers.

## Key Results
- PanguIR achieved the best accuracy (0.7008 overall) using multi-model collaboration and prompt optimization
- UCLWI excelled in rank correlation metrics (τ = 0.4704, ρ = 0.5074) using ensemble similarity measures and logistic regression
- The text expansion subtask proved most challenging, with highest accuracy reaching only 0.5581
- GPT-4o direct prompting baseline achieved strong performance, suggesting potential for LLM-as-judges approaches

## Why This Works (Mechanism)

### Mechanism 1: LLM-as-Judge Evaluation
- Claim: Direct prompting of strong LLMs like GPT-4o can serve as a viable reference-free evaluator for generative tasks, achieving high correlation with human judgments.
- Mechanism: Strong LLMs can assess the quality of open-ended responses by directly processing the question and answer pair through a prompt designed for evaluation.
- Core assumption: The LLM-as-judge has internalized sufficient knowledge about language quality and task-specific criteria to approximate human judgment without external references or fine-tuning.
- Evidence anchors: GPT-4o baseline achieved strong performance; PanguIR Technical Report details winning submission using related techniques.
- Break condition: The mechanism may fail when evaluation requires criteria outside the judge model's training or when it exhibits systematic biases not present in human annotation pool.

### Mechanism 2: Multi-model Collaboration
- Claim: Combining outputs from multiple LLMs can improve the robustness and accuracy of automatic evaluation compared to single-model approaches.
- Mechanism: Different LLMs may have complementary strengths in assessing various aspects of answer quality. By aggregating their evaluations, the ensemble can mitigate individual model biases and errors.
- Core assumption: The errors and biases of individual models in the ensemble are somewhat independent or can be averaged out.
- Evidence anchors: PanguIR proposes Multi-model Collaboration to approximate human ratings across various subtasks and achieved best performance in overall accuracy.
- Break condition: The mechanism's advantage diminishes if all models share similar fundamental biases or if aggregation method fails to weight more reliable models appropriately.

### Mechanism 3: In-Context Learning (ICL) Optimization
- Claim: Selecting optimal in-context examples based on task-specific feedback enhances the evaluation capability of an LLM without fine-tuning.
- Mechanism: Instead of using random or static examples, a retrieval model is trained to select few-shot examples that are semantically relevant and have proven effective based on evaluation feedback from the training set.
- Core assumption: The effectiveness of in-context examples is correlated with semantic relevance and feedback signals, and a specialized retrieval model can identify these effective examples better than random selection.
- Evidence anchors: PanguIR proposes ICL Optimization to jointly identify the most effective in-context learning examples and achieves best overall accuracy.
- Break condition: Performance degrades if the retrieval model overfits to the training set's feedback or if selected examples inadvertently introduce bias.

## Foundational Learning

- Concept: Kendall's tau (τ) and Spearman's rank correlation coefficient (ρ)
  - Why needed here: These are the primary metrics used to measure how well an automatic evaluation method's rankings of LLM answers correlate with human rankings.
  - Quick check question: Both metrics measure rank correlation, but how do they differ in their sensitivity to the size of the data and the nature of the rank differences?

- Concept: Reference-Free vs. Reference-Based Evaluation
  - Why needed here: The AEOLLM task explicitly encourages reference-free methods to overcome limitations of metrics like BLEU or ROUGE, which require a fixed "gold standard" answer.
  - Quick check question: Why might a reference-based metric like BLEU fail to accurately evaluate a high-quality but creatively different LLM response in a generative task?

- Concept: Prompt Engineering for LLM-as-Judge
  - Why needed here: Several methods, including the strong baseline, rely on designing prompts to instruct an LLM to perform evaluation.
  - Quick check question: What are the key components of a prompt that instructs an LLM to score a response on a scale of 1-5, and how might the prompt wording bias the output score?

## Architecture Onboarding

- Component map: Question Set -> Answer Set (from 7 LLMs) -> Evaluator System -> Predicted Scores/Ranks -> Comparison Module (calculates acc, τ, ρ) against Human Annotated Ground Truth

- Critical path: The most important part of the system to validate is the Evaluator System's logic. Its inputs are the question and an answer. Its output is a score. The pipeline's success hinges entirely on how well this internal logic maps (question, answer) -> score to the human-derived mapping.

- Design tradeoffs:
  - Accuracy vs. Correlation: Optimizing for accuracy might encourage the system to make safe, similar choices, while optimizing for rank correlation (τ, ρ) might encourage it to make clear distinctions, even if it makes more pairwise errors.
  - Complexity vs. Performance: A simple GPT-4o prompt (Baseline4) performed very well. A more complex system like PanguIR's multi-component approach achieved better overall accuracy but requires more engineering, more inference cost, and is harder to debug.

- Failure signatures:
  - Metric Disagreement: High accuracy but low correlation, or vice-versa. This indicates the system is getting some overall patterns right but failing on pairwise comparisons or specific ranking nuances.
  - Task-Specific Collapse: Strong performance on some subtasks (e.g., Dialogue Generation) but near-random performance on others (e.g., Text Expansion). This indicates the evaluation method is not generalizable.
  - Score Clustering: The evaluator assigning the same score to most answers, failing to make the necessary distinctions for ranking.

- First 3 experiments:
  1. Establish a baseline by implementing a direct GPT-4o (or equivalent accessible strong model) evaluator with a simple prompt. Measure its performance on all four subtasks to confirm the paper's reported baseline levels.
  2. Implement a simple ensemble method by averaging scores from 2-3 different open-source LLMs (e.g., Llama, Mistral, Qwen) on the same prompt. Compare its performance to the single-model baseline to test the multi-model collaboration hypothesis.
  3. Design and test two different prompt strategies for a single LLM evaluator (e.g., one focusing on holistic quality, one with a checklist of criteria). Compare the resulting scores and correlations to understand prompt sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific evaluation method improvements can achieve accuracy above 0.56 on long-form text generation tasks (e.g., text expansion), where current methods plateau?
- Basis in paper: The paper states "Text Expansion dataset is the most challenging, with the highest acc being only 0.5581. This presents a challenging scenario for future method optimization."
- Why unresolved: Current participant methods and baselines all struggle on text expansion, with accuracy barely above chance. The paper attributes this to longer answer lengths but offers no solution.
- What evidence would resolve it: A method achieving significantly higher accuracy (e.g., >0.70) on text expansion through techniques specifically designed for long-form content evaluation.

### Open Question 2
- Question: To what extent do LLM-as-judge approaches (e.g., GPT-4o direct prompting) exhibit task-specific biases versus generalizable evaluation capabilities?
- Basis in paper: "Interestingly, our simple baseline, which directly prompts GPT-4o, achieves the best τ and ρ in Text Expansion and Non-Factual QA" while underperforming on other subtasks.
- Why unresolved: The strong baseline performance suggests LLM-as-judge viability, but the inconsistent performance across subtasks indicates potential task-specific biases that remain uncharacterized.
- What evidence would resolve it: Systematic analysis of GPT-4o's evaluation behavior across controlled variations of the same task type, identifying specific failure modes or bias patterns.

### Open Question 3
- Question: How should researchers weigh accuracy versus rank correlation metrics (τ, ρ) when they diverge, and what does their divergence indicate about evaluation method behavior?
- Basis in paper: "acc sometimes differs from the results of these two coefficients. This suggests that considering multiple metrics is necessary to provide a more comprehensive assessment."
- Why unresolved: The paper observes metric disagreement but does not analyze specific cases where they diverge or provide guidance on interpretation when they conflict.
- What evidence would resolve it: Case-level analysis showing where acc and rank correlation disagree, with theoretical or empirical justification for preferring one metric over another in specific evaluation contexts.

## Limitations

- The strong performance of GPT-4o baseline raises questions about whether advanced methods truly provide sufficient incremental value to justify their complexity
- The dataset composition and annotation process details are limited, making it difficult to assess generalizability to other domains or evaluation contexts
- The specific prompts used for baseline evaluation are not disclosed, creating uncertainty about reproducibility of the reported strong baseline performance

## Confidence

- **High Confidence**: The overall framework of using automatic methods to evaluate LLM outputs, the four subtask structure, and the three evaluation metrics (accuracy, τ, ρ) are clearly specified and well-documented
- **Medium Confidence**: The comparative performance rankings between methods (PanguIR achieving best accuracy, UCLWI excelling at correlation metrics) are reliable within this specific task context, though external generalizability remains uncertain
- **Low Confidence**: The relative effectiveness of different methodological components (multi-model collaboration vs. ICL optimization) cannot be fully disentangled from implementation details that were not disclosed

## Next Checks

1. Replicate the GPT-4o baseline evaluation using multiple prompt variants to establish the sensitivity of baseline performance to prompt engineering
2. Conduct ablation studies on PanguIR's method to isolate the contribution of each component (multi-model collaboration, prompt optimization, ICL) to overall performance
3. Test the best-performing methods on an independently collected dataset with different question types to assess domain transfer capability