---
ver: rpa2
title: 'Not All Correct Answers Are Equal: Why Your Distillation Source Matters'
arxiv_id: '2505.14464'
source_url: https://arxiv.org/abs/2505.14464
tags:
- reasoning
- data
- am-thinking-v1
- datasets
- distilled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how the choice of distillation source affects
  the performance of reasoning-oriented language models. The authors construct three
  large-scale datasets (1.89 million queries) by collecting verified reasoning traces
  from AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1.
---

# Not All Correct Answers Are Equal: Why Your Distillation Source Matters

## Quick Facts
- **arXiv ID:** 2505.14464
- **Source URL:** https://arxiv.org/abs/2505.14464
- **Reference count:** 31
- **Primary result:** AM-Thinking-v1 distillation outperforms Qwen3-235B-A22B and DeepSeek-R1 distillation across reasoning benchmarks

## Executive Summary
This paper investigates how the choice of teacher model affects the quality of distilled reasoning capabilities in language models. The authors construct three large-scale datasets (1.89 million queries) by collecting verified reasoning traces from AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1. Through careful data preprocessing and quality assurance, they find that AM-Thinking-v1-distilled data exhibits greater token length diversity and lower perplexity. Student models trained on each dataset are evaluated on reasoning benchmarks, with AM-Thinking-v1 consistently achieving the best performance. The AM-distilled model also demonstrates adaptive generation length—producing longer responses for harder tasks and shorter ones for simpler tasks—aligning with the token-level distribution of its training data.

## Method Summary
The authors construct three distilled datasets from AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1 teachers using 1.89M shared queries. Each query is processed through category-specific verification (Math-Verify for math, sandbox testing for code, reward model scoring for reasoning), with regeneration until verification score ≥0.9. Quality filters include perplexity scoring via 32B model, high-frequency n-gram removal, and structural validation. Student models (Qwen2.5-32B) are fine-tuned for 2 epochs with LR=8e-5, max_seq=32k with packing. Evaluation uses temperature 0.6, top_p 0.95, max_gen 49152 tokens with unified system prompt.

## Key Results
- AM-Thinking-v1-distilled data shows greater token length diversity and lower perplexity than alternatives
- AM-distilled student achieves 84.3 on AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench
- AM-distilled model demonstrates adaptive generation length—longer for harder tasks, shorter for simpler ones
- Training loss is consistently lower for AM-Thinking-v1Distilled across all benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training data with broader token length distributions enables student models to learn adaptive computation allocation—producing longer outputs for harder tasks and shorter for simpler ones.
- **Mechanism:** When training data includes both short (<1024 tokens) and very long (>10240 tokens) reasoning traces across the same query types, the student model learns a mapping between task difficulty and appropriate response length.
- **Core assumption:** Task difficulty correlates with optimal reasoning length; the training distribution teaches this correlation.
- **Evidence anchors:** AM-distilled model produces 3495.7 tokens on MATH500 vs. 18199.2 on AIME2025, aligning with task difficulty.

### Mechanism 2
- **Claim:** Lower perplexity in teacher outputs correlates with more learnable supervision signals, resulting in lower training loss and better generalization.
- **Mechanism:** Lower perplexity indicates teacher outputs are more coherent and structurally consistent with the base model's prior, reducing the optimization burden during distillation.
- **Core assumption:** Perplexity measured by a strong reference model (32B) proxies for output quality and learnability.
- **Evidence anchors:** AM-Thinking-v1 achieves the lowest mean PPL (2.5), and AM-Thinking-v1Distilled maintains consistently lower training loss across benchmarks.

### Mechanism 3
- **Claim:** Iterative verification with category-specific scoring (≥0.9 threshold) ensures only correctly-solved reasoning traces enter training, eliminating false-positive supervision.
- **Mechanism:** Each query is regenerated until the verification criterion passes; math uses symbolic verification, code uses sandbox testing, and reasoning uses reward-model scoring.
- **Core assumption:** Verification procedures accurately distinguish correct from incorrect reasoning within each category.
- **Evidence anchors:** Distillation process repeated on same query until verification score ≥0.9, with category-specific verification methods detailed.

## Foundational Learning

- **Concept: Knowledge Distillation for Reasoning**
  - Why needed here: The entire methodology depends on transferring reasoning capabilities from teacher to student via supervised training on teacher-generated chain-of-thought traces.
  - Quick check question: Can you explain why distillation differs from direct fine-tuning on ground-truth answers?

- **Concept: Perplexity as Quality Proxy**
  - Why needed here: The paper uses perplexity to rank teacher output quality; understanding what perplexity measures is essential to interpret claims.
  - Quick check question: Would lower perplexity always indicate better reasoning, or could it reflect stylistic similarity to the reference model?

- **Concept: Adaptive Computation / Test-Time Scaling**
  - Why needed here: The paper's key behavioral finding is that AM-distilled models allocate more tokens to harder problems—this connects to broader research on compute allocation at inference time.
  - Quick check question: What signals might a model use to decide when to generate longer vs. shorter responses?

## Architecture Onboarding

- **Component map:** Query preprocessing (deduplication → filtering → decontamination) → Distillation loop (teacher inference → category-specific verification → regeneration) → Quality filters (perplexity scoring, high-frequency n-gram removal, structural validation) → Student training (Qwen2.5-32B fine-tuning)
- **Critical path:** Verification quality determines training signal purity. If verification accepts incorrect reasoning, student learns spurious patterns. Start by auditing verification accuracy on held-out samples per category.
- **Design tradeoffs:**
  - Stricter verification thresholds reduce false positives but increase regeneration cost and may shrink dataset size
  - Longer max sequence length (32k) captures extended reasoning but increases training compute; excluding >32k samples may bias against complex problems
  - Single-teacher vs. multi-teacher: Paper uses three teachers independently; mixing or ensembling not explored
- **Failure signatures:**
  - Student produces over-long responses on simple tasks → training data may lack short correct examples
  - Training loss plateaus higher than baseline → verification may be passing noisy samples
  - Large gap between train and benchmark performance → data contamination not fully removed
- **First 3 experiments:**
  1. Ablate verification threshold (0.9 vs. 0.8 vs. 0.95) on a 100K subset; measure downstream accuracy vs. dataset yield
  2. Control for token length: train on AM data filtered to match Qwen3's length distribution; test if adaptive behavior persists
  3. Cross-teacher evaluation: train student on AM data, evaluate on queries where AM and DeepSeek-R1 disagree but both pass verification; analyze which teacher's reasoning style transfers better

## Open Questions the Paper Calls Out
- Can reinforcement learning (RL) techniques like PPO or GRPO further enhance the reasoning capabilities and alignment of models trained on these distilled datasets?
- Do the performance benefits of the AM-Thinking-v1 distilled data generalize to student models with different architectures or significantly smaller parameter counts?
- To what extent is the improved student performance driven by token length diversity versus lower perplexity in the training data?

## Limitations
- Only two of three distilled datasets are publicly available (AM-Thinking-v1 and Qwen3-235B-A22B), limiting full reproducibility
- Evaluation on only four benchmarks may not fully capture generalization across diverse reasoning domains
- The relationship between token length diversity and adaptive computation remains correlative rather than causally established

## Confidence
- **High confidence:** AM-Thinking-v1 achieves superior benchmark performance compared to Qwen3-235B-A22B distillation
- **Medium confidence:** Token length diversity enables adaptive generation behavior (longer for harder tasks)
- **Medium confidence:** Lower perplexity correlates with better distillation quality and lower training loss
- **Low confidence:** Verification threshold of 0.9 optimally balances quality and dataset size (no ablation shown)

## Next Checks
1. Conduct ablation study on verification threshold (0.8, 0.9, 0.95) using released AM-Thinking-v1 dataset to measure impact on downstream accuracy vs. dataset yield
2. Control for token length by training on AM data filtered to match Qwen3's length distribution, then test whether adaptive behavior persists
3. Perform cross-teacher evaluation on held-out samples where AM and DeepSeek-R1 disagree but both pass verification, analyzing which teacher's reasoning style transfers better to the student model