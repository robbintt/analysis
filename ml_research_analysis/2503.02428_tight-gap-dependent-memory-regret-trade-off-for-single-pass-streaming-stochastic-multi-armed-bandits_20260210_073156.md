---
ver: rpa2
title: Tight Gap-Dependent Memory-Regret Trade-Off for Single-Pass Streaming Stochastic
  Multi-Armed Bandits
arxiv_id: '2503.02428'
source_url: https://arxiv.org/abs/2503.02428
tags:
- udcurlymod
- parenleft
- parenright
- alt3
- summation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes the first tight gap-dependent regret bounds
  for single-pass streaming stochastic multi-armed bandits. The problem involves $n$
  arms arriving sequentially, with only $m<n$ arms storable in memory at any time.
---

# Tight Gap-Dependent Memory-Regret Trade-Off for Single-Pass Streaming Stochastic Multi-Armed Bandits

## Quick Facts
- **arXiv ID:** 2503.02428
- **Source URL:** https://arxiv.org/abs/2503.02428
- **Reference count:** 40
- **Primary result:** First tight gap-dependent regret bounds for single-pass streaming stochastic MAB with memory constraints

## Executive Summary
This paper establishes the first tight gap-dependent regret bounds for single-pass streaming stochastic multi-armed bandits with memory constraints. The authors provide matching upper and lower bounds showing how regret scales with memory size m, time horizon T, number of arms n, and the gaps Δ_i between arm means. The key insight is that the optimal algorithm architecture depends critically on whether m ≥ 2n/3 or m < 2n/3, necessitating two distinct algorithmic approaches. These results improve upon previous work by providing tight bounds that correctly capture the dependency on all parameters.

## Method Summary
The paper proposes two algorithms based on memory regime. For m ≥ 2n/3, Algorithm 1 uses a batch pairwise comparison approach where the first m arms are compared in pairs to eliminate 2(n-m) arms, then the remaining stream is read into memory for standard UCB exploitation. For m < 2n/3, Algorithm 2 uses a streaming duel approach where each new arm challenges a random memory occupant with L samples per comparison. Both algorithms use L = (2α/e)^(α/(α+1))·(T/n or T/m)^(1/(α+1)) samples per comparison, balancing exploration cost against retention probability. The exploitation phase uses standard UCB on the final m arms.

## Key Results
- Tight gap-dependent regret bounds for single-pass streaming MAB with memory constraints
- Matching upper and lower bounds showing regret O_α(T^(1/(α+1))/m^(1/(α+1))·∑Δ_i^(1-2α)) for m < 2n/3
- Separate bound O_α((n-m)T^(1/(α+1))/n^(1+1/(α+1))·∑Δ_i^(1-2α)) for m ≥ 2n/3
- Critical threshold at m = 2n/3 requiring different algorithmic architectures
- First tight bounds that correctly capture memory-regret trade-off

## Why This Works (Mechanism)

### Mechanism 1: Thresholded Memory Partitioning (2/3 n)
The efficiency of the algorithm depends critically on whether available memory m exceeds 2n/3, necessitating two distinct architectural strategies. When m ≥ 2n/3, the system has sufficient slack to perform pairwise comparisons among buffered arms and batch-replace losers with the remaining stream. When m < 2n/3, this batch operation is impossible; the system must switch to a sequential "streaming duel" where a new arm challenges a random occupant of the memory, ensuring the memory buffer turns over without overflowing.

### Mechanism 2: Calibrated Exploration Length (L)
Regret is minimized by setting the exploration duration L proportional to (T/n)^(1/(α+1)), balancing the cost of sampling against the risk of discarding the optimal arm. The algorithm does not compare arms indefinitely but sets a fixed sample budget L for each comparison. A larger L decreases the probability of the best arm losing a duel but increases the regret incurred by pulling suboptimal arms during the comparison phase.

### Mechanism 3: Reduction to Best Arm Retention (BAR)
The lower bound proof works by reducing the streaming problem to the "Best Arm Retention" (BAR) problem, demonstrating that any algorithm must essentially solve a BAR instance at the stream's transition point. The authors construct hard instances where the first k arms are statistically similar, and to avoid linear regret, the algorithm must identify the best arm among these k before the stream moves to the final arms.

## Foundational Learning

- **Stochastic Multi-Armed Bandits (MAB):** Base environment where regret is the loss incurred by not pulling the optimal arm. Quick check: If you pull an arm with mean reward μ=0.5 instead of the optimal arm μ*=0.8 for 100 rounds, what is the accumulated regret? (Answer: 30).

- **Gap-Dependent Bounds (Δ_i):** Difficulty scales with how close suboptimal arms are to the best arm. Quick check: Why is the problem "harder" if the gap Δ between best and second-best arm is very small (e.g., 0.001)? (Answer: It requires exponentially more samples to statistically distinguish them).

- **Hoeffding's Inequality / Concentration:** Core logic relies on bounding the probability that a bad arm looks better than a good arm after L samples. Quick check: If you sample an arm L times, how does the confidence interval around the empirical mean shrink as L increases? (Answer: Proportional to 1/√L).

## Architecture Onboarding

- **Component map:** Stream Source -> Memory Buffer -> Sampling Engine -> Strategy Switch -> (Batch-Duel or Stream-Duel) -> UCB Module
- **Critical path:**
  1. Initialization: Load first m (or m-1) arms into Buffer
  2. Calibration: Compute comparison sample budget L based on T, n, α
  3. Streaming Phase (The Filter): 
     - (If Algo 1): Perform internal pairwise duels among buffered arms to clear space; ingest remaining stream
     - (If Algo 2): Ingest new arm → Pull L times → Duel with random buffer occupant → Evict loser
  4. Exploitation Phase: Run standard UCB on surviving buffer population until T expires
- **Design tradeoffs:**
  - Batch vs. Sequential: Algorithm 1 (Batch) is more sample-efficient if memory allows but fragile if memory estimation is wrong
  - Parameter α: Acts as hyperparameter for exploration budget, trading off dependency on T vs. Δ
  - Random Selection (Algo 2): Simple but creates non-smooth regret curve
- **Failure signatures:**
  - Catastrophic Drop: If L is too low, best arm loses early duel and is discarded, resulting in linear regret
  - Memory Overflow: Attempting Algorithm 1 with m < 2n/3 crashes when trying to ingest remainder of stream
- **First 3 experiments:**
  1. Regret vs. Memory (m): Sweep m from 2 to n-1 to verify elbow at m ≈ 2n/3
  2. Gap Sensitivity: Test instances with varying minimum gaps Δ_min to verify ∑Δ^(1-2α) dependency
  3. Algorithm Boundary Stress Test: Run Algorithm 1 at m = ⌊0.66n⌋ and m = ⌈0.67n⌉ to observe performance jump

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the tight gap-dependent regret bounds for the multi-pass streaming MAB setting (P > 1)?
- **Basis:** The paper establishes tight bounds for single-pass case while related work settled minimax regret for multi-pass, leaving gap-dependent case unexplored
- **Why unresolved:** Single-pass constraint forces hard decisions on discarding arms; multiple passes likely change optimal strategy fundamentally
- **What evidence would resolve it:** Analysis deriving matching upper and lower bounds for gap-dependent regret as functions of passes P, memory m, and gaps Δ_i

### Open Question 2
- **Question:** Can an algorithm achieve stated regret bounds without requiring prior knowledge of parameter α?
- **Basis:** Algorithms 1 and 2 explicitly list constant α ≥ 1 as input for determining sampling parameter L
- **Why unresolved:** Analysis relies on tuning sampling length L based on specific trade-off exponent α; not shown if single universal strategy can adapt online
- **What evidence would resolve it:** Design of parameter-free algorithm achieving regret O_α(·) for all α ≥ 1 simultaneously, or proof such adaptivity is impossible

### Open Question 3
- **Question:** How do regret bounds change if arms arrive in random order rather than worst-case order?
- **Basis:** Section 2 explicitly states "We consider the worst-case order of the stream"
- **Why unresolved:** Adversarial order necessitates conservative strategies; random arrival might allow tighter bounds or simpler algorithms
- **What evidence would resolve it:** Theoretical analysis proving improved regret bounds under uniformly random permutation assumption

## Limitations
- Theoretical bounds derived under asymptotic assumptions about "sufficiently large" T without quantifying minimum required T
- Constants in bounds are not optimized, and practical significance for finite problems remains unclear
- Choice of random challenger in Algorithm 2 creates non-smooth regret curves and additional variance

## Confidence
- **High Confidence:** Core theoretical contribution establishing tight gap-dependent bounds for both memory regimes
- **Medium Confidence:** Practical performance of algorithms, particularly Algorithm 2's non-smooth behavior around m = 2n/3 threshold
- **Low Confidence:** Exact constants in bounds and their practical significance for finite problems

## Next Checks
1. **Empirical Threshold Validation:** Run controlled experiments sweeping m from 2 to n-1 with fixed α and T to empirically verify predicted performance jump at m ≈ 2n/3
2. **Constant Sensitivity Analysis:** Test Algorithm 1 at exactly m = ⌊0.66n⌋ and m = ⌈0.67n⌉ to quantify performance difference and validate threshold sharpness
3. **Minimum T Verification:** Systematically vary T from small to large values while keeping other parameters fixed to identify minimum T where theoretical bounds accurately predict empirical performance