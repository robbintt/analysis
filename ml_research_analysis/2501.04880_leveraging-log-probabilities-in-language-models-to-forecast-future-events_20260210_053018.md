---
ver: rpa2
title: Leveraging Log Probabilities in Language Models to Forecast Future Events
arxiv_id: '2501.04880'
source_url: https://arxiv.org/abs/2501.04880
tags:
- probability
- forecast
- event
- future
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of improving AI-driven event
  forecasting by leveraging Large Language Models (LLMs) and their log probability
  outputs. The core method involves a three-component system: a Forecast Generator
  that creates potential future events from topics using current trends, a Probability
  Estimator that calculates event probabilities and uncertainties using weighted log
  probabilities from LLM outputs, and a Fact Checker that verifies if events occurred.'
---

# Leveraging Log Probabilities in Language Models to Forecast Future Events

## Quick Facts
- **arXiv ID:** 2501.04880
- **Source URL:** https://arxiv.org/abs/2501.04880
- **Reference count:** 12
- **Primary result:** Achieved Brier score of 0.186 on 72 forecasts, a 19% improvement over vanilla GPT-4o and 26% better than random chance

## Executive Summary
This paper presents a three-component system for AI-driven event forecasting that leverages Large Language Models and their log probability outputs. The system generates future event forecasts from topics using current trends, estimates probabilities using weighted log probabilities, and verifies outcomes through fact-checking. The key innovation is using log probabilities to compute both weighted probability estimates and uncertainty measures, achieving a 19% improvement over vanilla GPT-4o and 26% better than random chance on 72 backtested forecasts.

## Method Summary
The method employs a three-component system: a Forecast Generator that creates potential future events from topics using current trends, a Probability Estimator that calculates event probabilities and uncertainties using weighted log probabilities from LLM outputs, and a Fact Checker that verifies if events occurred. The Probability Estimator is the core innovation, using log probabilities to compute both weighted averages and uncertainty measures. The system was tested on 72 backtested forecasts, achieving a Brier score of 0.186.

## Key Results
- Achieved Brier score of 0.186 on 72 backtested forecasts
- 19% improvement over vanilla GPT-4o (0.236)
- 26% better than random chance (0.250)
- Fact Checker achieved 150/150 manual accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Weighting probability estimates by log probabilities improves forecast accuracy over single-token sampling.
- **Mechanism:** The system samples multiple token-level probability guesses from the LLM, then computes a weighted average using exp(logprob) as weights. This captures the model's internal uncertainty distribution rather than committing to a single completion.
- **Core assumption:** The log probability of a token reflects the model's calibrated confidence in that estimate being correct.
- **Evidence anchors:** [abstract] "estimate their probabilities via a multi-step approach based on log probabilities"; [section III-B] "ours is the first LLM-based forecasting approach that exploits logprobs to compute the final probability value"; [corpus] Related paper documents that LLM probabilities exhibit incoherence violating probability axioms—suggesting raw outputs need correction.
- **Break condition:** If the LLM's token probability distribution is systematically miscalibrated (e.g., overconfident on rare events), weighting won't fix underlying calibration errors.

### Mechanism 2
- **Claim:** Trend-augmented context compensates for knowledge cutoff and improves forecast grounding.
- **Mechanism:** A semantic filter retrieves relevant trends from ~45,000 articles (6 months ending February 2024), injecting current trajectory information into prompts before generation and estimation.
- **Core assumption:** Recent news articles contain signal predictive of near-future outcomes.
- **Evidence anchors:** [abstract] "employ data on current trends and their trajectories for generating forecasts"; [section III-A] "This method updates the model's knowledge, which is typically limited to some past cut-off date"; [corpus] "Wisdom of the Crowds in Forecasting" suggests ensemble and summarization approaches help—but no direct corpus evidence on trend extraction specifically.
- **Break condition:** If trend sources are biased, stale, or irrelevant to the target domain, injected context adds noise rather than signal.

### Mechanism 3
- **Claim:** Post-hoc calibration via SVR maps raw LLM outputs to better-calibrated probabilities.
- **Mechanism:** A held-out calibration set (N=72 after filtering) trains a Support Vector Regression to learn a transformation from LLM-returned probabilities to observed outcome rates.
- **Core assumption:** The relationship between predicted and actual probabilities is learnable and generalizes to unseen forecasts.
- **Evidence anchors:** [section III-D] "we learn a transformation function between LLM-returned values and the final probability output"; [section IV, Figure 3] Calibration plots show improved alignment between predicted probabilities and actual conversion rates compared to vanilla GPT-4o; [corpus] Weak direct evidence on SVR calibration specifically for LLM forecasting.
- **Break condition:** If test distribution diverges from calibration set (different topics, time horizons), learned transformation may over-correct or under-correct.

## Foundational Learning

- **Concept: Log Probabilities**
  - Why needed here: The method's core innovation requires understanding how LLMs assign logprobs to tokens and why exp(logprob) yields a valid weight.
  - Quick check question: If a token has logprob -0.51, what is its probability weight?

- **Concept: Brier Score**
  - Why needed here: All performance claims use Brier score; interpreting "0.186 vs 0.236" requires understanding the metric.
  - Quick check question: What Brier score would a perfectly calibrated forecaster achieve? What about random guessing on binary events?

- **Concept: Probability Calibration**
  - Why needed here: The system explicitly learns a calibration function; understanding miscalibration is prerequisite.
  - Quick check question: If a model predicts 70% probability for 100 events but only 40% occur, is it overconfident or underconfident?

## Architecture Onboarding

- **Component map:**
  - Forecast Generator: topic → trend retrieval (semantic filter) → LLM → {title, description, timeframe}
  - Probability Estimator: forecast + trend search + news sources → 6-step pipeline → {P̂, Û, trends}
  - Fact Checker: forecast + timeframe → news search → LLM → {1, 0, -1}
  - Calibration layer: SVR trained on held-out labeled outcomes

- **Critical path:** Probability Estimator is the bottleneck—requires 6 sequential LLM calls plus external search. Latency dominated by step 3 (semantic news search) and step 6 (multi-token sampling for probability distribution).

- **Design tradeoffs:**
  - Temperature/top-P set to minimum for determinism (reduces variance but may suppress diverse reasoning paths)
  - 50/50 train-test split for calibration reduces training data; Assumption: smaller calibration set acceptable given SVR sample efficiency
  - Fact Checker achieved 150/150 manual accuracy; Assumption: this holds for future automated tracking

- **Failure signatures:**
  - Incoherence: same event, different probabilities across runs (target: <1% divergence)
  - Inconsistency: mutually exclusive events summing >100%
  - Topic-specific miscalibration: Climate Change underestimates, Energy overestimates (Figure 4)

- **First 3 experiments:**
  1. Reproduce Brier score on held-out test set with and without logprob weighting to isolate contribution.
  2. Ablate trend retrieval: compare forecasts with raw LLM knowledge vs. trend-augmented context.
  3. Calibration stability test: train SVR on different random splits; report variance in learned transformation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can topic-aware calibration models correct the systematic probability over- and under-estimation observed in specific domains like Climate Change and Energy?
- **Basis in paper:** [explicit] The conclusion states, "In the future, even the semantic dimension may be used to calibrate probabilities better," referencing the performance variance by topic shown in Figure 4.
- **Why unresolved:** The current approach uses a global Support Vector Regression model for calibration, which does not account for the systematic biases found in specific semantic topics.
- **What evidence would resolve it:** Ablation studies showing that applying topic-specific calibration functions significantly reduces Brier scores for previously biased domains compared to the global model.

### Open Question 2
- **Question:** How does the practical utility of autonomous "blue-sky" forecasts compare to that of pre-defined forecasting questions in strategic planning?
- **Basis in paper:** [explicit] The authors note that while accuracy is tested, "future evaluations may target the utility of the forecast themselves."
- **Why unresolved:** The current evaluation relies entirely on Brier scores (accuracy), which measures correctness but does not capture the novelty, actionability, or economic value of the generated scenarios.
- **What evidence would resolve it:** User studies or decision-theoretic metrics assessing the marginal value added by generated forecasts over standard question-answering forecasting systems.

### Open Question 3
- **Question:** Does the log-probability weighting approach outperform advanced, non-vanilla baselines that use retrieval-augmented generation or tool-use?
- **Basis in paper:** [inferred] The paper benchmarks against "vanilla GPT-4o" and random chance but acknowledges related systems (e.g., Reasoning and Tools) without directly comparing against them on the same dataset.
- **Why unresolved:** It is undetermined if the performance improvement stems from the proposed log-probability method or simply from the system's trend retrieval and synthesis capabilities.
- **What evidence would resolve it:** A head-to-head comparison on a standardized dataset between this method and other state-of-the-art retrieval-based forecasting agents.

## Limitations
- Small test set (N=72) may not be representative of broader forecasting domains
- Lack of transparency in semantic trend extraction process and validation against future events
- Fact Checker's 150/150 accuracy claim represents only a single point-in-time validation

## Confidence
- **High confidence:** The mechanical implementation of log probability weighting and the mathematical framework for probability estimation
- **Medium confidence:** The calibration layer's effectiveness due to small test set size and potential topic-specific miscalibration
- **Low confidence:** The external validity of results beyond the specific 15 topics tested

## Next Checks
1. **Cross-topic calibration stability:** Train the SVR calibration layer on different random splits of the test set and report variance in Brier scores across topics
2. **Temporal robustness test:** Re-run the forecasting system on a different 6-month period to assess whether trend extraction and probability estimation maintain their advantage when underlying event distributions shift
3. **Baseline comparison expansion:** Compare against alternative probability aggregation methods beyond vanilla GPT-4o, such as ensemble approaches combining multiple LLM calls or traditional statistical forecasting models