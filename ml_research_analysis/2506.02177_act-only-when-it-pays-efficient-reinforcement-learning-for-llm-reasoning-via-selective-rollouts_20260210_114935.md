---
ver: rpa2
title: 'Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning
  via Selective Rollouts'
arxiv_id: '2506.02177'
source_url: https://arxiv.org/abs/2506.02177
tags:
- training
- rollout
- prompts
- greso
- zero-variance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GRESO, a method to speed up reinforcement learning
  for LLM reasoning by selectively skipping uninformative prompts before rollout.
  It observes that prompts yielding zero variance in rewards tend to remain uninformative
  across training epochs, and leverages this temporal consistency to predict and skip
  them using a lightweight online filtering algorithm based on reward dynamics.
---

# Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts

## Quick Facts
- **arXiv ID**: 2506.02177
- **Source URL**: https://arxiv.org/abs/2506.02177
- **Reference count**: 40
- **Primary result**: GRESO achieves up to 2.4x speedup in rollout and 2.0x speedup in total training time while preserving model accuracy on math reasoning tasks.

## Executive Summary
This paper addresses the computational inefficiency in reinforcement learning for LLM reasoning, where many prompts yield uninformative responses that provide zero gradient signal. GRESO (Group Relative Policy Optimization with Efficient Selective Rollouts) introduces a selective rollout mechanism that predicts and skips zero-variance prompts before expensive generation, achieving significant speedups without accuracy degradation. The method leverages temporal correlation in prompt informativeness, probabilistic filtering for exploration, and adaptive controllers for hyperparameter-free operation across different models and datasets.

## Method Summary
GRESO operates by maintaining per-prompt statistics tracking consecutive zero-variance observations, then computing a skip probability that exponentially decreases with more zero-variance occurrences. Before each rollout, it filters prompts probabilistically rather than deterministically, allowing informative prompts to be revisited. An adaptive controller maintains separate exploration probabilities for easy and hard zero-variance prompts, adjusting based on observed versus target zero-variance ratios. The method also includes an adaptive batch sizing mechanism to avoid oversampling while maintaining throughput. GRESO is specifically designed for GRPO (Group Relative Policy Optimization) where zero-variance groups produce zero advantage and thus no learning signal.

## Key Results
- Up to 2.4x speedup in rollout time and 2.0x speedup in total training time across three models and six math reasoning benchmarks
- Maintains accuracy within 0.5% of baseline Dynamic Sampling method while significantly reducing computational overhead
- Achieves 95%+ effective prompt ratio in late training stages compared to ~20% for Dynamic Sampling
- Eliminates manual hyperparameter tuning through self-adjustable exploration probabilities

## Why This Works (Mechanism)

### Mechanism 1
Zero-variance prompts (where all responses receive identical rewards) are temporally correlated, making them predictable candidates for skipping. When GRPO computes advantages within response groups, zero-variance groups yield zero advantage and no gradient signal. GRESO exploits this by tracking consecutive zero-variance counts (z_i) and computing skip probability p_f(x_i) = 1 - p_e^{z_i}, which increases exponentially with z_i. The method relies on observed >90% temporal correlation in math reasoning tasks, with ~20% of zero-variance prompts becoming effective again in later epochs.

### Mechanism 2
Probabilistic filtering preserves exploration while achieving efficiency gains. Rather than deterministically pruning zero-variance prompts, GRESO samples each prompt with probability p_e^{z_i}, allowing even highly-skippable prompts a chance to be revisited. This probabilistic approach accounts for model improvement over time, where previously ineffective prompts may become informative. The method maintains a minimum 5% exploration rate to ensure recoverable prompts aren't permanently pruned.

### Mechanism 3
Self-adjustable base exploration probability eliminates manual hyperparameter tuning. GRESO maintains separate p_e values for easy (all-incorrect) and hard (all-correct) zero-variance prompts, adjusting them based on observed versus target zero-variance ratios. Using a PID-like control mechanism, p_e decreases when observed ratio exceeds target and increases when below target. The 1:2 allocation ratio between easy and hard targets reflects the intuition that more capable models benefit from increased exploration of difficult examples.

## Foundational Learning

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed here: GRESO is built specifically for GRPO's advantage computation mechanism. Understanding that GRPO normalizes rewards within groups of responses to the same prompt—and that zero-variance groups yield zero advantage—is essential to understanding why filtering works.
  - Quick check question: If you sample 8 responses to a prompt and 6 are correct (reward=1) and 2 are incorrect (reward=0), what is the advantage for a correct response?

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: The paper assumes binary or scalar verifiable rewards (e.g., math answer correctness). The zero-variance phenomenon is specific to settings where rewards are determined by external verification rather than learned reward models.
  - Quick check question: Why would zero-variance prompts be less common if rewards came from a continuous learned reward model versus a binary verifier?

- **Concept: Dynamic Sampling baseline**
  - Why needed here: GRESO's efficiency claims are relative to Dynamic Sampling (oversample, filter post-rollout). Understanding that DS wastes computation on prompts that will be discarded is critical to understanding GRESO's value proposition.
  - Quick check question: If 80% of prompts are zero-variance, how many rollouts does Dynamic Sampling need to fill a batch of 256 effective prompts?

## Architecture Onboarding

- **Component map**: Training Dynamics Tracker → Probabilistic Filter → Rollout Engine → Post-Rollout Filter → GRPO Update → Adaptive Controller

- **Critical path**: The filter must complete before rollout begins; tracker update happens after rewards are computed. Training Dynamics Tracker maintains per-prompt history, Probabilistic Filter computes skip probability, Rollout Engine performs standard vLLM-based generation, Post-Rollout Filter removes zero-variance prompts, Adaptive Controller adjusts exploration probabilities.

- **Design tradeoffs**: Probabilistic vs. deterministic pruning preserves exploration but introduces variance in batch composition. Separate easy/hard p_e values provide more adaptive control but require difficulty classification. Adaptive batch size reduces waste but adds complexity with safety factor β=1.25.

- **Failure signatures**: Low effective prompt ratio suggests p_e too low or target ratio mis-specified. Accuracy degradation indicates over-aggressive filtering requiring higher minimum exploration rate. No rollout time reduction suggests adaptive batch size not triggering. Wild p_e oscillations indicate Δp too large or unachievable target ratios.

- **First 3 experiments**:
  1. Baseline comparison: Replicate Table 1 on single model/dataset pair, verify rollout count reduction matches 2-3.35x while accuracy stays within 0.5% of Dynamic Sampling.
  2. Ablation on adaptive components: Run GRESO with fixed p_e (no adaptation), then without adaptive batch size. Isolate contribution using rollout time per step as metric.
  3. Sensitivity to target ratio: Sweep target zero-variance ratio (0%, 25%, 50%, 100%) per Appendix F.1. Confirm accuracy robustness while efficiency varies; identify optimal range.

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness depends on temporal correlation of zero-variance prompts, which may not hold in non-math domains or with different reward structures
- The 20% recovery rate observed in math reasoning may be domain-specific with no validation on text generation or other verifiable-reward tasks
- Adaptive controller assumes stable target ratios but lacks extensive validation across hyperparameter ranges

## Confidence
- **High confidence**: Rollout time reduction claims (2.0-2.4x) are well-supported by controlled experiments comparing against Dynamic Sampling baseline
- **Medium confidence**: Accuracy preservation relies on probabilistic filtering maintaining sufficient exploration, but minimum 5% rate may be insufficient for some domains
- **Low confidence**: Claims about generalizability to non-math domains are unsupported - all experiments use math reasoning benchmarks with binary correctness rewards

## Next Checks
1. **Domain generalization test**: Apply GRESO to text summarization with ROUGE-based verifiable rewards. Measure whether temporal correlation of zero-variance prompts still holds and whether 20% recovery rate is maintained.

2. **Reward structure sensitivity**: Test GRESO with continuous rewards (e.g., graded math problems with partial credit) rather than binary correctness. Evaluate whether zero-variance patterns persist and whether filtering mechanism remains effective.

3. **Adaptive controller robustness**: Systematically vary target zero-variance ratios (0%, 10%, 25%, 50%, 75%, 100%) and measure impact on both efficiency gains and final accuracy. Identify range maintaining accuracy within 1% of baseline while maximizing speedup.