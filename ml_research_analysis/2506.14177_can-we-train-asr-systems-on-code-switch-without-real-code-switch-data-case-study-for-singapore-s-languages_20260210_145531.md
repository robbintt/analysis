---
ver: rpa2
title: Can we train ASR systems on Code-switch without real code-switch data? Case
  study for Singapore's languages
arxiv_id: '2506.14177'
source_url: https://arxiv.org/abs/2506.14177
tags:
- data
- speech
- code-switching
- mono
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates building code-switching ASR systems without
  real code-switch data by generating synthetic data through phrase-level mixing of
  monolingual corpora. The method translates and aligns monolingual texts, then replaces
  10-30% of phrases per sentence to mimic natural code-switching patterns, creating
  synthetic speech by splicing aligned audio segments.
---

# Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages

## Quick Facts
- arXiv ID: 2506.14177
- Source URL: https://arxiv.org/abs/2506.14177
- Reference count: 0
- Can train CS-ASR on synthetic data generated from monolingual corpora, achieving 43.6-83.1% relative WER improvement

## Executive Summary
This paper demonstrates that high-quality code-switching ASR systems can be built without real code-switched data by generating synthetic code-switched speech from monolingual corpora. The method translates and aligns monolingual texts, then replaces 10-30% of phrases per sentence to mimic natural code-switching patterns, creating synthetic speech by splicing aligned audio segments. Experiments on three under-resourced Southeast Asian language pairs (BM-EN, ZH-BM, TA-EN) show that training on monolingual augmented with synthetic code-switched data improves ASR performance across all language pairs.

## Method Summary
The approach involves translating monolingual text from one language to another, aligning the translated text with the original using FAST_ALIGN, and then replacing 10-30% of consecutive tokens per sentence with their translated equivalents to create synthetic code-switched text. Audio is generated by training HMM-DNN models for forced alignment on monolingual audio, then splicing segments according to the synthetic code-switched text with amplitude normalization. The resulting synthetic code-switched datasets (3,000 hours total, 1,000 hours per pair) are used to fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T) alongside monolingual data.

## Key Results
- Phrase-level synthetic code-switching improves ASR performance when real CS data is unavailable
- Best model (SeamlessM4T-v2-Large) achieved relative WER improvements of 83.1% on BM-EN, 43.6% on ZH-BM, and 64.4% on TA-EN
- Performance gains correlate with distributional alignment between synthetic training and real test data
- Synthetic CS training mitigates language bias in pretrained multilingual models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Phrase-level synthetic code-switching improves ASR performance when real CS data is unavailable
- Mechanism: Monolingual text is translated (L1→L2) and aligned using FAST_ALIGN. Then 10-30% of consecutive tokens per sentence are replaced with translated equivalents, creating phrase-level switches rather than isolated word substitutions. Audio is generated by training HMM-DNN models for forced alignment, then splicing segments according to the synthetic CS text with amplitude normalization.
- Core assumption: Natural code-switching in the target population occurs primarily at phrase level (rather than word level), and distributional similarity between synthetic and real CS patterns drives transfer.
- Evidence anchors:
  - [abstract] "We propose a phrase-level mixing method to generate synthetic CS data that mimics natural patterns."
  - [Section 2.2] "Singaporean code-switching occurs primarily at the phrase level rather than the word level... we replace the fixed 20% lexicon substitution with a more flexible 10-30% mixing per sentence."
  - [corpus] Weak external validation; neighboring papers (TSPC, HiKE, AdaCS) address CS-ASR but do not validate phrase-mixing specifically.
- Break condition: If target population exhibits word-level rather than phrase-level CS, or if train-test distributional mismatch is large (as seen in ZH-BM), gains diminish significantly.

### Mechanism 2
- Claim: Large-scale multilingual pretrained models can be adapted to CS-ASR via fine-tuning on synthetic data
- Mechanism: Pretrained models (Whisper, MMS, SeamlessM4T) with existing multilingual representations are fine-tuned on combined MONO + synthetic CS data. The pretrained cross-lingual representations provide a foundation; synthetic CS data teaches language boundary handling without requiring real CS transcripts.
- Core assumption: Pretrained multilingual representations contain transferable phonetic/linguistic knowledge; synthetic CS exposure is sufficient to activate CS capability.
- Evidence anchors:
  - [abstract] "...fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T)"
  - [Section 4.3, Table 1] MONO+CS consistently outperforms BASELINE and MONO-only across all three models for CS test sets.
  - [corpus] Adapting Whisper via Soft Prompt Tuning (arXiv:2506.21576) shows related parameter-efficient CS adaptation, supporting the pretrained-adaptation paradigm.
- Break condition: If base model has poor coverage of one language in the pair (e.g., MMS character-level struggles with Tamil), synthetic CS data alone cannot compensate without vocabulary/architecture changes.

### Mechanism 3
- Claim: Performance gains correlate with distributional alignment between synthetic training and real test CS patterns
- Mechanism: Code-Mixing Index (CMI), I-Index (switch frequency), and M-Index (language balance) quantify CS characteristics. When synthetic training data closely matches real test data on these metrics, models achieve larger WER reductions. BM-EN showed strong alignment and 83.1% relative improvement; ZH-BM showed mismatched M-Index (14.68 vs 21.40) and only 43.6% improvement.
- Core assumption: These three indices capture the linguistically relevant dimensions of CS that affect ASR difficulty.
- Evidence anchors:
  - [Section 4.1, Table 3] Shows CMI, I-Index, M-Index for training vs test sets; BM-EN aligns well, ZH-BM shows language balance gap.
  - [Section 4.3] "...gains vary by language pair: BM-EN shows the largest improvement... aligning with the linguistic analysis in Section 4.1"
  - [corpus] No direct external validation of CMI/I-Index/M-Index as predictive metrics for CS-ASR transfer.
- Break condition: If real CS patterns involve prosodic, syntactic, or pragmatic features not captured by token-level mixing indices, alignment on these metrics will not guarantee transfer.

## Foundational Learning

- Concept: **Code-Switching Types (Intra-sentential vs. Inter-sentential)**
  - Why needed here: The method targets phrase-level (intra-sentential) CS; inter-sentential CS (sentence-level alternation) is easier and may not require synthetic augmentation. The TA-EN test set was inter-sentential and showed different error patterns.
  - Quick check question: In your target use case, do speakers switch languages mid-sentence (phrase-level) or between sentences?

- Concept: **Forced Alignment for Audio Splicing**
  - Why needed here: Synthetic CS audio generation requires precise word/phrase boundaries in source monolingual audio to splice correctly. HMM-DNN or HMM-GMM alignment provides these timestamps.
  - Quick check question: Do your monolingual audio corpora have quality alignments, or will you need to train aligners first?

- Concept: **Language Bias in Multilingual Models**
  - Why needed here: Pretrained models exhibit language bias (e.g., MMS and SeamlessM4T showed strong ZH bias on ZH-BM, initially failing to detect BM). Synthetic CS training mitigates but does not eliminate this.
  - Quick check question: Does your base model already show balanced performance across both languages in your target pair?

## Architecture Onboarding

- Component map:
  - Monolingual corpus → Translation (Google/Mesolitica) → FAST_ALIGN alignment → Phrase-level replacement (10-30%) → CS text
  - Monolingual audio → HMM-DNN forced alignment → Amplitude normalization → Segment splicing guided by CS text → CS audio
  - MONO + CS datasets → Fine-tune pretrained ASR (Whisper/MMS/SeamlessM4T) → Evaluate on CS test sets
  - CMI/I-Index/M-Index for distributional analysis; WER/CER/MER for accuracy

- Critical path:
  1. Verify monolingual corpus quality and coverage for both languages
  2. Validate phrase-mixing produces realistic CS patterns for your target population
  3. Train forced aligners if not available
  4. Generate synthetic CS data (aim for 1000+ hours as in paper)
  5. Fine-tune with combined MONO+CS, monitor for language bias

- Design tradeoffs:
  - **Translation quality vs. speed**: Google Translate is fast but may introduce errors; specialized models (Mesolitica for ZH-BM) may be better but require setup
  - **Adapter vs. full fine-tuning**: MMS adapters preserved monolingual performance but underperformed on CS; full fine-tuning with BPE vocabulary worked better
  - **Synthetic data volume**: Paper used 1000 hours per pair; diminishing returns unknown

- Failure signatures:
  - High insertion errors on one language (WHISPER TURBO-V3 on TA-EN baseline showed >100% WER due to insertions)
  - Complete failure to recognize one language (MMS/SeamlessM4T on ZH-BM initially showed strong ZH bias, near-zero BM detection)
  - MONO fine-tuning degrades English performance on local accents (observed in WHISPER TURBO-V3)

- First 3 experiments:
  1. **Baseline characterization**: Run pretrained models on monolingual test sets for both languages; identify existing biases and coverage gaps
  2. **Small-scale synthesis pilot**: Generate 10-50 hours of synthetic CS data; compute CMI/I-Index/M-Index and compare to any available real CS samples
  3. **Ablation on mixing ratio**: Test phrase-level replacement at 10%, 20%, 30% to find optimal mixing intensity for your target population's CS patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating prosodic modeling or syntactic constraints into the phrase-mixing pipeline improve performance for language pairs with poor synthetic-to-real distribution alignment, such as ZH-BM?
- Basis in paper: [explicit] The Conclusion states: "Future work should focus on enhancing the linguistic realism of synthetic data, for example by integrating prosody or syntactic rules."
- Why unresolved: The current random 10-30% phrase replacement resulted in a "big gap" in language balance (M-Index) between training and test sets for ZH-BM, causing lower performance gains compared to BM-EN.
- What evidence would resolve it: A comparative study showing improved WER/CER on ZH-BM when using syntactically-constrained synthetic data versus the current random phrase-mixing approach.

### Open Question 2
- Question: Does the phrase-mixed training strategy generalize to natural, intra-sentential Tamil-English code-switching despite being evaluated only on a synthetic, inter-sentential test set in this study?
- Basis in paper: [inferred] The authors used a "synthetic sentence-mixed TA-EN test set" due to resource constraints, which exhibits significantly lower switching frequency (I-Index 6.50 vs 37.33) than the training data.
- Why unresolved: The model's effectiveness on natural, conversational TA-English code-switching remains unverified because the evaluation benchmark was structurally different (inter-sentential) from the training data (phrase-mixed).
- What evidence would resolve it: Evaluating the fine-tuned TA-EN models on a newly collected corpus of naturally spoken Singaporean Tamil-English code-switching.

### Open Question 3
- Question: Does the audio splicing technique introduce acoustic artifacts at code-switch boundaries that limit the model's ability to learn natural prosodic transitions?
- Basis in paper: [inferred] The method relies on splicing audio segments with amplitude-based normalization. While noted as an improvement over energy-based methods, it artificially joins disconnected acoustic segments.
- Why unresolved: The study does not isolate the acoustic impact of the "Audio Splicing" step; performance gains might be text-driven, while the spliced audio could be introducing learnable "synthetic" artifacts rather than natural code-switch patterns.
- What evidence would resolve it: An ablation study comparing performance when training on spliced audio versus training on resynthesized code-switched speech (TTS) or naturally spoken code-switched data.

## Limitations
- Phrase-level synthetic mixing method assumes natural CS patterns match 10-30% consecutive token replacement strategy
- Approach relies heavily on translation quality, which can introduce errors for under-resourced languages
- Evaluation depends on relatively small test sets (25 utterances per pair) that may not capture full CS diversity

## Confidence
*High Confidence*: The core empirical finding that synthetic code-switching data improves ASR performance when real CS data is unavailable is well-supported by consistent WER improvements across all three language pairs and all three model architectures tested.

*Medium Confidence*: The correlation between distributional alignment (CMI/I-Index/M-Index) and performance gains is suggestive but based on only three language pairs.

*Low Confidence*: The claim that phrase-level mixing specifically is optimal for mimicking natural code-switching patterns lacks external validation.

## Next Checks
1. **Distributional Alignment Validation**: Compute CMI, I-Index, and M-Index on a held-out subset of real code-switching data (if available) and systematically vary the synthetic mixing parameters to identify which configurations best match real distributions.

2. **Cross-Linguistic Generalization Test**: Apply the exact methodology to at least two additional language pairs with different typological distances and varying CS patterns to test whether the 10-30% phrase-level replacement strategy generalizes.

3. **Synthetic Data Scaling Analysis**: Systematically vary the amount of synthetic code-switching data while keeping monolingual data constant to identify the point of diminishing returns and test whether synthetic CS data can compensate for reduced monolingual data availability.