---
ver: rpa2
title: 'AGZO: Activation-Guided Zeroth-Order Optimization for LLM Fine-Tuning'
arxiv_id: '2601.17261'
source_url: https://arxiv.org/abs/2601.17261
tags:
- uni00000011
- agzo
- gradient
- activation
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AGZO, an activation-guided zeroth-order optimization
  method for memory-efficient fine-tuning of large language models. The key insight
  is that gradients of linear layers are confined to the subspace spanned by forward
  activations, allowing more informative perturbations than isotropic methods.
---

# AGZO: Activation-Guided Zeroth-Order Optimization for LLM Fine-Tuning

## Quick Facts
- arXiv ID: 2601.17261
- Source URL: https://arxiv.org/abs/2601.17261
- Reference count: 40
- Primary result: AGZO outperforms state-of-the-art zeroth-order methods on Qwen3 and Pangu models while maintaining memory footprints nearly identical to inference-only approaches.

## Executive Summary
AGZO introduces an activation-guided zeroth-order optimization method for memory-efficient fine-tuning of large language models. The key insight is that gradients of linear layers are confined to the subspace spanned by forward activations, allowing more informative perturbations than isotropic methods. AGZO extracts compact activation subspaces via lightweight power iteration and restricts perturbations to these subspaces, achieving better gradient alignment while maintaining minimal memory overhead. Empirically, AGZO consistently outperforms state-of-the-art ZO methods across multiple benchmarks while nearly matching first-order fine-tuning performance with significantly reduced memory requirements.

## Method Summary
AGZO builds on MeZO's two-point estimator framework but improves gradient estimation by constraining perturbations to the subspace spanned by input activations for linear layers. During the forward pass, it captures activation matrices Hℓ at each linear layer, applies power iteration (K=3 steps) to extract top-r singular vectors forming basis Aℓ, then generates perturbations Δℓ = RA⊤ℓ within this subspace. For nonlinear layers, it falls back to standard Gaussian perturbations. The method applies in-place perturbations that are regenaratable from stored seeds, allowing immediate cleanup of activation matrices Hℓ to maintain minimal memory footprint. The rank r=1 is used throughout to minimize memory overhead while concentrating exploration on dominant directions.

## Key Results
- AGZO consistently outperforms state-of-the-art zeroth-order methods (MeZO, LOZO) on Qwen3-0.6B, Qwen3-4B, and Pangu-1B models across multiple benchmarks
- Achieves significantly better gradient alignment (cosine similarity) compared to isotropic baselines while maintaining nearly identical memory footprints
- Narrowly bridges the performance gap with first-order fine-tuning methods while requiring only forward passes
- Memory usage scales nearly identically to inference-only approaches, with AGZO≈MeZO≪FO (where FO often OOMs early)

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Activation Subspace Confinement
- Claim: Linear layer gradients lie strictly within the subspace spanned by input activations
- Mechanism: Backpropagation computes weight gradients as ∇Wℓf = QℓH⊤ℓ, so row(∇Wℓf) ⊆ col(Hℓ). Sampling perturbations outside this subspace wastes queries on directions carrying zero gradient signal.
- Core assumption: The layer is linear (nonlinear layers fall back to Gaussian perturbations)
- Evidence anchors: [Section 3.1], [Figure 1(a)], [corpus] "Elucidating Subspace Perturbation in Zeroth-Order Optimization"

### Mechanism 2: Activation Spectral Concentration
- Claim: Activation matrices exhibit rapid singular value decay, making low-rank approximation effective
- Mechanism: Power iteration approximates top-r left singular vectors of HℓH⊤ℓ with minimal overhead. Leading directions capture most activation energy, which correlates with gradient energy via Mechanism 1.
- Core assumption: Spectral decay pattern holds across training iterations and tasks
- Evidence anchors: [Figure 1(c)], [Section 3.2], [corpus] "Towards Fast LLM Fine-tuning through Zeroth-Order Optimization with Projected Gradient-Aligned Perturbations"

### Mechanism 3: Improved Directional Alignment
- Claim: Subspace-constrained perturbations yield higher expected cosine similarity to true gradients than isotropic baselines
- Mechanism: Theorem 5.4 shows E[cos(ĜAGZO, G)] = β(dout·r) · ‖GA‖F/‖G‖F. The β coefficient scales as O(1/√(dout·r)), while energy ratio is ≈1 when r captures activation subspace. Isotropic MeZO only gets β(dout·din) without energy benefit.
- Core assumption: Assumption 22 in Theorem 5.6—upstream gradient energy not adversarially concentrated in low-singular-value directions
- Evidence anchors: [Theorem 5.6], [Figure 2], [corpus] "Robust and Efficient Zeroth-Order LLM Fine-Tuning via Adaptive Bayesian Subspace Optimizer"

## Foundational Learning

- Concept: Zeroth-order gradient estimation via finite differences
  - Why needed here: AGZO builds on MeZO's two-point estimator (f(W+μΔ) − f(W))/μ to approximate gradients without backpropagation
  - Quick check question: Can you explain why ZO methods avoid storing activations for backward passes?

- Concept: Power iteration for approximate SVD
  - Why needed here: AGZO uses Algorithm 1 to extract top-r singular vectors from HℓH⊤ℓ efficiently during forward pass
  - Quick check question: How many matrix multiplications are needed for K=3 power iteration steps on a din×m activation matrix?

- Concept: Frobenius inner product and cosine similarity for matrices
  - Why needed here: Theoretical analysis (Theorem 5.4) and empirical validation (Figure 2) measure gradient alignment via matrix cosine similarity
  - Quick check question: What does ⟨A, B⟩F = tr(A⊤B) compute for two matrices?

## Architecture Onboarding

- Component map: Forward pass hooks → capture Hℓ → SUBSPACEEXTRACT (Algorithm 1) → compute orthonormal basis Aℓ → Perturbation generator → sample Rℓ ~ N(0,I) → form Δℓ = RℓA⊤ℓ for linear layers → In-place perturbation → Wℓ += μΔℓ → Gradient estimator → (f+ − f0)/μ · Δℓ → Memory cleanup → discard Hℓ after Aℓ computed; keep only Aℓ ∈ R^(din×r)

- Critical path: Forward pass (compute f0, extract {Aℓ}) → In-place perturbation → Forward pass (compute f+) → Regenerate Δℓ → Update: Wℓ ← Wℓ − η·g·Δℓ

- Design tradeoffs:
  - Rank r: r=1 minimizes memory (Section 4.2) and concentrates on dominant direction; higher r may capture more gradient energy but dilutes signal
  - Power steps K: K=3 balances approximation quality vs compute (Section 4.1)
  - Smoothing μ: μ=10⁻⁷ (Qwen3) or 10⁻⁴ (Pangu BF16)—too small causes numerical noise; too large introduces smoothing bias

- Failure signatures:
  - Memory not releasing: Hℓ not discarded after Aℓ extraction → memory matches FO training
  - Cosine similarity not improving: Power iteration failing to converge → check K steps and orthonormalization
  - Training instability: μ too large for BF16 → increase perturbation scale

- First 3 experiments:
  1. Baseline comparison: Run MeZO, LOZO, AGZO on SST-2 with Qwen3-0.6B; log cosine similarity to true gradient every 100 steps. Expected: AGZO > LOZO > MeZO in alignment.
  2. Memory sweep: Vary sequence length [128, 256, 512, 1024] with batch=4 on DROP; compare peak GPU memory. Expected: AGZO ≈ MeZO ≪ FO (FO OOMs early).
  3. Rank ablation: Test r ∈ {1, 4, 16} on COPA task; measure final accuracy and training time. Expected: r=1 performs best due to signal concentration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the activation-guided subspace approach be extended to nonlinear trainable layers (e.g., layer normalization, embeddings), which currently fall back to standard Gaussian perturbations?
- Basis in paper: [explicit] The paper states: "For nonlinear trainable layers, AGZO falls back to standard Gaussian perturbations to preserve general applicability across architectures."
- Why unresolved: The gradient confinement property (row(∇W) ⊆ col(H)) was derived specifically for linear layers; nonlinear layers have more complex gradient structures that the current analysis does not address.
- What evidence would resolve it: A theoretical characterization of gradient structure in nonlinear layers and an empirical comparison of alternative perturbation strategies versus the Gaussian fallback.

### Open Question 2
- Question: How should the subspace rank r be selected adaptively across different layers or training stages, rather than using a fixed rank of 1?
- Basis in paper: [inferred] The paper uses r=1 throughout all experiments, justified as minimizing memory overhead and concentrating exploration on dominant directions, but provides no ablation on how performance varies with different rank choices.
- Why unresolved: Different layers exhibit different spectral decay rates (Figure 1 shows variation), suggesting that a uniform rank choice may be suboptimal.
- What evidence would resolve it: An ablation study varying r across layers and training iterations, measuring both task performance and gradient alignment.

### Open Question 3
- Question: Does AGZO's theoretical advantage hold in practical noisy settings with finite smoothing parameter μ, beyond the asymptotic noiseless analysis?
- Basis in paper: [inferred] Theorem 5.4 and 5.6 analyze expected cosine similarity in a "noiseless setting (μ→0) where the finite difference oracle returns exact directional derivatives," but practical implementations use finite μ (10^-7 or 10^-4).
- Why unresolved: Finite μ introduces bias from the smoothing approximation; the interaction between this bias and the subspace projection remains unquantified.
- What evidence would resolve it: A convergence rate analysis incorporating both finite-μ bias and subspace projection error, validated with empirical measurements across μ values.

### Open Question 4
- Question: Can AGZO be effectively combined with parameter-efficient fine-tuning methods (e.g., LoRA) that already impose low-rank structure?
- Basis in paper: [inferred] The paper focuses on full parameter fine-tuning and cites LoRA as motivation for low-rank structure but does not explore whether AGZO's activation-guided perturbations are compatible with or complementary to LoRA-style adapter training.
- Why unresolved: Both approaches exploit low-rank structure but through different mechanisms (activation subspaces vs. learned adapters); their interaction is unknown.
- What evidence would resolve it: Experiments applying AGZO to LoRA adapter parameters and comparing against standard LoRA with backpropagation.

## Limitations

- Implementation complexity: The method requires precise in-place perturbations and careful memory management, with multiple failure points where incorrect implementation could revert performance to baseline levels.
- Assumption sensitivity: The theoretical advantage depends on Assumption 22 (upstream gradient energy not adversarially concentrated in low-singular-value directions).
- Hyperparameter dependence: The reported performance relies on specific choices (r=1, K=3, μ values) that may not generalize across tasks or architectures.

## Confidence

- **High confidence**: The core mechanism of gradient confinement within activation subspaces (Mechanism 1) and the empirical superiority of AGZO over isotropic ZO methods (Figure 2, benchmark results)
- **Medium confidence**: The theoretical claims about subspace-smoothed objectives (Theorem 5.4, 5.6) and the specific choice of r=1
- **Low confidence**: The exact hyperparameter values (learning rates, batch sizes) used in the experiments are not specified

## Next Checks

1. **Theoretical assumption validation**: Test AGZO on synthetic tasks where gradient energy is intentionally concentrated in trailing singular directions. Measure whether performance degrades as predicted by Theorem 5.6 when Assumption 22 is violated.

2. **Memory profiling verification**: Implement AGZO with and without proper memory cleanup (keeping vs. discarding Hℓ). Measure peak GPU memory at different sequence lengths to confirm the claimed O(1) memory footprint relative to MeZO.

3. **Rank sensitivity analysis**: Run AGZO with r ∈ {1, 4, 16} on the same benchmark tasks, measuring both final accuracy and training stability. This validates whether r=1 is universally optimal or task-dependent.