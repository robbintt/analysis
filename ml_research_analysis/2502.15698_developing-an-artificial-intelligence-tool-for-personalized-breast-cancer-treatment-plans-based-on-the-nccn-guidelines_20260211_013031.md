---
ver: rpa2
title: Developing an Artificial Intelligence Tool for Personalized Breast Cancer Treatment
  Plans based on the NCCN Guidelines
arxiv_id: '2502.15698'
source_url: https://arxiv.org/abs/2502.15698
tags:
- treatment
- nccn
- cancer
- clinical
- guidelines
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study developed two AI-driven systems\u2014Agentic-RAG and\
  \ Graph-RAG\u2014to automate personalized breast cancer treatment recommendations\
  \ based on NCCN guidelines. Agentic-RAG used a three-step LLM process with iterative\
  \ refinement and insufficiency checks, while Graph-RAG followed a Microsoft-developed\
  \ graph-based framework to map treatment relationships."
---

# Developing an Artificial Intelligence Tool for Personalized Breast Cancer Treatment Plans based on the NCCN Guidelines

## Quick Facts
- arXiv ID: 2502.15698
- Source URL: https://arxiv.org/abs/2502.15698
- Reference count: 26
- Primary result: Agentic-RAG achieved 100% adherence to NCCN guidelines (24/24) with no hallucinations, outperforming Graph-RAG (95.8%) and ChatGPT-4 (91.6%) baselines

## Executive Summary
This study developed two AI-driven systems—Agentic-RAG and Graph-RAG—to automate personalized breast cancer treatment recommendations based on NCCN guidelines. Agentic-RAG used a three-step LLM process with iterative refinement and insufficiency checks, while Graph-RAG followed a Microsoft-developed graph-based framework to map treatment relationships. Both systems were evaluated against 16 patient descriptions with 4 question variations each, totaling 64 queries. Agentic-RAG achieved 100% adherence to NCCN guidelines (24/24) with no hallucinations, Graph-RAG reached 95.8% adherence (23/24) with one incorrect treatment, and ChatGPT-4 scored 91.6% (22/24) with two incorrect treatments. All systems avoided hallucinations, but only the proposed systems provided detailed NCCN document references, enhancing transparency and clinical reliability.

## Method Summary
The study implemented two retrieval-augmented generation (RAG) approaches using NCCN Breast Cancer guidelines converted from PDFs to JSON format. Agentic-RAG employed a three-step LLM pipeline with iterative insufficiency checking, while Graph-RAG used a Microsoft-developed graph-based framework. Both systems were evaluated against 16 patient descriptions with 4 question variations each, using a board-certified physician to assess adherence to guidelines, hallucinations, and treatment errors.

## Key Results
- Agentic-RAG achieved 100% adherence to NCCN guidelines (24/24) with no hallucinations
- Graph-RAG reached 95.8% adherence (23/24) with one incorrect treatment
- ChatGPT-4 baseline scored 91.6% adherence (22/24) with two incorrect treatments
- All systems avoided hallucinations, but only proposed systems provided detailed NCCN document references

## Why This Works (Mechanism)

### Mechanism 1: Iterative Refinement via Insufficiency Checking
- Claim: Agentic-RAG's three-step LLM pipeline with explicit insufficiency checks produces higher guideline adherence than single-pass generation.
- Mechanism: A first LLM (GPT-4o) selects relevant clinical titles from the guideline corpus. Retrieved JSON content is passed to a second LLM (o1-preview) for treatment generation. A third LLM call evaluates output completeness against a structured checklist; if gaps are detected, the loop repeats.
- Core assumption: The insufficiency checklist accurately captures all clinically necessary treatment components, and LLMs can reliably detect their own omissions when prompted explicitly.
- Evidence anchors: [abstract] "Agentic-RAG used a three-step Large Language Model (LLM) process to select clinical titles from NCCN guidelines, retrieve matching JSON content, and iteratively refine recommendations based on insufficiency checks."
- Break condition: If the insufficiency checklist is incomplete or the LLM fails to recognize missing treatments, iterations terminate prematurely with gaps remaining.

### Mechanism 2: Graph-Based Relationship Mapping for Structured Retrieval
- Claim: Converting guideline flowcharts into entity-relationship graphs enables more systematic retrieval of interconnected treatment pathways than text-only indexing.
- Mechanism: NCCN JSON data is converted to text chunks, then processed by an LLM to extract medical entities and their relationships. These are grouped into graph communities, summarized, and queried for treatment recommendations.
- Core assumption: The LLM accurately extracts entities and relationships from guideline text, and community summarization preserves clinically relevant distinctions without over-generalizing.
- Evidence anchors: [abstract] "Graph-RAG followed a Microsoft-developed framework with proprietary prompts, where JSON data was converted to text via an LLM, summarized, and mapped into graph structures representing key treatment relationships."
- Break condition: If entity extraction conflates distinct treatment pathways or community formation merges clinically separate scenarios, retrieval will return incomplete or mixed recommendations.

### Mechanism 3: Source Attribution as a Hallucination Constraint
- Claim: Requiring systems to output specific document references (page numbers) alongside recommendations reduces hallucinations and enables clinician verification.
- Mechanism: Both Agentic-RAG and Graph-RAG are designed to return not just treatment recommendations but explicit citations to NCCN document pages.
- Core assumption: The retrieval system correctly associates recommendations with source pages, and clinicians will actually verify citations in practice.
- Evidence anchors: [abstract] "Both Agentic-RAG and Graph-RAG provided detailed treatment recommendations with accurate references to relevant NCCN document page numbers."
- Break condition: If retrieval indexing errors associate wrong page numbers with content, citations become misleading rather than helpful.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Both systems extend base LLMs with external knowledge retrieval to ground recommendations in current NCCN guidelines rather than stale training data.
  - Quick check question: Can you explain why a standard LLM without retrieval would struggle with frequently updated clinical guidelines?

- **Knowledge Graphs and Community Detection**
  - Why needed here: Graph-RAG relies on understanding how entities (treatments, conditions, outcomes) relate to each other and cluster into meaningful groups for efficient querying.
  - Quick check question: What is the difference between a text index and a knowledge graph when querying for multi-step treatment pathways?

- **Multi-Agent/Agentic LLM Patterns**
  - Why needed here: Agentic-RAG orchestrates multiple LLM calls with distinct roles (selection, generation, validation), a pattern increasingly common in complex reasoning tasks.
  - Quick check question: Why might a single LLM call be insufficient for tasks requiring both retrieval and self-validation?

## Architecture Onboarding

- **Component map:** NCCN PDFs → JSON extraction → [BRANCH A: Agentic-RAG] Title Selector → JSON Retriever → Treatment Generator → Insufficiency Checker (loop if needed) → [BRANCH B: Graph-RAG] Text Chunker → Entity/Relation Extractor → Graph Builder → Community Summarizer → Query Engine → Both branches → Treatment Recommendation + Page Citations

- **Critical path:** PDF-to-JSON conversion is shared infrastructure. For Agentic-RAG, the insufficiency check loop determines final quality. For Graph-RAG, entity extraction accuracy determines whether graph queries return correct branches.

- **Design tradeoffs:**
  - Agentic-RAG: Higher LLM call count (3+ per query) vs. simpler preprocessing. Better for dynamic queries but more latency/cost.
  - Graph-RAG: Significant upfront graph construction vs. faster queries once built. Better for repeated queries on stable guidelines but may miss nuanced edge cases in community summarization.
  - ChatGPT-4 baseline: No preprocessing, no citations—fastest but least transparent and lowest adherence.

- **Failure signatures:**
  - Missing treatments (Graph-RAG showed 4, ChatGPT-4 showed 2): Indicates retrieval gaps or over-aggressive summarization.
  - Unnecessary treatments (ChatGPT-4 showed 1): Indicates insufficient grounding in retrieved content.
  - Wrong page citations (not observed but risk): Indicates indexing errors in JSON-to-page mapping.

- **First 3 experiments:**
  1. Reproduce the 16-patient evaluation with logged intermediate outputs at each pipeline stage to identify where errors originate (retrieval vs. generation vs. validation).
  2. Stress-test edge cases: patients with multiple comorbidities or overlapping treatment pathways to evaluate whether insufficiency checks and graph communities handle ambiguity.
  3. Citation accuracy audit: manually verify a sample of page references against original NCCN documents to validate the grounding mechanism.

## Open Questions the Paper Calls Out

- **Open Question 1:** How do Agentic-RAG and Graph-RAG perform across diverse cancer types and staging scenarios beyond breast cancer?
  - Basis in paper: [explicit] "Future work will involve broadening the testing of Agentic-RAG and Graph-RAG by evaluating them on a more extensive set of patient descriptions across various types and stages of cancer."
  - Why unresolved: Current evaluation used only 16 breast cancer patient descriptions; generalizability to other cancers with different guideline complexities is unknown.
  - What evidence would resolve it: Evaluation across multiple cancer types (e.g., lung, colorectal, prostate) with statistically significant sample sizes showing comparable adherence rates.

- **Open Question 2:** Does multi-evaluator clinical validation maintain the reported adherence rates when assessed by diverse oncologists in real-world settings?
  - Basis in paper: [explicit] "Involving more oncologists in the evaluation process will provide critical clinical insights and feedback. Their expertise will be invaluable in assessing the practicality, relevance, and acceptance of the systems' recommendations in real-world settings."
  - Why unresolved: Only one board-certified physician evaluated all responses; inter-rater variability and real-world clinical acceptance remain untested.
  - What evidence would resolve it: Multi-center validation study with multiple oncologists evaluating recommendations, including inter-rater reliability metrics.

- **Open Question 3:** What causes Graph-RAG to miss nuanced or overlapping guideline information that Agentic-RAG successfully captures?
  - Basis in paper: [inferred] Graph-RAG missed 4 treatments vs. Agentic-RAG's 0, with the paper noting "its reliance on graph-based retrieval occasionally resulted in missed treatments."
  - Why unresolved: The mechanism underlying Graph-RAG's retrieval failures for complex guideline relationships was not investigated.
  - What evidence would resolve it: Error analysis identifying which treatment categories or guideline structures correlate with Graph-RAG omissions.

## Limitations

- Manual evaluation by a single board-certified physician introduces potential subjectivity and limits generalizability
- The 16-patient evaluation set and its clinical scenarios are not disclosed, preventing independent validation of representativeness
- The insufficiency-checklist content—critical to Agentic-RAG's success—is not specified, making it unclear whether the checklist captures all clinically relevant dimensions

## Confidence

- **High confidence:** The mechanism of iterative insufficiency checking improving guideline adherence is well-supported by the evaluation results (Agentic-RAG: 100% adherence vs. baseline 91.6%).
- **Medium confidence:** The source attribution mechanism reducing hallucinations is plausible given the absence of hallucinations, but the causal link between citations and hallucination prevention is not experimentally isolated.
- **Low confidence:** The transferability of these specific RAG architectures to other cancer types or guideline systems cannot be assessed without testing on different clinical domains.

## Next Checks

1. Conduct inter-rater reliability testing by having multiple clinicians independently evaluate a subset of Agentic-RAG and Graph-RAG outputs for adherence and hallucination rates.
2. Systematically remove the insufficiency-check step in Agentic-RAG to measure its isolated impact on adherence and hallucination rates.
3. Perform a citation accuracy audit by randomly sampling 20% of generated page references and verifying them against the original NCCN documents.