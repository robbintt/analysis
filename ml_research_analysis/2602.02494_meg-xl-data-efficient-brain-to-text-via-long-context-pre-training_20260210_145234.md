---
ver: rpa2
title: 'MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training'
arxiv_id: '2602.02494'
source_url: https://arxiv.org/abs/2602.02494
tags:
- data
- context
- pre-training
- neural
- meg-xl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MEG-XL introduces long-context MEG pre-training to improve data-efficient
  brain-to-text decoding. By pre-training with 2.5-minute MEG samples (191k tokens),
  5-300x longer than prior work, MEG-XL learns extended neural context that short-context
  models discard.
---

# MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training

## Quick Facts
- arXiv ID: 2602.02494
- Source URL: https://arxiv.org/abs/2602.02494
- Authors: Dulhan Jayalath; Oiwi Parker Jones
- Reference count: 40
- Pre-training on 2.5-minute MEG samples (191k tokens) enables matching supervised performance with 1 hour of subject data versus 50 hours

## Executive Summary
MEG-XL introduces long-context MEG pre-training to dramatically improve data efficiency in brain-to-text decoding. By pre-training with 150-second MEG samples containing 191k tokens—5-300x longer than prior work—MEG-XL learns extended neural context that short-context models discard. Fine-tuned for word decoding from brain data, MEG-XL matches supervised performance with only 1 hour of subject data versus 50 hours, and outperforms brain foundation models by 10-25% in low-data regimes. This addresses the critical clinical challenge of decoding speech from paralyzed patients who cannot provide extensive training recordings.

## Method Summary
MEG-XL combines long-context pre-training with contrastive fine-tuning for brain-to-text decoding. The system tokenizes continuous MEG signals using a residual vector quantization tokenizer (BioCodec), then processes these tokens through a Criss-Cross Transformer that factorizes attention across spatial and temporal dimensions. Pre-training uses masked token prediction over 150-second windows with 3-second contiguous masking blocks, forcing the model to learn long-range dependencies. Fine-tuning maps brain signals to T5 word embeddings using a contrastive loss, with decoding performed via nearest-neighbor retrieval. The key innovation is extending context length from typical 3-10 second windows to 150 seconds, enabling learning of extended neural patterns that improve generalization to new subjects with limited data.

## Key Results
- Matches supervised performance with only 1 hour of subject data versus 50 hours required by standard approaches
- Outperforms brain foundation models by 10-25% in low-data regimes (13% of available training data)
- 150-second pre-training context length provides consistent performance improvements across all data amounts tested

## Why This Works (Mechanism)

### Mechanism 1
Pre-training on extended timescales (150s) enables the model to acquire "long-context priors" that substitute for missing subject-specific training data during fine-tuning. The model minimizes masked prediction loss over minutes of continuous data, learning statistical dependencies (subject-specific noise patterns, slow drifts) that short windows discard. These learned priors generalize to new subjects with limited data. Performance degrades to random-initialization baseline if pre-training context length is reduced to <10s.

### Mechanism 2
Long-context pre-training induces a shift from diffuse to selective, hierarchical attention patterns. Optimization over long sequences favors a "local-to-global" strategy: early layers focus on high-frequency local details while later layers integrate distant context. Short-context models fail to learn this differentiation. Attention entropy remains high (uniform) across all layers in short-context models, with no differentiation between early and late layers.

### Mechanism 3
Large contiguous block masking (3s) forces the model to leverage long-range temporal context rather than simple interpolation. By masking blocks larger than the signal's autocorrelation window, the model cannot trivially predict masked tokens from immediate neighbors and must rely on distant unmasked regions. Validation loss drops with 1s masking but stalls with 3s masking, indicating the model cannot bridge the temporal gap.

## Foundational Learning

**Concept: Residual Vector Quantization (RVQ)**
- Why needed here: Converts continuous MEG signals into discrete tokens using multi-level codebook (BioCodec). You must understand how residual levels capture different frequency resolutions to debug tokenization errors.
- Quick check question: If the tokenizer uses 6 residual levels, how does the model aggregate information from these levels into a single embedding?

**Concept: Factorized Attention (Criss-Cross)**
- Why needed here: Standard Transformer attention is $O(N^2)$; modeling 191k tokens is computationally impossible without factorization. Understanding how spatial and temporal attention are split is critical for optimization.
- Quick check question: How does separating attention into "Temporal" and "Spatial" modules reduce complexity compared to a full attention matrix over all (Channel, Time) tokens?

**Concept: Contrastive Learning / Retrieval**
- Why needed here: The fine-tuning stage maps brain signals to T5 word embeddings using a contrastive loss. Decoding is performed by nearest-neighbor retrieval, not classification.
- Quick check question: Why is "Top-10 Balanced Accuracy" used instead of raw accuracy for the word retrieval task?

## Architecture Onboarding

**Component map**: Raw MEG → BioCodec (Tokenizer) → [Sensor Embeds + Token Embeds] → Criss-Cross Transformer → [Masked Token Head (Pre-train) / Contrastive Head (Fine-tune)]

**Critical path**: The **BioCodec tokenizer** dictates the temporal resolution (50Hz) and token count. If the tokenizer discards speech-relevant info, the transformer cannot recover it.

**Design tradeoffs**:
- *Tokenizer*: Uses BioCodec (EEG-trained) rather than BrainTokenizer (MEG-trained) to preserve channel independence, despite domain mismatch
- *Sampling*: Uses 50Hz (below Nyquist for 40Hz lowpass) to extend context length to 150s on fixed GPU memory. Paper notes this theoretically risks aliasing

**Failure signatures**:
- *Attention Collapse*: Uniform attention distribution across all layers (indicates failure to learn long-context priors)
- *Low-Data Collapse*: Pre-trained model performing at chance levels (similar to baselines) in the 13% data regime

**First 3 experiments**:
1. **Context Length Ablation**: Pre-train identical models on 10s, 30s, and 150s windows. Verify performance scaling (Fig 4).
2. **Masking Strategy Test**: Compare 3s block masking vs. random token masking on validation loss. Confirm block masking enforces long-range context.
3. **Tokenizer Reconstruction Check**: Verify BioCodec can reconstruct 50Hz MEG data (MSE check) before training the main model (Appendix E).

## Open Questions the Paper Calls Out
None

## Limitations
- Tokenization domain mismatch: Uses EEG-trained BioCodec for MEG tokenization, which could impact tokenization quality despite claims of preserved channel independence
- Context length trade-offs: 50Hz sampling rate operates below Nyquist rate for MEG's 40Hz lowpass filter, potentially introducing aliasing artifacts
- Attention pattern attribution: Causal relationship between hierarchical attention patterns and performance gains is not rigorously established

## Confidence

**High Confidence** (Strong empirical support, clear mechanism):
- MEG-XL achieves 10-25% better performance than brain foundation models in low-data regimes
- 150s pre-training context length provides consistent performance improvements across data amounts
- Long-context pre-training enables matching supervised performance with 1 hour versus 50 hours of subject data

**Medium Confidence** (Supported by evidence but with methodological gaps):
- Hierarchical attention patterns are induced by long-context pre-training
- Large contiguous block masking (3s) forces long-range temporal context utilization
- Learned statistical priors substitute for missing subject-specific training data

**Low Confidence** (Limited empirical validation, assumptions untested):
- The specific architectural choices (Criss-Cross attention, residual vector quantization) are optimal for this task
- Domain mismatch between EEG-trained tokenizer and MEG data has negligible impact
- The 50Hz sampling rate does not introduce performance-degrading artifacts

## Next Checks
1. **Ablation on Tokenization Domain**: Train and evaluate MEG-XL using both EEG-trained BioCodec and MEG-trained BrainTokenizer on identical datasets to quantify the impact of tokenization domain mismatch on final word decoding accuracy.

2. **Sampling Rate Analysis**: Train models at multiple sampling rates (25Hz, 50Hz, 100Hz) while holding context length constant through window size adjustments to determine whether the 50Hz choice is optimal or if aliasing artifacts are present.

3. **Attention Pattern Intervention**: Implement attention regularization to force uniform attention distribution across layers during pre-training, then measure the impact on both word decoding accuracy and zero-shot brain signal prediction to establish causal links between hierarchical attention and performance gains.