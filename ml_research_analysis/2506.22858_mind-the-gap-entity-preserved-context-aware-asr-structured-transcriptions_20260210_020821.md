---
ver: rpa2
title: 'Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions'
arxiv_id: '2506.22858'
source_url: https://arxiv.org/abs/2506.22858
tags:
- entity
- entities
- second
- chunk
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of improving named entity recognition
  (NER) and formatting accuracy in automatic speech recognition (ASR) systems, particularly
  for long-form audio and complex entity types like phone numbers and monetary values.
  The authors propose a novel training approach that extends the semantic context
  of ASR models by adding overlapping context windows during training.
---

# Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions

## Quick Facts
- **arXiv ID:** 2506.22858
- **Source URL:** https://arxiv.org/abs/2506.22858
- **Authors:** Duygu Altinok
- **Reference count:** 13
- **Primary result:** 40-second context windows with entity reassignment reduce WER from 38% to 26% and improve NER F1 from 0.50 to 0.65 for PERSON entities on Spoken Wikipedia.

## Executive Summary
This work addresses the challenge of improving named entity recognition (NER) and formatting accuracy in automatic speech recognition (ASR) systems, particularly for long-form audio and complex entity types like phone numbers and monetary values. The authors propose a novel training approach that extends the semantic context of ASR models by adding overlapping context windows during training. Specifically, they slide 5-second overlaps on both sides of 30-second training chunks, creating a 40-second "effective semantic window," while focusing predictions on the central 30 seconds. Entities spanning chunk boundaries are reassigned entirely to the right-hand chunk to ensure proper formatting. The model is trained on enriched data with embedded entity labels to learn both recognition and type-specific formatting. Evaluated on the Spoken Wikipedia dataset, the proposed method significantly improves performance across semantic tasks, including NER and entity formatting.

## Method Summary
The authors extend Whisper-Medium ASR by modifying both training data and model architecture. They create 40-second training chunks with 5-second overlaps on both sides, but compute loss only on the central 30 seconds. During data preparation, entities spanning chunk boundaries are reassigned entirely to the right-hand chunk. Training transcripts are enriched with 44 entity tag tokens (e.g., `<MONEY>...</MONEY>`). The encoder positional embeddings are extended from 30s to 40s using Glorot-uniform initialization, pre-trained on VoxPopuli. The model learns to output entity tags alongside transcribed text, enabling joint recognition and formatting. Inference uses 35-second sliding windows with left-context prompting.

## Key Results
- WER improved from 38% (baseline Whisper-Medium) to 26% (fine-tuned)
- NER F1 scores increased (PERSON: 0.50 → 0.65, ORG: 0.42 → 0.56)
- Numerical entity CER reduced (CARDINAL: 0.25 → 0.12)
- Jaro-Winkler distance improved for textual entities
- Boundary reassignment eliminated formatting errors in entities spanning chunks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Overlapping context windows during training enhance named entity recognition and formatting by providing semantic context beyond prediction boundaries.
- **Mechanism:** The model processes 40-second audio segments (5-second left + 30-second middle + 5-second right overlap) but computes loss only on the central 30-second window. This creates a 40-second "effective semantic window" that gives the model access to surrounding linguistic context when predicting entities near chunk boundaries. The additional context reduces errors where entities spanning chunk boundaries would otherwise be split or misclassified.
- **Core assumption:** Entities near chunk boundaries require surrounding acoustic and linguistic context to be correctly recognized and formatted; the model can learn to use this context without generating text for overlap regions.
- **Evidence anchors:**
  - [abstract] "By sliding 5-second overlaps on both sides of 30-second chunks, we create a 40-second 'effective semantic window,' improving entity recognition and formatting while focusing predictions on the central 30 seconds."
  - [section 4.4.4] "The mask ensures that only tokens between ⟨mid⟩ and ⟨right⟩ are included in the loss computation."
  - [corpus] Related work "Whispering Context" (arXiv:2508.13376) similarly addresses syntactic/semantic accuracy in long transcripts through context distillation, supporting the premise that extended context improves downstream NER.

### Mechanism 2
- **Claim:** Reassigning entities spanning chunk boundaries entirely to the right-hand chunk reduces formatting errors by teaching the model to delay incomplete entity predictions.
- **Mechanism:** During training data preparation, entities that cross chunk boundaries are reassigned entirely to the right-hand chunk rather than being split. The model learns to recognize when an entity has begun but lacks sufficient context to complete it, and thus delays prediction until the full entity is available. This prevents partial entity outputs like "$4-" being generated without the full "$4-$6 billion" context.
- **Core assumption:** Entities should be treated as atomic units; partial entity outputs are more harmful than delayed outputs. The model can learn this delay behavior through boundary-aware training.
- **Evidence anchors:**
  - [abstract] "To address entities spanning chunk boundaries, we reassign such entities entirely to the right-hand chunk, ensuring proper formatting."
  - [section 6.2, Figure 6] Shows error example: "$4-$6 billion" split across chunks produced "four" with incorrect CARDINAL tag, illustrating the problem this mechanism addresses.
  - [corpus] No direct corpus evidence for this specific boundary reassignment technique.

### Mechanism 3
- **Claim:** Embedding entity tags directly in training transcripts enables joint learning of entity recognition and type-specific formatting.
- **Mechanism:** Training transcripts are enriched with inline entity labels (e.g., `<MONEY>$15,000</MONEY>`, `<LOC>United States</LOC>`). The model learns to output these tags as part of transcription, which requires it to simultaneously recognize entity boundaries, classify entity types, and apply appropriate formatting. The tokenizer is extended with 44 new tokens for entity tags.
- **Core assumption:** Entity type information is acoustically inferrable from context and the model can learn to produce structured output tags alongside transcribed text without degrading transcription quality.
- **Evidence anchors:**
  - [abstract] "Additionally, enriched training data with embedded entity labels enables the model to learn both recognition and type-specific formatting."
  - [section 4.1] "This process aimed to identify entity spans and embed the corresponding NER labels into the text."
  - [corpus] Gaido et al. (2023) cited in [section 2] used comparable embedded entity tags for speech translation, validating the approach of tag embedding.

## Foundational Learning

- **Concept:** Sequence-to-sequence ASR architecture with encoder-decoder structure
  - **Why needed here:** Understanding how Whisper's encoder processes audio frames and the decoder generates tokens is essential for grasping why input length extension (40 vs. 30 seconds) requires embedding layer modification.
  - **Quick check question:** Why can't you directly feed 40-second audio into a Whisper model trained on 30-second chunks?

- **Concept:** Named Entity Recognition (NER) from speech
  - **Why needed here:** The paper addresses the specific challenge of extracting and formatting entities (names, locations, monetary amounts) from audio, which differs from pure transcription due to formatting requirements.
  - **Quick check question:** What makes extracting "two million dollars" as `<MONEY>$2,000,000</MONEY>` harder than transcribing it as text?

- **Concept:** Cross-entropy loss with masking
  - **Why needed here:** The method computes loss only on the middle window tokens; understanding masked loss is critical for implementing the training objective correctly.
  - **Quick check question:** In the formula `Loss = Σ(mi · CEi) / Σ(mi)`, what happens if all `mi` values are zero?

## Architecture Onboarding

- **Component map:** Audio encoder (modified) -> Token decoder (extended) -> Data preparation pipeline -> Training loss (masked) -> Inference pipeline

- **Critical path:**
  1. Extend encoder embeddings (100h VoxPopuli pre-training for new positions)
  2. Prepare tagged training data with boundary-aware entity reassignment
  3. Fine-tune with masked loss on mid-window tokens
  4. Implement inference with 5s stride and left-context prompting

- **Design tradeoffs:**
  - **Context vs. latency:** 40s training window improves accuracy but requires 5s future audio during inference (not suitable for real-time streaming)
  - **Tag count vs. vocabulary:** 44 entity tokens add output complexity; may impact rare word transcription
  - **Masking vs. gradient signal:** Masking overlap regions reduces training signal but focuses model on prediction window

- **Failure signatures:**
  - **Early training (<20 epochs):** Model fails to emit entity tags correctly
  - **Mid training (<50 epochs):** Hallucinations around entity boundaries
  - **Inference failure:** No timestamp tokens emitted (token ID ordering issue); fixed by adding timestamp tokens to training chunks
  - **Boundary splitting:** Numerical entities split across chunks produce malformed output (e.g., "$1.2.211")

- **First 3 experiments:**
  1. **Baseline sanity check:** Fine-tune Whisper-puncted (30s chunks, no overlap) and evaluate NER F1 and CER on Spoken Wikipedia test split to establish improvement baselines.
  2. **Ablation on window size:** Train variants with 0s, 2.5s, and 5s overlaps to measure context contribution; if F1 plateaus at 2.5s, larger windows add compute cost without benefit.
  3. **Boundary handling validation:** Create synthetic test cases with entities at chunk boundaries; compare reassignment strategy vs. naive splitting on entity formatting CER.

## Open Questions the Paper Calls Out
None

## Limitations
- The approach requires 5 seconds of future audio during inference, limiting applicability in real-time streaming scenarios.
- Performance depends heavily on the availability of high-quality entity-annotated training data.
- The method assumes entity type information is acoustically inferable from context, which may not hold for all entity types or domains.

## Confidence
- **High Confidence:** WER improvement from 38% to 26% through fine-tuning
- **Medium Confidence:** Overlapping context windows enhance NER and formatting
- **Medium Confidence:** Boundary reassignment reduces formatting errors
- **Medium Confidence:** Embedding entity tags enables joint recognition and formatting

## Next Checks
1. **Context Size Ablation Study:** Train and evaluate models with different overlap sizes (e.g., 0s, 2.5s, 5s, 7.5s) to quantify the marginal benefit of context extension and determine the optimal trade-off between accuracy and latency.

2. **Real-Time Inference Evaluation:** Implement and test the model in a streaming setting with strict latency constraints to assess the practical limitations of the 5-second context requirement and explore potential optimizations for real-time applications.

3. **Generalizability Test:** Evaluate the model on a different long-form audio dataset (e.g., audiobooks, podcasts) with entity annotations to assess the robustness of the approach across domains and entity distributions.