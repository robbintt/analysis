---
ver: rpa2
title: Markov Chain Estimation with In-Context Learning
arxiv_id: '2508.03934'
source_url: https://arxiv.org/abs/2508.03934
tags:
- training
- markov
- transition
- transformer
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We investigate whether transformers trained on next-token prediction
  can learn to estimate Markov chain transition probabilities in-context rather than
  memorizing training patterns. We train transformers on Markov chains with randomly
  sampled transition matrices and evaluate their ability to generalize to unseen matrices.
---

# Markov Chain Estimation with In-Context Learning

## Quick Facts
- arXiv ID: 2508.03934
- Source URL: https://arxiv.org/abs/2508.03934
- Authors: Simon Lepage; Jeremie Mary; David Picard
- Reference count: 19
- Primary result: Transformers can learn to estimate Markov chain transition probabilities in-context rather than memorizing training patterns

## Executive Summary
This paper investigates whether transformers trained on next-token prediction can learn to estimate Markov chain transition probabilities in-context rather than memorizing training patterns. The authors train transformers on Markov chains with randomly sampled transition matrices and evaluate their ability to generalize to unseen matrices. They find that above certain thresholds in model size and training set size, transformers learn to estimate transition probabilities from context instead of memorizing them. The study proposes two state encoding schemes - permutation-based and random orthogonal embeddings - that enable training on a single Markov chain while maintaining generalization to chains with different numbers of states or transition statistics.

## Method Summary
The authors train transformers on synthetic Markov chains with randomly sampled transition matrices, using a next-token prediction objective. They evaluate the models' ability to generalize to unseen transition matrices and different numbers of states. Two state encoding approaches are proposed: permutation-based embeddings and random orthogonal embeddings. The orthogonal embedding approach allows transformers to handle chains with up to twice as many states as seen during training and shows better robustness to changes in transition probability distributions compared to standard learned embeddings.

## Key Results
- Above certain thresholds in model size and training set size, transformers learn to estimate transition probabilities from context instead of memorizing them
- Orthogonal embedding approach allows handling chains with up to twice as many states as seen during training
- Orthogonal embeddings show better robustness to changes in transition probability distributions compared to standard learned embeddings

## Why This Works (Mechanism)
The paper demonstrates that transformers can learn to extract transition probability patterns from context rather than memorizing specific transition matrices. When trained on sufficiently large datasets with appropriate model sizes, the attention mechanisms in transformers can develop the capability to estimate transition probabilities for unseen Markov chains. The orthogonal embedding scheme provides a structured way to represent states that enables generalization across different chain configurations.

## Foundational Learning
- Markov chains: Why needed - fundamental sequential data structure being modeled. Quick check - verify understanding of transition probabilities and stationary distributions.
- Transformer architecture: Why needed - core model being evaluated for in-context learning capability. Quick check - understand self-attention mechanism and next-token prediction.
- In-context learning: Why needed - central concept being investigated as alternative to memorization. Quick check - distinguish between memorization and generalization capabilities.
- Embedding techniques: Why needed - critical for representing states in transformer models. Quick check - compare learned embeddings vs orthogonal embeddings.
- Generalization: Why needed - key outcome measure for evaluating model capabilities. Quick check - understand limits of what models can transfer to unseen data.

## Architecture Onboarding
Component map: Markov chain states -> Embedding layer -> Transformer layers -> Output layer
Critical path: Input sequence → State embeddings → Self-attention computation → Next token prediction
Design tradeoffs: Learned embeddings (better for single chain) vs orthogonal embeddings (better for generalization)
Failure signatures: Memorization of training chains, inability to handle different state counts, poor performance on chains with different transition statistics
First experiments:
1. Train transformer on single Markov chain with learned embeddings
2. Evaluate performance on same chain vs unseen chains
3. Repeat with orthogonal embedding approach

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Experimental validation focused primarily on synthetic Markov chains with specific structural properties
- Exact nature of thresholds for successful in-context learning remains unclear
- Limited investigation of performance on real-world sequential data

## Confidence
High Confidence: State encoding schemes (orthogonal embeddings clearly validated)
Medium Confidence: In-context learning of transition probabilities (well-supported but thresholds unclear)
Low Confidence: Generalization capabilities across more complex Markov structures (limited evidence beyond synthetic cases)

## Next Checks
1. Test transformer performance on Markov chains with longer state sequences and higher-order dependencies to assess scalability
2. Evaluate the orthogonal embedding approach on real-world sequential data (e.g., natural language, biological sequences)
3. Systematically investigate the transition between memorization and in-context learning by varying model size, training set size, and chain complexity