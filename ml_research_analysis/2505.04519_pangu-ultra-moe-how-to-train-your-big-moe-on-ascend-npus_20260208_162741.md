---
ver: rpa2
title: 'Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs'
arxiv_id: '2505.04519'
source_url: https://arxiv.org/abs/2505.04519
tags:
- training
- expert
- experts
- communication
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Pangu Ultra MoE demonstrates an efficient training recipe for large-scale
  sparse MoE models on Ascend NPUs, achieving 30.0% MFU with 718 billion parameters
  on 6K NPUs. The approach combines simulation-driven hyperparameter optimization,
  hierarchical expert parallelism communication, fine-grained memory recomputation,
  and dynamic load balancing.
---

# Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs

## Quick Facts
- arXiv ID: 2505.04519
- Source URL: https://arxiv.org/abs/2505.04519
- Reference count: 40
- Primary result: 30.0% MFU on 718B-parameter MoE with 1.46M TPS on 6K Ascend NPUs

## Executive Summary
Pangu Ultra MoE presents an efficient training recipe for large-scale sparse MoE models on Ascend NPUs, achieving 30.0% MFU with 718 billion parameters on 6K NPUs. The approach combines simulation-driven hyperparameter optimization, hierarchical expert parallelism communication, fine-grained memory recomputation, and dynamic load balancing. Key results include competitive performance on general and reasoning benchmarks (90.8% MMLU, 81.3% AIME2024), superior medical domain accuracy (87.1% MedQA), and efficient training throughput comparable to DeepSeek R1.

## Method Summary
The method trains a 718B-parameter MoE model on Ascend NPUs using 5D parallelism (TP=8, PP=16, VPP=2, EP=4, MBS=2) with 256 experts and 8 active per token. The training employs dropless routing with EP-Group auxiliary loss, hierarchical expert parallelism communication with Adaptive Pipe Overlap achieving 95% overlap rate, and fine-grained memory recomputation (MLA KV-only, permute, SwigLU activation) combined with tensor swapping. A simulation-driven architecture search framework models compute/communication/memory on Ascend 910B to optimize hyperparameters, validated at 88-90% accuracy against small-scale runs.

## Key Results
- Achieves 30.0% MFU on 718B-parameter MoE with 1.46M TPS on 6K Ascend NPUs
- Competitive benchmark performance: 90.8% MMLU, 81.3% AIME2024, 87.1% MedQA
- 25% training throughput improvement over prior Pangu models through optimized communication and memory strategies

## Why This Works (Mechanism)
The system achieves high efficiency through hierarchical communication that separates inter-node AllGather from intra-node All-to-All operations, allowing 95% overlap of TP/EP/PP communications via Adaptive Pipe Overlap scheduling. Fine-grained recomputation reduces memory overhead by recomputing only specific activations (MLA KV, permute, SwigLU) while preserving others, enabling dropless routing without OOM. Dynamic load balancing with greedy algorithms and sliding window prediction prevents expert capacity saturation. The simulation framework enables rapid architecture search by modeling NPU-specific characteristics before deployment.

## Foundational Learning

**Expert Parallelism (EP)**: Splits experts across devices to balance load and reduce communication. Needed because MoE models have more experts than available devices. Quick check: Verify token routing respects expert capacity constraints.

**5D Parallelism**: Combines tensor, pipeline, data, expert, and model batch parallelism to scale training. Needed to distribute massive models across thousands of NPUs. Quick check: Confirm all five dimensions are active in the configuration.

**Hierarchical All-to-All**: Organizes communication in layers to reduce synchronization overhead. Needed because flat All-to-All scales poorly with node count. Quick check: Measure communication time reduction compared to baseline.

**Adaptive Pipe Overlap**: Schedules communication to overlap with computation using 1F1B_overlap pattern. Needed to mask communication latency in expert parallelism. Quick check: Verify overlap rate approaches 95% as claimed.

**Dropless Routing**: Maintains all tokens through expert routing without dropping. Needed for maximum accuracy but requires careful memory management. Quick check: Confirm no tokens are dropped during forward pass.

## Architecture Onboarding

**Component Map**: Data Parallel -> Pipeline Parallel -> Tensor Parallel -> Expert Parallel -> Model Batch Parallel

**Critical Path**: Token generation → Expert routing (top-k=8) → Hierarchical All-to-All → Expert computation → Token gathering → Next layer

**Design Tradeoffs**: Dropless routing maximizes accuracy but increases memory pressure; hierarchical communication reduces latency but adds implementation complexity; fine-grained recomputation saves memory but adds compute overhead.

**Failure Signatures**: Low MFU (<20%) indicates communication bottlenecks; OOM errors suggest insufficient memory optimization; expert load imbalance causes training stalls and reduced throughput.

**First Experiments**: 1) Validate simulation framework accuracy on 4.2B model; 2) Test hierarchical All-to-All communication on medium-scale configuration; 3) Measure memory savings from fine-grained recomputation on 25B model.

## Open Questions the Paper Calls Out

**Open Question 1**: Can simulation-based architecture search be refined to accurately model micro-architectural stochasticities like cache misses and stragglers? The paper notes current simulations are "consistently shorter" than reality due to idealized assumptions, suggesting room for improvement in modeling unpredictable factors.

**Open Question 2**: Does token drop rate in capacity-constrained MoE training scale predictably with model size? The paper observes larger models drop more tokens (8% vs 6%) but doesn't establish if this trend continues linearly or accelerates, leaving open questions about trillion-parameter efficiency limits.

**Open Question 3**: Are Ascend-specific optimizations transferable to GPU architectures with different memory-to-computation ratios? The paper explicitly attributes high throughput to Ascend-specific features like the Cube Unit, suggesting optimizations may not generalize without modification.

**Open Question 4**: What mechanisms drive the observed "front-expert" bias and increased specialization in deeper layers? The paper notes this behavior but offers only speculative interpretation about knowledge flow, without isolating whether it results from routing algorithms, data distribution, or model depth.

## Limitations

The reported 30.0% MFU and benchmark performance cannot be independently verified without full training infrastructure details, including MindSpeed/Megatron fork specifics and exact Ascend 910B kernel optimizations. Critical implementation details for the Adaptive Pipe Overlap mechanism and hierarchical communication optimizations remain unspecified. The training data composition, exact hyperparameter schedules, and post-training fine-tuning procedures are not fully disclosed, limiting reproducibility of benchmark results.

## Confidence

**High Confidence**: Architectural specifications (61L/7680H/256E) and 5D parallelism configuration are clearly defined and represent plausible MoE design. Memory optimization techniques are well-established with predictable effects.

**Medium Confidence**: Benchmark results are internally consistent with model scale but require full training corpus and evaluation protocols for verification. MFU claim of 30.0% is specific but depends critically on implementation details.

**Low Confidence**: Throughput claim of 1.46M tokens/second cannot be verified without complete training infrastructure details. Simulation framework accuracy claims are asserted but not demonstrated for target scale.

## Next Checks

1. **Communication Overlap Verification**: Implement Adaptive Pipe Overlap scheduling on 4.2B model with 128 NPUs and measure actual overlap rate between TP/EP/PP communications to validate hierarchical optimization approach.

2. **Memory Optimization Benchmarking**: Test fine-grained recomputation (MLA KV-only, permute, SwigLU) on 25B-parameter MoE to measure memory savings and training stability, verifying tensor swapping prevents OOM during dropless routing.

3. **Simulation Framework Accuracy**: Build and validate simulation framework by predicting performance metrics for 4.2B model configuration, then compare predictions against actual measurements to assess scaling accuracy before full-scale deployment.