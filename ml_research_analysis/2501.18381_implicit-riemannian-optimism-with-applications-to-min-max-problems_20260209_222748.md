---
ver: rpa2
title: Implicit Riemannian Optimism with Applications to Min-Max Problems
arxiv_id: '2501.18381'
source_url: https://arxiv.org/abs/2501.18381
tags:
- riemannian
- g-convex
- where
- holds
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RIOD, an implicit Riemannian optimistic online
  learning algorithm for Hadamard manifolds, and RIODA, a corresponding min-max optimization
  method. RIOD improves upon prior work by handling in-manifold constraints without
  requiring strong assumptions and achieving regret bounds independent of geometric
  constants like the minimum curvature.
---

# Implicit Riemannian Optimism with Applications to Min-Max Problems

## Quick Facts
- **arXiv ID:** 2501.18381
- **Source URL:** https://arxiv.org/abs/2501.18381
- **Reference count:** 40
- **Primary result:** Introduces RIOD (implicit Riemannian optimistic online learning) and RIODA (min-max optimization method) achieving curvature-independent regret bounds and near-optimal gradient oracle complexity for Hadamard manifolds.

## Executive Summary
This paper introduces RIOD, an implicit Riemannian optimistic online learning algorithm for Hadamard manifolds, and RIODA, a corresponding min-max optimization method. RIOD improves upon prior work by handling in-manifold constraints without requiring strong assumptions and achieving regret bounds independent of geometric constants like the minimum curvature. The method uses inexact implicit updates, making it computationally efficient for smooth losses. RIODA builds on RIOD to solve g-convex, g-concave smooth min-max problems, achieving near-optimal gradient oracle complexity (O(LR²/ε) for ε duality gap) that matches Euclidean algorithms up to logarithmic factors. This is the first Riemannian min-max algorithm to remove dependence on geometric terms ζ, except for log factors. Experiments on robust Karcher mean problems in symmetric positive definite manifolds and hyperbolic spaces validate the theoretical results, showing linear convergence.

## Method Summary
The paper proposes RIOD, which uses inexact implicit updates by minimizing a regularized version of the loss function $\ell_t(x) + \frac{1}{2\eta}d(x, x_t)^2$, ensuring subproblems remain g-convex. It maintains two iterates: a conservative secondary update using the true loss and an optimistic primary action using a hint function. The method RIODA extends this to solve g-convex, g-concave min-max problems by applying RIOD to both primal and dual variables. The approach achieves regret bounds independent of geometric constants like minimum curvature by leveraging the strong convexity of the squared distance function on Hadamard manifolds.

## Key Results
- RIOD achieves regret bounds independent of geometric constants (like minimum curvature $\zeta$), matching best-known Euclidean rates
- RIODA solves g-convex, g-concave min-max problems with gradient oracle complexity of O(LR²/ε) for ε duality gap
- First Riemannian min-max algorithm to remove dependence on geometric terms ζ, except for log factors
- Experiments on SPD manifolds and hyperbolic spaces show linear convergence for robust Karcher mean problems

## Why This Works (Mechanism)

### Mechanism 1: Preservation of Geodesic Convexity via Implicit Updates
The algorithm guarantees regret bounds and convergence by avoiding the linearization of loss functions, which preserves the geodesic convexity (g-convexity) of subproblems. Unlike standard Riemannian gradient methods that linearize losses (resulting in non-g-convex approximations), RIOD solves an inexact implicit update by minimizing the full regularized loss function $\ell_t(x) + \frac{1}{2\eta}d(x, x_t)^2$. This ensures the subproblem remains g-convex and solvable within manifold constraints.

### Mechanism 2: Two-Step Optimism for Error Correction
The algorithm decouples the conservative "secondary" update from the optimistic "primary" action to prevent the accumulation of prediction errors common in single-step optimistic methods. RIOD maintains two iterates: $x_t$ (secondary, updated using the true loss $\ell_{t-1}$) and $\tilde{x}_t$ (primary, updated using the hint $\tilde{\ell}_t$). This structure allows the agent to leverage the optimistic hint for better actions while anchoring the update sequence to the actual loss history, ensuring the secondary iterates remain independent of prediction errors.

### Mechanism 3: Curvature-Independent Regularization
The algorithm achieves regret bounds independent of geometric constants (like minimum curvature $\zeta$) by leveraging the strong convexity of the squared distance function on Hadamard manifolds. The regularizer $\frac{1}{2\eta}d(x, x_t)^2$ is $(1/\eta)$-strongly g-convex. By solving the implicit update exactly (or to sufficient precision), the analysis relies on the intrinsic contraction properties of the proximal step rather than comparing tangent spaces across potentially large distances where curvature terms usually scale.

## Foundational Learning

- **Hadamard Manifolds**
  - Why needed here: The theoretical guarantees rely heavily on the geometric properties of non-positive curvature (unique geodesics, non-expansive projection).
  - Quick check question: Does the target manifold (e.g., Hyperbolic space, SPD matrices) have strictly non-positive sectional curvature everywhere?

- **Implicit Online Learning**
  - Why needed here: RIOD differs from standard online gradient descent by solving a proximal minimization problem rather than taking a gradient step.
  - Quick check question: Can you define the difference between an "explicit" gradient step and an "implicit" proximal step in Euclidean space?

- **Geodesic Convexity (g-convexity)**
  - Why needed here: The mechanism requires the loss landscape to be convex along geodesics, not just straight lines, to guarantee the subproblems are solvable.
  - Quick check question: Is the function $f(x) = -\log(\det(X))$ g-convex on the SPD manifold?

## Architecture Onboarding

- **Component map:** Inputs (X, $\ell_t$, $\tilde{\ell}_t$) -> Sub-solver (CRGD/PRGD) -> RIOD Core (implicit update with precision controller) -> Outputs ($\tilde{x}_t$)
- **Critical path:** The execution of the Sub-solver (CRGD/PRGD) at every iteration. The theoretical validity depends on achieving the specific precision $\epsilon_t$ defined in Algorithm 1 efficiently.
- **Design tradeoffs:**
  - **CRGD vs. PRGD for Subproblems:** Using Composite RGD (CRGD) is computationally heavier per step but theoretically guarantees curvature-independent complexity ($\tilde{O}(1)$ oracle calls). Using Projected RGD (PRGD) is simpler to implement but introduces a dependency on geometric constants ($\tilde{O}((L\eta + \zeta D)\zeta\tilde{R})$).
  - **Hint Choice:** A "perfect" hint ($\tilde{\ell}_t = \ell_t$) drives regret to zero (static case), while a poor hint degrades performance to standard online learning rates.
- **Failure signatures:**
  - **Divergence of Sub-solver:** If the sub-solver fails to find an $\epsilon_t$-minimizer because the condition number is too high or the function is not sufficiently smooth, the regret bounds break.
  - **Constraint Violation:** If the implementation relies on unconstrained RGD but the iterates drift beyond the radius where g-convexity holds, convergence is not guaranteed.
- **First 3 experiments:**
  1. Implement RIODA on the SPD manifold or hyperbolic space for the robust averaging problem to verify linear convergence.
  2. Compare the wall-clock time and iteration count of using CRGD vs. PRGD as the subroutine for the implicit update to validate the trade-off between complexity and geometric dependence.
  3. Test the algorithm's stability by varying the precision $\epsilon_t$ of the sub-solver to see if looser tolerances degrade the final duality gap.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can RIOD and RIODA be extended to general Riemannian manifolds, particularly those with positive curvature? The paper notes that for manifolds with positive curvature, the metric projection is not a non-expansive operator, which complicates the analysis of enforcing constraints.
- **Open Question 2:** Is it possible to design a Riemannian optimistic algorithm that effectively utilizes linearized hints instead of full-function implicit updates? The text notes that unlike the Euclidean setting, a function linearized at a point is not g-convex, and therefore the authors chose a two-step variant requiring the minimization of the full loss function.
- **Open Question 3:** Can the Composite Riemannian Gradient Descent (CRGD) subproblems required by the optimal RIODA implementation be solved efficiently in practice? The authors state that implementing the CRGD update steps might be a hard computational problem, despite it offering the best theoretical gradient oracle complexity.

## Limitations
- The algorithm's performance heavily depends on the availability of a good hint function $\tilde{\ell}_t$ for the optimistic step. Poor hints degrade the method to standard rates.
- The theoretical regret bounds rely on achieving specific precision $\epsilon_t$ in the sub-solver, but practical implementations may use fixed tolerances, potentially affecting guarantees.
- While the method handles in-manifold constraints, the projection operator $P_X$ is assumed available but can be expensive or complex to compute in some manifolds.

## Confidence

- **High Confidence:** Curvature-independent regret bounds for g-convex losses on Hadamard manifolds (Theorem 1, Corollary 2)
- **Medium Confidence:** Near-optimal gradient complexity for Riemannian min-max problems (Theorem 3, Corollary 4). The proof structure is sound, but constants and log factors require careful tracking
- **Medium Confidence:** Experimental validation on SPD and hyperbolic spaces. The linear convergence plots are compelling, but the robustness to hyperparameter choices is not fully characterized

## Next Checks

1. **Sensitivity Analysis:** Test RIODA's convergence with deliberately poor hint functions to quantify the degradation from the optimal $O(LR^2/\varepsilon)$ rate
2. **Solver Precision Study:** Systematically vary the sub-solver's precision $\epsilon_t$ and measure its impact on the final duality gap to validate the theoretical tolerance requirements
3. **Constraint Set Scaling:** Evaluate the algorithm's performance as the constraint set radius $R$ changes, particularly testing the assumption that the optimal solution lies within $B(x^*, 8R)$