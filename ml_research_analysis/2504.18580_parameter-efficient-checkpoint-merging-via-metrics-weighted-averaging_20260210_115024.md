---
ver: rpa2
title: Parameter-Efficient Checkpoint Merging via Metrics-Weighted Averaging
arxiv_id: '2504.18580'
source_url: https://arxiv.org/abs/2504.18580
tags:
- last
- merging
- checkpoint
- base
- averaging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Metrics-Weighted Averaging (MWA), a parameter-efficient
  checkpoint merging method for LoRA-based fine-tuning that weights checkpoints by
  training loss or steps to improve model performance. Experiments on mathematical
  reasoning, alignment, and instruction tasks show that MWA consistently outperforms
  naive uniform averaging, with loss-weighted merging achieving up to 5.05% higher
  accuracy than the baseline and even surpassing the final checkpoint.
---

# Parameter-Efficient Checkpoint Merging via Metrics-Weighted Averaging

## Quick Facts
- arXiv ID: 2504.18580
- Source URL: https://arxiv.org/abs/2504.18580
- Authors: Shi Jie Yu; Sehyun Choi
- Reference count: 11
- Primary result: MWA consistently outperforms naive averaging, with loss-weighted merging achieving up to 5.05% higher accuracy than the baseline and even surpassing the final checkpoint

## Executive Summary
This paper introduces Metrics-Weighted Averaging (MWA), a parameter-efficient checkpoint merging method for LoRA-based fine-tuning that weights checkpoints by training loss or steps to improve model performance. Experiments on mathematical reasoning, alignment, and instruction tasks show that MWA consistently outperforms naive uniform averaging, with loss-weighted merging achieving up to 5.05% higher accuracy than the baseline and even surpassing the final checkpoint. The method uses a single hyperparameter to adjust weight distribution and requires minimal computational overhead, making it effective for enhancing fine-tuned models in low-resource scenarios.

## Method Summary
Metrics-Weighted Averaging (MWA) is a parameter-efficient checkpoint merging technique that creates superior models by averaging multiple checkpoints weighted by performance metrics. The method operates on LoRA adapters during fine-tuning, using a weighted average formula where weights are derived from training metrics. For loss-weighted merging, checkpoints are sorted by training loss and assigned weights using the formula φ_x|p,q = p^(pos(ℓx)) · 1/ℓx, where p is a penalty factor and q is a power factor. The method requires minimal computational overhead compared to other merging techniques and uses a single hyperparameter to control weight distribution. Experiments were conducted on Gemma-2b with LoRA fine-tuning across three task domains: mathematical reasoning (Orca-Math), alignment (hh-rlhf with SimPO), and instruction tuning (OpenHermes 2.5).

## Key Results
- MWA consistently outperforms naive uniform averaging across all tested scenarios
- Loss-weighted merging achieves up to 5.05% higher accuracy than baseline uniform averaging
- The merged model can outperform the final checkpoint in mathematical reasoning tasks
- Optimal checkpoint window varies by task: last 10 for math, last 5-3 for alignment
- Minimal computational overhead compared to other model merging techniques

## Why This Works (Mechanism)
MWA leverages the observation that not all checkpoints contribute equally to final model performance. By weighting checkpoints based on training metrics (loss or steps), the method prioritizes checkpoints that achieved better optimization states during training. Loss-weighted averaging performs particularly well because training loss serves as a proxy for checkpoint quality - lower loss indicates the model was closer to an optimal solution at that point. The penalty factor p controls how aggressively weights are distributed, with values closer to 1.0 creating more uniform distributions and values closer to 0.5 creating more extreme weighting. The power factor q (though fixed heuristically in experiments) further modulates the weight distribution to prevent extreme skewing while still emphasizing better-performing checkpoints.

## Foundational Learning
- **LoRA (Low-Rank Adaptation)**: Efficient fine-tuning method that inserts low-rank matrices into transformer layers, allowing parameter-efficient adaptation without full model fine-tuning. Why needed: MWA operates specifically on LoRA adapters, making understanding LoRA structure essential for implementation.
- **Checkpoint Merging**: Technique that combines multiple model states to create a superior averaged model. Why needed: MWA is fundamentally a checkpoint merging method, so understanding the concept and goals of merging is crucial.
- **Weight Averaging Schemes**: Different methods for combining model parameters (uniform, loss-weighted, steps-weighted). Why needed: MWA introduces a new weighted averaging approach that needs to be compared against existing schemes.
- **Fine-tuning Optimization Dynamics**: Understanding how models progress through training and how checkpoint quality varies over time. Why needed: MWA's effectiveness relies on identifying and leveraging checkpoints from optimal training states.

## Architecture Onboarding

**Component Map:** LoRA Adapters -> Checkpoint Storage -> Weight Calculation -> Parameter Averaging -> Merged Model

**Critical Path:** Fine-tune model with LoRA → Save checkpoints at regular intervals → Calculate weights using MWA formula → Perform weighted parameter average → Evaluate merged model

**Design Tradeoffs:** MWA trades off simplicity and computational efficiency for potentially suboptimal weight distributions compared to more sophisticated merging methods. The method assumes training loss is a reliable quality indicator, which may not hold in all training scenarios.

**Failure Signatures:** Merged model underperforms uniform average (indicates weights too skewed or metric not meaningful), no improvement over final checkpoint (training losses too similar or early plateaus), or unstable performance across runs (sensitive to hyperparameter choices).

**First Experiments:**
1. Fine-tune Gemma-2b with LoRA on Orca-Math, save checkpoints every 1000 steps, apply MWA with p=0.7 to last 10 checkpoints, evaluate on GSM8K
2. Test different penalty factors (p=0.5, 0.7, 0.9) on the same checkpoint set to analyze sensitivity
3. Compare loss-weighted vs steps-weighted MWA on alignment task with hh-rlhf dataset

## Open Questions the Paper Calls Out

### Open Question 1
What metrics beyond training loss and training steps would be most effective for metrics-weighted averaging in checkpoint merging? The study was limited to only two types of metrics, and while training loss proved effective, the relationship between metric characteristics and merging effectiveness remains poorly understood. Systematic experiments with additional metrics (e.g., gradient norms, Fisher information, task-specific validation scores, parameter magnitude changes) across diverse tasks and model architectures would resolve this.

### Open Question 2
Does metrics-weighted averaging generalize effectively to full fine-tuning and pre-training scenarios beyond LoRA-based PEFT? All experiments used LoRA modules with only small adapter parameters being merged; the dynamics of merging full model parameters may differ significantly due to scale and parameter interactions. Experiments applying MWA to fully fine-tuned checkpoints and pre-training checkpoints, comparing performance gains against PEFT baselines, would resolve this.

### Open Question 3
Can metrics-weighted averaging be effectively extended to multi-task learning and multirun model fusion settings? Current experiments only address single-task training runs; multi-task scenarios involve different optimization landscapes with potentially conflicting gradients that may affect weight distribution heuristics. Experiments merging checkpoints from different tasks or different training runs, measuring task-specific performance retention and emergence of cross-task capabilities, would resolve this.

### Open Question 4
What theoretical mechanisms explain why loss-weighted averaging consistently outperforms steps-weighted averaging? While the empirical advantage is clear, the paper does not provide theoretical analysis of why training loss serves as a more reliable proxy for checkpoint quality than temporal position in the training trajectory. Theoretical analysis of loss landscapes during training, correlation studies between checkpoint loss and parameter-space proximity to optima, and experiments tracking gradient behavior across training would resolve this.

## Limitations
- Lack of concrete hyperparameter specifications, particularly power factor q values used in experiments
- LoRA configuration details (rank, alpha, target modules) completely unspecified
- Tie-breaking mechanism for pos() when training losses are equal not addressed
- Limited to LoRA-based PEFT without evaluation on full fine-tuning or pre-training scenarios
- Insufficient evidence for effectiveness in low-resource scenarios beyond theoretical claims

## Confidence

**High Confidence Claims:**
- MWA outperforms naive uniform averaging in all tested scenarios
- The method requires minimal computational overhead compared to other merging techniques
- A single hyperparameter (penalty factor p) effectively controls weight distribution

**Medium Confidence Claims:**
- Loss-weighted merging consistently achieves better results than step-weighted merging
- The optimal checkpoint window varies by task (last 10 for math, last 5-3 for alignment)
- The merged model can outperform the final checkpoint by up to 5.05%

**Low Confidence Claims:**
- MWA is particularly effective for low-resource scenarios (insufficient evidence provided)
- The heuristic for choosing q values generalizes well across different tasks and models

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically test different combinations of penalty factor p ∈ [0.5, 1.0] and power factor q ∈ [1.5, 3.0] to determine their impact on performance and identify optimal ranges for each task type.

2. **LoRA Configuration Impact**: Experiment with different LoRA configurations (varying ranks and alpha values) to assess how adapter structure affects merging effectiveness and whether the MWA benefits generalize across different LoRA setups.

3. **Checkpoint Timing Analysis**: Evaluate MWA performance using checkpoints from different training stages (early, middle, late) to determine whether the method's effectiveness depends on selecting checkpoints from specific phases of training convergence.