---
ver: rpa2
title: Efficient Data Selection at Scale via Influence Distillation
arxiv_id: '2505.19051'
source_url: https://arxiv.org/abs/2505.19051
tags:
- training
- arxiv
- samples
- influence
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Influence Distillation is a novel data selection framework that
  assigns optimal weights to training samples by measuring their influence on a target
  distribution using second-order approximations. It computes these weights efficiently
  via a landmark-based approach that projects gradients into a low-dimensional embedding
  space, eliminating the need for expensive proxy model training.
---

# Efficient Data Selection at Scale via Influence Distillation

## Quick Facts
- **arXiv ID**: 2505.19051
- **Source URL**: https://arxiv.org/abs/2505.19051
- **Reference count**: 40
- **Primary result**: Novel data selection framework using influence functions with landmark-based gradient projection achieves up to 3.5× speedup while matching or exceeding baseline performance

## Executive Summary
Influence Distillation introduces a computationally efficient approach to data selection that leverages influence functions to optimally weight training samples based on their impact on a target distribution. The method uses second-order approximations to measure influence without requiring expensive proxy model training, instead employing a landmark-based approach that projects gradients into a low-dimensional embedding space. Experiments demonstrate strong Pareto efficiency, achieving state-of-the-art performance while significantly reducing selection time across multiple model architectures.

## Method Summary
The framework operates by computing influence scores through second-order Taylor approximations of the loss function, which capture how individual training samples affect model parameters. Instead of training proxy models for each candidate sample, Influence Distillation projects gradients into a low-dimensional space using landmarks - carefully selected reference points that capture the essential geometry of the gradient space. This projection enables efficient computation of influence scores while maintaining accuracy. The method then uses these scores to assign optimal weights to training samples, effectively prioritizing those that most improve performance on the target distribution.

## Key Results
- Achieves up to 3.5× faster data selection compared to baseline methods
- Matches or exceeds state-of-the-art performance on instruction tuning tasks
- Demonstrates strong Pareto efficiency across Llama and Qwen model families
- Eliminates need for expensive proxy model training through landmark-based gradient projection

## Why This Works (Mechanism)
The efficiency gains stem from the landmark-based gradient projection that reduces the dimensionality of the influence computation problem. By projecting high-dimensional gradients into a lower-dimensional space spanned by carefully chosen landmarks, the method can approximate influence functions without computing full second-order derivatives for every candidate sample. This approximation maintains sufficient accuracy because the landmark space captures the essential directions of parameter change that matter most for the target distribution.

## Foundational Learning

**Influence Functions**: Measure how upweighting or removing individual training points affects model parameters and predictions. Why needed: Provides theoretical foundation for identifying influential training samples. Quick check: Verify second-order Taylor approximation accurately captures parameter sensitivity.

**Second-Order Approximations**: Use curvature information (Hessian) to approximate how model parameters change with respect to training sample weights. Why needed: Enables efficient computation of influence without full retraining. Quick check: Validate approximation error remains bounded for practical learning rates.

**Landmark-based Embeddings**: Select reference points in gradient space to create a low-dimensional representation that preserves essential geometric properties. Why needed: Reduces computational complexity from O(d) to O(k) where k << d. Quick check: Ensure landmark selection captures diverse gradient directions.

## Architecture Onboarding

**Component Map**: Training data -> Gradient computation -> Landmark projection -> Influence scoring -> Sample weighting -> Fine-tuned model

**Critical Path**: The bottleneck lies in gradient computation and landmark projection stages. Efficient implementation requires parallel gradient computation across data batches and optimized linear algebra operations for the projection step.

**Design Tradeoffs**: Dimensionality of landmark space (k) vs. approximation accuracy - higher k improves accuracy but reduces speedup. Number of landmarks vs. coverage of gradient space - more landmarks capture more diversity but increase computation.

**Failure Signatures**: Poor landmark selection leads to biased influence estimates; insufficient dimensionality causes loss of important gradient directions; numerical instability in second-order approximation can amplify errors.

**First Experiments**: 1) Verify gradient projection preserves top-k influence scores on small synthetic dataset; 2) Test sensitivity to landmark count on real instruction tuning data; 3) Compare approximation error vs. exact influence computation across different model sizes.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical error bounds for landmark-based approximations are not characterized
- Performance generalization beyond instruction tuning tasks remains unverified
- Limited ablation studies on optimal number of landmarks and embedding dimensions

## Confidence

| Claim | Confidence |
|-------|------------|
| Computational efficiency gains (3.5× speedup) | High |
| Performance matching/exceeding baselines | Medium |
| Theoretical soundness of second-order approximations | Medium |
| Generalizability across task types | Low |

## Next Checks

1. Conduct experiments across diverse task types (classification, regression, domain adaptation) to assess generalizability beyond instruction tuning

2. Perform systematic ablation studies varying the number of landmarks and embedding dimensions to characterize the approximation error trade-off

3. Test the method on smaller model architectures (7B and below) to verify the efficiency gains hold across the full model size spectrum