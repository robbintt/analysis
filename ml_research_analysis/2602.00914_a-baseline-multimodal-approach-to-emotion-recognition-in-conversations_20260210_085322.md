---
ver: rpa2
title: A Baseline Multimodal Approach to Emotion Recognition in Conversations
arxiv_id: '2602.00914'
source_url: https://arxiv.org/abs/2602.00914
tags:
- emotion
- recognition
- multimodal
- audio
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a lightweight multimodal baseline for emotion
  recognition in conversations using the SemEval-2024 Task 3 dataset. The approach
  combines transformer-based text classifiers (RoBERTa, DistilBERT, DeBERTa, DistilRoBERTa)
  with a self-supervised speech representation model (Wav2Vec2), employing simple
  late-fusion ensemble.
---

# A Baseline Multimodal Approach to Emotion Recognition in Conversations

## Quick Facts
- arXiv ID: 2602.00914
- Source URL: https://arxiv.org/abs/2602.00914
- Reference count: 40
- Accuracy: 62.97% ensemble accuracy on SemEval-2024 Task 3 dataset

## Executive Summary
This study presents a lightweight multimodal baseline for emotion recognition in conversations using the SemEval-2024 Task 3 dataset. The approach combines transformer-based text classifiers (RoBERTa, DistilBERT, DeBERTa, DistilRoBERTa) with a self-supervised speech representation model (Wav2Vec2), employing simple late-fusion ensemble. Under a limited training protocol, the ensemble model achieved 62.97% accuracy, outperforming unimodal text models (RoBERTa: 50.68%) and audio models (Wav2Vec2: 35.43%). The work documents an accessible reference implementation and highlights the benefits of multimodal fusion, while acknowledging limitations including lightweight evaluation, limited tuning, and domain-specific dataset constraints.

## Method Summary
The method fine-tunes transformer text models and Wav2Vec2 audio models separately on the SemEval-2024 Task 3 dataset (Friends sitcom dialogues with cause-emotion annotations), then combines their predictions through late-fusion ensemble. Text models process utterances through transformer encoders with classification heads, while Wav2Vec2 extracts self-supervised speech representations from 16kHz audio. The ensemble combines RoBERTa (best text model) and Wav2Vec2 (best audio model) predictions. Training uses an 80/20 train/test split with accuracy as the primary metric.

## Key Results
- Ensemble model achieved 62.97% accuracy, outperforming RoBERTa (50.68%) and Wav2Vec2 (35.43%) unimodal baselines
- Late fusion leverages modality-specific strengths, improving robustness over single-modality approaches
- Text models significantly outperform audio models on this dataset, suggesting scripted dialogue may suppress natural vocal emotional expression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Late fusion of text and audio modalities improves emotion recognition accuracy compared to unimodal approaches
- Mechanism: Each modality is processed independently through specialized models (transformers for text, Wav2Vec2 for audio); predictions are combined at the decision level, allowing each modality to contribute complementary information—semantic content from text and paralinguistic cues (tone, pitch, rhythm) from audio
- Core assumption: Emotional states are expressed through both verbal and non-verbal channels that provide non-redundant information
- Evidence anchors:
  - [abstract]: "The ensemble model achieved 62.97% accuracy, outperforming unimodal text models (RoBERTa: 50.68%) and audio models (Wav2Vec2: 35.43%)"
  - [section 2.3.1]: "Late Fusion: Each modality is processed independently, and predictions are combined at the decision level using ensemble techniques such as weighted averaging or voting. Late fusion improves system robustness by leveraging modality-specific strengths"
  - [corpus]: Related work on MERC surveys (arXiv:2505.20511) confirms fusion benefits, though specific late-fusion weightings are dataset-dependent
- Break condition: If modalities are misaligned temporally, or if audio quality degrades (noise, overlapping speech), fusion benefits may diminish or reverse

### Mechanism 2
- Claim: Transformer-based text models capture contextual dependencies for emotion classification more effectively than the audio pipeline in this dataset
- Mechanism: Pre-trained transformers with bidirectional attention process conversational utterances, encoding semantic relationships and contextual cues that map to emotion categories
- Core assumption: Textual content carries sufficient emotional signal, and pre-trained language representations transfer to this specific emotion taxonomy
- Evidence anchors:
  - [abstract]: "transformer-based text classifiers (RoBERTa, DistilBERT, DeBERTa, DistilRoBERTa)"
  - [section 3.4.1]: "RoBERTa achieved the highest accuracy among the text models (50.68%), likely due to its optimized training and ability to capture contextual dependencies effectively"
  - [corpus]: AIMA at SemEval-2024 Task 3 (arXiv:2501.11170) confirms transformer effectiveness for emotion-cause analysis on related data
- Break condition: Sarcasm, irony, or context-dependent emotions that require cross-utterance or speaker-specific modeling may exceed single-utterance transformer capacity

### Mechanism 3
- Claim: Self-supervised speech representations from Wav2Vec2 extract paralinguistic emotional cues, though with lower accuracy than text in this domain
- Mechanism: CNN encoder transforms raw waveforms into latent features; transformer layers capture temporal dependencies; fine-tuning adapts representations to emotion classification
- Core assumption: Vocal attributes (tone, stress, rhythm variations) correlate with emotional states in the dataset
- Evidence anchors:
  - [abstract]: "self-supervised speech representation model (Wav2Vec2)"
  - [section 3.4.2]: "Wav2Vec2 outperformed the other audio models (35.43%), likely benefiting from its robust feature extraction capabilities"
  - [corpus]: Limited direct comparison; "Qieemo" (arXiv:2503.22687) notes audio-only approaches face alignment challenges without multimodal context
- Break condition: Environmental noise, speaker variability, or scripted dialogue (sitcom context) may suppress naturalistic vocal emotional expression

## Foundational Learning

- Concept: **Transformer Attention and Bidirectional Context**
  - Why needed here: Understanding why RoBERTa outperforms DistilRoBERTa despite longer inference time; bidirectional attention captures full utterance context for emotion disambiguation
  - Quick check question: Given the sentence "Oh great, another meeting," how would bidirectional attention help distinguish sarcasm from genuine enthusiasm?

- Concept: **Self-supervised Pre-training Transfer**
  - Why needed here: Both Wav2Vec2 and RoBERTa leverage pre-training on large unlabeled corpora; understanding transfer boundaries helps diagnose when fine-tuning fails
  - Quick check question: Why might Wav2Vec2 pre-trained on general speech underperform on emotion recognition without substantial fine-tuning data?

- Concept: **Fusion Strategy Tradeoffs**
  - Why needed here: Late fusion was chosen for simplicity and robustness; understanding alternatives (early, hybrid) guides future improvements
  - Quick check question: If audio quality varies significantly across samples, why might late fusion be safer than early feature-level fusion?

## Architecture Onboarding

- Component map:
  - Text Pipeline: Raw utterance → Tokenizer → Transformer encoder (RoBERTa/DistilBERT/DeBERTa) → Classification head → Emotion logits
  - Audio Pipeline: Raw waveform (16kHz) → Wav2Vec2 CNN encoder → Transformer layers → Classification head → Emotion logits
  - Fusion Layer: Text logits + Audio logits → Ensemble combination (assumed weighted averaging) → Final prediction

- Critical path:
  1. Data preprocessing: Text-audio alignment, 16kHz standardization, train/test split (80/20)
  2. Unimodal model fine-tuning for each modality independently
  3. Ensemble construction combining best-performing models (RoBERTa + Wav2Vec2)
  4. Evaluation on held-out test set

- Design tradeoffs:
  - **Accuracy vs. Latency**: RoBERTa (50.68%, 74.6s) vs. DistilRoBERTa (41.83%, 5.61s)—12% accuracy gap for 13× speedup
  - **Model complexity vs. tuning budget**: Paper explicitly notes limited hyperparameter search; results may improve with systematic tuning
  - **Domain specificity**: Friends sitcom data may not transfer to real-world conversational domains

- Failure signatures:
  - **Audio underperformance (~35%)**: Check audio-text alignment, sampling consistency, or whether scripted dialogue suppresses natural vocal variation
  - **DistilRoBERTa-sentiment fine-tuned underperforms base models**: Domain mismatch between sentiment labels and emotion taxonomy
  - **Ensemble fails to exceed best unimodal model**: Fusion weights may need calibration; modalities may be correlated rather than complementary

- First 3 experiments:
  1. **Reproduce unimodal baselines**: Fine-tune RoBERTa and Wav2Vec2 separately with documented hyperparameters; verify accuracy ranges (RoBERTa: ~50%, Wav2Vec2: ~35%)
  2. **Implement simple late fusion**: Combine logits with equal weights; target improvement to ~55-60% accuracy
  3. **Fusion weight sweep**: Test weighted combinations (0.6/0.4, 0.7/0.3 text/audio) to determine if 62.97% is achievable with current architecture; document sensitivity to weight selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the late-fusion ensemble change when evaluated using class-imbalance-aware metrics (e.g., Macro-F1) and statistical stability analyses across multiple random seeds?
- Basis in paper: [explicit] The authors state in Section 4 (Limitations and Scope) that they "primarily report accuracy" and that "class-imbalance-aware metrics (e.g., Macro-F1) and statistical stability analyses (multiple random seeds) are left for future work."
- Why unresolved: The current study utilized a limited training protocol focused only on accuracy and wall-clock time, omitting statistical robustness checks.
- What evidence would resolve it: Re-running the experiments using Macro-F1 scores and reporting variance across multiple seeds to confirm that the improvements are statistically significant and not artifacts of class imbalance.

### Open Question 2
- Question: Can the proposed multimodal baseline maintain its accuracy when applied to conversational datasets from domains other than the sitcom *Friends*?
- Basis in paper: [explicit] Section 4 explicitly lists "Dataset and domain constraints" as a limitation, noting the dataset is derived from *Friends* and "may not generalize to other domains, languages, or interaction styles."
- Why unresolved: The model was trained and evaluated exclusively on the SemEval-2024 Task 3 dataset, which consists of scripted dialogue from a specific cultural context.
- What evidence would resolve it: Evaluating the fine-tuned models on out-of-domain conversation datasets (e.g., call center logs, real-life daily dialogs) without further training to test cross-domain generalization.

### Open Question 3
- Question: Does the inclusion of visual features (facial expressions) significantly improve classification accuracy over the text-audio baseline?
- Basis in paper: [explicit] Section 5 (Conclusions) states that "Future research should explore the inclusion of additional modalities, such as facial expressions," and Section 4 lists "Modality coverage: we do not use visual information" as a limitation.
- Why unresolved: The study was restricted to textual and audio data; visual data streams were not processed or fused.
- What evidence would resolve it: Extending the architecture to include a visual encoder (e.g., CNN or Vision Transformer) and comparing the trimodal performance against the current 62.97% accuracy baseline.

### Open Question 4
- Question: Is the late-fusion ensemble robust to modality failure, specifically the low performance of the audio component (35.43%)?
- Basis in paper: [inferred] While the authors report that the ensemble outperforms unimodal models, Section 4 notes that "missing/noisy audio segments can reduce the contribution of the speech modality." The low unimodal audio score (35.43%) suggests the fusion might be heavily weighted toward text or sensitive to audio noise.
- Why unresolved: The paper provides aggregate accuracy but does not perform ablation studies on noisy data or analyze how the fusion mechanism handles the significant performance gap between the text and audio models.
- What evidence would resolve it: A sensitivity analysis measuring ensemble performance when the audio input is corrupted, missing, or replaced with random noise to determine the effective weight of the audio modality.

## Limitations
- The study lacks critical implementation details (fusion mechanism, training hyperparameters) that prevent exact reproduction
- Results are based on a limited training protocol with no statistical stability analysis or class-imbalance-aware metrics
- The Friends sitcom dataset may not generalize to real-world conversational domains or other interaction styles

## Confidence

- **High confidence**: Multimodal fusion improves accuracy over unimodal baselines (62.97% vs 50.68% text, 35.43% audio). This finding is directly supported by reported results and aligns with established MERC research.
- **Medium confidence**: Transformer-based text models capture contextual dependencies effectively. While RoBERTa's superiority over DistilRoBERTa is documented, the paper doesn't explore why or test whether bidirectional attention is the causal factor.
- **Low confidence**: Self-supervised speech representations from Wav2Vec2 are effective for emotion recognition in this domain. Audio-only performance (35.43%) is substantially lower than text, and the paper doesn't analyze whether this reflects model limitations, dataset constraints (scripted sitcom dialogue), or implementation choices.

## Next Checks
1. **Reproduce unimodal baselines**: Fine-tune RoBERTa-base and Wav2Vec2-base separately with documented hyperparameters; verify accuracy ranges (RoBERTa: ~50%, Wav2Vec2: ~35%) match paper claims
2. **Implement and test late fusion variants**: Compare simple averaging, weighted averaging, and learned fusion; document accuracy sensitivity to fusion method selection
3. **Analyze class-wise performance**: Compute per-class accuracy and confusion matrices to determine whether ensemble improvements are uniform across emotion categories or driven by specific labels