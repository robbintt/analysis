---
ver: rpa2
title: 'The Nuclear Route: Sharp Asymptotics of ERM in Overparameterized Quadratic
  Networks'
arxiv_id: '2505.17958'
source_url: https://arxiv.org/abs/2505.17958
tags:
- where
- learning
- case
- error
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes the high-dimensional asymptotics of empirical\
  \ risk minimization in over-parameterized two-layer neural networks with quadratic\
  \ activations, focusing on the regime with quadratically many samples. The authors\
  \ map the \u21132-regularized learning problem to a convex matrix sensing task with\
  \ nuclear norm penalization, revealing that capacity control emerges from a low-rank\
  \ structure in the learned feature maps."
---

# The Nuclear Route: Sharp Asymptotics of ERM in Overparameterized Quadratic Networks

## Quick Facts
- arXiv ID: 2505.17958
- Source URL: https://arxiv.org/abs/2505.17958
- Reference count: 40
- Primary result: Sharp asymptotic characterization of ERM generalization in overparameterized quadratic neural networks with quadratically many samples

## Executive Summary
This paper establishes sharp asymptotic expressions for training and test errors in overparameterized two-layer neural networks with quadratic activations. The authors demonstrate that under specific high-dimensional scaling conditions, the learning problem maps to a convex matrix sensing task with nuclear norm penalization, revealing an implicit low-rank bias in the learned feature maps. Their main result provides closed-form expressions for the test error that explicitly depend on the rank structure of the target function, bridging concepts from spin-glass theory, matrix factorization, and convex optimization.

The analysis identifies a critical threshold for learnability determined by the interplay between the width of the target function and the overparameterization of the network. By establishing this connection between nuclear norm regularization and neural network training, the work provides theoretical justification for the implicit regularization observed in practice and offers quantitative predictions for generalization performance in this specific regime.

## Method Summary
The authors analyze the high-dimensional limit of empirical risk minimization in two-layer quadratic neural networks with ℓ2 regularization. They establish a reduction from the non-convex neural network learning problem to a convex matrix sensing problem by exploiting the quadratic structure of the activations. Specifically, they show that the regularized training objective can be expressed as a linear measurement of the matrix of second-order features, with the regularization emerging naturally as a nuclear norm penalty. This reduction allows the application of convex optimization theory to characterize the global minima and their statistical properties. The analysis leverages the replica method from statistical physics to derive sharp asymptotic expressions for the test error in the limit of large input dimension d and student width m, with sample size N scaling quadratically with d.

## Key Results
- Theorem 1 provides closed-form expressions for training and test errors at global minima, showing explicit dependence on the rank structure of the target function
- The interpolation threshold occurs at a critical sample complexity determined by the width of the target function rather than its rank alone
- Generalization performance can be achieved with zero label noise, demonstrating the benign overfitting phenomenon in this setting
- The analysis reveals that capacity control emerges from an implicit low-rank structure in the learned feature maps, equivalent to nuclear norm regularization

## Why This Works (Mechanism)
The paper's approach works by exploiting the algebraic properties of quadratic activations to linearize the neural network training problem. When the activation is quadratic, the output can be written as a quadratic form x^T S x where S is a matrix parameterizing the network. This allows the entire training procedure to be reformulated as a linear sensing problem with respect to S, where the measurements are the quadratic features of the input data. The ℓ2 regularization on the network weights translates into a nuclear norm constraint on S, making the problem convex. This convexification is the key mechanism that enables sharp asymptotic analysis using techniques from random matrix theory and statistical physics.

## Foundational Learning

**Random Matrix Theory**: Needed to analyze the spectral properties of large random matrices that arise in the high-dimensional limit. Quick check: Verify concentration of spectral norms and eigenvalue distributions.

**Replica Method from Statistical Physics**: Required for deriving sharp asymptotic expressions for the free energy and test error in high-dimensional random systems. Quick check: Validate the self-consistent equations derived from replica symmetry.

**Matrix Sensing and Nuclear Norm Regularization**: Essential for understanding the convex relaxation and its statistical properties. Quick check: Confirm equivalence between nuclear norm minimization and low-rank recovery guarantees.

**Spin Glass Theory**: Provides the framework for analyzing energy landscapes of random systems with many local minima. Quick check: Verify the stability of the replica symmetric solution.

**Convex Optimization Theory**: Needed to establish convergence properties and optimality conditions for the nuclear norm regularized problem. Quick check: Confirm KKT conditions hold at the solution.

## Architecture Onboarding

**Component Map**: Input x -> Quadratic Features (x⊗x) -> Feature Matrix S -> Output (x^T S x) -> Loss Function -> Regularization -> Optimization

**Critical Path**: The essential computational path is from input to quadratic features to matrix S, as this determines the entire learning dynamics and generalization properties.

**Design Tradeoffs**: The choice between student width m and target function rank r creates a fundamental tradeoff between expressivity and generalization. Wider students can fit higher-rank targets but may overfit, while narrower students face non-convex optimization challenges.

**Failure Signatures**: Non-convexity when m < d, breakdown of the nuclear norm equivalence, and poor generalization when the target rank exceeds the student capacity.

**First Experiments**: (1) Verify the spectral distribution of the empirical feature covariance matches theoretical predictions, (2) Test whether training with SGD converges to solutions matching the nuclear norm regularized optimum, (3) Empirically validate the predicted test error scaling across different target ranks.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the non-convexity of the narrow student regime (m < d) affect generalization performance compared to the convex overparameterized limit?
- **Basis in paper:** Section 4 notes that for κ < 1, the rank constraint leads to non-convexity and the failure of Theorem 1. Section 5 lists the requirement m > d as a limitation.
- **Why unresolved:** The reduction to a convex matrix sensing problem with nuclear norm penalization relies on the student width being sufficient to ignore rank constraints (m ≥ d), which breaks down in the narrow regime.
- **What evidence would resolve it:** A rigorous derivation of sharp asymptotics for the test error and spectral properties of the global minima specifically for the rank-constrained case where κ < 1.

### Open Question 2
- **Question:** Can the mapping to convex matrix sensing be generalized to deep architectures with more than two layers?
- **Basis in paper:** Section 5 explicitly lists the "single hidden-layer architecture" as a limitation presenting "highly non-trivial technical challenges."
- **Why unresolved:** The current mapping relies on representing the quadratic network as a quadratic form x^T S x, a structure that does not directly apply to the hierarchical feature representations in deeper networks.
- **What evidence would resolve it:** An analytical framework that extends the nuclear norm regularization equivalence to multi-layer networks, or a proof that such a convexification is impossible for specific deep architectures.

### Open Question 3
- **Question:** Do the low-rank spectral properties and generalization thresholds persist when using non-polynomial activations such as ReLU?
- **Basis in paper:** Section 5 identifies the "choice of quadratic activation" as a limitation. The Introduction posits quadratic networks as the "next natural candidate" after linear models, implying further complexity is unresolved.
- **Why unresolved:** The analysis hinges on the specific algebraic properties of quadratic activations which allow the problem to be mapped to a linear matrix sensing task; non-polynomial activations introduce higher-order statistical dependencies.
- **What evidence would resolve it:** A characterization of the learning curves for networks with ReLU activations in the same high-dimensional regime, demonstrating whether a similar implicit low-rank bias occurs.

## Limitations
- Analysis restricted to quadratic activations, limiting applicability to modern deep learning architectures with nonlinear activations
- Assumes quadratically many samples (N ~ d²), representing a specific scaling regime that may not generalize to other data regimes
- Focus on global minima may miss important phenomena occurring at other stationary points or local minima

## Confidence

**High**: The main theoretical claims about error scaling and interpolation thresholds are supported by the convex relaxation approach and connection to established matrix sensing results.

**Medium**: The practical implications for real-world neural network training have Medium confidence, as the quadratic activation assumption is restrictive.

**Low**: The claim that low-rank target functions achieve optimal generalization has Low confidence without empirical validation across diverse target functions.

## Next Checks
- Numerical experiments comparing predicted vs. actual test errors for quadratic networks across different widths and ranks
- Analysis of how the results extend or fail to extend to networks with ReLU or other common activations
- Empirical study of whether the implicit low-rank bias emerges under standard training procedures (SGD, Adam) rather than nuclear norm regularization