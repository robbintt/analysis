---
ver: rpa2
title: '"As Eastern Powers, I will veto." : An Investigation of Nation-level Bias
  of Large Language Models in International Relations'
arxiv_id: '2511.10695'
source_url: https://arxiv.org/abs/2511.10695
tags:
- bias
- nation
- unsc
- united
- russia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study systematically examines nation-level biases in Large
  Language Models (LLMs) applied to International Relations (IR) tasks. Using real-world
  UNSC voting records, the authors developed a multi-faceted evaluation framework
  comprising three distinct tests: Direct Question-Answering, Association Tests, and
  Persona-Assigned Vote Simulations.'
---

# "As Eastern Powers, I will veto." : An Investigation of Nation-level Bias of Large Language Models in International Relations

## Quick Facts
- **arXiv ID**: 2511.10695
- **Source URL**: https://arxiv.org/abs/2511.10695
- **Reference count**: 40
- **Primary result**: Large Language Models exhibit consistent nation-level biases in International Relations tasks, showing positive bias toward Western powers and negative bias toward Russia

## Executive Summary
This study systematically examines nation-level biases in Large Language Models (LLMs) when applied to International Relations (IR) tasks. Using real-world UNSC voting records, the authors developed a comprehensive evaluation framework with three distinct tests: Direct Question-Answering, Association Tests, and Persona-Assigned Vote Simulations. The research reveals consistent patterns of positive bias toward the U.K., France, and U.S., and negative bias toward Russia across different models, though bias patterns vary by model and evaluation context. Notably, the study finds that models with stronger reasoning capabilities exhibit reduced bias and improved performance. To address these biases, the authors propose a debiasing framework combining Retrieval-Augmented Generation with Reflexion-based self-reflection, which significantly reduces nation-level bias and improves performance, particularly for GPT-4o-mini and Llama-3.3-70B.

## Method Summary
The authors developed a multi-faceted evaluation framework to assess nation-level bias in LLMs applied to International Relations tasks. The framework consists of three distinct tests using real-world UNSC voting records: Direct Question-Answering tests where models predict voting outcomes, Association Tests measuring implicit associations between nations and positive/negative attributes, and Persona-Assigned Vote Simulations where models simulate voting as specific nations. The study tested multiple LLM models across these evaluation methods to identify bias patterns. Additionally, the authors proposed and tested a debiasing framework that combines Retrieval-Augmented Generation (RAG) with Reflexion-based self-reflection techniques to reduce nation-level bias in model outputs.

## Key Results
- Consistent positive bias toward the U.K., France, and U.S., and negative bias toward Russia across tested models
- Bias patterns vary by model and evaluation context, with single models showing different biases across different tests
- Models with stronger reasoning capabilities exhibit reduced bias and improved performance
- The proposed debiasing framework combining RAG with Reflexion significantly reduces nation-level bias and improves performance, particularly for GPT-4o-mini and Llama-3.3-70B

## Why This Works (Mechanism)
The study demonstrates that nation-level bias in LLMs emerges from their training data and manifests differently depending on the evaluation context and task type. The proposed debiasing framework works by incorporating external knowledge through RAG to provide context-aware information, while Reflexion enables self-reflection and correction of biased outputs. This combination helps models recognize and adjust for their inherent biases by leveraging both external validation and internal reasoning capabilities.

## Foundational Learning

**UNSC Voting Records Analysis**
*Why needed*: Provides real-world diplomatic scenarios for evaluating model bias in international relations contexts
*Quick check*: Verify that voting records represent diverse geopolitical situations and time periods

**Association Testing Methodology**
*Why needed*: Captures implicit biases that may not appear in explicit responses
*Quick check*: Ensure test pairs are balanced and culturally neutral

**Persona-based Simulation**
*Why needed*: Evaluates how models represent different national perspectives and decision-making
*Quick check*: Confirm persona instructions are clear and representative of actual diplomatic positions

## Architecture Onboarding

**Component Map**
RAG Module -> Reflexion Engine -> LLMs -> Evaluation Framework

**Critical Path**
The critical path flows from RAG Module providing context through Reflexion Engine enabling self-correction to LLMs generating outputs, which are then evaluated through the framework. The effectiveness depends on seamless integration between retrieval accuracy, reflection quality, and model reasoning.

**Design Tradeoffs**
- RAG vs. fine-tuning: RAG offers flexibility without retraining costs but may have retrieval latency
- Reflexion depth vs. efficiency: More reflection steps improve accuracy but increase computational cost
- Evaluation comprehensiveness vs. practicality: More tests provide better bias coverage but require more resources

**Failure Signatures**
- Retrieval failures leading to irrelevant context provision
- Reflection loops getting stuck in biased reasoning patterns
- Model outputs showing inconsistent bias patterns across similar contexts
- Evaluation metrics failing to capture subtle forms of bias

**First Experiments**
1. Test RAG retrieval accuracy with simple diplomatic queries before integration
2. Validate Reflexion engine's ability to identify and correct known biases in controlled scenarios
3. Run single-model bias evaluation across all three test types to establish baseline patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation framework relies heavily on UNSC voting records, which may not capture full complexity of international relations scenarios
- Sample size of historical voting cases may not represent complete spectrum of diplomatic situations
- Debiasing framework tested primarily on limited set of models (GPT-4o-mini and Llama-3.3-70B), raising generalizability questions

## Confidence

**High**: Systematic observation of consistent bias patterns toward specific nations (U.K., France, U.S.) and against Russia across multiple evaluation methods

**Medium**: Relationship between model reasoning capabilities and reduced bias, as this correlation needs further validation across diverse model families

**Medium**: Effectiveness of proposed debiasing framework given limited testing scope across different model architectures

## Next Checks

1. Test debiasing framework across broader range of LLM architectures and sizes, including open-source models with different training paradigms

2. Evaluate bias patterns using diverse international relations scenarios beyond UNSC voting records, including crisis simulations and trade negotiations

3. Conduct longitudinal studies to assess whether debiased models maintain reduced bias over extended periods and varied use cases