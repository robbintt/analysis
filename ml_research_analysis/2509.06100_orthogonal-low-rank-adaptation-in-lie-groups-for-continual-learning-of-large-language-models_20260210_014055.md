---
ver: rpa2
title: Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of Large
  Language Models
arxiv_id: '2509.06100'
source_url: https://arxiv.org/abs/2509.06100
tags:
- learning
- task
- tasks
- updates
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces OLieRA, a continual learning framework for\
  \ large language models that addresses catastrophic forgetting through Lie group\u2013\
  based multiplicative updates while enforcing orthogonality across task subspaces.\
  \ Unlike existing methods that apply additive updates, OLieRA preserves the intrinsic\
  \ geometry of model parameters by modeling updates as elements of a Lie group, mapping\
  \ them via the exponential map to ensure structure-preserving adaptations."
---

# Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of Large Language Models

## Quick Facts
- arXiv ID: 2509.06100
- Source URL: https://arxiv.org/abs/2509.06100
- Reference count: 40
- Achieves state-of-the-art 79.6% average accuracy on Standard CL benchmark

## Executive Summary
OLieRA introduces a novel continual learning framework for large language models that leverages Lie groupâ€“based multiplicative updates to address catastrophic forgetting. By modeling parameter updates as elements of Lie groups and enforcing orthogonality constraints across task subspaces, OLieRA preserves the intrinsic geometry of model parameters while minimizing task interference. The method achieves performance approaching multi-task learning upper bounds while maintaining replay-free and task-ID-free inference capabilities.

## Method Summary
OLieRA operates by applying multiplicative updates to model parameters using the exponential map of Lie groups, ensuring structure-preserving adaptations that maintain parameter space geometry. Unlike traditional additive update methods, OLieRA enforces orthogonality constraints not only on low-rank matrices but across the entire updated parameter space. This geometric approach to parameter updates reduces interference between tasks while preserving knowledge of previously learned tasks. The framework builds upon O-LoRA's advantages while introducing improved parameter utilization and expressiveness through controlled updates along sensitive parameter directions.

## Key Results
- Achieves state-of-the-art average accuracy of 79.6% on Standard CL benchmark
- Approaches upper bound performance of multi-task learning
- Maintains competitive performance of 72.6% average accuracy under long task sequences (20 tasks)
- Preserves replay-free and task-ID-free inference capabilities

## Why This Works (Mechanism)
OLieRA works by leveraging the mathematical properties of Lie groups to perform structure-preserving parameter updates. The exponential map transforms updates from the Lie algebra to the Lie group, ensuring that parameter modifications maintain the intrinsic geometric structure of the model space. Orthogonality constraints prevent interference between task subspaces by keeping updated parameters in mutually orthogonal directions, reducing catastrophic forgetting. This geometric approach allows for more efficient knowledge sharing between tasks while preserving prior learning.

## Foundational Learning
**Lie Groups**: Mathematical structures that combine group properties with smooth manifold structure; needed for modeling continuous parameter transformations in model space; quick check: verify group axioms and smoothness conditions
**Exponential Map**: Function mapping elements from Lie algebra to Lie group; needed to transform parameter updates while preserving geometric structure; quick check: confirm bijectivity and local diffeomorphism properties
**Orthogonal Constraints**: Mathematical conditions ensuring perpendicularity between parameter subspaces; needed to prevent interference between tasks; quick check: verify orthogonality preservation through update operations
**Catastrophic Forgetting**: Phenomenon where learning new tasks degrades performance on previously learned tasks; needed context for continual learning challenges; quick check: measure performance drop across task sequences
**Parameter Geometry**: Intrinsic structure of model parameter space; needed for understanding why geometric updates matter; quick check: analyze parameter manifold properties

## Architecture Onboarding
**Component Map**: Input Data -> Task Subspaces -> Lie Group Updates -> Exponential Map -> Orthogonal Constraints -> Updated Parameters -> Model Output
**Critical Path**: The exponential map application combined with orthogonality enforcement represents the critical path, as these operations directly determine parameter update quality and task interference levels.
**Design Tradeoffs**: Lie group operations provide geometric advantages but increase computational complexity; orthogonality constraints reduce interference but may limit parameter expressiveness; multiplicative updates preserve structure but require more sophisticated optimization.
**Failure Signatures**: Performance degradation in long task sequences suggests limitations in extreme continual learning scenarios; computational overhead increases with model size and task complexity.
**First Experiments**: 1) Ablation study comparing Lie group updates versus standard additive updates; 2) Analysis of orthogonality constraint impact on parameter utilization; 3) Scalability testing across different model sizes (1B, 7B, 13B parameters)

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to extremely long task sequences beyond 20 tasks remains untested
- Performance degradation suggests potential limitations in very long continual learning scenarios
- Computational overhead of Lie group operations may scale poorly with model size
- Theoretical geometric advantages lack extensive empirical validation
- Limited testing across different model architectures and task domains

## Confidence
**Major Claim Clusters Confidence:**
- State-of-the-art performance on Standard CL benchmark: **High** (supported by quantitative results)
- Geometric advantages of Lie group updates: **Medium** (theoretically justified but limited empirical validation)
- Scalability to long task sequences: **Medium** (results show degradation but remain competitive)

## Next Checks
1. Test OLieRA on non-language tasks (e.g., computer vision or multimodal tasks) to verify cross-domain applicability
2. Evaluate computational overhead and memory requirements across different model scales (1B, 7B, 13B parameters)
3. Conduct ablation studies isolating the contributions of Lie group updates versus orthogonality constraints