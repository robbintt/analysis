---
ver: rpa2
title: 'Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on
  Chinese Multi-Subject Physics Problems'
arxiv_id: '2509.15839'
source_url: https://arxiv.org/abs/2509.15839
tags:
- uni00000048
- physics
- reasoning
- evaluation
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Multi-Physics, a comprehensive Chinese multimodal
  benchmark for physics reasoning that addresses gaps in existing benchmarks regarding
  fine-grained subject coverage, step-by-step reasoning evaluation, and visual information
  assessment. The benchmark comprises 1,412 image-associated multiple-choice questions
  across 11 high-school physics subjects and 5 difficulty levels.
---

# Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems

## Quick Facts
- arXiv ID: 2509.15839
- Source URL: https://arxiv.org/abs/2509.15839
- Reference count: 0
- Primary result: Visual information improves physics reasoning accuracy across all evaluated models

## Executive Summary
This paper introduces Multi-Physics, a Chinese multimodal benchmark for physics reasoning that addresses gaps in existing benchmarks regarding fine-grained subject coverage, step-by-step reasoning evaluation, and visual information assessment. The benchmark comprises 1,412 image-associated multiple-choice questions across 11 high-school physics subjects and 5 difficulty levels. A dual evaluation framework is proposed, combining final answer accuracy with step-by-step chain-of-thought (CoT) integrity assessment using model-based evaluation. The evaluation of 20 different multimodal LLMs shows that visual information significantly improves performance across all models, with some models like o4-mini and Gemini-2.5-Pro demonstrating particularly notable improvements when images are provided. The benchmark reveals varying reasoning capabilities across physics subjects and demonstrates that while CoT accuracy correlates with final answer accuracy, models sometimes arrive at correct answers through flawed reasoning. The work provides a valuable resource for understanding multimodal reasoning in physics and highlights the importance of visual information in complex scientific problem-solving.

## Method Summary
Multi-Physics evaluates multimodal LLMs on 1,412 Chinese high-school physics questions with associated images across 11 subjects and 5 difficulty levels. The dual evaluation framework computes both answer accuracy (ACC) using a partial credit scoring function and chain-of-thought integrity via Average Step Accuracy (ASA) and Average Step Count (ASC). Models are evaluated under two conditions: with images (w/) and without images (w/o). A model-based evaluator (Gemini-2.5-Flash) judges the correctness of individual reasoning steps extracted from model outputs. The benchmark tests 20 MLLMs (12 closed-source, 8 open-source) and reveals that visual information significantly improves performance across all models, with varying capabilities across physics subjects.

## Key Results
- Visual information significantly improves performance across all models, with o4-mini and Gemini-2.5-Pro showing particularly notable gains
- Models exhibit varying capabilities across different physics subjects, with mechanics problems showing stronger visual grounding benefits than electromagnetism problems
- Correct final answers sometimes arise from flawed reasoning chains, as revealed by the CoT evaluation framework
- The benchmark achieves 88% agreement with human judgment in pilot tests and demonstrates consistency across evaluation rounds

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Visual information provides task-relevant constraints that improve physics reasoning accuracy across model architectures.
- **Mechanism:** Images supply spatial relationships, force vectors, and circuit configurations that would otherwise require verbose textual description. When models process these visual inputs alongside text, they ground their reasoning in concrete physical configurations rather than abstract descriptions, reducing ambiguity in problem interpretation.
- **Core assumption:** Models possess sufficient cross-modal alignment capabilities to map visual features to corresponding physical concepts.
- **Evidence anchors:** "visual information significantly improves performance across all models, with some models like o4-mini and Gemini-2.5-Pro demonstrating particularly notable improvements when images are provided"; "When image input is provided, the ACC and ASA of all MLLMs in the overall subject are significantly higher than when no image input is provided"; PhysicsArena paper confirms "physics reasoning presents unique challenges, requiring grounding in physical conditions and the interpretation of multimodal information"
- **Break condition:** If visual encoding quality degrades (low-resolution diagrams, complex multi-object scenes), the mechanism may fail to provide useful grounding.

### Mechanism 2
- **Claim:** Chain-of-thought step evaluation reveals reasoning-path independence from final-answer correctness.
- **Mechanism:** The dual evaluation decomposes problem-solving into intermediate reasoning steps (ACCstep) and final selection (ACCtotal). Model-based evaluation using Gemini-2.5-Flash judges each step's physical validity, enabling detection of cases where models reach correct answers through invalid logic chains—a phenomenon the paper explicitly documents.
- **Core assumption:** The evaluator model (Gemini-2.5-Flash) can reliably judge physics reasoning correctness at the step level.
- **Evidence anchors:** "models sometimes arrive at correct answers through flawed reasoning"; "By evaluating the model's CoT, we can precisely point out the logical flaws in the steps rather than merely judging the correctness of the final answer"; UGPhysics benchmark similarly emphasizes "evaluating LLMs' abilities" beyond surface-level metrics, though specific CoT mechanisms vary
- **Break condition:** If evaluator model exhibits systematic bias (e.g., over-penalizing unconventional but valid approaches), step-accuracy scores become unreliable indicators of true reasoning quality.

### Mechanism 3
- **Claim:** Physics-subject granularity exposes domain-specific reasoning gaps invisible in aggregate benchmarks.
- **Mechanism:** By partitioning 1,412 questions across 11 subjects (mechanics through electromagnetism), the benchmark reveals performance variation tied to conceptual abstraction levels and visual-representation types. Mechanics problems benefit from direct spatial visualization; electromagnetic problems require more symbolic-formula integration alongside visual interpretation.
- **Core assumption:** High-school physics subject boundaries meaningfully differentiate reasoning skill requirements.
- **Evidence anchors:** "models also exhibit varying capabilities across different physics subjects, which highlights the necessity of a detailed assessment of physics knowledge"; "For mechanics problems (A-F), VI can directly provide key details such as object position and velocity direction... For electromagnetism problems (G-K), VI is equally important, but the model's textual understanding and formula application capabilities also play a large role"; MME-SCI and MDK12-Bench similarly use multi-domain partitioning, confirming this granularity approach, though neither focuses specifically on physics sub-subjects
- **Break condition:** If subject boundaries are too coarse (e.g., "electromagnetism" conflates distinct reasoning patterns), intra-subject variation may exceed inter-subject variation.

## Foundational Learning

- **Concept: Cross-modal grounding in scientific diagrams**
  - **Why needed here:** The benchmark's core finding is that images improve reasoning, but this requires understanding how models map visual elements (force arrows, circuit nodes) to textual physical concepts.
  - **Quick check question:** Given a diagram showing a block on an inclined plane with friction, can you trace how a model might associate the visual angle marker with the textual "θ = 30°" in the problem statement?

- **Concept: Step-wise verification vs. outcome evaluation**
  - **Why needed here:** The paper's dual evaluation framework treats these as separate signals; understanding their relationship is essential for interpreting benchmark results.
  - **Quick check question:** If a model achieves 80% final-answer accuracy but only 45% step-accuracy, what hypotheses might explain this gap?

- **Concept: Difficulty scaling in physics problems**
  - **Why needed here:** The benchmark uses 5 difficulty levels; the paper shows different models respond differently to difficulty increases with/without images.
  - **Quick check question:** What features distinguish a Level-1 mechanics problem from a Level-5 problem in the benchmark's taxonomy (calculation steps, abstraction, visual complexity)?

## Architecture Onboarding

- **Component map:** Data layer (1,412 questions in JSON format partitioned into 11 subject files A-K) -> Evaluation layer (answer-matching scoring function S(q) and step-extraction + Gemini-2.5-Flash judge for ACCstep) -> Experiment layer (image-present vs. image-absent conditions; ASC/ASA metrics computed per model per subject)

- **Critical path:** 1. Load question JSON + associated image 2. Prompt target MLLM with question text ± image 3. Extract final answer choice → compute ACCtotal 4. Parse reasoning steps from model output → submit each to evaluator → compute ACCstep, ASC 5. Aggregate across questions → compute ASA, compare w/ vs. w/o image conditions

- **Design tradeoffs:** Using Gemini-2.5-Flash as step evaluator introduces dependency on a specific model; evaluator bias may systematically affect all judged models; Multiple-choice format enables automated scoring but constrains problem expressiveness compared to open-ended physics problems; Chinese-only benchmark limits cross-linguistic generalization; corpus shows other benchmarks (Multi-TW, MCIF) address different language contexts

- **Failure signatures:** Model generates correct answer but step-evaluator marks all steps incorrect → check for evaluation-prompt misalignment; Large ACC/ASA discrepancy on specific subjects → inspect whether subject-specific visual conventions (circuit diagrams vs. free-body diagrams) are poorly encoded; Model fails to follow CoT template in "w/o images" mode (observed with QvQ-Max) → indicates instruction-following fragility without visual grounding

- **First 3 experiments:** 1. Baseline replication: Run Gemini-2.5-Pro on 50 random questions in both image conditions; verify ACC improvement matches paper's reported ~13-point gain 2. Evaluator robustness check: Manually annotate 20 reasoning chains; compare human judgments vs. Gemini-2.5-Flash step-evaluator agreement rate 3. Subject-specific ablation: For Mechanics (subjects A-F) vs. Electromagnetism (G-K), compute per-subject ACC delta (w/ image − w/o image); test whether visual-grounds utility differs significantly between domains

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can "deep thinking" MLLMs be stabilized to maintain strict instruction following and template adherence when visual inputs are removed?
- **Basis in paper:** The authors note that QvQ-Max in the "w/o images" mode failed to adequately follow the CoT evaluation template (Page 4).
- **Why unresolved:** The paper identifies the failure mode but does not propose architectural or prompting solutions to ensure consistent behavior across modalities.
- **What evidence would resolve it:** A study showing consistent template adherence rates across "w/ image" and "w/o image" conditions using improved prompting or fine-tuning.

### Open Question 2
- **Question:** What specific interventions are effective at preventing MLLMs from reaching correct final answers through flawed reasoning steps in physics?
- **Basis in paper:** The evaluation revealed cases where models (e.g., Claude-4-Sonnet) produced correct answers despite flawed intermediate logic (Page 3).
- **Why unresolved:** The benchmark detects the phenomenon (low ASA with high ACC) but does not explore methods to align the reasoning path with the final result.
- **What evidence would resolve it:** Experiments with process-supervision reward models (PRMs) or reasoning-alignment techniques showing increased correlation between ASA and ACC.

### Open Question 3
- **Question:** Does the performance gain from visual information depend on whether the problem's solution is logically impossible without the image?
- **Basis in paper:** The paper reports general gains from images but does not classify problems by "visual necessity," a distinction made in related work like Mathverse [37].
- **Why unresolved:** It is unclear if models improve because they read text from images, utilize spatial data, or simply use the image as a general context anchor.
- **What evidence would resolve it:** A subset analysis comparing model performance on "text-redundant" vs. "text-insufficient" physics problems.

### Open Question 4
- **Question:** To what extent does the choice of Gemini-2.5-Flash as a judge introduce bias into the Average Step Accuracy (ASA) metrics?
- **Basis in paper:** The methodology relies entirely on a single model (Gemini-2.5-Flash) for step-by-step evaluation without reporting human inter-annotator agreement (Page 3).
- **Why unresolved:** Model-based evaluation may inherently favor outputs that resemble the judge model's own reasoning style or syntax.
- **What evidence would resolve it:** A validation study comparing Gemini-2.5-Flash's judgment scores against a human-annotated gold standard subset of the CoT data.

## Limitations

- The benchmark covers only Chinese high-school physics, limiting generalizability to other educational contexts or scientific domains
- Performance differences across difficulty levels may reflect test-taking strategies rather than genuine reasoning improvements
- The evaluator model (Gemini-2.5-Flash) may introduce systematic bias in step-wise correctness judgments

## Confidence

- **High confidence:** Primary claims about visual information improving physics reasoning are well-supported by clear quantitative improvements (ACC and ASA) across all evaluated models when images are provided
- **High confidence:** Dual evaluation framework showing correct answers can arise from flawed reasoning is directly observed across multiple subjects
- **Medium confidence:** Subject-specific performance differences rely on assumptions about visual encoding interactions with symbolic reasoning that weren't explicitly tested
- **Low confidence:** Generalizability beyond Chinese high-school physics is limited by language and curriculum specificity

## Next Checks

1. **Evaluator reliability test**: Manually annotate 50 reasoning chains from different models and compare human vs. Gemini-2.5-Flash judgments to quantify evaluator bias
2. **Cross-linguistic generalization**: Translate 100 benchmark questions into English and evaluate the same models to test whether visual benefits transfer across languages
3. **Open-ended extension**: Convert 20 multiple-choice questions to open-ended format and compare model performance to assess if multiple-choice format constrains observed capabilities