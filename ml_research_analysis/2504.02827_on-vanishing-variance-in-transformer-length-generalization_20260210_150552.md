---
ver: rpa2
title: On Vanishing Variance in Transformer Length Generalization
arxiv_id: '2504.02827'
source_url: https://arxiv.org/abs/2504.02827
tags:
- length
- attention
- sequence
- variance
- normalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates why transformers struggle with length\
  \ generalization\u2014specifically, their inability to robustly extrapolate to longer\
  \ sequences than seen during training. The authors propose that this stems from\
  \ a \"vanishing variance\" phenomenon: as sequence length increases, the variance\
  \ of attention outputs decreases, causing a distribution shift that hinders performance."
---

# On Vanishing Variance in Transformer Length Generalization

## Quick Facts
- arXiv ID: 2504.02827
- Source URL: https://arxiv.org/abs/2504.02827
- Authors: Ruining Li; Gabrijel Boduljak; Jensen; Zhou
- Reference count: 12
- One-line primary result: Post-attention layer normalization mitigates vanishing variance, improving transformer length generalization on synthetic tasks.

## Executive Summary
This paper investigates why transformers struggle to extrapolate to longer sequences than seen during training. The authors identify a "vanishing variance" phenomenon: as sequence length increases, the variance of attention outputs decreases, causing a distribution shift that degrades downstream performance. They formalize this theoretically and observe it empirically in both synthetic and frontier LLMs. To address this, they apply layer normalization immediately after attention outputs, which stabilizes the distribution of attention features. Experiments on two synthetic tasks show that layer normalization significantly improves accuracy on out-of-distribution sequence lengths.

## Method Summary
The authors study length generalization using synthetic order-invariant tasks (argmax retrieval and dictionary lookup) with single-layer, single-head transformers. Models train on sequences up to length 16 and are evaluated on sequences up to 2^14. The key intervention is inserting layer normalization immediately after the attention outputs, before the residual connection. They also test standardization as an ablation. Training uses 100k steps for argmax and 10k for dictionary lookup, with hyperparameters based on prior work. Variance profiling and accuracy evaluation across sequence lengths quantify the phenomenon and the effectiveness of the proposed solution.

## Key Results
- Attention output variance decays as a power law (σ ∝ N^(-0.5)) with increasing sequence length.
- Post-attention layer normalization significantly improves length generalization accuracy on synthetic tasks.
- Standardization without learnable parameters yields similar but weaker improvements, highlighting the importance of addressing distribution shift.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Increasing input sequence length causes the variance of attention outputs to decrease (vanishing variance), inducing a distribution shift that degrades downstream performance.
- **Mechanism:** As sequence length N increases, the softmax attention weights tend to disperse (mass spreads over more tokens). The attention output is a weighted sum of values; as the weights flatten (approaching a uniform average) and N grows, the variance of this aggregate output shrinks toward zero (consistent with Proposition 1). The subsequent MLP, trained on higher-variance features from shorter sequences, receives out-of-distribution (OOD) low-variance inputs during inference, causing accuracy loss.
- **Core assumption:** Input tokens are approximately i.i.d. (or sufficiently unstructured) such that the central limit behavior applies, and E[W V x] ≈ 0.
- **Evidence anchors:**
  - [abstract] "...a longer sequence length results in a decrease in variance in the output of the multi-head attention modules."
  - [section 1] Proposition 1 formalizes the limit of variance approaching 0 as N → ∞.
  - [section 1, Figure 1] Empirical plots show standard deviation σ decaying with a power law (∝ N^(-0.5)) in Llama-3.2-1B.
- **Break condition:** If positional encodings enforce strict locality or if the task requires retrieving a unique "needle" with extremely high confidence (sharp attention peaks), variance may not vanish as rapidly.

### Mechanism 2
- **Claim:** Applying Layer Normalization (LN) immediately after attention outputs mitigates the distribution shift by stabilizing global mean and variance.
- **Mechanism:** LN standardizes features across the dimension D for each token, re-scaling them to have zero mean and unit variance (modified by learnable γ, β). This forces the input distribution to the subsequent MLP to remain within the training distribution's statistical range, even if the raw attention outputs have collapsed variance.
- **Core assumption:** The relevant semantic information is preserved through the normalization process (i.e., the relative magnitudes of features are not the sole carriers of signal, or the learnable parameters can recover them).
- **Evidence anchors:**
  - [abstract] "...applying layer normalization after the attention outputs leads to significantly better length generalization."
  - [section 3.1, Figure 3] LN reduces the drift in global mean and the decay in global variance compared to the baseline.
- **Break condition:** If the absolute magnitude of the attention output is the critical signal (rather than relative feature ratios), LN might suppress this information.

### Mechanism 3
- **Claim:** The "vanishing variance" is partially driven by the "dispersion" of softmax attention weights (inability to maintain sharp focus).
- **Mechanism:** Softmax saturation limits the maximum attention weight (A_n) achievable on longer sequences, forcing the model to aggregate more inputs. This aggregation smooths out the signal, directly leading to the variance drop described in Mechanism 1.
- **Core assumption:** The query-key dot products do not scale sufficiently to maintain sharp peaks as the sequence grows (related to the "softmax is not enough" observation).
- **Evidence anchors:**
  - [section 4] Cites Veličković et al. (2024) regarding attention weight dispersion.
  - [appendix A] Proof relies on the bound (max A_n)^2 < C/N^2, linking weight dispersion directly to the variance bound.
  - [corpus] Related work "Delayed Attention Training..." confirms length generalization issues in attention mechanisms.
- **Break condition:** Mechanisms like adaptive temperature or sharp activation functions (e.g., Sparsemax) that prevent weight dispersion would likely reduce the vanishing variance effect.

## Foundational Learning

- **Concept: Distribution Shift (Covariate Shift)**
  - **Why needed here:** The paper frames length generalization failure not as a reasoning failure, but as a statistical failure where the input statistics (variance) to the MLP change between training (short) and testing (long).
  - **Quick check question:** If you train a model on data with variance σ^2=10 and test it on data with σ^2=0.1, do you expect the ReLU activations to fire in the same proportions?

- **Concept: Softmax Temperature and Dispersion**
  - **Why needed here:** Understanding why attention weights "disperse" (flatten) on longer sequences is key to grasping the root cause of the variance collapse.
  - **Quick check question:** For a fixed set of logits, does increasing the sequence length N (and thus the denominator size in potential comparisons) implicitly change the effective temperature or relative probability mass?

- **Concept: Order-Invariance vs. Positional Encoding**
  - **Why needed here:** The authors deliberately use order-invariant tasks (argmax, dictionary lookup) to isolate the variance issue from the distinct failure mode of positional encoding extrapolation.
  - **Quick check question:** If a task requires knowing that Token A comes before Token B, would removing positional encoding (as discussed in the paper's setup) break the model's ability to learn, regardless of variance issues?

## Architecture Onboarding

- **Component map:**
  - Standard: Input → Attention(Q,K,V) → Attn_Output → Residual_Add → LN → MLP
  - Proposed (Post-Attn Norm): Input → Attention → Attn_Output → LN → Residual_Add → MLP
  - *Note:* The critical insertion point is immediately after the attention mechanism and before the residual connection (or subsequent projection), to normalize the features before they interact with the rest of the network.

- **Critical path:**
  - The path from Softmax → Weighted Sum (Attn Output) → LN is the critical region. The Softmax produces the dispersing weights; the Weighted Sum reduces variance; the LN attempts to correct this scaling.

- **Design tradeoffs:**
  - Standardization vs. LN: Standardization (fixed mean/var) helps but learnable LN performs better (Table 3), suggesting the model needs flexibility to re-scale features.
  - Capacity vs. Robustness: The authors note that standardization strictly constrains model capacity, yet it improves OOD generalization.
  - Placement: Placing LN too late (after the residual addition) allows the "collapsed" signal to mix with the residual, potentially masking the issue or making it harder to normalize effectively.

- **Failure signatures:**
  - Log-Log Decay: Plotting standard deviation of Attn_Output vs. Sequence Length N. A linear decay on a log-log plot (slope ≈ -0.5) confirms the phenomenon.
  - Accuracy Cliff: Accuracy remains high until N exceeds training length, then drops precipitously.
  - Mean Drift: The global mean of attention outputs shifts away from the training baseline as N increases.

- **First 3 experiments:**
  1. **Variance Profiling (Reproduction):** Run a standard pre-trained model (e.g., Llama or a synthetic variant) on random i.i.d. tokens of increasing length. Plot the std dev of the attention outputs in the first layer to confirm σ ∝ N^(-0.5).
  2. **Post-Attn LN Ablation:** Implement the "Argmax Retrieval" task. Train a 1-layer transformer on N ≤ 16. Compare test accuracy on N=2^14 between a baseline and a model with LN inserted immediately after the attention calculation.
  3. **Global Statistics Check:** On the failing baseline model from Experiment 2, visualize the histogram of attention output features for N=16 vs N=2^14. Observe the "narrowing" of the distribution (Figure 2 visualization).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does applying layer normalization immediately after attention outputs improve length generalization in large-scale, multi-layer LLMs on complex benchmarks?
- **Basis in paper:** [explicit] The authors state their conclusions may not generalize to multi-layer/multi-head architectures and explicitly call for validating normalization strategies on benchmarks like CLRS and real-world LLMs.
- **Why unresolved:** The experiments were restricted to a single-layer, single-head synthetic setup (argmax retrieval and dictionary lookup) to isolate the mechanism from confounding factors.
- **What evidence would resolve it:** Demonstrating consistent accuracy improvements on out-of-distribution sequence lengths in multi-billion parameter models or the CLRS algorithmic reasoning benchmark using the proposed normalization.

### Open Question 2
- **Question:** Can transformer architectures be designed to be provably invariant to sequence length variations?
- **Basis in paper:** [explicit] The conclusion expresses hope that the findings will motivate research into architectures that are "provably robust (e.g., invariant) to varying sequence lengths."
- **Why unresolved:** The current proposed solution (layer normalization) acts as a mitigation strategy that reduces but does not completely eliminate the distribution shift caused by vanishing variance.
- **What evidence would resolve it:** The derivation of a new attention mechanism or network layer that mathematically guarantees constant output variance regardless of input sequence length.

### Open Question 3
- **Question:** How does vanishing variance propagate and compound in deep, multi-layer transformer stacks?
- **Basis in paper:** [inferred] The paper acknowledges that real-world models are typically multi-layer and multi-head, and the conclusions drawn from a single-layer architecture may not fully generalize to these complex setups.
- **Why unresolved:** A single-layer analysis cannot capture recursive distribution shifts or how variance decay in early layers affects the inputs of subsequent layers in deep networks.
- **What evidence would resolve it:** An empirical study tracking variance statistics layer-by-layer in deep transformers (e.g., Llama-70B) when exposed to varying context lengths.

## Limitations
- The synthetic tasks used are order-invariant and lack semantic structure, which may limit how directly findings transfer to natural language or other structured domains.
- The proposed solution—layer normalization immediately after attention—improves length generalization in the reported experiments, but the effect size and generalizability across architectures and tasks remain uncertain.
- The theoretical analysis relies on Proposition 1, which shows that attention output variance approaches zero as sequence length grows under i.i.d. input assumptions, but the practical relevance depends on how closely real-world data and model behavior match these assumptions.

## Confidence
- **High Confidence:** The empirical observation that attention output variance decreases with sequence length is well-supported by Figure 1 and consistent across multiple models. The mechanism of distribution shift causing downstream performance degradation is logically sound.
- **Medium Confidence:** The theoretical bound (Proposition 1) correctly describes the limit behavior, but its tightness and practical impact depend on input statistics and model configuration. The effectiveness of post-attention normalization is demonstrated, but the optimal placement and parameterization may vary.
- **Low Confidence:** Whether the vanishing variance is the dominant factor in length generalization failures across diverse, real-world tasks. The synthetic setup isolates this factor but may overstate its importance relative to other failure modes (e.g., positional encoding extrapolation).

## Next Checks
1. **Variance Profiling on Natural Language Data:** Apply the same variance analysis (std dev of attention outputs vs. sequence length) to a pre-trained language model (e.g., Llama-3.2-1B) on natural text sequences. Verify if the power-law decay (σ ∝ N^(-0.5)) persists outside synthetic, i.i.d. settings.

2. **Ablation on Positional Encoding:** Repeat the synthetic task experiments with and without sinusoidal positional encodings. Determine whether the proposed post-attention normalization complements or overlaps with existing positional encoding fixes for length generalization.

3. **Alternative Normalization Schemes:** Test whether other normalization strategies (e.g., weight normalization on attention weights, or normalization after residual addition) achieve similar or superior length generalization, to isolate whether the benefit is specifically from stabilizing attention output statistics.