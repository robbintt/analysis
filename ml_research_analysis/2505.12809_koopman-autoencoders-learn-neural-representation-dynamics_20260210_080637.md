---
ver: rpa2
title: Koopman Autoencoders Learn Neural Representation Dynamics
arxiv_id: '2505.12809'
source_url: https://arxiv.org/abs/2505.12809
tags:
- neural
- representations
- koopman
- space
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Koopman autoencoders as a framework for modeling
  neural representation dynamics in deep networks. The approach treats neural representations
  as states in a dynamical system, learning a surrogate model that captures how representations
  evolve from input to output layers.
---

# Koopman Autoencoders Learn Neural Representation Dynamics

## Quick Facts
- arXiv ID: 2505.12809
- Source URL: https://arxiv.org/abs/2505.12809
- Reference count: 12
- Key outcome: Koopman autoencoders model neural representation dynamics, enabling targeted class unlearning with minimal accuracy loss on other classes

## Executive Summary
This paper introduces Koopman autoencoders (KAEs) as a framework for modeling how neural representations evolve through deep network layers. By lifting representations into a higher-dimensional observable space with linear dynamics, the method enables both analysis and targeted editing of neural representations. The approach captures the progressive topological simplification observed in neural networks and demonstrates practical application through targeted class unlearning on MNIST, where class 4 can be removed with accuracy dropping from 98.29% to 0.0% while maintaining 98.53% accuracy on other classes.

## Method Summary
The Koopman autoencoder framework learns to map neural representations between layers using an encoder that lifts states into an observable space, a linear Koopman operator that advances them, and a decoder that maps back. The model is trained with a combined loss including reconstruction, linear prediction, state prediction, and an isometry objective that preserves original topology. For practical applications like class unlearning, the linear operator can be edited directly in observable space, and the framework enables interpolation between representations that naturally replicates the topological simplification seen in trained networks.

## Key Results
- KAE surrogate models achieve 98.75% accuracy on Yin-Yang and 98.53% accuracy on MNIST classification tasks
- The learned dynamics naturally replicate progressive topological simplification, with intermediate representations showing decreasing Betti numbers
- Targeted class unlearning successfully removes class 4 from MNIST with accuracy dropping from 98.29% to 0.0% while maintaining 98.53% on other classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lifting neural representations into a higher-dimensional observable space via an autoencoder enables linear dynamics to approximate the nonlinear transformations between network layers.
- Mechanism: The encoder ϕ maps neural representations xi to observables, where a learnable linear operator K advances them. The decoder ϕ⁻¹ maps back to the state space. This is the core Koopman operator approach adapted for layer-to-layer dynamics.
- Core assumption: There exists a finite-dimensional observable space where the layer-wise transformations can be well-approximated by linear dynamics.
- Evidence anchors: Abstract states the method operates in a linear space via lifting; section 2.3 defines the Koopman operator K and lifting function ϕ; section 3.1 provides the mathematical formulation.

### Mechanism 2
- Claim: Regularizing the autoencoder with an isometry objective (L_dist) preserves the topological properties of the original neural representations in the observable space.
- Mechanism: The L_dist loss (Eq. 8) penalizes differences between inter-point distances in the original state space and the observable space. This encourages the encoder to be a topology-preserving map (isometry).
- Core assumption: The Euclidean distance metric in the state space and observable space is meaningful for capturing the relevant topological properties.
- Evidence anchors: Abstract mentions preserving topologies by regularizing the autoencoding objective; section 3.2 defines L_dist; section 4.1 demonstrates stronger isometry regularization leads to observables with topological signatures closer to original representations.

### Mechanism 3
- Claim: The linear dynamics in observable space naturally replicate the progressive topological simplification of neural representations seen in deep networks, enabling interpolation.
- Mechanism: The Koopman operator is parameterized as K = exp(G/k)^k (Eq. 14), allowing for smooth k-step transformations. The KAE, trained only on start (xi) and end (xj) representations, generates intermediate steps that show decreasing Betti numbers.
- Core assumption: The learned linear dynamics in observable space follow a trajectory that, when projected back, mimics the geometric path taken by the actual network's intermediate layers.
- Evidence anchors: Abstract states surrogate models naturally replicate progressive topological simplification; section 3.4 parameterizes K for smooth transformations; section 4.2 shows Betti numbers of KAE's intermediate outputs decreasing over iterations.

## Foundational Learning

- **Koopman Operator Theory**
  - Why needed here: Provides the method for turning nonlinear dynamics into linear ones in a transformed (observable) space, which is the entire premise of the paper's approach.
  - Quick check question: Can you explain why finding a linear operator in observable space is advantageous for analyzing or controlling a nonlinear dynamical system?

- **Autoencoders**
  - Why needed here: The autoencoder's encoder/decoder pair constitutes the mapping to and from the observable space. The "autoencoder" part also implies a reconstruction loss, which is a key component of the overall objective.
  - Quick check question: What is the purpose of the encoder and decoder in a standard autoencoder, and how is that role modified or extended in a Koopman autoencoder?

- **Algebraic Topology (Betti Numbers)**
  - Why needed here: The paper's primary validation and motivation rely on quantifying the "shape" or topology of neural representations. Betti numbers (β₀ for connected components, β₁ for holes) are the metrics used to show progressive simplification.
  - Quick check question: What do Betti numbers measure, and why is a decrease in Betti numbers interpreted as a "simplification" of a data manifold's topology?

## Architecture Onboarding

- **Component map:**
    - Preprocessed representation ˆxi → Encoder ϕ → Observable z → Linear Operator K → Observable z' → Decoder ϕ⁻¹ → Preprocessed representation ˆxj

- **Critical path:**
    1. Extract representations (xi, xj) from trained base model for a given input
    2. Preprocess xi and xj to get ˆxi and ˆxj
    3. Train the KAE on pairs (ˆxi, ˆxj) using the combined loss L_total
    4. Use the parameterized operator K to generate intermediate observables or edit K to produce modified observables, then decode them back to state space for use by the base model's classifier

- **Design tradeoffs:**
    - Observable dimension (p): Larger p may better capture dynamics but increases memory and computation (paper uses p=20 for Yin-Yang, p=800 for MNIST)
    - Isometry regularization weight (λ₄): Higher value better preserves topology but may constrain autoencoder, potentially harming reconstruction or prediction accuracy
    - Operator parameterization (k): Number of steps k for K = exp(G/k)^k determines granularity of interpolation; higher k yields more intermediate points but requires more matrix multiplications

- **Failure signatures:**
    - High reconstruction/state prediction loss: KAE fails to learn mapping between layer representations, perhaps due to insufficient model capacity or inappropriate learning rate
    - Observable representations do not cluster by class: Indicates neural collapse is not preserved, making model editing impossible
    - Interpolated representations have chaotic topology: Betti numbers of intermediate decoded states do not decrease progressively, suggesting learned dynamics are not meaningful

- **First 3 experiments:**
    1. Sanity Check - Linear Dynamics: Train KAE on representations of a simple residual network and verify high prediction accuracy on penultimate layer representations and that linear operator can be inverted
    2. Topology Preservation Ablation: Train multiple KAEs with varying isometry regularization strengths (λ₄ = 0, 10⁻³, 1) and compare Betti curves of observable representations to original model's
    3. Interpolation & Simplification: Using parameterized operator K = exp(G/k)^k with k=10, generate intermediate observables, decode them, and compute their Betti numbers to see if similar simplification trend emerges

## Open Questions the Paper Calls Out

- How can the KAE dynamics be regularized to interpolate through all intermediate neural representations of a model, rather than only between two representations? (Current approach is limited to interpolating between two neural representations)
- Does spectral analysis of the learned Koopman operator K provide mechanistic insights into the original neural network's behavior? (No spectral analysis is performed in experiments)
- Can the KAE framework enable concept unlearning in large language models and transformer architectures? (Experiments are limited to residual MLPs on image classification)
- Can the model editing approach succeed on representations that lack neural collapse? (The approach depends on neural collapse being present for reliable editing)

## Limitations

- The framework's generalizability to more complex architectures and datasets beyond simple residual MLPs on image classification remains uncertain
- The choice of observable dimension, isometry regularization strength, and operator parameterization appear to be empirical choices without clear theoretical guidance for new problems
- The method depends on neural collapse being present in representations for reliable model editing to work

## Confidence

- **High confidence:** The theoretical framework combining Koopman operator theory with autoencoders is sound, and experimental demonstrations on Yin-Yang and MNIST provide strong evidence that the approach works for these specific cases
- **Medium confidence:** The claim that linear dynamics in observable space naturally replicate progressive topological simplification is supported by Betti number analysis, but the mechanism for why this occurs and whether it generalizes is not fully explained
- **Low confidence:** The framework's scalability to deeper networks, larger datasets, and more complex topologies is uncertain, and optimal hyperparameter choices are not theoretically derived

## Next Checks

1. **Generalization Test:** Apply the Koopman autoencoder framework to a deeper network (e.g., ResNet-18) on CIFAR-10 to evaluate whether learned dynamics still show progressive topological simplification and whether targeted class editing remains effective

2. **Topology Preservation Ablation:** Systematically vary the isometry regularization weight λ₄ across multiple orders of magnitude (0, 10⁻⁴, 10⁻³, 10⁻², 1) on the Yin-Yang task and compare Betti curves of original representations, observable space representations, and decoded intermediate representations

3. **Operator Parameterization Analysis:** For interpolation experiments, vary the number of steps k used in the parameterized operator K = exp(G/k)^k (k = 2, 5, 10, 20) and analyze how smoothness and progression of topological simplification change with k