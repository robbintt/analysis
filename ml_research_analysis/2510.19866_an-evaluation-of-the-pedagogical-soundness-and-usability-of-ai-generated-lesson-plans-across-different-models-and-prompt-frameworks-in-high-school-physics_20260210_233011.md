---
ver: rpa2
title: An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson
  Plans Across Different Models and Prompt Frameworks in High-School Physics
arxiv_id: '2510.19866'
source_url: https://arxiv.org/abs/2510.19866
tags:
- lesson
- plans
- prompt
- framework
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared five AI models (ChatGPT, Claude, Gemini, DeepSeek,
  Grok) and three prompt frameworks (TAG, RACE, COSTAR) for generating high-school
  physics lesson plans on the electromagnetic spectrum. Automated metrics assessed
  readability, factual accuracy, NGSS alignment, and cognitive demand.
---

# An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics

## Quick Facts
- arXiv ID: 2510.19866
- Source URL: https://arxiv.org/abs/2510.19866
- Reference count: 0
- Five AI models (ChatGPT, Claude, Gemini, DeepSeek, Grok) and three prompt frameworks (TAG, RACE, COSTAR) were evaluated for generating high-school physics lesson plans on the electromagnetic spectrum.

## Executive Summary
This study evaluated the pedagogical soundness and usability of AI-generated lesson plans by testing five large language models and three prompt frameworks on a high-school physics topic. The research employed automated metrics to assess readability, factual accuracy, NGSS alignment, and cognitive demand. DeepSeek produced the most readable plans while Claude generated the densest text. The RACE framework showed the lowest hallucination rate and highest NGSS alignment, though all models concentrated learning objectives at lower Bloom taxonomy levels.

## Method Summary
The study generated 15 lesson plans (5 models × 3 prompt frameworks) for a 60-minute high-school physics lesson on the electromagnetic spectrum. Automated metrics evaluated each plan: readability (FKGL, FRE, FOG, TTR), hallucination index (HI = 2×major_errors + 1×minor_errors), NGSS alignment via Sentence-BERT cosine similarity to HS-PS4 standards, and cognitive demand index from Bloom's taxonomy verb classification. The evaluation pipeline used Python 3.10 with pandas, nltk, scikit-learn, and sentence-transformers, employing dependency parsing for verb extraction and embeddings for semantic similarity.

## Key Results
- DeepSeek achieved the highest readability (FKGL = 8.64) while Claude produced the densest text (FKGL = 19.89)
- RACE framework yielded the lowest hallucination index (2.00) and highest NGSS alignment (0.082)
- All models concentrated learning objectives at lower Bloom levels, predominantly "Remember" and "Understand" tiers
- The most effective configuration combined a readability-optimized model with the RACE framework plus explicit checklist of physics concepts, standards, and higher-order objectives

## Why This Works (Mechanism)
The study systematically isolates the impact of different AI models and prompt engineering techniques on pedagogical output quality. By using standardized automated metrics, it quantifies trade-offs between readability, factual accuracy, standards alignment, and cognitive demand across multiple dimensions. The controlled comparison across five models and three frameworks provides empirical evidence for optimizing lesson plan generation in educational contexts.

## Foundational Learning
- Readability metrics (FKGL, FRE, FOG, TTR): Essential for ensuring lesson plans are accessible to target student populations
  - Quick check: Calculate FKGL for a sample paragraph and verify it falls within 8-12 range for high school
- Hallucination index calculation: Critical for assessing factual accuracy in scientific content
  - Quick check: Count major and minor errors in a physics explanation and compute HI score
- NGSS alignment via sentence embeddings: Enables semantic comparison of lesson objectives to educational standards
  - Quick check: Compute cosine similarity between two sample sentences using sentence-transformers
- Bloom's taxonomy verb classification: Measures cognitive demand levels in learning objectives
  - Quick check: Extract verbs from objectives and classify according to Bloom's six levels

## Architecture Onboarding
- Component map: Prompt Generation -> Lesson Plan Generation -> Metric Evaluation Pipeline
- Critical path: Models (ChatGPT, Claude, Gemini, DeepSeek, Grok) → Frameworks (TAG, RACE, COSTAR) → Automated Metrics (Readability, Hallucination, NGSS, Cognitive)
- Design tradeoffs: Automated metrics provide scalability but may miss nuanced pedagogical quality; single topic limits generalizability but enables controlled comparison
- Failure signatures: Low NGSS similarity scores indicate misalignment with standards; high hallucination index reveals factual errors; clustered Bloom levels suggest insufficient cognitive challenge
- First experiments: 1) Generate sample lesson plans with each model-framework combination, 2) Calculate readability metrics for comparison, 3) Extract and classify learning objective verbs

## Open Questions the Paper Calls Out
- Do the observed advantages of the RACE framework and specific AI models generalize to subjects outside of high-school physics?
- Does adding an explicit checklist of higher-order verbs to prompts reliably elevate the cognitive demand of AI-generated lesson plans?
- To what extent do automated metrics like NGSS semantic similarity correlate with actual pedagogical efficacy in a live classroom?

## Limitations
- Exclusive reliance on automated metrics rather than human expert evaluation may miss nuanced pedagogical quality
- Evaluation limited to one physics topic (electromagnetic spectrum) with fixed 60-minute timeframe
- Unspecified verb taxonomy list and incomplete hallucination detection criteria transparency

## Confidence
- High confidence: Automated metric calculations (FKGL, FRE, FOG, TTR) and their implementation using standard formulas
- Medium confidence: NGSS alignment scores and hallucination index rankings between models and frameworks
- Medium confidence: Relative performance comparisons between models and frameworks
- Low confidence: Absolute metric values without human validation and real-world classroom applicability

## Next Checks
1. Replicate the study with human expert evaluation across multiple physics topics to validate automated metric rankings
2. Test the same prompt frameworks with updated model versions after scheduled knowledge cutoffs to assess stability
3. Implement the evaluation pipeline with the complete verb taxonomy list and hallucination detection criteria to verify reproducibility of reported scores