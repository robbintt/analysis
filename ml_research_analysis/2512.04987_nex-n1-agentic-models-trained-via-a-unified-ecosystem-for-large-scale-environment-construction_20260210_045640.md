---
ver: rpa2
title: 'Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment
  Construction'
arxiv_id: '2512.04987'
source_url: https://arxiv.org/abs/2512.04987
tags:
- agent
- agentic
- nex-n1
- arxiv
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents Nex-N1, an agentic model trained through a
  unified ecosystem designed for large-scale environment construction. The key innovation
  is a three-component system addressing the challenge of scaling interactive environments:
  NexAU provides a modular agent runtime enabling complex agent hierarchies via simple
  configurations; NexA4A automatically generates diverse agent architectures from
  natural language; and NexGAP bridges the simulation-reality gap by integrating real-world
  tools for grounded trajectory synthesis.'
---

# Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction

## Quick Facts
- arXiv ID: 2512.04987
- Source URL: https://arxiv.org/abs/2512.04987
- Authors: Nex-AGI Team; Yuxuan Cai; Lu Chen; Qiaoling Chen; Yuyang Ding; Liwen Fan; Wenjie Fu; Yufei Gao; Honglin Guo; Pinxue Guo; Zhenhua Han; Zhengfu He; Hanglei Hu; Kai Hu; Shengjia Hua; Tianyu Huai; Baodai Huang; Li Ji; Zhen Jiang; Zhikai Lei; Bufan Li; Jiahang Lin; Lizhi Lin; Jinxiu Liu; Shichun Liu; Ziming Liu; Yuchen Ni; Pengfang Qian; Yujiong Shen; Qingyun Shi; Wentao Shu; Peng Sun; Yiran Suo; Tian Tang; Boyu Tian; Guoteng Wang; Junzhe Wang; Peixin Wang; Zhiheng Xi; Hang Yan; Jie Yang; Zhixiong Yang; Tianchu Yao; Guangze Ye; Qianxi Yu; Shuo Zhang; Xinyue Zhang; Yiqi Zhang; Jiarong Zhao; Miao Zheng; Rui Zheng; Enyu Zhou; Jiazheng Zhou; Maosen Zhou; Yuhao Zhou; Tao Gui; Yining Zheng; Xinchi Chen; Jie Zhou; Siyuan Feng; Qin Chen; Liang He; Qi Zhang; Xuanjing Huang; Xipeng Qiu
- Reference count: 7
- Primary result: Nex-N1 achieves state-of-the-art performance on SWE-bench, τ2, GAIA 2, and BFCL benchmarks, outperforming open-source models and matching frontier proprietary models on complex agentic tasks.

## Executive Summary
Nex-N1 presents an agentic model trained through a unified ecosystem designed for large-scale environment construction. The key innovation is a three-component system addressing the challenge of scaling interactive environments: NexAU provides a modular agent runtime enabling complex agent hierarchies via simple configurations; NexA4A automatically generates diverse agent architectures from natural language; and NexGAP bridges the simulation-reality gap by integrating real-world tools for grounded trajectory synthesis. Trained on this infrastructure, Nex-N1 achieves state-of-the-art performance on benchmarks including SWE-bench, τ2, GAIA 2, and BFCL, outperforming other open-source models and matching frontier proprietary models on complex agentic tasks. The system generates over 200 diverse agent frameworks and environments, providing robust generalization across different agent architectures. The authors open-source the Nex ecosystem and model weights to accelerate research in agentic scaling.

## Method Summary
Nex-N1 is trained using a unified ecosystem consisting of three components: NexAU (agent framework), NexA4A (automatic agent generation), and NexGAP (trajectory synthesis with real MCP tools). The method involves generating diverse agent architectures from natural language specifications, executing them against real-world tools via MCP integration, and synthesizing grounded trajectories for training. The training data includes over 200 agent frameworks ranging from 1-34 nodes, 100+ curated MCP tools, and trajectories filtered for quality issues including truncation, repetition, hallucination, and reward hacking. Models are fine-tuned on normalized trajectories across multiple tool-call formats, achieving state-of-the-art performance on several agentic benchmarks.

## Key Results
- Nex-N1 achieves state-of-the-art performance on SWE-bench, τ2, GAIA 2, and BFCL benchmarks
- Outperforms other open-source models including DeepSeek-V3.1, Qwen3-32B, and InternLM3-8B
- Matches frontier proprietary models on complex agentic tasks
- Generates over 200 diverse agent frameworks with 1-34 nodes each
- Demonstrates robust generalization across different agent architectures

## Why This Works (Mechanism)

### Mechanism 1: Recursive Hierarchical Decomposition with State Isolation
Complex agentic tasks can be learned through recursive delegation where sub-agents appear as unified tools to parent agents, with isolated context windows preventing overflow during long-horizon execution. Each agent runs a localized ReAct loop; parent agents invoke sub-agents as tools, instantiating a child execution context with its own system prompt, state, and toolset. The child's reasoning traces do not pollute the parent's context window. Core assumption: The competency "conditioned on context, determine next action" transfers across different hierarchical structures and depth levels.

### Mechanism 2: Declarative-to-Executable Agent Synthesis at Scale
Diverse agent architectures can be procedurally generated from natural language specifications via declarative schemas, enabling training diversity without manual framework engineering. NexA4A takes NL descriptions → MetaAgent interprets and selects workflow pattern → decomposes into hierarchical tasks → produces declarative YAML config → NexAU runtime binds to executable logic. FrameworkBuilder coordinates multi-agent system construction (1-3 layers deep). Core assumption: The space of useful agent topologies is adequately covered by compositional generation from language specifications.

### Mechanism 3: Simulation-Reality Bridging via Real MCP Tools
Training on trajectories from real external APIs (via MCP) grounds agents in authentic latency, error modes, and state transitions, reducing tool-use hallucinations. NexGAP integrates 100+ curated production-ready MCP tools → executes agents against real servers (GitHub, databases) → captures authentic error dynamics and latency → normalizes trajectories into multiple tool-call formats (OpenAI, XML variants) for training. Core assumption: The distribution of real API behaviors during training is representative of deployment conditions.

## Foundational Learning

- Concept: ReAct Loop (Reasoning + Acting)
  - Why needed here: NexAU's entire execution model is built on the ReAct paradigm where agents alternate between internal reasoning ("Thought") and environment actions.
  - Quick check question: Can you explain why separating "Thought" from "Action" enables better error recovery compared to direct action generation?

- Concept: Model Context Protocol (MCP)
  - Why needed here: NexGAP's grounding mechanism depends on MCP integration for real-world API access and stateful tool interactions.
  - Quick check question: What distinguishes MCP from a standard REST API wrapper in terms of how agents discover and invoke capabilities?

- Concept: Inverse-Frequency Weighted Sampling
  - Why needed here: NexGAP uses this strategy to mitigate sampling bias and ensure coverage of underrepresented problem types in query synthesis.
  - Quick check question: Why would uniform sampling over a problem taxonomy fail to produce balanced training data in practice?

## Architecture Onboarding

- Component map:
  NexA4A -> NexAU -> NexGAP -> Quality Assessment Agent -> Training pipeline

- Critical path:
  1. Define agent topology in YAML (or let NexA4A synthesize from NL)
  2. NexAU instantiates runtime with MCP tool bindings
  3. NexGAP generates task queries via Problem Type Tree and information fusion
  4. Execute agents, capture raw traces
  5. Quality Assessment Agent filters truncation, hallucination, reward hacking
  6. Normalize trajectories to target format; use for training

- Design tradeoffs:
  - Declarative vs. imperative definitions: Enables synthesis but may limit expressiveness for edge-case control flow
  - State isolation vs. shared context: Prevents overflow but complicates tightly-coupled multi-agent coordination
  - Real MCP tools vs. synthetic mocks: Provides grounding but introduces non-determinism, latency, and API rate limits

- Failure signatures:
  - Reward hacking: Agents fabricating test results using nonexistent files
  - Tool verbosity: Excessive return payloads degrading context efficiency
  - Placeholder outputs: Ineffective tool designs returning mock data instead of real results

- First 3 experiments:
  1. Run a minimal NexAU agent (single ReAct loop, 2-3 tools) with a provided YAML config to verify the declarative → execution pipeline.
  2. Generate a 2-layer multi-agent framework via NexA4A from a simple NL description (e.g., "code reviewer system") and inspect the output config structure.
  3. Execute NexGAP with one MCP tool to observe grounding dynamics—capture at least one error case and verify the Quality Assessment Agent correctly flags it.

## Open Questions the Paper Calls Out
None

## Limitations
- Exact training hyperparameters (learning rate schedule, epochs, batch size, compute budget) are not provided
- Full training dataset and data generation pipeline details for NexA4A/NexGAP are only partially released
- Unknown MCP tool reliability and potential API changes over time may affect reproducibility

## Confidence
- Method reproducibility: Medium (core components open-sourced but some details missing)
- Benchmark results: High (clear reporting and comparison to baselines)
- Claims about scaling: Medium (evidence from generated environments but no explicit scaling analysis)

## Next Checks
1. Verify the open-sourced training data subset includes representative examples of all seven tool-call formats
2. Test the Quality Assessment Agent's filtering capabilities on synthetic error cases
3. Validate the format normalization pipeline by converting sample trajectories between different tool-call formats