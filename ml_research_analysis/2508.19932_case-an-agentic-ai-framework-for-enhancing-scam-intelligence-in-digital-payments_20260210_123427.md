---
ver: rpa2
title: 'CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments'
arxiv_id: '2508.19932'
source_url: https://arxiv.org/abs/2508.19932
tags:
- agent
- user
- scam
- framework
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The proliferation of digital payment platforms has created an\
  \ intelligence gap for scam detection, as social engineering scams are often orchestrated\
  \ on external platforms, leaving payment systems with insufficient signals to understand\
  \ the scam\u2019s methodology. CASE, a novel Agentic AI framework, addresses this\
  \ by proactively interviewing potential victims to elicit detailed intelligence,\
  \ which is then converted into structured data for automated and manual enforcement."
---

# CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments

## Quick Facts
- **arXiv ID:** 2508.19932
- **Source URL:** https://arxiv.org/abs/2508.19932
- **Reference count:** 27
- **Primary result:** 21% uplift in scam enforcement volume through proactive victim interviews

## Executive Summary
CASE is a dual-agent AI framework that addresses the intelligence gap in digital payment scam detection by proactively interviewing potential victims to extract detailed scam methodologies. The system uses Google's Gemini models to conduct structured conversations and convert transcripts into actionable intelligence for automated and manual enforcement. Deployed on Google Pay India, CASE achieved a 21% uplift in scam enforcement volume while maintaining 99.9% safety compliance.

## Method Summary
CASE implements a two-phase architecture: a real-time Conversational Agent interviews users about suspected scams, storing transcripts per turn, while an asynchronous Information Extractor processes these transcripts into structured JSON using schema-guided in-context learning. The framework uses Gemini 2.0 Flash for all components, with a parallel Safety Filter LLM to prevent harmful outputs. The system was evaluated through red teaming, accuracy benchmarking, and business impact measurement.

## Key Results
- 21% uplift in scam enforcement volume compared to baseline
- 99.9% safety compliance on egregious violation policies
- 75.3% success rate in eliciting scam modus operandi from users

## Why This Works (Mechanism)

### Mechanism 1
Dynamic, interview-style elicitation captures the external "Modus Operandi" (MO) of social engineering scams that transaction signals miss. The system flips the standard chatbot role from "answerer" to "investigator" by proactively asking clarifying questions based on user input to reconstruct the scam narrative that occurred off-platform.

### Mechanism 2
Decoupling real-time conversation from asynchronous data processing enables scalable, reliable enforcement signal generation. The architecture separates the "Intelligence Collection Phase" (storing transcripts) from the "Data Processing Phase" (extracting structured JSON), allowing batch processing for fault tolerance and cost management.

### Mechanism 3
Multi-layered safety filters are prerequisite to deploying agentic AI in high-stakes financial domains. A "Safety Filter LLM" runs in parallel with the "Generator LLM" to intercept policy-violating inputs or prevent the agent from making forbidden outputs.

## Foundational Learning

- **Goal-Oriented Dialogue (Information Elicitation):** Why needed here: Unlike typical chatbots that react to user queries, this agent must drive the conversation to hit specific "Success Criteria" (identifying the scam MO). Quick check question: Does the prompt architecture instruct the model to *answer* questions or *ask* them to fill information gaps?

- **Schema-Guided In-Context Learning (ICL):** Why needed here: The Information Extractor must output strict JSON. Understanding how to provide "shots" (examples) in the prompt to guide the model toward a specific schema format is critical. Quick check question: If the user mentions a new scam type not in your "golden dataset" examples, how does the model know which category to pick?

- **Responsible AI & Red Teaming:** Why needed here: Operating in financial services implies a zero-tolerance threshold for harmful advice (e.g., "send money to recover funds"). Quick check question: Have you tested the agent with adversarial inputs trying to trick it into promising a refund?

## Architecture Onboarding

- **Component map:** Frontend -> In-app support interface -> Real-time Backend (Generator LLM + Safety Filter LLM + Decision Logic) -> Transcript Store -> Async Backend (Information Extractor Agent) -> Structured Data Store -> Enforcement Systems

- **Critical path:** The Intelligence Collection Phase is user-facing and latency-sensitive. The Information Extractor is throughput-sensitive. The critical data dependency is the quality of the Transcript Store.

- **Design tradeoffs:** Prompt Engineering vs. Fine-Tuning (flexibility vs. robustness); Batch vs. Real-time Extraction (reliability/cost vs. signal timeliness).

- **Failure signatures:** Topic Drift (agent chats about general topics), Refusal Loop (overly sensitive Safety Filter), Schema Mismatch (incorrect JSON structure).

- **First 3 experiments:** 1) Red Teaming Safety validation with adversarial prompts; 2) Extractor Accuracy Benchmarking on held-out golden dataset; 3) Engagement Funnel Analysis measuring user drop-off rates.

## Open Questions the Paper Calls Out

- **Multimodal Inputs:** Does incorporating multimodal user inputs (e.g., screenshots, audio) significantly improve the accuracy of the Information Extractor compared to text-only transcripts?

- **Direct Enforcement Autonomy:** What are the safety implications and performance requirements for granting the CASE framework direct autonomy to execute enforcement actions?

- **Multilingual Generalization:** Can the current prompt-based architecture effectively generalize to non-English languages (specifically Indic languages) without requiring model fine-tuning?

## Limitations

- The framework's effectiveness depends heavily on user willingness to engage in multi-turn conversations, with no drop-off rate data provided.
- Information Extractor accuracy shows room for improvement (83.8% binary detection, 75.1% multiclass classification), particularly for novel scam types.
- Safety evaluations rely on internally curated adversarial prompt sets that may not capture all real-world attack vectors.

## Confidence

**High Confidence:** Architectural separation of real-time conversation from batch processing is technically sound; safety compliance metrics are rigorously measured with structured red teaming.

**Medium Confidence:** 21% uplift in scam enforcement volume lacks context about baseline conditions and statistical significance; Information Extractor accuracy metrics are measured but evaluation methodology is not fully specified.

**Low Confidence:** Claims about user engagement and willingness to participate in scam interviews are not empirically validated with drop-off rate data or user satisfaction metrics.

## Next Checks

1. Deploy the Safety Filter against a broader, publicly available adversarial prompt dataset to validate the 99.9% compliance claim under more diverse attack scenarios.

2. Implement funnel tracking to measure drop-off rates at each conversation turn and conduct user experience interviews to identify friction points.

3. Evaluate the Information Extractor on transcripts containing scam types not present in the golden dataset to assess its ability to handle novel MOs without additional fine-tuning.