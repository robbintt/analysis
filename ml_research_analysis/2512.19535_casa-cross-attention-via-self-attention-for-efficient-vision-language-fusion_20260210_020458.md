---
ver: rpa2
title: 'CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion'
arxiv_id: '2512.19535'
source_url: https://arxiv.org/abs/2512.19535
tags:
- casa
- image
- tokens
- text
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of token insertion
  in vision-language models (VLMs) when processing high-resolution images or long
  videos. The authors propose CASA (Cross-Attention via Self-Attention), a fusion
  mechanism that injects visual information through dedicated attention layers where
  text tokens attend to both image and text tokens in local windows.
---

# CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion

## Quick Facts
- **arXiv ID**: 2512.19535
- **Source URL**: https://arxiv.org/abs/2512.19535
- **Authors**: Moritz Böhle; Amélie Royer; Juliette Marrie; Edouard Grave; Patrick Pérez
- **Reference count**: 40
- **Primary result**: CASA achieves constant memory cost for streaming video captioning while matching token-insertion performance on VQA tasks

## Executive Summary
CASA introduces a novel cross-attention mechanism for vision-language models that injects visual information through dedicated attention layers where text tokens attend to both image and local text tokens. By preserving text-to-text interactions within cross-attention layers, CASA enables implicit gating that regulates visual information flow without explicit mechanisms. The approach achieves constant memory scaling for streaming video applications while maintaining competitive performance on general VQA tasks, though it trails token-insertion models on fine-grained visual understanding tasks like chart and document comprehension.

## Method Summary
CASA replaces or supplements standard self-attention layers with attention mechanisms where text tokens attend to concatenated image and local text tokens within windowed attention blocks. The architecture uses asymmetric query-key design where image tokens serve only as keys/values (never as queries), enabling constant-memory streaming inference by keeping images out of the KV cache. CASA can be integrated in three ways: replacing every 4th self-attention layer (CASA∨), running in parallel (CASA⊕), or before self-attention (CASA→). The method is trained from scratch or adapted from pretrained VLMs, with FlashAttention-2 blockwise attention enabling efficient windowed processing.

## Key Results
- CASA matches token-insertion performance on general VQA tasks (GQA, MME, TextVQA) while outperforming cross-attention baselines
- Achieves constant memory usage for streaming video captioning with 2-3× faster inference than token-insertion methods
- Outperforms insertion models on chart and document understanding when adapted from pretrained VLMs
- Demonstrates 6-8 point performance gaps on fine-grained visual tasks (DocVQA, ChartQA, InfoVQA) compared to full token insertion

## Why This Works (Mechanism)

### Mechanism 1: Implicit Gating via Self-Attention in Cross-Attention Layers
The inclusion of text-to-text attention within CASA layers creates natural gating that regulates visual information flow. When text tokens attend to both image tokens and themselves, the softmax distribution creates an implicit gate that controls how much visual information modifies the text embedding. The self-attention component dominates, creating this gating without requiring explicit gating functions. Evidence shows removing self-attention collapses performance dramatically (from 54.7 to 40.3 average), while attention-to-self is orders of magnitude larger than attention to image tokens.

### Mechanism 2: Local Window Attention for Scalability
Constraining attention to local windows bounded by image insertion points maintains O(T + N) scaling while preserving task-relevant context. Each CASA window contains one image plus associated text, avoiding cross-window contamination while keeping computation bounded. Global text coherence is maintained through standard self-attention layers spanning the full sequence. The design assumes visual information is most relevant to immediately adjacent text, with long-range dependencies handled separately.

### Mechanism 3: Asymmetric Query-Key Design for Inference Efficiency
Using text tokens only as queries and keeping images out of the KV cache enables constant-memory streaming inference. When new frames arrive, only cross-attention key/value buffers update while the text KV cache remains untouched, preventing memory growth with video length. This design assumes vision encoders can produce sufficiently informative representations without LLM FFN updates, though updating image embeddings through FFNs provides only modest gains (~2 points) at significant memory cost.

## Foundational Learning

- **Cross-Attention vs. Self-Attention**: Cross-attention uses queries from one sequence to attend to keys/values from another, while self-attention stays within one sequence. CASA bridges these by having text queries attend to concatenated text+image keys/values. Why needed: Understanding the distinction clarifies why standard cross-attention loses text coherence. Quick check: In standard cross-attention for VLMs, can a text token attend to previous text tokens within the cross-attention layer?

- **Token Insertion Paradigm**: The dominant VLM approach interleaves image patch embeddings directly into the text token sequence, processed identically through self-attention and FFNs. Why needed: All of CASA's design decisions position it as an alternative to insertion—the costs (quadratic attention over T+N tokens, KV cache growth) motivate the mechanism. Quick check: Why does token insertion scale poorly for video compared to cross-attention?

- **Gating in Neural Networks**: Explicit gates (e.g., $\tanh$ or sigmoid gates in Flamingo) learn to modulate information flow; implicit gates emerge from architecture dynamics without dedicated parameters. Why needed: The paper argues CASA's self-attention creates implicit gating superior to explicit approaches in prior cross-attention VLMs. Quick check: What would an explicit gating mechanism for cross-attention look like, and how does CASA avoid needing it?

## Architecture Onboarding

- **Component map**: Vision Encoder → Image Tokens → CASA Attention (text queries × [image keys; local text keys]) → Text Tokens → Self-Attention Layer → Output

- **Critical path**:
  1. Image → Vision Encoder → flatten to N tokens
  2. Text → Tokenizer → embedding → LLM embedding layer
  3. At each layer: Text embeddings → [SA layer → CASA layer] OR [SA ⊕ CASA] OR [CASA replacing SA for ∨ variant]
  4. CASA attention: text queries × [image_keys; local_text_keys] → weighted sum → residual add to text stream
  5. Output: Text-only token predictions (image tokens never appear in output)

- **Design tradeoffs**:
  - CASA ⊕ vs. → vs. ∨: ⊕ (parallel) most robust for adaptation; → (serial) comparable from-scratch; ∨ (replacement) most efficient but requires careful placement (every 4th layer optimal)
  - FFN updates to image tokens: +2 points on reading tasks but +300% memory—rarely worth it
  - Window size: Natural image boundaries work well; artificial windows would break causality

- **Failure signatures**:
  - ChartQA/DocVQA still ~7 points below insertion: Fine-grained spatial reasoning may need deeper visual-text interaction
  - InfoVQA shows largest gap: Complex diagram understanding likely requires global attention across visual elements
  - CASA ∨ with too many replacements (>every 2 blocks): Local attention insufficient for long-range reasoning

- **First 3 experiments**:
  1. Train a 2-layer transformer with CASA on a synthetic task (e.g., copy visual patterns to text) to verify attention masking and gradient flow
  2. Compare vanilla cross-attention vs. CASA on a single dataset (e.g., TextVQA) to reproduce the implicit gating benefit before full training
  3. Run inference with progressively more frames (10, 50, 100, 200) on both insertion and CASA variants to confirm constant-memory property and identify KV cache growth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the specific performance gap between CASA and full token insertion on fine-grained tasks (like InfoVQA and ChartVQA) be closed without compromising the efficiency benefits of the cross-attention architecture?
- Basis in paper: The authors state, "A performance gap to token insertion remains... The largest drops are observed on InfoVQA and ChartVQA, two datasets requiring fine-grained visual understanding of charts and diagrams."
- Why unresolved: While CASA narrows the gap, the paper acknowledges it does not fully reach the performance of insertion-based models on high-resolution document/chart understanding tasks.
- What evidence would resolve it: A modification to the CASA mechanism or training pipeline that achieves statistical parity with insertion models on ChartVQA/InfoVQA while maintaining constant memory usage in streaming scenarios.

### Open Question 2
- Question: Does the self-attention component in CASA layers function primarily as a "gating" mechanism, or does it provide necessary representational capacity for text-text interactions that standard gating approximates?
- Basis in paper: The paper states, "We hypothesize that the inclusion of text tokens' self-attention in CASA layers creates a natural gating mechanism... as opposed to standard cross-attention-based approaches."
- Why unresolved: The authors demonstrate that removing self-attention hurts performance and visualise attention scores, but the specific functional role (gating vs. representation) is inferred from the "implicit gating" behavior rather than proven definitively.
- What evidence would resolve it: Mechanistic interpretability studies comparing the activation distributions of CASA layers against explicit gated cross-attention layers to determine if they learn identical functional mappings.

### Open Question 3
- Question: To what extent can visual token compression techniques (like Q-Formers) be combined with CASA to further reduce computational load without degrading the fine-grained visual understanding that CASA aims to preserve?
- Basis in paper: The authors note, "We note that token compression is orthogonal to CASA, and both could be combined for particularly constrained memory or compute budgets."
- Why unresolved: The paper evaluates CASA and compression separately but does not experiment with combining them, leaving the interaction between CASA's local attention and compressed visual tokens unexplored.
- What evidence would resolve it: Experiments applying CASA to pre-compressed visual tokens (e.g., from a Q-Former) and evaluating the trade-off curve between FLOPs/memory and accuracy on benchmarks like OCRBench.

## Limitations

- **Performance gaps on complex visual tasks**: CASA trails token-insertion by 6-8 points on fine-grained tasks like DocVQA and ChartQA, suggesting local-window design may inherently limit cross-image reasoning
- **Streaming video evaluation scope**: Impressive streaming results on LiveSports3K don't test cross-frame visual reasoning or mixed text-video input where text might reference multiple visual elements
- **Cross-attention baseline definition**: Comparison against "naive cross-attention" baseline lacks clarity on whether it includes standard VLM optimizations or serves as a strawman for CASA's implicit gating claims

## Confidence

**High confidence**: The efficiency claims (constant memory scaling, O(T+N) complexity) are well-supported by theoretical analysis and empirical memory profiling. The mechanism of asymmetric query-key design enabling streaming inference is clearly demonstrated.

**Medium confidence**: The performance parity with token-insertion on general VQA tasks is convincing, but the performance gaps on fine-grained visual tasks suggest limitations that aren't fully explored. The implicit gating mechanism is well-supported by ablations, but the exact nature of what makes it superior to explicit gating remains somewhat hand-wavy.

**Low confidence**: The claim that CASA is "better suited for real-time streaming video captioning" is supported only by LiveSports3K results. The paper doesn't explore edge cases like mixed text-video input or tasks requiring cross-frame visual reasoning.

## Next Checks

1. **Cross-frame reasoning test**: Evaluate CASA on a video task requiring visual tracking or object persistence across frames (e.g., ActivityNet or a custom tracking dataset) to determine if local windows truly capture necessary temporal dependencies

2. **Fine-grained visual reasoning stress test**: Create a controlled experiment where text queries must attend to multiple distant visual elements simultaneously (e.g., "What's the relationship between the object in the top-left and bottom-right of the chart?") to quantify CASA's limitations on cross-image reasoning

3. **Mixed modality streaming**: Test streaming inference on input streams that interleave text and images at arbitrary points (not just text followed by images) to verify the KV cache stability claim holds under realistic multimodal conditions