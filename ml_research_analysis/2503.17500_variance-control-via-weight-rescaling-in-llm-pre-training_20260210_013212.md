---
ver: rpa2
title: Variance Control via Weight Rescaling in LLM Pre-training
arxiv_id: '2503.17500'
source_url: https://arxiv.org/abs/2503.17500
tags:
- standard
- deviation
- init
- weight
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Variance Control via Weight Rescaling in LLM Pre-training This
  work addresses the challenge of managing weight variance during Large Language Model
  (LLM) pre-training, where weights tend to diverge from their initial scale, undermining
  training stability and downstream performance. The authors introduce two techniques:
  Layer Index Rescaling (LIR) for initialization and Target Variance Rescaling (TVR)
  for variance control during training.'
---

# Variance Control via Weight Rescaling in LLM Pre-training

## Quick Facts
- arXiv ID: 2503.17500
- Source URL: https://arxiv.org/abs/2503.17500
- Reference count: 40
- Primary result: Introduces Layer Index Rescaling (LIR) and Target Variance Rescaling (TVR) to control weight variance during LLM pre-training, improving downstream task performance by up to 4.6%

## Executive Summary
This work addresses the challenge of managing weight variance during Large Language Model (LLM) pre-training, where weights tend to diverge from their initial scale, undermining training stability and downstream performance. The authors introduce two techniques: Layer Index Rescaling (LIR) for initialization and Target Variance Rescaling (TVR) for variance control during training. LIR scales weights by the inverse square root of the layer index, while TVR adjusts weights to maintain target standard deviation values during training. Experiments on a 1B parameter LLaMA model show that these methods improve downstream task performance by up to 4.6% on common benchmarks (e.g., HellaSwag, PIQA, ARC-Challenge) and reduce extreme activation values, mitigating risks associated with quantization and low-precision training.

## Method Summary
The authors propose two variance control techniques for LLM pre-training. Layer Index Rescaling (LIR) modifies weight initialization by scaling weights inversely proportional to the square root of their layer index, helping maintain appropriate variance across layers from the start. Target Variance Rescaling (TVR) monitors weight statistics during training and periodically rescales weights to maintain target standard deviations, preventing runaway variance growth. These methods work together to keep weights closer to their initial scale throughout training, improving stability and downstream performance. The approach is evaluated on a 1B parameter LLaMA model, demonstrating improved performance on multiple benchmarks and reduced extreme activation values.

## Key Results
- Improved downstream task performance by up to 4.6% on benchmarks including HellaSwag, PIQA, and ARC-Challenge
- Reduced extreme activation values, mitigating risks for quantization and low-precision training
- Enhanced training stability and throughput demonstrated on 1B parameter LLaMA model

## Why This Works (Mechanism)
The proposed methods work by maintaining tighter control over weight variance throughout the training process. LIR ensures that weights start with appropriate relative scales across different layers, with deeper layers receiving smaller initial weights. This initialization scheme prevents the compounding of variance that typically occurs in deep networks. TVR then actively monitors and corrects weight statistics during training, preventing the gradual drift of weight magnitudes that can destabilize training and degrade downstream performance. By keeping weights closer to their intended scale, the methods reduce the occurrence of extreme activations that can cause numerical instability and poor generalization.

## Foundational Learning

**Weight variance dynamics**: Understanding how weight variances evolve during training is crucial for identifying why models drift from their initial scales. Quick check: Monitor weight statistics throughout training to identify variance growth patterns.

**Initialization sensitivity**: Different initialization schemes affect how quickly and in what ways weight variances change during training. Quick check: Compare training trajectories under different initialization methods.

**Activation clipping**: Methods to prevent extreme activations that can destabilize training and hurt downstream performance. Quick check: Measure activation distributions and identify outlier thresholds.

## Architecture Onboarding

**Component map**: Input -> Embedding -> Transformer layers (with LIR initialization) -> TVR monitoring -> Output

**Critical path**: The forward pass through transformer layers with variance monitoring and potential rescaling at each layer forms the critical computational path.

**Design tradeoffs**: The TVR method introduces computational overhead during training but provides benefits in stability and downstream performance. The frequency of rescaling operations represents a key hyperparameter tradeoff between computational cost and variance control effectiveness.

**Failure signatures**: Loss spikes, NaN values, or degraded downstream performance can indicate variance control failures. Monitoring weight statistics and activation distributions helps identify these issues early.

**First experiments**: 1) Train with only LIR initialization to isolate initialization benefits, 2) Apply TVR to a pre-trained model to test post-hoc variance correction, 3) Vary the rescaling frequency in TVR to find optimal computational/accuracy tradeoff

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation limited to 1B parameter LLaMA model, raising questions about scalability to larger models
- Mechanism behind inverse square root layer indexing not theoretically explained, remaining empirical
- Computational overhead of TVR during training not quantified, potentially significant for large-scale training

## Confidence

High confidence: The experimental results demonstrating performance improvements on the tested benchmarks and the reduction in extreme activation values are well-supported by the presented data.

Medium confidence: The claim about improved training stability and throughput is supported but lacks detailed analysis of the trade-offs involved, particularly regarding computational overhead.

Low confidence: The assertion that these methods will scale effectively to much larger models and different architectures is not substantiated with experiments beyond the 1B parameter LLaMA model.

## Next Checks

1. Scale experiments to 7B and 70B parameter models to verify if the variance control benefits persist at larger scales

2. Test the methods on different transformer architectures (e.g., GPT-style, hybrid MoE models) to assess generalizability

3. Conduct ablation studies to quantify the computational overhead of TVR and determine if the performance gains justify the additional training cost