---
ver: rpa2
title: 'AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents'
arxiv_id: '2507.14897'
source_url: https://arxiv.org/abs/2507.14897
tags:
- reward
- action
- tool
- environment
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces AgentFly, a scalable and extensible reinforcement
  learning framework for training language model agents. The framework addresses the
  challenge of multi-turn interactions by adapting traditional RL methods with token-level
  masking, enabling efficient learning from agent-generated responses while filtering
  out environment tokens.
---

# AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents

## Quick Facts
- arXiv ID: 2507.14897
- Source URL: https://arxiv.org/abs/2507.14897
- Reference count: 35
- The framework achieves stable RL training across 6 diverse tasks with 7B models outperforming 3B models, demonstrating effective token-level masking and asynchronous execution.

## Executive Summary
AgentFly is a reinforcement learning framework designed to train language model agents in multi-turn tool-calling environments. It introduces token-level masking to isolate agent-generated tokens from environment observations during policy updates, enabling effective credit assignment over long trajectories. The framework features a decorator-based interface for tools and rewards, supporting both stateful and non-stateful interactions. AgentFly scales training through asynchronous execution and centralized resource management, achieving stable performance across six tasks using PPO, GRPO, REINFORCE++, and RLOO algorithms.

## Method Summary
AgentFly builds on the Verl framework to train LM agents through token-level masking and asynchronous execution. The framework masks environment tokens (Mt=0) during loss computation while preserving agent-generated tokens (Mt=1), preventing policy updates from being influenced by uncontrollable observations. Tools and rewards are defined via decorators, with stateful tools requiring environment pools for isolation. The system supports PPO, GRPO, REINFORCE++, and RLOO algorithms, using Qwen2.5-Instruct 3B/7B models with learning rate 5×10⁻⁷. Training runs asynchronously across 16 chains per query with max_turns of 4 or 8, managing environment instances through centralized resource pools.

## Key Results
- 7B models consistently outperform 3B models across all six tasks, with ALFWorld showing particular scalability benefits
- Short-turn settings (4 turns) demonstrate more stable gradient norms and rewards compared to long-turn settings (8 turns)
- AgentFly successfully trains agents using all four RL algorithms (PPO, GRPO, REINFORCE++, RLOO) with GRPO showing strong performance
- Hallucination rates decrease over training, indicating effective learning of tool consistency and environment interaction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token-level masking enables RL credit assignment over multi-turn agent trajectories by isolating model-generated tokens from environment-returned tokens.
- Mechanism: During loss computation, the framework applies a binary mask Mt=1 for tokens belonging to agent responses and Mt=0 for observation tokens from tools/environment. This prevents the model from being penalized for tokens it did not generate.
- Core assumption: Environment observations are not under the agent's direct control and should not contribute to policy gradient updates.
- Evidence anchors:
  - [abstract]: "adapting traditional RL methods with token-level masking"
  - [Section 2.2]: Defines Mt = 1 if at ∈ {r1, r2, . . . , rk}, otherwise 0; masks also applied to advantage Â
  - [corpus]: Weak direct corpus support—related Agent-RL papers (RAGEN, L0) discuss multi-turn but do not explicitly analyze masking effects.
- Break condition: If observation tokens carry implicit signal about response quality (e.g., error messages indicating bad actions), zero-masking could discard useful learning signal.

### Mechanism 2
- Claim: Asynchronous execution of tool calls and reward computations with pooled environment instances enables high-throughput rollouts without blocking generation.
- Mechanism: A centralized resource manager maintains pools of pre-initialized environment instances. When a stateful tool is invoked with a new chain ID, an instance is allocated and bound; on chain completion, it is reset and returned. Tool and reward calls execute concurrently across chains via async handlers.
- Core assumption: Tool execution latency is non-trivial and benefits from parallelism; environments can be safely reset without cross-contamination.
- Evidence anchors:
  - [abstract]: "asynchronous execution of tool calls and reward computations"
  - [Section 2.4]: Describes environment pool allocation, binding by ID, and recycling
  - [Section 2.3]: Distinguishes stateful tools requiring isolation from stateless tools
  - [corpus]: MLE-Dojo (FMR=0.587) and EnvScaler (corpus) similarly emphasize scalable environment orchestration for agent RL.
- Break condition: If environment state is not fully isolatable or reset introduces residual artifacts, cross-chain contamination could corrupt learning.

### Mechanism 3
- Claim: Decorator-based interface for tools and rewards reduces integration friction, enabling rapid iteration on agent capabilities without modifying core training infrastructure.
- Mechanism: Users annotate Python functions with @tool or @reward decorators specifying metadata. The framework extracts schemas for prompt injection and routes calls appropriately. This decouples agent workflow definition from RL training logic.
- Core assumption: Users can express tools/rewards as pure functions or async coroutines; complexity of environment binding is abstracted by the decorator.
- Evidence anchors:
  - [abstract]: "decorator-based interface for defining tools and reward functions"
  - [Section 2.3, Figure 2]: Shows @tool decorator examples for stateful and non-stateful tools
  - [Appendix A, Figure 7]: Shows @reward decorator examples with/without environment binding
  - [corpus]: AgentScope 1.0 (FMR=0.645) also emphasizes developer-centric abstractions for agentic applications.
- Break condition: If tools require complex lifecycle hooks or state transitions beyond simple step/reset, the decorator abstraction may leak implementation details.

## Foundational Learning

- Concept: PPO / Policy Gradient Fundamentals
  - Why needed here: The framework builds on Verl and implements PPO, REINFORCE++, GRPO, and RLOO. Understanding advantage estimation, clipping, and token-level loss is required to interpret training dynamics.
  - Quick check question: Can you explain why masking the advantage Â alongside the loss matters for multi-token sequence optimization?

- Concept: Multi-turn Agent Interaction Patterns
  - Why needed here: AgentFly trajectories consist of alternating responses and observations. You must understand how credit flows across turns and why masking is necessary.
  - Quick check question: In a 4-turn trajectory, what happens to gradient signal if observation tokens are not masked?

- Concept: Async / Concurrent Programming in Python
  - Why needed here: Tool calls and reward computations use async handlers; environment pools rely on non-blocking allocation. Familiarity with asyncio and resource pooling is assumed.
  - Quick check question: How would you implement a bounded pool with timeout-based eviction for stateful environments?

## Architecture Onboarding

- Component map:
  RL Training Core (Verl-based) -> Agent Rollout Module -> Tool System -> Environment Resource Manager -> Reward System

- Critical path:
  1. Define tools and rewards using decorators (schema extraction is automatic)
  2. Initialize environment pools with specified pool_size per stateful tool
  3. Rollout generates k × n chains asynchronously; each chain binds environment instances by ID
  4. Trajectories collected with token-level masks applied
  5. RL core computes masked advantages and PPO-style loss
  6. Gradients update policy; environments reset and returned to pools

- Design tradeoffs:
  - Short vs long turns: 4-turn settings show more stable gradients than 8-turn (Figure 6), but may underutilize multi-step reasoning
  - Stateful vs non-stateful tools: Stateful enables complex environments (OS, browser) but requires pool management; non-stateful is simpler but limited
  - On-policy vs offline: Framework focuses on high-throughput online rollouts; offline alternatives reduce rollout cost but sacrifice adaptability

- Failure signatures:
  - Gradient instability with long trajectories: Figure 6 shows higher grad norm variance for 8-turn vs 4-turn
  - Hallucination early in training: Figure 5 shows high initial hallucination rate, decreasing as model learns tool consistency
  - Resource exhaustion: If pool_size is underspecified relative to parallel chains, requests queue and throughput drops
  - REINFORCE++ underperformance: Section 3.1 hypothesizes token-level advantage masking may hinder learning vs PPO/GRPO/RLOO

- First 3 experiments:
  1. Reproduce Figure 3 (code interpreter + math tasks) with PPO vs GRPO on Qwen2.5-3B; log reward curves and grad norms
  2. Ablate pool_size for a stateful tool (e.g., code interpreter from 2 to 16 instances); measure rollout throughput and queue wait time
  3. Compare 4-turn vs 8-turn max lengths on a retrieval task (HotpotQA); track reward stability and hallucination rate as in Figure 5

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does REINFORCE++ underperform other RL algorithms (PPO, GRPO, RLOO) in the multi-turn AgentFly setup?
- Basis in paper: [explicit] Section 3.1 states: "different from previous observations... we find that REINFORCE++ shows slightly lower performance." The authors hypothesize this is due to masking applied to the advantage function causing discrete token-level rewards.
- Why unresolved: The paper offers a hypothesis regarding advantage masking but does not validate it through ablation studies or provide a theoretical guarantee.
- What evidence would resolve it: An ablation study comparing REINFORCE++ performance with and without advantage masking, or an analysis of gradient variance under the specific masking scheme used by AgentFly.

### Open Question 2
- Question: How can the gradient instability observed in long-turn agent trajectories be effectively mitigated?
- Basis in paper: [explicit] Section 4.2 demonstrates that "rewards and gradient norm for the short-turn setting are more stable than the long-turn setting," noting that updates for agents with more turns are unstable.
- Why unresolved: While the paper identifies the correlation between turn count and instability (gradient norm fluctuations), it does not propose or test mechanisms to stabilize training for longer horizons.
- What evidence would resolve it: Experiments applying stabilization techniques (e.g., layer normalization, gradient clipping, or trust region methods specifically tuned for long horizons) to the 8-turn setting, resulting in smoother gradient norms and consistent reward improvements.

### Open Question 3
- Question: Is the failure of smaller models (3B) on complex tasks like ALFWorld due to capacity limitations or the specific implementation of token-level masking?
- Basis in paper: [explicit] Section 3.2 notes that for ALFWorld, "we find it is too difficult for the 3B model, and the reward keeps around zero," while the 7B model succeeds.
- Why unresolved: It is unclear if the 3B model's failure is an inherent lack of reasoning capability or if the token-level masking strategy (which filters environment tokens) removes necessary context that smaller models rely on more heavily than larger models.
- What evidence would resolve it: A comparison of 3B model performance on ALFWorld using AgentFly's masking versus a baseline that retains environment tokens (increasing context length), or an analysis of attention heads to see if critical environmental state is being ignored.

## Limitations
- The framework shows gradient instability with long-turn trajectories (8 turns vs 4 turns), though the underlying cause remains unclear
- REINFORCE++ consistently underperforms other RL algorithms, with the paper only offering speculative explanations about masking effects
- Smaller models (3B) fail on complex tasks like ALFWorld while 7B models succeed, raising questions about whether this is capacity limitation or masking strategy issues

## Confidence
- High Confidence: Token-level masking mechanism and decorator-based tool/reward interface are well-specified with clear implementation details
- Medium Confidence: Asynchronous execution benefits and resource management claims are supported by architectural description but lack quantitative throughput measurements
- Low Confidence: The speculation about REINFORCE++ underperformance due to masking effects lacks direct experimental evidence

## Next Checks
1. **Ablation Study on Masking Effects**: Run identical training with token-level masking enabled vs disabled for REINFORCE++ and PPO on the Code Interpreter task. Measure reward convergence speed and final performance to test the hypothesis about masking hindering learning.

2. **Long-Turn Stability Analysis**: Train the same agent on HotpotQA with 4-turn vs 8-turn maximum lengths while monitoring gradient norm variance, reward stability, and hallucination rates. Determine if the instability is algorithmic (masking/advantage estimation) or architectural (resource contention in pools).

3. **Throughput Benchmarking**: Measure end-to-end rollout latency with varying pool_size configurations (2, 8, 16 instances) for stateful tools on the Code Interpreter task. Correlate throughput measurements with training stability to quantify the claimed benefits of asynchronous execution.