---
ver: rpa2
title: 'When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language
  Models'
arxiv_id: '2507.13868'
source_url: https://arxiv.org/abs/2507.13868
tags:
- factual
- heads
- counterfactual
- knowledge
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how vision-language models resolve conflicts
  between visual input and internal factual knowledge. The authors construct WHOOPS-AHA!,
  a dataset of multimodal counterfactual queries designed to provoke such conflicts,
  pairing unusual images with prompts that encourage commonsense responses.
---

# When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models

## Quick Facts
- arXiv ID: 2507.13868
- Source URL: https://arxiv.org/abs/2507.13868
- Authors: Francesco Ortu; Zhijing Jin; Diego Doimo; Alberto Cazzaniga
- Reference count: 34
- Key outcome: Targeted interventions on a small set of attention heads can shift vision-language models between factual knowledge and visual context, improving factual accuracy from 22% to 74% in LLaVA-NeXT.

## Executive Summary
This paper investigates how vision-language models resolve conflicts between visual input and internal factual knowledge. The authors construct WHOOPS-AHA!, a dataset of multimodal counterfactual queries designed to provoke such conflicts, pairing unusual images with prompts that encourage commonsense responses. Using logit inspection, they identify a small set of attention heads that mediate these conflicts, with counterfactual heads attending more to image tokens. Targeted interventions on these heads can shift model predictions toward either factual knowledge or visual context. Attention-based attribution from conflict-resolution heads more precisely identifies image regions driving counterfactual predictions than gradient-based methods.

## Method Summary
The authors use Logit Lens to project intermediate hidden states to vocabulary space, identifying attention heads that consistently favor either factual or counterfactual tokens. They then apply targeted multiplicative interventions to these heads' attention weights during inference, scaling image token attention for counterfactual heads and text token attention for factual heads. The WHOOPS-AHA! dataset provides controlled test cases where images contain anomalies that conflict with commonsense knowledge, allowing measurement of intervention effectiveness through factual accuracy.

## Key Results
- A small subset of attention heads (20 factual, 20 counterfactual) mediates visual-factual conflicts in VLMs
- Counterfactual heads attend significantly more to image tokens (60%) than factual heads (28%)
- Targeted interventions shift factual accuracy from 22% to 74% in LLaVA-NeXT
- Attention-based attribution more precisely identifies conflict-driving image regions than gradient methods

## Why This Works (Mechanism)

### Mechanism 1: Specialized Attention Head Localization
The paper identifies that conflict resolution is mediated by a small subset of attention heads rather than the entire network. Logit Lens analysis reveals heads in final layers that disproportionately increase logits for factual or counterfactual tokens, suggesting specialized reasoning modules.

### Mechanism 2: Modality-Biased Attention Routing
Counterfactual heads function as direct conduits for visual override, allocating ~60% of attention to image tokens at the final generation step versus ~28% for factual heads. This allows visual data to override internal text-based priors without intermediate diffusion.

### Mechanism 3: Functional Separation of Blocks
VLMs exhibit functional division where attention blocks in later layers push predictions toward visual context while MLP blocks simultaneously push toward factual knowledge, creating a conflict zone in the residual stream.

## Foundational Learning

- **Logit Lens / Interpretable Activation Inspection**
  - Why needed here: The entire localization strategy depends on projecting intermediate hidden states to vocabulary space to see which tokens are "winning" at each step
  - Quick check question: Can you explain how projecting a hidden state $x_l$ using the unembedding matrix $W_U$ helps identify which token the model is "thinking of" at layer $l$?

- **Residual Stream Decomposition**
  - Why needed here: Understanding that the output is a sum of contributions ($x^l = x^{l-1} + a^l + m^l$) is critical to isolating whether an attention head or an MLP is responsible for a specific prediction shift
  - Quick check question: If intervening on an attention head changes the output, does it imply the MLP had no role in the final decision?

- **Targeted Attention Interventions**
  - Why needed here: The paper moves beyond correlation to causation by modifying attention weights during inference to steer the model
  - Quick check question: Why is the intervention applied multiplicatively to the attention weights after the softmax, specifically at the last token position?

## Architecture Onboarding

- **Component map:** Vision Encoder -> Projector/Adapter -> LLM Backbone (Attention + MLP blocks) -> Conflict Resolution Heads (final layers)
- **Critical path:** Input (Image + Text Prompt) → Early Layers (Processing) → Late Layers (Conflict Zone: Attention heads push counterfactual, MLPs push factual) → Output (Dominant signal determines final token)
- **Design tradeoffs:** Attention intervention vs. gradient attribution (more precise for finding conflict regions), precision vs. stability (aggressive intervention risks text coherence degradation)
- **Failure signatures:** Visual Hallucination Override (ignores commonsense when counterfactual heads dominate), Knowledge Blindness (ignores image when factual heads over-boosted)
- **First 3 experiments:** 1) Dataset Validation (confirm visual override phenomenon exists), 2) Head Localization (implement Logit Lens to identify specific heads), 3) Intervention Ablation (apply multiplicative intervention and measure factual accuracy shift)

## Open Questions the Paper Calls Out

- Do conflict-resolution attention heads generalize across diverse VLM architectures, or are they specific to LLaVA-NeXT and Gemma3 models studied?
- What is the precise information flow path for counterfactual visual signals—do they reach the output token only directly, or through intermediate token positions and layers?
- How do MLP layers and visual encoders contribute to multimodal conflict resolution beyond the attention-head mechanisms identified?
- Do these conflict-resolution mechanisms operate similarly in naturalistic settings where visual-textual conflicts are less cleanly delineated than in the curated WHOOPS-AHA! dataset?

## Limitations

- The study focuses on only two architectures (LLaVA-NeXT and Gemma3-12b), leaving architectural generalization unexplored
- The WHOOPS-AHA! dataset construction introduces potential selection bias through GPT-4o filtering that may skew toward visual override scenarios
- The causal chain from attention modification to final prediction is not fully established, as compensatory mechanisms in later layers could attenuate intervention effects

## Confidence

**High confidence:** The core finding that a small subset of attention heads mediates visual-factual conflicts is well-supported by consistent logit inspection across layers and models.

**Medium confidence:** The functional separation between attention and MLP blocks in conflict resolution is supported but less definitive, as additive decomposition could mask complex interactions.

**Low confidence:** The precise attention-attribution method for identifying image regions needs further validation, as the comparison with gradient-based methods is suggestive but not comprehensive.

## Next Checks

1. Apply the Logit Lens localization and intervention protocol to at least two additional VLM architectures (e.g., Qwen-VL, InternVL) to test whether the small-head specialization pattern generalizes.

2. Conduct ablation studies where image patches identified by counterfactual heads are removed before the intervention to verify whether the intervention's effect is additive to or dependent on these specific visual features.

3. Track attention weight distributions across multiple decoding steps (not just the final token) to determine whether the observed modality bias in counterfactual heads is consistent throughout generation or emerges specifically at decision boundaries.