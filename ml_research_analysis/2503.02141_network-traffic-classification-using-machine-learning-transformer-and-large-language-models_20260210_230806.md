---
ver: rpa2
title: Network Traffic Classification Using Machine Learning, Transformer, and Large
  Language Models
arxiv_id: '2503.02141'
source_url: https://arxiv.org/abs/2503.02141
tags:
- traffic
- classification
- learning
- accuracy
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses network traffic classification using multiple
  models including machine learning, transformer, and large language models. A comprehensive
  dataset of 30,959 observations with 19 features was collected from Arbor Edge Defender
  devices.
---

# Network Traffic Classification Using Machine Learning, Transformer, and Large Language Models

## Quick Facts
- arXiv ID: 2503.02141
- Source URL: https://arxiv.org/abs/2503.02141
- Reference count: 28
- Primary result: Transformer and XGBoost achieved highest accuracy at 98.95% and 97.56% respectively

## Executive Summary
This study evaluates multiple approaches for network traffic classification including traditional machine learning, transformer models, and large language models. The research collected a comprehensive dataset of 30,959 observations with 19 features from Arbor Edge Defender devices. Various models were tested including Naive Bayes, Decision Tree, Random Forest, Gradient Boosting, XGBoost, Deep Neural Networks, Transformer, and two Large Language Models (GPT-4o and Gemini) using both zero- and few-shot learning approaches. The results demonstrate that transformer models and XGBoost achieve the highest accuracy rates, while LLMs show promising results with few-shot learning but still face challenges with complex categories like IPSec and Backup.

## Method Summary
The study employed a comprehensive evaluation framework testing multiple classification approaches on network traffic data. The dataset comprised 30,959 observations with 19 features collected from Arbor Edge Defender devices. Traditional machine learning models (Naive Bayes, Decision Tree, Random Forest, Gradient Boosting, XGBoost) were compared against deep learning approaches (Deep Neural Networks) and transformer models. Large Language Models including GPT-4o and Gemini were tested using both zero-shot and few-shot learning paradigms. Model performance was evaluated across multiple network traffic categories with particular attention to classification accuracy and handling of complex traffic types.

## Key Results
- Transformer and XGBoost achieved the highest accuracy of 98.95% and 97.56% respectively
- LLMs showed significant improvement with few-shot learning compared to zero-shot performance
- Both LLMs and traditional models struggled with complex categories like IPSec and Backup

## Why This Works (Mechanism)
The high performance of transformer and XGBoost models stems from their ability to effectively capture complex patterns in network traffic data. Transformers excel at learning sequential dependencies and relationships between different traffic features, while XGBoost's gradient boosting framework optimally combines multiple weak learners to handle diverse traffic patterns. The success of few-shot learning with LLMs demonstrates their capacity to leverage contextual understanding when provided with relevant examples, though their performance still lags behind specialized models for highly technical classification tasks.

## Foundational Learning
- Network traffic classification: Essential for identifying and managing different types of network communications; quick check: can you distinguish between encrypted and unencrypted traffic patterns?
- Machine learning model selection: Different models excel at different classification tasks; quick check: do you understand when to use ensemble methods versus single models?
- Zero-shot vs few-shot learning: Understanding how models perform with and without training examples; quick check: can you explain the difference in LLM performance between these approaches?
- Feature engineering for network data: The importance of selecting relevant features from raw network traffic; quick check: can you identify key features that distinguish different traffic types?
- Model evaluation metrics: Understanding accuracy, precision, and recall in classification contexts; quick check: do you know which metric is most important for your specific use case?

## Architecture Onboarding
Component map: Network traffic data -> Feature extraction -> Model training -> Classification output
Critical path: Data collection -> Preprocessing -> Model training -> Validation -> Deployment
Design tradeoffs: Model complexity vs accuracy, computational cost vs real-time performance, generalization vs specialization
Failure signatures: Poor performance on complex categories, overfitting to specific traffic patterns, high computational resource requirements
First experiments: 1) Test model performance on single traffic category, 2) Evaluate cross-validation performance, 3) Measure inference time for real-time deployment

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Dataset limited to single source (Arbor Edge Defender devices), potentially limiting generalizability
- LLMs still struggle with complex categories like IPSec and Backup despite few-shot learning improvements
- No cross-validation details or external dataset testing provided to assess model robustness

## Confidence
- High confidence in Transformer and XGBoost performance metrics (98.95% and 97.56% accuracy)
- Medium confidence in LLM practical utility due to struggles with complex categories
- Low confidence in generalizability without cross-validation or external testing

## Next Checks
1. Test models on multiple independent network traffic datasets from different organizational sources to verify generalizability
2. Conduct detailed cross-validation and hyperparameter sensitivity analysis to establish confidence intervals for reported accuracies
3. Evaluate computational cost and inference time for each model in real-time classification scenarios to assess practical deployment feasibility