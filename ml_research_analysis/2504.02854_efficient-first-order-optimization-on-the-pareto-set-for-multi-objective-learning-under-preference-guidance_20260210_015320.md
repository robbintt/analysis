---
ver: rpa2
title: Efficient First-Order Optimization on the Pareto Set for Multi-Objective Learning
  under Preference Guidance
arxiv_id: '2504.02854'
source_url: https://arxiv.org/abs/2504.02854
tags:
- then
- optimization
- function
- problem
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of multi-objective learning under
  user-specified preference, framing it as a semivectorial bilevel optimization problem.
  The core idea is to convert the multi-objective constraints into a single-objective
  constraint using a smoothed merit function with easy-to-evaluate gradients, and
  then reformulate the bilevel optimization as a penalty-based single-level problem.
---

# Efficient First-Order Optimization on the Pareto Set for Multi-Objective Learning under Preference Guidance

## Quick Facts
- arXiv ID: 2504.02854
- Source URL: https://arxiv.org/abs/2504.02854
- Reference count: 40
- Primary result: Proposes an efficient first-order algorithm for multi-objective learning under user-specified preference, with convergence guarantees and experiments showing effectiveness on synthetic and real-world datasets

## Executive Summary
This paper addresses multi-objective learning under user-specified preference by formulating it as a semivectorial bilevel optimization problem. The core innovation is converting the multi-objective constraints into a single-objective constraint using a smoothed merit function with easy-to-evaluate gradients. The method reformulates the bilevel optimization as a penalty-based single-level problem and proposes an efficient first-order algorithm (FOOPS) with convergence guarantees. Experiments demonstrate its effectiveness in finding preference-guided optimal solutions to the multi-objective problem.

## Method Summary
The method tackles multi-objective learning under preference by framing it as semivectorial bilevel optimization, where the upper level optimizes a preference function and the lower level enforces Pareto optimality. The key innovation is a smoothed merit function that approximates the non-smooth Pareto constraint using Log-Sum-Exp smoothing and proximal regularization, enabling gradient-based optimization without second-order derivatives. This reformulation converts the bilevel problem into a penalty-based single-level optimization, where violations of the Pareto constraint are penalized. The FOOPS algorithm alternates between finding an inner solution (approximating the smoothed merit function) and updating the main parameters, avoiding expensive Hessian computations while maintaining convergence guarantees.

## Key Results
- The smoothed merit function preserves $\epsilon$-weak Pareto optimality and has computable gradients
- Theoretical guarantees establish relationships between solutions of the penalty reformulation and the original problem
- Experiments on synthetic and real-world datasets demonstrate effectiveness in finding preference-guided optimal solutions
- The method improves Hypervolume compared to baselines like EPO or FERERO on multi-task benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Smoothed Merit Function Relaxation
The framework enables first-order optimization on the Pareto set by approximating the non-smooth multi-objective constraint with a differentiable scalar function. It replaces the traditional max-min merit function with a smoothed version $v_{l,\tau}(x)$ using Log-Sum-Exp (LSE) to smooth the max-min operation and a proximal term ($l\|x-y\|^2$) to ensure strong convexity and unique solutions in the inner minimization. This allows using Danskin's theorem to compute gradients without second-order derivatives. The core assumption is that objective functions $f_m$ are twice continuously differentiable, locally Lipschitz, and $\mu$-weakly convex. If the smoothing parameter $\tau$ or regularization $l$ is set too high, $v_{l,\tau}$ deviates significantly from the true Pareto constraint, potentially changing the solution landscape.

### Mechanism 2: Penalty-Based Single-Level Reformulation
The method simplifies the bilevel optimization structure into a tractable single-level problem by penalizing violations of the Pareto constraint. It formulates the problem as minimizing the upper-level preference $f_0(x)$ plus a penalty term $\gamma p(x)$, where $p(x)$ measures the violation of the smoothed merit function. Under the HÖlderian Error Bound (HEB) property, increasing the penalty parameter $\gamma$ drives the solution toward the feasible Pareto set while optimizing the preference. The core assumption is that objectives are subanalytic, which implies the error bound property holds. If the penalty parameter $\gamma$ is not adapted correctly relative to the error bound exponent $\eta_p$, the solution may not converge to a KKT point of the original problem.

### Mechanism 3: Alternating First-Order Descent (FOOPS)
The FOOPS algorithm efficiently finds stationary points by decoupling the optimization into an inner approximation of the smoothed merit function and an outer update for the preference objective. The inner loop performs projected gradient descent to find $y^*_{l,\tau}(x)$, required to compute the gradient of the merit function. The outer loop updates the model parameters $x$ using the gradient of the penalized objective. This avoids expensive second-order Hessian computations found in prior bilevel methods. The core assumptions are smoothness and bounded trajectories, with regularization $l > \ell_{f,1}$. If the inner loop iterations $K$ are insufficient to approximate $y^*$, or if the regularization parameter $l$ does not exceed the smoothness constant $\ell_{f,1}$, the theoretical contraction guarantees fail.

## Foundational Learning

- **Concept: Bilevel Optimization**
  - **Why needed here:** The paper frames preference-guided learning as a "semivectorial bilevel" problem, where the lower level enforces Pareto optimality and the upper level optimizes the preference.
  - **Quick check question:** Can you distinguish between the upper-level variable (preference weights/parameters) and the lower-level constraint (Pareto set membership) in the formulation $\min f_0(x) \text{ s.t. } x \in \text{argmin} F(x)$?

- **Concept: Pareto Optimality & Weak Dominance**
  - **Why needed here:** The core constraint of the method is finding a "Weak Pareto Optimal" solution. The merit function $v_{l,\tau}$ is designed to equal zero if and only if this condition is met.
  - **Quick check question:** Why does a "Pareto stationary" point not necessarily imply global Pareto optimality for non-convex functions?

- **Concept: Kurdyka-Lojasiewicz (KL) Inequality & Subanalyticity**
  - **Why needed here:** These theoretical tools are essential for proving that the merit function satisfies the HÖlderian Error Bound, which guarantees that minimizing the penalty term actually drives the solution toward the Pareto set.
  - **Quick check question:** Why is the "subanalyticity" assumption crucial for establishing the error bound without requiring strict convexity?

## Architecture Onboarding

- **Component map:** Multi-objective functions $F(x)$ and preference function $f_0(x)$ -> Smoothed Merit Function $v_{l,\tau}(x)$ -> Penalty Term $\gamma p(x)$ -> FOOPS Optimizer
- **Critical path:**
  1. **Inner Loop:** For current $x_t$, solve $\min_y h_{l,\tau}(x_t, y)$ to get $y^*$.
  2. **Gradient Eval:** Compute $\nabla v_{l,\tau}(x_t)$ using the $y^*$ found above.
  3. **Outer Loop:** Update $x_{t+1}$ using the gradient of the total penalized objective.
- **Design tradeoffs:**
  - **$\tau$ (Smoothing):** Small $\tau$ improves approximation accuracy but increases gradient stiffness; large $\tau$ smooths optimization but may distort the Pareto front geometry.
  - **$l$ (Regularization):** Must be large enough ($l > \ell_{f,1}$) to guarantee strong convexity of the inner loop, but too large a value may bias the solution away from the true Pareto set.
- **Failure signatures:**
  - **Divergence:** If inner loop step size $\beta$ is too large or regularization $l$ is too small, the algorithm may fail to find the valid $y^*$, causing outer loop instability.
  - **Constraint Violation:** If penalty $\gamma$ grows too slowly, the algorithm prioritizes $f_0$ over the Pareto constraint, resulting in non-optimal solutions.
  - **Local Traps:** Poor initialization can still lead to local stationary points, though the method handles non-convex fronts better than Linear Scalarization.
- **First 3 experiments:**
  1. **Synthetic Validation:** Replicate Example 3.15 to verify that the method converges to the desired intersection of the preference constraint and the Pareto front, explicitly comparing against Linear Scalarization and PMTL.
  2. **Hyperparameter Sensitivity:** Run an ablation study on $\tau$ and $l$ on a low-dimensional convex problem to visualize how the smoothed merit function shape changes and affects convergence speed.
  3. **Multi-Task Benchmark:** Apply the method to the Multi-MNIST/Fashion dataset to verify that it improves Hypervolume compared to baselines like EPO or FERERO.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can convergence guarantees be established for the FOOPS algorithm when θ < 1, where the penalty function p(x) becomes nonsmooth?
  - **Basis in paper:** The current convergence analysis requires θ ≥ 1 to ensure differentiability of the penalty function. Nonsmooth optimization theory would require different analytical tools.
  - **Why unresolved:** The paper notes that for θ < 1, p(x) can be nonsmooth, and while nonsmooth optimization approaches exist, they may require additional assumptions not yet explored.
  - **What evidence would resolve it:** A convergence rate theorem for Algorithm 1 with θ < 1 under appropriate regularity conditions, along with empirical verification on problems where θ < 1 is beneficial.

- **Open Question 2:** What are the non-asymptotic convergence rates for stochastic variants of FOOPS that use unbiased gradient estimates instead of deterministic gradients?
  - **Basis in paper:** The paper establishes O(1/T) convergence for deterministic settings but notes that stochastic analysis requires handling variance in gradient estimates for both the inner y-loop and outer x-loop.
  - **Why unresolved:** Stochastic analysis requires additional complexity from penalty parameter updates and variance handling in both loops.
  - **What evidence would resolve it:** Convergence bounds for stochastic FOOPS with explicit dependence on stochastic gradient variance, validated empirically on large-scale stochastic multi-objective learning problems.

- **Open Question 3:** Can the preference alignment performance of FOOPS be improved while maintaining its strong hypervolume performance?
  - **Basis in paper:** Results show that FOOPS is better at obtaining large hypervolumes but worse at aligning with preferences compared to other methods.
  - **Why unresolved:** The paper does not analyze why FOOPS underperforms on preference alignment despite theoretical guarantees, which may relate to penalty parameter scheduling or initialization sensitivity.
  - **What evidence would resolve it:** Ablation studies on penalty parameter γ schedules, comparison of different preference function formulations, or modifications to the algorithm that improve preference alignment without sacrificing hypervolume.

## Limitations
- Empirical scope is limited to synthetic problems and two image datasets (Multi-MNIST/Fashion), with no experiments on continuous control or NLP tasks common in multi-objective RL/NLP settings.
- Performance heavily depends on the interplay between smoothing parameter $\tau$, regularization $l$, and penalty schedule $\gamma_t$, which are not fully explored.
- Theoretical guarantees establish stationary point convergence but lack empirical or theoretical bounds on Pareto front approximation quality (e.g., hypervolume error bounds).
- Scalability concerns exist as the inner loop's $K$ iterations and evaluation of $M$ objectives at each step may become prohibitive as $M$ grows.

## Confidence
- **High:** The theoretical framework for the smoothed merit function and its gradient computation is well-founded (Propositions 3.3, 3.5).
- **Medium:** The penalty-based reformulation and its connection to the HÖlderian Error Bound (Theorem 3.9) are sound, but the practical choice of penalty schedule is critical.
- **Medium:** The convergence of the FOOPS algorithm (Theorem E.6) is proven under stated assumptions, but its robustness to poor initialization or ill-conditioned problems is not fully explored.

## Next Checks
1. **Hyperparameter Ablation:** Systematically vary $\tau$ and $l$ on a convex synthetic problem (e.g., Example 3.15) to quantify their impact on convergence speed and the final Pareto set approximation.
2. **Constraint Violation Monitoring:** For the Multi-MNIST experiment, explicitly track $v_{l,\tau}(x)$ and the preference function $f_0(x)$ throughout training to ensure the method does not violate the preference constraint.
3. **Scalability Test:** Apply the method to a larger-scale multi-objective problem (e.g., a neural architecture search with >2 objectives) to assess its computational efficiency and the quality of the Pareto front as $M$ increases.