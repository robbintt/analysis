---
ver: rpa2
title: The benefits of query-based KGQA systems for complex and temporal questions
  in LLM era
arxiv_id: '2507.11954'
source_url: https://arxiv.org/abs/2507.11954
tags:
- entity
- question
- kgqa
- knowledge
- sparql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the limitations of large language models in
  answering complex, multi-hop, and temporal questions by proposing a query-based
  KGQA system that generates executable SPARQL queries over Wikidata. The system employs
  a multi-stage pipeline with entity linking, predicate matching, and text-to-SPARQL
  generation, using both fine-tuned models and Chain-of-Thought reasoning for disambiguation.
---

# The benefits of query-based KGQA systems for complex and temporal questions in LLM era

## Quick Facts
- arXiv ID: 2507.11954
- Source URL: https://arxiv.org/abs/2507.11954
- Reference count: 40
- Primary result: Query-based KGQA system achieves F1 scores up to 39.0% on LC-QuAD 2.0, outperforming direct QA and few-shot methods

## Executive Summary
This work addresses the limitations of large language models in answering complex, multi-hop, and temporal questions by proposing a query-based KGQA system that generates executable SPARQL queries over Wikidata. The system employs a multi-stage pipeline with entity linking, predicate matching, and text-to-SPARQL generation, using both fine-tuned models and Chain-of-Thought reasoning for disambiguation. The approach improves robustness through generalization and rejection studies. Experimental results show strong performance on challenging datasets (LC-QuAD 2.0, QALD-10, RuBQ 2.0, PAT), with F1 scores ranging from 20.8% to 39.0% in cross-validation and outperforming direct QA and few-shot methods, particularly on multi-hop and temporal questions. The system also achieves high rejection accuracy (up to 84.5%) for incorrect queries. Code and data are available at https://github.com/ar2max/NLDB-KGQA-System.

## Method Summary
The proposed KGQA system follows a multi-stage pipeline architecture that transforms natural language questions into executable SPARQL queries over Wikidata. The system first performs entity linking to identify relevant entities from the question, then applies predicate matching to determine relationships between entities, and finally generates the SPARQL query using either fine-tuned models or Chain-of-Thought reasoning for disambiguation. The approach is designed to handle complex, multi-hop, and temporal questions that pose challenges for direct LLM-based question answering. The system includes a rejection mechanism that can identify when queries are likely to be incorrect, with accuracy up to 84.5% in experimental evaluations.

## Key Results
- Achieves F1 score of 39.0% on LC-QuAD 2.0 benchmark, outperforming direct QA and few-shot methods
- Demonstrates strong performance on temporal questions and multi-hop reasoning tasks
- Shows high rejection accuracy of up to 84.5% for identifying incorrect queries
- Achieves F1 score of 20.8% on RuBQ 2.0 for Russian-language queries

## Why This Works (Mechanism)
The system's effectiveness stems from its structured approach to query generation that combines the reasoning capabilities of LLMs with the precision of formal query languages. By breaking down the problem into distinct stages (entity linking, predicate matching, and SPARQL generation), the system can handle the complexity of multi-hop questions that require traversing multiple relationships in the knowledge graph. The use of Chain-of-Thought reasoning helps resolve ambiguities in entity and predicate identification, while the rejection mechanism provides a safety net for filtering out incorrect or nonsensical queries. The multi-stage pipeline architecture allows for targeted improvements at each step and enables the system to handle temporal questions by explicitly modeling time-related predicates in the knowledge graph.

## Foundational Learning
- **Entity Linking**: The process of identifying and disambiguating named entities in natural language questions to match them with knowledge graph entities. This is needed because questions often contain ambiguous references that must be resolved to specific entities in the knowledge base. Quick check: Verify that entities identified in sample questions correctly map to Wikidata entities.
- **SPARQL Query Generation**: The conversion of natural language questions into formal SPARQL queries that can be executed against knowledge graphs. This is essential because it provides a precise, executable representation of the question that avoids the ambiguity inherent in natural language. Quick check: Validate that generated SPARQL queries return expected results for simple test questions.
- **Chain-of-Thought Reasoning**: A prompting technique that guides LLMs through step-by-step reasoning to improve their performance on complex tasks. This is used to disambiguate entities and predicates when multiple interpretations are possible. Quick check: Compare performance with and without Chain-of-Thought on ambiguous questions.
- **Multi-hop Reasoning**: The ability to answer questions that require traversing multiple relationships in a knowledge graph. This is crucial for handling complex questions that cannot be answered by looking up a single fact. Quick check: Test system on questions requiring 2-3 hops in the knowledge graph.
- **Temporal Query Handling**: The capability to process questions involving time-related information and generate appropriate temporal constraints in SPARQL queries. This is needed because many real-world questions involve temporal reasoning. Quick check: Validate that temporal queries correctly filter results by time periods.

## Architecture Onboarding

Component map:
Natural Language Question -> Entity Linking -> Predicate Matching -> SPARQL Generation -> Query Execution -> Answer

Critical path: The entity linking and SPARQL generation stages are the most critical, as errors in entity identification propagate through the pipeline and incorrect query generation directly impacts answer accuracy.

Design tradeoffs: The system trades off computational efficiency for accuracy by using a multi-stage pipeline rather than direct question answering. This approach requires more processing time but achieves better results on complex questions. The use of both fine-tuned models and Chain-of-Thought reasoning provides flexibility but increases model complexity.

Failure signatures: Common failure modes include entity linking errors (identifying wrong entities), predicate matching failures (missing or incorrect relationships), and SPARQL generation errors (syntactically correct but semantically wrong queries). The rejection mechanism helps catch some of these failures but may also reject valid complex queries.

First experiments:
1. Test entity linking accuracy on a set of questions with ambiguous entity mentions
2. Validate SPARQL generation correctness for simple single-hop questions
3. Evaluate rejection mechanism performance on a mixed set of correct and incorrect queries

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on datasets with noisy or ambiguous entity mentions is not fully characterized
- Limited validation of cross-KG generalization beyond Wikidata
- Computational overhead of the multi-stage pipeline versus simpler QA approaches is not quantified
- Rejection mechanism's false positive rate for valid but complex queries is unclear

## Confidence
- Experimental methodology: High (rigorous cross-validation and comparison methods)
- Real-world deployment readiness: Medium (limited out-of-domain testing)
- Cross-KG generalization claims: Low (not validated beyond Wikidata)

## Next Checks
1. Test the system on noisy, real-world question datasets with ambiguous entity mentions to assess robustness
2. Evaluate performance on a different knowledge graph schema (e.g., DBpedia) to validate cross-KG generalization claims
3. Conduct ablation studies to quantify the contribution of each pipeline stage and the computational overhead compared to simpler QA approaches