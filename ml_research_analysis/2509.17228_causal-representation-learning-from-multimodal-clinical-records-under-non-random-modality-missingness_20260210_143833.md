---
ver: rpa2
title: Causal Representation Learning from Multimodal Clinical Records under Non-Random
  Modality Missingness
arxiv_id: '2509.17228'
source_url: https://arxiv.org/abs/2509.17228
tags:
- modality
- clinical
- data
- learning
- missingness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses clinical patient representation learning from
  multimodal EHR data under modality missing-not-at-random (MMNAR) patterns, where
  clinician decision-making drives which modalities are observed. The CRL-MMNAR framework
  explicitly models both observed data and informative missingness patterns through
  three components: MMNAR-aware modality fusion that conditions on clinician-assigned
  observation patterns, modality reconstruction with contrastive learning to ensure
  semantic sufficiency, and multitask outcome prediction with rectifier correction
  for observation-pattern-specific biases.'
---

# Causal Representation Learning from Multimodal Clinical Records under Non-Random Modality Missingness

## Quick Facts
- **arXiv ID:** 2509.17228
- **Source URL:** https://arxiv.org/abs/2509.17228
- **Reference count:** 40
- **Primary result:** CRL-MMNAR achieves up to 13.8% AUC improvement for hospital readmission and 13.1% for ICU admission over 13 baselines

## Executive Summary
This work addresses clinical patient representation learning from multimodal EHR data under modality missing-not-at-random (MMNAR) patterns, where clinician decision-making drives which modalities are observed. The CRL-MMNAR framework explicitly models both observed data and informative missingness patterns through three components: MMNAR-aware modality fusion that conditions on clinician-assigned observation patterns, modality reconstruction with contrastive learning to ensure semantic sufficiency, and multitask outcome prediction with rectifier correction for observation-pattern-specific biases. Evaluated on MIMIC-IV and eICU datasets across three clinical tasks, CRL-MMNAR achieves up to 13.8% AUC improvement for hospital readmission and 13.1% for ICU admission over 13 state-of-the-art baselines, demonstrating robust performance across different healthcare settings and modality configurations.

## Method Summary
CRL-MMNAR employs a three-component framework: (1) MMNAR-aware fusion with missingness gating, where binary observation patterns δ_i are transformed via MLP into dense embeddings z_i that gate modality-specific embeddings, (2) cross-modality reconstruction + InfoNCE contrastive loss where masked modalities are reconstructed from partial representations to ensure semantic sufficiency, and (3) multitask heads with cross-fitted rectifier that corrects for residual bias from specific modality observation patterns. The model is trained end-to-end with L_total = L_miss + L_rep + L_pred, then applies post-training correction using residuals averaged per missingness pattern via cross-fitting.

## Key Results
- Up to 13.8% AUC improvement for hospital readmission prediction over baselines
- Up to 13.1% AUC improvement for post-discharge ICU admission prediction
- Robust performance across MIMIC-IV and eICU datasets with different modality configurations

## Why This Works (Mechanism)

### Mechanism 1: Missingness Pattern as Informative Signal
Conditioning on missingness patterns δ_i captures latent clinical state and improves prediction by transforming δ_i via MLP into a dense embedding z_i that gates modality-specific embeddings, allowing the model to weight modalities based on clinician-driven selection. This relies on the MMNAR assumption that missingness is driven by latent patient health state and clinician decisions, not random noise. If missingness is truly random (MCAR) or driven by non-clinical factors, the signal degrades and gains diminish.

### Mechanism 2: Cross-Modal Reconstruction for Semantic Sufficiency
Reconstructing masked modalities from available ones ensures fused representations capture essential content by using a decoder to reconstruct randomly masked modalities from partial representations, with contrastive loss aligning reconstructions with originals while distinguishing from other patients. This assumes modalities share latent semantic structure where information in one can be recovered from others. If modalities are semantically independent, reconstruction fails and representation quality drops.

### Mechanism 3: Rectifier for Pattern-Specific Bias Correction
Post-training correction via cross-fitted residuals reduces bias from observation-pattern-specific effects by averaging residuals per missingness pattern (τ̂_δ,t) via cross-fitting and adjusting predictions when |τ̂_δ,t| > κ. This assumes observation patterns exert direct effects on outcomes not fully captured by the representation. If pattern-specific effects are negligible or sample sizes per pattern are too small, rectifier adds noise.

## Foundational Learning

- **Concept: Missing Not At Random (MNAR)**
  - Why needed: The entire framework hinges on distinguishing MMNAR (informative missingness) from MCAR/MAR; misunderstanding this invalidates the causal assumptions.
  - Quick check: Can you explain why MAR-based imputation would fail when clinicians order tests based on patient severity?

- **Concept: Contrastive Learning (InfoNCE)**
  - Why needed: Stage 1B uses contrastive loss to prevent trivial reconstructions; understanding positive/negative pairs is essential.
  - Quick check: In this setting, what constitutes a positive vs. negative pair for the contrastive objective?

- **Concept: Cross-Fitting for Debiasing**
  - Why needed: The rectifier uses sample-splitting to avoid overfitting when estimating pattern-specific residuals.
  - Quick check: Why must τ̂_δ,t be estimated on a different fold than where it's applied?

## Architecture Onboarding

- **Component map:** Modality encoders → Missingness embedding z_i → Gating → Transformer fusion → h_i → Mask modalities → Partial representation → Decoder reconstruction → L_rec + L_cont → Task-specific heads → Base predictions → Cross-fitted rectifier → Final predictions

- **Critical path:** The missingness embedding z_i gates all modality inputs; if z_i is poorly learned (L_miss high), downstream fusion and rectifier both degrade.

- **Design tradeoffs:** Threshold κ controls rectifier selectivity; higher κ reduces noise but may miss true bias. Contrastive weight λ_cont vs reconstruction λ_cont; over-weighting contrastive may harm reconstruction fidelity. Virtual modalities in eICU (10 groups) vs raw modalities in MIMIC-IV; granularity affects pattern cardinality.

- **Failure signatures:** Rectifier correction oscillates or increases variance → τ̂_δ,t unstable (insufficient samples per pattern). Reconstruction loss plateaus high → modalities too independent; consider reducing reliance on cross-modal signals. Embedding analysis shows z_i doesn't predict missingness patterns → MMNAR assumption violated.

- **First 3 experiments:**
  1. Ablate each component (Table 2) on validation set; confirm MMNAR-aware fusion provides largest gain.
  2. Synthetic MMNAR injection: Randomly mask modalities with known severity-correlated probability; verify rectifier recovers injected bias.
  3. Sensitivity analysis on κ and λ_cont (Table 4 ranges); identify stable operating region before full training.

## Open Questions the Paper Calls Out

- **Negative transfer in multitask prediction:** As the number of clinical outcome tasks increases, negative transfer may occur where shared representations harm accuracy for certain tasks, making its mitigation an important future direction.

- **Causal structure validity:** The assumed causal structure (latent health state driving both modality content and missingness) may not hold across diverse healthcare systems with different documentation practices.

- **Performance under different missingness types:** CRL-MMNAR performance degrades when missingness patterns shift from clinician-driven (MMNAR) toward random missingness or non-clinical administrative factors.

- **Rectifier theoretical justification:** The rectifier's threshold κ and optimal cross-fitting fold selection lack theoretical justification connecting them to bias-variance tradeoffs or providing consistency guarantees.

## Limitations
- Exact transformer architecture (layers, heads, dimensions) not specified, though defaults around 128-dim embeddings are suggested.
- Missingness pattern granularity differs between datasets (10 virtual modalities in eICU vs 4 in MIMIC-IV), complicating cross-dataset generalization claims.
- Rectifier effectiveness depends on sufficient samples per missingness pattern; patterns with <100 samples risk overfitting.

## Confidence

- **High:** CRL-MMNAR framework design, ablation results showing component contributions, overall AUC improvements over baselines
- **Medium:** Mechanism explanations linking missingness to clinical decision-making, rectifier debiasing claims (methodologically sound but pattern-specific effects hard to verify)
- **Low:** Causal interpretation of missingness as latent health signal (requires external validation beyond observational correlation)

## Next Checks

1. **Pattern stability test:** Re-estimate τ̂_δ,t across different cross-validation splits; high variance indicates rectifier overfitting.
2. **Synthetic MMNAR injection:** Artificially inject severity-correlated missingness into MAR data; verify CRL-MMNAR recovers injected bias while baselines fail.
3. **Long-tail pattern analysis:** For rare missingness patterns (V_δ < 50), measure whether rectifier corrections increase variance without improving accuracy.