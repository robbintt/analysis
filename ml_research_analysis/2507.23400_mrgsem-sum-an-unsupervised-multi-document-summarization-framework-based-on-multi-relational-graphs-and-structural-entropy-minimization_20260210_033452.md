---
ver: rpa2
title: 'MRGSEM-Sum: An Unsupervised Multi-document Summarization Framework based on
  Multi-Relational Graphs and Structural Entropy Minimization'
arxiv_id: '2507.23400'
source_url: https://arxiv.org/abs/2507.23400
tags:
- mrgsem-sum
- graph
- summarization
- unsupervised
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MRGSEM-Sum introduces a novel unsupervised multi-document summarization
  framework that addresses the dual challenges of complex document relationships and
  information redundancy through multi-relational graph construction and structural
  entropy minimization clustering. The method constructs a multi-relational graph
  integrating semantic and discourse relationships between sentences, then applies
  a two-dimensional structural entropy minimization algorithm to automatically determine
  optimal cluster numbers without predefined parameters.
---

# MRGSEM-Sum: An Unsupervised Multi-document Summarization Framework based on Multi-Relational Graphs and Structural Entropy Minimization

## Quick Facts
- arXiv ID: 2507.23400
- Source URL: https://arxiv.org/abs/2507.23400
- Reference count: 6
- Key outcome: MRGSEM-Sum achieves ROUGE-1 of 43.21 on Multi-News, outperforming previous best unsupervised method by 0.89 points

## Executive Summary
MRGSEM-Sum introduces a novel unsupervised multi-document summarization framework that addresses the dual challenges of complex document relationships and information redundancy through multi-relational graph construction and structural entropy minimization clustering. The method constructs a multi-relational graph integrating semantic and discourse relationships between sentences, then applies a two-dimensional structural entropy minimization algorithm to automatically determine optimal cluster numbers without predefined parameters. A position-aware compression mechanism further distills clusters into concise summaries. Experimental results demonstrate MRGSEM-Sum consistently outperforms state-of-the-art unsupervised methods across four benchmark datasets (Multi-News, DUC-2004, PubMed, and WikiSum), achieving ROUGE-1 scores of 43.21 on Multi-News compared to 42.32 for the previous best unsupervised method. Human evaluation confirms superior performance in consistency and coverage, with MRGSEM-Sum approaching human-level quality while maintaining competitiveness with supervised methods and large language models.

## Method Summary
MRGSEM-Sum employs a three-stage pipeline: (1) Multi-relational graph construction using TF-IDF semantic edges and discourse edges (markers, coreference, entity linking) merged via max-pooling; (2) 2D structural entropy minimization clustering that automatically determines cluster count through greedy merging of encoding trees; (3) Position-aware compression that ranks clusters by positional importance and applies K-shortest paths extraction. The framework eliminates the need for predefined cluster numbers and demonstrates superior performance across diverse document types through its integrated approach to modeling document relationships and salience detection.

## Key Results
- Achieves ROUGE-1 score of 43.21 on Multi-News benchmark, outperforming previous best unsupervised method (42.32)
- Maintains consistent performance across all four tested datasets (Multi-News, DUC-2004, PubMed, WikiSum)
- Human evaluation shows superior consistency (4.62/5) and coverage (4.51/5) compared to baselines
- Ablation studies demonstrate critical contributions from each component: removing multi-relational graph drops R-1 by 4.25 on Multi-News, removing 2D SE clustering drops R-1 by 5.35, and removing position-aware compression drops R-1 by 3.61 on Multi-News

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating both semantic and discourse relationships into a multi-relational graph captures richer cross-document connections than single-relation approaches.
- Mechanism: Semantic edges (E_r1) are computed via TF-IDF cosine similarity between sentences; discourse edges (E_r2) leverage discourse markers, coreference, and entity linking. These are merged via max-pooling (Eq. 1) into an integrated adjacency matrix for downstream clustering.
- Core assumption: Semantic and discourse relations provide complementary signals that, when combined, yield more coherent sentence groupings than either alone.
- Evidence anchors:
  - [abstract] "construct a multi-relational graph that integrates semantic and discourse relations between sentences, comprehensively modeling the intricate and dynamic connections"
  - [section] Ablation: removing multi-relation graph drops R-1 by 4.25 on Multi-News and 7.19 on PubMed (Table 4)
  - [corpus] Weak direct corpus evidence on multi-relational graphs for MDS; neighboring work focuses on hierarchical organization and retrieval frameworks rather than multi-relation graph construction
- Break condition: If semantic and discourse signals are highly correlated (low complementarity), merging via max may not improve over single-relation baselines.

### Mechanism 2
- Claim: Two-dimensional structural entropy minimization enables adaptive, parameter-free clustering that automatically determines the optimal number of clusters.
- Mechanism: An encoding tree is constructed and iteratively merged using a greedy strategy that maximizes entropy reduction (ΔSE). The algorithm terminates when no merge yields negative ΔSE, producing cluster partition {C_1, ..., C_c} without pre-specifying c.
- Core assumption: The optimal cluster structure corresponds to a local minimum of structural entropy on the integrated sentence graph.
- Evidence anchors:
  - [abstract] "apply a two-dimensional structural entropy minimization algorithm for clustering, automatically determining the optimal number of clusters"
  - [section] Algorithm 1 (lines 17-30): greedy merge continues until ΔSE ≥ 0; ablation shows removing 2D SE clustering drops R-1 by 5.35 on Multi-News (Table 4)
  - [corpus] Corpus lacks direct validation of SE minimization for MDS clustering; related work uses fixed-cluster methods or neural approaches
- Break condition: If the graph has near-uniform edge weights or highly noisy structure, greedy SE minimization may over-fragment or under-cluster.

### Mechanism 3
- Claim: Position-aware compression prioritizes clusters containing sentences from earlier document positions, improving salience selection especially for long documents.
- Mechanism: Position importance w_pos(s_{i,j}) = 1 - (pos_{i,j} - 1)/(N_i - 1) gives higher weight to earlier sentences. Clusters are ranked by summed position scores, and top c* clusters are retained before K-shortest path compression.
- Core assumption: Sentences appearing earlier in source documents are more likely to contain salient, summary-worthy information.
- Evidence anchors:
  - [abstract] "position-aware compression mechanism to distill each cluster, generating concise and informative summaries"
  - [section] Ablation on PubMed (long-document dataset): removing position-aware drops R-1 from 37.96 to 23.37, R-SU from 14.32 to 4.20 (Table 4)—much larger degradation than on Multi-News
  - [corpus] No direct corpus validation for position-aware mechanisms in MDS; related work emphasizes retrieval and hierarchical organization
- Break condition: If documents have inverted pyramid structure (less common) or key information is distributed uniformly, position bias may hurt coverage.

## Foundational Learning

- Concept: TF-IDF and Cosine Similarity for Semantic Edge Construction
  - Why needed here: Used to compute E_r1 edge weights between sentences; understanding sparse vector representations and similarity metrics is essential for reproducing the graph construction module.
  - Quick check question: Given two sentences with TF-IDF vectors [0.3, 0, 0.4] and [0.3, 0.5, 0], what is their cosine similarity?

- Concept: Structural Entropy on Graphs (Li and Pan 2016)
  - Why needed here: The 2D SE minimization algorithm is not standard in NLP pipelines; understanding encoding trees, entropy contributions from cut edges (g_α), and volume terms (vol(λ)) is critical for implementing Algorithm 1.
  - Quick check question: In a 3-node graph with edge weights [1, 1, 0.5], does merging the two nodes connected by weight 1 increase or decrease structural entropy?

- Concept: K-Shortest Paths for Sentence Compression (Boudin and Morin 2013)
  - Why needed here: Used in the final compression stage to generate candidate compressions from each cluster before keyphrase-based ranking.
  - Quick check question: How does the K-shortest paths approach differ from simply selecting the highest-scoring sentence in a cluster?

## Architecture Onboarding

- Component map:
  - (A) Multi-relational Graph Construction: TF-IDF semantic edges + discourse edges (markers, coreference, entity linking) → merged adjacency via Eq. 1
  - (B) 2D SE Clustering: Initialize 1-sentence clusters → build encoding trees → greedy merge by ΔSE (Algorithm 1) → final partition P
  - (C) Position-aware Compression: Compute w_pos for all sentences → rank clusters by Score_pos → select top c* → K-shortest paths + keyphrase ranking → concatenate Top-1 compressions

- Critical path: Graph construction → adjacency merge → SE minimization clustering → position scoring → compression. If graph edges are sparse or noisy, downstream clustering and compression degrade substantially.

- Design tradeoffs:
  - Multi-relation merge strategy (max vs. sum vs. learned weighting): Paper uses max (Eq. 1); sum or attention-weighted merge may better capture complementary signals but adds complexity.
  - Cluster count c*: Paper uses position-based truncation; alternatively, could use compression ratio constraints or validation-set tuning.
  - Compression candidate count K: Larger K increases recall but raises computational cost and potential redundancy.

- Failure signatures:
  - Very low R-1 with high R-2: Over-compression; check c* and K-shortest paths configuration.
  - Large performance gap between news and non-news domains: Discourse features may not transfer; validate discourse parser on target domain.
  - Cluster fragmentation (many single-sentence clusters): SE threshold may be too strict; inspect ΔSE distribution during merge loop.

- First 3 experiments:
  1. Ablate each edge type (semantic-only, discourse-only, both) on Multi-News and PubMed to quantify complementarity; expect larger drops on long documents when discourse is removed.
  2. Vary merge strategy (max, sum, learned linear combination) to test whether max-pooling is optimal; report ROUGE-1/2/SU and clustering metrics (e.g., NMI if pseudo-labels available).
  3. Control c* manually (5, 10, 15, ALL) on PubMed to verify that position-aware truncation is beneficial vs. fixed-count baselines; expect degradation when using ALL clusters due to noise inclusion.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the max-pooling operation used to merge semantic and discourse edges optimal compared to learned or weighted aggregation strategies?
- Basis in paper: [inferred] Equation 1 defines the adjacency matrix of the integrated graph using `max(A_Er1, A_Er2)`, but provides no justification for this specific fusion method over alternatives.
- Why unresolved: Alternative fusion strategies (e.g., concatenation or attention-based weighting) might capture the relative importance of semantic vs. discourse relations better than a simple element-wise maximum.
- What evidence would resolve it: An ablation study comparing `max` pooling against learned weighted sums or neural fusion mechanisms on the same benchmarks.

### Open Question 2
- Question: Does the linear decay assumption in position-aware compression generalize effectively to non-narrative documents like scientific papers?
- Basis in paper: [inferred] Section C (Methodology) defines positional importance as a linear function ($1 - \frac{pos-1}{N-1}$), strictly prioritizing early sentences.
- Why unresolved: While standard for news, this structural bias may be suboptimal for datasets like PubMed, where critical summary content (e.g., conclusions) often resides at the end of the document rather than the beginning.
- What evidence would resolve it: Experiments comparing the current linear decay against uniform or inverse-position weighting schemes specifically on the PubMed dataset.

### Open Question 3
- Question: How does the computational complexity of the 2D structural entropy minimization algorithm scale with increasing document cluster sizes compared to standard clustering methods?
- Basis in paper: [inferred] Algorithm 1 describes an iterative, greedy merging process to build an encoding tree, but the paper omits a time complexity analysis.
- Why unresolved: While the method removes the need to predefine $k$, the iterative search for the optimal encoding tree may incur higher computational costs than fixed-clustering baselines like SummPip, potentially limiting application to very large clusters.
- What evidence would resolve it: A comparison of execution times between MRGSEM-Sum and baseline graph clustering methods across varying numbers of input documents.

## Limitations
- Edge weight thresholding for semantic and discourse relations is not specified, potentially affecting graph sparsity and downstream clustering quality
- Initial subgraph size (n) in Algorithm 1 and K value for K-shortest paths compression are not detailed, impacting reproducibility
- Method for determining optimal cluster count (c*) lacks transparency, with Table 5 showing values but no selection criteria
- Position-aware mechanism assumes documents follow standard structure (salient info at beginning), which may not hold across all domains

## Confidence

**Major uncertainties:**
- Edge weight thresholding for semantic and discourse relations is not specified, potentially affecting graph sparsity and downstream clustering quality
- Initial subgraph size (n) in Algorithm 1 and K value for K-shortest paths compression are not detailed, impacting reproducibility
- Method for determining optimal cluster count (c*) lacks transparency, with Table 5 showing values but no selection criteria
- No direct validation of the structural entropy minimization algorithm for MDS clustering in the broader corpus
- Position-aware mechanism assumes documents follow standard structure (salient info at beginning), which may not hold across all domains

**Confidence labels:**
- Multi-relational graph integration mechanism: **High** (strong ablation evidence, clear theoretical motivation)
- 2D structural entropy minimization clustering: **Medium** (algorithm described but limited corpus validation of SE minimization for this task)
- Position-aware compression effectiveness: **Medium** (significant ablation impact on PubMed but limited cross-domain validation)

## Next Checks
1. Implement edge weight thresholding experiments (varying cosine similarity cutoffs for semantic edges and discourse edge inclusion criteria) to quantify impact on ROUGE scores
2. Test alternative merge strategies (sum vs. max pooling) for multi-relational graph construction to verify optimality of current approach
3. Conduct domain transfer experiments by applying MRGSEM-Sum to single-document summarization datasets to assess position-aware mechanism's generalizability beyond multi-document settings