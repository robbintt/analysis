---
ver: rpa2
title: Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning
arxiv_id: '2601.01904'
source_url: https://arxiv.org/abs/2601.01904
tags:
- noise
- learning
- reward
- uniform
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces feature-dependent noise (FDN) models within
  preference-based reinforcement learning (PbRL), where noise in preference labels
  is systematically linked to trajectory features. The authors formalize FDN and propose
  variants such as trajectory similarity noise, trajectory feature magnitude noise,
  uncertainty-aware noise, and language model (LM) noise.
---

# Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.01904
- Source URL: https://arxiv.org/abs/2601.01904
- Reference count: 40
- Introduces feature-dependent noise models in preference-based reinforcement learning

## Executive Summary
This paper introduces feature-dependent noise (FDN) models within preference-based reinforcement learning (PbRL), where noise in preference labels is systematically linked to trajectory features. The authors formalize FDN and propose variants such as trajectory similarity noise, trajectory feature magnitude noise, uncertainty-aware noise, and language model (LM) noise. They evaluate these noise models across DMControl and Meta-world domains using a state-of-the-art denoising algorithm, RIME. The experiments show that FDN is harder to detect than uniform noise, particularly hybrid noise, which combines behavioral and model-uncertainty factors. FDN significantly degrades learning performance, and existing denoising methods struggle to handle it. Additionally, LM-based feedback exhibits characteristics similar to FDN, further complicating preference learning. The study highlights the need for robust denoising techniques tailored to feature-dependent noise.

## Method Summary
The authors formalize feature-dependent noise (FDN) in preference-based reinforcement learning, where noise in preference labels is systematically linked to trajectory features. They propose several FDN variants including trajectory similarity noise, trajectory feature magnitude noise, uncertainty-aware noise, and language model noise. The study evaluates these noise models using RIME, a state-of-the-art denoising algorithm, across DMControl and Meta-world domains. Experiments compare FDN against uniform noise and assess the effectiveness of denoising methods under different noise conditions.

## Key Results
- FDN is harder to detect than uniform noise, particularly hybrid noise combining behavioral and model-uncertainty factors
- FDN significantly degrades learning performance in both DMControl and Meta-world domains
- Existing denoising methods, including RIME, struggle to handle FDN effectively
- LM-based feedback exhibits characteristics similar to FDN, complicating preference learning

## Why This Works (Mechanism)
Feature-dependent noise creates systematic correlations between trajectory features and preference label errors, making it more challenging to detect and correct than uniform random noise. The noise patterns are tied to specific aspects of the trajectories, such as similarity between trajectories, feature magnitudes, or model uncertainty, which can mimic genuine preference patterns and confuse denoising algorithms.

## Foundational Learning
- Preference-based Reinforcement Learning: Learning policies from preference feedback rather than explicit rewards; needed for understanding the core problem domain
- Trajectory-based Feedback: Using sequences of states/actions to generate preferences; critical for understanding how FDN manifests
- Noise Modeling: Different types of noise distributions in supervised learning; essential for grasping FDN variants
- Preference Denoising: Methods to remove noise from preference labels; key for understanding evaluation approach
- Language Model Feedback: Using LLMs to generate preference labels; relevant for understanding LM noise variant
- Hybrid Noise Models: Combining multiple noise sources; important for understanding the most challenging FDN variant

## Architecture Onboarding

Component map:
Human Preferences -> FDN Models -> RIME Denoiser -> Policy Learning -> Performance Metrics

Critical path:
Preference collection → FDN injection → RIME denoising → Policy update → Performance evaluation

Design tradeoffs:
- Synthetic vs. real human preferences for noise generation
- Complexity of FDN models vs. practical applicability
- Computational cost of denoising vs. learning performance

Failure signatures:
- Systematic correlation between trajectory features and label errors
- Degradation in learning performance that persists despite denoising
- Difficulty distinguishing FDN from genuine preference patterns

First experiments:
1. Compare uniform noise vs. FDN detection rates using RIME
2. Evaluate learning performance degradation under different FDN variants
3. Test LM-based feedback for FDN-like characteristics

## Open Questions the Paper Calls Out
The study relies on synthetic noise generation, which may not fully capture the complexity and distribution of noise in human-provided preferences. Additionally, while RIME was evaluated as a denoising method, the effectiveness of other state-of-the-art preference denoising algorithms on FDN remains untested. The assumption that LM-based feedback behaves similarly to FDN also requires further empirical validation.

## Limitations
- Synthetic noise generation may not fully represent real-world human preference noise
- Limited evaluation of alternative denoising algorithms beyond RIME
- Assumption about LM-based feedback characteristics requires further validation

## Confidence

High: FDN is harder to detect than uniform noise and degrades learning performance significantly
Medium: LM-based feedback exhibits characteristics similar to FDN
Medium: Existing denoising methods struggle to handle FDN

## Next Checks

1. Test FDN models with human-provided preference labels in real-world robotics or game-playing scenarios to assess practical robustness
2. Evaluate additional denoising algorithms (e.g., meta-learning-based methods) on FDN to identify more effective solutions
3. Conduct ablation studies on LM-based feedback noise to disentangle linguistic patterns from feature-dependent noise