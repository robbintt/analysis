---
ver: rpa2
title: Evaluating the Meta- and Object-Level Reasoning of Large Language Models for
  Question Answering
arxiv_id: '2502.10338'
source_url: https://arxiv.org/abs/2502.10338
tags:
- reasoning
- which
- llms
- object-level
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the meta- and object-level reasoning capabilities
  of large language models (LLMs) in question answering tasks. The authors reframe
  existing reasoning tasks in terms of high-level strategic reasoning (meta-level)
  and lower-level mathematical/natural language reasoning (object-level).
---

# Evaluating the Meta- and Object-Level Reasoning of Large Language Models for Question Answering

## Quick Facts
- arXiv ID: 2502.10338
- Source URL: https://arxiv.org/abs/2502.10338
- Reference count: 8
- LLMs excel at meta-level planning but struggle with object-level execution in multi-step QA tasks

## Executive Summary
This paper evaluates large language models' capabilities for meta-level reasoning (high-level strategic planning and problem decomposition) versus object-level reasoning (low-level execution including arithmetic, data retrieval, and factual recall) in question answering tasks. Through human annotation studies using the novel FRANKLIN dataset and three established benchmarks, the authors find that LLMs consistently demonstrate strong meta-level reasoning abilities, successfully generating rational approaches and clear step-by-step plans. However, they frequently fail to provide answers for object-level reasoning tasks, even when prompted with successful plans, due to challenges with precise data retrieval, arithmetic operations, and grounding tokens in external truth.

## Method Summary
The study employs two human annotation studies using four datasets (FRANKLIN, GSM8k, HotpotQA, StrategyQA) and four LLMs (Llama 3.1 8B, Phi 3.5 Mini, Gemma 2 9B, GPT-4o-mini). Participants rate model responses on 5-point Likert scales for presence of answers, plans, rational approaches, and step execution. Metrics include Answer Failure Rate (AFR), Rational Approach Rate (RAR), and Plan Creation Rate (PCR). Study 1 uses single-prompt answer generation, while Study 2 employs a two-stage plan-then-execute protocol. Each example receives 4 annotations, aggregated via 3-point mapping and majority vote.

## Key Results
- LLMs consistently demonstrate strong meta-level reasoning abilities, with RAR and PCR frequently exceeding 95% across model/dataset combinations
- LLMs frequently fail to provide answers for object-level reasoning tasks, even when prompted with successful plans (high AFR)
- FRANKLIN dataset presents particular challenges for object-level reasoning, with models making errors including data fabrication, inaccurate data, and incorrect arithmetic
- Plan-first prompting reduces AFR for most models by making object-level subtasks more salient, though it does not guarantee correct execution

## Why This Works (Mechanism)

### Mechanism 1: Plan Generation via Language Pattern Matching
- Claim: LLMs can generate rational, structured plans for complex QA tasks with high consistency, even when they cannot successfully execute those plans.
- Mechanism: LLMs emulate meta-level reasoning by retrieving and composing familiar problem-decomposition patterns from pre-training data. The model does not possess a formal reasoning architecture; it generates text that *appears* to be strategic planning through statistical pattern completion.
- Core assumption: The planning patterns required for the evaluated datasets are sufficiently represented in the model's pre-training corpus.
- Evidence anchors:
  - [abstract] "LLMs consistently demonstrate strong meta-level reasoning abilities, successfully creating rational approaches and clear step-by-step plans for solving problems."
  - [section 5.2] "Results at this meta-level reasoning task show both stronger, and more consistent levels of performance... frequently over 95% for many model/dataset combinations."
  - [corpus] Related work (Meta-R1) identifies that current reasoning models "lack a dedicated meta-level cognitive system," supporting the interpretation that meta-level behavior is emergent pattern-matching rather than architectural.
- Break condition: Tasks requiring novel planning schemas not represented in training data; questions requiring domain-specific strategic knowledge the model lacks.

### Mechanism 2: Object-Level Execution Failure via Precision Demands
- Claim: LLMs frequently fail at object-level reasoning tasks (arithmetic, data retrieval, factual recall) even when prompted with successful plans, due to precision and grounding requirements.
- Mechanism: Object-level tasks require accurate retrieval of specific values (e.g., population statistics) and precise arithmetic operations. LLMs lack reliable access to structured knowledge bases and cannot guarantee numerical precision, leading to fabrication, rounding errors, or calculation mistakes.
- Core assumption: Object-level failures stem from the model's inability to reliably ground tokens in external truth rather than from misunderstanding the task.
- Evidence anchors:
  - [abstract] "LLMs frequently fail to provide answers (high Answer Failure Rate) for object-level reasoning tasks... despite successfully planning solutions."
  - [section 5.3] "Different error modes are present when an answer is attempted, including data fabrication... inaccurate or low-precision data... and incorrect arithmetic."
  - [corpus] The Knowledge-Reasoning Dissociation paper similarly identifies "fundamental limitations of LLMs" in structured reasoning tasks requiring precise inference, suggesting a broader pattern.
- Break condition: Tasks where approximate answers are acceptable; tasks where external tools (calculators, databases) are provided to the model.

### Mechanism 3: Plan-First Prompting Reduces Execution Avoidance
- Claim: Instructing models to create plans before answering reduces Answer Failure Rate for most models, though it does not guarantee correct execution.
- Mechanism: Decomposing the task into explicit steps makes the object-level subtasks more salient, reducing the likelihood that the model will avoid attempting an answer. The plan serves as a cognitive scaffold that structures subsequent generation.
- Core assumption: The reduction in AFR reflects genuine task decomposition benefits, not merely increased generation length improving calibration.
- Evidence anchors:
  - [section 5.1] "AFR is consistently lower in the setting of study 2, indicating that the generation of a plan enabled models [to] attempt answers more frequently."
  - [section 4.1] Study 2 "was designed to observe the models' meta-level reasoning ability, and also the inﬂuence of the breaking down of a problem on their ability to produce answers."
  - [corpus] Insufficient corpus evidence for the specific mechanism; related work on Chain-of-Thought focuses on accuracy improvements rather than answer attempt rates.
- Break condition: Safety-aligned models (e.g., GPT-4o-mini) that refuse to execute plans after generating them; tasks where the plan does not meaningfully decompose the problem.

## Foundational Learning

- **Meta-level vs. Object-level Reasoning Distinction**
  - Why needed here: The entire framework depends on correctly classifying reasoning steps as strategic planning (meta) or execution (object). Without this distinction, you cannot diagnose where failures occur.
  - Quick check question: Given the step "Compare the population values to find the maximum," is this meta-level or object-level reasoning? (Answer: Object-level—it performs the comparison operation. The decision to compare is meta-level.)

- **Chain-of-Thought Prompting**
  - Why needed here: The study builds directly on CoT techniques, extending from "let's think step-by-step" to explicit plan-then-execute protocols. Understanding CoT is prerequisite to understanding why plan-first prompting was tested.
  - Quick check question: What is the core claim of CoT prompting? (Answer: Intermediate reasoning steps in natural language improve performance on multi-step tasks.)

- **LLM Hallucination and Precision Limitations**
  - Why needed here: The object-level failures (data fabrication, inaccurate values) are instances of hallucination. Understanding why LLMs struggle with precise factual recall explains the persistent AFR even with good plans.
  - Quick check question: Why might an LLM report "The population of Togo in 2020 was 8.43 million" when the true value is 8,442,580? (Answer: The model generates plausible-sounding approximations rather than retrieving exact values; it lacks grounded access to the data source.)

## Architecture Onboarding

- **Component map:**
  - Datasets (FRANKLIN, GSM8k, HotpotQA, StrategyQA) -> Model responses (Llama 3.1 8B, Phi 3.5 Mini, Gemma 2 9B, GPT-4o-mini) -> Human annotations (5-point Likert scale) -> Aggregated metrics (AFR, RAR, PCR) -> Comparative analysis

- **Critical path:**
  1. Select question from dataset -> 2. Generate response (with/without plan-first prompt) -> 3. Present to 4 annotators -> 4. Aggregate annotations -> 5. Compute AFR/RAR/PCR -> 6. Compare across models and datasets

- **Design tradeoffs:**
  - **Dataset scope**: FRANKLIN focuses on geopolitical indicators; generalization to other domains requires validation
  - **Model size**: Only smaller models tested (8-9B parameters, plus GPT-4o-mini); findings may not transfer to frontier models
  - **Annotation depth**: Evaluates presence of reasoning behaviors, not correctness of final answers. Low AFR ≠ high accuracy
  - **Prompt sensitivity**: Study 2 uses conversation-based prompting unavailable in Gemma 2, introducing prompt heterogeneity

- **Failure signatures:**
  - **High AFR + High RAR/PCR**: Model can plan but cannot or will not execute (object-level failure or refusal)
  - **GPT-4o-mini pattern**: AFR increases in Study 2 (100% on FRANKLIN)—plan generated but execution declined due to safety guardrails
  - **FRANKLIN-specific high AFR**: Object-level requirements (precise data retrieval + arithmetic) are the bottleneck

- **First 3 experiments:**
  1. **Replicate AFR analysis on a larger model** (e.g., Llama 70B or GPT-4) to test whether object-level failures persist at scale. Hypothesis: AFR will decrease but fabrication/inaccuracy error modes will remain.
  2. **Tool-augmented execution**: Provide models with calculator and web search tools during object-level execution. Measure AFR reduction and error type shifts. Hypothesis: External tools will reduce arithmetic errors but may not eliminate fabrication if retrieval is ungrounded.
  3. **Cross-domain FRANKLIN extension**: Create analogous templates for financial indicators, scientific metrics, or sports statistics. Test whether meta/object gap generalizes or is domain-specific. Hypothesis: Gap persists across domains requiring precise numerical data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning or increased model scale specifically improve object-level reasoning performance on datasets like FRANKLIN, or is the gap between meta- and object-level reasoning a fundamental architectural limitation?
- Basis in paper: [explicit] Conclusion states: "we plan continued development of FRANKLIN, and evaluation of FRANKLIN on LLMs both off-the-shelf and fine-tuned, as well as at larger parameter counts."
- Why unresolved: The study only tested off-the-shelf models at 8B-9B parameters; the paper finds consistent object-level failures across all tested models regardless of architecture.
- What evidence would resolve it: Evaluation of fine-tuned variants and larger parameter models (e.g., 70B+) on FRANKLIN, measuring whether AFR decreases significantly without degrading meta-level performance.

### Open Question 2
- Question: How can systems be designed to bridge the gap between strong meta-level planning capabilities and weak object-level execution in LLMs?
- Basis in paper: [explicit] The paper's central finding is that "LLMs demonstrate meta-level reasoning with high frequency, but struggle with object-level reasoning tasks," yet offers no solution for closing this gap.
- Why unresolved: The study characterizes the asymmetry but does not investigate interventions that could transfer meta-level plan quality into successful object-level execution.
- What evidence would resolve it: Experiments with hybrid architectures (e.g., LLM planners paired with external tools for arithmetic/retrieval) showing improved answer success rates while preserving planning quality.

### Open Question 3
- Question: How does the distinction between reasoning and imitation manifest in meta- versus object-level tasks, and can probing methods distinguish true reasoning from pattern matching?
- Basis in paper: [explicit] The paper states: "There is debate over whether LLMs are actually reasoning rather than emulating or imitating it" and notes they "share this scepticism" while claiming models "are able to imitate meta-level reasoning."
- Why unresolved: The study evaluates surface-level outputs via human annotation but does not employ methods to probe internal representations or test robustness to distributional shifts that would distinguish reasoning from imitation.
- What evidence would resolve it: Probing experiments (e.g., counterfactual interventions, out-of-distribution generalization tests) comparing performance stability between meta- and object-level reasoning tasks.

## Limitations
- Domain Generalization: FRANKLIN focuses on geopolitical indicators; findings may not generalize to other domains requiring precise numerical reasoning
- Model Scale: Only tested 8-9B parameter models and GPT-4o-mini; results may not extend to frontier models
- Annotation Subjectivity: Human evaluation relies on Likert scales and majority voting, which may mask nuanced disagreements

## Confidence
- **High Confidence**: LLMs consistently demonstrate strong meta-level reasoning (plan generation) across datasets and models, with RAR and PCR frequently exceeding 95%
- **Medium Confidence**: Object-level reasoning failures (high AFR) are primarily due to precision demands (data fabrication, inaccurate values, arithmetic errors) rather than fundamental inability to execute plans
- **Medium Confidence**: Plan-first prompting reduces AFR by making object-level subtasks more salient, though this mechanism requires further validation

## Next Checks
1. **Scale Extension**: Replicate the meta/object reasoning evaluation on larger frontier models (e.g., Llama 70B, GPT-4) to test whether object-level failures persist at scale
2. **Tool-Augmented Execution**: Provide models with external tools (calculator, web search) during object-level execution to measure AFR reduction and shifts in error types
3. **Cross-Domain FRANKLIN**: Create analogous question templates for financial indicators, scientific metrics, or sports statistics to test whether the meta/object reasoning gap generalizes beyond geopolitical domains