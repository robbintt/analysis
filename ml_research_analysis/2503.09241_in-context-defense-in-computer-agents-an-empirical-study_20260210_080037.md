---
ver: rpa2
title: 'In-Context Defense in Computer Agents: An Empirical Study'
arxiv_id: '2503.09241'
source_url: https://arxiv.org/abs/2503.09241
tags:
- defense
- attacks
- agent
- agents
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic study on defending computer
  agents against context deception attacks, which embed misleading content into an
  agent's operational environment to manipulate behavior. The authors propose an in-context
  defense approach that uses carefully curated exemplars containing both malicious
  environments and defensive responses, combined with chain-of-thought reasoning.
---

# In-Context Defense in Computer Agents: An Empirical Study

## Quick Facts
- arXiv ID: 2503.09241
- Source URL: https://arxiv.org/abs/2503.09241
- Reference count: 15
- Primary result: 91.2% reduction in attack success rate for pop-up window attacks using few-shot exemplars with defensive reasoning

## Executive Summary
This paper presents the first systematic study on defending computer agents against context deception attacks using in-context learning. The authors propose augmenting agents with carefully curated exemplars containing both malicious environments and defensive responses, combined with chain-of-thought reasoning. This approach teaches agents to perform explicit defensive reasoning before action planning, enabling them to identify and avoid deceptive elements like pop-up windows and HTML injections. The method demonstrates significant effectiveness across different VLM backbones while maintaining minimal performance degradation in benign scenarios.

## Method Summary
The approach involves augmenting computer agents' context with few-shot exemplars containing malicious environments and corresponding defensive responses. These exemplars include structured chain-of-thought reasoning with "Risk/Distraction Analysis" followed by action planning. The defense requires that defensive reasoning precedes action planning in the reasoning sequence. Agents are tested on VisualWebArena tasks with pop-up window attacks, environment injection attacks from Mind2Web, and distracting advertisements from a proprietary EDA dataset. The defense uses 9 defensive exemplars (3 per attack type) plus 0-3 benign exemplars, achieving robust performance across GPT-4o, Gemini 1.5, and Claude 3.5 backbones.

## Key Results
- 91.2% reduction in attack success rate for pop-up window attacks
- 74.6% average reduction in attack success rate for environment injection attacks
- 100% successful defenses against distracting advertisements
- Minimal performance degradation in benign scenarios (≤3.3%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context exemplars with explicit defensive reasoning teach agents to identify and ignore deceptive elements before action.
- Mechanism: Few-shot exemplars containing (Q_malicious, A_gold) pairs with structured "Risk/Distraction Analysis" reasoning condition the VLM to perform threat identification as a learned intermediate step.
- Core assumption: The VLM's in-context learning capability can generalize defensive patterns from exemplars to unseen attack instances.
- Evidence anchors: [abstract] "Our approach involves augmenting the agent's context with a small set of carefully curated exemplars"; [section 3.2] "Defensive exemplars h_defensive = (Q_malicious, A_gold)"; [corpus] Weak direct evidence; ICLShield (arXiv:2511.09114) discusses ICL vulnerability to backdoor attacks.
- Break condition: If attack patterns diverge significantly from exemplar distribution (OOD), ASR reduction drops from 99.5% to 89.0% (Table 4).

### Mechanism 2
- Claim: Defensive reasoning must precede action planning in the CoT sequence for effective defense.
- Mechanism: Causal autoregressive generation means early tokens condition later ones. Placing defense-first decomposes the task into: (1) identify risky elements, then (2) plan actions given identified risks.
- Core assumption: The VLM processes CoT steps sequentially with limited lookahead, making order causally significant.
- Evidence anchors: [section 3.4] "Defensive reasoning should appear before action planning reasoning"; [Table 6] Defense-first: 99.5% ASR reduction vs 90.3% planning-first; [Figure 6] Failure case shows agent rationalizing suspicious elements.
- Break condition: If agent architecture uses non-causal or bidirectional reasoning, ordering effects may diminish.

### Mechanism 3
- Claim: As few as 1-3 defensive exemplars are sufficient to induce robust defensive behavior.
- Mechanism: In-context learning exhibits sample-efficient pattern transfer. The exemplar provides a reasoning template that the VLM applies compositionally to new inputs.
- Core assumption: Exemplars share structural similarity with deployment scenarios sufficient for pattern transfer.
- Evidence anchors: [abstract] "a minimal number of exemplars (fewer than three) is sufficient"; [Table 5] 1 exemplar: 96.2% ASR reduction; [section 4.4] "requiring only one carefully crafted example to suppress 96.2% of attacks".
- Break condition: Exemplar must be "carefully crafted"—poorly constructed exemplars likely degrade performance.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - Why needed here: The entire defense mechanism relies on ICL to transfer defensive patterns from exemplars to new scenarios without weight updates.
  - Quick check question: Can you explain why ICL enables few-shot adaptation without fine-tuning, and what "context window" constraints might apply?

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed here: Defensive reasoning is implemented as a structured CoT process ("Risk/Distraction Analysis" → "Action planning"). Understanding CoT's role in decomposing complex tasks is essential.
  - Quick check question: How does step-by-step reasoning in CoT differ from direct output generation, and why might ordering matter for autoregressive models?

- **Concept: VLM-based Computer Agent Architecture**
  - Why needed here: The defense targets agents that perceive environments via screenshots/HTML and output actions. Understanding the input-output pipeline clarifies where exemplars are injected.
  - Quick check question: What modalities does a VLM agent process, and where in the conversation history would in-context exemplars be placed?

## Architecture Onboarding

- **Component map:** Screenshot + Task Description → Conversation History (with injected exemplars) → VLM Backbone → Defensive CoT Output → Action

- **Critical path:**
  1. Exemplar construction (curate Q_malicious + A_gold with structured defensive CoT)
  2. Exemplar injection into agent conversation history (before current task)
  3. Ensure defense-first ordering in output format specification
  4. Validate output parsing extracts action from final line

- **Design tradeoffs:**
  - **IND vs OOD exemplars:** In-distribution yields 99.5% ASR reduction; OOD drops to 89.0%. Trade specificity for generalization.
  - **Exemplar quantity:** More exemplars marginally improve SR but increase first-round inference cost. Prompt caching mitigates subsequent costs.
  - **Benign exemplars:** Including 0-3 benign examples prevents over-sensitivity but adds context length. Paper shows 13.8% SR improvement in benign conditions for pop-up tasks.

- **Failure signatures:**
  - Agent rationalizes suspicious elements as legitimate → check if CoT ordering is planning-first
  - High false positive rate (ignoring legitimate UI) → increase benign exemplars or reduce defensive exemplar count
  - Defense ineffective against new attack type → exemplars may be OOD; curate attack-specific exemplars
  - Output format violations → reinforce format in exemplar responses

- **First 3 experiments:**
  1. **Baseline replication:** Run VisualWebArena agent under pop-up attack with no defense. Confirm ASR ≈ 0.58. Add 3 defensive exemplars (IND). Verify ASR drops to ~0.05.
  2. **Ablation on ordering:** Swap CoT order (planning-first) on same attack. Confirm ASR increases to ~0.057 vs ~0.003 defense-first.
  3. **Cross-model validation:** Apply identical exemplars to Gemini 1.5 or Claude 3.5. Confirm ASR reduction remains >88%.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the exemplar curation process be automated while maintaining defense effectiveness?
- Basis in paper: [explicit] Authors state in limitations: "Future works could address these challenges for defense with better efficiency, controllability, and reliability."
- Why unresolved: Current approach requires manually crafted defensive exemplars with specific CoT reasoning structures; automation methods remain unexplored.
- What evidence would resolve it: Experiments comparing manually curated vs. automatically generated exemplars showing comparable ASR reductions.

### Open Question 2
- Question: How can the generalization gap between in-distribution and out-of-distribution exemplars be reduced?
- Basis in paper: [inferred] Table 4 shows OOD exemplars achieve 89.0% ASR reduction vs. 99.5% for IND exemplars.
- Why unresolved: The paper demonstrates the gap exists but does not propose methods to close it; UI aesthetics and deception mechanism variations impair transfer.
- What evidence would resolve it: Novel exemplar design strategies achieving <5% performance gap between IND and OOD scenarios.

### Open Question 3
- Question: Does in-context defense generalize to adversarial attacks beyond context deception attacks?
- Basis in paper: [inferred] Section 2.2 distinguishes adversarial attacks from context deception attacks; defense was only evaluated on the latter category.
- Why unresolved: The unified framework may not transfer to imperceptible perturbations or adversarial strings, which operate differently from human-perceptible deception.
- What evidence would resolve it: Experiments applying the same in-context defense methodology to adversarial attack benchmarks with measurable ASR reductions.

## Limitations

- Limited exemplar availability: Only three examples shown in Figure 2, while the paper uses 9 defensive exemplars and 0-3 benign exemplars, making faithful reproduction challenging.
- Proprietary datasets: The EDA dataset and custom agent implementation are not publicly available, limiting full replication of all results.
- Sparse attack implementation details: Attack patterns are described but not fully specified, particularly for environment injection attacks beyond the "P+1" setting.

## Confidence

- **High Confidence**: Core mechanism that defensive reasoning must precede action planning (supported by Table 6's 99.5% vs 90.3% ASR reduction difference) and that few exemplars (1-3) are sufficient (Table 5 showing 96.2-99.5% ASR reduction)
- **Medium Confidence**: Cross-model robustness claims (GPT-4o, Gemini 1.5, Claude 3.5) due to limited public replication of these specific models
- **Low Confidence**: Claims about maintaining task performance in benign scenarios (≤3.3% degradation) due to proprietary agent implementations and datasets

## Next Checks

1. **CoT Ordering Validation**: Systematically test ASR with planning-first vs defense-first CoT ordering on identical pop-up attacks to confirm the 9-fold difference in effectiveness
2. **Exemplar Quantity Sensitivity**: Evaluate ASR reduction with 0, 1, 3, and 9 defensive exemplars on the same attack types to verify the sample-efficiency claims
3. **Cross-Attack Generalization**: Test the same defensive exemplars on novel attack variants (different pop-up text, HTML injection positions) to measure OOD performance degradation from the claimed 99.5% to 89.0% ASR reduction