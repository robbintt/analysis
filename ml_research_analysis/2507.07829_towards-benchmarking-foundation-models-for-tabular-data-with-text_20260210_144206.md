---
ver: rpa2
title: Towards Benchmarking Foundation Models for Tabular Data With Text
arxiv_id: '2507.07829'
source_url: https://arxiv.org/abs/2507.07829
tags:
- text
- tabular
- data
- datasets
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper benchmarks how foundation models handle tabular data\
  \ with text by curating a new dataset collection and testing three embedding strategies\
  \ (fastText, Skrub\u2019s TableVectorizer, AutoGluon\u2019s encoder) across models\
  \ like TabPFNv2 and XGBoost. Experiments show text embeddings generally improve\
  \ accuracy over no-text, but no single embedding method dominates, and downsampling\
  \ is crucial for performance and efficiency."
---

# Towards Benchmarking Foundation Models for Tabular Data With Text

## Quick Facts
- arXiv ID: 2507.07829
- Source URL: https://arxiv.org/abs/2507.07829
- Authors: Martin Mráz; Breenda Das; Anshul Gupta; Lennart Purucker; Frank Hutter
- Reference count: 40
- Primary result: Text embeddings generally improve accuracy over no-text baseline across 13 tabular datasets tested with three embedding strategies and models

## Executive Summary
This paper benchmarks foundation models for tabular data with text by curating a new dataset collection and systematically comparing three embedding strategies (fastText, Skrub's TableVectorizer, AutoGluon's encoder) across models like TabPFNv2 and XGBoost. Experiments show text embeddings generally improve accuracy over no-text, but no single embedding method dominates. The authors identify that downsampling is crucial for performance and efficiency, and provide qualitative analysis revealing limitations of common embeddings under synonym variation, noise, and ambiguity. The study highlights the need for better embedding methods and provides rules and datasets for improved benchmarking of multimodal tabular models.

## Method Summary
The authors curate 13 tabular datasets with text columns and evaluate three embedding strategies: fastText sentence vectors, Skrub's GapEncoder, and AutoGluon's n-gram feature generator. These embeddings are concatenated with numerical/categorical features and fed to tabular models (TabPFNv2, XGBoost). Due to memory constraints, features are downsampled to max 300 dimensions and datasets to 3000 rows using various selection methods (SHAP, variance, PCA, etc.). Performance is measured via 5-fold cross-validation using accuracy for classification and R² for regression, comparing text vs no-text scenarios.

## Key Results
- Text embeddings improve accuracy over no-text baseline in 11/13 datasets across all models
- FastText embeddings performed best in 7/13 datasets, but no single method dominates universally
- SHAP-based feature selection most frequently gives best performance, though not always dominant
- BERT embeddings underperformed, possibly due to substantial downsampling required by hardware constraints

## Why This Works (Mechanism)

### Mechanism 1
Incorporating text embeddings into tabular pipelines generally improves predictive performance over structured features alone. Text columns are converted to dense vector representations via per-column embedding strategies (fastText sentence vectors, Skrub's GapEncoder, AutoGluon's n-gram features), then concatenated with numerical/categorical features and fed to tabular models. This works because text columns contain predictive information complementary to structured features (the "dual-signal" requirement). Evidence: experiments show text embeddings generally improve accuracy over no-text, Table 2 shows text embeddings outperform no-text in 11/13 datasets across all models. Break condition: when text columns are non-semantic (e.g., IDs, codes) or when downsampling aggressively removes informative dimensions.

### Mechanism 2
Feature downsampling is critical for both computational tractability and noise mitigation when working with text embeddings. Embedding methods produce high-dimensional outputs (e.g., fastText produces 300-dim vectors per text column). Downsampling techniques (SHAP-based importance, variance thresholding, PCA, statistical tests) select the most informative dimensions before model training, reducing memory consumption and filtering noise. This works because not all embedding dimensions carry equal predictive signal; some dimensions may be redundant or noisy. Evidence: SHAP-based selection most frequently gives the best performance, but it doesn't always dominate. Break condition: over-aggressive downsampling removes critical semantic information.

### Mechanism 3
Different embedding strategies exhibit distinct robustness profiles to synonym variation, noise, and semantic ambiguity. TF-IDF relies on exact token frequency—breaks when test data contains OOD synonyms. FastText averages word vectors—robust to synonyms but disrupted by random noise diluting the signal. BERT captures context—confused by conflicting semantic signals within the same text. This works because real-world text contains variations (synonyms), irrelevant content (noise), and contradictory signals (ambiguity). Evidence: qualitative experiments show TF-IDF underperforms with OOD synonyms, FastText degrades with random noise, and BERT confused by semantic ambiguity. Break condition: when text characteristics align with an embedding method's specific weakness—no single method dominates universally.

## Foundational Learning

- Concept: **Per-column embedding vs. row-as-text serialization**
  - Why needed here: The paper focuses on per-column embeddings (preserving tabular structure) rather than serializing entire rows as text prompts for LLMs. Understanding this distinction clarifies why certain methods are evaluated.
  - Quick check: Does your task have >32 training samples and meaningful structured features alongside text? If yes, per-column embeddings are appropriate; if text dominates and samples are few, row-as-text LLM approaches may be better.

- Concept: **Feature importance-based selection (SHAP) vs. unsupervised dimensionality reduction (PCA, variance)**
  - Why needed here: Downsampling strategy choice affects which embedding dimensions survive. Supervised methods (SHAP, Lasso, t-test) leverage target information; unsupervised methods (PCA, variance) don't.
  - Quick check: Is your target variable reliable and available during feature selection? If yes, SHAP-based selection often outperforms; if not (or avoiding leakage), use unsupervised methods.

- Concept: **In-context learning (TabPFNv2) vs. gradient boosting (XGBoost) for small tabular data**
  - Why needed here: TabPFNv2 is a foundation model using in-context learning, effective on small datasets (<10K samples). XGBoost is a traditional boosted tree. Both are evaluated with text embeddings, but have different memory/compute profiles.
  - Quick check: Does your dataset have <3000 rows and <300 features after embedding? TabPFNv2 excels here; for larger data, XGBoost or AutoGluon may be more practical.

## Architecture Onboarding

- Component map: Raw tabular data with text columns → preprocessing (HTML stripping, imputation, type classification) → Text embedding (fastText, Skrub GapEncoder, or AutoGluon n-grams) → Feature concatenation with numerical/categorical features → Downsampling (SHAP, variance, PCA, correlation, t-test/ANOVA) → Model training/inference (TabPFNv2, XGBoost, or AutoGluon)

- Critical path: 1. Identify text columns via heuristic (high cardinality, low numeric structure) 2. Choose embedding method based on text characteristics (short text → n-grams may suffice; long text with synonyms → fastText/BERT) 3. Apply downsampling to meet memory constraints (≤300 features, ≤3000 rows for TabPFNv2) 4. Train and evaluate; iterate on embedding/downsampling choices

- Design tradeoffs:
  - fastText vs. Skrub vs. AutoGluon: fastText most frequently best (7/13 datasets) but no universal winner. Skrub's GapEncoder is lightweight (30-dim) but may under-represent complex text. AutoGluon's n-gram features handle short text well but break on synonym variation.
  - SHAP vs. variance downsampling: SHAP is supervised (requires target), often best but not always. Variance is unsupervised, faster, but may retain noisy high-variance features.
  - Local models vs. APIs: Local TabPFNv2/XGBoost with custom embeddings often achieve higher accuracy than TabPFNv2 API or AutoGluon defaults, but require more engineering.

- Failure signatures:
  - Memory overflow: TabPFNv2 fails with >3000 rows or >300 features—indicates insufficient downsampling
  - Near-random performance on text-heavy datasets: Suggests embedding method mismatch (e.g., n-grams on synonym-rich text)
  - Text features underperforming no-text baseline: May indicate text columns are non-semantic (IDs, codes) or preprocessing introduced leakage

- First 3 experiments:
  1. Baseline comparison: Run model with text vs. without text using fastText embeddings + SHAP downsampling. Confirm text adds value on your dataset.
  2. Embedding ablation: Compare fastText, Skrub, and AutoGluon embeddings on held-out validation set. Identify which handles your text characteristics (synonyms, noise, ambiguity).
  3. Downsampling sensitivity: Vary feature budget (100, 200, 300 features) with SHAP selection. Find the knee point where performance saturates or degrades.

## Open Questions the Paper Calls Out

### Open Question 1
How can we develop text embedding techniques that are robust to synonym variations, random noise, and semantic ambiguity in tabular data? The authors conclude that standard n-gram and NLP-based embeddings fail under these specific conditions and "encourage research into resolving these limitations." This remains unresolved because current embeddings (TF-IDF, FastText, BERT) each fail qualitatively on synthetic tests designed to mimic real-world text noise. A new embedding method that maintains high accuracy on the qualitative "break" tests would resolve this.

### Open Question 2
Do high-dimensional LLM embeddings (e.g., BERT) outperform simpler baselines like fastText when aggressive downsampling is not required? The paper notes BERT underperformed likely due to the "substantial downsampling" required by hardware constraints, contrasting with other work showing LLMs are effective. This remains unresolved because the study limited feature dimensions to 300, potentially destroying the semantic information in high-dimensional BERT vectors. Experiments evaluating BERT embeddings on hardware that supports larger feature sets would resolve this.

### Open Question 3
What is the optimal strategy for integrating text into tabular pipelines given that no single embedding method dominates across all datasets? The authors ask "Which embedding strategy works best, and under what conditions?" and conclude that "no single best embedding method" exists. This remains unresolved because FastText, Skrub, and AutoGluon embeddings show inconsistent performance across the 13 curated datasets. A meta-analysis or automated selection mechanism that predicts the best embedding strategy based on dataset statistics would resolve this.

## Limitations
- Only three embedding strategies were evaluated, leaving uncertainty about how more sophisticated approaches (BERT, RoBERTa, or domain-specific embeddings) would perform under identical conditions
- The study relies on a curated dataset collection rather than systematic synthetic generation, limiting generalizability to unseen text patterns
- The preprocessing pipeline (max 100k rows, 50% missing value threshold) may filter out edge cases relevant to real-world deployment

## Confidence

- High confidence: Text embeddings generally improve accuracy over no-text baseline (supported by 11/13 dataset results in Table 2, consistent across all three models tested)
- Medium confidence: No single embedding method dominates universally (fastText performed best on 7/13 datasets, but the sample size is small and the margin of victory varies significantly)
- Medium confidence: Downsampling is critical for performance and efficiency (supported by hardware constraints and ablation results, but optimal selection strategy varies by dataset)

## Next Checks
1. Test BERT, RoBERTa, or domain-specific embeddings with gradient-based dimensionality reduction to determine if contextual models can outperform fastText when properly downsampled
2. Generate controlled synthetic datasets with known synonym, noise, and ambiguity patterns to systematically validate which embedding strategies fail under which conditions
3. Vary the 50% missing value threshold and 100k row cap to determine how preprocessing choices affect embedding quality and downstream model performance