---
ver: rpa2
title: 'LLM-Explorer: Towards Efficient and Affordable LLM-based Exploration for Mobile
  Apps'
arxiv_id: '2505.10593'
source_url: https://arxiv.org/abs/2505.10593
tags:
- llm-explorer
- exploration
- knowledge
- state
- abstract
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-Explorer addresses the inefficiency and high cost of existing
  LLM-based mobile app exploration by introducing a novel approach that leverages
  LLMs primarily for maintaining app knowledge rather than generating actions. The
  system organizes app knowledge through abstract UI states, elements, and actions,
  creating an Abstract Interaction Graph to guide exploration.
---

# LLM-Explorer: Towards Efficient and Affordable LLM-based Exploration for Mobile Apps

## Quick Facts
- **arXiv ID:** 2505.10593
- **Source URL:** https://arxiv.org/abs/2505.10593
- **Reference count:** 40
- **Key outcome:** Achieves 4%-35% higher activity coverage than five strong baselines while reducing LLM costs by 148x, with exploration efficiency approaching human performance on simpler apps.

## Executive Summary
LLM-Explorer addresses the inefficiency and high cost of existing LLM-based mobile app exploration by introducing a novel approach that leverages LLMs primarily for maintaining app knowledge rather than generating actions. The system organizes app knowledge through abstract UI states, elements, and actions, creating an Abstract Interaction Graph to guide exploration. Knowledge-guided exploration then selects actions based on this structured knowledge, with LLMs only occasionally invoked for complex tasks like text input generation. Tested on 20 typical apps, LLM-Explorer achieved 4%-35% higher activity coverage than five strong baselines while reducing LLM costs by 148x compared to state-of-the-art approaches, with exploration efficiency approaching human performance on simpler apps.

## Method Summary
LLM-Explorer introduces a two-module system: (1) LLM-assisted Knowledge Maintenance that builds abstract UI states/actions and an Abstract Interaction Graph (AIG) using rule-based state matching and LLM grouping of similar elements; (2) Knowledge-guided Exploration that uses an app-wide action selector, fault-tolerant path finder on AIG, and LLM-based text input generation. The approach decouples knowledge organization (LLM-heavy) from action selection (LLM-less), reducing token costs while maintaining coverage. The system was implemented on DroidBot with GPT-3.5-turbo-1106 and evaluated over 2-hour runs across 20 apps from F-Droid and Google Play.

## Key Results
- 4%-35% higher activity coverage than five strong baselines
- 148x reduction in LLM costs compared to state-of-the-art approaches
- Exploration efficiency approaching human performance on simpler apps

## Why This Works (Mechanism)

### Mechanism 1: Semantic State and Action Abstraction
- **Claim:** Replacing raw UI states with semantically abstract representations reduces the exploration search space and prevents redundant interactions.
- **Mechanism:** The system groups dynamically changing UI states (e.g., a contact list with different names) into a single Abstract UI State. Similarly, it clusters repetitive elements (e.g., number pads) into Abstract UI Actions. This creates a compact Abstract Interaction Graph (AIG) rather than an infinite raw UI transition graph.
- **Core assumption:** Functionally similar UI elements can be reliably identified and grouped by an LLM or simple structural rules without losing access to distinct unique functionalities.
- **Evidence anchors:** [abstract] "The system organizes app knowledge through abstract UI states, elements, and actions, creating an Abstract Interaction Graph." [section 3.2.1] "Abstract UI actions group together similar user interactions... identifying repetitive components... that usually result in the same abstract UI state."
- **Break condition:** If an app relies on subtle visual differences in seemingly identical elements to trigger different behaviors, the abstraction will merge them, causing the explorer to miss unique activities (false negatives in exploration).

### Mechanism 2: Decoupling Knowledge Maintenance from Action Selection
- **Claim:** Separating the "mapping" process (LLM-heavy) from the "driving" process (LLM-less) drastically reduces token costs and latency while maintaining coverage.
- **Mechanism:** LLMs are invoked only when the app knowledge needs updating (discovering a new abstract state) or for complex text input. Action selection is handled by a deterministic policy that picks "unexplored" actions from the AIG, avoiding the per-step LLM query tax.
- **Core assumption:** The exploration bottleneck is primarily the cost and latency of per-step LLM inference, not the quality of the LLM's planning reasoning.
- **Evidence anchors:** [abstract] "LLM-Explorer uses LLMs primarily for maintaining the knowledge instead of generating actions... reducing LLM costs by 148x." [section 3.4] "LLM queries... are determined by the number of unique states/actions... guaranteed to be smaller than the number of steps."
- **Break condition:** If an app has a highly non-deterministic state space where every step reveals a genuinely new abstract structure (e.g., infinite scrolling feeds with unique semantic contexts per item), LLM query frequency will approach per-step levels, negating cost savings.

### Mechanism 3: Global Action Prioritization via Fault-Tolerant Navigation
- **Claim:** Selecting actions based on a global "unexplored" list across the entire app, rather than just the current screen, improves coverage speed and thoroughness.
- **Mechanism:** The App-wide Action Selector picks any unexplored abstract action from the knowledge base. If the required UI element is not on the current screen, the system uses the AIG to compute a navigation path (sequence of actions) to reach the target state.
- **Core assumption:** The AIG accurately reflects the app's navigational logic, allowing the agent to reliably return to specific states to execute deferred actions.
- **Evidence anchors:** [section 3.3.2] "LLM-Explorer selects and executes an abstract UI action from the app knowledge... This differs significantly from existing methods that focus on UI state granularity." [section 3.3.3] "Should the shortest path fail, the system will attempt alternative paths... LLM-Explorer will restart the app."
- **Break condition:** If the app requires complex cross-app interactions or deep linking to reach certain states, and the AIG path is broken or non-deterministic, the navigation module fails and marks reachable actions as impossible.

## Foundational Learning

- **Concept: UI Transition Graph (UTG) vs. Abstract Interaction Graph (AIG)**
  - **Why needed here:** The core innovation is compressing the raw UTG (which can be infinite due to dynamic data) into a finite AIG. Without understanding this, the "knowledge maintenance" module seems like just a logger.
  - **Quick check question:** How does LLM-Explorer handle a "Contacts" page where the list of names changes every time you open it?

- **Concept: Activity Coverage**
  - **Why needed here:** This is the primary success metric used in the paper. It measures the percentage of unique app screens (Activities) reached, distinct from code coverage.
  - **Quick check question:** Why is "Activity Coverage" a preferred metric over "Code Coverage" for this type of black-box GUI testing?

- **Concept: LLM Token Economics**
  - **Why needed here:** The paper claims a 148x cost reduction. Understanding why requires knowing that LLM costs scale with token count and frequency of queries.
  - **Quick check question:** Which specific operation does LLM-Explorer avoid doing on every step that causes other agents (like GPTDroid) to be expensive?

## Architecture Onboarding

- **Component map:**
  Raw UI Trace (Screenshots/Hierarchy) -> LLM-assisted Knowledge Maintenance -> Abstract UI States/Actions/AIG -> Knowledge-guided Exploration -> Activity Coverage
  Knowledge-guided Exploration: App-wide Action Selector -> Fault-tolerant Navigation Path Finder -> Content-aware Input Text Generator -> Execute Action

- **Critical path:**
  Observe Screen -> (Check if New Abstract State) -> [LLM Query for Abstraction] -> Update AIG -> Select Unexplored Global Action -> (Check if On Screen) -> [Navigate via AIG] -> Execute Action.

- **Design tradeoffs:**
  - **Abstraction Granularity:** The rule-based state merging is faster/cheaper but less precise than LLM-based merging. The paper notes LLM-merging was tried but found less effective/slower (Section 3.2.2).
  - **Creativity vs. Cost:** By removing LLMs from action selection, the agent loses "semantic reasoning" at the moment of choice (e.g., "I should click 'Settings' because I want to change volume") and relies on systematic coverage (e.g., "I click 'Settings' because I haven't clicked it yet").

- **Failure signatures:**
  - **Over-abstraction:** Merging elements with different functions (e.g., "Privacy Policy" and "Licenses" buttons grouped as "Footer Links") results in lost coverage (Section 4.5).
  - **Navigation Loops:** Fault-tolerant navigation fails on non-deterministic transitions, leading to app restarts and lost session state.

- **First 3 experiments:**
  1. **Sanity Check:** Run LLM-Explorer on a simple app (e.g., Calculator) and visualize the AIG to verify that number buttons are grouped into single abstract actions.
  2. **Stress Test:** Run on an app with a dynamic feed (e.g., a social media app) to observe if the "New Abstract State" logic triggers too frequently, spiking costs.
  3. **Ablation:** Disable the "App-wide Action Selector" and force local-only choices to measure the drop in coverage speed.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can cross-app navigation be effectively integrated to allow exploration of activities requiring external app transitions?
- **Basis in paper:** [explicit] The authors note that LLM-Explorer "performed poorly for activities that need to be navigated across apps" and suggest merging abstract interaction graphs as a potential solution.
- **Why unresolved:** The current implementation restricts the agent to the target app, automatically returning when a transition occurs, which prevents reaching valid activities triggered by external intents.
- **Evidence:** A comparative study measuring coverage on activities known to require cross-app jumps before and after implementing a unified abstract interaction graph.

### Open Question 2
- **Question:** Can Visual Language Models (VLMs) be utilized for knowledge maintenance without negating the system's affordability?
- **Basis in paper:** [explicit] The authors suggest incorporating visual modalities (screenshots) to improve knowledge quality but note this "typically leads to increased token consumption."
- **Why unresolved:** While VLMs could better interpret icons and visual layouts to reduce abstraction errors, they currently impose a cost/efficiency trade-off that contradicts the paper's primary goal of affordability.
- **Evidence:** An ablation study comparing the cost-to-coverage ratio of text-based LLMs versus multimodal models for the knowledge organization task.

### Open Question 3
- **Question:** How can the element abstraction mechanism be refined to prevent the loss of distinct functional paths?
- **Basis in paper:** [explicit] The paper identifies a failure mode where the LLM "falsely classifies elements with different functions into the same group," causing the explorer to ignore distinct UI actions.
- **Why unresolved:** The current approach relies on LLM summarization which occasionally over-generalizes semantic similarity, causing the system to treat distinct buttons (e.g., "Privacy" vs "Licenses") as identical.
- **Evidence:** A modified validation step that verifies transition outcomes before merging elements, showing a reduction in missed activities.

## Limitations
- **Abstraction granularity:** May merge elements with different functions, causing missed coverage of unique activities.
- **Cross-app navigation:** Limited by automatic app return behavior, preventing exploration of activities requiring external app transitions.
- **Cost savings dependency:** 148x cost reduction claim depends on app state space structure and may not hold for highly non-deterministic apps.

## Confidence
- **High Confidence:** The 4%-35% activity coverage improvement over five baselines is well-supported by the experimental setup and metrics. The core mechanism of decoupling knowledge maintenance from action selection is clearly explained and logically sound.
- **Medium Confidence:** The 148x cost reduction claim, while supported by the methodology, depends on specific app characteristics and token usage patterns that weren't fully detailed. The comparison to human exploration efficiency is more qualitative and harder to verify.
- **Low Confidence:** The long-term scalability of the approach on apps with extremely large state spaces or those requiring sophisticated semantic reasoning for exploration decisions remains uncertain.

## Next Checks
1. **Ablation Study on Abstraction Granularity:** Run LLM-Explorer on an app where subtle UI differences matter (e.g., a form with similarly named fields triggering different validations) and measure the impact of over-abstraction on coverage compared to a baseline without abstraction.

2. **Cost Analysis on Dynamic Content Apps:** Test the system on apps with infinite scrolling feeds or real-time updates to verify if the 148x cost reduction holds when the app knowledge needs frequent updating.

3. **Navigation Robustness Test:** Create a test app with complex cross-app interactions and deep linking to evaluate how often the fault-tolerant navigation fails and the system resorts to app restarts, and measure the impact on coverage and session state retention.