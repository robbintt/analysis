---
ver: rpa2
title: Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function
  Approximation
arxiv_id: '2510.01721'
source_url: https://arxiv.org/abs/2510.01721
tags:
- robust
- function
- dual
- uncertainty
- kmix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first finite-time analysis of distributionally
  robust temporal-difference learning with linear function approximation. The algorithm
  tackles model uncertainty by optimizing the worst-case discounted reward within
  a prescribed uncertainty set, using data from a nominal model while ensuring robustness
  to deviations.
---

# Finite-Time Bounds for Distributionally Robust TD Learning with Linear Function Approximation
## Quick Facts
- arXiv ID: 2510.01721
- Source URL: https://arxiv.org/abs/2510.01721
- Reference count: 40
- This paper introduces the first finite-time analysis of distributionally robust temporal-difference learning with linear function approximation, achieving $\tilde{O}(1/\epsilon^2)$ sample complexity.

## Executive Summary
This work establishes the first finite-time convergence guarantees for distributionally robust temporal-difference (TD) learning with linear function approximation. The algorithm tackles model uncertainty by optimizing the worst-case discounted reward within a prescribed uncertainty set, using data from a nominal model while ensuring robustness to deviations. The key innovation is a two-time-scale stochastic approximation scheme that handles the inner optimization via dual formulation with function approximation in both the value and dual spaces, combined with a target-network mechanism to overcome non-contraction issues in the projected robust Bellman operator. The method achieves an $\tilde{O}(1/\epsilon^2)$ sample complexity to obtain an $\epsilon$-accurate value estimate, closing a significant gap between empirical success and theoretical guarantees in robust RL.

## Method Summary
The algorithm employs a two-time-scale stochastic approximation framework with an outer-loop target network. The fast time-scale updates dual variables to solve the inner robust optimization problem, while the slow time-scale performs TD learning updates using a robust target computed from the averaged fast-loop iterates. Linear function approximation is used in both the value and dual spaces, with suffix-averaging of dual parameters to reduce variance. The target network stabilizes convergence by converting a non-contractive projected robust Bellman operator into a sequence of tractable fixed-point problems.

## Key Results
- Achieves $\tilde{O}(1/\epsilon^2)$ sample complexity for $\epsilon$-accurate value estimates
- First finite-time analysis of distributionally robust TD learning with linear function approximation
- Extends to robust Q-learning with function approximation

## Why This Works (Mechanism)

### Mechanism 1
A target network stabilizes convergence by converting a non-contractive projected robust Bellman operator into a sequence of tractable fixed-point problems. The algorithm freezes a target parameter $\hat{\theta}_t$ for an outer loop epoch, solving $\Phi\theta \approx \Pi T_r^\pi(\Phi\hat{\theta}_t)$. This decouples the recursive dependency that breaks contraction in standard TD updates, effectively treating the robust Bellman update as a time-varying linear system.

### Mechanism 2
A two-time-scale stochastic approximation decouples the error in estimating the worst-case transition (inner optimization) from the error in value function learning (outer optimization). The fast loop updates dual variables $\nu$ (approximating the robust penalty) using a super-gradient ascent. The slow loop updates value weights $\theta$ using a TD target that relies on the averaged fast-loop iterates. This ensures the value update sees a low-variance approximation of the worst-case cost.

### Mechanism 3
Linear function approximation in the dual space generalizes the worst-case transition probability constraints across state-action pairs. Instead of maintaining a tabular dual variable $\lambda_{s,a}$ for every pair (infeasible for large $|S|$), the method parameterizes $\lambda_{s,a} \approx \psi(s,a)^\top \nu$. This allows the algorithm to infer robustness constraints for unseen states based on their features.

## Foundational Learning

**Projected Bellman Operators**
- Why needed here: Standard TD learning converges to the fixed point of the Bellman operator. With function approximation, one must analyze the *projected* operator $\Pi T_r^\pi$, which loses the contraction property essential for standard convergence proofs.
- Quick check question: Can you explain why projecting a contraction mapping onto a linear subspace might destroy its contraction property in the $\ell_\infty$ norm?

**Dual Formulation of Robust Optimization**
- Why needed here: The core difficulty is minimizing $q^\top V$ over an uncertainty set $\mathcal{P}$. The paper transforms this primal minimization into a dual maximization problem to derive a stochastic super-gradient estimator.
- Quick check question: Why is a super-gradient estimator derived from a dual formulation often more "sample-efficient" for stochastic approximation than trying to sample directly from the worst-case primal distribution?

**Markov Chain Mixing Times**
- Why needed here: The finite-time analysis relies on "lagged" filtration arguments where the state distribution must be close to stationary after $\tau$ steps. The bound depends on the mixing rate $\rho$.
- Quick check question: If the policy induces a periodic Markov chain rather than an irreducible aperiodic one, which assumption in Theorem 1 is violated?

## Architecture Onboarding

**Component map:**
Outer Loop ($T$) -> Updates Target Network weights $\hat{\theta}$
Inner Loop ($K$) -> Fast Scale ($\beta$) -> Updates Dual Weights $\nu$ via projected super-gradient
Inner Loop ($K$) -> Slow Scale ($\alpha$) -> Updates Value Weights $\theta$ via semi-gradient TD
Buffers -> Half-tail averaging buffer for $\nu$ to reduce variance for the slow scale

**Critical path:** Sample $(S_k, A_k, S_{k+1})$ -> Compute target value using frozen $\hat{\theta}$ -> Fast loop updates $\nu$ (robustness estimate) -> Compute robust TD target using averaged $\bar{\nu}$ -> Slow loop updates $\theta$

**Design tradeoffs:**
- Uncertainty Set: Total Variation (TV) allows simpler gradient computation (Eq. 17) compared to Wasserstein-$\ell$ (Eq. 20), which requires a minimization over all states $y$ at every step
- Step Sizes: Theorem 1 shows that for $c\mu < 2$, convergence is slower ($\tilde{O}(1/\epsilon^4)$ implied by rates), while $c\mu \geq 2$ yields better complexity but requires tuning $c$ carefully based on unknown constants

**Failure signatures:**
- Divergence: If $\alpha_k$ is too large relative to the smallest eigenvalue $\mu$ of the feature covariance, the mean drift term dominates
- Lagging: If $K$ (inner iterations) is insufficient, the target network updates before the inner loop solves the fixed point, causing oscillation
- Non-contraction instability: If the target network update period $K$ is too small or step sizes are too aggressive

**First 3 experiments:**
1. Sanity Check (Tabular): Run on a small chain MDP using one-hot features (where $\Psi$ and $\Phi$ are identity) to verify the algorithm recovers tabular Robust TD results
2. Ablation on Averaging: Disable the half-tail averaging of $\nu$ (use current $\nu_{t,k}$ directly) to demonstrate the variance explosion predicted in the analysis of the "noise term"
3. Wasserstein vs. TV: Compare sample complexity on a "FrozenLake" style environment where the transition uncertainty is structured (favoring Wasserstein) vs. unstructured (favoring TV)

## Open Questions the Paper Calls Out

**Open Question 1**
Can the computational complexity of calculating the super-gradient for the Wasserstein-$\ell$ uncertainty set be reduced for large state spaces? The super-gradient computation (Eq. 20) requires a minimization over all states, which becomes prohibitive as the state-space cardinality increases.

**Open Question 2**
Can finite-time bounds be established for the Cressie-Read family of f-divergences within this framework? Extending the results to Cressie-Read f-divergences would require the addition of one more timescale, which is not analyzed in the current work.

**Open Question 3**
Do the convergence guarantees extend to robust TD learning with non-linear function approximation (e.g., deep neural networks)? The current proofs rely on the properties of linear projection and feature matrices ($\Phi$), which do not hold directly for non-linear representations.

## Limitations

- Theoretical analysis is limited to finite state-action spaces with full-rank features, restricting direct applicability to large-scale problems
- The algorithm's performance hinges critically on the choice of uncertainty set and corresponding hyperparameters
- The projection radius $B_\nu$ for dual variables lacks a clear heuristic for practical selection

## Confidence

**High Confidence:**
- The sample complexity bound of $\tilde{O}(1/\epsilon^2)$ is rigorously proven under stated assumptions (Theorem 1)
- The two-time-scale mechanism effectively decouples dual estimation from value learning (Mechanism 2)
- The target network approach addresses the non-contraction issue of the projected robust Bellman operator (Mechanism 1)

**Medium Confidence:**
- The linear dual function approximation generalizes well across state-action pairs (Mechanism 3)
- The suffix-averaging of dual parameters sufficiently reduces variance in the slow loop updates
- The algorithm performs comparably to tabular methods in small-scale verification experiments

**Low Confidence:**
- Performance on large-scale continuous control tasks with neural network function approximation
- Behavior under misspecified uncertainty sets or when the true model deviates significantly from the nominal model
- Computational feasibility of the Wasserstein-$\ell$ variant for problems with large state spaces

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary the projection radius $B_\nu$, step sizes $\alpha$ and $\beta$, and inner loop length $K$ across multiple environments to map the algorithm's performance landscape and identify stable operating regimes.

2. **Large-Scale Empirical Evaluation**: Implement the TV variant on benchmark RL environments (e.g., CartPole, Acrobot) with learned linear features to assess whether the theoretical $\tilde{O}(1/\epsilon^2)$ sample complexity translates to practical sample efficiency gains over non-robust baselines.

3. **Uncertainty Set Robustness Test**: Construct a gridworld with structured transition noise and compare the performance of TV vs Wasserstein variants. Measure both the achieved robustness (worst-case performance across perturbed models) and the computational overhead to quantify the trade-off between robustness guarantees and implementation complexity.