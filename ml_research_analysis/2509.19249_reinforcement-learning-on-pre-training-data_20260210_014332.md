---
ver: rpa2
title: Reinforcement Learning on Pre-Training Data
arxiv_id: '2509.19249'
source_url: https://arxiv.org/abs/2509.19249
tags:
- uni00000015
- uni00000011
- reasoning
- rlpt
- uni00000017
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new training-time scaling paradigm for large
  language models called Reinforcement Learning on Pre-Training data (RLPT). The method
  addresses the challenge of finite high-quality text data constraining conventional
  scaling approaches.
---

# Reinforcement Learning on Pre-Training Data

## Quick Facts
- arXiv ID: 2509.19249
- Source URL: https://arxiv.org/abs/2509.19249
- Reference count: 9
- This paper proposes Reinforcement Learning on Pre-Training data (RLPT) to address finite high-quality text data constraints in large language model training.

## Executive Summary
This paper introduces Reinforcement Learning on Pre-Training data (RLPT), a novel training-time scaling paradigm that addresses the limitation of finite high-quality text data in large language model training. RLPT enables autonomous exploration of meaningful reasoning trajectories through reinforcement learning on pre-training data, using a next-segment reasoning objective that rewards accurate prediction of subsequent text segments. The method eliminates the need for human annotation required in existing reinforcement learning approaches.

The approach is demonstrated on Qwen3-4B-Base, showing substantial performance improvements across multiple reasoning and knowledge benchmarks including MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and AIME25. The results indicate favorable scaling behavior with training compute and suggest that RLPT provides a strong foundation that extends reasoning boundaries while enhancing subsequent reinforcement learning from human feedback (RLHF) performance.

## Method Summary
RLPT introduces a next-segment reasoning objective that uses reinforcement learning to guide models in predicting subsequent text segments within pre-training data. The method autonomously discovers meaningful reasoning trajectories by rewarding accurate segment prediction, eliminating the need for human-annotated data typically required in reinforcement learning approaches. This enables the model to explore diverse reasoning paths and improve its ability to generate coherent, logically consistent responses.

The approach leverages the inherent structure of pre-training data to create a self-supervised reinforcement learning environment where the model learns to navigate between related text segments. By focusing on next-segment prediction as the reward signal, RLPT encourages the development of more sophisticated reasoning capabilities while maintaining alignment with the original pre-training corpus structure.

## Key Results
- RLPT yields absolute improvements of 3.0 points on MMLU benchmark
- Model shows 8.1 point improvement on GPQA-Diamond benchmark
- Demonstrated 6.6 and 5.3 point gains on AIME24 and AIME25 respectively

## Why This Works (Mechanism)
RLPT works by transforming pre-training data into a reinforcement learning environment where models learn to navigate between related text segments. The next-segment reasoning objective creates a natural reward structure that encourages coherent reasoning paths, as accurate prediction of subsequent segments indicates successful understanding of logical connections. This approach leverages the existing knowledge in pre-training data while enabling autonomous discovery of reasoning strategies that generalize beyond simple next-token prediction.

The method addresses the fundamental limitation of finite high-quality data by enabling models to explore and extract more value from existing pre-training corpora. Rather than requiring additional annotated data or synthetic examples, RLPT allows models to learn from the inherent structure and relationships within their training data, effectively increasing the utility of finite resources.

## Foundational Learning
- **Reinforcement Learning**: Used to optimize model behavior through reward signals rather than supervised loss functions
  - Why needed: Enables autonomous discovery of reasoning strategies without human annotation
  - Quick check: Verify that reward signal properly captures meaningful reasoning progress

- **Pre-training Data Structure**: Leverages inherent relationships between text segments in training corpora
  - Why needed: Provides the foundation for next-segment reasoning objectives
  - Quick check: Ensure pre-training data contains sufficient logical connections for navigation

- **Next-segment Prediction**: Focuses on predicting subsequent text segments rather than individual tokens
  - Why needed: Encourages coherent reasoning paths and logical consistency
  - Quick check: Validate that segment boundaries align with meaningful reasoning transitions

## Architecture Onboarding
Component map: Pre-training Data -> Segment Extractor -> RL Agent -> Reward Module -> Model Parameters -> Updated Model

Critical path: Pre-training data is segmented and fed into the RL agent, which generates predictions for next segments. The reward module evaluates prediction accuracy and provides feedback to update model parameters through reinforcement learning. The updated model is then evaluated on downstream benchmarks.

Design tradeoffs: The method trades off computational efficiency during training for improved reasoning capabilities, as reinforcement learning typically requires more compute than standard fine-tuning. Additionally, the approach balances exploration of diverse reasoning paths against maintaining coherence with pre-training data structure.

Failure signatures: Poor performance may indicate issues with reward signal design, inadequate pre-training data structure for meaningful segment relationships, or insufficient exploration of the reasoning trajectory space. Degradations in general capabilities could suggest over-specialization to the specific pre-training corpus structure.

First experiments:
1. Ablation study on reward shaping parameters to isolate their impact on performance gains
2. Comparison of different segment boundary definitions and their effect on reasoning quality
3. Analysis of reasoning trajectory diversity before and after RLPT training

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation scope is narrow, focusing primarily on reasoning and knowledge benchmarks without assessing general capabilities or safety considerations
- Claims about eliminating human annotation needs should be viewed cautiously as reward shaping still requires careful validation
- Scaling analysis based only on Qwen3-4B model limits generalizability to other model sizes

## Confidence
- Performance improvements on reasoning benchmarks: Medium confidence due to limited implementation details
- Elimination of human annotation requirement: Low confidence given need for reward shaping validation
- Favorable scaling behavior claims: Medium confidence due to single model scale evaluation

## Next Checks
1. Replicate RLPT training process with detailed ablation studies on reward shaping, trajectory selection, and hyperparameter sensitivity
2. Evaluate models on broader benchmark suite including general language understanding, safety metrics, and diverse reasoning types
3. Test method across multiple model scales to verify claimed scaling behavior and determine if improvements are consistent or scale-dependent