---
ver: rpa2
title: First-Order Representation Languages for Goal-Conditioned RL
arxiv_id: '2512.19355'
source_url: https://arxiv.org/abs/2512.19355
tags:
- goal
- state
- learning
- lifted
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces three variants of Hindsight Experience Replay
  (HER) for generalized planning: state HER, propositional HER, and lifted HER. The
  core idea is to relabel unsuccessful trajectories with alternative goals to enable
  more data-efficient learning in sparse reward environments.'
---

# First-Order Representation Languages for Goal-Conditioned RL

## Quick Facts
- arXiv ID: 2512.19355
- Source URL: https://arxiv.org/abs/2512.19355
- Reference count: 18
- Propositional and lifted HER achieve 82.4% coverage on unseen test instances vs 51.0% for state HER

## Executive Summary
This paper introduces three variants of Hindsight Experience Replay (HER) for generalized planning in goal-conditioned reinforcement learning. The core innovation is relabeling unsuccessful trajectories with alternative goals to enable more data-efficient learning in sparse reward environments. By exploiting first-order relational representations, the method generates curriculum-like learning sequences by progressively identifying harder goals. The approach significantly outperforms previous methods, solving 82.4% of test instances compared to 51.0% for state HER, while often producing shorter solutions than baseline planners.

## Method Summary
The method extends HER to first-order relational representations for generalized planning. Three variants are introduced: state HER (relabels to full achieved states), propositional HER (relabels to subsets of goal atoms), and lifted HER (relabels to first-order goal patterns). The core mechanism relabels unsuccessful trajectories by replacing the original goal with one that was actually achieved, transforming zero-reward trajectories into supervised training data. A relational graph neural network (R-GNN) with 100 layers processes variable-size inputs, while the relabeling mechanism automatically creates curricula of increasing complexity. The approach is evaluated across 18 STRIPS planning domains, showing significant improvements in both coverage and solution quality compared to prior methods.

## Key Results
- Propositional HER solves 82.4% of test instances versus 51.0% for state HER
- Lifted HER solves 75.7% of test instances, showing strong generalization across domains
- Learned policies often produce shorter solutions than the baseline LAMA planner while solving more instances overall
- The method handles domains with 20-50 blocks, significantly larger than prior methods limited to 8 blocks

## Why This Works (Mechanism)

### Mechanism 1: Hindsight Goal Relabeling for Sparse Reward Credit Assignment
Reinterpreting failed trajectories as successful for achieved goals provides dense learning signals in sparse-reward environments. When an episode fails to reach goal G, HER substitutes G′ (a state actually reached) as the new goal, then computes rewards and Q-targets relative to G′. This transforms a zero-reward trajectory into supervised training data for goal-conditioned Q-values. The mechanism assumes Q-functions can generalize across different (state, goal) pairs, enabling skills learned for simpler goals to transfer to harder goals.

### Mechanism 2: Automatic Curriculum via Progressive Subgoal Expansion
Propositional and lifted HER implicitly construct curricula by generating subgoals of increasing complexity as agent capability improves. Early in training, random exploration achieves only small goal subsets, which become hindsight goals. As policy improves, it achieves larger subsets, which become new goals. Goal size and trajectory length increase over training episodes, creating an automatic progression from easier to harder goals without manual curriculum design.

### Mechanism 3: First-Order Abstraction for Cross-Instance Generalization
Lifting propositional goals to first-order schemas with variables enables policy learning that generalizes to unseen object configurations. The goal-dependency graph identifies connected goal atom clusters, and constants are replaced with variables plus inequality constraints. The grounding function finds instantiations satisfied in achieved states, allowing trajectories with specific objects to train policies for goal patterns that apply across instances.

## Foundational Learning

- **Concept: Goal-Conditioned Q-Learning**
  - Why needed: The entire method builds on Q(s, a, G)—estimating expected return for action a in state s given goal G. Without understanding how goals condition value functions, the relabeling mechanism is opaque.
  - Quick check: Given Q(s, a, G) and a new goal G′ achieved from s, how should Q(s, a, G′) relate to the trajectory actually taken?

- **Concept: STRIPS/PDDL First-Order Representations**
  - Why needed: States and goals are sets of ground atoms; actions are grounded schemas. The distinction between lifted (variables) and ground (constants) representations is essential for lifted HER.
  - Quick check: What is the difference between ON(a, b) as a ground atom versus ON(X, Y) as a lifted atom? What additional constraint is needed?

- **Concept: Relational Graph Neural Networks (R-GNN)**
  - Why needed: The Q-function must handle varying numbers of objects and applicable actions. R-GNNs process sets of atoms and generate object embeddings aggregated for Q-value prediction.
  - Quick check: Why can't a standard MLP directly compute Q(s, a, G) when s and G are sets of atoms?

## Architecture Onboarding

- **Component map**: Input Encoder -> R-GNN Backbone -> Q-Head -> HER Refinement Module -> Replay Buffer
- **Critical path**: Environment step produces (s_t, a_t, r_t, s_{t+1}, G) → HER refinement extracts cycle-free subtrajectories, computes hindsight goals → Relabeled transitions stored in replay buffer → Mini-batch sampling → R-GNN forward pass → Q-values for all applicable actions → Huber loss on Q(s_t, a_t, G) vs. y_t = r_t + γ·max_{a′} Q(s_{t+1}, a′, G)
- **Design tradeoffs**: State HER is simplest but relabels to full states (often harder than original goal). Propositional HER restricts to goal subsets but fails when goal is single atom. Lifted HER generalizes across objects but requires goal-dependency graph construction. 100 R-GNN layers with auxiliary readout needed for large grids but vanilla training fails.
- **Failure signatures**: Childsnack/Spanner dead-end relabeling converts dead-ends into successes for partial goals, providing no signal to avoid dead-ends. Delivery multi-package failure: random exploration almost never delivers multiple packages to same location. Propositional HER in Maze: single location goal provides no proper subsets. Large grid communication failure: 100-layer R-GNN may not propagate information across distant cells.
- **First 3 experiments**:
  1. **Blocks domain comparison**: Run all three HER variants on Blocks (14-block training, 20-50 block test). Expect lifted HER to learn fastest with lowest variance; state HER to plateau lower. Verify curriculum by logging goal size over episodes.
  2. **Ablation on goal size**: Construct synthetic task where goal has controlled number of atoms (1, 3, 5, 10). Measure propositional HER success rate vs. goal size. Hypothesis: single-atom goals show no propositional HER advantage.
  3. **Dead-end diagnosis on Childsnack**: Log proportion of relabeled goals that correspond to dead-end states. Add small penalty for entering known dead-end configurations. Measure impact on test coverage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Hindsight Experience Replay (HER) variants be modified to handle dead-end states without erroneously relabeling them as successful subgoals?
- Basis in paper: The authors note that "every trajectory ending in a dead-end is relabeled as a success for the relabeled goal," resulting in no training data on how to avoid dead-ends in domains like Childsnack.
- Why unresolved: The current relabeling mechanism converts dead-end trajectories into positive training examples for easier goals, inadvertently encouraging policies that lead to unrecoverable states.
- What evidence would resolve it: A modified HER algorithm that identifies dead-ends and penalizes or filters them during relabeling, resulting in improved coverage in domains with irreversible failures.

### Open Question 2
- Question: Can advanced exploration strategies be integrated with lifted HER to overcome learning failures in domains where random exploration rarely achieves informative states?
- Basis in paper: The paper states that in the Hiking domain, "the probability of generating informative trajectories through random exploration is simply too low for effective learning."
- Why unresolved: The current method relies on Boltzmann exploration, which fails to generate the diverse trajectories necessary for the relabeling mechanism to function in these specific environments.
- What evidence would resolve it: Demonstrating that combining intrinsic motivation or count-based exploration with lifted HER enables learning in the Hiking domain.

### Open Question 3
- Question: Does increasing the effective receptive field or depth of the Relational Graph Neural Network (R-GNN) resolve performance degradation in large-scale grid domains?
- Basis in paper: The authors hypothesize that decreasing coverage in Visitall and Reward is "due to the model's depth: with larger grids, distant cells may not communicate effectively."
- Why unresolved: While the authors suspect a limit in the GNN's message-passing capabilities, they do not experimentally validate if deeper networks or long-range attention mechanisms solve this scaling issue.
- What evidence would resolve it: Experiments showing that R-GNNs with increased depth or global attention maintain high coverage as grid size increases.

## Limitations
- The relabeling mechanism can mask pathological states like dead-ends, potentially preventing learning to avoid unrecoverable configurations
- Scalability to truly large domains (hundreds of objects) remains unproven despite handling 20-50 block instances
- Performance depends on careful construction of goal-dependency graphs; failure cases when graphs are disconnected are not fully explored

## Confidence
- **High confidence**: The core mechanism of hindsight goal relabeling providing dense learning signals in sparse reward environments is well-established and theoretically sound
- **Medium confidence**: The automatic curriculum generation through progressive subgoal expansion is observed empirically but lacks strong theoretical guarantees
- **Medium confidence**: The first-order abstraction enabling cross-instance generalization is mechanistically plausible but requires careful construction of goal-dependency graphs

## Next Checks
1. **Dead-end avoidance diagnosis**: Instrument the algorithm to track and penalize trajectories ending in known dead-end configurations. Measure whether this improves test coverage on instances requiring dead-end avoidance strategies.
2. **Curriculum strength quantification**: Design controlled experiments varying the structural complexity of goals (number of atoms, connectivity) and measure how quickly each HER variant learns to achieve increasingly complex goals during training.
3. **Scalability stress test**: Evaluate the method on significantly larger instances (100+ blocks or grid cells) to identify whether the 100-layer R-GNN architecture hits fundamental limits in information propagation across large state spaces.