---
ver: rpa2
title: Do You Really Need Public Data? Surrogate Public Data for Differential Privacy
  on Tabular Data
arxiv_id: '2504.14368'
source_url: https://arxiv.org/abs/2504.14368
tags:
- agent
- data
- unif
- public
- claude
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces surrogate public data for differential privacy
  on tabular data. The core idea is to use large language models to generate synthetic
  data from schema-level metadata (variable names, types, ranges) without accessing
  sensitive records, creating a privacy-preserving alternative to traditional public
  data.
---

# Do You Really Need Public Data? Surrogate Public Data for Differential Privacy on Tabular Data

## Quick Facts
- arXiv ID: 2504.14368
- Source URL: https://arxiv.org/abs/2504.14368
- Authors: Shlomi Hod; Lucas Rosenblatt; Julia Stoyanovich
- Reference count: 40
- Primary result: LLM-generated surrogate data significantly improves DP classification pretraining, especially for smaller datasets under 10k records

## Executive Summary
This paper addresses the challenge of heterogeneous public data availability in differentially private machine learning by introducing LLM-generated surrogate public data. The authors propose two methods—direct CSV generation and agent-based structural causal model construction—that create synthetic tabular data using only schema-level metadata without accessing sensitive records. Their evaluation across three real-world datasets demonstrates that surrogate data effectively replaces traditional public data for pretraining differentially private classifiers, with particular benefits for smaller datasets and lower privacy budgets. While showing promise for hyperparameter tuning, surrogate data still underperforms traditional public data for privacy-utility tradeoff estimation.

## Method Summary
The authors automate surrogate public data generation using large language models through two approaches: CSV direct generation (single prompt producing tabular records) and an agent-based structural causal model construction (11-step state machine producing DAGs and structural equations). Both methods operate solely on schema-level metadata—variable names, types, ranges, and brief descriptions. For evaluation, they pretrain FTTransformer models on surrogate data, then fine-tune using DP-SGD on private data across multiple epsilon values (1, 2, 4, 8, 16). They also assess surrogate data for hyperparameter tuning of DP synthesizers and privacy-utility tradeoff estimation using three datasets: ACS (7 features), EDAD (11 features), and WE (12 features).

## Key Results
- Surrogate public data pretraining provides 0.10-0.17 AUC advantage over no pretraining for DP classification, particularly effective for datasets under 10k records
- For hyperparameter tuning, the "Arbitrary" random Bayesian network baseline performs nearly as well as LLM-generated SCMs, suggesting structured dependencies may matter more than distributional accuracy
- Privacy-utility tradeoff estimation still performs best with traditional public data, indicating dataset similarity remains important for this task
- Claude 3.5 Sonnet outperformed GPT-4o and Llama 3.3 in generating high-quality surrogate data across all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1: LLM Statistical Knowledge Encoding
LLMs trained on diverse corpora encode statistical knowledge about variable relationships; when prompted with schema (variable names, types, permissible ranges), they generate records reflecting plausible distributions and inter-variable dependencies. This works because LLMs possess statistical priors from training on tabular data across domains. Break condition: when the private data domain is novel or severely underrepresented in LLM training data.

### Mechanism 2: Agent-Based Structural Causal Modeling
A state machine guides the LLM through 11 sequential steps to elicit coherent structural causal models from schema metadata. Each step includes automated validation with re-prompting on failure. This works because LLMs possess sufficient causal reasoning capability to propose meaningful dependency structures. Break condition: when variable names are ambiguous or use domain-specific jargon unfamiliar to the LLM.

### Mechanism 3: Pretraining Benefit Scaling with Dataset Size
Surrogate pretraining benefits DP classification proportionally more for smaller datasets and lower privacy budgets. With smaller private datasets, DP noise magnitude is larger relative to signal, so pretraining initialization in a useful optimization basin reduces data required for fine-tuning. Break condition: when private dataset exceeds ~10k records or privacy budget is sufficiently high.

## Foundational Learning

- **Differential Privacy Budget (ε, δ)**: Understanding why public data operations don't consume privacy budget is fundamental to this work's premise. Quick check: If you run hyperparameter tuning on surrogate public data and then train on private data with ε=2, what is your total privacy expenditure?

- **Two-Phase Neural Network Optimization**: Explains why public pretraining helps even under distribution shift—initialization quality matters for non-convex optimization. Quick check: Why might public data suffice for finding an optimization basin but not for final convergence?

- **Structural Causal Models (DAGs + Conditional Distributions)**: The Agent method generates data by constructing a generative model; understanding SCMs clarifies what the agent produces. Quick check: How does specifying a DAG over variables differ from specifying their joint distribution directly?

## Architecture Onboarding

- **Component map**: Schema Metadata → [CSV Direct Generation OR Agent SCM Builder] → Surrogate Public Dataset → Pretraining for DP Classification / Hyperparameter Tuning / Privacy-Utility Estimation

- **Critical path**: Schema quality → LLM selection → Dataset size assessment

- **Design tradeoffs**:
  - CSV vs. Agent: CSV generation is faster (single prompt) but produces less structurally coherent data; Agent method yields consistent dependency structure but requires ~11 API calls
  - Unif. vs. Max Cov. mixing: Uniform sampling from multiple datasets vs. selecting subset maximizing total variation coverage

- **Failure signatures**:
  - Pretraining shows no AUC advantage: Check dataset size (>10k may show minimal benefit) and schema description quality
  - Hyperparameter tuning degrades performance on marginals: Surrogate data may lack accurate correlation structure; consider Agent method over CSV
  - Memorization concerns: Verify LLM cutoff dates and run Bordt et al. (2024) memorization tests

- **First 3 experiments**:
  1. Generate surrogate data for a held-out validation dataset post-dating LLM training cutoffs and measure TVD/3-way marginal error
  2. Compare CSV vs. Agent methods on pretraining AUC advantage at ε=1 across two dataset sizes
  3. Run hyperparameter tuning for one DP synthesizer using surrogate data and measure degradation relative to optimal private-data-selected hyperparameters

## Open Questions the Paper Calls Out

- **Metric for trade-off estimation**: Identifying metrics that better predict which surrogate data provides accurate trade-off estimations would be beneficial, as standard metrics like TVD showed no clear relationship with trade-off estimation performance.

- **Structural dependency importance**: The strong performance of the "Arbitrary" baseline raises questions about potential theoretical justifications for why any dependency structure suffices for hyperparameter tuning, suggesting structural complexity rather than distributional fidelity may drive transfer.

- **Additional DP auxiliary tasks**: Several tasks remain unexplored, including using public data for seeding synthetic data generation and assessing the success rate of privacy attacks, which could benefit from surrogate data approaches.

- **Fairness implications**: Given substantial evidence that LLMs encode biases, these biases could be reflected in the generated data and impact the fairness of downstream classifiers after private fine-tuning.

## Limitations

- The paper relies on specific LLM capabilities that vary by domain and implementation details not fully specified, limiting reproducibility
- Schema quality significantly impacts performance, with ambiguous variable names degrading results
- Privacy implications of potential memorization remain inadequately addressed with only preliminary checks suggested
- Cross-dataset generalization limits are unclear, as all evaluations used datasets with similar characteristics

## Confidence

- **Core claim (surrogate data effectiveness)**: Medium confidence - results show promise but depend on specific implementation details and dataset characteristics
- **Mechanism 1 (LLM statistical knowledge)**: Medium confidence - supported by general LLM capabilities but lacks direct validation for tabular data generation
- **Mechanism 2 (Agent SCM construction)**: Low confidence - theoretical soundness but inconsistent performance across evaluation metrics
- **Privacy considerations**: Low confidence - memorization concerns raised but not comprehensively addressed

## Next Checks

1. **Schema sensitivity analysis**: Systematically degrade schema descriptions and measure impact on pretraining AUC advantage across all three datasets to understand sensitivity to metadata quality

2. **Cross-dataset generalization test**: Generate surrogate data for one dataset using another dataset's schema as template, then measure pretraining effectiveness to assess domain transfer limits

3. **Comprehensive privacy leakage audit**: Implement Bordt et al. (2024) memorization tests on generated data against public versions of private datasets, measuring completion accuracy for schema components and record values