---
ver: rpa2
title: 'SpikingGamma: Surrogate-Gradient Free and Temporally Precise Online Training
  of Spiking Neural Networks with Smoothed Delays'
arxiv_id: '2602.01978'
source_url: https://arxiv.org/abs/2602.01978
tags:
- training
- temporal
- neural
- neuron
- spiking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SpikingGamma enables online training of spiking neural networks
  at arbitrary temporal resolution without surrogate gradients. It uses adaptive recursive
  memory structures combined with sigma-delta spike-coding to directly backpropagate
  errors, eliminating the need for surrogate gradients.
---

# SpikingGamma: Surrogate-Gradient Free and Temporally Precise Online Training of Spiking Neural Networks with Smoothed Delays

## Quick Facts
- arXiv ID: 2602.01978
- Source URL: https://arxiv.org/abs/2602.01978
- Reference count: 40
- Primary result: Eliminates surrogate gradients while maintaining competitive accuracy (92.8% on SHD, 75.6% on SSC, 95.1% on DVS Gesture)

## Executive Summary
SpikingGamma introduces a novel approach for online training of spiking neural networks that eliminates the need for surrogate gradients. The method uses sigma-delta spike coding combined with internal recursive memory structures (buckets) to enable direct error backpropagation through time. This architecture supports sparse spike coding and precise temporal learning while maintaining stable performance across varying temporal resolutions. The model achieves competitive accuracy on standard benchmarks without the computational overhead of BPTT or surrogate gradient methods.

## Method Summary
SpikingGamma implements sigma-delta spike coding where neurons maintain a running signal estimate that approximates their internal state via spike-triggered refractory responses. The model uses K temporal kernels (buckets) that are leaky integrators cascading into one another, creating a feedforward temporal memory structure. This eliminates the need for recurrent connections and BPTT, enabling online training with constant memory per timestep. The sigma-delta mechanism ensures that the approximation error between the true signal and its estimate has a derivative of 1, allowing direct gradient flow without surrogate functions.

## Key Results
- Achieves 92.8% accuracy on SHD benchmark without surrogate gradients
- Maintains stable performance across temporal resolutions (500-3000 frames) on SHD
- Demonstrates sparse spike coding while preserving competitive accuracy (75.6% on SSC, 95.1% on DVS Gesture)

## Why This Works (Mechanism)

### Mechanism 1: Surrogate-Gradient-Free Error Backpropagation via Sigma-Delta Coding
The model uses sigma-delta spike coding where neurons emit spikes only when the error between their internal signal and running estimate exceeds a threshold. The key insight is that the running estimate maintains a derivative of 1 with respect to the internal signal through adaptive thresholding, making the spiking process gradient-transparent and enabling direct backpropagation without surrogate functions.

### Mechanism 2: Temporal Credit Assignment via Internal Recursive Memory (Buckets)
Each synapse or neuron contains K temporal kernels that act as leaky integrators, creating a multi-scale delayed representation of past inputs. This feedforward structure provides explicit temporal memory at each connection, allowing gradients to flow directly through bucket states rather than requiring backpropagation through time.

### Mechanism 3: Online Training and Temporal-Resolution Independence
By replacing self-recurrency with feedforward bucket structures, the model only needs to store current bucket states, decoupling memory cost from sequence length. Bucket transfer rates can be scaled inversely with timestep size, maintaining consistent continuous-time dynamics as resolution changes and enabling stable performance across different temporal resolutions.

## Foundational Learning

- **Surrogate Gradients**: Smooth functions used to approximate derivatives of non-differentiable spike functions during backpropagation. SpikingGamma bypasses this by making the spiking process gradient-transparent through sigma-delta coding.
  - Quick check: What problem does the surrogate function solve in standard SNN training, and how does SpikingGamma claim to bypass it?

- **Sigma-Delta Modulation**: Signal conversion technique where changes in a signal are accumulated and transmitted only when a threshold is exceeded. Converts continuous neuron signals into spike trains that represent running estimates.
  - Quick check: When does a neuron emit a spike in sigma-delta coding, and what is added to its internal signal estimate upon spiking?

- **Online vs. Offline (BPTT) Training**: Online methods compute gradients locally in time, while offline methods like BPTT require unrolling the entire computation graph backward. SpikingGamma enables online training by making temporal dependencies local through bucket structures.
  - Quick check: What is the key difference in memory complexity between BPTT and SpikingGamma as sequence length increases?

## Architecture Onboarding

- **Component map**: Input spikes -> bucket cascade -> weighted sum & ReLU -> comparison with own estimate -> output spike
- **Critical path**: Input spikes trigger bucket state updates, which feed into neuron signal computation, then comparison with running estimate determines spike emission, with error gradients flowing back through the sigma-delta approximation
- **Design tradeoffs**: 
  - Number of buckets (K): More buckets = longer temporal memory but increased computation
  - Bucket weights per neuron vs. per synapse: Per-synapse offers more expressive power but scales parameters O(N²·K) vs O(N·K)
  - Adaptive thresholding: Necessary for tracking high signal values but adds complexity
  - Sparsity vs accuracy: Gain loss regularization reduces spikes but may impact accuracy

- **Failure signatures**:
  - Diverging loss/no learning: Likely initialization issues with bucket transfer rates or weight initialization
  - Performance collapse with long sequences: Check if bucket transfer rates were properly scaled for timestep size
  - Too many spikes (dense coding): Network isn't learning temporal codes; increase gain loss regularization

- **First 3 experiments**:
  1. **Bucket Transfer Rate Initialization Ablation**: Sweep transfer rate factor F on a simple task and observe kernel response shape and accuracy
  2. **Temporal Resolution Scaling Test**: Train on SHD/DVS Gesture with 500, 1000, 2000, 3000 frames, scaling α_k appropriately, and compare against BPTT baseline
  3. **Sparsity-Accuracy Tradeoff**: Train on SHD with increasing gain loss constant G and plot accuracy vs spike density

## Open Questions the Paper Calls Out

### Open Question 1
Can SpikingGamma be adapted to Transformer-based architectures for scalable deep sequence learning? The paper suggests this is promising but only validates on FC and CNN architectures.

### Open Question 2
How much does SpikingGamma improve efficiency on benchmarks designed for precise temporal coding versus rate-based datasets? Current evaluation uses rate-coded benchmarks that may mask full benefits.

### Open Question 3
What are the practical resource trade-offs when mapping SpikingGamma's bucket structures onto physical neuromorphic hardware? The model claims direct hardware mapping but simulations were on GPUs only.

## Limitations

- Does not rigorously prove gradient transparency holds under all signal conditions, particularly high-frequency or highly nonlinear dynamics
- Relies on power-law bucket initialization with specific F values that are not fully explained theoretically
- Online training assumes bucket states remain within operational ranges without explicit saturation checks

## Confidence

- **High Confidence**: Elimination of surrogate gradients and competitive accuracy on standard benchmarks
- **Medium Confidence**: Online training memory efficiency claims and temporal-resolution independence
- **Medium Confidence**: Sparsity advantages due to variable gain loss impact with hyperparameters

## Next Checks

1. **Gradient Transparency Verification**: Implement controlled experiment comparing analytical derivative ∂ŷ/∂y against assumed value of 1 across different signal amplitudes and frequencies

2. **Memory State Saturation Analysis**: Design long-sequence task exceeding bucket receptive field and measure performance degradation point corresponding to saturation or gradient vanishing

3. **Transfer Rate Sensitivity Sweep**: Systematically vary power-law factor F and number of buckets K on simple temporal task, plotting performance against effective temporal receptive field length