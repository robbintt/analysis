---
ver: rpa2
title: Alleviating Attention Hacking in Discriminative Reward Modeling through Interaction
  Distillation
arxiv_id: '2508.02618'
source_url: https://arxiv.org/abs/2508.02618
tags:
- uni00000013
- uni00000011
- attention
- reward
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies attention hacking as a fundamental limitation
  in discriminative reward modeling, where the unidirectional causal attention mechanism
  and independent Siamese-encoding paradigm lead to inadequate token-level interaction,
  making judgment signals vulnerable to misallocated attention. To address this, the
  authors propose Interaction Distillation, a novel training framework that introduces
  an interaction-based natural language understanding model as a teacher to provide
  comprehensive attention patterns, then guides the reward model to simulate these
  patterns through an attentional alignment objective.
---

# Alleviating Attention Hacking in Discriminative Reward Modeling through Interaction Distillation

## Quick Facts
- **arXiv ID**: 2508.02618
- **Source URL**: https://arxiv.org/abs/2508.02618
- **Reference count**: 12
- **Primary result**: Interaction Distillation improves discriminative reward modeling by addressing attention hacking through attention-level knowledge distillation from an NLU teacher

## Executive Summary
This paper identifies attention hacking as a fundamental limitation in discriminative reward modeling, where unidirectional causal attention and independent Siamese-encoding paradigms lead to inadequate token-level interaction and vulnerable judgment signals. The authors propose Interaction Distillation, a training framework that uses an NLU teacher model to provide comprehensive attention patterns, guiding the reward model to simulate these patterns through an attentional alignment objective. Experimental results show the method delivers more stable and generalizable reward signals, achieving superior performance in both RLHF tasks and out-of-distribution preference perception tasks without additional inference costs.

## Method Summary
The method introduces a novel training framework that addresses attention hacking by distilling attention patterns from an NLU teacher into a reward model. A DeBERTa-large model fine-tuned on SNLI serves as the frozen teacher, producing full attention maps from concatenated chosen/rejected sequences. The student reward model (LLaMA3-8B) computes simulated attention maps using its query/key matrices and aligns them with teacher maps via L2 loss, combined with standard Bradley-Terry preference loss. This approach transfers comprehensive token-level interactions without modifying the inference architecture, teaching the RM what interactions it should capture for robust preference modeling.

## Key Results
- Achieves 68.25% average accuracy on Reward Bench benchmark for out-of-distribution preference perception
- Win rates exceeding 0.5 against all baselines in RLHF tasks
- Shows compatibility with other optimization algorithms while introducing no additional inference costs
- Ablation studies demonstrate teacher fine-tuning on SNLI is critical (5.41% win rate drop without tuning)

## Why This Works (Mechanism)

### Mechanism 1: Causal Attention Forward-Decay
Unidirectional causal attention in decoder-only RMs creates forward-decaying attention patterns where position $i$ cannot attend to positions $j > i$. This induces recency bias, causing the RM to base decisions disproportionately on response-end tokens rather than prompt-response relationships. Evidence shows successful decisions exhibit steady attention growth across sequence stages, while failed decisions show abrupt attention spikes in later stages.

### Mechanism 2: Siamese Encoding Attention Gap
The independent Siamese-encoding paradigm encodes chosen and rejected responses separately, yielding representations $H^c$ and $H^r$ without any attention computation between sequences. This prevents fine-grained token-level contrast necessary for robust preference judgment. Analysis shows successful RMs assign large negative values to strongly conflicting words (e.g., "expedited" vs. "wait"), while failed RMs show no significant differences in token attention patterns.

### Mechanism 3: Attention-Level Knowledge Transfer
Distilling comprehensive attention patterns from a bidirectional NLU teacher guides the student RM to simulate missing interactions. The teacher produces full attention maps $A^{c \to c}, A^{r \to r}, A^{c \to r}, A^{r \to c}$, which the student aligns via L2 loss. This teaches the RM what interactions it should have captured, improving representation quality without changing inference architecture. Ablation shows removing intra-sequence distillation causes more severe performance decline than inter-sequence.

## Foundational Learning

- **Causal vs. Bidirectional Attention**
  - Why needed: Understanding why decoder-only models cannot attend backward is essential to grasping the intra-sequence attention deficiency
  - Quick check: Can a token at position 10 in a causal Transformer attend to position 15? (No)

- **Siamese Encoding**
  - Why needed: The preference modeling paradigm encodes chosen/rejected responses independently; understanding this clarifies the inter-sequence attention gap
  - Quick check: In a Siamese RM, does the encoding of the chosen response depend on the rejected response? (No)

- **Attention-Level Knowledge Distillation**
  - Why needed: The method transfers knowledge via attention map alignment, not output logits; this differs from standard response-based distillation
  - Quick check: What is being minimized in attention-level distillation vs. soft-label distillation? (Attention map alignment vs. output distribution)

## Architecture Onboarding

- **Component map**: Teacher Model (DeBERTa-SNLI) -> Student RM (LLaMA3-8B) -> Simulated Attention Computation -> Loss Calculation
- **Critical path**: 
  1. Forward pass through teacher with concatenated sequences → extract attention maps from top K blocks
  2. Forward pass through RM with separate sequences → compute simulated inter-sequence attention using Q/K matrices
  3. Compute L_ID alignment loss + L_PM preference loss
  4. Backprop only through RM (teacher frozen)
- **Design tradeoffs**:
  - Teacher selection: DeBERTa vs. BERT shows minimal difference, but SNLI fine-tuning is critical (Table 3: -5.41 win rate drop without tuning)
  - Block alignment K: Best results at K=1 (last block only); deeper alignment degrades performance due to non-equivalent processing across architectures
  - Weight η: Stable in range [0.4, 1.0]; higher values prioritize interaction distillation over preference modeling
- **Failure signatures**:
  - Forward-decaying attention: Check cumulative attention distribution across sequence stages; failed cases show late-stage spikes
  - Missing inter-sequence contrast: Visualize A^{Sim}_{c→r}; near-uniform values indicate inadequate cross-sequence learning
  - Teacher mismatch: Using un-tuned teacher causes >5 point win rate drop (Table 3)
- **First 3 experiments**:
  1. Train standard BT-RM on HH-RLHF, plot cumulative attention across sequence stages to confirm forward-decaying pattern
  2. Compare DeBERTa-SNLI vs. DeBERTa-uncached vs. BERT-SNLI on held-out preference subset to validate teacher sensitivity
  3. Run small-scale sweep on η ∈ {0.4, 0.6, 0.8, 1.0} and K ∈ {1, 2, 4} with 10% training data

## Open Questions the Paper Calls Out
None

## Limitations

- **Correlation vs. Causation**: Evidence shows attention patterns correlate with preference modeling success but doesn't definitively prove that redistributing attention through distillation fixes the root cause rather than masking symptoms
- **Teacher Domain Transfer**: Critical dependence on transferring NLU attention patterns (from SNLI) to preference modeling tasks, without analysis of whether teacher bias introduces task-specific artifacts
- **Siamese Encoding Adequacy**: Claims about Siamese encoding being inherently deficient conflate architectural constraints with learning capability; doesn't prove architectural modification wouldn't be more effective

## Confidence

- **High Confidence**: Experimental results demonstrating ID-RM outperforms baselines on RLHF win rates (>0.5 against all) and Reward Bench accuracy (68.25% average)
- **Medium Confidence**: The causal mechanism linking attention patterns to preference modeling quality, though correlation doesn't prove causation
- **Low Confidence**: Claims about Siamese encoding being inherently deficient for preference modeling without ruling out alternative explanations

## Next Checks

1. **Architectural Ablation**: Train a baseline RM with bidirectional attention and cross-encoded preference pairs to isolate whether attention distillation or architectural modification primarily drives improvements
2. **Teacher Domain Analysis**: Train multiple teachers with different fine-tuning tasks (NLI, sentiment analysis, next-sentence prediction) to measure whether SNLI-specific attention patterns transfer better than generic linguistic patterns
3. **Attention Pattern Attribution**: Use attention intervention studies to determine if redistributing attention through distillation causally improves preference modeling by masking specific patterns in successful/failed cases and measuring impact on reward accuracy