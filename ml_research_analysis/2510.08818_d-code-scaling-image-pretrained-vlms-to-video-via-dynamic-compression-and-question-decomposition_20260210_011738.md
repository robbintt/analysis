---
ver: rpa2
title: 'D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and
  Question Decomposition'
arxiv_id: '2510.08818'
source_url: https://arxiv.org/abs/2510.08818
tags:
- video
- question
- visual
- compression
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenges of adapting image-pretrained
  vision-language models (VLMs) to video understanding, specifically the perception
  bottleneck and token overload. The perception bottleneck arises from static compression
  strategies that fail to retain salient information distributed unevenly across temporal
  and spatial dimensions.
---

# D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition

## Quick Facts
- arXiv ID: 2510.08818
- Source URL: https://arxiv.org/abs/2510.08818
- Authors: Yiyang Huang; Yizhou Wang; Yun Fu
- Reference count: 27
- Key outcome: First training-free method to surpass training-required models on EgoSchema (58.0% vs 51.7%)

## Executive Summary
This paper addresses the challenges of adapting image-pretrained vision-language models (VLMs) to video understanding, specifically the perception bottleneck and token overload. The perception bottleneck arises from static compression strategies that fail to retain salient information distributed unevenly across temporal and spatial dimensions. Token overload occurs when compressed video inputs still exceed the processing capacity of image-pretrained VLMs. To address these issues, the authors propose D-CoDe, a training-free framework that combines dynamic compression with question decomposition. Dynamic compression adaptively selects representative frames and performs content-aware spatial token pruning and merging to preserve essential visual information while reducing redundancy. Question decomposition reformulates complex queries into focused sub-questions, guiding the model to attend to distinct aspects of the video and enabling comprehensive understanding. Experiments show that D-CoDe consistently improves performance across multiple-choice and open-ended VideoQA benchmarks.

## Method Summary
D-CoDe is a training-free framework that adapts image-pretrained VLMs to video understanding by addressing two key challenges: perception bottleneck and token overload. The method combines dynamic compression and question decomposition. Dynamic compression adaptively selects representative frames through uniform sampling and semantic dissimilarity-based supplementary frame selection using CLIP global features. It then performs content-aware spatial token pruning (keeping top β tokens by L2 norm) and merging (combining tokens with cosine similarity ≥τ). Question decomposition uses GPT-3.5 to break down complex queries into focused sub-questions, which are answered individually before being combined for the final answer. The framework operates without training, relying on LLaVA-NeXT-7B with RoPE scaling for 8192 context, processing videos resized to 336×336.

## Key Results
- Achieves 58.0% accuracy on EgoSchema, surpassing the best training-required model (51.7%) by 6.3 percentage points
- Improves multiple-choice VideoQA performance: +4.7% on NExT-QA, +3.8% on IntentQA
- Demonstrates strong performance on open-ended tasks: +0.4 GPT-Accuracy on MSVD-QA, +0.1 GPT-Score
- Shows effectiveness on long-video tasks, highlighting potential for complex video-language understanding

## Why This Works (Mechanism)
The framework addresses two fundamental bottlenecks in video understanding with image-pretrained VLMs. The perception bottleneck occurs when static compression fails to retain salient information distributed across temporal and spatial dimensions. D-CoDe's dynamic compression adapts to video content by selecting frames based on both uniform sampling and semantic dissimilarity, ensuring representative coverage. Content-aware token pruning and merging preserve essential visual information while reducing redundancy. The token overload bottleneck arises when compressed video inputs still exceed the model's processing capacity. Question decomposition mitigates this by reformulating complex queries into focused sub-questions, allowing the model to attend to distinct video aspects and generate comprehensive answers from multiple perspectives.

## Foundational Learning
- **Vision-Language Models (VLMs)**: Multimodal models trained on image-text pairs for joint understanding. Why needed: D-CoDe adapts these pre-trained image models to video tasks without additional training.
- **RoPE Scaling**: Rotary Position Embedding with scaling factor to extend context window. Why needed: Enables processing longer video sequences within the 8192 token limit.
- **Token Pruning and Merging**: Spatial token reduction techniques that preserve important visual information. Why needed: Reduces token count while maintaining salient content for downstream processing.
- **Question Decomposition**: Breaking complex queries into focused sub-questions. Why needed: Enables comprehensive understanding by addressing distinct aspects of video content separately.
- **CLIP Global Features**: Contrastive language-image pre-training embeddings for semantic similarity. Why needed: Used for frame selection and token merging decisions based on semantic content.

## Architecture Onboarding
**Component Map**: Video -> Frame Sampling -> Dynamic Compression -> Sub-Question Generation -> Individual Answering -> Final Answer

**Critical Path**: Input video frames → CLIP feature extraction → Frame selection → Token pruning/merging → Question decomposition → LLM answering → Answer synthesis

**Design Tradeoffs**: Training-free approach sacrifices potential performance gains from fine-tuning but offers broader applicability and lower computational costs. Fixed hyperparameters may not be optimal across all datasets. Dependency on external LLM (GPT-3.5) for question decomposition introduces variability.

**Failure Signatures**: Accuracy plateaus as token count increases on vanilla baseline (token overload). Question decomposition should widen this gap at higher counts. Lower performance on videos with frequent scene transitions (MSRVTT-QA).

**3 First Experiments**:
1. Verify dynamic compression effectiveness by comparing accuracy vs token count curves between baseline and D-CoDe
2. Test question decomposition impact by measuring performance on complex vs simple queries
3. Evaluate frame selection quality by measuring similarity between selected frames and ground truth salient frames

## Open Questions the Paper Calls Out
None

## Limitations
- Fixed hyperparameters (α=0.85, β=0.625, τ=0.9, t=0.5) were not optimized across datasets, suggesting potential for improvement through tuning
- Performance gains may be influenced by visual encoder differences between training-free and training-required approaches
- Dependency on GPT-3.5 for question decomposition means improvements may not scale with weaker or stronger LLMs
- CLIP-based frame selection assumes global features adequately capture salient information, which may fail when relevant details are spatially distributed

## Confidence
**High confidence**: Performance improvements on multiple-choice benchmarks (EgoSchema 58.0% vs 51.7%, NExT-QA +4.7%, IntentQA +3.8%) and open-ended metrics (MSVD-QA +0.4 GPT-Accuracy, +0.1 GPT-Score)

**Medium confidence**: Claims about question decomposition improving handling of complex queries, as effectiveness depends on sub-question generation quality and may vary with LLM capabilities

**Medium confidence**: Claims about overcoming token overload and perception bottlenecks, as these improvements are demonstrated through ablation studies but may be dataset-dependent

## Next Checks
1. Test framework sensitivity to GPT-3.5-turbo-0125 by comparing performance with different LLMs (e.g., GPT-4, open-source alternatives) for question decomposition
2. Evaluate frame selection quality by measuring similarity between selected frames and ground truth salient frames across datasets with varying scene transition frequencies
3. Conduct ablation studies varying hyperparameters (α, β, τ) across different video characteristics (scene change frequency, object density, video length) to identify optimal settings per dataset type