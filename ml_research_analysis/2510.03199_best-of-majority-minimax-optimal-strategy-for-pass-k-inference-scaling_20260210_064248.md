---
ver: rpa2
title: 'Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference Scaling'
arxiv_id: '2510.03199'
source_url: https://arxiv.org/abs/2510.03199
tags:
- regret
- arxiv
- inference
- bound
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the Pass@k inference scaling problem, where
  an LLM generates N responses and up to k are selected for evaluation. The authors
  show that existing strategies like majority voting and Best-of-N are suboptimal
  and not scaling-monotonic, meaning their performance degrades or fails to improve
  as N increases.
---

# Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference Scaling
## Quick Facts
- arXiv ID: 2510.03199
- Source URL: https://arxiv.org/abs/2510.03199
- Reference count: 7
- This paper analyzes the Pass@k inference scaling problem, where an LLM generates N responses and up to k are selected for evaluation.

## Executive Summary
This paper addresses the Pass@k inference scaling problem, where an LLM generates N responses and up to k are selected for evaluation. The authors show that existing strategies like majority voting and Best-of-N are suboptimal and not scaling-monotonic, meaning their performance degrades or fails to improve as N increases. To address this, they propose Best-of-Majority (BoM), which first filters responses by frequency and then selects the top k based on reward model predictions. They prove BoM is minimax optimal, achieving regret O(ϵopt + √(ϵ²RMC∗/k)) with sample complexity eΘ(C∗), where C∗ is the coverage coefficient, ϵRM is the reward model error, and ϵopt is the optimal response error. Experiments on math problems (GSM8K, MATH-500, AIME24) confirm BoM outperforms baselines, especially for small k and large N.

## Method Summary
The method samples N responses from a reference policy, clusters them by semantic equivalence, computes frequency per cluster, and applies a frequency threshold α to filter candidates. The top-k responses are then selected based on reward model scores from the filtered set. Key parameters include α (typically 0.005-0.015), N (100-2000), and k (1-10). The algorithm aims to combine frequency-based pessimism with quality selection to achieve minimax optimality.

## Key Results
- Best-of-Majority (BoM) outperforms majority voting and Best-of-N baselines on GSM8K, MATH-500, and AIME24 datasets
- BoM maintains or improves accuracy as N increases, while Best-of-N exhibits non-scaling-monotonic behavior (degrades with larger N)
- The method is theoretically proven to be minimax optimal with regret bound O(ϵopt + √(ϵ²RMC∗/k))
- Performance gains are most pronounced for small k values (k=1,2) and large N

## Why This Works (Mechanism)
### Mechanism 1: Frequency-Based Pessimism
Restricting selection to high-frequency responses acts as a regularizer against reward model uncertainty. The algorithm filters out responses with empirical frequency below threshold α, reducing the search space to regions where the reward model is theoretically more reliable. This relies on the assumption that the optimal response has sufficiently high probability under the reference policy (πref(y∗) ≥ 1/C∗) to survive the filter.

### Mechanism 2: Hybrid Consensus-and-Quality Selection
BoM avoids the failure modes of pure Majority Voting (mode collapse) and pure Best-of-N (reward hacking) by ordering operations. A response must be "popular enough" (frequency) before its "quality score" (reward) is considered. This prevents selection of high-reward outliers that appear only once while avoiding the selection of frequent but incorrect answers.

### Mechanism 3: Scaling Monotonicity and Minimax Optimality
The algorithm guarantees performance improves (or maintains) as sampling budget N increases, matching a theoretical lower bound. The regret upper bound O(ϵopt + √(ϵ²RM C∗/k)) does not degrade with N, unlike Best-of-N which suffers from degradation effects where increasing N introduces more high-reward noise.

## Foundational Learning
- **Regret (Pass@k context)**: The specific loss function optimized, defined as 1 - max(rewards). Understanding this explains why "Best-of-N" fails—it optimizes for expected reward of a single pick, not the worst-case gap or coverage needed for k picks. Quick check: If I submit 2 answers and one is perfect (reward 1.0) and the other is wrong (reward 0.0), what is the Regret?

- **Coverage Coefficient (C∗)**: Represents the inverse probability of the optimal response under the base model (1/πref(y∗)). It determines the "difficulty" of the problem and the required sample complexity (N ≈ Θ(C∗)). Quick check: If a correct answer is very rare in the base model (low πref), does C∗ increase or decrease, and does this require more or fewer samples?

- **Scaling-Monotonicity**: Defines whether "more compute = better results." The paper claims existing baselines violate this. Understanding this is critical for deciding when to scale inference budgets in production. Quick check: Why does Best-of-N fail to be scaling-monotonic according to the paper? (Hint: What does increasing N introduce more of?)

## Architecture Onboarding
- **Component map**: Sampler -> Aggregator (clustering + frequency counting) -> Filter (threshold α) -> Reward Model (RM) -> Selector (Top-k)
- **Critical path**: Generation → Frequency Counting → Filtering → RM Scoring. You cannot score in parallel with counting because the filter depends on the count.
- **Design tradeoffs**: Threshold α vs. Recall (higher α reduces candidate set but risks excluding correct answers), Batch Size N vs. Latency (N→∞ for optimality but constrained by latency), RM vs. Verifier (learned RM vs. deterministic verifier).
- **Failure signatures**: Coverage Failure (N too small relative to C∗, correct answer filtered), RM Hallucination (RM assigns high scores to plausible but incorrect reasoning in high-frequency set), Cluster Granularity (poor semantic equivalence clustering dilutes frequency counts).
- **First 3 experiments**: 1) Monotonicity Stress Test: Fix k=3, vary N (100, 500, 2000) on AIME24, plot accuracy to verify BoN dips while BoM flattens or rises. 2) Threshold Sensitivity: Sweep α relative to estimated coverage, plot Regret vs. α to find recall cliff. 3) Small-k Benchmark: Compare BoM vs. BoN specifically for k=1 and k=2 on GSM8K to verify superiority at small k.

## Open Questions the Paper Calls Out
### Open Question 1
Is it inherently impossible to derive a regret upper bound for Best-of-N (BoN) with the optimal 1/√k scaling under the Pass@k setting? The authors conjecture this is impossible and leave the proof to future work, as existing techniques rely on auxiliary distributions difficult to generalize when selecting k distinct responses.

### Open Question 2
How can Pass@k inference strategies be effectively integrated into the post-training (e.g., RL fine-tuning) of LLMs? The current work focuses exclusively on inference-time computation, leaving the interaction between these selection strategies and model weight updates unexplored.

### Open Question 3
Can the Best-of-Majority (BoM) algorithm maintain its theoretical guarantees in tasks with multiple valid optimal responses? The theoretical optimality relies on a unique optimal response y∗, and experiments are restricted to math problems with single correct answers.

### Open Question 4
How sensitive is the BoM algorithm to the estimation error of the coverage coefficient C∗ used to set the frequency threshold α? Theorem 5.1 requires setting α based on C∗, but the experiments use fixed small values, implying practical difficulty of estimating C∗ and a potential gap between theory and practice.

## Limitations
- The theoretical optimality depends critically on accurate estimation of the coverage coefficient C∗ to set the frequency threshold α
- The method's effectiveness hinges on the reward model providing accurate scores for high-frequency responses
- Requires complex semantic equivalence clustering for mathematical problems, which the paper doesn't fully specify

## Confidence
- **High Confidence**: The empirical demonstration that Best-of-N exhibits non-scaling-monotonic behavior on tested datasets is well-supported by experimental results
- **Medium Confidence**: The theoretical proof of minimax optimality is mathematically sound given stated assumptions, but practical applicability depends on accurate estimation of unknown quantities
- **Low Confidence**: The claim that BoM is universally superior across all inference scaling scenarios may be overstated due to sensitivity to threshold choice, reward model quality, and clustering effectiveness

## Next Checks
1. **Threshold sensitivity analysis**: Systematically vary α across multiple orders of magnitude and plot resulting Pass@k accuracy to identify optimal range and potential cliff effects
2. **Reward model error characterization**: Measure correlation between frequency and reward model accuracy by partitioning response space into frequency bins and computing error rate in each bin
3. **Alternative clustering approaches**: Compare the mathematical equivalence clustering method against simpler baselines (string matching, numeric parsing) to quantify performance contribution from sophisticated clustering vs. frequency-filtering mechanism