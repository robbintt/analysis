---
ver: rpa2
title: Unimodal-driven Distillation in Multimodal Emotion Recognition with Dynamic
  Fusion
arxiv_id: '2503.23721'
source_url: https://arxiv.org/abs/2503.23721
tags:
- multimodal
- fusion
- emotion
- teacher
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SUMMER, a multimodal emotion recognition framework
  that integrates Sparse Dynamic Mixture of Experts (SDMoE), Hierarchical Cross-Modal
  Fusion (HCMF), and Interactive Knowledge Distillation (IKD) to address modal heterogeneity
  and disorientation in multimodal learning. The method employs a unimodal teacher
  model to guide a multimodal student model, enabling effective cross-modal knowledge
  transfer and reducing feature distribution gaps.
---

# Unimodal-driven Distillation in Multimodal Emotion Recognition with Dynamic Fusion

## Quick Facts
- arXiv ID: 2503.23721
- Source URL: https://arxiv.org/abs/2503.23721
- Reference count: 19
- Key outcome: SUMMER achieves 79.11% weighted accuracy and 78.95% weighted F1 on IEMOCAP, outperforming state-of-the-art multimodal emotion recognition methods.

## Executive Summary
This paper introduces SUMMER, a novel multimodal emotion recognition framework that addresses the challenges of modal heterogeneity and disorientation through a unique combination of Sparse Dynamic Mixture of Experts (SDMoE), Hierarchical Cross-Modal Fusion (HCMF), and Interactive Knowledge Distillation (IKD). The key innovation is using a unimodal teacher (text-only) to guide a multimodal student, enabling effective cross-modal knowledge transfer while reducing feature distribution gaps. The framework demonstrates significant performance improvements on benchmark datasets IEMOCAP and MELD, particularly in recognizing minority and semantically similar emotions.

## Method Summary
SUMMER employs a three-stage architecture where unimodal encoders (RoBERTa for text, OpenSMILE for audio, MTCNN for visual) process input features, which are then enhanced by SDMoE with dynamic routing for adaptive feature selection. The HCMF module performs sequential fusion starting with Text+Audio, then integrating Visual features. A frozen text-only teacher model guides the multimodal student through IKD, using a composite loss function that balances cross-modal alignment with label consistency. The framework is trained end-to-end with specific hyperparameters including Adam optimizer (lr=1e-4), batch sizes of 32/100, and temperature/temperature settings of τ=0.5 and α=2.

## Key Results
- Achieves 79.11% weighted accuracy and 78.95% weighted F1 on IEMOCAP dataset
- Achieves 68.78% weighted accuracy and 69.81% weighted F1 on MELD dataset
- Demonstrates significant improvements in recognizing minority and semantically similar emotions compared to state-of-the-art methods
- SDMoE with dynamic routing provides +2.08% accuracy gain over baseline models

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Intra-Modal Sparsity via Dynamic Routing
The SDMoE replaces fixed Top-K expert selection with distribution-aware thresholds. Using a gating network, it generates weights and calculates a valid range (μ ± ασ), deactivating weights outside this range. Gumbel noise maintains differentiability during discrete sampling. This allows the model to adaptively filter noise based on token-specific complexity, with evidence showing +2.08% accuracy gain on IEMOCAP when adding SDMoE to baseline.

### Mechanism 2: Hierarchical Cross-Modal Alignment
HCMF processes modalities sequentially rather than simultaneously, first fusing Text and Audio, then using this intermediate representation as Query to fuse with Visual features. This prevents "disorientation" from merging heterogeneous feature spaces at once. Evidence shows Text+Audio performs better (71.18%) than Text+Visual (69.80%) on IEMOCAP, supporting the T+A precedence.

### Mechanism 3: Retrograde Knowledge Distillation
IKD uses a pre-trained unimodal teacher to guide multimodal fusion, providing stable, low-noise gradients that align heterogeneous features. The student's intermediate multimodal features are passed through the teacher's classifier to compute Cross-KD loss (KL Divergence). Evidence shows this approach achieves optimal performance by mitigating gradient conflicts that occur with multimodal teachers.

## Foundational Learning

- **Concept: Sparse Mixture of Experts (MoE)**
  - Why needed here: SUMMER builds upon standard MoE but introduces dynamic routing
  - Quick check question: How does the proposed dynamic routing differ from standard Top-K selection in traditional MoE layers?

- **Concept: Knowledge Distillation (KD)**
  - Why needed here: The core contribution involves transferring knowledge from teacher to student
  - Quick check question: Why might soft labels from a teacher be more informative than hard ground-truth labels for training a student model?

- **Concept: Cross-Attention Mechanisms**
  - Why needed here: HCMF relies on DynAttn using Query/Key/Value pairs across modalities
  - Quick check question: In the HCMF fusion of Text+Audio+Visual, which modality serves as the Query when integrating the final Visual features?

## Architecture Onboarding

- **Component map:** Input → Encoders → Speaker Embeddings → SDMoE → HCMF → Classifier
- **Critical path:** Input → Encoders → Speaker Embeddings → **SDMoE** (selects features) → **HCMF** (Text+Audio → Text+Audio+Visual) → Classifier
- **Design tradeoffs:**
  - Retrograde vs. Standard KD: Unimodal teacher is computationally cheaper and more stable but risks suppressing non-textual emotional cues
  - Dynamic vs. Fixed Routing: SDMoE allows variable compute per token but introduces non-determinism and complexity
- **Failure signatures:**
  - Mode Collapse: If α is too small in SDMoE, valid range becomes too narrow, potentially zeroing out all gradients
  - Teacher Overfitting: If L_cross^KD dominates, student may learn to mimic text teacher perfectly but ignore audio/video
- **First 3 experiments:**
  1. Teacher Validation: Train unimodal (Text) model alone to ensure Teacher is high-performing before distillation
  2. SDMoE Ablation: Replace Dynamic Router with standard Top-2 selection to quantify dynamic mechanism gains
  3. Hyperparameter Sensitivity (α, τ): Sweep threshold factor α and temperature τ to ensure Gumbel-Softmax approximates discrete selection effectively

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can SUMMER be enhanced to resolve performance degradation in specific categories (e.g., "sad") caused by multimodal conflicts and overlapping emotional boundaries?
- Basis in paper: Error Analysis states underperformance in categories like "sad" "may stem from multimodal conflicts, overlapping emotional boundaries (e.g., sadness and frustration)"
- Why unresolved: IKD aligns general feature distributions but doesn't explicitly model decision boundaries for semantically similar classes where modalities contradict
- What evidence would resolve it: Modified loss function or fusion mechanism designed to maximize margins between confusing emotion pairs, resulting in higher F1-scores for specific classes

### Open Question 2
- Question: Can a multimodal teacher be effectively integrated into the IKD framework without introducing overfitting risks and complexity?
- Basis in paper: Ablation study notes combining text with other modalities yields minor gains but "increased model complexity and overfitting risks favor a unimodal teacher for efficiency"
- Why unresolved: Paper establishes retrograde distillation strategy using text-only teacher for stability, leaving multimodal teacher potential unexplored
- What evidence would resolve it: Comparative study where regularized multimodal teacher is used, demonstrating student can absorb richer cross-modal information without gradient disorientation or overfitting

### Open Question 3
- Question: To what extent does SDMoE's dynamic routing performance depend on chosen threshold hyperparameters (α and τ) across datasets with varying noise levels?
- Basis in paper: Methodology introduces specific hyperparameters (τ for temperature, α for selection threshold) and notes "A smaller α imposes stricter constraints"
- Why unresolved: Paper sets fixed values (τ=0.5, α=2) but doesn't analyze robustness across diverse data conditions or noise profiles
- What evidence would resolve it: Sensitivity analysis plotting performance against varying α and τ values on datasets with artificially injected noise

## Limitations

- Dynamic routing mechanism relies on threshold parameter α without thorough sensitivity analysis, creating uncertainty about robustness to hyperparameter tuning
- Performance improvements demonstrated on relatively small benchmark datasets (IEMOCAP and MELD) may not generalize to more diverse conversational contexts
- The specific mechanism by which "smaller" teacher signals reduce gradient conflict compared to standard multimodal-to-multimodal KD is not empirically validated

## Confidence

- **High Confidence**: Core architectural contributions (SDMoE, HCMF, IKD) are well-defined and experimental setup is reproducible; improvement over baseline models is consistent and statistically meaningful
- **Medium Confidence**: Claim that unimodal teachers provide more effective guidance than multimodal teachers is supported by results but lacks mechanistic analysis
- **Low Confidence**: Assertion that dynamic routing provides meaningful benefits over fixed Top-K selection is based on single ablation study (+2.08% accuracy) without broader hyperparameter sweeps

## Next Checks

1. **Dynamic Routing Sensitivity**: Conduct systematic sweep of threshold parameter α (e.g., α ∈ [0.5, 1.0, 1.5, 2.0, 2.5]) to determine whether performance gains are robust to hyperparameter choices or represent overfitting to specific α value

2. **Teacher Comparison Experiment**: Implement parallel distillation setup using multimodal teacher to quantitatively compare gradient stability and final performance against unimodal teacher approach

3. **Minority Class Performance Analysis**: Given claim of improved recognition of "minority and semantically similar emotions," conduct detailed per-class breakdown of F1 scores, particularly for minority classes, to verify improvements are not solely driven by majority class performance gains