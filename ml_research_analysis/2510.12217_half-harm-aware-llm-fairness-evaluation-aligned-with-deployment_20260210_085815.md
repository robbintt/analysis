---
ver: rpa2
title: 'HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment'
arxiv_id: '2510.12217'
source_url: https://arxiv.org/abs/2510.12217
tags:
- bias
- across
- fairness
- gender
- demographic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'HALF introduces a harm-aware evaluation framework that assesses
  LLM fairness across nine application domains by weighting outcomes according to
  real-world consequences. Using a five-stage pipeline, HALF aggregates fairness scores
  with domain-specific harm weights (Severe: 3, Moderate: 2, Mild: 1), enabling deployment-relevant
  comparisons.'
---

# HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment

## Quick Facts
- **arXiv ID**: 2510.12217
- **Source URL**: https://arxiv.org/abs/2510.12217
- **Reference count**: 40
- **Primary result**: Introduces HALF, a harm-aware evaluation framework that assesses LLM fairness across nine domains using domain-specific harm weights, showing no universally fair model exists.

## Executive Summary
HALF introduces a harm-aware evaluation framework that assesses LLM fairness across nine application domains by weighting outcomes according to real-world consequences. Using a five-stage pipeline, HALF aggregates fairness scores with domain-specific harm weights (Severe: 3, Moderate: 2, Mild: 1), enabling deployment-relevant comparisons. Across eight models, results show that performance does not guarantee fairness, reasoning models excel in high-stakes domains but underperform in low-stakes tasks, and no universally fair model exists. Claude 4 achieved the highest overall score (60.7 weighted), while open-weight models showed lower fairness despite competitive performance in severe harm contexts. Bias patterns were inconsistent across domains, underscoring the need for domain-specific evaluation aligned with deployment risks.

## Method Summary
HALF employs a five-stage pipeline to evaluate LLM fairness across nine deployment domains. The process begins with dataset search and adaptation, where tasks are recast or augmented with demographic variants (18 combinations for most tasks, 6 for recruitment/education). Task formulation creates controlled demographic variants for evaluation, using zero-shot prompting with system prompts specified in Appendix E. Evaluation execution computes domain-specific metrics per dataset, with per-dataset normalization using sigmoid of z-scores across all models. Finally, harm-weighted aggregation combines normalized scores using weights {3,2,1} for {Severe,Moderate,Mild} tiers to produce a unified HALF score (0-100). The framework uses 12 datasets across domains, with metrics including accuracy gaps, macro-F1 with group disparity, weighted F1 with demographic perturbation, flip rates, and bias scores.

## Key Results
- Performance does not guarantee fairness: GPT-4.1-mini scored highest on performance (59.7 unweighted) but only 6th on fairness (48.9 weighted)
- Reasoning models excel in high-stakes domains but underperform in low-stakes tasks: o4-mini showed 12.7 percentage point deficit on moderate-harm tasks versus standard models
- No universally fair model exists: Claude-4-Sonnet achieved highest overall fairness score (60.7 weighted), while open-weight models showed lower fairness despite competitive performance in severe harm contexts

## Why This Works (Mechanism)
The framework works by aligning fairness evaluation with deployment consequences through harm-weighted aggregation. By organizing domains into harm tiers and applying appropriate weights, HALF ensures that fairness failures in high-stakes contexts (medical, legal) carry more weight than those in low-stakes contexts (recommendation, summarization). The domain-specific metrics capture the unique fairness challenges of each application, while the normalization process enables meaningful comparison across heterogeneous tasks. The use of demographic variants in evaluation reveals model behavior across different population groups, exposing biases that would be missed by aggregate metrics alone.

## Foundational Learning
**Domain-specific harm weighting**: Why needed - ensures fairness evaluation reflects real-world consequences where failures in medical/legal contexts are more severe than in entertainment/recommendation. Quick check - verify that harm weights align with stakeholder priorities through surveys or regulatory guidance.
**Demographic variant evaluation**: Why needed - exposes model biases across different population groups that would be hidden by aggregate metrics. Quick check - ensure demographic categories are comprehensive and intersectional effects are captured where relevant.
**Z-score normalization with sigmoid**: Why needed - enables comparison of heterogeneous metrics across different scales and distributions. Quick check - verify that normalization doesn't compress meaningful performance differences when score distributions are narrow.

## Architecture Onboarding

**Component Map**: Dataset Search -> Dataset Adaptation -> Task Formulation -> Evaluation Execution -> Harm-Weighted Aggregation

**Critical Path**: The core pipeline follows dataset adaptation through to harm-weighted aggregation, with each stage building on the previous. Dataset adaptation must complete before task formulation, which must complete before evaluation execution, which feeds into the final aggregation.

**Design Tradeoffs**: The framework prioritizes deployment relevance over model-agnostic fairness metrics, accepting that different stakeholders may disagree on harm priorities. It trades comprehensiveness for focus on nine high-impact domains rather than attempting exhaustive coverage. The use of static demographic categories simplifies evaluation but may miss nuanced intersectional effects.

**Failure Signatures**: Models refusing sensitive prompts (particularly open-source models), outputs falling outside allowed label sets (especially in legal tasks), and refusal rates varying significantly across demographic groups. These failures manifest as missing or invalid outputs rather than low fairness scores.

**First 3 Experiments**:
1. Evaluate a single model on MedBullets dataset to verify demographic variant generation and accuracy gap calculation
2. Test dataset adaptation pipeline on one dataset (e.g., CAMS) to confirm demographic augmentation works correctly
3. Run normalization and aggregation on a small subset of datasets to verify the sigmoid-based scoring and harm-weighted combination

## Open Questions the Paper Calls Out
**Open Question 1**: What factors determine whether bias patterns transfer across deployment domains, and can cross-domain bias predictability be improved through targeted training interventions? The paper demonstrates inconsistent cross-domain bias but doesn't identify underlying mechanisms that cause bias to manifest differently across applications.

**Open Question 2**: How sensitive are HALF model rankings to alternative harm tier weightings, and what stakeholder consensus exists on harm severity classifications? The paper uses a single weighting scheme and acknowledges that rankings may change under different weights, but doesn't quantify this sensitivity or establish consensus weightings.

**Open Question 3**: Why do reasoning-optimized models show superior fairness in high-stakes domains but degraded performance in moderate-harm applications? The paper documents this trade-off but doesn't explain whether it stems from reasoning training emphasizing STEM domains or other factors.

**Open Question 4**: Can the HALF framework's harm-aware weighting approach be extended to other capability dimensions (safety, factuality, empathy) in a way that maintains deployment alignment? The paper proposes this extension but doesn't address whether harm-weighted aggregation is appropriate for these qualitatively different capability dimensions.

## Limitations
- Generalizability constrained by English-only datasets and static harm-weighting schema
- Fixed weights assume domain-agnostic harm priorities that may not reflect local regulatory contexts
- Evaluation pipeline assumes static demographic categories without accounting for intersectional effects
- Performance metrics for open-source models may be inflated by refusal filtering
- Sigmoid-based normalization could compress meaningful performance differences

## Confidence

**High Confidence**: Domain-specific metric implementations and the overall finding that no model achieves universal fairness are well-supported by the methodology.

**Medium Confidence**: Cross-domain harm-weighted aggregation is methodologically sound, but the fixed weighting schema's real-world validity remains unverified. Model ranking stability is consistent across metrics but sensitive to normalization approach.

**Low Confidence**: Generalization of results to non-English contexts, dynamic harm-weighting scenarios, or domains outside the nine evaluated requires additional validation.

## Next Checks
1. Replicate the evaluation pipeline using a subset of three diverse datasets with explicit logging of refusal rates and malformed outputs to verify handling of model limitations
2. Conduct sensitivity analysis by varying the harm weights and observe model ranking stability to assess robustness of conclusions to weighting assumptions
3. Test the framework on a non-English dataset to evaluate cross-lingual applicability of the harm-aware evaluation approach