---
ver: rpa2
title: The impact of intrinsic rewards on exploration in Reinforcement Learning
arxiv_id: '2501.11533'
source_url: https://arxiv.org/abs/2501.11533
tags:
- intrinsic
- state
- exploration
- reward
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the impact of different levels of diversity
  on exploration in Reinforcement Learning (RL). We categorize intrinsic rewards based
  on the type of diversity they impose (State, State+Dynamics, Policy, and Skill levels)
  and evaluate their performance on MiniGrid environments with both grid encodings
  and RGB observations.
---

# The impact of intrinsic rewards on exploration in Reinforcement Learning

## Quick Facts
- arXiv ID: 2501.11533
- Source URL: https://arxiv.org/abs/2501.11533
- Reference count: 40
- State Count leads to best exploration performance in low-dimensional observation spaces but degrades in RGB observations

## Executive Summary
This study examines how different levels of diversity in intrinsic rewards affect exploration in Reinforcement Learning. The authors categorize intrinsic rewards based on the type of diversity they impose (State, State+Dynamics, Policy, and Skill levels) and evaluate their performance on MiniGrid environments with both grid encodings and RGB observations. Their empirical results show that State Count, representing State level diversity, leads to the best exploration performance in low-dimensional observation spaces, effectively covering the state space and finding rewards quickly. However, its performance degrades significantly in RGB observations due to representation learning challenges. Maximum Entropy, promoting Policy level diversity, proves more robust to high-dimensional observations, though it is not always optimal. Surprisingly, learning diverse skills with DIAYN, often associated with improved robustness and generalization, does not promote exploration in MiniGrid environments. This is attributed to the difficulty of learning the skill space and the tendency of DIAYN to prioritize skill differentiation over uniform state visitation.

## Method Summary
The study systematically evaluates intrinsic reward methods across different diversity levels (State, State+Dynamics, Policy, and Skill) in MiniGrid environments. The authors test these methods using both grid encodings and RGB observations to assess performance across different observation space dimensionalities. They implement and compare State Count for state-level diversity, Maximum Entropy for policy-level diversity, and DIAYN for skill-level diversity. The evaluation focuses on how effectively each method promotes exploration and state space coverage in these controlled environments.

## Key Results
- State Count achieves best exploration performance in low-dimensional observation spaces by effectively covering the state space and finding rewards quickly
- State Count performance degrades significantly in RGB observations due to representation learning challenges
- DIAYN, despite being designed for skill diversity, fails to promote exploration in MiniGrid environments due to difficulties in learning the skill space

## Why This Works (Mechanism)
Intrinsic rewards that align with the structure of the observation space and task objectives promote more effective exploration. State-level diversity works well when states can be easily distinguished and counted in low-dimensional spaces, but fails when representation learning becomes necessary. Policy-level diversity through entropy maximization provides more robust exploration across different observation spaces because it doesn't rely on precise state identification. The failure of skill-based diversity methods like DIAYN suggests that the overhead of learning a skill space and maintaining skill differentiation can actually hinder exploration when the primary goal is simply to find rewards through state coverage.

## Foundational Learning

**Reinforcement Learning fundamentals** - why needed: Understanding the basics of RL agents, rewards, and exploration-exploitation tradeoffs is essential for grasping how intrinsic rewards modify behavior
Quick check: Can you explain the difference between extrinsic and intrinsic rewards in RL?

**Exploration strategies in RL** - why needed: The study focuses on exploration methods, requiring understanding of how agents balance exploring new states versus exploiting known rewards
Quick check: What are the main categories of exploration strategies used in RL?

**Representation learning** - why needed: The performance differences between grid encodings and RGB observations highlight the importance of how agents represent their observations
Quick check: How does the dimensionality of observation spaces affect representation learning challenges?

## Architecture Onboarding

**Component map**: Environment -> Observation Encoder -> Intrinsic Reward Module -> Policy Network -> Action Selection -> Environment

**Critical path**: State/Observation → Intrinsic Reward Calculation → Policy Update → Action → New State

**Design tradeoffs**: Low-dimensional observations enable simple counting-based exploration (State Count) but high-dimensional observations require more sophisticated representation learning; policy entropy maximization trades off precise state coverage for robustness to observation complexity; skill-based methods add overhead that may hinder rather than help exploration

**Failure signatures**: State Count fails with RGB observations when state representations become ambiguous; DIAYN fails when skill space learning is difficult relative to the environment complexity; Maximum Entropy may underperform when precise state coverage is more important than policy diversity

**3 first experiments**: 1) Compare State Count performance between grid encodings and RGB observations in the same environment 2) Test Maximum Entropy with different entropy coefficient values to find optimal exploration-exploitation balance 3) Evaluate whether pre-training a representation model improves State Count performance in high-dimensional observations

## Open Questions the Paper Calls Out
None

## Limitations
- Results are specific to MiniGrid environments and may not generalize to more complex domains with different reward structures
- The negative results for DIAYN are surprising and may indicate methodological considerations that affect interpretation
- Representation learning challenges in RGB observations warrant further investigation to understand their impact on exploration performance

## Confidence

| Claim | Confidence |
|-------|------------|
| State Count performance in low-dimensional observations | High |
| Performance degradation in RGB observations | Medium |
| DIAYN's limited exploration benefits | Medium |

## Next Checks
1. Test the relative performance of intrinsic rewards on continuous control environments (e.g., MuJoCo) to assess whether the observed hierarchy holds across different task types and state space structures.

2. Investigate whether pre-training representation learning modules improves the performance of State Count and DIAYN in high-dimensional observation spaces, particularly for RGB inputs.

3. Evaluate whether curriculum-based approaches to skill discovery can address the limitations observed with DIAYN by gradually increasing the difficulty of skill differentiation during training.