---
ver: rpa2
title: Prediction of Stocks Index Price using Quantum GANs
arxiv_id: '2509.12286'
source_url: https://arxiv.org/abs/2509.12286
tags:
- quantum
- data
- price
- stock
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the application of Quantum Generative Adversarial
  Networks (QGANs) for stock price prediction, addressing the complexity and volatility
  of financial markets. The research implements and evaluates a QGAN model tailored
  for stock price prediction using historical FTSE stock index data, training on the
  AWS Braket SV1 simulator.
---

# Prediction of Stocks Index Price using Quantum GANs

## Quick Facts
- arXiv ID: 2509.12286
- Source URL: https://arxiv.org/abs/2509.12286
- Reference count: 37
- This study implements and evaluates quantum GANs for stock price prediction, showing that hybrid quantum GANs outperform classical LSTM and GAN models in convergence speed and prediction accuracy.

## Executive Summary
This research investigates Quantum Generative Adversarial Networks (QGANs) for stock price prediction using historical FTSE data. The study compares classical GANs, hybrid quantum-classical GANs, and fully quantum GANs, finding that the hybrid approach achieves superior performance with a test RMSE of 84.27. The fully quantum GAN using SWAP test discrimination achieved a test RMSE of 251.89 after only 5 epochs, demonstrating the potential of quantum-enhanced models in financial forecasting despite limited training duration.

## Method Summary
The study implements three QGAN variants on AWS Braket SV1 simulator using 10 years of FTSE stock index data. The classical GAN uses technical indicators (7/21-day MA, EMA, momentum, Fourier transforms) with 150 epochs, lr=0.00016, batch_size=128. The hybrid QGAN replaces the classical generator with a variational quantum circuit using angle encoding for past price windows, maintaining the same classical discriminator. The fully quantum GAN employs amplitude embedding with a SWAP test discriminator, trained for only 5 epochs with lr=0.016. The invertible FQGAN approach predicts overlapping windows to recover normalization factors through classical optimization.

## Key Results
- Hybrid QGAN achieved test RMSE of 84.27, outperforming classical GAN (RMSE 91.71) and LSTM models
- Fully quantum GAN with SWAP test achieved test RMSE of 251.89 after only 5 epochs of training
- Performance degrades with larger past window sizes, confirming angle encoding scalability limits beyond 5-10 day windows
- Invertible FQGAN successfully recovers normalization factors by minimizing difference between known input values and predicted overlapping values

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Quantum-Classical Architecture
The quantum generator uses parameterized rotation gates to encode historical price windows as quantum state angles, with quantum operations representing non-linear correlations in financial data. This hybrid approach leverages quantum expressivity for the generator while maintaining classical discriminator training, outperforming fully classical models in convergence speed and accuracy.

### Mechanism 2: SWAP Test as Parameter-Free Quantum Discriminator
The fully quantum GAN uses a SWAP test to measure fidelity between quantum states representing real and generated data. This eliminates discriminator parameters entirely, providing a similarity score directly from quantum measurement that serves as an effective adversarial signal for generator training.

### Mechanism 3: Invertible FQGAN for Normalization Recovery
The approach predicts overlapping windows (b+f values where first b overlap with input) and uses classical optimization to minimize the difference between known input values and predicted overlapping values, solving for the unknown normalization factor. This enables recovery of amplitude embedding normalization from training data statistics.

## Foundational Learning

- **Variational Quantum Circuits (VQCs):** Both hybrid and fully quantum generators use parameterized quantum gates whose angles are learned through classical optimization. Can you explain how a parameterized rotation gate differs from a fixed quantum gate, and how gradients flow through a VQC?

- **Amplitude vs. Angle Encoding:** Hybrid QGAN uses angle encoding (data → rotation angles); FQGAN uses amplitude embedding (data → probability amplitudes) with stricter normalization requirements. Why does amplitude embedding require L2 normalization while angle encoding does not?

- **GAN Training Dynamics & Mode Collapse:** Adversarial training balance is critical; paper notes that discriminator inadequacy causes unstable QGAN training. What happens to generator learning if the discriminator becomes too strong or too weak?

## Architecture Onboarding

- **Component map:** Data Encoder → Angle/Amplitude Embedding → Variational Quantum Generator → Classical/Quantum Discriminator → Loss Computation → Classical Optimizer (Adam) → Parameter Update
- **Critical path:** 1) Data preprocessing: Min-max scaling → L2 normalization (for amplitude embedding) → window slicing 2) Quantum circuit execution on AWS Braket SV1 simulator 3) Measurement and loss computation 4) Classical backpropagation through quantum circuit 5) Inverse transforms for prediction recovery
- **Design tradeoffs:** Angle encoding is simpler with lower qubit count but limited expressivity for larger windows; amplitude embedding is log₂(n) qubit efficient but requires deeper circuits and normalization constraints; SWAP test discriminator has no trainable parameters but potentially less stable training signal
- **Failure signatures:** Test RMSE >> Train RMSE indicates overfitting or insufficient discriminator training; performance degrades with window size > 5 (Hybrid) or > 10 (FQGAN) indicates encoding expressivity limits; high variance across runs suggests quantum simulator noise or optimization landscape issues
- **First 3 experiments:** 1) Reproduce hybrid QGAN baseline with 3-day past window → 1-day prediction, verify test RMSE ≈ 84, compare against classical GAN with same hyperparameters 2) Ablate window size with hybrid QGAN using [3, 5, 7, 10] to reproduce performance degradation curve 3) Validate Invertible FQGAN with window (16, 8), measure normalization factor recovery accuracy and compare predicted vs. actual prices

## Open Questions the Paper Calls Out

### Open Question 1
How does the Fully Quantum GAN (FQGAN) perform on real quantum processing units (QPUs) compared to the SV1 simulator? The authors state they "were unable to perform tests on real hardware" due to resource constraints. Simulators like SV1 model ideal states, whereas real NISQ devices introduce noise and decoherence that may disrupt the adversarial training balance.

### Open Question 2
Can efficient data encoding schemes mitigate the exponential scaling of circuit depth for larger window sizes? The paper notes that "the depth of the circuit scales exponentially because we use Amplitude Embedding," limiting the manageable window size. Amplitude embedding creates deep circuits that are difficult to execute on near-term hardware without error correction.

### Open Question 3
Is the "Invertible FQGAN" strategy robust against statistical arbitrage opportunities or drift in live markets? The paper proposes a strategy to recover normalization factors by predicting known past data points, but relies on the assumption that these predictions are accurate enough to minimize the objective function. If the quantum generator fails to accurately reproduce the "known" past window, the calculated normalization factor will be incorrect.

## Limitations
- Short training duration for FQGAN (5 epochs) raises questions about whether performance can be sustained with longer training
- Quantum circuit ansatz details for both hybrid and fully quantum generators remain unspecified
- The invertible normalization technique for FQGAN lacks empirical validation across multiple window sizes and datasets

## Confidence
- High confidence: Classical GAN baseline performance claims (RMSE ~91.71) with specified hyperparameters
- Medium confidence: Hybrid QGAN performance advantage (RMSE ~84.27) due to specified architecture but incomplete quantum circuit details
- Low confidence: FQGAN's test RMSE of 251.89 given only 5 epochs of training and the novelty of the invertible normalization approach

## Next Checks
1. Cross-validate on multiple financial indices: Test the hybrid QGAN architecture on S&P 500, NASDAQ, and other major indices to verify the 15% RMSE improvement generalizes beyond FTSE data
2. Hardware vs. simulator comparison: Run the hybrid QGAN on real quantum hardware (e.g., Rigetti Aspen-M-3) and compare against AWS Braket SV1 simulator to assess quantum noise impact on financial predictions
3. Extended FQGAN training: Increase FQGAN training epochs from 5 to 50-100 while monitoring test RMSE convergence, and test the invertible normalization technique with window sizes ranging from (8,4) to (32,16)