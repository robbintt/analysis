---
ver: rpa2
title: Can Large Language Models Function as Qualified Pediatricians? A Systematic
  Evaluation in Real-World Clinical Contexts
arxiv_id: '2511.13381'
source_url: https://arxiv.org/abs/2511.13381
tags:
- pediatric
- clinical
- medical
- performance
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the capability of large language models (LLMs)
  in pediatric medicine using a new benchmark framework, PEDIASBench, which assesses
  models across three dimensions: basic knowledge application, dynamic diagnosis and
  treatment, and medical safety/ethics. Twelve models, including GPT-4o, DeepSeek-V3,
  and Qwen3-235B-A22B, were tested on 211 prototypical diseases across 19 pediatric
  subspecialties.'
---

# Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts

## Quick Facts
- arXiv ID: 2511.13381
- Source URL: https://arxiv.org/abs/2511.13381
- Reference count: 40
- Primary result: Current LLMs show promise for pediatric decision support but cannot independently perform pediatric care, with significant performance gaps in complex reasoning and dynamic clinical scenarios

## Executive Summary
This study evaluates 12 large language models on pediatric medical competency using the PEDIASBench framework across three dimensions: basic knowledge application, dynamic diagnosis and treatment, and medical safety/ethics. While state-of-the-art models like Qwen3-235B-A22B achieve >90% accuracy on foundational knowledge, performance drops significantly with increased task complexity and dynamic reasoning demands. The research demonstrates that current LLMs are not yet ready for independent pediatric practice but show potential for clinical decision support when used under supervision.

## Method Summary
The study evaluates 12 LLMs including GPT-4o, DeepSeek-V3, and Qwen3-235B-A22B on PEDIASBench, a benchmark covering 19 pediatric subspecialties and 211 prototypical diseases. Models are tested across three dimensions using zero-shot prompting via official APIs: basic knowledge through single/multiple-choice questions at four difficulty levels (resident through senior), dynamic diagnosis via two-phase case reasoning, and medical safety/ethics assessment. All items are validated by ≥2 pediatricians with 5+ years experience, with a senior adjudicator (10+ years) resolving disagreements. Performance is measured using accuracy, F1 scores, macro recall, and BERTScore metrics.

## Key Results
- Qwen3-235B-A22B achieved >90% accuracy on licensing-level single-choice questions
- Performance declined ~15% as task complexity increased, with senior-level multiple-choice accuracy dropping to 20.50% for some models
- DeepSeek-R1 scored highest (mean 0.58) in dynamic diagnosis scenarios but still struggled with real-time patient changes
- Qwen2.5-72B led medical safety/ethics with 92.05% accuracy, though humanistic sensitivity remained limited
- Oncology and cardiology subspecialties showed near-zero accuracy, indicating data gaps

## Why This Works (Mechanism)

### Mechanism 1: Structured Knowledge Encoding from Medical Pre-training
- Claim: LLMs accurately recall and apply foundational pediatric knowledge when tasks map to encoded patterns from medical corpora
- Mechanism: Pre-training on large-scale medical texts creates dense associative representations of disease-taxonomy relationships, enabling high performance on standardized assessments
- Core assumption: High licensing-exam accuracy reflects quality of encoded medical knowledge rather than surface pattern matching
- Evidence anchors: Qwen3-235B-A22B achieving >90% accuracy on resident-level tasks; similar patterns in pediatric endocrinology benchmarks

### Mechanism 2: Complexity-Induced Performance Degradation
- Claim: LLM performance degrades predictably as task complexity increases, revealing limitations in multi-step clinical reasoning
- Mechanism: Complex tasks require integrating multiple knowledge graphs simultaneously; current architectures lack explicit compositional reasoning modules
- Core assumption: Observed degradation reflects reasoning limitations rather than data sparsity alone
- Evidence anchors: 15% accuracy drop from single-choice to multiple-choice at senior level; Mistral-Small-3.1-24B-Instruct reaching only 20.50% accuracy at senior level

### Mechanism 3: Temporal-Dynamic Reasoning Deficit
- Claim: LLMs struggle with longitudinal clinical reasoning where patient state evolves across timepoints
- Mechanism: Text-only models process each query independently without persistent state tracking; dynamic diagnosis requires maintaining and updating an internal patient model
- Core assumption: T1→T2 case structure reveals temporal reasoning gaps rather than insufficient training data
- Evidence anchors: DeepSeek-R1 highest at 0.58 mean score in dynamic diagnosis but still struggled with real-time patient changes; mean overall score of 0.54 across all LLMs

## Foundational Learning

- **Concept: Multi-step clinical reasoning chains**
  - Why needed here: Pediatric diagnosis often requires sequential hypothesis refinement (history → exam → tests → diagnosis → treatment)
  - Quick check question: Given partial case information, can you generate both a differential diagnosis AND the next diagnostic step with explicit reasoning?

- **Concept: Pediatric developmental stage variability**
  - Why needed here: Normal values, presentations, and treatments differ dramatically by age
  - Quick check question: How does the normal respiratory rate range differ for a 2-month-old vs. a 6-year-old, and why does this matter for diagnostic thresholds?

- **Concept: Weight-based dosing and safety margins**
  - Why needed here: Pediatric pharmacology requires precise calculations with narrow therapeutic windows
  - Quick check question: A 15kg child needs Drug X at 10mg/kg/dose divided Q8H. What is the single-dose amount, and what verification step prevents 10x errors?

## Architecture Onboarding

- **Component map:** Evaluation harness → API call → response parsing → Scoring modules → Aggregate by subspecialty/dimension
- **Critical path:** 1. Load disease taxonomy and map to question sets 2. Format prompts per task type 3. Call model APIs with uniform parameters 4. Parse outputs and compute metrics 5. Aggregate by subspecialty, difficulty, and dimension
- **Design tradeoffs:** Zero-shot evaluation trades realism for fairness; text-only evaluation excludes multimodal cues; automated BERTScore assists recall but may miss semantic errors
- **Failure signatures:** Multi-choice F1 collapse at senior levels suggests integrative reasoning failure; oncology/cardiology near-zero accuracy indicates subspecialty data gaps; ethics performance heterogeneity suggests inconsistent value encoding
- **First 3 experiments:**
  1. Baseline stratification: Run all 12 models on resident-level single-choice; confirm top-tier (>85%) vs. tail (<75%) separation
  2. Complexity gradient test: Compare single-choice vs. multi-choice accuracy delta for top-3 models; expect 15–20% drop at senior level
  3. Dynamic reasoning probe: Evaluate DeepSeek-R1 on T1→T2 case pairs; measure score persistence vs. degradation between phases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the integration of multimodal data (imaging, physiological signals) improve LLM dynamic reasoning accuracy compared to text-only baselines?
- Basis in paper: The authors identify "multimodal integration" as a critical future direction to address the study's limitation of evaluating only text-based tasks
- Why unresolved: Current PEDIASBench framework and evaluated models could not process visual or signal data
- What evidence would resolve it: Comparative benchmark results from Vision-Language Models on PEDIASBench tasks incorporating clinical imaging and physiological data

### Open Question 2
- Question: Can a "clinical feedback–model iteration loop" effectively enhance model adaptation to real-time patient changes?
- Basis in paper: The authors conclude that future development should focus on this loop to mitigate the observed inability of models to adapt to real-time patient changes
- Why unresolved: The study utilized zero-shot inference without domain-specific fine-tuning or clinician-in-the-loop validation
- What evidence would resolve it: Longitudinal performance metrics showing error reduction in dynamic diagnostic tasks after models are retrained on clinician-corrected outputs

### Open Question 3
- Question: What specific training data or architectural modifications are required to transcend factual accuracy and achieve genuine "humanistic sensitivity"?
- Basis in paper: The authors note that despite high accuracy in safety/ethics, "humanistic sensitivity remained limited"
- Why unresolved: Current metrics successfully evaluate ethical knowledge recall but fail to capture the nuance of empathy or developmentally appropriate communication
- What evidence would resolve it: Qualitative and quantitative validation via patient/caregiver satisfaction scores and standardized empathy metrics in simulated interactions

## Limitations

- Temporal reasoning deficit remains unexplained - could reflect architectural limitations or insufficient training data
- Safety implications are uncertain - laboratory accuracy may not translate to clinical safety
- Generalizability constraints - zero-shot evaluation using text-only models excludes multimodal clinical inputs standard in pediatric practice

## Confidence

- **High confidence**: Basic knowledge application patterns (single-choice accuracy >90% for top models) - directly measured and consistent
- **Medium confidence**: Complexity-induced performance degradation (15% drop) - observed but mechanism not isolated
- **Medium confidence**: Dynamic reasoning limitations - clear performance gaps but unclear whether architectural or data-driven
- **Low confidence**: Readiness for clinical decision support - based on laboratory metrics, not clinical outcomes

## Next Checks

1. **Temporal reasoning probe**: Test whether decomposing dynamic cases into single consolidated prompts improves performance, isolating temporal from reasoning limitations
2. **Clinical translation study**: Evaluate model outputs in simulated clinical workflows with expert reviewers scoring real-world applicability and safety
3. **Multimodal capability assessment**: Test same models on pediatric cases with integrated imaging and vitals data to assess gap between text-only and multimodal performance