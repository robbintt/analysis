---
ver: rpa2
title: 'OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving
  Detection'
arxiv_id: '2511.21064'
source_url: https://arxiv.org/abs/2511.21064
tags:
- visual
- reasoning
- ovod-agent
- detection
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OVOD-Agent introduces a lightweight, LLM-free framework that transforms
  open-vocabulary object detection from static category matching into proactive visual
  reasoning. By modeling visual context transitions as a weakly Markovian decision
  process over eight compact states, it enables interpretable, multi-step refinement
  through seven primitive visual actions (color, texture, geometry, background, lighting,
  spatial, and dictionary-based operations).
---

# OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection

## Quick Facts
- arXiv ID: 2511.21064
- Source URL: https://arxiv.org/abs/2511.21064
- Reference count: 40
- OVOD-Agent improves rare-category detection by +2.7 AP_r on LVIS val with <100 ms overhead

## Executive Summary
OVOD-Agent is a lightweight, LLM-free framework that transforms open-vocabulary object detection from static category matching into proactive visual reasoning. By modeling visual context transitions as a weakly Markovian decision process over eight compact states, it enables interpretable, multi-step refinement through seven primitive visual actions. A UCB-based bandit module explores uncertain regions, while Markov transition statistics and bandit trajectories train a self-supervised reward model in a closed loop, enabling self-evolving detection. Experiments on COCO and LVIS show consistent improvements in rare-category detection (e.g., +2.7 AP r on LVIS val) across multiple backbones, with minimal inference overhead (<100 ms latency, <100 MB disk). The approach outperforms LLM-based CoT methods in accuracy-efficiency trade-offs, providing a scalable foundation for open-world visual reasoning.

## Method Summary
OVOD-Agent introduces a novel Markov-bandit framework for proactive visual reasoning in open-vocabulary object detection. It transforms the detection process from static category matching into a multi-step reasoning chain by modeling visual context transitions as a weakly Markovian decision process over eight compact states. The system uses seven primitive visual actions (color, texture, geometry, background, lighting, spatial, and dictionary-based operations) to iteratively refine detection proposals. A UCB-based bandit module explores uncertain regions, while Markov transition statistics and bandit trajectories train a self-supervised reward model in a closed loop. This enables self-evolving detection without external supervision. Experiments demonstrate consistent improvements in rare-category detection across multiple backbones with minimal overhead.

## Key Results
- +2.7 AP_r improvement on LVIS val for rare-category detection
- <100 ms additional inference latency
- <100 MB disk storage requirement
- Outperforms LLM-based CoT methods in accuracy-efficiency trade-offs

## Why This Works (Mechanism)
OVOD-Agent works by transforming open-vocabulary object detection from a static matching problem into a dynamic reasoning process. The key insight is that visual contexts can be modeled as Markov states, where transitions between states represent meaningful visual transformations. By using seven primitive visual actions (color, texture, geometry, background, lighting, spatial, and dictionary-based operations), the system can iteratively refine detection proposals in an interpretable manner. The UCB-based bandit module explores uncertain regions, gathering trajectories that train a self-supervised reward model. This closed-loop mechanism enables the system to self-evolve and improve detection accuracy over time without requiring additional labeled data.

## Foundational Learning
- **Markov Decision Process (MDP)**: Why needed - Models visual context transitions as states and actions. Quick check - Verify state transitions follow weak Markov property.
- **Upper Confidence Bound (UCB) Algorithm**: Why needed - Balances exploration of uncertain regions with exploitation of known good regions. Quick check - Ensure bandit exploration covers diverse visual contexts.
- **Self-Supervised Learning**: Why needed - Trains reward model without external labels. Quick check - Validate reward model improves with bandit trajectories.
- **Visual Action Space Design**: Why needed - Seven primitive operations cover essential visual transformations. Quick check - Confirm each action meaningfully alters visual context.
- **Weak Markov Property**: Why needed - Allows approximate modeling of complex visual transitions. Quick check - Test if transition probabilities stabilize over time.

## Architecture Onboarding

Component map:
Visual Input -> Markov State Detector -> Action Selector -> Visual Transformer -> Detection Output
                      â†“
                UCB Bandit Explorer -> Trajectory Collector -> Reward Model Trainer

Critical path:
Visual input flows through Markov state detection, action selection, and visual transformation, with the bandit module providing exploration data for reward model training.

Design tradeoffs:
- Lightweight vs. accuracy: LLM-free design reduces computational overhead but may miss some complex reasoning patterns
- Exploration vs. exploitation: UCB bandit balances discovering new visual contexts with refining known ones
- State granularity: Eight states provide sufficient context without overwhelming the decision process

Failure signatures:
- Bandit exploration gets stuck in local visual patterns
- Markov transitions become too predictable, limiting exploration
- Reward model overfits to specific training trajectories

First experiments to run:
1. Test basic Markov state detection on diverse visual contexts
2. Validate UCB bandit exploration covers the full visual action space
3. Measure self-supervised reward model improvement over training iterations

## Open Questions the Paper Calls Out
None

## Limitations
- May miss complex reasoning patterns that LLMs could capture
- Performance depends on quality of initial Markov state detection
- Exploration-exploitation balance may need tuning for different datasets

## Confidence

| Claim | Confidence |
|-------|------------|
| +2.7 AP_r improvement on LVIS val | High |
| <100 ms inference overhead | High |
| Outperforms LLM-based methods | Medium |
| Self-evolving capability | Medium |

## Next Checks
1. Verify Markov state transitions follow weak Markov property on diverse visual datasets
2. Test UCB bandit exploration coverage across different visual contexts and action spaces
3. Validate self-supervised reward model improves with bandit trajectory training