---
ver: rpa2
title: We Still Don't Understand High-Dimensional Bayesian Optimization
arxiv_id: '2512.00170'
source_url: https://arxiv.org/abs/2512.00170
tags:
- linear
- optimization
- kernel
- bayesian
- kernels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates high-dimensional Bayesian optimization
  (BO), demonstrating that a simple linear Gaussian process (GP) model with a geometric
  transformation can match or exceed state-of-the-art performance across 60-6000 dimensional
  tasks. The key insight is that linear models suffer from boundary-seeking behavior
  in standard BO, which the authors address by projecting inputs onto a unit hypersphere
  using inverse stereographic projection.
---

# We Still Don't Understand High-Dimensional Bayesian Optimization

## Quick Facts
- arXiv ID: 2512.00170
- Source URL: https://arxiv.org/abs/2512.00170
- Reference count: 40
- Primary result: Simple linear Gaussian process models with geometric transformation match or exceed state-of-the-art performance on 60-6000 dimensional Bayesian optimization tasks.

## Executive Summary
This paper challenges conventional wisdom in high-dimensional Bayesian optimization by demonstrating that simple linear models, often dismissed in the literature, can match or exceed state-of-the-art performance when properly regularized through geometric transformations. The authors show that standard linear kernels fail due to boundary-seeking behavior in high-dimensional spaces, which they correct by projecting inputs onto a unit hypersphere using inverse stereographic projection. Their method achieves competitive performance on classic benchmarks while offering significant computational advantages—linear time complexity in data size and exact Thompson sampling—compared to O(N³) complexity for standard kernels. The work highlights that we still don't fully understand what properties make surrogate models successful in high-dimensional BO.

## Method Summary
The method uses a Bayesian linear regression model with a spherical projection layer that maps inputs from a hypercube to a unit hypersphere. The linear kernel is defined as k(x,x') = b₀ + b₁·P(z)ᵀP(z') where P is the inverse stereographic projection. Inputs are first scaled by dimension-specific lengthscales ℓᵢ ~ LogNormal(√2, √3) and a global lengthscale a = √D·c. The projection ensures that posterior mean and variance do not increase with input magnitude, preventing the boundary-seeking pathology that plagues standard linear models. The method uses LogEI acquisition and optimizes hyperparameters via marginal likelihood. For large datasets (N > 20,000), the O(ND²) complexity enables exact Thompson sampling.

## Key Results
- Linear models with spherical projection match state-of-the-art performance on 60-6000 dimensional tasks
- Standard linear kernels without projection fail catastrophically, acquiring only boundary points
- The method achieves significant computational advantages for large-N problems (N = 20,000+)
- Performance is competitive across diverse benchmarks including Rover, MOPTA08, Lasso-DNA, SVM, Ant, Humanoid, and molecular optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Spherical projection eliminates boundary-seeking pathology in high-dimensional linear Bayesian optimization.
- Mechanism: Standard Bayesian linear regression on a hypercube maximizes acquisition at boundaries because posterior statistics scale with input magnitude. Mapping inputs to a unit hypersphere removes this magnitude scaling, allowing optimization to focus on direction rather than magnitude.
- Core assumption: Poor performance of standard linear kernels is primarily due to geometric pathology, not lack of expressiveness.
- Evidence anchors: Theorem 1 proves boundary-seeking behavior for acquisition functions increasing in posterior mean and variance; inverse stereographic projection corrects this by fixing input norm.
- Break condition: Boundary-seeking theorem does not apply to acquisition functions not monotonically increasing in posterior mean/variance.

### Mechanism 2
- Claim: Linear models provide sufficient fidelity when N ≈ D in high-dimensional BO.
- Mechanism: In data-sparse regimes, expressive models overfit while linear models offer useful locally-linear approximations matching available data's information capacity.
- Core assumption: Objective function structure can be meaningfully approximated by linear functions given sparse data.
- Evidence anchors: Linear models match state-of-the-art performance across 60-6000 dimensional tasks with N=1000 evaluations.
- Break condition: If true objective is highly non-linear and budget N is large enough to learn this structure.

### Mechanism 3
- Claim: Linear models offer superior computational scalability for large-N problems.
- Mechanism: Linear GPs use O(D) feature maps reducing inference to O(ND²) vs O(N³) for RBF kernels, enabling exact Thompson sampling.
- Core assumption: Cubic GP inference cost is primary bottleneck for scaling to large N.
- Evidence anchors: Linear models exploit closed-form sampling and linear scaling for molecular optimization with >20,000 observations.
- Break condition: If dimensionality D is so large that O(ND²) is also prohibitive.

## Foundational Learning

- Concept: **Bayesian Linear Regression as a Gaussian Process**
  - Why needed here: The paper frames linear models as GPs with linear kernels. Understanding the equivalence to Bayesian linear models with Gaussian priors on weights is essential for grasping computational advantages.
  - Quick check question: How does computational complexity of prediction differ between GP with linear kernel vs RBF kernel?

- Concept: **The "Curse of Dimensionality" in Bayesian Optimization**
  - Why needed here: Core motivation is overcoming exponential scaling of regret with dimensionality. The paper argues simple models counterintuitively perform better in this regime.
  - Quick check question: Why does a linear model's "smoothness" help in high-dimensional, data-sparse regime?

- Concept: **Acquisition Function Pathology**
  - Why needed here: Key mechanism is that standard linear models fail because they push acquisitions to hypercube boundary. Understanding how acquisition functions balance posterior mean and variance is key to seeing why constant input norm solves this.
  - Quick check question: For Bayesian linear model, how does posterior variance change as input magnitude increases?

## Architecture Onboarding

- Component map: Input Scaler -> Spherical Projection P -> Linear GP Surrogate -> Acquisition Optimizer
- Critical path: The spherical projection P is most critical component. Ablation studies show removing it causes catastrophic failure.
- Design tradeoffs:
  - Linearity vs. Expressiveness: Trades RBF kernel capacity for linear model tractability, beneficial in N ≈ D or N ≫ D regimes
  - Projection Choice: Inverse stereographic projection chosen for identity property on unit-norm inputs while fixing boundary pathology
- Failure signatures:
  - Standard Linear Kernel: Optimization gets stuck querying only boundary/corner points (||x||∞ = 1)
  - Expressive Kernels on large-N: Runs out of memory/time due to O(N³) computational scaling
- First 3 experiments:
  1. Ablation on Projection: Re-run HDBO benchmark with proposed linear kernel but replace inverse stereographic projection with (a) no projection and (b) simple normalization. Compare optimization curves.
  2. Boundary Behavior Visualization: For 2D problem, visualize acquisition function landscape for (a) standard linear model and (b) proposed spherically-projected model. Confirm maximum locations.
  3. Scalability Test: On large-N molecular optimization task, compare proposed method against scalable GP baseline measuring best-found value and wall-clock time per iteration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific properties beyond model expressiveness or generalization actually determine success for surrogate models in high-dimensional Bayesian optimization?
- Basis in paper: Section 5 states "fully characterizing which properties actually matter for HDBO success—beyond standard notions of expressiveness or generalization—remains an important open question."
- Why unresolved: Linear and spherical linear models have similar predictive accuracy on random data, yet vastly different optimization performance, breaking link between supervised learning metrics and BO efficiency.
- What evidence would resolve it: Theoretical framework or new metric correlating with optimization performance better than RMSE, accounting for acquisition function behavior on adaptive data.

### Open Question 2
- Question: Why does spherically-projected linear model significantly outperform "Vanilla" RBF-based BO on latent-space molecular optimization tasks?
- Basis in paper: Appendix E.1 notes method "significantly outperforms Vanilla BO on all these benchmarks" and leaves thorough analysis to further work.
- Why unresolved: Paper establishes empirical success but lacks theoretical justification for why linear kernel is better suited for VAE latent spaces than universal approximator like RBF kernel.
- What evidence would resolve it: Analysis of intrinsic dimensionality or loss landscapes of latent spaces showing properties favorable to linear surrogates or detrimental to RBF lengthscale scaling.

### Open Question 3
- Question: How can drastic performance improvement of spherical mappings be reconciled with fact they are asymptotically equivalent to standard linear models almost everywhere?
- Basis in paper: Section 5 highlights tension where spherical mapping prevents boundary-seeking behavior, yet Section A.5 proves mapping behaves like identity operation for most points in high dimensions.
- Why unresolved: Authors note this contradiction is "puzzling": if mapping doesn't meaningfully increase expressiveness or alter most input space, mechanism by which it fixes optimization pathology remains unclear.
- What evidence would resolve it: Proof characterizing how projection alters search dynamics in thin-shell region versus boundary corners, showing small geometric changes in high-dimensional volumes have non-linear effects on acquisition optimization.

## Limitations
- Linear model simplicity may break down for highly non-linear functions where more expressive surrogates would be beneficial
- Performance gains vary by task, with no universally superior method across all benchmarks
- Practical computational speedup depends on implementation details and comparison baselines

## Confidence

**High:** The geometric projection mechanism effectively prevents boundary-seeking behavior in linear models, as demonstrated by ablation studies and visualizations.

**Medium:** Linear models with spherical projection match or exceed state-of-the-art performance across diverse benchmarks, though absolute performance gains vary by task.

**Medium:** The O(ND²) computational complexity offers practical advantages for large datasets, but the magnitude depends on implementation and comparison methods.

## Next Checks

1. Perform ablation study comparing inverse stereographic projection to other spherical projections (e.g., simple normalization) on representative HDBO benchmark.
2. Visualize acquisition function landscapes for both standard and spherically-projected linear models in 2D setting to confirm boundary-seeking pathology is resolved.
3. On large-N molecular optimization task, benchmark proposed method against scalable GP baseline (e.g., SVGP) measuring both optimization performance and wall-clock time per iteration.