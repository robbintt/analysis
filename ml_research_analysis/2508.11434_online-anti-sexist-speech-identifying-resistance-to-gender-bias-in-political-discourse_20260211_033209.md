---
ver: rpa2
title: 'Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political
  Discourse'
arxiv_id: '2508.11434'
source_url: https://arxiv.org/abs/2508.11434
tags:
- sexist
- political
- anti-sexist
- prompt
- sexism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Automated content moderation systems, powered by large language
  models (LLMs), frequently misclassify anti-sexist speech as harmful, especially
  during politically charged events. This study evaluates five LLMs on UK political
  tweets about female MPs, revealing that models often fail to distinguish anti-sexist
  resistance from sexism, leading to false positives.
---

# Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse

## Quick Facts
- **arXiv ID**: 2508.11434
- **Source URL**: https://arxiv.org/abs/2508.11434
- **Reference count**: 40
- **Primary result**: LLMs misclassify anti-sexist speech as harmful, especially during politically charged events

## Executive Summary
This study evaluates five large language models on their ability to distinguish anti-sexist speech from sexist content in UK political discourse. The models consistently misclassify resistance speech as harmful, particularly during politically salient events where rhetorical styles converge. Standard uncertainty metrics like perplexity and entropy fail to reliably indicate when models are wrong, showing flat confidence profiles regardless of accuracy. The findings highlight the need for moderation systems to recognize anti-sexist speech as a distinct category and integrate human review during sensitive events.

## Method Summary
The study collected 1,343 English tweets mentioning UK female MPs from 2022, filtered around 8 trigger events like political controversies. Tweets were annotated using minority voting by 9 expert annotators. Five open-source LLMs were evaluated using four prompt types with increasing instruction complexity: roleplay, content, zero-shot linguistic, and few-shot linguistic. Classification performance was measured using macro-F1, accuracy, precision, and recall, with perplexity and predictive entropy serving as uncertainty metrics.

## Key Results
- LLMs show high false positive rates for anti-sexist speech, especially during politically charged events
- Model confidence (perplexity and entropy) remains flat regardless of classification accuracy
- Prompt complexity affects performance but improvements are limited
- Anti-sexist recall remains consistently low (<20% for some models)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Anti-sexist speech is systematically misclassified because it shares linguistic and emotional characteristics with the sexist content it opposes, leading models to conflate stance with toxicity.
- **Mechanism**: LLMs rely heavily on surface lexical cues rather than pragmatic intent. When anti-sexist speech critiques sexism using emotionally charged or confrontational language—especially during politically salient events—models trained on binary harmful/not-harmful schemas cannot distinguish resistance from harm.
- **Core assumption**: Classification depends on recognizing speaker stance (opposition to sexism vs. reinforcement of sexism), not just lexical similarity.
- **Evidence anchors**:
  - [abstract]: "models frequently misclassify anti-sexist speech as harmful, particularly during politically charged events where rhetorical styles of harm and resistance converge"
  - [section 2]: "anti-sexist speech presents a unique challenge: it often shares linguistic and emotional characteristics (particularly tone, vocabulary and phrasing) with the sexist content it opposes"
- **Break condition**: If stance-detection mechanisms could be incorporated (e.g., identifying who is being criticized and why), the overlap problem may be partially mitigated.

### Mechanism 2
- **Claim**: Standard uncertainty metrics (perplexity, predictive entropy) do not reliably indicate when models are wrong, creating false confidence in moderation decisions.
- **Mechanism**: Models produce low perplexity and near-constant predictive entropy (~5.0 across categories) regardless of classification accuracy. This flat confidence profile suggests models lack sensitivity to category-ambiguous inputs and cannot self-assess reliability on socially nuanced tasks.
- **Core assumption**: Model uncertainty should correlate with classification difficulty; when it does not, the uncertainty signal is miscalibrated.
- **Evidence anchors**:
  - [section 6.2]: "we find minimal variation in PE across categories and prompt strategies, implying that models are often uniformly confident—even when wrong"
  - [section 6.2]: "models exhibit flat, non-discriminative confidence, raising concerns about their sensitivity to nuanced classification tasks"
- **Break condition**: If entropy/perplexity were replaced or augmented with semantic uncertainty measures or ensemble disagreement metrics, the confidence signal may become more informative.

### Mechanism 3
- **Claim**: Politically salient "trigger events" amplify misclassification by increasing emotional intensity and rhetorical overlap between sexist and anti-sexist discourse.
- **Mechanism**: Offline events (scandals, leadership changes, controversies) correlate with surges in online activity where both sexist attacks and anti-sexist resistance intensify. The confrontational style of counter-speech during these periods mirrors the toxicity it opposes, confusing models further.
- **Core assumption**: Event salience is a proxy for discourse complexity; models do not adjust classification thresholds dynamically based on context.
- **Evidence anchors**:
  - [section 3]: "offline events such as elections, protests, or political controversies often correspond with increases in online hate speech"
  - [section 6.2]: "event-linked trends suggest that models are particularly vulnerable to misclassification during high-salience political moments... where the linguistic overlap between harmful and resistant speech is greatest"
- **Break condition**: If event-aware context windows or dynamic thresholding were implemented during high-salience periods, misclassification rates may be reduced.

## Foundational Learning

- **Concept: Stance Detection vs. Toxicity Detection**
  - **Why needed here**: The core failure mode is confusing what a text *says* (toxic language) with what it *means* (opposing sexism). Stance detection identifies speaker orientation toward an entity or claim; toxicity detection flags surface-level harm signals.
  - **Quick check question**: Given "This misogynist smear campaign is disgusting," would a toxicity classifier flag it? Would a stance classifier interpret it as anti-sexist?

- **Concept: Uncertainty Calibration in LLMs**
  - **Why needed here**: The paper shows perplexity and entropy do not correlate with accuracy. Understanding calibration—whether model confidence reflects true probability of correctness—is essential for designing human-in-the-loop triggers.
  - **Quick check question**: If a model outputs 90% confidence on a prediction that is 50% accurate, is it well-calibrated?

- **Concept: Minority/Disagreement-Aware Annotation**
  - **Why needed here**: The paper uses minority voting rather than majority voting to preserve interpretive diversity. In subjective tasks, disagreement is signal, not noise.
  - **Quick check question**: If three annotators label a tweet as [sexist, neither, neither], what label does minority voting produce? What does majority voting produce?

## Architecture Onboarding

- **Component map**: Input Layer -> Prompt Engineering Module -> Classification Layer -> Uncertainty Estimation -> Post-Processing -> Human-in-the-Loop Trigger
- **Critical path**: Collect tweets around trigger events → clean and filter → annotate using minority voting scheme → prompt LLMs with four instruction types → compute uncertainty metrics → compare against human agreement → analyze false positive rates
- **Design tradeoffs**:
  - Binary vs. ternary classification: Binary is simpler but silences counter-speech; ternary preserves nuance but increases annotation cost and model confusion
  - Majority vs. minority voting: Majority reduces noise but erases interpretive diversity; minority preserves disagreement but may amplify outliers
  - Zero-shot vs. few-shot prompting: Few-shot adds examples but can override pre-trained biases unpredictably; zero-shot relies on parametric knowledge but may underperform on domain-specific nuance
- **Failure signatures**: High false positive rate on anti-sexist speech during politically charged months; flat predictive entropy (~5.0) across all categories; models over-predict sexism; anti-sexist recall remains consistently low
- **First 3 experiments**:
  1. Prompt sensitivity analysis: Run the same tweets through all four prompt types; measure JSD between output distributions
  2. Event-stratified evaluation: Split test set by trigger event type; compare accuracy, recall, and perplexity across event types
  3. Human-in-the-loop threshold tuning: Vary perplexity threshold for flagging content; measure tradeoff between review volume and false positive reduction

## Open Questions the Paper Calls Out
- **Open Question 1**: Does the misclassification of anti-sexist speech persist across different linguistic and cultural contexts outside of UK political discourse? The current study is restricted to English-language tweets directed at UK Members of Parliament.
- **Open Question 2**: How does the salience of a political event interact with specific platform dynamics to influence model error rates? This study focused on tweet content but did not analyze platform algorithmic effects.
- **Open Question 3**: Can novel uncertainty metrics be developed to reliably indicate model correctness when standard measures like perplexity fail? Standard probability-based confidence measures are insufficient for capturing the nuance of socially situated speech acts.

## Limitations
- Dataset focuses exclusively on UK political discourse around female MPs, limiting generalizability
- Ground truth determined by minority voting may amplify interpretive outliers rather than consensus
- Only 8 trigger events examined, with April showing highest misclassification rates

## Confidence
- **High confidence**: LLMs show higher false positive rates on anti-sexist speech during politically charged events; prompt complexity affects performance; uncertainty metrics don't reliably indicate correctness
- **Medium confidence**: The convergence of linguistic features between sexist and anti-sexist speech explains misclassification; event salience amplifies these errors
- **Low confidence**: Specific threshold values for human review triggers; generalizability across different political systems or social media platforms

## Next Checks
1. **Cross-cultural validation**: Test the same models on anti-sexist speech datasets from different countries and languages to assess generalizability
2. **Uncertainty metric expansion**: Compare perplexity and entropy against alternative uncertainty measures (e.g., Monte Carlo dropout variance, ensemble disagreement) to identify more reliable confidence signals
3. **Dynamic threshold evaluation**: Implement event-aware classification thresholds that adjust based on political salience indicators and measure impact on false positive rates for anti-sexist speech