---
ver: rpa2
title: 'Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage
  Task'
arxiv_id: '2505.21850'
source_url: https://arxiv.org/abs/2505.21850
tags:
- panel
- stage
- objects
- dependent
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MultiStAR, a Multi-Stage Abstract Visual
  Reasoning benchmark that evaluates Multimodal Large Language Models (MLLMs) on intermediate
  reasoning steps rather than just final outcomes. The benchmark consists of two tasks:
  Direct Answer, which tests perception and reasoning at varying complexity levels,
  and Logical Chain, which measures sequential reasoning with dependencies between
  stages.'
---

# Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task

## Quick Facts
- arXiv ID: 2505.21850
- Source URL: https://arxiv.com/abs/2505.21850
- Reference count: 40
- One-line primary result: Introduces MultiStAR benchmark and MSEval metric to evaluate intermediate reasoning steps in abstract visual reasoning, revealing MLLMs struggle with complex rule detection despite good perception performance.

## Executive Summary
This paper introduces MultiStAR, a Multi-Stage Abstract Visual Reasoning benchmark that evaluates Multimodal Large Language Models (MLLMs) on intermediate reasoning steps rather than just final outcomes. The benchmark consists of two tasks: Direct Answer, which tests perception and reasoning at varying complexity levels, and Logical Chain, which measures sequential reasoning with dependencies between stages. To better assess correctness across all steps, the authors propose MSEval, a novel metric that incorporates the accuracy of intermediate stages alongside the final outcome using conditional mutual information for weighting. Experiments on 17 representative MLLMs show that while models perform well on basic perception tasks, they struggle with complex rule detection stages, highlighting the gap between current MLLMs and human reasoning abilities.

## Method Summary
MultiStAR is built on the RAVEN dataset and introduces two evaluation tasks: Direct Answer (independent questions at 6 complexity levels: 1P-Basic, 1P-Comparison, 2P-Comparison, 1R-Deduction, 2R-Deduction, Final) and Logical Chain (sequential reasoning with dependencies between stages). The benchmark uses template-based question generation with GPT-4o paraphrasing and functional program execution for ground truth answers. The novel MSEval metric computes weighted joint probability of correct answers across dependent nodes using normalized conditional mutual information (NCMI) for weighting. The study evaluates 17 MLLMs (both closed-source like GPT-4o and Gemini-1.5-pro, and open-source like Qwen2-VL and InternVL2) using zero-shot inference with Huggingface transformers.

## Key Results
- MLLMs achieve ~85% accuracy on basic perception tasks but performance drops to near-random levels (~25-40%) on complex rule deduction stages
- MSEval reveals that models often get final answers correct through memorization rather than proper intermediate reasoning
- Prior information injection improves intermediate reasoning stages but shows limited utility for final puzzle solutions
- Qwen2-VL-72B shows anomalous high final accuracy (65.7%) suggesting potential pattern memorization rather than genuine reasoning

## Why This Works (Mechanism)

### Mechanism 1: Multi-Stage Decomposition Exposes Reasoning Failure Points
Breaking abstract visual reasoning into intermediate stages reveals where MLLMs fail, rather than just showing that they fail. The Direct Answer subtask isolates six configurations (1P-Basic, 1P-Comparison, 2P-Comparison, 1R-Deduction, 2R-Deduction, Final). Performance degradation from ~85% accuracy on basic perception (GPT-4o: 88.1% on 1P-B) to near-random on complex rule deduction (~25-40%) exposes a depth-vs-perception gap. Stage-wise accuracy reflects genuine reasoning failures, not just task format artifacts.

### Mechanism 2: Conditional Mutual Information Weighting Captures Inter-Stage Dependencies
NCMI-based weighting in MSEval provides a more informative measure of multi-step reasoning success than binary accuracy. MSEval computes joint probability across dependent stages and weights each node's contribution by its influence on the current node (measured via perturbation-based conditional mutual information). This penalizes models that get the final answer right with incorrect intermediate reasoning (e.g., pattern memorization).

### Mechanism 3: Prior Information Improves Intermediate But Not Final Stage Performance
Injecting ground-truth or predicted prior information helps intermediate reasoning stages but has limited or negative impact on the final RAVEN puzzle solution. The Logical Chain subtask provides reformatted prior answers (Ht) to the current stage. This reduces perception errors but does not consistently improve final-stage accuracy, suggesting models struggle to integrate explicit rules into final-panel selection.

## Foundational Learning

- **Abstract Visual Reasoning (AVR)**: Why needed here: MultiStAR is built on RAVEN, a classic AVR benchmark. Understanding that AVR requires identifying rules from arbitrary visual patterns (not real-world knowledge) is essential to interpret performance gaps. Quick check question: "What distinguishes AVR from commonsense visual reasoning tasks like OK-VQA?"

- **Multi-Step Reasoning Chains**: Why needed here: The Logical Chain subtask requires tracking dependencies across 5 stages (1P→2P→1R→2R→Final). Understanding how errors propagate or compound is key to interpreting MSEval scores. Quick check question: "If a model gets 2P wrong but 1R right, does MSEval penalize this more or less than accuracy would?"

- **Evaluation Metrics Beyond Accuracy**: Why needed here: MSEval is motivated by the failure of accuracy to capture partial success. Understanding metrics like joint probability, CMI, and log-space aggregation is necessary to debug and extend the evaluation. Quick check question: "Why does MSEval use log-space aggregation instead of raw probability products?"

## Architecture Onboarding

- **Component map**: RAVEN XML -> Template-based question generation -> GPT-4o paraphrasing -> Functional program execution for ground truth -> Direct Answer Evaluator (6-stage accuracy) -> Logical Chain Engine (pre-defined corpus-level dependency graph) -> MSEval Calculator (logit extraction -> softmax probabilities -> NCMI weighting via perturbation -> log-space aggregation)

- **Critical path**: 1) Verify dataset integrity: Check template→functional program→ground truth alignment on a sample. 2) Validate MSEval implementation: Reproduce scores on the provided qualitative examples before scaling. 3) Run baseline: Evaluate a small open-source model (e.g., InternVL2-2B) on Direct Answer to establish expected performance ranges.

- **Design tradeoffs**: 1) Corpus-level vs instance-level chains: Current chains are fixed across instances; some rules may require second-row info not in the chain. 2) Closed-source vs open-source evaluation: MSEval requires logit access; closed-source models report only accuracy. 3) Prompt length vs context utility: Prior injection increases prompt length to ~261 tokens, potentially degrading attention.

- **Failure signatures**: 1) High Final accuracy, low MSEval: Indicates memorization without reasoning. 2) Large accuracy-MSEval divergence on intermediate stages: Suggests brittle dependencies or overfitting to local patterns. 3) No improvement from GT prior info: May indicate prompt integration failure or architectural inability to use explicit rules.

- **First 3 experiments**: 1) Ablate the chain depth: Truncate the logical chain at 1P→2P and measure impact on 1R performance to isolate dependency value. 2) Error type analysis: Manually label a subset of Logical Chain failures as perception vs reasoning vs propagation errors. 3) Instance-level chain pilot: For a single RAVEN configuration (e.g., Type="5"), design instance-specific chains that include all necessary prior info and compare to corpus-level chain performance.

## Open Questions the Paper Calls Out
- Can automatic "Instance-Level" chain construction methods enable models to dynamically generate logical chains based on individual patterns, overcoming the limitations of fixed "Corpus-Level" chains?
- Is the MSEval metric effective for evaluating multi-step reasoning in non-visual domains such as mathematics or science?
- To what extent does performance on the final RAVEN puzzle in the Logical Chain task rely on pattern memorization rather than genuine logical deduction from intermediate steps?

## Limitations
- Current implementation uses fixed "Corpus-Level" chains that may not provide sufficient prior information for specific instances requiring second-row data
- MSEval requires access to model logits, limiting evaluation to open-source models or those with logit access
- Performance anomalies in models like Qwen2-VL-72B suggest potential dataset contamination or memorization issues that require further investigation

## Confidence
- **High confidence**: Perception-reasoning gap existence (supported by consistent performance drops across all tested models from 85% to 25-40% accuracy)
- **Medium confidence**: MSEval's superiority over accuracy (supported by qualitative examples but lacking ablation studies or human validation)
- **Medium confidence**: Prior information benefits intermediate reasoning (supported by specific performance improvements in 1R/2R stages, though Final-stage limitations remain)
- **Low confidence**: NCMI weighting's correlation with human reasoning quality (no external validation or comparison to alternative weighting schemes)

## Next Checks
1. **Ablate the chain depth**: Run Direct Answer evaluations with truncated question sequences (1P→2P only) to determine if performance gains from intermediate stages transfer when dependencies are shortened, isolating whether the reasoning bottleneck is stage depth or stage complexity.

2. **Cross-validate MSEval with human judges**: Have human annotators rate the coherence of model reasoning chains on a subset of Logical Chain examples, then correlate these ratings with MSEval scores to establish whether the metric captures human-perceived reasoning quality beyond accuracy.

3. **Test instance-level vs corpus-level chains**: For a single RAVEN configuration type (e.g., "5"), design instance-specific dependency chains that include all necessary prior information for each rule, then compare performance to the corpus-level chains to determine if the current methodology's fixed dependency structure artificially limits model performance.