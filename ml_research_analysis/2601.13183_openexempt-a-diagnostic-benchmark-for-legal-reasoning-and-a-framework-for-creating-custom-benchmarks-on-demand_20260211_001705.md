---
ver: rpa2
title: 'OpenExempt: A Diagnostic Benchmark for Legal Reasoning and a Framework for
  Creating Custom Benchmarks on Demand'
arxiv_id: '2601.13183'
source_url: https://arxiv.org/abs/2601.13183
tags:
- task
- reasoning
- legal
- exemption
- asset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OpenExempt introduces a dynamic, diagnostic benchmark for legal
  reasoning that addresses the limitations of static evaluation methods. The framework
  uses expert-crafted symbolic representations of U.S.
---

# OpenExempt: A Diagnostic Benchmark for Legal Reasoning and a Framework for Creating Custom Benchmarks on Demand

## Quick Facts
- **arXiv ID:** 2601.13183
- **Source URL:** https://arxiv.org/abs/2601.13183
- **Reference count:** 40
- **Primary result:** OpenExempt introduces a dynamic, diagnostic benchmark for legal reasoning using expert-crafted symbolic representations of U.S. Bankruptcy Code statutes to generate complex reasoning tasks and machine-computable solutions on demand, revealing sharp performance cliffs in current language models under longer reasoning paths and obfuscating statements.

## Executive Summary
OpenExempt introduces a dynamic, diagnostic benchmark for legal reasoning that addresses the limitations of static evaluation methods. The framework uses expert-crafted symbolic representations of U.S. Bankruptcy Code statutes to generate complex reasoning tasks and machine-computable solutions on demand, enabling fine-grained control over task complexity and isolation of specific reasoning skills. The resulting OpenExempt Benchmark includes 9,765 samples across nine evaluation suites, with experiments on 13 diverse language models revealing sharp performance cliffs under longer reasoning paths and in the presence of obfuscating statements. The benchmark is publicly released to support research in legal reasoning and NLP communities.

## Method Summary
OpenExempt generates diagnostic benchmarks for legal reasoning by encoding U.S. Bankruptcy Code statutes into symbolic representations, then using expert-validated templates to create natural language tasks paired with machine-computable ground truth solutions. The framework employs a branch-and-bound solver to compute optimal exemption allocations and includes nine evaluation suites that isolate specific reasoning dimensions like temporal logic, asset scaling, and obfuscation resistance. The system produces 9,765 samples across five sequential tasks (Allowable Exemptions, Exemption Classification, Exemption Valuation, Non-exempt Assets, Optimal Exemptions) evaluated via zero-shot prompting on 13 language models.

## Key Results
- Performance collapses sharply on estate-level tasks (Task OE) when asset count exceeds 8 items, with efficient models dropping to F1 below 0.1
- Four of six reasoning models perform worse when provided with gold intermediate solutions, suggesting reinforcement of end-to-end reasoning trajectories
- Gemma-3 shows highest malformed response rate (2.54%) among evaluated models, though all remain below 3% overall
- Standard suites show F1 scores ranging from 0.12 to 0.63 across tasks, with temporal reasoning and obfuscation resistance presenting the steepest challenges

## Why This Works (Mechanism)

### Mechanism 1: Dual-Representation Task Generation
- **Claim:** Symbolic encodings paired with natural language templates enable controlled, verifiable benchmark generation.
- **Mechanism:** Legal experts encode statutes (constraints, dependencies) and assets into structured representations. The TaskGenerator samples from these, then renders via manually validated templates—preserving both linguistic variety and ground truth alignment.
- **Core assumption:** Encoded constraints capture the operative logic needed for correct solutions; ambiguous provisions are excluded rather than approximated.
- **Evidence anchors:**
  - [abstract] "uses expert-crafted symbolic representations... to generate complex reasoning tasks and machine-computable solutions on demand"
  - [Section 3.2] "each asset or statute exists as a pair: a natural language form used in constructing the task prompt, and a structured representation used in computing gold solutions"
  - [corpus] Weak/no direct corpus evidence for this specific dual-representation approach in legal benchmarks.

### Mechanism 2: Branch-and-Bound Combinatorial Solver
- **Claim:** Exhaustive search over legally valid allocations yields objectively correct solutions for estate-level optimization.
- **Mechanism:** The solver explores all valid exemption assignments using branch-and-bound, pruning partial solutions that cannot exceed known optima. Constraints (caps, mutual exclusions, fallbacks) are enforced symbolically, ensuring only legally valid paths are searched.
- **Core assumption:** The constraint representation fully captures statutory logic; the search space remains tractable after pruning (exponential in assets but bounded).
- **Evidence anchors:**
  - [Section 3.4] "performs a branch and bound search over all legally valid exemption assignments... pruning partial solutions that cannot surpass the best known allocation"
  - [Section 3.4.1] "ensures that our derived ground truth solutions remain computationally verifiable"
  - [corpus] SARA dataset (Holzenberger et al., 2020) uses Prolog encodings but with hand-crafted scenarios—OpenExempt extends this with dynamic generation.

### Mechanism 3: Single-Axis Diagnostic Isolation
- **Claim:** Varying one complexity dimension while holding others constant isolates specific failure modes.
- **Mechanism:** Diagnostic suites (Temporal Reasoning, Asset Scaling, Obfuscation, etc.) increment single parameters (domicile count, asset count, distractor density) while fixing others. Performance deltas across these controlled variations reveal which reasoning skills break down.
- **Core assumption:** Failure modes are sufficiently separable; interaction effects between dimensions do not dominate.
- **Evidence anchors:**
  - [abstract] "revealing sharp performance cliffs that emerge only under longer reasoning paths and in the presence of obfuscating statements"
  - [Section 4.2] "six diagnostic suites isolate and vary one specific dimension of task complexity"
  - [corpus] Related work (DiagnosisArena, Neural-MedBench) similarly advocates deeper diagnostic evaluation beyond accuracy metrics.

## Foundational Learning

- **Concept: Asset Exemption as Combinatorial Optimization**
  - **Why needed here:** The core task is a knapsack-like problem—assets compete for finite statutory limits. Understanding this frames why complexity grows super-linearly.
  - **Quick check question:** Given 3 assets eligible for the same $10,000 exemption cap, how many allocation combinations must be evaluated?

- **Concept: Temporal Residency Rules (730-Day Rule)**
  - **Why needed here:** Jurisdiction determination requires reasoning about domicile duration and location; errors propagate to all downstream tasks.
  - **Quick check question:** If a debtor lived in State A for 400 days, then State B for 400 days, which state's exemptions apply?

- **Concept: Exemption Constraints (Caps, Mutual Exclusion, Fallbacks)**
  - **Why needed here:** These dependencies encode the logic the solver must enforce and models must infer.
  - **Quick check question:** If Exemption X has a fallback to Exemption Y's unused limit, what happens to Y's remaining capacity after X claims from it?

## Architecture Onboarding

- **Component map:** CaseGenerator -> TaskGenerator -> Solver -> Evaluator -> Knowledge Layer
- **Critical path:** Config → CaseGenerator → (symbolic case object) → TaskGenerator → (prompt + ground truth) → Model → Evaluator → Metrics. For estate-level tasks (NA, OE), the Solver must run before prompt assembly.
- **Design tradeoffs:**
  - Template-based NL generation vs. LM-generated text: Templates sacrifice diversity for precision (avoids hallucinated facts).
  - Exclusion of subjective provisions vs. realism: Prioritizes objective correctness over domain coverage.
  - Zero-shot evaluation vs. few-shot: Chosen for baseline clarity; few-shot left for future work.
- **Failure signatures:**
  - Malformed JSON responses (<3% for most models, but up to 2.5% for Gemma-3; see Table 3)
  - Performance collapse on Task OE with 8+ assets (efficient models drop to <0.1 F1)
  - Negative delta when gold intermediate steps are provided (reasoning models show this paradoxically)
- **First 3 experiments:**
  1. **Asset scaling stress test:** Run Task OE with 2→10 assets on your target model; plot F1 to identify the breaking point.
  2. **Obfuscation robustness:** Compare baseline vs. distractor/obfuscation suites on Task EC and Task OE to measure delta per task type.
  3. **Reasoning decomposition:** Provide gold AE/EC/EV solutions to the model for Task NA/OE and measure whether performance improves or degrades—this reveals if the model relies on end-to-end reasoning trajectories.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does few-shot prompting improve performance on OpenExempt tasks, and if so, does it reduce the performance cliffs observed in zero-shot evaluation?
- **Basis in paper:** [explicit] "We focus this work on evaluating models in a zero-shot setting to establish baseline performance, but leave exploration of few-shot learning for future work."
- **Why unresolved:** The authors deliberately constrained experiments to zero-shot evaluation to establish baselines, citing prior work showing LMs struggle with in-context demonstrations in legal domains.
- **What evidence would resolve it:** Experiments comparing zero-shot vs. few-shot performance across all nine evaluation suites, particularly measuring whether few-shot examples mitigate degradation under asset scaling and obfuscation.

### Open Question 2
- **Question:** Why does providing gold intermediate solutions sometimes degrade reasoning model performance on downstream estate-level tasks (NA, OE)?
- **Basis in paper:** [inferred] The authors observe that "four of six reasoning models perform worse on Task NA when provided with gold EC or EV solutions" and hypothesize that "reasoning oriented post-training reinforces end-to-end reasoning trajectories, rather than conditional reasoning from partially solved states," but do not test this hypothesis.
- **Why unresolved:** This counterintuitive finding suggests reasoning models may be trained in ways that make them less effective when given partial solutions, but the mechanism remains unexplored.
- **What evidence would resolve it:** Systematic study comparing reasoning models' performance when given gold vs. model-generated intermediate steps, potentially analyzing attention patterns or chain-of-thought behavior to understand trajectory disruption.

### Open Question 3
- **Question:** How effectively can instruction tuning improve stepwise legal reasoning performance on tasks requiring strategic optimization across multiple constraints?
- **Basis in paper:** [explicit] The authors explicitly identify "developing and evaluating new approaches to instruction tuning for stepwise legal reasoning" as a direction for future work in the conclusion.
- **Why unresolved:** Current models show sharp performance drops on estate-level optimization tasks (Task OE F1 below 0.63 for all models on advanced suite), but targeted training approaches remain unexplored.
- **What evidence would resolve it:** Training experiments using OpenExempt's dev set for instruction tuning, then measuring improvement on held-out test sets across competency tiers, particularly on Task NA and OE.

## Limitations
- The framework's focus on objectively correct tasks excludes common legal ambiguity, limiting real-world applicability to domains requiring subjective interpretation.
- Zero-shot evaluation represents an artificial constraint that doesn't reflect practical deployment scenarios where few-shot or fine-tuning would likely be used.
- Single-axis diagnostic isolation may mask compound failure modes from interactions between multiple complexity dimensions.

## Confidence
- **High Confidence:** The benchmark's generation mechanism (symbolic encoding + template rendering), the branch-and-bound solver implementation, and the diagnostic suite design.
- **Medium Confidence:** The performance claims across 13 models, particularly the "sharp performance cliffs" observed. Model-specific API details and exact prompt templates are unspecified.
- **Low Confidence:** Claims about the framework's applicability to other domains. The paper suggests extensibility to domains like medical billing, but doesn't validate this beyond theoretical discussion.

## Next Checks
1. **Interaction Effect Analysis:** Systematically test compound scenarios combining multiple complexity dimensions (e.g., obfuscation + 8+ assets) to determine if single-axis isolation masks critical interaction effects that determine real-world model failure.
2. **Few-Shot Performance Evaluation:** Repeat the 13-model evaluation with optimized few-shot prompts (e.g., 3-5 examples) to establish realistic performance baselines versus the artificial zero-shot constraint, measuring delta improvements.
3. **Domain Transfer Validation:** Implement the framework for a non-legal domain (e.g., medical billing) with domain experts to verify that the dual-representation generation mechanism maintains objective correctness outside the bankruptcy code context.