---
ver: rpa2
title: A Scoresheet for Explainable AI
arxiv_id: '2502.09861'
source_url: https://arxiv.org/abs/2502.09861
tags:
- system
- explanations
- scoresheet
- wellington
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a scoresheet to specify and assess explainability
  requirements for autonomous and intelligent systems. The scoresheet is designed
  by considering the needs of multiple stakeholders and covers both global explanations
  (how the system works and how well it works) and local explanations (specific outputs).
---

# A Scoresheet for Explainable AI

## Quick Facts
- arXiv ID: 2502.09861
- Source URL: https://arxiv.org/abs/2502.09861
- Reference count: 40
- This paper develops a scoresheet to specify and assess explainability requirements for autonomous and intelligent systems.

## Executive Summary
This paper introduces a structured scoresheet for specifying and assessing explainability requirements in autonomous systems. The scoresheet is designed around stakeholder needs, distinguishing between global explanations (how the system works) and local explanations (specific outputs). It includes sections for explanation reliability (veracity), concepts used in explanations, explanation types supported, and level of automation. The tool is validated across six diverse systems including ChatGPT, medical imaging AI, and planning systems, demonstrating its generality and utility in identifying explainability gaps across different domains.

## Method Summary
The method involves qualitative assessment using a structured scoresheet that captures system information, explanation veracity, global and local explanations, and automation levels. Assessors analyze system documentation, query the system with specific question types ("Why?", "What if?", "How to be?"), and evaluate the alignment of explanations with actual system logic. The approach distinguishes between explanations derived from the behavior-generating module (high veracity) versus those from proxy models (requiring alignment assessment).

## Key Results
- The scoresheet effectively identifies explainability gaps across diverse AI architectures including ChatGPT, medical imaging systems, and planning systems
- Veracity assessment reveals critical limitations in systems like ChatGPT that provide fluent but potentially fabricated explanations
- The tool captures the distinction between global system documentation and local output-specific explanations, preventing documentation from masking local opacity

## Why This Works (Mechanism)

### Mechanism 1
If explainability frameworks are derived from specific stakeholder needs rather than general principles, they are more likely to identify gaps in high-risk domains. The scoresheet moves from high-level transparency requirements to granular assessment based on roles and functionality. By mapping features like "access to developers" or "edge case handling" to specific stakeholders, it forces an audit of whether the system supports the actual decision-making needs of that role. Core assumption: Stakeholders cannot effectively assess trust or accountability if the explanation type does not match their specific goal. Break condition: If stakeholders cannot define their specific decision-goals, the scoresheet becomes a checkbox exercise.

### Mechanism 2
If explanation veracity (fidelity) is not explicitly scored, systems using surrogate explanation models may provide plausible but false justifications, undermining trust. The scoresheet distinguishes between explanations derived directly from the behavior-generating module (High reliability) versus those derived from proxy models (requires alignment assessment). Core assumption: Users assume explanations reflect the system's actual reasoning process unless told otherwise. Break condition: If the system is a black box with no internal logging and relies solely on post-hoc rationalizations, veracity scores flatline.

### Mechanism 3
If a system fails to support counterfactual or hypothetical queries ("What if?", "How to be?"), its utility for debugging and user learning is significantly reduced. The scoresheet operationalizes explanation types beyond simple "Why?" questions, capturing "Future-looking" and "Hypothetical" capabilities. Core assumption: Human mental models of system behavior are updated most effectively through contrastive case analysis. Break condition: If the underlying model is non-invertible or lacks simulation capability, the "Hypothetical" sections cannot be checked.

## Foundational Learning

**Veracity vs. Plausibility**: Engineers often confuse "fluent output" with "factual explanation." In LLMs, an explanation can be perfectly grammatical yet entirely fabricated regarding the system's internal state. Quick check: "If I changed a random seed in the model, would this explanation still be generated? If yes, is it explaining the logic or just hallucinating a reason?"

**Global vs. Local Explainability**: A system might have excellent documentation (Global) but fail to explain a specific loan rejection (Local). The scoresheet separates these to prevent global documentation from masking local opacity. Quick check: "Does the system tell you how the engine works (Global), or does it tell you why it decided to turn left at this specific intersection (Local)?"

**Stakeholder Role Fluidity**: Designing for a single "User" is insufficient. A developer needs debugging logs while a regulator needs bias reports. Quick check: "If I were a safety certifier rather than an end-user, would the current explanation interface satisfy my requirements for liability and edge-case handling?"

## Architecture Onboarding

**Component map**: Input (System artifacts + Stakeholder Definitions) -> Assessor (Scoresheet with Basic Info, Veracity, Global, Local) -> Checklists (Auxiliary Global Explanation Checklist) -> Output (Profile of explainability gaps)

**Critical path**: 1. Stakeholder ID: Define who needs explanations 2. Veracity Audit: Determine if explanations come from logs/model (High) or post-hoc surrogates (Low) 3. Global Check: Verify existence of performance/limitation docs 4. Local Capability Mapping: Test specific query types (Why, What-if, How-to-be)

**Design tradeoffs**: Completeness vs. Burden - The scoresheet is comprehensive but filling it for complex multi-module systems may require multiple scoresheets, increasing assessment overhead. Veracity vs. Performance - High veracity often requires logging heavy execution traces, which may impact system performance or storage.

**Failure signatures**: The "ChatGPT Trap" - High scores on "Explanation Types" but Low/Unsure "Veracity" because the system makes things up. The "Manual Mirage" - A system scores well on "Explanation Types" because developers can manually derive answers from logs, but "Automation" is Manual, meaning it doesn't scale for real-time users.

**First 3 experiments**: 1. LLM Baseline: Apply the scoresheet to an off-the-shelf LLM on a task like medical triage to verify if it produces "Low" veracity despite high fluency. 2. The "Why Not" Test: Take a classification system and attempt to ask "Why not?" and "How to be?" questions. If it fails, mark the Local Explanation section as incomplete. 3. Hybrid Module Split: For a system using both Planning and RL, fill out two partial scoresheets. Compare how the Planning module offers "High" veracity while the RL module offers "None," highlighting the architectural bottleneck in explainability.

## Open Questions the Paper Calls Out

**Open Question 1**: How effective is the scoresheet when used by diverse stakeholders with varying roles and demographics? Basis: Section 6.1 states the scoresheet "has only been used by the authors" and requires "further use and evaluation" by "a range of people" covering various roles and demographics. Why unresolved: Current validation was performed exclusively by the authors, lacking external verification. What evidence would resolve it: User studies involving diverse stakeholders assessing the same system to measure usability and inter-rater reliability.

**Open Question 2**: Can the scoresheet be effectively utilized to specify requirements for systems during the design phase rather than just assessing existing ones? Basis: Section 6.1 notes the need to assess "how well the scoresheet can be used for other use cases (e.g. specifying... requirements... rather than assessing)". Why unresolved: The paper demonstrates application on six operational systems but does not validate its efficacy as a prospective engineering tool. What evidence would resolve it: Application during the requirements engineering phase of a new project.

**Open Question 3**: Is the tool sufficiently general to define explainability requirements for an entire industry sector or regulatory framework? Basis: Section 6.1 suggests it "might be possible... to specify the explainability requirements for a whole sector... or even to specify regulatory requirements". Why unresolved: The paper only validates the tool on specific individual systems, not on its ability to generate macro-level policy. What evidence would resolve it: A case study applying the scoresheet to draft regulatory standards for a specific domain.

## Limitations
- The scoresheet's effectiveness depends heavily on assessor interpretation, introducing subjectivity
- Veracity scoring lacks clear quantitative thresholds, relying on qualitative judgment
- The tool has not been tested for automated application and may not scale well for complex multi-module systems
- The paper does not address how to handle systems with evolving or adaptive behavior

## Confidence
- **High Confidence**: The core claim that separating global and local explanations, along with explicit veracity assessment, provides a more rigorous framework than existing transparency standards
- **Medium Confidence**: The assertion that the scoresheet captures key explainability features across diverse architectures, based on the six system examples provided
- **Medium Confidence**: The mechanism claim that stakeholder-specific explanation types improve gap identification, though this relies on subjective interpretation of stakeholder needs

## Next Checks
1. **Automated Scoring Pipeline**: Develop a semi-automated tool that can parse system documentation and logs to populate the scoresheet fields, reducing assessor subjectivity
2. **Cross-Assessor Reliability Test**: Have multiple independent assessors fill out the scoresheet for the same system (e.g., ChatGPT) and measure inter-rater reliability, particularly for the veracity and concept categorization sections
3. **Dynamic System Evaluation**: Apply the scoresheet to a system with adaptive behavior (e.g., online learning model) at multiple time points to assess whether the scoresheet can capture temporal changes in explainability capabilities