---
ver: rpa2
title: Diffusion Counterfactuals for Image Regressors
arxiv_id: '2503.20595'
source_url: https://arxiv.org/abs/2503.20595
tags:
- image
- regression
- explanations
- counterfactual
- changes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two novel methods for generating counterfactual
  explanations for image regression tasks using diffusion-based generative models.
  The first method, Adversarial Counterfactual Regression Explanations (AC-RE), adapts
  Adversarial Counterfactual Explanations (ACE) to operate directly in pixel space.
---

# Diffusion Counterfactuals for Image Regressors

## Quick Facts
- arXiv ID: 2503.20595
- Source URL: https://arxiv.org/abs/2503.20595
- Authors: Trung Duc Ha; Sidney Bender
- Reference count: 0
- This paper introduces two novel methods for generating counterfactual explanations for image regression tasks using diffusion-based generative models.

## Executive Summary
This paper presents two methods for generating counterfactual explanations in image regression tasks using diffusion models. The first method (AC-RE) operates in pixel space using adversarial perturbations through a diffusion generative model, while the second method (Diff-AE-RE) works in the latent space using a Diffusion Autoencoder. Both approaches successfully generate realistic, semantic counterfactuals that reveal insights into model decision-making and can uncover spurious correlations. The methods demonstrate that feature changes in regression counterfactuals depend on the prediction region, with larger semantic alterations required for significant value shifts.

## Method Summary
The paper introduces two diffusion-based counterfactual explanation methods for image regression. AC-RE adapts ACE to pixel space by using a DDPM to generate adversarial perturbations that minimize prediction error while maintaining visual plausibility. Diff-AE-RE operates in the latent space of a pre-trained Diffusion Autoencoder, optimizing semantic latent codes to achieve target predictions. Both methods employ early stopping when the counterfactual reaches within 0.05 MAE of the target value. The methods are evaluated on CelebA-HQ for age prediction and a synthetic dataset, using metrics including validity (Oracle MAE), sparsity (MNAC, FVA, Face Similarity), and realism (FID).

## Key Results
- Both AC-RE and Diff-AE-RE successfully generate realistic counterfactuals that achieve target predictions on CelebA-HQ and synthetic datasets
- Diff-AE-RE reveals spurious correlations, such as adding glasses to make a person appear older
- There exists a trade-off between sparsity and quality, with pixel-space changes offering greater sparsity and latent-space edits providing higher quality and semantic flexibility

## Why This Works (Mechanism)
The methods leverage diffusion models' ability to generate realistic images while allowing controlled modifications. AC-RE uses adversarial optimization through the diffusion process to find minimal changes that achieve target predictions. Diff-AE-RE exploits the semantic structure of the latent space to make meaningful attribute changes. Both approaches work because diffusion models provide a smooth, high-dimensional manifold where counterfactuals can be generated while maintaining visual plausibility. The regression-specific adaptations ensure that modifications actually change the predicted value rather than just the appearance.

## Foundational Learning

**Diffusion Models**: Generative models that learn to denoise data progressively. Why needed: Core mechanism for generating realistic counterfactuals. Quick check: Can generate coherent images from pure noise.

**Adversarial Attacks**: Optimization-based methods for finding minimal perturbations that change model predictions. Why needed: AC-RE uses adversarial optimization to find counterfactuals. Quick check: Small perturbations cause significant prediction changes.

**Latent Space Optimization**: Direct manipulation of learned feature representations. Why needed: Diff-AE-RE modifies semantic attributes through latent code optimization. Quick check: Changes in latent space correspond to interpretable visual changes.

## Architecture Onboarding

**Component Map**: Input Image -> Regressor -> Prediction -> Diffusion Model (AC-RE) OR Diffusion Autoencoder (Diff-AE-RE) -> Counterfactual Optimization -> Output Counterfactual

**Critical Path**: The optimization loop is critical: compute prediction error → apply gradient update through diffusion process or latent space → check stopping criterion (MAE ≤ 0.05) → repeat until convergence.

**Design Tradeoffs**: Pixel-space (AC-RE) offers sparser changes but may include noise/artifacts; latent-space (Diff-AE-RE) provides smoother semantic edits but may require larger modifications to achieve same prediction change.

**Failure Signatures**: Non-convergence (counterfactuals fail to reach target MAE), unrealistic outputs (artifacts or implausible images), or semantic inconsistencies (changes don't match prediction direction).

**3 First Experiments**:
1. Generate a simple counterfactual on a single CelebA-HQ image targeting a nearby age value
2. Compare sparsity of AC-RE vs Diff-AE-RE on the same example
3. Test early stopping behavior by monitoring MAE per iteration

## Open Questions the Paper Calls Out
None

## Limitations
- Pre-trained model weights (ACE DDPM, Diffusion Autoencoder) are not directly accessible, only vaguely referenced as being on "GitHub"
- Regression model fine-tuning procedure lacks specific hyperparameters (learning rate, batch size, epochs)
- Evaluation relies on a separately fine-tuned "Oracle" regressor, potentially introducing evaluation bias

## Confidence

**High Confidence**: Core theoretical framework connecting diffusion models to counterfactual explanations; overall experimental design on CelebA-HQ and synthetic datasets; qualitative observation that latent-space edits produce smoother semantic changes while pixel-space edits yield sparser modifications.

**Medium Confidence**: Quantitative results showing Diff-AE-RE revealing spurious correlations and the sparsity-quality trade-off claim, as these depend heavily on exact implementation details that are underspecified.

**Low Confidence**: Specific numerical performance values (FID scores, MAE convergence rates) and the precise impact of λ_d=10^-5 on latent regularization, given the lack of accessible pre-trained models and complete training procedures.

## Next Checks

1. Verify availability of ACE DDPM and Diff-AE pre-trained weights through the referenced GitHub repository, or establish clear procedures for training these models from scratch with specified hyperparameters.

2. Implement and test the exact regression model fine-tuning procedure (optimizer settings, data splits, convergence criteria) to ensure the Oracle and target regressors are comparable.

3. Conduct controlled experiments varying λ_d (e.g., 10^-4, 10^-3, 10^-6) to empirically validate its role in balancing semantic coherence against overfitting in Diff-AE-RE outputs.