---
ver: rpa2
title: 'Prompting Science Report 1: Prompt Engineering is Complicated and Contingent'
arxiv_id: '2503.04818'
source_url: https://arxiv.org/abs/2503.04818
tags:
- mini
- formatted
- please
- order
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Prompting Science Report 1: Prompt Engineering is Complicated and Contingent

## Quick Facts
- arXiv ID: 2503.04818
- Source URL: https://arxiv.org/abs/2503.04818
- Authors: Lennart Meincke; Ethan Mollick; Lilach Mollick; Dan Shapiro
- Reference count: 40
- Key outcome: LLM performance is highly contingent on measurement standards and prompt choices—no universal best practices exist

## Executive Summary
This paper systematically tests whether prompt engineering techniques have universal effects on LLM performance. Through 100 repeated attempts on each of 198 PhD-level questions, the study reveals that prompt format constraints can help or hurt performance depending on the question, that polite and commanding prefixes show question-level variability with no aggregate effect, and critically, that the choice of correctness threshold (51%, 90%, or 100%) fundamentally changes whether a model appears capable. The core finding is that there are no universal prompting best practices—effects are highly contingent on specific questions, models, and measurement standards.

## Method Summary
The study evaluates GPT-4o and GPT-4o-mini on the GPQA Diamond benchmark (198 PhD-level multiple-choice questions in biology, physics, and chemistry) using four prompt conditions: formatted (with explicit output structure), unformatted (no format constraint), polite (prefix "Please answer..."), and commanding (prefix "I order you to answer..."). Each question is tested 100 times per condition at temperature=0. Performance is measured at three thresholds: 51% majority correct, 90% correct, and 100% perfect accuracy, with paired bootstrap-permutation tests (5,000 replicates) comparing against random baseline.

## Key Results
- No prompt prefix (polite or commanding) showed consistent aggregate performance effects across all questions
- Formatted prompts significantly outperformed unformatted prompts for both GPT-4o and GPT-4o-mini
- Using 100% correctness threshold, both models performed only 5 percentage points better than random guessing—statistically insignificant
- Question-level analysis revealed significant performance differences between prompt conditions for specific questions, but these effects disappeared in aggregate analysis

## Why This Works (Mechanism)

### Mechanism 1: Output Format Constraints Stabilize Performance
- Claim: Explicit output formatting instructions improve model performance consistency on structured tasks
- Mechanism: Constraining the response space reduces ambiguity in how the model should respond, channeling computation toward answer extraction rather than managing response structure
- Core assumption: The model has learned to associate specific output formats with successful task completion during training
- Evidence anchors:
  - [abstract]: "We also find that constraining the AI's answers helps performance in some cases, though it may lower performance in other cases"
  - [section]: "In our case, we find that using the unformatted prompt, model performance drops significantly for GPT-4o (RD = 0.086; 95% CI [0.040, 0.136]; p < 0.001) and GPT-4o mini (RD = 0.121; 95% CI [0.056, 0.187]; p < 0.001)"
  - [corpus]: Weak direct evidence—related paper "Which Prompting Technique Should I Use?" evaluates 14 prompting techniques across SE tasks, suggesting format effects vary by domain
- Break condition: Tasks requiring open-ended creative responses where format constraints would limit useful exploration

### Mechanism 2: Prompt Tone Effects Are Question-Contingent
- Claim: Polite or commanding prompt prefixes do not produce universal performance effects; they help some questions and harm others
- Mechanism: Different question types may activate different reasoning patterns in latent space; tone markers interact unpredictably with question embeddings
- Core assumption: The model's internal representations have varying sensitivity to tone markers based on training data associations
- Evidence anchors:
  - [abstract]: "Specifically, we find that sometimes being polite to the LLM helps performance, and sometimes it lowers performance"
  - [section]: "Interestingly, at the question-level, we can find significant differences between many questions... These differences disappear when aggregating across all questions"
  - [corpus]: Related paper "Prompting Science Report 3" tests incentives and threats in prompting, suggesting tone effects remain an active research area with mixed findings
- Break condition: When aggregate performance is all that matters and question-level optimization is infeasible

### Mechanism 3: Measurement Standards Gate Perceived Capability
- Claim: The choice of correctness threshold (100%, 90%, 51%) fundamentally changes whether a model appears to have genuine knowledge
- Mechanism: LLMs exhibit response variance even at temperature 0; single-attempt evaluations capture lucky guesses, while high-threshold standards reveal consistency limitations
- Core assumption: Consistent correct responses across multiple attempts indicate more robust knowledge than occasional correct responses
- Evidence anchors:
  - [abstract]: "There is no single standard for measuring whether a Large Language Model (LLM) passes a benchmark, and that choosing a standard has a big impact on how well the LLM does on that benchmark"
  - [section]: "Notably, using the formatted prompt and 100% correct condition, both GPT-4o and GPT-4o mini only perform 5 percentage points... better than a random guess... both differences insignificant"
  - [corpus]: Weak corpus support—related work focuses on prompting techniques rather than measurement methodology
- Break condition: Tasks requiring only at-least-one-success (e.g., code generation with verification)

## Foundational Learning

- Concept: **Response Variance in LLMs**
  - Why needed here: The methodology of testing each question 100 times reveals that even at temperature 0, LLMs give inconsistent answers—this variance is fundamental to understanding measurement
  - Quick check question: If you run the same prompt 10 times at temperature 0 and get 7 correct, 3 incorrect answers, what does this suggest about the model's "knowledge"?

- Concept: **Benchmark Thresholds vs. Real-World Requirements**
  - Why needed here: The paper maps correctness standards (PASS@1, PASS@100, consensus, majority) to different use cases, requiring practitioners to think about error tolerance
  - Quick check question: Your application cannot tolerate any errors. Which threshold from the paper (51%, 90%, 100%) should you use?

- Concept: **Question-Level vs. Aggregate Effects**
  - Why needed here: Prompting techniques showing no effect in aggregate can have large positive or negative effects on individual questions—this challenges "best practice" thinking
  - Quick check question: If a prompting technique helps 20% of questions and hurts 20% equally, what will aggregate analysis show?

## Architecture Onboarding

- Component map:
  ```
  Benchmark Dataset (GPQA Diamond, 198 questions)
         ↓
    Prompt Template
    [prefix + question + suffix + system prompt]
         ↓
    LLM (GPT-4o / GPT-4o-mini)
         ↓
    Response Parser (format-dependent)
         ↓
    Aggregation Engine
    (100 attempts per question)
         ↓
    Threshold Evaluator
    (51% / 90% / 100% standards)
  ```

- Critical path: Prompt template design → response parsing reliability → threshold selection determines whether you conclude your model "works"

- Design tradeoffs:
  - Rigor vs. cost: 100 attempts provides statistical power but increases API costs 100x
  - Single metric vs. nuance: Aggregate accuracy masks question-level variability that could be actionable
  - Format freedom vs. parsing reliability: Unformatted prompts feel natural but degrade performance and complicate automated evaluation

- Failure signatures:
  - High aggregate accuracy but low 100% threshold: Model is inconsistent, not reliable
  - Prompt A beats B on some questions, loses on others: No universal winner exists
  - Unformatted underperforms formatted: Output structure is constraining performance

- First 3 experiments:
  1. **Threshold sensitivity test**: Run your benchmark with 100 attempts per question and compare results at 51%, 90%, and 100% thresholds—reveals reliability ceiling
  2. **Format constraint A/B test**: Test formatted vs. unformatted prompts on 20 questions with 50 attempts each—expect formatted to win but identify exceptions
  3. **Tone prefix mapping**: Test polite vs. neutral vs. commanding prefixes and identify which specific questions respond positively/negatively to each—build question-to-prompt mapping

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific semantic or structural features of a question determine whether a polite or commanding prompt prefix will improve versus degrade LLM performance?
- Basis in paper: [explicit] The authors note that while aggregate performance showed negligible differences, significant variance appeared at the question level, suggesting specific techniques work for specific questions for "unclear reasons."
- Why unresolved: The study identified that "Please" helped on some questions and hurt others, but did not conduct a feature analysis to explain the driver of this contingency.
- What evidence would resolve it: A regression analysis correlating question attributes (e.g., subject matter, ambiguity, complexity) with the direction and magnitude of prompt sensitivity.

### Open Question 2
- Question: Do the observed instabilities and prompt sensitivities persist in more advanced models or reasoning-based architectures?
- Basis in paper: [explicit] The authors state that because they used GPT-4o and 4o-mini, the findings regarding variability "may not apply to all benchmarks, nor to larger models."
- Why unresolved: The study deliberately focused on variability within specific mid-tier models, leaving the behavior of frontier or reasoning models under these conditions unknown.
- What evidence would resolve it: Replication of the 100-sample repeated measures protocol on reasoning models (e.g., o1) or frontier models to test if consistency improves.

### Open Question 3
- Question: How can correctness thresholds (e.g., 51% vs. 100%) be rigorously mapped to specific real-world application risks?
- Basis in paper: [explicit] The authors conclude that "Future work should justify the standards used for measuring AI performance" and that the choice of standard depends on usage goals.
- Why unresolved: While the paper demonstrates that the choice of threshold changes the perception of capability, it does not provide a framework for selecting the correct threshold for specific business or safety contexts.
- What evidence would resolve it: A set of case studies linking specific benchmark thresholds (e.g., 90% correct) to concrete operational outcomes (e.g., error rates in medical summarization).

### Open Question 4
- Question: Under what conditions does strict formatting constrain performance rather than enhance it?
- Basis in paper: [explicit] The authors found formatting improved results in this benchmark but acknowledged that whether it helps or hurts "is likely to vary by model and setting," citing contrasting literature.
- Why unresolved: The results in this specific benchmark (GPQA Diamond) showed performance degradation without formatting, which creates tension with prior research suggesting formatting can be restrictive.
- What evidence would resolve it: A comparative study measuring the "formatting effect" across diverse task types (e.g., creative writing vs. logic puzzles) within the same model.

## Limitations

- **Limited generalizability**: Findings based on GPQA Diamond (PhD-level scientific questions) may not transfer to other domains or difficulty levels
- **Reproducibility barrier**: 100 attempts per question (158,400 total API calls) makes replication expensive and time-consuming
- **No mechanism analysis**: The study identifies question-level effects but doesn't explain why certain questions respond differently to prompt variations

## Confidence

- **High Confidence**: The core finding that measurement standards fundamentally change conclusions about LLM capability is robust and well-supported
- **Medium Confidence**: The formatted prompt consistently outperforming unformatted prompts is credible but may be task-dependent
- **Low Confidence**: The claim that "no universal prompting best practice exists" is technically correct but potentially misleading—while aggregate effects wash out, practitioners still need actionable guidance

## Next Checks

1. **Domain Transfer Test**: Replicate the formatted vs. unformatted prompt comparison on a completely different benchmark (e.g., coding problems or commonsense reasoning) with 50 attempts per question. This validates whether output format constraints are a general principle or GPQA-specific.

2. **Measurement Threshold Mapping**: For your specific application, run 30 questions with 10 attempts each at temperature=0, then calculate results at 51%, 90%, and 100% thresholds. This reveals your model's reliability ceiling and whether "PASS@1" would mislead you about capability.

3. **Question-Level Prompt Profiling**: Take 10 questions where your model struggles and test 3-4 different prompt prefixes (polite, commanding, neutral, formatted) with 5 attempts each. Map which prompt styles work for which question types to build a question-to-prompt lookup table rather than seeking universal rules.