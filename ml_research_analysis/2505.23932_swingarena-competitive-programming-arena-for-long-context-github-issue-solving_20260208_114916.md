---
ver: rpa2
title: 'SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving'
arxiv_id: '2505.23932'
source_url: https://arxiv.org/abs/2505.23932
tags:
- test
- code
- patch
- software
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SwingArena is a benchmark framework that evaluates large language
  models in realistic software development workflows. It simulates the collaborative
  process of submitting patches and reviewing them through continuous integration
  pipelines.
---

# SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving

## Quick Facts
- arXiv ID: 2505.23932
- Source URL: https://arxiv.org/abs/2505.23932
- Reference count: 40
- Primary result: Evaluates LLMs through competitive submitter-reviewer battles on 400 real GitHub issues using CI-driven validation

## Executive Summary
SwingArena introduces a competitive programming benchmark that evaluates large language models through adversarial submitter-reviewer battles on real-world GitHub issues. Models alternate between generating patches to fix issues and creating test cases to validate those patches, with all evaluations running through actual CI pipelines. The framework includes a Retrieval-Augmented Code Generation (RACG) module that handles long-context codebases across multiple programming languages. Experiments reveal model-specific tradeoffs between aggressive patch generation and conservative correctness, with GPT-4o excelling at patch generation while DeepSeek and Gemini demonstrate stronger CI validation performance.

## Method Summary
The SwingArena framework evaluates LLMs through an adversarial battle protocol where models take turns as submitters (generating patches) and reviewers (creating test cases). All patches and tests must pass through full CI pipelines executed locally via `act` in Docker containers. The RACG module handles long-context retrieval through hierarchical processing: BM25 for file-level pruning, syntax-aware chunking for code structure, and CodeBERT for semantic reranking. The benchmark uses 400 curated GitHub issues across C++, Python, Rust, and Go, selected from 2,300 candidates based on quality filters including CI pass rates and human review.

## Key Results
- GPT-4o achieves win rates ≥0.90 as submitter but shows lower overall correctness scores
- DeepSeek and Gemini achieve highest CI pass rates (0.66 and 0.64 respectively) demonstrating stronger correctness
- RACG retrieval ablation shows consistent improvements: Best@3 increases by 0.04-0.09 and Win Rate by 0.03-0.13 across languages
- Models exhibit self-consistency with high self-play win rates but different behaviors against cross-play opponents

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Battle Protocol Creates Differential Pressure on Correctness vs. Coverage
The competitive framework reveals model-specific tradeoffs between aggressive patch generation and conservative correctness. Models earn +1 if their patch passes all tests including reviewer-generated ones, -1 on any failure. This creates pressure that differentiates models along a correctness-aggressiveness axis, as evidenced by GPT-4o's high win rates but lower correctness scores versus DeepSeek and Gemini's stronger CI validation performance.

### Mechanism 2: Hierarchical Retrieval with Syntax-Aware Chunking Improves Patch Localization
RACG combines coarse-to-fine retrieval with syntax-aware chunking for accurate code localization. The pipeline uses BM25 for file-level pruning, language-specific parsers for syntactic unit decomposition, and CodeBERT for dense semantic reranking. This approach more accurately identifies relevant code than pure lexical matching, with class-level retrieval more than doubling Top-10 hit rates compared to BM25 alone.

### Mechanism 3: CI Pipeline Fidelity Ensures Realistic Failure Modes
Executing actual GitHub Actions workflows via `act` in isolated Docker containers produces failure signatures that synthetic test cases miss. This approach preserves exact build logic, dependencies, and toolchains, ensuring that patches must pass comprehensive CI checks including unit tests, linters, and security scans before scoring.

## Foundational Learning

- Concept: Continuous Integration (CI) pipelines
  - Why needed here: The entire evaluation is structured around CI-driven validation. Understanding what CI checks do is essential to interpret why a model's patch might fail.
  - Quick check question: Can you name three distinct types of checks that typically run in a GitHub Actions workflow beyond unit tests?

- Concept: Retrieval-Augmented Generation (RAG) for code
  - Why needed here: The RACG module is the primary mechanism for handling long-context codebases. Understanding sparse vs. dense retrieval is essential for debugging retrieval failures.
  - Quick check question: Given a query about "fixing memory leaks in Rust," would BM25 retrieve based on exact term overlap or semantic similarity?

- Concept: Adversarial evaluation and self-play
  - Why needed here: The battle protocol uses adversarial pressure to surface model weaknesses. Self-consistency vs. cross-play reveals model-specific behaviors.
  - Quick check question: If Model A has 97% win rate against itself but 89% against Model B, what does that suggest about Model A's test generation strictness?

## Architecture Onboarding

- Component map:
  Data Construction Pipeline -> Repository Mining -> CI Test Filtering -> LLM Filtering -> Expert Filtering -> 400 instances
  Arena Evaluation -> Battle Protocol -> Submitter/Reviewer Roles -> CI Execution -> Scoring
  RACG Module -> FileRetriever (BM25) -> CodeChunker (syntax-aware) -> CodeReranker (CodeBERT) -> Token Budget Manager

- Critical path:
  1. Load task instance (problem statement, repo metadata, base commit SHA)
  2. RACG retrieves top-5 files, chunks, reranks, and packs context within token budget
  3. Submitter agent generates patch; CI validates against existing tests
  4. Reviewer agent generates test case; CI validates against golden patch
  5. Patch + test applied together; final CI run determines scoring
  6. Roles reverse for balanced assessment

- Design tradeoffs:
  - RACG retrieval depth vs. token budget: Paper limits to 5 files and 16 chunks. More context improves hit rate but increases latency and cost.
  - CI fidelity vs. evaluation speed: Full CI pipeline execution is resource-intensive. The paper explicitly notes this as a limitation.
  - Adversarial strictness vs. signal: Very strict reviewers may reject valid patches; lenient reviewers may miss flaws.

- Failure signatures:
  - Low SPR with high RPR: Submitter generates syntactically correct but semantically wrong patches (common in Python).
  - High Top-20 but low Top-2 hit rate: Reranker finds relevant content but doesn't surface it early—may need reranker fine-tuning.
  - CI timeout or environment mismatch: Docker configuration missing dependencies; check .yaml or Dockerfile completeness.
  - Self-play win rate significantly higher than cross-play: Model's test generation is tailored to its own patch style, indicating potential overfitting.

- First 3 experiments:
  1. Reproduce RACG ablation: Run 100-sample ablation set with and without RACG. Expect Best@3 improvement of 0.04-0.09 and Win Rate improvement of 0.03-0.13.
  2. Compare retrieval granularities: Test Top-2 vs. Top-10 vs. Top-20 retrieval on patch localization accuracy. Verify the paper's finding that Top-10 captures most gains.
  3. Run self-play vs. cross-play for a single model: Pick Qwen2.5-Coder-7B and measure win rate against itself vs. against DeepSeek to verify if reviewer identity shifts outcomes.

## Open Questions the Paper Calls Out

### Open Question 1
How can static code analysis be most effectively integrated into Retrieval-Augmented Generation (RAG) to handle multi-language repositories? While the paper proposes RACG using syntax-aware chunking, it notes most current approaches lack fine-grained code structure understanding, limiting performance on complex, multi-file tasks.

### Open Question 2
Can the resource intensity of simulating full CI pipelines be reduced to allow for high-frequency, large-scale model evaluation? The paper notes that computational overhead of simulating full CI pipelines iteratively limits the scale and frequency of testing.

### Open Question 3
To what extent does RACG performance degrade when applied to poorly structured or highly domain-specific legacy codebases? The system's ability to retrieve relevant context may degrade with extremely large, poorly structured, or highly domain-specific codebases not adequately represented in the training data of the dense reranker.

## Limitations
- Benchmark relies on 400 curated GitHub issues, limiting generalizability to broader software development landscape
- RACG assumes semantic similarity via CodeBERT correlates with patch relevance without specific validation for bug localization
- CI execution via `act` may introduce discrepancies with cloud-based GitHub Actions environments
- Adversarial protocol may favor models with specific submission strategies rather than general correctness

## Confidence

**High Confidence** (Multiple independent evidence anchors, mechanistic clarity):
- RACG retrieval ablation showing consistent Best@k and Win Rate improvements across languages
- SPR/RPR differential patterns revealing model-specific tradeoffs between patch generation and test generation
- Self-play vs. cross-play analysis demonstrating that model behavior varies with reviewer identity

**Medium Confidence** (Single evidence stream or indirect support):
- Top-k retrieval diminishing returns hypothesis (no explicit curve shown in paper)
- CI fidelity advantage over synthetic testing (no direct comparison performed)
- Model-specific strategy claims (GPT-4o aggressive, DeepSeek/Gemini conservative) based on aggregate statistics

**Low Confidence** (Speculative or minimally tested):
- The mechanism by which adversarial pressure creates differential selection between models
- Whether RACG's syntax-aware chunking consistently improves over alternative retrieval strategies
- Generalizability of the 400-instance benchmark to real-world development scenarios

## Next Checks

1. Implement a synthetic test suite that matches the breadth of CI checks (build, lint, security, unit tests) and compare failure detection rates against full `act` execution. This directly tests whether CI fidelity provides measurable advantage over faster synthetic validation.

2. Run a language-agnostic retrieval baseline using sentence transformers on flattened code files and compare Top-2/Top-10 hit rates against RACG. This isolates whether CodeBERT reranking or the syntax-aware chunking provides the primary retrieval benefit.

3. Create a stratified sample of SwingArena instances by difficulty (based on SPR scores) and measure whether Top-5 retrieval consistently outperforms Top-2 across difficulty levels. This tests the diminishing returns hypothesis and informs optimal context-window allocation.