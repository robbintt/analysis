---
ver: rpa2
title: Comparing BFGS and OGR for Second-Order Optimization
arxiv_id: '2512.06969'
source_url: https://arxiv.org/abs/2512.06969
tags:
- bfgs
- line
- hessian
- search
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper compares the classical BFGS quasi-Newton method with\
  \ a novel Online Gradient Regression (OGR) approach for second-order optimization\
  \ in high-dimensional problems like neural network training. BFGS maintains a positive-definite\
  \ Hessian approximation via the Sherman-Morrison update but is limited in non-convex\
  \ settings and requires O(D\xB2) memory."
---

# Comparing BFGS and OGR for Second-Order Optimization

## Quick Facts
- arXiv ID: 2512.06969
- Source URL: https://arxiv.org/abs/2512.06969
- Reference count: 15
- Primary result: OGR consistently outperforms BFGS in final objective value and convergence speed on standard test functions, particularly in non-convex landscapes.

## Executive Summary
This paper compares the classical BFGS quasi-Newton method with a novel Online Gradient Regression (OGR) approach for second-order optimization in high-dimensional problems like neural network training. BFGS maintains a positive-definite Hessian approximation via the Sherman-Morrison update but is limited in non-convex settings and requires O(D²) memory. OGR instead estimates curvature online by performing exponential moving average regression of gradients against positions, allowing it to capture both positive and negative eigenvalues without explicit Hessian computation or inversion. Across standard test functions, OGR consistently outperformed BFGS in both final objective value and convergence speed, particularly in non-convex landscapes.

## Method Summary
OGR estimates local curvature by regressing gradients against positions using exponentially weighted statistics, avoiding explicit Hessian computation and inversion. The method models the local quadratic relation g(θ) ≈ H(θ - p), where H is the Hessian and p is the predicted extremum or saddle point. It maintains EMA statistics of position-gradient pairs and solves a weighted least-squares problem online to recover H and p via closed-form estimators. Unlike BFGS, which enforces positive definiteness, OGR estimates an unconstrained Hessian that can capture negative curvature for saddle point navigation, using eigenvalue clipping by absolute value for numerical stability.

## Key Results
- OGR consistently outperformed BFGS on 9 standard test functions in both final objective value and convergence speed
- On Rosenbrock function, OGR with fixed step size outperformed its line-searched version, suggesting line search can be overly conservative in narrow valleys
- OGR makes more effective use of curvature information at near-first-order computational cost

## Why This Works (Mechanism)

### Mechanism 1
- OGR estimates local curvature by regressing gradients against positions using an exponential moving average to avoid explicit Hessian computation and inversion
- The method models the local quadratic relation g(θ) ≈ H(θ - p), maintaining EMA statistics of position-gradient pairs and solving a weighted least-squares problem online to recover H and p
- Core assumption: The objective is locally well-approximated by a quadratic model within a small neighborhood
- Break condition: If the local landscape is highly non-quadratic, the linear gradient model fails and H estimates become unreliable

### Mechanism 2
- A symmetrized Hessian estimator ensures consistency with the mathematical requirement that Hessians are symmetric
- The direct estimator is not symmetric; the symmetrized form assumes H = H^T from the outset, solving via eigendecomposition of the weighted covariance matrix
- Core assumption: Sufficient coverage of directions in recent (θt, gt) pairs to make the covariance matrix well-conditioned
- Break condition: If covariance is rank-deficient or near-singular, eigenvalues approach zero, causing numerical instability in element-wise division

### Mechanism 3
- OGR can estimate non-positive-definite Hessians, enabling navigation of non-convex landscapes with saddle points
- Unlike BFGS, OGR estimates an unconstrained H; negative eigenvalues indicate negative curvature, and eigenvalue clipping by absolute value preserves curvature sign information
- Core assumption: Non-convex objectives contain regions where negative curvature is informative for escaping saddles
- Break condition: If eigenvalue clipping threshold is too aggressive, negative curvature information is lost; if too permissive, near-zero eigenvalues cause unstable inversions

## Foundational Learning

- Concept: Quasi-Newton Methods and BFGS
  - Why needed here: The paper frames OGR as an alternative to classical BFGS; understanding BFGS's positive definiteness constraint is essential to see why it struggles in non-convex settings
  - Quick check question: Explain why BFGS maintains a positive-definite Hessian approximation and what happens when the true Hessian has negative eigenvalues

- Concept: Exponential Moving Average (EMA) for Online Statistics
  - Why needed here: OGR relies on EMA to track weighted covariance statistics; understanding decay parameter β is critical for tuning responsiveness vs. stability
  - Quick check question: If β = 0.2 (as used in experiments), approximately how many recent iterations contribute significantly to the statistics?

- Concept: Eigendecomposition and Eigenvalue Regularization
  - Why needed here: The symmetrized estimator and inversion step use eigendecomposition with clipping; understanding this is necessary to debug numerical instability
  - Quick check question: Why does the implementation clip eigenvalues by absolute value rather than setting negative eigenvalues to zero?

## Architecture Onboarding

- Component map: Statistics module -> Symmetrized Hessian estimator -> Eigenvalue regularization -> Step computation -> Optional line search
- Critical path:
  1. Compute gradient g_t at current θ_t
  2. Update EMA statistics with (θ_t, g_t)
  3. Compute symmetrized H via eigendecomposition and equation (6)
  4. Clip eigenvalues, invert, and compute step direction
  5. Apply step size α, optional line search, and norm clipping
- Design tradeoffs:
  - β (EMA decay): Lower values improve responsiveness to changing curvature but increase noise sensitivity
  - Eigenvalue threshold ϵ: Too small → numerical instability; too large → loss of curvature information
  - Step clipping τ: Prevents unstable large jumps but may slow convergence on well-conditioned problems
  - Line search: Improves robustness but adds function evaluations
- Failure signatures:
  - Covariance matrix near-singular: Causes division instability in symmetrized estimator
  - Flat/infinite curvature regions: Eigenvalues near zero cause unstable inversions
  - Non-quadratic local structure: H estimates become inconsistent
- First 3 experiments:
  1. Reproduce the 2D test function comparison with and without line search
  2. Ablation on EMA decay β: Test β ∈ {0.1, 0.2, 0.5, 0.8} on a non-convex function
  3. Compare OGR vs. L-BFGS on a higher-dimensional problem (d ≥ 20)

## Open Questions the Paper Calls Out

- How does OGR performance scale when applied to high-dimensional neural network training compared to standard first-order methods?
- Can combining OGR with momentum or Adam-style adaptivity improve convergence stability or speed?
- Can an automated schedule for the EMA decay rate and subspace selection be developed to reduce sensitivity to hyperparameters?

## Limitations
- The specific step clipping threshold τ and Armijo constant c for line search are not specified, requiring reasonable defaults
- Dimensionality for the 200-point experiments is not stated; results may scale differently in higher dimensions
- The OGR implementation assumes locally quadratic behavior; performance in highly non-smooth or discontinuous landscapes is untested

## Confidence

**High confidence**: The core claim that OGR can estimate non-positive-definite Hessians and outperform BFGS in non-convex settings, supported by trajectory plots and final loss comparisons on multiple test functions.

**Medium confidence**: The claim that OGR operates at near first-order computational cost, as memory scaling and wall-clock benchmarks are not provided.

**Low confidence**: The mechanism's robustness to highly non-quadratic local structures, since break conditions are only theoretically described, not empirically tested.

## Next Checks

1. Implement and test OGR on a high-dimensional non-convex problem (d ≥ 20) to verify scalability claims and measure wall-clock time per iteration
2. Conduct an ablation study varying the EMA decay β across a wider range (0.1 to 0.8) on multiple non-convex functions to identify optimal responsiveness vs. stability tradeoffs
3. Test OGR on discontinuous or highly non-smooth objective functions to assess robustness outside the quadratic approximation regime