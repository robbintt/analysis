---
ver: rpa2
title: 'Seeking Flat Minima over Diverse Surrogates for Improved Adversarial Transferability:
  A Theoretical Framework and Algorithmic Instantiation'
arxiv_id: '2504.16474'
source_url: https://arxiv.org/abs/2504.16474
tags:
- adversarial
- surrogate
- transferability
- attack
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving adversarial transferability
  in black-box attacks, where adversarial examples (AEs) crafted on surrogate models
  must remain effective against unseen target models. The authors propose a theoretical
  framework that provides provable guarantees for adversarial transferability by decomposing
  the target adversarial risk into a surrogate adversarial risk and a transferability
  gap.
---

# Seeking Flat Minima over Diverse Surrogates for Improved Adversarial Transferability: A Theoretical Framework and Algorithmic Instantiation

## Quick Facts
- arXiv ID: 2504.16474
- Source URL: https://arxiv.org/abs/2504.16474
- Reference count: 40
- One-line primary result: Proposes DRAP, a method that improves black-box adversarial transferability by optimizing adversarial examples toward flat minima over a diverse surrogate model set.

## Executive Summary
This paper addresses the challenge of improving adversarial transferability in black-box attacks, where adversarial examples (AEs) crafted on surrogate models must remain effective against unseen target models. The authors propose a theoretical framework that provides provable guarantees for adversarial transferability by decomposing the target adversarial risk into a surrogate adversarial risk and a transferability gap. The key insight is that optimizing AEs toward flat minima over a diverse set of surrogate models, while controlling the surrogate-target model shift using a novel adversarial model discrepancy measure, yields a comprehensive guarantee for transferability.

Algorithmically, the authors introduce a method called DRAP (Model-Diversity-compatible Reverse Adversarial Perturbation). DRAP constructs a diverse surrogate model set and generates model-specific reverse adversarial perturbations to effectively promote the flatness of AEs over these diverse models. Extensive experiments on NIPS2017 and CIFAR-10 datasets demonstrate that DRAP achieves significant improvements in attack success rates compared to 14 state-of-the-art baseline attacks, particularly on adversarially trained target models. The results validate the importance of considering both flatness and model diversity in improving adversarial transferability.

## Method Summary
DRAP constructs a diverse surrogate model set using 5 architectures (ResNet-50, ConvNeXt-T, ViT, ResNet-50(AT), XCiT-S(AT)). Each surrogate is fine-tuned for 40 epochs with constant learning rates, and weight checkpoints are sampled to form the model set. The attack optimization uses a late-start strategy (nLS=5) where model-specific reverse perturbations are computed in an inner loop to maximize loss locally, then adversarial examples are updated in an outer loop using aggregated gradients. This process seeks flat minima across diverse models while controlling the surrogate-target discrepancy, with hyperparameters including Î³=4/255 (untargeted) or 16/255 (targeted), and 200 total iterations.

## Key Results
- DRAP outperforms 14 baseline attacks on NIPS2017 and CIFAR-10 datasets, particularly against adversarially trained models.
- AEs generated by DRAP show significantly higher flatness over diverse surrogate models compared to baseline methods.
- Model-specific reverse perturbations in DRAP are crucial for maintaining attack effectiveness when using diverse surrogate sets.

## Why This Works (Mechanism)

### Mechanism 1: Flat Minima as Generalization Against Model Shift
- Claim: If adversarial examples are optimized to reside in flat regions of the surrogate loss landscape, then they are more likely to remain effective against target models with shifted loss landscapes.
- Mechanism: The paper utilizes a PAC-Bayesian bound to show that surrogate risk is upper-bounded by empirical risk plus a sharpness penalty. By minimizing this sharpness (seeking flat minima), the AE becomes robust to the perturbations in the loss landscape caused by switching from the surrogate to the target model.
- Core assumption: The loss landscapes of surrogate and target models are sufficiently correlated such that a flat region in one corresponds to a low-loss region in the other.
- Evidence anchors:
  - [abstract] "optimizing AEs toward flat minima over the surrogate model set... yields a comprehensive guarantee for AE transferability."
  - [section IV-D] Equation 24 explicitly separates the "sharpness" term from the discrepancy term to bound target risk.
  - [corpus] The paper "A Flat Minima Perspective on Understanding Augmentations and Model Robustness" generally supports the link between flat minima and robustness, though DRAP applies this specifically to adversarial transferability.
- Break condition: The mechanism fails if the surrogate and target loss landscapes are uncorrelated or shifted so drastically that flat regions in the surrogate map to high-loss regions in the target.

### Mechanism 2: Adversarial Model Discrepancy Control via Diversity
- Claim: If the surrogate model set captures a diverse range of adversarial vulnerabilities similar to the target distribution, then the transferability gap (difference between surrogate and target risk) is bounded.
- Mechanism: The paper derives a bound on the "transferability gap" using a novel "adversarial model discrepancy" measure (based on $\phi$-divergence). To minimize this discrepancy, the algorithm constructs a surrogate set with high "between-distribution" (different architectures) and "within-distribution" (weight trajectory sampling) diversity.
- Core assumption: The target model distribution can be approximated by a mixture of diverse surrogate components (prototypes).
- Evidence anchors:
  - [abstract] "controlling the surrogate-target model shift measured by the adversarial model discrepancy... yields a comprehensive guarantee."
  - [section V-B] "between-distribution diversity... [and] within-distribution diversity... is valuable in improving diversity... to provide a better approximation to future target models."
  - [corpus] Corpus neighbors focus on transferability but lack specific theoretical grounding on $\phi$-divergence discrepancy, making this paper's theoretical contribution distinct.
- Break condition: The mechanism fails if the target model employs a defense mechanism or vulnerability profile entirely orthogonal to the prototypes chosen for the surrogate set.

### Mechanism 3: Model-Specific Reverse Perturbation (DRAP)
- Claim: If diverse surrogate models have conflicting loss geometries, then generating model-specific reverse perturbations ($\epsilon_k$) is required to effectively flatten the loss landscape across all models, rather than using a single global perturbation.
- Mechanism: Standard RAP uses a global reverse perturbation which averages gradients, potentially canceling out effective attack directions for diverse models. DRAP optimizes a distinct perturbation $\epsilon_k$ for each model in the inner loop, ensuring the AE finds a region flat for that specific model, before aggregating updates in the outer loop.
- Core assumption: The optimal "worst-case" perturbation (to find the flat region) differs significantly between diverse models.
- Evidence anchors:
  - [abstract] "DRAP constructs a diverse surrogate model set and generates model-specific reverse adversarial perturbations."
  - [section V-C] Equation 31 vs 28 contrasts the model-specific approach against the global approach of original RAP.
  - [section VI-C1] Table VI shows "DRAP" outperforming "Flat-RAP" (global perturbation), validating that model-specific perturbations are superior for diverse sets.
  - [corpus] Weak support; "Boosting Adversarial Transferability via Residual Perturbation Attack" discusses residual attacks but does not explicitly compare global vs. model-specific strategies for diverse ensembles.
- Break condition: The mechanism degrades if the surrogate models are too similar, making the computational overhead of specific perturbations unnecessary, or if the inner loop iterations ($T$) are insufficient to find the local maxima for each model.

## Foundational Learning

- Concept: **Transfer-based Black-box Attacks**
  - Why needed here: The fundamental problem setting. You must understand that the attacker has access only to "surrogate" models but intends to fool "target" models.
  - Quick check question: Can you explain why minimizing loss on a ResNet-50 surrogate doesn't guarantee transferability to a ViT target?

- Concept: **PAC-Bayesian Bounds**
  - Why needed here: The theoretical engine of the paper. It connects "flatness" (robustness to perturbation) to generalization (low risk on unseen models).
  - Quick check question: How does the "sharpness" term in a PAC-Bayes bound relate to the "flatness" of a minimum?

- Concept: **$\phi$-divergence**
  - Why needed here: Used to measure the "adversarial model discrepancy." It provides the mathematical justification for why increasing model diversity helps (it minimizes this divergence).
  - Quick check question: Why is Total Variation (TV) distance used as an instantiation of the discrepancy measure in Corollary 1?

## Architecture Onboarding

- Component map: Surrogate Pool -> Sampler -> Inner Loop (Max) -> Outer Loop (Min)
- Critical path:
  1. Select diverse surrogate architectures (ResNet, ViT, etc.).
  2. Fine-tune and collect model weights to build the set $M_S$.
  3. For each iteration, sample a model $w_k$.
  4. Calculate $\epsilon_k$ (Inner Max) - *This is the DRAP specific step.*
  5. Update $\hat{x}$ using gradient from $w_k$ and $\epsilon_k$ (Outer Min).
- Design tradeoffs:
  - **Diversity vs. Convergence:** High diversity (many prototypes) ensures theoretical bounds are tight but creates conflicting gradients. DRAP solves this via model-specific perturbations, but increases compute cost (inner loop steps).
  - **Late Start Strategy:** Starting the flatness optimization ($\epsilon_k \neq 0$) too early prevents the AE from reaching a high-loss region initially. The paper uses $n_{LS}=5$ to delay flatness seeking.
- Failure signatures:
  - **Low Transfer, High White-box:** Indicates overfitting to surrogates. The sharpness penalty is likely insufficient, or the surrogate set lacks diversity.
  - **Stalled Optimization:** If momentum is not used or if the global perturbation approach (Flat-RAP) is mistakenly used, conflicting gradients from diverse models can stall the attack.
- First 3 experiments:
  1. Verify Flatness Contribution: Run ablation removing the sharpness penalty (set $\rho=0$ or use standard I-FGSM) on the diverse set to confirm the performance drop shown in Table V.
  2. Verify Diversity Contribution: Construct surrogates using only one prototype (e.g., only ConvNets) and measure the drop in transferability to other prototypes (Table VII).
  3. Compare Optimization Solvers: Implement both Flat-RAP (global $\epsilon$) and DRAP (specific $\epsilon_k$) on the same diverse set to empirically validate Table VI.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can DRAP maintain its superiority when adversarial examples are expected to transfer to unseen prototypes with fundamentally different adversarial vulnerabilities not captured by current surrogate categories?
- Basis in paper: [explicit] Section VI-C2 states that due to the "endless evolution of model architectures," achieving ideal diversity is challenging and raises concerns about performance against "unseen prototypes with unexpected adversarial vulnerabilities."
- Why unresolved: The experiments primarily evaluate transfer within the same prototypical categories or "shifted" sets where the target category is known but removed from surrogates, not against architecturally novel categories.
- What evidence would resolve it: Evaluation against target models employing architectures outside the ConvNet/Metaformer paradigm (e.g., state-space models or fully neural operators) not included in the surrogate set.

### Open Question 2
- Question: Is the specific strategy of gathering SGD trajectory proposals the optimal method for achieving within-distribution diversity, or would other sampling methods better approximate the surrogate model distribution?
- Basis in paper: [inferred] Section V-B justifies using SGD trajectory proposals to sample high-performing models from the loss valley, but does not compare this against alternative posterior sampling or model zoo generation techniques.
- Why unresolved: The choice of SGD trajectory sampling is based on prior work ([39], [40]) but is not empirically compared against other methods for generating diverse within-distribution samples.
- What evidence would resolve it: An ablation study comparing the transferability of AEs generated using SGD trajectory samples versus those generated using dropout-based sampling or deep ensembles.

### Open Question 3
- Question: How tight is the relationship between the theoretical transferability bound (Theorem 4) and the empirical success rate of DRAP under varying hyperparameter settings?
- Basis in paper: [inferred] The paper derives a theoretical bound involving adversarial model discrepancy and sharpness, then proposes an algorithm to optimize an empirical instantiation of this bound, but does not quantify the tightness of this estimation.
- Why unresolved: While the algorithm is "inspired" by the theory, the direct correlation between reductions in the theoretical bound terms and actual attack success rates is assumed but not rigorously quantified.
- What evidence would resolve it: A correlation analysis plotting the theoretical bound values (sharpness and discrepancy penalties) against attack success rates across different attack iterations and hyperparameter settings.

## Limitations
- The theoretical bounds assume strong correlation between surrogate and target loss landscapes, which may not hold for architectures with fundamentally different inductive biases.
- The computational cost of DRAP's inner loop scales linearly with model diversity, making it potentially prohibitive for larger ensembles.
- The paper does not quantify how many prototypes are sufficient or provide explicit coverage guarantees for the target model distribution.

## Confidence
- **High Confidence:** The empirical superiority of DRAP over baselines (Tables V-VII) is well-supported by extensive experiments across 31 target models.
- **Medium Confidence:** The theoretical framework provides sound mathematical grounding for the approach, but the assumptions about loss landscape correlation and prototype coverage are difficult to verify independently.
- **Medium Confidence:** The mechanism that model-specific perturbations are superior to global perturbations is empirically validated, but the analysis could be more rigorous about when this advantage diminishes.

## Next Checks
1. Systematically vary the number and composition of surrogate prototypes to empirically determine how diversity impacts transferability bounds and identify saturation points.
2. Measure the correlation between surrogate and target loss landscapes using gradient cosine similarity to validate the assumption that flat regions transfer across architectures.
3. Implement a variant of DRAP that uses model clustering to reduce the number of model-specific perturbations, then measure the tradeoff between attack success rate and computational cost.