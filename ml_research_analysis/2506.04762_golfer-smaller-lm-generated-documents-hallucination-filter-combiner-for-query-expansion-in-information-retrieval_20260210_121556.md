---
ver: rpa2
title: 'GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner for
  Query Expansion in Information Retrieval'
arxiv_id: '2506.04762'
source_url: https://arxiv.org/abs/2506.04762
tags:
- query
- documents
- retrieval
- golfer
- expansion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GOLFer addresses the high cost and computational demands of large
  language models (LLMs) in query expansion for information retrieval by using smaller
  open-source LMs instead. The core idea is to detect and filter non-factual or inconsistent
  sentences generated by smaller LMs and then combine the filtered content with the
  original query using a weight vector to balance their influence.
---

# GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner for Query Expansion in Information Retrieval

## Quick Facts
- **arXiv ID**: 2506.04762
- **Source URL**: https://arxiv.org/abs/2506.04762
- **Reference count**: 35
- **Primary result**: Uses smaller LMs with hallucination filtering to achieve competitive query expansion performance while reducing computational costs compared to LLMs

## Executive Summary
GOLFer addresses the computational and cost barriers of using large language models for query expansion in information retrieval by leveraging smaller open-source language models. The method introduces a hallucination detection and filtering mechanism to remove non-factual or inconsistent generated content, then combines the filtered expansions with original queries using optimized weighting. This approach maintains competitive retrieval performance while significantly reducing resource requirements, achieving notable improvements on both web search and low-resource retrieval benchmarks.

## Method Summary
GOLFer operates by first generating query expansions using smaller open-source language models, then applying a hallucination detection module to filter out unreliable content. The filtered expansions are combined with the original query through a weighted combination mechanism where the weights are optimized on development data. The system evaluates on three web search datasets (MS MARCO, TREC DL19/20) and ten low-resource BEIR datasets, using standard retrieval metrics like MRR@10, Recall@1k, and nDCG@10 to measure performance.

## Key Results
- On MS MARCO dev, achieves MRR@10 of 0.199 (+1.5 over query2doc) and Recall@1k of 0.883 (+7.9)
- On BEIR, improves nDCG@10 by up to 4.9 points on NQ and SciFact datasets
- Consistently outperforms other smaller LM-based methods while remaining competitive with LLM-based approaches

## Why This Works (Mechanism)
The method works by exploiting the cost-effectiveness of smaller LMs while mitigating their hallucination tendencies through detection and filtering. By removing unreliable generated content before combination with the original query, GOLFer preserves the benefits of query expansion without introducing noise that could degrade retrieval quality. The weighted combination allows flexible control over the balance between original query intent and expanded context, optimizing for each specific retrieval task.

## Foundational Learning

**Query Expansion**: The process of automatically adding terms to a search query to improve retrieval recall and precision. Needed because users often express information needs imprecisely, and expansion can capture related concepts and synonyms.

**Hallucination Detection**: Identifying when language models generate content that is factually incorrect or inconsistent with source material. Quick check: Evaluate detection accuracy on annotated hallucination datasets.

**Weighted Combination**: Merging two text representations with adjustable influence weights. Needed to balance original query intent with expanded context. Quick check: Test retrieval performance across different weight configurations.

**Retrieval Metrics**: Standard IR evaluation measures including MRR@10 (Mean Reciprocal Rank at 10), Recall@1k, and nDCG@10 (normalized Discounted Cumulative Gain at 10). Quick check: Ensure metric calculations follow standard conventions.

## Architecture Onboarding

**Component Map**: Input Query -> Smaller LM Generator -> Hallucination Filter -> Weight Vector Optimizer -> Weighted Combination -> Output Expanded Query -> Retriever

**Critical Path**: The generation-filtering-weighting pipeline forms the critical path, as each stage depends on the previous one's output and directly impacts final retrieval performance.

**Design Tradeoffs**: Smaller LMs reduce computational cost but increase hallucination risk; the filtering mechanism adds complexity but enables reliable use of smaller models. The weight optimization balances original query fidelity against expansion benefits.

**Failure Signatures**: Poor retrieval performance when hallucination filtering misses significant noise; suboptimal weights causing over-reliance on either original query or expansions; generation failures producing insufficient or irrelevant expansions.

**First Experiments**:
1. Test retrieval performance with and without hallucination filtering on a validation set
2. Evaluate different weight configurations on development data to find optimal balance
3. Compare generation quality and quantity between different smaller LM architectures

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation lacks detailed computational efficiency measurements including inference time, memory usage, and cost comparisons
- Hallucination filtering relies on relatively small annotated datasets (600 examples for MS MARCO, 500 for BEIR) limiting generalizability
- Weight vector optimization uses only 81 grid search points, potentially missing optimal configurations
- Assumes static query expansion without dynamic or context-aware weighting strategies

## Confidence

**High Confidence**: Retrieval performance improvements over smaller LM baselines are well-supported by experimental results across multiple datasets. Methodology for hallucination detection and filtering is clearly described and reproducible.

**Medium Confidence**: Comparison with LLM-based methods is limited by not accounting for model size variations, prompting strategies, or comprehensive cost trade-offs. Generalizability to non-web search domains remains uncertain.

**Low Confidence**: Practical cost-benefit analysis lacks concrete metrics, making real-world applicability assessment difficult for resource-constrained scenarios.

## Next Checks

1. Conduct controlled experiments measuring actual inference time, memory consumption, and API costs for GOLFer versus both smaller LMs and LLM-based query expansion methods across different hardware configurations.

2. Evaluate the hallucination detection module on a held-out test set from a different domain (e.g., medical or scientific literature) to assess cross-domain robustness and potential domain-specific hallucination patterns.

3. Perform ablation studies systematically varying the weight vector granularity and optimization method to determine sensitivity of retrieval performance to these hyperparameters and identify potential improvements.