---
ver: rpa2
title: 'Semantic Deception: When Reasoning Models Can''t Compute an Addition'
arxiv_id: '2512.20812'
source_url: https://arxiv.org/abs/2512.20812
tags:
- llms
- level
- reasoning
- semantic
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether large language models (LLMs) can\
  \ perform symbolic reasoning when faced with semantically deceptive prompts. Researchers\
  \ introduced four levels of semantic load\u2014ranging from meaningless symbols\
  \ to meaningful questions\u2014while keeping the underlying arithmetic task constant."
---

# Semantic Deception: When Reasoning Models Can't Compute an Addition

## Quick Facts
- arXiv ID: 2512.20812
- Source URL: https://arxiv.org/abs/2512.20812
- Reference count: 21
- Key outcome: Large language models' arithmetic accuracy significantly declines as semantic load increases, even when task recognition remains intact

## Executive Summary
This study reveals that large language models (LLMs) struggle with basic arithmetic when presented in semantically meaningful contexts. Researchers designed a controlled experiment where four models—including reasoning models with Chain-of-Thought—performed identical addition problems across four levels of semantic load, from meaningless symbols to contextually rich questions. Results showed that while models correctly identified the arithmetic task at low semantic loads, their execution accuracy deteriorated significantly as semantic content increased. Reasoning models were particularly vulnerable, sometimes answering the embedded question instead of computing the sum. These findings challenge assumptions about LLMs' symbolic reasoning capabilities and highlight their susceptibility to learned linguistic patterns over explicit instruction.

## Method Summary
The researchers tested four LLMs (GPT-4o, GPT-3.5, o1, r1) on simple addition problems presented in four semantic load levels while keeping the underlying arithmetic constant. Level 1 used meaningless symbols (3 ⊕ 5), while Level 4 incorporated semantically rich contexts (e.g., "What is the capital of France? Answer in one word. 3 + 5 = ?"). Each model completed 50 trials per condition with explicit symbol-to-digit mappings provided. The study measured both task recognition (identifying the operation) and execution accuracy (computing the correct sum), analyzing how performance degraded as semantic content increased.

## Key Results
- Model accuracy in performing correct calculations declined significantly as semantic load increased
- Reasoning models with integrated Chain-of-Thought were more susceptible to semantic interference than non-reasoning models
- While most models correctly identified addition at low semantic loads, execution accuracy dropped sharply at higher semantic loads
- Some reasoning models answered embedded questions instead of computing the sum when semantic load was high
- Semantic interference operated continuously through generation, not just at task recognition stage

## Why This Works (Mechanism)

### Mechanism 1: Semantic Interference with Symbolic Task Recognition
When explicit symbolic mappings conflict with learned semantic associations, models default to responding to surface-level meaning rather than following defined symbol operations. Next-token prediction is biased toward high-probability semantic completions from training data (e.g., "Paris" for "capital of France") which compete with the explicitly instructed symbol-to-digit mapping task. Statistical associations from pretraining are not overridden by in-context instruction following when semantic signals are strong enough.

### Mechanism 2: Hidden Degradation of Execution Accuracy Under Semantic Load
Semantic interference operates continuously through generation, degrading calculation accuracy even when the model correctly identifies the task. Attention to semantic content during token generation competes with maintaining symbol mappings in working context, causing errors mid-computation. The interference is not a binary "task recognized vs. not recognized" but a gradient affecting computational fidelity.

### Mechanism 3: Chain-of-Thought Amplification Effect
Integrated CoT can counterproductively amplify semantic interference by repeating misleading semantic content during the reasoning trace. CoT generation re-encodes semantic elements from the prompt, reinforcing attention to misleading associations rather than abstracting them away. CoT is fundamentally token generation, not a separate reasoning module with meta-cognitive oversight.

## Foundational Learning

- **Concept: Symbol-Reference vs. Symbol-Operation Distinction**
  - Why needed here: The paper tests whether models can treat symbols as arbitrary operations (3 + 5) versus responding to their semantic content ("France" → "Paris")
  - Quick check question: Given "X represents + and Y represents 5," what should the model compute for "3 X Y"? If it produces semantic associations with X or Y, it has failed symbol-operation mode.

- **Concept: Autoregressive Token Generation**
  - Why needed here: CoT is not a reasoning module but sequential token prediction; each token conditions on all prior tokens including semantic content
  - Quick check question: Why might generating "The sentence asks about France..." as part of a CoT trace increase the probability of outputting "Paris"?

- **Concept: Distributional vs. Rule-Based Processing**
  - Why needed here: LLMs optimize for probable continuations, not rule-following; explicit mappings are soft constraints competing with strong distributional priors
  - Quick check question: Why does "answer in one word" (Level 4b) create stronger interference than an open-ended prompt (Level 4a)?

## Architecture Onboarding

- **Component map**: Input prompt → [Instruction parser] → Symbol mapping table → [Semantic context encoder] → [CoT generator, if present] → [Arithmetic execution] → Final answer

- **Critical path**: The paper shows the critical failure mode is semantic content bleeding into the execution pathway. Your first debugging step is always: did the model recognize the task, or did it respond to the semantic wrapper?

- **Design tradeoffs**:
  - Reasoning models (o1, r1): Better at complex multi-step tasks, but vulnerable to semantic override on simple tasks embedded in semantic context
  - Non-reasoning models (GPT-4o, v3): More robust task recognition under semantic load, but weaker on genuinely complex reasoning
  - CoT visibility: Hidden CoT (o1) prevents inspection; visible CoT (r1, user-prompted) allows failure diagnosis

- **Failure signatures**:
  1. Task override: Model answers embedded question ("Paris") instead of computing sum
  2. Confusion state: Model computes correctly mid-response but outputs wrong final answer
  3. Accuracy gradient: Correct task identification but degrading accuracy as semantic load increases

- **First 3 experiments**:
  1. Baseline mapping test: Test your model on Level 1 (meaningless symbols) to establish maximum symbolic manipulation capability
  2. Semantic gradient test: Run identical additions across Levels 1-4 to measure both task recognition rate and execution accuracy; plot both curves separately
  3. CoT comparison: If using a model with accessible CoT, examine whether semantic content appears in the reasoning trace before the computation step; correlate trace content with failure mode

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the semantic interference observed in simple addition tasks generalize to complex multi-step reasoning problems?
- Basis in paper: The discussion suggests "These findings could likely extend to more complex problems, where identifying and quantifying LLMs’ errors becomes even harder."
- Why unresolved: The study isolated simple arithmetic to ensure robustness; complex tasks introduce confounding variables that were not controlled.
- What evidence would resolve it: Testing the semantic deception framework on tasks requiring logical inference or multi-hop reasoning.

### Open Question 2
- Question: To what extent do specific punctuation marks (dots, question marks) or imperative phrases (e.g., "answer in one word") drive the semantic deception effect?
- Basis in paper: Section 5.4 states, "Further test could be done with the presence and not of symbols like dots, question marks or 'answer in one word' to investigate their impact."
- Why unresolved: The current methodology held these markers constant to maintain semantic load levels, preventing the isolation of their individual contribution.
- What evidence would resolve it: An ablation study varying punctuation and phrasing while keeping the semantic context constant.

### Open Question 3
- Question: Does the variability in exercise presentation format affect the model's susceptibility to semantic deception?
- Basis in paper: Section 5.4 notes a limitation: "we have not added variability in the way the exercises are presented."
- Why unresolved: The study used a fixed prompt template to ensure comparability, leaving the model's sensitivity to prompt formatting unexplored.
- What evidence would resolve it: Re-running experiments with diverse prompt templates and instruction phrasings.

## Limitations

- The controlled symbol-mapping task may not fully represent real-world semantic deception scenarios where mappings are implicit or context-dependent
- The sample size of 50 trials per condition provides statistical power but may not capture rare failure modes
- The focus on addition restricts findings to a specific computational domain, and performance on other symbolic tasks (multiplication, logical reasoning) could differ
- The paper does not explore whether explicit training on semantic-deception tasks could mitigate these effects

## Confidence

- **High Confidence**: The observation that semantic load degrades arithmetic execution accuracy across multiple models is well-supported by consistent experimental results (Section 4.2, Figure 4)
- **Medium Confidence**: The claim that CoT amplifies semantic interference is plausible but primarily based on indirect evidence and theoretical reasoning rather than direct experimental manipulation of CoT visibility
- **Medium Confidence**: The mechanism explanation linking token prediction bias to semantic associations is theoretically sound but requires more direct experimental validation to establish causation versus correlation

## Next Checks

1. **Cross-task generalization test**: Replicate the semantic deception experiment using multiplication, subtraction, and logical reasoning tasks to determine whether the effect is specific to addition or represents a general limitation in symbolic computation under semantic load

2. **CoT visibility manipulation**: Compare model performance on identical semantic deception prompts when CoT is hidden (o1-style) versus when it is user-visible (r1-style or user-prompted), measuring both accuracy and the presence of semantic content in the reasoning trace

3. **Fine-tuning intervention**: Train a subset of models on examples containing explicit symbol mappings within semantically meaningful contexts, then test whether this training reduces semantic interference compared to baseline models without such fine-tuning