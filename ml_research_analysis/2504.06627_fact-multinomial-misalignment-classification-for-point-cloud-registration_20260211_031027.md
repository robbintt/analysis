---
ver: rpa2
title: 'FACT: Multinomial Misalignment Classification for Point Cloud Registration'
arxiv_id: '2504.06627'
source_url: https://arxiv.org/abs/2504.06627
tags:
- point
- cloud
- registration
- fact
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FACT is a multinomial misalignment classification method for point
  cloud registration that predicts alignment quality by classifying point cloud pairs
  into discrete error classes. It extends binary classification approaches by using
  a custom regression-by-classification loss combining cross-entropy and Wasserstein
  losses, allowing finer-grained detection of registration errors.
---

# FACT: Multinomial Misalignment Classification for Point Cloud Registration

## Quick Facts
- arXiv ID: 2504.06627
- Source URL: https://arxiv.org/abs/2504.06627
- Reference count: 40
- Primary result: FACT achieves 97.4% accuracy classifying synthetic registration perturbations, substantially outperforming CorAl's 75.3%

## Executive Summary
FACT introduces multinomial misalignment classification for point cloud registration, extending binary classification approaches by predicting alignment quality across discrete error classes. The method employs a custom regression-by-classification loss combining cross-entropy and Wasserstein distances to respect ordinal relationships between misalignment degrees. Using a point transformer-based network with local entropy and optimal transport features, FACT successfully classifies both synthetic and real ICP/GeoTransformer registrations with substantially better performance than existing baselines.

## Method Summary
FACT classifies registered point cloud pairs into discrete misalignment classes using a point transformer network. The method extracts local features including differential entropy (separate and joint), Sinkhorn divergence, and reliability weights from co-visible regions. A custom loss function combines cross-entropy and 1D Wasserstein distance to leverage ordinal class relationships. The network processes 8-dimensional feature maps through point transformer blocks with transition down blocks, global pooling, and an MLP head for K-class classification.

## Key Results
- FACT achieves 97.4% accuracy on synthetic 10-class perturbed data versus CorAl's 75.3%
- Successfully generalizes to real ICP and GeoTransformer registrations with high accuracy
- Outperforms standard metrics like Chamfer distance and ICP residuals for detecting misalignment
- Demonstrates practical utility by enabling automatic correction of misaligned point cloud maps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multinomial misalignment classification with regression-by-classification loss provides finer-grained alignment quality prediction than binary classification.
- Mechanism: Maps alignment quality to K discrete, ordered classes using a custom loss combining cross-entropy and Wasserstein distance to respect class ordering.
- Core assumption: Misalignment degrees lie on a continuous spectrum that can be meaningfully discretized with ordinal relationships.
- Evidence anchors: [abstract] introduces multinomial misclassification; [section 4.3] explains regression-by-classification suitability.
- Break condition: If misalignment types are qualitatively different rather than degree-based, ordinal regression may be inappropriate.

### Mechanism 2
- Claim: Local entropy and optimal transport features capture alignment quality more robustly than global metrics.
- Mechanism: Extracts per-point features including local differential entropy and Sinkhorn divergence in neighborhoods to measure distributional mismatch.
- Core assumption: Misalignment manifests locally as inconsistencies affecting local entropy and distribution matching.
- Evidence anchors: [section 4.2] defines separate/joint local differential entropy and Sinkhorn divergence as features.
- Break condition: If point density varies drastically, local entropy becomes scene-dependent rather than alignment-dependent.

### Mechanism 3
- Claim: Co-visibility filtering and reliability weights focus computation on regions where alignment quality can actually be assessed.
- Mechanism: Uses Hidden Point Removal to determine visible points, applies co-visibility scores, and incorporates reliability weights to prevent penalizing non-overlapping regions.
- Core assumption: Alignment quality can only be assessed where both point clouds observe the same surface.
- Evidence anchors: [section 4.1] restricts PCMC to co-visible regions; [appendix B] visualizes co-visibility score sensitivity.
- Break condition: If sensors have very different viewpoints with limited overlap, aggressive filtering may discard most points.

## Foundational Learning

- Concept: Optimal Transport / Sinkhorn Divergence
  - Why needed here: Third feature dimension measuring distributional mismatch from misalignment.
  - Quick check question: Why does Sinkhorn divergence subtract self-transport terms W(α,α) and W(β,β)?

- Concept: Differential Entropy of Gaussian Point Distributions
  - Why needed here: First two features; local entropy decreases when correctly aligned clouds merge.
  - Quick check question: How does h(Ω) relate to covariance determinant, and why does alignment reduce joint entropy?

- Concept: Wasserstein-1 Loss for Ordinal Classification
  - Why needed here: Core training objective penalizing based on CDF differences, treating nearby class errors as less severe.
  - Quick check question: Why does W1 loss use CDF differences rather than direct class probability differences?

## Architecture Onboarding

- Component map: Point cloud pair → co-visibility filtering → FPS sampling → local feature extraction → Point Transformer → logits → argmax
- Critical path: Preprocessing (co-visibility, FPS) → Feature Extraction (8D features) → Classification (Point Transformer) → Output (K-class logits)
- Design tradeoffs:
  - Multinomial vs. binary: More granular but needs more data per class
  - Local vs. global: Robust but heavier compute
  - CE+W1 vs. either alone: Combines gradient strength with ordinal awareness; requires hyperparameter balancing
- Failure signatures:
  - High adjacent-class confusion: Reduce class count or adjust thresholds
  - Good on synthetic, poor on real: Distribution shift; need more real data
  - Low ground truth correlation: Verify features; check co-visibility aggressiveness
  - Vanishing gradients with W1-only: Use combined CE+W1 loss
- First 3 experiments:
  1. Binary comparison: Train on 2-class synthetic data; compare to CorAl (Tab. 1)
  2. Feature ablation: Train with entropy-only, Sinkhorn-only, full features on nuScenes ICP data
  3. Loss ablation: Compare regression-by-classification vs. MSE vs. CE-only on 5-class real task (Tab. 2)

## Open Questions the Paper Calls Out

- Can FACT be effectively integrated as a self-supervision signal to improve the training or automatic correction of point cloud registration networks?
  - Basis: [explicit] Conclusion hopes FACT will find use for self-supervision in learning settings.
  - Unresolved: Paper demonstrates classification but doesn't implement feedback loop for active registration improvement.

- Can the feature extraction process be optimized to support real-time, online misalignment detection in SLAM?
  - Basis: [inferred] Appendix C reports feature extraction takes ~1.5s, classification only 5ms.
  - Unresolved: High latency makes method currently suitable only for offline post-processing.

- Does FACT generalize to registration errors produced by methods fundamentally different from ICP and GeoTransformer?
  - Basis: [inferred] Experiments limited to ICP and GeoTransformer; different algorithms may produce distinct error distributions.
  - Unresolved: Performance on other registration methods (FGR, RANSAC) remains untested.

## Limitations

- Limited ablation studies for the custom regression-by-classification loss make it difficult to isolate the Wasserstein component's contribution
- Absent feature importance analysis prevents empirical validation of entropy and Sinkhorn divergence effectiveness
- No comparison to non-learned baselines in terms of computational efficiency or density variation robustness

## Confidence

- **High**: FACT's superior classification accuracy on synthetic perturbed data (97.4% vs. 75.3% for CorAl) and generalization to real registrations
- **Medium**: Claims about local entropy and Sinkhorn divergence features being more robust than global metrics
- **Medium**: Practical utility claim of automatic correction, as results show improvement but not comprehensive validation

## Next Checks

1. Conduct controlled ablation study varying Wasserstein loss weight (α) to determine optimal contribution and assess sensitivity
2. Implement feature ablation experiment training FACT with entropy-only, Sinkhorn-only, and combined features on real registration data
3. Compare FACT's computational requirements and runtime against standard geometric metrics (Chamfer distance, ICP residuals) on identical datasets