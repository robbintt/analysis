---
ver: rpa2
title: 'Conan-Embedding-v2: Training an LLM from Scratch for Text Embeddings'
arxiv_id: '2509.12892'
source_url: https://arxiv.org/abs/2509.12892
tags:
- training
- embedding
- data
- arxiv
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Conan-embedding-v2 addresses the data and training gaps between
  LLMs and text embedding models by training a 1.4B-parameter LLM from scratch with
  news and multilingual data, using a soft-masking mechanism to bridge causal and
  bidirectional training paradigms, and introducing cross-lingual retrieval datasets
  and dynamic hard negative mining. The model achieves state-of-the-art performance
  on both English and Chinese MTEB benchmarks, with average scores of 73.52 and 74.24
  respectively, while maintaining high inference speed and model efficiency.
---

# Conan-Embedding-v2: Training an LLM from Scratch for Text Embeddings

## Quick Facts
- **arXiv ID**: 2509.12892
- **Source URL**: https://arxiv.org/abs/2509.12892
- **Reference count**: 19
- **Primary result**: Achieves SOTA performance on both English (73.52) and Chinese (74.24) MTEB benchmarks with a 1.4B-parameter LLM trained from scratch

## Executive Summary
Conan-embedding-v2 addresses the data and training gaps between LLMs and text embedding models by training a 1.4B-parameter LLM from scratch with news and multilingual data, using a soft-masking mechanism to bridge causal and bidirectional training paradigms, and introducing cross-lingual retrieval datasets and dynamic hard negative mining. The model achieves state-of-the-art performance on both English and Chinese MTEB benchmarks, with average scores of 73.52 and 74.24 respectively, while maintaining high inference speed and model efficiency.

## Method Summary
Conan-embedding-v2 employs a 4-stage training pipeline starting from a 1.4B-parameter LLM (8 layers, 3584 hidden, 32 heads). The approach introduces a novel soft-masking mechanism that gradually transitions from causal to bidirectional attention during weakly-supervised training, followed by supervised fine-tuning with dynamic hard negative mining and Matryoshka representation learning. The model is trained on a massive corpus of ~3T tokens (News, QA, Web) plus ~1.7B weakly-supervised pairs and ~10M supervised pairs, including 10M cross-lingual pairs across 26 languages.

## Key Results
- Achieves SOTA performance on English MTEB with average score of 73.52
- Achieves SOTA performance on Chinese MTEB with average score of 74.24
- Maintains high inference speed and model efficiency while outperforming larger models

## Why This Works (Mechanism)
The soft-masking mechanism bridges the gap between causal language modeling and bidirectional embedding training by gradually unmasking attention connections during the training process. This allows the model to leverage its strong pretraining while adapting to the bidirectional nature required for effective embeddings. Dynamic hard negative mining ensures the model continuously learns from challenging examples, preventing performance degradation from static negative samples.

## Foundational Learning
- **Soft Masking**: Gradually transitions from causal to bidirectional attention by interpolating mask values using a scheduling function. Why needed: Enables smooth adaptation from LLM pretraining to bidirectional embedding training. Quick check: Verify attention mask transitions smoothly from triangular to fully unmasked.
- **Dynamic Hard Negative Mining**: Replaces negative samples when their similarity scores fall below certain thresholds. Why needed: Prevents model from learning from stale, easy negatives that don't provide meaningful training signal. Quick check: Monitor negative similarity scores during training to ensure they stay within optimal range.
- **Matryoshka Representation Learning**: Enables flexible dimensionality for embeddings. Why needed: Allows efficient inference at different dimensionalities while maintaining performance. Quick check: Validate that lower-dimensional representations maintain reasonable performance.

## Architecture Onboarding
- **Component Map**: Pretraining (3T tokens) -> Soft Mask (Weakly-supervised) -> DHNM (Supervised) -> MRL (Post-processing)
- **Critical Path**: The soft-masking mechanism during weakly-supervised training is critical - improper implementation leads to optimization instability
- **Design Tradeoffs**: Chose 1.4B parameters for efficiency vs. larger models; implemented from-scratch training vs. fine-tuning existing models
- **Failure Signatures**: Loss spikes during transition from pretraining to weakly-supervised training indicate soft mask implementation issues; stagnant retrieval performance suggests DHNM not functioning properly
- **First Experiments**: 1) Implement soft mask on small dataset to verify attention transition, 2) Test DHNM logic with static negatives, 3) Validate MRL implementation with dimension reduction

## Open Questions the Paper Calls Out
- Can RAG or numerical data enrichment enable embedding models to accurately distinguish quantitative semantic differences?
- Does balanced data sampling significantly mitigate performance gaps for low-resource and typologically distant languages?
- To what extent can the current architecture be extended to support cross-modal retrieval capabilities?

## Limitations
- The paper doesn't specify exact pretraining data mixing ratios, which could affect reproducibility
- Current models struggle with numerical reasoning, treating numbers as regular tokens rather than understanding quantitative relationships
- Performance bias exists favoring high-resource languages like English and Chinese over typologically distant languages

## Confidence
- **High**: Model architecture (1.4B parameters, 8 layers, 3584 hidden, 32 heads) and overall 4-stage training pipeline are clearly specified
- **Medium**: Soft-masking mechanism and DHNM logic are described sufficiently but have some implementation ambiguity
- **Low**: Specific data mixing ratios for pretraining corpus are not provided

## Next Checks
1. Implement and validate soft-masking mechanism on small dataset to ensure attention transition behaves as expected
2. Verify DHNM logic by logging similarity scores of hard negatives during training
3. Conduct ablation study comparing baseline full fine-tuning vs. Conan-embedding-v2 to isolate impact of training innovations