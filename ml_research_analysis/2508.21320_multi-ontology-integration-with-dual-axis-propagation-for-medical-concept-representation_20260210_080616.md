---
ver: rpa2
title: Multi-Ontology Integration with Dual-Axis Propagation for Medical Concept Representation
arxiv_id: '2508.21320'
source_url: https://arxiv.org/abs/2508.21320
tags:
- concept
- medical
- ontology
- linko
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of limited medical concept
  representation in electronic health records (EHRs) by integrating multiple medical
  ontologies (diseases, drugs, procedures) into a unified learning framework. The
  proposed LINKO method uses a large language model (LLM) for graph-augmented initialization
  of concept embeddings, then performs dual-axis knowledge propagation: horizontal
  message passing across ontologies at each hierarchical level and vertical message
  passing within each ontology.'
---

# Multi-Ontology Integration with Dual-Axis Propagation for Medical Concept Representation

## Quick Facts
- arXiv ID: 2508.21320
- Source URL: https://arxiv.org/abs/2508.21320
- Reference count: 40
- Outperforms state-of-the-art by 2-3 percentage points in AUPRC for diagnosis prediction

## Executive Summary
This paper addresses the challenge of limited medical concept representation in electronic health records by integrating multiple medical ontologies (diseases, drugs, procedures) into a unified learning framework. The proposed LINKO method uses a large language model for graph-augmented initialization of concept embeddings, then performs dual-axis knowledge propagation: horizontal message passing across ontologies at each hierarchical level and vertical message passing within each ontology. Extensive experiments on MIMIC-III and MIMIC-IV datasets demonstrate significant performance improvements over state-of-the-art baselines, with AUPRC scores of 32.38% on MIMIC-IV and 31.79% on MIMIC-III.

## Method Summary
LINKO constructs a Meta-Knowledge Graph (Meta-KG) that integrates three medical ontologies (ICD-9 diagnosis/procedure, ATC drugs) with 3-level hierarchies. The method employs LLM-initialized embeddings using graph-augmented prompts, then applies dual-axis propagation: Horizontal Message Passing (HMP) using Graph Attention Networks or Hypergraph Attention Networks across ontologies at each level based on co-occurrence patterns, and Vertical Message Passing (VMP) using sequential bottom-up (HGIP) and top-down (GRAM) propagation within each ontology's hierarchy. The resulting encoder can be plugged into existing models like Transformer for diagnosis prediction tasks.

## Key Results
- Achieves AUPRC of 32.38% on MIMIC-IV (vs 30.21% for best baseline)
- Achieves AUPRC of 31.79% on MIMIC-III (vs 29.38% for best baseline)
- Shows enhanced robustness for rare disease prediction scenarios
- Functions as a plug-in encoder improving existing EHR predictive models

## Why This Works (Mechanism)

### Mechanism 1: Dual-Axis Knowledge Fusion
Integrating intra-ontology hierarchy with inter-ontology co-occurrence enriches sparse medical concept representations by merging domain structure with clinical usage patterns. The system propagates information along two axes: vertical propagation (intra-ontology) flows through parent-child hierarchies, transferring generalized knowledge to specific codes, while horizontal propagation (inter-ontology) links concepts across different types at the same hierarchical level based on statistical co-occurrence in EHR data. This allows a rare disease to gain features not just from its parent category, but from frequently co-occurring procedures or medications.

### Mechanism 2: Semantic Warm-Start via LLM Context
Initializing graph nodes with LLM-derived embeddings injects external clinical knowledge, improving convergence and handling rare codes better than random initialization. Instead of training embeddings from scratch, LINKO queries an LLM with a prompt containing the code's description and its hierarchical ancestors. The LLM returns a dense vector that already encapsulates semantic relationships, which the Graph Neural Network then refines to the specific dataset.

### Mechanism 3: Sequential Hierarchical Aggregation (HGIP)
Treating the ontology as a sequence of subgraphs (rather than a flat structure) preserves the topological order of information flow, leading to more robust parent representations. The Vertical Message Passing module processes the ontology bottom-up, creating subgraphs for adjacent levels. Parent nodes are updated only after their children are processed, ensuring a parent concept accurately reflects the distilled information of all its descendants before passing information further up or back down.

## Foundational Learning

- **Concept:** Graph Message Passing (GNNs)
  - **Why needed here:** The core of LINKO is passing messages between nodes in a graph. You must understand how a node updates its state by looking at its neighbors.
  - **Quick check question:** Can you explain how a Graph Attention Network (GAT) weights the importance of different neighbors differently compared to a standard GCN?

- **Concept:** Hypergraphs
  - **Why needed here:** The paper uses a Hypergraph structure at the leaf level where a single visit connects multiple codes simultaneously, capturing high-order correlations.
  - **Quick check question:** How does a hyperedge differ from a standard edge, and why is this useful for representing a patient visit containing 10+ medical codes?

- **Concept:** Transfer Learning / Fine-tuning
  - **Why needed here:** LINKO initializes weights using an LLM and then "refines" them. This is a form of transfer learning where the GNN fine-tunes the LLM's knowledge to the specific EHR data.
  - **Quick check question:** Why might freezing the LLM embeddings lead to suboptimal performance in a specialized downstream task?

## Architecture Onboarding

- **Component map:** Medical codes -> LLM Prompt Engine -> text-embedding-3-small -> Initial Vectors -> HMP Module (GAT/HAT) -> VMP Module (HGIP + GRAM) -> Encoder Output -> Downstream models

- **Critical path:** First perform Horizontal Message Passing to infuse inter-ontology context at every level, then perform Vertical Message Passing (HGIP then GRAM) to distill this information up and down the hierarchy. Swapping this order or running them in parallel without synchronization may break the hierarchical dependency logic.

- **Design tradeoffs:**
  - **HAT vs. GAT:** Uses Hypergraph Attention for the leaf level where nodes are numerous and visits define complex connections, and standard GAT for ancestor levels where nodes are fewer. HAT is computationally heavier but captures group dynamics; GAT is faster but only captures pairwise dynamics.
  - **LLM Initialization:** Using GPT-3-small embeddings adds external dependency and cost. +2% performance gain and faster convergence vs. the complexity of managing an external embedding API.

- **Failure signatures:**
  - **Performance Plateau:** If AUPRC gains are minimal, check the co-occurrence threshold τ. If too low, the graph is too dense (noise); if too high, it's too sparse (disconnected).
  - **Rare Code Collapse:** If rare disease prediction fails, it likely indicates the "Horizontal" edges are missing for these sparse codes. Check if ancestor levels are correctly propagating down via GRAM.
  - **Over-smoothing:** If all code embeddings converge to similar vectors, reduce the number of GNN layers or increase dropout.

- **First 3 experiments:**
  1. **Plug-in Validation:** Run the base Transformer model on MIMIC-III diagnosis prediction, then swap the standard embedding layer with LINKO's encoder (freezing the main model initially) to verify the "plug-and-play" boost.
  2. **Ablation of Axes:** Run LINKO with only Vertical Propagation, then only Horizontal Propagation. Quantify the contribution of inter-ontology edges to confirm it is the novel driver of performance.
  3. **Rare vs. Common Analysis:** Stratify the test set by code frequency. Plot the performance delta (LINKO vs. Baseline) specifically for the 0-25% frequency bucket to demonstrate robustness to data scarcity.

## Open Questions the Paper Calls Out

### Open Question 1
Can an adaptive mechanism be developed to automatically select or combine graph architectures (e.g., GAT vs. Hypergraph Attention Networks) for Horizontal Message Passing based on dataset characteristics? The authors observe that "HAT achieved the best performance on MIMIC-IV... In contrast, GAT excelled on MIMIC-III," linking performance to code space density, but offer no unified solution for this divergence.

### Open Question 2
How can the Vertical Message Passing module be adapted to handle non-hierarchical or multi-relational ontologies, such as SNOMED CT, which contain polyhierarchies and non-tree structures? The methodology explicitly relies on sequential "parent-child" relationships in a strict hierarchy, while the Related Work section contrasts this with models that use relational ontologies.

### Open Question 3
To what extent does the "graph-retrieval-augmented initialization" depend on the specific LLM version or knowledge cutoff, and does this dependency introduce brittleness for novel or updated medical codes? The method relies on a specific frozen LLM for initialization; however, the paper does not analyze how the model behaves when the LLM lacks knowledge of a concept or if prompt phrasing changes significantly.

## Limitations
- Performance gains represent only 2-3 percentage points over strong baselines
- Computational overhead is substantial due to LLM initialization for each code
- Validation is limited to MIMIC datasets with specific ontologies (ICD-9, ATC)
- Claims about rare disease prediction robustness need more extensive testing

## Confidence

**High Confidence:** The core architectural claims about dual-axis propagation and plug-in compatibility are well-supported by ablation studies and quantitative results.

**Medium Confidence:** The LLM initialization benefits are demonstrated but could be influenced by specific prompt engineering choices not fully detailed in the methodology.

**Low Confidence:** The robustness claims for rare disease prediction scenarios would benefit from more extensive testing across different rare disease prevalence levels and healthcare systems.

## Next Checks

1. **Co-occurrence threshold sensitivity:** Systematically vary the co-occurrence threshold τ across levels to determine optimal values and test if performance is robust to these choices.

2. **Cross-ontology generalization:** Test LINKO on different medical coding systems (e.g., ICD-10, SNOMED CT) to validate claims about broader applicability beyond MIMIC's specific ontologies.

3. **Computational cost-benefit analysis:** Measure training time and inference latency with and without LLM initialization to quantify the practical tradeoff between performance gains and resource requirements.