---
ver: rpa2
title: Heterogeneous-Modal Unsupervised Domain Adaptation via Latent Space Bridging
arxiv_id: '2506.15971'
source_url: https://arxiv.org/abs/2506.15971
tags:
- domain
- target
- source
- adaptation
- hmuda
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel setting called Heterogeneous-Modal
  Unsupervised Domain Adaptation (HMUDA) that enables knowledge transfer between completely
  different modalities (e.g., 2D images to 3D point clouds) by leveraging a bridge
  domain containing unlabeled samples from both modalities. To address this setting,
  the authors propose Latent Space Bridging (LSB), a specialized framework for semantic
  segmentation tasks that employs a dual-branch architecture with a feature consistency
  loss to align representations across modalities and a domain alignment loss to reduce
  discrepancies between class centroids.
---

# Heterogeneous-Modal Unsupervised Domain Adaptation via Latent Space Bridging

## Quick Facts
- arXiv ID: 2506.15971
- Source URL: https://arxiv.org/abs/2506.15971
- Reference count: 40
- Proposes novel HMUDA setting for 2D-3D knowledge transfer

## Executive Summary
This paper introduces a novel setting called Heterogeneous-Modal Unsupervised Domain Adaptation (HMUDA) that enables knowledge transfer between completely different modalities (e.g., 2D images to 3D point clouds) by leveraging a bridge domain containing unlabeled samples from both modalities. To address this setting, the authors propose Latent Space Bridging (LSB), a specialized framework for semantic segmentation tasks that employs a dual-branch architecture with a feature consistency loss to align representations across modalities and a domain alignment loss to reduce discrepancies between class centroids. Extensive experiments on six benchmark datasets demonstrate that LSB achieves state-of-the-art performance, outperforming existing UDA methods by a significant margin. For example, on the 2D-to-3D task Virt→A2D2, LSB achieves an mIoU of 33.37, outperforming Source-Only and Pseudo-labeling baselines by 14.37 and 16.53, respectively.

## Method Summary
The paper proposes Latent Space Bridging (LSB) to address the HMUDA setting by bridging heterogeneous modalities through a shared latent space. The method uses a dual-branch architecture where a source branch processes labeled 2D images and a target branch processes unlabeled 3D point clouds. Both branches project their features to a shared latent space using linear layers. The training objective combines four losses: segmentation loss on source data, bridge segmentation loss using pseudo-labels from an EMA teacher, feature consistency loss to align representations of paired bridge data, and domain alignment loss to minimize class centroid discrepancies between source and target. The method leverages an unlabeled bridge domain containing paired samples from both modalities to enable cross-modal knowledge transfer without requiring target labels.

## Key Results
- LSB achieves mIoU of 33.37 on 2D-to-3D task Virt→A2D2, outperforming Source-Only by 14.37 and Pseudo-labeling by 16.53
- On 3D-to-2D task SemanticKITTI→VirtualKITTI, LSB achieves mIoU of 33.21, surpassing baselines by 12.39 and 13.46 respectively
- LSB demonstrates state-of-the-art performance across all six benchmark datasets evaluated

## Why This Works (Mechanism)
LSB works by creating a shared semantic space that bridges heterogeneous modalities through feature projection and alignment. The dual-branch architecture allows each modality to be processed by its optimal network (ResNet34+U-Net for images, SparseConvNet+U-Net for point clouds), while linear projection layers map both to a common latent space. The feature consistency loss ensures that paired bridge samples (same scene, different modalities) have similar representations in this shared space, establishing cross-modal correspondence. The domain alignment loss further reduces distribution gaps by minimizing the distance between class centroids across modalities. This combination enables knowledge transfer without requiring target labels, leveraging the bridge domain as a semantic anchor point between modalities.

## Foundational Learning
- **HMUDA Setting**: Unsupervised domain adaptation across different data modalities using a bridge domain with paired samples from both modalities. Needed to enable knowledge transfer between incompatible representations like 2D images and 3D point clouds.
- **Dual-Branch Architecture**: Separate processing streams for each modality with distinct network architectures optimized for their respective data structures. Critical for handling the inherent differences between image grids and point cloud voxels.
- **Feature Consistency Loss**: L2 distance between projected features of paired bridge samples to ensure cross-modal alignment. Establishes semantic correspondence between different modalities.
- **Domain Alignment Loss**: Cosine distance between class centroids to minimize distribution gaps. Reduces domain shift without requiring target labels.
- **EMA Teacher**: Exponential moving average of source network parameters to generate pseudo-labels for bridge target data. Provides stable targets for training the target branch.

## Architecture Onboarding

**Component map:** Bridge Data (2D, 3D) → Source Branch (ResNet34+U-Net) + Target Branch (SparseConvNet+U-Net) → Projection Layers → Shared Latent Space → Segmentation Heads

**Critical path:** Source Labeled Data → Source Branch → Segmentation Loss; Bridge Paired Data → Both Branches → Feature Consistency Loss + Bridge Segmentation Loss; Source+Target Data → Domain Alignment Loss

**Design tradeoffs:** Separate architectures optimized for each modality vs. unified architecture (chosen: separate for optimal performance); EMA teacher vs. direct student prediction (chosen: EMA for stability); feature consistency vs. adversarial alignment (chosen: consistency for explicit pairing)

**Failure signatures:** 
- Projection collapse: Features converge to zero vector, destroying semantic information
- Noisy pseudo-labels: Early training produces incorrect bridge labels, misguiding target branch
- Domain misalignment: Class centroids remain distant despite training

**First experiments:**
1. Validate projection layer dimension (256 vs 512) and its effect on feature consistency loss
2. Test EMA initialization strategy and its impact on bridge segmentation performance
3. Compare feature consistency loss vs. adversarial alignment approaches

## Open Questions the Paper Calls Out
- Can LSB be generalized to tasks beyond semantic segmentation like image classification or object detection?
- Is LSB effective for non-vision modalities such as text-to-image or audio-to-video adaptation?
- How sensitive is LSB to the strict requirement of paired data within the bridge domain?

## Limitations
- Requires paired bridge data which may be difficult to obtain in practice
- Currently only evaluated on vision-based modalities (2D images and 3D point clouds)
- Method specialized for semantic segmentation, generalization to other tasks requires further validation

## Confidence
- High confidence in overall framework architecture and loss formulation
- Medium confidence in implementation details due to missing hyperparameters
- Low confidence in exact reproduction of results without clarification on latent space dimension and EMA initialization

## Next Checks
1. Verify latent space projection dimension (256 vs 512) and its impact on feature consistency loss
2. Test different EMA initialization strategies for the teacher model (α_init starting values)
3. Validate the learning rate decay schedule by comparing polynomial decay with step decay baselines