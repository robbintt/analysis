---
ver: rpa2
title: Event-based evaluation of abstractive news summarization
arxiv_id: '2507.01160'
source_url: https://arxiv.org/abs/2507.01160
tags:
- summaries
- event
- events
- articles
- event-overlap
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an event-based evaluation metric for abstractive
  news summarization. The core idea is to evaluate summary quality by measuring overlapping
  events between generated summaries, reference summaries, and source articles using
  structured event extraction.
---

# Event-based evaluation of abstractive news summarization

## Quick Facts
- arXiv ID: 2507.01160
- Source URL: https://arxiv.org/abs/2507.01160
- Reference count: 5
- Primary result: Event-based evaluation metric using structured event extraction to assess abstractive news summarization quality

## Executive Summary
This paper introduces an event-based evaluation metric for abstractive news summarization that goes beyond traditional lexical overlap measures. The approach leverages structured event extraction to measure how well generated summaries capture the key events and their relationships compared to reference summaries and source articles. By analyzing event type overlap, argument role overlap, and full argument overlap with adapted precision and recall measures, the metric provides insights into whether language models are identifying and conveying the same events that human summarizers consider important. Experiments on the Norwegian NorSumm dataset reveal that while standard metrics like ROUGE-L and BERTScore provide one perspective, event-overlap scores expose meaningful differences in how models handle event identification and summarization.

## Method Summary
The event-based evaluation framework works by first extracting structured events from generated summaries, reference summaries, and source articles using a specialized event extraction system. These extracted events include event types, argument roles, and full argument information. The evaluation then computes overlap metrics across three dimensions: event types (what happened), argument roles (who did what), and complete argument sets (full event representation). Precision and recall are adapted for summary evaluation to measure how much of the important event content from references and sources is captured in generated summaries, and conversely, how much of the generated content represents valid events. The approach provides a structured way to assess whether models are focusing on the same key story elements that human summarizers identified.

## Key Results
- Normistral-11b-warm achieved the highest event-overlap scores among tested models, though rankings differed slightly from traditional metrics
- Analysis revealed that language models often focus on different events than human summarizers, highlighting challenges in identifying key story elements
- Event-overlap scores provided insights beyond standard metrics, showing that high ROUGE-L or BERTScore doesn't necessarily indicate good event coverage
- The approach demonstrated that event-based evaluation offers a valuable additional dimension for assessing abstractive summarization quality

## Why This Works (Mechanism)
Event-based evaluation works because news articles are fundamentally structured around key events, their participants, and relationships. By extracting and comparing these structured elements across generated summaries, reference summaries, and source articles, the metric directly measures whether summaries capture the core informational content that defines news stories. This approach is more semantically meaningful than surface-level lexical overlap because it focuses on the actual events and their participants rather than just word matching. The precision and recall adaptation ensures that both coverage of important events and avoidance of irrelevant content are evaluated appropriately for the summarization task.

## Foundational Learning
- **Event extraction fundamentals**: Understanding how events, arguments, and roles are identified from text is crucial because the entire evaluation depends on accurate extraction
  - Why needed: The metric's reliability hinges on correctly identifying events in all three text sources
  - Quick check: Verify that extracted events align with human understanding of key story elements

- **Precision/recall adaptation for summarization**: Standard information retrieval metrics need modification for summary evaluation contexts
  - Why needed: Summaries have different length constraints and content requirements than typical retrieval tasks
  - Quick check: Ensure adapted metrics properly handle partial matches and varying summary lengths

- **Cross-metric comparison methodology**: Understanding how different evaluation approaches capture different aspects of summary quality
  - Why needed: Event-overlap provides complementary insights to traditional metrics rather than replacing them
  - Quick check: Compare rankings across metrics to identify where they agree and disagree

## Architecture Onboarding

Component Map: Event extraction system -> Overlap computation -> Precision/recall adaptation -> Final evaluation scores

Critical Path: Source article → Event extraction → Reference summary → Event extraction → Generated summary → Event extraction → Overlap computation → Evaluation scores

Design Tradeoffs:
- **Extraction complexity vs. coverage**: More sophisticated event extraction captures nuanced relationships but may miss events or introduce noise
- **Granularity vs. interpretability**: Detailed argument role matching provides fine-grained assessment but may be harder to interpret than broader event type matching
- **Precision vs. recall balance**: Different weighting affects whether the metric prioritizes comprehensive coverage or focused relevance

Failure Signatures:
- Low event extraction accuracy manifests as artificially deflated scores across all models
- Domain-specific terminology issues cause systematic underestimation of model performance
- Complex nested events may be incorrectly parsed, leading to missed overlaps

First Experiments:
1. Test event extraction accuracy on a small manually annotated subset of the dataset
2. Compare event-overlap scores across multiple runs of the same model to assess stability
3. Analyze specific failure cases where high traditional metrics coexist with low event-overlap scores

## Open Questions the Paper Calls Out
None

## Limitations
- Event extraction accuracy directly impacts metric reliability, with complex sentence structures and ambiguous temporal relationships posing challenges
- Limited dataset size (NorSumm) constrains statistical significance and generalizability to other news domains or languages
- Focus on explicit event extraction may miss important aspects like narrative coherence, writing style, and implicit information conveyance
- Assumes event overlap correlates with summary quality, which may not hold uniformly across different summarization objectives

## Confidence

Event extraction reliability: Medium confidence
Cross-metric correlation findings: Medium confidence
LLM focus analysis: Low confidence

## Next Checks

1. Conduct ablation studies comparing event-overlap scores with varying event extraction thresholds and alternative extraction systems to quantify the impact of extraction accuracy on final metrics

2. Expand evaluation to additional language pairs and news domains using multilingual datasets to test the metric's generalizability and identify language-specific challenges

3. Perform human evaluation studies specifically designed to assess whether event-based metrics align with human judgments of summary quality across different dimensions (informativeness, coherence, relevance)