---
ver: rpa2
title: Online Social Support Detection in Spanish Social Media Texts
arxiv_id: '2502.09640'
source_url: https://arxiv.org/abs/2502.09640
tags:
- support
- social
- subtask
- f1-score
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel approach to detecting online social
  support in Spanish-language social media texts, specifically focusing on YouTube
  comments. A new annotated dataset comprising 3,189 comments was created and used
  to evaluate various machine learning models, including traditional ML, deep learning,
  transformers, and GPT-4o.
---

# Online Social Support Detection in Spanish Social Media Texts

## Quick Facts
- **arXiv ID:** 2502.09640
- **Source URL:** https://arxiv.org/abs/2502.09640
- **Reference count:** 6
- **Primary result:** Novel dataset of 3,189 Spanish YouTube comments for social support detection; GPT-4o achieves Macro F1 = 0.8531 on unbalanced binary task

## Executive Summary
This study introduces a novel approach to detecting online social support in Spanish-language social media texts, specifically focusing on YouTube comments. A new annotated dataset comprising 3,189 comments was created and used to evaluate various machine learning models, including traditional ML, deep learning, transformers, and GPT-4o. To address class imbalance, GPT-4o was employed to generate paraphrased comments for a balanced dataset. Results show that GPT-4o achieved the highest performance on the unbalanced dataset (Macro F1 = 0.8531) for binary support detection, while transformer models performed best on the balanced dataset for multiclass tasks. This research highlights the potential of automated social support detection and lays the groundwork for future studies in fostering supportive online environments.

## Method Summary
The study created a novel dataset of 3,189 Spanish YouTube comments annotated for social support across three hierarchical classification tasks: binary support detection (Support vs Non-Support), support target classification (Individual vs Group), and multiclass group classification (Nation, Other, LGBTQ, Black Community, Women, Religion). The research evaluated traditional ML models (TF-IDF + SVM, XGBoost), deep learning models (CNN/BiLSTM with GloVe/FastText), transformer models (RoBERTuito, XLM-RoBERTa, BERT), and GPT-4o. GPT-4o was used both for classification on the unbalanced dataset and for generating paraphrased comments to create a balanced dataset for the multiclass tasks. The primary metric was Macro F1-score, with 5-fold cross-validation for most experiments and an 80/20 train-test split for balanced dataset experiments.

## Key Results
- GPT-4o achieved the highest performance on the unbalanced dataset (Macro F1 = 0.8531) for binary support detection
- Transformer models (particularly pysentimiento/robertuito-sentiment-analysis) performed best on the balanced dataset for multiclass tasks
- GPT-4o-based paraphrasing effectively addressed class imbalance by generating synthetic variations of minority class samples
- Language-specific pre-training (RoBERTuito) provided superior feature extraction for Spanish social media text compared to generic multilingual models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large Language Models (specifically GPT-4o) appear to possess sufficient implicit knowledge to detect binary social support without fine-tuning, outperforming fine-tuned transformers on imbalanced data for coarse-grained tasks.
- **Mechanism:** The model leverages extensive pre-training to understand nuanced semantic definitions of "support" (emotional aid, defense, admiration) that are often under-represented in small, imbalanced training sets. By relying on emergent reasoning rather than statistical frequency in the training set, it mitigates the bias toward majority classes (Non-Support).
- **Core assumption:** The model's pre-training corpus included sufficient examples of Spanish social media interactions and supportive language structures to generalize zero-shot or few-shot.
- **Evidence anchors:**
  - [abstract] "GPT-4o achieved the highest performance on the unbalanced dataset... for binary support detection."
  - [section 6.4] "GPT4-o model performs exceptionally well... accuracy of 89.97%... macro F1-scores of 0.8531."
  - [corpus] Neighbors (e.g., *A Comparative Evaluation of Large Language Models...*) confirm a broader trend of LLMs performing strongly on sentiment/social tasks without specific fine-tuning, though this paper provides specific evidence for the Spanish support domain.
- **Break condition:** If the definition of "support" drifts significantly from general internet discourse to highly domain-specific jargon not present in pre-training data, this mechanism may fail, requiring fine-tuning.

### Mechanism 2
- **Claim:** Addressing class imbalance via LLM-based paraphrasing improves the performance of standard Transformers on fine-grained, multiclass tasks.
- **Mechanism:** Paraphrasing generates synthetic variations of minority class samples (e.g., LGBTQ, Religion), effectively increasing the decision boundary density for these classes. This prevents the gradient descent optimization in fine-tuning from being dominated by the majority class (Non-Support or Individual), allowing the Transformer to learn distinct features for under-represented groups.
- **Core assumption:** The LLM-generated paraphrases preserve the semantic label of the original comment while introducing sufficient lexical diversity to act as effective training augmentations rather than mere duplicates.
- **Evidence anchors:**
  - [abstract] "Transformer models performed best on the balanced dataset for multiclass tasks."
  - [section 6.6] "The balanced dataset approach achieved the highest F1-scores for SubTask 2 and SubTask 3."
  - [corpus] Weak/Missing direct corpus evidence for *paraphrasing* specifically as an augmentation strategy in neighbors, making this a specific contribution of this paper.
- **Break condition:** If the paraphrasing model hallucinates context or drifts semantically, it may introduce noise that degrades the classifier's precision on real-world data.

### Mechanism 3
- **Claim:** Language-specific pre-training (RoBERTuito) provides superior feature extraction for Spanish social media text compared to generic multilingual models (XLM-RoBERTa).
- **Mechanism:** Models pre-trained specifically on Spanish social media (tweets/comments) encode domain-specific slang, orthographic variations, and syntactic structures more effectively than multilingual models which must dilute their capacity across languages.
- **Core assumption:** The vocabulary and syntax of YouTube comments in the dataset share high similarity with the tweets used to pre-train RoBERTuito.
- **Evidence anchors:**
  - [section 6.3] "pysentimiento/robertuito-sentiment-analysis stands out with the highest performance... significantly outpacing the others [XLM-RoBERTa, BERT]."
  - [section 2] The paper notes the importance of analyzing Spanish to address "linguistic and cultural differences."
  - [corpus] Related work like *MuSeD* (Multimodal Spanish Dataset) also emphasizes the necessity of language-specific datasets, implicitly supporting the value of language-specific pre-training.
- **Break condition:** If the target text is formal Spanish (e.g., legal documents) rather than social media, the social-media-specific tokenization/pre-training might offer less advantage over standard multilingual models.

## Foundational Learning

- **Concept:** **Macro F1-Score vs. Weighted F1-Score**
  - **Why needed here:** The dataset is heavily imbalanced (2,510 Non-Support vs. 679 Support). A model could achieve high Accuracy or Weighted F1 by simply predicting "Non-Support" for everything. Macro F1 is required to measure if the model actually learns to identify the minority "Support" class.
  - **Quick check question:** If a model predicts "Non-Support" for every single comment in this dataset (3,189 total), what would the Accuracy be? (Answer: ~78.7% — highlighting why Accuracy is misleading).

- **Concept:** **Hierarchical Classification (Subtasks)**
  - **Why needed here:** The architecture is not flat; it relies on a cascade. Subtask 2 (Individual vs Group) is likely dependent on the output of Subtask 1 (Support vs Non-Support).
  - **Quick check question:** Does the paper feed the output of Subtask 1 as an input feature for Subtask 2, or does it filter the dataset? (Answer: It filters/comments are only classified for Subtask 2 if they are identified as Support in Subtask 1).

- **Concept:** **LLM Paraphrasing as Oversampling**
  - **Why needed here:** Traditional oversampling (SMOTE) works on feature vectors, but for text, it creates meaningless data. LLM paraphrasing allows for "semantic oversampling" where new sentences are created that maintain the label's meaning.
  - **Quick check question:** Why is generating a new sentence with an LLM generally better than duplicating existing minority sentences? (Answer: Duplication leads to overfitting/memorization; paraphrasing introduces lexical variety which improves generalization).

## Architecture Onboarding

- **Component map:**
  1. **Input:** Raw Spanish YouTube Comments.
  2. **Preprocessing:** Cleaning, Tokenization.
  3. **Augmentation (Conditional):** If training Multiclass (Task 2/3), route minority classes to **GPT-4o Paraphraser** to generate synthetic samples.
  4. **Embeddings:** Convert text to vectors.
     - *Path A (Traditional):* TF-IDF
     - *Path B (Deep Learning):* GloVe/FastText
     - *Path C (Transformer/LLM):* Contextual embeddings (RoBERTuito, GPT-4o internal).
  5. **Classifier:**
     - *Binary Task:* GPT-4o (Zero-shot/Few-shot) OR RoBERTuito (Fine-tuned).
     - *Multiclass Task:* RoBERTuito (Fine-tuned on Balanced Data).

- **Critical path:** The **Annotation → Paraphrasing → Fine-tuning** loop. The quality of the system depends heavily on the GPT-4o paraphrases maintaining the "Support" label integrity for minority classes (e.g., LGBTQ support). If the paraphraser drifts, the fine-tuned Transformer learns garbage.

- **Design tradeoffs:**
  - **GPT-4o (Binary)**: Highest performance (0.85 F1) but highest latency/cost. Best for offline batch analysis or low-volume high-stakes detection.
  - **RoBERTuito (Multiclass)**: Good performance (0.89 F1 Balanced) with lower latency. Best for real-time filtering or moderation queues.
  - **Imbalanced vs. Balanced Training:** Balanced training helps the model "see" minority groups (Task 2/3), but arguably reduces raw accuracy on the majority class.

- **Failure signatures:**
  - **The "Majority Bias":** High Accuracy (>85%) but Macro F1 < 0.50. The model is ignoring the "Support" class.
  - **Semantic Drift:** High recall but low precision in multiclass tasks. The model might start confusing "National" support with "Religious" support if paraphrases conflated the two.

- **First 3 experiments:**
  1. **Sanity Check:** Train a Logistic Regression model on the raw (imbalanced) data. If Macro F1 is near 0 for the Support class, confirm the dataset is the problem.
  2. **Binary Baseline:** Run GPT-4o on a held-out test set (unbalanced) to establish the upper bound for Task 1.
  3. **Augmentation Ablation:** Train the RoBERTuito model on Task 3 *with* and *without* the GPT-4o paraphrased data. Compare Macro F1 to quantify the specific lift provided by the synthetic data generation.

## Open Questions the Paper Calls Out

- **Multimodal approaches:** The authors suggest that exploring multimodal approaches incorporating text, images, and video content could significantly improve social support detection beyond text-only methods.

- **Cross-platform generalizability:** The current dataset is limited to YouTube comments, and the authors recommend evaluating model performance across diverse social media platforms like Twitter, Facebook, and Reddit to assess generalizability.

- **Cultural and linguistic variations:** The study acknowledges that cultural and linguistic variations in social support expressions across Spanish-speaking regions were not extensively analyzed and should be explored in future work.

## Limitations

- **Data Representativeness:** The dataset comprises 3,189 YouTube comments with significant class imbalance (2,510 Non-Support vs. 679 Support), which may limit generalizability to other social media platforms or contexts.

- **Generalizability Across Languages:** While the study focuses on Spanish social media, the identified mechanisms may not translate directly to other languages with different syntactic structures or cultural contexts around online support.

- **Temporal Stability:** The study does not address how social support detection performance might change over time as language evolves, new slang emerges, or platform norms shift.

## Confidence

- **High Confidence:** The observation that GPT-4o achieves superior performance on binary classification tasks for imbalanced datasets is well-supported by the reported results (Macro F1 = 0.8531) and aligns with broader literature on LLM capabilities for zero-shot classification.

- **Medium Confidence:** The claim that language-specific pre-training (RoBERTuito) outperforms multilingual models for Spanish social media text is supported by comparative results but could vary depending on the specific characteristics of the target corpus.

- **Low Confidence:** The effectiveness of LLM-based paraphrasing for generating balanced training data, while showing promising results in this study, lacks extensive validation across different domains and may be sensitive to the quality of generated paraphrases.

## Next Checks

1. **Cross-platform validation:** Apply the trained models to social media data from other platforms (Twitter, Reddit, Facebook) to assess whether the performance patterns hold across different online environments and user demographics.

2. **Temporal robustness test:** Re-evaluate model performance on a held-out test set collected 6-12 months after the original dataset to determine if language drift or evolving discourse patterns affect detection accuracy.

3. **Paraphrase quality audit:** Manually review a stratified sample of GPT-4o-generated paraphrases to quantify semantic drift rates and assess whether label-preserving properties hold across different minority classes, particularly for sensitive categories like LGBTQ and religious support.