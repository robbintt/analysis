---
ver: rpa2
title: 'LCFed: An Efficient Clustered Federated Learning Framework for Heterogeneous
  Data'
arxiv_id: '2501.01850'
source_url: https://arxiv.org/abs/2501.01850
tags:
- lcfed
- learning
- chen
- federated
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes LCFed, an efficient clustered federated learning\
  \ (CFL) framework that addresses two key challenges: (1) the lack of global knowledge\
  \ integration in existing CFL methods, and (2) high computational costs of clustering\
  \ in large-scale deployments. LCFed employs model partitioning, splitting each device's\
  \ model into an embedding sub-model (\u03D5) and a decision sub-model (h)."
---

# LCFed: An Efficient Clustered Federated Learning Framework for Heterogeneous Data

## Quick Facts
- **arXiv ID:** 2501.01850
- **Source URL:** https://arxiv.org/abs/2501.01850
- **Reference count:** 40
- **Primary result:** Achieves up to 98.53% accuracy on MNIST (α=0.1, n=3) and 64.53% on CIFAR-100 (α=0.1, n=10) while reducing clustering computation by over 90,000×

## Executive Summary
This paper introduces LCFed, a novel clustered federated learning framework that addresses two key challenges in heterogeneous data scenarios: the lack of global knowledge integration in existing CFL methods and the high computational costs of clustering in large-scale deployments. LCFed achieves this through model partitioning and low-rank similarity computation, splitting each device's model into an embedding sub-model (φ) and a decision sub-model (h). The embedding captures global knowledge through a shared global embedding across all devices, while the decision captures cluster-specific knowledge. Extensive experiments demonstrate LCFed outperforms state-of-the-art CFL methods across multiple datasets while significantly reducing computational overhead.

## Method Summary
LCFed partitions each device's model into an embedding sub-model (φ) and a decision sub-model (h), enabling simultaneous global and cluster-specific knowledge integration. During training, devices minimize a three-term loss function combining supervised loss, cluster regularization, and global regularization. The server maintains a global embedding Φ (aggregated from all devices' φ) and cluster centers {Ωk} (aggregated within clusters). For clustering efficiency, LCFed uses a low-rank model similarity measurement method that computes similarities in a compressed model space, reducing computational overhead by over 90,000× compared to traditional methods. The framework operates through a client-server architecture where devices send both full models (for aggregation) and low-rank representations (for clustering) to the server each round.

## Key Results
- Achieves up to 98.53% accuracy on MNIST (α=0.1, n=3) and 64.53% on CIFAR-100 (α=0.1, n=10)
- Reduces clustering computation costs by over 90,000× compared to traditional similarity measures
- Outperforms state-of-the-art CFL methods including FedAvg, FedPer, LG-FedAvg, IFCA, and Per-FedAvg
- Demonstrates that both global and intra-cluster knowledge are essential for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Splitting models into embedding and decision sub-components enables simultaneous global and cluster-specific knowledge integration.
- Mechanism: Each device model ω is partitioned into a shallow embedding sub-model (φ) and a deep decision sub-model (h). The embedding captures foundational representations less affected by data heterogeneity, making it suitable for global sharing. The decision sub-model captures cluster-specific patterns. The server aggregates φ across all devices into a global embedding Φ, while aggregating h only within clusters into cluster centers {Ωk}.
- Core assumption: Low-dimensional latent features learned by shallow layers are more transferable across heterogeneous distributions than decision layers.
- Evidence anchors:
  - [abstract] "LCFed employs model partitioning... The embedding captures global knowledge through a shared global embedding across all devices, while the decision captures cluster-specific knowledge."
  - [section III-A] "The embedding φ processes low-dimensional features of input data, providing a foundational representation... This representation is less affected by data heterogeneity, making it well-suited for collaborative learning across the global scope."
  - [corpus] Related work (FedProto, FedPer) similarly exploits representation sharing, but LCFed uniquely combines this with cluster-level aggregation.
- Break condition: If the split point is chosen too deep, the embedding may capture distribution-specific features, reducing global transferability.

### Mechanism 2
- Claim: A three-term loss function balances local, cluster-level, and global knowledge during training.
- Mechanism: Local updates minimize L = Lsup + (μ/2)||ωi - Ωk*||² + (λ/2)||φi - Φ||². Lsup is the supervised loss on local data, Lcluster pulls the full model toward the cluster center, and Lglobal pulls the embedding toward the global embedding. Hyperparameters μ and λ control the strength of cluster and global regularization respectively.
- Core assumption: Optimal performance requires balancing all three knowledge sources; over-reliance on any single source degrades performance.
- Evidence anchors:
  - [section III-A] Eq. (2) shows the full objective function with all three terms.
  - [section IV-B] "Under both heterogeneous settings, optimal performance is achieved through the combined influence of global and intra-cluster knowledge. This validates the limitations of other CFL methods that confine knowledge sharing within clusters."
  - [corpus] Corpus lacks direct validation of the three-term loss design in other CFL methods.
- Break condition: If μ = 0 under pathological distributions, performance degrades significantly (shown in ablation, Fig. 2), indicating high intra-cluster similarity requires cluster regularization.

### Mechanism 3
- Claim: Low-rank model projection reduces clustering computation by ~90,000× while maintaining assignment quality.
- Mechanism: At initialization, a subset of devices is sampled to construct Wd ∈ R|Sd|×dim(ω). PCA (or similar) yields a projection matrix M ∈ Rdim(ω)×D where D ≪ dim(ω). Each round, devices compute M·ωi locally and send this low-rank representation alongside their full model. The server computes similarity as cosine similarity in the compressed space: Simi(ωi, Ωk) = (M·ωi · M·Ωk) / (||M·ωi||₂ ||M·Ωk||₂).
- Core assumption: The principal components of model parameter space capture sufficient information for accurate similarity measurement.
- Evidence anchors:
  - [section III-B] "By using low-rank models, LCFed reduces the similarity computation cost by a factor of around D/dim(ω)."
  - [section IV-C] "LCFed reduces the server's computational overhead by over 9.048 × 10⁴ times, lowering the parameter dimensionality of the LeNet-5 model from more than 4,800,000 to D = 50."
  - [corpus] Corpus does not report comparable low-rank clustering efficiency gains in other CFL frameworks.
- Break condition: If the sampled devices Sd are unrepresentative, the projection M may not capture relevant variance, degrading clustering accuracy.

## Foundational Learning

- Concept: **Clustered Federated Learning (CFL)**
  - Why needed here: CFL groups devices by data distribution similarity, enabling personalized models per cluster. LCFed extends CFL by adding global knowledge sharing.
  - Quick check question: Can you explain why standard FL (single global model) fails under non-IID data?

- Concept: **Non-IID Data Heterogeneity**
  - Why needed here: The core problem LCFed addresses. Understanding Dirichlet distribution (α parameter) and pathological distribution (n labels per device) is essential for interpreting results.
  - Quick check question: How does a smaller α value in Dirichlet distribution affect data heterogeneity?

- Concept: **Model Partitioning / Split Learning**
  - Why needed here: LCFed's first innovation relies on splitting models into embedding and decision components. Understanding which layers capture transferable vs. task-specific features is critical for choosing the split point.
  - Quick check question: In a CNN, would you expect earlier convolutional layers or later fully-connected layers to be more transferable across different data distributions?

## Architecture Onboarding

- Component map:
  - **Server**: Maintains global embedding Φ, cluster centers {Ωk}, assignment matrix R, and projection matrix M. Performs aggregation and similarity computation.
  - **Device i**: Holds local model ωi = (φi, hi), local data Di, and projection matrix M. Performs local training and computes M·ωi.
  - **Communication channel**: Transmits full models ωi (for aggregation) and low-rank models M·ωi (for clustering) from devices to server; transmits Φ and Ωk* from server to devices.

- Critical path:
  1. **Initialization**: Server samples devices to compute M via PCA on concatenated model weights.
  2. **Per-round flow**: Server broadcasts Φ and relevant Ωk* → Devices perform LocalUpdate with three-term loss → Devices return ωi and M·ωi → Server updates assignments R using low-rank similarity → Server aggregates new Φ and {Ωk}.

- Design tradeoffs:
  - **Split point depth**: Earlier split = more global sharing but less cluster-specific adaptation. Later split = opposite.
  - **Low-rank dimension D**: Smaller D = faster clustering but risk of information loss. Paper uses D = 50 for LeNet-5.
  - **Regularization weights (μ, λ)**: Must be tuned per dataset. Ablation shows both terms are necessary for optimal performance.

- Failure signatures:
  - Clustering fails to converge: Check if M was computed from representative devices.
  - Accuracy plateaus below baseline: Verify both μ and λ are non-zero (ablation shows either being zero degrades performance).
  - Communication cost exceeds budget: Low-rank models add overhead; ensure D is appropriately sized.

- First 3 experiments:
  1. **Reproduce Table I on CIFAR-10**: Train LCFed with α = 0.1, K = 10, m = 100. Compare against FedAvg and IFCA. Verify accuracy improvement.
  2. **Ablation on split point**: Move the split earlier/later in LeNet-5. Measure impact on convergence speed and final accuracy.
  3. **Scalability stress test**: Increase m to 1000 devices. Measure server clustering time per round with D = 50 vs. full-parameter similarity. Confirm >10⁴× speedup.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the low-rank projection matrix M, computed via offline sampling at initialization, remain effective for clustering as the model parameters evolve throughout the training process?
- **Basis in paper:** [inferred] Section III-B states that the matrix M is computed "at the beginning of LCFed training" and used for all subsequent similarity updates.
- **Why unresolved:** The sub-space of model parameters likely shifts as training converges. A static projection matrix may fail to capture relevant variance in later rounds, potentially degrading clustering accuracy.
- **What evidence would resolve it:** An analysis comparing the clustering quality and final accuracy when using a static M versus a matrix updated periodically during training.

### Open Question 2
- **Question:** How should the model partitioning point between the embedding (φ) and decision (h) sub-models be optimally determined for complex, heterogeneous neural network architectures?
- **Basis in paper:** [inferred] Section IV-A manually splits the LeNet-5 model at the final fully connected layer without providing a criterion for this decision.
- **Why unresolved:** The definition of "global" vs. "cluster-specific" knowledge is architecture-dependent. An arbitrary split might place cluster-relevant features in the global embedding or vice-versa, limiting generalization to deeper networks like ResNets.
- **What evidence would resolve it:** Ablation studies on deeper models showing the impact of different split depths on test accuracy and convergence speed.

### Open Question 3
- **Question:** Can the regularization weights μ (cluster) and λ (global) be dynamically adjusted to optimize the balance between global and intra-cluster knowledge as training progresses?
- **Basis in paper:** [inferred] Figure 2 demonstrates that optimal performance relies on specific fixed values for μ and λ, implying a sensitivity to the trade-off between global and local knowledge.
- **Why unresolved:** The utility of global vs. cluster knowledge likely changes over time (e.g., global features may be learned first). Fixed weights may constrain the model's ability to fine-tune cluster-specific features in later epochs.
- **What evidence would resolve it:** Experiments implementing an adaptive scheduling mechanism for μ and λ compared against the static best-performance baseline.

## Limitations

- The paper lacks explicit specification of regularization parameters μ and λ for main experiments, requiring empirical tuning
- The initial PCA computation for low-rank projection M depends on unspecified sampling strategy and dataset size
- The clustering stability under dynamic membership (devices joining/leaving) is not evaluated, which is critical for real-world deployments

## Confidence

- **High confidence**: Model partitioning mechanism (splitting into embedding/decision) and its theoretical justification. The three-term loss function design and its ablation validation.
- **Medium confidence**: Low-rank similarity computation claims (~90,000× speedup). While methodology is sound, the exact computational savings depend on implementation details not fully specified.
- **Low confidence**: Generalizability to larger, deeper architectures beyond LeNet-5. The split point choice and low-rank dimension D may not scale optimally.

## Next Checks

1. **Parameter sensitivity analysis**: Systematically vary μ and λ across multiple orders of magnitude to map performance landscape and identify optimal values.
2. **Cross-architecture validation**: Apply LCFed to ResNet or Vision Transformer architectures. Measure whether the embedding/decision split point and D=50 low-rank dimension remain effective.
3. **Dynamic cluster stability**: Implement device churn simulation where 10-20% of devices randomly join/leave each round. Measure clustering reassignment frequency and impact on final accuracy.