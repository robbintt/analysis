---
ver: rpa2
title: 'Stable Thompson Sampling: Valid Inference via Variance Inflation'
arxiv_id: '2505.23260'
source_url: https://arxiv.org/abs/2505.23260
tags:
- thompson
- sampling
- bandit
- algorithm
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of constructing valid confidence
  intervals for model parameters when data is collected via Thompson Sampling in multi-armed
  bandit problems. The authors propose Stable Thompson Sampling, a variant that inflates
  the posterior variance by a logarithmic factor, enabling asymptotically normal estimates
  of arm means despite the non-i.i.d.
---

# Stable Thompson Sampling: Valid Inference via Variance Inflation

## Quick Facts
- arXiv ID: 2505.23260
- Source URL: https://arxiv.org/abs/2505.23260
- Authors: Budhaditya Halder; Shubhayan Pan; Koulik Khamaru
- Reference count: 40
- Primary result: Valid confidence intervals for Thompson Sampling via logarithmic variance inflation with only logarithmic regret cost

## Executive Summary
This paper addresses the challenge of constructing valid confidence intervals for model parameters when data is collected via Thompson Sampling in multi-armed bandit problems. The authors propose Stable Thompson Sampling, a variant that inflates the posterior variance by a logarithmic factor, enabling asymptotically normal estimates of arm means despite the non-i.i.d. nature of adaptive sampling. The method comes at a modest cost: regret increases only logarithmically compared to standard Thompson Sampling. The key theoretical result shows that this modification satisfies the Lai-Wei stability condition, allowing valid inference. Empirical simulations confirm that the stabilized algorithm yields valid coverage across confidence levels while maintaining efficient exploration.

## Method Summary
The paper proposes Stable Thompson Sampling, which modifies standard Thompson Sampling by inflating the posterior variance by a factor γ_T/n_{a,t-1} instead of 1/n_{a,t-1}. The algorithm samples θ_{a,t} ~ N(μ̂_{a,t-1}, γ_T/n_{a,t-1}) for each arm and selects the arm with maximum sampled value. The variance inflation factor γ_T must satisfy two asymptotic conditions: γ_T/log log T → ∞ and √log T/γ_T → ∞. This modification ensures that the arm pull ratios converge to deterministic limits (Lai-Wei stability), enabling asymptotically normal estimates of arm means. The regret bound increases by only a logarithmic factor compared to standard Thompson Sampling.

## Key Results
- Stable Thompson Sampling satisfies the Lai-Wei stability condition, ensuring valid asymptotic inference
- Coverage probability of confidence intervals matches nominal level under the stabilized algorithm
- Regret increases only logarithmically compared to standard Thompson Sampling
- The method works for both equal and unequal arm means in Gaussian bandits

## Why This Works (Mechanism)
Standard Thompson Sampling produces non-i.i.d. samples due to adaptive arm selection, violating the conditions needed for standard confidence interval construction. By inflating the posterior variance with factor γ_T, the algorithm artificially increases exploration, ensuring that arm pull ratios converge to deterministic limits (stability). This stability condition allows the application of central limit theorems for dependent sequences, yielding asymptotically normal estimates of arm means. The variance inflation is carefully calibrated to be large enough for stability but small enough to avoid excessive regret.

## Foundational Learning
- **Thompson Sampling**: Bayesian bandit algorithm that samples from posterior to balance exploration/exploitation. Needed for context of adaptive sampling breaking inference.
- **Lai-Wei stability**: Condition requiring arm pull ratios to converge to deterministic limits. Quick check: verify n_{a,T}/T concentrates as T→∞.
- **Variance inflation**: Technique of artificially increasing posterior variance. Quick check: ensure γ_T satisfies both asymptotic conditions.
- **Regret analysis**: Measures cumulative loss from not always playing optimal arm. Quick check: compare regret bounds with and without stabilization.

## Architecture Onboarding

**Component Map**: Prior → Posterior Update → Variance Inflation (γ_T) → Arm Selection → Reward Observation → Next Iteration

**Critical Path**: For valid inference, the variance inflation factor γ_T must satisfy asymptotic conditions; failure here breaks stability and invalidates confidence intervals.

**Design Tradeoffs**: Larger γ_T improves stability but increases regret; smaller γ_T maintains exploration efficiency but may fail to achieve stability. The logarithmic factor represents an optimal balance.

**Failure Signatures**: Under-coverage in confidence intervals indicates insufficient variance inflation or violation of stability conditions; non-converging arm pull ratios indicate γ_T too small.

**3 First Experiments**:
1. Implement standard Thompson Sampling and verify non-normal distribution of sample means
2. Implement Stable Thompson Sampling with γ_T = 4·(log T)^0.4 and verify convergence of arm pull ratios
3. Compare coverage probabilities of confidence intervals between standard and stable variants

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical validation limited to two-armed Gaussian bandits, generalizability to K>2 or non-Gaussian settings unknown
- Choice of γ_T = 4·(log T)^0.4 appears heuristic without sensitivity analysis
- Simulations use T = 10⁴ trials, potentially insufficient for full asymptotic characterization
- No analysis of computational overhead or robustness to model misspecification

## Confidence
- Regret bound (Corollary 4.1): High confidence - rigorous theoretical proof provided
- Validity of inference: Medium confidence - supported by theory and simulations but limited empirical scope
- Generalizability to non-Gaussian or high-dimensional settings: Low confidence - not empirically tested

## Next Checks
1. Test Stable Thompson Sampling across a grid of γ_T values to determine sensitivity and identify near-optimal choices
2. Extend simulations to K > 2 arms and non-Gaussian reward distributions to assess generalizability
3. Implement a runtime benchmark comparing computational costs of Stable vs. standard Thompson Sampling at scale