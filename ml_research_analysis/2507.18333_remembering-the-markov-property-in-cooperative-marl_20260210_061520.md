---
ver: rpa2
title: Remembering the Markov Property in Cooperative MARL
arxiv_id: '2507.18333'
source_url: https://arxiv.org/abs/2507.18333
tags:
- agents
- learning
- marl
- policies
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether deep MARL policies actually recover
  a Markovian signal, or if they learn brittle conventions instead. Through experiments
  in a custom "Prediction Game" and analysis of popular benchmarks (Hanabi, MaBrax,
  SMAX), the authors show that co-adapting agents often converge on non-robust conventions
  that bypass grounded observations and memory.
---

# Remembering the Markov Property in Cooperative MARL

## Quick Facts
- arXiv ID: 2507.18333
- Source URL: https://arxiv.org/abs/2507.18333
- Reference count: 32
- Primary result: Co-adapting agents often learn brittle conventions that bypass grounded observations and memory, challenging whether deep MARL recovers Markovian signals

## Executive Summary
This paper investigates whether deep MARL policies actually recover a Markovian signal or if they learn brittle conventions instead. Through experiments in a custom "Prediction Game" and analysis of popular benchmarks (Hanabi, MaBrax, SMAX), the authors show that co-adapting agents often converge on non-robust conventions that bypass grounded observations and memory. Blind agents can achieve surprisingly high performance, and policies that work well in training fail when partnered with non-adaptive agents. However, when the environment design requires it, the same models can learn grounded, memory-based policies. The paper argues that modern MARL benchmarks often fail to adequately test Dec-POMDP assumptions, and advocates for new environments that require behaviors grounded in observations and memory-based reasoning about other agents.

## Method Summary
The authors introduce a custom "Prediction Game" environment where one agent (Alice) observes a sequence of binary vectors and must communicate a target pattern to another agent (Bob), who must predict the next bit in the sequence. This environment allows controlled testing of whether agents learn grounded policies versus conventions. The paper also analyzes three popular MARL benchmarks: Hanabi, MaBrax, and SMAX, examining whether agents in these environments develop grounded, memory-based policies or non-robust conventions. The methodology includes testing blind agents (those with observation access restricted) and evaluating policy robustness by testing trained policies with non-adaptive partners.

## Key Results
- Blind agents can achieve surprisingly high performance in cooperative MARL tasks, suggesting agents may bypass grounded observations
- Policies that work well in training fail when partnered with non-adaptive agents, indicating learned conventions rather than robust, grounded reasoning
- When environment design requires it, the same models can learn grounded, memory-based policies that satisfy the Markov property

## Why This Works (Mechanism)
The paper demonstrates that when agents co-adapt without sufficient environmental pressure, they can develop conventions that work well within their training cohort but fail to generalize. These conventions often bypass the need for memory and observation grounding, achieving performance through learned signaling patterns rather than genuine state estimation. The mechanism involves agents developing a shared "language" of actions that signals information directly, rather than inferring states from observations. This is particularly problematic in current benchmarks where the environment may not sufficiently challenge agents to maintain the Markov property, allowing them to succeed through co-adapted conventions instead.

## Foundational Learning

**Markov Property**: The principle that future states depend only on the current state, not the history of past states. Why needed: Essential for tractable decision-making in MARL; violations lead to non-generalizable policies. Quick check: Can an agent's optimal action be determined from its current observation alone?

**Dec-POMDP (Decentralized Partially Observable Markov Decision Process)**: Framework for multi-agent coordination under partial observability. Why needed: Provides theoretical foundation for understanding when and how agents should coordinate. Quick check: Does the environment require agents to reason about others' beliefs and maintain memory?

**Co-adaptation**: When agents evolve policies that work well together but may not generalize to new partners. Why needed: Explains how agents can achieve high training performance through conventions rather than robust reasoning. Quick check: Does performance degrade significantly when testing with non-trained partners?

## Architecture Onboarding

**Component Map**: Environment -> Agent(s) with observation space -> Policy network -> Action selection -> Reward signal -> Policy update

**Critical Path**: Observation → Policy network → Action → Environment transition → Reward → Policy update. The critical insight is that agents may shortcut this path by developing conventions that bypass genuine state estimation.

**Design Tradeoffs**: The paper highlights the tradeoff between environmental complexity (which should force grounded reasoning) and trainability. Too simple environments allow conventions; too complex may prevent learning altogether.

**Failure Signatures**: High in-training performance but low out-of-distribution performance; blind agents achieving competitive scores; significant performance drops when paired with non-trained agents.

**3 First Experiments**:
1. Train agents in the Prediction Game and test with blind partners to measure convention dependence
2. Analyze existing benchmarks by training agents with and without memory components to test necessity
3. Modify popular benchmarks to require explicit memory use and observe policy changes

## Open Questions the Paper Calls Out

The paper identifies several open questions: How can we design MARL benchmarks that reliably require grounded, memory-based policies? What quantitative metrics can detect convention dependence in trained policies? How do different MARL algorithms compare in their tendency to learn conventions versus grounded policies? What is the relationship between environmental stochasticity and the emergence of conventions?

## Limitations

- The custom "Prediction Game" experiments, while clear, may not fully capture the complexity of real-world MARL problems
- The methodology for detecting conventions in existing benchmarks relies on indirect analysis rather than direct measurement
- The distinction between "blind" and "non-blind" agents depends on specific architectural choices that may not generalize across frameworks

## Confidence

**High**: Agents can achieve high performance through non-robust conventions that bypass the Markov property (supported by clear Prediction Game experiments)
**Medium**: Current MARL benchmarks may fail to adequately test Dec-POMDP assumptions (supported by suggestive but indirect evidence)
**Low**: Proposed solutions for designing better benchmarks (largely conceptual at this stage)

## Next Checks

1. Replicate the Prediction Game experiments across multiple MARL algorithms (MADDPG, QMIX, MAPPO) to verify the phenomenon is not algorithm-specific
2. Develop quantitative metrics for measuring "convention dependence" that can be applied to existing benchmarks to strengthen claims about Hanabi and other environments
3. Test whether adding explicit memory requirements or observation grounding constraints to existing benchmark environments forces agents to learn more robust, Markov-compliant policies