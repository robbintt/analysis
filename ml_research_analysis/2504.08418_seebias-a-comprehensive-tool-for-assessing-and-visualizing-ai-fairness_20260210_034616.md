---
ver: rpa2
title: 'seeBias: A Comprehensive Tool for Assessing and Visualizing AI Fairness'
arxiv_id: '2504.08418'
source_url: https://arxiv.org/abs/2504.08418
tags:
- fairness
- seebias
- performance
- white
- black
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: seeBias is an R package for comprehensive fairness evaluation of
  AI prediction models, addressing the limitation of existing tools that focus primarily
  on classification performance disparities while overlooking other critical aspects
  like calibration and ranking consistency. The package implements conventional group
  fairness metrics alongside visual assessments of calibration curves, prediction
  distributions, and rank-based fairness, with customizable visualizations for transparent
  reporting.
---

# seeBias: A Comprehensive Tool for Assessing and Visualizing AI Fairness

## Quick Facts
- **arXiv ID:** 2504.08418
- **Source URL:** https://arxiv.org/abs/2504.08418
- **Reference count:** 0
- **Primary result:** seeBias is an R package for comprehensive fairness evaluation of AI prediction models, addressing the limitation of existing tools that focus primarily on classification performance disparities while overlooking other critical aspects like calibration and ranking consistency.

## Executive Summary
seeBias is an R package designed to provide comprehensive fairness evaluation of AI prediction models by integrating classification, calibration, and ranking assessments. The tool addresses a critical gap in existing fairness toolkits that focus primarily on classification performance disparities while overlooking other important aspects like calibration and ranking consistency. Using two case studies in criminal justice and healthcare, seeBias revealed systematic biases not captured by conventional metrics, such as overestimation of risk for specific groups and disparities in positive predictive values across racial groups. The package requires minimal input parameters, making it accessible to practitioners with varying programming experience, and is available on GitHub with a Python version under development.

## Method Summary
The seeBias package implements conventional group fairness metrics alongside visual assessments of calibration curves, prediction distributions, and rank-based fairness, with customizable visualizations for transparent reporting. It requires three key inputs: sensitive variables, observed labels, and model predictions (probabilities or scores). The tool processes these inputs through evaluation functions that create a seeBias object containing computed metrics, then generates fairness tables and ggplot2 visualizations for comprehensive assessment. Case studies demonstrated the tool using COMPAS recidivism data (5,278 individuals) and ROSC prediction from cardiac arrest registry data (58,648 patients), revealing biases missed by conventional fairness metrics.

## Key Results
- Revealed systematic biases in risk prediction models not captured by conventional fairness metrics, including calibration disparities and ranking inconsistencies
- Demonstrated that a model can appear fair by classification metrics while systematically overestimating risk for specific groups
- Showed practical significance of disparities through "Number Needed" metrics that translate statistical differences into actionable counts for domain experts

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Integrating classification, calibration, and ranking assessments reveals disparities that isolated classification metrics miss.
- **Mechanism:** Standard toolkits often rely on binary classification parity (e.g., Equal Opportunity). By adding calibration (alignment of predicted probability with observed rates) and ranking assessments (consistency of risk scores across groups), seeBias creates a multi-dimensional error profile. This detects cases where a model might appear fair by error rates (TPR/FPR) but systematically overestimates risk (miscalibration) or misranks individuals within a subgroup.
- **Core assumption:** Fairness is multi-faceted; a model can satisfy classification fairness while failing calibration or ranking fairness.
- **Evidence anchors:**
  - [abstract]: "...uncovered disparities in calibration and risk ranking that conventional fairness metrics missed..."
  - [section]: "Focusing predominantly on fairness metrics also limits current toolkits... calibration is not easily captured by simple metrics..."
  - [corpus]: Related work 'Fairness in Survival Analysis' supports the need for specific fairness approaches beyond standard classification in complex tasks.
- **Break condition:** If the underlying data distributions across groups are identical, or if the model is perfectly calibrated, the additional assessments provide diminishing returns.

### Mechanism 2
- **Claim:** Translating predictive values into "Number Needed" metrics allows domain experts to assess the practical significance of bias.
- **Mechanism:** Statistical parity thresholds (like the 80% rule) are often opaque to non-technical stakeholders. seeBias converts Positive Predictive Value (PPV) into "Number Needed for True Positive" (NNTP)â€”the count of individuals screened to find one true case. This grounds abstract ratios in operational costs, helping users decide if a statistical disparity constitutes a practical harm.
- **Core assumption:** Domain experts interpret "counts of people" more accurately than "probability ratios," and this interpretation drives better decision-making.
- **Evidence anchors:**
  - [section]: "...we translate PPV and NPV to the number of individuals needed... aiding in more intuitive interpretations of model fairness."
  - [section]: "By visualizing NNTP and NNTN across groups... enable straightforward comparisons..."
  - [corpus]: 'FairVizARD' emphasizes visualization systems for assessing multi-party fairness, reinforcing the value of visual interpretation.
- **Break condition:** If the cost of false positives/negatives is not uniform across groups or is irrelevant to the specific deployment context.

### Mechanism 3
- **Claim:** Visualizing actual performance metrics alongside fairness thresholds highlights specific model inadequacies for targeted mitigation.
- **Mechanism:** Existing tools often report fairness as a ratio or difference (fairness metric) relative to a reference group. seeBias plots the *actual* performance (e.g., TPR = 0.5) against a reference band. This distinguishes between "fairness" achieved by both groups performing poorly vs. one group excelling and the other failing, preventing the masking of low utility in the pursuit of parity.
- **Core assumption:** Users need to diagnose *why* unfairness exists (e.g., low sensitivity in Group A) rather than just detecting its presence.
- **Evidence anchors:**
  - [section]: "...seeBias instead visualizes actual performance metrics to support both performance and fairness evaluations."
  - [section]: "...highlights areas for improvements in bias mitigation, such as the inadequate TPR for White individuals..."
  - [corpus]: 'BiasGuard' discusses guardrailing fairness in production, implying the need for mechanisms that identify specific failure modes for mitigation.
- **Break condition:** If the user strictly requires only a pass/fail compliance check and does not intend to retrain or adjust the model.

## Foundational Learning

- **Concept: Calibration-in-the-large vs. Calibration Slope**
  - **Why needed here:** The paper distinguishes between these two levels of calibration. Understanding this is required to interpret the plots showing "observed vs. predicted" proportions (Large) versus the slope of the calibration curve (Refinement).
  - **Quick check question:** If a model predicts 20% risk for a group, and the actual event rate is 40%, is this a failure of calibration-in-the-large or calibration slope?

- **Concept: The Four-Fifths Rule (80% Rule)**
  - **Why needed here:** seeBias uses this heuristic (0.8 to 1.25 range) as the green shading in its main visualization to flag potential disparities.
  - **Quick check question:** If Group A has a selection rate of 50% and Group B has a rate of 39%, does this violate the 80% rule?

- **Concept: Platt Scaling**
  - **Why needed here:** Case Study 2 deals with risk scores rather than probabilities. The tool applies Platt scaling (logistic regression on scores) to convert these to probabilities for calibration analysis.
  - **Quick check question:** Why is a raw integer score (e.g., 85) insufficient for plotting a calibration curve without this transformation?

## Architecture Onboarding

- **Component map:** Input Functions -> Processing Object -> Output Generators
- **Critical path:**
  1. Format data (Sensitive Variables + Outcomes + Predictions)
  2. Call `evaluate_prediction_*` to instantiate the object
  3. Call `summary()` for tabular compliance checks
  4. Call `plot()` to diagnose specific failure modes (Calibration/Ranking)

- **Design tradeoffs:**
  - **Assessment vs. Mitigation:** The tool is lightweight and focuses *only* on assessment. It does not include algorithms to re-weight data or adjust thresholds (unlike AIF360 or Fairlearn).
  - **Input Flexibility:** It accepts raw scores and probabilities but does not require a specific model object class, trading integration depth for broader applicability.

- **Failure signatures:**
  - **Small Subgroups:** As seen in Case Study 2 (Asian/Hispanic groups), small sample sizes result in wide Confidence Intervals (CIs), potentially masking significant miscalibration or making the "fairness" assessment statistically uncertain.
  - **Over-plotting:** With >7 groups (the current optimization limit), visualizations may become cluttered and unreadable.

- **First 3 experiments:**
  1. **Baseline Reproduction:** Run `evaluate_prediction_prob` on the provided COMPAS dataset to replicate the "inadequate TPR" finding for White females.
  2. **Threshold Sensitivity:** Vary the prediction threshold in `evaluate_prediction_score` to observe how the NNTP (Number Needed) visualization changes. Does a lower threshold make the model look "fairer" by the 80% rule?
  3. **Calibration Check:** Apply `plot()` to a model outputting raw scores *without* Platt scaling (if manually bypassed) to verify the visualization's robustness to non-probabilistic inputs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Should AI models be adjusted to capture observed demographic differences in outcome rates, or do such disparities reflect underlying biases that models should help mitigate?
- **Basis in paper:** [explicit] "This raises important questions of whether the model should be adjusted to better capture these differences or whether the observed disparities reflect underlying biases that the model should help mitigate. Addressing this issue requires additional clinical input."
- **Why unresolved:** The appropriate response depends on context, domain expertise, and whether differences reflect biological reality versus historical inequity.
- **What evidence would resolve it:** Domain-specific studies linking outcome disparities to causal mechanisms, combined with ethical framework development for when to preserve vs. adjust for group differences.

### Open Question 2
- **Question:** How can fairness visualizations be effectively scaled to handle comparisons across many groups without overwhelming users?
- **Basis in paper:** [explicit] "Another limitation of seeBias is the difficulty in handling comparisons across a large number of groups... The current implementation is optimized for up to seven groups, and future development aims to address this limitation."
- **Why unresolved:** Visualization design for high-dimensional group comparisons remains an open challenge in information visualization research.
- **What evidence would resolve it:** User studies testing alternative visualization approaches for many-group fairness comparisons, measuring comprehension and decision-making quality.

### Open Question 3
- **Question:** What additional interpretable metrics beyond NNTP/NNTN can help domain experts assess the practical significance of fairness disparities?
- **Basis in paper:** [explicit] "We believe further exploration of such translations can help evaluate the real-world significance of group disparities, improving the interpretability of fairness studies for domain experts."
- **Why unresolved:** Translating statistical fairness metrics into domain-meaningful quantities requires deep understanding of specific application contexts.
- **What evidence would resolve it:** Cross-domain studies with domain experts evaluating which metric translations best support real-world decision-making about model deployment.

## Limitations
- The tool's reliance on visualization for interpretation may not scale well to datasets with many sensitive attributes or complex intersectional categories
- While the case studies provide compelling examples, the generalizability of findings to other domains requires further validation
- The package focuses exclusively on assessment without mitigation capabilities, limiting its utility in production pipelines where both capabilities are needed

## Confidence
- **High:** The core claim that integrating classification, calibration, and ranking assessments provides more comprehensive fairness evaluation is well-supported by the case study evidence and aligns with established fairness theory.
- **Medium:** The assertion that "Number Needed" metrics improve stakeholder understanding is plausible but not directly tested with domain experts in this work.
- **Medium:** The claim about revealing biases missed by conventional metrics is supported by case studies but would benefit from systematic comparison across more datasets.

## Next Checks
1. **Statistical Power Analysis:** Systematically evaluate how sample size affects the reliability of fairness assessments, particularly for small demographic groups where wide confidence intervals may mask meaningful disparities.
2. **Cross-Domain Applicability:** Apply seeBias to non-binary classification tasks (multi-class, regression) and survival analysis to validate the tool's flexibility beyond the demonstrated use cases.
3. **Mitigation Integration Test:** Assess how effectively seeBias identifies specific failure modes that can be addressed through post-processing techniques, comparing its diagnostic value against existing fairness toolkits that combine assessment and mitigation.