---
ver: rpa2
title: Selective Labeling with False Discovery Rate Control
arxiv_id: '2510.14581'
source_url: https://arxiv.org/abs/2510.14581
tags:
- labeling
- conformal
- power
- score
- selective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of obtaining high-quality labels
  for large datasets at a low cost by using AI models to predict labels while controlling
  the labeling error. The proposed method, Conformal Labeling, constructs conformal
  p-values using a small calibration set to estimate AI model accuracy and applies
  the Benjamini-Hochberg procedure at an adjusted significance level to identify a
  subset of test instances with provable false discovery rate (FDR) control.
---

# Selective Labeling with False Discovery Rate Control

## Quick Facts
- arXiv ID: 2510.14581
- Source URL: https://arxiv.org/abs/2510.14581
- Reference count: 40
- One-line primary result: Conformal Labeling controls FDR at 9.85% on Zebra-Logic while labeling 63.73% of data, outperforming existing methods.

## Executive Summary
This paper proposes Conformal Labeling, a method for selective labeling that controls the False Discovery Rate (FDR) when using AI models to label large datasets. The method constructs conformal p-values using a small calibration set and applies an adjusted Benjamini-Hochberg procedure to identify a subset of test instances with provable FDR control. Extensive experiments demonstrate that Conformal Labeling achieves tight FDR control while labeling a large proportion of data, significantly outperforming existing heuristic methods.

## Method Summary
Conformal Labeling addresses selective labeling by using a small labeled calibration set to estimate AI model accuracy and construct conformal p-values for test instances. The method applies an adjusted Benjamini-Hochberg procedure at a significance level that accounts for the estimated proportion of incorrect predictions. This approach guarantees that the expected proportion of incorrect AI-assigned labels is below a user-specified level while maximizing the number of labeled instances. The method works by computing uncertainty scores (e.g., Maximum Softmax Probability), generating p-values by comparing test instance scores against calibration set errors, and selecting instances based on adjusted FDR thresholds.

## Key Results
- Achieves tight FDR control (e.g., 9.85% on Zebra-Logic) while labeling a large proportion of data (e.g., 63.73% on Zebra-Logic)
- Significantly outperforms existing heuristic methods in both FDR control and power across image labeling, LLM QA, and LLM open-ended generation tasks
- Maintains robustness with small calibration sets (e.g., 5% of data) and provides theoretical guarantees under i.i.d. assumptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A labeled calibration set enables the method to construct conformal p-values that are distribution-free and valid under finite samples.
- Mechanism: The method compares the uncertainty score of a test instance against the empirical distribution of scores from known misclassified instances in the calibration set. This rank-based comparison yields a p-value for each test instance.
- Core assumption: The calibration data and test data are sampled i.i.d. from the same joint distribution.
- Evidence anchors:
  - [abstract] The method "constructs conformal p-values using a small calibration set to estimate AI model accuracy."
  - [section] "In this work, we assume access to a small labeled calibration dataset... since the labeling cost of a small dataset by human annotators is typically affordable, this assumption is practical."
  - [corpus] Similar to COIN (2506.20178), which uses a split conformal approach requiring a labeled calibration set.

### Mechanism 2
- Claim: The False Discovery Rate (FDR) is controlled by using an adjusted Benjamini-Hochberg (BH) procedure based on an estimate of model accuracy from the calibration set.
- Mechanism: Standard BH is conservative, controlling FDR at level π₀α, where π₀ is the proportion of incorrect predictions. By estimating π₀ from the calibration set (as 1+n₀/1+n), the method applies BH at a higher significance level (1+n/1+n₀)α, achieving tighter control and higher power.
- Core assumption: The accuracy on the calibration set is representative of the accuracy on the test set.
- Evidence anchors:
  - [abstract] "applies the Benjamini-Hochberg procedure at an adjusted significance level to identify a subset... with provable false discovery rate (FDR) control."
  - [section] Theorem 3.2 provides the guarantee: "FDR ≤ [1-(1-p)^{n+1}]α ≤ α."
  - [corpus] Related work on conformal novelty detection (2601.02610) also uses the BH procedure with conformal p-values for FDR control.

### Mechanism 3
- Claim: The method's practical power is determined by the quality of the chosen uncertainty score function.
- Mechanism: The conformal p-value framework guarantees FDR control regardless of the score. However, to maximize the number of selected instances (power), the score must effectively separate correct from incorrect predictions. Better separation leads to lower p-values for correct predictions and higher power.
- Core assumption: There exists a score function that correlates with prediction correctness.
- Evidence anchors:
  - [abstract] Experiments show "Conformal Labeling achieves tight FDR control... while labeling a large proportion of data."
  - [section] "As shown in prior work... the statistical power of this method depends on the quality of the uncertainty score. In particular, a score that better separates correct from incorrect predictions directly increases statistical power."
  - [corpus] Corpus signals are weak on the direct impact of specific score functions but align with the general goal of selection.

## Foundational Learning

- Concept: **False Discovery Rate (FDR)**
  - Why needed here: This is the core metric the paper's entire guarantee is built around. Understanding FDR as the expected proportion of false positives among all selected instances is crucial.
  - Quick check question: If you have a target FDR of 10% and select 100 instances for AI labeling, what is the maximum *expected* number of incorrect labels in that set?

- Concept: **Conformal Inference**
  - Why needed here: This is the statistical framework used to create p-values with distribution-free guarantees. The core idea is using a labeled calibration set to create prediction sets or p-values that are valid under minimal assumptions.
  - Quick check question: How does conformal inference provide a guarantee without making strong assumptions about the data distribution (e.g., Gaussian)?

- Concept: **Benjamini-Hochberg (BH) Procedure**
  - Why needed here: BH is the classical method for multiple hypothesis testing that controls the FDR. The paper builds upon and modifies this procedure to achieve its goals.
  - Quick check question: How does the BH procedure adjust for multiple comparisons differently than a simple Bonferroni correction?

## Architecture Onboarding

- Component map:
  - **Input:** Unlabeled test set, a small labeled calibration set, and a pre-trained AI model.
  - **Uncertainty Scoring:** A function (e.g., Maximum Softmax Probability) assigns a score to each instance.
  - **Calibration & P-value Generation:** The method identifies mislabeled instances in the calibration set, and for each test instance, computes a conformal p-value by ranking its score against the scores of mislabeled calibration instances.
  - **Adaptive Selection:** The method estimates model accuracy (π̂₀) from the calibration set and applies the BH procedure with an adjusted significance level (α / π̂₀).
  - **Output:** A subset R of test instances, labeled by the AI model, with a guarantee on the expected error rate.

- Critical path:
  1. Acquire/prepare a representative labeled calibration set. This is the most critical step, as all guarantees depend on it.
  2. Choose or design an appropriate uncertainty score function. This directly impacts the method's power.
  3. Run the conformal labeling pipeline. The selection procedure is automatic and requires no tuning.

- Design tradeoffs:
  - **Power vs. Guarantee:** The method guarantees FDR control but power (the proportion of data labeled by AI) is not guaranteed and depends on the quality of the uncertainty score and the underlying model's accuracy.
  - **Score Function Choice:** The paper empirically shows that logits-based scores (MSP) often outperform verbalized scores from LLMs, but this requires white-box access to model logits.
  - **Calibration Set Size:** A larger set reduces variance in the FDR and power estimates but incurs higher labeling cost. The method is robust to smaller sizes (e.g., 5%).

- Failure signatures:
  - **Distribution Shift:** Realized FDR consistently exceeds the target level α. The paper notes this violates the i.i.d. assumption.
  - **Poor Score Function:** Realized power is very low (<10-20%), causing the method to defer most instances to humans, even if the model is reasonably accurate.

- First 3 experiments:
  1. **Sanity Check:** Reproduce the main result on a standard dataset (e.g., ImageNet with ResNet-34). Use the provided calibration ratio (10%) and MSP score. Verify that the realized FDR is at or below the target level α.
  2. **Robustness Test:** Systematically reduce the calibration set size (e.g., from 10% down to 1%). Plot the resulting FDR and power over 1000 trials to observe the increase in variance, as shown in Figure 3.
  3. **Score Ablation:** Compare the power and FDR achieved by MSP against another score (e.g., Energy score) on the same model/dataset. Confirm that while both control FDR, the better score yields significantly higher power, as per Table 9.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Conformal Labeling maintain theoretical FDR guarantees under distribution shift (covariate shift)?
- Basis in paper: [explicit] Appendix I.1 states that while the method is empirically robust to moderate shift, it lacks theoretical guarantees in non-i.i.d. settings. The authors explicitly identify integrating weighted conformal inference as an "interesting direction for subsequent works."
- Why unresolved: The finite-sample guarantee in Theorem 3.2 relies strictly on the exchangeability of calibration and test data. Real-world deployment often involves drift, breaking this assumption and invalidating the proof.
- What evidence would resolve it: A theoretical extension of Theorem 3.2 using weighted p-values (likelihood ratios) that bounds the FDR even when the test distribution P_{test} differs from P_{cal}.

### Open Question 2
- Question: How can the uncertainty score function be optimized or learned to maximize selection power while preserving validity?
- Basis in paper: [inferred] Section 3.2 and Appendix I.2 demonstrate that the choice of score function (e.g., MSP vs. Energy) drastically affects power (e.g., 80% vs. 50% on ImageNet), but the paper relies on heuristic or existing scores.
- Why unresolved: The paper treats the score function S(X) as a fixed input. It does not explore how to adaptively learn a score that maximizes the separation between correct and incorrect predictions to increase the "AI-labeled ratio."
- What evidence would resolve it: A methodology for learning the uncertainty score S (perhaps jointly with the classifier or via a separate calibration objective) that provably maintains the conservatism of the conformal p-value.

### Open Question 3
- Question: Is Conformal Labeling robust to label noise within the calibration set itself?
- Basis in paper: [inferred] Section 2 assumes access to a "small labeled calibration dataset" where Y_i are ground truth. However, human annotators can also introduce errors, which the current method does not account for.
- Why unresolved: If the calibration set contains mislabeled instances (false positives/negatives regarding "correctness"), the estimation of model accuracy (π₀) and the construction of the misclassified subset D_{cal}^0 will be biased, potentially breaking FDR control.
- What evidence would resolve it: Theoretical analysis or empirical simulation showing the degradation of FDR control as the noise rate in the human-annotated calibration set increases, or a modified algorithm that accounts for annotator uncertainty.

## Limitations
- The method requires a labeled calibration set, which may not be available in all selective labeling scenarios
- Theoretical FDR guarantees rely on the i.i.d. assumption between calibration and test sets, which can be violated in practice
- The practical power of the method heavily depends on the quality of the uncertainty score function, which is not theoretically optimized

## Confidence

- **High Confidence:** The theoretical FDR control guarantee (Theorem 3.2) is mathematically sound given the i.i.d. assumption and proper implementation of the conformal p-value procedure.
- **Medium Confidence:** The empirical results demonstrating superior performance to heuristic methods are convincing but may be sensitive to specific experimental choices (e.g., calibration set size, uncertainty score selection).
- **Medium Confidence:** The claim that the method is "practical" and "affordable" for a small calibration set is reasonable but depends on the specific labeling cost context.

## Next Checks

1. **Distribution Shift Robustness:** Systematically evaluate the method's FDR control and power under controlled distribution shifts between calibration and test sets to quantify the impact of violating the i.i.d. assumption.

2. **Score Function Ablation:** Conduct a comprehensive ablation study comparing multiple uncertainty score functions (e.g., MSP, Energy, Entropy) across different model architectures and tasks to identify which scores yield the best power-FDR tradeoff.

3. **Calibration Set Size Sensitivity:** Analyze the method's performance across a wider range of calibration set sizes (e.g., 1% to 20%) to determine the minimum size required for reliable FDR control and acceptable power, and quantify the variance in performance at smaller sizes.