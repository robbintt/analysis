---
ver: rpa2
title: 'PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language
  Reasoning'
arxiv_id: '2506.14907'
source_url: https://arxiv.org/abs/2506.14907
tags:
- image
- reasoning
- arxiv
- multi-image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-image positional reasoning
  in vision-language models, where models often fail to correctly align textual references
  with the appropriate images when the order of input images is permuted. The proposed
  PeRL method tackles this by introducing permutation of image sequences and rephrasing
  text to simulate varied positional relationships, combined with a rollout filtering
  mechanism to focus learning on informative trajectories.
---

# PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning

## Quick Facts
- arXiv ID: 2506.14907
- Source URL: https://arxiv.org/abs/2506.14907
- Reference count: 40
- Achieves 76.4% accuracy on Mantis-Eval multi-image benchmark

## Executive Summary
PeRL addresses the challenge of multi-image positional reasoning in vision-language models, where models often fail to correctly align textual references with the appropriate images when the order of input images is permuted. The proposed method tackles this by introducing permutation of image sequences and rephrasing text to simulate varied positional relationships, combined with a rollout filtering mechanism to focus learning on informative trajectories. This multi-stage data processing and permutation-enhanced reinforcement learning approach improves learning efficiency and task performance. Evaluated on 5 multi-image and 3 single-image benchmarks, PeRL achieves state-of-the-art results on multi-image tasks while maintaining competitive performance on single-image tasks.

## Method Summary
PeRL uses permutation augmentation with GRPO (Group Relative Policy Optimization) to improve multi-image positional reasoning. The method involves: (1) data preprocessing with rule-based filtering and semantic variation checking using GPT-4o, (2) rollout filtering to balance training difficulty by computing average accuracy across 10 rollouts per sample, and (3) permutation-enhanced GRPO where image sequences are randomly permuted during training with linearly decaying probability. The model generates rollouts for each permutation order, and answers are updated via permutation-aware equations for semantically sensitive samples. Training uses a mixture of 22K multi-image examples from Mantis-Instruct and 36K single-image examples from K12 dataset, with 2 epochs of training on 8×H100 GPUs.

## Key Results
- Achieves 76.4% accuracy on Mantis-Eval, outperforming baselines (71.5% Base, 75.1% GRPO)
- Maintains competitive single-image performance at 73.0% on MathVista
- Improves BLINK spatial reasoning from 56.91% to 59.50%
- Shows ablation results demonstrating ns=1 permutation provides optimal trade-off between performance and training efficiency

## Why This Works (Mechanism)
The method addresses a fundamental limitation in VLM positional reasoning where models fail to correctly associate textual references with images when input order changes. By systematically permuting image sequences during training and updating answers accordingly for semantically sensitive samples, PeRL forces the model to learn permutation-invariant representations. The rollout filtering mechanism ensures the model focuses on informative samples by balancing difficulty distribution, preventing overfitting to easy examples or collapse on hard ones.

## Foundational Learning
- **Permutation invariance**: Understanding why models should produce consistent outputs regardless of input order - needed because VLM positional reasoning often fails when image order changes; quick check: test model on permuted versions of same question
- **GRPO optimization**: Group Relative Policy Optimization mechanics for reinforcement learning with multiple rollouts - needed to compute stable advantages across permutation groups; quick check: monitor advantage variance during training
- **Rollout filtering**: Statistical analysis of model performance distributions to select informative training samples - needed to prevent training on samples that are too easy or too hard; quick check: verify filtered distribution matches target (μ≈0.38)
- **Semantic variation analysis**: Detecting when text meaning changes under image permutation - needed to determine when answers should change with image order; quick check: manually verify GPT-4o labels on sample questions
- **KV-cache optimization**: Understanding how sequence length affects memory usage in transformers - needed to explain training efficiency trade-offs with permutation; quick check: monitor memory usage with different ns values

## Architecture Onboarding

**Component Map**: Data Preprocessing -> Rollout Filtering -> Permutation GRPO -> Model Training

**Critical Path**: The core pipeline processes each training sample through multiple stages: initial data filtering (question type + image count), GPT-4o semantic variation labeling, rollout generation and filtering, permutation augmentation during training, and final GRPO update. The rollout filtering stage is particularly critical as it determines which samples receive permutation augmentation.

**Design Tradeoffs**: Permutation intensity (ns parameter) creates a fundamental tradeoff between performance gains and training efficiency. Higher ns values (more permutations) improve multi-image performance but reduce KV-cache reuse and increase training cost. The method also trades off between multi-image specialization and single-image generalization, as permutation augmentation can degrade performance on single-image tasks if not properly balanced.

**Failure Signatures**: 
- Positional bias persists: model accuracy changes significantly when image order is permuted
- Training instability: advantage distribution peaks at zero, indicating lack of exploration
- Single-image task degradation: performance on MathVista drops significantly during training

**First 3 Experiments**:
1. **Rollout filtering validation**: Generate 10 rollouts per sample using base model, compute accuracy distribution, verify filtering produces balanced distribution (mean ~0.38)
2. **Permutation invariance test**: After training, test model on permuted versions of same questions, measure consistency across semantically equivalent permutations
3. **Ablation study**: Train separate models with only filtering, only permutation, and both combined to quantify individual contributions

## Open Questions the Paper Calls Out
- Can adaptive permutation strategies dynamically adjust image ordering based on task difficulty or model uncertainty to further improve learning efficiency?
- What is the optimal balance between permutation intensity and training efficiency for different task types (spatial vs. mathematical reasoning)?
- Does the semantic variation checking dependency on GPT-4o introduce biases or limitations that affect generalization to out-of-distribution multi-image tasks?
- How does the rollout filtering threshold affect the exploration-exploitation trade-off and final model performance across varying difficulty distributions?

## Limitations
- Relies heavily on GPT-4o for semantic variation detection, introducing API dependency and potential biases
- Shows smaller improvements on single-image tasks (73.0% vs 72.4%), suggesting limited generalizability beyond multi-image scenarios
- Key implementation details underspecified, including exact filtering thresholds and permutation schedule parameters

## Confidence
- **High confidence** in core technical approach: permutation-enhanced GRPO with rollout filtering is conceptually sound
- **Medium confidence** in quantitative results: substantial improvements reported but depend on precise implementation details
- **Low confidence** in complete reproducibility: missing critical hyperparameters and prompt templates

## Next Checks
1. **Reproduce the rollout filtering distribution**: Generate 10 rollouts per sample using the base model, compute accuracy distributions, and verify the filtering threshold produces the balanced distribution shown in Figure 3b (mean accuracy ~0.38). Test whether different thresholds significantly impact final performance.

2. **Validate permutation invariance**: After training with PeRL, test the model on permuted versions of the same questions. Measure whether accuracy remains consistent across semantically equivalent permutations, confirming the model has learned true positional invariance rather than memorizing specific orderings.

3. **Ablation of filtering vs permutation**: Train separate models with (a) only rollout filtering, (b) only permutation augmentation, and (c) both combined. Compare performance to quantify the individual contributions of each component to the overall improvement, particularly on multi-image benchmarks where gains are largest.