---
ver: rpa2
title: Do explanations generalize across large reasoning models?
arxiv_id: '2601.11517'
source_url: https://arxiv.org/abs/2601.11517
tags:
- arxiv
- dapo
- reasoning
- transfer
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether large reasoning model (LRM) chain-of-thought
  (CoT) explanations generalize across different models. The core method evaluates
  whether explanations from one LRM lead other LRMs to the same answers, measuring
  this via cross-model consistency.
---

# Do explanations generalize across large reasoning models?

## Quick Facts
- arXiv ID: 2601.11517
- Source URL: https://arxiv.org/abs/2601.11517
- Authors: Koyena Pal; David Bau; Chandan Singh
- Reference count: 40
- Primary result: Shared chain-of-thought explanations increase cross-model answer consistency from ~25% to ~66% on MedCalc-Bench and from ~54% to ~62% on Instruction Induction.

## Executive Summary
This paper investigates whether chain-of-thought explanations from large reasoning models (LRMs) generalize across different models. Through controlled experiments, the authors demonstrate that explanations from one LRM can successfully guide other LRMs to similar answers, with consistency improvements of up to 41 percentage points. The study introduces a sentence-level ensembling method that further improves cross-model consistency and shows that models trained with reinforcement learning produce more generalizable explanations. However, the findings also reveal a critical limitation: high consistency does not guarantee correctness, as models can converge on the same wrong answers.

## Method Summary
The study evaluates cross-model explanation generalization using two benchmarks: MedCalc-Bench (medical calculation problems) and Instruction Induction (pattern recognition tasks). Four chain-of-thought generation methods are tested: Empty (baseline), Default (greedy/sampled), Transfer (using another model's CoT), and Ensembled (multiple generators with evaluator selection). The evaluator model selects the candidate with lowest perplexity, and explicit answers are removed from CoTs before transfer to prevent shortcut reasoning. Consistency is measured as the pairwise agreement rate between models given the same explanation, while accuracy measures match to ground truth.

## Key Results
- Shared explanations increase answer consistency between models from ~25% to ~66% on MedCalc-Bench and from ~54% to ~62% on Instruction Induction
- Sentence-level ensembling further improves consistency beyond single-model transfer
- More consistent explanations correlate with higher human preference ratings
- Reinforcement learning post-training yields CoTs that are more consistent with other LRMs
- Models can converge on the same wrong answers despite high consistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shared chain-of-thought explanations increase cross-model answer consistency, indicating they capture generalizable reasoning patterns rather than model-specific artifacts.
- Mechanism: When an explanation from model A is inserted into model B's thinking context, it constrains B's reasoning trajectory toward similar intermediate conclusions. The natural language explanation acts as a computational scaffold that shapes attention over the reasoning space.
- Core assumption: Models share underlying representations of reasoning patterns that can be activated via natural language prompts; consistency reflects genuine pattern capture rather than superficial linguistic matching.
- Evidence anchors:
  - [abstract] "shared explanations increase answer consistency between models from ~25% to ~66% on MedCalc-Bench and from ~54% to ~62% on Instruction Induction"
  - [section 3.1] "Transfer and ensemble CoTs substantially improve consistency in both benchmarks"
- Break condition: If models were merely pattern-matching surface features rather than reasoning, we would expect consistency gains without accuracy correlation. However, Figure 3 shows CoTs can systematically steer models toward the same incorrect answers.

### Mechanism 2
- Claim: Sentence-level ensembling reduces model-specific idiosyncrasies by aggregating diverse reasoning paths and selecting the most broadly compatible continuation.
- Mechanism: Multiple generator models each propose candidate sentence completions. An evaluator model selects the candidate with lowest perplexity—the one it finds least surprising—implying broader compatibility across the model family. This iterative process produces an explanation that no single model would generate alone.
- Core assumption: Low perplexity correlates with cross-model generalizability; ensemble selection preferentially identifies reasoning steps that multiple models find natural.
- Evidence anchors:
  - [section 2] "The evaluator then selects the candidate with the lowest perplexity, which is appended to the ensembled CoT"
  - [figure 2] Ensemble CoT configurations achieve higher consistency than single-model transfer in most conditions
- Break condition: If evaluator preference simply reflects the evaluator's own biases rather than generalizability, ensemble CoT would not outperform direct transfer from the evaluator model.

### Mechanism 3
- Claim: Reinforcement learning post-training improves explanation generalizability by implicitly training models to produce reasoning traces that align with verifiable solution paths.
- Core assumption: GRPO training on reasoning tasks shapes not just final-answer accuracy but the form of intermediate reasoning, making it more recognizable and usable by other models trained on similar distributions.
- Evidence anchors:
  - [table 4] GRPO training increased consistency from 0.11→0.39 (Deepseek) and 0.25→0.46 (Llama) on transfer evaluation
  - [section 3.3] "RL post-training yields CoTs that are more consistent with other LRMs"
- Break condition: Consistency and accuracy improvements are separable—Deepseek's GRPO model showed large consistency gains without accuracy improvement on transfer.

## Foundational Learning

- **Chain-of-thought (CoT) as intervention**: 
  - Why needed: This paper treats CoT not as an interpretability tool but as a treatment variable—inserting one model's reasoning into another's context to measure behavioral convergence.
  - Quick check: Can you articulate the difference between using CoT for explaining a model vs. using CoT to change another model's behavior?

- **Consistency vs. Accuracy decomposition**:
  - Why needed: The paper's central finding is that high consistency does not guarantee correctness—models can converge on wrong answers. Understanding this distinction is essential for interpreting the safety implications.
  - Quick check: If two models agree 90% of the time but are only 60% accurate, what does this tell you about their explanation generalization?

- **Perplexity as generalization proxy**:
  - Why needed: The ensemble method uses perplexity to select sentences, assuming lower perplexity indicates broader compatibility. This is a non-obvious design choice requiring understanding of how language models score sequence probability.
  - Quick check: Why might a sentence with low perplexity for one model have high perplexity for another? What does this imply about their training distributions?

## Architecture Onboarding

- **Component map**: Generator models (e.g., QwQ-32B, DAPO) -> Evaluator model (e.g., GPT-OSS-20B) -> Target models -> Answer sanitization -> Final answer comparison

- **Critical path**:
  1. Sample problem from benchmark (MedCalc-Bench or Instruction Induction)
  2. Generate CoT via one of four methods: Empty, Default, Transfer, Ensemble
  3. For ensemble: loop generator→evaluator selection until end-of-thought
  4. Sanitize CoT to remove explicit answers
  5. Insert CoT into target model's thinking context
  6. Extract final answer; compare against ground truth (accuracy) and other models' answers (consistency)

- **Design tradeoffs**:
  - Transfer vs. Ensemble: Transfer is cheaper (single forward pass) but ensemble produces more generalizable explanations at ~N× generation cost for N generators
  - Evaluator choice matters: OSS as evaluator outperforms OSS as generator, suggesting better "generalization recognition" capability
  - Answer removal adds noise: Sanitization prevents direct answer-copying but may remove useful reasoning context

- **Failure signatures**:
  - High consistency, low accuracy: Models converging on wrong answers indicates explanation captures misleading patterns
  - Generator dominates ensemble: If one generator's candidates are consistently selected (>70%), ensemble degenerates to transfer from that model
  - Evaluator-evaluator mismatch: When evaluator model differs substantially from target models in training distribution, perplexity selection becomes unreliable proxy for generalization

- **First 3 experiments**:
  1. Establish baseline consistency: Run Empty CoT and Default (Greedy) conditions across all model pairs to measure natural agreement rates before intervention
  2. Validate transfer effect: Select your strongest model's CoT, transfer to all others, measure consistency gain
  3. Implement minimal ensemble: Two generators + one evaluator, sentence-by-sentence construction on 10 examples; verify perplexity selection produces different outputs than either generator alone

## Open Questions the Paper Calls Out

1. How can reinforcement learning post-training pipelines be explicitly optimized to incentivize explanations that improve transfer accuracy rather than just cross-model consistency?
   - Basis: Discussion suggests future work explore "improving RL post-training pipelines to incentivize generating generalizable explanations." Table 4 shows GRPO training improved consistency for Deepseek-R1 but failed to improve average transfer accuracy.

2. To what extent does cross-LRM generalizability predict generalizability to human users, particularly when models share a common bias?
   - Basis: Discussion notes that while the framework helps evaluate generalizability to humans, "currently this assumption may fail in the case that different LRMs share a common bias for a particular explanation."

3. Can chain-of-thought explanations that generalize across diverse examples facilitate the discovery of new knowledge in scientific domains?
   - Basis: Discussion states a future line of work "may seek to find CoTs that generalize across diverse examples for the purpose of generating new knowledge."

## Limitations

- The study's central finding relies on a relatively small evaluation set (100 MedCalc-Bench examples, 20 Instruction Induction tasks) that may not generalize to broader domains
- The answer sanitization process using GPT-4o-mini may remove useful intermediate reasoning that could affect consistency measurements
- The study focuses on pairwise consistency rather than multi-model agreement, potentially underestimating variance in explanation interpretability

## Confidence

**High confidence** in the core empirical finding that shared CoT explanations increase cross-model consistency (supported by direct measurements showing substantial consistency gains across two distinct benchmarks).

**Medium confidence** in the interpretation that this reflects genuine reasoning pattern capture rather than surface-level pattern matching (evidence shows CoTs can steer models to same wrong answers, but mechanism isn't definitively proven).

**Low confidence** in the generalizability of these findings beyond the specific model families and task types studied (results based on 5 LRMs from specific training distributions).

## Next Checks

1. Extend consistency measurements beyond pairwise comparisons to measure agreement rates across 3+ models simultaneously, better capturing the robustness of explanation generalizability.

2. Systematically analyze cases where high consistency leads to wrong answers—what reasoning patterns are being captured and shared? This would clarify whether explanation generalization is desirable or potentially harmful.

3. Evaluate explanation generalization when transferring between models trained on different domains (e.g., medical reasoning vs. general instruction following) to test the limits of the proposed mechanism.