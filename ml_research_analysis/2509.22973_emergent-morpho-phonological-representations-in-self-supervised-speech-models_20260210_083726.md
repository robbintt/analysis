---
ver: rpa2
title: Emergent morpho-phonological representations in self-supervised speech models
arxiv_id: '2509.22973'
source_url: https://arxiv.org/abs/2509.22973
tags:
- word
- noun
- figure
- verb
- wav2vec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Self-supervised speech models (S3Ms) can recognize spoken words
  with high accuracy, but the underlying linguistic representations remain unclear.
  This paper investigates whether S3Ms rely on distinct phonological, morphological,
  or lexical representations for word recognition.
---

# Emergent morpho-phonological representations in self-supervised speech models

## Quick Facts
- arXiv ID: 2509.22973
- Source URL: https://arxiv.org/abs/2509.22973
- Reference count: 35
- Self-supervised speech models rely on abstract morpho-phonological patterns rather than strictly separated linguistic levels for word recognition

## Executive Summary
Self-supervised speech models (S3Ms) can recognize spoken words with high accuracy, but the underlying linguistic representations remain unclear. This paper investigates whether S3Ms rely on distinct phonological, morphological, or lexical representations for word recognition. The authors develop a word-optimized probe that isolates the subspace of S3M representations used for word contrast, then test whether these representations encode morphological distinctions (noun vs. verb inflection), phonological differences (allomorph variation), lexical contrasts (morphological vs. non-morphological word forms), or distributional phonological patterns. The probe successfully links inflected forms to their base forms through a global linear geometry, performing well above chance across multiple experiments. However, this geometry does not directly track morphological or phonological units. Instead, it captures the distributional constraints governing word-final sounds [z], [s], and [Iz] in English, which determine how noun plurals and verb inflections surface. This finding suggests that S3Ms may rely on abstract, cross-cutting morpho-phonological patterns rather than strictly separated linguistic levels, offering new hypotheses about the representational strategies supporting human word recognition.

## Method Summary
The authors develop a word-optimized probe that isolates the subspace of S3M representations used for word contrast. This probe is designed to test whether these representations encode morphological distinctions (noun vs. verb inflection), phonological differences (allomorph variation), lexical contrasts (morphological vs. non-morphological word forms), or distributional phonological patterns. The probe's performance is evaluated on a set of inflected and base forms of English words, and its ability to link inflected forms to their base forms through a global linear geometry is measured. The probe's success in this task is used as evidence for the presence of morpho-phonological representations in S3Ms.

## Key Results
- The probe successfully links inflected forms to their base forms through a global linear geometry, performing well above chance across multiple experiments
- The probe's geometry does not directly track morphological or phonological units, but instead captures the distributional constraints governing word-final sounds [z], [s], and [Iz] in English
- S3Ms may rely on abstract, cross-cutting morpho-phonological patterns rather than strictly separated linguistic levels for word recognition

## Why This Works (Mechanism)
The probe's success in linking inflected forms to their base forms through a global linear geometry suggests that S3Ms may rely on abstract, cross-cutting morpho-phonological patterns rather than strictly separated linguistic levels. This mechanism allows the model to capture the distributional constraints governing word-final sounds in English, which determine how noun plurals and verb inflections surface. By relying on these abstract patterns, S3Ms can achieve high accuracy in word recognition without explicitly representing morphological or phonological units.

## Foundational Learning
- **Morphological inflection**: The process of changing a word's form to express different grammatical categories (e.g., noun vs. verb inflection). Why needed: To test whether S3Ms encode morphological distinctions.
- **Phonological allomorphy**: The variation in the pronunciation of a morpheme depending on its phonological context (e.g., allomorph variation). Why needed: To test whether S3Ms encode phonological differences.
- **Lexical contrast**: The distinction between morphological and non-morphological word forms. Why needed: To test whether S3Ms encode lexical contrasts.
- **Distributional phonology**: The study of how phonological patterns are shaped by the distributional properties of sounds in a language. Why needed: To understand the morpho-phonological patterns captured by the probe.
- **Self-supervised learning**: A learning paradigm where the model learns to predict some part of the input from another part of the input, without explicit labels. Why needed: To understand the training process of S3Ms.
- **Word recognition**: The process of identifying a word from its acoustic signal. Why needed: To understand the task that S3Ms are designed to perform.

## Architecture Onboarding
Component map: S3M -> Word-optimized probe -> Morpho-phonological patterns
Critical path: The probe's ability to link inflected forms to their base forms through a global linear geometry
Design tradeoffs: The probe's success in capturing morpho-phonological patterns comes at the cost of not directly tracking morphological or phonological units
Failure signatures: The probe's performance would be at chance level if S3Ms relied on strictly separated linguistic levels
First experiments:
1. Test the probe on other languages with different morphological and phonological systems
2. Compare the performance of the probe on different S3M architectures
3. Develop and apply alternative probing methods to investigate the nature of the morpho-phonological patterns

## Open Questions the Paper Calls Out
None

## Limitations
- The study does not test whether the findings generalize to other languages, other S3M architectures, or other probing methods
- The study does not investigate whether the morpho-phonological patterns captured by the probe are explicitly represented in the model or merely emergent from the distributional statistics of the training data
- The study does not address potential confounds from other linguistic or non-linguistic factors that could influence the probe's performance

## Confidence
- High confidence in the probe's ability to link inflected forms to their base forms through a global linear geometry, as this is directly measured and validated
- Medium confidence in the claim that S3Ms rely on abstract morpho-phonological patterns rather than strictly separated linguistic levels, as this is inferred from the probe's performance on a limited set of contrasts in English
- Low confidence in the generalizability of the findings to other languages, S3M architectures, or probing methods, as these are not tested in the study

## Next Checks
1. Test the probe on other languages with different morphological and phonological systems to assess the generalizability of the findings
2. Compare the performance of the probe on different S3M architectures (e.g., HuBERT, wav2vec 2.0) to determine whether the morpho-phonological patterns are specific to a particular model
3. Develop and apply alternative probing methods (e.g., local linear geometry, non-linear probes) to investigate whether the morpho-phonological patterns are explicitly represented in the model or merely emergent from distributional statistics