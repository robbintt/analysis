---
ver: rpa2
title: Model-Based Reinforcement Learning under Random Observation Delays
arxiv_id: '2509.20869'
source_url: https://arxiv.org/abs/2509.20869
tags:
- delays
- observations
- learning
- observation
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies reinforcement learning under random observation
  delays in partially observable Markov decision processes (POMDPs), where observations
  may arrive out-of-sequence. The authors propose a model-based filtering approach
  that maintains a belief state over latent states using a learned world model, enabling
  agents to handle delayed and out-of-sequence observations effectively.
---

# Model-Based Reinforcement Learning under Random Observation Delays

## Quick Facts
- **arXiv ID:** 2509.20869
- **Source URL:** https://arxiv.org/abs/2509.20869
- **Reference count:** 40
- **Primary result:** Model-based RL approach handles random observation delays in POMDPs by maintaining belief states over latent states, outperforming baselines on MuJoCo and Meta-World tasks.

## Executive Summary
This paper addresses reinforcement learning under random observation delays where observations may arrive out-of-sequence. The authors propose a model-based filtering approach that maintains a belief state over latent states using a learned world model, enabling agents to handle delayed and out-of-sequence observations effectively. The method builds on the Dreamer framework, using an auxiliary transition distribution that updates the latent state using the world model when observations are available and defaults to prior dynamics otherwise. Experiments show consistent performance across various delay distributions and strong generalization to unseen delay patterns.

## Method Summary
The approach maintains a belief state over latent states in partially observable environments with random delays. It uses a learned world model to perform filtering: when observations arrive on time, the belief is updated via the variational posterior; when observations are delayed, the belief is updated via the prior dynamics. The world model is trained on complete, ordered trajectories after episodes end, avoiding the noise of random delays during representation learning. The policy is trained to act on the computed belief state rather than raw observations, restoring the Markov property for decision-making.

## Key Results
- The proposed method consistently outperforms baselines designed for MDPs and practical heuristics on both fully observable MuJoCo and partially observable Meta-World tasks
- Strong generalization to unseen delay distributions, maintaining performance across various delay patterns
- In Meta-World visual control tasks, achieves success rates exceeding 84% even under long delays, significantly outperforming memoryless and waiting strategies
- Demonstrates graceful degradation as delays increase, with the method maintaining relative performance advantage

## Why This Works (Mechanism)

### Mechanism 1: Time-Dependent Auxiliary Transition (Filtering OOS Data)
The agent maintains an accurate belief state despite out-of-sequence observations by switching between a learned posterior and a prior dynamics model. The system uses an auxiliary kernel ψ that recursively computes the belief φ_t by alternating between q (measurement update when observation present) and p (prediction step when observation missing). This allows the agent to fill in missing perceptual time-steps with imagined dynamics.

### Mechanism 2: Decoupled Model Training (Hindsight Buffering)
The world model learns robust dynamics by training on complete, ordered trajectories, avoiding the noise of random delays during representation learning. After each episode terminates, the system waits until all pending observations arrive before storing the ordered trajectory in the replay buffer. The model learns p and q using standard ELBO maximization on this clean data, ensuring the "physics engine" inside the agent is not corrupted by temporal jitter.

### Mechanism 3: Belief as a Sufficient Statistic
Conditioning the policy on the computed belief φ_t allows standard RL algorithms to act optimally relative to available information. Random delays convert an MDP into a POMDP, and the filtering process compresses the history into a dense latent belief φ_t. This restores the Markov property for the policy network, simplifying the learning objective.

## Foundational Learning

- **Concept: POMDPs (Partially Observable Markov Decision Processes)**
  - Why needed: The paper explicitly reframes delayed RL as a POMDP problem, requiring understanding that the "state" is no longer observable
  - Quick check: If I receive an observation generated at t-5 at time t, is the environment state at t fully known? (Answer: No, it is partially observable)

- **Concept: RSSM (Recurrent State Space Model) / World Models**
  - Why needed: The architecture relies on Dreamer's RSSM to perform the filtering, with both deterministic and stochastic paths
  - Quick check: What are the two distinct roles of the RSSM in this paper? (Answer: 1. Generative modeling to "imagine" missing states during filtering; 2. Providing the latent space where the policy operates)

- **Concept: Variational Inference (ELBO)**
  - Why needed: The model is trained by maximizing evidence lower bound, explaining the roles of prior p and posterior q
  - Quick check: In Eq. (5), why do we swap q for p when an observation is missing? (Answer: q conditions on the observation; if the observation is missing, we default to the prior dynamics p)

## Architecture Onboarding

- **Component map:** Delay Buffer (õ_t) -> Filtering Loop (ψ) -> World Model (RSSM) -> Policy (π(·|ϕ_t))
- **Critical path:** The execution flow diverges from standard RL at the Inference Step (Alg 1, Line 4). You must identify κ_t (most recent complete timestamp), forward-rollout the latent state from κ_t to t using the auxiliary kernel, then sample action from π(ϕ_t)
- **Design tradeoffs:**
  - Particle Count (K): Single particle (K=1) performs well due to RSSM's deterministic path; increasing K adds robustness but linear compute cost
  - Training vs. Inference: Model sees clean data; policy sees filtered data. Domain gap bridged by quality of dynamics model
  - Max Delay Horizon (D): Buffer size limited; if delays exceed buffer size, old data is dropped, degrading performance
- **Failure signatures:**
  - Drift/Blur: Visualizations show reconstructions become blurry as delay increases; tasks requiring fine-grained detail may struggle
  - Long Horizon Error: Recursive prediction errors accumulate; beliefs become hallucinatory under long delays (>20 steps) with chaotic dynamics
- **First 3 experiments:**
  1. Unit Test the Buffer: Create synthetic OOS observation stream, verify compute_phi function correctly applies q when data present and p when absent
  2. Ablation on Delay Distribution: Train on U{0,5} vs U{0,20} delays, verify graceful degradation compared to Stack-Dreamer baseline
  3. Generalization Check: Train on wide delay distribution (U{0,20}) and test on fixed delay (constant 0 or 20) to confirm robustness to distribution shift

## Open Questions the Paper Calls Out
- Can Transformer-based or multi-step architectures effectively mitigate the one-step prediction error accumulation inherent in the recursive filtering approach?
- How can the delay-aware framework be specifically redesigned to handle permanently missing observations (packet loss) rather than finite delays?
- Does the proposed filtering method maintain performance stability when delay distributions are state-dependent or non-stationary?

## Limitations
- The exact implementation details of the auxiliary kernel ψ and κ_t computation are not fully specified, potentially hiding edge cases or performance bottlenecks
- The success with single-particle filtering (K=1) relies heavily on the RSSM's deterministic path, which may not generalize to environments with more complex stochastic dynamics
- The method assumes all observations eventually arrive; it does not handle permanently missing observations or packet loss scenarios

## Confidence

- **High Confidence:** The conceptual framework of decoupling world model training from policy training under delays is well-supported and logically sound
- **Medium Confidence:** The effectiveness of belief-based policy learning is supported by experiment results, but analysis relies on paper's claims without independent verification
- **Medium Confidence:** The filtering mechanism is described, but practical implementation details and potential failure modes under extreme delay conditions are not fully explored

## Next Checks

1. **Implementation Verification:** Implement delay wrapper and belief computation (Eq. 5) as specified. Run unit test with synthetic OOS observation streams to verify κ_t computation and auxiliary kernel ψ alternates correctly between q_θ and p_θ.

2. **Robustness to Delay Distribution Shift:** Train proposed method on wide delay distribution (e.g., U{0,20}) and evaluate performance on fixed delay (e.g., constant 0 or constant 20) to directly test generalization claim.

3. **Visualization of Belief Degradation:** Train model under varying delay conditions and visualize reconstructed observations from belief state φ_t. Systematically increase maximum delay and document point where reconstructions become unacceptably blurry or hallucinatory for the task.