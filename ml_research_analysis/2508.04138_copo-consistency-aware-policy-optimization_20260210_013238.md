---
ver: rpa2
title: 'COPO: Consistency-Aware Policy Optimization'
arxiv_id: '2508.04138'
source_url: https://arxiv.org/abs/2508.04138
tags:
- global
- optimization
- grpo
- copo
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses a critical challenge in reinforcement learning
  for Large Language Models: when multiple responses to a single prompt yield identical
  outcomes, group-based advantage estimation collapses to zero, leading to vanishing
  gradients and training inefficiency. To solve this, the authors propose COPO, a
  consistency-aware policy optimization framework that introduces a structured global
  reward based on outcome consistency and an entropy-based soft blending mechanism.'
---

# COPO: Consistency-Aware Policy Optimization

## Quick Facts
- arXiv ID: 2508.04138
- Source URL: https://arxiv.org/abs/2508.04138
- Reference count: 2
- Primary result: COPO improves Qwen2.5-Instruct 7B MATH-500 accuracy by 2.22% over GRPO baseline

## Executive Summary
COPO addresses a critical limitation in group-relative policy optimization (GRPO) for LLM fine-tuning: when multiple responses to a prompt yield identical outcomes, advantage estimation collapses to zero, causing training stagnation. The method introduces a consistency-aware framework that blends local group-relative optimization with global batch-level optimization weighted by response entropy. This ensures meaningful learning signals even when intra-group consistency is high, enabling stable training across diverse sample difficulties.

## Method Summary
COPO builds on GRPO by adding two key components: (1) global advantage estimation via batch-level reward standardization, and (2) consistency entropy-based soft blending between local and global objectives. For each prompt, COPO extracts answers from multiple responses, computes local rewards, then calculates consistency entropy from the answer distribution. The blending weight determines the relative contribution of local vs global optimization, with global optimization dominating when responses are highly consistent. The method is evaluated on mathematical reasoning benchmarks using Qwen2.5-Instruct models.

## Key Results
- COPO improves Qwen2.5-Instruct 7B MATH-500 accuracy by 2.22% (65.8% vs 63.58%) over GRPO
- AIME24 performance increases by 0.99% (13.85% vs 12.86%) compared to GRPO
- Ablation studies confirm global optimization alone provides 3.05% improvement over baseline
- Performance gains are consistent across majority voting metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inter-group global optimization provides non-zero gradients when intra-group advantages collapse.
- Mechanism: When all G responses yield identical outcomes, local advantage = 0. COPO computes prompt-level global reward, then standardizes it against batch distribution to yield non-zero advantages.
- Core assumption: Prompts with different difficulty levels coexist in each batch.
- Evidence anchors: [abstract] ensures meaningful signals even with high intra-group consistency; [Preliminary] GRPO gradient vanishes as variance → 0.

### Mechanism 2
- Claim: Consistency entropy adaptively routes optimization between local and global objectives.
- Mechanism: Entropy over outcome distributions determines blending weight - low entropy favors global, high entropy favors local optimization.
- Core assumption: Entropy correlates with local optimization reliability.
- Evidence anchors: [COPO section] describes entropy-driven dominance switching; [Table 4] shows γ=20, ρ=1.5 achieves best performance.

### Mechanism 3
- Claim: Fully incorrect samples provide global learning signals despite zero local advantage.
- Mechanism: Samples where all responses are incorrect receive w_local=0, forcing global optimization with negative rewards.
- Core assumption: Incorrect trajectories carry information about what to avoid.
- Evidence anchors: [Ablation] GO-Selective improves by 3.05% over baseline; [Table 5] COPO-Selective underperforms by 1.2% mean@8.

## Foundational Learning

- **Group-Relative Advantage (GRPO)**
  - Why needed: COPO is built on GRPO's foundation. Understand that GRPO computes advantages as standardized deviations from group mean.
  - Quick check: For 6 responses with rewards [1,1,1,1,1,1], what is GRPO advantage? (Answer: undefined/zero)

- **Rule-based Rewards for Reasoning**
  - Why needed: COPO uses binary outcome correctness as the sole reward signal. Understanding this simplification clarifies why variance collapse occurs.
  - Quick check: Why might rule-based rewards reduce "reward hacking" compared to learned reward models?

- **Entropy as Uncertainty Quantification**
  - Why needed: COPO uses Shannon entropy over outcome distributions to measure response consistency.
  - Quick check: For 6 responses with answers [2,2,2,3,3,4], calculate H(q). (Answer: ≈1.46)

## Architecture Onboarding

- **Component map**:
  Policy Model → Generate G responses → Answer Extraction → Local Rewards → ┌─────────────┬────────────────┐
                                                                         │ Local Path │ Global Path │
                                                                         │ A^local    │ A^global     │
                                                                         └─────────────┴────────────────┘
                                                                         ↓                    ↓
                                                                         Consistency Entropy ← Answer distribution
                                                                         ↓
                                                                         Soft Blending w_local
                                                                         ↓
                                                                         Combined Loss → Policy Update

- **Critical path**: Entropy calculation → blending weight determination → loss combination must be computed per-prompt before batch aggregation.

- **Design tradeoffs**:
  - Higher γ: More binary switching between local/global
  - Higher ρ: More prompts assigned to global optimization
  - Larger G: More stable entropy estimation but higher inference cost
  - Paper recommends γ=20, ρ=1.5 for 3B models

- **Failure signatures**:
  - Training stagnation with >50% samples showing zero local advantage
  - Entropy values clustering near 0 or log(G)
  - Global rewards collapsing to uniform distribution
  - Small models (<1.5B) failing to maintain output format

- **First 3 experiments**:
  1. Baseline GRPO with logging: Monitor zero-advantage sample fraction to confirm the problem exists
  2. GO-Only ablation: Set w_local=0 for all samples to isolate global optimization behavior
  3. Entropy distribution analysis: Compute H(q) across dataset using frozen policy to check diversity

## Open Questions the Paper Calls Out

- **Performance degradation on small models**: COPO underperforms GRPO on 1.5B models by ~1%. The paper hypothesizes conflicting gradients or capacity constraints but doesn't validate the root cause.

- **Cross-domain generalization**: While claiming "general applicability," all experiments are on mathematical reasoning. The entropy mechanism's effectiveness on non-binary outcome domains remains untested.

- **Cold-start strategies for base models**: RL fails to maintain formatting on small base models (<1.5B). The paper identifies the failure but doesn't propose curriculum learning or warm-up strategies.

## Limitations

- Performance gains are concentrated on mathematical reasoning benchmarks with binary outcomes
- Answer extraction process from chain-of-thought responses is not detailed
- Global reward standardization assumes sufficient inter-prompt variance in each batch
- Format degradation occurs for small models (<1.5B), limiting applicability

## Confidence

- **High confidence**: Vanishing gradient problem in GRPO is well-documented; experimental improvements on MATH-500/AIME24 are significant
- **Medium confidence**: Entropy weighting mechanism is theoretically justified but relies on assumptions that may not generalize
- **Low confidence**: Exact implementation details of answer extraction are unspecified; optimal hyperparameters presented as dataset-specific

## Next Checks

1. **Gradient flow validation**: Instrument baseline GRPO to log fraction of zero local advantage prompts per step. Verify COPO maintains non-zero gradients on these samples through global loss.

2. **Entropy sensitivity analysis**: Systematically vary γ and ρ across wider range on MATH-500 subset. Plot performance vs hyperparameters to verify robustness.

3. **Cross-task generalization**: Implement COPO on non-mathematical RLHF task (e.g., summarization). Compare performance to GRPO and test consistency entropy mechanism effectiveness on less binary outcomes.