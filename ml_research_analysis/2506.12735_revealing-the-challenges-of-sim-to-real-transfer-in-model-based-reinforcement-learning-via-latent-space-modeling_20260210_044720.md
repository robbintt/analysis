---
ver: rpa2
title: Revealing the Challenges of Sim-to-Real Transfer in Model-Based Reinforcement
  Learning via Latent Space Modeling
arxiv_id: '2506.12735'
source_url: https://arxiv.org/abs/2506.12735
tags:
- environment
- latent
- space
- real
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates challenges of sim-to-real transfer in model-based
  reinforcement learning (MBRL). The authors propose a latent space based method to
  quantify and mitigate the sim-to-real gap by learning mappings from both simulation
  and real environment observations into a shared latent space.
---

# Revealing the Challenges of Sim-to-Real Transfer in Model-Based Reinforcement Learning via Latent Space Modeling

## Quick Facts
- arXiv ID: 2506.12735
- Source URL: https://arxiv.org/abs/2506.12735
- Reference count: 36
- Key outcome: Latent space method improves moderate sim-to-real transfer but struggles with large gaps; cross-domain mapping reveals partial gap quantification

## Executive Summary
This paper investigates the challenges of sim-to-real transfer in model-based reinforcement learning (MBRL) by proposing a latent space modeling approach. The authors learn mappings from both simulation and real environment observations into a shared latent space, enabling explicit modeling of cross-domain dynamic discrepancies. Their method uses environment models trained in each domain to initialize a latent dynamics model, then jointly optimizes auto-encoding, prediction, and cross-domain alignment objectives. Experiments in MuJoCo HalfCheetah-v2 with various perturbations reveal that policies trained in simulation degrade significantly when transferred to perturbed environments, while the latent space method shows better performance under small perturbations but struggles with large gaps.

## Method Summary
The approach extends MBPO by adding latent space modeling components. Two encoders (p_sim, p_real) project states from simulation and real environments into a common latent space Z̄. The latent space is trained via three objectives: (1) model prediction accuracy for transitions and rewards, (2) auto-encoding fidelity to reconstruct original states, and (3) cross-domain alignment via mapping function m. The policy is trained using k-step branched rollouts in latent space, initialized from real-environment data. The cross-domain mapping function m is initialized as identity and provides a quantitative proxy for the sim-to-real gap magnitude through the distance between m ∘ p_real(o) and p_real(o).

## Key Results
- Policies trained in standard HalfCheetah simulation degrade significantly when transferred to environments with perturbed dynamics (gravity, torso length, thigh length)
- Latent space methods outperform non-latent baselines under moderate perturbations (1.05-1.1×) but struggle with large gaps (2×)
- The cross-domain mapping distance generally increases with perturbation magnitude, suggesting it can partially measure the sim-to-real gap
- Analysis reveals challenges including model imperfection, latent representation shifts across domains, and difficulty in information exchange between simulation and real environment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mapping observations from simulation and real environments into a shared latent space enables explicit modeling of cross-domain dynamic discrepancies.
- Mechanism: Two encoder functions (p_sim, p_real) project states from each environment into a common latent space Z̄. The latent space is trained via three objectives: (1) model prediction accuracy for transitions and rewards, (2) auto-encoding fidelity to reconstruct original states, and (3) cross-domain alignment via mapping function m. This forces the model to learn where observations differ in dynamics versus where they share structure.
- Core assumption: The sim-to-real gap manifests as partially observable dynamics—same observations may have different transition behaviors across domains, which can be disentangled in a higher-dimensional latent representation.
- Evidence anchors:
  - [abstract]: "maps states from both simulation and real environments into a shared latent space, enabling explicit modeling of cross-domain discrepancies"
  - [section 4.1]: "For the same observed state in the simulation and the real environment, we distinguish them in the latent space because of their different transition dynamics"
  - [corpus]: Weak direct validation; neighbor papers focus on domain randomization and zero-shot transfer, not latent space modeling for gap quantification.
- Break condition: If simulation and real environments have fundamentally incompatible observation semantics (e.g., different sensor modalities), the shared latent space may fail to find meaningful correspondences.

### Mechanism 2
- Claim: The cross-domain mapping function m provides a quantitative proxy for the sim-to-real gap magnitude.
- Mechanism: Initialize m as identity mapping. During training, the distance between m ∘ p_real(o) and p_real(o) (or equivalently p_sim(o)) reflects how much the latent representations diverge for identical observations. Larger divergence indicates larger dynamic discrepancy between environments.
- Core assumption: The learned mapping distance correlates monotonically with underlying dynamic differences, not just optimization artifacts.
- Evidence anchors:
  - [section 5.3]: Tables 4 and 5 show Euclidean distance between m ∘ p_real(o) and p_real(o) generally increases with perturbation magnitude (e.g., gravity 2× shows 20.7380 vs. 0.3589 at 1.05×).
  - [section 6.3]: KL divergence ratios in Table 6 show latent space samples become more separated as perturbation increases.
  - [corpus]: No direct validation; corpus papers use different gap measurement approaches (trajectory matching, domain randomization breadth).
- Break condition: If the latent model overfits to one environment's data distribution, m may reflect optimization bias rather than true dynamic gap.

### Mechanism 3
- Claim: Short-horizon latent rollouts combined with real-environment initialization reduce negative transfer under moderate perturbations.
- Mechanism: Initialize latent dynamics model using offline real-environment data, then augment with k-step branched rollouts in latent space during policy optimization. This provides exploration beyond offline data while limiting error accumulation from imperfect models.
- Core assumption: The initialized latent model captures sufficient real-environment dynamics to guide policy improvement before simulation data introduces bias.
- Evidence anchors:
  - [section 5.2]: Under moderate perturbations (1.05×, 1.1×), latent space methods outperform non-latent baselines (e.g., Table 2 gravity 1.05×: 19653 vs. 21331 combined return without latent space—actually shows mixed results; latent space helps more at larger perturbations).
  - [section 4.2]: "optimizing J̄(π) is equivalent to maximizing the true return in both environments, up to a change in initial state distribution"
  - [corpus]: Consistent with MBPO-style approaches in neighbor papers using short rollouts, but no corpus papers validate latent-space augmentation specifically.
- Break condition: Under large perturbations (2×), simulation data may overwhelm real-environment signal, causing policy to prioritize simulation performance over real performance.

## Foundational Learning

- Concept: **Partially Observable MDP (POMDP)**
  - Why needed here: The paper frames sim-to-real transfer as a POMDP where the observation doesn't capture environment-specific dynamics (e.g., friction differences). Understanding POMDP structure explains why latent augmentation is proposed.
  - Quick check question: Can you explain why two environments with identical observation spaces might still form a POMDP when jointly considered?

- Concept: **Model-Based Policy Optimization (MBPO) architecture**
  - Why needed here: The method extends MBPO by adding latent space modeling. Understanding MBPO's branched rollout strategy and model ensemble approach is prerequisite to following Algorithm 1.
  - Quick check question: How does MBPO limit error accumulation from imperfect dynamics models during training?

- Concept: **Transfer learning dynamics shift**
  - Why needed here: The paper categorizes transfer scenarios by dynamics heterogeneity (transition probability or reward function shifts). This taxonomy frames why sim-to-real is particularly challenging for model-based methods.
  - Quick check question: Why are model-based RL methods more sensitive to dynamics shifts than model-free methods?

## Architecture Onboarding

- Component map:
  - **p_sim, p_real**: Encoders mapping environment states → latent states
  - **q_sim, q_real**: Decoders reconstructing original states from latent
  - **m**: Cross-domain mapping function (real latent → simulation latent)
  - **P̄_M, R̄_M**: Latent space transition and reward models
  - **π_φ**: Policy trained in latent space using SAC backbone

- Critical path:
  1. Pre-train single-environment model using offline real data or simulation data
  2. Initialize latent space models from pre-trained components
  3. Joint training: update encoders, decoders, m, and dynamics models using mixed-domain data
  4. Policy optimization: k-step latent rollouts from real-buffer states → SAC updates
  5. Evaluation: map environment observations through p_sim/p_real before policy execution

- Design tradeoffs:
  - Simulation-initiated vs. real-initiated latent model: Real-initiated provides better real-environment grounding but requires sufficient offline data
  - Lightweight m network vs. p_sim ∘ q_real: Separate m improves stability but adds parameters
  - Perturbation magnitude: Moderate gaps benefit from latent alignment; large gaps may cause negative transfer

- Failure signatures:
  - Policy excels in simulation but degrades in real environment → likely simulation data overwhelming real signal
  - m remains near identity despite known dynamics differences → latent space not learning cross-domain structure
  - High KL divergence ratio (latent vs. original space) → model treating domains as separate rather than finding correspondences

- First 3 experiments:
  1. Replicate Table 1 baseline: Train MBPO in standard HalfCheetah, evaluate under gravity/torso/thigh perturbations to quantify direct transfer degradation.
  2. Ablate latent components: Compare (a) full latent method, (b) no cross-domain mapping m, (c) no auto-encoding objectives to isolate contribution of each component.
  3. Probe m interpretability: Visualize latent space embeddings for matched observations across environments; verify that m divergence correlates with perturbation magnitude as claimed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can model-based methods discover cross-domain dynamic equivalences (e.g., that normal posture in standard gravity corresponds to low-center-of-gravity posture in high gravity), rather than simply separating samples from simulation and real environments?
- Basis in paper: [explicit] The authors state in Section 6.3 that "the model often finds it difficult to discover the association between samples from different environments... but tends to separate samples from the two environments," supported by KL divergence analysis showing samples become more scattered in latent space than original space.
- Why unresolved: Current latent space approaches successfully distinguish same observations across environments but fail to identify functional equivalences between different observations that share dynamic properties.
- What evidence would resolve it: Demonstrating that latent representations cluster functionally equivalent state-action pairs from different environments together, with corresponding policy transfer improvements.

### Open Question 2
- Question: How can environment models be improved to generalize beyond the training distribution when transferred to environments with different dynamics?
- Basis in paper: [explicit] Section 6.1 identifies that models are "locally good"—accurate near current replay buffer distribution but worse than random models outside—causing transferred models to explore poorly-performing regions in new environments.
- Why unresolved: The fundamental representation limitation of learned dynamics models creates a distribution shift problem when policy-dependent state distributions change across environments.
- What evidence would resolve it: Model prediction accuracy metrics evaluated on out-of-distribution states from perturbed environments showing maintained or improved performance compared to baseline.

### Open Question 3
- Question: Can a unified environment model across simulation and real environments be fundamentally achieved given the latent representation shift phenomenon?
- Basis in paper: [explicit] Section 6.2 concludes that "attempts to build a unified environment model in model-based methods may be fundamentally limited" because "the meaning of 'representation' in simulation and real environment may be completely different for inputs that look the same."
- Why unresolved: The semantic gap between observationally identical states with different dynamic contexts suggests fundamental barriers to unified modeling.
- What evidence would resolve it: Analysis showing whether context-aware encoders or auxiliary tasks can align latent representations across domains while preserving predictive accuracy for domain-specific dynamics.

## Limitations
- Lack of detailed architectural specifications for encoder, decoder, and mapping networks
- Absence of reported hyperparameters for loss weighting and latent space dimensionality
- Evaluation on only one MuJoCo environment with specific perturbations
- Use of D4RL data as a proxy for real-world data without validation on actual physical systems

## Confidence

**High Confidence:** The claim that simulation-to-real transfer degrades performance under perturbations is well-supported by experimental results showing consistent return reductions across gravity, torso length, and thigh length modifications.

**Medium Confidence:** The claim that latent space methods provide better performance under moderate perturbations (1.05-1.1×) is supported by experimental data, though results are mixed and the method struggles with larger gaps.

**Low Confidence:** The claim that the cross-domain mapping distance provides a reliable quantitative proxy for the sim-to-real gap magnitude lacks rigorous validation. While correlations exist in Table 4/5, no ablation studies confirm this isn't an artifact of the training process.

## Next Checks

1. **Ablation study on mapping network m:** Remove the cross-domain mapping component and retrain to verify whether observed performance improvements stem from latent space modeling itself or specifically from the alignment mechanism.

2. **Correlation validation:** Systematically vary perturbation magnitudes and measure both performance degradation and m-distance divergence to establish whether the claimed monotonic relationship holds across the full perturbation spectrum.

3. **Architecture sensitivity analysis:** Implement alternative latent space architectures (e.g., variational autoencoders, contrastive learning approaches) to determine whether the specific design choices significantly impact the method's effectiveness.