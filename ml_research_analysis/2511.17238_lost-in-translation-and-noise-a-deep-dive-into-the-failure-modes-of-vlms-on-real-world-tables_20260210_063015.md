---
ver: rpa2
title: 'Lost in Translation and Noise: A Deep Dive into the Failure Modes of VLMs
  on Real-World Tables'
arxiv_id: '2511.17238'
source_url: https://arxiv.org/abs/2511.17238
tags:
- table
- reasoning
- pairs
- tables
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MirageTVQA, a new large-scale benchmark designed
  to evaluate vision-language models (VLMs) on table reasoning under realistic conditions.
  Unlike previous benchmarks that use clean, English-only tables, MirageTVQA features
  nearly 60,000 QA pairs across 24 languages and includes visually noisy table images
  to simulate real-world document conditions.
---

# Lost in Translation and Noise: A Deep Dive into the Failure Modes of VLMs on Real-World Tables

## Quick Facts
- **arXiv ID:** 2511.17238
- **Source URL:** https://arxiv.org/abs/2511.17238
- **Reference count:** 18
- **Primary result:** VLMs experience >35% performance drop with visual noise and exhibit English-first bias on multilingual tables

## Executive Summary
This paper introduces MirageTVQA, a new large-scale benchmark designed to evaluate vision-language models (VLMs) on table reasoning under realistic conditions. Unlike previous benchmarks that use clean, English-only tables, MirageTVQA features nearly 60,000 QA pairs across 24 languages and includes visually noisy table images to simulate real-world document conditions. The benchmark is constructed using a pipeline that translates and renders tables from multiple sources, then generates and validates question-answer pairs with human annotators and LLMs.

The evaluation reveals two major failure modes in current VLMs: a severe performance drop—over 35%—when faced with visual noise, and a consistent English-first bias where reasoning abilities fail to transfer to other languages. These findings highlight the need for more robust, multilingual, and visually resilient models for real-world table understanding.

## Method Summary
The MirageTVQA benchmark is constructed through a pipeline that translates tables from multiple sources into 24 languages, renders them as images with varying degrees of visual noise (blur, rotation, compression artifacts), and generates question-answer pairs validated by both human annotators and large language models. The benchmark covers a diverse set of table structures and reasoning tasks, with nearly 60,000 QA pairs in total.

## Key Results
- VLMs experience over 35% performance drop when processing visually noisy tables compared to clean versions
- Consistent English-first bias observed: models perform significantly worse on non-English tables despite strong performance on English-only datasets
- Current VLMs struggle with real-world table understanding that involves both multilingual content and visual noise

## Why This Works (Mechanism)
The benchmark's design directly exposes the limitations of current VLMs by creating realistic conditions that differ from the clean, English-centric datasets these models were trained on. The combination of multilingual content and visual noise creates compound challenges that reveal fundamental weaknesses in both the visual processing and language understanding capabilities of existing models.

## Foundational Learning
- **Multilingual table understanding**: Essential for evaluating cross-language reasoning transfer; quick check: compare model performance across different language pairs
- **Visual noise resilience**: Critical for real-world applicability; quick check: measure performance degradation with increasing noise levels
- **Table structure comprehension**: Necessary for accurate reasoning; quick check: evaluate on tables with varying layouts and complexity
- **Vision-language integration**: Fundamental for combining visual and textual understanding; quick check: analyze errors from visual vs. language processing failures
- **Cross-modal reasoning**: Required for answering questions that require both visual and textual information; quick check: test on questions requiring multiple modalities

## Architecture Onboarding
**Component Map:** Table Rendering -> Visual Noise Injection -> QA Generation -> Human Validation -> Model Evaluation

**Critical Path:** Table data → Multilingual translation → Image rendering with noise → Question generation → Answer validation → VLM evaluation

**Design Tradeoffs:** The benchmark prioritizes realism over controlled conditions, introducing visual noise that may not be uniformly distributed and may affect different table structures differently.

**Failure Signatures:** Performance degradation manifests as both reduced accuracy and increased variance across runs, with the most severe drops occurring for complex reasoning questions on visually degraded tables.

**First Experiments:**
1. Evaluate VLMs on clean vs. noisy versions of identical tables with same questions
2. Compare performance across languages for same table content and question types
3. Analyze error patterns for different types of visual noise (blur, rotation, compression)

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark evaluation based on specific VLM models, results may vary with different or updated models
- Focus on table understanding may not generalize to other visual reasoning tasks
- Visual noise types and languages covered are limited to those included in the benchmark

## Confidence
**High:** The two main claims—that VLMs experience a substantial performance drop with visual noise and exhibit an English-first bias—are directly supported by the reported evaluation results. The reported >35% drop in performance with visual noise is a precise quantitative claim, and the assertion of an English-first bias is clearly grounded in the benchmark results.

**Medium:** The claim that these findings "highlight the need for more robust, multilingual, and visually resilient models" is a logical inference from the results, but it is more of a forward-looking implication than a direct empirical finding.

**Medium:** The characterization of the benchmark as "large-scale" and the assertion that it "features nearly 60,000 QA pairs across 24 languages" is based on the paper's own description of the dataset.

## Next Checks
1. Replicate the benchmark evaluation with a broader set of VLMs, including the latest publicly available models and private state-of-the-art systems, to confirm the robustness and generalizability of the observed failure modes.
2. Conduct a detailed error analysis on a subset of the benchmark to identify which types of visual noise (e.g., blur, rotation, compression artifacts) are most detrimental, and whether certain languages or table structures are more affected than others.
3. Evaluate the same VLMs on a subset of tables rendered in both noisy and clean versions, using the same questions, to directly quantify the impact of visual noise on performance and isolate it from other factors such as language or question complexity.