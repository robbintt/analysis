---
ver: rpa2
title: 'QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction
  in Catalytic Adsorption'
arxiv_id: '2512.20084'
source_url: https://arxiv.org/abs/2512.20084
tags:
- prediction
- qe-catalytic
- text
- graph
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes QE-Catalytic, a multimodal framework that deeply
  couples an E(3)-equivariant graph Transformer with a large language model for relaxed-energy
  prediction in catalytic adsorption. The method integrates three-dimensional atomic
  coordinates and structured configuration text within a unified backbone, allowing
  the model to function as a high-performance text-based predictor when precise coordinates
  are unavailable, and to autoregressively generate CIF files for target-energy-driven
  structure design.
---

# QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption

## Quick Facts
- arXiv ID: 2512.20084
- Source URL: https://arxiv.org/abs/2512.20084
- Reference count: 40
- Primary result: Reduces MAE of relaxed adsorption energy from 0.713 eV to 0.486 eV

## Executive Summary
QE-Catalytic introduces a multimodal framework that integrates three-dimensional atomic coordinates and structured configuration text within a unified backbone for relaxed-energy prediction in catalytic adsorption. The method employs Equiformer-V2 as the geometric encoder and Qwen2.5-VL as the language backbone, with graph-text alignment training to inject geometric information into the language channel. This enables high-performance text-based prediction when precise coordinates are unavailable and allows autoregressive generation of CIF files for inverse design. Experimental results on OC20 and OC20-Dense demonstrate consistent outperformance over baseline models, achieving significant reductions in MAE across multiple evaluation protocols.

## Method Summary
QE-Catalytic uses a three-stage training pipeline: (1) freeze the LLM and train the Equiformer-V2 encoder with a contrastive alignment loss to project geometric embeddings into the language embedding space, (2) jointly pretrain all parameters on graph-text paired data using a Max–Min tanh-gated multitask loss, and (3) fine-tune the LLM with frozen geometric encoder on instruction-based prompts. The model employs dual output heads for regression and autoregressive generation, averaged for final predictions. The geometric encoder provides E(3)-equivariant representations that are max-pooled and linearly projected to match the LLM embedding dimension, enabling cross-modal alignment and text-only inference capability.

## Key Results
- Reduces MAE of relaxed adsorption energy from 0.713 eV (CatBERTa) to 0.486 eV
- Text-only inference (QE-Catalytic*) achieves 0.584 eV MAE, outperforming CatBERTa by 18%
- Dual-head architecture improves R² from 0.894 (regression-only) to 0.931

## Why This Works (Mechanism)

### Mechanism 1
Cross-modal contrastive alignment injects geometric priors into the language channel, enabling text-only inference to outperform purely text-trained baselines. Equiformer-V2 extracts E(3)-equivariant graph embeddings that are projected to LLM embedding space and aligned via contrastive loss during Stage 1, forcing text representations to encode geometric distinctions. The core assumption is that the projection preserves enough geometric information after pooling and linear mapping to improve text-only discrimination of near-degenerate configurations.

### Mechanism 2
Max–Min tanh-gated loss prevents one task from dominating gradients, improving joint regression–generation optimization. The loss uses L_max = max(L_MAE, L_CE) as the dominant term and gates with 1 − λ·tanh(L_min), adaptively focusing on the harder subtask while maintaining bounded influence from the easier one. The core assumption is that task difficulty correlates with loss magnitude and adaptive gating yields better multitask balance than fixed λ weighting.

### Mechanism 3
Dual-head output provides complementary supervision, improving prediction stability and extrapolation. The regression head directly predicts energy while the autoregressive head generates energy tokens; their outputs are averaged, combining continuous regression signals with discrete language priors. The core assumption is that averaging predictions from fundamentally different heads yields regularization benefits rather than introducing noise.

## Foundational Learning

- **E(3)-equivariant representations**
  - Why needed here: Equiformer-V2 uses irreducible representations to ensure rotation/translation equivariance; the LLM must consume projected geometric embeddings that respect these symmetries
  - Quick check question: Can you explain why a rotation of the 3D structure should produce the same energy prediction?

- **Contrastive cross-modal alignment**
  - Why needed here: Stage 1 aligns graph and text embeddings via contrastive learning; understanding InfoNCE-style objectives is prerequisite to debugging alignment quality
  - Quick check question: What does a diagonal structure in the graph–text similarity matrix indicate?

- **Autoregressive generation with structured output**
  - Why needed here: QE-Catalytic generates CIF files autoregressively; understanding tokenization and stopping criteria is essential for inverse design
  - Quick check question: Why might generating physically valid CIFs require post-hoc filtering?

## Architecture Onboarding

- **Component map:** Equiformer-V2 (geometric encoder) -> linear projection -> Qwen2.5-VL (LLM backbone) -> dual output heads (regression + autoregressive)

- **Critical path:** Stage 1 alignment quality directly determines whether text-only inference can leverage geometric priors. Verify diagonal structure in similarity matrices before proceeding.

- **Design tradeoffs:** Max-pooling atom embeddings loses atom-level detail but enables fixed-size system representation—assumption is that system-level energy can be predicted from pooled features. Three-stage training adds complexity vs end-to-end but stabilizes alignment before joint training. Generated CIFs are "indicative" not physically accurate—requires downstream filtering.

- **Failure signatures:** Text-only MAE approaches CatBERTa levels → alignment failed; check projection layer and contrastive loss convergence. MMTG-Loss oscillates → λ may be too large or loss scales mismatched. CIF generation produces invalid structures → training data may lack sufficient CIF examples or tokenization is flawed.

- **First 3 experiments:**
  1. Validate alignment: Visualize graph–text similarity matrix on held-out samples; confirm diagonal emergence and off-diagonal suppression.
  2. Ablate training stages: Train without Stage 1 alignment; compare text-only MAE to full QE-Catalytic* to quantify alignment contribution.
  3. Stress-test MMTG-Loss: Compare against fixed λ = 0.5 weighting; track convergence curves and final MAE/R² on OC20-Dense.

## Open Questions the Paper Calls Out

### Open Question 1
Can reinforcement-learning-based fine-tuning techniques (such as DPO or GRPO) improve the quality of generated CIF files to be physically valid structures rather than merely "indicative" ones? The Conclusion states future directions include leveraging reinforcement-learning-based fine-tuning techniques such as DPO and GRPO to improve the quality of CIF generation and enable genuinely effective inverse design. This is unresolved because the Method section notes that generated CIFs are currently "indicative," meaning they are chemically plausible but not atomically accurate enough for direct GNN input, necessitating a text-extraction workaround.

### Open Question 2
Can the multimodal framework be extended to jointly predict atomic forces and kinetic barriers alongside adsorption energy? The Conclusion lists integrating energy prediction with kinetics modeling and force-field learning and multitask learning for energy, forces as future directions. This is unresolved because the current model focuses on scalar energy regression and text generation; predicting forces requires equivariant vector outputs, which may necessitate architectural modifications to the output heads.

### Open Question 3
Does incorporating experimental data and literature-derived information improve the model's transferability and reliability under realistic operating conditions? The Conclusion suggests further incorporating experimental data and literature-derived information to improve transferability and reliability under realistic operating conditions. This is unresolved because the model is currently trained and validated exclusively on the OC20 and OC20-Dense datasets, which are derived from DFT calculations and may not capture the noise or non-ideal conditions present in experimental catalysis.

## Limitations

- The central mechanisms lack direct external validation, particularly cross-modal alignment efficacy and Max–Min tanh-gated loss design
- Experimental comparisons primarily against single-modality baselines rather than competing multimodal approaches
- CIF generation capability is described as "indicative" rather than physically accurate, with no systematic evaluation of structural validity
- Three-stage training procedure complexity and reliance on specific pretrained models may limit reproducibility and generalizability

## Confidence

- **High confidence**: Quantitative improvements over CatBERTa (0.584→0.486 eV MAE reduction) and ablation showing dual-head outperforms single-head variants
- **Medium confidence**: Cross-modal alignment mechanism is theoretically sound but lacks external validation; improvements could partially stem from pretraining effects
- **Low confidence**: Max–Min tanh-gated loss claim lacks external validation, and adaptive gating mechanism may not generalize to tasks with vastly different loss scales

## Next Checks

1. **Cross-modal alignment validation**: Visualize the graph-text similarity matrix on held-out samples to verify diagonal structure and off-diagonal suppression, confirming that geometric information is being effectively injected into language representations through contrastive alignment.

2. **Training stage ablation study**: Train a version without Stage 1 alignment (directly joint training from random initialization) and compare text-only MAE to QE-Catalytic* to quantify the actual contribution of the alignment mechanism versus pretraining effects.

3. **Loss function comparison**: Implement and compare MMTG-Loss against a fixed λ=0.5 weighted sum loss across multiple random seeds, tracking convergence curves and final performance to assess whether the adaptive gating provides consistent benefits beyond hyperparameter tuning.