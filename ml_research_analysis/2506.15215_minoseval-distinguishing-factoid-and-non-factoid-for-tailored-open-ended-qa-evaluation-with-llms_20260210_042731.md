---
ver: rpa2
title: 'MinosEval: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended
  QA Evaluation with LLMs'
arxiv_id: '2506.15215'
source_url: https://arxiv.org/abs/2506.15215
tags:
- answer
- question
- evaluation
- factoid
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MinosEval introduces a novel evaluation framework for open-ended
  question answering that addresses the limitations of existing methods by distinguishing
  between factoid and non-factoid questions. For factoid questions, it employs an
  adaptive key-point scoring strategy that extracts key information from reference
  answers and uses natural language inference to assess how well model responses capture
  these points.
---

# MinosEval: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended QA Evaluation with LLMs

## Quick Facts
- **arXiv ID:** 2506.15215
- **Source URL:** https://arxiv.org/abs/2506.15215
- **Reference count:** 26
- **Primary result:** Introduces a bifurcated evaluation framework that distinguishes factoid from non-factoid questions, achieving superior alignment with human annotations compared to traditional metrics and LLM-based methods.

## Executive Summary
MinosEval addresses a fundamental limitation in open-ended QA evaluation by recognizing that factoid and non-factoid questions require fundamentally different assessment approaches. For factoid questions, it employs a key-point scoring strategy that extracts specific facts from reference answers and uses NLI to assess candidate responses. For non-factoid questions, it applies an instance-aware listwise ranking strategy that generates quality-calibrated silver instances to guide LLM ranking. The framework demonstrates improved alignment with human judgments across four datasets, including two self-built Chinese datasets, while providing more interpretable evaluation results.

## Method Summary
The framework first classifies questions as factoid or non-factoid using an LLM-based classifier. Factoid questions are evaluated by extracting key points from reference answers and scoring candidate responses using NLI entailment probabilities. Non-factoid questions are evaluated through listwise ranking, where the LLM generates silver answer instances at five quality levels (Excellent to Bad) that serve as calibration anchors for ranking candidate responses. The method uses GPT-4o for classification and ranking, and mDeBERTa-v3-base-mnli-xnli for NLI scoring.

## Key Results
- Achieves superior Spearman's Rho alignment with human annotations compared to traditional metrics (ROUGE, BERTScore) and baseline LLM methods
- Demonstrates cost-effectiveness relative to specialized factoid and non-factoid evaluation methods
- Provides interpretable evaluation through clear key-point extraction and quality-level calibration
- Shows robustness across both Chinese and English datasets with different response counts

## Why This Works (Mechanism)

### Mechanism 1: Bifurcated Evaluation Strategy
- **Claim:** Tailoring the evaluation logic to the question type (factoid vs. non-factoid) yields higher alignment with human judgment than generic, one-size-fits-all methods.
- **Mechanism:** The system first employs a "Fact Detection" module (LLM-based classifier) to route the sample into one of two pipelines. Factoid questions require strict entity verification, while non-factoid questions require qualitative assessment of coherence and creativity.
- **Core assumption:** The distinction between "factoid" and "non-factoid" is crisp enough for a classifier to resolve, and the optimal evaluation metric differs fundamentally between these classes.
- **Evidence anchors:** [Abstract] "Most notably, existing methods overlook the distinction between factoid and non-factoid questions." [Section 1] "By failing to account for this distinction, such a one-size-fits-all strategy undermines the precision and effectiveness of evaluation."

### Mechanism 2: Key-Point Atomic Scoring (Factoid)
- **Claim:** Decomposing a reference answer into atomic "key points" and measuring entailment against these points is more robust than comparing full-text semantic similarity.
- **Mechanism:** An LLM extracts a list of specific key points from the reference. An NLI model then calculates the probability that the *model response* (acting as the premise) entails each *key point* (acting as the hypothesis). The final score is the average entailment minus contradiction.
- **Core assumption:** An NLI model can accurately judge the truth value of specific facts within a longer text without being distracted by style or fluency.
- **Evidence anchors:** [Section 3.2.2] "It is modeled as a Natural Language Inference (NLI) task... `NLI(ri, kj) = se_ij - sc_ij`." [Section B.5] "Using model outputs as premises rather than key points... ensures that NLI labels reflect meaningful logical relationships grounded in complete information."

### Mechanism 3: Instance-Aware Calibration (Non-Factoid)
- **Claim:** LLMs perform better at ranking open-ended responses when calibrated against generated "silver instances" of known quality levels.
- **Mechanism:** Instead of asking the LLM to rank raw candidate responses blindly, the system generates synthetic answers at five distinct quality levels (Excellent to Bad) based on the reference. These "silver instances" are inserted into the list to be ranked, serving as an in-context rubric.
- **Core assumption:** The LLM generating the silver instances can reliably produce distinct quality gradations, and the ranking LLM uses these as anchors to normalize its judgment.
- **Evidence anchors:** [Section 3.2.3] "We use LLMs to automatically generate silver instances... using them to enhance the performance of listwise ranking." [Table B1] Ablation studies show performance drops when specific strategies are removed.

## Foundational Learning

- **Concept: Natural Language Inference (NLI)**
  - **Why needed here:** MinosEval relies on NLI not just for semantic similarity, but specifically for "Entailment" and "Contradiction" probabilities to score factoid accuracy.
  - **Quick check question:** Given the response "The event happened in 1999" and the key point "The event occurred in the late 20th century," does entailment hold?

- **Concept: Listwise Ranking vs. Pointwise Scoring**
  - **Why needed here:** The paper contrasts scoring individual responses (Pointwise) against ranking a set of responses (Listwise). MinosEval uses Listwise for non-factoid questions to handle subjectivity.
  - **Quick check question:** Why might a model give a "Good" score to two contradictory answers in a pointwise setting, and how does listwise ranking prevent this?

- **Concept: Factoid vs. Non-Factoid Taxonomy**
  - **Why needed here:** This is the architectural pivot of the system. Understanding that "creative freedom" vs. "entity constraints" defines the split is essential.
  - **Quick check question:** Is the question "Summarize the plot of Hamlet" factoid or non-factoid according to the paper's definition? (Hint: It depends on whether strict factual accuracy or creative synthesis is the primary constraint).

## Architecture Onboarding

- **Component map:** Input (Q, A, R) → Router (Fact Detector) → Pipeline A (Key-Point Extractor → NLI Model → Score Aggregator) OR Pipeline B (Silver Instance Generator → LLM Ranker) → Output (Ranked List)

- **Critical path:**
  1. **Classification Accuracy:** The Router is the single point of failure. A misclassification forces the wrong metric pipeline (e.g., trying to find facts in a poem).
  2. **NLI Premise-Hypothesis Setup:** For Factoid QA, ensure the *Candidate Response* is the Premise and the *Key Point* is the Hypothesis. Reversing this breaks the logic (see Appendix B.5).

- **Design tradeoffs:**
  - **Cost vs. Consistency:** MinosEval is more expensive than simple ROUGE or single-prompt LLM evaluation because it requires multiple inference steps (Extraction + NLI or Generation + Ranking).
  - **Robustness vs. Latency:** Generating 5 silver instances for every non-factoid question introduces significant latency compared to direct ranking.

- **Failure signatures:**
  - **Tie-breaking loops:** If the NLI scores are identical for distinct responses, the system needs a robust tie-breaking logic (mentioned in Appendix B.2).
  - **Hallucinated Silver Standards:** If the Generator creates a "Bad" silver instance that is accidentally better than the "Fair" instance, the ranking logic destabilizes.

- **First 3 experiments:**
  1. **Router Validation:** Run the classifier on a held-out set of clearly factoid/non-factoid questions to establish the baseline error rate of the routing mechanism.
  2. **NLI Directionality Check:** Test the Factoid pipeline with swapped Premise/Hypothesis inputs to empirically verify the paper's claim in Appendix B.5 regarding directionality bias.
  3. **Silver Instance Quality Audit:** Manually inspect generated silver instances for a few non-factoid samples to ensure the "Excellent" to "Bad" gradation is distinct and logical before running the full ranking pipeline.

## Open Questions the Paper Calls Out
- How can an evaluation framework effectively handle open-ended questions that contain both factoid and non-factoid characteristics without forcing a binary classification?
- To what extent does the performance of MinosEval degrade in highly specialized domains (e.g., legal or medical) when using generalized NLI models compared to specialized models?
- How robust is the "Instance-Aware Listwise Ranking" strategy when the LLM generates low-quality or incoherent silver answer instances?

## Limitations
- Relies on external LLM APIs for multiple inference steps, creating cost and latency concerns
- Requires careful prompt engineering for fact detection and silver instance generation components
- Assumes factoid/non-factoid distinction is binary and clear-cut, which may break down for hybrid questions

## Confidence
- **High:** Core claim that distinguishing factoid/non-factoid improves evaluation accuracy (supported by theoretical reasoning and experimental results)
- **Medium:** NLI-based scoring mechanism (depends heavily on key-point extraction quality and NLI model factual reasoning)
- **Low:** Instance-aware ranking approach (effectiveness depends on quality of generated silver instances)

## Next Checks
1. **Cross-lingual generalization:** Test the system on factoid/non-factoid classification and evaluation accuracy across languages beyond Chinese and English
2. **Routing error impact analysis:** Systematically evaluate how misclassifications affect downstream scores by intentionally introducing classification errors
3. **NLI model robustness:** Test the NLI scoring component with adversarial cases where key points are ambiguous or responses contain plausible but incorrect facts