---
ver: rpa2
title: 'Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large'
arxiv_id: '2505.17059'
source_url: https://arxiv.org/abs/2505.17059
tags:
- medical
- gpt-4
- text
- summarization
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Medalyze is an AI-powered application designed to enhance understanding
  of complex medical texts through three specialized FLAN-T5-Large models: one for
  summarizing medical reports, one for extracting health issues from doctor-patient
  conversations, and one for identifying key questions in medical passages. The system
  was deployed across web and mobile platforms with real-time inference and data synchronization
  via YugabyteDB.'
---

# Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large

## Quick Facts
- arXiv ID: 2505.17059
- Source URL: https://arxiv.org/abs/2505.17059
- Reference count: 40
- Medalyze outperformed GPT-4 on domain-specific medical summarization tasks with higher semantic similarity scores

## Executive Summary
Medalyze is an AI-powered application that enhances understanding of complex medical texts through three specialized FLAN-T5-Large models. The system was deployed across web and mobile platforms with real-time inference and data synchronization via YugabyteDB. Evaluation against GPT-4 showed Medalyze outperformed GPT-4 in domain-specific summarization tasks, achieving higher semantic similarity scores (BERTScore: 0.6533 vs 0.5453; SpaCy Similarity: 0.8413 vs 0.7810) while maintaining low lexical overlap, confirming effective medical information retention through paraphrasing rather than direct copying.

## Method Summary
The approach involves fine-tuning three separate FLAN-T5-Large models on filtered subsets of a Stanford University medical dataset. The models were trained on Intel i5-12400F, NVIDIA Quadro RTX 5000, with 16GB RAM using batch size 2, gradient accumulation 2, learning rate 1e-5, FP16 precision, and weight decay 0.01. The system uses a Flask-based REST API with Docker deployment, storing summaries in YugabyteDB. Models were evaluated using BLEU, ROUGE-L, BERTScore, and SpaCy Similarity metrics, comparing performance against GPT-4 baselines.

## Key Results
- M-Passage achieved BERTScore 0.6533 vs GPT-4's 0.5453 and SpaCy Similarity 0.8413 vs 0.7810
- Low lexical overlap (BLEU 0.0982) confirmed effective paraphrasing rather than direct copying
- Real-time inference achieved with lightweight architecture on modest hardware (16GB RAM, single GPU)

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific fine-tuning on FLAN-T5-Large yields better semantic retention for medical summarization than general-purpose LLMs. The Seq2Seq architecture encodes input sequences into latent representations and decodes them into summaries. Fine-tuning on curated medical corpora aligns token embeddings with clinical terminology, reducing semantic drift while allowing paraphrasing. Core assumption: Medical text summarization benefits more from domain alignment than from raw model scale. Evidence: M-Passage achieved BERTScore 0.6533 vs GPT-4's 0.5453 and SpaCy Similarity 0.8413 vs 0.7810, indicating stronger semantic alignment despite low lexical overlap (BLEU 0.0982). Break condition: If the fine-tuning dataset is too small or lacks diversity, the model may overfit to specific phrasing patterns and fail to generalize to unseen medical subdomains.

### Mechanism 2
Task-specific model specialization enables optimized performance across distinct input types (reports, conversations, questions). Each of the three FLAN-T5-Large models is fine-tuned on a filtered subset of the source dataset aligned to its task objective. M-Passage learns to condense structured findings; M-Conversation filters conversational noise; M-Question isolates interrogative intent. Core assumption: The optimal summarization strategy differs significantly by input genre, and a single unified model would underperform on at least one task. Evidence: M-Conversation showed near-zero BLEU (0.0018) but highest SpaCy Similarity (0.8801), reflecting its extraction-focused design, while M-Passage showed higher lexical retention (BLEU 0.0982) appropriate for report compression. Break condition: If task boundaries overlap significantly (e.g., a passage contains embedded questions), the specialized models may misclassify the appropriate summarization strategy.

### Mechanism 3
Lightweight Seq2Seq models enable real-time inference with competitive accuracy under hardware constraints. FLAN-T5-Large (~780M parameters) uses an encoder-decoder architecture optimized for conditional generation. Combined with FP16 precision, gradient accumulation, and Dockerized deployment, the model achieves low-latency inference on modest hardware (16GB RAM, single GPU). Core assumption: Inference efficiency is critical for user-facing applications, and marginal accuracy gains from larger models do not justify increased latency and infrastructure costs. Evidence: Training was completed on Intel i5-12400F, NVIDIA Quadro RTX 5000, and 16GB RAM with batch size 2 and gradient accumulation. Models are packaged as `.safetensors` for optimized memory usage and deployed via Flask APIs in Docker containers for consistent performance. Break condition: If input sequences exceed the 512-token context window, the model will truncate inputs, potentially losing critical medical information.

## Foundational Learning

- **Concept: Sequence-to-Sequence (Seq2Seq) models**
  - Why needed here: FLAN-T5-Large uses an encoder-decoder architecture; understanding how attention mechanisms compress input sequences into latent representations is essential for debugging summarization quality.
  - Quick check question: Given an input of 200 tokens and a target summary of 30 tokens, which component (encoder or decoder) determines the output length?

- **Concept: Fine-tuning vs. In-Context Learning**
  - Why needed here: Medalyze's performance advantage over GPT-4 stems from weight updates on medical data, not prompt engineering; distinguishing these paradigms clarifies why domain adaptation works.
  - Quick check question: If you change the prompt template for M-Passage, will the model's BERTScore change? Why or why not?

- **Concept: Lexical vs. Semantic Evaluation Metrics**
  - Why needed here: The paper shows low BLEU but high BERTScore, indicating paraphrasing; understanding what each metric captures prevents misinterpreting results.
  - Quick check question: If a generated summary uses synonyms for all medical terms (e.g., "hypertension" → "high blood pressure"), will BLEU increase, decrease, or stay the same?

## Architecture Onboarding

- **Component map:**
  Frontend (Web/Android) -> API Layer (Flask) -> Model Layer (Three FLAN-T5-Large) -> Database (YugabyteDB)

- **Critical path:**
  1. User submits text via web/mobile → API receives POST request
  2. API tokenizes input using T5Tokenizer (512-token limit)
  3. Model generates summary via beam search or greedy decoding
  4. Summary stored in YugabyteDB with UUID
  5. Response returned to frontend for display

- **Design tradeoffs:**
  - **Specialization vs. Maintainability:** Three separate models increase accuracy but require independent versioning and monitoring.
  - **Context Window vs. Input Flexibility:** 512-token limit restricts long documents; M-Conversation handles up to ~3000 words, suggesting preprocessing truncation or chunking (not explicitly documented).
  - **Single-Node DB vs. Scalability:** YugabyteDB is deployed as a single instance; horizontal scaling would require re-architecture.

- **Failure signatures:**
  - **Token truncation:** If input exceeds 512 tokens, later content is silently dropped; summary may miss critical findings.
  - **Model timeout:** Large inputs on limited hardware may exceed API timeout thresholds; monitor `/health` endpoint.
  - **Semantic drift:** Low BLEU scores indicate paraphrasing; verify medical accuracy manually for high-stakes use cases.

- **First 3 experiments:**
  1. **Token length stress test:** Submit inputs of 100, 300, 500, and 700 tokens to M-Passage; measure latency and compare summary completeness to identify truncation threshold.
  2. **Metric correlation analysis:** Generate 50 summaries, compute all four metrics, and correlate BLEU/ROUGE-L with BERTScore to quantify paraphrasing vs. copying behavior.
  3. **Cross-model task misrouting:** Deliberately send conversational input to M-Passage and vice versa; document performance degradation to validate task specialization claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance stability of the M-Passage model be improved across varying text complexities without sacrificing accuracy?
- Basis in paper: The authors observe that the model shows "instability" and "fluctuates" in its scores, suggesting a need to "stabilize M-Passage's performance across different text segments."
- Why unresolved: The paper identifies the instability through visual analysis of score fluctuations but does not propose or test architectural or algorithmic solutions to smooth out performance.
- What evidence would resolve it: A comparative study showing reduced variance in BLEU and BERTScore across short, medium, and long inputs compared to the current baseline.

### Open Question 2
- Question: What specific constraints or fine-tuning techniques are required to minimize semantic drift during medical paraphrasing?
- Basis in paper: Section VII.C explicitly notes that relying on paraphrasing over direct word overlap "introduces potential risks of semantic drift" and suggests "incorporating additional constraints" as a foundation for improvement.
- Why unresolved: While the authors detect the risk of meaning alteration due to low lexical overlap (BLEU), they do not implement or evaluate mechanisms to enforce strict factual consistency.
- What evidence would resolve it: Implementation of a factuality metric or constraint mechanism that demonstrates high semantic retention even when lexical overlap is low.

### Open Question 3
- Question: To what extent do high semantic similarity scores (BERTScore, SpaCy) correlate with factual accuracy and safety in a clinical setting?
- Basis in paper: The evaluation relies solely on automated NLP metrics. The authors acknowledge that the application is not for diagnosis, yet the link between these specific metric scores and verified clinical safety remains unproven.
- Why unresolved: High semantic similarity does not guarantee the absence of hallucinations or critical medical errors, which is a known limitation of automated summarization metrics not addressed by human expert evaluation in the study.
- What evidence would resolve it: A human expert evaluation (e.g., by physicians) assessing the generated summaries for clinical accuracy and potential harm, cross-referenced with the automated metric scores.

## Limitations

- **Dataset provenance and task filtering remain opaque.** The paper references a Stanford dataset but does not disclose the exact filtering logic that produced the three specialized subsets, making faithful reproduction impossible.
- **Context window mismatch with long inputs is unaddressed.** While the paper lists input lengths up to 3,050 words, FLAN-T5-Large has a 512-token context limit, raising concerns about silent information loss for longer clinical narratives.
- **Evaluation metric interpretation lacks clinical validation.** High semantic similarity scores are reported, but no human-in-the-loop assessment confirms that paraphrased summaries retain clinically actionable accuracy.

## Confidence

- **High Confidence:** The architectural deployment (three specialized FLAN-T5-Large models, Flask API, YugabyteDB) and hardware specifications are clearly specified and reproducible.
- **Medium Confidence:** The comparative performance claim over GPT-4 is credible given the reported metric improvements, but reproducibility is limited by missing dataset filtering details and unknown prompt templates.
- **Low Confidence:** The clinical utility and safety of the summaries cannot be guaranteed from reported metrics alone; human validation of paraphrased medical content is essential.

## Next Checks

1. **Reconstruct the task-specific datasets.** Obtain the Stanford source dataset, implement the described filtering heuristics to create three subsets (reports, conversations, questions), and verify their alignment with the task descriptions in the paper.

2. **Audit the token truncation strategy.** Design and execute a controlled experiment varying input lengths (e.g., 100, 300, 500, 700 tokens) through the API; measure summary completeness and latency to identify the effective truncation threshold and document the handling of overflow.

3. **Validate clinical semantic retention.** Generate a test set of 50 medical summaries, compute all four evaluation metrics (BLEU, ROUGE-L, BERTScore, SpaCy Similarity), and correlate metric scores with a small-scale human review by medical professionals to confirm that high semantic similarity corresponds to clinically accurate paraphrasing.