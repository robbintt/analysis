---
ver: rpa2
title: Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit
arxiv_id: '2506.06607'
source_url: https://arxiv.org/abs/2506.06607
tags:
- arxiv
- tokens
- embedding
- performance
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a training-free method for tokenizer transplantation
  in large language models (LLMs) using Orthogonal Matching Pursuit (OMP). The approach
  reconstructs embeddings for out-of-vocabulary tokens by approximating them as sparse
  linear combinations of shared tokens in the donor embedding space, then transferring
  these coefficients to the base model's embedding space.
---

# Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit

## Quick Facts
- arXiv ID: 2506.06607
- Source URL: https://arxiv.org/abs/2506.06607
- Reference count: 40
- Presents training-free method for tokenizer transplantation using OMP

## Executive Summary
This paper introduces a novel training-free approach for tokenizer transplantation in large language models using Orthogonal Matching Pursuit (OMP). The method reconstructs embeddings for out-of-vocabulary tokens by approximating them as sparse linear combinations of shared tokens in the donor embedding space, then transfers these coefficients to the base model's embedding space. The approach achieves superior zero-shot preservation of base model performance across multiple benchmarks compared to existing methods.

The method is evaluated on challenging cross-tokenizer tasks including Llama→Mistral NeMo (12B) and Qwen→Llama (1B), demonstrating significant improvements over baseline approaches like zero-init, mean-init, and existing methods such as WECHSEL, FOCUS, and ZETT. The approach has been integrated into the open-source mergekit-tokensurgeon tool, making it practically accessible for the research community.

## Method Summary
The paper presents a training-free tokenizer transplantation method that leverages Orthogonal Matching Pursuit to reconstruct embeddings for out-of-vocabulary tokens. The approach works by first identifying shared tokens between the donor and base models, then using these shared tokens to approximate the embeddings of out-of-vocabulary tokens in the donor space through sparse linear combinations. These approximation coefficients are then transferred to the base model's embedding space, allowing the reconstruction of embeddings for tokens that were previously out-of-vocabulary. This process preserves the semantic relationships encoded in the original embeddings while adapting them to the target model's vocabulary structure.

## Key Results
- OMP preserves MMLU accuracy at 62.2% versus 58% for naive methods on Llama→Mistral NeMo task
- Maintains better performance on reasoning tasks compared to WECHSEL, FOCUS, and ZETT baselines
- Numerical tokenization mismatches cause severe reasoning performance drops, with GSM8K scores degrading by 73.8% when schemes are mismatched versus 5.6% when aligned

## Why This Works (Mechanism)
The method works by exploiting the linear structure of embedding spaces and the sparsity of token representations. OMP iteratively selects the most correlated shared tokens and computes optimal weights to reconstruct out-of-vocabulary embeddings in the donor space. By preserving the sparse representation structure during transfer, the method maintains the semantic relationships and distributional properties of the original embeddings while adapting them to the target vocabulary.

## Foundational Learning
- Orthogonal Matching Pursuit (OMP): Iterative greedy algorithm for sparse approximation - needed for efficient reconstruction of high-dimensional embeddings - quick check: verify convergence and sparsity levels
- Cross-tokenizer embedding spaces: Understanding how different tokenizers map to shared semantic spaces - needed to identify transferable representations - quick check: measure overlap in shared token distributions
- Sparse linear combinations: Mathematical framework for representing complex tokens as combinations of simpler ones - needed for reconstruction without training - quick check: validate coefficient stability across different initializations

## Architecture Onboarding
Component Map: Shared tokens -> OMP approximation -> Sparse coefficients -> Base embedding space reconstruction
Critical Path: Token overlap detection → OMP coefficient computation → Coefficient transfer → Embedding reconstruction
Design Tradeoffs: Training-free vs. fine-tuning (speed vs. optimality), sparse vs. dense representations (efficiency vs. accuracy), donor vs. base vocabulary alignment (compatibility vs. coverage)
Failure Signatures: Performance degradation on numerical tasks indicates tokenization scheme mismatches, poor OOV reconstruction indicates insufficient shared token coverage
First Experiments: 1) Measure token overlap between donor and base vocabularies, 2) Test OMP reconstruction accuracy on synthetic OOV tokens, 3) Compare zero-shot performance on small-scale cross-tokenizer task

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Scalability concerns for models exceeding 12B parameters remain untested
- Computational efficiency claims of 38-74 seconds may not hold for larger models
- Limited analysis of edge cases with extensive token distribution overlap
- GSM8K performance issues suggest numerical tokenization mismatches may be a fundamental limitation

## Confidence
- Zero-shot preservation claims: Medium confidence (strong benchmark results but limited task diversity)
- Computational efficiency claims: Medium confidence (measured on specific hardware, may not generalize)
- Numerical tokenization impact: High confidence (clear empirical demonstration with GSM8K)
- Scalability claims: Low confidence (no testing beyond 12B parameters)

## Next Checks
1. Test OMP transplantation on models exceeding 30B parameters to verify computational efficiency claims scale appropriately
2. Evaluate long-term stability by fine-tuning transplanted models and measuring catastrophic forgetting or performance drift
3. Conduct ablation studies isolating the impact of numerical tokenization differences from other vocabulary mismatch factors