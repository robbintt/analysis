---
ver: rpa2
title: Automated Hazard Detection in Construction Sites Using Large Language and Vision-Language
  Models
arxiv_id: '2511.15720'
source_url: https://arxiv.org/abs/2511.15720
tags:
- construction
- safety
- page
- hazard
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis proposes a multimodal AI framework combining large
  language models (LLMs) and vision-language models (VLMs) for automated construction
  safety hazard detection. The framework processes unstructured OSHA accident reports
  using GPT-4o-mini for classification and analysis, and analyzes construction site
  images using GPT-4o Vision and open-source VLMs (Molmo 7B and Qwen2 VL 2B) for hazard
  detection.
---

# Automated Hazard Detection in Construction Sites Using Large Language and Vision-Language Models

## Quick Facts
- arXiv ID: 2511.15720
- Source URL: https://arxiv.org/abs/2511.15720
- Authors: Islem Sahraoui
- Reference count: 40
- Primary result: 89% accuracy on OSHA report classification; 72.6% F1 score for Qwen2 VL 2B on safety rule detection

## Executive Summary
This thesis presents a multimodal AI framework that combines large language models and vision-language models to automate construction safety hazard detection. The system processes unstructured OSHA accident reports using GPT-4o-mini for classification and analysis, while simultaneously analyzing construction site images using GPT-4o Vision and open-source VLMs (Molmo 7B and Qwen2 VL 2B) to identify and localize safety hazards. The framework demonstrates that cost-effective, lightweight open-source models can perform competitively with larger commercial alternatives for construction safety monitoring applications.

## Method Summary
The proposed framework employs a two-pronged approach: a textual pipeline that classifies and analyzes unstructured OSHA accident reports using GPT-4o-mini, achieving 89% accuracy across 43 accident categories from 100 reports; and a visual pipeline that processes construction site images using both commercial (GPT-4o Vision) and open-source VLMs (Molmo 7B and Qwen2 VL 2B) for hazard detection and localization. The system specifically targets common construction safety issues including falls, PPE violations, and other hazardous conditions. The framework's architecture allows for seamless integration of textual and visual data streams to provide comprehensive safety monitoring.

## Key Results
- Textual pipeline achieved 89% accuracy in classifying 100 OSHA reports into 43 accident categories
- Qwen2 VL 2B open-source VLM achieved 72.6% F1 score on safety rule detection tasks
- Molmo 7B open-source VLM achieved 67.2% F1 score on safety rule detection tasks
- Framework successfully identified and localized safety hazards including falls and PPE violations in construction site images

## Why This Works (Mechanism)
The framework leverages the complementary strengths of LLMs for textual analysis and VLMs for visual understanding, creating a comprehensive safety monitoring system. GPT-4o-mini's strong language understanding capabilities enable accurate classification of complex OSHA reports with diverse terminology and accident descriptions. The VLMs, particularly the open-source variants, demonstrate sufficient visual reasoning capabilities to detect construction-specific hazards while maintaining computational efficiency. The multimodal approach allows for cross-validation between textual incident reports and visual evidence, enhancing overall detection reliability.

## Foundational Learning
- OSHA accident report classification: Understanding construction safety incident documentation is crucial for proper hazard categorization and prevention strategies. Quick check: Verify classification accuracy across diverse accident types.
- Construction safety hazard types: Knowledge of common construction hazards (falls, PPE violations, equipment operation) is essential for effective detection. Quick check: Ensure model sensitivity to high-frequency hazard categories.
- Vision-language model capabilities: Understanding how VLMs process and integrate visual and textual information is key to interpreting their performance. Quick check: Validate model performance on known VLM benchmarks.
- Multimodal AI integration: Combining textual and visual AI systems requires careful architecture design for seamless data flow. Quick check: Test system performance with mixed input types.
- Open-source model optimization: Leveraging smaller, efficient models requires understanding their performance characteristics and limitations. Quick check: Compare resource usage versus accuracy trade-offs.

## Architecture Onboarding

Component Map:
OSHA reports -> GPT-4o-mini -> Classification results
Construction images -> GPT-4o Vision/Molmo 7B/Qwen2 VL 2B -> Hazard detection and localization
Integration layer -> Combined safety assessment output

Critical Path:
OSHA report input → GPT-4o-mini processing → Category classification → Safety analysis
Construction image input → VLM processing → Hazard detection → Risk assessment
Combined results → Safety monitoring dashboard

Design Tradeoffs:
- Commercial vs. open-source VLMs: Balance between performance and cost/computational requirements
- Model size versus accuracy: Smaller models offer efficiency but may sacrifice some detection capability
- Single versus multimodal approach: Comprehensive monitoring versus simplified, focused detection

Failure Signatures:
- Misclassification of OSHA reports due to ambiguous terminology or incomplete descriptions
- Visual detection failures in poor lighting conditions or occluded hazard scenarios
- Integration errors when combining textual and visual analysis results

First Experiments:
1. Test OSHA report classification accuracy on a diverse set of accident descriptions
2. Evaluate VLM performance on standard computer vision benchmarks for construction safety
3. Assess system robustness across varying lighting and weather conditions in test images

## Open Questions the Paper Calls Out
None

## Limitations
- Textual pipeline evaluated on small dataset (100 reports across 43 categories) may not represent real-world complexity
- Visual pipeline performance not benchmarked against established construction safety computer vision approaches
- Framework generalization across diverse construction site types, cultural contexts, and regional regulations remains unverified

## Confidence

High confidence:
- Feasibility of using open-source VLMs for construction safety monitoring, as demonstrated by competitive performance metrics

Medium confidence:
- Framework's effectiveness in classifying OSHA reports and detecting common hazards like falls and PPE violations
- Cost-effectiveness proposition for using smaller open-source models compared to commercial alternatives

## Next Checks
1. Conduct extensive field testing across multiple construction sites with varied conditions (lighting, weather, site complexity) to evaluate real-world performance and generalization capabilities
2. Benchmark the visual pipeline against established computer vision models and industry-specific safety detection systems using standardized construction safety datasets
3. Perform longitudinal studies to assess model performance consistency over time and its ability to adapt to evolving construction practices and safety regulations