---
ver: rpa2
title: Evaluating the Effectiveness of Large Language Models in Automated News Article
  Summarization
arxiv_id: '2502.17136'
source_url: https://arxiv.org/abs/2502.17136
tags:
- few-shot
- news
- risk
- zero-shot
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models (LLMs) for automated
  news summarization in supply chain risk analysis. Multiple LLMs were tested using
  zero-shot, few-shot, and fine-tuning approaches on a dataset of 1,535 news articles.
---

# Evaluating the Effectiveness of Large Language Models in Automated News Article Summarization

## Quick Facts
- arXiv ID: 2502.17136
- Source URL: https://arxiv.org/abs/2502.17136
- Authors: Lionel Richy Panlap Houamegni; Fatih Gedikli
- Reference count: 23
- Primary Result: Few-Shot GPT-4o mini achieved highest performance in news summarization for supply chain risk analysis

## Executive Summary
This study systematically evaluates large language models for automated news summarization in supply chain risk analysis. The research tested multiple LLMs using zero-shot, few-shot, and fine-tuning approaches on a dataset of 1,535 news articles. Performance was measured through automated metrics (ROUGE, BLEU, BERTScore) and human evaluation. The Few-Shot GPT-4o mini model emerged as the top performer, excelling in summary quality, coherence, and risk identification while maintaining cost-efficiency. Open-source models like LLaMA and Mistral also demonstrated strong capabilities, confirming LLMs' significant potential in enhancing news summarization for risk analysis.

## Method Summary
The study employed a comprehensive evaluation framework testing LLMs across three prompting strategies: zero-shot, few-shot, and fine-tuning. A dataset of 1,535 news articles was used for benchmarking. Models were assessed using multiple automated metrics including ROUGE, BLEU, and BERTScore to measure summary quality and content relevance. Human evaluation supplemented the automated metrics to provide qualitative assessment of summary coherence and risk identification accuracy. The research compared both proprietary and open-source models to evaluate performance differences and cost-efficiency trade-offs.

## Key Results
- Few-Shot GPT-4o mini achieved highest overall performance in summary quality and risk identification
- Open-source models (LLaMA, Mistral) demonstrated competitive capabilities against proprietary alternatives
- LLMs significantly improved news summarization efficiency for supply chain risk analysis applications

## Why This Works (Mechanism)
The effectiveness of LLMs in news summarization stems from their ability to capture contextual relationships and semantic meaning across large text corpora. Few-shot prompting leverages in-context learning to adapt to specific summarization tasks without extensive fine-tuning, allowing models to generalize from limited examples. The success of GPT-4o mini in particular suggests that smaller, more efficient models can achieve high performance when properly prompted, likely due to their optimized architecture and training for specific task domains.

## Foundational Learning
This research builds upon established work in NLP and automated summarization while introducing novel applications to supply chain risk analysis. The study demonstrates that LLMs can effectively transfer knowledge from general language understanding to specialized domain tasks. The findings suggest that prompt engineering and few-shot learning may be more valuable than previously thought for domain-specific applications, potentially reducing the need for extensive fine-tuning while maintaining high performance standards.

## Architecture Onboarding
Component Map: News Article Corpus -> LLM Model (Zero/Few/Fine-tune) -> Summary Output -> Evaluation (Automated + Human)
Critical Path: Data Preparation -> Model Selection -> Prompt Engineering -> Evaluation
Design Tradeoffs: Computational Cost vs. Performance Quality vs. Model Accessibility
Failure Signatures: Over-generalization in risk identification, loss of context in summarization, metric-score misalignment with human judgment
First Experiments: 1) Baseline comparison with traditional NLP methods, 2) Cross-domain risk summarization testing, 3) Cost-benefit analysis across model scales

## Open Questions the Paper Calls Out
Unknown: How well these findings generalize to other domains beyond supply chain risk analysis
Unknown: The long-term stability and consistency of LLM performance as news volume scales
Unknown: Whether current automated evaluation metrics adequately capture the nuanced quality requirements for risk-focused summarization

## Limitations
- Heavy reliance on automated evaluation metrics that may not capture nuanced quality
- Dataset of 1,535 articles may not represent full diversity of supply chain risk scenarios
- Cost-efficiency analysis doesn't account for hidden costs like model maintenance or workflow integration

## Confidence
High confidence in: The comparative performance ranking of LLMs across different prompting strategies
Medium confidence in: The absolute quality scores achieved by top-performing models
Low confidence in: The generalizability of findings to other domains beyond supply chain risk analysis

## Next Checks
1. Conduct blind human evaluation with at least 10 independent reviewers to validate automated metrics' correlation with perceived summary quality
2. Test model performance on diverse news corpus covering multiple risk domains to assess generalizability
3. Perform longitudinal study tracking summary quality and risk identification accuracy over time as news volume increases