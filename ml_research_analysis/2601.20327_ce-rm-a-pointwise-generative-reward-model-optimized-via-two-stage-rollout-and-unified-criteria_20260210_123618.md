---
ver: rpa2
title: 'CE-RM: A Pointwise Generative Reward Model Optimized via Two-Stage Rollout
  and Unified Criteria'
arxiv_id: '2601.20327'
source_url: https://arxiv.org/abs/2601.20327
tags:
- evaluation
- criteria
- reward
- response
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap between generative reward model (GRM)
  performance on benchmarks and their effectiveness in real-world reinforcement learning
  (RL) practice. The authors identify that existing GRMs predominantly use pairwise
  evaluation protocols and lack dedicated optimization of evaluation criteria, limiting
  their utility in RL scenarios.
---

# CE-RM: A Pointwise Generative Reward Model Optimized via Two-Stage Rollout and Unified Criteria

## Quick Facts
- arXiv ID: 2601.20327
- Source URL: https://arxiv.org/abs/2601.20327
- Authors: Xinyu Hu; Yancheng He; Weixun Wang; Tao Feng; Li Lin; Jiashun Liu; Wenbo Su; Bo Zheng; Xiaojun Wan
- Reference count: 13
- Key outcome: CE-RM-4B achieves superior performance on multiple reward model benchmarks using only 5.7K curated data, particularly excelling in Best-of-N scenarios and downstream RL practice.

## Executive Summary
This paper addresses the gap between generative reward model (GRM) performance on benchmarks and their effectiveness in real-world reinforcement learning (RL) practice. The authors identify that existing GRMs predominantly use pairwise evaluation protocols and lack dedicated optimization of evaluation criteria, limiting their utility in RL scenarios. The proposed method, CE-RM-4B, introduces a two-stage evaluation process where unified query-based criteria are first generated and then used to evaluate each response independently. The model is trained using a specialized reinforcement learning approach that converts pairwise preference labels into fine-grained rewards for both criteria and evaluation trajectories.

## Method Summary
CE-RM-4B is a pointwise generative reward model that generates unified query-based criteria and then evaluates each response independently against those criteria. The model is trained using a two-stage rollout method: first generating multiple criteria sets, then evaluating responses. Rewards are computed separately for criteria generation (win rates of subsequent evaluations) and evaluation quality (win rates conditioned on correct format). The model uses only ~5.7K curated data from Skywork-Reward-Preference-80K-v0.2, filtered for uncertainty and diversity, and is trained on Qwen3-4B-Instruct-2507 with cold-start SFT followed by GRPO-based RL.

## Key Results
- Achieves superior performance on RewardBench, RewardBench2, RM-Bench, PPE Correctness, and JudgeBench benchmarks
- Excels in Best-of-N scenarios where unified criteria show increasing advantage as response count grows
- Delivers more effective improvements in downstream RL practice with GRPO compared to larger pairwise GRMs like CompassJudger1-32B
- Outperforms models trained on much larger datasets using only ~5.7K high-quality curated samples

## Why This Works (Mechanism)

### Mechanism 1
Unified, query-based criteria improve consistency and evaluation performance as the number of responses per query grows. By generating criteria conditioned solely on the query rather than jointly with each response, the model prevents inconsistent criteria across responses, reducing bias in comparison. This is particularly effective in Best-of-N scenarios where evaluation consistency becomes critical.

### Mechanism 2
Two-stage rollout with separate reward signals for criteria and evaluation trajectories enables more stable and targeted optimization. The first rollout generates multiple criteria sets, while the second produces evaluations for chosen/rejected responses. This separation allows the model to optimize criteria generation quality independently from evaluation quality through appropriate reward partitioning.

### Mechanism 3
Training on a small, high-quality curated dataset can outperform larger models trained on more data by focusing on uncertain or diverse examples. The data curation process filters for instances where the base model is uncertain (accuracy â‰¤0.6) and ensures diversity through task-type clustering and stratified sampling, increasing per-instance information content.

## Foundational Learning

**Concept: Generative Reward Models (GRMs)**
- Why needed: Understanding how LLMs can output evaluations (text + scores) as rewards rather than scalar values
- Quick check: Can you explain the difference between a discriminative scalar reward model and a generative reward model that outputs evaluation text?

**Concept: Reinforcement Learning with GRPO (Group Relative Policy Optimization)**
- Why needed: The paper builds on GRPO for RL training; understanding how group-based rollouts and relative advantages work is key
- Quick check: How does GRPO differ from PPO in handling grouped rollouts and advantage estimation?

**Concept: Pairwise vs. Pointwise Evaluation Protocols**
- Why needed: The core motivation is moving from pairwise (compare two responses) to pointwise (score each independently) evaluation for RL compatibility
- Quick check: What are the computational and practical trade-offs between pairwise and pointwise evaluation in RLHF?

## Architecture Onboarding

**Component map:**
Data Curation Module -> Cold-Start SFT Stage -> Two-Stage RL Module -> Inference Engine

**Critical path:**
1. Start with Qwen3-4B-Instruct-2507 as the foundation model
2. Run data curation pipeline to produce DSFT and DRL
3. Perform cold-start SFT (batch size 64, lr 1e-5, 3 epochs)
4. Run two-stage RL (batch size 64, 4 mini-batches, lr 2e-6, KL coeff 1e-3, nc=4, ne=2)
5. At inference, use temperature 0; optionally apply test-time scaling (k=2 or 4)

**Design tradeoffs:**
- Unified criteria vs. response-conditioned criteria: Unified criteria improve consistency but may miss response-specific aspects; the model mitigates this by allowing "Other Point(s)" adjustments
- Two-stage rollout complexity: More trajectories improve performance but increase compute; Table 3 suggests 4 criteria + 2 evaluation trajectories is a good balance
- Small high-quality data vs. large-scale data: Improves efficiency but requires careful filtering; noise in pairwise labels can still affect reward estimation

**Failure signatures:**
- High variance in evaluation scores across different criteria sets indicates unstable criteria or evaluation
- RL training divergence if KL coefficient is too low or if reward signals are noisy
- Degraded performance on general chat tasks if tool-use is naively integrated

**First 3 experiments:**
1. Reproduce the main benchmark results (RewardBench, RewardBench2, RM-Bench, PPE Correctness, JudgeBench) with CE-RM-4B and compare against ablations
2. Run the RL practice experiment with GRPO on Qwen3-8B using CE-RM-4B vs. CompassJudger1-32B, tracking Arena-Hard scores over training steps
3. Conduct a trajectory number ablation (varying nc and ne) to verify the optimal two-stage rollout configuration

## Open Questions the Paper Calls Out

**Open Question 1**
How can tool-integrated evaluation be implemented to improve performance on verifiable tasks without causing the observed degradation in general chat scenarios? The paper notes that while tool assistance yields gains for mathematics and code, "it degrades performance on general chat." Experiments demonstrating dynamic tool routing that improves scores on factuality benchmarks while maintaining or improving performance on general chat benchmarks like Arena-Hard would resolve this.

**Open Question 2**
Can incorporating a small amount of human-annotated pointwise score data effectively calibrate the reward signals and improve model accuracy? The paper suggests using "a small amount of data annotated with reliable pointwise score labels as anchors... to perform calibration." A comparative study showing that adding a small percentage of gold-standard pointwise anchors reduces variance and improves correlation with human judgments would resolve this.

**Open Question 3**
Does the advantage of unified, query-based criteria persist when scaling training data from 5.7K to significantly larger datasets? The paper emphasizes the efficiency of using only 5.7K curated samples, but it's unclear if the two-stage rollout benefits hold or diminish with the noise inherent in larger datasets. Evaluation results of the CE-RM model trained on the full Skywork-Preference-80K dataset compared to the curated 5.7K version would resolve this.

## Limitations
- Data curation relies heavily on base model's accuracy threshold for uncertainty detection, potentially introducing selection bias
- Two-stage rollout approach significantly increases computational complexity compared to single-stage methods
- Model shows degraded performance on tool-use tasks when naively integrated into GRPO
- Effectiveness of unified criteria may depend on query complexity and response diversity

## Confidence
- **High confidence**: Benchmark performance improvements (RewardBench, RM-Bench, etc.) and RL practice results with GRPO
- **Medium confidence**: Superiority of unified query-based criteria over response-conditioned criteria
- **Medium confidence**: Data curation approach's effectiveness relies on assumptions about base model's uncertainty detection

## Next Checks
1. Evaluate CE-RM-4B on tool-use and code-generation tasks to quantify performance degradation and test the proposed two-stage tool-use integration approach
2. Repeat data curation and training using different base models (GPT-4, Claude) to determine generalizability across foundation models
3. Design a benchmark specifically testing CE-RM-4B's ability to handle complex, multi-step reasoning tasks where criteria consistency becomes more critical