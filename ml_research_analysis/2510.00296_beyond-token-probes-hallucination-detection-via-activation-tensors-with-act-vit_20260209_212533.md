---
ver: rpa2
title: 'Beyond Token Probes: Hallucination Detection via Activation Tensors with ACT-ViT'
arxiv_id: '2510.00296'
source_url: https://arxiv.org/abs/2510.00296
tags:
- arxiv
- act-vit
- llms
- should
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting hallucinations in
  Large Language Models (LLMs), which is crucial for their safe deployment. Traditional
  probing classifiers, which operate on isolated layer-token pairs, are limited by
  their inability to capture dynamic error signals that vary across layers and tokens,
  and they cannot generalize across different LLMs.
---

# Beyond Token Probes: Hallucination Detection via Activation Tensors with ACT-ViT

## Quick Facts
- **arXiv ID:** 2510.00296
- **Source URL:** https://arxiv.org/abs/2510.00296
- **Reference count:** 40
- **Primary result:** ACT-ViT consistently outperforms traditional probing methods across 15 LLM-dataset combinations, achieving improvements of up to 4.33 AUC points.

## Executive Summary
This paper addresses the critical problem of detecting hallucinations in Large Language Models (LLMs) by introducing ACT-ViT, a novel approach that treats activation tensors as images and applies Vision Transformer techniques. Traditional probing classifiers are limited by their inability to capture dynamic error signals that vary across layers and tokens, and they cannot generalize across different LLMs. ACT-ViT uses per-LLM linear adapters to map different activation tensors to a shared representation space, followed by a shared ViT backbone for hallucination detection. The method is trained on data from multiple LLMs simultaneously and demonstrates superior performance with strong zero-shot generalization capabilities.

## Method Summary
ACT-ViT processes the full activation tensor (layers × tokens × hidden dimension) of an LLM as a spatial structure analogous to an image. The approach uses per-LLM linear adapters to project varying hidden dimensions into a unified space, followed by a shared Vision Transformer backbone that applies self-attention to adaptively locate the most predictive activation patterns. Max-pooling is used to compress the spatial dimensions while retaining signal, and the model is trained on multiple LLMs simultaneously to capture universal truthfulness representations. At inference, ACT-ViT can detect hallucinations in approximately 10^-5 seconds per instance while requiring less than three hours of training time across all 15 tested combinations.

## Key Results
- ACT-ViT consistently outperforms traditional probing methods across 15 LLM-dataset combinations, achieving improvements of up to 4.33 AUC points
- The model demonstrates strong zero-shot generalization to unseen datasets
- ACT-ViT can be effectively adapted to new LLMs through fine-tuning only the linear adapter, keeping the ViT backbone frozen
- The approach is highly efficient, requiring less than three hours of training time on all 15 combinations together

## Why This Works (Mechanism)

### Mechanism 1: Spatial Inductive Bias via Image Analogy
- **Claim:** Treating the full activation tensor as a 2D spatial structure (layers × tokens) allows the detector to capture dynamic error signals that fixed, static probes miss.
- **Mechanism:** Traditional probes rely on isolated layer-token pairs. ACT-ViT treats the activation tensor analogous to an RGB image, where layers and tokens form "spatial" dimensions. By applying a Vision Transformer backbone, the model uses self-attention to adaptively locate the most predictive "activation pixels" regardless of their position.
- **Core assumption:** Hallucination signals are not strictly localized to specific tokens or layers but are distributed patterns that retain spatial coherence relative to the layer-depth and token-sequence axes.
- **Evidence anchors:** The location of predictive signals for hallucinations widely varies across LLMs, requiring a more sophisticated approach that can adaptively attend to the most predictive locations.

### Mechanism 2: Linear Representation Alignment
- **Claim:** Diverse LLMs encode a "universal truthfulness" representation that can be mapped to a shared geometric space via simple linear transformations.
- **Mechanism:** The architecture uses per-LLM Linear Adapters to project varying hidden dimensions into a unified dimension, allowing a single shared ViT backbone to process internal states from different model architectures as if they were the same input modality.
- **Core assumption:** Despite different training data and architectures, LLMs converge towards a linearly exploitable representation of factual truth.
- **Evidence anchors:** Hidden representations of LLMs can be put in approximate correspondence by some linear transformations, and there exist "internal activation features crucial for lie detection."

### Mechanism 3: Efficient Spatial Compression
- **Claim:** Aggressive max-pooling of the spatial dimensions retains sufficient signal for detection while drastically reducing computational overhead.
- **Mechanism:** Raw activation tensors are large (e.g., 0.2GB per instance). The architecture applies max-pooling to reduce the tensor to a fixed size before processing, acting as a hard attention mechanism that preserves only the most active neuron states across regions.
- **Core assumption:** The "peak" activation values within a local region are the primary carriers of the error signal, and surrounding context is secondary or redundant.
- **Evidence anchors:** Even with aggressive pooling, the model significantly outperforms static probes, demonstrating that compressed representations retain sufficient discriminative information.

## Foundational Learning

- **Activation Tensors (Internal States)**
  - **Why needed here:** This method requires white-box access to the hidden states across all layers and tokens, representing the "thought process" of the model rather than just output probability.
  - **Quick check question:** Can you explain why the tensor shape is defined as L_M × N × D_M and not just the final layer's output?

- **Vision Transformers (ViT) & Patching**
  - **Why needed here:** The core innovation is repurposing a vision architecture for text activations. Understanding how ViTs chop inputs into patches and add positional embeddings is crucial to grasp how the model "sees" the activation map.
  - **Quick check question:** How does the "patching" operation in ACT-ViT differ from standard tokenization in a text-based LLM?

- **Parameter-Efficient Fine-Tuning (Adapters)**
  - **Why needed here:** The system relies on training tiny linear adapters for new LLMs while keeping the main backbone frozen. Understanding the separation between "generalizable features" (backbone) and "model-specific geometry" (adapter) is critical.
  - **Quick check question:** If you add a new LLM to the system, which weights must you train from scratch, and which do you freeze?

## Architecture Onboarding

- **Component map:** Raw Activation Tensor → Pooling Layer → Linear Adapter → ViT Backbone → Head
- **Critical path:** The extraction of the activation tensor after the FFN and residual connection, and the correct alignment via the Linear Adapter. If the tensor is extracted from the wrong layer norm or the adapter is not trained, the shared ViT sees noise.
- **Design tradeoffs:**
  - **Pooling Resolution (L_p, N_p):** High resolution captures more signal but increases training time. Low resolution is faster but may miss fine-grained cues.
  - **Shared vs. Separate Adapters:** Using a shared adapter is theoretically possible but combinatorially complex due to feature permutation symmetries; separate adapters are safer but require storage per LLM.
- **Failure signatures:**
  - **Performance Collapse on New LLM:** ViT backbone fails to generalize. Check if the Linear Adapter has converged and may need more pre-training data or a higher learning rate relative to the frozen backbone.
  - **High Variance Across Datasets:** The pooling window may be too small, destroying necessary spatial context.
- **First 3 experiments:**
  1. **Baselines:** Implement Probe[*] (Logistic Regression on best layer-token) vs. ACT-ViT(s) (single dataset) to verify the "image" approach beats static probing on a known dataset like HotpotQA.
  2. **Ablation on Pooling:** Sweep pooling factors on a single LLM to find the inflection point where AUC drops vs. compute time saved.
  3. **Zero-Shot Transfer:** Train on 2 LLMs, freeze the ViT, and only train the Linear Adapter on the 3rd LLM to validate the "universal truthfulness" hypothesis.

## Open Questions the Paper Calls Out

- **Can the ACT-ViT framework be successfully adapted for tasks beyond hallucination detection, such as identifying data contamination or detecting LLM-generated content?** The authors state their work opens future directions including extending ACT-ViT to tasks like "data contamination and LLM-generated content detection."
- **Can a shared module that exploits permutation symmetries effectively replace the current per-LLM linear adapters?** The authors suggest exploring advanced architectures that replace per-LLM adapters with a shared module exploiting "permutation symmetries across ATs."
- **Can dimensionality reduction techniques be improved to preserve signal fidelity better than simple pooling?** The authors note that the current pooling operation might "discard potentially informative signals" despite being effective for computational costs.

## Limitations
- **White-box requirement:** The method requires internal activation access, limiting practical deployment scenarios where such access isn't available.
- **Label quality dependence:** Performance heavily relies on accurate ground truth labels, which may be noisy or incomplete in real-world applications.
- **Activation tensor size and efficiency:** While pooling reduces computation, the initial tensor extraction and storage remain resource-intensive for production use.

## Confidence
- **High Confidence:** ACT-ViT's superior performance over traditional probing methods (AUC improvements up to 4.33 points) is well-supported by experimental results across 15 LLM-dataset combinations.
- **Medium Confidence:** The mechanism of spatial inductive bias via ViT processing of activation tensors is plausible but relies on the assumption that hallucination signals have consistent spatial patterns across diverse architectures.
- **Medium Confidence:** The universal truthfulness hypothesis and linear adapter effectiveness are supported by empirical results but lack theoretical grounding for why different LLMs would converge to similar representations.

## Next Checks
1. **Architecture-specific performance analysis:** Systematically test ACT-ViT across model families (decoder-only, encoder-decoder, mixture-of-experts) to identify which architectures benefit most from the approach and whether performance correlates with architectural similarities.
2. **Label noise robustness testing:** Introduce controlled noise into ground truth labels and measure degradation in ACT-ViT performance versus traditional probes to quantify sensitivity to labeling errors.
3. **Activation extraction point sensitivity:** Test performance when extracting activations from different internal points (e.g., before vs. after residual connections, different normalization layers) to determine if the FFN+residual extraction point is optimal or arbitrary.