---
ver: rpa2
title: 'Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and
  Insights'
arxiv_id: '2502.12521'
source_url: https://arxiv.org/abs/2502.12521
tags:
- reasoning
- tasks
- planning
- llms
- inference-time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Sys2Bench, a comprehensive benchmark for\
  \ evaluating the reasoning and planning capabilities of large language models (LLMs)\
  \ across eleven diverse tasks in five categories: arithmetic reasoning, logical\
  \ reasoning, common sense reasoning, algorithmic reasoning, and planning. The authors\
  \ evaluate four widely-used inference-time techniques\u2014Chain-of-Thought (CoT),\
  \ Self Consistency (SC), Tree of Thought (ToT), and Reasoning as Planning (RAP)\u2014\
  across seven different LLMs, including LLaMA 3.1 variants, GPT models, and OpenAI\u2019\
  s O1 reasoning models."
---

# Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights

## Quick Facts
- arXiv ID: 2502.12521
- Source URL: https://arxiv.org/abs/2502.12521
- Reference count: 11
- Large-scale benchmark evaluation shows inference-time scaling techniques have task-dependent performance with significant computational trade-offs

## Executive Summary
This paper introduces Sys2Bench, a comprehensive benchmark for evaluating reasoning and planning capabilities of large language models across eleven diverse tasks. The study systematically evaluates four inference-time techniques—Chain-of-Thought, Self Consistency, Tree of Thought, and Reasoning as Planning—across seven different LLMs. Key findings reveal that while inference-time scaling improves performance on certain tasks, it faces fundamental limitations including computational cost, reasoning path bias, and inability to handle out-of-distribution spatial reasoning tasks like Rubik's Cube.

## Method Summary
The authors evaluate inference-time techniques on eleven tasks across five categories using seven LLMs. Four inference methods are tested: Chain-of-Thought with 5-shot prompts (temp 0.8), Self Consistency with 5 samples and majority voting, Tree of Thought with beam search (beam size 5-10, depth 4-20), and Reasoning as Planning with Monte Carlo Tree Search (≤10 rollouts). Evaluation covers arithmetic, logical, common sense, algorithmic, and planning tasks using standard benchmarks plus a novel Bin Packing task.

## Key Results
- CoT and SC excel in arithmetic reasoning but show limited gains on planning tasks
- Tree search methods (ToT/RAP) struggle with complex reasoning tasks due to LLM evaluator weakness
- LRMs like O1 achieve high arithmetic accuracy but fail on spatial reasoning (0.6% on Rubik's Cube)
- Inference-time compute scaling is bounded by LLM bias toward certain reasoning paths
- ToT can require 10x-50x more tokens than SC for equal or worse performance

## Why This Works (Mechanism)

### Mechanism 1
Linear step-decomposition (CoT) combined with statistical aggregation (Self-Consistency) stabilizes outputs for tasks with verifiable intermediate states like arithmetic. CoT forces intermediate reasoning steps, likely activating relevant training distributions, while Self-Consistency samples multiple paths and uses majority voting to filter stochastic errors. This works when the model has non-trivial probability of generating correct reasoning paths and errors are uncorrelated across samples.

### Mechanism 2
Tree-search methods (ToT/RAP) facilitate combinatorial tasks by exploring solution space non-linearly, relying on the model's ability to discriminate its own partial outputs. The LLM generates multiple potential next steps while a separate evaluation process prunes the tree, keeping high-potential paths. This assumes the LLM is a better discriminator than generator and can accurately predict utility of partial states.

### Mechanism 3
Inference-time scaling is bounded by models' inherent biases and lack of genuine spatial/logical simulation capabilities. As search depth increases, models must simulate action consequences but instead rely on pattern matching and retrieval of training data rather than state dynamics simulation. This causes performance to plateau or degrade with complexity.

## Foundational Learning

- **Self-Consistency (SC)**: A primary baseline trading compute (multiple samples) for robustness (voting) to address variance in stochastic generation. Quick check: If a model has 20% error rate per sample, does generating 5 samples and voting guarantee a correct answer?

- **Tree Search Pruning (Beam Search/MCTS)**: Essential for understanding ToT and RAP, which build search structures rather than sampling parallel paths. Quick check: In Tree of Thoughts, if your "Evaluator" module has 30% false positive rate (rating bad step as good), what happens to solution quality as tree depth grows?

- **In-Distribution vs. Out-of-Distribution (OOD) Generalization**: Critical concept explaining why models fail at Rubik's Cubes despite excelling at common planning tasks. Quick check: Why might a model excel at "Trip Planning" but fail at "Rubik's Cube" despite both being "planning" tasks?

## Architecture Onboarding

- **Component map**: Input Interface -> Generator (base LLM) -> Evaluator/Discriminator (LLM or logic) -> Aggregator/Search Controller (algorithm managing flow)
- **Critical path**: The Evaluator is the bottleneck; paper shows that for small models (8B) or complex tasks (Rubik's), the evaluator cannot distinguish valid reasoning from noise
- **Design tradeoffs**: CoT/SC offer low latency and high robustness for arithmetic but poor for multi-step planning; ToT/RAP support backtracking but cost 10x-50x more and risk degraded performance; LRMs provide highest performance but are opaque and potentially rely on retrieval
- **Failure signatures**: Early stopping/repetition (stuck in "popular moves" loop), error propagation (single arithmetic error ruins CoT), bias saturation (increased compute yields no gains due to sampling same incorrect paths)
- **First 3 experiments**: 1) Calibrate Evaluator by testing LLM's classification accuracy on "Good State vs. Bad State" pairs; if <70%, tree search will likely fail 2) Run CoT vs. ToT on tasks with increasing depth to identify crossover point where ToT degrades below CoT 3) Measure token usage for SC (N=5) vs. ToT (Depth=4) to validate cost findings

## Open Questions the Paper Calls Out

### Open Question 1
How can inference-time scaling techniques be adapted for tasks lacking verifiable intermediate steps, such as Common Sense Reasoning? The authors note current improvements rely on verifiers, and without them performance gains disappear. This remains unresolved because ToT and RAP depend on evaluating intermediate step correctness, undefined for subjective common sense tasks.

### Open Question 2
What architectural or training modifications are required to enable LRMs to handle spatial reasoning tasks currently "out-of-distribution"? The paper concludes both LLMs and LRMs lack reasoning capabilities for Rubik's Cube, indicating current "reasoning" relies on pattern matching rather than spatial simulation. This is unresolved as even advanced models like O1 fail completely.

### Open Question 3
How can inference-time search algorithms be redesigned to mitigate the "reasoning path bias" of underlying LLMs rather than amplifying it? Section 5.3 states inference-time scaling is limited by LLM bias toward certain paths. Results show ToT performance degrades with complexity because search fails to overcome model's inherent preferences. This remains unresolved as no strategy maintains performance relative to CoT as tree depth increases.

## Limitations
- Benchmark may not capture all real-world reasoning and planning challenges
- Focus on English-language tasks and Western-centric datasets introduces potential cultural bias
- Evaluation relies on specific parameter settings that may not generalize
- Claims about LRMs "relying on retrieval rather than true understanding" primarily inferred from Rubik's Cube performance

## Confidence

**High Confidence**: Comparative performance patterns of CoT and SC on arithmetic reasoning align with previous literature; observation that LRMs excel at arithmetic but struggle with spatial reasoning is consistent across multiple tasks.

**Medium Confidence**: Relative performance of tree search methods shows variability that may depend on implementation details; Rubik's Cube results are compelling but based on a single spatial reasoning task.

**Low Confidence**: Claims about LRMs "relying on retrieval rather than true understanding" primarily inferred from Rubik's Cube performance and would benefit from additional evidence from diverse reasoning tasks.

## Next Checks

1. **Evaluator Calibration**: Test LLM's classification accuracy on "Good State vs. Bad State" pairs for target task before deploying ToT/RAP; if accuracy < 70%, tree search will likely fail.

2. **Complexity Scaling**: Run CoT vs. ToT on tasks with increasing depth (e.g., Trip Plan with 2 vs. 10 cities) to identify crossover point where ToT degrades below CoT.

3. **Cost-Benefit Analysis**: Measure token usage for SC (N=5) vs. ToT (Depth=4) to validate finding that ToT often costs 10x-50x more for equal or worse performance on standard reasoning tasks.