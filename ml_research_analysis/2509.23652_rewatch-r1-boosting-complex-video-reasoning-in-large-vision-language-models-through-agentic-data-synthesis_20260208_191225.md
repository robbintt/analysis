---
ver: rpa2
title: 'ReWatch-R1: Boosting Complex Video Reasoning in Large Vision-Language Models
  through Agentic Data Synthesis'
arxiv_id: '2509.23652'
source_url: https://arxiv.org/abs/2509.23652
tags:
- video
- reasoning
- arxiv
- answer
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of complex video reasoning in
  Large Vision-Language Models (LVLMs) by introducing ReWatch, a novel multi-stage
  agentic data synthesis pipeline that generates high-quality video-grounded training
  data. The key innovation is a Multi-Agent ReAct framework that simulates human-like
  "re-watching" to create video-grounded Chain-of-Thought (CoT) traces with explicit
  information retrieval and verification steps.
---

# ReWatch-R1: Boosting Complex Video Reasoning in Large Vision-Language Models through Agentic Data Synthesis

## Quick Facts
- arXiv ID: 2509.23652
- Source URL: https://arxiv.org/abs/2509.23652
- Authors: Congzhi Zhang; Zhibin Wang; Yinchao Ma; Jiawei Peng; Yihan Wang; Qiang Zhou; Jun Song; Bo Zheng
- Reference count: 40
- Primary result: State-of-the-art average performance on five challenging video reasoning benchmarks through agentic data synthesis and O&R reward mechanism

## Executive Summary
ReWatch-R1 addresses the critical data bottleneck for complex video reasoning in LVLMs by introducing a novel multi-stage agentic data synthesis pipeline. The key innovation is a Multi-Agent ReAct framework that simulates human-like "re-watching" to generate high-quality video-grounded Chain-of-Thought (CoT) traces with explicit information retrieval and verification steps. Building on this dataset, ReWatch-R1 achieves state-of-the-art performance through Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Reward (RLVR), featuring a novel Observation & Reasoning (O&R) reward mechanism that evaluates both final answer correctness and reasoning process grounding in video content.

## Method Summary
ReWatch-R1 employs a three-stage data synthesis pipeline: hierarchical captioning using Gemini2.5-Flash, contrastive QA generation with three-layer filtering (verification, text-bias elimination, summary-bias elimination), and multi-agent ReAct CoT synthesis with Reasoner and Observer agents. The model is trained through SFT on caption, QA, and CoT tasks with a composite loss, followed by GRPO-style RL with O&R reward decomposition. The O&R reward evaluates observation grounding and reasoning sufficiency alongside answer accuracy, directly penalizing hallucination. The approach successfully addresses the data bottleneck by providing challenging multi-hop questions and high-quality video-grounded reasoning traces necessary for effective RLVR training.

## Key Results
- Achieves state-of-the-art average performance on five challenging video reasoning benchmarks
- Demonstrates significant improvements over existing models, particularly on complex multi-hop reasoning tasks
- Shows robust performance across both understanding (MMVU, LVBench, VideoMME, VideoMMMU) and reasoning (VCR-Bench, MINERVA, Video Holmes, VideoMathQA, CG-AV-Counting) benchmarks
- The O&R reward mechanism effectively reduces hallucination while maintaining or improving accuracy

## Why This Works (Mechanism)

### Mechanism 1
Multi-agent ReAct framework generates video-grounded CoT by simulating human-like "re-watching" with explicit retrieval and verification steps. A Reasoner agent produces thoughts and actions (segment_retrieval, segment_query); an Observer agent executes these against timestamped captions to return grounded observations. This loop continues until answer derivation, creating traces that teach models to actively query video content rather than hallucinate.

### Mechanism 2
Three-layer filtering eliminates textual and summary shortcuts, forcing genuine video dependency in QA pairs. Layer 1 verifies answer correctness against detailed captions. Layer 2 probes LLMs without video access; if they answer correctly above threshold θ_text, the question is rejected as text-solvable. Layer 3 similarly rejects questions answerable from summaries alone (θ_sum threshold).

### Mechanism 3
O&R reward mechanism penalizes hallucination by evaluating both observation grounding and reasoning sufficiency, not just final answer accuracy. Parse model output for {Act_i, Obs_i} pairs. robs compares each observation against ground-truth captions via judge. rrea tests whether observations alone suffice to answer correctly. Final reward: r_O&R = r_acc × (1 + r_obs + r_rea) + r_fmt.

### Mechanism 4
SFT establishes reasoning format; RL refines reasoning efficiency and accuracy, reducing action count while improving performance. SFT trains on multi-task objectives to instill structured response patterns. RL then uses GRPO with O&R reward to optimize policy. Critically, SFT alone increases action count; RL decreases it while boosting accuracy—pruning redundant steps.

## Foundational Learning

- **ReAct Framework (Reasoning + Acting)**: Why needed: The multi-agent CoT synthesis explicitly uses ReAct's thought-action-observation loop. Understanding this paradigm is essential to grasp how the Observer and Reasoner agents coordinate. Quick check: Can you explain how ReAct differs from standard chain-of-thought prompting in terms of external tool integration?

- **RLVR (Reinforcement Learning with Verifiable Rewards)**: Why needed: The entire RL stage depends on RLVR with GRPO algorithm. The O&R reward is a specialized verifiable reward design. Quick check: What distinguishes RLVR from RLHF, and why is verifiability critical for video reasoning tasks?

- **Multi-stage Data Synthesis Pipelines**: Why needed: ReWatch's three-stage pipeline (caption → QA → CoT) with progressive filtering is central to solving the data bottleneck. Quick check: Why might filtering for video-dependency (eliminating text-shortcuts) be more important for video reasoning than for image reasoning?

## Architecture Onboarding

- **Component map**: Hierarchical Captioner → Contrastive QA Generator → Three-Layer Filter → Multi-Agent ReAct CoT Synthesizer → SFT Model → RL with O&R Reward

- **Critical path**: Caption quality determines downstream QA and CoT fidelity; semantic segmentation errors propagate. Three-layer filtering thresholds (θ_text, θ_sum) control dataset difficulty-size tradeoff. SFT model quality is prerequisite for RL stability; ablation shows catastrophic drop without SFT. O&R reward parsing robustness determines RL convergence; malformed outputs yield zero reward.

- **Design tradeoffs**: Frame count (192 vs. 384): Higher frames improve long-video reasoning but increase compute. Thinking vs. non-thinking mode: Thinking mode has higher ceiling but slower convergence. Dataset mixing in RL: 20k ReWatch-QA + 10k Video-R1-QA + 10k LongVideoReason-QA used; pure ReWatch may overfit to synthesis distribution.

- **Failure signatures**: High text-only accuracy on QA dataset: Indicates filtering failed. RL instability with accuracy-only reward: Suggests need for O&R decomposition. Action count increasing during RL: Suggests reward not encouraging efficiency. Catastrophic forgetting on understanding benchmarks: Check caption task weighting in SFT composite loss.

- **First 3 experiments**: Validate caption quality: Sample 50 videos; manually verify timestamp alignment and event completeness. Ablate filtering layers: Train models with F1-only, F1+F2, and full F1+F2+F3 filtering; measure text-only accuracy. SFT→RL transfer test: Train SFT model on ReWatch-CoT vs. Video-R1-CoT; run identical RL on both.

## Open Questions the Paper Calls Out

### Open Question 1
Can the O&R reward mechanism be made self-contained without relying on external judge/inference models, while maintaining equivalent training effectiveness? The paper uses Qwen3-30B-A3B-Instruct as both inference model M_infer and judge model M_judge, creating computational overhead and external dependency.

### Open Question 2
What specific architectural or training modifications could further reduce the "Long Video Tax"—the performance degradation from 40.38% (short videos) to 27.46% (long videos)? While ReWatch-R1 shows more attenuated degradation, long-video reasoning remains "a pervasive and yet-unsolved challenge."

### Open Question 3
To what extent does the data synthesis pipeline depend on the capabilities of the specific teacher models (Gemini 2.5-Flash, GPT-4.1), and can comparable dataset quality be achieved with smaller open-source models? The paper does not analyze the quality-cost tradeoff of using proprietary models for synthesis.

### Open Question 4
How well does ReWatch-R1's training paradigm generalize to other LVLM architectures beyond Qwen2.5-VL-7B? The paper exclusively uses Qwen2.5-VL-7B as the base model without testing architecture transferability.

## Limitations
- Synthetic data generation without human verification of CoT traces may not reflect human-like reasoning patterns
- Heavy reliance on judge model quality for O&R reward mechanism parsing and evaluation
- Computational cost of generating synthetic dataset not discussed, limiting practical applicability
- No human evaluation of whether generated questions truly require multi-hop reasoning

## Confidence
- **High confidence**: SFT training methodology and positive impact on RL stability
- **Medium confidence**: Three-layer filtering system effectiveness in eliminating text/shortcuts
- **Medium confidence**: O&R reward mechanism's ability to reduce hallucination
- **Medium confidence**: Overall benchmark performance improvements

## Next Checks
1. **Human evaluation study**: Recruit annotators to rate 100 questions for multi-hop reasoning requirement and human-like reasoning process
2. **Judge model robustness test**: Systematically corrupt action/observation parsing in O&R reward and measure RL training stability
3. **Distribution shift analysis**: Train ReWatch-R1 on ReWatch-CoT and evaluate on Video-R1-CoT to measure generalization to natural CoT patterns