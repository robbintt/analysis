---
ver: rpa2
title: 'V-CECE: Visual Counterfactual Explanations via Conceptual Edits'
arxiv_id: '2509.16567'
source_url: https://arxiv.org/abs/2509.16567
tags:
- edits
- image
- classifier
- counterfactual
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: V-CECE addresses the gap between human and neural model reasoning
  in visual counterfactual explanations by proposing optimal semantic edits and applying
  them through a diffusion model. It leverages a knowledge graph and bipartite matching
  to determine minimal, actionable conceptual edits (insertions, deletions, substitutions)
  to transform an image from one class to another.
---

# V-CECE: Visual Counterfactual Explanations via Conceptual Edits

## Quick Facts
- arXiv ID: 2509.16567
- Source URL: https://arxiv.org/abs/2509.16567
- Authors: Nikolaos Spanos; Maria Lymperaiou; Giorgos Filandrianos; Konstantinos Thomas; Athanasios Voulodimos; Giorgos Stamou
- Reference count: 40
- Primary result: V-CECE generates counterfactuals without training, achieving 88.9–99.5% success rates on BDD100K and avg. 2.54–4.77 edits across models.

## Executive Summary
V-CECE addresses the gap between human and neural model reasoning in visual counterfactual explanations by proposing optimal semantic edits and applying them through a diffusion model. It leverages a knowledge graph and bipartite matching to determine minimal, actionable conceptual edits (insertions, deletions, substitutions) to transform an image from one class to another. Edits are ordered either locally via an LVLM, globally by classifier importance, or a hybrid approach. Applied to BDD100K and Visual Genome, V-CECE generates counterfactuals without training, showing strong performance: SR of 88.9–99.5% on BDD100K and Avg.|E| of 2.54–4.77 across models. Human evaluation shows LVLMs like Claude 3.5 Sonnet align better with human reasoning than CNNs, which often require more edits and produce lower-quality images, revealing differing semantic understanding. V-CECE thus offers a zero-training, interpretable pipeline for probing model biases and semantic comprehension.

## Method Summary
V-CECE generates visual counterfactual explanations through a three-stage pipeline: (1) optimal semantic edit selection using WordNet-based bipartite matching to find minimal edit sets transforming source to target class concepts, (2) edit ordering via either LVLM-guided local selection, classifier-importance-based global ordering, or a hybrid approach, and (3) black-box counterfactual generation using diffusion inpainting with grounded object detection and segmentation. The method requires no training on target data, instead relying on frozen pre-trained models for both reasoning (LVLM) and generation (diffusion), enabling fair comparison across different classifier types while probing their semantic understanding.

## Key Results
- V-CECE achieves 88.9–99.5% success rates on BDD100K with 2.54–4.77 average edits across CNN and LVLM classifiers
- CNN classifiers (DenseNet, EfficientNet) require more edits (5.37–4.77 avg) and produce more artifacts (30–60% visually incorrect) compared to LVLMs (~2.5 edits, ~80% visually correct)
- Human evaluation shows humans would flip labels 3 edits earlier on average for CNN-generated counterfactuals, suggesting CNNs rely on spurious features
- Global edit ordering performs best for CNNs (85.8% SR) while Local-Global ordering excels for LVLMs (98.1% SR), indicating different semantic processing mechanisms

## Why This Works (Mechanism)

### Mechanism 1: Optimal Semantic Edit Selection via Bipartite Matching
- Claim: Conceptual edits are provably minimal through graph-based optimization on semantic knowledge structures.
- Mechanism: WordNet provides a semantic graph where concepts are nodes and edges represent semantic relationships. For any source class L and target class L*, the algorithm constructs a bipartite graph with source concepts on one side, target concepts on the other, and dummy nodes representing insert/delete operations. Edge weights correspond to semantic distances (shortest paths via Dijkstra). The Hungarian algorithm then solves the assignment problem in O(mn log n) time, yielding the minimum-cost edit set E = {I, D, S} that transforms L → L*.
- Core assumption: WordNet path distances meaningfully capture semantic relatedness for visual concepts, and optimal semantic distance correlates with minimal, human-interpretable edits.
- Assumption: The semantic relationships in WordNet generalize to visual semantics in driving/scene contexts.
- Evidence anchors:
  - [Section 3.1]: "The solution of equation 1 can be deterministically provided using bipartite matching... The minimization of this matching is performed via the Hungarian algorithm, resulting in a set E={I, D, S} of minimum cost edits"
  - [Section 4.2]: V-CECE requires ~2.5 edits on Visual Genome vs. ~12.8 for prior semantic counterfactual methods
  - [Corpus]: "Flexible Counterfactual Explanations with Generative Models" notes fixed mutable feature sets limit flexibility; V-CECE's graph-based approach provides structured edit spaces
- Break condition: If WordNet lacks domain-specific concepts (e.g., autonomous driving terminology) or if visual semantic similarity diverges significantly from lexical semantic similarity, edit optimality may degrade.

### Mechanism 2: Edit Ordering via LVLM Reasoning and Classifier Importance Scoring
- Claim: Iterative edit ordering—guided by either LVLM commonsense reasoning or classifier-specific importance scores—reduces unnecessary edits and improves counterfactual interpretability.
- Mechanism: Three ordering strategies operate on the optimal edit set E: (1) **Local Edits** query an LVLM (Claude 3.5 Sonnet) at each step with the current image and remaining edits, selecting the most commonsense action; (2) **Global Edits** score each edit by Importance = (|Insert| - |Delete| + |Substitute_forward| - |Substitute_backward|) / |occurrences| across all L→L* transitions, then apply highest-scored edits first; (3) **Local-Global** combines both by filtering to image-specific edits but ordering by global importance scores.
- Core assumption: LVLMs possess sufficient spatial and commonsense reasoning to identify semantically salient edits, and classifier importance scores reflect causal decision factors.
- Assumption: Global importance scores generalize across images within the same class transition.
- Evidence anchors:
  - [Section 3.2]: "Supplying the updated image each round prevents the logical inconsistencies that arise when the whole sequence is produced at once"
  - [Table 1]: Local-Global ordering achieves 98.10% success rate with Claude 3.5 Sonnet vs. 88.9% with DenseNet, suggesting edit ordering effectiveness varies by classifier type
  - [Corpus]: "Explanation-Driven Counterfactual Testing for Faithfulness" (FMR=0.55) addresses VLM explanation faithfulness—relevant to whether LVLM edit ordering truly reflects reasoning
  - [Corpus]: Weak direct evidence for edit ordering mechanisms in counterfactual generation specifically
- Break condition: If LVLMs hallucinate spatial relationships or if classifier importance scores capture spurious correlations rather than causal features, edit ordering may prioritize irrelevant changes.

### Mechanism 3: Black-box Counterfactual Generation via Diffusion Inpainting with Semantic Grounding
- Claim: Pre-trained diffusion models combined with grounded object detection can execute semantic edits without classifier access or training, preserving semantic integrity while achieving label flips.
- Mechanism: GroundingDINO localizes objects specified by edit operations (insert/delete/substitute), SAM generates precise segmentation masks, and Stable Diffusion v1.5 Inpainting executes edits using the mask and text prompt. LVLM guidance (Claude 3.5 Sonnet) provides commonsense constraints: object placement suggestions for insertions (e.g., "pillow on bed") and background inference for deletions. Generation proceeds iteratively until classifier prediction flips or edit set exhausts.
- Core assumption: Pre-trained diffusion models maintain sufficient semantic grounding to execute conceptual edits faithfully, and artifacts from out-of-distribution edits do not systematically confound classifier decisions.
- Assumption: The frozen diffusion model's biases are uniform across classifier types, enabling fair comparison.
- Evidence anchors:
  - [Section 3.3]: "By relying on a model not directly trained on our data, we ensure that any inherent bias remains consistent across all experiments"
  - [Table 2]: 59.7% of DenseNet counterfactuals contain visual artifacts vs. 81.2% visual correctness for Claude 3.5 Sonnet, suggesting classifier-diffusion alignment varies
  - [Corpus]: "Diffusion Counterfactuals for Image Regressors" (FMR=0.56) applies diffusion to regression counterfactuals, supporting generalizability of diffusion-based generation
- Break condition: If diffusion inpainting introduces systematic artifacts that classifiers associate with class changes independently of semantic content, counterfactual explanations become confounded by generation artifacts.

## Foundational Learning

- **Bipartite Matching and the Assignment Problem**:
  - Why needed here: Core to understanding how V-CECE guarantees edit optimality through Hungarian algorithm optimization on semantic graphs
  - Quick check question: Given source concepts {car, person} and target concepts {bicycle, tree}, how would dummy nodes represent insert/delete operations in the bipartite graph?

- **WordNet Semantic Hierarchies**:
  - Why needed here: Edit costs derive from shortest-path distances in WordNet; understanding hypernym/hyponym relationships is essential for interpreting edit semantics
  - Quick check question: Why might path distance in WordNet inadequately capture visual semantic similarity for driving scenarios?

- **Diffusion Model Inpainting Mechanisms**:
  - Why needed here: V-CECE relies on Stable Diffusion's inpainting capability; understanding how text prompts, masks, and denoising schedules interact is critical for debugging generation failures
  - Quick check question: What happens to semantic coherence when inpainting a large occluding object (e.g., a car) in a complex scene?

## Architecture Onboarding

- **Component map**:
  - Explanation Component: WordNet graph → Semantic distance computation → Bipartite matching (Hungarian algorithm) → Optimal edit set E
  - Edit Ordering: Local (LVLM per-step selection) / Global (importance scores) / Local-Global (hybrid) → Ordered edit sequence
  - Generative Component: GroundingDINO (object detection) → SAM (mask generation) → Claude 3.5 Sonnet (placement/background guidance) → Stable Diffusion Inpainting → Edited image → Classifier query → Loop until flip or exhaust
  - Evaluation: FID, CMMD, SimSiam (visual quality); Success Rate, Avg|E| (effectiveness); Human survey (interpretability)

- **Critical path**:
  1. Semantic graph construction from source/target class object annotations
  2. Bipartite matching to extract optimal edit set E
  3. Per-step: LVLM/local-global ordering selects edit → GroundingDINO+SAM masks → Diffusion inpaints → Classifier predicts → Terminate if flip

- **Design tradeoffs**:
  - Local vs. Global ordering: Local captures scene-specific context but ignores classifier-wide biases; Global exploits systematic biases but may miss image-specific relevance
  - Frozen vs. fine-tuned diffusion: Frozen ensures fair comparison across classifiers; fine-tuned may improve generation quality but introduce dataset-specific biases
  - WordNet vs. custom knowledge graphs: WordNet provides broad coverage but lacks domain-specific visual concepts

- **Failure signatures**:
  - High edit counts with low success rate (e.g., DenseNet: 5.37 avg edits, 85.8% SR for Global) → Classifier semantic space misaligned with human semantics
  - Low visual correctness despite label flip (e.g., EfficientNet: 30.17% visually correct) → Artifacts driving classification, not semantic edits
  - LVLM requires more edits with "thinking" enabled (Claude 3.7: 3.03→3.78 avg edits) → Chain-of-thought impairing visual reasoning

- **First 3 experiments**:
  1. **Reproduce BDD100K results with DenseNet and Claude 3.5 Sonnet**: Compare avg|E|, success rate, and visual correctness; expect DenseNet ~5 edits/85% SR/60% correctness vs. Claude ~2.5 edits/98% SR/80% correctness
  2. **Ablate edit ordering strategies**: Run Local/Global/Local-Global on same image-classifier pairs; quantify how ordering affects edit count and image quality
  3. **Test semantic gap hypothesis**: For DenseNet counterfactuals where humans say label should flip earlier (avg 3 edits earlier), examine intermediate images for semantic coherence vs. artifacts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the V-CECE framework generalize to high-stakes, field-dependent domains such as medical imaging?
- Basis in paper: [explicit] The authors state an intention to "extend the evaluation of the framework across additional disciplines, including the medical domain, to surface challenges and explanation needs in settings with greater field dependence."
- Why unresolved: Current experiments are restricted to autonomous driving (BDD100K) and general scenes (Visual Genome), which may not reflect the nuanced semantic requirements of specialized fields.
- What evidence would resolve it: Successful application of V-CECE to a medical dataset (e.g., CheXpert) demonstrating high Success Rates and semantically coherent edits without domain-specific training.

### Open Question 2
- Question: To what extent does inter-individual human variability affect the measured explanatory gap between humans and models?
- Basis in paper: [explicit] The authors note the current human cohort is modest and plan to "quantify inter-individual variability... to assess inter-rater reliability and identify sources of disagreement."
- Why unresolved: The study relies on a limited pool of participants, which limits the statistical power regarding what constitutes a "human-level" explanation.
- What evidence would resolve it: A large-scale user study measuring the variance in the "Avg |E| Human" metric across diverse demographic groups to establish a confidence interval for the explanatory gap.

### Open Question 3
- Question: How does realistic noise in real-world data impact the stability of the semantic edit proposals?
- Basis in paper: [explicit] The authors plan to "assess robustness to realistic noise in real-world data and measure its impact on both performance and interpretability."
- Why unresolved: The framework is currently evaluated on relatively clean datasets, leaving its sensitivity to sensor noise or environmental distortions unknown.
- What evidence would resolve it: Benchmarking V-CECE performance on perturbed versions of BDD100K (e.g., adding weather noise) to observe changes in Success Rate and edit count.

### Open Question 4
- Question: Is the additive bias introduced by training white-box generative models detrimental to the counterfactual editing process?
- Basis in paper: [explicit] The authors propose to "evaluate white-box generative models and examine whether additive bias is detrimental for the editing modules."
- Why unresolved: V-CECE uses a frozen diffusion model to avoid bias, but it remains untested whether a fine-tuned white-box editor might offer superior performance or suffer from dataset-specific artifacts.
- What evidence would resolve it: A comparative study contrasting the current frozen Stable Diffusion backbone against a white-box diffusion model fine-tuned on the target dataset.

## Limitations

- WordNet semantic distances may inadequately capture visual semantics for domain-specific concepts, potentially leading to suboptimal edit selections
- Visual artifacts from diffusion inpainting may systematically influence classifier decisions independent of semantic edits, particularly for models with different inductive biases
- The effectiveness of LVLM-guided edit ordering depends on the model's spatial reasoning capabilities and potential hallucination tendencies

## Confidence

- **High**: The bipartite matching mechanism for optimal edit selection (proven mathematical foundation, directly validated by edit count comparisons)
- **Medium**: The overall counterfactual generation pipeline (multiple components with complex interactions, though individual parts are well-supported)
- **Low**: The interpretation of classifier performance differences as semantic understanding gaps (alternative explanations like architectural bias or training data differences not fully excluded)

## Next Checks

1. **Cross-domain semantic validation**: Test V-CECE on non-driving datasets (e.g., ImageNet) where WordNet's semantic structure better matches visual semantics to isolate whether performance gaps stem from lexical-visual semantic mismatches
2. **Artifact ablation study**: Generate counterfactuals using a perfect semantic editor (ground truth masks and placements) while keeping classifier frozen, comparing success rates to identify how much performance differences stem from generation artifacts vs. semantic understanding
3. **Classifier-specific semantic probing**: For each classifier type, analyze the semantic content of edits that achieve label flips vs. those that don't, mapping these to known architectural biases to determine whether observed differences reflect true semantic comprehension gaps or model-specific artifacts