---
ver: rpa2
title: Uncovering Emergent Physics Representations Learned In-Context by Large Language
  Models
arxiv_id: '2508.12448'
source_url: https://arxiv.org/abs/2508.12448
tags:
- energy
- learning
- llms
- context
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether large language models (LLMs) can
  learn and represent physical concepts, specifically energy, during in-context learning
  (ICL). The researchers use a dynamics forecasting task involving coupled mass-spring
  and coupled pendulum systems, where the LLM predicts future trajectories based on
  past data.
---

# Uncovering Emergent Physics Representations Learned In-Context by Large Language Models

## Quick Facts
- arXiv ID: 2508.12448
- Source URL: https://arxiv.org/abs/2508.12448
- Reference count: 40
- Primary result: LLMs develop energy-correlated representations during in-context learning, with representation strength scaling with context length

## Executive Summary
This study investigates whether large language models (LLMs) can implicitly learn and represent physical concepts, specifically energy, during in-context learning. Using a dynamics forecasting task with coupled mass-spring and pendulum systems, researchers demonstrate that LLMs' prediction accuracy improves with longer input contexts. Through sparse autoencoder (SAE) analysis of residual stream activations, they identify features that correlate with physical energy quantities. The findings suggest that LLMs can develop functional physics representations during ICL, with energy-related features playing a crucial role in accurate predictions.

## Method Summary
The researchers use Qwen3 to predict future trajectories of coupled mass-spring and pendulum systems from historical data without parameter updates. Trajectories are simulated using Hamiltonian mechanics, tokenized with a single-digit scheme, and processed independently per degree of freedom. Sparse autoencoders (2H expansion ratio, λ=10^-3) are trained on residual stream activations across 17 transformer blocks and 5 context lengths. Energy correlations are computed using Pearson correlation with system energies (E, KE, PE), and top-correlated activations are ablated to test functional relevance.

## Key Results
- Prediction accuracy improves monotonically with context length for both mass-spring and pendulum systems
- Sparse autoencoder analysis reveals features correlating with physical energy quantities, with correlations strengthening as context length increases
- Energy-correlated sparse activations show dual synchronization with both kinetic and potential energy
- Ablating top 1% of energy-correlated activations causes measurable prediction degradation, particularly in high-context regimes

## Why This Works (Mechanism)

### Mechanism 1: Context-Dependent Emergence of Physics Representations
- Claim: LLMs develop internal representations correlating with physical energy during ICL, with strength scaling with context length
- Mechanism: Longer trajectory histories encode patterns in residual stream activations that correlate with conserved quantities rather than surface-level numerical patterns; middle transformer blocks show peak energy correlations
- Core assumption: Correlation pattern reflects structured physics representation, not coincidental numerical alignment
- Evidence anchors:
  - Features captured by SAEs correlate with key physical variables such as energy
  - Energy correlations increase with context length while random baseline correlations decrease monotonically
  - Dual synchronization with both KE and PE distinguishes genuine physics encoding from numerical matching

### Mechanism 2: Sparse Autoencoder Disentanglement of Residual Stream
- Claim: SAEs extract sparse, interpretable features from polysemantic residual stream activations that correlate with energy quantities
- Mechanism: SAEs apply sparse dictionary learning (expansion ratio 2, L1 penalty λ=10^-3) to decompose high-dimensional activations into ~2H sparse features per block, enabling correlation analysis with physical quantities
- Core assumption: Energy-relevant information is linearly decomposable from the residual stream
- Evidence anchors:
  - SAEs enable mechanistic interpretability analysis by disentangling high dimensional polysemantic activations in transformer residual streams into monosemantic features
  - Training curves show SAEs achieve low reconstruction loss with sparse activations
  - SAEs have been applied to ICL interpretability in RL and task vector settings, supporting general methodology

### Mechanism 3: Functional Causality of Energy Representations
- Claim: Energy-correlated sparse activations appear functionally relevant for accurate physics prediction, based on intervention evidence
- Mechanism: Ablating the top 1% of energy-correlated activations removes energy-related features from the residual stream, causing measurable prediction degradation—particularly in high-context regimes where synchronization strength is high
- Core assumption: Correlation reflects functional encoding, not spurious association
- Evidence anchors:
  - Removing sparse activation ci with high energy correlation effectively deletes energy-related features from the residual stream, leading to significant degradation in prediction accuracy
  - Intervention shows larger relative error increases in pendulum system at longer contexts (where dual synchronization is stronger)

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - Why needed here: The entire framework examines how LLMs perform physics tasks from trajectory examples without parameter updates
  - Quick check question: Why does prediction accuracy improve with context length in ICL but not necessarily in fine-tuning?

- **Concept: Transformer Residual Stream**
  - Why needed here: SAE analysis targets residual stream activations—the shared communication bus where attention heads and MLPs interact
  - Quick check question: Why might middle layers show stronger energy correlations than early or late layers?

- **Concept: Hamiltonian Mechanics / Energy Conservation**
  - Why needed here: Energy is the target physical concept; it's a natural candidate because it's conserved in the studied systems, providing a verifiable ground truth
  - Quick check question: Why would a conserved quantity be easier for a model to discover than a non-conserved one?

## Architecture Onboarding

- **Component map:** Simulated trajectories -> Tokenizer (LLMTime-style) -> Qwen3 (single-digit) -> Residual stream activations -> SAE modules (2H expansion) -> Correlation analyzer -> Intervention handler

- **Critical path:**
  1. Simulate coupled mass-spring or pendulum trajectories (Δt=0.1, L_hist + 32 steps)
  2. Tokenize with single-digit scheme; feed univariate sequences independently
  3. Extract residual stream activations at blocks [0, 4, 8, ..., 60, 63]
  4. Train SAEs (10 epochs, lr=10^-4, batch=4096)
  5. Compute activation-energy correlations; verify dual synchronization
  6. Ablate high-correlation activations; measure relative error increase

- **Design tradeoffs:**
  - Single-digit tokenization trades sequence length for robustness to digit perturbations
  - Channel independence simplifies inference but may miss cross-variable coupling
  - Middle-layer focus captures peak energy encoding but may miss other physics in early/late layers

- **Failure signatures:**
  - Energy correlations ~random baseline → no physics representation
  - Low KE-PE synchronization → numerical coincidence, not genuine physics
  - No intervention effect → representations not functionally used
  - Chaotic systems requiring longer contexts for similar patterns → intrinsic predictability difference

- **First 3 experiments:**
  1. Replicate context sweep (L_hist ∈ [64, 128, 256, 512, 1024]) on mass-spring; verify prediction error decreases and energy correlations increase
  2. Train SAE on block 24 at L_hist=1024; confirm top-100 energy correlations exceed random baseline by >2× and dual synchronization >0.5
  3. Ablate top 1% energy-correlated activations at L_hist=1024; verify relative error increase ε > 0.1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the inclusion of chain-of-thought reasoning or structured prompts strengthen the emergence of physics representations in LLMs?
- Basis in paper: [explicit] The authors state in the Discussion, "It remains an open question whether incorporating scratchpads, chain-of-thought reasoning, or structured prompts would further strengthen or clarify the observed correlations."
- Why unresolved: The study utilized only raw trajectory values as input without external instructions or reasoning steps to isolate implicit learning
- What evidence would resolve it: Repeating the sparse autoencoder (SAE) analysis on model activations while using chain-of-thought prompts to see if energy-correlated features appear with higher fidelity or in different layers

### Open Question 2
- Question: Can LLMs learn representations of physical concepts beyond classical mechanics, such as quantum mechanics or electromagnetism, through in-context learning?
- Basis in paper: [explicit] The Discussion suggests, "Future work could explore physics concepts beyond classical mechanics," noting the current study focused only on coupled mass-spring and pendulum systems
- Why unresolved: The experiments were restricted to deterministic and chaotic classical systems governed by Hamiltonian mechanics; other domains involve different governing principles (e.g., probabilistic or field-based)
- What evidence would resolve it: Testing the ICL performance and internal representations on datasets from quantum systems or fluid dynamics to identify if distinct, physically interpretable features (e.g., wavefunctions) emerge

### Open Question 3
- Question: Are the emergent energy representations dependent on the specific tokenization strategy (e.g., single-digit) used by the model?
- Basis in paper: [inferred] The authors selected Qwen3 specifically because it employs single-digit tokenization, which prior works show aids arithmetic tasks. They note other methods can fail if digits change slightly
- Why unresolved: The results may be conflated with the model's ability to process numerical tokens effectively; models with less precise numerical tokenization might fail to form these representations
- What evidence would resolve it: Conducting a comparative analysis across models with different tokenizers (e.g., BPE vs. single-digit) or fine-tuning a model on different tokenization schemes to see if the energy-correlated features persist

### Open Question 4
- Question: Do LLMs form internal representations for other conserved quantities, such as linear or angular momentum, in addition to energy?
- Basis in paper: [inferred] The Introduction asks if "recognizable physical quantities (e.g. mass, velocity, energy, etc.) emerge," but the Results focus almost exclusively on total, kinetic, and potential energy
- Why unresolved: While energy is a central concept, the paper does not verify if the model learns a complete state representation involving other invariants necessary for a full physical description
- What evidence would resolve it: Performing correlation analysis between sparse activations and other system invariants (momentum, angular momentum) to see if distinct features exist for these quantities or if they are entangled with energy features

## Limitations

- Correlation evidence does not definitively prove "understanding" of physics rather than coincidental numerical patterns
- SAE analysis assumes sparse features capture meaningful structure, but interpretability of individual features remains unvalidated
- Analysis is limited to specific systems (mass-spring and pendulum) and may not generalize to other physical domains
- Channel independence assumption in tokenization may miss cross-variable coupling patterns

## Confidence

**High Confidence**: Prediction accuracy improves with context length and energy correlations increase accordingly; intervention experiments show degradation when ablating high-correlation activations

**Medium Confidence**: Interpretation that sparse activations encode meaningful "physics representations" rather than coincidental numerical patterns; dual synchronization with KE and PE is compelling but alternative explanations cannot be ruled out

**Low Confidence**: Claim that LLMs are "learning" physics in a generalizable sense; study demonstrates specific correlation patterns in controlled systems but generalization to broader physical reasoning remains untested

## Next Checks

1. **Cross-Domain Generalization Test**: Apply the same SAE correlation analysis to a different physical system (e.g., fluid dynamics or electromagnetic systems) to determine if energy representations emerge similarly across physics domains

2. **Feature Interpretability Validation**: Manually inspect the top 100 energy-correlated sparse features to determine if they capture interpretable physical patterns (e.g., energy conservation across time steps, velocity-position relationships) beyond simple numerical correlations

3. **Alternative Model Comparison**: Repeat the analysis on a non-LLM architecture (e.g., a physics-informed neural network trained on the same task) to determine whether the energy-correlated representations are specific to transformer-based ICL or would emerge in any architecture solving the same task