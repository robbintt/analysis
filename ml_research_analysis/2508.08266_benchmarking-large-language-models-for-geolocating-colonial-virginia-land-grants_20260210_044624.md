---
ver: rpa2
title: Benchmarking Large Language Models for Geolocating Colonial Virginia Land Grants
arxiv_id: '2508.08266'
source_url: https://arxiv.org/abs/2508.08266
tags:
- error
- mean
- tool
- accuracy
- virginia
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) were benchmarked for geolocating colonial
  Virginia land grants using 43 curated test cases. Direct-to-coordinate prompting
  achieved a mean error of 23 km, outperforming a single-analyst GIS baseline by 67%
  and a Stanford NER geoparser by 70%.
---

# Benchmarking Large Language Models for Geolocating Colonial Virginia Land Grants

## Quick Facts
- **arXiv ID:** 2508.08266
- **Source URL:** https://arxiv.org/abs/2508.08266
- **Reference count:** 33
- **Primary result:** Direct-to-coordinate prompting achieved 23 km mean error, outperforming GIS analyst baseline by 67%.

## Executive Summary
This study benchmarks large language models for geolocating colonial Virginia land grants described in 17th/18th-century metes-and-bounds text. Using 43 curated test cases with ground truth coordinates, the research evaluates six OpenAI models across three approaches: direct prompting, tool-augmented chain-of-thought, and ensemble methods. The best single-call model (o3-2025-04-16) achieved a mean error of 23 km—67% better than a GIS analyst baseline and 70% better than a Stanford NER geoparser. A five-call ensemble further reduced error to 19 km. Tool-augmented approaches degraded accuracy by 30%, highlighting risks of external geocoder dependencies.

## Method Summary
The study evaluated six OpenAI models (o-series, GPT-4, GPT-3.5) on 43 Virginia land grants using three approaches: direct prompting (M-series) requesting DMS coordinates, tool-augmented chain-of-thought (T-series) using Google Geocoding API plus spherical centroid computation, and ensemble methods (E-series) with five calls to o3 clustered via DBSCAN. Performance was measured against ground truth coordinates from the Central VA Patents GIS layer using Haversine distance. The full OCR corpus contains 5,471 abstracts, but only 43 with verified coordinates were used for benchmarking.

## Key Results
- Direct prompting achieved 23 km mean error, outperforming GIS analyst baseline by 67% and Stanford NER geoparser by 70%.
- Five-call ensemble of o3-2025-04-16 reduced error to 19 km (median 12 km) at minimal additional cost.
- Tool-augmented chain-of-thought degraded accuracy by 30% due to spurious geocoder matches.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models encode sufficient geographic knowledge to resolve colonial-era place descriptions without external tools.
- Mechanism: The model parses metes-and-bounds text, identifies toponyms and spatial relations ("S. side of James River," "by run of Holloway Sw"), and internally retrieves or interpolates coordinates through learned statistical associations between place names and locations.
- Core assumption: Training corpora contain geographic information (modern gazetteers, historical texts, maps) that overlaps with colonial Virginia toponyms.
- Evidence anchors:
  - [abstract] "A patentee-name-redaction ablation increased error by about 9%, indicating reliance on textual landmark and adjacency descriptions rather than memorization."
  - [section 6.1] "The best single-call model, M-2 (o3-2025-04-16), attains a mean error of 23 km—a 67% improvement over the GIS analyst baseline."
  - [corpus] Weak direct support; neighbor papers address cartography digitization and general RAG systems, not historical geolocation specifically.
- Break condition: If input text contains only obsolete toponyms with no modern cognates, or if training data lacks geographic coverage for the target region, spatial reasoning will fail or hallucinate.

### Mechanism 2
- Claim: Stochastic ensembling reduces error by averaging away random outliers.
- Mechanism: Multiple independent model calls with different random seeds produce a distribution of predictions; DBSCAN clustering identifies the consensus region, and centroid computation yields the final coordinate.
- Core assumption: Errors are approximately symmetric and uncorrelated across calls; systematic biases (e.g., consistent misplacement of a specific toponym) are not averaged away.
- Evidence anchors:
  - [abstract] "A five-call ensemble further reduced errors to 19 km (median 12 km) at minimal additional cost."
  - [section 4.7] "If at least three predictions fall within the same 0.5 km cluster, their spherical centroid becomes the final answer."
  - [corpus] No direct corpus evidence for this specific geolocation task; ensemble methods are well-established in ML literature but untested for this domain.
- Break condition: If model predictions are highly correlated (same systematic error across calls) or if the task has high irreducible error, ensembling yields diminishing returns.

### Mechanism 3
- Claim: Tool-augmented geocoding can introduce cascading errors that degrade accuracy.
- Mechanism: When the first geocode_place call returns a spurious match (e.g., modern place name coinciding with historical toponym), subsequent reasoning anchors to this incorrect location, and centroid computation propagates the error.
- Core assumption: External geocoders prioritize modern place names and lack historical disambiguation capacity.
- Evidence anchors:
  - [section 6.1] "The tool-chain variant T-4 performed 30% worse than its pure-prompt counterpart."
  - [section 7.2.1] "A notable example involves the Stanford NER method... matched 'St. Paul' to the modern town of Saint Paul, located approximately 400 km from the intended historical Anglican parish."
  - [corpus] No corpus contradiction; neighbor papers on RAG systems suggest external retrieval can help or hurt depending on retrieval quality.
- Break condition: If external tools return only high-confidence, historically-grounded matches, or if the model has explicit fallback heuristics to reject implausible geocoder outputs, this failure mode is mitigated.

## Foundational Learning

- Concept: **Metes-and-bounds descriptions**
  - Why needed here: Understanding that colonial land grants describe boundaries through narrative traversals ("beg. on S. side the Black Water; to the Nottoway Path") rather than coordinate-based surveys.
  - Quick check question: Can you identify the boundary markers and their sequence in this excerpt: "on S. side of James River; Beg. on S. side the Black Water; to the Nottoway Path"?

- Concept: **Great-circle distance (Haversine formula)**
  - Why needed here: Evaluation metric converts coordinate pairs to kilometers; understanding this clarifies what "23 km error" means geometrically.
  - Quick check question: If two points differ by 0.3° latitude in Virginia (~37°N), approximately what distance does that represent?

- Concept: **Bootstrap confidence intervals**
  - Why needed here: Reported error ranges (e.g., "23.4 [17.4, 29.3] km") derive from resampling; understanding this prevents overinterpreting small differences.
  - Quick check question: Why might bootstrap CIs be preferred over parametric CIs for a distribution with long-tail outliers?

## Architecture Onboarding

- Component map: OCR-normalized abstract text → Prompt template (one-shot or tool-augmented) → Model (OpenAI API) → Optional tool layer (geocode_place + compute_centroid) → Coordinate string → Ensemble post-processing (DBSCAN clustering) → Final centroid

- Critical path:
  1. Ingest abstract text and validate format (word count, hash)
  2. Apply one-shot prompt; call model API with fixed seed
  3. Parse coordinate output; compute Haversine distance to ground truth
  4. (If ensemble) Repeat 5× with different seeds; cluster and aggregate
  5. Log token usage, latency, and tool traces

- Design tradeoffs:
  - Accuracy vs. cost: o3-2025-04-16 achieves 23 km error at ~$0.13/grant; gpt-4o achieves 28 km at ~$0.001/grant
  - Tools vs. pure prompting: Tools add latency (2–4×) and can degrade accuracy; avoid unless external knowledge is provably helpful
  - Single-call vs. ensemble: 18% accuracy gain for 5× cost; worthwhile for high-stakes geolocation

- Failure signatures:
  - Spurious geocoder match: Tool output places location in wrong county or state
  - Obsolete toponym: Model hallucinates coordinates for place name with no modern equivalent
  - Bearing-only description: No named landmarks → median error >70 km
  - Cascading error: Early tool mistake propagates through compute_centroid

- First 3 experiments:
  1. Reproduce baseline comparison on 43-grant test set using provided code; verify M-2 (o3) vs. H-1 (GIS analyst) gap.
  2. Ablate prompt: Remove county name from input; measure error increase to quantify county-level cue importance.
  3. Test alternative geocoder: Replace Google with Nominatim (OpenStreetMap); compare tool-chain performance to isolate geocoder-specific bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-generated point estimates be successfully combined with GIS operations to recover full parcel polygons?
- Basis in paper: [explicit] Section 9 (Future Work) proposes combining point estimates with chained GIS operations like bearing decoding and river buffering to approximate parcel outlines for environmental history applications.
- Why unresolved: The study focused solely on generating latitude/longitude coordinates and did not attempt to trace the boundary lines described in the metes-and-bounds text.
- What evidence would resolve it: A pipeline that converts bearing descriptions into GIS polygons and validates them against ground-truth geometry, such as the One Shared Story dataset.

### Open Question 2
- Question: Do these geolocation capabilities generalize to open-source model families (e.g., Llama, Mistral)?
- Basis in paper: [explicit] Section 8 (Limitations) notes that the study limited models to OpenAI's series to control confounders, leaving generalization to other families "to future replication under matched settings."
- Why unresolved: Tokenization, decoding, and tool-call semantics differ across vendors, making it unclear if the success of GPT/o-series models transfers to open-weight alternatives.
- What evidence would resolve it: A replication of the benchmark using the released dataset on open-source models with matched hyperparameters and prompts.

### Open Question 3
- Question: To what extent does training-data contamination inflate the accuracy of LLMs on historical geolocation tasks?
- Basis in paper: [explicit] Section 8 identifies training data contamination as a "significant limitation" and calls for future "systematic contamination analysis" or evaluation on guaranteed unseen spatial datasets.
- Why unresolved: While the text corpus is obscure, the GIS datasets used for ground truth (e.g., One Shared Story layer) may be present in training corpora, and standard leakage audits provided inconclusive results.
- What evidence would resolve it: Evaluation on a "clean" dataset of historical grants confirmed to be absent from model training data, or the use of data influence attribution methods.

## Limitations
- Historical place-name disambiguation: Models may fail on obsolete or ambiguous toponyms, with cascading errors from geocoders causing >100km errors.
- Generalizability: Results from Central Virginia (1710-1735) may not extend to frontier regions or later periods with sparser training data coverage.
- Tool-augmented degradation: External geocoders introduced 30% accuracy drop due to spurious modern matches, with no tested mitigation strategies.

## Confidence
- **High confidence**: LLM superiority over GIS analyst baseline (67% error reduction) and Stanford NER geoparser (70% reduction) supported by controlled A/B tests.
- **Medium confidence**: Ensemble effectiveness (18% additional error reduction) statistically significant but assumes uncorrelated errors across calls.
- **Low confidence**: Tool-augmented accuracy claims - 30% degradation is stark but lacks testing of alternative geocoders or mitigation strategies.

## Next Checks
1. **Replicate with alternative geocoder**: Replace Google Geocoding API with OpenStreetMap's Nominatim in the T-series pipeline. If accuracy improves, the issue is Google-specific; if not, the flaw is inherent to external API reliance.
2. **Test frontier grants**: Run the benchmark on 10-20 grants from Virginia's western frontier (1740s-1760s), where toponyms are more likely obsolete. Compare error rates to the Central Virginia baseline to assess regional generalizability.
3. **Ablate county-level cues**: Remove county names from prompts for 10 grants and measure error increase. This isolates whether county constraints are critical scaffolding or merely helpful priors.