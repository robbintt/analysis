---
ver: rpa2
title: 'Likert or Not: LLM Absolute Relevance Judgments on Fine-Grained Ordinal Scales'
arxiv_id: '2505.19334'
source_url: https://arxiv.org/abs/2505.19334
tags:
- ranking
- scoring
- query
- documents
- relevance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study challenges the prevailing view that large language models
  (LLMs) are better at relative than absolute relevance judgments. We find that pointwise
  scoring with a sufficiently large ordinal relevance scale (11-point) performs comparably
  to listwise ranking methods across multiple models and datasets.
---

# Likert or Not: LLM Absolute Relevance Judgments on Fine-Grained Ordinal Scales

## Quick Facts
- arXiv ID: 2505.19334
- Source URL: https://arxiv.org/abs/2505.19334
- Reference count: 40
- This study challenges the prevailing view that large language models (LLMs) are better at relative than absolute relevance judgments, showing pointwise scoring with 11-point scales performs comparably to listwise ranking.

## Executive Summary
This paper challenges the conventional wisdom that LLMs excel at relative (listwise) ranking but struggle with absolute (pointwise) relevance judgments. Through extensive experiments across four LLMs and ten benchmark datasets, the authors demonstrate that using an 11-point ordinal relevance scale for pointwise scoring achieves comparable performance to listwise ranking methods. The study introduces ranking+scoring methods that output both scores and ranks, offering improved interpretability without sacrificing ranking quality. These findings have significant implications for the efficiency and evaluation practices in information retrieval systems.

## Method Summary
The study compares three approaches for LLM-based relevance judgment: pointwise scoring (document-by-document absolute scoring), listwise ranking (relative ordering of document sets), and ranking+scoring (simultaneous output of scores and ranks). Using zero-shot prompting, the authors evaluate these methods across 10 benchmark datasets with four different LLMs, employing an 11-point ordinal relevance scale (0-10) with detailed semantic rubrics. For listwise methods, telescoping implementations handle document lists longer than the model's context window. The primary evaluation metric is NDCG@10, with bootstrap resampling used to compute 95% confidence intervals for significance testing.

## Key Results
- Pointwise scoring with 11-point scales performs comparably to listwise ranking methods across multiple models and datasets
- The performance gap between pointwise scoring and listwise ranking shrinks significantly, with no statistically significant difference in NDCG@10 for many model-dataset combinations
- Ranking+scoring methods simultaneously output scores and ranks, enabling direct comparison of label quality without significantly impacting ranking performance
- Fine-grained ordinal scales (11-point) significantly improve LLM scoring performance compared to coarse scales (2-point)

## Why This Works (Mechanism)
The paper's central finding hinges on the use of fine-grained ordinal scales. By providing LLMs with an 11-point scale instead of a binary classification, the models can better capture the nuanced differences between document relevance levels. This approach leverages the LLM's ability to understand ordinal relationships while avoiding the complexity of relative ranking operations. The structured output format and detailed semantic rubric in the prompts help ensure consistent interpretation of relevance levels across different documents and queries.

## Foundational Learning
- **Concept: Pointwise vs. Listwise Relevance Judgment**
  - Why needed here: This is the central comparison of the paper. You must understand that pointwise = score one document at a time, while listwise = rank a set of documents relative to one another.
  - Quick check question: If you have 100 documents, which method requires N LLM calls and which can be done with fewer, more complex calls?

- **Concept: Ordinal Relevance Scale**
  - Why needed here: The paper's key finding hinges on using a fine-grained scale (11-point: 0-10) instead of a coarse one (2-point: relevant/irrelevant). The number of "bins" is a critical hyperparameter.
  - Quick check question: On a 2-point scale, how would you differentiate between a document that perfectly answers a query versus one that only partially answers it? How does an 11-point scale help?

- **Concept: NDCG@10 (Normalized Discounted Cumulative Gain at 10)**
  - Why needed here: This is the primary evaluation metric. It measures ranking quality, giving more credit for relevant items appearing higher in the list.
  - Quick check question: If a system swaps the top two documents (rank 1 and rank 2), will the NDCG change more if they are both highly relevant or if one is highly relevant and the other is not?

## Architecture Onboarding
- **Component map**: Input Module -> Prompt Engineering Layer -> LLM Core -> Output Parser -> Sorting/Aggregation Engine -> Evaluator
- **Critical path**: Prompt formulation (especially the relevance rubric for the 11-point scale) → Structured output generation and parsing → Correct implementation of the telescoping sort for listwise methods
- **Design tradeoffs**: Efficiency vs. Performance (pointwise is parallel and token-efficient, listwise is serial but was thought to be superior in quality) → Simplicity vs. Interpretability (pure permutation is harder to interpret than explicit score) → Scale Granularity (larger scale improves performance but requires more complex rubric)
- **Failure signatures**: Pointwise Score Clustering (outputs scores in narrow range like 7-9 on 0-10 scale) → Listwise Inconsistency (returns non-transitive ranking like A > B, B > C, but C > A) → Schema Violation (fails to return valid JSON, especially for ranking+scoring method)
- **First 3 experiments**: 1) Scale Ablation: Implement pointwise scoring on single benchmark with 2, 3, 5, 7, and 11-point scales to verify NDCG@10 increases with scale size → 2) Pointwise vs. Listwise Comparison: On 2-3 benchmarks, compare NDCG@10 of 11-point pointwise scoring against baseline listwise method (bubble sort) → 3) Ranking+Scoring Validation: Implement ranking+scoring prompt and verify its NDCG@10 is not significantly worse than pure permutation generation

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What specific properties of a retrieval task (e.g., query distribution, corpus structure) determine whether LLM relative relevance judgments (listwise) offer a significant advantage over absolute judgments (pointwise)?
- Basis in paper: [explicit] The authors state: "We leave an in-depth investigation of what properties of a retrieval task (query distribution, corpus, etc.) make LLM relative relevance judgments more valuable to future work."
- Why unresolved: While the study shows listwise ranking significantly outperforms pointwise scoring on the novel Tax & Accounting datasets, the gap is statistically insignificant on many public benchmarks. The precise dataset characteristics driving this divergence are not isolated.

### Open Question 2
- Question: Does data pollution (memorization) in public benchmarks artificially compress the performance gap between pointwise and listwise ranking methods?
- Basis in paper: [explicit] In Footnote 6 (Page 6), the authors hypothesize: "Hence it's possible for data pollution to make the performance of the two methods appear more similar than it would have been absent data pollution."
- Why unresolved: The study found that listwise methods outperformed pointwise methods on novel, proprietary data more often than on public data. It is unclear if this is due to the novel domain or because memorization allows both methods to achieve near-perfect accuracy on contaminated public benchmarks.

### Open Question 3
- Question: Does utilizing task-specific relevance rubrics (e.g., for argument retrieval) alter the comparative effectiveness of pointwise scoring versus listwise ranking?
- Basis in paper: [inferred] Page 7 notes that the Touche dataset was an outlier, which "possibly stems from a mismatch between our generic 'rank/score search results by relevance' prompts... and the more nuanced argument retrieval task the Touche dataset was constructed to evaluate."
- Why unresolved: The study applied uniform, generic relevance prompts across all datasets. It is not tested whether the observed performance differences on Touche are fundamental limitations of the ranking methods or artifacts of using a generic prompt for a nuanced task.

## Limitations
- The study lacks statistical significance testing for most pairwise comparisons between methods, making it difficult to definitively claim "no significant difference" between approaches
- Heavy reliance on proprietary datasets (Tax & Accounting) that are not publicly available, limiting reproducibility and external validation
- Focus on NDCG@10 as the primary metric may not capture all aspects of ranking quality, particularly for datasets where precision at lower ranks matters more

## Confidence
- **High Confidence**: The finding that fine-grained ordinal scales (11-point vs 2-point) significantly improve LLM scoring performance is well-supported by Table 1's systematic ablation results across multiple models
- **Medium Confidence**: The claim that pointwise scoring with 11-point scales performs "comparably" to listwise ranking is supported but requires more rigorous statistical testing, particularly for the proprietary Tax & Accounting datasets
- **Low Confidence**: The ranking+scoring method's purported advantages in interpretability and efficiency are not empirically validated beyond basic performance comparisons, and the lack of significance testing for most comparisons weakens these claims

## Next Checks
1. **Statistical Significance Analysis**: Recompute all pairwise comparisons between pointwise, listwise, and ranking+scoring methods using bootstrap confidence intervals. Flag any comparisons where confidence intervals overlap substantially as "no significant difference."

2. **Scale Granularity Impact**: Systematically vary the ordinal scale (2, 3, 5, 7, 11 points) on a held-out dataset to quantify the marginal benefit of additional granularity beyond what's shown in Table 1.

3. **Cross-Dataset Generalization**: Test the ranking+scoring method on a broader set of public datasets (beyond BEIR) to validate whether the performance parity observed in Table 2 generalizes to different domains and query types.