---
ver: rpa2
title: Thai Semantic End-of-Turn Detection for Real-Time Voice Agents
arxiv_id: '2510.04016'
source_url: https://arxiv.org/abs/2510.04016
tags:
- zero-shot
- thai
- detection
- turn
- fine-tuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce the first systematic study of Thai text-only end-of-turn
  (EOT) detection for real-time voice agents, using transcribed subtitles from the
  YODAS corpus. We compare zero-shot and few-shot prompting of compact LLMs with supervised
  fine-tuning of lightweight transformers, formulating EOT as a binary decision over
  token boundaries.
---

# Thai Semantic End-of-Turn Detection for Real-Time Voice Agents

## Quick Facts
- arXiv ID: 2510.04016
- Source URL: https://arxiv.org/abs/2510.04016
- Reference count: 38
- First systematic study of Thai text-only end-of-turn (EOT) detection for real-time voice agents, using transcribed subtitles from the YODAS corpus

## Executive Summary
This work introduces the first systematic study of Thai text-only end-of-turn (EOT) detection for real-time voice agents, using transcribed subtitles from the YODAS corpus. The study compares zero-shot and few-shot prompting of compact LLMs with supervised fine-tuning of lightweight transformers, formulating EOT as a binary decision over token boundaries. The fine-tuned Llama3.2-T yphoon2-1B achieved the highest F1-score of 0.881 with low CPU latency (110ms), while zero-shot methods required threshold calibration and instruction prompting was too slow for real-time use. This work establishes a Thai EOT baseline and shows that small, fine-tuned models can deliver near-instant EOT decisions suitable for on-device agents.

## Method Summary
The study formulates EOT detection as a binary classification task over token boundaries in transcribed Thai dialogue. Using the YODAS corpus of Thai subtitles, the researchers compared three approaches: zero-shot prompting with compact LLMs, few-shot prompting with instruction-tuned models, and supervised fine-tuning of lightweight transformers. The fine-tuning approach used Llama3.2-T yphoon2-1B, a 1B parameter model optimized for Thai. Models were evaluated on F1-score and CPU inference latency, with the goal of identifying approaches suitable for real-time voice agents. The dataset contained approximately 10,000 annotated token boundaries, split into training, validation, and test sets.

## Key Results
- Fine-tuned Llama3.2-T yphoon2-1B achieved highest F1-score of 0.881
- Zero-shot prompting required threshold calibration to achieve reasonable performance
- Instruction-tuned prompting exceeded acceptable latency thresholds for real-time applications (110ms CPU latency for best model)

## Why This Works (Mechanism)
The study demonstrates that end-of-turn detection in Thai dialogue can be effectively learned from subtitle-derived boundaries through supervised fine-tuning. By framing EOT as a binary classification over token boundaries rather than requiring full utterance understanding, the task becomes tractable for compact models. The yphoon2-1B architecture, optimized for Thai language with appropriate tokenization and subword modeling, captures the linguistic cues that signal turn completion. The success of fine-tuning over zero-shot approaches suggests that EOT detection requires domain-specific patterns that cannot be easily inferred from general language understanding alone.

## Foundational Learning
- **Thai language processing**: Thai lacks explicit word boundaries, requiring specialized tokenization approaches; needed for accurate token-level classification
- **End-of-turn detection concepts**: Understanding turn-taking dynamics in dialogue; needed to formulate the EOT task appropriately
- **Subtitle alignment challenges**: Subtitle timing doesn't always align with natural conversational turn boundaries; needed to understand dataset limitations
- **Latency-aware model selection**: Trade-off between model accuracy and inference speed; needed for real-time voice agent deployment
- **Prompt engineering vs. fine-tuning**: Different approaches to adapting pre-trained models; needed to compare methodology effectiveness
- **Binary sequence classification**: Standard NLP task formulation; needed to implement the EOT detection framework

## Architecture Onboarding

**Component Map**
Tokenized input -> yphoon2-1B model -> Binary classifier -> EOT decision output

**Critical Path**
Tokenization → Embedding layer → Transformer blocks → Classification head → Latency measurement

**Design Tradeoffs**
The study prioritized CPU latency over absolute accuracy, accepting a slight performance drop to achieve real-time inference speeds. This trade-off favored smaller models (1B parameters) over larger alternatives, demonstrating that modest architectures can handle EOT detection effectively when properly fine-tuned for the specific task and language.

**Failure Signatures**
Models may misclassify turns when encountering: incomplete sentences, ellipsis or trailing particles that don't indicate turn completion, overlapping speech not present in subtitle data, or domain-specific vocabulary patterns not represented in the training corpus.

**First Experiments**
1. Test zero-shot performance on held-out test set with varying threshold values
2. Compare CPU latency across different batch sizes for the fine-tuned model
3. Evaluate model robustness on artificially shortened/longer turns to assess boundary sensitivity

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can integrating lightweight prosodic features (pitch, energy, pause duration) significantly improve Thai EOT detection accuracy compared to text-only approaches?
- Basis in paper: [explicit] Conclusion states: "Future work will integrate lightweight prosodic features and extend to multi-party overlap."
- Why unresolved: This study deliberately excluded acoustic cues, focusing only on text-based detection. The contribution of prosody to Thai EOT remains unquantified.
- What evidence would resolve it: A multimodal model combining text with extracted prosodic features, evaluated on the same YODAS benchmark with statistically significant F1 improvements.

### Open Question 2
- Question: How does text-only EOT detection performance degrade in multi-party dialogue scenarios with speaker overlap and interruptions?
- Basis in paper: [explicit] Section VII states: "We do not model acoustic cues (prosody, overlap), which matter in multi-party settings."
- Why unresolved: The YODAS subtitle data and binary EOT formulation assume single-speaker turns, not overlapping speech common in multi-party conversations.
- What evidence would resolve it: Evaluation on a multi-party Thai dialogue corpus measuring precision/recall specifically on overlapping or interrupted turns.

### Open Question 3
- Question: To what extent do subtitle-derived labels diverge from ground-truth conversational turn boundaries in Thai spoken dialogue?
- Basis in paper: [explicit] Section VII notes: "Our labels inherit subtitle biases and timing drift; true conversational EOT may diverge from subtitle line breaks."
- Why unresolved: Subtitle segmentation follows editorial conventions rather than natural turn-taking behavior, potentially introducing systematic label noise.
- What evidence would resolve it: Human annotation of turn boundaries on audio-aligned transcripts, with comparison of model performance against subtitle labels versus human-verified labels.

### Open Question 4
- Question: How well do fine-tuned Thai EOT models generalize across conversational domains (e.g., customer service, casual chat, formal meetings)?
- Basis in paper: [inferred] The dataset derived from YODAS subtitles contains heterogeneous content; the paper does not report per-domain performance or out-of-distribution robustness.
- Why unresolved: Domain-specific linguistic patterns (vocabulary, formality, particle usage) may affect EOT cue distributions differently.
- What evidence would resolve it: Cross-domain evaluation where models trained on one domain (e.g., media subtitles) are tested on held-out domains (e.g., banking dialogs, phone conversations).

## Limitations
- Relies exclusively on transcribed subtitles which may not fully represent natural conversational turn-taking patterns
- Text-only approach excludes potentially valuable acoustic-prosodic cues for EOT detection
- Evaluation metrics don't account for deployment-specific constraints like memory usage or energy consumption

## Confidence
- Fine-tuned Llama3.2-T yphoon2-1B achieves highest F1-score of 0.881: High confidence
- Zero-shot methods require threshold calibration: Medium confidence
- Instruction prompting is too slow for real-time use: Medium confidence
- Small, fine-tuned models deliver near-instant EOT decisions suitable for on-device agents: Medium confidence

## Next Checks
1. Evaluate model performance on out-of-domain Thai conversational data to assess robustness across different speaking styles and contexts
2. Compare text-only EOT detection against multimodal approaches that incorporate acoustic features to determine if the 0.881 F1-score represents a ceiling for text-only methods
3. Conduct deployment testing on actual edge devices to measure real-world latency, memory usage, and energy consumption under varying workload conditions