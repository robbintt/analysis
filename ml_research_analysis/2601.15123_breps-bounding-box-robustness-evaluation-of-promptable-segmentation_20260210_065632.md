---
ver: rpa2
title: 'BREPS: Bounding-Box Robustness Evaluation of Promptable Segmentation'
arxiv_id: '2601.15123'
source_url: https://arxiv.org/abs/2601.15123
tags:
- segmentation
- image
- tight
- bbox
- bounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BREPS, a white-box adversarial attack method
  to evaluate the robustness of promptable segmentation models (e.g., SAM) to natural
  bounding box variations. The authors conducted a large-scale user study with 2,500
  annotators, collecting thousands of real bounding boxes across desktop and mobile
  devices.
---

# BREPS: Bounding-Box Robustness Evaluation of Promptable Segmentation

## Quick Facts
- arXiv ID: 2601.15123
- Source URL: https://arxiv.org/abs/2601.15123
- Reference count: 40
- Primary result: BREPS reveals 30% average IoU gap in promptable segmentation models under realistic bounding box variations

## Executive Summary
BREPS introduces a white-box adversarial attack method to evaluate the robustness of promptable segmentation models (e.g., SAM) to natural bounding box variations. The authors conducted a large-scale user study with 2,500 annotators, collecting thousands of real bounding boxes across desktop and mobile devices. They found significant variability in segmentation quality across users for the same model and instance, indicating high sensitivity to prompt noise. To address computational infeasibility of exhaustive testing, they reformulated robustness evaluation as an optimization problem, using a differentiable naturalness regularizer derived from real-user data. BREPS generates adversarial bounding boxes that maximize or minimize segmentation error while remaining realistic.

## Method Summary
BREPS reformulates robustness evaluation as a gradient-based optimization problem over bounding box coordinates, constrained by a realism regularizer derived from empirical user annotations. The method performs gradient descent in 4D bbox parameter space (x1, y1, x2, y2) while regularizing with log-probability from a Gamma distribution fitted to real-user CIoU-Loss values. This prevents degenerate solutions while finding adversarially-effective perturbations. The attack runs 50 Adam steps with combined loss: Dice loss plus λ×(−log-PDF) where λ=0.1, targeting both IoU maximization and minimization to reveal performance boundaries.

## Key Results
- Average performance gap of 30% IoU under realistic prompt variations across 10 datasets
- Tight boxes often overestimate real-world performance, indicating model overfitting to synthetic prompts
- Mobile bounding boxes show significantly more deviation from tight boxes than desktop (U-test p<0.01)
- Cross-dataset IoU-Δ values correlate (r=0.87) with visual complexity metrics

## Why This Works (Mechanism)

### Mechanism 1: Naturalness-Constrained Adversarial Optimization
Reformulating robustness evaluation as optimization over bounding box coordinates, constrained by real-user distributions, efficiently identifies performance boundaries that exhaustive search cannot reach. The method performs gradient descent directly in 4D bbox parameter space while regularizing with log-probability from empirical CIoU-Loss distributions.

### Mechanism 2: CIoU-Based Realism Distribution Fitting
Fitting a Gamma distribution to CIoU-Loss between user-drawn boxes and tight boxes provides a differentiable proxy for human annotation behavior. The empirical distribution from 25,000 annotations was fit to Gamma(k=1.789, θ=0.121), serving as the realism regularizer during optimization.

### Mechanism 3: IoU Sensitivity Through Spatial Heatmap Analysis
Exhaustive search over centered bounding box configurations reveals discontinuous IoU landscapes where 1-pixel coordinate shifts can cause >50% IoU drops. Per-pixel IoU heatmaps identify spatial regions where models are fragile to prompt perturbations.

## Foundational Learning

- **Concept: Complete IoU (CIoU) Loss**
  - Why needed here: Core metric for measuring bounding box quality, used in both user study analysis and realism regularizer
  - Quick check question: Given two non-overlapping boxes, would standard IoU provide useful gradient information for optimization?

- **Concept: White-Box Adversarial Attacks**
  - Why needed here: BREPS operates as white-box attack, requiring access to model gradients through prompt coordinates
  - Quick check question: What information do you need access to in order to compute gradients with respect to input prompt coordinates?

- **Concept: Promptable Segmentation Architecture**
  - Why needed here: Understanding how prompts are encoded and fused with image features determines whether gradient-based prompt optimization is feasible
  - Quick check question: In SAM-like architectures, where are prompt embeddings combined with image embeddings, and what does this imply for gradient flow?

## Architecture Onboarding

- **Component map**: User Study Pipeline -> CIoU Distribution Fitting -> BREPS Attack Pipeline -> Evaluation Metrics
- **Critical path**: 1) Collect CIoU distribution parameters, 2) Initialize tight bbox, 3) Run 50 Adam steps with combined loss, 4) Clip coordinates and enforce validity
- **Design tradeoffs**: λ=0.1 balances realism vs attack strength; 50 steps chosen for gradient convergence; single Gamma fitted across all datasets for simplicity
- **Failure signatures**: Degenerate boxes if λ too low, unrealistic adversarial examples, convergence to local minima, resolution mismatch in LR scaling
- **First 3 experiments**: 1) Validate realism regularizer via Langevin sampling, 2) Ablate λ values on held-out subset, 3) Cross-device validation with separate regularizers

## Open Questions the Paper Calls Out

### Open Question 1
Can incorporating BREPS-generated adversarial bounding boxes into the training pipeline improve the robustness of promptable segmentation models without sacrificing baseline performance on tight boxes?

### Open Question 2
Does utilizing device-specific (mobile vs. desktop) or dataset-specific realism regularizers yield significantly different robustness evaluations compared to the unified Gamma distribution used in the study?

### Open Question 3
Are the adversarial bounding boxes identified by BREPS global minima, or do non-gradient search methods reveal significantly lower performance bounds?

## Limitations
- BREPS relies on white-box access to model gradients, limiting applicability to black-box scenarios
- Single Gamma distribution fitted across all datasets/devices may not capture domain-specific annotation behaviors
- Gradient-based optimization may get trapped in local minima, potentially underestimating true vulnerability

## Confidence

- **High**: Optimization framework, user study protocol, dataset selection
- **Medium**: CIoU-Loss distribution fitting, cross-device annotation consistency, λ selection
- **Low**: Generalization to non-SAM architectures, expert vs. crowd annotation equivalence, domain transfer of realism regularizer

## Next Checks

1. **Cross-Domain Distribution Transfer**: Train CIoU regularizer on one domain (e.g., GrabCut) and evaluate BREPS on disjoint domain (e.g., ACDC). Measure segmentation quality degradation.

2. **Expert Annotator Validation**: Conduct focused study with medical imaging experts to collect bounding boxes for ACDC instances. Fit separate Gamma distribution and compare to crowd-worker distribution.

3. **Black-Box Gradient Approximation**: Implement zeroth-order optimization variant of BREPS using finite-difference perturbations. Compare IoU-Δ between white-box and black-box approaches on subset of datasets.