---
ver: rpa2
title: 'AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models'
arxiv_id: '2505.22662'
source_url: https://arxiv.org/abs/2505.22662
tags:
- reasoning
- autol2s
- short
- long
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AutoL2S, a framework for efficient reasoning
  in large language models. It addresses the overthinking problem by enabling models
  to dynamically choose between long and short reasoning paths based on question complexity.
---

# AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models

## Quick Facts
- arXiv ID: 2505.22662
- Source URL: https://arxiv.org/abs/2505.22662
- Reference count: 40
- Key outcome: Reduces reasoning length by up to 71% with minimal accuracy loss across multiple benchmarks

## Executive Summary
AutoL2S addresses the overthinking problem in large language models by enabling dynamic selection between long and short reasoning paths based on question complexity. The framework introduces an `<EASY>` token to supervise instance-wise selection between reasoning modes, combined with GRPO-style fine-tuning to improve efficiency while preserving accuracy. Experiments demonstrate substantial improvements in inference time and token usage, with up to 71% reduction in reasoning length and 59.6% latency improvement while maintaining competitive accuracy.

## Method Summary
AutoL2S operates in two stages: first, a lightweight `<EASY>` token is introduced during supervised fine-tuning (SFT) to enable instance-wise selection between long and short reasoning paths. The model learns to predict this token based on input complexity, with valid short paths identified through rejection sampling. Second, GRPO-style reinforcement learning refines the SFT model to favor shorter rollouts when correctness is preserved. The framework leverages strong teacher models (DeepSeek-R1, Qwen2.5-Math) to generate paired long-short reasoning paths, with the SFT stage conditioning short reasoning on verified long paths to reduce learning uncertainty.

## Key Results
- Reduces reasoning length by up to 71% on MATH benchmark with minimal accuracy loss
- Achieves 59.6% latency reduction while maintaining competitive accuracy
- Demonstrates strong Pareto frontier performance between accuracy and token efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditioning short reasoning paths on verified long reasoning paths during training reduces the learning uncertainty for concise generation.
- Mechanism: The framework concatenates long and short Chain-of-Thought (CoT) paths. The paper posits (via Lemma 1) that the long path provides auxiliary context (mutual information) that lowers the conditional entropy of the short path tokens. Early in training, attention maps show high correlation between long and short paths; later, they decouple, suggesting the model first uses the long path to ground the short one, then learns to generate the short path independently.
- Core assumption: The long reasoning path contains sufficient signal to inform the compressed version without introducing noise.
- Evidence anchors:
  - [section 3.3] "Lemma 1... suggesting that conditioning short CoT reasoning paths on long reasoning can reduce learning uncertainty."
  - [section 4.8] "In the early stages... long CoT reasoning paths significantly impact the attention patterns... As training progresses... correlation... significantly diminishes."
- Break condition: If the long reasoning path contains irrelevant or erroneous steps, conditioning on it may increase noise rather than reducing uncertainty.

### Mechanism 2
- Claim: A lightweight switching token (`<EASY>`) allows the model to learn an instance-wise allocation policy between computational cost and reasoning sufficiency.
- Mechanism: AutoL2S introduces a special token `<EASY>`. During Supervised Fine-Tuning (SFT), this token is prepended to questions where a verified short path exists. The model learns to predict this token based on input complexity. At inference, the emission of `<EASY>` (followed by a trigger) routes generation to a short path; otherwise, it defaults to the standard long path.
- Core assumption: Question complexity can be reliably predicted from the input embedding before generating the full reasoning trace.
- Evidence anchors:
  - [abstract] "introduces a lightweight <EASY> switching token during distillation to enable instance-wise selection."
  - [section 3.1] "For inputs admitting both valid long and short CoT... we annotate the question with the <EASY> token."
- Break condition: If the distribution of "easy" vs. "complex" questions in the training set is heavily skewed or mislabeled, the switching classifier will suffer from high false positive/negative rates.

### Mechanism 3
- Claim: GRPO-style reinforcement learning refines the SFT model to favor shorter rollouts when correctness is preserved.
- Mechanism: The SFT model generates long-short rollouts. The optimization objective (GRPO-style) maximizes an advantage function based on outcome correctness. By initializing from the SFT model (which already knows how to switch modes) and using a clipping function, the policy is pushed toward correct outcomes while implicitly favoring the efficient "short" trajectories learned in Stage 1.
- Core assumption: The SFT model provides a sufficiently diverse set of rollouts (both long and short) for the RL stage to effectively explore the efficiency-accuracy frontier.
- Evidence anchors:
  - [abstract] "This is followed by GRPO-style fine-tuning using long-short reasoning rollouts induced by the <EASY> token."
  - [section 3.2.2] "The objective function... maximizes the model accuracy while keeping the output length distribution close to the SFT model."
- Break condition: If the KL constraint (clipping) is too tight, the model cannot deviate enough to correct SFT errors; if too loose, it may hallucinate shorter but incorrect paths.

## Foundational Learning

- Concept: **Knowledge Distillation**
  - Why needed here: AutoL2S is fundamentally a distillation framework transferring reasoning capabilities from strong teacher models (e.g., DeepSeek-R1) to non-reasoning student models.
  - Quick check question: Can you explain how a student model minimizes the difference (divergence) between its output distribution and a teacher's "reasoning" distribution?

- Concept: **Reinforcement Learning from Human Feedback (RLHF) / GRPO**
  - Why needed here: The paper utilizes a Group Relative Policy Optimizer (GRPO) loss in Stage 2. Understanding policy gradients and reward signals is required to grasp how the model optimizes for accuracy vs. length.
  - Quick check question: In a policy gradient setting, how does the "advantage" function modify the probability of a specific action (token generation)?

- Concept: **Inference-time Compute / "Overthinking"**
  - Why needed here: The paper addresses the "overthinking" problem where models waste compute on simple queries. Understanding the trade-off between thinking time (tokens) and accuracy is the core motivation.
  - Quick check question: Why does simply forcing a model to generate fewer tokens often lead to a "sharp" drop in accuracy?

## Architecture Onboarding

- Component map: Data Engine -> Stage 1 (SFT) -> Stage 2 (GRPO) -> Inference Engine
- Critical path: The **Rejection Sampling** process (Section 3.1) is the critical dependency. You must generate `k` candidate short paths and verify them against ground truth. If this verification fails (e.g., false positives where a short path looks right but is wrong), the `<EASY>` token supervision will be corrupted.
- Design tradeoffs:
  - **Rejection Sampling Size (`k`)**: Higher `k` (e.g., 8 vs 4) increases the probability of finding a valid short path, leading to more compression. However, aggressive compression in SFT can lower accuracy, requiring the RL stage to recover it (Section 4.7).
  - **Force-Short vs. Adaptive**: Forcing short reasoning (`w/ Force-short`) degrades accuracy significantly (Table 2). The system relies on the model's autonomy to reject short reasoning for complex problems.
- Failure signatures:
  - **Collapse to Short**: Model emits `<EASY>` for complex questions, resulting in high token reduction but >5% accuracy drop (Table 2 "w/ Force-short" behavior).
  - **Stuck in Long**: Model generates `<Long Trigger>` for all inputs, resulting in 0% token reduction (failure to learn the `<EASY>` token prediction).
- First 3 experiments:
  1.  **Pareto Frontier Plotting**: Replicate Figure 3 (Accuracy vs. Token Length). Vary the rejection sampling `k` to see if the model moves toward the bottom-left (high efficiency, high accuracy).
  2.  **Trigger Ablation**: Force the model to use only `<Short Trigger>` vs. only `<Long Trigger>` (Table 2) to quantify the performance gap filled by the adaptive mechanism.
  3.  **Attention Map Inspection**: Visualize attention (Figure 4) to confirm that early training attends to long reasoning while late training attends to short reasoning, validating the "uncertainty reduction" hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework be extended to support multi-granular reasoning modes rather than just the binary long-short distinction?
- Basis in paper: [explicit] The Limitations section states that "the binary distinction between long and short reasoning paths may not fully capture more fine-grained variations in reasoning complexity."
- Why unresolved: The current architecture relies on a single <EASY> token to trigger a specific short mode, limiting the model's ability to select intermediate reasoning lengths.
- What evidence would resolve it: A modification of the token triggering mechanism to support a spectrum of lengths, showing improved efficiency-accuracy trade-offs on datasets with high variance in difficulty.

### Open Question 2
- Question: How does AutoL2S perform in domains where obtaining verified paired long-short reasoning data is costly or infeasible?
- Basis in paper: [explicit] The authors note that "performance depends on the availability and quality of paired long and short reasoning annotations, which may be costly to obtain for some domains."
- Why unresolved: The method relies on rejection sampling to generate verified short paths (S) for supervision, which may not scale to specialized domains lacking strong short-reasoning teachers.
- What evidence would resolve it: Experiments applying AutoL2S to low-resource domains using synthetic or weak supervision for short paths, analyzing the degradation of the switching mechanism.

### Open Question 3
- Question: Do the efficiency gains from AutoL2S persist under alternative inference paradigms, such as speculative or non-autoregressive decoding?
- Basis in paper: [explicit] The paper states, "Our efficiency measurements focus on autoregressive decoding settings, and the gains may differ under alternative inference paradigms."
- Why unresolved: The reduction in token count directly correlates to speed in standard autoregressive decoding, but the relationship between token count and latency is different in parallelized or speculative decoding setups.
- What evidence would resolve it: Benchmarks measuring wall-clock time and throughput for AutoL2S models when served using speculative decoding frameworks.

## Limitations
- Verification dependency: The entire switching mechanism hinges on the rejection sampling process that identifies valid short reasoning paths, with no details on verification methodology or precision/metrics.
- Distribution shift risk: The model learns to predict when to use short reasoning based on training data characteristics, but real-world question distributions may differ significantly.
- Performance ceiling unclear: While demonstrating strong efficiency gains, the framework trades accuracy for efficiency without clearly establishing where this trade-off becomes unacceptable for practical applications.

## Confidence
- High Confidence (Experimental Validation):
  - Token reduction metrics across multiple benchmarks (MATH, AIME, SVAMP, GSM8K)
  - Runtime latency improvements (up to 59.6% reduction)
  - Ablation studies showing the necessity of both SFT and RL stages
- Medium Confidence (Theoretical Claims):
  - Lemma 1 regarding mutual information between long and short reasoning paths (proof not provided in paper)
  - The attention map interpretation showing "uncertainty reduction" mechanism (visual correlation doesn't prove causation)
  - GRPO optimization dynamics (implementation details sparse)
- Low Confidence (Implementation Details):
  - Exact rejection sampling verification methodology
  - Trigger detection mechanism at inference (how `<EASY>` is identified as a routing signal)
  - Hyperparameter sensitivity, particularly for KL constraint in RL stage

## Next Checks
1. **Verification Pipeline Audit**: Reconstruct the rejection sampling process and measure precision/recall of identifying valid short reasoning paths. Test whether the verification methodology introduces systematic bias toward certain problem types or complexity levels.

2. **Distribution Shift Experiment**: Train the model on a balanced dataset of easy/complex questions, then evaluate on datasets with varying proportions of each type. Measure how performance degrades as the deployment distribution diverges from training distribution.

3. **Error Analysis of Switching Decisions**: For questions where the model incorrectly chooses short reasoning, analyze whether failures cluster around specific mathematical domains or reasoning patterns. Determine if the switching mechanism has predictable failure modes that could be mitigated through additional supervision.