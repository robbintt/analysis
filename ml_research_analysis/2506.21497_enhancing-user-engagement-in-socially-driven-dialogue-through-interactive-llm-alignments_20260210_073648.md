---
ver: rpa2
title: Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM
  Alignments
arxiv_id: '2506.21497'
source_url: https://arxiv.org/abs/2506.21497
tags:
- user
- engagement
- interactive
- simulator
- conversation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the challenge of enhancing user engagement\
  \ in interactive LLM applications like emotional support and persuasive dialogues.\
  \ Instead of focusing on knowledge or dialogue act planning, the authors propose\
  \ aligning LLMs using a direct engagement signal\u2014the user's future reaction\
  \ tied to the dialogue's intended goal."
---

# Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments

## Quick Facts
- **arXiv ID:** 2506.21497
- **Source URL:** https://arxiv.org/abs/2506.21497
- **Reference count:** 19
- **Primary result:** i×MCTS + DPO alignment significantly improves simulated user engagement in emotional support (64.06% → 80.47%) and persuasion (donation $0.58 → $1.29).

## Executive Summary
This paper addresses the challenge of enhancing user engagement in interactive LLM applications such as emotional support and persuasion dialogues. The authors propose a novel approach that aligns LLMs using future user reactions tied to dialogue goals, rather than turn-level optimization. They develop a user simulator and employ an MCTS-based method (i×MCTS) to explore high-quality conversation trajectories, generating preference data that is used to fine-tune the LLM via Direct Preference Optimization (DPO). Experiments demonstrate substantial improvements in engagement metrics across two distinct tasks while maintaining conversation length.

## Method Summary
The method combines user simulation with Monte Carlo Tree Search to explore conversation space and generate preference data. A user simulator interacts with the target LLM, and i×MCTS explores trajectories to identify high-engagement responses. Preference pairs (chosen vs rejected responses) are extracted from successful and unsuccessful paths, then used to fine-tune the LLM using Direct Preference Optimization. The approach focuses on future outcome alignment rather than intermediate turn-level rewards.

## Key Results
- Emotional support model engagement rate increased from 64.06% to 80.47%
- Persuasion model donation amount doubled from $0.58 to $1.29
- Conversation length remained consistent across models
- Significant improvements achieved in both tasks using the same alignment framework

## Why This Works (Mechanism)

### Mechanism 1: Future Outcome Alignment via DPO
Optimizing LLMs with preferences derived from simulated future user reactions may improve engagement more effectively than turn-level optimization. A user simulator interacts with the target model, i×MCTS explores conversation paths, and DPO tunes the model to prefer responses associated with successful future outcomes. Core assumption: the user simulator accurately models real user behaviors and the end-of-conversation engagement signal is a reliable proxy for interaction quality.

### Mechanism 2: Scalable Interaction Exploration with i×MCTS
Exploring interaction space via structured search allows discovery of high-engagement dialogue strategies missed by standard fine-tuning. i×MCTS builds a search tree, expands promising paths, and uses pruning to manage computational cost. The process generates diverse trajectories essential for creating preference datasets. Core assumption: the chosen pruning heuristic (e.g., cosine similarity, sentiment) validly proxies potential for high engagement.

### Mechanism 3: Domain-Specific Engagement Signals
Directly optimizing for task's core objective provides clearer learning signals than intermediate proxies. Engagement is defined differently per task: binary expression detection for emotional support via regex, and donation amount for persuasion. This reward propagates through conversation trees. Core assumption: end-of-conversation signals are attributable to turn-by-turn decisions and can be effectively learned via DPO.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** DPO is the core algorithm used to fine-tune the LLM, replacing RLHF by directly optimizing on preference data and simplifying training.
  - **Quick check question:** Can you explain how DPO uses a preference dataset of (chosen, rejected) pairs to shift the probability mass of a language model's policy without training a separate reward model?

- **Concept: Monte Carlo Tree Search (MCTS)**
  - **Why needed here:** MCTS is the search algorithm used to explore conversation space, with four phases—selection, expansion, rollout, and backpropagation—key to generating training data.
  - **Quick check question:** In the context of this paper, what serves as the "rollout" and what is the "reward" that gets backpropagated through the tree nodes?

- **Concept: User Simulation**
  - **Why needed here:** The entire data generation pipeline relies on a user simulator, making understanding its capabilities and limitations critical for evaluating real-world applicability.
  - **Quick check question:** How is the user simulator trained, and what are the two primary heuristics used to evaluate its simulated engagement in the two different tasks?

## Architecture Onboarding

- **Component map:** User Simulator (Help-Seeker/Persuadee LLM) -> i×MCTS Module -> Target Interactive LLM (Supporter/Persuader LLM) -> DPO Trainer
- **Critical path:**
  1. SFT Pre-training: Train both User Simulator and Target LLM on task datasets
  2. Preference Data Generation: Run i×MCTS, explore conversations, prune paths, assign rewards, pair successful/unsuccessful paths
  3. DPO Alignment: Fine-tune Target LLM using generated preference dataset, possibly augmented with reward model ranked pairs
- **Design tradeoffs:**
  - Simulator Fidelity vs. Real-World Generalization: High-fidelity simulators crucial for useful data but risk overfitting to simulator artifacts
  - Search Breadth/Depth vs. Compute Cost: MCTS parameters directly control data quality vs. generation computational cost
  - Simple vs. Complex Rewards: Regex-based binary signals are efficient but may be coarse proxies missing subtle positive interactions
- **Failure signatures:**
  - Reward Hacking: Model learns to game simulator or regex-based reward rather than genuinely engaging
  - Simulator Gap: Simulator responses not representative of real users leads to poor human evaluation performance
  - Pruning Bias: Overly aggressive or poorly chosen pruning prevents learning unconventional but effective strategies
- **First 3 experiments:**
  1. Ablation on Reward Signal: Replace future-outcome reward with turn-level rewards to measure contribution of long-term signal
  2. Cross-Task Generalization: Train using emotional support preference data, evaluate on persuasion task to test strategy specificity
  3. Human-in-the-Loop Validation: Conduct user study comparing SFT and DPO-aligned models to validate simulated engagement improvements

## Open Questions the Paper Calls Out
1. How can interactive LLMs be effectively adapted for personalized engagement, given challenges in collecting specific data and training few-shot user simulators?
2. Does relying on SFT-based user simulators for reward modeling introduce "reward hacking" or distributional bias not present in real human interactions?
3. Can integrating intermediate, process-based rewards improve alignment efficiency compared to current outcome-based rewards?

## Limitations
- Primary reliance on user simulator that may not accurately capture human behavior complexity
- Use of simple reward functions (regex-based binary signals, donation amounts) may miss nuanced engagement forms
- Computational cost of i×MCTS for preference data generation could limit scalability
- Lack of human evaluation to confirm simulated engagement improvements translate to real-world satisfaction

## Confidence
- **High Confidence:** Core methodology (MCTS + DPO) is technically sound and well-established; reported experimental results are internally consistent
- **Medium Confidence:** Improvements in simulated engagement are likely real but practical significance for real users is uncertain without human evaluation
- **Low Confidence:** Generalizability to real-world scenarios and users not represented in simulator training data is highly uncertain; potential for reward hacking is significant concern

## Next Checks
1. Conduct human evaluation comparing SFT and DPO-aligned models, measuring user-reported satisfaction, perceived empathy, and interaction naturalness
2. Perform ablation study systematically replacing future-outcome reward with turn-level rewards to isolate contribution of long-term engagement signal
3. Test robustness with diverse simulators by training multiple simulators with different architectures/training data and using their collective preferences to assess consistency vs. overfitting