---
ver: rpa2
title: Robustness of AutoML on Dirty Categorical Data
arxiv_id: '2602.00412'
source_url: https://arxiv.org/abs/2602.00412
tags:
- categorical
- automl
- features
- data
- encoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of AutoML methods on dirty
  categorical data, which often have high-cardinality features arising from issues
  like lack of curation and automated data collection. The authors propose a pipeline
  that transforms categorical data into numerical data using morphological encoders
  (similarity encoding, min-hash encoding, and Gamma-Poisson encoding) so that AutoML
  can handle categorical data with advanced encoding schemes.
---

# Robustness of AutoML on Dirty Categorical Data

## Quick Facts
- **arXiv ID**: 2602.00412
- **Source URL**: https://arxiv.org/abs/2602.00412
- **Reference count**: 28
- **Primary result**: AutoML performance improves significantly when using morphological encoders for dirty categorical data

## Executive Summary
This paper investigates how AutoML systems handle dirty categorical data with high-cardinality features. The authors propose a preprocessing pipeline that uses morphological encoders (similarity encoding, min-hash encoding, and Gamma-Poisson encoding) to transform categorical data into numerical features that AutoML can process more effectively. Experiments using the GAMA AutoML system on seven dirty datasets demonstrate that while GAMA can process most datasets without the proposed pipeline, performance improves significantly when the pipeline is applied. For instance, accuracy on the Midwest dataset increased from 0.50 to 0.68, and GAMA produced models for previously problematic datasets like Trafficviolations and Metobjects.

## Method Summary
The proposed pipeline uses ptype to infer feature types, then applies dirty-cat's SuperVectorizer to morphologically encode the most predictive categorical feature (identified via Random Forest feature importance). The encoded dataset is then passed to GAMA AutoML for Combined Algorithm Selection and Hyperparameter optimization (CASH) with a 1-hour budget per dataset. The pipeline specifically uses similarity encoding for ≤40 levels, and min-hash or GAP encoding for higher cardinality. Experiments were conducted on seven dirty datasets using 75/25 train/validation splits with accuracy as the primary metric.

## Key Results
- GAMA processed most datasets without preprocessing but showed significant performance gains with the pipeline
- Midwest dataset accuracy improved from 0.50 to 0.68 when using the proposed pipeline
- GAMA produced models for Trafficviolations and Metobjects (previously problematic datasets) only when using the pipeline
- Tree-based models (Random Forest, Gradient Boosting) performed best on the Midwest dataset

## Why This Works (Mechanism)

### Mechanism 1
Morphological encoders capture structural similarity in dirty categorical strings that standard encoders miss. These encoders transform categorical levels into numeric representations by analyzing sub-string patterns through n-grams, string distance measures, and latent topic learning. The core assumption is that dirty categorical values contain morphological structure (typos, acronyms, partial matches) that correlates with the target.

### Mechanism 2
The preprocessing pipeline enables AutoML to handle dirty categorical data by decoupling encoding from model selection. The pipeline first infers feature types, isolates categorical features, applies morphological encoders based on cardinality heuristics, and passes transformed numeric data to AutoML. This allows AutoML's CASH optimization to operate on a cleaner feature space.

### Mechanism 3
Tree-based models are particularly effective on morphologically-encoded dirty categorical data because they can handle high-dimensional sparse features without requiring feature scaling. They can partition on individual topic activations or similarity dimensions, capturing non-linear interactions in the encoded features.

## Foundational Learning

- **Concept: High-cardinality categorical features**
  - Why needed here: Dirty datasets often have categorical features with many distinct levels, making standard one-hot encoding problematic due to extreme sparsity and dimensionality explosion
  - Quick check question: Given a categorical column with 50,000 unique string values (many with typos), what problems arise with one-hot encoding?

- **Concept: The CASH problem (Combined Algorithm Selection and Hyperparameter optimization)**
  - Why needed here: AutoML systems solve CASH by searching over algorithms + hyperparameters + preprocessing, explaining why adding morphological encoders expands the search space
  - Quick check question: If you add 3 new encoder options to an AutoML system with 10 algorithms and 5 hyperparameters each, how does the search space change?

- **Concept: String similarity and n-gram representations**
  - Why needed here: Morphological encoders rely on n-gram extraction and string distance metrics, making understanding how "3-gram Jaccard similarity" works essential
  - Quick check question: What is the 2-gram representation of "hello"? How would Jaccard similarity compare it to "hallo"?

## Architecture Onboarding

- **Component map**: Raw dirty dataset -> [ptype] Feature type inference -> identifies categorical columns -> [SuperVectorizer] Heuristic encoder selection -> Encoded numeric dataset -> [GAMA AutoML] CASH optimization -> Best pipeline + ensemble of evaluated pipelines

- **Critical path**:
  1. Type inference accuracy: If ptype misclassifies a dirty categorical column as numeric, it bypasses morphological encoding entirely
  2. Encoder selection threshold: The 40-level cutoff for similarity vs. min-hash/GAP is heuristic; wrong choice causes dimensionality explosion or information loss
  3. Most predictive feature selection: Paper experiments encode only the most predictive categorical feature; encoding all high-cardinality columns may exceed time/memory budget

- **Design tradeoffs**:
  - Accuracy vs. compute: Similarity encoding is accurate but O(n²) in categories; min-hash is O(n) but approximates
  - Feature count vs. information: GAP creates interpretable topics but fewer features than one-hot; similarity encoding can create n features per categorical column
  - Encoding scope: Paper encodes only the top predictive feature to control dimensionality; full encoding may improve accuracy but risks timeout

- **Failure signatures**:
  - Timeout errors without model output: Table 2 shows GAMA produced 0 pipelines for Trafficviolations and Metobjects without the proposed pipeline
  - Low accuracy with high log-loss: Midwest baseline shows 0.50 accuracy with 1.46 log-loss, indicating uncertain/conflicted predictions
  - Dimensionality explosion: One-hot on high-cardinality columns causes memory issues; morphological encoders may still create too many features if misapplied

- **First 3 experiments**:
  1. Baseline diagnostic: Run GAMA on a dirty dataset (e.g., Midwest) without preprocessing. Record accuracy, log-loss, timeout occurrences, and pipeline count.
  2. Single-feature encoding test: Identify the most predictive categorical feature via Random Forest importance, encode only that feature using SuperVectorizer, then run GAMA. Compare accuracy and pipeline diversity against baseline.
  3. Encoder ablation: Replace SuperVectorizer's heuristic selection with fixed encoders (similarity only, min-hash only, GAP only) on the same feature. Measure performance and runtime to validate heuristic thresholds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed pipeline maintain its performance benefits when encoding all dirty categorical features, rather than just the single most predictive one?
- Basis in paper: The conclusion states there is "room for future evaluations on the AutoML performance if more categorical features were converted as well."
- Why unresolved: The experiments restricted the encoding step to only the single feature determined to be most predictive by an external Random Forest model.

### Open Question 2
- Question: Do the robustness improvements generalize to AutoML systems that use optimization strategies other than genetic programming?
- Basis in paper: The authors note they plan to "extend the evaluation to other AutoML methods."
- Why unresolved: The entire empirical evaluation relied exclusively on GAMA, a genetic programming-based AutoML tool.

### Open Question 3
- Question: Is the proposed pipeline effective for regression tasks, or is its utility limited to classification?
- Basis in paper: The authors explicitly state they "plan to use regression datasets" in future work to create a broader benchmark.
- Why unresolved: The paper's methodology and results were restricted entirely to classification datasets.

## Limitations
- Dataset availability and preprocessing details are not fully specified, making exact reproduction challenging
- The 40-level threshold for encoder selection is heuristic and may not generalize across domains
- Only encoding the most predictive categorical feature limits potential improvements and may miss interactions

## Confidence
- **High**: Morphological encoders improve AutoML performance on dirty categorical data (supported by direct experimental results)
- **Medium**: Tree-based models are particularly effective on morphologically-encoded features (supported by pipeline analysis but not rigorously tested)
- **Low**: The proposed pipeline enables AutoML to handle previously problematic datasets (inferred from results but causality not fully established)

## Next Checks
1. **Statistical significance testing**: Perform paired t-tests or Wilcoxon signed-rank tests comparing baseline vs. pipeline accuracy across datasets
2. **Encoder ablation study**: Test performance when encoding all high-cardinality categorical features vs. only the most predictive one
3. **Generalization test**: Apply the pipeline to additional dirty categorical datasets from different domains to assess robustness