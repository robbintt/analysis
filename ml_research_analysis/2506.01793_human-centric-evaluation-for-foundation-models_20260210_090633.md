---
ver: rpa2
title: Human-Centric Evaluation for Foundation Models
arxiv_id: '2506.01793'
source_url: https://arxiv.org/abs/2506.01793
tags:
- evaluation
- subjective
- performance
- arxiv
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the limitations of objective benchmarks in
  evaluating foundation models, which fail to capture authentic human experiences
  and subjective perceptions. The authors propose a Human-Centric Evaluation (HCE)
  framework that assesses models across three dimensions: problem-solving ability,
  information quality, and interaction experience through human-rated experiments.'
---

# Human-Centric Evaluation for Foundation Models

## Quick Facts
- arXiv ID: 2506.01793
- Source URL: https://arxiv.org/abs/2506.01793
- Reference count: 26
- Deepseek R1, OpenAI o3 mini, Grok 3, and Gemini 2.5 ranked across 540+ subjective evaluations using a 9-dimension framework

## Executive Summary
This study addresses the limitations of objective benchmarks in evaluating foundation models, which fail to capture authentic human experiences and subjective perceptions. The authors propose a Human-Centric Evaluation (HCE) framework that assesses models across three dimensions: problem-solving ability, information quality, and interaction experience through human-rated experiments. Testing Deepseek R1, OpenAI o3 mini, Grok 3, and Gemini 2.5 across 540+ participant-driven evaluations revealed distinct model strengths and adaptability patterns. Grok 3 achieved the highest overall score (4.30/5), followed by Deepseek R1 and Gemini 2.5 (4.08-4.09/5), while OpenAI o3 mini performed comparatively poorly (3.91/5). The framework and resulting dataset offer a novel approach for standardized, automated subjective evaluation, advancing LLM development for research and practical applications.

## Method Summary
The HCE framework conducts human-centric subjective evaluation through 20-minute free-form dialogue sessions where participants complete open-ended research tasks using foundation models. The study employs 540 evaluations across 4 models (Deepseek R1, OpenAI o3 mini, Grok 3, Gemini 2.5) using official web interfaces, with tasks spanning 8 disciplines and 2 problem types. Participants rate models on 9 evaluation dimensions (5-point Likert scale) across 3 categories: Problem-solving Ability (Analytical Accuracy, Comprehensiveness, Assistance Efficiency), Information Quality (Reliability, Exploration Depth), and Interaction Experience (Content Relevance, Feedback Adaptability, Expression Naturalness, Response Timeliness). The scoring aggregates through arithmetic mean per dimension/model.

## Key Results
- Grok 3 achieved the highest overall score (4.30/5) among tested models
- OpenAI o3 mini performed comparatively poorly (3.91/5) despite leading in timeliness
- Law discipline showed the largest cross-model performance gap (0.57 points), while medicine showed the smallest
- Advanced reasoning features (DeepThink/DeepSearch) exhibited significant limitations, resulting in longer processing times and outputs falling short of evaluator expectations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Open-ended human-AI collaboration tasks may reveal performance dimensions that static objective benchmarks miss.
- Mechanism: The HCE framework replaces quiz-style evaluation with a time-limited free-form interaction (20 minutes) where participants complete research tasks through multi-turn dialogue. This design exposes contextual adaptability, feedback responsiveness, and task persistence—qualities not captured by single-turn accuracy metrics.
- Core assumption: Participants can reliably distinguish model quality differences through subjective experience within a 20-minute window.
- Evidence anchors:
  - [abstract] "humans and models collaborate on open-ended research tasks, yielding a comprehensive subjective dataset"
  - [section 3.4] "engages in multiple rounds of multilingual Q&A within set time limits to gradually develop task solutions"
  - [corpus] Weak direct evidence—neighbor papers focus on domain-specific benchmarks, not methodology validation.
- Break condition: If inter-rater reliability is low or task completion rates fall below 50%, the mechanism degrades.

### Mechanism 2
- Claim: Multi-dimensional scoring (9 sub-metrics across 3 categories) enables finer-grained model differentiation than aggregate scores.
- Mechanism: The framework decomposes evaluation into problem-solving ability (analytical accuracy, comprehensiveness, assistance efficiency), information quality (reliability, exploration depth), and interaction experience (content relevance, feedback adaptability, expression naturalness, response timeliness). This structure reveals specific weaknesses—e.g., OpenAI o3 mini scored highest on timeliness but lowest on comprehensiveness.
- Core assumption: Human raters can consistently distinguish between these 9 dimensions without confounding effects.
- Evidence anchors:
  - [section 3.2] "our subjective evaluation of Problem-solving Ability adopts three dimensions: analytical accuracy, comprehensiveness and assistance efficiency"
  - [table 2a] Shows dimension-level variance: OpenAI o3 mini scores 4.05 on assistance efficiency but 3.66 on comprehensiveness.
  - [corpus] No direct validation of the 9-dimension structure in neighbor papers.
- Break condition: If dimensions correlate strongly (r > 0.85), the decomposition adds no discriminative value.

### Mechanism 3
- Claim: Disciplinary diversity in task design may surface model specialization patterns invisible to single-domain benchmarks.
- Mechanism: Eight disciplines (computer science, law, finance, medicine, sociology, environmental engineering, biology, education) combined with two problem types (literature synthesis, innovation-driven) create 16 task categories. Results show law has the largest cross-model performance gap (0.57 points) while medicine shows the smallest.
- Core assumption: Student evaluators have sufficient domain expertise to judge model outputs in their selected discipline.
- Evidence anchors:
  - [section 4.2] "law showing the largest performance gap among models and medicine the smallest"
  - [table 2b] Grok 3 leads in 7/8 disciplines; OpenAI o3 mini underperforms in law (3.84) relative to Grok 3 (4.41).
  - [corpus] Neighbor paper "DeepSeek-R1 Outperforms Gemini 2.0 Pro" shows domain-specific variations in ophthalmology, suggesting specialization is plausible.
- Break condition: If evaluators lack domain knowledge, ratings reflect surface fluency rather than substantive quality.

## Foundational Learning

- Concept: **Model-centric vs. Human-centric Evaluation**
  - Why needed here: The paper's core argument is that existing benchmarks (MMLU, SWE-bench) optimize for objective correctness but miss user experience dimensions. Understanding this distinction is prerequisite to interpreting why HCE uses subjective ratings.
  - Quick check question: Can you name one subjective dimension that MMLU cannot measure?

- Concept: **Multi-turn Dialogue Evaluation**
  - Why needed here: HCE rates models on "feedback adaptability" and "comprehensiveness"—qualities that emerge only across multiple interaction turns. Single-turn benchmarks cannot assess these.
  - Quick check question: Why might a model score well on accuracy but poorly on feedback adaptability?

- Concept: **Disciplinary Adaptability**
  - Why needed here: The paper shows models have non-uniform performance across domains (e.g., Gemini 2.5 excels in finance but trails in education). This suggests model selection should be task-contextual.
  - Quick check question: Which model would you select for a legal research task based on Table 2b?

## Architecture Onboarding

- Component map:
  Task Layer (8 disciplines × 2 problem types = 16 task categories) → Interaction Layer (20-minute free-form dialogue, bilingual) → Evaluation Layer (9 sub-metrics aggregated into 3 dimensions) → Data Layer (540 evaluations, 5-point Likert scale)

- Critical path:
  1. Participant selects task based on major/interest
  2. Participant interacts freely with assigned model for 20 minutes
  3. Participant completes questionnaire rating all 9 sub-metrics
  4. Scores aggregated via arithmetic mean

- Design tradeoffs:
  - Breadth vs. depth: 8 disciplines provide coverage, but 5 evaluators per language per model limits statistical power for any single domain.
  - Open-ended vs. standardized: Free-form interaction captures authentic experience but complicates reproducibility.
  - Assumption: Student evaluators are described as "proficient" but no calibration data is provided.

- Failure signatures:
  - High variance within evaluator groups for the same model-task pair suggests unclear scoring criteria.
  - Consistent low scores on "exploration depth" across all models may indicate task design issues rather than model limitations.
  - Long response times combined with low depth scores (noted for DeepThink/DeepSearch features) indicate efficiency-quality tradeoffs.

- First 3 experiments:
  1. Replicate a single discipline (e.g., computer science) with 20 evaluators per model to establish baseline inter-rater reliability.
  2. Run ablation: compare 20-minute vs. 10-minute interaction windows to test time-sensitivity of scoring.
  3. Add a "calibration" round where evaluators rate a shared reference output before main tasks to reduce individual bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can AI-driven initial scoring be effectively integrated with human feedback to automate the HCE framework while reducing evaluation costs?
- Basis in paper: [explicit] The conclusion states that future work involves "automating subjective evaluation by combining AI-driven initial scoring with human feedback," but does not provide the methodology for this integration.
- Why unresolved: The current study relies entirely on human participants for scoring, and the mechanism for an AI "initial score" within a subjective paradigm is undefined.
- What evidence would resolve it: A study demonstrating a high correlation between a hybrid AI-human scoring system and the pure human evaluation baseline used in this paper.

### Open Question 2
- Question: Can a dedicated evaluation model successfully integrate HCE subjective metrics with objective criteria to provide comprehensive, scalable assessments?
- Basis in paper: [explicit] The authors explicitly list "developing a dedicated evaluation model that integrates subjective metrics with objective criteria" as a direction for future work to support robust optimization.
- Why unresolved: The paper establishes the subjective framework and dataset but does not propose or test an architecture that bridges the gap between subjective experience and objective data.
- What evidence would resolve it: The proposal and validation of a new model architecture that can predict HCE scores (subjective) while optimizing for benchmarks like MMLU (objective).

### Open Question 3
- Question: Why do advanced reasoning features like DeepThink and DeepSearch result in suboptimal interaction experiences compared to standard modes in research tasks?
- Basis in paper: [explicit] The "Other Findings" section notes that these features "exhibit significant limitations," resulting in longer processing times and outputs that fell short of evaluator expectations.
- Why unresolved: The paper identifies the phenomenon where advanced features lower subjective satisfaction but does not analyze the specific behavioral or output failures causing this.
- What evidence would resolve it: A comparative error analysis of model outputs in "DeepThink" mode versus standard mode, specifically measuring the trade-off between response latency and information depth.

## Limitations
- High inter-evaluator variance due to subjective interpretation without reliability metrics reported
- Only 3-5 ratings per model-task-language combination limits statistical power for domain-specific conclusions
- Claim that participants can reliably distinguish model quality within 20 minutes lacks validation

## Confidence

- **High**: Relative model rankings within the study (Grok 3 > Deepseek R1 ≈ Gemini 2.5 > OpenAI o3 mini) are internally consistent across metrics and disciplines.
- **Medium**: Cross-disciplinary performance patterns (law showing largest gaps, medicine smallest) appear plausible but require replication with more domain-expert evaluators.
- **Low**: The framework's generalizability to other model families and task types remains untested.

## Next Checks

1. **Inter-rater Reliability Validation**: Replicate 50 evaluations from the dataset and compute ICC scores for each dimension. Target ICC > 0.7 to establish rating consistency.

2. **Expert Domain Validation**: Recruit 10 domain experts per discipline (vs. student evaluators) to re-rate a subset of model outputs. Compare expert vs. student scoring patterns to assess domain knowledge impact.

3. **Time Sensitivity Test**: Conduct parallel 10-minute and 20-minute evaluation sessions with the same participants and tasks. Analyze whether dimension scores converge or diverge based on interaction duration.