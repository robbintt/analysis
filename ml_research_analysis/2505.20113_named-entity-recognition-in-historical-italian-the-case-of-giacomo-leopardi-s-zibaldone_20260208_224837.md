---
ver: rpa2
title: 'Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi''s
  Zibaldone'
arxiv_id: '2505.20113'
source_url: https://arxiv.org/abs/2505.20113
tags:
- historical
- entity
- texts
- such
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of Named Entity Recognition
  (NER) in historical Italian texts, focusing on Giacomo Leopardi's Zibaldone. The
  authors propose a new dataset containing 2,899 manually annotated references to
  people, locations, and literary works from the 19th century.
---

# Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone

## Quick Facts
- arXiv ID: 2505.20113
- Source URL: https://arxiv.org/abs/2505.20113
- Reference count: 32
- This paper addresses NER in historical Italian texts, showing fine-tuned models significantly outperform LLMs (68.98% vs 30.71% F1-score)

## Executive Summary
This paper tackles the challenge of Named Entity Recognition in 19th-century Italian scholarly texts, specifically Giacomo Leopardi's Zibaldone. The authors create a new dataset of 2,899 manually annotated references and compare fine-tuned domain-specific models against instruction-tuned large language models. Their results demonstrate that compact fine-tuned models significantly outperform larger LLMs, achieving 68.98% F1-score compared to 30.71% for LLaMa3.1. The study concludes that fine-tuned models are more effective for NER in historical literary texts, though human supervision remains necessary for complete accuracy.

## Method Summary
The authors propose a new dataset containing 2,899 manually annotated references to people, locations, and literary works from Leopardi's Zibaldone. They compare LLaMa3.1-8B-Instruct (zero-shot) with a domain-specific fine-tuned BERT-based model (GliNER). Training data consisted of 688 notes (pp. 1000-2001 & 3001-4000), filtered for length ≤ 350 tokens. Testing used 260 notes (pp. 2700-3000). The GliNER model was fine-tuned with specific learning rates ($5 \times 10^{-6}$ for NER components, $1 \times 10^{-5}$ for transformer backbone) to prevent catastrophic forgetting while adapting to the historical Italian domain.

## Key Results
- Fine-tuned GliNER model achieves 68.98% F1-score (exact) and 75.64% (fuzzy), compared to 30.71% and 33.78% for LLaMa3.1
- The WORK class presents the highest difficulty due to metonymic ambiguity (author names substituting for work titles)
- GliNER fine-tuned consistently outperforms other models, reaching 92% precision in fuzzy matching
- Exact span detection remains challenging for LLMs, with GliNER showing superior boundary precision

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning compact encoder models on domain-specific historical data yields higher entity recognition fidelity than relying on generalist instruction-tuned LLMs.
- **Mechanism:** The model updates its weights to align vector representations of non-standard orthography (e.g., "Venez." for Venice) and archaic syntax with the target entity classes, effectively narrowing the distribution gap between 19th-century scholarly Italian and modern training corpora.
- **Core assumption:** The training split (pp. 1000-2001) shares sufficient lexical and stylistic similarity with the evaluation split (pp. 2700-3000) to allow effective generalization rather than mere memorization.

### Mechanism 2
- **Claim:** Span-based classification architectures (GliNER) handle exact boundary detection more robustly than generative instruction-following approaches.
- **Mechanism:** GliNER explicitly models entity spans and class vectors in a shared embedding space, optimizing boundaries directly. In contrast, generative LLMs must produce exact text markers (e.g., `<type>`) which are susceptible to hallucination or misalignment with the source token stream.

### Mechanism 3
- **Claim:** Recognition of bibliographic references ("WORK") relies heavily on resolving metonymic ambiguity where an author's name substitutes for their work.
- **Mechanism:** The paper suggests models often misclassify references like "Il Forcellini" (a person) as the entity, when the text implies the "Lexicon Totius Latinitatis" (a work). Correct classification requires external knowledge bases or broader context windows to resolve this specific semantic ambiguity.

## Foundational Learning

- **Concept: Exact vs. Fuzzy Matching**
  - **Why needed here:** The study reports a significant gap between these metrics (e.g., LLaMa generative jumps from 30.71% to 33.78%). Understanding this distinction is critical to distinguish whether a model fails to identify an entity at all or merely miscalculates its start/end character indices.
  - **Quick check question:** If a model identifies "Degli Scritt." as a work when the ground truth is "Degli Scritt. del Trecento," does this count as a True Positive in fuzzy matching?

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** The authors explicitly select a low learning rate ($5 \times 10^{-6}$) for the NER components to prevent the model from losing its pre-trained linguistic knowledge while adapting to the Zibaldone dataset.
  - **Quick check question:** Why would fine-tuning a model on a small dataset of 19th-century notes cause it to perform poorly on modern Italian text?

- **Concept: Zero-shot Generalization**
  - **Why needed here:** The paper contrasts the zero-shot capabilities of LLaMa3.1 and a base GliNER model against the fine-tuned version. This concept defines the baseline performance one can expect without investing in data annotation.
  - **Quick check question:** Does the "zero-shot" setting require providing the model with examples of "PER", "LOC", and "WORK" in the prompt, or only definitions of these classes?

## Architecture Onboarding

- **Component map:** HTML scraper -> Text/CSV extraction -> Annotation Store (CSVs with doc_id, surface, start_pos, end_pos, type) -> Model A (LLaMa3.1 with Prompt Engineering -> Regex Parser) OR Model B (GliNER-ITA-BASE -> Fine-tuning Loop -> Span Predictor) -> Evaluator (Micro/Macro F1 calculator with Exact/Fuzzy modes)

- **Critical path:** The filtering of the training dataset is the most sensitive step. The authors restricted training to notes ≤ 350 tokens to ensure "concentrated" information. Deviating from this data curation strategy may introduce noise that lowers the model's precision on the specific task of recognizing abbreviated works.

- **Design tradeoffs:**
  - **LLM Approach:** High recall potential on easy classes (PER), low infrastructure requirements for training (inference-only), but poor boundary precision (low exact F1)
  - **GliNER Approach:** High precision/overall F1, requires GPU training (A6000 used) and data labeling, but smaller footprint (90M params) and faster inference

- **Failure signatures:**
  - Low Exact Match + High Fuzzy Match: Indicates the model is detecting the concept but failing to capture the full abbreviation (e.g., predicting "Cic." instead of "Cic. pro Roscio")
  - PER/LOC Confusion: Confusing "Il Forcellini" (Person) with his work (Work) or non-capitalized toponyms (e.g., "italia") with common nouns

- **First 3 experiments:**
  1. Reproducing Zero-Shot Baselines: Run LLaMa3.1-8B-Instruct on the evaluation set (pages 2700-3000) using the extractive prompt to confirm the ~32-38% F1 baseline
  2. Ablation on Entity Classes: Fine-tune GliNER on only PER and LOC, excluding WORK, to quantify the difficulty introduced by bibliographic references
  3. Error Analysis on "WORK" class: Manually inspect the 50 lowest-confidence predictions for the "WORK" class to verify if the "author-as-work" ambiguity is the primary failure mode

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent can automated systems successfully perform Entity Linking (EL) by connecting recognized entities in the Zibaldone to their respective Wikidata entries?
- **Basis in paper:** The authors state that future extensions will "test models not only for entity recognition but also for EL, i.e., the disambiguation of a reference by linking it to its respective Wikidata element."
- **Why unresolved:** The current study strictly evaluates Named Entity Recognition (extracting surface forms) without assessing the ability to disambiguate or link these extractions to a knowledge base.

### Open Question 2
- **Question:** Can integrating temporal and logical constraints from external Knowledge Bases (KBs) reduce errors in recognizing historical entities?
- **Basis in paper:** The authors propose "a methodology for correcting and improving the performance of predictive systems... by integrating them with semantic and logical conditions" derived from KBs like Wikidata or Yago.
- **Why unresolved:** The current experiments rely solely on language model predictions without external validation layers to filter results based on historical plausibility.

### Open Question 3
- **Question:** Does the domain-specific fine-tuning on Leopardi's Zibaldone transfer effectively to other 19th-century Italian literary texts?
- **Basis in paper:** The authors acknowledge the dataset is sampled from a "single author" and specific "scholarly notes," which limits the understanding of how well the fine-tuned model generalizes to other writing styles or authors of the same period.
- **Why unresolved:** It is unclear if the model has learned general historical Italian orthography or if it has overfit to Leopardi's specific vocabulary, abbreviations, and syntax.

## Limitations
- The model is tested on a narrow chronological window (pages 2700-3000) of a single author's work, raising questions about generalizability to other 19th-century Italian texts or different literary genres
- The annotation process relies on manual labeling that may introduce systematic biases, particularly for the WORK class where semantic ambiguity proved challenging
- The reported performance metrics represent the best-case scenario for this specific dataset but don't indicate how these models would perform on raw, uncurated historical texts

## Confidence

**High Confidence Claims:**
- Fine-tuned models outperform instruction-tuned LLMs on this specific historical Italian NER task (supported by clear numerical evidence across multiple metrics)
- The WORK class presents significantly more difficulty than PER and LOC classes due to metonymic ambiguity (consistent across both model types)

**Medium Confidence Claims:**
- Span-based classification architectures are inherently better suited for boundary detection than generative approaches (inferred from performance differences but not directly tested through architectural ablation)
- The 350-token filtering criterion meaningfully improves model performance (supported by training methodology but not experimentally validated)

**Low Confidence Claims:**
- The proposed models would generalize to other historical Italian texts beyond Leopardi's Zibaldone (no cross-corpus validation performed)
- The specific learning rate schedule ($5 \times 10^{-6}$ for NER components) is optimal (chosen based on literature rather than systematic hyperparameter search)

## Next Checks
1. **Cross-Corpus Generalization Test**: Evaluate the fine-tuned GliNER model on another 19th-century Italian text corpus (e.g., correspondence or literary works from a different author) to assess whether the 68.98% F1 score generalizes beyond Leopardi's Zibaldone.

2. **Knowledge Base Augmentation Experiment**: Implement the suggested Knowledge Base lookup for the WORK class and measure the improvement in disambiguation accuracy, particularly for cases where author names substitute for work titles.

3. **Ablation Study on Data Filtering**: Train two versions of the model—one using the 350-token filtered training data and another using all available training data—to empirically validate whether the filtering criterion significantly impacts performance on the test set.