---
ver: rpa2
title: Rademacher learning rates for iterated random functions
arxiv_id: '2506.13946'
source_url: https://arxiv.org/abs/2506.13946
tags:
- learning
- random
- function
- rademacher
- iterated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies supervised machine learning when training data
  are generated by an iterated random function (a time-homogeneous Markov chain) rather
  than i.i.d. observations.
---

# Rademacher learning rates for iterated random functions

## Quick Facts
- arXiv ID: 2506.13946
- Source URL: https://arxiv.org/abs/2506.13946
- Reference count: 39
- This paper establishes uniform convergence and learnability of empirical risk minimization for training data generated by iterated random functions, with rates expressed in terms of Rademacher complexities.

## Executive Summary
This paper addresses supervised machine learning when training data are generated by an iterated random function (a time-homogeneous Markov chain) rather than independent and identically distributed (i.i.d.) observations. The work establishes uniform convergence and learnability of the approximate empirical risk minimization algorithm under contractivity and Lipschitz conditions on the generating function and loss. The key innovation is expressing both uniform convergence and learnability bounds in terms of Rademacher complexities, making them sensitive to the data distribution. The rates are exponential in sample size and involve the Wasserstein distance to the invariant measure.

## Method Summary
The paper develops a theoretical framework for analyzing learning from iterated random functions by establishing uniform convergence bounds for the empirical risk minimization algorithm. The approach uses Rademacher complexities to measure the richness of function classes and derive sample complexity bounds. Under contractivity and Lipschitz conditions on the generating function and loss, the paper proves that the empirical risk minimizer converges to the true risk minimizer at rates that depend on the Wasserstein distance between the empirical distribution and the invariant measure. The analysis generalizes previous results that required irreducibility or aperiodicity assumptions.

## Key Results
- Uniform convergence and learnability bounds are established for empirical risk minimization with data generated by iterated random functions
- Sample complexity bounds are expressed in terms of Rademacher complexities, making them distribution-sensitive
- The rates are exponential in sample size and involve the Wasserstein distance to the invariant measure
- The approach generalizes prior results requiring irreducibility or aperiodicity, and connects Rademacher complexity to growth and shattering dimensions in specific cases

## Why This Works (Mechanism)
The paper leverages the mixing properties of time-homogeneous Markov chains to establish concentration inequalities that control the deviation between empirical and true risks. The contractivity and Lipschitz conditions ensure that the chain converges to its invariant measure at a geometric rate, which translates into exponential concentration bounds. The Rademacher complexity provides a data-dependent measure of the function class complexity that adapts to the structure of the Markov chain.

## Foundational Learning
- Iterated random functions: Time-homogeneous Markov chains where each state depends only on the previous state through a fixed function with random noise
  - Why needed: The paper's main focus is learning from data generated by such processes rather than i.i.d. samples
  - Quick check: Verify that the Markov property holds and that the chain has a unique invariant measure

- Rademacher complexity: A measure of the richness of a function class that captures how well it can fit random noise
  - Why needed: Provides distribution-sensitive bounds on uniform convergence that adapt to the data structure
  - Quick check: Calculate the empirical Rademacher complexity for simple function classes like linear functions

- Wasserstein distance: A metric on probability measures that captures the cost of transporting mass between distributions
  - Why needed: Appears in the sample complexity bounds as a measure of convergence to the invariant distribution
  - Quick check: Compute the Wasserstein distance between two simple distributions like Gaussians with different means

## Architecture Onboarding
Component map: Data generation -> Markov chain mixing -> Empirical risk minimization -> Learning bounds
Critical path: The mixing time of the Markov chain determines how quickly the empirical distribution converges to the invariant measure, which directly affects the sample complexity bounds
Design tradeoffs: The paper trades generality (no irreducibility or aperiodicity assumptions) for stronger contractivity and Lipschitz conditions
Failure signatures: If the contractivity condition fails, the exponential concentration bounds may not hold, leading to slower convergence rates
First experiments:
1. Test the bounds on a simple iterated function system with known invariant measure
2. Compare the Rademacher complexity-based bounds to traditional VC-dimension bounds for the same problem
3. Apply the framework to a simple image generation task using iterated function systems

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- The contractivity and Lipschitz assumptions may be too restrictive for many practical learning problems, particularly those involving non-smooth or discontinuous losses
- The results rely on specific mixing properties of the Markov chain that may not be satisfied in all applications, especially for chains with slow convergence to stationarity
- The exponential sample complexity bounds, while theoretically appealing, may be loose for finite sample sizes and could lead to overly conservative sample size requirements in practice

## Confidence
- Theoretical claims: High - The mathematical framework is rigorous and the conditions are clearly stated
- Practical applicability: Medium - The assumptions and bounds may be too stringent for many real-world scenarios
- Connection to applications: Low - The paper mentions iterated function systems for image generation but does not thoroughly explore these applications

## Next Checks
1. Conduct empirical studies to test the tightness of the sample complexity bounds on synthetic and real-world datasets generated by iterated random functions
2. Investigate the impact of violating the contractivity and Lipschitz assumptions on the convergence rates and learning performance
3. Explore alternative learning algorithms beyond empirical risk minimization to see if similar convergence results can be established under the same or relaxed conditions